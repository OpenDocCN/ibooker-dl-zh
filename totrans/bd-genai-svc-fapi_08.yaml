- en: Chapter 5\. Achieving Concurrency in AI Workloads
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章\. 实现AI工作负载的并发性
- en: In this chapter, you will learn more about the role and benefits of asynchronous
    programming in boosting the performance and scalability of your GenAI services.
    As part of this, you’ll learn to manage concurrent user interactions and interface
    with external systems such as databases, implement RAG, and read web pages to
    enrich the context of model prompts. You’ll acquire techniques for effectively
    dealing with I/O-bound and CPU-bound operations, especially when dealing with
    external services or handling long-running inference tasks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解异步编程在提高您的GenAI服务性能和可扩展性方面的作用和好处。作为这部分内容，您将学习如何管理并发用户交互，与外部系统（如数据库）交互，实现RAG，并读取网页以丰富模型提示的上下文。您将获得有效处理I/O绑定和CPU绑定操作的技术，尤其是在处理外部服务或处理长时间运行的推理任务时。
- en: We will also dive into strategies for efficiently handling long-running Generative
    AI inference tasks, including the use of FastAPI event loop for background tasks
    execution.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将深入研究高效处理长时间运行的生成式AI推理任务的策略，包括使用FastAPI事件循环执行后台任务。
- en: Optimizing GenAI Services for Multiple Users
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为多个用户优化GenAI服务
- en: AI workloads are computationally expensive operations that can inhibit your
    GenAI services from serving multiple simultaneous requests. In most production
    scenarios, multiple users will be using your applications. Therefore, your services
    will be expected to serve requests *concurrently* such that multiple overlapping
    tasks can be executed. However, if you’re interfacing with GenAI models and external
    systems such as databases, the filesystem, or the internet, there will be operations
    that can block other tasks from executing on your server. Long-running operations
    that can halt the program execution flow are considered *blocking*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: AI工作负载是计算密集型操作，可能会阻碍您的GenAI服务处理多个并发请求。在大多数生产场景中，多个用户将使用您的应用程序。因此，您的服务预计将能够并发地处理请求，以便多个重叠的任务可以执行。然而，如果您与GenAI模型和外部系统（如数据库、文件系统或互联网）交互，将会有一些操作可能会阻塞其他任务在您的服务器上执行。可以阻止程序执行流程的长时间运行操作被认为是*阻塞*的。
- en: 'These blocking operations can be twofold:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些阻塞操作可能有两方面：
- en: Input/output (I/O) bound
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 输入/输出（I/O）绑定
- en: Where a process has to wait because of data input/output operation, which can
    come from a user, file, database, network, etc. Examples include reading or writing
    a file to a disk, making network requests and API calls, sending or receiving
    data from databases, or waiting for user input.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个进程因为数据输入/输出操作而必须等待时，这些操作可能来自用户、文件、数据库、网络等。例如包括将文件读取或写入磁盘、进行网络请求和API调用、从数据库发送或接收数据，或等待用户输入。
- en: Compute bound
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算密集型
- en: Where a process has to wait because of a compute-intensive operation on CPU
    or GPU. Compute-bound programs push the CPU or GPU cores to their limit by performing
    intensive computations, often blocking them from performing other tasks.^([1](ch05.html#id795))
    Examples include data processing, AI model inference or training, 3D object rendering,
    running simulations, etc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个进程因为CPU或GPU上的计算密集型操作而必须等待时。计算密集型程序通过执行密集型计算将CPU或GPU核心推到极限，通常阻止它们执行其他任务。[1](ch05.html#id795)
    例如包括数据处理、AI模型推理或训练、3D对象渲染、运行模拟等。
- en: 'You have a few strategies to serve multiple users:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您有几种策略来服务多个用户：
- en: System optimization
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 系统优化
- en: For I/O-bound tasks like fetching data from a database, working with files on
    disk, making network requests, or reading web pages
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于I/O绑定任务，如从数据库获取数据、在磁盘上处理文件、进行网络请求或读取网页
- en: Model optimization
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化
- en: For memory- and compute-bound tasks such as model loading and inference
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内存和计算密集型任务，如模型加载和推理
- en: Queuing system
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 队列系统
- en: For handling long-running inference tasks to avoid delays in responding
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理长时间运行的推理任务以避免响应延迟
- en: 'In this section, we will look at each strategy in more detail. To help solidify
    your learning, we will also implement several features together that leverage
    the aforementioned strategies:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更详细地审视每种策略。为了帮助巩固您的学习，我们还将一起实施几个利用上述策略的功能：
- en: Building a *web page scraper* for bulk fetching and parsing of HTTP URLs pasted
    in the chat, so that you can ask your LLM about the content of web pages
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个*网页抓取器*，用于批量获取和解析粘贴在聊天中的HTTP URL，以便您可以询问您的LLM关于网页内容
- en: Adding a *retrieval augmented generation* (RAG) module to your service with
    a self-hosted vector database such as `qdrant` so that you can upload and talk
    to your documents via your LLM service
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过添加一个*检索增强生成*（RAG）模块到你的服务中，并使用如`qdrant`之类的自托管向量数据库，以便你可以通过你的LLM服务上传和与你的文档进行交互
- en: Adding a *batch image generation system* so that you can run image generation
    workloads as background tasks
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个*批图像生成系统*，以便你可以将图像生成工作负载作为后台任务运行
- en: Before I can show you how to build the aforementioned features, we should dive
    deeper into the topic of *concurrency* and *parallelism* as understanding both
    concepts will help you identify the correct strategies to use for your own use
    cases.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我可以向你展示如何构建上述功能之前，我们应该更深入地探讨*并发*和*并行性*的主题，因为理解这两个概念将帮助你确定适用于你自己的用例的正确策略。
- en: '*Concurrency* refers to the ability of a service in handling multiple requests
    or tasks at the same time, without completing one after another. During concurrent
    operations, the timeline of multiple tasks can overlap and may start and end at
    different times.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*并发*指的是服务同时处理多个请求或任务的能力，而无需依次完成。在并发操作期间，多个任务的时序可以重叠，并且可能在不同时间开始和结束。'
- en: In Python, you can implement concurrency with a single CPU core by switching
    between tasks on a single thread (via asynchronous programming) or across different
    threads (via multithreading).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，你可以通过在单个线程（通过异步编程）或不同线程（通过多线程）之间切换任务来实现单CPU核心的并发。
- en: With multiple cores, you can also implement a subset of concurrency called *parallelism*
    where tasks are split among several independent workers (via multiprocessing),
    with each executing tasks simultaneously on their own isolated resources and separate
    processes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个核心，你还可以实现一种称为*并行性*的并发子集，其中任务被分配给几个独立的工作者（通过多进程），每个工作者在自己的隔离资源和单独的进程中同时执行任务。
- en: Note
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although there are plans to remove the GIL from Python soon, at the time of
    this writing it is not possible for multiple threads to simultaneously work through
    tasks. Therefore, concurrency on a single core can give an illusion of parallelism
    even though there is one process doing all the work. The single process can only
    multitask by switching active threads to minimize waiting times of I/O blocking
    operations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有计划很快从Python中移除全局解释器锁（GIL），但在撰写本文时，多个线程同时处理任务是不可能的。因此，单核上的并发可以给人一种并行性的错觉，尽管只有一个进程在完成所有工作。单个进程只能通过切换活动线程来多任务处理，以最小化I/O阻塞操作的等待时间。
- en: You can only achieve true parallelism with multiple workers (in multiprocessing).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你只能通过多个工作者（在多进程的情况下）实现真正的并行性。
- en: Even though concurrency and parallelism have many similarities, they aren’t
    exactly the same concepts. The big difference between them is that concurrency
    can help you manage multiple tasks by interleaving their execution, which is useful
    for I/O-bound tasks. Parallelism, on the other hand, involves executing multiple
    tasks simultaneously, typically on multicore machines, which is more useful for
    CPU-bound tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管并发和并行有许多相似之处，但它们并不是完全相同的概念。它们之间的主要区别在于，并发可以通过交错执行来帮助管理多个任务，这对于I/O密集型任务很有用。另一方面，并行性涉及同时执行多个任务，通常在多核机器上，这对于CPU密集型任务更有用。
- en: You can implement concurrency using approaches like threading or asynchronous
    programming (i.e., time-slicing on a single-core machine, where tasks are interleaved
    to give the appearance of simultaneous execution).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用线程或异步编程（即在单核机器上的时间切片，其中任务被交错以产生同时执行的外观）等方法实现并发。
- en: '[Figure 5-1](#concurrency_parallelism) shows the relationship between concurrency
    and parallelism.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-1](#concurrency_parallelism) 展示了并发与并行之间的关系。'
- en: '![bgai 0501](assets/bgai_0501.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0501](assets/bgai_0501.png)'
- en: Figure 5-1\. Concurrency and parallelism
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 并发与并行
- en: In most scalable systems, you can witness both concurrency and parallelism.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数可扩展系统中，你可以观察到并发和并行。
- en: Imagine that you’re visiting a fast-food restaurant and placing an order. In
    a concurrent system, you’ll see the restaurant owner taking orders while cooking
    burgers, attending to each task time to time, and effectively multitasking by
    switching between tasks. In a parallel system, you’ll see multiple staff members
    taking orders and a few others cooking the burgers at the same time. Here different
    workers handle each task simultaneously.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在访问一家快餐店并下单。在一个并发系统中，你会看到店主在烹饪汉堡的同时接受订单，不时地处理每个任务，并通过在任务之间切换来有效地进行多任务处理。在一个并行系统中，你会看到多个员工同时接受订单，同时有少数人在烹饪汉堡。这里不同的工人同时处理每个任务。
- en: Without any multithreading or asynchronous programming in a single-threaded
    process, the process has to wait for blocking operations to finish before it can
    start new tasks. Without multiprocessing implementing parallelism on multiple
    cores, computationally expensive operations can block the application from starting
    other tasks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在单线程进程中，如果没有多线程或异步编程，进程必须等待阻塞操作完成才能开始新任务。如果没有在多个核心上实现并行的多进程，计算密集型操作可能会阻止应用程序启动其他任务。
- en: '[Figure 5-2](#concurrency_parallelism_timeline) shows the distinctions between
    nonconcurrent execution, concurrent execution without parallelism (single core),
    and concurrent execution with parallelism (multiple cores).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-2](#concurrency_parallelism_timeline)展示了非并发执行、无并行性的并发执行（单核）和有并行性的并发执行（多核）之间的区别。'
- en: 'The three Python execution models shown in [Figure 5-2](#concurrency_parallelism_timeline)
    are as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-2](#concurrency_parallelism_timeline)中显示的三个Python执行模型如下：'
- en: No concurrency (synchronous)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 没有并发（同步）
- en: A single process (on one core) executes tasks sequentially.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 单个进程（在一个核心上）按顺序执行任务。
- en: Concurrent and non-parallel
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 并发和非并行
- en: Multiple threads in a single process (on a core) handle tasks concurrently but
    not in parallel due to Python’s GIL.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 单个进程（在一个核心上）中的多个线程并发处理任务，但由于Python的GIL，它们不是并行处理的。
- en: Concurrent and parallel
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 并发和并行
- en: Multiple processes on multiple cores perform the tasks in parallel, making the
    most of multicore processors for maximum efficiency.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 多个核心上的多个进程并行执行任务，最大限度地利用多核处理器以实现最大效率。
- en: '![bgai 0502](assets/bgai_0502.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0502](assets/bgai_0502.png)'
- en: Figure 5-2\. Concurrency with and without parallelism
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 带和不带并行的并发
- en: In multiprocessing, each process has access to its own memory space and resources
    to complete a task in isolation from other processes. This isolation can make
    processes more stable—since if a process crashes, it won’t affect others—but makes
    inter-process communication more complex compared to threads, which share the
    same memory space, as shown in [Figure 5-3](#multiprocessing_resources).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在多进程处理中，每个进程都有自己的内存空间和资源，以独立于其他进程完成任务。这种隔离可以使进程更加稳定——因为如果一个进程崩溃，它不会影响其他进程——但与共享相同内存空间的线程相比，进程间的通信更加复杂，如图[图5-3](#multiprocessing_resources)所示。
- en: '![bgai 0503](assets/bgai_0503.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0503](assets/bgai_0503.png)'
- en: Figure 5-3\. Resource sharing in multithreading and multiprocessing
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3\. 多线程和多进程中的资源共享
- en: Distributed workloads often use a managing process that coordinates the execution
    and collaboration of these processes to avoid issues such as data corruption and
    duplicating work. A good example of multiprocessing is when you serve requests
    with a load balancer managing traffic to multiple containers, each running an
    instance of your application.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式工作负载通常使用一个管理进程来协调这些进程的执行和协作，以避免数据损坏和重复工作等问题。多进程的一个好例子是，当使用负载均衡器管理多个容器的流量时，每个容器运行你的应用程序的一个实例来提供服务。
- en: Both multithreading and asynchronous programming reduce wait time in I/O tasks
    because the processor can do other work while waiting for I/O. However, they don’t
    help with tasks that require heavy computation, like AI inference, because the
    process is busy with computing some results. Therefore, to serve a large self-hosted
    GenAI model to multiple users, you should either scale services with multiprocessing
    or use algorithmic model optimizations (via specialized model inference servers
    like vLLM).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程和异步编程可以减少I/O任务的等待时间，因为处理器可以在等待I/O时做其他工作。然而，它们对需要大量计算的任务（如AI推理）没有帮助，因为进程正忙于计算某些结果。因此，为了向多个用户提供服务的大型自托管GenAI模型，你应该通过多进程扩展服务或使用算法模型优化（通过专门的模型推理服务器如vLLM）。
- en: Your first instinct when working with slow models may be to adopt parallelism
    by creating multiple instances of your FastAPI service (multiprocessing) in a
    single machine to serve requests in parallel.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当您与慢速模型一起工作时，您的第一反应可能是通过在单台机器上创建多个FastAPI服务实例（多进程）来采用并行性，以并行处理请求。
- en: Unfortunately, multiple workers running in separate processes will not have
    access to a shared memory space. As a result, you can’t share artifacts—​like
    a GenAI model—​loaded in memory between separate instances of your app in FastAPI.
    Sadly, a new instance of your model will also need to be loaded, which will significantly
    eat up your hardware resources. This is because FastAPI is a general-purpose web
    server that doesn’t natively optimize serving GenAI models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在单独的进程中运行的多个工作进程将无法访问共享内存空间。因此，您不能在FastAPI应用程序的不同实例之间共享内存中加载的工件（例如，GenAI模型）。遗憾的是，您的模型的新实例也需要加载，这将显著消耗您的硬件资源。这是因为FastAPI是一个通用型Web服务器，它没有原生优化托管GenAI模型。
- en: The solution is not parallelism on its own, but to adopt the external model-serving
    strategy, as discussed in [Chapter 3](ch03.html#ch03).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案不是并行本身，而是采用外部模型托管策略，如第3章所述。
- en: The only instance where you can treat AI inference workloads as I/O-bound, instead
    of compute-bound, is when you’re relying on third-party AI provider APIs (e.g.,
    OpenAI API). In this case, you’re offloading the compute-bound tasks to the model
    provider through network requests.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将AI推理工作负载视为I/O密集型，而不是计算密集型的情况，仅当你依赖于第三方AI提供商API（例如，OpenAI API）时。在这种情况下，你通过网络请求将计算密集型任务卸载给模型提供商。
- en: On your side, the AI inference workloads become I/O-bound through network requests,
    allowing for the use of concurrency through time slicing. The third-party provider
    has to worry about scaling their services to handle model inferences—​that are
    compute-bound—​across their hardware resources.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的端，AI推理工作负载通过网络请求成为I/O密集型，允许通过时间切片使用并发。第三方提供商必须担心扩展他们的服务以处理跨其硬件资源的计算密集型模型推理。
- en: You can externalize the serving and inference of larger GenAI models such as
    an LLM, with specialized servers like vLLM, Ray Serve, or NVIDIA Triton.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用像vLLM、Ray Serve或NVIDIA Triton这样的专用服务器，将大型GenAI模型（如LLM）的服务和推理外部化。
- en: Later in this chapter, I will detail how these servers maximize inference efficiency
    of compute-bound operations during model inference while minimizing the model’s
    memory footprint during the data generation process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我将详细介绍这些服务器如何在模型推理期间最大化计算密集型操作的推理效率，同时在数据生成过程中最小化模型的内存占用。
- en: To help you digest what was discussed so far, have a look at the comparison
    table of concurrency strategies in [Table 5-1](#concurrency_comparison) to understand
    when and why to use each.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您消化到目前为止所讨论的内容，请查看[表5-1](#concurrency_comparison)中并发策略的比较表，以了解何时以及为什么使用每种策略。
- en: Table 5-1\. Comparison of concurrency strategies
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-1\. 并发策略比较
- en: '| Strategy | Features | Challenges | Use cases |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 特点 | 挑战 | 用例 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| No concurrency (synchronous) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 无并发（同步） |'
- en: Simple, readable, easy-to-understand code to debug
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单、可读、易于理解的代码，便于调试
- en: A single CPU core and thread
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个CPU核心和线程
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Potential long waiting times depending on I/O or CPU blocking operations halting
    the process execution
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据I/O或CPU阻塞操作停止进程执行，可能存在潜在的长时间等待
- en: Can’t serve multiple users simultaneously
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不能同时服务多个用户
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Single user applications where users can wait for tasks to finish
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单用户应用程序，用户可以等待任务完成
- en: Infrequently used services or applications
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不常使用的服务或应用程序
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Async IO (asynchronous) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 异步I/O（异步） |'
- en: A single CPU core and thread
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个CPU核心和线程
- en: Multitasking managed by an event loop within the Python process
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Python进程内部的事件循环管理的多任务
- en: Thread-safe as the Python process manages tasks
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于Python进程管理任务，因此是线程安全的
- en: Maximizes the CPU utilization rate
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化CPU利用率
- en: Faster than multithreading and multiprocessing for I/O tasks
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于I/O任务来说，比多线程和多进程更快
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Harder to implement in code and can make debugging harder
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码中实现起来更困难，并且可能会使调试更困难
- en: Requires libraries and dependencies that use Async IO features
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要使用具有异步I/O功能的库和依赖项
- en: Easy to make mistakes that block the main process (and event loop)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易出错，阻塞主进程（和事件循环）
- en: '| Applications that have blocking I/O tasks |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 具有阻塞I/O任务的应用程序 |'
- en: '| Multithreading |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 多线程 |'
- en: A single CPU core but multiple threads within the same process
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个CPU核心但同一进程内有多个线程
- en: Threads share data and resources
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程共享数据和资源
- en: Simpler than Async IO to implement in code
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码中实现比Async IO更简单
- en: Multitasking across threads orchestrated by the OS
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由操作系统编排的线程间的多任务处理
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Difficult to lock resources for each thread to avoid thread-safety issues that
    can lead to nonreproducible bugs and data corruption
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以锁定每个线程的资源以避免线程安全问题，这可能导致不可复现的错误和数据损坏
- en: Threads can block each other indefinitely (deadlocks)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程可能会无限期地阻塞彼此（死锁）
- en: Concurrent access to resources can cause inconsistent results (race conditions)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对资源的并发访问可能导致不一致的结果（竞争条件）
- en: A thread can be denied resources by monopolizing threads (starvation)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程可能会因为线程垄断（饥饿）而被拒绝资源
- en: Creating and destroying threads is computationally expensive
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和销毁线程是计算密集型的
- en: '| Applications or services that have blocking I/O tasks |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 具有阻塞I/O任务的应用程序或服务 |'
- en: '| Multiprocessing |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 多进程 |'
- en: Multiple processes running on several CPU cores
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个CPU核心上运行的多个进程
- en: Each process is allocated a CPU core and isolated resources
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个进程都分配了一个CPU核心和隔离的资源
- en: Work can be distributed across CPU cores and managed by an orchestrator process
    using tools like Celery
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用像Celery这样的工具将工作负载分配到CPU核心，并由编排进程管理
- en: '|'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Sharing hardware resources and objects like a large AI model or data between
    processes can be complex and requires inter-process communication (IPC) mechanisms
    or a dedicated shared memory
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进程之间共享硬件资源以及像大型AI模型或数据这样的对象可能很复杂，并且需要进程间通信（IPC）机制或专用共享内存
- en: Difficult to keep multiple isolated processes in sync
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以保持多个隔离进程的同步
- en: Creating and destroying processes is computationally expensive
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和销毁进程是计算密集型的
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Applications or services that have blocking compute-bound tasks
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有阻塞计算密集型任务的应用程序或服务
- en: Divide-and-conquer type of tasks where processing can be done in isolated chunks
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在隔离的块中处理的分而治之类型的任务
- en: Distributing workloads or processing requests across multiple CPU cores
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个CPU核心之间分配工作负载或处理请求
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Now that we’ve explored various concurrency strategies, let’s continue by enhancing
    your services with asynchronous programming to efficiently manage I/O-bound operations.
    Later we’ll focus on optimizing compute-bound tasks, specifically model inference
    via specialized servers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了各种并发策略，接下来让我们通过增强异步编程来提高你的服务效率，以有效地管理I/O密集型操作。稍后我们将专注于优化计算密集型任务，特别是通过专用服务器进行模型推理。
- en: Optimizing for I/O Tasks with Asynchronous Programming
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用异步编程优化I/O任务
- en: In this section, we’ll explore the use of asynchronous programming to prevent
    blocking the main server process with I/O-bound tasks during AI workloads. You’ll
    also learn about the `asyncio` framework that enables writing asynchronous applications
    in Python.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨使用异步编程来防止在AI工作负载期间因I/O密集型任务而阻塞主服务器进程。你还将了解`asyncio`框架，它允许在Python中编写异步应用程序。
- en: Synchronous Versus Asynchronous (Async) Execution
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步与异步（Async）执行的比较
- en: What is considered an asynchronous application? To answer the question, let’s
    compare both synchronous and asynchronous programs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是异步应用程序？为了回答这个问题，让我们比较一下同步和异步程序。
- en: An application is considered *synchronous* when tasks are performed in a sequential
    order with each task waiting for the previous one to complete before starting.
    For applications that run infrequently and take only a few seconds to process,
    synchronous code rarely causes a problem and can make implementations faster and
    easier. However, if you need concurrency and want the efficiency of your services
    to be maximized on every core, your services should multitask without waiting
    for blocking operations to complete. That’s where implementing *asynchronous*
    (async) concurrency can help.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务按顺序执行，每个任务在开始之前都等待前一个任务完成时，应用程序被认为是**同步**的。对于运行不频繁且处理时间仅几秒的应用程序，同步代码很少引起问题，并且可以使实现更快、更简单。然而，如果你需要并发并且希望服务的效率在每颗核心上最大化，你的服务应该在不等待阻塞操作完成的情况下进行多任务处理。这就是实现**异步**（async）并发可以帮助的地方。
- en: Let’s look at a few examples of synchronous and async functions to understand
    how much of a performance boost an async code can give you. In both examples,
    I will use sleeping to simulate I/O blocking operation, but you can imagine other
    I/O tasks being performed in real-world scenarios.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个同步和异步函数的例子，以了解异步代码能给你带来多大的性能提升。在两个例子中，我将使用睡眠来模拟 I/O 阻塞操作，但你可以想象在现实场景中执行其他
    I/O 任务。
- en: '[Example 5-1](#sync_execution) shows an example of a synchronous code that
    simulates an I/O blocking operation with the blocking `time.sleep()` function.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-1](#sync_execution) 展示了一个使用阻塞的 `time.sleep()` 函数模拟 I/O 阻塞操作的同步代码示例。'
- en: Example 5-1\. Synchronous execution
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-1\. 同步执行
- en: '[PRE0]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO1-1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png) (#co_achieving_concurrency_in_ai_workloads_CO1-1)'
- en: Use `sleep()` to simulate an I/O blocking operation such as sending a network
    request.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `sleep()` 模拟发送网络请求等 I/O 阻塞操作。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO1-2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png) (#co_achieving_concurrency_in_ai_workloads_CO1-2)'
- en: Call the `task()` three times, sequentially. The loop simulates sending multiple
    network requests, one after another.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序调用 `task()` 三次。循环模拟依次发送多个网络请求。
- en: Calling `task()` three times in [Example 5-1](#sync_execution) takes 15 seconds
    to complete as Python waits for the blocking operation `sleep()` to complete.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 5-1](#sync_execution) 中，调用 `task()` 三次需要 15 秒才能完成，因为 Python 正在等待阻塞操作 `sleep()`
    完成。
- en: To develop async programs in Python, you can use the `asyncio` package as part
    of the standard library of Python 3.5 and later versions. Using `asyncio`, asynchronous
    code looks similar to sequential synchronous code but with additions of `async`
    and `await` keywords to perform nonblocking I/O operations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Python 中开发异步程序，你可以使用 `asyncio` 包，它是 Python 3.5 及以后版本的标准库的一部分。使用 `asyncio`，异步代码看起来与顺序同步代码相似，但增加了
    `async` 和 `await` 关键字以执行非阻塞 I/O 操作。
- en: '[Example 5-2](#async_execution) shows how you can use `async` and `await` keywords
    with `asyncio` to run [Example 5-1](#sync_execution) asynchronously.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-2](#async_execution) 展示了如何使用 `async` 和 `await` 关键字与 `asyncio` 一起异步运行
    [示例 5-1](#sync_execution)。'
- en: Example 5-2\. Asynchronous execution
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. 异步执行
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO2-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png) (#co_achieving_concurrency_in_ai_workloads_CO2-1)'
- en: Implement a `task` coroutine that cedes control to the event loop on blocking
    operations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个 `task` 协程，在阻塞操作上交出控制权给事件循环。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO2-2)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png) (#co_achieving_concurrency_in_ai_workloads_CO2-2)'
- en: The nonblocking five-second sleep signals to the event loop to run another task
    while waiting.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 非阻塞的五秒睡眠信号告知事件循环在等待的同时运行另一个任务。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO2-3)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png) (#co_achieving_concurrency_in_ai_workloads_CO2-3)'
- en: Use `asyncio.create_task` to spawn task instances to chain (or gather) and run
    them concurrently using `asyncio.gather`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `asyncio.create_task` 创建任务实例，并通过 `asyncio.gather` 并发地链式（或收集）并运行它们。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO2-4)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png) (#co_achieving_concurrency_in_ai_workloads_CO2-4)'
- en: Create an event loop to schedule async tasks with via the `asyncio.run` method.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `asyncio.run` 方法创建事件循环以安排异步任务。
- en: '[![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO2-5)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png) (#co_achieving_concurrency_in_ai_workloads_CO2-5)'
- en: Execution time is 1/3 of the synchronous example since the Python process wasn’t
    blocked this time.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间是同步示例的 1/3，因为这次 Python 进程没有被阻塞。
- en: After running [Example 5-2](#async_execution), you will notice that the `task()`
    function was concurrently called three times. On the other hand, the code in [Example 5-1](#sync_execution)
    calls the `task()` function three times sequentially. The async function ran inside
    the `asyncio`’s event loop, which was responsible for executing the code without
    waiting.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 [示例 5-2](#async_execution) 后，你会注意到 `task()` 函数被并发调用了三次。另一方面，[示例 5-1](#sync_execution)
    中的代码顺序调用了三次 `task()` 函数。异步函数在 `asyncio` 的事件循环中运行，该循环负责在不等待的情况下执行代码。
- en: In any async code, the `await` keyword flags the I/O blocking operations to
    Python so that they’re executed in a *nonblocking* manner (i.e., they can run
    without blocking the main process). By being made aware of blocking operations,
    Python can go and do something else while waiting for blocking operations to finish.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何异步代码中，`await` 关键字将 I/O 阻塞操作标记给 Python，以便它们以非阻塞方式（即，它们可以在不阻塞主进程的情况下运行）执行。通过了解阻塞操作，Python
    可以在等待阻塞操作完成时去做其他事情。
- en: '[Example 5-3](#await_keyword) shows how to use the `async` and `await` keywords
    to declare and run async functions.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-3](#await_keyword) 展示了如何使用 `async` 和 `await` 关键字声明和运行异步函数。'
- en: Example 5-3\. How to use `async` and `await` keywords
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. 如何使用 `async` 和 `await` 关键字
- en: '[PRE2]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO3-1)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO3-1)'
- en: Simulate a nonblocking I/O operation by `await`ing the `asyncio.sleep()` so
    that Python can go and do other things while waiting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `await` `asyncio.sleep()` 来模拟非阻塞 I/O 操作，这样 Python 就可以在等待时去做其他事情。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO3-2)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO3-2)'
- en: You need to call `main()` inside the `asyncio.run()` to execute it as it’s an
    async function. Otherwise, it will not be executed and returns a *coroutine* object
    instead. I will cover coroutines shortly.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在 `asyncio.run()` 内部调用 `main()` 来执行它，因为它是一个异步函数。否则，它将不会执行，并返回一个 *协程* 对象。我将在稍后介绍协程。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO3-3)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO3-3)'
- en: If you run the code, the second statement will be printed 3 seconds after the
    first one. In this instance, as there are no other operations to run beyond sleeping,
    Python runs in idle until the sleep operation is completed.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行代码，第二个语句将在第一个语句后 3 秒打印。在这种情况下，因为没有其他操作要运行，除了睡眠操作，Python 将在空闲状态下运行，直到睡眠操作完成。
- en: In [Example 5-3](#await_keyword), I used sleeping as a way to simulate I/O blocking
    operations such as making network requests.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 5-3](#await_keyword) 中，我使用睡眠来模拟 I/O 阻塞操作，例如发起网络请求。
- en: Caution
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: You can only use the `await` keyword inside a function declared with `async
    def`. Using `await` outside of an `async` function will raise a `SyntaxError`
    in Python. Another common pitfall is using blocking code that’s not asynchronous
    within an `async` function that will inadvertently prevent Python from doing other
    tasks while waiting.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你只能在用 `async def` 声明的函数中使用 `await` 关键字。在 `async` 函数外部使用 `await` 将在 Python 中引发
    `SyntaxError`。另一个常见的陷阱是在 `async` 函数中使用非异步的阻塞代码，这会无意中阻止 Python 在等待时执行其他任务。
- en: 'So, you now understand that in async programs, to keep the main process from
    being blocked, Python switches between functions as soon as it hits a blocking
    operation. You now may be wondering:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你现在理解了在异步程序中，为了防止主进程被阻塞，Python 一旦遇到阻塞操作就会在函数之间切换。你现在可能想知道：
- en: How does Python leverage `asyncio` to pause and resume functions?
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 如何利用 `asyncio` 暂停和恢复函数？
- en: What is the mechanism that Python’s `asyncio` uses to move from one function
    to another without forgetting about those that are suspended?
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 的 `asyncio` 使用什么机制从一个函数移动到另一个函数，而不会忘记那些被挂起的函数？
- en: How can functions be paused or resumed without losing their state?
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在不丢失其状态的情况下暂停或恢复函数？
- en: To answer the aforementioned questions, let’s dive deeper into the underlying
    mechanisms within `asyncio`, as understanding the answers to these questions will
    help you significantly to debug async code in your services.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答上述问题，让我们更深入地了解 `asyncio` 内部的底层机制，因为理解这些问题的答案将极大地帮助你调试服务中的异步代码。
- en: At the heart of `asyncio` lies a first-class object called an *event loop*,
    responsible for efficient handling of I/O events, system events, and application
    context changes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `asyncio` 的核心是一个称为 *事件循环* 的一等对象，负责高效处理 I/O 事件、系统事件和应用上下文变化。
- en: '[Figure 5-4](#event_loop) shows how the `asyncio` event loop undertakes task
    orchestration in Python.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](#event_loop) 展示了 `asyncio` 事件循环在 Python 中如何进行任务编排。'
- en: '![bgai 0504](assets/bgai_0504.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0504](assets/bgai_0504.png)'
- en: Figure 5-4\. Async IO event loop
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 异步 I/O 事件循环
- en: The event loop can be compared to a `while True` loop that watches for events
    or messages emitted by *coroutine functions* within the Python process and dispatches
    events to switch between functions while waiting for I/O blocking operations to
    complete. This orchestration allows other functions to execute asynchronously
    without interruption.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 事件循环可以比作一个 `while True` 循环，它监视 Python 进程中由 *协程函数* 发射的事件或消息，并在等待 I/O 阻塞操作完成时将事件分派给函数以切换执行。这种编排允许其他函数在不受干扰的情况下异步执行。
- en: Async Programming with Model Provider APIs
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型提供者 API 进行异步编程
- en: All three examples I’ve shown you so far are considered to be the “Hello World”
    examples of async programming. Now, let’s look at a real-world scenario related
    to building GenAI services where you need to use a model provider’s API—such as
    OpenAI, Anthropic, or Mistral—since it may be more expensive to serve LLMs yourself.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前向您展示的所有三个示例都被认为是异步编程的“Hello World”示例。现在，让我们看看一个与构建 GenAI 服务相关的真实场景，其中您需要使用模型提供商的
    API——例如 OpenAI、Anthropic 或 Mistral——因为自己提供 LLM 可能会更昂贵。
- en: Additionally, if you stress test the generation endpoints you created in [Chapter 3](ch03.html#ch03)
    by sending multiple requests in a short timeframe, you will notice long waiting
    times before each request is processed. This is because you were preloading and
    hosting the model in the same Python process and CPU core that the server is running
    on. When you send the first request, the whole server becomes blocked while the
    inference workload is complete. Since during inference the CPU is working as hard
    as it can, the inference/generation process is a CPU-bound blocking operation.
    However, it doesn’t have to be.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您通过在短时间内发送多个请求来对[第 3 章](ch03.html#ch03)中创建的生成端点进行压力测试，您将注意到在处理每个请求之前会有很长的等待时间。这是因为您在同一个
    Python 进程和 CPU 核心上预加载和托管模型，而服务器正在运行。当您发送第一个请求时，整个服务器在推理工作负载完成之前都会被阻塞。由于在推理过程中
    CPU 正在尽可能努力工作，推理/生成过程是一个 CPU 密集型的阻塞操作。然而，这不必是这种情况。
- en: When you use a provider’s API, you no longer have CPU-bound AI workloads to
    worry about since they become I/O-bound for you, and you offload the CPU-bound
    workloads to the provider. Therefore, it makes sense to know how to leverage async
    programming to concurrently interact with the model provider’s API.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用提供商的 API 时，您不再需要担心 CPU 密集型的 AI 工作负载，因为它们对您来说变成了 I/O 密集型，并且您将 CPU 密集型工作负载卸载到提供商。因此，了解如何利用异步编程来并发交互模型提供商的
    API 是有意义的。
- en: The good news is API owners will often release both synchronous and asynchronous
    *clients* and *software development kits* (SDKs) to reduce the work needed to
    interact with their endpoints.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，API 所有者通常会同时发布同步和异步的 *客户端* 和 *软件开发工具包* (SDK)，以减少与他们的端点交互所需的工作量。
- en: Caution
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you need to make requests to other external services, fetch some data from
    databases, or ingest content from files, you will add other I/O blocking tasks
    to the process. These blocking tasks can force the server to keep waiting if you
    don’t leverage asynchronous programming.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要向其他外部服务发送请求、从数据库中获取数据或从文件中摄取内容，您将向过程中添加其他 I/O 阻塞任务。如果您不利用异步编程，这些阻塞任务可能会迫使服务器持续等待。
- en: However, any synchronous code can be made async using a [process or thread pool
    executor](https://oreil.ly/hIDNI) to avoid running the task within the event loop.
    Instead, you run the asynchronous task on a separate process or thread to prevent
    blocking the event loop.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，任何同步代码都可以通过使用 [进程或线程池执行器](https://oreil.ly/hIDNI) 来异步化，以避免在事件循环中运行任务。相反，您可以在单独的进程或线程上运行异步任务，以防止阻塞事件循环。
- en: You can also verify any async support by checking library documentation or source
    code for mentions of `async` or `await` keywords. Otherwise, you can try testing
    whether the tool can be used within an async function without raising a `TypeError`
    when you use `await` on it.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过检查库文档或源代码中是否提及 `async` 或 `await` 关键字来验证任何异步支持。否则，您可以在使用 `await` 时尝试测试工具是否可以在异步函数中使用而不引发
    `TypeError`。
- en: If a tool, such as a database library, only has a synchronous implementation,
    then you can’t implement asynchronicity with that tool. The solution will be to
    switch the tool to an asynchronous equivalent so that can you can use them with
    the `async` and `await` keywords.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个工具，例如数据库库，只有同步实现，那么您就不能使用该工具实现异步。解决方案是将该工具切换到异步等效版本，这样您就可以使用 `async` 和 `await`
    关键字来使用它们。
- en: In [Example 5-4](#openai_clients), you will interact with OpenAI GPT-3.5 API
    via both synchronous and asynchronous OpenAI clients to understand the performance
    difference between the two.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 5-4](#openai_clients)中，您将通过同步和异步的 OpenAI 客户端与 OpenAI GPT-3.5 API 进行交互，以了解两种方式之间的性能差异。
- en: Note
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You will need to install the `openai` library:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装 `openai` 库：
- en: '[PRE3]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Example 5-4\. Comparing synchronous and asynchronous OpenAI clients
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. 比较同步和异步 OpenAI 客户端
- en: '[PRE4]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The difference between the sync and async clients is that with the async version,
    FastAPI can start processing user inputs in parallel without waiting for a response
    from the OpenAI API for the previous user input.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 同步和异步客户端之间的区别在于，使用异步版本时，FastAPI 可以在不等待 OpenAI API 对前一个用户输入的响应的情况下并行处理用户输入。
- en: By leveraging asynchronous code, you can get a massive boost in throughput and
    scale to a larger volume of concurrent requests. However, you must be careful
    when writing asynchronous (async) code.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用异步代码，你可以获得巨大的吞吐量提升，并扩展到更大的并发请求数量。然而，在编写异步（async）代码时必须小心。
- en: 'Here are some common pitfalls and problems you might face with async code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些你可能会在异步代码中遇到的一些常见陷阱和问题：
- en: Understanding and debugging errors can be more complex due to the nonlinear
    execution flow of concurrent tasks.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于并发任务的非线性执行流程，理解和调试错误可能更加复杂。
- en: Some libraries, like `aiohttp`, require nested async context managers for proper
    implementation. This can get confusing pretty fast.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些库，如 `aiohttp`，需要嵌套异步上下文管理器才能正确实现。这可能会很快变得令人困惑。
- en: Mixing asynchronous and synchronous code can negate any performance benefits,
    such as if you forget to mark functions with the `async` and `await` keywords.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合异步和同步代码可能会抵消任何性能优势，例如，如果你忘记为函数标记 `async` 和 `await` 关键字。
- en: Not using async-compatible tools and libraries can also cancel out any performance
    benefits; for example, using the `requests` package instead of `aiohttp` for making
    async API calls.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用与异步兼容的工具和库也可能抵消任何性能优势；例如，使用 `requests` 包而不是 `aiohttp` 来进行异步 API 调用。
- en: Forgetting to await coroutines within any async function or awaiting non-coroutines
    can lead to unexpected behavior. All `async` keywords must be followed by an `await`.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何异步函数中忘记等待协程或等待非协程可能导致意外的行为。所有 `async` 关键字之后都必须跟一个 `await`。
- en: Improperly managing resources (e.g., open API/database connections or file buffers)
    can cause memory leaks that freeze your computer. You can also leak memory if
    you don’t limit the number of concurrent operations in async code.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不当管理资源（例如，开放的 API/数据库连接或文件缓冲区）可能导致内存泄漏，冻结你的计算机。如果你在异步代码中不限制并发操作的数量，也可能发生内存泄漏。
- en: You might also run into concurrency and race condition issues where the thread-safety
    principle is violated, causing deadlocks on resources leading to data corruption.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还可能遇到并发和竞态条件问题，其中违反了线程安全原则，导致资源死锁，进而导致数据损坏。
- en: This list is not exhaustive, and as you can see, there are several pitfalls
    to using asynchronous programming. Therefore, I recommend starting with writing
    synchronous programs first, to understand the basic flow and logic of your code,
    before dealing with the complexities of migrating to an async implementation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表并不详尽，正如你所见，使用异步编程存在几个陷阱。因此，我建议首先从编写同步程序开始，以了解代码的基本流程和逻辑，然后再处理迁移到异步实现的复杂性。
- en: Event Loop and Thread Pool in FastAPI
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FastAPI 中的事件循环和线程池
- en: Under the hood, FastAPI can handle both async and sync blocking operations.
    It does this by running sync handlers in its *thread pool* so that blocking operations
    don’t stop the *event loop* from executing tasks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，FastAPI 可以处理异步和同步阻塞操作。它是通过在其 *线程池* 中运行同步处理程序来做到这一点的，这样阻塞操作就不会停止 *事件循环* 执行任务。
- en: As I mentioned in [Chapter 2](ch02.html#ch02), FastAPI runs on the ASGI web
    framework via Starlette. If it didn’t, the server would effectively run synchronously,
    so you would have to wait for each process to finish before it could serve the
    next. However, using ASGI, the FastAPI server supports concurrency via both multithreading
    (via a thread pool) and asynchronous programming (via an event loop) to serve
    multiple requests in parallel, while keeping the main server process from being
    blocked.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如我在[第 2 章](ch02.html#ch02)中提到的，FastAPI 通过 Starlette 运行在 ASGI 网络框架上。如果不是这样，服务器将实际上以同步方式运行，因此你必须等待每个进程完成，然后才能为下一个请求提供服务。然而，使用
    ASGI，FastAPI 服务器通过多线程（通过线程池）和异步编程（通过事件循环）支持并发，以并行处理多个请求，同时保持主服务器进程不被阻塞。
- en: FastAPI sets up the thread pool by instantiating a collection of threads at
    application startup to reduce the runtime burden of thread creation.^([4](ch05.html#id822))
    It then delegates background tasks and synchronous workloads to the thread pool
    to prevent the event loop from being blocked by any blocking operations inside
    the synchronous handlers. The event loop is also referred to as the main FastAPI
    server thread that is responsible for orchestrating the processing of requests.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 通过在应用启动时实例化一组线程来设置线程池，以减少线程创建的运行时负担.^([4](ch05.html#id822)) 然后，它将后台任务和同步工作负载委托给线程池，以防止事件循环被同步处理程序内的任何阻塞操作阻塞。事件循环也被称为负责协调请求处理的
    FastAPI 主服务器线程。
- en: As I mentioned, the event loop is the core component of every application built
    on top of `asyncio`, including FastAPI that implements concurrency. Event loops
    run asynchronous tasks and callbacks, including performing network I/O operations,
    and running subprocesses. In FastAPI, the event loop is also responsible for orchestrating
    the asynchronous processing of requests.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我提到的，事件循环是每个基于 `asyncio` 构建的应用程序的核心组件，包括实现并发的 FastAPI。事件循环运行异步任务和回调，包括执行网络
    I/O 操作和运行子进程。在 FastAPI 中，事件循环还负责协调请求的异步处理。
- en: If possible, you should run handlers on the event loop (via asynchronous programming)
    as it can be even more efficient than running them on the thread pool (via multithreading).
    This is because each thread in the thread pool has to acquire the GIL before it
    can execute any code bytes, and that requires some computational effort.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，你应该在事件循环（通过异步编程）上运行处理程序，因为它可能比在线程池（通过多线程）上运行更有效率。这是因为线程池中的每个线程在执行任何代码字节之前都必须获取
    GIL，这需要一些计算努力。
- en: Imagine if multiple concurrent users were using both the synchronous and asynchronous
    OpenAI GPT-3.5 handlers (endpoints) of your FastAPI service, as shown in [Example 5-4](#openai_clients).
    FastAPI will run the async handler requests on the event loop since that handler
    uses a nonblocking async OpenAI client. On the other hand, FastAPI has to delegate
    the synchronous handler requests to the thread pool to protect the event loop
    from blocking. Since delegating requests (to threads) and switching between threads
    in a thread pool is more work, the synchronous requests will finish later than
    their async counterparts.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果有多个并发用户同时使用你的 FastAPI 服务中的同步和异步 OpenAI GPT-3.5 处理程序（端点），如图 [示例 5-4](#openai_clients)
    所示。FastAPI 将在事件循环上运行异步处理程序请求，因为该处理程序使用非阻塞的异步 OpenAI 客户端。另一方面，FastAPI 必须将同步处理程序请求委托给线程池，以保护事件循环不被阻塞。由于委托请求（到线程）和在线程池之间切换线程需要更多的工作，因此同步请求将比它们的异步对应者完成得晚。
- en: Note
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that all of this work—processing both synchronous and async handler
    requests—is running on a single CPU core within the same FastAPI Python process.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，所有这些工作——处理同步和异步处理程序请求——都是在同一 FastAPI Python 进程的单个 CPU 内核上运行的。
- en: This is so that the CPU idle time is minimized while waiting for responses from
    OpenAI API.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是为了在等待来自 OpenAI API 的响应时，将 CPU 空闲时间最小化。
- en: The differences in performance are shown in [Figure 5-5](#multithreading_vs_async).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 性能差异在 [图 5-5](#multithreading_vs_async) 中显示。
- en: '![bgai 0505](assets/bgai_0505.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0505](assets/bgai_0505.png)'
- en: Figure 5-5\. How multithreading and Async IO handle I/O blocking operations
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 多线程和异步 I/O 处理 I/O 阻塞操作
- en: '[Figure 5-5](#multithreading_vs_async) shows that with I/O-bound workloads,
    async implementations are faster and should be your preferred method if you need
    concurrency. However, FastAPI does still do a solid job of serving multiple concurrent
    requests even if it has to work with a synchronous OpenAI client. It simply sends
    the synchronous API calls within threads of the thread pool to implement some
    form of concurrency for you. That’s why the FastAPI official documentation tells
    you to not worry too much about declaring your handler functions as `async def`
    or `def`.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-5](#multithreading_vs_async) 显示，对于 I/O 密集型工作负载，异步实现更快，如果你需要并发，这应该是你的首选方法。然而，即使
    FastAPI 必须与同步的 OpenAI 客户端一起工作，它仍然能够很好地处理多个并发请求。它只是将同步 API 调用发送到线程池中的线程，为你实现某种形式的并发。这就是为什么
    FastAPI 官方文档告诉你不必太担心将处理程序函数声明为 `async def` 或 `def`。'
- en: However, keep in mind that when you declare handlers with `async def`, FastAPI
    trusts you with performing only nonblocking operations. When you break that trust
    and execute blocking operations inside `async` routes, the event loop will be
    blocked and can no longer continue with executing tasks until the blocking operation
    is finished.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，当你使用 `async def` 声明处理程序时，FastAPI 信任你只执行非阻塞操作。当你打破这种信任并在 `async` 路由中执行阻塞操作时，事件循环将被阻塞，并且无法继续执行任务，直到阻塞操作完成。
- en: Blocking the Main Server
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阻塞主服务器
- en: If you’re using the `async` keyword when defining your functions, make sure
    you’re also using the `await` keyword somewhere inside your function and that
    none of the package dependencies you use inside the function are synchronous.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 `async` 关键字来定义你的函数，请确保你在函数内部某处也使用 `await` 关键字，并且你函数内部使用的任何包依赖项都不是同步的。
- en: Avoid declaring route handler functions as `async` if their implementation is
    synchronous. Otherwise, requests to the affected route handlers will block the
    main server from processing other requests while the server is waiting for the
    blocking operation to complete. It won’t matter if the blocking operation is I/O-bound
    or compute-bound. Therefore, any calls to databases or AI models can still cause
    the blockage if you’re not careful.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果路由处理器的实现是同步的，请避免将其声明为 `async`。否则，对受影响的路由处理器的请求将阻塞主服务器处理其他请求，直到服务器等待阻塞操作完成。无论阻塞操作是
    I/O 密集型还是计算密集型，这都不会产生影响。因此，如果你不小心，任何对数据库或 AI 模型的调用都可能导致阻塞。
- en: This is an easy mistake to make. For instance, you may use a synchronous dependency
    inside handlers you’ve declared as async, as shown in [Example 5-5](#blocking_main_thread).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个容易犯的错误。例如，你可能在已声明为异步的处理程序中使用同步依赖项，如[示例 5-5](#blocking_main_thread)所示。
- en: Example 5-5\. Incorrect implementation of asynchronous handlers in FastAPI
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-5\. FastAPI 中异步处理程序的错误实现
- en: '[PRE5]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO4-1)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO4-1)'
- en: I/O blocking operation to get ChatGPT API response. Because the route handler
    is marked async, FastAPI trusts us to not run blocking operations, but as we are,
    the request will block the event loop (main server thread). Other requests are
    now blocked until the current request is processed.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: I/O 阻塞操作以获取 ChatGPT API 响应。因为路由处理器被标记为异步，FastAPI 信任我们不运行阻塞操作，但正如我们所做的那样，请求将阻塞事件循环（主服务器线程）。其他请求现在被阻塞，直到当前请求被处理。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO4-2)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO4-2)'
- en: A simple synchronous route handler with blocking operation that doesn’t leverage
    asynchronous features. Sync requests are handed off to the thread pool to run
    in the background so that the main server is not blocked.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的同步路由处理器，具有阻塞操作，不利用异步功能。同步请求被传递给线程池在后台运行，以便主服务器不会被阻塞。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO4-3)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO4-3)'
- en: An asynchronous route that is nonblocking.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非阻塞的异步路由。
- en: The request won’t block the main thread and doesn’t need to be handed off to
    the thread pool. As a result, the FastAPI event loop can process the request much
    faster using the async OpenAI client.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 请求不会阻塞主线程，也不需要将其传递给线程池。因此，FastAPI 事件循环可以使用异步 OpenAI 客户端快速处理请求。
- en: You now should feel more comfortable implementing new features in your FastAPI
    service that require performing I/O-bound tasks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该更自在地在你 FastAPI 服务中实现需要执行 I/O 密集型任务的新功能。
- en: 'To help solidify your understanding of the I/O concurrency concepts, in the
    next few sections you will build several new features using concurrency into your
    FastAPI service. These features include:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你巩固对 I/O 并发概念的理解，在接下来的几节中，你将使用并发在你的 FastAPI 服务中构建几个新功能。这些功能包括：
- en: Talk to the web
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与网络对话
- en: Build and integrate a web scraper module that allows you to ask questions to
    your self-hosted LLM about the content of a website by providing an HTTP URL.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和集成一个网络爬虫模块，允许你通过提供 HTTP URL 来向你的自托管 LLM 提问有关网站内容的问题。
- en: Talk to documents
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与文档对话
- en: Build and integrate a RAG module to process documents into a vector database.
    A vector database stores data in a way that supports efficient similarity searches.
    You can then use semantic search, which understands the meaning of queries, to
    interact with uploaded documents using your LLM.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 构建并集成一个RAG模块，将文档处理成向量数据库。向量数据库以支持高效相似性搜索的方式存储数据。然后，您可以使用理解查询含义的语义搜索，通过您的LLM与上传的文档进行交互。
- en: Both projects will give you a hands-on experience interacting asynchronously
    with external systems such as websites, a database, and a filesystem.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个项目都将为您提供与外部系统（如网站、数据库和文件系统）异步交互的实践经验。
- en: 'Project: Talk to the Web (Web Scraper)'
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：与网络对话（网页抓取器）
- en: Companies often host a series of internal web pages for manuals, processes,
    and other documentation as HTML pages. For longer pages, your users may want to
    provide URLs when asking questions and expect your LLM to fetch and read the content.
    This is where having a built-in web scraper can come in handy.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 公司通常会为手册、流程和其他文档创建一系列内部网页，以HTML页面形式呈现。对于较长的页面，用户在提问时可能会提供URL，并期望你的LLM能够抓取并读取内容。这时，内置的网页抓取器就能派上用场。
- en: 'There are many ways to build a web scraper for your self-hosted LLM. Depending
    on your use case, you can use a combination of the following methods:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以为您的自托管LLM构建网页抓取器。根据您的用例，您可以使用以下方法的组合：
- en: Fetch web pages as HTML and feed the raw HTML (or inner text content) to your
    LLM to parse the content into your desired format.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将网页作为HTML抓取，并将原始HTML（或内部文本内容）提供给LLM以解析内容到您期望的格式。
- en: Use *web scraping frameworks* such as `BeautifulSoup` and `ScraPy` to parse
    the content of web pages after fetching.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`BeautifulSoup`和`ScraPy`等网页抓取框架在抓取后解析网页内容。
- en: Use *headless web browsers* such as Selenium and Microsoft Playwright to dynamically
    navigate nodes in pages and parse content. Headless browsers are great for navigation
    single-page applications (SPAs).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Selenium和Microsoft Playwright等无头浏览器动态导航页面中的节点并解析内容。无头浏览器非常适合导航单页应用程序（SPAs）。
- en: Caution
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'You or your users should avoid LLM-powered web scraping tools for illegal purposes.
    Make sure you have permission before extracting content from URLs:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 您或您的用户应避免使用LLM驱动的网页抓取工具进行非法目的。在从URL提取内容之前，请确保您已获得许可：
- en: Review each website’s terms of use, especially if there is a mention of web
    scraping.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查阅每个网站的条款，特别是如果提到网页抓取。
- en: Use APIs when possible.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当可能时使用API。
- en: Ask website owners for permission directly if unsure.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不确定，请直接向网站所有者请求许可。
- en: For this mini-project, we will only fetch and feed raw inner text of HTML pages
    to our LLM since implementing a production-ready scraper can become a book of
    its own.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个小型项目，我们只会抓取并馈送HTML页面的原始内部文本给我们的LLM，因为实现一个生产就绪的抓取器可能需要一本书的内容。
- en: 'The process for building a simple asynchronous scraper is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 构建简单异步抓取器的过程如下：
- en: Develop a function to match URL patterns using regex on user prompts to the
    LLM.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个函数，使用正则表达式在用户对LLM的提示中匹配URL模式。
- en: If found, loop over the list of provided URLs and asynchronously fetch the pages.
    We will use an asynchronous HTTP library called `aiohttp` instead of the `requests`
    since `requests` can only make synchronous network requests.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果找到，遍历提供的URL列表并异步抓取页面。我们将使用名为`aiohttp`的异步HTTP库，而不是`requests`，因为`requests`只能进行同步网络请求。
- en: Develop a parsing function to extract the textual content from fetched HTML.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个解析函数，从抓取的HTML中提取文本内容。
- en: Feed the parsed page content to the LLM alongside the original user prompt.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解析后的页面内容与原始用户提示一起提供给LLM。
- en: '[Example 5-6](#web_scraper) demonstrates how you can implement the aforementioned
    steps.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例5-6](#web_scraper)演示了您如何实现上述步骤。'
- en: Note
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You will need to install a few additional dependencies to run this example:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此示例需要安装一些额外的依赖项：
- en: '[PRE6]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Example 5-6\. Building an asynchronous web scraper
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-6：构建异步网页抓取器
- en: '[PRE7]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO5-1)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO5-1)'
- en: A simple regex pattern that captures the URLs into a named group called `url`
    and matches both `http` and `https` protocols. For simplicity, this pattern matches
    more loosely defined URLs and doesn’t validate the structure of a domain name
    or path, nor does it account for query strings or anchors in a URL.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的正则表达式模式，将URL捕获到名为`url`的命名组中，并匹配`http`和`https`协议。为了简单起见，此模式对URL的定义较为宽松，不验证域名或路径的结构，也不考虑URL中的查询字符串或锚点。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO5-2)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[(#co_achieving_concurrency_in_ai_workloads_CO5-2)]'
- en: Find all nonoverlapping matches of the regex pattern in the text.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本中找到所有非重叠的正则表达式模式匹配。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO5-3)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)[(#co_achieving_concurrency_in_ai_workloads_CO5-3)]'
- en: Use the `bs4` Beautiful Soup package to parse the HTML string. In Wikipedia
    pages, the article content is nested within a `div` container with the `id="bodyContent"`,
    so the parsing logic assumes only Wikipedia URLs will be passed in. You can change
    this logic for other URLs or just use `soup.getText()` to grab any text content
    nested within the HTML. However, bear in mind that there will be lots of noise
    in the parsed content if you parse the raw HTML like that, which can confuse the
    LLM.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`bs4` Beautiful Soup包解析HTML字符串。在维基百科页面中，文章内容嵌套在一个具有`id="bodyContent"`的`div`容器中，因此解析逻辑假设只有维基百科URL会被传入。你可以更改此逻辑以处理其他URL，或者只需使用`soup.getText()`来获取HTML中嵌套的任何文本内容。然而，请注意，如果你以这种方式解析原始HTML，解析内容中会有很多噪声，这可能会混淆LLM。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO5-4)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)[(#co_achieving_concurrency_in_ai_workloads_CO5-4)]'
- en: Given an `aiohttp` session and a URL, perform an asynchronous `get` request.
    Create a `response` async context manager and `await` the response within this
    context manager.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个`aiohttp`会话和一个URL，执行异步的`get`请求。创建一个`response`异步上下文管理器，并在该上下文管理器内`await`响应。
- en: '[![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO5-5)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)[(#co_achieving_concurrency_in_ai_workloads_CO5-5)]'
- en: Given a list of URLs, create a client session async context manager to asynchronously
    perform multiple fetch calls. Since `fetch()` is a coroutine function (i.e., it
    uses the `await` keyword), `fetch_all()` will need to run multiple `fetch()` coroutines
    inside the `asyncio.gather()` to be scheduled for asynchronous execution on the
    event loop.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个URL列表，创建一个客户端会话异步上下文管理器以异步执行多个获取调用。由于`fetch()`是一个协程函数（即它使用`await`关键字），`fetch_all()`需要在`asyncio.gather()`内部运行多个`fetch()`协程，以便在事件循环上异步执行。
- en: '[![6](assets/6.png)](#co_achieving_concurrency_in_ai_workloads_CO5-6)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)[(#co_achieving_concurrency_in_ai_workloads_CO5-6)]'
- en: Check that all URLs have been fetched successfully and, if not, raise a warning.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 检查所有URL是否已成功获取，如果没有，则发出警告。
- en: You now have the utility scraper functions you need to implement the web scraping
    feature in your `/generate/text` endpoint.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在拥有了实现`/generate/text`端点中网络爬虫功能的所需实用爬虫函数。
- en: Next, upgrade the text-to-text handler to use the scraper functions via a dependency
    in an asynchronous manner, as shown in [Example 5-7](#web_scraper_fastapi).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将文本到文本处理器升级为使用爬虫函数，通过依赖项以异步方式执行，如[示例5-7](#web_scraper_fastapi)所示。
- en: Example 5-7\. Injecting web scraper functionality as a dependency into the FastAPI
    LLM handler
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-7：将网络爬虫功能作为依赖项注入FastAPI LLM处理器
- en: '[PRE8]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO6-1)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[(#co_achieving_concurrency_in_ai_workloads_CO6-1)]'
- en: Implement a `get_urls_content` FastAPI dependency that gets a user prompt from
    the request body and finds all URLs. It then returns the content of all URLs as
    a long string. The dependency has exception handling built in to handle any I/O
    errors by returning an empty string and logging a warning on the server.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个`get_urls_content` FastAPI依赖项，它从请求体中获取用户提示并找到所有URL。然后它返回所有URL的内容作为一个长字符串。该依赖项内置异常处理，以处理任何I/O错误，通过返回一个空字符串并在服务器上记录警告。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO6-2)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[(#co_achieving_concurrency_in_ai_workloads_CO6-2)]'
- en: When using `aiohttp` inside FastAPI, you don’t need to manage the event loop
    yourself because FastAPI, as an asynchronous framework, handles the event loop.
    You can define your endpoint as an async function and use `aiohttp` to make asynchronous
    HTTP requests within the handler or via a dependency like in this example.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 当在FastAPI中使用`aiohttp`时，你不需要自己管理事件循环，因为FastAPI作为一个异步框架，会处理事件循环。你可以定义你的端点为一个异步函数，并在处理器内部或通过依赖项（如本例所示）使用`aiohttp`来发送异步HTTP请求。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO6-3)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)[(#co_achieving_concurrency_in_ai_workloads_CO6-3)]'
- en: Inject the results of the `get_urls_content` dependency call to the handler
    via the FastAPI’s `Depends` class. Using a FastAPI dependency here kept the controller
    logic small, clean, and readable.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 通过FastAPI的`Depends`类将`get_urls_content`依赖项调用的结果注入到处理器中。在这里使用FastAPI依赖项使得控制器逻辑保持小巧、简洁和可读。
- en: Now, run the Streamlit client in the browser and try your shiny new feature.
    [Figure 5-6](#llm_summary) shows my experiment.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在浏览器中运行Streamlit客户端，尝试您的新特性。[图5-6](#llm_summary)展示了我的实验。
- en: '![bgai 0506](assets/bgai_0506.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0506](assets/bgai_0506.png)'
- en: Figure 5-6\. Asking the self-hosted TinyLlama model to summarize a Wikipedia
    article
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. 请自托管TinyLlama模型总结维基百科文章
- en: Congratulations! You’ve learned how to build a simple nonblocking web scraper
    to work with your own LLM. In this mini-project, you leveraged `re` package to
    match URL patterns in the user prompt and then used the `aiohttp` library to asynchronously
    fetch multiple pages concurrently. You then used the `BeautifulSoup` package to
    parse the content of Wikipedia articles by grabbing the text content of the `div`
    container with the ID of `bodyContent` within the fetched HTML string. For other
    websites or internal company web pages, you can always alter the parsing logic
    for appropriate parsing. Finally, you wrapped the whole scraping logic inside
    a FastAPI dependency with exception handling built-in to make use of dependency
    injection while upgrading the text model-serving handler.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经学会了如何构建一个简单的非阻塞网络爬虫，与您自己的LLM一起工作。在这个小型项目中，您利用`re`包匹配用户提示中的URL模式，然后使用`aiohttp`库异步并发地抓取多个页面。然后，您使用`BeautifulSoup`包通过获取抓取的HTML字符串中`bodyContent`
    ID的`div`容器的文本内容来解析维基百科文章的内容。对于其他网站或内部公司网页，您始终可以修改解析逻辑以进行适当的解析。最后，您将整个抓取逻辑封装在FastAPI依赖项中，内置异常处理，以便在升级文本模型服务处理程序时使用依赖注入。
- en: Bear in mind that your scraper can’t handle complex pages with dynamic layouts
    that are server-rendered. In such cases, you can add a headless browser to your
    web scraper for navigating dynamic pages.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您的爬虫无法处理具有动态布局且由服务器渲染的复杂页面。在这种情况下，您可以在您的网络爬虫中添加一个无头浏览器来导航动态页面。
- en: Additionally, fetching content of external sites will be challenging since most
    sites may implement anti-scraping protections such as *IP blocking* or *CAPTCHAs*
    as common deterrents. Maintaining *data quality* and *consistency* with external
    websites is also an ongoing challenge as you may need to update your scraping
    scripts regularly to ensure accurate and reliable extraction.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，抓取外部网站的内容将具有挑战性，因为大多数网站可能实施了如*IP封锁*或*CAPTCHAs*等常见的防爬措施。与外部网站保持*数据质量*和*一致性*也是一个持续性的挑战，因为您可能需要定期更新您的抓取脚本以确保准确和可靠的数据提取。
- en: You should now feel more comfortable building GenAI-powered services that need
    to interact with the web via making asynchronous network requests.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该对构建需要通过异步网络请求与网络交互的GenAI服务感到更加得心应手。
- en: Next, we will look at other I/O asynchronous interactions such as those with
    databases and the filesystem by building a *talk to your documents* feature.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过构建“与您的文档对话”功能来查看其他I/O异步交互，例如与数据库和文件系统的交互。
- en: This functionality allows users to upload documents through the Streamlit interface
    to your service. The content of the uploaded documents is then extracted, processed,
    and saved in a database. Subsequently, during user interactions with the LLM,
    an asynchronous retrieval system retrieves semantically relevant content from
    the database, which is then used to augment the context provided to the LLM.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能允许用户通过Streamlit界面上传文档到您的服务。上传文档的内容随后被提取、处理并保存在数据库中。随后，在用户与LLM交互期间，一个异步检索系统从数据库中检索语义相关的内容，然后用于增强LLM提供的上下文。
- en: This process is referred to as RAG, which we will build as a module for your
    LLM next.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为RAG，我们将为您的LLM构建一个模块。
- en: 'Project: Talk to Documents (RAG)'
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：与文档对话（RAG）
- en: In this project, we will build a RAG module into your GenAI service to give
    you a hands-on experience interacting asynchronously with external systems such
    as a database and a filesystem.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将把一个RAG模块构建到您的GenAI服务中，让您亲身体验异步与外部系统（如数据库和文件系统）交互的过程。
- en: You might be curious about the purpose of a RAG module and its necessity. RAG
    is simply a technique for augmenting the context of LLM prompts with custom data
    sources for knowledge-intensive tasks.^([5](ch05.html#id830)) It is an effective
    technique to ground LLM responses in facts contained within data without the need
    for complex and expensive LLM fine-tuning.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能对RAG模块的用途及其必要性感到好奇。RAG是一种简单的方法，用于通过为知识密集型任务提供自定义数据源来增强LLM提示的上下文.^([5](ch05.html#id830))这是一种有效的技术，可以在不进行复杂且昂贵的LLM微调的情况下，将LLM的响应基于数据中的事实。
- en: Organizations are eager to implement RAG with their own LLMs since it allows
    their employees to engage with their massive internal knowledge bases via the
    LLM. With RAG, businesses expect that the internal knowledge bases, systems, and
    procedures will be made accessible and readily available to anyone who needs them
    to answer questions, just when they need it. This accessibility to the company’s
    body of information is expected to enhance productivity, cut costs and time looking
    for information, and boost profits for any business.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 组织渴望使用自己的 LLM 实施RAG，因为它允许员工通过 LLM 与他们庞大的内部知识库互动。通过 RAG，企业期望内部知识库、系统和程序对需要它们来回答问题的人变得可访问和易于获取。这种对公司信息库的访问预期将提高生产力，减少查找信息的时间和成本，并提高任何企业的利润。
- en: However, LLMs are susceptible to generating responses that don’t adhere to the
    instructions given by the user. In other words, the LLM can *hallucinate* responses
    with information or data that is not based on facts or reality.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLM 容易生成不符合用户指令的响应。换句话说，LLM 可以使用基于事实或现实的信息或数据来*虚构*响应。
- en: These hallucinations can occur due to the model’s reliance on patterns in the
    data it was trained on rather than direct access to external, up-to-date, and
    factual data. LLMs can manifest hallucinations with confidently presented yet
    incorrect or nonsensical answers, fabricated stories, or claims without a basis
    in truth.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这些虚构可能是因为模型依赖于其训练数据中的模式，而不是直接访问外部、最新和事实性的数据。LLM 可以以自信的方式呈现不正确或无意义的答案、编造的故事或缺乏事实依据的声明。
- en: Therefore, for more complex and knowledge-intensive tasks, you will want your
    LLM to access external knowledge sources to complete tasks. This enables more
    factual consistency and improves the reliability of the generated responses. [Figure 5-7](#rag)
    shows the full process.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于更复杂和知识密集型任务，您希望您的 LLM 访问外部知识源以完成任务。这可以确保生成响应的事实一致性并提高其可靠性。[图 5-7](#rag)
    展示了整个流程。
- en: '![bgai 0507](assets/bgai_0507.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0507](assets/bgai_0507.png)'
- en: Figure 5-7\. RAG
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. RAG
- en: In this project, you will build a simple RAG module for your LLM service such
    that users can upload and talk to their documents.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，您将为您的 LLM 服务构建一个简单的 RAG 模块，以便用户可以上传和与他们的文档进行交流。
- en: Note
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There is a lot to know about RAG. It’s enough to fill several textbooks with
    new papers being published every day for new techniques and algorithms.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 RAG 有很多要了解的内容。这足以用每天出版的新论文填满几本教科书，每天都有新技术和算法被发布。
- en: I recommend checking out other publications on LLMs to learn about the RAG process
    and advanced RAG techniques.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议查看其他关于 LLM 的出版物，以了解 RAG 流程和高级 RAG 技术。
- en: 'The pipeline for RAG consists of the following stages:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 的管道包括以下阶段：
- en: '*Extraction* of documents from a filesystem to load the textual content in
    chunks onto memory.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*提取* 从文件系统中的文档提取以将文本内容分块加载到内存中。'
- en: '*Transformation* of the textual content by cleaning, splitting, and preparing
    them to be passed into an embedding model to produce embedding vectors that represent
    a chunk’s semantic meaning.'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*转换* 通过清理、分割和准备文本内容，以便将其传递到嵌入模型中，以生成表示块语义意义的嵌入向量。'
- en: '*Storage* of embedding vectors alongside metadata, such as the source and text
    chunk, in a vector store such as Qdrant.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*存储* 将嵌入向量及其元数据（如来源和文本块）存储在 Qdrant 等向量存储中。'
- en: '*Retrieval* of semantically relevant embedding vectors by performing a semantic
    search on the user’s query to the LLM. The original text chunks—​stored as metadata
    of the retrieved vectors—​are then used to augment (i.e., enhance the context
    within) the initial prompt provided to the LLM.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*检索* 通过对用户查询 LLM 的语义搜索来检索语义相关的嵌入向量。原始文本块——存储为检索向量的元数据——然后用于增强（即，增强 LLM 收到的初始提示中的上下文）。'
- en: '*Generation* of LLM response bypassing both the query and retrieved chunks
    (i.e., context) to the LLM for getting a response.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*生成* LLM 响应时绕过查询和检索的块（即上下文）直接提交给 LLM 以获取响应。'
- en: You can see the full pipeline in [Figure 5-8](#rag_pipeline).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[图 5-8](#rag_pipeline)中看到完整的管道。
- en: '![bgai 0508](assets/bgai_0508.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0508](assets/bgai_0508.png)'
- en: Figure 5-8\. RAG pipeline
  id: totrans-299
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. RAG 管道
- en: You can take the pipeline shown in [Figure 5-8](#rag_pipeline) and build it
    to your existing service. [Figure 5-9](#rag_module) shows the system architecture
    of a “talk to your documents” service enabled with RAG.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将 [图 5-8](#rag_pipeline) 中所示的流程图应用到您现有的服务中。[图 5-9](#rag_module) 展示了启用 RAG
    的“与您的文档对话”服务的系统架构。
- en: '![bgai 0509](assets/bgai_0509.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0509](assets/bgai_0509.png)'
- en: Figure 5-9\. Talk to your documents system architecture
  id: totrans-302
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 与您的文档对话系统架构
- en: '[Figure 5-9](#rag_module) outlines how the documents uploaded by users via
    the Streamlit interface are stored and then fetched for processing and storage
    into the database for later retrieval to augment the LLM prompts.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-9](#rag_module) 概述了用户通过 Streamlit 接口上传的文档的存储方式，然后用于处理并存储到数据库中，以便稍后检索以增强
    LLM 提示。'
- en: The first step before implementing the RAG system in [Figure 5-9](#rag_module)
    is to include a file upload functionality in both the Streamlit client and your
    backend API.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-9](#rag_module) 中实现 RAG 系统之前的第一步是在 Streamlit 客户端和您的后端 API 中包含文件上传功能。
- en: Using FastAPI’s `UploadFile` class, you can accept documents from users in chunks
    and save them into the filesystem or any other file storage solution such as a
    blob storage. The important item to note here is that this I/O operation is nonblocking
    through asynchronous programming, which FastAPI’s `UploadFile` class supports.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FastAPI 的 `UploadFile` 类，您可以分块接收用户的文档并保存到文件系统或任何其他文件存储解决方案，例如 blob 存储。这里需要注意的重要事项是，通过异步编程，这种
    I/O 操作是非阻塞的，FastAPI 的 `UploadFile` 类支持这种异步操作。
- en: Tip
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Since users may upload large documents, FastAPI’s `UploadFile` class supports
    *chunking* to store the uploaded documents, one piece at a time.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 由于用户可能上传大文档，FastAPI 的 `UploadFile` 类支持 *分块* 存储，一次存储一个上传的文档。
- en: This will prevent your service’s memory from being clogged up. You will also
    want to protect your service by disallowing users from uploading documents above
    a certain size.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这将防止您的服务内存被阻塞。您还希望通过禁止用户上传超过一定大小的文档来保护您的服务。
- en: '[Example 5-8](#upload_file) shows how to implement an asynchronous file upload
    functionality.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-8](#upload_file) 展示了如何实现异步文件上传功能。'
- en: Tip
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'You will need to install `aiofiles` package to asynchronously upload files
    alongside `python-multipart` to receive uploaded files from HTML forms:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装 `aiofiles` 包以异步上传文件，同时安装 `python-multipart` 以从 HTML 表单接收上传的文件：
- en: '[PRE9]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Example 5-8\. Implementing an asynchronous file upload endpoint
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-8\. 实现异步文件上传端点
- en: '[PRE10]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You should now be able to upload files via the Streamlit UI, as you can see
    in [Figure 5-10](#streamlit_upload).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该能够通过 Streamlit UI 上传文件，正如您在 [图 5-10](#streamlit_upload) 中所看到的。
- en: '![bgai 0510](assets/bgai_0510.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0510](assets/bgai_0510.png)'
- en: Figure 5-10\. Uploading files via Streamlit to the FastAPI service
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. 通过 Streamlit 将文件上传到 FastAPI 服务
- en: With upload functionality implemented, you can now turn your attention to building
    the RAG module. [Figure 5-11](#rag_module_detailed) shows the detailed pipeline,
    which opens up the data transformation component in [Figure 5-9](#rag_module).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现上传功能后，您现在可以将注意力转向构建 RAG 模块。[图 5-11](#rag_module_detailed) 展示了详细流程，它打开了 [图
    5-9](#rag_module) 中的数据转换组件。
- en: '![bgai 0511](assets/bgai_0511.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0511](assets/bgai_0511.png)'
- en: Figure 5-11\. Detailed RAG data processing pipeline
  id: totrans-320
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-11\. 详细 RAG 数据处理流程
- en: As you can see in [Figure 5-11](#rag_module_detailed), you need to asynchronously
    fetch the stored files from the hard disk and pass them through a data transformation
    pipeline prior to storage via an asynchronous database client.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 5-11](#rag_module_detailed) 所示，您需要异步地从硬盘上获取存储的文件，并通过异步数据库客户端将它们传递给数据转换流程，然后再进行存储。
- en: 'The data transformation pipeline consists of the following parts:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换流程包括以下部分：
- en: Extractor
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 提取器
- en: Extract content of PDFs and store in text files back onto the hard disk.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 从 PDF 中提取内容并存储到硬盘上的文本文件中。
- en: Loader
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 加载器
- en: Asynchronously load a text file into memory in chunks.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 异步地将文本文件分块加载到内存中。
- en: Cleaner
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 清洁器
- en: Remove any redundant whitespace or formatting characters from text chunks.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本块中移除任何多余的空白或格式化字符。
- en: Embedder
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入器
- en: Use a pretrained and self-hosted embedding model to convert text into embedding
    vectors.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的自托管嵌入模型将文本转换为嵌入向量。
- en: Once users upload their PDF files onto your server’s filesystem via the process
    shown in [Example 5-8](#upload_file), you can immediately convert them into text
    files via the `pypdf` library. Since there is no asynchronous library for loading
    binary PDF files, you will want to convert them into text files first.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦用户通过[示例5-8](#upload_file)中所示的过程将PDF文件上传到您的服务器文件系统，您就可以立即通过`pypdf`库将它们转换为文本文件。由于没有用于加载二进制PDF文件的异步库，您可能希望首先将它们转换为文本文件。
- en: '[Example 5-9](#rag_extract) shows how to load PDFs, extract and process their
    content, and then store them as text files.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例5-9](#rag_extract)展示了如何加载PDF文件，提取和处理其内容，然后将它们存储为文本文件。'
- en: Note
  id: totrans-333
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: 'You will need to install several packages to run the upcoming examples:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装几个包才能运行即将到来的示例：
- en: '[PRE11]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Example 5-9\. RAG PDF-to-text extractor
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-9. RAG PDF-to-text提取器
- en: '[PRE12]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO7-1)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_achieving_concurrency_in_ai_workloads_CO7-1]'
- en: Use the `pypdf` library to open a stream pointer to a PDF file with `strict=True`
    so that any read errors are logged to the terminal. Note that there is no asynchronous
    implementation of the `pypdf` library, so the function is declared with a normal
    `def` keyword. It is important to avoid using this function within an asynchronous
    function to avoid blocking the event loop that runs the main server thread. You
    will see how FastAPI background tasks can help solve this problem.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pypdf`库以`strict=True`打开指向PDF文件的流指针，以便将任何读取错误记录到终端。请注意，`pypdf`库没有异步实现，因此函数使用正常的`def`关键字声明。避免在异步函数中使用此函数，以避免阻塞运行主服务器线程的事件循环。您将看到FastAPI后台任务如何帮助解决这个问题。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO7-2)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[#co_achieving_concurrency_in_ai_workloads_CO7-2]'
- en: Loop over every page in the PDF document, and extract and append all text content
    into a long string.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历PDF文档中的每一页，提取并追加所有文本内容到一个长字符串中。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO7-3)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)[#co_achieving_concurrency_in_ai_workloads_CO7-3]'
- en: Write the content of the PDF document into a text file for downstream processing.
    Specify `encoding="utf-8"` to avoid problems on platforms like Windows.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 将PDF文档的内容写入文本文件以进行下游处理。指定`encoding="utf-8"`以避免在Windows等平台上出现问题。
- en: The text extractor will convert the PDF files into simple text files that we
    can stream into memory in chunks using an asynchronous file loader. Each chunk
    can then be cleaned and embedded into an embedding vector using an open source
    embedding model such as `jinaai/jina-embeddings-v2-base-en`, available to download
    from the [Hugging Face model hub](https://oreil.ly/gI74r).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 文本提取器将PDF文件转换为简单的文本文件，我们可以使用异步文件加载器分块将其流式传输到内存中。然后，每个块可以使用开源嵌入模型（如`jinaai/jina-embeddings-v2-base-en`）进行清理和嵌入到嵌入向量中，该模型可以从[Hugging
    Face模型库](https://oreil.ly/gI74r)下载。
- en: Note
  id: totrans-345
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: I selected the Jina base embedder since it matches the performance of OpenAI’s
    proprietary `text-embedding-ada-002` model.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了Jina基础嵌入器，因为它与OpenAI的专有`text-embedding-ada-002`模型的性能相匹配。
- en: '[Example 5-10](#rag_transform) shows the implementation of the RAG data transformation
    pipeline including the async text loader, cleaner, and embedding functions.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例5-10](#rag_transform)展示了RAG数据转换管道的实现，包括异步文本加载器、清理器和嵌入函数。'
- en: Example 5-10\. RAG data transformation functions
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-10. RAG数据转换函数
- en: '[PRE13]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO8-1)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_achieving_concurrency_in_ai_workloads_CO8-1]'
- en: Download and use the open source `jina-embeddings-v2-base-en` model to embed
    text strings into embedding vectors. Set `trust_remote_code=True` to download
    model weights and tokenizer configurations. Without this parameter set to `True`,
    the downloaded model weights will be initialized with random values instead of
    trained values.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 下载并使用开源的`jina-embeddings-v2-base-en`模型将文本字符串嵌入到嵌入向量中。将`trust_remote_code=True`设置为下载模型权重和分词器配置。如果没有将此参数设置为`True`，则下载的模型权重将使用随机值初始化而不是训练值。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO8-2)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[#co_achieving_concurrency_in_ai_workloads_CO8-2]'
- en: Use the `aiofiles` library to open an asynchronous connections to a file on
    the filesystem.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`aiofiles`库以异步方式打开文件系统上的文件连接。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO8-3)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)[#co_achieving_concurrency_in_ai_workloads_CO8-3]'
- en: Load the content of text documents in chunks for memory-efficient I/O operation.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 以块的形式加载文本文档的内容，以进行内存高效的I/O操作。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO8-4)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)[#co_achieving_concurrency_in_ai_workloads_CO8-4]'
- en: Instead of returning a `chunk`, yield it so that the `load()` function becomes
    an *asynchronous generator*. Asynchronous generators can be iterated with `async
    for loop`s so that blocking operations within them can be `await`ed to let the
    event loop start/resume other tasks. Both async `for` loops and normal `for` loops,
    iterate sequentially over the iterable but async `for` loops allow for iteration
    over an async iterator.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是返回一个 `chunk`，生成它，使 `load()` 函数成为一个 *异步生成器*。异步生成器可以用 `async for loop` 迭代，这样它们内部的阻塞操作就可以通过
    `await` 来执行，以便事件循环可以开始/恢复其他任务。异步 `for` 循环和普通 `for` 循环都按顺序遍历可迭代对象，但异步 `for` 循环允许遍历异步迭代器。
- en: '[![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO8-5)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)[#co_achieving_concurrency_in_ai_workloads_CO8-5]'
- en: Clean the text by removing any extra spaces, commas, dots, and line breaks.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 清理文本，删除任何额外的空格、逗号、句点和换行符。
- en: '[![6](assets/6.png)](#co_achieving_concurrency_in_ai_workloads_CO8-6)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)[#co_achieving_concurrency_in_ai_workloads_CO8-6]'
- en: Use the Jina embedding model to convert a text chunk to an embedding vector.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Jina 嵌入模型将文本块转换为嵌入向量。
- en: Once the data is processed into embedding vectors, you can store them into the
    *vector database*.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被处理成嵌入向量，您可以将它们存储到 *矢量数据库* 中。
- en: Unlike conventional alternatives such as relational databases, a vector database
    is specifically designed for handling data storage and retrieval operations optimized
    for *semantic searching*, which yields better results compared to keyword searches
    that can return suboptimal or incomplete results.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的替代品，如关系数据库不同，矢量数据库专门设计用于处理针对 *语义搜索* 优化的数据存储和检索操作，与可以返回次优或不完整结果的键词搜索相比，它能够提供更好的结果。
- en: The following code examples require you to run a local instance of the `qdrant`
    vector database on your local machine for the RAG module. Having a local database
    setup will give you the hands-on experience of working asynchronously with production-grade
    vector databases. To run the database in a container, you should have Docker installed
    on your machine and then pull and run the `qdrant` vector database container.^([7](ch05.html#id843))
    If you aren’t familiar with Docker, don’t worry. You will learn more about Docker
    and containerization in [Chapter 12](ch12.html#ch12).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例需要您在本地机器上运行 `qdrant` 矢量数据库的本地实例，以便使用 RAG 模块。拥有本地数据库设置将为您提供与生产级矢量数据库异步工作的实际操作经验。要在容器中运行数据库，您应该在您的机器上安装
    Docker，然后从 Docker 仓库中拉取并运行 `qdrant` 矢量数据库容器.^([7](ch05.html#id843)) 如果您不熟悉 Docker，不要担心。您将在第
    12 章[中](ch12.html#ch12)学习更多关于 Docker 和容器化的知识。
- en: '[PRE14]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO9-1)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_achieving_concurrency_in_ai_workloads_CO9-1]'
- en: Download the `qdrant` vector database image from the `qdrant` repository in
    the Docker registry.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Docker 仓库中的 `qdrant` 仓库下载 `qdrant` 矢量数据库镜像。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO9-2)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[#co_achieving_concurrency_in_ai_workloads_CO9-2]'
- en: Run the `qdrant/qdrant` image, and then expose and map container ports `6333`
    and `6334` to the same ports on the host machine.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `qdrant/qdrant` 镜像，然后暴露并映射容器端口 `6333` 和 `6334` 到主机机器上的相同端口。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO9-3)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)[#co_achieving_concurrency_in_ai_workloads_CO9-3]'
- en: Mount the `qdrant` database storage to the host machine filesystem at your project’s
    root directory.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `qdrant` 数据库存储挂载到主机机器文件系统的项目根目录下。
- en: Since database storage and retrieval are I/O operations, you should use an asynchronous
    database client. Thankfully, `qdrant` provides an asynchronous database client
    to work with.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据库存储和检索是 I/O 操作，您应该使用异步数据库客户端。幸运的是，`qdrant` 提供了一个异步数据库客户端来工作。
- en: Tip
  id: totrans-373
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can use other vector database providers such as Weaviate, Elastic, Milvus,
    Pinecone, Chroma, or others in replacement of Qdrant. Each has a set of features
    and limitations to consider for your own use case.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用其他矢量数据库提供商，例如 Weaviate、Elastic、Milvus、Pinecone、Chroma 或其他，以替代 Qdrant。每个提供商都有自己的特性和限制，您需要根据您的具体用例进行考虑。
- en: If you’re picking another database provider, make sure there is an asynchronous
    database client available that you can use.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择其他数据库提供商，请确保有可用的异步数据库客户端供您使用。
- en: Instead of writing several functions to store and retrieve data from the database,
    you can use the repository pattern mentioned in [Chapter 2](ch02.html#ch02). With
    the repository pattern, you can abstract low-level create, read, update, and delete
    database operations with defaults that match your use case.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是编写多个函数来存储和检索数据库中的数据，您可以使用第 2 章中提到的存储库模式。使用存储库模式，您可以使用与您的用例匹配的默认值来抽象低级别的创建、读取、更新和删除数据库操作。
- en: '[Example 5-11](#rag_repository) shows the repository pattern implementation
    for the Qdrant vector database.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-11](#rag_repository) 展示了 Qdrant 向量数据库的存储库模式实现。'
- en: Example 5-11\. Vector database client setup using the repository pattern
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-11\. 使用存储库模式设置向量数据库客户端
- en: '[PRE15]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO10-1)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO10-1)'
- en: Use the repository pattern to interact with the vector database via an asynchronous
    client. Normally, in the repository pattern you will implement the `create`, `get`,
    `update`, and `delete` methods. But for now let’s implement the `create_​col⁠lection`,
    `delete_collection`, `create`, and `search` methods.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 使用存储库模式通过异步客户端与向量数据库交互。通常，在存储库模式中，您将实现 `create`、`get`、`update` 和 `delete` 方法。但现在是时候实现
    `create_collection`、`delete_collection`、`create` 和 `search` 方法了。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO10-2)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO10-2)'
- en: Vectors need to be stored in a collection. A collection is a named set of points
    that you can use during a search. Collections are similar to tables in a relational
    database.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 向量需要存储在集合中。集合是在搜索期间可以使用的命名点集，类似于关系数据库中的表。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO10-3)'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO10-3)'
- en: Let the database know that any vectors in this collection should be compared
    via the cosine similarity calculation that calculates distances between vectors.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 让数据库知道此集合中的任何向量都应通过计算向量之间距离的余弦相似度来比较。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO10-4)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO10-4)'
- en: Check whether a collection exists before creating a new one. Otherwise, re-create
    the collection.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建新集合之前检查集合是否存在。否则，重新创建集合。
- en: '[![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO10-5)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO10-5)'
- en: Set the `retrieval_limit` and `score_threshold` to limit the number of items
    in the search results.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 `retrieval_limit` 和 `score_threshold` 以限制搜索结果中的项目数量。
- en: The `VectorRepository` class should now make it easier to interact with the
    database.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorRepository` 类现在应该使与数据库的交互更加容易。'
- en: When storing vector embeddings, you will also store some *metadata* including
    the name of the source document, the location of the text within source, and the
    original extracted text. RAG systems rely on this metadata to augment the LLM
    prompts and to show source information to the users.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 当存储向量嵌入时，您还将存储一些 *元数据*，包括源文档的名称、源中文本的位置以及原始提取的文本。RAG 系统依赖于这些元数据来增强 LLM 提示并向用户显示源信息。
- en: Tip
  id: totrans-392
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Currently, converting text to embedding vectors is an irreversible process.
    Therefore, you will need to store the text that created the embedding with the
    embedding vector as metadata.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，将文本转换为嵌入向量是一个不可逆的过程。因此，您需要将创建嵌入向量时使用的文本与嵌入向量一起作为元数据存储。
- en: You can now extend the `VectorRepository` and create the `VectorService` that
    allow you to chain together the data processing and storage pipeline, as shown
    in [Example 5-12](#rag_db_service).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以扩展 `VectorRepository` 并创建 `VectorService`，允许您将数据处理和存储管道连接起来，如 [示例 5-12](#rag_db_service)
    所示。
- en: Example 5-12\. Vector database service
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-12\. 向量数据库服务
- en: '[PRE16]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO11-1)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO11-1)'
- en: Create the `VectorService` class by inheriting the `VectorRepository` class
    so that you can use and extend common database operation methods from [Example 5-11](#rag_repository).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 通过继承 `VectorRepository` 类创建 `VectorService` 类，以便您可以使用和扩展来自 [示例 5-11](#rag_repository)
    的常见数据库操作方法。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO11-2)'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO11-2)'
- en: Use the `store_file_content_in_db` service method to asynchronously load, transform,
    and store raw text documents into the database in chunks.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `store_file_content_in_db` 服务方法异步加载、转换并将原始文本文档分块存储到数据库中。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO11-3)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO11-3)'
- en: Use an asynchronous generator `load()` to load text chunks from a file asynchronously.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 使用异步生成器 `load()` 异步地从文件中加载文本块。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO11-4)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO11-4)'
- en: Create an instance of the `VectorService` to import and use across the application.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 `VectorService` 实例以导入并在整个应用程序中使用。
- en: The final step in the RAG data processing and storage pipeline is to run the
    text extraction and storage logic within the `file_upload_controller` as background
    tasks. The implementation is shown in [Example 5-13](#rag_data_processor) so that
    the handler can trigger both operations in the background after responding to
    the user.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 数据处理和存储管道的最后一步是在 `file_upload_controller` 中作为后台任务运行文本提取和存储逻辑。实现方式如 [示例 5-13](#rag_data_processor)
    所示，以便处理程序在响应用户后可以在后台触发这两个操作。
- en: Example 5-13\. Update the upload handler to process and store PDF file content
    in the vector database
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-13\. 更新上传处理程序以处理和存储 PDF 文件内容到向量数据库
- en: '[PRE17]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO12-1)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO12-1)'
- en: Inject the FastAPI background tasks feature into the handler for processing
    file uploads in the background. FastAPI background tasks will be executed in order
    shortly after the handler sends a response to the client.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 将 FastAPI 后台任务功能注入到处理程序中，以在后台处理文件上传。FastAPI 后台任务将在处理程序向客户端发送响应后不久按顺序执行。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO12-2)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO12-2)'
- en: Run the PDF text-extraction function in the background after retuning a response
    to the client. Since the `pdf_text_extractor` is a synchronous function, FastAPI
    will run this function on a separate thread within the thread pool to avoid blocking
    the event loop.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在向客户端返回响应后，在后台运行 PDF 文本提取函数。由于 `pdf_text_extractor` 是一个同步函数，FastAPI 将在线程池中的单独线程上运行此函数，以避免阻塞事件循环。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO12-3)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO12-3)'
- en: Run the `vector_service.store_file_content_in_db` asynchronous function in the
    background on the FastAPI managed event loop as soon as the `pdf_text_extractor`
    has finished processing. Set the function to load content of the text document
    in chunks of 512 characters and store them in the `knowledgebase` vector collection,
    which accepts vectors of size 768.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `pdf_text_extractor` 完成处理之后，立即在 FastAPI 管理的事件循环上以异步方式运行 `vector_service.store_file_content_in_db`
    函数。将函数设置为以 512 个字符的块加载文本文档的内容，并将它们存储在大小为 768 的 `knowledgebase` 向量集合中。
- en: After building the RAG data storage pipeline, you can now focus on the search-and-retrieval
    system, which will allow you to augment the user prompts to the LLM, with knowledge
    from the database. [Example 5-14](#rag_generation) integrates the RAG search-and-retrieval
    operations with the LLM handler to augment the LLM prompts with additional context.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 RAG 数据存储管道后，你现在可以专注于搜索和检索系统，这将允许你使用数据库中的知识来增强 LLM 的用户提示。[示例 5-14](#rag_generation)
    将 RAG 搜索和检索操作与 LLM 处理器集成，以增强 LLM 提示的额外上下文。
- en: Example 5-14\. RAG integration with the LLM-serving endpoint
  id: totrans-415
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-14\. 与 LLM-serving 端点的 RAG 集成
- en: '[PRE18]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO13-1)'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO13-1)'
- en: Create the `get_rag_content` dependency function for injection into the LLM-serving
    handler. This dependency has access to the request `body` and subsequently the
    user `prompt`.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `get_rag_content` 依赖函数以注入到 LLM-serving 处理程序中。此依赖项可以访问请求 `body` 以及随后的用户 `prompt`。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO13-2)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO13-2)'
- en: Use the `vector_service` to search the database for content relevant to the
    user `prompt`. Convert the user `prompt` to an embedding using the `embed` function
    when passing to the `vector_service.search` function. Only retrieve the three
    most relevant items if their cosine similarity score is above `0.7` (or 70%).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `vector_service` 在数据库中搜索与用户 `prompt` 相关的内容。当传递给 `vector_service.search` 函数时，使用
    `embed` 函数将用户 `prompt` 转换为嵌入。如果它们的余弦相似度分数高于 `0.7`（或 70%），则仅检索三个最相关项。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO13-3)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO13-3)'
- en: Merge the text payload of the top three most relevant retrieved items as `rag_​con⁠tent_str`
    and return it.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 将检索到的最相关的前三个项目的文本有效负载合并为`rag_内容_str`并返回。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO13-4)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO13-4)'
- en: Inject the results of the `get_rag_content` dependency function into the LLM
    handler to augment the final prompt to the LLM with content from the vector database
    `knowledgebase`. The LLM handler can now fetch content of web pages and the RAG
    vector database.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 将`get_rag_content`依赖函数的结果注入到LLM处理器中，以增强最终提示，将来自向量数据库`knowledgebase`的内容添加到LLM中。LLM处理器现在可以获取网页内容和RAG向量数据库的内容。
- en: If you now visit your browser and upload a PDF document, you should be able
    to ask questions about it to your LLM. [Figure 5-12](#rag_results) shows my experiment
    with the service by uploading a sample of this book in its raw form and asking
    the LLM to describe who I am.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在访问你的浏览器并上传一个PDF文档，你应该能够向你的LLM提出关于它的问题。[图5-12](#rag_results)展示了我的实验，通过上传这本书的原始样本并让LLM描述我是谁。
- en: Note
  id: totrans-426
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Depending on the model and size of the inputs, you may observe performance degradations
    or exceptions like token length limit issues.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型和输入的大小，你可能观察到性能下降或异常，如令牌长度限制问题。
- en: '![bgai 0512](assets/bgai_0512.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0512](assets/bgai_0512.png)'
- en: Figure 5-12\. Leveraging RAG to provide answers in response to user queries
  id: totrans-429
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-12\. 利用RAG响应用户查询提供答案
- en: Congratulations! You now have a fully working RAG system enabled by open source
    models and a vector database.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在拥有了一个由开源模型和向量数据库支持的完全工作的RAG系统。
- en: 'This longer project served as a hands-on tutorial for learning concepts related
    to asynchronous programming and I/O operations with the filesystem and a vector
    database by building a RAG module for your LLM system. Note that the RAG system
    we just built together still has many limitations:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更长的项目作为一个动手教程，通过构建LLM系统的RAG模块来学习与异步编程和文件系统以及向量数据库相关的概念。请注意，我们刚刚一起构建的RAG系统仍然存在许多限制：
- en: Text splitting may split words in half leading to poor retrieval and LLM confusion.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分割可能会将单词一分为二，导致检索效果不佳和LLM混淆。
- en: The LLM may still produce hallucinations and inconsistent outputs even with
    the augmented prompts.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使有增强提示，LLM仍然可能产生幻觉和不一致的结果。
- en: The search-and-retrieval system may perform poorly in certain instances.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索和检索系统在某些情况下可能表现不佳。
- en: The augmented prompts may exceed the LLM context window.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强提示可能超过LLM上下文窗口。
- en: The retrieved information from the database may lack the relevant facts due
    to an outdated or incomplete knowledge base, ambiguous queries, or poor retrieval
    algorithm.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于知识库过时或不完整、模糊查询或检索算法不佳，从数据库检索的信息可能缺少相关事实。
- en: The retrieved context may not be ordered based on relevance to the user query.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索的上下文可能不是根据与用户查询的相关性排序的。
- en: 'You can work on improving the RAG module further by implementing various other
    techniques, which I will not cover in this book:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过实施其他各种技术来进一步改进RAG模块，这些技术我将在本书中不涉及：
- en: Optimize text splitting, chunk sizing, cleaning and embedding operations.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化文本分割、块大小、清理和嵌入操作。
- en: Perform query transformations using the LLM to aid the retrieval and augmentation
    system via techniques such as prompt compression, chaining, refining, and aggregating,
    etc., to reduce hallucinations and improve LLM performance.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM执行查询转换，通过提示压缩、链式、精炼和聚合等技术，帮助检索和增强系统，以减少幻觉并提高LLM性能。
- en: Summarize or break down large augmented prompts to feed the context into the
    models using a sliding window approach.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用滑动窗口方法总结或分解大型增强提示，将上下文输入到模型中。
- en: Enhance retrieval algorithms to handle ambiguous queries and implement fallback
    mechanisms for incomplete data.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强检索算法以处理模糊查询，并实现不完整数据的回退机制。
- en: Enhance the retrieval performance with methods such as *maximal marginal relevance*
    (MMR) to enrich the augmentation process with more diverse documents.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用如*最大边际相关性*（MMR）等方法增强检索性能，以丰富增强过程，包含更多样化的文档。
- en: Implement other advanced RAG techniques like retrieval reranking and filtering,
    hierarchical database indices, RAG fusion, retrieval augmented thoughts (RAT),
    etc., to improve the overall generation performance.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施其他高级RAG技术，如检索重排序和过滤、分层数据库索引、RAG融合、检索增强思维（RAT）等，以提高整体生成性能。
- en: I’ll let you research these techniques in more detail and implement them as
    additional exercises on your own.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我会让你更详细地研究这些技术，并将它们作为额外的练习自己实现。
- en: In the next section, well review other techniques for optimizing your GenAI
    services to avoid blocking the server with compute-bound operations such as model
    inference.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾其他优化GenAI服务的技术，以避免使用计算受限操作（如模型推理）阻塞服务器。
- en: Optimizing Model Serving for Memory- and Compute-Bound AI Inference Tasks
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化内存和计算受限的AI推理任务的服务
- en: So far, we’ve looked at optimizing the operations of our service that are I/O
    bound. You learned to leverage asynchronous programming to interact with the web,
    databases, and files by building a web scraper and a RAG module.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了优化服务中I/O受限的操作。你通过构建网络爬虫和RAG模块，学习了如何利用异步编程与网络、数据库和文件进行交互。
- en: Using async tools and techniques, your service remained responsive when interacting
    with the web, the filesystem, and databases. However, if you’re self-hosting the
    model, switching to async programming techniques won’t fully eliminate the long
    waiting times. This is because the bottleneck will be model inference operations.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 使用异步工具和技术，你的服务在与网络、文件系统和数据库交互时保持响应。然而，如果你是自行托管模型，切换到异步编程技术并不能完全消除长时间的等待。这是因为瓶颈将是模型推理操作。
- en: Compute-Bound Operations
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算受限操作
- en: You can speed up the inference by running models on GPUs to massively parallelize
    computations. Modern GPUs have staggering compute power measured by the number
    of *floating-point* operations per second (FLOPS), with modern GPUs reaching teraflops
    (NVIDIA A100) or petaflops (NVIDIA H100) of compute. However, despite their significant
    power and parallelization capabilities, modern GPU cores are often underutilized
    under concurrent workloads with larger models.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在GPU上运行模型来加速推理，从而大规模并行化计算。现代GPU的计算能力惊人，以每秒浮点运算次数（FLOPS）来衡量，现代GPU的计算能力达到每秒兆次（NVIDIA
    A100）或每秒千兆次（NVIDIA H100）。然而，尽管它们具有显著的计算能力和并行化能力，现代GPU核心在处理大型模型的同时并发工作负载时往往利用率不高。
- en: When self-hosting models on GPUs, model parameters are loaded from disk to RAM
    (I/O bound) and then moved from RAM to the GPU high-bandwidth memory by the CPU
    (memory bound). Once model parameters are loaded on the GPU memory, inference
    is performed (compute bound).
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 当在GPU上自行托管模型时，模型参数从磁盘加载到RAM（I/O受限）中，然后由CPU（内存受限）将它们从RAM移动到GPU的高带宽内存中。一旦模型参数加载到GPU内存中，就会进行推理（计算受限）。
- en: Counterintuitively, model inference for larger GenAI models such as SDXL and
    LLMs is not I/O- or compute-bound, but rather memory-bound. This means it takes
    more time to load 1 MB of data into GPU’s compute cores than it takes for those
    compute cores to process 1 MB of data. Inevitably, to maximize the concurrency
    of your service, you will need to *batch* the inference requests and fit the largest
    batch size you can into the GPU high-bandwidth memory.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 与直觉相反，对于像SDXL和LLMs这样的大型GenAI模型，模型推理不是I/O或计算受限，而是内存受限。这意味着将1 MB的数据加载到GPU的计算核心中所需的时间，比这些计算核心处理1
    MB数据所需的时间更长。不可避免地，为了最大限度地提高服务的并发性，你需要将推理请求进行批处理，并将尽可能大的批次大小放入GPU的高带宽内存中。
- en: Therefore, even when using async techniques and latest GPUs, your server can
    be blocked waiting for billions of model parameters to be loaded to the GPU high-bandwidth
    memory during each request. To avoid blocking the server, you can decouple the
    memory-bound model-serving operations from your FastAPI server by externalizing
    model serving, as we touched upon in [Chapter 3](ch03.html#ch03).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使使用异步技术和最新的GPU，你的服务器在每次请求期间也可能因为等待数十亿个模型参数加载到GPU高带宽内存而被阻塞。为了避免阻塞服务器，你可以通过外部化模型服务，就像我们在[第3章](ch03.html#ch03)中提到的那样，将内存受限的模型服务操作与你的FastAPI服务器解耦。
- en: Let’s see how to delegate model serving to another process.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将模型服务委托给另一个进程。
- en: Externalizing Model Serving
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部化模型服务
- en: You have several options available to you when externalizing your model-serving
    workloads. You can either host models on another FastAPI server or use specialized
    model inference servers.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 当你外部化你的模型服务负载时，你有几个选项可供选择。你可以将模型托管在另一个FastAPI服务器上，或者使用专门的模型推理服务器。
- en: Specialized inference servers support only a limited set of GenAI model architectures.
    However, if your model architecture is supported, you will save a lot of time
    not having to implement inference optimizations yourself. For instance, if you
    need to self-host LLMs, LLM-serving frameworks can perform several inference optimizations
    for you such as batch processing, tensor parallelism, quantization, caching, streaming
    outputs, GPU memory management, etc.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 专门的推理服务器仅支持有限的一组 GenAI 模型架构。然而，如果您的模型架构得到支持，您将节省大量时间，无需自己实现推理优化。例如，如果您需要自行托管
    LLM，LLM-serving 框架可以为您执行多个推理优化，例如批量处理、张量并行、量化、缓存、流式输出、GPU 内存管理等。
- en: Since we’ve been mostly working with LLMs in this chapter, I will show you how
    to integrate vLLM, an open source LLM server that can start a FastAPI server for
    you matching the OpenAI API specification. vLLM also has seamless integration
    with popular open source Hugging Face model architectures including GPT, Llama,
    Gemma, Mistral, Falcon, etc.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们本章主要使用 LLM，我将向您展示如何集成 vLLM，这是一个开源的 LLM 服务器，可以为您启动一个符合 OpenAI API 规范的 FastAPI
    服务器。vLLM 还与流行的开源 Hugging Face 模型架构无缝集成，包括 GPT、Llama、Gemma、Mistral、Falcon 等。
- en: Note
  id: totrans-460
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, other LLM hosting servers you can use include NVIDIA
    Triton Inference Server, Ray Serve, Hugging Face Inference, and OpenLLM, among
    others.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，您还可以使用其他 LLM 托管服务器，包括 NVIDIA Triton Inference Server、Ray Serve、Hugging
    Face Inference 和 OpenLLM 等。
- en: There are features, benefits, and drawbacks to using each including the supported
    model architectures. I recommend researching these servers prior to adopting them
    in your own use cases.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每个服务器都有其特点、优势和劣势，包括支持的模型架构。我建议在将它们用于自己的用例之前对这些服务器进行研究。
- en: 'You can start your own vLLM FastAPI server via a single command, as shown in
    [Example 5-15](#vllm). To run the code in [Example 5-15](#vllm), you will need
    to install `vllm` using:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过单个命令启动自己的 vLLM FastAPI 服务器，如[示例 5-15](#vllm)所示。要运行[示例 5-15](#vllm)中的代码，您需要使用以下命令安装
    `vllm`：
- en: '[PRE19]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Warning
  id: totrans-465
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: At the time of writing, vLLM only supports Linux platforms (including WSL) with
    NVIDIA-compatible GPUs to run CUDA toolkit dependencies. Unfortunately, you can’t
    install vLLM on Mac or Windows machines for local testing.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，vLLM 仅支持 Linux 平台（包括 WSL）以及与 NVIDIA 兼容的 GPU 来运行 CUDA 工具包依赖项。不幸的是，您无法在
    Mac 或 Windows 机器上安装 vLLM 进行本地测试。
- en: vLLM is designed for production inference workloads on NVIDIA GPUs in Linux
    environments where the server can delegate requests to multiple GPU cores via
    *tensor parallelism*. It does also support distributed computing when scaling
    services beyond a single machine via its Ray Serve dependency.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: vLLM 设计用于在 Linux 环境中的 NVIDIA GPU 上进行生产推理工作负载，其中服务器可以通过 *张量并行* 将请求委派给多个 GPU 核心。它还通过其
    Ray Serve 依赖项支持分布式计算，当扩展服务超出单台机器时。
- en: Please consult vLLM documentation for more details related to distributed inference
    and serving.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 vLLM 文档以获取有关分布式推理和服务的更多详细信息。
- en: Example 5-15\. Starting the vLLM FastAPI OpenAI API server for TinyLlama on
    a Linux machine with 4x 16 GB NVIDIA T4 GPUs
  id: totrans-469
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-15\. 在具有 4x 16 GB NVIDIA T4 GPU 的 Linux 机器上启动 vLLM FastAPI OpenAI API 服务器用于
    TinyLlama
- en: '[PRE20]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO14-1)'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO14-1)'
- en: Start an OpenAI-compatible API server with FastAPI to serve the TinyLlama model.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FastAPI 启动一个与 OpenAI 兼容的 API 服务器以提供 TinyLlama 模型。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO14-2)'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO14-2)'
- en: Use the `float16` medium precision data type. `float16` is compatible with GPU
    hardware, whereas `bfloat16` is generally compatible with CPU hardware.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `float16` 中精度数据类型。`float16` 与 GPU 硬件兼容，而 `bfloat16` 通常与 CPU 硬件兼容。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO14-3)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO14-3)'
- en: Leverage vLLM tensor parallelism feature to run the API server on four GPUs.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 vLLM 张量并行特性在四个 GPU 上运行 API 服务器。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO14-4)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO14-4)'
- en: Set a secret token for basic authentication to secure the LLM server. This is
    useful for secure machine-to-machine communication, for instance, to directly
    communicate with your current FastAPI service.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 为基本身份验证设置一个秘密令牌以保护 LLM 服务器。这对于安全的机器到机器通信很有用，例如，直接与您的当前 FastAPI 服务进行通信。
- en: With the vLLM FastAPI server up and running, you can now replace the model-serving
    logic in your current service with network calls to the vLLM server. Refer to
    [Example 5-16](#vllm_fastapi_text_generation) for implementation details.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-16\. Replace model serving with asynchronous API calls to the new
    vLLM server
  id: totrans-480
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO15-1)'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Use `aiohttp` to create an asynchronous session for sending `POST` requests
    to the vLLM FastAPI server. This logic replaces the Hugging Face model pipeline
    inference logic on the current FastAPI server.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO15-2)'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: Since the vLLM server is OpenAI compatible, you can access the output content
    by following the OpenAI API specification.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: Next, remove the code related to the FastAPI lifespan so that your current service
    won’t load the TinyLlama model. You can achieve this by following the code in
    [Example 5-17](#vllm_fastapi_handler).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-17\. Remove the FastAPI lifespan and update the text generation handler
    to be asynchronous
  id: totrans-487
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO16-1)'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: There is no need to use FastAPI `lifespan` anymore since the model is now served
    by an external vLLM FastAPI server.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO16-2)'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: Make `serve_text_to_text_controller` an async route handler as it is now performing
    I/O operations to the vLLM server. It is no longer running synchronous compute-bound
    model inference operations as those are delegated to the vLLM server to manage.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you’ve now achieved concurrency with your AI inference workloads.
    You implemented a form of multiprocessing on a single machine by moving your LLM
    inference workloads to another server. Both servers are now running on separate
    cores with your LLM server delegating work to multiple GPU cores, leveraging parallelism.
    This means your main server is now able to process multiple incoming requests
    and do other tasks than processing one LLM inference operation at a time.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-494
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bear in mind that any concurrency you’ve achieved so far has been limited to
    a single machine.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: To support more concurrent users, you may need more machines with CPU and GPU
    cores. At that point, distributed computing frameworks like Ray Serve and Kubernetes
    can help to scale and orchestrate your services beyond a single worker machine
    using parallelism.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: Before integrating vLLM, you would experience long waiting times between requests
    because your main server was too busy running inference operations. With vLLM,
    there is now a massive reduction in latency and increase in throughput of your
    LLM service.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: In addition to model compression mechanisms like quantization, vLLM uses other
    optimization techniques including continuous request batching, cache partitioning
    (paged attention), reduced GPU memory footprint via memory sharing, and streaming
    outputs to achieve smaller latency and high throughput.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at both the request batching and paged attention mechanisms in more
    detail to understand how to further optimize LLM inference.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看请求批处理和分页注意力机制，以了解如何进一步优化 LLM 推理。
- en: Request batching and continuous batching
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 请求批处理和连续批处理
- en: As we discussed in [Chapter 3](ch03.html#ch03), LLMs produce the next token
    prediction in an autoregressive manner, as you can see in [Figure 5-13](#autoregressive_prediction5).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第 3 章](ch03.html#ch03)中讨论的，LLM 以自回归的方式生成下一个标记预测，正如你在[图 5-13](#autoregressive_prediction5)中可以看到的那样。
- en: '![bgai 0513](assets/bgai_0513.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0513](assets/bgai_0513.png)'
- en: Figure 5-13\. Autoregressive prediction
  id: totrans-503
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-13\. 自回归预测
- en: This means the LLMs must perform several inference iterations in a loop to produce
    a response, and each iteration produces a single output token. The input sequence
    grows as each iteration’s output token is appended to the end, and the new sequence
    is forwarded to the model in the next iteration step. Once the model generates
    an end-of-sequence token, the generation loop stops. Essentially, the LLM produces
    a sequence of completion tokens, stopping only after producing a stop token or
    reaching a maximum sequence length.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 LLM 必须在循环中执行多次推理迭代以生成响应，并且每次迭代生成一个输出标记。随着每个迭代的输出标记被附加到末尾，输入序列会增长，并在下一个迭代步骤中将新序列转发给模型。一旦模型生成一个序列结束标记，生成循环就会停止。本质上，LLM
    生成一系列的完成标记，只有在生成一个停止标记或达到最大序列长度后才会停止。
- en: The LLM must calculate several attention maps for each token in the sequence
    so that it can iteratively make the next token predictions.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 必须为序列中的每个标记计算几个注意力图，以便它可以迭代地做出下一个标记预测。
- en: Fortunately, GPUs can parallelize the attention map calculations for each iteration.
    As you learned, these attention maps are capturing the meaning and context of
    each token within the input sequence and are expensive to calculate. Therefore,
    to optimize inference, LLMs use *key-value* (KV) *caching* to store calculated
    maps in the GPU memory.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，GPU 可以并行化每个迭代的注意力图计算。正如你所学的，这些注意力图正在捕捉输入序列中每个标记的意义和上下文，并且计算成本很高。因此，为了优化推理，LLM
    使用 *键值* (KV) *缓存* 来在 GPU 内存中存储计算出的图。
- en: Tip
  id: totrans-507
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The attention map formula computes a *value (V)* based on a given *query (Q)*
    and a *key (K)*.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力图公式根据给定的 *查询 (Q)* 和 *键 (K)* 计算一个 *值 (V)*。
- en: Q = KV
  id: totrans-509
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Q = KV
- en: This calculation has to be done for each token in the sequence but luckily can
    be vectorized using large matrix multiplication operations on a GPU.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算必须在序列中的每个标记上执行，但幸运的是，可以使用在 GPU 上进行的大矩阵乘法操作进行向量化。
- en: However, storing parameters on the GPU memory for reuse between iterations can
    consume huge chunks of GPU memory. For instance, a 13B-parameter model consumes
    nearly 1 MB of state for each token in a sequence on top of all those 13B model
    parameters. This means there is a limited number of tokens you can store in memory
    for reuse.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将参数存储在 GPU 内存中以供迭代间重用可能会消耗大量的 GPU 内存。例如，一个 1300 亿参数的模型在所有 1300 亿模型参数之上，每个序列中的每个标记会消耗近
    1 MB 的状态。这意味着你可以存储在内存中用于重用的标记数量是有限的。
- en: If you’re using a higher-end GPU, such as the A100 with 40 GB RAM, you can only
    hold 14 K tokens in memory at once, while the rest of the memory is used up for
    storing 26 GB of model parameters. In short, the GPU memory consumed scales with
    the base model size plus the length of the token sequence.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是高端 GPU，例如具有 40 GB RAM 的 A100，你一次只能保留 14 K 个标记在内存中，其余的内存用于存储 26 GB 的模型参数。简而言之，GPU
    内存消耗与基础模型大小加上标记序列长度成正比。
- en: To make matters worse, if you need to serve multiple users concurrently by batching
    requests, your GPU memory has to be shared between multiple LLM inferences. As
    a result, you have less memory to store longer sequences, and your LLM is constrained
    to a shorter context window. On the other hand, if you want to maintain a large
    context window, then you can’t handle more concurrent users. As an example, a
    sequence length of 2048 means that your batch size will be limited to 7 concurrent
    requests (or 7 prompt sequences). Realistically, this is an upper-bound limit
    and doesn’t leave room for storing intermediate computations, which will reduce
    the aforementioned numbers even further.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，如果你需要通过批处理请求同时服务多个用户，你的GPU内存必须由多个LLM推理共享。因此，你存储较长序列的内存更少，你的LLM被限制在一个较短的上下文窗口内。另一方面，如果你想保持一个大的上下文窗口，那么你无法处理更多的并发用户。例如，序列长度为2048意味着你的批次大小将限制在7个并发请求（或7个提示序列）。实际上，这是一个上限，没有为存储中间计算留出空间，这将进一步减少上述数字。
- en: What this all means is that LLMs are failing to fully saturate the GPU’s available
    resources. The primary reason is that a significant portion of the GPU’s memory
    bandwidth is consumed in loading the model parameters instead of processing inputs.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切意味着LLM未能充分利用GPU的可供资源。主要原因是一个很大的部分GPU的内存带宽被消耗在加载模型参数上，而不是处理输入。
- en: The first step to reduce the load on your services is to integrate the most
    efficient models. Often, smaller and more compressed models could do the job you’re
    asking of them, with a similar performance to their larger counterparts.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 减轻你的服务负载的第一步是集成最有效的模型。通常，较小且更压缩的模型可以完成你所要求的工作，其性能与其较大的对应物相似。
- en: Another suitable solution to the GPU underutilization problem is to implement
    *request batching* where the model processes multiple inputs in groups, reducing
    the overhead of loading model parameters for each request. This is more efficient
    in using the chip’s memory bandwidth, leading to higher compute utilization, higher
    throughput, and less expensive LLM inference. LLM inference servers like vLLM
    take advantage of batching plus fast attention, KV caching, and paged attention
    mechanisms to maximize throughput.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 解决GPU利用率不足的另一个合适方案是实施*请求批处理*，其中模型以组的形式处理多个输入，减少了每个请求加载模型参数的开销。这更有效地利用了芯片的内存带宽，从而提高了计算利用率、吞吐量和更经济的LLM推理。LLM推理服务器如vLLM利用批处理加上快速注意力、KV缓存和分页注意力机制来最大化吞吐量。
- en: You can see the difference of response latency and throughput with and without
    batching in [Figure 5-14](#with_without_batching).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图5-14](#with_without_batching)中看到带批处理和不带批处理时的响应延迟和吞吐量的差异。
- en: '![bgai 0514](assets/bgai_0514.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0514](assets/bgai_0514.png)'
- en: Figure 5-14\. LLM server response latency and throughput with and without batching
  id: totrans-519
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-14\. 带批处理和不带批处理的LLM服务器响应延迟和吞吐量
- en: 'There are two ways to implement batching:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 实现批处理有两种方式：
- en: Static batching
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 静态批处理
- en: The size of the batch remains constant.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 批次的大小保持不变。
- en: Dynamic or continuous batching
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 动态或连续批处理
- en: The size of batch is determined based on demand.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 批次的大小是根据需求确定的。
- en: In *static batching*, we wait for a predetermined number of incoming requests
    to arrive before we batch and process them through the model. However, since requests
    can finish at any time in a batch, we’re effectively delaying responses to every
    request—​and increasing latency—​until the whole batch is processed.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在*静态批处理*中，我们在将请求批处理并通过模型处理之前，等待预定数量的传入请求到达。然而，由于请求可以在批次中的任何时间完成，我们实际上是在延迟每个请求的响应——并增加延迟——直到整个批次处理完毕。
- en: Releasing the GPU resource can also be tricky when processing a batch and adding
    new requests to the batch that may be at different completion states. As a result,
    the GPU remains underutilized as the generated sequences within a batch vary and
    don’t match the length of the longest sequence in that batch.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理批次并添加可能处于不同完成状态的新请求时，释放GPU资源也可能很棘手。因此，由于批次内生成的序列变化且不匹配该批次中最长序列的长度，GPU仍然处于低利用率状态。
- en: '[Figure 5-15](#static_batching) illustrates static batching in the context
    of LLM inference.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-15](#static_batching)说明了在LLM推理上下文中的静态批处理。'
- en: '![bgai 0515](assets/bgai_0515.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0515](assets/bgai_0515.png)'
- en: Figure 5-15\. Static batching with fixed batch size
  id: totrans-529
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-15\. 固定批次大小的静态批处理
- en: In [Figure 5-15](#static_batching) you will notice the white blocks representing
    underutilized GPU computation time. Only one input sequence in the batch saturated
    the GPU across the batch’s processing timeline.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5-15](#static_batching)中，你会注意到代表未充分利用的GPU计算时间的白色块。只有批次中一个输入序列在整个处理时间线上饱和了GPU。
- en: Aside from adding unnecessary waiting times and not saturating the GPU utilization,
    what makes static batching problematic is that users of an LLM-powered chatbot
    service won’t be providing fixed-length prompts or expect fixed-length outputs.
    The variance in generation outputs could cause massive underutilization of GPUs.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 除了增加不必要的等待时间和没有充分利用GPU利用率之外，静态分批的问题在于，LLM驱动的聊天机器人服务的用户不会提供固定长度的提示或期望固定长度的输出。生成输出的变化可能导致GPU的巨大低利用率。
- en: A solution is to avoid assuming fixed input or output sequences and instead
    set dynamic batch sizes during the processing of a batch. In *dynamic* or *continuous
    batching*, the size of batch can be set based on the incoming request sequence
    length and the available GPU resource. With this approach, new generation requests
    can be inserted in a batch by replacing completed requests to yield higher GPU
    utilization than static batching.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是避免假设固定的输入或输出序列，而是在处理批次的期间设置动态批大小。在*动态*或*连续分批*中，批大小可以根据传入的请求序列长度和可用的GPU资源来设置。采用这种方法，可以通过替换完成的请求来在批次中插入新的请求，从而比静态分批提供更高的GPU利用率。
- en: '[Figure 5-16](#dynamic_batching) shows how dynamic or continuous batching can
    fully saturate the GPU resource.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-16](#dynamic_batching)展示了动态或连续分批如何完全利用GPU资源。'
- en: '![bgai 0516](assets/bgai_0516.png)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0516](assets/bgai_0516.png)'
- en: Figure 5-16\. Dynamic/continuous batching with variable batch size
  id: totrans-535
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-16\. 可变批量大小的动态/连续分批
- en: While the model parameters are loaded, requests can keep flowing in, and the
    LLM inference server schedules and insert them into the batch to maximize GPU
    usage. This approach leads to higher throughput and reduced latency.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型参数被加载时，请求可以持续流入，LLM推理服务器会调度并将它们插入到批次中，以最大化GPU利用率。这种方法导致更高的吞吐量和更低的延迟。
- en: 'If you’re building a LLM inference server, you will probably want to bake in
    the continuous batching mechanism into your server. However, the good news is
    that the vLLM server already provides continuous batching out of the box with
    its FastAPI server, so you don’t have to implement all of that yourself. Additionally,
    it also ships with another important GPU optimization feature, which sets it apart
    from other alternative LLM inference frameworks: paged attention.'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建LLM推理服务器，你可能希望将连续分批机制嵌入到你的服务器中。然而，好消息是vLLM服务器已经通过其FastAPI服务器提供了开箱即用的连续分批，因此你不必自己实现所有这些。此外，它还附带另一个重要的GPU优化功能，使其与其他LLM推理框架不同：分页注意力。
- en: Paged attention
  id: totrans-538
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分页注意力
- en: Efficient memory usage is a critical challenge for systems that handle high-throughput
    serving, particularly for LLMs. For faster inference, today’s models rely on *KV
    caches* to store and reuse attention maps, which grow exponentially as input sequence
    lengths increase.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 高吞吐量服务系统，尤其是对于大型语言模型（LLMs）来说，高效使用内存是一个关键的挑战。为了实现更快的推理，当前的模型依赖于*键值缓存（KV caches）*来存储和重用注意力图，随着输入序列长度的增加，这些图会呈指数增长。
- en: '*Paged attention* is a novel solution designed to minimize the memory demands
    of these KV caches, subsequently enhancing the memory efficiency of LLMs and making
    them more viable for use on devices with limited resources. In transformer-based
    LLMs, attention key and value tensors are generated for each input token to capture
    essential context. Instead of recalculating these tensors at every step, they’re
    saved in the GPU memory as a KV cache, which serves as the model’s memory. However,
    the KV cache can grow to enormous sizes, such as 40 GB for a model with 13B parameters,
    posing a significant challenge for efficient storage and access, particularly
    on hardware with constrained resources.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '*分页注意力*是一种旨在最小化这些KV缓存内存需求的新颖解决方案，从而提高LLMs的内存效率，使它们在资源有限的设备上使用变得更加可行。在基于transformer的LLMs中，为每个输入标记生成注意力键和值张量以捕获关键上下文。而不是在每一步重新计算这些张量，它们被保存在GPU内存中作为KV缓存，充当模型的内存。然而，KV缓存可以增长到巨大的大小，例如，对于具有130亿个参数的模型，其大小可达40
    GB，这对高效存储和访问构成了重大挑战，尤其是在资源受限的硬件上。'
- en: Paged attention introduces a method that breaks down the KV cache into smaller,
    more manageable segments called *pages*, each holding a KV vector for a set number
    of tokens. With this segmentation, paged attention can efficiently load and access
    KV caches during the attention computations. You can compare this technique to
    how the virtual memory is managed by operating systems, where the logical arrangement
    of data is separated from its physical storage. Essentially, a block table maps
    the logical blocks to physical ones, allowing for dynamic allocation of memory
    as new tokens are processed. The core idea is to avoid memory fragmentation by
    leveraging logical blocks (instead of physical ones) and use a mapping table to
    quickly access data stored in a paged physical memory.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 分页注意力引入了一种方法，将KV缓存分解成更小、更易于管理的段，称为*页面*，每个页面包含一定数量的标记的KV向量。通过这种分割，分页注意力可以在注意力计算期间有效地加载和访问KV缓存。你可以将这种技术比作操作系统如何管理虚拟内存，其中数据的逻辑排列与其物理存储分离。本质上，一个块表将逻辑块映射到物理块，允许在处理新标记时动态分配内存。核心思想是通过利用逻辑块（而不是物理块）来避免内存碎片化，并使用映射表快速访问存储在分页物理内存中的数据。
- en: 'You can break down the paged attention mechanism into several steps:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将分页注意力机制分解为几个步骤：
- en: Partitioning the KV cache
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 分区KV缓存
- en: The cache is split into fixed-size pages, with each containing a portion of
    the key-value pairs.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存被分成固定大小的页面，每个页面包含部分键值对。
- en: Building the lookup table
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 构建查找表
- en: A table is created to map query keys to their corresponding pages, facilitating
    quick allocation and retrieval.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个表来映射查询键到它们对应的页面，从而便于快速分配和检索。
- en: Selective loading
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性加载
- en: Only the necessary pages for the current input sequence are loaded during inference,
    reducing the memory footprint.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，仅加载当前输入序列所需的必要页面，从而减少内存占用。
- en: Attention computation
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力计算
- en: The model computes attention using the key-value pairs from the loaded pages.
    This approach aims to make LLMs more accessible by addressing the memory bottleneck,
    potentially enabling their deployment on a wider range of devices.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用从加载的页面中提取的关键值对来计算注意力。这种方法旨在通过解决内存瓶颈，使LLM更加易于访问，从而可能使其能够在更广泛的设备上部署。
- en: The aforementioned steps enable the vLLM server to maximize memory usage efficiency
    through the mapping of physical and logical memory blocks so that the KV cache
    is efficiently stored and retrieved during generation.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤通过映射物理和逻辑内存块，使vLLM服务器能够最大化内存使用效率，从而在生成过程中有效地存储和检索KV缓存。
- en: In a [blog post published on Anyscale.com](https://oreil.ly/WgRfJ), the authors
    have researched and compared the performance of various LLM-serving frameworks
    during inference. The authors concluded that leveraging both paged attention and
    continuous batching mechanisms are so powerful in optimizing GPU memory usage
    that the vLLM server was able to reduce latencies by 4 times and throughput by
    up to 23 times.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇发布在Anyscale.com的[博客文章](https://oreil.ly/WgRfJ)中，作者们研究了比较了各种LLM服务框架在推理过程中的性能。作者们得出结论，利用分页注意力和连续批处理机制在优化GPU内存使用方面非常强大，这使得vLLM服务器能够将延迟降低4倍，吞吐量提高至23倍。
- en: In the next section, we will turn our attention to GenAI workloads that can
    take a long time to process and are compute-bound. This is mostly the case with
    large non-LLM models such as SDXL where performing batch inferences (such as batch
    image generation) for multiple users may prove challenging.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将关注那些处理时间较长且计算密集型的GenAI工作负载。这通常适用于大型非LLM模型，如SDXL，为多个用户执行批量推理（如批量图像生成）可能具有挑战性。
- en: Managing Long-Running AI Inference Tasks
  id: totrans-554
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理长时间运行的AI推理任务
- en: With the ability to host models in a separate process outside the FastAPI event
    loop, you can turn your attention to blocking operations that take a long time
    to complete.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在FastAPI事件循环之外单独的进程中托管模型，你可以将注意力转向那些需要较长时间才能完成的阻塞操作。
- en: In the previous section, you leveraged specialized frameworks such as vLLM to
    externally host and optimize the inference workloads of your LLMs. However, you
    may still run into models that can take significant time to generate results.
    To prevent your users from waiting, you should manage tasks that generate models
    and take a long time to complete.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你利用了如vLLM之类的专用框架来外部托管和优化你的LLM的推理工作负载。然而，你可能仍然会遇到那些生成结果需要大量时间的模型。为了防止你的用户等待，你应该管理那些生成模型且耗时较长的任务。
- en: Several GenAI models such as Stable Diffusion XL may take several minutes, even
    on a GPU, to produce results. In most cases, you can ask your users to wait until
    the generation process is complete. But if users are using a single model simultaneously,
    the server will have to queue these requests. When your users work with generative
    models, they need to interact with it several times to guide the model to the
    results they want. This usage pattern creates a large backlog of requests, and
    users at the end of the queue will have to wait a long time before they see any
    results.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 一些GenAI模型，如Stable Diffusion XL，即使在GPU上也可能需要几分钟的时间来生成结果。在大多数情况下，你可以要求用户等待生成过程完成。但如果用户同时使用单个模型，服务器将不得不排队这些请求。当用户与生成模型一起工作时，他们需要多次与之交互，以引导模型达到他们想要的结果。这种使用模式会创建大量的请求积压，队列末尾的用户将不得不等待很长时间才能看到任何结果。
- en: If there was a way to handle long-running tasks without making the users wait,
    that would be perfect. Luckily, FastAPI provides a mechanism for solving these
    kinds of problems.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一种方法可以处理长时间任务而不让用户等待，那将是完美的。幸运的是，FastAPI提供了一种机制来解决这类问题。
- en: FastAPI’s *background tasks* is a mechanism you can leverage to respond to users
    while your models are busy processing the request. You’ve been briefly introduced
    to this feature while building the RAG module where a background task was populating
    a vector database with the content of the uploaded PDF documents.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI的*后台任务*是一种机制，你可以在模型忙于处理请求时利用它来响应用户。你在构建RAG模块时已经简要介绍了这个功能，其中后台任务正在用上传的PDF文档的内容填充向量数据库。
- en: Using background tasks, your users can continue sending requests or carry on
    with their day without having to wait. You can either save the results to disk
    or a database for later retrieval or provide a polling system so that their client
    can ping for updates as the model processes the requests. Another option is to
    create a live connection between the client and the server so that their UI is
    updated with the results as soon as it becomes available. All these solutions
    are doable with FastAPI’s background tasks.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 使用后台任务，用户可以继续发送请求或继续他们的日常活动，而无需等待。你可以将结果保存到磁盘或数据库中以便稍后检索，或者提供一个轮询系统，以便客户端可以在模型处理请求时获取更新。另一种选择是创建客户端和服务器之间的实时连接，以便他们的UI在结果可用时立即更新。所有这些解决方案都可以使用FastAPI的后台任务实现。
- en: '[Example 5-18](#fastapi_background_tasks) shows how to implement background
    tasks to handle long-running model inferences.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-18](#fastapi_background_tasks)展示了如何实现后台任务来处理长时间运行的模型推理。'
- en: Example 5-18\. Using background tasks to handle long-running model inference
    (e.g., batch generating images)
  id: totrans-562
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-18\. 使用后台任务处理长时间运行模型推理（例如，批量生成图像）
- en: '[PRE23]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO17-1)'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_achieving_concurrency_in_ai_workloads_CO17-1)'
- en: Generate multiple images in a batch using an external model-serving API like
    [Ray Serve](https://oreil.ly/NjlV4).
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 使用外部模型服务API如[Ray Serve](https://oreil.ly/NjlV4)批量生成多个图像。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO17-2)'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_achieving_concurrency_in_ai_workloads_CO17-2)'
- en: Loop over the generated images and asynchronously save each to disk using the
    `aiofiles` library. In production, you can also save output images to cloud storage
    solutions that clients can directly fetch from.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`aiofiles`库循环遍历生成的图像，并异步将每个图像保存到磁盘。在生产环境中，你还可以将输出图像保存到客户端可以直接获取的云存储解决方案中。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO17-3)'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_achieving_concurrency_in_ai_workloads_CO17-3)'
- en: Enable the controller to perform background tasks.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 启用控制器执行后台任务。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO17-4)'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_achieving_concurrency_in_ai_workloads_CO17-4)'
- en: Pass the `batch_generate_image` function definition to a FastAPI background
    tasks handler with the required arguments.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 将`batch_generate_image`函数定义及其所需参数传递给FastAPI后台任务处理器。
- en: '[![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO17-5)'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)(#co_achieving_concurrency_in_ai_workloads_CO17-5)'
- en: Return a generic success message to the client before processing the background
    task so that the user is not kept waiting.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理后台任务之前向客户端返回一个通用的成功消息，这样用户就不会被长时间等待。
- en: In [Example 5-18](#fastapi_background_tasks), you’re allowing FastAPI to run
    inference operations in the background (via an external model server API) such
    that the event loop remains unblocked to process other incoming requests. You
    can even run multiple tasks in the background, such as generating images in batches
    (in separate processes) and sending notification emails. These tasks are added
    to a queue and processed sequentially without blocking the user. You can then
    store the generated images and expose an additional endpoint that clients can
    use to poll for status updates and to retrieve the inference results.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例5-18](#fastapi_background_tasks)中，你允许FastAPI在后台（通过外部模型服务器API）运行推理操作，这样事件循环就不会被阻塞以处理其他传入的请求。你甚至可以在后台运行多个任务，例如批量生成图像（在单独的进程中）和发送通知电子邮件。这些任务被添加到队列中，并按顺序处理，不会阻塞用户。然后你可以存储生成的图像，并公开一个额外的端点，客户端可以使用它来轮询状态更新并检索推理结果。
- en: Warning
  id: totrans-575
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Background tasks run in the same event loop. They won’t provide true parallelism;
    they only provide concurrency.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 背景任务在同一个事件循环中运行。它们不会提供真正的并行性；它们只提供并发性。
- en: If you run heavy CPU-bound operations like AI inference in background tasks,
    it’ll block the main event loop until all background tasks are completed. Similarly,
    be careful with async background tasks. If you don’t await the blocking I/O operations,
    the task will block the main server from responding to other requests, even if
    it runs in the background. FastAPI runs nonasync background tasks in an internal
    thread pool.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行像AI推理这样的重CPU密集型操作作为后台任务，它将阻塞主事件循环，直到所有后台任务完成。同样，对异步后台任务也要小心。如果你不等待阻塞I/O操作，即使它在后台运行，任务也会阻塞主服务器响应其他请求。FastAPI在内部线程池中运行非异步后台任务。
- en: While FastAPI’s background tasks are a wonderful tool for handling simple batch
    jobs, it doesn’t scale and can’t handle exceptions or retries as well as specialized
    tools. Other ML-serving frameworks like Ray Serve, BentoML, and vLLM may handle
    model serving better at scale by providing features such as request batching.
    More sophisticated tools like Celery (a queue manager), Redis (a caching database),
    and RabbitMQ (a message broker) can also be used in combination to implement a
    more robust and reliable inference pipeline.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然FastAPI的后台任务是一个处理简单批量作业的出色工具，但它无法扩展，并且无法像专用工具那样很好地处理异常或重试。其他ML服务框架，如Ray Serve、BentoML和vLLM，可以通过提供请求批处理等功能，在更大规模上更好地处理模型服务。更复杂的工具，如Celery（一个队列管理器）、Redis（一个缓存数据库）和RabbitMQ（一个消息代理），也可以组合使用，以实现更健壮和可靠的推理管道。
- en: Summary
  id: totrans-579
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explored the complex aspects of applying concurrency in AI systems.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了在AI系统中应用并发性的复杂方面。
- en: You were introduced to concurrency and parallelism concepts, including several
    types of blocking operations that prevent you from simultaneously serving users.
    You discovered concurrency techniques such as multithreading, multiprocessing,
    and asynchronous programming alongside their differences, similarities, benefits,
    and drawbacks in various use cases.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 你被介绍到并发和并行性概念，包括几种阻止你同时服务用户的阻塞操作类型。你发现了并发技术，如多线程、多进程和异步编程，以及它们在不同用例中的差异、相似之处、优点和缺点。
- en: Next, you learned about thread pools and event loops, particularly in a FastAPI
    server environment, and understood their roles in processing requests concurrently.
    This involved understanding how and why the server can be blocked if you’re not
    careful how you declare your route handlers.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你学习了关于线程池和事件循环的知识，特别是在FastAPI服务器环境中，并理解了它们在处理并发请求中的作用。这包括理解如果你不仔细声明路由处理程序，服务器为什么会阻塞。
- en: Later, you discovered how to implement asynchronous programming to manage I/O
    blocking operations. Through hands-on examples, you developed a deeper understanding
    of asynchronous interactions with databases and the web content, constructing
    both a web scraper and a RAG module.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你发现了如何实现异步编程来管理I/O阻塞操作。通过实际示例，你深入理解了与数据库和网页内容的异步交互，构建了网络爬虫和RAG模块。
- en: Furthermore, you saw why larger GenAI models can be memory hungry and create
    memory-bound blocking operations. As part of this, you were introduced to memory
    optimization techniques such as continuous batching and paged attention in serving
    LLMs to minimize memory-related bottlenecks.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你看到了为什么更大的GenAI模型可能会消耗大量内存并创建内存限制的阻塞操作。作为这部分内容，你被介绍到内存优化技术，如连续批处理和分页注意力，这些技术用于在服务LLM时最小化内存相关的瓶颈。
- en: Finally, you learned about approaches for handling long-running AI inference
    processes, ensuring your service remains responsive over prolonged operations.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学习了处理长时间运行的AI推理过程的方法，确保服务在长时间操作中保持响应。
- en: With your knowledge from this chapter, you’re now prepared to apply concurrency
    principles to your own services, crafting resilient, scalable, and high-performing
    AI applications.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的知识，你现在已经准备好将并发原则应用到自己的服务中，打造出弹性、可扩展且高性能的AI应用。
- en: The ability to handle multiple users simultaneously is a significant milestone.
    But there are additional optimizations you can perform to improve the user experience
    of your GenAI services even further. You can provide real-time updates via streaming
    technologies to progressively show near real-time results to users during generation.
    This is particularly useful for LLMs that may have longer generation times in
    conversation scenarios.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 同时处理多个用户的能力是一个重要的里程碑。但你可以进行额外的优化，以进一步提高你的GenAI服务的用户体验。你可以通过流式技术提供实时更新，在生成过程中逐步向用户展示接近实时的结果。这在对话场景中可能具有较长的生成时间的LLM中尤其有用。
- en: The upcoming chapter will explore AI streaming workloads, detailing the use
    of real-time communication technologies like server-sent events (SSE) and WebSocket
    (WS). You will learn the difference between these technologies and how to implement
    model streaming by building endpoints for real-time text-to-text, text-to-speech,
    and speech-to-text interactions.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将探讨AI流式工作负载，详细介绍了实时通信技术如服务器端事件（SSE）和WebSocket（WS）的使用。你将了解这些技术之间的区别，以及如何通过构建实时文本到文本、文本到语音和语音到文本交互的端点来实现模型流。
- en: Additional References
  id: totrans-589
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他参考文献
- en: Kwon, W., et al. (2023). [“Efficient Memory Management for Large Language Model
    Serving with PagedAttention”](https://oreil.ly/PtCqL). arXiv preprint arXiv:2309.06180.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon, W. 等. (2023). [“使用PagedAttention进行大型语言模型服务的有效内存管理”](https://oreil.ly/PtCqL).
    arXiv预印本 arXiv:2309.06180.
- en: Lewis, P., et al. (2022). [“Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks”](https://oreil.ly/r5yVL). arXiv preprint arXiv:2005.11401.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis, P. 等. (2022). [“用于知识密集型NLP任务的检索增强生成”](https://oreil.ly/r5yVL). arXiv预印本
    arXiv:2005.11401.
- en: ^([1](ch05.html#id795-marker)) A core is an individual processing unit within
    a CPU or GPU that executes instructions. Modern CPUs and GPUs have multiple cores
    to perform tasks simultaneously.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#id795-marker)) 核心是CPU或GPU中的一个独立处理单元，它执行指令。现代CPU和GPU具有多个核心以同时执行任务。
- en: ^([2](ch05.html#id802-marker)) Multithreading in most languages is parallel
    (running on multiple cores) and not concurrent. Python is changing over the next
    coming versions to do the same (free-threaded Python).
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.html#id802-marker)) 大多数语言中的多线程是并行（在多个核心上运行）而不是并发。Python将在接下来的版本中做出改变以实现同样的效果（免费线程Python）。
- en: ^([3](ch05.html#id817-marker)) You can also find a custom implementation in
    [OpenAI Cookbook on GitHub](https://oreil.ly/8E7GQ).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.html#id817-marker)) 你也可以在GitHub上的[OpenAI食谱](https://oreil.ly/8E7GQ)中找到一个自定义实现。
- en: ^([4](ch05.html#id822-marker)) The cost of setting up the threads is still incurred;
    it’s just done early to avoid doing it on the fly later.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.html#id822-marker)) 设置线程的成本仍然存在；只是提前完成，以避免稍后即时执行。
- en: ^([5](ch05.html#id830-marker)) P. Lewis et al. (2022), [“Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks”](https://oreil.ly/GCk08), arXiv
    preprint arXiv:2005.11401.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.html#id830-marker)) P. Lewis等. (2022), [“用于知识密集型NLP任务的检索增强生成”](https://oreil.ly/GCk08),
    arXiv预印本 arXiv:2005.11401.
- en: ^([6](ch05.html#id841-marker)) A dot product operation multiplies components
    of two vectors and then sums the results. It can be used to calculate the cosine
    of the angle between the vectors to quantify their similarity in direction (i.e.,
    alignment). Vector databases use it to perform semantic search on document embeddings.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch05.html#id841-marker)) 点积操作是两个向量的分量相乘然后求和。它可以用来计算向量之间角度的余弦值，以量化它们在方向上的相似性（即对齐）。向量数据库使用它来对文档嵌入进行语义搜索。
- en: ^([7](ch05.html#id843-marker)) Refer to the [Docker documentation](https://oreil.ly/V4itQ)
    for installation instructions.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch05.html#id843-marker)) 请参阅[Docker文档](https://oreil.ly/V4itQ)以获取安装说明。
