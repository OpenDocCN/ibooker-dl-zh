- en: Capitolo 5\. Raggiungere la concomitanza nei carichi di lavoro dell'IA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章\. 实现IA工作负载的同步
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作是用AI翻译的。我们很高兴收到你的反馈和评论：[translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)
- en: In questo capitolo imparerai a conoscere il ruolo e i vantaggi della programmazione
    asincrona per aumentare le prestazioni e la scalabilità dei tuoi servizi GenAI.
    Imparerai a gestire le interazioni simultanee con gli utenti e a interfacciarti
    con sistemi esterni come i database, a implementare il RAG e a leggere le pagine
    web per arricchire il contesto delimitato dai prompt del modello. Acquisirai le
    tecniche per gestire in modo efficace le operazionidelimitate dall'I/O e dalla
    CPU, soprattutto quando hai a che fare con servizi esterni o gestisci attività
    di inferenza di lunga durata.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解异步编程在提高你的GenAI服务性能和可扩展性方面的作用和优势。你将学习如何管理与用户的并发交互，如何与外部系统（如数据库）交互，如何实现RAG以及如何读取网页以丰富模型的提示上下文。你将掌握有效管理受I/O和CPU限制的操作的技术，尤其是在处理外部服务或管理长期推理活动时。
- en: Ci addentreremo anche nelle strategie per gestire in modo efficiente i compiti
    di inferenza dell'IA generativa di lunga durata, compreso l'uso del ciclo di eventi
    FastAPI per l'esecuzione dei compiti in background.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨有效管理长期AI生成任务（包括使用FastAPI事件循环执行后台任务）的策略。
- en: Ottimizzare i servizi GenAI per più utenti
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化GenAI服务以支持更多用户
- en: I carichi di lavoro dell'IA sono operazioni computazionalmente costose che possono
    impedire ai tuoi servizi GenAI di servire più richieste simultanee. Nella maggior
    parte degli scenari di produzione, più utenti utilizzeranno le tue applicazioni.
    Pertanto, i tuoi servizi dovranno servire le richieste *in modo simultaneo*in
    modo da poter eseguire più attività sovrapposte. Tuttavia, se ti interfacci con
    i modelli GenAI e con sistemi esterni come i database, il filesystem o internet,
    ci saranno operazioni che possono bloccare l'esecuzione di altre attività sul
    tuo server.Le operazioni di lunga durata chepossono interrompere il flusso di
    esecuzione del programma sono considerate *bloccanti*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: IA工作负载是计算成本高昂的操作，可能会阻止你的GenAI服务处理更多并发请求。在大多数生产场景中，更多用户将使用你的应用程序。因此，你的服务需要以**同时**的方式处理请求，以便能够执行更多重叠活动。然而，如果你与GenAI模型和外部系统（如数据库、文件系统或互联网）交互，将会有一些可能阻塞服务器上其他活动执行的操作。可能中断程序执行流程的长时间操作被认为是**阻塞的**。
- en: 'Queste operazioni di blocco possono essere di due tipi:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些阻塞操作可以分为两种类型：
- en: Ingresso/uscita (I/O) delimitato
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有限输入/输出（I/O）
- en: Quando un processo deve attendere a causa di un'operazione di input/output di
    dati, che possono provenire da un utente, da un file, da un database, da una rete
    e così via.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个进程因为数据输入/输出操作而必须等待时，这些操作可能来自用户、文件、数据库、网络等。
- en: Calcolo delimitato
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有限计算
- en: Quando un processo deve attendere a causa di un'operazione ad alta intensità
    di calcolo sulla CPU o sulla GPU. I programmi delimitati dal calcolo spingono
    i core della CPU o della GPU al loro limite eseguendo calcoli intensivi, spesso
    bloccando l'esecuzione di altre attività.^([1](ch05.html#id795)) Alcuni esempi
    sono l'elaborazione dei dati, l'inferenza o l'addestramento di modelli di intelligenza
    artificiale, il rendering di oggetti 3D, l'esecuzione di simulazioni e così via.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个进程因为CPU或GPU上的高计算强度操作而必须等待时。计算限制的程序通过执行密集计算将CPU或GPU核心推到极限，通常阻止其他活动的执行。[^1](ch05.html#id795)
    一些例子包括数据处理、模型推理或训练、3D渲染、执行模拟等。
- en: 'Hai a disposizione alcune strategie per servire più utenti:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一些策略可以用来服务更多用户：
- en: Ottimizzazione del sistema
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 系统优化
- en: Per le attività legate all'I/O, come il recupero di dati da un database, l'utilizzo
    di file su disco, le richieste di rete o la lettura di pagine web.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于I/O相关的活动，如从数据库中检索数据、使用磁盘上的文件、网络请求或读取网页。
- en: Ottimizzazione del modello
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化
- en: Per compiti delimitati dalla memoria e dal calcolo, come il caricamento e l'inferenza
    del modello
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于受内存和计算限制的任务，如模型加载和推理
- en: Sistema di accodamento
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 队列系统
- en: Per gestire compiti di inferenza di lunga durata per evitare ritardi nelle risposte.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理长时间推断任务，以避免响应延迟。
- en: 'In questa sezione analizzeremo ogni strategia in modo più dettagliato. Per
    aiutarti a consolidare l''apprendimento, implementeremo anche diverse funzioni
    che sfruttano le strategie sopra citate:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更详细地分析每种策略。为了帮助你巩固学习，我们还将实施多种利用上述策略的功能：
- en: Costruisci uno *scraper di pagine web* per recuperare e analizzare in massa
    gli URL HTTP incollati nella chat, in modo da poter chiedere al tuo LLM informazioni
    sul contenuto delle pagine web.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个*网页爬虫*，以从聊天中批量恢复和分析粘贴的 HTTP URL，以便你可以向你的 LLM 询问有关网页内容的详细信息。
- en: Aggiungi un modulo *RAG (retrieval augmented generation* ) al tuo servizio con
    un database vettoriale self-hosted come `qdrant`, in modo da poter caricare e
    dialogare con i tuoi documenti tramite il tuo servizio LLM.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一个*检索增强生成（RAG）模块*添加到你的服务中，使用自托管数据库如`qdrant`，以便可以通过你的 LLM 服务加载和与你的文档进行对话。
- en: Aggiunta di un *sistema di generazione di immagini in batch*in modo da poter
    eseguire i carichi di lavoro di generazione di immagini come attività in background
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个*批量图像生成系统*，以便能够将图像生成工作作为后台活动执行
- en: Prima di mostrarti come costruire le funzionalità sopra citate, dobbiamo approfondire
    l'argomento della *concorrenza* e del *parallelismo*, perché la comprensione di
    entrambi i concetti ti aiuterà a individuare le strategie corrette da utilizzare
    per i tuoi casi d'uso.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在向你展示如何构建上述功能之前，我们必须深入探讨*竞争*和*并行性*的主题，因为对这两个概念的理解将帮助你确定适用于你的用例的正确策略。
- en: La*concomitanza* si riferisce alla capacità di un servizio di gestire più richieste
    o attività contemporaneamente, senza completarne una dopo l'altra. Durante le
    operazioni concomitanti, le tempistiche di più attività possono sovrapporsi e
    possono iniziare e terminare in momenti diversi.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*并发性*指的是服务同时处理多个请求或活动的能力，而无需依次完成它们。在并发操作期间，多个活动的时间表可能会重叠，并且它们可以在不同的时间开始和结束。'
- en: In Python, puoi implementare la concorrenza con un singolo core della CPU passando
    da un compito all'altro su un singolo thread (tramite la programmazione asincrona)
    o tra diversi thread (tramite il multithreading).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，你可以通过在单个线程（通过异步编程）或多个线程之间（通过多线程）切换任务来在单个 CPU 核心上实现竞争。
- en: Con più core, puoi anche implementare un sottoinsieme della concorrenza chiamato
    *parallelismo*, in cui i compiti vengono suddivisi tra più lavoratori indipendenti
    (tramite il multiprocessing), ognuno dei quali esegue i compiti simultaneamente
    sulle proprie risorse isolate e su processi separati.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多核心，你还可以实现竞争的一个子集，即*并行性*，其中任务被分配给多个独立工作者（通过多进程处理），每个工作者在自己的隔离资源和独立进程中同时执行任务。
- en: Nota
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Anche se si prevede di rimuovere presto il GIL da Python, al momento in cui
    scriviamo non è possibile che più thread lavorino simultaneamente attraverso i
    task. Pertanto, la concorrenza su un singolo core può dare un'illusione di parallelismo
    anche se c'è un solo processo che sta facendo tutto il lavoro. Il singolo processo
    può essere multitasking solo cambiando i thread attivi per ridurre al minimo i
    tempi di attesa delle operazioni di blocco dell'I/O.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然预计很快就会从 Python 中移除 GIL，但在我们撰写本文时，多个线程同时通过任务工作是不可能的。因此，单核上的竞争可能会产生并行性的错觉，即使只有一个进程在完成所有工作。单个进程可以通过切换活动线程来执行多任务，以最大限度地减少
    I/O 阻塞操作的时间。
- en: Il vero parallelismo si può ottenere solo con più lavoratori (in multiprocessing).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 只有通过更多的工作者（在多进程处理中）才能实现真正的并行性。
- en: Anche se la concorrenza e il parallelismo hanno molte somiglianze, non sono
    esattamente gli stessi concetti. La grande differenza tra loro è che la concorrenza
    può aiutarti a gestire più attività intersecando la loro esecuzione, il che è
    utile per le attività legate all'I/O. Il parallelismo, invece, prevede l'esecuzione
    di più attività simultaneamente, in genere su macchine multicore, il che è più
    utile per le attività legate alla CPU.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管竞争和并行性有很多相似之处，但它们并不是完全相同的概念。它们之间最大的区别是，竞争可以帮助你通过交叉执行来管理更多活动，这对于与 I/O 相关的活动很有用。相反，并行性涉及同时执行多个活动，通常在多核机器上，这对于与
    CPU 相关的活动更有用。
- en: Puoi implementare la concorrenza utilizzando approcci come il threading o la
    programmazione asincrona (ad esempio, il time slice su una macchina single-core,
    in cui i compiti vengono interlacciati per dare l'impressione di un'esecuzione
    simultanea).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用线程或异步编程（例如，在单核机器上的时间片）等方法来实现竞争，以实现任务的交错执行，从而产生同时执行的感觉。
- en: La[Figura 5-1](#concurrency_parallelism) mostra la relazione tra concurrency
    e parallelismo.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](#concurrency_parallelism) 展示了并发与并行之间的关系。'
- en: '![bgai 0501](assets/bgai_0501.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0501](assets/bgai_0501.png)'
- en: Figura 5-1\. Concorrenza e parallelismo
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 竞争与并行
- en: Nella maggior parte dei sistemi scalabili, è possibile assistere sia alla concorrenza
    che al parallelismo.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数可扩展系统中，可以观察到竞争和并行。
- en: In un sistema concorrente, vedrai il proprietario del ristorante prendere le
    ordinazioni e cucinare gli hamburger, occupandosi di volta in volta di ciascun
    compito e passando da un'attività all'altra in modo efficace e multitasking. In
    un sistema parallelo, vedrai diversi membri del personale prendere le ordinazioni
    e altri cucinare gli hamburger allo stesso tempo. In questo caso, diversi lavoratori
    si occupano di ciascun compito contemporaneamente.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个竞争系统中，你会看到餐厅老板接订单并烹饪汉堡，一次处理一个任务，有效地多任务处理从一项活动切换到另一项活动。在一个并行系统中，你会看到不同的员工同时接订单和烹饪汉堡。在这种情况下，不同的工人同时处理每个任务。
- en: Senza il multithreading o la programmazione asincrona in un processo a thread
    singolo, il processo deve aspettare che le operazioni bloccanti finiscano prima
    di poter avviare nuove attività. Senza il multiprocessing che implementa il parallelismo
    su più core, le operazioni computazionalmente costose possono bloccare l'applicazione
    dall'avvio di altre attività.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在单线程进程中没有多线程或异步编程，进程必须等待阻塞操作完成才能启动新的活动。没有实现多核并行的多进程，计算成本高昂的操作可能会阻止应用程序启动其他活动。
- en: La[Figura 5-2](#concurrency_parallelism_timeline) mostra la distinzione tra
    esecuzione non concorrente, esecuzione concorrente senza parallelismo (singolo
    core) ed esecuzione concorrente con parallelismo (più core).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-2](#concurrency_parallelism_timeline) 展示了非并发执行、无并行性的并发执行（单个核心）以及有并行性的并发执行（多个核心）之间的区别。'
- en: 'I tre modelli di esecuzione di Python mostrati nella [Figura 5-2](#concurrency_parallelism_timeline)
    sono i seguenti:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-2](#concurrency_parallelism_timeline) 中展示的 Python 的三个执行模型如下：
- en: Nessuna concomitanza (sincrona)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Nessuna concomitanza (sincrona)
- en: Un singolo processo (su un solo core) esegue i compiti in modo sequenziale.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 单个进程（在单个核心上）以顺序方式执行任务。
- en: Concorrente e non parallelo
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争且非并行
- en: Più thread in un singolo processo (su un core) gestiscono i compiti in modo
    concorrente ma non in parallelo a causa del GIL di Python.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 单个进程（在单个核心上）中的更多线程以并发方式管理任务，但由于 Python 的 GIL，它们不是并行执行的。
- en: Concorrente e parallelo
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 并发与并行
- en: Più processi su più core eseguono i compiti in parallelo, sfruttando al massimo
    i processori multicore per ottenere la massima efficienza.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个核心上运行更多进程以并行执行任务，充分利用多核处理器以获得最大效率。
- en: '![bgai 0502](assets/bgai_0502.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0502](assets/bgai_0502.png)'
- en: Figura 5-2\. Concorrenza con e senza parallelismo
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 有无并行性的竞争
- en: Nel multiprocesso, ogni processo ha accesso al proprio spazio di memoria e alle
    proprie risorse per completare un'attività in modo isolato dagli altri processi.
    Questo isolamento può rendere i processi più stabili, in quanto se un processo
    si blocca non si ripercuoterà sugli altri, ma rende la comunicazione tra i processi
    più complessa rispetto ai thread, che condividono lo stesso spazio di memoria,
    come mostrato nella [Figura 5-3](#multiprocessing_resources).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在多进程模式下，每个进程都有自己的内存空间和资源，以独立于其他进程的方式完成活动。这种隔离可以使进程更加稳定，因为如果一个进程阻塞，不会影响到其他进程，但相对于共享相同内存空间的线程，它使得进程间的通信更加复杂，如图
    [图 5-3](#multiprocessing_resources) 所示。
- en: '![bgai 0503](assets/bgai_0503.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0503](assets/bgai_0503.png)'
- en: Figura 5-3\. La condivisione delle risorse nel multithreading e nel multiprocessing
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 多线程和多进程中的资源共享
- en: I carichi di lavoro distribuiti spesso utilizzano un processo di gestione che
    coordina l'esecuzione e la collaborazione di questi processi per evitare problemi
    come la corruzione dei dati e la duplicazione del lavoro. Un buon esempio di multiprocesso
    è quello di servire le richieste con un bilanciatore di carico che gestisce il
    traffico verso più container, ognuno dei quali esegue un'istanza della tua applicazione.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式工作负载通常使用一个管理过程来协调这些进程的执行和协作，以避免如数据损坏和工作重复等问题。一个多进程的好例子是使用负载均衡器来管理流量，该负载均衡器将流量分配到多个容器，每个容器都运行你的应用程序的一个实例。
- en: Sia il multithreading che la programmazione asincrona riducono i tempi di attesa
    delle attività di I/O perché il processore può svolgere altre attività in attesa
    dell'I/O. Tuttavia, non sono utili per le attività che richiedono calcoli pesanti,
    come l'inferenza dell'intelligenza artificiale, perché il processo è impegnato
    a calcolare alcuni risultati. Pertanto, per servire un modello GenAI self-hosted
    di grandi dimensioni a più utenti, è necessario scalare i servizi con il multiprocessing
    o utilizzare ottimizzazioni algoritmiche del modello (tramite server specializzati
    nell'inferenza del modello come vLLM).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程和异步编程可以减少I/O活动等待时间，因为处理器可以在等待I/O时执行其他任务。然而，对于需要大量计算的活动，如人工智能推理，它们并不适用，因为进程正忙于计算某些结果。因此，为了服务于大型自托管GenAI模型的多用户，需要通过多进程或使用模型算法优化（通过专门进行模型推理的服务器如vLLM）来扩展服务。
- en: Il tuo primo istinto quando lavori con modelli lenti potrebbe essere quello
    di adottare il parallelismo creando istanze multiple del tuo servizio FastAPI
    (multiprocessing) in una singola macchina per servire le richieste in parallelo.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当你与慢速模型工作时的第一个直觉可能是采用并行化，通过在单个机器上创建多个FastAPI服务实例（多进程）来并行处理请求。
- en: Sfortunatamente, più worker in esecuzione in processi separati non avranno accesso
    a uno spazio di memoria condiviso. Di conseguenza, non puoi condividere artefatti
    come un modello GenAI caricato in memoria tra istanze separate della tua app in
    FastAPI. Purtroppo, sarà necessario caricare anche una nuova istanza del modello,
    il che consumerà significativamente le tue risorse hardware. Questo perché FastAPI
    è un server web generico che non ottimizza in modo nativo il servizio dei modelli
    GenAI.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在单独进程中运行的更多工作者将无法访问共享内存空间。因此，你无法在FastAPI应用程序的不同实例之间共享如加载到内存中的GenAI模型这样的工件。不幸的是，你还需要加载一个新的模型实例，这将显著消耗你的硬件资源。这是因为FastAPI是一个通用的Web服务器，它没有原生优化GenAI模型服务。
- en: La soluzione non è il parallelismo in sé, ma l'adozione della strategia del
    model-serving esterno, come illustrato nel [Capitolo 3](ch03.html#ch03).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案不是并行本身，而是采用外部模型服务的策略，如第3章[Capitolo 3](ch03.html#ch03)中所述。
- en: L'unico caso in cui puoi trattare i carichi di lavoro dell'inferenza dell'intelligenza
    artificiale come I/O-bound, invece che come compute-bound, è quando ti affidi
    alle API di provider di AI di terze parti (ad esempio, OpenAI API). In questo
    caso, stai scaricando i compiti legati al calcolo al provider del modello attraverso
    le richieste di rete.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你唯一可以将人工智能推理工作负载视为I/O受限而不是计算受限的情况是当你依赖第三方AI提供商的API（例如，OpenAI API）时。在这种情况下，你通过网络请求将计算任务卸载给模型提供商。
- en: Dal tuo lato, i carichi di lavoro dell'inferenza dell'intelligenza artificiale
    diventano vincolati all'I/O attraverso le richieste di rete, consentendo l'uso
    della concorrenza attraverso il time slice. Il fornitore di terze parti deve preoccuparsi
    di scalare i propri servizi per gestire le inferenze del modello - che sono delimitate
    dal calcolo - attraverso le proprie risorse hardware.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从你的角度来看，人工智能推理的工作负载通过网络请求被绑定到I/O，从而允许通过时间片使用竞争。第三方服务提供商必须关注扩展自己的服务以管理模型推理——这些推理受限于计算——通过自己的硬件资源。
- en: Puoi esternalizzare il servizio e l'inferenza di modelli GenAI più grandi, come
    un LLM, con server specializzati come vLLM, Ray Serve o NVIDIA Triton.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用专门的服务器，如vLLM、Ray Serve或NVIDIA Triton，将大型GenAI模型（如LLM）的服务和推理外部化。
- en: Più avanti in questo capitolo, spiegherò come questi server massimizzano l'efficienza
    delle operazioni delimitate dal calcolo durante l'inferenza del modello, riducendo
    al contempo l'ingombro in memoria del modello durante il processo di generazione
    dei dati.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，我将解释这些服务器如何通过在模型推理过程中最大化计算操作的效率，同时减少数据生成过程中的模型内存占用。
- en: Per aiutarti a digerire quanto discusso finora, dai un'occhiata alla tabella
    di confronto delle strategie di concorrenza nella [Tabella 5-1](#concurrency_comparison)
    per capire quando e perché usarle.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您消化到目前为止所讨论的内容，请查看[表5-1](#concurrency_comparison)中的竞争策略比较表，以了解何时以及为什么使用它们。
- en: Tabella 5-1\. Confronto tra le strategie di circolazione
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-1\. 竞争策略比较
- en: '| Strategia | Caratteristiche | Sfide | Casi d''uso |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 特点 | 挑战 | 用例 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Nessuna concomitanza (sincrona) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 无并发（同步） |'
- en: Codice semplice, leggibile e di facile comprensione per il debug
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单、可读性强、易于理解的代码，便于调试
- en: Un singolo core e thread della CPU
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个核心和CPU线程
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Potenziali lunghi tempi di attesa a causa di operazioni di blocco dell'I/O o
    della CPU che bloccano l'esecuzione del processo.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于I/O或CPU阻塞操作导致执行进程的长时间等待。
- en: Non può servire più utenti contemporaneamente
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不能同时服务多个用户
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Applicazioni per utente singolo in cui gli utenti possono aspettare che le attività
    vengano completate
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单用户应用程序，其中用户可以等待活动完成
- en: Servizi o applicazioni utilizzati di rado
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很少使用的服务或应用程序
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Async IO (asincrono) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 异步I/O（异步） |'
- en: Un singolo core e thread della CPU
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个核心和CPU线程
- en: Il multitasking gestito da un ciclo di eventi all'interno del processo di Python
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python进程内部由事件循环管理的多任务
- en: Thread-safe, in quanto il processo Python gestisce i compiti
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程安全，因为Python进程管理任务
- en: Massimizza il tasso di utilizzo della CPU
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化CPU利用率
- en: Più veloce del multithreading e del multiprocessing per le attività di I/O
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比多线程和多进程在I/O活动上更快
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: È più difficile da implementare nel codice e può rendere più difficile il debug.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码中实现更困难，并且可能使调试更困难。
- en: Richiede librerie e dipendenze che utilizzano le funzionalità di Async IO
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要使用Async IO功能的库和依赖项
- en: È facile commettere errori che bloccano il processo principale (e il ciclo degli
    eventi)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易犯错误，这些错误会阻塞主进程（和事件循环）
- en: '| Applicazioni con attività di I/O bloccanti |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 阻塞I/O的应用 |'
- en: '| Multithreading |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 多线程 |'
- en: Un singolo core della CPU ma più thread all'interno dello stesso processo
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个CPU核心但同一进程内有更多线程
- en: I thread condividono dati e risorse
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程共享数据和资源
- en: Più semplice dell'IO asincrono da implementare nel codice
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码中实现比I/O异步更简单
- en: Multitasking su thread orchestrato dal sistema operativo
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由操作系统协调的线程多任务处理
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: È difficile bloccare le risorse per ogni thread per evitare problemi di thread-safety
    che possono portare a bug non riproducibili e alla corruzione dei dati.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以阻塞每个线程的资源以避免线程安全问题，这可能导致不可复现的bug和数据损坏。
- en: I thread possono bloccarsi l'un l'altro all'infinito (deadlock)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程可以无限期地相互阻塞（死锁）
- en: L'accesso concorrente alle risorse può causare risultati incoerenti (race conditions)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发访问资源可能导致不一致的结果（竞争条件）
- en: A un thread possono essere negate le risorse se monopolizza i thread (starvation)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个线程垄断了线程（饥饿），则可以撤销其资源
- en: La creazione e la distruzione di thread è computazionalmente costosa
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程的创建和销毁在计算上很昂贵
- en: '| Applicazioni o servizi che hanno attività di I/O bloccanti |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 阻塞I/O的应用或服务 |'
- en: '| Multiprocesso |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 多进程 |'
- en: Processi multipli in esecuzione su più core della CPU
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个CPU核心上运行的多个进程
- en: A ogni processo viene assegnato un core della CPU e risorse isolate.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个进程都被分配了一个CPU核心和隔离的资源。
- en: Il lavoro può essere distribuito tra i core della CPU e gestito da un processo
    di orchestrazione utilizzando strumenti come Celery
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过使用Celery等工具将工作分配到CPU核心，并由协调进程管理
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: La condivisione di risorse hardware e di oggetti come un modello di AI di grandi
    dimensioni o di dati tra i processi può essere complessa e richiede meccanismi
    di comunicazione inter-processo (IPC) o una memoria condivisa dedicata.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进程之间共享硬件资源和对象，如大型AI模型或数据，可能很复杂，需要进程间通信（IPC）机制或专用的共享内存。
- en: Difficile mantenere sincronizzati più processi isolati
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护多个隔离进程的同步很困难
- en: Creare e distruggere processi è computazionalmente costoso
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和销毁进程在计算上是昂贵的
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Applicazioni o servizi che hanno compiti di calcolo delimitati e bloccanti
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有计算任务界定和阻塞的应用或服务
- en: Attività di tipo "divide et impera" in cui l'elaborazione può essere eseguita
    in parti isolate.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以独立执行的部分的“分而治之”类型活动。
- en: Distribuire i carichi di lavoro o le richieste di elaborazione su più core della
    CPU
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将工作负载或计算请求分布到CPU的多个核心上
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Ora che abbiamo esplorato varie strategie di concorrenza, continuiamo a migliorare
    i tuoi servizi con la programmazione asincrona per gestire in modo efficiente
    le operazioni legate all'I/O.In seguito ci concentreremo sull'ottimizzazione delle
    attività legate al calcolo, in particolare l'inferenza di modelli tramite server
    specializzati.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了各种并发策略，让我们继续通过异步编程来改进你的服务，以有效地管理与I/O相关的操作。随后我们将专注于与计算相关的活动的优化，特别是通过专用服务器进行模型推断。
- en: Ottimizzazione delle attività di I/O con la programmazione asincrona
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用异步编程优化I/O活动
- en: In questa sezione esploreremo l'uso della programmazione asincrona per evitare
    di bloccare il processo principale del server con attività delimitate dall'I/O
    durante i carichi di lavoro dell'intelligenza artificiale. Conoscerai anche il
    framework `asyncio` che permette di scrivere applicazioni asincrone in Python.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨使用异步编程来避免在人工智能工作负载期间，由I/O界定活动阻塞服务器的主要进程。你还将了解允许在Python中编写异步应用的`asyncio`框架。
- en: Esecuzione sincrona contro esecuzione asincrona (Async)
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步执行与异步执行（Async）
- en: Cosa si intende per applicazione asincrona? Per rispondere a questa domanda,
    confrontiamo i programmi sincroni e quelli asincroni.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是异步应用？为了回答这个问题，我们将同步程序和异步程序进行比较。
- en: Un'applicazione è considerata *sincrona*quando i compiti vengono eseguiti in
    ordine sequenziale e ogni compito attende il completamento del precedente prima
    di iniziare. Per le applicazioni che vengono eseguite di rado e che richiedono
    solo pochi secondi per essere elaborate, il codice sincrono raramente causa problemi
    e può rendere le implementazioni più veloci e semplici. Tuttavia, se hai bisogno
    della concorrenza e vuoi che l'efficienza dei tuoi servizi sia massimizzata su
    ogni core, i tuoi servizi devono lavorare in multitasking senza attendere il completamento
    di operazioni bloccanti. È qui che l'implementazione della concorrenza *asincrona*
    (async) può aiutarti.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务按顺序执行，每个任务在开始之前等待前一个任务完成时，一个应用程序被认为是*同步*的。对于很少执行且仅需要几秒钟即可处理的应用程序，同步代码很少引起问题，并且可以使实现更快、更简单。然而，如果你需要并发并且想要最大化每个核心的服务效率，你的服务必须在多任务处理中工作，而不必等待阻塞操作完成。这就是异步（async）并发实现可以帮助你的地方。
- en: Vediamo alcuni esempi di funzioni sincrone e asincrone per capire quanto un
    codice asincrono possa aumentare le prestazioni. In entrambi gli esempi, utilizzerò
    sleeping per simulare un'operazione di blocco dell'I/O, ma puoi immaginare altre
    operazioni di I/O eseguite in scenari reali.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一些同步和异步函数的例子，以了解异步代码如何提高性能。在两个例子中，我将使用sleep来模拟I/O阻塞操作，但你可以想象在现实场景中执行的其他I/O操作。
- en: L['esempio 5-1](#sync_execution) mostra un esempio di codice sincrono che simula
    un'operazione di I/O bloccante con la funzione bloccante `time.sleep()`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[esempio 5-1](#sync_execution)示例展示了使用阻塞函数`time.sleep()`模拟I/O阻塞操作的同步代码示例。'
- en: Esempio 5-1\. Esecuzione sincrona
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-1\. 同步执行
- en: '[PRE0]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO1-1)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO1-1)'
- en: Usa `sleep()` per simulare un'operazione di blocco dell'I/O come l'invio di
    una richiesta di rete.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sleep()`来模拟I/O阻塞操作，例如发送网络请求。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO1-2)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO1-2)'
- en: Chiama il sito `task()` per tre volte, in sequenza. Il ciclo simula l'invio
    di più richieste di rete, una dopo l'altra.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 连续三次调用`task()`，顺序执行。循环模拟了连续发送多个网络请求。
- en: La chiamata a `task()` per tre volte nell'[Esempio 5-1](#sync_execution) richiede
    15 secondi perché Python aspetta che l'operazione bloccante `sleep()` venga completata.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Esempio 5-1](#sync_execution)中，三次调用`task()`需要15秒，因为Python等待阻塞操作`sleep()`完成。
- en: Per sviluppare programmi asincroni in Python, puoi utilizzare il pacchetto `asyncio`
    che fa parte della libreria standard di Python 3.5 e versioni successive. Utilizzando
    `asyncio`, il codice asincrono è simile al codice sincrono sequenziale ma con
    l'aggiunta delle parole chiave `async` e `await` per eseguire operazioni di I/O
    non bloccanti.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Python 中开发异步程序，你可以使用 `asyncio` 包，它是 Python 3.5 及更高版本标准库的一部分。使用 `asyncio`，异步代码与同步顺序代码类似，但增加了
    `async` 和 `await` 关键字来执行非阻塞 I/O 操作。
- en: L['Esempio 5-2](#async_execution) mostra come puoi usare le parole chiave `async`
    e `await` con `asyncio` per eseguire l'[Esempio 5-1](#sync_execution) in modo
    asincrono.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-2](#async_execution) 展示了如何使用 `async` 和 `await` 关键字与 `asyncio` 一起异步执行
    [示例 5-1](#sync_execution)。'
- en: Esempio 5-2\. Esecuzione asincrona
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. 异步执行
- en: '[PRE1]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO2-1)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[(#co_achieving_concurrency_in_ai_workloads_CO2-1)]'
- en: Implementa una coroutine `task` che cede il controllo al ciclo degli eventi
    durante le operazioni bloccanti.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个 `task` 协程，在阻塞操作期间将控制权交给事件循环。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO2-2)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[(#co_achieving_concurrency_in_ai_workloads_CO2-2)]'
- en: Lo sleep non bloccante di cinque secondi segnala al ciclo degli eventi di eseguire
    un'altra attività durante l'attesa.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 五秒的非阻塞 sleep 通知事件循环在等待期间执行其他活动。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO2-3)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)[(#co_achieving_concurrency_in_ai_workloads_CO2-3)]'
- en: Utilizza `asyncio.create_task` per generare istanze di task da concatenare (o
    riunire) ed eseguirle simultaneamente utilizzando `asyncio.gather`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `asyncio.create_task` 生成要连接（或合并）并使用 `asyncio.gather` 同时执行的 task 实例。
- en: '[![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO2-4)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)[(#co_achieving_concurrency_in_ai_workloads_CO2-4)]'
- en: Crea un ciclo di eventi per programmare attività asincrone con il metodo `asyncio.run`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `asyncio.run()` 方法创建事件循环来编程异步活动。
- en: '[![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO2-5)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)[(#co_achieving_concurrency_in_ai_workloads_CO2-5)]'
- en: Il tempo di esecuzione è 1/3 rispetto all'esempio sincrono, poiché questa volta
    il processo Python non è stato bloccato.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与同步示例相比，执行时间减少了 1/3，因为这次 Python 进程没有被阻塞。
- en: Dopo aver eseguito l'[Esempio 5-2](#async_execution), noterai che la funzione
    `task()` è stata chiamata tre volte in modo concomitante. D'altra parte, il codice
    dell'[Esempio 5-1](#sync_execution) chiama la funzione `task()` tre volte in modo
    sequenziale. La funzione async è stata eseguita all'interno del ciclo di eventi
    di `asyncio`, che era responsabile dell'esecuzione del codice senza aspettare.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 [示例 5-2](#async_execution) 后，你会注意到 `task()` 函数被同时调用了三次。另一方面，[示例 5-1](#sync_execution)
    中的代码以顺序方式三次调用 `task()` 函数。异步函数是在 `asyncio` 的事件循环中执行的，它负责在不等待的情况下执行代码。
- en: In qualsiasi codice asincrono, la parola chiave `await` segnala a Python le
    operazioni di I/O bloccanti in modo che vengano eseguite in modo *non bloccante*
    (cioè che possano essere eseguite senza bloccare il processo principale). Essendo
    a conoscenza delle operazioni bloccanti, Python può andare a fare qualcos'altro
    mentre aspetta che le operazioni bloccanti finiscano.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何异步代码中，`await` 关键字通知 Python 哪些是阻塞 I/O 操作，以便它们可以以非阻塞方式执行（即可以在不阻塞主进程的情况下执行）。由于知道哪些操作是阻塞的，Python
    可以在等待这些操作完成时去做其他事情。
- en: L['esempio 5-3](#await_keyword) mostra come utilizzare le parole chiave `async`
    e `await` per dichiarare ed eseguire funzioni asincrone.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-3](#await_keyword) 展示了如何使用 `async` 和 `await` 关键字来声明和执行异步函数。'
- en: Esempio 5-3\. Come utilizzare le parole chiave `async` e `await`
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. 如何使用 `async` 和 `await` 关键字
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO3-1)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[(#co_achieving_concurrency_in_ai_workloads_CO3-1)]'
- en: Simula un'operazione di I/O non bloccante`await`ing `asyncio.sleep()` in modo
    che Python possa fare altre cose durante l'attesa.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过模拟 `asyncio.sleep()` 的非阻塞 I/O 操作 `await`，让 Python 在等待期间执行其他任务。
- en: '[![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO3-2)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[(#co_achieving_concurrency_in_ai_workloads_CO3-2)]'
- en: Devi chiamare `main()` all'interno di `asyncio.run()` per eseguirla, poiché
    si tratta di una funzione asincrona, altrimenti non verrà eseguita e restituirà
    un oggetto *coroutine*. Tratterò le coroutine a breve.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须在 `asyncio.run()` 内部调用 `main()` 来执行它，因为这是一个异步函数，否则它将不会执行并返回一个 *协程* 对象。我将在稍后处理协程。
- en: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO3-3)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO3-3)'
- en: Se esegui il codice, la seconda istruzione verrà stampata 3 secondi dopo la
    prima. In questo caso, dato che non ci sono altre operazioni da eseguire oltre
    allo sleep, Python gira in idle fino al completamento dell'operazione di sleep.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行代码，第二个指令将在第一个指令后 3 秒打印出来。在这种情况下，由于没有其他操作要执行，除了 sleep，Python 会空闲直到 sleep
    操作完成。
- en: Nell'[Esempio 5-3](#await_keyword), ho usato lo sleep per simulare le operazioni
    di blocco dell'I/O, come le richieste di rete.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-3](#await_keyword)，我使用了 sleep 来模拟 I/O 阻塞操作，例如网络请求。'
- en: Attenzione
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Puoi usare la parola chiave `await` solo all'interno di una funzione dichiarata
    con `async def`. L'uso di `await` al di fuori di una funzione `async` solleverà
    un `SyntaxError` in Python. Un'altra insidia comune è l'uso di codice bloccante
    non asincrono all'interno di una funzione `async` che inavvertitamente impedirà
    a Python di svolgere altre attività durante l'attesa.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你只能在用 `async def` 声明的函数中使用 `await` 关键字。在 `async` 函数外部使用 `await` 将在 Python 中引发
    `SyntaxError`。另一个常见的陷阱是在 `async` 函数中使用阻塞的同步代码，这无意中阻止了 Python 在等待时执行其他活动。
- en: 'Ora hai capito che nei programmi asincroni, per evitare che il processo principale
    venga bloccato, Python passa da una funzione all''altra non appena si verifica
    un''operazione bloccante. Ora ti starai chiedendo:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了在异步程序中，为了避免主进程被阻塞，Python 一旦发生阻塞操作，就会从一个函数切换到另一个函数。现在你可能想知道：
- en: Come fa Python a sfruttare `asyncio` per mettere in pausa e riprendere le funzioni?
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 如何利用 `asyncio` 来暂停和恢复函数？
- en: Qual è il meccanismo utilizzato da `asyncio` di Python per passare da una funzione
    all'altra senza dimenticare quelle sospese?
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 的 `asyncio` 使用什么机制在函数之间切换，同时不会忘记挂起的函数？
- en: Come si possono mettere in pausa o riprendere le funzioni senza perdere il loro
    stato?
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何暂停或恢复函数而不会丢失其状态？
- en: Per rispondere alle domande di cui sopra, approfondiamo i meccanismi alla base
    di `asyncio`, poiché la comprensione delle risposte a queste domande ti aiuterà
    notevolmente a eseguire il debug del codice async nei tuoi servizi.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答上述问题，我们深入探讨 `asyncio` 的基础机制，因为对这些问题的理解将极大地帮助你调试在服务中运行的异步代码。
- en: Il cuore di `asyncio` è costituito da un oggetto di prima classe chiamato *ciclo
    di eventi*, responsabile della gestione efficiente degli eventi di I/O, degli
    eventi di sistema e dei cambiamenti di contesto dell'applicazione.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`asyncio` 的核心是一个称为 *事件循环* 的一等对象，负责高效地管理 I/O 事件、系统事件和应用程序上下文变化。'
- en: '[La Figura 5-4](#event_loop) mostra come il ciclo di eventi di `asyncio` intraprende
    l''orchestrazione dei task in Python.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](#event_loop) 展示了 `asyncio` 的事件循环如何对 Python 中的任务进行编排。'
- en: '![bgai 0504](assets/bgai_0504.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0504](assets/bgai_0504.png)'
- en: Figura 5-4\. Ciclo di eventi IO asincrono
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 异步 I/O 事件循环
- en: Il ciclo degli eventi può essere paragonato a un ciclo `while True` che osserva
    gli eventi o i messaggi emessi dalle *funzioni coroutine* all'interno del processo
    Python e distribuisce gli eventi per passare da una funzione all'altra in attesa
    del completamento delle operazioni di blocco I/O. Questa orchestrazione permette
    alle altre funzioni di essere eseguite in modo asincrono senza interruzioni.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 事件循环可以比作一个 `while True` 循环，它观察 Python 进程中由 *协程函数* 发出的事件或消息，并将事件分配给其他函数，以便在 I/O
    阻塞操作完成时从一个函数切换到另一个函数。这种编排允许其他函数异步执行，而不会中断。
- en: Programmazione asincrona con le API dei fornitori di modelli
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型提供商的 API 进行异步编程
- en: Tutti e tre gli esempi che ti ho mostrato finora sono considerati come esempi
    "Hello World" di programmazione asincrona. Ora, analizziamo uno scenario reale
    legato alla creazione di servizi GenAI in cui è necessario utilizzare le API di
    un fornitore di modelli, come OpenAI, Anthropic o Mistral, poiché potrebbe essere
    più costoso servire LLMsda soli.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前展示的所有三个示例都被视为异步编程的“Hello World”示例。现在，让我们分析一个与创建 GenAI 服务相关的实际场景，在这种情况下，需要使用模型提供商的
    API，如 OpenAI、Anthropic 或 Mistral，因为单独服务 LLMs 可能会更昂贵。
- en: Inoltre, se sottoponi a stress test gli endpoint di generazione creati nel [Capitolo
    3](ch03.html#ch03) inviando più richieste in un breve lasso di tempo, noterai
    lunghi tempi di attesa prima che ogni richiesta venga elaborata. Questo perché
    hai precaricato e ospitato il modello nello stesso processo Python e nello stesso
    core della CPU su cui gira il server. Quando invii la prima richiesta, l'intero
    server si blocca mentre il carico di lavoro dell'inferenza viene completato. Poiché
    durante l'inferenza la CPU lavora al massimo delle sue possibilità, il processo
    di inferenza/generazione è un'operazione delimitata dalla CPU. Tuttavia, non è
    necessario che lo sia.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你在 [第3章](ch03.html#ch03) 中创建的生成端点上执行压力测试，发送大量请求，你会在每个请求被处理之前注意到长时间的等待。这是因为你在同一个Python进程和同一个CPU核心上预载和托管了模型。当你发送第一个请求时，整个服务器会阻塞，直到推理工作完成。由于推理期间CPU处于最大工作状态，推理/生成过程是受CPU限制的操作。然而，这并不一定必须如此。
- en: Quando utilizzi l'API di un provider, non devi più preoccuparti dei carichi
    di lavoro dell'intelligenza artificiale legati alla CPU, poiché questi diventano
    legati all'I/O. Pertanto, ha senso sapere come sfruttare la programmazione asincrona
    per interagire simultaneamente con l'API del provider del modello.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用模型提供者的API时，你不再需要担心与CPU相关的AI工作负载，因为这些工作负载现在与I/O相关。因此，了解如何利用异步编程来同时与模型提供者的API交互是有意义的。
- en: La buona notizia è che i proprietari delle API spesso rilasciano *client* e*kit
    di sviluppo software* (SDK) sia sincroni che asincroni per ridurre il lavoro necessario
    per interagire con i loro endpoint.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，API的所有者通常会发布同步和异步的 *客户端* 和 *软件开发工具包* (SDK)，以减少与他们的端点交互所需的工作量。
- en: Attenzione
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Se hai bisogno di fare richieste ad altri servizi esterni, di recuperare dati
    dai database o di ingerire contenuti dai file, aggiungerai al processo altre attività
    bloccanti di I/O. Queste attività bloccanti possono costringere il server a rimanere
    in attesa se non sfrutti la programmazione asincrona.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要向其他外部服务发出请求、从数据库中检索数据或从文件中读取内容，你将向进程添加其他阻塞I/O活动。如果不使用异步编程，这些阻塞活动可能会迫使服务器保持等待状态。
- en: Tuttavia, qualsiasi codice sincrono può essere reso asincrono utilizzando un
    [processo o un esecutore di pool di thread](https://oreil.ly/hIDNI) per evitare
    di eseguire l'attività all'interno del ciclo di eventi. Invece, si esegue l'attività
    asincrona su un processo o un thread separato per evitare di bloccare il ciclo
    di eventi.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，任何同步代码都可以通过使用 [进程或线程池执行器](https://oreil.ly/hIDNI) 来异步化，以避免在事件循环中执行活动。相反，将在单独的进程或线程上执行异步活动，以避免阻塞事件循环。
- en: Puoi anche verificare l'eventuale supporto asincrono controllando la documentazione
    della libreria o il codice sorgente alla ricerca di menzioni delle parole chiave
    `async` o `await`. Altrimenti, puoi provare a verificare se lo strumento può essere
    utilizzato all'interno di una funzione asincrona senza sollevare un `TypeError`
    quando usi `await` su di essa.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过检查库的文档或源代码来查找关键字 `async` 或 `await` 的提及，以验证是否存在异步支持。否则，你可以尝试检查该工具是否可以在一个异步函数中使用，而不会在对其使用
    `await` 时引发 `TypeError`。
- en: Se uno strumento, come ad esempio una libreria di database, ha solo un'implementazione
    sincrona, allora non puoi implementare l'asincronia con quello strumento. La soluzione
    sarà quella di passare lo strumento a un equivalente asincrono in modo da poterlo
    utilizzare con leparole chiave`async` e `await`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个工具，例如数据库库，只有同步实现，那么你不能使用该工具实现异步。解决方案是将工具转换为异步等效工具，以便可以使用 `async` 和 `await`
    关键字使用它。
- en: Nell'[Esempio 5-4](#openai_clients), interagisci con l'API OpenAI GPT-3.5 tramite
    client OpenAI sia sincroni che asincroni per capire la differenza di prestazioni
    tra i due.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例5-4](#openai_clients) 中，通过OpenAI客户端以同步和异步方式与OpenAI GPT-3.5 API交互，以了解两种方法之间的性能差异。
- en: Nota
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'È necessario installare la libreria `openai`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 需要安装 `openai` 库：
- en: '[PRE3]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4] import os from fastapi import FastAPI, Body from openai import OpenAI,
    AsyncOpenAI  app = FastAPI()  sync_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    async_client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))  @app.post("/sync")
    def sync_generate_text(prompt: str = Body(...)):     completion = sync_client.chat.completions.create(         messages=[             {                 "role":
    "user",                 "content": prompt,             }         ],         model="gpt-3.5-turbo",     )     return
    completion.choices[0].message.content  @app.post("/async") async def async_generate_text(prompt:
    str = Body(...)):     completion = await async_client.chat.completions.create(         messages=[             {                 "role":
    "user",                 "content": prompt,             }         ],         model="gpt-3.5-turbo",     )     return
    completion.choices[0].message.content [PRE5]`  [PRE6]` ## Ciclo di eventi e pool
    di thread in FastAPI    FastAPI è in grado di gestire operazioni di blocco sia
    asincrone che sincrone, eseguendo i gestori di sincronizzazione nel suo *pool
    di thread* in modo che le operazioni di blocco non interrompano l''esecuzione
    dei compiti del *ciclo degli eventi*.    Come ho detto nel [Capitolo 2](ch02.html#ch02),
    FastAPI funziona con il framework web ASGI tramite Starlette. Se non fosse così,
    il server verrebbe eseguito in modo sincrono, quindi dovresti aspettare che ogni
    processo finisca prima di poter servire il successivo. Tuttavia, utilizzando ASGI,
    il server FastAPI supporta la concorrenza sia tramite il multithreading (tramite
    un pool di thread) che la programmazione asincrona (tramite un ciclo di eventi)
    per servire più richieste in parallelo, evitando che il processo principale del
    server venga bloccato.    FastAPI crea il pool di thread istanziando una collezione
    di thread all''avvio dell''applicazione per ridurre l''onere della creazione di
    thread in runtime.^([4](ch05.html#id822)) In seguito, delega le attività in background
    e i carichi di lavoro sincroni al pool di thread per evitare che il ciclo degli
    eventi venga bloccato da operazioni bloccanti all''interno dei gestori sincroni.
    Il ciclo degli eventi è anche indicato come il thread principale del server FastAPI,
    responsabile dell''elaborazione delle richieste.    Come ho già detto, il ciclo
    degli eventi è il componente centrale di ogni applicazione costruita su `asyncio`,
    compresa FastAPI che implementa la concurrency. I cicli degli eventi eseguono
    attività asincrone e callback, tra cui l''esecuzione di operazioni di I/O di rete
    e l''esecuzione di sottoprocessi. In FastAPI, il ciclo degli eventi è anche responsabile
    dell''orchestrazione dell''elaborazione asincrona delle richieste.    Se possibile,
    dovresti eseguire i gestori nel ciclo degli eventi (tramite la programmazione
    asincrona) perché può essere ancora più efficiente che eseguirli nel pool di thread
    (tramite il multithreading). Questo perché ogni thread nel pool di thread deve
    acquisire il GIL prima di poter eseguire qualsiasi byte di codice e questo richiede
    un certo sforzo computazionale.    Immagina che più utenti simultanei utilizzino
    gli handler (endpoint) OpenAI GPT-3.5 sincroni e asincroni del tuo servizio FastAPI,
    come mostrato nell''[Esempio 5-4](#openai_clients). FastAPI eseguirà le richieste
    dell''handler asincrono nel ciclo degli eventi, poiché tale handler utilizza un
    client OpenAI asincrono non bloccante. D''altra parte, FastAPI deve delegare le
    richieste di handler sincroni al pool di thread per proteggere il loop di eventi
    dal blocco. Poiché delegare le richieste (ai thread) e passare da un thread all''altro
    in un pool di thread comporta un lavoro maggiore, le richieste sincrone termineranno
    più tardi rispetto alle loro controparti asincrone.    ###### Nota    Ricorda
    che tutto questo lavoro - l''elaborazione delle richieste di handler sincroni
    e asincroni - viene eseguito su un singolo core della CPU all''interno dello stesso
    processo FastAPI Python.    Questo per ridurre al minimo i tempi di inattività
    della CPU in attesa delle risposte dell''API OpenAI.    Le differenze di prestazioni
    sono mostrate nella [Figura 5-5](#multithreading_vs_async).  ![bgai 0505](assets/bgai_0505.png)  ######
    Figura 5-5\. Come il multithreading e l''Async IO gestiscono le operazioni di
    blocco dell''I/O    La[Figura 5-5](#multithreading_vs_async) mostra che con i
    carichi di lavoro delimitati dall''I/O, le implementazioni asincrone sono più
    veloci e dovrebbero essere il metodo da preferire se hai bisogno di concorrenza.
    Tuttavia, FastAPI fa comunque un buon lavoro nel servire più richieste simultanee
    anche se deve lavorare con un client OpenAI sincrono. Invia semplicemente le chiamate
    API sincrone all''interno dei thread del pool di thread per implementare una forma
    di concorrenza per l''utente. Ecco perché la documentazione ufficiale di FastAPI
    ti dice di non preoccuparti troppo di dichiarare le tue funzioni handler come
    `async def` o `def`.    Tuttavia, tieni presente che quando dichiari i gestori
    con `async def`, FastAPI si fida di eseguire solo operazioni non bloccanti. Quando
    rompi questa fiducia ed esegui operazioni bloccanti all''interno delle rotte `async`,
    il ciclo dell''evento sarà bloccato e non potrà più continuare a eseguire attività
    fino a quando l''operazione bloccante non saràterminata.    ## Blocco del server
    principale    Se usi la parola chiave `async` quando definisci le tue funzioni,
    assicurati di usare anche la parola chiave `await` da qualche parte all''interno
    della funzione e che nessuna delle dipendenze del pacchetto che usi all''interno
    della funzione sia sincrona.    Evita di dichiarare le funzioni dei gestori di
    rotte come `async` se la loro implementazione è sincrona. In caso contrario, le
    richieste ai gestori di rotte interessati bloccheranno il server principale dall''elaborazione
    di altre richieste mentre il server attende il completamento dell''operazione
    bloccante. Non importa se l''operazione bloccante è legata all''I/O o al calcolo.
    Pertanto, qualsiasi chiamata ai database o ai modelli di intelligenza artificiale
    può comunque causare il blocco se non stai attento.    Questo è un errore facile
    da commettere. Ad esempio, puoi usare una dipendenza sincrona all''interno di
    gestori che hai dichiarato asincroni, come mostrato nell''[Esempio 5-5](#blocking_main_thread).    #####
    Esempio 5-5\. Implementazione errata dei gestori asincroni in FastAPI    [PRE7]    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO4-1)      Operazione
    di I/O bloccante per ottenere la risposta dell''API ChatGPT. Poiché il gestore
    della rotta è contrassegnato come asincrono, FastAPI si fida di noi per non eseguire
    operazioni bloccanti, ma poiché lo siamo, la richiesta bloccherà il ciclo degli
    eventi (thread principale del server). Altre richieste sono ora bloccate fino
    a quando la richiesta corrente non viene elaborata.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO4-2)      Un
    semplice gestore di rotte sincrone con operazioni bloccanti che non sfrutta le
    funzionalità asincrone. Le richieste di sincronizzazione vengono affidate al pool
    di thread per essere eseguite in background, in modo da non bloccare il server
    principale.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO4-3)      Un
    percorso asincrono non bloccante.      La richiesta non blocca il thread principale
    e non deve essere trasferita al pool di thread. Di conseguenza, il ciclo di eventi
    FastAPI può elaborare la richiesta molto più velocemente utilizzando il client
    OpenAI async.    Ora dovresti sentirti più a tuo agio nell''implementare nuove
    funzionalità nel tuo servizio FastAPI che richiedono l''esecuzione di compiti
    delimitati da I/O.    Per aiutarti a consolidare la comprensione dei concetti
    di I/O concurrency, nelle prossime sezioni costruirai diverse nuove funzionalità
    che utilizzano la concurrency nel tuo servizio FastAPI. Queste funzionalità includono:    Parla
    con il web      Costruisci e integra un modulo web scraper che ti permette di
    porre domande al tuo LLM self-hosted sul contenuto di un sito web fornendo un
    URL HTTP.      Parla con i documenti      Costruisci e integra un modulo RAG per
    elaborare i documenti in un database vettoriale. Un database vettoriale memorizza
    i dati in modo da supportare ricerche di similarità efficienti. Puoi quindi utilizzare
    la ricerca semantica, che comprende il significato delle query, per interagire
    con i documenti caricati utilizzando il tuo LLM.      Entrambi i progetti ti faranno
    fare esperienza pratica di interazione asincrona con sistemi esterni come siti
    web, database e filesystem.    ## Progetto: Parla con il web (Web Scraper)    Le
    aziende spesso ospitano una serie di pagine web interne per manuali, processi
    e altra documentazione sotto forma di pagine HTML. Per le pagine più lunghe, i
    tuoi utenti potrebbero voler fornire degli URL quando pongono delle domande e
    aspettarsi che LLM recuperi e legga il contenuto. È qui che può essere utile avere
    un web scraper integrato.    Ci sono molti modi per creare un web scraper per
    il tuo LLM self-hosted. A seconda del tuo caso d''uso, puoi utilizzare una combinazione
    dei seguenti metodi:    *   Recupera le pagine web come HTML e invia il contenuto
    HTML grezzo (o il testo interno) al tuo LLM per analizzare il contenuto nel formato
    desiderato.           *   Utilizza *framework di web scraping* come `BeautifulSoup`
    e `ScraPy` per analizzare il contenuto delle pagine web dopo averle recuperate.           *   Usa
    i *browser web headless* come Selenium e Microsoft Playwright per navigare dinamicamente
    tra i nodi delle pagine e analizzare i contenuti. I browser headless sono ottimi
    per la navigazione di applicazioni a pagina singola (SPA).              ######
    Attenzione    Tu o i tuoi utenti dovreste evitare gli strumenti di web scraping
    alimentati da LLM per scopi illegali. Assicurati di avere l''autorizzazione prima
    di estrarre contenuti dagli URL:    *   Esamina le condizioni d''uso di ogni sito
    web, soprattutto se si parla di web scraping.           *   Usa le API quando
    è possibile.           *   Se non sei sicuro, chiedi direttamente il permesso
    ai proprietari dei siti web.              Per questo mini-progetto, ci limiteremo
    a recuperare e inviare al nostro LLM il testo interno grezzo delle pagine HTML,
    poiché l''implementazione di uno scraper pronto per la produzione può diventare
    un libro a sé stante.    Il processo di costruzione di un semplice scraper asincrono
    è il seguente:    1.  Sviluppa una funzione per abbinare i modelli di URL usando
    la regex sui prompt degli utenti al LLM.           2.  Se viene trovata, esegue
    un ciclo sull''elenco degli URL forniti e recupera le pagine in modo asincrono.
    Utilizzeremo una libreria HTTP asincrona chiamata `aiohttp` invece di `requests`poiché
    `requests` può effettuare solo richieste di rete sincrone.           3.  Sviluppa
    una funzione di parsing per estrarre il contenuto testuale dall''HTML recuperato.           4.  Il
    contenuto della pagina analizzata viene inviato al LLM insieme al prompt originale
    dell''utente.              L[''esempio 5-6](#web_scraper) mostra come puoi implementare
    i passaggi sopra citati.    ###### Nota    Per eseguire questo esempio è necessario
    installare alcune dipendenze aggiuntive:    [PRE8]   [PRE9] # scraper.py  import
    asyncio import re  import aiohttp from bs4 import BeautifulSoup from loguru import
    logger  def extract_urls(text: str) -> list[str]:     url_pattern = r"(?P<url>https?:\/\/[^\s]+)"
    ![1](assets/1.png)     urls = re.findall(url_pattern, text) ![2](assets/2.png)     return
    urls   def parse_inner_text(html_string: str) -> str:     soup = BeautifulSoup(html_string,
    "lxml")     if content := soup.find("div", id="bodyContent"): ![3](assets/3.png)         return
    content.get_text()     logger.warning("Could not parse the HTML content")     return
    ""   async def fetch(session: aiohttp.ClientSession, url: str) -> str:     async
    with session.get(url) as response: ![4](assets/4.png)         html_string = await
    response.text()         return parse_inner_text(html_string)   async def fetch_all(urls:
    list[str]) -> str:     async with aiohttp.ClientSession() as session: ![5](assets/5.png)         results
    = await asyncio.gather(             *[fetch(session, url) for url in urls], return_exceptions=True         )     success_results
    = [result for result in results if isinstance(result, str)]     if len(results)
    != len(success_results): ![6](assets/6.png)         logger.warning("Some URLs
    could not be fetched")     return " ".join(success_results) [PRE10] # dependencies.py  from
    fastapi import Body from loguru import logger  from schemas import TextModelRequest
    from scraper import extract_urls, fetch_all  async def get_urls_content(body:
    TextModelRequest = Body(...)) -> str: ![1](assets/1.png)     urls = extract_urls(body.prompt)     if
    urls:         try:             urls_content = await fetch_all(urls)             return
    urls_content         except Exception as e:             logger.warning(f"Failed
    to fetch one or several URls - Error: {e}")     return ""  # main.py  from fastapi
    import Body, Depends, Request from dependencies import construct_prompt from schemas
    import TextModelResponse  @app.post("/generate/text", response_model_exclude_defaults=True)
    ![2](assets/2.png) async def serve_text_to_text_controller(     request: Request,     body:
    TextModelRequest = Body(...),     urls_content: str = Depends(get_urls_content)
    ![3](assets/3.png) ) -> TextModelResponse:     ... # rest of controller logic     prompt
    = body.prompt + " " + urls_content     output = generate_text(models["text"],
    prompt, body.temperature)     return TextModelResponse(content=output, ip=request.client.host)
    [PRE11]`  [PRE12] ## Progetto: Parla con i documenti (RAG)    In questo progetto,
    inseriremo un modulo RAG nel tuo servizio GenAI per farti sperimentare l''interazione
    asincrona con sistemi esterni come un database e un filesystem.    Potresti essere
    curioso di sapere qual è lo scopo di un modulo RAG e la sua necessità. Il RAG
    è semplicemente una tecnica per aumentare il contesto dei prompt di LLM con fonti
    di dati personalizzate per attività ad alta intensità di conoscenza.^([5](ch05.html#id830))
    Si tratta di una tecnica efficace per fondare le risposte di LLM su fatti contenuti
    nei dati, senza dover ricorrere a una complessa e costosa messa a punto di LLM.    Le
    organizzazioni sono ansiose di implementare il RAG con il proprio LLM, poiché
    consente ai dipendenti di accedere alle enormi basi di conoscenza interne attraverso
    il LLM. Con il RAG, le aziende si aspettano che le basi di conoscenza interne,
    i sistemi e le procedure siano accessibili e prontamente disponibili a chiunque
    ne abbia bisogno per rispondere alle domande, proprio quando ne ha bisogno. Si
    prevede che questa accessibilità alle informazioni aziendali aumenti la produttività,
    riduca i costi e i tempi di ricerca delle informazioni e incrementi i profitti
    di qualsiasi azienda.    Tuttavia, le LLMs sono suscettibili di generare risposte
    che non aderiscono alle istruzioni fornite dall''utente. In altre parole, le LLMs
    possono *allucinare*risposte con informazioni o dati che non si basano su fatti
    o realtà.    Queste allucinazioni possono verificarsi a causa del fatto che il
    modello si basa su schemi presenti nei dati su cui è stato addestrato piuttosto
    che sull''accesso diretto a dati esterni, aggiornati e reali. Le LLM possono manifestare
    allucinazioni con risposte sicure ma errate o insensate, storie inventate o affermazioni
    prive di una base di verità.    Pertanto, per i compiti più complessi e ad alta
    intensità di conoscenza, vorrai che il tuo LLM acceda a fonti di conoscenza esterne
    per completare i compiti. Questo permette una maggiore coerenza fattuale e migliora
    l''affidabilità delle risposte generate.[La Figura 5-7](#rag) mostra il processo
    completo.  ![bgai 0507](assets/bgai_0507.png)  ###### Figura 5-7\. RAG    In questo
    progetto costruirai un semplice modulo RAG per il tuo servizio LLM in modo che
    gli utenti possano caricare e parlare dei loro documenti.    ###### Nota    C''è
    molto da sapere sul RAG, tanto da riempire diversi libri di testo e ogni giorno
    vengono pubblicati nuovi articoli su nuove tecniche e algoritmi.    Ti consiglio
    di consultare altre pubblicazioni su LLMs per conoscere il processo RAG e le tecniche
    RAG avanzate.    La pipeline di RAG consiste nelle seguenti fasi:    1.  *Estrazione*
    di documenti da un filesystem per caricare il contenuto testuale in pezzi sulla
    memoria.           2.  *Trasformazione* dei contenuti testuali pulendoli, dividendoli
    e preparandoli per essere passati in un modello di embedding per produrre vettori
    di embedding che rappresentano il significato semantico di un brano.           3.  *Memorizzazione*
    dei vettori di incorporamento insieme ai metadati, come la fonte e il chunk di
    testo, in un archivio vettoriale come Qdrant.           4.  *Recupero* di vettori
    di incorporamento semanticamente rilevanti eseguendo una ricerca semantica sulla
    query dell''utente al LLM. I pezzi di testo originali - memorizzati come metadati
    dei vettori recuperati - vengono poi utilizzati per aumentare (cioè migliorare
    il contesto) il prompt iniziale fornito al LLM.           5.  *Generazione* della
    risposta del LLM che bypassa sia la query che i chunk recuperati (cioè il contesto)
    al LLM per ottenere una risposta.              Puoi vedere la pipeline completa
    nella [Figura 5-8](#rag_pipeline).  ![bgai 0508](assets/bgai_0508.png)  ######
    Figura 5-8\. Pipeline RAG    Puoi prendere la pipeline mostrata nella [Figura
    5-8](#rag_pipeline) e adattarla al tuo servizio esistente. La[Figura 5-9](#rag_module)
    mostra l''architettura di sistema di un servizio "parla con i tuoi documenti"
    abilitato con RAG.  ![bgai 0509](assets/bgai_0509.png)  ###### Figura 5-9\. Parla
    con l''architettura del tuo sistema di documenti    La[Figura 5-9](#rag_module)
    illustra il modo in cui i documenti caricati dagli utenti tramite l''interfaccia
    Streamlit vengono memorizzati e poi recuperati per l''elaborazione e l''archiviazione
    nel database per essere successivamente recuperati per aumentare i prompt di LLM.    Il
    primo passo da compiere prima di implementare il sistema RAG della [Figura 5-9](#rag_module)
    è quello di includere la funzionalità di caricamento dei file sia nel client Streamlit
    che nell''API backend.    Utilizzando la classe `UploadFile` di FastAPI, puoi
    accettare documenti dagli utenti in pezzi e salvarli nel filesystem o in qualsiasi
    altra soluzione di archiviazione file, come ad esempio un archivio blob. L''elemento
    importante da notare è che questa operazione di I/O non è bloccante grazie alla
    programmazione asincrona, che la classe `UploadFile` di FastAPI supporta.    ######
    Suggerimento    Poiché gli utenti possono caricare documenti di grandi dimensioni,
    la classe `UploadFile` di FastAPI supporta il *chunking*per memorizzare i documenti
    caricati, un pezzo alla volta.    In questo modo eviterai di intasare la memoria
    del tuo servizio. Dovrai anche proteggere il tuo servizio impedendo agli utenti
    di caricare documenti di dimensioni superiori a una certa soglia.    L[''esempio
    5-8](#upload_file) mostra come implementare una funzionalità di caricamento asincrono
    di file.    ###### Suggerimento    Dovrai installare il pacchetto `aiofiles` per
    caricare i file in modo asincrono e `python-multipart` per ricevere i file caricati
    dai moduli HTML:    [PRE13]py   [PRE14]`py [PRE15]py` [PRE16]py [PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE4] 导入 os 模块，从 fastapi 模块导入 FastAPI、Body，从 openai 模块导入 OpenAI、AsyncOpenAI。创建
    FastAPI 应用实例 app，同步客户端 sync_client 和异步客户端 async_client。@app.post("/sync") 定义了一个同步生成文本的函数
    sync_generate_text，它接收一个提示字符串 prompt，并使用 OpenAI 的同步客户端创建聊天完成，返回完成的选择的第一个消息的内容。@app.post("/async")
    定义了一个异步生成文本的函数 async_generate_text，它同样接收一个提示字符串 prompt，并使用 OpenAI 的异步客户端创建聊天完成，返回完成的选择的第一个消息的内容。[PRE5]`  [PRE6]`
    ## FastAPI 的事件循环和线程池    FastAPI 能够处理同步和异步的阻塞操作，通过在其 *线程池* 中执行同步处理程序，以确保阻塞操作不会中断事件循环中的任务执行。    正如我在
    [第二章](ch02.html#ch02) 中所说的，FastAPI 通过 Starlette 框架与 ASGI 框架一起工作。如果不是这样，服务器将以同步方式执行，因此您需要等待每个进程完成才能服务下一个请求。然而，使用
    ASGI，FastAPI 服务器支持通过多线程（通过线程池）和异步编程（通过事件循环）来并行处理多个请求，从而避免阻塞主服务器进程。    FastAPI
    通过在应用程序启动时创建线程集合来创建线程池，以减少在运行时创建线程的负担。[4](ch05.html#id822)。随后，它将后台任务和同步工作负载委托给线程池，以避免事件循环被同步处理程序中的阻塞操作所阻塞。事件循环也被称为
    FastAPI 服务器的线程主循环，负责处理请求。    正如我之前所说的，事件循环是每个基于 `asyncio` 的应用程序的核心组件，包括实现并发的 FastAPI。事件循环执行异步活动和回调，包括执行网络
    I/O 操作和执行子进程。在 FastAPI 中，事件循环还负责协调异步请求的处理。    如果可能的话，您应该在事件循环中（通过异步编程）执行处理程序，因为这可能比在线程池中（通过多线程）执行更有效。这是因为线程池中的每个线程在执行任何代码字节之前都必须获取
    GIL，这需要一定的计算开销。    想象一下，多个用户同时使用你的 FastAPI 服务的 OpenAI GPT-3.5 同步和异步处理程序（如 [示例
    5-4](#openai_clients) 所示）。FastAPI 将在事件循环中执行异步处理程序的请求，因为该处理程序使用了一个非阻塞的 OpenAI 客户端。另一方面，FastAPI
    必须将同步处理程序的请求委托给线程池，以保护事件循环免受阻塞。由于将请求（到线程）和在线程池中从一个线程切换到另一个线程都需要更多的工作，因此同步请求的完成时间将比它们的异步对应物晚。    ######
    注意    请记住，所有这些工作——处理同步和异步处理程序的请求——都是在同一进程的 FastAPI Python 单个 CPU 内部执行的。    这是为了将
    CPU 等待 OpenAI API 响应的空闲时间降到最低。    性能差异在 [图 5-5](#multithreading_vs_async) 中显示。  ![bgai
    0505](assets/bgai_0505.png)  ###### 图 5-5\. 多线程和 Async IO 如何处理 I/O 阻塞操作    [图
    5-5](#multithreading_vs_async) 显示，对于受 I/O 限制的工作负载，异步实现更快，如果您需要并发，则应首选此方法。然而，即使必须与同步的
    OpenAI 客户端一起工作，FastAPI 仍然能够很好地服务多个并发请求。只需简单地将同步 API 调用发送到线程池中的线程即可实现用户层面的并发。这就是为什么
    FastAPI 的官方文档告诉您不要过于担心将您的处理程序函数声明为 `async def` 或 `def`。    然而，请注意，当您使用 `async
    def` 声明处理程序时，FastAPI 信任您只执行非阻塞操作。当您打破这种信任并在 `async` 路由中执行阻塞操作时，事件循环将被阻塞，并且无法继续执行活动，直到阻塞操作完成。    ##
    服务器主进程的阻塞    当您使用 `async` 关键字定义您的函数时，请确保在函数内部也使用 `await` 关键字，并且您使用的任何包的依赖项都不是同步的。    如果处理程序的实现是同步的，请避免将路由处理程序函数声明为
    `async`。否则，相关的路由处理程序请求将阻塞主服务器处理其他请求，直到阻塞操作完成。无论阻塞操作是关于 I/O 还是计算。因此，任何数据库或人工智能模型的调用都可能导致阻塞，如果您不小心的话。    这是一个很容易犯的错误。例如，您可以在声明的异步处理程序中使用同步依赖项，如
    [示例 5-5](#blocking_main_thread) 所示。    ##### 示例 5-5\. FastAPI 中异步处理程序的错误实现    [PRE7]    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO4-1)      阻塞
    I/O 操作以获取 ChatGPT API 的响应。由于路由处理程序被标记为异步，FastAPI 信任我们不执行阻塞操作，但由于我们执行了，请求将阻塞事件循环（服务器的主线程）。其他请求现在将阻塞，直到当前请求被处理。      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO4-2)      一个简单的同步路由处理程序，具有阻塞操作，但没有利用异步功能。同步请求被委托给线程池以在后台执行，这样就不会阻塞主服务器。      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO4-3)      一个非阻塞的异步路由。      请求不会阻塞主线程，也不需要转移到线程池。因此，FastAPI
    的事件循环可以非常快速地使用 OpenAI async 客户端处理请求。    现在，您应该对在您的 FastAPI 服务中实现需要执行 I/O 阻塞任务的新功能感到更加自在了。    为了帮助您巩固对
    I/O 并发概念的理解，在接下来的几节中，您将构建几个新的功能，这些功能将使用您的 FastAPI 服务中的并发。这些功能包括：    与 Web 交谈      构建和集成一个网络爬虫模块，允许您通过提供
    HTTP URL 来向您的自托管 LLM 提问网站内容。      与文档交谈      构建和集成一个 RAG 模块来处理数据库中的文档。向量数据库以支持高效相似性搜索的方式存储数据。然后，您可以使用语义搜索，它包括对查询的理解，来使用您的
    LLM 与上传的文档进行交互。      这两个项目都将使您获得与网站、数据库和文件系统等外部系统进行异步交互的实践经验。    ## 项目：与 Web 交谈（Web
    Scraper）    企业通常在其内部托管一系列网页，用于手册、流程和其他文档，形式为 HTML 页面。对于较长的页面，您的用户可能希望在提出问题时提供
    URL，并期望 LLM 恢复并读取内容。这就是集成网络爬虫可能很有用的地方。    为您的自托管 LLM 创建网络爬虫有多种方法。根据您的用例，您可以使用以下方法的组合：    *   将网页作为
    HTML 恢复，并将原始的 HTML 内容（或内部文本）发送到您的 LLM 以分析所需格式的内容。           *   使用 *网络爬虫框架*，如
    `BeautifulSoup` 和 `ScraPy`，在恢复网页后分析网页内容。           *   使用 *无头浏览器*，如 Selenium 和
    Microsoft Playwright，在页面节点之间动态导航并分析内容。无头浏览器非常适合导航单页应用程序（SPA）。              ######
    注意    您或您的用户应避免使用由 LLM 驱动的网络爬虫进行非法目的。在从 URL 提取内容之前，请确保您有授权：    *   检查每个网站的条款和条件，特别是如果涉及网络爬虫。           *   当可能时使用
    API。           *   如果不确定，请直接向网站所有者请求许可。              对于这个迷你项目，我们将仅恢复 HTML 页面的内部文本并将其发送到我们的
    LLM，因为生产就绪的爬虫实现可能成为一本独立的书籍。    构建简单的异步爬虫的过程如下：    1.  开发一个函数，使用正则表达式在用户的提示中匹配
    URL 模式。           2.  如果找到，则对提供的 URL 列表执行循环，并异步地恢复页面。我们将使用名为 `aiohttp` 的异步 HTTP
    库而不是 `requests`，因为 `requests` 只能执行同步网络请求。           3.  开发一个解析函数，从恢复的 HTML 中提取文本内容。           4.  分析的页面内容与用户的原始提示一起发送到
    LLM。              [L[''示例 5-6](#web_scraper)](#web_scraper) 展示了如何实现上述步骤。    ######
    注意    要运行此示例，需要安装一些额外的依赖项：    [PRE8]   [PRE9] # scraper.py  import asyncio import
    re  import aiohttp from bs4 import BeautifulSoup from loguru import logger  def
    extract_urls(text: str) -> list[str]:     url_pattern = r"(?P<url>https?:\/\/[^\s]+)"
    ![1](assets/1.png)     urls = re.findall(url_pattern, text) ![2](assets/2.png)     return
    urls   def parse_inner_text(html_string: str) -> str:     soup = BeautifulSoup(html_string,
    "lxml")     if content := soup.find("div", id="bodyContent"): ![3](assets/3.png)         return
    content.get_text()     logger.warning("Could not parse the HTML content")     return
    ""   async def fetch(session: aiohttp.ClientSession, url: str) -> str:     async
    with session.get(url) as response: ![4](assets/4.png)         html_string = await
    response.text()         return parse_inner_text(html_string)   async def fetch_all(urls:
    list[str]) -> str:     async with aiohttp.ClientSession() as session: ![5](assets/5.png)         results
    = await asyncio.gather(             *[fetch(session, url) for url in urls], return_exceptions=True         )     success_results
    = [result for result in results if isinstance(result, str)]     if len(results)
    != len(success_results): ![6](assets/6.png)         logger.warning("Some URLs
    could not be fetched")     return " ".join(success_results) [PRE10] # dependencies.py  from
    fastapi import Body from loguru import logger  from schemas import TextModelRequest
    from scraper import extract_urls, fetch_all  async def get_urls_content(body:
    TextModelRequest = Body(...)) -> str: ![1](assets/1.png)     urls = extract_urls(body.prompt)     if
    urls:         try:             urls_content = await fetch_all(urls)             return
    urls_content         except Exception as e:             logger.warning(f"Failed
    to fetch one or several URls - Error: {e}")     return ""  # main.py  from fastapi
    import Body, Depends, Request from dependencies import construct_prompt from schemas
    import TextModelResponse  @app.post("/generate/text", response_model_exclude_defaults=True)
    ![2](assets/2.png) async def serve_text_to_text_controller(     request: Request,     body:
    TextModelRequest = Body(...),     urls_content: str = Depends(get_urls_content)
    ![3](assets/3.png) ) -> TextModelResponse:     ... # rest of controller logic     prompt
    = body.prompt + " " + urls_content     output = generate_text(models["text"],
    prompt, body.temperature)     return TextModelResponse(content=output, ip=request.client.host)
    [PRE11]`  [PRE12] ## 项目：与文档交谈（RAG）    在这个项目中，我们将向您的 GenAI 服务中插入一个 RAG 模块，让您体验与数据库和文件系统等外部系统进行异步交互。    您可能想知道
    RAG 模块的目的及其必要性。RAG 是一种技术，用于通过使用针对高知识密集型活动定制的自定义数据源来增加 LLM 提示的上下文。[5](ch05.html#id830)。这是一种有效的技术，可以在不进行复杂且昂贵的
    LLM 训练的情况下，基于数据中的事实来建立 LLM 的响应。    组织机构渴望使用自己的 LLM 实现 RAG，因为它允许员工通过 LLM 访问内部巨大的知识库。通过
    RAG，企业期望内部知识库、系统和程序'
