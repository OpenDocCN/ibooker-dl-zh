<html><head></head><body>
  <div class="readable-text" id="p1"> &#13;
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span> </span> <span class="chapter-title-text">Dimensionality reduction</span></h1> &#13;
  </div> &#13;
  <div class="introduction-summary"> &#13;
   <h3 class="introduction-header">This chapter covers</h3> &#13;
   <ul> &#13;
    <li class="readable-text" id="p2">The curse of dimensionality and its disadvantages</li> &#13;
    <li class="readable-text" id="p3">Various methods of reducing dimensions </li> &#13;
    <li class="readable-text" id="p4">Principal component analysis </li> &#13;
    <li class="readable-text" id="p5">Singular value decomposition </li> &#13;
    <li class="readable-text" id="p6">Python solutions for both principal component analysis and singular value decomposition </li> &#13;
    <li class="readable-text" id="p7">A case study on dimension reduction </li> &#13;
   </ul> &#13;
  </div> &#13;
  <div class="readable-text" id="p8"> &#13;
   <blockquote>&#13;
    <div>&#13;
     Knowledge is a process of piling up facts; wisdom lies in their simplification.  &#13;
     <div class=" quote-cite">&#13;
       —Martin H. Fischer &#13;
     </div>&#13;
    </div>&#13;
   </blockquote> &#13;
  </div> &#13;
  <div class="readable-text" id="p9"> &#13;
   <p>We face complex situations in life. Life throws multiple options at us, and we choose a few viable ones from them. This decision of shortlisting is based on the significance, feasibility, utility, and perceived profit from each of the options. The ones that fit the bill are then chosen. A perfect example can be selecting your vacation destination. Based on the weather, travel time, safety, food, budget, and several other options, we choose a few where we would like to spend our next vacation. In this chapter, we study precisely the same—how to reduce the number of options—albeit in the data science and machine learning world. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p10"> &#13;
   <p>In the last chapter, we covered major clustering algorithms. We also went over a case study. The datasets we generate and use in such real-world examples have a lot of variables. Sometimes, there can be more than 100 variables or <em>dimensions</em> in the data. But not all of them are important. Having a lot of dimensions in the dataset is referred to as the curse of dimensionality. To perform any further analysis, we choose a few from the list of all of the dimensions or variables. In this chapter, we study the need for dimension reductions, various dimensionality techniques, and the respective pros and cons. We will dive deeper into the concepts of principal component analysis (PCA) and singular value decomposition (SVD) and their mathematical foundations and complement these with Python implementation. Also, continuing our structure from the last chapter, we will examine a real-world case study in the telecommunication sector. There are other advanced dimensionality reduction techniques like t-distributed stochastic neighbor embedding (t-SNE) and linear discriminant analysis (LDA), which we will explore in later chapters. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p11"> &#13;
   <p>Clustering and dimensionality reductions are the major categories of unsupervised learning. We studied major clustering methods in the last chapter, and we discuss dimensionality reduction in this chapter. With these two solutions, we cover a lot of ground in the unsupervised learning domain. But there are many more advanced topics to be covered, which are part of the latter chapters of the book. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p12"> &#13;
   <p>Let’s first understand what we mean by the “curse of dimensionality.”</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p13"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.1</span> Technical toolkit </h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p14"> &#13;
   <p>We are using the same version of Python as in the last chapters. Jupyter Notebook will be used in this chapter too. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p15"> &#13;
   <p>All the datasets and code files are available at the GitHub repository at (<a href="https://mng.bz/ZlBR">https://mng.bz/ZlBR</a>). You need to install the following Python libraries to execute the code: <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>, <code>scipy</code>, and <code>sklearn</code>. Since you have used the same packages in the last chapter, you don’t need to install them again. CPU is good enough for execution, but if you face some computing problems, switch to GPU or Google Colab. Refer to the appendix if you face any problems with the installation of any of these packages. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p16"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.2</span> The curse of dimensionality </h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p17"> &#13;
   <p>Let us continue with the vacation destination example we introduced earlier. The choice of destination is dependent on several parameters: safety, availability, food, nightlife, weather, budget, health, and so on. Having too many parameters is confusing. Let us understand by a real-life example.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p18"> &#13;
   <p>Consider this: a retailer wishes to launch a new range of shoes in the market, and for that, a target group of customers should be chosen. This target group will be reached through email, SMS, newsletter, etc. The business objective is to entice these customers to buy the newly launched shoes. From the entire customer base, the target group of customers can be chosen based on variables like customer age, gender, budget, preferred category, average spend, frequency of shopping, and so on. These many variables or <em>dimensions</em> make it hard to shortlist the customers based on a sound data analysis technique. We would be analyzing too many parameters simultaneously, examining the effect of each on the shopping probability of the customer, and hence it becomes too tedious and confusing of a task. It is the curse of dimensionality problem we face in real-world data science projects. We can face the curse of dimensionality in one more situation wherein the number of observations is fewer than the number of variables. Consider a dataset where the number of observations is <em>X</em>, while the number of variables is more than <em>X</em>—in such a case, we face the curse of dimensionality. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p19"> &#13;
   <p>An easy method to understand any dataset is through visualization. Let’s visualize a dataset in a vector-space diagram. If we have only one attribute or feature in the dataset, we can represent it in one dimension (see the left diagram in figure 3.1). For example, we might wish to capture only the height of an object using a single dimension. If we have two attributes, we need two dimensions, as shown in the middle diagram in figure 3.1, wherein to get the area of an object, we will require both length and width. If we have three attributes, for example, to calculate the volume, which requires length, width, and height, we require a 3D space, as shown in the diagram at right in figure 3.1. This requirement will continue to grow based on the number of attributes.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p20">  &#13;
   <img alt="figure" src="../Images/CH03_F01_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.1</span> Only one dimension is required to represent the data points—for example, to represent the height of an object (left). We need two dimensions to represent a data point. Each data point can correspond to the length and width of an object, which can be used to calculate the area (middle). Three dimensions are required to show a point (right). Here, it can be length, width, and height, which are required to get the volume of an object. This process continues based on the number of dimensions present in the data. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p21"> &#13;
   <p>Consider a dataset where you have an attribute for a data point—for example, gender. Then we add age and then education, address, and so on. To represent these attributes, the number of dimensions will keep on increasing. Hence, it is quite easy for us to conclude that with an increase in the number of dimensions, the amount of space required to represent increases by leaps and bounds. This is referred to as the c<em>urse of dimensionality</em>. The term was introduced by Richard E. Bellman and is used to refer to the problem of having too many variables in a dataset—some of which are significant while many others may be less important. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p22"> &#13;
   <p>There is another well-known theory named the <em>Hughes phenomenon,</em> shown in figure 3.2. Generally, in data science and machine learning, we wish to have as many variables as possible to train our model. The performance of the supervised learning classifier algorithm will increase to a certain limit and will peak with the most optimal number of variables. But, using the same amount of training data and with an increased number of dimensions, there is a decrease in the performance of a supervised classification algorithm. In other words, it is not advisable to have the variables in a dataset if they are not contributing to the accuracy of the solution. We should remove such variables from the dataset. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p23">  &#13;
   <img alt="figure" src="../Images/CH03_F02_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.2</span> The Hughes phenomenon shows that the performance of a machine learning model will improve initially with an increase in the number of dimensions. But a further increase leads to a decrease in the model’s performance. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p24"> &#13;
   <p>An increase in the number of dimensions has the following effects on the machine learning model: </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p25"> As the model deals with an increased number of variables, the mathematical complexity increases. For example, in the case of the k-means clustering method we discussed in the last chapter, when we have a greater number of variables, the distance calculation between respective points will become complex. Hence the overall model becomes more complex. </li> &#13;
   <li class="readable-text" id="p26"> The dataset generated in a larger dimensional space can be much sparser as compared to a smaller number of variables. The dataset will be sparser as some of the variables will have missing values, NULLs, etc. Therefore, space is much emptier, the dataset is less dense, and a smaller number of variables have values associated with them. </li> &#13;
   <li class="readable-text" id="p27"> With increased complexity in the model, the processing time required increases. The system feels the pressure to deal with so many dimensions. </li> &#13;
   <li class="readable-text" id="p28"> The overall solution becomes more complex to comprehend and execute. Recall chapter 1, where we discussed supervised learning algorithms. Due to the high number of dimensions, we might face the problem of overfitting in supervised learning models. </li> &#13;
  </ul> &#13;
  <div class="readable-text print-book-callout" id="p29"> &#13;
   <p><span class="print-book-callout-head">DEFINITION </span> When a supervised learning model has good accuracy on training data but lesser accuracy on unseen data, it is referred to as <em>overfitting</em>. Overfitting is a nuisance as the very aim of machine learning models is to work well on unseen datasets, and overfitting defeats this purpose.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p30"> &#13;
   <p>Let us relate things to a real-world example. Consider an insurance company offering different types of insurance policies like life insurance, vehicle insurance, health insurance, home insurance, etc. The company wishes to use data science and execute clustering use cases to enhance the customer base and the total number of policies sold. They have customer details like age, gender, profession, policy amount, historical transactions, number of policies held, annual income, type of policy, number of historical defaults, etc. At the same time, let us assume that variables like whether the customer is left-handed or right-handed, whether they wear black or brown shoes, what shampoo brand they use, the color of their hair, and their favorite restaurant are also captured. If we include all the variables in the dataset, the total number of variables in the resultant dataset will be quite high. The distance calculation will be more complex for a k-means clustering algorithm, the processing time will increase, and the overall solution will be quite complex. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p31"> &#13;
   <p>It is also imperative to note that <em>not</em> all the dimensions or variables are significant. Hence, it is vital to filter out the important ones from all the variables we have. Remember, nature always prefers simpler solutions! In the case discussed previously, it is highly likely that variables like hair color and favorite restaurant, etc., will not affect the outputs. So it is in our best interest to reduce the number of dimensions to ease the complexity and reduce the computation time. At the same time, it is also vital to note that dimensionality reduction is not always desired. It depends on the type of dataset and the business problem we wish to resolve. We will explore this more when we work on the case study in subsequent sections of the chapter. </p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p32"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 3.1</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p33"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p34"> The curse of dimensionality refers to having a lot of data. True or False? </li> &#13;
    <li class="readable-text" id="p35"> Having a high number of variables will always increase the accuracy of a solution. True or False? </li> &#13;
    <li class="readable-text" id="p36"> How does a large number of variables in a dataset affect the model? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p37"> &#13;
   <p>We have established that having a lot of dimensions is a challenge for us. We next examine the various methods to reduce the number of dimensions.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p38"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.3</span> Dimension reduction methods</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p39"> &#13;
   <p>We studied the disadvantages of having really high-dimensional data in the last section. A fewer number of dimensions might result in a simpler structure for our data, which will be computationally efficient. At the same time, we should be careful when reducing the number of variables. The output of the dimension reduction method should be complete enough to represent the original data and should not lead to any information loss. In other words, if originally we had, for example, 500 variables and we reduced it to 120 significant ones, still these 120 <em>should</em> be robust enough to capture <em>almost</em> all the information. Let us understand using a simple example.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p40"> &#13;
   <p>Consider this: we wish to predict the amount of rainfall a city will receive in the next month. The rainfall prediction for that city might be dependent on temperature over a period, wind speed measurements, pressure, distance from the sea, elevation above sea level, etc. These variables make sense if we wish to predict rainfall. At the same time, variables like the number of cinema halls in the city, whether the city is the capital of the country, or the number of red cars in the city will not affect the prediction of rainfall. In such a case, if we do not use the number of cinema halls in the city to predict the amount of rainfall, it will not reduce the capability of the system. The solution, in all probability, will still be able to perform quite well. Hence, in such a case, no information will be lost by dropping such a variable, and surely we can drop it from the dataset. On the other hand, removing variables such as temperature or distance from the ocean will very likely negatively affect the prediction accuracy. This is a very simple example highlighting the need to reduce the number of variables. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p41"> &#13;
   <p>The dimensions or the number of variables can be reduced by a combination of manual and algorithm-based methods. But before studying them in detail, there are a few mathematical terms and components we should be aware of, which we will discuss next.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p42"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">3.3.1</span> Mathematical foundation</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p43"> &#13;
   <p>There are quite a few mathematical terms that one must know to develop a thorough understanding of dimensionality reduction methods. We are trying to reduce the number of dimensions of a dataset. A dataset is nothing but a matrix of values—thus, a lot of the concepts are related to matrix manipulation methods, their geometrical representation, and performing transformations on such matrices. The mathematical concepts are discussed in the appendix. You also need an understanding of eigenvalues and eigenvectors. These concepts will be reused throughout the book; they are been put in the appendix for quick reference. You are advised to go through them before proceeding. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p44"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.4</span> Manual methods of dimensionality reduction</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p45"> &#13;
   <p>To tackle the curse of dimensionality, we wish to reduce the number of variables in a dataset. The reduction can be done by removing the variables from the dataset. Or a very simple solution for dimensionality reduction can be combining the variables that can be grouped logically or can be represented using a common mathematical operation.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p46"> &#13;
   <p>For example, as shown in figure 3.3, the data can be from a retail store where different customers have generated different transactions. We will get the sales, the number of invoices, and the number of items bought by each customer over a period. In the table, customer 1 has generated two invoices, bought five items in total, and generated a total sale of 100. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p47">  &#13;
   <img alt="figure" src="../Images/CH03_F03_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.3</span> In the first table, we have the sales, invoices, and number of items as the variables. In the second table, they have been combined to create new variables. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p48"> &#13;
   <p>If we wish to reduce the number of variables, we might combine three variables into two variables. Here we have introduced variables average transaction value (ATV) and average basket size (ABS) wherein ATV = Sales/Invoices and ABS = Number Of Items/Invoices. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p49"> &#13;
   <p>So, in the second table for customer 1, we have ATV as 50 and ABS as 2.5. Hence, the number of variables has been reduced from three to two. The process here is only an example of how we can combine various variables. It does not mean that we should replace sales with ATV as a variable. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p50"> &#13;
   <p>This process can continue to reduce the number of variables. Similarly, for a telecom subscriber, say we have the minutes of mobile calls made during 30 days in a month. We can add them to create a single variable—minutes used in a month. These examples are very basic ones to start with. Using the manual process, we can employ two other commonly used methods: manual selection and using correlation coefficient.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p51"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">3.4.1</span> Manual feature selection</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p52"> &#13;
   <p>Continuing from the rainfall prediction example we discussed in the last section, a data scientist might be able to drop a few variables. This will be based on a deep understanding of the business problem at hand and the corresponding dataset being used. However, it is an underlying assumption that the dataset is quite comprehensible for the data scientist and that they understand the business domain well. Most of the time, the business stakeholders will be able to guide on such methods. The variables must also be unique, and not much dependency should exist. As shown in figure 3.4, we can remove a few of the variables that might not be useful for predicting rainfall. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p53">  &#13;
   <img alt="figure" src="../Images/CH03_F04_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.4</span> In the first table, we have all the variables present in the dataset. Using business logic, some of the variables that might not be of much use have been discarded in the second table. But this is to be done with due caution; the best way is to get guidance from the business stakeholders. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p54"> &#13;
   <p>Sometimes, feature selection methods are also referred to as <em>wrapper methods</em>. Here, a machine learning model is wrapped or fitted with a subset of variables. In each iteration, we will get a different set of results. The set that generates the best results is selected for the final model.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p55"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">3.4.2</span> Correlation coefficient</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p56"> &#13;
   <p>Correlation between two variables simply means that they have a mutual relationship with each other. The change in the value of one variable will affect the value of another, which means that data points with similar values in one variable have similar values for the other variable. The variables that are highly correlated with each other supply similar information, so one of them can be dropped. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p57"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Correlation is described in detail in the appendix.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p58"> &#13;
   <p>For example, for a retail store, the number of invoices generated in a day will be highly correlated with the amount of sales generated, so one of them can be dropped. Another example is students who study for a higher number of hours will have better grades than the ones who study less (mostly!). </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p59"> &#13;
   <p>But we should be careful in dropping the variables and not trust correlation alone. The business context of a variable should be thoroughly understood before making any decision.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p60"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> It is a good idea to discuss this with the business stakeholders before dropping any variables from the study. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p61"> &#13;
   <p>Correlation-based methods are sometimes called <em>filter methods</em>. Using correlation coefficients, we can filter and choose the variables that are most significant.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p62"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 3.2</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p63"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p64"> We can drop a variable simply if we feel it is not required. True or False? </li> &#13;
    <li class="readable-text" id="p65"> If two variables are correlated, always drop one of them. True or False? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p66"> &#13;
   <p>Manual methods are easier solutions and can be executed quite efficiently. The dataset size is reduced, and we can proceed with the analysis. But manual methods are sometimes subjective and depend a lot on the business problem at hand. Many times, it is also not possible to employ manual methods for dimension reduction. In such situations, we have algorithm-based methods, which we study in the next section.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p67"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">3.4.3</span> Algorithm-based methods for reducing dimensions</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p68"> &#13;
   <p>We examined manual methods in the last section. Continuing from there, we examine algorithm-based methods in this section. The algorithm-based techniques are based on a more mathematical base and hence prove to be more scientific methods. In real-world business problems, we use a combination of both manual and algorithm-based techniques. Manual methods are straightforward to execute as compared to algorithm-based techniques. Also, we cannot comment on the comparison of both techniques, as they are based on different foundations. But at the same time, it is imperative that you put due diligence into the implementation of algorithm-based techniques.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p69"> &#13;
   <p>The major techniques used in dimensionality reductions are listed as follows. We explore some of them in this book:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p70"> PCA </li> &#13;
   <li class="readable-text" id="p71"> SVD </li> &#13;
   <li class="readable-text" id="p72"> LDA </li> &#13;
   <li class="readable-text" id="p73"> Generalized discriminant analysis </li> &#13;
   <li class="readable-text" id="p74"> Non-negative matrix factorization </li> &#13;
   <li class="readable-text" id="p75"> Multidimension scaling </li> &#13;
   <li class="readable-text" id="p76"> Locally linear embeddings </li> &#13;
   <li class="readable-text" id="p77"> IsoMaps </li> &#13;
   <li class="readable-text" id="p78"> Autoencoders </li> &#13;
   <li class="readable-text" id="p79"> t-SNE </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p80"> &#13;
   <p>These techniques are utilized for the common end goal: transform the data from a high-dimensional space to a low-dimensional one. Some of the data transformations are linear in nature, while some are nonlinear. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p81"> &#13;
   <p>We discuss PCA and SVD in detail in this chapter. In the later chapters of the book, other major techniques will be explored. PCA is perhaps the most quoted dimensionality reduction method, which is explored in the next section.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p82"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.5</span> Principal component analysis</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p83"> &#13;
   <p>Consider this: you are working on a dataset that has 250 variables. It is almost impossible to visualize such a high-dimensional space. Some of the 250 variables might be correlated with each other and some of them might not be, and there is a need to reduce the number of variables without losing much information. PCA allows us to mathematically select the most important features and leave the rest. PCA does reduce the number of dimensions but also preserves the most important relationships between the variables and the important structures in the dataset. Hence, the number of variables is reduced, but the important information in the dataset is kept safe.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p84"> &#13;
   <p>PCA is a projection of high-dimensional data in lower dimensions. In simpler terms, we are reducing an <em>n</em>-dimensional space into an <em>m</em>-dimensional one where <em>n</em> &gt; <em>m</em> while maintaining the nature and the essence of the original dataset. In the process, the old variables are reduced to newer ones while maintaining the crux of the original dataset. The new variables thus created are called <em>principal components</em>. The principal components are a linear combination of the raw variables. As a result of this transformation, the first principal component captures the maximum randomness or the highest variance in the dataset. The second principal component created is orthogonal to the first component. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p85"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> If two straight lines are orthogonal to each other, it means they are at an angle of 90˚ to each other. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p86"> &#13;
   <p>The process continues to the third component and so on. Orthogonality allows us to maintain that there is no correlation between subsequent principal components. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p87"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> PCA utilizes linear transformation of the dataset, and such methods are sometimes referred to as feature projections. The resultant dataset or the projection is used for further analysis. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p88"> &#13;
   <p>Let us understand this better using an example. In figure 3.5, we have represented the total perceived value of a home using some variables. The variables are area (sq m), number of bedrooms, number of balconies, distance from the airport, distance from the train station, and so on; we have 100+ variables. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p89">  &#13;
   <img alt="figure" src="../Images/CH03_F05_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.5</span> The variables on which the price of a house can be estimated</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p90"> &#13;
   <p>We can combine some of the variables mathematically and logically. PCA will create a new variable that is a linear combination of some of the variables, as shown in the following example. It will get the best <em>linear</em> combination of original variables so that the new variable is able to capture the maximum variance of the dataset. Equation 3.1 is only an example shown for illustration purposes wherein we are showing a new variable created by a combination of other variables. </p> &#13;
  </div> &#13;
  <div class="browsable-container equation-container" id="p91"> &#13;
   <h5 class=" browsable-container-h5">(3.1)</h5> &#13;
   <p>new_variable = <em>a</em>*area – <em>b</em>*bedrooms + <em>c</em>*distance – <em>d</em>*schools</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p92"> &#13;
   <p>Now let’s understand the concept visually. In a vector-space diagram, we can represent the dataset, as shown in figure 3.6. The left figure represents the raw data where we can visualize the variables in an x-y diagram. As discussed earlier, we wish to create a linear combination of variables. In other words, we wish to create a mathematical equation that will be able to explain the relationship between x and y. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p93">  &#13;
   <img alt="figure" src="../Images/CH03_F06_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.6</span> The dataset can be represented in a vector-space diagram (left). The straight line can be called the line of best fit having the projections of all the data points on it (middle). The differences between the actual value and the projections are the error terms (right). </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p94"> &#13;
   <p>The output of such a process will be a straight line as shown in the middle diagram in figure 3.6. This straight line is sometimes referred to as the <em>line of best fit. </em>Using this line of best fit, we can predict a value of y for a given value of x. These predictions are nothing but the projections of data points on a straight line.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p95"> &#13;
   <p>The difference between the actual value and the projections is the error, as shown in the right diagram in figure 3.6. The total sum of these errors is called the total projection error. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p96"> &#13;
   <p>There can be multiple options for this straight line, as shown in figure 3.7. These different straight lines will have different errors and different values of variances captured. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p97">  &#13;
   <img alt="figure" src="../Images/CH03_F07_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.7</span> The dataset can be captured by several lines, but not all the straight lines will be able to capture the maximum variance. The equation that gives the minimum error will be the one chosen. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p98"> &#13;
   <p>The straight line that can capture the maximum variance will be the chosen one. In other words, it gives the minimum error. It will be the <em>first principal component,</em> and the direction of maximum spread will be the <em>principal axis</em>. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p99"> &#13;
   <p>The second principal component will be derived in a similar fashion. Since we know the first principal axis, we can subtract the variance along this principal axis from the total variance to get the residual variance. In other words, using the first principal component, we would capture some variance in the dataset. But there will be a portion of the total variance in the dataset that is still unexplained by the first principal component. The portion of the total variance unexplained is the residual variance. Using the second principal component, we wish to capture as much variance as we can. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p100"> &#13;
   <p>Using the same process to capture the direction of maximum variance, we will get the second principal component. The second principal component can be at several angles with respect to the first one, as shown in figure 3.8. It is mathematically proven that if the second principal component is orthogonal (i.e., 90˚)<sup> </sup>to the first principal component, this allows us to capture the maximum variance using the two principal components. In figure 3.8, we can observe that the two principal components are at an angle of 90˚ to each other. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p101">  &#13;
   <img alt="figure" src="../Images/CH03_F08_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.8</span> The first figure on the left is the first principal component. The second principal component can be at different angles with respect to the first principal component (middle). We should find the second principle that allows us to capture the maximum variance. To capture the maximum variance, the second principal component should be orthogonal to the first one, and thus the combined variance captured is maximized (right). </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p102"> &#13;
   <p>The process continues for the third and fourth principal components and so on. With more principal components, the representation in a vector space becomes difficult to visualize. You can think of a vector space diagram with more than three axes. Once all the principal components are derived, the dataset is projected onto these axes. The columns in this transformed dataset are the <em>principal components</em>. The principal components created will be fewer than the number of original variables and will capture the maximum information present in the dataset. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p103"> &#13;
   <p>Before we examine the process of PCA in-depth, let’s study its important characteristics: </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p104"> PCA aims to reduce the number of dimensions in the resultant dataset. </li> &#13;
   <li class="readable-text" id="p105"> PCA produces principal components that aim to reduce the noise in the dataset by maximizing the feature variance. </li> &#13;
   <li class="readable-text" id="p106"> At the same time, the principal components reduce the redundancy in the dataset. This is achieved by minimizing the covariance between the pairs of features. </li> &#13;
   <li class="readable-text" id="p107"> The original variables no longer exist in the newly created dataset. Instead, new variables are created using these variables. </li> &#13;
   <li class="readable-text" id="p108"> It is not necessary that the principal components map one-to-one with all the variables present in the dataset. They are a new combination of the existing variables. Hence, they can be a combination of several different variables in one principal component (as shown in equation 3.1). </li> &#13;
   <li class="readable-text" id="p109"> The new features created from the dataset do not share the same column names. </li> &#13;
   <li class="readable-text" id="p110"> The original variables might be correlated with each other, but the newly created variables are unrelated to each other. </li> &#13;
   <li class="readable-text" id="p111"> The number of newly created variables is fewer than the original number of variables. The process to select the number of principal components has been described in section 3.5.2. After all, that is the whole purpose of dimensionality reduction. </li> &#13;
   <li class="readable-text" id="p112"> If PCA has been used for reducing the number of variables in a training dataset, the testing/validation datasets should be reduced by using PCA. </li> &#13;
   <li class="readable-text" id="p113"> PCA is not synonymous with dimensionality reduction only. It can be put into use for a number of other usages beyond dimensionality reduction like feature extraction, data visualization, multicollinearity detection, preprocessing, etc. Using a PCA only for dimensionality reduction will be a misnomer for sure. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p114"> &#13;
   <p>We will now examine the approach used while implementing PCA, and then we will develop a Python solution using PCA. We need not apply all the steps while we develop the codes, as the heavy lifting has already been done by the packages and libraries. The steps given here are taken care of by the packages, but still, it is imperative that you understand these steps to properly appreciate how PCA works:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p115"> In PCA, we start with <em>normalizing our dataset</em> as a first step. It ensures that all our variables have a common representation and become comparable. We have methods to perform the normalization in Python, which we will study when we develop the code. To explore more about normalizing the dataset, see the appendix. </li> &#13;
   <li class="readable-text" id="p116"> Get the covariance in the normalized dataset. It allows us to study the relationship between the variables. We generally create a covariance matrix, as shown in the Python example in the next section. </li> &#13;
   <li class="readable-text" id="p117"> We can then calculate the eigenvectors and eigenvalues of the covariance matrix. The mathematical concept of eigenvectors is given in the appendix. </li> &#13;
   <li class="readable-text" id="p118"> We then sort the eigenvalues in decreasing order of eigenvalues. We choose the eigenvectors corresponding to the maximum value of eigenvalues. The components chosen will be able to capture the maximum variance in the dataset. There are other methods to shortlist the principal components, which we will explore while we develop the Python code. </li> &#13;
  </ol> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p119"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 3.3</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p120"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p121"> PCA will result in the same number of variables in the dataset. True or False? </li> &#13;
    <li class="readable-text" id="p122"> PCA will be able to capture 100% of the information in the dataset. True or False? </li> &#13;
    <li class="readable-text" id="p123"> What is the logic of selecting principal components in PCA? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p124"> &#13;
   <p>So, in essence, principal components are the linear combinations of the original variables. The weight in this linear combination is the eigenvector satisfying the error criteria of the least square method.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p125"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">3.5.1</span> Eigenvalue decomposition</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p126"> &#13;
   <p>In the context of PCA, the eigenvector will represent the direction of the vector and the eigenvalue will be the variance that is captured along that eigenvector. See figure 3.9, where we break the original <em>n</em> x <em>n</em> matrix into components.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p127">  &#13;
   <img alt="figure" src="../Images/CH03_F09_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.9</span> Using eigenvalue decomposition, the original matrix can be broken into an eigenvector matrix, an eigenvalue matrix, and an inverse of an eigenvector matrix. We implement PCA using this methodology.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p128"> &#13;
   <p>Mathematically, we can show the relation with equation 3.2 </p> &#13;
  </div> &#13;
  <div class="browsable-container equation-container" id="p129"> &#13;
   <h5 class=" browsable-container-h5">(3.2)</h5> &#13;
   <p><em>A</em>*<em>v</em> = <em class="obliqued">λ</em>*<em>v</em></p> &#13;
  </div> &#13;
  <div class="readable-text" id="p130"> &#13;
   <p>where <em>A</em> is a square matrix, <em>v</em> is the eigenvector, and <em class="obliqued">λ</em> is the eigenvalue. Here, it is important to note that the eigenvector matrix is the orthonormal matrix, and its columns are eigenvectors. The eigenvalue matrix is the diagonal matrix, and its eigenvalues are the diagonal elements. The last component is the inverse of the eigenvector matrix. Once we have the eigenvalues and the eigenvectors, we can choose the significant eigenvectors for getting the principal components. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p131"> &#13;
   <p>We present PCA and SVD as two separate methods in this book. Both methods are used to reduce high-dimensional data into lower-dimensional ones and, in the process, retain the maximum information in the dataset. The difference between the two is SVD exists for any sort of matrix (rectangular or square), whereas eigen decomposition is possible only for square matrices. You will understand it better once we have covered SVD later in this chapter.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p132"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">3.5.2</span> Python solution using PCA</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p133"> &#13;
   <p>We have studied the concepts of PCA and the process using eigenvalue decomposition. It is time for us to dive into Python and develop a PCA solution on a dataset. I will show you how to create eigenvectors and eigenvalues on the dataset. To implement the PCA algorithms, we will use the <code>sklearn</code> library. Libraries and packages provide a faster solution for implementing algorithms. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p134"> &#13;
   <p>We use the Iris dataset for this problem. It is one of the most popular datasets used for machine learning problems. The dataset contains data of three iris species with 50 samples each and having properties of each flower, like petal length, sepal length, etc. The objective of the problem is to predict the species using the properties of the flower. The independent variables, hence, are the flower properties, whereas the variable “species” is the target variable. The dataset and the code are checked in at the GitHub repository. Here we are using the inbuilt PCA functions, which reduce the effort required to implement PCA. The steps are as follows: </p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p135"> Load all the necessary libraries. We are going to use <code>numpy</code>, <code>pandas</code>, <code>seaborn</code>, <code>matplotlib</code>, and<span class="code-char"> </span><code>sklearn</code>. Note that we have imported PCA from <code>sklearn</code>. </li> &#13;
  </ol> &#13;
  <div class="readable-text print-book-callout" id="p136"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> The following are the standard libraries. You will find that almost all the machine learning solutions would import these libraries in the solution notebook:</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p137"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import numpy as np&#13;
import pandas as pd&#13;
import seaborn as sns&#13;
import matplotlib.pyplot as plt&#13;
from sklearn.decomposition import PCA&#13;
from sklearn.preprocessing import StandardScaler</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p138"><span class="faux-ol-li-counter">2. </span> Load the dataset now. It is a .csv file: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p139"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">iris_df = pd.read_csv('IRIS.csv')</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p140"><span class="faux-ol-li-counter">3. </span> We will now perform a basic check on the dataset, looking at the first five rows, the shape of the data, the spread of the variables, etc. We are not performing an extensive exploratory data analysis here as the steps are covered in chapter 2. The dataset has 150 rows and 6 columns (see figure 3.10). </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p141"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">iris_df.head()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p142"> &#13;
   <img alt="figure" src="../Images/CH03_UN01_Verdhan.png"/> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p143"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">iris_df.describe()&#13;
iris_df.shape<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p144">  &#13;
   <img alt="figure" src="../Images/CH03_F10_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.10</span> Code output</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p145"><span class="faux-ol-li-counter">4. </span> Here, we should break the dataset into independent variables and a target variable. <code>X_variables</code> here represent the independent variables, which are in columns 2–5 of the dataset while <code>y_variable</code> is the target variable, which is “species” in this case and is the final column in the dataset. Recall we wish to predict the species of a flower using the other properties. Hence, we have separated the target variable “species” and other independent variables: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p146"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">X_variables = iris_df.iloc[:,1:5]&#13;
X_variables&#13;
y_variable = iris_df.iloc[:,5]</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p147"><span class="faux-ol-li-counter">5. </span> Normalize the dataset. The built-in method of <code>StandardScalar()</code> does the job for us quite easily. </li> &#13;
  </ol> &#13;
  <div class="readable-text print-book-callout" id="p148"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> The <code>StandardScalar()</code> method normalizes the dataset for us. It subtracts the mean from the variable and divides it by the standard deviation. For more details on normalization, refer to the appendix.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p149"> &#13;
   <p>We invoke the method and then use it on our dataset to get the transformed dataset. Since we are working on independent variables, we are using <code>X_variables</code> here. First, we invoke the <code>StandardScalar()</code> method. Then we use the <code>fit_transform</code> method. The <code>fit_transform</code> method first fits the transformers to <em>X</em> and <em>Y</em> and then returns a transformed version of <em>X</em>:</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p150"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">sc = StandardScaler()&#13;
transformed_df = sc.fit_transform(X_variables)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p151"><span class="faux-ol-li-counter">6. </span> Calculate the covariance matrix and print it. The output is shown in figure 3.11. Getting the covariance matrix is straightforward using <code>numpy</code>: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p152"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">covariance_matrix = np.cov(transformed_df.T)&#13;
covariance_matrix<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p153">  &#13;
   <img alt="figure" src="../Images/CH03_F11_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.11</span> The covariance matrix</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p154"><span class="faux-ol-li-counter">7. </span> Calculate the eigenvalues. Inside the <code>numpy</code> library, we have the built-in functionality to calculate the eigenvalues. We will then sort the eigenvalues in descending order. To shortlist the principal components, we can choose eigenvalues greater than 1. This criterion is called <em>Kaiser criteria.</em> We are exploring other methods too. </li> &#13;
  </ol> &#13;
  <div class="readable-text print-book-callout" id="p155"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> The eigenvalue represents how good a component is as a summary of the data. If the eigenvalue is 1, it means that the component contains the same amount of information as a single variable; hence, we choose the eigenvalue that is greater than 1. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p156"> &#13;
   <p>In this code, first we get the <code>eigen_values</code> and <code>eigen_vectors</code>, and then we arrange them in descending order (see figure 3.12):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p157"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)&#13;
eigen_pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]&#13;
print('Eigenvalues arranged in descending order:')&#13;
for i in eigen_pairs:&#13;
    print(i[0])<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p158">  &#13;
   <img alt="figure" src="../Images/CH03_F12_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.12</span> Eigenvalues arranged in descending order</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p159"><span class="faux-ol-li-counter">8. </span> Invoke the PCA method from the <code>sklearn</code> library. The method is used to fit the data here. Note we have not yet determined the number of principal components we wish to use in this problem:  </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p160"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">pca = PCA()&#13;
pca = pca.fit(transformed_df)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p161"><span class="faux-ol-li-counter">9. </span> The principal components are now set. Let’s have a look at the variance explained by them. We can observe that the first component captures 72.77% variation, the second captures 23.03% variation, and so on (figure 3.13): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p162"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">explained_variance = pca.explained_variance_ratio_&#13;
explained_variance<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p163">  &#13;
   <img alt="figure" src="../Images/CH03_F13_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.13</span> The degree of variance of the principal components</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p164"><span class="faux-ol-li-counter">10. </span> We now plot the components in a bar plot for better visualization (see figure 3.14): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p165"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">dataframe = pd.DataFrame({'var':pca.explained_variance_ratio_,&#13;
             'PC':['PC1','PC2','PC3','PC4']})&#13;
sns.barplot(x='PC',y="var", &#13;
           data=dataframe, color="b");<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p166">  &#13;
   <img alt="figure" src="../Images/CH03_F14_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.14</span> Bar plot of the principal components</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p167"><span class="faux-ol-li-counter">11. </span> Here we draw a scree plot to visualize the cumulative variance being explained by the principal components (see figure 3.15): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p168"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">plt.plot(np.cumsum(pca.explained_variance_ratio_))&#13;
plt.xlabel('number of components')&#13;
plt.ylabel('cumulative explained variance')&#13;
plt.show()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p169">  &#13;
   <img alt="figure" src="../Images/CH03_F15_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.15</span> Scree plot of cumulative variance</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p170"><span class="faux-ol-li-counter">12. </span> In this case study, we choose the top two principal components as the final solutions, as these two capture 95.08% of the total variance in the dataset: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p171"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">pca_2 = PCA(n_components =2 )&#13;
pca_2 = pca_2.fit(transformed_df)&#13;
pca_2d = pca_2.transform(X_variables)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p172"><span class="faux-ol-li-counter">13. </span> We will now plot the dataset with respect to two principal components. For that, species must be tied back to the actual values of the species variable, which are <code>Iris-setosa</code>, <code>Iris-versicolor</code>, and <code>Iris-virginica</code>. Here, <code>0</code> is mapped to <code>Iris-setosa</code>, <code>1</code> is <code>Iris-versicolor</code>, and <code>2</code> is <code>Iris-virginica</code>. In the following code, first the species variable gets its values replaced by using the mapping discussed earlier: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p173"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">iris_df['Species'] = iris_df['Species'].replace({'Iris-setosa':0, 'Iris-&#13;
versicolor':1, 'Iris-virginica':2})</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p174"><span class="faux-ol-li-counter">14. </span> We will now plot the results with respect to two principal components. The plot shows the dataset reduced to two principal components we have just created. These principal components can capture 95.08% variance of the dataset. The first principal component represents the x-axis in the plot while the second principal component represents the y-axis in the plot (see figure 3.16). The color represents the various classes of Species. The print version of the book will not show the different colors, but the output of the Python code will. The same output is also available at the GitHub repository: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p175"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">plt.figure(figsize=(8,6))&#13;
plt.scatter(pca_2d[:,0], pca_2d[:,1],c=iris_df['Species'])&#13;
plt.show()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p176">  &#13;
   <img alt="figure" src="../Images/CH03_F16_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.16</span> The results for two principal components</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p177"> &#13;
   <p>This solution has reduced the number of components from four to two and still is able to retain most of the information. Here, we have examined three approaches to select the principal components based on the Kaiser criteria, the variance captured, and the scree plot. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p178"> &#13;
   <p>Let us quickly analyze what we have achieved using PCA. Figure 3.17 shows two representations of the same dataset. The one on the left is the original dataset of <span class="code-char">X_variables</span>. It has four variables and 150 rows. The right is the output of PCA. It has 150 rows but only two variables. Recall we have reduced the number of dimensions from four to two. So, the number of observations has remained 150, while the number of variables has reduced from four to two. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p179">  &#13;
   <img alt="figure" src="../Images/CH03_F17_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.17</span> The figure on the left shows the original dataset, which has 150 rows and four variables. After the implementation of PCA at right, the number of variables has been reduced to two. The number of rows remains the same as 150, which is shown by the length of pca_2d. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p180"> &#13;
   <p>Once we have reduced the number of components, we can continue to implement a supervised learning or an unsupervised learning solution. We can implement the preceding solution for any of the other real-world problems where we aim to reduce the number of dimensions. We explore this more in section 3.8. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p181"> &#13;
   <p>With this, we have covered PCA. The GitHub repository contains a very interesting PCA decomposition with variables and a corresponding plot.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p182"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.6</span> Singular value decomposition</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p183"> &#13;
   <p>PCA transforms the data linearly and generates principal components that are not correlated with each other. But the process followed in eigenvalue decomposition can only be applied to <em>square matrices</em>, whereas SVD can be implemented to any <em>m</em> × <em>n</em> matrix. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p184"> &#13;
   <p>Say we have matrix <em>A</em>. The shape of <em>A</em> is <em>m</em> × <em>n</em>, or it contains <em>m</em> rows and <em>n</em> columns. The transpose of <em>A</em> can be represented as <em>A</em><sup><em>T</em></sup>. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p185"> &#13;
   <p>We can create two other matrices using <em>A</em> and <em>A</em><sup><em>T</em></sup> as <em>A A</em><sup><em>T</em></sup><sup> </sup>and <em>A</em><sup><em>T</em></sup><em>A</em>. These resultant matrices <em>A A</em><sup><em>T</em></sup><sup> </sup>and <em>A</em><sup><em>T</em></sup><em>A</em> have some special properties, which are as follows (the mathematical proof of the properties is beyond the scope of the book):</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p186"> They are symmetric and square matrices. </li> &#13;
   <li class="readable-text" id="p187"> Their eigenvalues are either positive or zero. </li> &#13;
   <li class="readable-text" id="p188"> Both <em>A A</em><sup><em>T</em></sup><sup> </sup>and <em>A</em><sup><em>T</em></sup><em>A</em> have the same eigenvalue. </li> &#13;
   <li class="readable-text" id="p189"> Both <em>A A</em><sup><em>T</em></sup><sup> </sup>and <em>A</em><sup><em>T</em></sup><em>A</em> have the same rank as the original matrix A. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p190"> &#13;
   <p>The eigenvectors of <em>A A</em><sup><em>T</em></sup><sup> </sup>and <em>A</em><sup><em>T</em></sup><em>A </em>are referred to as singular vectors of A. The square root of their eigenvalues is called singular values. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p191"> &#13;
   <p>Since both matrices (<em>A A</em><sup><em>T</em></sup><sup> </sup>and <em>A</em><sup><em>T</em></sup><em>A</em>) are symmetrical, their eigenvectors are orthonormal to each other. In other words, because they are symmetrical, the eigenvectors are perpendicular to each other and can be of unit length. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p192"> &#13;
   <p>Now, with this mathematical understanding, we can define SVD. As per the SVD method, it is possible to factorize any matrix A, as shown in equation 3.3:</p> &#13;
  </div> &#13;
  <div class="browsable-container equation-container" id="p193"> &#13;
   <h5 class=" browsable-container-h5">(3.3)</h5> &#13;
   <p><em>A</em> = <em>U</em> * <em>S</em> * <em>V</em><sup><em>T</em></sup></p> &#13;
  </div> &#13;
  <div class="readable-text" id="p194"> &#13;
   <p>Here, <em>A</em> is the original matrix, <em>U</em> and <em>V</em> are the orthogonal matrices with orthonormal eigenvectors taken from <em>A A</em><sup><em>T</em></sup><sup> </sup>and <em>A</em><sup><em>T</em></sup><em>A</em>, respectively, and <em>S</em> is the diagonal matrix with <em>r</em> elements equal to the singular values. In simple terms, SVD can be seen as an enhancement of the PCA methodology using eigenvalue decomposition. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p195"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Singular values are better and numerically more robust than eigenvalues decomposition.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p196"> &#13;
   <p>PCA was defined as the linear transformation of input variables using principal components. All those concepts of linear transformation, such as choosing the best components, etc., remain the same. The major process steps also remain similar, except in SVD we use a slightly different approach wherein the eigenvalue decomposition is replaced by singular vectors and singular values. It is often advisable to use SVD when we have a sparse dataset; in the case of a denser dataset, PCA can be utilized. </p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p197"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 3.4</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p198"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p199"> SVD works on the eigenvalue decomposition technique. True or False? </li> &#13;
    <li class="readable-text" id="p200"> PCA is a much more robust methodology than SVD. True or False? </li> &#13;
    <li class="readable-text" id="p201"> What are singular values and singular vectors in SVD? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p202"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">3.6.1</span> Python solution using SVD</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p203"> &#13;
   <p>In this case study, we are using the <em>mushrooms</em> dataset. This dataset contains descriptions of 23 species of grilled mushrooms. There are two classes: either the mushroom is <em>e</em>, which means it is edible, or the mushroom is <em>p</em>, meaning it is poisonous. The steps are as follows:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p204"> Import the libraries: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p205"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import numpy as np&#13;
import pandas as pd&#13;
import seaborn as sns&#13;
import matplotlib.pyplot as plt&#13;
from sklearn.preprocessing import LabelEncoder, StandardScaler</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p206"><span class="faux-ol-li-counter">2. </span> Import the dataset and check for shape, head, etc. (see figure 3.18): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p207"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">mushrooms_df = pd.read_csv('mushrooms.csv')&#13;
mushrooms_df.shape&#13;
mushrooms_df.head()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p208">  &#13;
   <img alt="figure" src="../Images/CH03_F18_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.18</span> Code output</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p209"><span class="faux-ol-li-counter">3. </span> As we can observe, the values are categorical in nature in the dataset. They should be first encoded into numeric values. This is not the only approach for dealing with categorical variables. There are other techniques too, which we will explore throughout the book. </li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p210"> &#13;
   <p>First, invoke the <code>LabelEncoder</code> and then apply it to all the columns in the dataset. The <code>LabelEncoder</code> converts the categorical variables into numeric ones using the one-hot encoding method:</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p211"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">encoder = LabelEncoder()&#13;
for col in mushrooms_df.columns:&#13;
           mushrooms_df[col] = encoder.fit_transform(mushrooms_df[col])</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p212"><span class="faux-ol-li-counter">4. </span> Have another look at the dataset. All the categorical values have been converted to numeric ones (see figure 3.19): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p213"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">mushrooms_df.head()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p214">  &#13;
   <img alt="figure" src="../Images/CH03_F19_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.19</span> Code output</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p215"><span class="faux-ol-li-counter">5. </span> The next two steps are the same as the last case study, wherein we break the dataset into <code>X_variables</code> and <code>y_label</code>. Then the dataset is normalized: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p216"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">X_variables = mushrooms_df.iloc[:,1:23]&#13;
y_label = mushrooms_df.iloc[:, 0]&#13;
scaler = StandardScaler()&#13;
X_features = scaler.fit_transform(X_variables)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p217"><span class="faux-ol-li-counter">6. </span> Implement the SVD. There is a method in <code>numpy</code> that implements SVD. The output is <code>u</code>, <code>s</code>, and <code>v</code>, where <code>u</code> and <code>v</code> are the singular vectors and <code>s</code> is the singular value. If you wish, you can analyze their respective shapes and dimensions: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p218"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">u, s, v = np.linalg.svd(X_features, full_matrices=True)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p219"><span class="faux-ol-li-counter">7. </span> We know that singular values allow us to compute variance explained by each of the singular vectors. We will now analyze the percentage variance explained by each singular vector and plot it (see figure 3.20). The results are shown to three decimal places. Then we plot the results as a histogram plot. On the x-axis, we have the singular vectors while on the y-axis we have the percent of variance explained: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p220"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">variance_explained = np.round(s**2/np.sum(s**2), decimals=3)&#13;
variance_explained&#13;
sns.barplot(x=list(range(1,len(variance_explained)+1)),&#13;
            y=variance_explained, color="blue")&#13;
plt.xlabel('SVs', fontsize=16)&#13;
plt.ylabel('Percent of the variance explained', fontsize=15)<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p221">  &#13;
   <img alt="figure" src="../Images/CH03_F20_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.20</span> Code output</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p222"><span class="faux-ol-li-counter">8. </span> Create a dataframe (see figure 3.21). This new dataframe <span class="code-char">svd_df</span> contains the first two singular vectors and the metadata. We then print the first five rows using the head command: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p223"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">col_labels= ['SV'+str(i) for i in range(1,3)]&#13;
svd_df = pd.DataFrame(u[:,0:2], index=mushrooms_df["class"].tolist(), &#13;
columns=col_labels)&#13;
svd_df=svd_df.reset_index()&#13;
svd_df.rename(columns={'index':'Class'}, inplace=True)&#13;
svd_df.head()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p224">  &#13;
   <img alt="figure" src="../Images/CH03_F21_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.21</span> Dataframe containing the first two singular vectors and the metadata </h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p225"><span class="faux-ol-li-counter">9. </span> Like the last case study, we replace numeric values with actual class labels; <code>1</code> is edible while <code>0</code> is poisonous: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p226"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">svd_df['Class'] = svd_df['Class'].replace({1:'Edible', 0:'Poison'})</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p227"><span class="faux-ol-li-counter">10. </span> We now plot the variance explained by the two components (see figure 3.22). Here, we have chosen only the first two components. You are advised to take the optimum number of components using the methods described in the last section and plot the respective scatter plots. Here, on the x-axis, we have shown the first singular vector SV1, and on the y-axis we have shown the second singular vector SV2. The print version of the book does not show the different colors, but the output of the Python code does. The same output is available at the GitHub repository too: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p228"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">color_dict = dict({'Edible':'Black',&#13;
                   'Poison': 'Red'})&#13;
sns.scatterplot(x="SV1", y="SV2", hue="Class", &#13;
                palette=color_dict, &#13;
                data=svd_df, s=105,&#13;
                alpha=0.5)&#13;
plt.xlabel('SV 1: {0}%'.format(variance_explained[0]*100), fontsize=15)&#13;
plt.ylabel('SV 2: {0}%'.format(variance_explained[1]*100), fontsize=15)<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p229">  &#13;
   <img alt="figure" src="../Images/CH03_F22_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.22</span> Plot of the variance explained by two components</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p230"> &#13;
   <p>We can observe the distribution of the two classes with respect to the two components. The two classes—<code>Edible</code> and <code>Poison</code>—are color-coded as black and red, respectively. As we have noted previously, we have chosen only two components to show the effect using a visualization plot. You should choose the optimum number of components using the methods described in the last case study and then visualize the results using different singular vectors. This solution can be used to reduce dimensions in a real-world dataset. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p231"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.7</span> Pros and cons of dimensionality reduction </h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p232"> &#13;
   <p>In the initial sections of the chapter, we discussed the drawbacks of the curse of dimensionality. In the last few sections, we discovered PCA and SVD and implemented them using Python. Now we will examine the advantages and challenges of these techniques. The major advantages of implementing PCA or SVD are</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p233"> A reduced number of dimensions leads to less complexity in the dataset. The correlated features are removed and transformed. Treating correlated variables manually is a tough task, which is quite manual and frustrating. Techniques like PCA and SVD do that job for us quite easily. The number of correlated features is minimized, and overall dimensions are reduced. </li> &#13;
   <li class="readable-text" id="p234"> Visualization of the dataset<strong> </strong>is better if the number of dimensions is fewer. It is very difficult to visualize and depict a very high-dimensional dataset. </li> &#13;
   <li class="readable-text" id="p235"> The accuracy<strong> </strong>of the machine learning model is improved if the correlated variables are removed. These variables do not add anything to the performance of the model. </li> &#13;
   <li class="readable-text" id="p236"> The training time is reduced as the dataset is less complex. Hence, less computation power and time are required. </li> &#13;
   <li class="readable-text" id="p237"> Overfitting<strong> </strong>is a nuisance in supervised machine learning models. It is a condition where the model behaves very well on the training dataset but not so well on the testing/validation dataset. It means that the model may not be able to perform well on real-world unseen datasets. And it defeats the entire purpose of building the machine learning model. PCA/SVD helps tackle overfitting by reducing the number of variables. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p238"> &#13;
   <p>At the same time, there are a few challenges we face with dimensionality reduction techniques, which are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p239"> The new components created by PCA/SVD are often less interpretable. They are a combination of the independent variables in the dataset and do not actually relate to the real world; hence it can be difficult to relate them to real-world scenarios. </li> &#13;
   <li class="readable-text" id="p240"> Numeric variables are required for PCA/SVD. Hence all the categorical variables should be represented in numeric form. </li> &#13;
   <li class="readable-text" id="p241"> Normalization/standardization of the dataset is required before the solution can be implemented. </li> &#13;
   <li class="readable-text" id="p242"> There might be information loss<strong> </strong>when we use PCA or SVD. The principal components <em>cannot</em> replace the original dataset, and hence there might be some loss of information when we implement these methods. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p243"> &#13;
   <p>However, despite a few challenges, PCA and SVD are used for reducing dimensions in a dataset. They are two of the most popular methods and are quite heavily used. Note that these are linear methods; we cover nonlinear methods of dimensionality reduction in a later part of the book.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p244"> &#13;
   <p>We have now covered the two most important techniques used in dimensionality reduction. We will examine more advanced techniques in the later chapters. It is time to move on to the case study. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p245"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.8</span> Case study for dimension reduction </h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p246"> &#13;
   <p>Let’s explore a real-world case to relate the use of PCA and SVD in real-world business scenarios. Consider this: you are working for a telecommunication service provider. You have a subscriber base, and you wish to cluster the consumers over several parameters. The challenge is the huge number of dimensions available to be analyzed. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p247"> &#13;
   <p>The objective will be to reduce the number of attributes using dimension reduction algorithms. The consumer dataset might include the following:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p248"> Demographic details of the subscriber, which will consist of age, gender, occupation, household size, marital status, etc. (see figure 3.23).<span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p249">  &#13;
   <img alt="figure" src="../Images/CH03_F23_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.23</span> Demographic details of a subscriber like age, gender, marital status, household size, city, etc.</h5>&#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p250"> Subscription details of the consumer, which might look like figure 3.24.<span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p251">  &#13;
   <img alt="figure" src="../Images/CH03_F24_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.24</span> Subscription details of a subscriber like tenure, postpaid/prepaid connection, etc.</h5>&#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p252"> Consumer usage, such as the minutes, call rates, data usages, services, etc. (see figure 3.25).<span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p253">  &#13;
   <img alt="figure" src="../Images/CH03_F25_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.25</span> Usage of a subscriber specifies the number of minutes used, SMS sent, data used, days spent in a network, national or international usage, etc.</h5>&#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p254"> Payment and transaction details of the subscribers, which could be the various transactions made, the mode of payment, frequency of payments, days since last payment made, etc. (see figure 3.26).<span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p255">  &#13;
   <img alt="figure" src="../Images/CH03_F26_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.26</span> Transaction details of a subscriber showing all the details of amount, mode, etc. </h5>&#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p256"> Many more attributes. So far, we have established that the number of variables involved are indeed high. Once we join all these data points, the number of dimensions in the final data can be huge (see figure 3.27). <span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p257">  &#13;
   <img alt="figure" src="../Images/CH03_F27_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.27</span> The final dataset is a combination of all the aforementioned datasets. It will be a big, really high-dimensional dataset to be analyzed.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p258"> &#13;
   <p>We should reduce the number of attributes before we proceed to any supervised or unsupervised solution. In this chapter, we focus on dimensionality reduction techniques, and hence the steps cover that aspect of the process. In later chapters, we will examine exploratory analysis in more detail.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p259"> &#13;
   <p>As a first step, we will perform a sanity check of the dataset and do the data cleaning. We will examine the number of data points, number of missing values, duplicates, junk values present, etc. This will allow us to delete any variables that might be very sparse or contain not much information. For example, if the gender is available for only 0.01% of the customer base, it might be a good idea to drop the variable. Or if all the customers state their gender is male, the variable is not adding any new information to us, and hence it can be discarded. Sometimes, using business logic, a variable might be dropped from the dataset. An example has been discussed in section 3.4. In this step, we might combine a few variables. For example, we might create a new variable as average transaction value by dividing the total amount spent by the total number of transactions. In this way, we will be able to reduce a few dimensions.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p260"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> A Python Jupyter notebook is available in the GitHub repository, where we have given a very detailed solution for the data cleaning step.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p261"> &#13;
   <p>Once we are done with the basic cleaning of the data, we start with the exploratory data analysis. As a part of exploratory analysis, we examine the spread of the variable, its distribution, mean/median/mode of numeric variables, and so on. This is sometimes referred to as <em>univariate analysis.</em> This step allows us to measure the spread of the variables, understand the central tendencies, examine the distribution of different classes for categorical variables, and look for any anomalies in the values. For example, using the dataset mentioned earlier, we will be interested in analyzing the maximum/minimum/average data usage or the percentage distribution of gender or age. We would want to know the most popular method to make a transaction, and we would also be interested to know the maximum/minimum/average amount of the transactions. The list goes on.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p262"> &#13;
   <p>Then we explore the relationships between variables, which is referred to as <em>bivariate analysis</em>. Crosstabs, or distribution of data, is a part of bivariate analysis. A correlation matrix is created during this step. Variables that are highly correlated are examined thoroughly. And based on business logic, one of them might be dropped. This step is useful to visualize and understand the behavior of one variable in the presence of other variables. We can examine their mutual relationships and the respective strength of the relationships. In this case study, we would answer questions such as, “Do subscribers who use more data spend more time on the network as compared to subscribers who send more SMS?”, “Do the subscribers who make a transaction using the online mode generate more revenue than the ones using cash?”, or “Is there a relationship between gender/age and the data usage?” Many such questions are answered during this phase of the project. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p263"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> A Python Jupyter notebook is available in the GitHub repository, which provides detailed steps and code for the univariate and bivariate phases. Check it out!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p264"> &#13;
   <p>At this stage, we have a dataset that has a huge number of dimensions, and we want to reduce the number of dimensions. Now is a good time to implement PCA or SVD. The techniques will reduce the number of dimensions and will make the dataset ready for the next steps in the process, as shown in figure 3.28. The figure is only representative<span class="aframe-location"/> in nature to depict the effect of dimensionality reduction methods. Notice how the large number of black lines in the left figure is reduced to a smaller number of red lines in the right figure.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p265">  &#13;
   <img alt="figure" src="../Images/CH03_F28_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.28</span> A very high-dimensional dataset will be reduced to a low-dimensional one by using principal components that capture the maximum variance in the dataset.</h5>&#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p266"> &#13;
   <p>The output of dimensionality reduction methods will be a dataset with a lower number of variables. The dataset can be then used for supervised or unsupervised learning. We have already looked at the examples using Python in the earlier sections of the chapter.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p267"> &#13;
   <p>This concludes our case study on telecom subscribers. The case can be extended to any other domain like retail; banking, financial services, and insurance; aviation; healthcare; manufacturing; and others. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p268"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.9</span> Concluding thoughts</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p269"> &#13;
   <p>Data is everywhere in various forms, levels, and dimensions and with varying levels of complexity. It is often mentioned that “the more data, the better.” It is indeed true to a certain extent. But with a really high number of dimensions, it becomes quite a herculean task to make sense of it. The analysis can become biased and very complex to deal with. We explored this curse of dimensionality in this chapter. We found PCA and SVD can be helpful to reduce this complexity. They make the dataset ready for the next steps. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p270"> &#13;
   <p>Dimensionality reduction is not as straightforward as it looks. It is not an easy task, but it is certainly a very rewarding one. And it requires a combination of business acumen, logic, and common sense. The resultant dataset might still require some additional work. But it is a very good point for building a machine learning model.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p271"> &#13;
   <p>This marks the end of the third chapter. It also ends the part 1 of the book. In this part, we have covered a few core algorithms. We started with the first chapter of the book, where we explored the fundamentals and basics of machine learning. In the second chapter, we examined three algorithms for clustering. In this third chapter, we explored PCA and SVD. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p272"> &#13;
   <p>In the second part of the book, we change gears and study more advanced topics. We start with association rules in the next chapter. Then we go into advanced clustering methods of time-series clustering, fuzzy clustering, Gaussian mixture mode clustering, etc. That is followed by a chapter on advanced dimensionality reduction algorithms like t-SNE and LDA. To conclude the second part, we examine unsupervised learning on text datasets. The third part of the book is even more advanced, so still a long way to go. Stay tuned!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p273"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">3.10</span> Practical next steps and suggested readings</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p274"> &#13;
   <p>The following provides suggestions for what to do next and offers some helpful reading:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p275"> Use the vehicles dataset used in the last chapter for clustering and implement PCA and SVD on it. Compare the performance on clustering before and after implementing PCA and SVD. </li> &#13;
   <li class="readable-text" id="p276"> Get the datasets from <a href="https://mng.bz/2y9g">https://mng.bz/2y9g</a>. You can find many datasets. Compare the performance of PCA and SVD on these datasets. </li> &#13;
   <li class="readable-text buletless-item" id="p277"> Go through the following papers on PCA: &#13;
    <ul> &#13;
     <li> <a href="https://mng.bz/1XKX">https://mng.bz/1XKX</a> </li> &#13;
     <li> <a href="https://mng.bz/Pd0w">https://mng.bz/Pd0w</a> </li> &#13;
     <li> <a href="https://mng.bz/JYeo">https://mng.bz/JYeo</a> </li> &#13;
     <li> <a href="https://mng.bz/wJqO">https://mng.bz/wJqO</a> </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text buletless-item" id="p278"> Go through the following research papers on SVD: &#13;
    <ul> &#13;
     <li> <a href="https://mng.bz/qxqA">https://mng.bz/qxqA</a> </li> &#13;
     <li> <a href="https://mng.bz/7pNm">https://mng.bz/7pNm</a> </li> &#13;
     <li> <a href="https://arxiv.org/pdf/1211.7102.pdf">https://arxiv.org/pdf/1211.7102.pdf</a> </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p279"> &#13;
   <h2 class=" readable-text-h2">Summary</h2> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p280"> The “curse of dimensionality” refers to problems arising from high-dimensional datasets with too many variables, complicating the analysis and model performance. </li> &#13;
   <li class="readable-text" id="p281"> High dimensions can lead to a sparse dataset, increased mathematical complexity, longer processing times, and potential overfitting in machine learning models. </li> &#13;
   <li class="readable-text" id="p282"> Hughes phenomenon shows that increasing variables only improves model performance up to a point, after which it declines. </li> &#13;
   <li class="readable-text" id="p283"> Not all dimensions are significant; some may not contribute meaningfully to a model’s accuracy and should be removed to reduce complexity. </li> &#13;
   <li class="readable-text" id="p284"> Data visualization can help explain datasets by reducing them to fewer dimensions that still capture significant information. </li> &#13;
   <li class="readable-text" id="p285"> Manual dimension reduction includes dropping insignificant variables or combining them logically to reduce dataset dimensions. </li> &#13;
   <li class="readable-text" id="p286"> Algorithm-based methods for dimension reduction include PCA, SVD, LDA, and t-SNE, among others, which transform high-dimensional data into low-dimensional spaces. </li> &#13;
   <li class="readable-text" id="p287"> PCA reduces dimensions by creating principal components that capture maximum variance while minimizing redundancy and noise. </li> &#13;
   <li class="readable-text" id="p288"> SVD enhances PCA, handling any matrix shape and decomposing them into singular values and vectors to maintain dataset information. </li> &#13;
   <li class="readable-text" id="p289"> Each reduction technique requires the normalization of data and converting categorical variables to numeric forms. </li> &#13;
   <li class="readable-text" id="p290"> Dimensionality reduction simplifies datasets, enhancing visualization and model accuracy, reducing computation time, and mitigating overfitting risks. </li> &#13;
   <li class="readable-text" id="p291"> Challenges with dimensionality reduction include the loss of interpretability, information loss, and the requirement for numerical data. </li> &#13;
   <li class="readable-text" id="p292"> Both PCA and SVD are widely used to effectively reduce dimensions, and each is suitable for different dataset densities. </li> &#13;
   <li class="readable-text" id="p293"> The techniques can be applied in various industries like retail; banking, financial services, and insurance; and healthcare to simplify high-dimensional datasets for analysis. </li> &#13;
   <li class="readable-text" id="p294"> The reduction process involves preliminary data cleaning and exploratory data analysis and then applying dimension-reduction techniques. </li> &#13;
  </ul>&#13;
 </body></html>