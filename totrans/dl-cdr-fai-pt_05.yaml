- en: Chapter 3\. Data Ethics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 数据伦理
- en: As we discussed in Chapters [1](ch01.xhtml#chapter_intro) and [2](ch02.xhtml#chapter_production),
    sometimes machine learning models can go wrong. They can have bugs. They can be
    presented with data that they haven’t seen before and behave in ways we don’t
    expect. Or they could work exactly as designed, but be used for something that
    we would much prefer they were never, ever used for.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](ch01.xhtml#chapter_intro)和[第2章](ch02.xhtml#chapter_production)中讨论的，有时机器学习模型可能出错。它们可能有错误。它们可能被呈现出以前没有见过的数据，并以我们意料之外的方式行事。或者它们可能完全按设计工作，但被用于我们非常希望它们永远不要被用于的事情。
- en: Because deep learning is such a powerful tool and can be used for so many things,
    it becomes particularly important that we consider the consequences of our choices.
    The philosophical study of *ethics* is the study of right and wrong, including
    how we can define those terms, recognize right and wrong actions, and understand
    the connection between actions and consequences. The field of *data ethics* has
    been around for a long time, and many academics are focused on this field. It
    is being used to help define policy in many jurisdictions; it is being used in
    companies big and small to consider how best to ensure good societal outcomes
    from product development; and it is being used by researchers who want to make
    sure that the work they are doing is used for good, and not for bad.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 因为深度学习是如此强大的工具，可以用于很多事情，所以我们特别需要考虑我们选择的后果。哲学上对*伦理*的研究是对对错的研究，包括我们如何定义这些术语，识别对错行为，以及理解行为和后果之间的联系。*数据伦理*领域已经存在很长时间，许多学者都专注于这个领域。它被用来帮助定义许多司法管辖区的政策；它被用在大大小小的公司中，考虑如何最好地确保产品开发对社会的良好结果；它被研究人员用来确保他们正在做的工作被用于好的目的，而不是坏的目的。
- en: As a deep learning practitioner, therefore, you will likely at some point be
    put in a situation requiring you to consider data ethics. So what is data ethics?
    It’s a subfield of ethics, so let’s start there.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为一个深度学习从业者，你很可能在某个时候会面临需要考虑数据伦理的情况。那么数据伦理是什么？它是伦理学的一个子领域，所以让我们从那里开始。
- en: Jeremy Says
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 杰里米说
- en: 'At university, philosophy of ethics was my main thing (it would have been the
    topic of my thesis, if I’d finished it, instead of dropping out to join the real
    world). Based on the years I spent studying ethics, I can tell you this: no one
    really agrees on what right and wrong are, whether they exist, how to spot them,
    which people are good and which bad, or pretty much anything else. So don’t expect
    too much from the theory! We’re going to focus on examples and thought starters
    here, not theory.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在大学里，伦理哲学是我的主要研究领域（如果我完成了论文，而不是辍学加入现实世界，它本来会是我的论文题目）。根据我花在研究伦理学上的年份，我可以告诉你这个：没有人真正同意什么是对什么是错，它们是否存在，如何识别它们，哪些人是好人哪些人是坏人，或者几乎任何其他事情。所以不要对理论抱太大期望！我们将在这里专注于例子和思考的起点，而不是理论。
- en: 'In answering the question [“What Is Ethics?”](https://oreil.ly/nyVh4) the Markkula
    Center for Applied Ethics says that the term refers to the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在回答问题[“什么是伦理？”](https://oreil.ly/nyVh4) 应用伦理马库拉中心说，这个术语指的是以下内容：
- en: Well-founded standards of right and wrong that prescribe what humans should
    do
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有根据的对人类应该做什么的正确和错误的标准
- en: The study and development of one’s ethical standards
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究和发展自己的伦理标准
- en: There is no list of right answers. There is no list of dos and don’ts. Ethics
    is complicated and context-dependent. It involves the perspectives of many stakeholders.
    Ethics is a muscle that you have to develop and practice. In this chapter, our
    goal is to provide some signposts to help you on that journey.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 没有正确答案的清单。没有应该和不应该做的清单。伦理是复杂的，依赖于背景。它涉及许多利益相关者的观点。伦理是一个你必须发展和实践的能力。在本章中，我们的目标是提供一些路标，帮助你在这个旅程中前进。
- en: Spotting ethical issues is best to do as part of a collaborative team. This
    is the only way you can really incorporate different perspectives. Different people’s
    backgrounds will help them to see things that may not be obvious to you. Working
    with a team is helpful for many “muscle-building” activities, including this one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 发现伦理问题最好是作为一个协作团队的一部分来做。这是你真正可以融入不同观点的唯一方式。不同人的背景将帮助他们看到你可能没有注意到的事情。与团队合作对于许多“锻炼肌肉”的活动都是有帮助的，包括这个。
- en: This chapter is certainly not the only part of the book where we talk about
    data ethics, but it’s good to have a place where we focus on it for a while. To
    get oriented, it’s perhaps easiest to look at a few examples. So, we picked out
    three that we think illustrate effectively some of the key topics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章当然不是本书中唯一讨论数据伦理的部分，但是有一个地方专注于它一段时间是很好的。为了定位，也许最容易看一些例子。所以，我们挑选了三个我们认为有效地说明了一些关键主题的例子。
- en: Key Examples for Data Ethics
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据伦理的关键例子
- en: 'We are going to start with three specific examples that illustrate three common
    ethical issues in tech (we’ll study these issues in more depth later in the chapter):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从三个具体的例子开始，这些例子说明了技术中三个常见的伦理问题（我们将在本章后面更深入地研究这些问题）：
- en: Recourse processes
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 救济程序
- en: Arkansas’s buggy healthcare algorithms left patients stranded.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 阿肯色州有缺陷的医疗保健算法让患者陷入困境。
- en: Feedback loops
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈循环
- en: YouTube’s recommendation system helped unleash a conspiracy theory boom.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube的推荐系统帮助引发了阴谋论繁荣。
- en: Bias
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见
- en: When a traditionally African-American name is searched for on Google, it displays
    ads for criminal background checks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当在谷歌上搜索传统的非裔美国人名字时，会显示犯罪背景调查的广告。
- en: In fact, for every concept that we introduce in this chapter, we are going to
    provide at least one specific example. For each one, think about what you could
    have done in this situation, and what kinds of obstructions there might have been
    to you getting that done. How would you deal with them? What would you look out
    for?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在本章中我们介绍的每个概念，我们都会提供至少一个具体的例子。对于每一个例子，想想在这种情况下你可以做什么，以及可能会有什么样的障碍阻止你完成。你会如何处理它们？你会注意什么？
- en: 'Bugs and Recourse: Buggy Algorithm Used for Healthcare Benefits'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Verge investigated software used in over half of the US states to determine
    how much healthcare people receive, and documented its findings in the article
    [“What Happens When an Algorithm Cuts Your Healthcare”](https://oreil.ly/25drC).
    After implementation of the algorithm in Arkansas, hundreds of people (many with
    severe disabilities) had their healthcare drastically cut.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aide to
    help her to get out of bed, to go to the bathroom, to get food, and more, had
    her hours of help suddenly reduced by 20 hours a week. She couldn’t get any explanation
    for why her healthcare was cut. Eventually, a court case revealed that there were
    mistakes in the software implementation of the algorithm, negatively impacting
    people with diabetes or cerebral palsy. However, Dobbs and many other people reliant
    on these health-care benefits live in fear that their benefits could again be
    cut suddenly and inexplicably.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedback Loops: YouTube’s Recommendation System'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feedback loops can occur when your model is controlling the next round of data
    you get. The data that is returned quickly becomes flawed by the software itself.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, YouTube has 1.9 billion users, who watch over 1 billion hours
    of YouTube videos a day. Its recommendation algorithm (built by Google), which
    was designed to optimize watch time, is responsible for around 70% of the content
    that is watched. But there was a problem: it led to out-of-control feedback loops,
    leading the *New York Times* to run the headline [“YouTube Unleashed a Conspiracy
    Theory Boom. Can It Be Contained?”](https://oreil.ly/Lt3aU) in February 2019\.
    Ostensibly, recommendation systems are predicting what content people will like,
    but they also have a lot of power in determining what content people even see.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Bias: Professor Latanya Sweeney “Arrested”'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dr. Latanya Sweeney is a professor at Harvard and director of the university’s
    data privacy lab. In the paper [“Discrimination in Online Ad Delivery”](https://oreil.ly/1qBxU)
    (see [Figure 3-1](#lantanya_arrested)), she describes her discovery that Googling
    her name resulted in advertisements saying “Latanya Sweeney, Arrested?” even though
    she is the only known Latanya Sweeney and has never been arrested. However, when
    she Googled other names, such as “Kirsten Lindquist,” she got more neutral ads,
    even though Kirsten Lindquist has been arrested three times.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![Google search showing ads about Professor Latanya Sweeney''s (nonexistent)
    arrest record](Images/dlcf_0301.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Google search showing ads about Professor Latanya Sweeney’s (nonexistent)
    arrest record
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Being a computer scientist, she studied this systematically and looked at over
    2,000 names. She found a clear pattern: historically Black names received advertisements
    suggesting that the person had a criminal record, whereas traditionally white
    names had more neutral advertisements.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of bias. It can make a big difference to people’s lives—for
    instance, if a job applicant is Googled, it may appear that they have a criminal
    record when they do not.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Why Does This Matter?
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One very natural reaction to considering these issues is: “So what? What’s
    that got to do with me? I’m a data scientist, not a politician. I’m not one of
    the senior executives at my company who make the decisions about what we do. I’m
    just trying to build the most predictive model I can.”'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: These are very reasonable questions. But we’re going to try to convince you
    that the answer is that everybody who is training models absolutely needs to consider
    how their models will be used, and consider how to best ensure that they are used
    as positively as possible. There are things you can do. And if you don’t do them,
    things can go pretty badly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: One particularly hideous example of what happens when technologists focus on
    technology at all costs is the story of IBM and Nazi Germany. In 2001, a Swiss
    judge ruled that it was not unreasonable “to deduce that IBM’s technical assistance
    facilitated the tasks of the Nazis in the commission of their crimes against humanity,
    acts also involving accountancy and classification by IBM machines and utilized
    in the concentration camps themselves.”
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当技术人员以任何代价专注于技术时，发生的一个特别可怕的例子是IBM与纳粹德国的故事。2001年，一名瑞士法官裁定认为“推断IBM的技术援助促进了纳粹在犯下反人类罪行时的任务，这些行为还涉及IBM机器进行的会计和分类，并在集中营中使用。”
- en: IBM, you see, supplied the Nazis with data tabulation products necessary to
    track the extermination of Jews and other groups on a massive scale. This was
    driven from the top of the company, with marketing to Hitler and his leadership
    team. Company President Thomas Watson personally approved the 1939 release of
    special IBM alphabetizing machines to help organize the deportation of Polish
    Jews. Pictured in [Figure 3-2](#meeting) is Adolf Hitler (far left) meeting with
    IBM CEO Tom Watson Sr. (second from left), shortly before Hitler awarded Watson
    a special “Service to the Reich” medal in 1937.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你看，IBM向纳粹提供了数据制表产品，以追踪大规模灭绝犹太人和其他群体。这是公司高层的决定，向希特勒及其领导团队推销。公司总裁托马斯·沃森亲自批准了1939年发布特殊的IBM字母排序机，以帮助组织波兰犹太人的驱逐。在[图3-2](#meeting)中，阿道夫·希特勒（最左）与IBM首席执行官汤姆·沃森（左二）会面，希特勒在1937年授予沃森特别的“对帝国的服务”奖章。
- en: '![A picture of IBM CEO Tom Watson Sr. meeting with Adolf Hitler](Images/dlcf_0302.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![IBM首席执行官汤姆·沃森与阿道夫·希特勒会面的图片](Images/dlcf_0302.png)'
- en: Figure 3-2\. IBM CEO Tom Watson Sr. meeting with Adolf Hitler
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2. IBM首席执行官汤姆·沃森与阿道夫·希特勒会面
- en: 'But this was not an isolated incident—the organization’s involvement was extensive.
    IBM and its subsidiaries provided regular training and maintenance onsite at the
    concentration camps: printing off cards, configuring machines, and repairing them
    as they broke frequently. IBM set up categorizations on its punch card system
    for the way that each person was killed, which group they were assigned to, and
    the logistical information necessary to track them through the vast Holocaust
    system (see [Figure 3-3](#punch_card)). IBM’s code for Jews in the concentration
    camps was 8: some 6,000,000 were killed. Its code for Romanis was 12 (they were
    labeled by the Nazis as “asocials,” with over 300,000 killed in the *Zigeunerlager*,
    or “Gypsy camp”). General executions were coded as 4, death in the gas chambers
    as 6.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是个案 - 该组织的涉入是广泛的。IBM及其子公司在集中营现场提供定期培训和维护：打印卡片，配置机器，并在它们经常出现故障时进行维修。IBM在其打孔卡系统上设置了每个人被杀害的方式，他们被分配到的组别以及跟踪他们通过庞大的大屠杀系统所需的后勤信息的分类。IBM在集中营中对犹太人的代码是8：约有600万人被杀害。对于罗姆人的代码是12（纳粹将他们标记为“不合群者”，在“吉普赛营”中有超过30万人被杀害）。一般处决被编码为4，毒气室中的死亡被编码为6。
- en: '![Picture of a punch card used by IBM in concentration camps](Images/dlcf_0303.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![IBM在集中营中使用的打孔卡的图片](Images/dlcf_0303.png)'
- en: Figure 3-3\. A punch card used by IBM in concentration camps
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3. IBM在集中营中使用的打孔卡
- en: 'Of course, the project managers and engineers and technicians involved were
    just living their ordinary lives. Caring for their families, going to the church
    on Sunday, doing their jobs the best they could. Following orders. The marketers
    were just doing what they could to meet their business development goals. As Edwin
    Black, author of *IBM and the Holocaust* (Dialog Press) observed: “To the blind
    technocrat, the means were more important than the ends. The destruction of the
    Jewish people became even less important because the invigorating nature of IBM’s
    technical achievement was only heightened by the fantastical profits to be made
    at a time when bread lines stretched across the world.”'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，参与其中的项目经理、工程师和技术人员只是过着普通的生活。照顾家人，周日去教堂，尽力做好自己的工作。服从命令。市场营销人员只是尽力实现他们的业务发展目标。正如《IBM与大屠杀》（Dialog
    Press）的作者埃德温·布莱克所观察到的：“对于盲目的技术官僚来说，手段比目的更重要。犹太人民的毁灭变得更不重要，因为IBM技术成就的振奋性只会因在面包排长队的时候赚取的奇幻利润而更加突出。”
- en: 'Step back for a moment and consider: How would you feel if you discovered that
    you had been part of a system that ended up hurting society? Would you be open
    to finding out? How can you help make sure this doesn’t happen? We have described
    the most extreme situation here, but there are many negative societal consequences
    linked to AI and machine learning being observed today, some of which we’ll describe
    in this chapter.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 退一步思考一下：如果你发现自己是一个最终伤害社会的系统的一部分，你会有什么感受？你会愿意了解吗？你如何帮助确保这种情况不会发生？我们在这里描述了最极端的情况，但今天观察到与人工智能和机器学习相关的许多负面社会后果，其中一些我们将在本章中描述。
- en: It’s not just a moral burden, either. Sometimes technologists pay very directly
    for their actions. For instance, the first person who was jailed as a result of
    the Volkswagen scandal, in which the car company was revealed to have cheated
    on its diesel emissions tests, was not the manager who oversaw the project, or
    an executive at the helm of the company. It was one of the engineers, James Liang,
    who just did what he was told.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这也不仅仅是道德负担。有时，技术人员会直接为他们的行为付出代价。例如，作为大众汽车丑闻的结果而被监禁的第一个人并不是监督该项目的经理，也不是公司的执行主管。而是其中一名工程师詹姆斯·梁，他只是听从命令。
- en: Of course, it’s not all bad—if a project you are involved in turns out to make
    a huge positive impact on even one person, this is going to make you feel pretty
    great!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，情况并非全是坏的 - 如果你参与的项目最终对一个人产生了巨大的积极影响，这会让你感到非常棒！
- en: OK, so hopefully we have convinced you that you ought to care. But what should
    you do? As data scientists, we’re naturally inclined to focus on making our models
    better by optimizing some metric or other. But optimizing that metric may not
    lead to better outcomes. And even if it *does* help create better outcomes, it
    almost certainly won’t be the only thing that matters. Consider the pipeline of
    steps that occurs between the development of a model or an algorithm by a researcher
    or practitioner, and the point at which this work is used to make a decision.
    This entire pipeline needs to be considered *as a whole* if we’re to have a hope
    of getting the kinds of outcomes we want.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，希望我们已经说服您应该关心这个问题。但是您应该怎么做呢？作为数据科学家，我们自然倾向于通过优化某些指标来改进我们的模型。但是优化这个指标可能不会导致更好的结果。即使它确实有助于创造更好的结果，几乎肯定不会是唯一重要的事情。考虑一下从研究人员或从业者开发模型或算法到使用这项工作做出决策之间发生的步骤流程。如果我们希望获得我们想要的结果，整个流程必须被视为一个整体。
- en: Normally, there is a very long chain from one end to the other. This is especially
    true if you are a researcher who might not even know if your research will ever
    get used for anything, or if you’re involved in data collection, which is even
    earlier in the pipeline. But no one is better placed to inform everyone involved
    in this chain about the capabilities, constraints, and details of your work than
    you are. Although there’s no “silver bullet” that can ensure your work is used
    the right way, by getting involved in the process, and asking the right questions,
    you can at the very least ensure that the right issues are being considered.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从一端到另一端有一条非常长的链。如果您是一名研究人员，甚至可能不知道您的研究是否会被用于任何事情，或者如果您参与数据收集，那就更早了。但是没有人比您更适合告知所有参与这一链的人您的工作的能力、约束和细节。虽然没有“灵丹妙药”可以确保您的工作被正确使用，但通过参与这个过程，并提出正确的问题，您至少可以确保正确的问题正在被考虑。
- en: 'Sometimes, the right response to being asked to do a piece of work is to just
    say “no.” Often, however, the response we hear is, “If I don’t do it, someone
    else will.” But consider this: if you’ve been picked for the job, you’re the best
    person they’ve found to do it—so if you don’t do it, the best person isn’t working
    on that project. If the first five people they ask all say no too, so much the
    better!'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，对于被要求做一项工作的正确回应就是说“不”。然而，我们经常听到的回应是：“如果我不做，别人会做。”但请考虑：如果您被选中做这项工作，那么您是他们找到的最合适的人——所以如果您不做，最合适的人就不会参与该项目。如果他们询问的前五个人也都说不，那就更好了！
- en: Integrating Machine Learning with Product Design
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将机器学习与产品设计整合
- en: Presumably, the reason you’re doing this work is that you hope it will be used
    for something. Otherwise, you’re just wasting your time. So, let’s start with
    the assumption that your work will end up somewhere. Now, as you are collecting
    your data and developing your model, you are making lots of decisions. What level
    of aggregation will you store your data at? What loss function should you use?
    What validation and training sets should you use? Should you focus on simplicity
    of implementation, speed of inference, or accuracy of the model? How will your
    model handle out-of-domain data items? Can it be fine-tuned, or must it be retrained
    from scratch over time?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您做这项工作的原因是希望它被用于某些目的。否则，您只是在浪费时间。因此，让我们假设您的工作最终会有所作为。现在，当您收集数据并开发模型时，您会做出许多决定。您将以什么级别的聚合存储数据？应该使用什么损失函数？应该使用什么验证和训练集？您应该专注于实现的简单性、推理的速度还是模型的准确性？您的模型如何处理域外数据项？它可以进行微调，还是必须随时间从头开始重新训练？
- en: These are not just algorithm questions. They are data product design questions.
    But the product managers, executives, judges, journalists, doctors—whoever ends
    up developing and using the system of which your model is a part—will not be well-placed
    to understand the decisions that you made, let alone change them.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不仅仅是算法问题。它们是数据产品设计问题。但是产品经理、高管、法官、记者、医生——最终会开发和使用您的模型的系统的人——将无法理解您所做的决定，更不用说改变它们了。
- en: For instance, two studies found that Amazon’s facial recognition software produced
    [inaccurate](https://oreil.ly/bL5D9) and [racially biased](https://oreil.ly/cDYqz)
    results. Amazon claimed that the researchers should have changed the default parameters,
    without explaining how this would have changed the biased results. Furthermore,
    it turned out that [Amazon was not instructing police departments](https://oreil.ly/I5OAj) that
    used its software to do this either. There was, presumably, a big distance between
    the researchers who developed these algorithms and the Amazon documentation staff
    who wrote the guidelines provided to the police.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，两项研究发现亚马逊的面部识别软件产生了[不准确](https://oreil.ly/bL5D9)和[种族偏见](https://oreil.ly/cDYqz)的结果。亚马逊声称研究人员应该更改默认参数，但没有解释这将如何改变有偏见的结果。此外，事实证明，[亚马逊并没有指导使用其软件的警察部门](https://oreil.ly/I5OAj)这样做。可以想象，开发这些算法的研究人员和为警察提供指导的亚马逊文档人员之间存在很大的距离。
- en: A lack of tight integration led to serious problems for society at large, the
    police, and Amazon. It turned out that its system erroneously matched 28 members
    of Congress to criminal mugshots! (And the Congresspeople wrongly matched to criminal
    mugshots were disproportionately people of color, as seen in [Figure 3-4](#congressmen).)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏紧密整合导致社会、警察和亚马逊出现严重问题。结果表明，其系统错误地将28名国会议员与犯罪照片匹配！（而与犯罪照片错误匹配的国会议员是有色人种，如[图3-4](#congressmen)所示。）
- en: '![Picture of the congresspeople matched to criminal mugshots by Amazon software,
    they are disproportionatedly people of color](Images/dlcf_0304.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![国会议员与亚马逊软件匹配的犯罪照片，他们是有色人种](Images/dlcf_0304.png)'
- en: Figure 3-4\. Congresspeople matched to criminal mugshots by Amazon software
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4. 亚马逊软件将国会议员与犯罪照片匹配
- en: Data scientists need to be part of a cross-disciplinary team. And researchers
    need to work closely with the kinds of people who will end up using their research.
    Better still, domain experts themselves could learn enough to be able to train
    and debug some models themselves—hopefully, a few of you are reading this book
    right now!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家需要成为跨学科团队的一部分。研究人员需要与最终使用他们研究成果的人密切合作。更好的是，领域专家们自己可以学到足够的知识，以便能够自己训练和调试一些模型——希望你们中的一些人正在阅读这本书！
- en: The modern workplace is a very specialized place. Everybody tends to have well-defined
    jobs to perform. Especially in large companies, it can be hard to know all the
    pieces of the puzzle. Sometimes companies even intentionally obscure the overall
    project goals being worked on, if they know that employees are not going to like
    the answers. This is sometimes done by compartmentalizing pieces as much as possible.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现代职场是一个非常专业化的地方。每个人都倾向于有明确定义的工作要做。特别是在大公司，很难知道所有的细节。有时公司甚至会故意模糊正在进行的整体项目目标，如果他们知道员工不会喜欢答案的话。有时通过尽可能地将部分隔离来实现这一点。
- en: In other words, we’re not saying that any of this is easy. It’s hard. It’s really
    hard. We all have to do our best. And we have often seen that the people who do
    get involved in the higher-level context of these projects, and attempt to develop
    cross-disciplinary capabilities and teams, become some of the most important and
    well rewarded members of their organizations. It’s the kind of work that tends
    to be highly appreciated by senior executives, even if it is sometimes considered
    rather uncomfortable by middle management.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们并不是说这些都很容易。这很难。真的很难。我们都必须尽力而为。我们经常看到那些参与这些项目更高层次背景的人，试图发展跨学科能力和团队的人，成为他们组织中最重要和最受奖励的成员之一。这是一种工作，往往受到高级主管的高度赞赏，即使有时被中层管理人员认为相当不舒服。
- en: Topics in Data Ethics
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据伦理学主题
- en: 'Data ethics is a big field, and we can’t cover everything. Instead, we’re going
    to pick a few topics that we think are particularly relevant:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据伦理学是一个广阔的领域，我们无法涵盖所有内容。相反，我们将选择一些我们认为特别相关的主题：
- en: The need for recourse and accountability
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 追索和问责制的需求
- en: Feedback loops
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈循环
- en: Bias
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见
- en: Disinformation
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚假信息
- en: Let’s look at each in turn.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们依次看看每一个。
- en: Recourse and Accountability
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 追索和问责制
- en: 'In a complex system, it is easy for no one person to feel responsible for outcomes.
    While this is understandable, it does not lead to good results. In the earlier
    example of the Arkansas healthcare system in which a bug led to people with cerebral
    palsy losing access to needed care, the creator of the algorithm blamed government
    officials, and government officials blamed those who implemented the software.
    NYU professor [Danah Boyd](https://oreil.ly/KK5Hf) described this phenomenon:
    “Bureaucracy has often been used to shift or evade responsibility….Today’s algorithmic
    systems are extending bureaucracy.”'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个复杂的系统中，很容易没有任何一个人感到对结果负责。虽然这是可以理解的，但这并不会带来好的结果。在早期的阿肯色州医疗保健系统的例子中，一个错误导致患有脑瘫的人失去了所需护理的访问权限，算法的创建者责怪政府官员，政府官员责怪那些实施软件的人。纽约大学教授[丹娜·博伊德](https://oreil.ly/KK5Hf)描述了这种现象：“官僚主义经常被用来转移或逃避责任……今天的算法系统正在扩展官僚主义。”
- en: 'An additional reason why recourse is so necessary is that data often contains
    errors. Mechanisms for audits and error correction are crucial. A database of
    suspected gang members maintained by California law enforcement officials was
    found to be full of errors, including 42 babies who had been added to the database
    when they were less than 1 year old (28 of whom were marked as “admitting to being
    gang members”). In this case, there was no process in place for correcting mistakes
    or removing people after they’d been added. Another example is the US credit report
    system: a large-scale study of credit reports by the Federal Trade Commission
    (FTC) in 2012 found that 26% of consumers had at least one mistake in their files,
    and 5% had errors that could be devastating.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 追索如此必要的另一个原因是数据经常包含错误。审计和纠错机制至关重要。加利福尼亚执法官员维护的一个涉嫌帮派成员的数据库发现充满了错误，包括42名不到1岁的婴儿被添加到数据库中（其中28名被标记为“承认是帮派成员”）。在这种情况下，没有流程来纠正错误或在添加后删除人员。另一个例子是美国信用报告系统：2012年联邦贸易委员会（FTC）对信用报告进行的大规模研究发现，26%的消费者的档案中至少有一个错误，5%的错误可能是灾难性的。
- en: Yet, the process of getting such errors corrected is incredibly slow and opaque.
    When public radio reporter [Bobby Allyn](https://oreil.ly/BUD6h) discovered that
    he was erroneously listed as having a firearms conviction, it took him “more than
    a dozen phone calls, the handiwork of a county court clerk and six weeks to solve
    the problem. And that was only after I contacted the company’s communications
    department as a journalist.”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，纠正这类错误的过程非常缓慢和不透明。当公共广播记者[鲍比·艾伦](https://oreil.ly/BUD6h)发现自己被错误列为有枪支罪时，他花了“十几个电话，一个县法院书记的手工操作和六周的时间来解决问题。而且这还是在我作为一名记者联系了公司的传播部门之后。”
- en: As machine learning practitioners, we do not always think of it as our responsibility
    to understand how our algorithms end up being implemented in practice. But we
    need to.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习从业者，我们并不总是认为理解我们的算法最终如何在实践中实施是我们的责任。但我们需要。
- en: Feedback Loops
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反馈循环
- en: We explained in [Chapter 1](ch01.xhtml#chapter_intro) how an algorithm can interact
    with its environment to create a feedback loop, making predictions that reinforce
    actions taken in the real world, which lead to predictions even more pronounced
    in the same direction. As an example, let’s again consider YouTube’s recommendation
    system. A couple of years ago, the Google team talked about how they had introduced
    reinforcement learning (closely related to deep learning, but your loss function
    represents a result potentially a long time after an action occurs) to improve
    YouTube’s recommendation system. They described how they used an algorithm that
    made recommendations such that watch time would be optimized.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](ch01.xhtml#chapter_intro)中解释了算法如何与环境互动以创建反馈循环，做出预测以加强在现实世界中采取的行动，从而导致更加明显朝着同一方向的预测。举个例子，让我们再次考虑YouTube的推荐系统。几年前，谷歌团队谈到他们如何引入了强化学习（与深度学习密切相关，但你的损失函数代表了潜在长时间后行动发生的结果）来改进YouTube的推荐系统。他们描述了如何使用一个算法，使推荐以优化观看时间为目标。
- en: However, human beings tend to be drawn to controversial content. This meant
    that videos about things like conspiracy theories started to get recommended more
    and more by the recommendation system. Furthermore, it turns out that the kinds
    of people who are interested in conspiracy theories are also people who watch
    a lot of online videos! So, they started to get drawn more and more toward YouTube.
    The increasing number of conspiracy theorists watching videos on YouTube resulted
    in the algorithm recommending more and more conspiracy theory and other extremist
    content, which resulted in more extremists watching videos on YouTube, and more
    people watching YouTube developing extremist views, which led to the algorithm
    recommending more extremist content. The system was spiraling out of control.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人类往往被争议性内容所吸引。这意味着关于阴谋论之类的视频开始越来越多地被推荐给用户。此外，事实证明，对阴谋论感兴趣的人也是那些经常观看在线视频的人！因此，他们开始越来越多地被吸引到YouTube。越来越多的阴谋论者在YouTube上观看视频导致算法推荐越来越多的阴谋论和其他极端内容，这导致更多的极端分子在YouTube上观看视频，更多的人在YouTube上形成极端观点，进而导致算法推荐更多的极端内容。系统失控了。
- en: 'And this phenomenon was not contained to this particular type of content. In
    June 2019, the *New York Times* published an article on YouTube’s recommendation
    system titled [“On YouTube’s Digital Playground, an Open Gate for Pedophiles”](https://oreil.ly/81BEy).
    The article started with this chilling story:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象并不局限于这种特定类型的内容。2019年6月，《纽约时报》发表了一篇关于YouTube推荐系统的文章，标题为[“在YouTube的数字游乐场，对恋童癖者敞开大门”](https://oreil.ly/81BEy)。文章以这个令人不安的故事开头：
- en: Christiane C. didn’t think anything of it when her 10-year-old daughter and
    a friend uploaded a video of themselves playing in a backyard pool…A few days
    later…the video had thousands of views. Before long, it had ticked up to 400,000…“I
    saw the video again and I got scared by the number of views,” Christiane said.
    She had reason to be. YouTube’s automated recommendation system…had begun showing
    the video to users who watched other videos of prepubescent, partially clothed
    children, a team of researchers has found.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当Christiane C.的10岁女儿和一个朋友上传了一个在后院游泳池玩耍的视频时，她并没有在意……几天后……视频的观看次数已经达到了数千次。不久之后，观看次数已经增加到40万……“我再次看到视频，看到观看次数，我感到害怕，”Christiane说。她有理由感到害怕。研究人员发现，YouTube的自动推荐系统……开始向观看其他预备期、部分穿着少儿视频的用户展示这个视频。
- en: On its own, each video might be perfectly innocent, a home movie, say, made
    by a child. Any revealing frames are fleeting and appear accidental. But, grouped
    together, their shared features become unmistakable.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 单独看，每个视频可能是完全无辜的，比如一个孩子制作的家庭影片。任何暴露的画面都是短暂的，看起来是偶然的。但是，当它们被组合在一起时，它们共享的特征变得明显。
- en: YouTube’s recommendation algorithm had begun curating playlists for pedophiles,
    picking out innocent home videos that happened to contain prepubescent, partially
    clothed children.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube的推荐算法开始为恋童癖者策划播放列表，挑选出偶然包含预备期、部分穿着少儿的无辜家庭视频。
- en: No one at Google planned to create a system that turned family videos into porn
    for pedophiles. So what happened?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌没有计划创建一个将家庭视频变成儿童色情片的系统。那么发生了什么？
- en: Part of the problem here is the centrality of metrics in driving a financially
    important system. When an algorithm has a metric to optimize, as you have seen,
    it will do everything it can to optimize that number. This tends to lead to all
    kinds of edge cases, and humans interacting with a system will search for, find,
    and exploit these edge cases and feedback loops for their advantage.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题之一是指标在推动一个财政重要系统中的核心性。当一个算法有一个要优化的指标时，正如你所看到的，它会尽其所能来优化这个数字。这往往会导致各种边缘情况，与系统互动的人类会寻找、发现并利用这些边缘情况和反馈循环以谋取利益。
- en: There are signs that this is exactly what has happened with YouTube’s recommendation
    system in 2018\. *The Guardian* ran an article called [“How an Ex-YouTube Insider
    Investigated Its Secret Algorithm”](https://oreil.ly/yjnPT) about Guillaume Chaslot,
    an ex-YouTube engineer who created a [website](https://algotransparency.org) that
    tracks these issues. Chaslot published the chart in [Figure 3-5](#ethics_yt_rt)
    following the release of Robert Mueller’s “Report on the Investigation Into Russian
    Interference in the 2016 Presidential Election.”
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有迹象表明，这正是发生在YouTube的推荐系统中的情况。*卫报*发表了一篇题为[“一位前YouTube内部人员是如何调查其秘密算法的”](https://oreil.ly/yjnPT)的文章，讲述了前YouTube工程师Guillaume
    Chaslot创建了一个[网站](https://algotransparency.org)来跟踪这些问题。Chaslot在罗伯特·穆勒“关于2016年总统选举中俄罗斯干预调查”的发布后发布了图表，如[图3-5](#ethics_yt_rt)所示。
- en: '![Coverage of the Mueller report](Images/dlcf_0305.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![穆勒报告的报道](Images/dlcf_0305.png)'
- en: Figure 3-5\. Coverage of the Mueller report
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5. 穆勒报告的报道
- en: Russia Today’s coverage of the Mueller report was an extreme outlier in terms
    of how many channels were recommending it. This suggests the possibility that
    Russia Today, a state-owned Russia media outlet, has been successful in gaming
    YouTube’s recommendation algorithm. Unfortunately, the lack of transparency of
    systems like this makes it hard to uncover the kinds of problems that we’re discussing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 俄罗斯今日电视台对穆勒报告的报道在推荐频道中是一个极端的离群值。这表明俄罗斯今日电视台，一个俄罗斯国有媒体机构，成功地操纵了YouTube的推荐算法。不幸的是，这种系统缺乏透明度，使我们很难揭示我们正在讨论的问题。
- en: 'One of our reviewers for this book, Aurélien Géron, led YouTube’s video classification
    team from 2013 to 2016 (well before the events discussed here). He pointed out
    that it’s not just feedback loops involving humans that are a problem. There can
    also be feedback loops without humans! He told us about an example from YouTube:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的一位审阅者Aurélien Géron，曾在2013年至2016年间领导YouTube的视频分类团队（远在这里讨论的事件之前）。他指出，涉及人类的反馈循环不仅是一个问题。也可能存在没有人类参与的反馈循环！他向我们讲述了YouTube的一个例子：
- en: One important signal to classify the main topic of a video is the channel it
    comes from. For example, a video uploaded to a cooking channel is very likely
    to be a cooking video. But how do we know what topic a channel is about? Well…in
    part by looking at the topics of the videos it contains! Do you see the loop?
    For example, many videos have a description which indicates what camera was used
    to shoot the video. As a result, some of these videos might get classified as
    videos about “photography.” If a channel has such a misclassified video, it might
    be classified as a “photography” channel, making it even more likely for future
    videos on this channel to be wrongly classified as “photography.” This could even
    lead to runaway virus-like classifications! One way to break this feedback loop
    is to classify videos with and without the channel signal. Then when classifying
    the channels, you can only use the classes obtained without the channel signal.
    This way, the feedback loop is broken.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对视频的主题进行分类的一个重要信号是视频的来源频道。例如，上传到烹饪频道的视频很可能是烹饪视频。但我们如何知道一个频道的主题是什么？嗯...部分是通过查看它包含的视频的主题！你看到循环了吗？例如，许多视频有描述，指示拍摄视频所使用的相机。因此，一些视频可能被分类为“摄影”视频。如果一个频道有这样一个错误分类的视频，它可能被分类为“摄影”频道，使得未来在该频道上的视频更有可能被错误分类为“摄影”。这甚至可能导致失控的病毒般的分类！打破这种反馈循环的一种方法是对有和没有频道信号的视频进行分类。然后在对频道进行分类时，只能使用没有频道信号获得的类别。这样，反馈循环就被打破了。
- en: There are positive examples of people and organizations attempting to combat
    these problems. Evan Estola, lead machine learning engineer at Meetup, [discussed
    the example](https://oreil.ly/QfHzT) of men expressing more interest than women
    in tech meetups. Taking gender into account could therefore cause Meetup’s algorithm
    to recommend fewer tech meetups to women, and as a result, fewer women would find
    out about and attend tech meetups, which could cause the algorithm to suggest
    even fewer tech meetups to women, and so on in a self-reinforcing feedback loop.
    So, Evan and his team made the ethical decision for their recommendation algorithm
    to not create such a feedback loop, by explicitly not using gender for that part
    of their model. It is encouraging to see a company not just unthinkingly optimize
    a metric, but consider its impact. According to Evan, “You need to decide which
    feature not to use in your algorithm… the most optimal algorithm is perhaps not
    the best one to launch into production.”
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有人和组织试图解决这些问题的积极例子。Meetup的首席机器学习工程师Evan Estola讨论了男性对科技见面会表现出比女性更感兴趣的例子。因此，考虑性别可能会导致Meetup的算法向女性推荐更少的科技见面会，结果导致更少的女性了解并参加科技见面会，这可能导致算法向女性推荐更少的科技见面会，如此循环反馈。因此，Evan和他的团队做出了道德决定，让他们的推荐算法不会创建这样的反馈循环，明确不在模型的那部分使用性别。看到一家公司不仅仅是盲目地优化指标，而是考虑其影响是令人鼓舞的。根据Evan的说法，“你需要决定在算法中不使用哪个特征...最优算法也许不是最适合投入生产的算法。”
- en: 'While Meetup chose to avoid such an outcome, Facebook provides an example of
    allowing a runaway feedback loop to run wild. Like YouTube, it tends to radicalize
    users interested in one conspiracy theory by introducing them to more. As Renee
    DiResta, a researcher on proliferation of disinformation, [writes](https://oreil.ly/svgOt):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Meetup选择避免这种结果，但Facebook提供了一个允许失控的反馈循环肆虐的例子。与YouTube类似，它倾向于通过向用户介绍更多阴谋论来激化用户。正如虚构信息传播研究员Renee
    DiResta所写的那样：
- en: Once people join a single conspiracy-minded [Facebook] group, they are algorithmically
    routed to a plethora of others. Join an anti-vaccine group, and your suggestions
    will include anti-GMO, chemtrail watch, flat Earther (yes, really), and “curing
    cancer naturally” groups. Rather than pulling a user out of the rabbit hole, the
    recommendation engine pushes them further in.
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一旦人们加入一个阴谋论倾向的[Facebook]群组，他们就会被算法路由到其他大量群组。加入反疫苗群组，你的建议将包括反转基因、化学尾迹观察、地平论者（是的，真的）和“自然治愈癌症”群组。推荐引擎不是将用户拉出兔子洞，而是将他们推得更深。
- en: It is extremely important to keep in mind that this kind of behavior can happen,
    and to either anticipate a feedback loop or take positive action to break it when
    you see the first signs of it in your own projects. Another thing to keep in mind
    is *bias*, which, as we discussed briefly in the previous chapter, can interact
    with feedback loops in very troublesome ways.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的是要记住这种行为可能会发生，并在看到自己项目中出现第一个迹象时，要么预见到一个反馈循环，要么采取积极行动来打破它。另一件要记住的事情是*偏见*，正如我们在上一章中简要讨论的那样，它可能与反馈循环以非常麻烦的方式相互作用。
- en: Bias
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏见
- en: Discussions of bias online tend to get pretty confusing pretty fast. The word
    “bias” means so many different things. Statisticians often think when data ethicists
    are talking about bias that they’re talking about the statistical definition of
    the term bias—but they’re not. And they’re certainly not talking about the biases
    that appear in the weights and biases that are the parameters of your model!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在线讨论偏见往往会变得非常混乱。 “偏见”一词有很多不同的含义。统计学家经常认为，当数据伦理学家谈论偏见时，他们在谈论统计学术语“偏见”，但他们并没有。他们当然也没有在谈论出现在模型参数中的权重和偏见中的偏见！
- en: What they’re talking about is the social science concept of bias. In [“A Framework
    for Understanding Unintended Consequences of Machine Learning”](https://oreil.ly/aF33V)
    MIT’s Harini Suresh and John Guttag describe six types of bias in machine learning,
    summarized in [Figure 3-6](#bias).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 他们所谈论的是社会科学概念中的偏见。在[“理解机器学习意外后果的框架”](https://oreil.ly/aF33V)中，麻省理工学院的Harini
    Suresh和John Guttag描述了机器学习中的六种偏见类型，总结在[图3-6](#bias)中。
- en: '![A diagram showing all sources where bias can appear in machine learning](Images/dlcf_0306.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![显示机器学习中偏见可能出现的所有来源的图表](Images/dlcf_0306.png)'
- en: Figure 3-6\. Bias in machine learning can come from multiple sources (courtesy
    of Harini Suresh and John V. Guttag)
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6。机器学习中的偏见可能来自多个来源（由Harini Suresh和John V. Guttag提供）
- en: We’ll discuss four of these types of bias, those that we’ve found most helpful
    in our own work (see the paper for details on the others).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论其中四种偏见类型，这些是我们在自己的工作中发现最有帮助的（有关其他类型的详细信息，请参阅论文）。
- en: Historical bias
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 历史偏见
- en: '*Historical bias* comes from the fact that people are biased, processes are
    biased, and society is biased. Suresh and Guttag say: “Historical bias is a fundamental,
    structural issue with the first step of the data generation process and can exist
    even given perfect sampling and feature selection.”'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*历史偏见*源于人们的偏见，过程的偏见，以及社会的偏见。苏雷什和古塔格说：“历史偏见是数据生成过程的第一步存在的基本结构性问题，即使进行了完美的抽样和特征选择，它也可能存在。”'
- en: 'For instance, here are a few examples of historical *race bias* in the US,
    from the *New York Times* article [“Racial Bias, Even When We Have Good Intentions”](https://oreil.ly/cBQop)
    by the University of Chicago’s Sendhil Mullainathan:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下是美国历史上*种族偏见*的几个例子，来自芝加哥大学Sendhil Mullainathan的《纽约时报》文章[“种族偏见，即使我们有良好意图”](https://oreil.ly/cBQop)：
- en: When doctors were shown identical files, they were much less likely to recommend
    cardiac catheterization (a helpful procedure) to Black patients.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当医生看到相同的档案时，他们更不可能向黑人患者推荐心脏导管化（一种有益的程序）。
- en: When bargaining for a used car, Black people were offered initial prices $700
    higher and received far smaller concessions.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在讨价还价购买二手车时，黑人被要求支付的初始价格高出700美元，并获得了远低于预期的让步。
- en: Responding to apartment rental ads on Craigslist with a Black name elicited
    fewer responses than with a white name.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Craigslist上回应带有黑人姓名的公寓出租广告比带有白人姓名的回应要少。
- en: An all-white jury was 16 percentage points more likely to convict a Black defendant
    than a white one, but when a jury had one Black member, it convicted both at the
    same rate.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个全白人陪审团比一个黑人被告有16个百分点更有可能定罪，但当陪审团有一个黑人成员时，他们以相同的比率定罪。
- en: The COMPAS algorithm, widely used for sentencing and bail decisions in the US,
    is an example of an important algorithm that, when tested by [ProPublica](https://oreil.ly/1XocO),
    showed clear racial bias in practice ([Figure 3-7](#bail_algorithm)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国用于判决和保释决定的COMPAS算法是一个重要算法的例子，当[ProPublica](https://oreil.ly/1XocO)进行测试时，实际上显示出明显的种族偏见（[图3-7](#bail_algorithm)）。
- en: '![Table showing the COMPAS algorithm is more likely to give bail to white people,
    even if they re-offend more](Images/dlcf_0307.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![表格显示，即使重新犯罪，COMPAS算法更有可能给白人保释](Images/dlcf_0307.png)'
- en: Figure 3-7\. Results of the COMPAS algorithm
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7。COMPAS算法的结果
- en: 'Any dataset involving humans can have this kind of bias: medical data, sales
    data, housing data, political data, and so on. Because underlying bias is so pervasive,
    bias in datasets is very pervasive. Racial bias even turns up in computer vision,
    as shown in the example of autocategorized photos shared on Twitter by a Google
    Photos user shown in [Figure 3-8](#google_photos).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 任何涉及人类的数据集都可能存在这种偏见：医疗数据、销售数据、住房数据、政治数据等等。由于潜在偏见是如此普遍，数据集中的偏见也非常普遍。甚至在计算机视觉中也会出现种族偏见，正如Twitter上一位Google照片用户分享的自动分类照片的例子所示，见[图3-8](#google_photos)。
- en: '![Screenshot of the use of Google photos labeling a black user and her friend
    as gorillas](Images/dlcf_0308.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Google照片使用黑人用户和她朋友的照片标记为大猩猩的屏幕截图](Images/dlcf_0308.png)'
- en: Figure 3-8\. One of these labels is very wrong…
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8。其中一个标签是非常错误的...
- en: 'Yes, that is showing what you think it is: Google Photos classified a Black
    user’s photo with their friend as “gorillas”! This algorithmic misstep got a lot
    of attention in the media. “We’re appalled and genuinely sorry that this happened,”
    a company spokeswoman said. “There is still clearly a lot of work to do with automatic
    image labeling, and we’re looking at how we can prevent these types of mistakes
    from happening in the future.”'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这正是你认为的：Google照片将一位黑人用户的照片与她的朋友一起分类为“大猩猩”！这种算法错误引起了媒体的广泛关注。一位公司女发言人表示：“我们对此感到震惊和真诚地抱歉。自动图像标记仍然存在许多问题，我们正在研究如何防止将来发生这类错误。”
- en: Unfortunately, fixing problems in machine learning systems when the input data
    has problems is hard. Google’s first attempt didn’t inspire confidence, as coverage
    by *The Guardian* suggested ([Figure 3-9](#gorilla-ban)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当输入数据存在问题时，修复机器学习系统中的问题是困难的。谷歌的第一次尝试并没有激发信心，正如*卫报*的报道所建议的那样（[图3-9](#gorilla-ban)）。
- en: '![Pictures of headlines from the Guardian when Google removed gorillas and
    other monkeys from the possible labels of its algorithm](Images/dlcf_0309.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![来自卫报的标题图片，当谷歌从其算法的可能标签中删除大猩猩和其他猴子时](Images/dlcf_0309.png)'
- en: Figure 3-9\. Google’s first response to the problem
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9。谷歌对问题的第一次回应
- en: These kinds of problems are certainly not limited to Google. MIT researchers
    studied the most popular online computer vision APIs to see how accurate they
    were. But they didn’t just calculate a single accuracy number—instead, they looked
    at the accuracy across four groups, as illustrated in [Figure 3-10](#face_recognition).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Table showing how various facial recognition systems perform way worse on
    darker shades of skin and females](Images/dlcf_0310.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Error rate per gender and race for various facial recognition
    systems
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: IBM’s system, for instance, had a 34.7% error rate for darker females, versus
    0.3% for lighter males—over 100 times more errors! Some people incorrectly reacted
    to these experiments by claiming that the difference was simply because darker
    skin is harder for computers to recognize. However, what happened was that, after
    the negative publicity that this result created, all of the companies in question
    dramatically improved their models for darker skin, such that one year later,
    they were nearly as good as for lighter skin. So what this showed is that the
    developers failed to utilize datasets containing enough darker faces, or test
    their product with darker faces.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the MIT researchers, Joy Buolamwini, warned: “We have entered the age
    of automation overconfident yet underprepared. If we fail to make ethical and
    inclusive artificial intelligence, we risk losing gains made in civil rights and
    gender equity under the guise of machine neutrality.”'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Part of the issue appears to be a systematic imbalance in the makeup of popular
    datasets used for training models. The abstract of the paper [“No Classification
    Without Representation: Assessing Geodiversity Issues in Open Data Sets for the
    Developing World”](https://oreil.ly/VqtOA) by Shreya Shankar et al. states, “We
    analyze two large, publicly available image data sets to assess geo-diversity
    and find that these data sets appear to exhibit an observable amerocentric and
    eurocentric representation bias. Further, we analyze classifiers trained on these
    data sets to assess the impact of these training distributions and find strong
    differences in the relative performance on images from different locales.” [Figure 3-11](#image_provenance)
    shows one of the charts from the paper, showing the geographic makeup of what
    were at the time (and still are, as this book is being written) the two most important
    image datasets for training models.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphs showing how the vast majority of images in popular training datasets
    come from the US or Western Europe](Images/dlcf_0311.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Image provenance in popular training sets
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The vast majority of the images are from the US and other Western countries,
    leading to models trained on ImageNet performing worse on scenes from other countries
    and cultures. For instance, research found that such models are worse at identifying
    household items (such as soap, spices, sofas, or beds) from lower-income countries.
    [Figure 3-12](#object_detect) shows an image from the paper [“Does Object Recognition
    Work for Everyone?”](https://oreil.ly/BkFjL) by Terrance DeVries et al. of Facebook
    AI Research that illustrates this point.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure showing an object detection algorithm performing better on western
    products](Images/dlcf_0312.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Object detection in action
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example, we can see that the lower-income soap example is a very long
    way away from being accurate, with every commercial image recognition service
    predicting “food” as the most likely answer!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: As we will discuss shortly, in addition, the vast majority of AI researchers
    and developers are young white men. Most projects that we have seen do most user
    testing using friends and families of the immediate product development group.
    Given this, the kinds of problems we just discussed should not be surprising.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar historical bias is found in the texts used as data for natural language
    processing models. This crops up in downstream machine learning tasks in many
    ways. For instance, it [was widely reported](https://oreil.ly/Vt_vT) that until
    last year, Google Translate showed systematic bias in how it translated the Turkish
    gender-neutral pronoun “o” into English: when applied to jobs that are often associated
    with males, it used “he,” and when applied to jobs that are often associated with
    females, it used “she” ([Figure 3-13](#turkish_gender)).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的历史偏见也存在于用作自然语言处理模型数据的文本中。这会在许多下游机器学习任务中出现。例如，据[广泛报道](https://oreil.ly/Vt_vT)，直到去年，Google翻译在将土耳其中性代词“o”翻译成英语时显示了系统性偏见：当应用于通常与男性相关联的工作时，它使用“he”，而当应用于通常与女性相关联的工作时，它使用“she”（[图3-13](#turkish_gender)）。
- en: '![Figure showing gender bias in data ets used to train language models showing
    up in translations](Images/dlcf_0313.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![显示语言模型训练中数据集中性别偏见在翻译中的体现的图表](Images/dlcf_0313.png)'
- en: Figure 3-13\. Gender bias in text datasets
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13。文本数据集中的性别偏见
- en: We also see this kind of bias in online advertisements. For instance, a [study](https://oreil.ly/UGxuh)
    in 2019 by Muhammad Ali et al. found that even when the person placing the ad
    does not intentionally discriminate, Facebook will show ads to very different
    audiences based on race and gender. Housing ads with the same text but picturing
    either a white or a Black family were shown to racially different audiences.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也在在线广告中看到这种偏见。例如，2019年穆罕默德·阿里等人的一项[研究](https://oreil.ly/UGxuh)发现，即使放置广告的人没有故意歧视，Facebook也会根据种族和性别向非常不同的受众展示广告。展示了同样文本但图片分别是白人家庭或黑人家庭的房屋广告被展示给了种族不同的受众。
- en: Measurement bias
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量偏见
- en: 'In [“Does Machine Learning Automate Moral Hazard and Error”](https://oreil.ly/79Qtn)
    in *American Economic Review*, Sendhil Mullainathan and Ziad Obermeyer look at
    a model that tries to answer this question: using historical electronic health
    record (EHR) data, what factors are most predictive of stroke? These are the top
    predictors from the model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在《“机器学习是否自动化了道德风险和错误”》一文中，Sendhil Mullainathan和Ziad Obermeyer研究了一个模型，试图回答这个问题：使用历史电子健康记录（EHR）数据，哪些因素最能预测中风？这是该模型的前几个预测因素：
- en: Prior stroke
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前的中风
- en: Cardiovascular disease
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 心血管疾病
- en: Accidental injury
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意外伤害
- en: Benign breast lump
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良性乳腺肿块
- en: Colonoscopy
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结肠镜检查
- en: Sinusitis
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鼻窦炎
- en: However, only the top two have anything to do with a stroke! Based on what we’ve
    studied so far, you can probably guess why. We haven’t really measured *stroke*,
    which occurs when a region of the brain is denied oxygen due to an interruption
    in the blood supply. What we’ve measured is who had symptoms, went to a doctor,
    got the appropriate tests, *and* received a diagnosis of stroke. Actually having
    a stroke is not the only thing correlated with this complete list—it’s also correlated
    with being the kind of person who goes to the doctor (which is influenced by who
    has access to healthcare, can afford their co-pay, doesn’t experience racial or
    gender-based medical discrimination, and more)! If you are likely to go to the
    doctor for an *accidental injury*, you are likely to also go the doctor when you
    are having a stroke.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，只有前两个与中风有关！根据我们迄今所学，你可能已经猜到原因。我们实际上并没有测量*中风*，中风是由于脑部某个区域由于血液供应中断而被剥夺氧气而发生的。我们测量的是谁有症状，去看医生，接受了适当的检查，*并且*被诊断出中风。实际上患中风不仅与这个完整列表相关联，还与那些会去看医生的人相关联（这受到谁能获得医疗保健、能否负担得起自付款、是否经历种族或性别歧视等影响）！如果你在发生*意外伤害*时可能会去看医生，那么在中风时你也可能会去看医生。
- en: This is an example of *measurement bias*. It occurs when our models make mistakes
    because we are measuring the wrong thing, or measuring it in the wrong way, or
    incorporating that measurement into the model inappropriately.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*测量偏见*的一个例子。当我们的模型因为测量错误、以错误方式测量或不恰当地将该测量纳入模型时，就会发生这种偏见。
- en: Aggregation bias
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合偏见
- en: '*Aggregation bias* occurs when models do not aggregate data in a way that incorporates
    all of the appropriate factors, or when a model does not include the necessary
    interaction terms, nonlinearities, or so forth. This can particularly occur in
    medical settings. For instance, the way diabetes is treated is often based on
    simple univariate statistics and studies involving small groups of heterogeneous
    people. Analysis of results is often done in a way that does not take into account
    different ethnicities or genders. However, it turns out that diabetes patients
    have [different complications across ethnicities](https://oreil.ly/gNS39), and
    HbA1c levels (widely used to diagnose and monitor diabetes) [differ in complex
    ways across ethnicities and genders](https://oreil.ly/nR4fx). This can result
    in people being misdiagnosed or incorrectly treated because medical decisions
    are based on a model that does not include these important variables and interactions.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*聚合偏见*发生在模型未以包含所有适当因素的方式聚合数据，或者模型未包含必要的交互项、非线性等情况下。这在医疗环境中尤其常见。例如，糖尿病的治疗通常基于简单的单变量统计和涉及小组异质人群的研究。结果分析通常未考虑不同种族或性别。然而，事实证明糖尿病患者在[不同种族之间有不同的并发症](https://oreil.ly/gNS39)，HbA1c水平（用于诊断和监测糖尿病的广泛指标）[在不同种族和性别之间以复杂方式不同](https://oreil.ly/nR4fx)。这可能导致人们被误诊或错误治疗，因为医疗决策基于不包含这些重要变量和交互作用的模型。'
- en: Representation bias
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表征偏见
- en: 'The abstract of the paper [“Bias in Bios: A Case Study of Semantic Representation
    Bias in a High-Stakes Setting”](https://oreil.ly/0iowq) by Maria De-Arteaga et
    al. notes that there is gender imbalance in occupations (e.g., females are more
    likely to be nurses, and males are more likely to be pastors), and says that “differences
    in true positive rates between genders are correlated with existing gender imbalances
    in occupations, which may compound these imbalances.”'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'Maria De-Arteaga等人的论文[“Bias in Bios: A Case Study of Semantic Representation
    Bias in a High-Stakes Setting”](https://oreil.ly/0iowq)的摘要指出，职业中存在性别不平衡（例如，女性更有可能成为护士，男性更有可能成为牧师），并表示“性别之间的真正阳性率差异与职业中现有的性别不平衡相关，这可能会加剧这些不平衡。”'
- en: In other words, the researchers noticed that models predicting occupation did
    not only *reflect* the actual gender imbalance in the underlying population, but
    *amplified* it! This type of *representation bias* is quite common, particularly
    for simple models. When there is a clear, easy-to-see underlying relationship,
    a simple model will often assume that this relationship holds all the time. As
    [Figure 3-14](#representation_bias) from the paper shows, for occupations that
    had a higher percentage of females, the model tended to overestimate the prevalence
    of that occupation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，研究人员注意到，预测职业的模型不仅*反映*了潜在人口中的实际性别不平衡，而且*放大*了它！这种*表征偏差*是相当常见的，特别是对于简单模型。当存在明显、容易看到的基本关系时，简单模型通常会假定这种关系始终存在。正如论文中的[图3-14](#representation_bias)所示，对于女性比例较高的职业，模型往往会高估该职业的普遍性。
- en: '![Graph showing how model predictions overamplify existing bias](Images/dlcf_0314.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![显示模型预测如何过度放大现有偏见的图表](Images/dlcf_0314.png)'
- en: Figure 3-14\. Model error in predicting occupation plotted against percentage
    of women in said occupation
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-14。预测职业中的模型误差与该职业中女性比例的关系
- en: For example, in the training dataset 14.6% of surgeons were women, yet in the
    model predictions only 11.6% of the true positives were women. The model is thus
    amplifying the bias existing in the training set.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在训练数据集中，14.6%的外科医生是女性，然而在模型预测中，真正阳性中只有11.6%是女性。因此，模型放大了训练集中存在的偏见。
- en: Now that we’ve seen that those biases exist, what can we do to mitigate them?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到这些偏见存在，我们可以采取什么措施来减轻它们呢？
- en: Addressing different types of bias
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决不同类型的偏见
- en: Different types of bias require different approaches for mitigation. While gathering
    a more diverse dataset can address representation bias, this would not help with
    historical bias or measurement bias. All datasets contain bias. There is no such
    thing as a completely debiased dataset. Many researchers in the field have been
    converging on a set of proposals to enable better documentation of the decisions,
    context, and specifics about how and why a particular dataset was created, what
    scenarios it is appropriate to use in, and what the limitations are. This way,
    those using a particular dataset will not be caught off guard by its biases and
    limitations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的偏见需要不同的缓解方法。虽然收集更多样化的数据集可以解决表征偏见，但这对历史偏见或测量偏见无济于事。所有数据集都包含偏见。没有完全无偏的数据集。该领域的许多研究人员一直在提出一系列建议，以便更好地记录决策、背景和有关特定数据集创建方式的细节，以及为什么在什么情况下使用它，以及其局限性。这样，使用特定数据集的人不会被其偏见和局限性所困扰。
- en: 'We often hear the question, “Humans are biased, so does algorithmic bias even
    matter?” This comes up so often, there must be some reasoning that makes sense
    to the people who ask it, but it doesn’t seem very logically sound to us! Independently
    of whether this is logically sound, it’s important to realize that algorithms
    (particularly machine learning algorithms!) and people are different. Consider
    these points about machine learning algorithms:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常听到这样的问题，“人类有偏见，那么算法偏见真的重要吗？”这个问题经常被提出，肯定有一些让提问者认为有道理的理由，但对我们来说似乎并不太合乎逻辑！独立于这是否合乎逻辑，重要的是要意识到算法（特别是机器学习算法！）和人类是不同的。考虑一下关于机器学习算法的这些观点：
- en: Machine learning can create feedback loops
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以创建反馈循环
- en: Small amounts of bias can rapidly increase exponentially because of feedback
    loops.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 少量偏见可能会因为反馈循环而迅速呈指数增长。
- en: Machine learning can amplify bias
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可能会放大偏见
- en: Human bias can lead to larger amounts of machine learning bias.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 人类偏见可能导致更多的机器学习偏见。
- en: Algorithms and humans are used differently
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 算法和人类的使用方式不同
- en: Human decision makers and algorithmic decision makers are not used in a plug-and-play
    interchangeable way in practice. These examples are given in the list on the next
    page.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，人类决策者和算法决策者并不是以插拔方式互换使用的。这些例子列在下一页的清单中。
- en: Technology is power
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 技术就是力量
- en: And with that comes responsibility.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 随之而来的是责任。
- en: 'As the Arkansas healthcare example showed, machine learning is often implemented
    in practice not because it leads to better outcomes, but because it is cheaper
    and more efficient. Cathy O’Neill, in her book *Weapons of Math Destruction* (Crown),
    described a pattern in which the privileged are processed by people, whereas the
    poor are processed by algorithms. This is just one of a number of ways that algorithms
    are used differently than human decision makers. Others include the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如阿肯色州医疗保健的例子所示，机器学习通常在实践中实施并不是因为它能带来更好的结果，而是因为它更便宜和更高效。凯西·奥尼尔在她的书《数学毁灭的武器》（Crown）中描述了一个模式，即特权人士由人处理，而穷人由算法处理。这只是算法与人类决策者使用方式的许多方式之一。其他方式包括以下内容：
- en: People are more likely to assume algorithms are objective or error-free (even
    if they’re given the option of a human override).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人们更有可能认为算法是客观或无误差的（即使他们有人类覆盖的选项）。
- en: Algorithms are more likely to be implemented with no appeals process in place.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法更有可能在没有上诉程序的情况下实施。
- en: Algorithms are often used at scale.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法通常以规模使用。
- en: Algorithmic systems are cheap.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法系统成本低廉。
- en: Even in the absence of bias, algorithms (and deep learning especially, since
    it is such an effective and scalable algorithm) can lead to negative societal
    problems, such as when used for *disinformation*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在没有偏见的情况下，算法（尤其是深度学习，因为它是一种如此有效和可扩展的算法）也可能导致负面社会问题，比如当用于*虚假信息*时。
- en: Disinformation
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚假信息
- en: '*Disinformation* has a history stretching back hundreds or even thousands of
    years. It is not necessarily about getting someone to believe something false,
    but rather often used to sow disharmony and uncertainty, and to get people to
    give up on seeking the truth. Receiving conflicting accounts can lead people to
    assume that they can never know whom or what to trust.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*虚假信息*的历史可以追溯到数百甚至数千年前。它不一定是让某人相信错误的事情，而是经常用来播撒不和谐和不确定性，并让人们放弃寻求真相。收到矛盾的说法可能会导致人们认为他们永远无法知道该信任谁或什么。'
- en: Some people think disinformation is primarily about false information or *fake
    news*, but in reality, disinformation can often contain seeds of truth, or half-truths
    taken out of context. Ladislav Bittman was an intelligence officer in the USSR
    who later defected to the US and wrote some books in the 1970s and 1980s on the
    role of disinformation in Soviet propaganda operations. In *The KGB and Soviet
    Disinformation* (Pergamon), he wrote “Most campaigns are a carefully designed
    mixture of facts, half-truths, exaggerations, and deliberate lies.”
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为虚假信息主要是关于错误信息或*假新闻*，但实际上，虚假信息经常包含真相的种子，或者是脱离上下文的半真相。拉迪斯拉夫·比特曼是苏联的一名情报官员，后来叛逃到美国，并在20世纪70年代和80年代写了一些关于苏联宣传行动中虚假信息角色的书籍。在《克格勃和苏联虚假信息》（Pergamon）中，他写道“大多数活动都是精心设计的事实、半真相、夸大和故意谎言的混合物。”
- en: 'In the US, this has hit close to home in recent years, with the FBI detailing
    a massive disinformation campaign linked to Russia in the 2016 election. Understanding
    the disinformation that was used in this campaign is very educational. For instance,
    the FBI found that the Russian disinformation campaign often organized two separate
    fake “grass roots” protests, one for each side of an issue, and got them to protest
    at the same time! The [Houston Chronicle](https://oreil.ly/VyCkL) reported on
    one of these odd events ([Figure 3-15](#teax)):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，近年来，FBI详细描述了与2016年选举中的俄罗斯有关的大规模虚假信息活动。了解在这次活动中使用的虚假信息非常有教育意义。例如，FBI发现俄罗斯的虚假信息活动经常组织两个独立的假“草根”抗议活动，一个支持某一方面，另一个支持另一方面，并让他们同时抗议！[休斯顿纪事报](https://oreil.ly/VyCkL)报道了其中一个奇怪事件（[图3-15](#teax)）：
- en: 'A group that called itself the “Heart of Texas” had organized it on social
    media—a protest, they said, against the “Islamization” of Texas. On one side of
    Travis Street, I found about 10 protesters. On the other side, I found around
    50 counterprotesters. But I couldn’t find the rally organizers. No “Heart of Texas.”
    I thought that was odd, and mentioned it in the article: What kind of group is
    a no-show at its own event? Now I know why. Apparently, the rally’s organizers
    were in Saint Petersburg, Russia, at the time. “Heart of Texas” is one of the
    internet troll groups cited in Special Prosecutor Robert Mueller’s recent indictment
    of Russians attempting to tamper with the US presidential election.'
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个自称为“德克萨斯之心”的团体在社交媒体上组织了一场抗议活动，他们声称这是反对“德克萨斯伊斯兰化”的。在特拉维斯街的一边，我发现大约有10名抗议者。在另一边，我发现大约有50名反对抗议者。但我找不到集会的组织者。没有“德克萨斯之心”。我觉得这很奇怪，并在文章中提到：一个团体在自己的活动中缺席是什么样的团体？现在我知道为什么了。显然，集会的组织者当时在俄罗斯的圣彼得堡。“德克萨斯之心”是特别检察官罗伯特·穆勒最近指控试图干预美国总统选举的俄罗斯人中引用的一个互联网喷子团体。
- en: '![Screenshot of an event organized by the group Heart of Texas](Images/dlcf_0315.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![德克萨斯之心组织的活动截图](Images/dlcf_0315.png)'
- en: Figure 3-15\. Event organized by the group Heart of Texas
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15。由德克萨斯之心组织的活动
- en: Disinformation often involves coordinated campaigns of inauthentic behavior.
    For instance, fraudulent accounts may try to make it seem like many people hold
    a particular viewpoint. While most of us like to think of ourselves as independent-minded,
    in reality we evolved to be influenced by others in our in-group, and in opposition
    to those in our out-group. Online discussions can influence our viewpoints, or
    alter the range of what we consider acceptable viewpoints. Humans are social animals,
    and as social animals, we are extremely influenced by the people around us. Increasingly,
    radicalization occurs in online environments; so influence is coming from people
    in the virtual space of online forums and social networks.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 虚假信息通常涉及协调的不真实行为活动。例如，欺诈账户可能试图让人们认为许多人持有特定观点。虽然大多数人喜欢认为自己是独立思考的，但实际上我们进化为受到内部群体的影响，并与外部群体对立。在线讨论可能会影响我们的观点，或改变我们认为可接受观点的范围。人类是社会动物，作为社会动物，我们受周围人的影响极大。越来越多的极端化发生在在线环境中；因此影响来自虚拟空间中的在线论坛和社交网络中的人们。
- en: Disinformation through autogenerated text is a particularly significant issue,
    due to the greatly increased capability provided by deep learning. We discuss
    this issue in depth when we delve into creating language models in [Chapter 10](ch10.xhtml#chapter_nlp).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动生成的文本进行虚假信息传播是一个特别重要的问题，这是由于深度学习提供的大大增强的能力。当我们深入研究创建语言模型时，我们会深入讨论这个问题[第10章](ch10.xhtml#chapter_nlp)。
- en: 'One proposed approach is to develop some form of digital signature, to implement
    it in a seamless way, and to create norms that we should trust only content that
    has been verified. The head of the Allen Institute on AI, Oren Etzioni, wrote
    such a proposal in an article titled [“How Will We Prevent AI-Based Forgery?”](https://oreil.ly/8z7wm):
    “AI is poised to make high-fidelity forgery inexpensive and automated, leading
    to potentially disastrous consequences for democracy, security, and society. The
    specter of AI forgery means that we need to act to make digital signatures de
    rigueur as a means of authentication of digital content.”'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一种提出的方法是开发某种形式的数字签名，以无缝方式实施它，并创建我们应该信任仅经过验证的内容的规范。艾伦人工智能研究所的负责人奥伦·艾齐奥尼在一篇题为[“我们将如何防止基于人工智能的伪造？”](https://oreil.ly/8z7wm)的文章中写道：“人工智能正准备使高保真伪造变得廉价和自动化，可能会对民主、安全和社会造成灾难性后果。人工智能伪造的幽灵意味着我们需要采取行动，使数字签名成为验证数字内容的手段。”
- en: While we can’t hope to discuss all the ethical issues that deep learning, and
    algorithms more generally, bring up, hopefully this brief introduction has been
    a useful starting point you can build on. We’ll now move on to the questions of
    how to identify ethical issues and what to do about them.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们无法讨论深度学习和算法带来的所有伦理问题，但希望这个简短的介绍可以成为您的有用起点。现在我们将继续讨论如何识别伦理问题以及如何处理它们。
- en: Identifying and Addressing Ethical Issues
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别和解决伦理问题
- en: Mistakes happen. Finding out about them, and dealing with them, needs to be
    part of the design of any system that includes machine learning (and many other
    systems too). The issues raised within data ethics are often complex and interdisciplinary,
    but it is crucial that we work to address them.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 错误是难免的。了解并处理错误需要成为包括机器学习在内的任何系统设计的一部分（还有许多其他系统）。数据伦理中提出的问题通常是复杂且跨学科的，但至关重要的是我们努力解决这些问题。
- en: 'So what can we do? This is a big topic, but here are a few steps toward addressing
    ethical issues:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们能做什么？这是一个重要的话题，但以下是一些解决伦理问题的步骤：
- en: Analyze a project you are working on.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析你正在进行的项目。
- en: Implement processes at your company to find and address ethical risks.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的公司实施流程以发现和解决伦理风险。
- en: Support good policy.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持良好的政策。
- en: Increase diversity.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加多样性。
- en: Let’s walk through each step, starting with analyzing a project you are working
    on.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步进行，从分析你正在进行的项目开始。
- en: Analyze a Project You Are Working On
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析你正在进行的项目
- en: 'It’s easy to miss important issues when considering ethical implications of
    your work. One thing that helps enormously is simply asking the right questions.
    Rachel Thomas recommends considering the following questions throughout the development
    of a data project:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑工作的伦理影响时很容易忽略重要问题。一个极大的帮助是简单地提出正确的问题。Rachel Thomas建议在数据项目的开发过程中考虑以下问题：
- en: Should we even be doing this?
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们甚至应该这样做吗？
- en: What bias is in the data?
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中存在什么偏见？
- en: Can the code and data be audited?
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码和数据可以进行审计吗？
- en: What are the error rates for different subgroups?
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同子群体的错误率是多少？
- en: What is the accuracy of a simple rule-based alternative?
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于简单规则的替代方案的准确性如何？
- en: What processes are in place to handle appeals or mistakes?
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哪些处理申诉或错误的流程？
- en: How diverse is the team that built it?
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建它的团队有多少多样性？
- en: These questions may be able to help you identify outstanding issues, and possible
    alternatives that are easier to understand and control. In addition to asking
    the right questions, it’s also important to consider practices and processes to
    implement.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题可能有助于您识别未解决的问题，以及更容易理解和控制的可能替代方案。除了提出正确的问题外，考虑实施的实践和流程也很重要。
- en: One thing to consider at this stage is what data you are collecting and storing.
    Data often ends up being used for different purposes the original intent. For
    instance, IBM began selling to Nazi Germany well before the Holocaust, including
    helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective
    at identifying far more Jewish people than had previously been recognized in Germany.
    Similarly, US census data was used to round up Japanese-Americans (who were US
    citizens) for internment during World War II. It is important to recognize how
    data and images collected can be weaponized later. Columbia professor [Tim Wu
    wrote](https://oreil.ly/6L0QM) “You must assume that any personal data that Facebook
    or Android keeps are data that governments around the world will try to get or
    that thieves will try to steal.”
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段需要考虑的一件事是你正在收集和存储的数据。数据往往最终被用于不同于最初意图的目的。例如，IBM在大屠杀之前就开始向纳粹德国出售产品，包括帮助纳粹德国进行的1933年人口普查，这次普查有效地识别出了比之前在德国被认可的犹太人更多。同样，美国人口普查数据被用来拘留二战期间的日裔美国人（他们是美国公民）。重要的是要认识到收集的数据和图像如何在以后被武器化。哥伦比亚大学教授[蒂姆·吴写道](https://oreil.ly/6L0QM)：“你必须假设Facebook或Android保存的任何个人数据都是世界各国政府将试图获取或盗贼将试图窃取的数据。”
- en: Processes to Implement
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施流程
- en: The Markkula Center has released [An Ethical Toolkit for Engineering/Design
    Practice](https://oreil.ly/vDGGC) that includes concrete practices to implement
    at your company, including regularly scheduled sweeps to proactively search for
    ethical risks (in a manner similar to cybersecurity penetration testing), expanding
    the ethical circle to include the perspectives of a variety of stakeholders, and
    considering the terrible people (how could bad actors abuse, steal, misinterpret,
    hack, destroy, or weaponize what you are building?).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 马库拉中心发布了[工程/设计实践的伦理工具包](https://oreil.ly/vDGGC)，其中包括在您的公司实施的具体实践，包括定期安排的扫描，以主动搜索伦理风险（类似于网络安全渗透测试），扩大伦理圈，包括各种利益相关者的观点，并考虑可怕的人（坏人如何滥用、窃取、误解、黑客、破坏或武器化您正在构建的东西？）。
- en: 'Even if you don’t have a diverse team, you can still try to proactively include
    the perspectives of a wider group, considering questions such as these (provided
    by the Markkula Center):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您没有多样化的团队，您仍然可以尝试主动包括更广泛群体的观点，考虑这些问题（由马库拉中心提供）：
- en: Whose interests, desires, skills, experiences, and values have we simply assumed,
    rather than actually consulted?
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否只是假设了谁/哪些团体和个人的利益、愿望、技能、经验和价值观，而没有实际咨询？
- en: Who are all the stakeholders who will be directly affected by our product? How
    have their interests been protected? How do we know what their interests really are—have
    we asked?
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁将直接受到我们产品影响的所有利益相关者？他们的利益是如何得到保护的？我们如何知道他们的真正利益是什么——我们有没有询问过？
- en: Who/which groups and individuals will be indirectly affected in significant
    ways?
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些团体和个人将受到重大影响而间接受到影响？
- en: Who might use this product that we didn’t expect to use it, or for purposes
    we didn’t initially intend?
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁可能会使用这个产品，而我们没有预料到会使用它，或者出于我们最初没有打算的目的？
- en: Ethical lenses
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 伦理镜头
- en: 'Another useful resource from the Markkula Center is its [Conceptual Frameworks
    in Technology and Engineering Practice](https://oreil.ly/QnRTt). This considers
    how different foundational ethical lenses can help identify concrete issues, and
    lays out the following approaches and key questions:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 马库拉中心的另一个有用资源是其[技术和工程实践中的概念框架](https://oreil.ly/QnRTt)。这考虑了不同基础伦理镜头如何帮助识别具体问题，并列出以下方法和关键问题：
- en: The rights approach
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 权利的观点
- en: Which option best respects the rights of all who have a stake?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个选项最尊重所有利益相关者的权利？
- en: The justice approach
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正义的观点
- en: Which option treats people equally or proportionately?
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个选项平等或成比例地对待人们？
- en: The utilitarian approach
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 功利主义的观点
- en: Which option will produce the most good and do the least harm?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个选项将产生最多的好处并造成最少的伤害？
- en: The common good approach
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 共同利益的观点
- en: Which option best serves the community as a whole, not just some members?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个选项最好地服务于整个社区，而不仅仅是一些成员？
- en: The virtue approach
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 美德的观点
- en: Which option leads me to act as the sort of person I want to be?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个选项会让我表现得像我想成为的那种人？
- en: 'Markkula’s recommendations include a deeper dive into each of these perspectives,
    including looking at a project through the lens of its *consequences*:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 马库拉的建议包括更深入地探讨这些观点，包括通过*后果*的视角来审视一个项目：
- en: Who will be directly affected by this project? Who will be indirectly affected?
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁将直接受到这个项目的影响？谁将间接受到影响？
- en: Will the effects in aggregate likely create more good than harm, and what types of
    good and harm?
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体上，这些影响可能会产生更多的好处还是伤害，以及什么*类型*的好处和伤害？
- en: Are we thinking about all relevant types of harm/benefit (psychological, political,
    environmental, moral, cognitive, emotional, institutional, cultural)?
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否考虑了*所有*相关类型的伤害/好处（心理、政治、环境、道德、认知、情感、制度、文化）？
- en: How might future generations be affected by this project?
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来的后代可能会受到这个项目的影响吗？
- en: Do the risks of harm from this project fall disproportionately on the least
    powerful in society? Will the benefits go disproportionately to the well-off?
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个项目可能会对社会中最弱势的人造成的伤害风险是否不成比例？好处是否会不成比例地给予富裕者？
- en: Have we adequately considered “dual-use” and unintended downstream effects?
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否充分考虑了“双重使用”和意外的下游影响？
- en: 'The alternative lens to this is the *deontological* perspective, which focuses
    on basic concepts of *right* and *wrong*:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种视角是*义务论*的视角，它侧重于*对*和*错*的基本概念：
- en: What rights of others and duties to others must we respect?
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须尊重他人的哪些*权利*和对他人的*义务*？
- en: How might the dignity and autonomy of each stakeholder be impacted by this project?
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个项目可能会如何影响每个利益相关者的尊严和自主权？
- en: What considerations of trust and of justice are relevant to this design/project?
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信任和正义的考虑对这个设计/项目有何影响？
- en: Does this project involve any conflicting moral duties to others, or conflicting
    stakeholder rights? How can we prioritize these?
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个项目是否涉及与他人的冲突道德责任，或者与利益相关者的冲突权利？我们如何能够优先考虑这些？
- en: One of the best ways to help come up with complete and thoughtful answers to
    questions like these is to ensure that the people asking the questions are *diverse*.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助提出完整和周到的答案的最佳方法之一是确保提出问题的人是*多样化*的。
- en: The Power of Diversity
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性的力量
- en: 'Currently, less than 12% of AI researchers are women, according to [a study
    from Element AI](https://oreil.ly/sO09p). The statistics are similarly dire when
    it comes to race and age. When everybody on a team has similar backgrounds, they
    are likely to have similar blind spots around ethical risks. The *Harvard Business
    Review* (HBR) has published a number of studies showing many benefits of diverse
    teams, including the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[Element AI的一项研究](https://oreil.ly/sO09p)，目前不到12%的人工智能研究人员是女性。在种族和年龄方面的统计数据同样令人堪忧。当团队中的每个人背景相似时，他们很可能在道德风险方面有相似的盲点。*哈佛商业评论*（HBR）发表了许多研究，显示了多样化团队的许多好处，包括以下内容：
- en: '[“How Diversity Can Drive Innovation”](https://oreil.ly/WRFSm)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“多样性如何推动创新”](https://oreil.ly/WRFSm)'
- en: '[“Teams Solve Problems Faster When They’re More Cognitively Diverse”](https://oreil.ly/vKy5b)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“当团队的认知多样性更高时，他们解决问题更快”](https://oreil.ly/vKy5b)'
- en: '[“Why Diverse Teams Are Smarter”](https://oreil.ly/SFVBF)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“为什么多样化的团队更聪明”](https://oreil.ly/SFVBF)'
- en: '[“Defend Your Research: What Makes a Team Smarter? More Women”](https://oreil.ly/A1A5n)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“捍卫您的研究：什么使一个团队更聪明？更多的女性”](https://oreil.ly/A1A5n)'
- en: Diversity can lead to problems being identified earlier, and a wider range of
    solutions being considered. For instance, Tracy Chou was an early engineer at
    Quora. She [wrote of her experiences](https://oreil.ly/n7WSn), describing how
    she advocated internally for adding a feature that would allow trolls and other
    bad actors to be blocked. Chou recounts, “I was eager to work on the feature because
    I personally felt antagonized and abused on the site (gender isn’t an unlikely
    reason as to why)…But if I hadn’t had that personal perspective, it’s possible
    that the Quora team wouldn’t have prioritized building a block button so early
    in its existence.” Harassment often drives people from marginalized groups off
    online platforms, so this functionality has been important for maintaining the
    health of Quora’s community.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: A crucial aspect to understand is that women leave the tech industry at over
    twice the rate that men do. According to the [Harvard Business Review](https://oreil.ly/ZIC7t),
    41% of women working in tech leave, compared to 17% of men. An analysis of over
    200 books, whitepapers, and articles found that the reason they leave is that
    “they’re treated unfairly; underpaid, less likely to be fast-tracked than their
    male colleagues, and unable to advance.”
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Studies have confirmed a number of the factors that make it harder for women
    to advance in the workplace. Women receive more vague feedback and personality
    criticism in performance evaluations, whereas men receive actionable advice tied
    to business outcomes (which is more useful). Women frequently experience being excluded
    from more creative and innovative roles, and not receiving high-visibility “stretch”
    assignments that are helpful in getting promoted. One study found that men’s voices
    are perceived as more persuasive, fact-based, and logical than women’s voices,
    even when reading identical scripts.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Receiving mentorship has been statistically shown to help men advance, but not
    women. The reason behind this is that when women receive mentorship, it’s advice
    on how they should change and gain more self-knowledge. When men receive mentorship,
    it’s public endorsement of their authority. Guess which is more useful in getting
    promoted?
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: As long as qualified women keep dropping out of tech, teaching more girls to
    code will not solve the diversity issues plaguing the field. Diversity initiatives
    often end up focusing primarily on white women, even though women of color face
    many additional barriers. In [interviews](https://oreil.ly/t5C6b) with 60 women
    of color who work in STEM research, 100% had experienced discrimination.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The hiring process is particularly broken in tech. One study indicative of
    the disfunction comes from Triplebyte, a company that helps place software engineers
    in companies, conducting a standardized technical interview as part of this process.
    The company has a fascinating dataset: the results of how over 300 engineers did
    on their exam, coupled with the results of how those engineers did during the
    interview process for a variety of companies. The number one finding from [Triplebyte’s
    research](https://oreil.ly/2Wtw4) is that “the types of programmers that each
    company looks for often have little to do with what the company needs or does.
    Rather, they reflect company culture and the backgrounds of the founders.”'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: This is a challenge for those trying to break into the world of deep learning,
    since most companies’ deep learning groups today were founded by academics. These
    groups tend to look for people “like them”—that is, people who can solve complex
    math problems and understand dense jargon. They don’t always know how to spot
    people who are actually good at solving real problems using deep learning.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: This leaves a big opportunity for companies that are ready to look beyond status
    and pedigree, and focus on results!
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Fairness, Accountability, and Transparency
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The professional society for computer scientists, the ACM, runs a data ethics
    conference called the Conference on Fairness, Accountability, and Transparency
    (ACM FAccT), which used to go under the acronym FAT but now uses the less objectionable
    FAccT. Microsoft also has a group focused on Fairness, Accountability, Transparency,
    and Ethics in AI (FATE). In this section, we’ll use the acronym FAccT to refer
    to the concepts of fairness, accountability, and transparency.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'FAccT is a lens some people have used for considering ethical issues. One helpful
    resource for this is the free online book [*Fairness and Machine Learning: Limitations
    and Opportunities*](https://fairmlbook.org) by Solon Barocas et al., which “gives
    a perspective on machine learning that treats fairness as a central concern rather
    than an afterthought.” It also warns, however, that it “is intentionally narrow
    in scope…A narrow framing of machine learning ethics might be tempting to technologists
    and businesses as a way to focus on technical interventions while sidestepping
    deeper questions about power and accountability. We caution against this temptation.”
    Rather than provide an overview of the FAccT approach to ethics (which is better
    done in books such as that one), our focus here will be on the limitations of
    this kind of narrow framing.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'One great way to consider whether an ethical lens is complete is to try to
    come up with an example in which the lens and our own ethical intuitions give
    diverging results. Os Keyes et al. explored this in a graphic way in their paper
    [“A Mulching Proposal: Analysing and Improving an Algorithmic System for Turning
    the Elderly into High-Nutrient Slurry”](https://oreil.ly/_qug9). The paper’s abstract
    says:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The ethical implications of algorithmic systems have been much discussed in
    both HCI and the broader community of those interested in technology design, development,
    and policy. In this paper, we explore the application of one prominent ethical
    framework—Fairness, Accountability, and Transparency—to a proposed algorithm that
    resolves various societal issues around food security and population aging. Using
    various standardised forms of algorithmic audit and evaluation, we drastically
    increase the algorithm’s adherence to the FAT framework, resulting in a more ethical
    and beneficent system. We discuss how this might serve as a guide to other researchers
    or practitioners looking to ensure better ethical outcomes from algorithmic systems
    in their line of work.
  id: totrans-247
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this paper, the rather controversial proposal (“Turning the Elderly into
    High-Nutrient Slurry”) and the results (“drastically increase the algorithm’s
    adherence to the FAT framework, resulting in a more ethical and beneficent system”)
    are at odds… to say the least!
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'In philosophy, and especially philosophy of ethics, this is one of the most
    effective tools: first, come up with a process, definition, set of questions,
    etc., which is designed to resolve a problem. Then try to come up with an example
    in which that apparent solution results in a proposal that no one would consider
    acceptable. This can then lead to a further refinement of the solution.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve focused on things that you and your organization can do. But sometimes
    individual or organizational action is not enough. Sometimes governments also
    need to consider policy implications.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Role of Policy
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often talk to people who are eager for technical or design fixes to be a
    full solution to the kinds of problems that we’ve been discussing; for instance,
    a technical approach to debias data, or design guidelines for making technology
    less addictive. While such measures can be useful, they will not be sufficient
    to address the underlying problems that have led to our current state. For example,
    as long as it is profitable to create addictive technology, companies will continue
    to do so, regardless of whether this has the side effect of promoting conspiracy
    theories and polluting our information ecosystem. While individual designers may
    try to tweak product designs, we will not see substantial changes until the underlying
    profit incentives change.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: The Effectiveness of Regulation
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To look at what can cause companies to take concrete action, consider the following
    two examples of how Facebook has behaved. In 2018, a UN investigation found that
    Facebook had played a “determining role” in the ongoing genocide of the Rohingya,
    an ethnic minority in Mynamar described by UN Secretary-General Antonio Guterres
    as “one of, if not the, most discriminated people in the world.” Local activists
    had been warning Facebook executives that their platform was being used to spread
    hate speech and incite violence since as early as 2013\. In 2015, they were warned
    that Facebook could play the same role in Myanmar that the radio broadcasts played
    during the Rwandan genocide (where a million people were killed). Yet, by the
    end of 2015, Facebook employed only four contractors who spoke Burmese. As one
    person close to the matter said, “That’s not 20/20 hindsight. The scale of this
    problem was significant and it was already apparent.” Zuckerberg promised during
    the congressional hearings to hire “dozens” to address the genocide in Myanmar
    (in 2018, years after the genocide had begun, including the destruction by fire
    of at least 288 villages in northern Rakhine state after August 2017).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: This stands in stark contrast to Facebook quickly [hiring 1,200 people in Germany](https://oreil.ly/q_8Dz)
    to try to avoid expensive penalties (of up to 50 million euros) under a new German
    law against hate speech. Clearly, in this case, Facebook was more reactive to
    the threat of a financial penalty than to the systematic destruction of an ethnic
    minority.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'In an [article on privacy issues](https://oreil.ly/K5YKf), Maciej Ceglowski
    draws parallels with the environmental movement:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: This regulatory project has been so successful in the First World that we risk
    forgetting what life was like before it. Choking smog of the kind that today kills
    thousands in Jakarta and Delhi was [once emblematic of London](https://oreil.ly/pLzU7).
    The Cuyahoga River in Ohio used to [reliably catch fire](https://oreil.ly/qrU5v).
    In a particularly horrific example of unforeseen consequences, tetraethyl lead
    added to gasoline [raised violent crime rates](https://oreil.ly/4ngvr) worldwide
    for fifty years. None of these harms could have been fixed by telling people to
    vote with their wallet, or carefully review the environmental policies of every
    company they gave their business to, or to stop using the technologies in question.
    It took coordinated, and sometimes highly technical, regulation across jurisdictional
    boundaries to fix them. In some cases, like the [ban on commercial refrigerants](https://oreil.ly/o839J)
    that depleted the ozone layer, that regulation required a worldwide consensus.
    We’re at the point where we need a similar shift in perspective in our privacy
    law.
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Rights and Policy
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clean air and clean drinking water are public goods that are nearly impossible
    to protect through individual market decisions, but rather require coordinated
    regulatory action. Similarly, many of the harms resulting from unintended consequences
    of misuses of technology involve public goods, such as a polluted information
    environment or deteriorated ambient privacy. Too often privacy is framed as an
    individual right, yet there are societal impacts to widespread surveillance (which
    would still be the case even if it was possible for a few individuals to opt out).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Many of the issues we are seeing in tech are human rights issues, such as when
    a biased algorithm recommends that Black defendants have longer prison sentences,
    when particular job ads are shown only to young people, or when police use facial
    recognition to identify protesters. The appropriate venue to address human rights
    issues is typically through the law.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: We need both regulatory and legal changes, *and* the ethical behavior of individuals.
    Individual behavior change can’t address misaligned profit incentives, externalities
    (where corporations reap large profits while offloading their costs and harms
    to the broader society), or systemic failures. However, the law will never cover
    all edge cases, and it is important that individual software developers and data
    scientists are equipped to make ethical decisions in practice.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Cars: A Historical Precedent'
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problems we are facing are complex, and there are no simple solutions. This
    can be discouraging, but we find hope in considering other large challenges that
    people have tackled throughout history. One example is the movement to increase
    car safety, covered as a case study in [“Datasheets for Datasets”](https://oreil.ly/nqG_r)
    by Timnit Gebru et al. and in the design podcast [99% Invisible](https://oreil.ly/2HGPd).
    Early cars had no seatbelts, metal knobs on the dashboard that could lodge in
    people’s skulls during a crash, regular plate glass windows that shattered in
    dangerous ways, and noncollapsible steering columns that impaled drivers. However,
    car companies were resistant to even discussing safety as something they could
    help address, and the widespread belief was that cars are just the way they are,
    and that it was the people using them who caused problems.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: It took consumer safety activists and advocates decades of work to change the
    national conversation to consider that perhaps car companies had some responsibility
    that should be addressed through regulation. When the collapsible steering column
    was invented, it was not implemented for several years as there was no financial
    incentive to do so. Major car company General Motors hired private detectives
    to try to dig up dirt on consumer safety advocate Ralph Nader. The requirement
    of seatbelts, crash test dummies, and collapsible steering columns were major
    victories. It was only in 2011 that car companies were required to start using
    crash test dummies that would represent the average woman, and not just average
    men’s bodies; prior to this, women were 40% more likely to be injured in a car
    crash of the same impact compared to a man. This is a vivid example of the ways
    that bias, policy, and technology have important consequences.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coming from a background of working with binary logic, the lack of clear answers
    in ethics can be frustrating at first. Yet, the implications of how our work impacts
    the world, including unintended consequences and the work becoming weaponized
    by bad actors, are some of the most important questions we can (and should!) consider.
    Even though there aren’t any easy answers, there are definite pitfalls to avoid
    and practices to follow to move toward more ethical behavior.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Many people (including us!) are looking for more satisfying, solid answers
    about how to address harmful impacts of technology. However, given the complex,
    far-reaching, and interdisciplinary nature of the problems we are facing, there
    are no simple solutions. Julia Angwin, former senior reporter at ProPublica who
    focuses on issues of algorithmic bias and surveillance (and one of the 2016 investigators
    of the COMPAS recidivism algorithm that helped spark the field of FAccT) said
    in [a 2019 interview](https://oreil.ly/o7FpP):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: I strongly believe that in order to solve a problem, you have to diagnose it,
    and that we’re still in the diagnosis phase of this. If you think about the turn
    of the century and industrialization, we had, I don’t know, 30 years of child
    labor, unlimited work hours, terrible working conditions, and it took a lot of
    journalist muckraking and advocacy to diagnose the problem and have some understanding
    of what it was, and then the activism to get laws changed. I feel like we’re in
    a second industrialization of data information… I see my role as trying to make
    as clear as possible what the downsides are, and diagnosing them really accurately
    so that they can be solvable. That’s hard work, and lots more people need to be
    doing it.
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It’s reassuring that Angwin thinks we are largely still in the diagnosis phase:
    if your understanding of these problems feels incomplete, that is normal and natural.
    Nobody has a “cure” yet, although it is vital that we continue working to better
    understand and address the problems we are facing.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: One of our reviewers for this book, Fred Monroe, used to work in hedge fund
    trading. He told us, after reading this chapter, that many of the issues discussed
    here (distribution of data being dramatically different from what a model was
    trained on, the impact of feedback loops on a model once deployed and at scale,
    and so forth) were also key issues for building profitable trading models. The
    kinds of things you need to do to consider societal consequences are going to
    have a lot of overlap with things you need to do to consider organizational, market,
    and customer consequences—so thinking carefully about ethics can also help you
    think carefully about how to make your data product successful more generally!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does ethics provide a list of “right answers”?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can working with people of different backgrounds help when considering ethical
    questions?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What was the role of IBM in Nazi Germany? Why did the company participate as
    it did? Why did the workers participate?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What was the role of the first person jailed in the Volkswagen diesel scandal?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What was the problem with a database of suspected gang members maintained by
    California law enforcement officials?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why did YouTube’s recommendation algorithm recommend videos of partially clothed
    children to pedophiles, even though no employee at Google had programmed this
    feature?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the problems with the centrality of metrics?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why did Meetup.com not include gender in its recommendation system for tech
    meetups?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the six types of bias in machine learning, according to Suresh and
    Guttag?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give two examples of historical race bias in the US.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where are most images in ImageNet from?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the paper “Does Machine Learning Automate Moral Hazard and Error?” why is
    sinusitis found to be predictive of a stroke?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is representation bias?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How are machines and people different, in terms of their use for making decisions?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is disinformation the same as “fake news”?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is disinformation through autogenerated text a particularly significant
    issue?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the five ethical lenses described by the Markkula Center?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where is policy an appropriate tool for addressing data ethics issues?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read the article [“What Happens When an Algorithm Cuts Your Healthcare”](https://oreil.ly/5Ziok).
    How could problems like this be avoided in the future?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Research to find out more about YouTube’s recommendation system and its societal
    impacts. Do you think recommendation systems must always have feedback loops with
    negative results? What approaches could Google take to avoid them? What about
    the government?
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the paper [“Discrimination in Online Ad Delivery”](https://oreil.ly/jgKpM).
    Do you think Google should be considered responsible for what happened to Dr.
    Sweeney? What would be an appropriate response?
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can a cross-disciplinary team help avoid negative consequences?
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the paper [“Does Machine Learning Automate Moral Hazard and Error?”](https://oreil.ly/tLLOf)
    What actions do you think should be taken to deal with the issues identified in
    this paper?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the article [“How Will We Prevent AI-Based Forgery?”](https://oreil.ly/6MQe4)
    Do you think Etzioni’s proposed approach could work? Why?
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete the section [“Analyze a Project You Are Working On”](#analzye_a_project).
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider whether your team could be more diverse. If so, what approaches might
    help?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deep Learning in Practice: That’s a Wrap!'
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You’ve made it to the end of the first section of the book.
    In this section, we’ve tried to show you what deep learning can do, and how you
    can use it to create real applications and products. At this point, you will get
    a lot more out of the book if you spend some time trying out what you’ve learned.
    Perhaps you have already been doing this as you go along—in which case, great!
    If not, that’s no problem either—now is a great time to start experimenting yourself.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t been to the [book’s website](https://book.fast.ai) yet, head
    over there now. It’s really important that you get yourself set up to run the
    notebooks. Becoming an effective deep learning practitioner is all about practice,
    so you need to be training models. So, please go get the notebooks running now
    if you haven’t already! And have a look on the website for any important updates
    or notices; deep learning changes fast, and we can’t change the words that are
    printed in this book, so the website is where you need to look to ensure you have
    the most up-to-date information.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that you have completed the following steps:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Connect to one of the GPU Jupyter servers recommended on the book’s website.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the first notebook yourself.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload an image that you find in the first notebook; then try a few images of
    different kinds to see what happens.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the second notebook, collecting your own dataset based on image search queries
    that you come up with.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think about how you can use deep learning to help you with your own projects,
    including what kinds of data you could use, what kinds of problems may come up,
    and how you might be able to mitigate these issues in practice.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section of the book, you will learn about how and why deep learning
    works, instead of just seeing how you can use it in practice. Understanding the
    how and why is important for both practitioners and researchers, because in this
    fairly new field, nearly every project requires some level of customization and
    debugging. The better you understand the foundations of deep learning, the better
    your models will be. These foundations are less important for executives, product
    managers, and so forth (although still useful, so feel free to keep reading!),
    but they are critical for anybody who is training and deploying models themselves.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
