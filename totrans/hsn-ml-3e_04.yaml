- en: Chapter 3\. Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#landscape_chapter) I mentioned that the most common
    supervised learning tasks are regression (predicting values) and classification
    (predicting classes). In [Chapter 2](ch02.html#project_chapter) we explored a
    regression task, predicting housing values, using various algorithms such as linear
    regression, decision trees, and random forests (which will be explained in further
    detail in later chapters). Now we will turn our attention to classification systems.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we will be using the MNIST dataset, which is a set of 70,000
    small images of digits handwritten by high school students and employees of the
    US Census Bureau. Each image is labeled with the digit it represents. This set
    has been studied so much that it is often called the “hello world” of machine
    learning: whenever people come up with a new classification algorithm they are
    curious to see how it will perform on MNIST, and anyone who learns machine learning
    tackles this dataset sooner or later.'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn provides many helper functions to download popular datasets. MNIST
    is one of them. The following code fetches the MNIST dataset from OpenML.org:⁠^([1](ch03.html#idm45720235827040))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sklearn.datasets` package contains mostly three types of functions: `fetch_*`
    functions such as `fetch_openml()` to download real-life datasets, `load_*` functions
    to load small toy datasets bundled with Scikit-Learn (so they don’t need to be
    downloaded over the internet), and `make_*` functions to generate fake datasets,
    useful for tests. Generated datasets are usually returned as an `(X, y)` tuple
    containing the input data and the targets, both as NumPy arrays. Other datasets
    are returned as `sklearn.utils.Bunch` objects, which are dictionaries whose entries
    can also be accessed as attributes. They generally contain the following entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"DESCR"`'
  prefs: []
  type: TYPE_NORMAL
- en: A description of the dataset
  prefs: []
  type: TYPE_NORMAL
- en: '`"data"`'
  prefs: []
  type: TYPE_NORMAL
- en: The input data, usually as a 2D NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: '`"target"`'
  prefs: []
  type: TYPE_NORMAL
- en: The labels, usually as a 1D NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fetch_openml()` function is a bit unusual since by default it returns
    the inputs as a Pandas DataFrame and the labels as a Pandas Series (unless the
    dataset is sparse). But the MNIST dataset contains images, and DataFrames aren’t
    ideal for that, so it’s preferable to set `as_frame=False` to get the data as
    NumPy arrays instead. Let’s look at these arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 70,000 images, and each image has 784 features. This is because each
    image is 28 × 28 pixels, and each feature simply represents one pixel’s intensity,
    from 0 (white) to 255 (black). Let’s take a peek at one digit from the dataset
    ([Figure 3-1](#some_digit_plot)). All we need to do is grab an instance’s feature
    vector, reshape it to a 28 × 28 array, and display it using Matplotlib’s `imshow()`
    function. We use `cmap="binary"` to get a grayscale color map where 0 is white
    and 255 is black:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0301](assets/mls3_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Example of an MNIST image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This looks like a 5, and indeed that’s what the label tells us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To give you a feel for the complexity of the classification task, [Figure 3-2](#more_digits_plot)
    shows a few more images from the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: But wait! You should always create a test set and set it aside before inspecting
    the data closely. The MNIST dataset returned by `fetch_openml()` is actually already
    split into a training set (the first 60,000 images) and a test set (the last 10,000
    images):^([2](ch03.html#idm45720239036336))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The training set is already shuffled for us, which is good because this guarantees
    that all cross-validation folds will be similar (we don’t want one fold to be
    missing some digits). Moreover, some learning algorithms are sensitive to the
    order of the training instances, and they perform poorly if they get many similar
    instances in a row. Shuffling the dataset ensures that this won’t happen.⁠^([3](ch03.html#idm45720238991712))
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0302](assets/mls3_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Digits from the MNIST dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Training a Binary Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s simplify the problem for now and only try to identify one digit—for example,
    the number 5\. This “5-detector” will be an example of a *binary classifier*,
    capable of distinguishing between just two classes, 5 and non-5\. First we’ll
    create the target vectors for this classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s pick a classifier and train it. A good place to start is with a *stochastic
    gradient descent* (SGD, or stochastic GD) classifier, using Scikit-Learn’s `SGDClassifier`
    class. This classifier is capable of handling very large datasets efficiently.
    This is in part because SGD deals with training instances independently, one at
    a time, which also makes SGD well suited for online learning, as you will see
    later. Let’s create an `SGDClassifier` and train it on the whole training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use it to detect images of the number 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The classifier guesses that this image represents a 5 (`True`). Looks like it
    guessed right in this particular case! Now, let’s evaluate this model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating a classifier is often significantly trickier than evaluating a regressor,
    so we will spend a large part of this chapter on this topic. There are many performance
    measures available, so grab another coffee and get ready to learn a bunch of new
    concepts and acronyms!
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Accuracy Using Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A good way to evaluate a model is to use cross-validation, just as you did
    in [Chapter 2](ch02.html#project_chapter). Let’s use the `cross_val_score()` function
    to evaluate our `SGDClassifier` model, using *k*-fold cross-validation with three
    folds. Remember that *k*-fold cross-validation means splitting the training set
    into *k* folds (in this case, three), then training the model *k* times, holding
    out a different fold each time for evaluation (see [Chapter 2](ch02.html#project_chapter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Wow! Above 95% accuracy (ratio of correct predictions) on all cross-validation
    folds? This looks amazing, doesn’t it? Well, before you get too excited, let’s
    look at a dummy classifier that just classifies every single image in the most
    frequent class, which in this case is the negative class (i.e., *non* 5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Can you guess this model’s accuracy? Let’s find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: That’s right, it has over 90% accuracy! This is simply because only about 10%
    of the images are 5s, so if you always guess that an image is *not* a 5, you will
    be right about 90% of the time. Beats Nostradamus.
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates why accuracy is generally not the preferred performance measure
    for classifiers, especially when you are dealing with *skewed datasets* (i.e.,
    when some classes are much more frequent than others). A much better way to evaluate
    the performance of a classifier is to look at the *confusion matrix* (CM).
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The general idea of a confusion matrix is to count the number of times instances
    of class A are classified as class B, for all A/B pairs. For example, to know
    the number of times the classifier confused images of 8s with 0s, you would look
    at row #8, column #0 of the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the confusion matrix, you first need to have a set of predictions
    so that they can be compared to the actual targets. You could make predictions
    on the test set, but it’s best to keep that untouched for now (remember that you
    want to use the test set only at the very end of your project, once you have a
    classifier that you are ready to launch). Instead, you can use the `cross_val_predict()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like the `cross_val_score()` function, `cross_val_predict()` performs
    *k*-fold cross-validation, but instead of returning the evaluation scores, it
    returns the predictions made on each test fold. This means that you get a clean
    prediction for each instance in the training set (by “clean” I mean “out-of-sample”:
    the model makes predictions on data that it never saw during training).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you are ready to get the confusion matrix using the `confusion_matrix()`
    function. Just pass it the target classes (`y_train_5`) and the predicted classes
    (`y_train_pred`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Each row in a confusion matrix represents an *actual class*, while each column
    represents a *predicted class*. The first row of this matrix considers non-5 images
    (the *negative class*): 53,892 of them were correctly classified as non-5s (they
    are called *true negatives*), while the remaining 687 were wrongly classified
    as 5s (*false positives*, also called *type I errors*). The second row considers
    the images of 5s (the *positive class*): 1,891 were wrongly classified as non-5s
    (*false negatives*, also called *type II errors*), while the remaining 3,530 were
    correctly classified as 5s (*true positives*). A perfect classifier would only
    have true positives and true negatives, so its confusion matrix would have nonzero
    values only on its main diagonal (top left to bottom right):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The confusion matrix gives you a lot of information, but sometimes you may prefer
    a more concise metric. An interesting one to look at is the accuracy of the positive
    predictions; this is called the *precision* of the classifier ([Equation 3-1](#equation_three_one)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3-1\. Precision
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>precision</mtext> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '*TP* is the number of true positives, and *FP* is the number of false positives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A trivial way to have perfect precision is to create a classifier that always
    makes negative predictions, except for one single positive prediction on the instance
    it’s most confident about. If this one prediction is correct, then the classifier
    has 100% precision (precision = 1/1 = 100%). Obviously, such a classifier would
    not be very useful, since it would ignore all but one positive instance. So, precision
    is typically used along with another metric named *recall*, also called *sensitivity*
    or the *true positive rate* (TPR): this is the ratio of positive instances that
    are correctly detected by the classifier ([Equation 3-2](#equation_three_two)).'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3-2\. Recall
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>recall</mtext> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '*FN* is, of course, the number of false negatives.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are confused about the confusion matrix, [Figure 3-3](#confusion_matrix_diagram)
    may help.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0303](assets/mls3_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. An illustrated confusion matrix showing examples of true negatives
    (top left), false positives (top right), false negatives (lower left), and true
    positives (lower right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Precision and Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides several functions to compute classifier metrics, including
    precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now our 5-detector does not look as shiny as it did when we looked at its accuracy.
    When it claims an image represents a 5, it is correct only 83.7% of the time.
    Moreover, it only detects 65.1% of the 5s.
  prefs: []
  type: TYPE_NORMAL
- en: It is often convenient to combine precision and recall into a single metric
    called the *F[1] score*, especially when you need a single metric to compare two
    classifiers. The F[1] score is the *harmonic mean* of precision and recall ([Equation
    3-3](#equation_three_three)). Whereas the regular mean treats all values equally,
    the harmonic mean gives much more weight to low values. As a result, the classifier
    will only get a high F[1] score if both recall and precision are high.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 3-3\. F[1] score
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi mathvariant="italic">F</mi> <mn>1</mn></msub>
    <mo>=</mo> <mfrac><mn>2</mn> <mrow><mfrac><mn>1</mn> <mtext>precision</mtext></mfrac><mo>+</mo><mfrac><mn>1</mn>
    <mtext>recall</mtext></mfrac></mrow></mfrac> <mo>=</mo> <mn>2</mn> <mo>×</mo>
    <mfrac><mrow><mtext>precision</mtext><mo>×</mo><mtext>recall</mtext></mrow> <mrow><mtext>precision</mtext><mo>+</mo><mtext>recall</mtext></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mfrac><mrow><mi>F</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow>
    <mn>2</mn></mfrac></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the F[1] score, simply call the `f1_score()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The F[1] score favors classifiers that have similar precision and recall. This
    is not always what you want: in some contexts you mostly care about precision,
    and in other contexts you really care about recall. For example, if you trained
    a classifier to detect videos that are safe for kids, you would probably prefer
    a classifier that rejects many good videos (low recall) but keeps only safe ones
    (high precision), rather than a classifier that has a much higher recall but lets
    a few really bad videos show up in your product (in such cases, you may even want
    to add a human pipeline to check the classifier’s video selection). On the other
    hand, suppose you train a classifier to detect shoplifters in surveillance images:
    it is probably fine if your classifier only has 30% precision as long as it has
    99% recall (sure, the security guards will get a few false alerts, but almost
    all shoplifters will get caught).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, you can’t have it both ways: increasing precision reduces recall,
    and vice versa. This is called the *precision/recall trade-off*.'
  prefs: []
  type: TYPE_NORMAL
- en: The Precision/Recall Trade-off
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand this trade-off, let’s look at how the `SGDClassifier` makes its
    classification decisions. For each instance, it computes a score based on a *decision
    function*. If that score is greater than a threshold, it assigns the instance
    to the positive class; otherwise it assigns it to the negative class. [Figure 3-4](#decision_threshold_diagram)
    shows a few digits positioned from the lowest score on the left to the highest
    score on the right. Suppose the *decision threshold* is positioned at the central
    arrow (between the two 5s): you will find 4 true positives (actual 5s) on the
    right of that threshold, and 1 false positive (actually a 6). Therefore, with
    that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the
    classifier only detects 4, so the recall is 67% (4 out of 6). If you raise the
    threshold (move it to the arrow on the right), the false positive (the 6) becomes
    a true negative, thereby increasing the precision (up to 100% in this case), but
    one true positive becomes a false negative, decreasing recall down to 50%. Conversely,
    lowering the threshold increases recall and reduces precision.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0304](assets/mls3_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-4\. The precision/recall trade-off: images are ranked by their classifier
    score, and those above the chosen decision threshold are considered positive;
    the higher the threshold, the lower the recall, but (in general) the higher the
    precision'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Scikit-Learn does not let you set the threshold directly, but it does give
    you access to the decision scores that it uses to make predictions. Instead of
    calling the classifier’s `predict()` method, you can call its `decision_function()`
    method, which returns a score for each instance, and then use any threshold you
    want to make predictions based on those scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SGDClassifier` uses a threshold equal to 0, so the preceding code returns
    the same result as the `predict()` method (i.e., `True`). Let’s raise the threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This confirms that raising the threshold decreases recall. The image actually
    represents a 5, and the classifier detects it when the threshold is 0, but it
    misses it when the threshold is increased to 3,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do you decide which threshold to use? First, use the `cross_val_predict()`
    function to get the scores of all instances in the training set, but this time
    specify that you want to return decision scores instead of predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With these scores, use the `precision_recall_curve()` function to compute precision
    and recall for all possible thresholds (the function adds a last precision of
    0 and a last recall of 1, corresponding to an infinite threshold):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, use Matplotlib to plot precision and recall as functions of the threshold
    value ([Figure 3-5](#precision_recall_vs_threshold_plot)). Let’s show the threshold
    of 3,000 we selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0305](assets/mls3_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Precision and recall versus the decision threshold
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You may wonder why the precision curve is bumpier than the recall curve in
    [Figure 3-5](#precision_recall_vs_threshold_plot). The reason is that precision
    may sometimes go down when you raise the threshold (although in general it will
    go up). To understand why, look back at [Figure 3-4](#decision_threshold_diagram)
    and notice what happens when you start from the central threshold and move it
    just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%).
    On the other hand, recall can only go down when the threshold is increased, which
    explains why its curve looks smooth.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this threshold value, precision is near 90% and recall is around 50%. Another
    way to select a good precision/recall trade-off is to plot precision directly
    against recall, as shown in [Figure 3-6](#precision_vs_recall_plot) (the same
    threshold is shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0306](assets/mls3_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Precision versus recall
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see that precision really starts to fall sharply at around 80% recall.
    You will probably want to select a precision/recall trade-off just before that
    drop—for example, at around 60% recall. But of course, the choice depends on your
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you decide to aim for 90% precision. You could use the first plot to
    find the threshold you need to use, but that’s not very precise. Alternatively,
    you can search for the lowest threshold that gives you at least 90% precision.
    For this, you can use the NumPy array’s `argmax()` method. This returns the first
    index of the maximum value, which in this case means the first `True` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To make predictions (on the training set for now), instead of calling the classifier’s
    `predict()` method, you can run this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check these predictions’ precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, you have a 90% precision classifier! As you can see, it is fairly easy
    to create a classifier with virtually any precision you want: just set a high
    enough threshold, and you’re done. But wait, not so fast–a high-precision classifier
    is not very useful if its recall is too low! For many applications, 48% recall
    wouldn’t be great at all.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If someone says, “Let’s reach 99% precision”, you should ask, “At what recall?”
  prefs: []
  type: TYPE_NORMAL
- en: The ROC Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *receiver operating characteristic* (ROC) curve is another common tool used
    with binary classifiers. It is very similar to the precision/recall curve, but
    instead of plotting precision versus recall, the ROC curve plots the *true positive
    rate* (another name for recall) against the *false positive rate* (FPR). The FPR
    (also called the *fall-out*) is the ratio of negative instances that are incorrectly
    classified as positive. It is equal to 1 – the *true negative rate* (TNR), which
    is the ratio of negative instances that are correctly classified as negative.
    The TNR is also called *specificity*. Hence, the ROC curve plots *sensitivity*
    (recall) versus 1 – *specificity*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To plot the ROC curve, you first use the `roc_curve()` function to compute
    the TPR and FPR for various threshold values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can plot the FPR against the TPR using Matplotlib. The following code
    produces the plot in [Figure 3-7](#roc_curve_plot). To find the point that corresponds
    to 90% precision, we need to look for the index of the desired threshold. Since
    thresholds are listed in decreasing order in this case, we use `<=` instead of
    `>=` on the first line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0307](assets/mls3_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. A ROC curve plotting the false positive rate against the true positive
    rate for all possible thresholds; the black circle highlights the chosen ratio
    (at 90% precision and 48% recall)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once again there is a trade-off: the higher the recall (TPR), the more false
    positives (FPR) the classifier produces. The dotted line represents the ROC curve
    of a purely random classifier; a good classifier stays as far away from that line
    as possible (toward the top-left corner).'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to compare classifiers is to measure the *area under the curve* (AUC).
    A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier
    will have a ROC AUC equal to 0.5\. Scikit-Learn provides a function to estimate
    the ROC AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since the ROC curve is so similar to the precision/recall (PR) curve, you may
    wonder how to decide which one to use. As a rule of thumb, you should prefer the
    PR curve whenever the positive class is rare or when you care more about the false
    positives than the false negatives. Otherwise, use the ROC curve. For example,
    looking at the previous ROC curve (and the ROC AUC score), you may think that
    the classifier is really good. But this is mostly because there are few positives
    (5s) compared to the negatives (non-5s). In contrast, the PR curve makes it clear
    that the classifier has room for improvement: the curve could really be closer
    to the top-right corner (see [Figure 3-6](#precision_vs_recall_plot) again).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now create a `RandomForestClassifier`, whose PR curve and F[1] score
    we can compare to those of the `SGDClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `precision_recall_curve()` function expects labels and scores for each
    instance, so we need to train the random forest classifier and make it assign
    a score to each instance. But the `RandomForestClassifier` class does not have
    a `decision_function()` method, due to the way it works (we will cover this in
    [Chapter 7](ch07.html#ensembles_chapter)). Luckily, it has a `predict_proba()`
    method that returns class probabilities for each instance, and we can just use
    the probability of the positive class as a score, so it will work fine.^([4](ch03.html#idm45720237468928))
    We can call the `cross_val_predict()` function to train the `RandomForestClassifier`
    using cross-validation and make it predict class probabilities for every image
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the class probabilities for the first two images in the training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The model predicts that the first image is positive with 89% probability, and
    it predicts that the second image is negative with 99% probability. Since each
    image is either positive or negative, the probabilities in each row add up to
    100%.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These are *estimated* probabilities, not actual probabilities. For example,
    if you look at all the images that the model classified as positive with an estimated
    probability between 50% and 60%, roughly 94% of them are actually positive. So,
    the model’s estimated probabilities were much too low in this case—but models
    can be overconfident as well. The `sklearn.calibration` package contains tools
    to calibrate the estimated probabilities and make them much closer to actual probabilities.
    See the extra material section in [this chapter’s notebook](https://homl.info/colab3)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second column contains the estimated probabilities for the positive class,
    so let’s pass them to the `precision_recall_curve()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’re ready to plot the PR curve. It is useful to plot the first PR curve
    as well to see how they compare ([Figure 3-8](#pr_curve_comparison_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0308](assets/mls3_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-8\. Comparing PR curves: the random forest classifier is superior
    to the SGD classifier because its PR curve is much closer to the top-right corner,
    and it has a greater AUC'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see in [Figure 3-8](#pr_curve_comparison_plot), the `RandomForestClassifier`’s
    PR curve looks much better than the `SGDClassifier`’s: it comes much closer to
    the top-right corner. Its F[1] score and ROC AUC score are also significantly
    better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Try measuring the precision and recall scores: you should find about 99.1%
    precision and 86.6% recall. Not too bad!'
  prefs: []
  type: TYPE_NORMAL
- en: You now know how to train binary classifiers, choose the appropriate metric
    for your task, evaluate your classifiers using cross-validation, select the precision/recall
    trade-off that fits your needs, and use several metrics and curves to compare
    various models. You’re ready to try to detect more than just the 5s.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whereas binary classifiers distinguish between two classes, *multiclass classifiers*
    (also called *multinomial classifiers*) can distinguish between more than two
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Some Scikit-Learn classifiers (e.g., `LogisticRegression`, `RandomForestClassifier`,
    and `GaussianNB`) are capable of handling multiple classes natively. Others are
    strictly binary classifiers (e.g., `SGDClassifier` and `SVC`). However, there
    are various strategies that you can use to perform multiclass classification with
    multiple binary classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: One way to create a system that can classify the digit images into 10 classes
    (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector,
    a 1-detector, a 2-detector, and so on). Then when you want to classify an image,
    you get the decision score from each classifier for that image and you select
    the class whose classifier outputs the highest score. This is called the *one-versus-the-rest*
    (OvR) strategy, or sometimes *one-versus-all* (OvA).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another strategy is to train a binary classifier for every pair of digits:
    one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s
    and 2s, and so on. This is called the *one-versus-one* (OvO) strategy. If there
    are *N* classes, you need to train *N* × (*N* – 1) / 2 classifiers. For the MNIST
    problem, this means training 45 binary classifiers! When you want to classify
    an image, you have to run the image through all 45 classifiers and see which class
    wins the most duels. The main advantage of OvO is that each classifier only needs
    to be trained on the part of the training set containing the two classes that
    it must distinguish.'
  prefs: []
  type: TYPE_NORMAL
- en: Some algorithms (such as support vector machine classifiers) scale poorly with
    the size of the training set. For these algorithms OvO is preferred because it
    is faster to train many classifiers on small training sets than to train few classifiers
    on large training sets. For most binary classification algorithms, however, OvR
    is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn detects when you try to use a binary classification algorithm
    for a multiclass classification task, and it automatically runs OvR or OvO, depending
    on the algorithm. Let’s try this with a support vector machine classifier using
    the `sklearn.svm.SVC` class (see [Chapter 5](ch05.html#svm_chapter)). We’ll only
    train on the first 2,000 images, or else it will take a very long time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'That was easy! We trained the `SVC` using the original target classes from
    0 to 9 (`y_train`), instead of the 5-versus-the-rest target classes (`y_train_5`).
    Since there are 10 classes (i.e., more than 2), Scikit-Learn used the OvO strategy
    and trained 45 binary classifiers. Now let’s make a prediction on an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s correct! This code actually made 45 predictions—one per pair of classes—and
    it selected the class that won the most duels. If you call the `decision_function()`
    method, you will see that it returns 10 scores per instance: one per class. Each
    class gets a score equal to the number of won duels plus or minus a small tweak
    (max ±0.33) to break ties, based on the classifier scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The highest score is 9.3, and it’s indeed the one corresponding to class 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'When a classifier is trained, it stores the list of target classes in its `classes_`
    attribute, ordered by value. In the case of MNIST, the index of each class in
    the `classes_` array conveniently matches the class itself (e.g., the class at
    index 5 happens to be class `''5''`), but in general you won’t be so lucky; you
    will need to look up the class label like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to force Scikit-Learn to use one-versus-one or one-versus-the-rest,
    you can use the `OneVsOneClassifier` or `OneVsRestClassifier` classes. Simply
    create an instance and pass a classifier to its constructor (it doesn’t even have
    to be a binary classifier). For example, this code creates a multiclass classifier
    using the OvR strategy, based on an `SVC`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s make a prediction, and check the number of trained classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Training an `SGDClassifier` on a multiclass dataset and using it to make predictions
    is just as easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Oops, that’s incorrect. Prediction errors do happen! This time Scikit-Learn
    used the OvR strategy under the hood: since there are 10 classes, it trained 10
    binary classifiers. The `decision_function()` method now returns one value per
    class. Let’s look at the scores that the SGD classifier assigned to each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that the classifier is not very confident about its prediction:
    almost all scores are very negative, while class 3 has a score of +1,824, and
    class 5 is not too far behind at –1,386\. Of course, you’ll want to evaluate this
    classifier on more than one image. Since there are roughly the same number of
    images in each class, the accuracy metric is fine. As usual, you can use the `cross_val_score()`
    function to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'It gets over 85.8% on all test folds. If you used a random classifier, you
    would get 10% accuracy, so this is not such a bad score, but you can still do
    much better. Simply scaling the inputs (as discussed in [Chapter 2](ch02.html#project_chapter))
    increases accuracy above 89.1%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Error Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If this were a real project, you would now follow the steps in your machine
    learning project checklist (see [Appendix A](app01.html#project_checklist_appendix)).
    You’d explore data preparation options, try out multiple models, shortlist the
    best ones, fine-tune their hyperparameters using `GridSearchCV`, and automate
    as much as possible. Here, we will assume that you have found a promising model
    and you want to find ways to improve it. One way to do this is to analyze the
    types of errors it makes.
  prefs: []
  type: TYPE_NORMAL
- en: First, look at the confusion matrix. For this, you first need to make predictions
    using the `cross_val_predict()` function; then you can pass the labels and predictions
    to the `confusion_matrix()` function, just like you did earlier. However, since
    there are now 10 classes instead of 2, the confusion matrix will contain quite
    a lot of numbers, and it may be hard to read.
  prefs: []
  type: TYPE_NORMAL
- en: 'A colored diagram of the confusion matrix is much easier to analyze. To plot
    such a diagram, use the `ConfusionMatrixDisplay.from_predictions()` function like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the left diagram in [Figure 3-9](#confusion_matrix_plot_1). This
    confusion matrix looks pretty good: most images are on the main diagonal, which
    means that they were classified correctly. Notice that the cell on the diagonal
    in row #5 and column #5 looks slightly darker than the other digits. This could
    be because the model made more errors on 5s, or because there are fewer 5s in
    the dataset than the other digits. That’s why it’s important to normalize the
    confusion matrix by dividing each value by the total number of images in the corresponding
    (true) class (i.e., divide by the row’s sum). This can be done simply by setting
    `normalize="true"`. We can also specify the `values_format=".0%"` argument to
    show percentages with no decimals. The following code produces the diagram on
    the right in [Figure 3-9](#confusion_matrix_plot_1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can easily see that only 82% of the images of 5s were classified correctly.
    The most common error the model made with images of 5s was to misclassify them
    as 8s: this happened for 10% of all 5s. But only 2% of 8s got misclassified as
    5s; confusion matrices are generally not symmetrical! If you look carefully, you
    will notice that many digits have been misclassified as 8s, but this is not immediately
    obvious from this diagram. If you want to make the errors stand out more, you
    can try putting zero weight on the correct predictions. The following code does
    just that and produces the diagram on the left in [Figure 3-10](#confusion_matrix_plot_2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0309](assets/mls3_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Confusion matrix (left) and the same CM normalized by row (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![mls3 0310](assets/mls3_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Confusion matrix with errors only, normalized by row (left) and
    by column (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now you can see much more clearly the kinds of errors the classifier makes.
    The column for class 8 is now really bright, which confirms that many images got
    misclassified as 8s. In fact this is the most common misclassification for almost
    all classes. But be careful how you interpret the percentages in this diagram:
    remember that we’ve excluded the correct predictions. For example, the 36% in
    row #7, column #9 does *not* mean that 36% of all images of 7s were misclassified
    as 9s. It means that 36% of the *errors* the model made on images of 7s were misclassifications
    as 9s. In reality, only 3% of images of 7s were misclassified as 9s, as you can
    see in the diagram on the right in [Figure 3-9](#confusion_matrix_plot_1).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to normalize the confusion matrix by column rather than
    by row: if you set `normalize="pred"`, you get the diagram on the right in [Figure 3-10](#confusion_matrix_plot_2).
    For example, you can see that 56% of misclassified 7s are actually 9s.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the confusion matrix often gives you insights into ways to improve
    your classifier. Looking at these plots, it seems that your efforts should be
    spent on reducing the false 8s. For example, you could try to gather more training
    data for digits that look like 8s (but are not) so that the classifier can learn
    to distinguish them from real 8s. Or you could engineer new features that would
    help the classifier—for example, writing an algorithm to count the number of closed
    loops (e.g., 8 has two, 6 has one, 5 has none). Or you could preprocess the images
    (e.g., using Scikit-Image, Pillow, or OpenCV) to make some patterns, such as closed
    loops, stand out more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyzing individual errors can also be a good way to gain insights into what
    your classifier is doing and why it is failing. For example, let’s plot examples
    of 3s and 5s in a confusion matrix style ([Figure 3-11](#error_analysis_digits_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0311](assets/mls3_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Some images of 3s and 5s organized like a confusion matrix
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, some of the digits that the classifier gets wrong (i.e., in
    the bottom-left and top-right blocks) are so badly written that even a human would
    have trouble classifying them. However, most misclassified images seem like obvious
    errors to us. It may be hard to understand why the classifier made the mistakes
    it did, but remember that the human brain is a fantastic pattern recognition system,
    and our visual system does a lot of complex preprocessing before any information
    even reaches our consciousness. So, the fact that this task feels simple does
    not mean that it is. Recall that we used a simple `SGDClassifier`, which is just
    a linear model: all it does is assign a weight per class to each pixel, and when
    it sees a new image it just sums up the weighted pixel intensities to get a score
    for each class. Since 3s and 5s differ by only a few pixels, this model will easily
    confuse them.'
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between 3s and 5s is the position of the small line that
    joins the top line to the bottom arc. If you draw a 3 with the junction slightly
    shifted to the left, the classifier might classify it as a 5, and vice versa.
    In other words, this classifier is quite sensitive to image shifting and rotation.
    One way to reduce the 3/5 confusion is to preprocess the images to ensure that
    they are well centered and not too rotated. However, this may not be easy since
    it requires predicting the correct rotation of each image. A much simpler approach
    consists of augmenting the training set with slightly shifted and rotated variants
    of the training images. This will force the model to learn to be more tolerant
    to such variations. This is called *data augmentation* (we’ll cover this in [Chapter 14](ch14.html#cnn_chapter);
    also see exercise 2 at the end of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Multilabel Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until now, each instance has always been assigned to just one class. But in
    some cases you may want your classifier to output multiple classes for each instance.
    Consider a face-recognition classifier: what should it do if it recognizes several
    people in the same picture? It should attach one tag per person it recognizes.
    Say the classifier has been trained to recognize three faces: Alice, Bob, and
    Charlie. Then when the classifier is shown a picture of Alice and Charlie, it
    should output `[True, False, True]` (meaning “Alice yes, Bob no, Charlie yes”).
    Such a classification system that outputs multiple binary tags is called a *multilabel
    classification* system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We won’t go into face recognition just yet, but let’s look at a simpler example,
    just for illustration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates a `y_multilabel` array containing two target labels for each
    digit image: the first indicates whether or not the digit is large (7, 8, or 9),
    and the second indicates whether or not it is odd. Then the code creates a `KNeighborsClassifier`
    instance, which supports multilabel classification (not all classifiers do), and
    trains this model using the multiple targets array. Now you can make a prediction,
    and notice that it outputs two labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: And it gets it right! The digit 5 is indeed not large (`False`) and odd (`True`).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to evaluate a multilabel classifier, and selecting the
    right metric really depends on your project. One approach is to measure the F[1]
    score for each individual label (or any other binary classifier metric discussed
    earlier), then simply compute the average score. The following code computes the
    average F[1] score across all labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This approach assumes that all labels are equally important, which may not be
    the case. In particular, if you have many more pictures of Alice than of Bob or
    Charlie, you may want to give more weight to the classifier’s score on pictures
    of Alice. One simple option is to give each label a weight equal to its *support*
    (i.e., the number of instances with that target label). To do this, simply set
    `average="weighted"` when calling the `f1_score()` function.⁠^([5](ch03.html#idm45720217874768))
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wish to use a classifier that does not natively support multilabel classification,
    such as `SVC`, one possible strategy is to train one model per label. However,
    this strategy may have a hard time capturing the dependencies between the labels.
    For example, a large digit (7, 8, or 9) is twice more likely to be odd than even,
    but the classifier for the “odd” label does not know what the classifier for the
    “large” label predicted. To solve this issue, the models can be organized in a
    chain: when a model makes a prediction, it uses the input features plus all the
    predictions of the models that come before it in the chain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that Scikit-Learn has a class called `ChainClassifier` that
    does just that! By default it will use the true labels for training, feeding each
    model the appropriate labels depending on their position in the chain. But if
    you set the `cv` hyperparameter, it will use cross-validation to get “clean” (out-of-sample)
    predictions from each trained model for every instance in the training set, and
    these predictions will then be used to train all the models later in the chain.
    Here’s an example showing how to create and train a `ChainClassifier` using the
    cross-validation strategy. As earlier, we’ll just use the first 2,000 images in
    the training set to speed things up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use this `ChainClassifier` to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Multioutput Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last type of classification task we’ll discuss here is called *multioutput–multiclass
    classification* (or just *multioutput classification*). It is a generalization
    of multilabel classification where each label can be multiclass (i.e., it can
    have more than two possible values).
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, let’s build a system that removes noise from images. It
    will take as input a noisy digit image, and it will (hopefully) output a clean
    digit image, represented as an array of pixel intensities, just like the MNIST
    images. Notice that the classifier’s output is multilabel (one label per pixel)
    and each label can have multiple values (pixel intensity ranges from 0 to 255).
    This is thus an example of a multioutput classification system.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The line between classification and regression is sometimes blurry, such as
    in this example. Arguably, predicting pixel intensity is more akin to regression
    than to classification. Moreover, multioutput systems are not limited to classification
    tasks; you could even have a system that outputs multiple labels per instance,
    including both class labels and value labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating the training and test sets by taking the MNIST images
    and adding noise to their pixel intensities with NumPy’s `randint()` function.
    The target images will be the original images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a peek at the first image from the test set ([Figure 3-12](#noisy_digit_example_plot)).
    Yes, we’re snooping on the test data, so you should be frowning right now.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0312](assets/mls3_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. A noisy image (left) and the target clean image (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'On the left is the noisy input image, and on the right is the clean target
    image. Now let’s train the classifier and make it clean up this image ([Figure 3-13](#cleaned_digit_example_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0313](assets/mls3_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. The cleaned-up image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looks close enough to the target! This concludes our tour of classification.
    You now know how to select good metrics for classification tasks, pick the appropriate
    precision/recall trade-off, compare classifiers, and more generally build good
    classification systems for a variety of tasks. In the next chapters, you’ll learn
    how all these machine learning models you’ve been using actually work.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Try to build a classifier for the MNIST dataset that achieves over 97% accuracy
    on the test set. Hint: the `KNeighborsClassifier` works quite well for this task;
    you just need to find good hyperparameter values (try a grid search on the `weights`
    and `n_neighbors` hyperparameters).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a function that can shift an MNIST image in any direction (left, right,
    up, or down) by one pixel.⁠^([6](ch03.html#idm45720217595248)) Then, for each
    image in the training set, create four shifted copies (one per direction) and
    add them to the training set. Finally, train your best model on this expanded
    training set and measure its accuracy on the test set. You should observe that
    your model performs even better now! This technique of artificially growing the
    training set is called *data augmentation* or *training set expansion*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tackle the Titanic dataset. A great place to start is on [Kaggle](https://kaggle.com/c/titanic).
    Alternatively, you can download the data from [*https://homl.info/titanic.tgz*](https://homl.info/titanic.tgz)
    and unzip this tarball like you did for the housing data in [Chapter 2](ch02.html#project_chapter).
    This will give you two CSV files, *train.csv* and *test.csv*, which you can load
    using `pandas.read_csv()`. The goal is to train a classifier that can predict
    the `Survived` column based on the other columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build a spam classifier (a more challenging exercise):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download examples of spam and ham from [Apache SpamAssassin’s public datasets](https://homl.info/spamassassin).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the datasets and familiarize yourself with the data format.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into a training set and a test set.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a data preparation pipeline to convert each email into a feature vector.
    Your preparation pipeline should transform an email into a (sparse) vector that
    indicates the presence or absence of each possible word. For example, if all emails
    only ever contain four words, “Hello”, “how”, “are”, “you”, then the email “Hello
    you Hello Hello you” would be converted into a vector [1, 0, 0, 1] (meaning [“Hello”
    is present, “how” is absent, “are” is absent, “you” is present]), or [3, 0, 0,
    2] if you prefer to count the number of occurrences of each word.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You may want to add hyperparameters to your preparation pipeline to control
    whether or not to strip off email headers, convert each email to lowercase, remove
    punctuation, replace all URLs with “URL”, replace all numbers with “NUMBER”, or
    even perform *stemming* (i.e., trim off word endings; there are Python libraries
    available to do this).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, try out several classifiers and see if you can build a great spam classifier,
    with both high recall and high precision.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm45720235827040-marker)) By default Scikit-Learn caches downloaded
    datasets in a directory called *scikit_learn_data* in your home directory.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.html#idm45720239036336-marker)) Datasets returned by `fetch_openml()`
    are not always shuffled or split.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#idm45720238991712-marker)) Shuffling may be a bad idea in some
    contexts—for example, if you are working on time series data (such as stock market
    prices or weather conditions). We will explore this in [Chapter 15](ch15.html#rnn_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#idm45720237468928-marker)) Scikit-Learn classifiers always have
    either a `decision_function()` method or a `predict_proba()` method, or sometimes
    both.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.html#idm45720217874768-marker)) Scikit-Learn offers a few other averaging
    options and multilabel classifier metrics; see the documentation for more details.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch03.html#idm45720217595248-marker)) You can use the `shift()` function
    from the `scipy.ndimage.interpolation` module. For example, `shift(image, [2,
    1], cval=0)` shifts the image two pixels down and one pixel to the right.
  prefs: []
  type: TYPE_NORMAL
