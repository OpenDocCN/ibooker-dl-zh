<html><head></head><body><section data-pdf-bookmark="Chapter 2. RAG Part I: Indexing Your Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch02_rag_part_i_indexing_your_data_1736545662500927">&#13;
<h1><span class="label">Chapter 2. </span>RAG Part I: Indexing Your Data</h1>&#13;
&#13;
<p>In the previous chapter, you learned about the important building blocks used to create an LLM application using LangChain. You also built a simple AI chatbot consisting of a prompt sent to the model and the output generated by the model. But there are<a contenteditable="false" data-primary="chatbots" data-secondary="limitation of simple" data-type="indexterm" id="id450"/><a contenteditable="false" data-primary="data indexing" data-secondary="limitations of LLM's knowledge corpus" data-type="indexterm" id="id451"/> major limitations to this simple chatbot.</p>&#13;
&#13;
<p>What if your use case requires knowledge that the model wasn’t trained on? For example, let’s say you want to use AI to ask questions about a company, but the information is contained in a private PDF or other type of document. While we’ve seen model providers enriching their training datasets to include more and more of the world’s public information (no matter what format it is stored in), two<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="limitations of knowledge corpus" data-type="indexterm" id="id452"/> major limitations continue to exist in LLM’s knowledge corpus:</p>&#13;
&#13;
<dl>&#13;
	<dt>Private data</dt>&#13;
	<dd>&#13;
	<p>Information<a contenteditable="false" data-primary="private data" data-type="indexterm" id="id453"/> that isn’t publicly available is, by definition, not included in the training data of LLMs.</p>&#13;
	</dd>&#13;
	<dt>Current events</dt>&#13;
	<dd>&#13;
	<p>Training<a contenteditable="false" data-primary="current events" data-type="indexterm" id="id454"/> an LLM is a costly and time-consuming process that can span multiple years, with data-gathering being one of the first steps. This results in what is called the knowledge cutoff, or a date beyond which the LLM has no knowledge of real-world events; usually this would be the date the training set was finalized. This can be anywhere from a few months to a few years into the past, depending on the model in question.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>In either case, the model will most likely<a contenteditable="false" data-primary="hallucinations" data-type="indexterm" id="id455"/> hallucinate (find misleading or false information) and respond with<a contenteditable="false" data-primary="inaccuracies" data-type="indexterm" id="id456"/> inaccurate information. Adapting the prompt won’t resolve the issue either because it relies on the model’s current knowledge.</p>&#13;
&#13;
<section data-pdf-bookmark="The Goal: Picking Relevant Context for LLMs" data-type="sect1"><div class="sect1" id="ch02_the_goal_picking_relevant_context_for_llms_1736545662501195">&#13;
<h1>The Goal: Picking Relevant Context for LLMs</h1>&#13;
&#13;
<p>If<a contenteditable="false" data-primary="data indexing" data-secondary="relevant content for LLMs" data-type="indexterm" id="id457"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="relevant content for LLMs" data-type="indexterm" id="id458"/> the only private/current data you needed for your LLM use case was one to two pages of text, this chapter would be a lot shorter: all you’d need to make that information available to the LLM is to include that entire text in every single prompt you sent to the model.</p>&#13;
&#13;
<p>The challenge in making data available to LLMs is first and foremost a quantity problem. You have more information than can fit in each prompt you send to the LLM. Which small subset of your large collection of text do you include each time you call the model? Or in other words, how do you pick (with the aid of the model) which text is most relevant to answer each question?</p>&#13;
&#13;
<p>In this chapter and the next, you’ll learn how to overcome this challenge in two steps:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p><em>Indexing</em> your documents, that is, preprocessing them in a way where your application can easily find the most relevant ones for each question</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><em>Retrieving </em>this<a contenteditable="false" data-primary="external data, retrieving" data-type="indexterm" id="id459"/> external data from the index and using it as<a contenteditable="false" data-primary="context" data-secondary="accurate output with" data-type="indexterm" id="id460"/> <em>context</em> for the LLM to generate an<a contenteditable="false" data-primary="output" data-secondary="accurate" data-type="indexterm" id="id461"/> accurate output based on your data</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>This chapter focuses on indexing, the first step, which involves<a contenteditable="false" data-primary="preprocessing" data-type="indexterm" id="id462"/><a contenteditable="false" data-primary="documents" data-secondary="preprocessing" data-type="indexterm" id="id463"/> preprocessing your documents into a format that can be understood and searched with LLMs. This technique is called<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="definition of term" data-type="indexterm" id="id464"/> <em>retrieval-augmented generation</em> (RAG). But before we begin, let’s discuss <em>why </em>your documents require preprocessing.</p>&#13;
&#13;
<p>Let’s assume you would like to use LLMs to analyze the financial performance and risks in <a href="https://oreil.ly/Bp51E">Tesla’s 2022 annual report</a>, which is stored as text in PDF format. Your goal is to be able to ask a question like “What key risks did Tesla face in 2022?” and get a humanlike response based on context from the risk factors section of the document.</p>&#13;
&#13;
<p>Breaking it down, there are four key steps (shown in <a data-type="xref" href="#ch02_figure_1_1736545662484110">Figure 2-1</a>) that you’d need to take in order to achieve this goal:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Extract the text from the document.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Split the text into manageable chunks.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Convert the text into numbers that computers can understand.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Store these number representations of your text somewhere that makes it easy and fast to retrieve the relevant sections of your document to answer a given question.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<figure><div class="figure" id="ch02_figure_1_1736545662484110"><img alt="A diagram of a diagram  Description automatically generated" src="assets/lelc_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>Four key steps to preprocess your documents for LLM usage</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#ch02_figure_1_1736545662484110">Figure 2-1</a> illustrates the flow of this preprocessing and transformation of your documents, a process known as<a contenteditable="false" data-primary="ingestions" data-type="indexterm" id="id465"/> ingestion. <em>Ingestion </em>is simply the process of converting your documents into numbers that computers can understand and analyze, and storing them in a special type of database for efficient retrieval. These numbers are formally known as<a contenteditable="false" data-primary="embeddings" data-secondary="definition of term" data-type="indexterm" id="id466"/> <em>embeddings</em>, and this special type of database is known as a<a contenteditable="false" data-primary="vector stores" data-secondary="definition of term" data-type="indexterm" id="id467"/> <em>vector store. </em>Let’s look a little more closely at what embeddings are and why they’re important, starting with something simpler than LLM-powered embeddings.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Embeddings: Converting Text to Numbers" data-type="sect1"><div class="sect1" id="ch02_embeddings_converting_text_to_numbers_1736545662501284">&#13;
<h1>Embeddings: Converting Text to Numbers</h1>&#13;
&#13;
<p><em>Embedding<a contenteditable="false" data-primary="data indexing" data-secondary="embeddings" data-type="indexterm" id="DIembed02"/></em><a contenteditable="false" data-primary="text" data-secondary="converting text to numbers" data-type="indexterm" id="Tnumbers02"/> refers to representing text as a (long) sequence of numbers. This is a lossy representation—that is, you can’t recover the original text from these number sequences, so you usually store both the original text and this numeric representation.</p>&#13;
&#13;
<p>So, why bother? Because<a contenteditable="false" data-primary="embeddings" data-secondary="benefits of" data-type="indexterm" id="id468"/> you gain the flexibility and power that comes with working with numbers: you can do math on words! Let’s see why that’s exciting.</p>&#13;
&#13;
<section data-pdf-bookmark="Embeddings Before LLMs" data-type="sect2"><div class="sect2" id="ch02_embeddings_before_llms_1736545662501402">&#13;
<h2>Embeddings Before LLMs</h2>&#13;
&#13;
<p>Long<a contenteditable="false" data-primary="embeddings" data-secondary="uses for" data-type="indexterm" id="id469"/> before LLMs, computer scientists were using embeddings—for instance, to enable full-text search capabilities in websites or to classify emails as spam. Let’s see an example:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Take these three sentences:</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>What a sunny day.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Such bright skies today.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>I haven’t seen a sunny day in weeks.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>List all unique words in them: <em>what</em>, <em>a</em>, <em>sunny</em>, <em>day</em>, <em>such</em>, <em>bright</em>, and so on.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>For each sentence, go word by word and assign the number 0 if not present, 1 if used once in the sentence, 2 if present twice, and so on.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p><a data-type="xref" href="#ch02_table_1_1736545662489691">Table 2-1</a> shows the result.</p>&#13;
&#13;
<table class="striped" id="ch02_table_1_1736545662489691">&#13;
	<caption><span class="label">Table 2-1. </span>Word embeddings for three sentences</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th>Word</th>&#13;
			<th>What a sunny day.</th>&#13;
			<th>Such bright skies today.</th>&#13;
			<th>I haven’t seen a sunny day in weeks.</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><em>what</em></td>&#13;
			<td>1</td>&#13;
			<td>0</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>a</em></td>&#13;
			<td>1</td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>sunny</em></td>&#13;
			<td>1</td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>day</em></td>&#13;
			<td>1</td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>such</em></td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>bright</em></td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>skies</em></td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>today</em></td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>I</em></td>&#13;
			<td>0</td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>haven’t</em></td>&#13;
			<td>0</td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>seen</em></td>&#13;
			<td>0</td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>in</em></td>&#13;
			<td>0</td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><em>weeks</em></td>&#13;
			<td>0</td>&#13;
			<td>0</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>In this model, the embedding for <em>I haven’t seen a sunny day in weeks</em> is the sequence of numbers <em>0 1 1 1 0 0 0 0 1 1 1 1 1</em>. This is called the<a contenteditable="false" data-primary="bag-of-words model" data-type="indexterm" id="id470"/><a contenteditable="false" data-primary="embeddings" data-secondary="bag-of-words model" data-type="indexterm" id="id471"/> <em>bag-of-words</em> model, and these embeddings are also called<a contenteditable="false" data-primary="sparse embeddings" data-type="indexterm" id="id472"/> <em>sparse embeddings</em> (or sparse vectors—<em>vector</em> is another word for a sequence of numbers), because a lot of the numbers will be 0. Most English sentences use only a very small subset of all existing English words.</p>&#13;
&#13;
<p>You can successfully use this model for:</p>&#13;
&#13;
<dl>&#13;
	<dt>Keyword search</dt>&#13;
	<dd>&#13;
	<p>You<a contenteditable="false" data-primary="keyword searches" data-type="indexterm" id="id473"/> can find which documents contain a given word or words.</p>&#13;
	</dd>&#13;
	<dt>Classification of documents</dt>&#13;
	<dd>&#13;
	<p>You<a contenteditable="false" data-primary="documents" data-secondary="classifying" data-type="indexterm" id="id474"/><a contenteditable="false" data-primary="classification problems" data-type="indexterm" id="id475"/> can calculate embeddings for a collection of examples previously labeled as email spam or not spam, average them out, and obtain average word frequencies for each of the classes (spam or not spam). Then, each new document is compared to those averages and classified accordingly.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>The limitation here is that the model has no awareness of meaning, only of the actual words used. For instance, the embeddings for <em>sunny day</em> and <em>bright skies</em> look very different. In fact they have no words in common, even though we know they have similar meaning. Or, in the email classification problem, a would-be spammer can trick the filter by replacing common “spam words” with their synonyms.</p>&#13;
&#13;
<p>In the next section, we’ll see how semantic embeddings address this limitation by using numbers to represent the meaning of the text, instead of the exact words found in the text.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="LLM-Based Embeddings" data-type="sect2"><div class="sect2" id="ch02_llm_based_embeddings_1736545662501472">&#13;
<h2>LLM-Based Embeddings</h2>&#13;
&#13;
<p>We’re<a contenteditable="false" data-primary="embeddings" data-secondary="LLM-based embeddings" data-type="indexterm" id="id476"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="LLM-based embeddings" data-type="indexterm" id="id477"/> going to skip over all the ML developments that came in between and jump straight to LLM-based embeddings. Just know that there was a gradual evolution from the simple method outlined in the previous section to the sophisticated method described in this one.</p>&#13;
&#13;
<p>You can think of embedding models as an offshoot from the training process of LLMs. If you remember from the <a data-type="xref" href="preface01.html#pr01_preface_1736545679069216">Preface</a>, the LLM training process (learning from vast amounts of written text) enables LLMs to complete a prompt (or input) with the most appropriate continuation (output). This capability stems from an understanding of the meaning of words and sentences in the context of the surrounding text, learned from how words are used together in the training texts. This <em>understanding</em> of the meaning (or semantics) of the prompt can be extracted as a numeric representation (or embedding) of the input text, and can be used directly for some very interesting use cases too.</p>&#13;
&#13;
<p>In practice, most embedding models are trained for that purpose alone, following somewhat similar architectures and training processes as LLMs, as that is more efficient and results in higher-quality embeddings.<sup><a data-type="noteref" href="ch02.html#id478" id="id478-marker">1</a></sup></p>&#13;
&#13;
<p>An<a contenteditable="false" data-primary="embedding models" data-type="indexterm" id="id479"/> <em>embedding model</em> then is an algorithm that takes a piece of text and outputs a numerical representation of its meaning—technically, a long list of floating-point (decimal) numbers, usually somewhere between 100 and 2,000 numbers, or<a contenteditable="false" data-primary="dimensions" data-type="indexterm" id="id480"/> <em>dimensions</em>. These are also called<a contenteditable="false" data-primary="dense embeddings" data-type="indexterm" id="id481"/> <em>dense</em> embeddings, as opposed to the <em>sparse</em> embeddings of the previous section, as here usually all dimensions are different from 0.</p>&#13;
&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Different models produce different numbers and different sizes of lists. All of these are specific to each model; that is, even if the size of the lists matches, you cannot compare embeddings from different models. Combining embeddings from different models should always be avoided.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Semantic Embeddings Explained" data-type="sect2"><div class="sect2" id="ch02_semantic_embeddings_explained_1736545662501549">&#13;
<h2>Semantic Embeddings Explained</h2>&#13;
&#13;
<p>Consider<a contenteditable="false" data-primary="embeddings" data-secondary="semantic embeddings" data-type="indexterm" id="Esemantic02"/><a contenteditable="false" data-primary="semantic embeddings" data-type="indexterm" id="semembed02"/> these three words: <em>lion</em>, <em>pet</em>, and <em>dog</em>. Intuitively, which pair of these words share similar characteristics to each other at first glance? The obvious answer is <em>pet</em> and <em>dog</em>. But computers do not have the ability to tap into this intuition or nuanced understanding of the English language. In order for a computer to differentiate between a lion, pet, or dog, you need to be able to translate them into the language of computers, which is numbers<em>.</em></p>&#13;
&#13;
<p><a data-type="xref" href="#ch02_figure_2_1736545662484146">Figure 2-2</a> illustrates converting each word into hypothetical number representations that retain their meaning.</p>&#13;
&#13;
<figure><div class="figure" id="ch02_figure_2_1736545662484146"><img alt="A diagram of a diagram of circles  Description automatically generated with medium confidence" src="assets/lelc_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>Semantic representations of words</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#ch02_figure_2_1736545662484146">Figure 2-2</a> shows each word alongside its corresponding semantic embedding. Note that the numbers themselves have no particular meaning, but instead the sequences of numbers for two words (or sentences) that are close in meaning should be <em>closer </em>than those of unrelated words. As you can see, each number is a<a contenteditable="false" data-primary="floating-point values" data-type="indexterm" id="id482"/> <em>floating-point value</em>, and each of them represents a semantic <em>dimension</em>. Let’s see what we mean by <em>closer</em>:</p>&#13;
&#13;
<p>If we plot these vectors in a three-dimensional space, it could look like <a data-type="xref" href="#ch02_figure_3_1736545662484170">Figure 2-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch02_figure_3_1736545662484170"><img alt="A diagram of a dog and a dog  Description automatically generated" src="assets/lelc_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>Plot of word vectors in a multidimensional space</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#ch02_figure_3_1736545662484170">Figure 2-3</a> shows the <em>pet</em> and <em>dog</em> vectors are closer to each other in distance than the <em>lion</em> plot. We can also observe that the angles between each plot varies depending on how similar they are. For example, the words <em>pet</em> and <em>lion</em> have a wider angle between one another than the <em>pet</em> and <em>dog</em> do, indicating more similarities shared by the latter word pairs. The narrower the angle or shorter the distance between two vectors, the closer their similarities.</p>&#13;
&#13;
<p>One effective way to calculate the degree of similarity between two vectors in a multidimensional space is called<a contenteditable="false" data-primary="cosine similarity" data-type="indexterm" id="id483"/> cosine similarity. <em>Cosine similarity</em> computes the dot product of vectors and divides it by the product of their magnitudes to output a number between –1 and 1, where 0 means the vectors share no correlation, –1 means they are absolutely dissimilar, and 1 means they are absolutely similar. So, in the case of our three words here, the cosine similarity between <em>pet</em> and <em>dog</em> could be 0.75, but between <em>pet</em> and <em>lion</em> it might be 0.1.</p>&#13;
&#13;
<p>The ability to convert sentences into embeddings<em> </em>that capture semantic meaning and then perform calculations to find semantic similarities between different sentences enables us to get an LLM to find the most relevant documents to answer questions about a large body of text like our Tesla PDF document. Now that you understand the big picture, let’s revisit the first step (indexing) of preprocessing your document.<a contenteditable="false" data-primary="" data-startref="Esemantic02" data-type="indexterm" id="id484"/><a contenteditable="false" data-primary="" data-startref="semembed02" data-type="indexterm" id="id485"/><a contenteditable="false" data-primary="" data-startref="Tnumbers02" data-type="indexterm" id="id486"/></p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch02_other_uses_for_embeddings_1736545662501623">&#13;
<h1>Other Uses for Embeddings</h1>&#13;
&#13;
<p>These<a contenteditable="false" data-primary="embeddings" data-secondary="uses for" data-type="indexterm" id="id487"/> sequences of numbers and vectors have a number of interesting properties:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>As you learned earlier, if you think of a vector as describing a point in high-dimensional space, points that are closer together have more similar meanings, so a distance function can be used to measure similarity.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Groups of points close together can be said to be related; therefore, a clustering algorithm can be used to identify topics (or clusters of points) and classify new inputs into one of those topics.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>If you average out multiple embeddings, the average embedding can be said to represent the overall meaning of that group; that is, you can embed a long document (for instance, this book) by:</p>&#13;
&#13;
	<ol>&#13;
		<li>&#13;
		<p>Embedding each page separately</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Taking the average of the embeddings of all pages as the book embedding</p>&#13;
		</li>&#13;
	</ol>&#13;
	</li>&#13;
	<li>&#13;
	<p>You can “travel” the “meaning” space by using the elementary math operations of addition and subtraction: for instance, the operation <em>king – man + woman = queen. </em>If you take the meaning (or semantic embedding) of <em>king</em>, subtract the meaning of <em>man</em>, presumably you arrive at the more abstract meaning of <em>monarch</em>, at which point, if you add the meaning of <em>woman</em>, you’ve arrived close to the meaning (or embedding) of the word <em>queen</em>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>There are models that can produce embeddings for nontext content, for instance, images, videos, and sounds, in addition to text. This enables, for instance, finding images that are most similar or relevant for a given sentence.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before less_space">We won’t explore all of these attributes in this book, but it’s useful to know they can be used for a number of <a href="https://oreil.ly/PU2C8">applications</a> such as:</p>&#13;
&#13;
<dl>&#13;
	<dt>Search</dt>&#13;
	<dd>&#13;
	<p>Finding the most relevant documents for a new query</p>&#13;
	</dd>&#13;
	<dt>Clustering</dt>&#13;
	<dd>&#13;
	<p>Given a body of documents, dividing them into groups (for instance, topics)</p>&#13;
	</dd>&#13;
	<dt>Classification</dt>&#13;
	<dd>&#13;
	<p>Assigning a new document to a previously identified group or label (for instance, a topic)</p>&#13;
	</dd>&#13;
	<dt>Recommendation</dt>&#13;
	<dd>&#13;
	<p>Given a document, surfacing similar documents</p>&#13;
	</dd>&#13;
	<dt>Detecting anomalies</dt>&#13;
	<dd>&#13;
	<p>Identifying documents that are very dissimilar from previously seen ones</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>We hope this leaves you with some intuition that embeddings are quite versatile and can be put to good use in your future projects.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Converting Your Documents into Text" data-type="sect1"><div class="sect1" id="ch02_converting_your_documents_into_text_1736545662501695">&#13;
<h1>Converting Your Documents into Text</h1>&#13;
&#13;
<p>As<a contenteditable="false" data-primary="" data-startref="DIembed02" data-type="indexterm" id="id488"/><a contenteditable="false" data-primary="data indexing" data-secondary="converting documents into text" data-type="indexterm" id="DItext02"/><a contenteditable="false" data-primary="text" data-secondary="converting documents into text" data-type="indexterm" id="Tdoctotext02"/> mentioned at the beginning of the chapter, the first step in preprocessing your document is to convert it to text. In order to achieve this, you would need to build logic to parse and extract the document with minimal loss of quality. Fortunately, LangChain provides<a contenteditable="false" data-primary="document loaders" data-type="indexterm" id="id489"/> <em>document loaders </em>that handle the parsing logic and enable you to “load” data from various sources into a <code>Document</code> class that consists of text and associated metadata.</p>&#13;
&#13;
<p>For example, consider a simple <em>.txt</em> file. You can simply import a LangChain <span class="keep-together"><code>TextLoader</code></span> class to extract the text, like this:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">TextLoader</code>&#13;
&#13;
<code class="n">loader</code> <code class="o">=</code> <code class="n">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">)</code>&#13;
<code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">TextLoader</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/document_loaders/fs/text"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">loader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">);</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code></pre>&#13;
&#13;
<p class="pagebreak-before less_space"><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
[Document(page_content='text content \n', metadata={'line_number': 0, 'source': &#13;
    './test.txt'})]</pre>&#13;
&#13;
<p>The previous code block assumes that you have a file named <code>test.txt</code> in your current directory. Usage of all LangChain document loaders follows a similar pattern:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Start by picking the loader for your type of document from the long list of <a href="https://oreil.ly/iLJ33">integrations</a>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Create an instance of the loader in question, along with any parameters to configure it, including the location of your documents (usually a filesystem path or web address).</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Load the documents by calling <code>load()</code>, which returns a list of documents ready to pass to the next stage (more on that soon).</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>Aside from <em>.txt</em> files, LangChain provides document loaders for other popular file types including <em>.csv</em>, <em>.json</em>, and Markdown, alongside integrations with popular platforms such as Slack and Notion.</p>&#13;
&#13;
<p>For example, you can use <code>WebBaseLoader</code> to load HTML from web URLs and parse it to text.</p>&#13;
&#13;
<p>Install the beautifulsoup4 package:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
pip install beautifulsoup4</pre>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="document loaders and" data-type="indexterm" id="id490"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">WebBaseLoader</code>&#13;
&#13;
<code class="n">loader</code> <code class="o">=</code> <code class="n">WebBaseLoader</code><code class="p">(</code><code class="s2">"https://www.langchain.com/"</code><code class="p">)</code>&#13;
<code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="document loaders and" data-type="indexterm" id="id491"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="c1">// install cheerio: npm install cheerio</code>&#13;
<code class="kr">import</code> <code class="p">{</code> &#13;
  <code class="nx">CheerioWebBaseLoader</code> &#13;
<code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/community/document_loaders/web/cheerio"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">loader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">CheerioWebBaseLoader</code><code class="p">(</code><code class="s2">"https://www.langchain.com/"</code><code class="p">);</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code></pre>&#13;
&#13;
<p class="pagebreak-before less_space">In the case of our Tesla PDF use case, we can utilize LangChain’s <code>PDFLoader</code> to extract text from the PDF document:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="text extraction with" data-type="indexterm" id="id492"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># install the pdf parsing library</code>&#13;
<code class="c1"># pip install pypdf</code>&#13;
&#13;
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">PyPDFLoader</code>&#13;
&#13;
<code class="n">loader</code> <code class="o">=</code> <code class="n">PyPDFLoader</code><code class="p">(</code><code class="s2">"./test.pdf"</code><code class="p">)</code>&#13;
<code class="n">pages</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="text extraction with" data-type="indexterm" id="id493"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="c1">// install the pdf parsing library: npm install pdf-parse</code>&#13;
&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PDFLoader</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/document_loaders/fs/pdf"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">loader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">PDFLoader</code><code class="p">(</code><code class="s2">"./test.pdf"</code><code class="p">);</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code></pre>&#13;
&#13;
<p>The text has been extracted from the PDF document and stored in the <code>Document</code> class. But there’s a problem. The loaded document is over 100,000 characters long, so it won’t fit into the<a contenteditable="false" data-primary="context windows" data-type="indexterm" id="id494"/> context window of the vast majority of LLMs or embedding models. In order to overcome this limitation, we need to split the <code>Document</code> into manageable chunks of text that we can later convert into embeddings and semantically search, bringing us to the second step (retrieving).</p>&#13;
&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>LLMs and embedding models are designed with a hard limit on the size of input and output tokens they can handle. This limit is usually called <em>context window</em>, and usually applies to the combination of input and output; that is, if the context window is 100 (we’ll talk about units in a second), and your input measures 90, the output can be at most of length 10. Context windows are usually measured in number of tokens, for instance 8,192 tokens. Tokens, as mentioned in the <a data-type="xref" href="preface01.html#pr01_preface_1736545679069216">Preface</a>, are a representation of text as numbers, with each token usually covering between three and four characters of English text.<a contenteditable="false" data-primary="" data-startref="DItext02" data-type="indexterm" id="id495"/><a contenteditable="false" data-primary="" data-startref="Tdoctotext02" data-type="indexterm" id="id496"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Splitting Your Text into Chunks" data-type="sect1"><div class="sect1" id="ch02_splitting_your_text_into_chunks_1736545662501763">&#13;
<h1>Splitting Your Text into Chunks</h1>&#13;
&#13;
<p>At<a contenteditable="false" data-primary="text" data-secondary="splitting text into chunks" data-type="indexterm" id="Tsplit02"/><a contenteditable="false" data-primary="data indexing" data-secondary="splitting text into chunks" data-type="indexterm" id="DIsplit02"/> first glance it may seem straightforward to split a large body of text into chunks, but keeping<a contenteditable="false" data-primary="semantic relationships" data-type="indexterm" id="id497"/> <em>semantically</em> related (related by meaning) chunks of text together is a complex process. To make it easier to split large documents into small, but still meaningful, pieces of text, LangChain provides <code>RecursiveCharacterTextSplitter</code>, which does the following:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Take a list of separators, in order of importance. By default these are:</p>&#13;
&#13;
	<ol>&#13;
		<li>&#13;
		<p>The paragraph separator: <code>\n\n</code></p>&#13;
		</li>&#13;
		<li>&#13;
		<p>The line separator: <code>\n</code></p>&#13;
		</li>&#13;
		<li>&#13;
		<p>The word separator: space character</p>&#13;
		</li>&#13;
	</ol>&#13;
	</li>&#13;
	<li>&#13;
	<p>To respect the given chunk size, for instance, 1,000 characters, start by splitting up paragraphs.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>For any paragraph longer than the desired chunk size, split by the next separator: lines. Continue until all chunks are smaller than the desired length, or there are no additional separators to try.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Emit each chunk as a <code>Document</code>, with the metadata of the original document passed in and additional information about the position in the original <span class="keep-together">document.</span></p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>Let’s see an example:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="text, splitting into chunks" data-type="indexterm" id="Pchunk02"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>&#13;
&#13;
<code class="n">loader</code> <code class="o">=</code> <code class="n">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">)</code> <code class="c1"># or any other loader</code>&#13;
<code class="n">docs</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code>&#13;
&#13;
<code class="n">splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="p">(</code>&#13;
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code>&#13;
    <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
<code class="n">splitted_docs</code> <code class="o">=</code> <code class="n">splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">docs</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="text, splitting into chunks" data-type="indexterm" id="JSchunk02"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">TextLoader</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/document_loaders/fs/text"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RecursiveCharacterTextSplitter</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/textsplitters"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">loader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">);</code> <code class="c1">// or any other loader </code>&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">splitter</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">RecursiveCharacterTextSplitter</code><code class="p">({</code>&#13;
  <code class="nx">chunkSize</code><code class="o">:</code> <code class="mi">1000</code><code class="p">,</code>&#13;
  <code class="nx">chunkOverlap</code><code class="o">:</code> <code class="mi">200</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">splittedDocs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">splitter</code><code class="p">.</code><code class="nx">splitDocuments</code><code class="p">(</code><code class="nx">docs</code><code class="p">)</code></pre>&#13;
&#13;
<p>In the preceding code, the documents created by the document loader are split into chunks of 1,000 characters each, with some overlap between chunks of 200 characters to maintain some context. The result is also a list of documents, where each document is up to 1,000 characters in length, split along the natural divisions of written text—paragraphs, new lines and finally, words. This uses the structure of the text to keep each chunk a consistent, readable snippet of text.</p>&#13;
&#13;
<p><code>RecursiveCharacterTextSplitter</code> can also be used to split code languages and Markdown into semantic chunks. This is done by using keywords specific to each language as the separators, which ensures, for instance, the body of each function is kept in the same chunk, instead of split between several. Usually, as programming languages have more structure than written text, there’s less need to use overlap between the chunks. LangChain contains separators for a number of popular languages, such as Python, JS, Markdown, HTML, and many more. Here’s an example:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="p">(</code>&#13;
    <code class="n">Language</code><code class="p">,</code>&#13;
    <code class="n">RecursiveCharacterTextSplitter</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">PYTHON_CODE</code> <code class="o">=</code> <code class="s2">"""</code>&#13;
<code class="s2">def hello_world():</code>&#13;
<code class="s2">    print("Hello, World!")</code>&#13;
&#13;
<code class="s2"># Call the function</code>&#13;
<code class="s2">hello_world()</code>&#13;
<code class="s2">"""</code>&#13;
<code class="n">python_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="o">.</code><code class="n">from_language</code><code class="p">(</code>&#13;
    <code class="n">language</code><code class="o">=</code><code class="n">Language</code><code class="o">.</code><code class="n">PYTHON</code><code class="p">,</code> <code class="n">chunk_size</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">0</code>&#13;
<code class="p">)</code>&#13;
<code class="n">python_docs</code> <code class="o">=</code> <code class="n">python_splitter</code><code class="o">.</code><code class="n">create_documents</code><code class="p">([</code><code class="n">PYTHON_CODE</code><code class="p">])</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RecursiveCharacterTextSplitter</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/textsplitters"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">PYTHON_CODE</code> <code class="o">=</code> <code class="sb">`</code>&#13;
<code class="sb">def hello_world():</code>&#13;
<code class="sb">  print("Hello, World!")</code>&#13;
&#13;
<code class="sb"># Call the function</code>&#13;
<code class="sb">hello_world()</code>&#13;
<code class="sb">`</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">pythonSplitter</code> <code class="o">=</code> <code class="nx">RecursiveCharacterTextSplitter</code><code class="p">.</code><code class="nx">fromLanguage</code><code class="p">(</code><code class="s2">"python"</code><code class="p">,</code> <code class="p">{</code>&#13;
  <code class="nx">chunkSize</code><code class="o">:</code> <code class="mi">50</code><code class="p">,</code>&#13;
  <code class="nx">chunkOverlap</code><code class="o">:</code> <code class="mi">0</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
<code class="kr">const</code> <code class="nx">pythonDocs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">pythonSplitter</code><code class="p">.</code><code class="nx">createDocuments</code><code class="p">([</code><code class="nx">PYTHON_CODE</code><code class="p">]);</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
[Document(page_content='def hello_world():\n    print("Hello, World!")'),&#13;
    Document(page_content='# Call the function\nhello_world()')]</pre>&#13;
&#13;
<p>Notice how we’re still using <code>RecursiveCharacterTextSplitter</code> as before, but now we’re creating an instance of it for a specific language, using the <code>from_language</code> method. This one accepts the name of the language, and the usual parameters for chunk size, and so on. Also notice we are now using the method <code>create_documents</code>, which accepts a list of strings, rather than the list of documents we had before. This method is useful when the text you want to split doesn’t come from a document loader, so you have only the raw text strings.</p>&#13;
&#13;
<p>You can also use the optional second argument to <code>create_documents</code> in order to pass a list of metadata to associate with each text string. This metadata list should have the same length as the list of strings and will be used to populate the metadata field of each <code>Document</code> returned.</p>&#13;
&#13;
<p>Let’s see an example for Markdown text, using the metadata argument as well:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">markdown_text</code> <code class="o">=</code> <code class="s2">"""</code>&#13;
<code class="s2"># LangChain</code>&#13;
&#13;
<code class="s2">⚡ Building applications with LLMs through composability ⚡</code>&#13;
&#13;
<code class="s2">## Quick Install</code>&#13;
&#13;
<code class="s2">```bash</code>&#13;
<code class="s2">pip install langchain</code>&#13;
<code class="s2">```</code>&#13;
&#13;
<code class="s2">As an open source project in a rapidly developing field, we are extremely open </code>&#13;
<code class="s2">    to contributions.</code>&#13;
<code class="s2">"""</code>&#13;
&#13;
<code class="n">md_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="o">.</code><code class="n">from_language</code><code class="p">(</code>&#13;
    <code class="n">language</code><code class="o">=</code><code class="n">Language</code><code class="o">.</code><code class="n">MARKDOWN</code><code class="p">,</code> <code class="n">chunk_size</code><code class="o">=</code><code class="mi">60</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">0</code>&#13;
<code class="p">)</code>&#13;
<code class="n">md_docs</code> <code class="o">=</code> <code class="n">md_splitter</code><code class="o">.</code><code class="n">create_documents</code><code class="p">([</code><code class="n">markdown_text</code><code class="p">],</code> &#13;
    <code class="p">[{</code><code class="s2">"source"</code><code class="p">:</code> <code class="s2">"https://www.langchain.com"</code><code class="p">}])</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">markdownText</code> <code class="o">=</code> <code class="sb">`</code>&#13;
<code class="sb"># LangChain</code>&#13;
&#13;
<code class="sb">⚡ Building applications with LLMs through composability ⚡</code>&#13;
&#13;
<code class="sb">## Quick Install</code>&#13;
&#13;
<code class="sb">\`\`\`bash</code>&#13;
<code class="sb">pip install langchain</code>&#13;
<code class="sb">\`\`\`</code>&#13;
&#13;
<code class="sb">As an open source project in a rapidly developing field, we are extremely </code>&#13;
<code class="sb">  open to contributions.</code>&#13;
<code class="sb">`</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">mdSplitter</code> <code class="o">=</code> <code class="nx">RecursiveCharacterTextSplitter</code><code class="p">.</code><code class="nx">fromLanguage</code><code class="p">(</code><code class="s2">"markdown"</code><code class="p">,</code> <code class="p">{</code>&#13;
  <code class="nx">chunkSize</code><code class="o">:</code> <code class="mi">60</code><code class="p">,</code>&#13;
  <code class="nx">chunkOverlap</code><code class="o">:</code> <code class="mi">0</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
<code class="kr">const</code> <code class="nx">mdDocs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">mdSplitter</code><code class="p">.</code><code class="nx">createDocuments</code><code class="p">([</code><code class="nx">markdownText</code><code class="p">],</code> &#13;
  <code class="p">[{</code><code class="s2">"source"</code><code class="o">:</code> <code class="s2">"https://www.langchain.com"</code><code class="p">}]);</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
[Document(page_content='# LangChain', &#13;
    metadata={"source": "https://www.langchain.com"}),&#13;
 Document(page_content='⚡ Building applications with LLMs through composability &#13;
    ⚡', metadata={"source": "https://www.langchain.com"}),&#13;
 Document(page_content='## Quick Install\n\n```bash', &#13;
    metadata={"source": "https://www.langchain.com"}),&#13;
 Document(page_content='pip install langchain', &#13;
    metadata={"source": "https://www.langchain.com"}),&#13;
 Document(page_content='```', metadata={"source": "https://www.langchain.com"}),&#13;
 Document(page_content='As an open source project in a rapidly developing field, &#13;
    we', metadata={"source": "https://www.langchain.com"}),&#13;
 Document(page_content='are extremely open to contributions.', &#13;
    metadata={"source": "https://www.langchain.com"})]</pre>&#13;
&#13;
<p>Notice two things:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The text is split along the natural stopping points in the Markdown document; for instance, the heading goes into one chunk, the line of text under it in a separate chunk, and so on.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The metadata we passed in the second argument is attached to each resulting document, which allows you to track, for instance, where the document came from and where you can go to see the original.<a contenteditable="false" data-primary="" data-startref="JSchunk02" data-type="indexterm" id="id498"/><a contenteditable="false" data-primary="" data-startref="Pchunk02" data-type="indexterm" id="id499"/><a contenteditable="false" data-primary="" data-startref="DIsplit02" data-type="indexterm" id="id500"/><a contenteditable="false" data-primary="" data-startref="Tsplit02" data-type="indexterm" id="id501"/></p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Generating Text Embeddings" data-type="sect1"><div class="sect1" id="ch02_generating_text_embeddings_1736545662501826">&#13;
<h1>Generating Text Embeddings</h1>&#13;
&#13;
<p>LangChain<a contenteditable="false" data-primary="data indexing" data-secondary="generating text embeddings" data-type="indexterm" id="DItextembed02"/><a contenteditable="false" data-primary="text" data-secondary="generating text embeddings" data-type="indexterm" id="Tembed02"/><a contenteditable="false" data-primary="embeddings" data-secondary="generating text embeddings" data-type="indexterm" id="Etext02"/> also has an <code>Embeddings</code> class designed to interface with text embedding models—including<a contenteditable="false" data-primary="OpenAI API" data-secondary="integration with LangChain" data-type="indexterm" id="id502"/> OpenAI,<a contenteditable="false" data-primary="Cohere" data-type="indexterm" id="id503"/> Cohere, and<a contenteditable="false" data-primary="Hugging Face" data-type="indexterm" id="id504"/> Hugging Face—and generate vector representations of text. This class provides two methods: one for embedding documents and one for embedding a query. The former takes a list of text strings as input, while the latter takes a single text string.</p>&#13;
&#13;
<p class="pagebreak-before less_space">Here’s an example of embedding a document using <a href="https://oreil.ly/9tnzQ">OpenAI’s embedding model</a>:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="text embeddings with" data-type="indexterm" id="id505"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>&#13;
&#13;
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">embed_documents</code><code class="p">([</code>&#13;
    <code class="s2">"Hi there!"</code><code class="p">,</code>&#13;
    <code class="s2">"Oh, hello!"</code><code class="p">,</code>&#13;
    <code class="s2">"What's your name?"</code><code class="p">,</code>&#13;
    <code class="s2">"My friends call me World"</code><code class="p">,</code>&#13;
    <code class="s2">"Hello World!"</code>&#13;
<code class="p">])</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="text embeddings with" data-type="indexterm" id="id506"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAIEmbeddings</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">();</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">embeddings</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">embeddings</code><code class="p">.</code><code class="nx">embedDocuments</code><code class="p">([</code>&#13;
  <code class="s2">"Hi there!"</code><code class="p">,</code>&#13;
  <code class="s2">"Oh, hello!"</code><code class="p">,</code>&#13;
  <code class="s2">"What' s your name?"</code><code class="p">,</code>&#13;
  <code class="s2">"My friends call me World"</code><code class="p">,</code>&#13;
  <code class="s2">"Hello World!"</code>&#13;
<code class="p">]);</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
[&#13;
  [&#13;
    -0.004845875, 0.004899438, -0.016358767, -0.024475135, -0.017341806,&#13;
      0.012571548, -0.019156644, 0.009036391, -0.010227379, -0.026945334,&#13;
      0.022861943, 0.010321903, -0.023479493, -0.0066544134, 0.007977734,&#13;
    0.0026371893, 0.025206111, -0.012048521, 0.012943339, 0.013094575,&#13;
    -0.010580265, -0.003509951, 0.004070787, 0.008639394, -0.020631202,&#13;
    ... 1511 more items&#13;
  ]&#13;
  [&#13;
      -0.009446913, -0.013253193, 0.013174579, 0.0057552797, -0.038993083,&#13;
      0.0077763423, -0.0260478, -0.0114384955, -0.0022683728, -0.016509168,&#13;
      0.041797023, 0.01787183, 0.00552271, -0.0049789557, 0.018146982,&#13;
      -0.01542166, 0.033752076, 0.006112323, 0.023872782, -0.016535373,&#13;
      -0.006623321, 0.016116094, -0.0061090477, -0.0044155475, -0.016627092,&#13;
    ... 1511 more items&#13;
  ]&#13;
  ... 3 more items&#13;
]</pre>&#13;
&#13;
<p class="pagebreak-before less_space">Notice that you can embed multiple documents at the same time; you should prefer this to embedding them one at a time, as it will be more efficient (due to how these models are constructed). You get back a list containing multiple lists of numbers—each inner list is a vector or embedding, as explained in an earlier section.<a contenteditable="false" data-primary="" data-startref="DItextembed02" data-type="indexterm" id="id507"/><a contenteditable="false" data-primary="" data-startref="Tembed02" data-type="indexterm" id="id508"/><a contenteditable="false" data-primary="" data-startref="Etext02" data-type="indexterm" id="id509"/></p>&#13;
&#13;
<p>Now <a contenteditable="false" data-primary="data indexing" data-secondary="end-to-end example" data-type="indexterm" id="DIend02"/>let’s see an end-to-end example using the three capabilities we’ve seen so far:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Document loaders, to convert any document to plain text</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Text splitters, to split each large document into many smaller ones</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Embeddings models, to create a numeric representation of the meaning of each split</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Here’s the code:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="data indexing" data-tertiary="end-to-end example" data-type="indexterm" id="id510"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">TextLoader</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>&#13;
&#13;
<code class="c1">## Load the document </code>&#13;
&#13;
<code class="n">loader</code> <code class="o">=</code> <code class="n">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">)</code>&#13;
<code class="n">doc</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code>&#13;
&#13;
<code class="sd">"""</code>&#13;
<code class="sd">[</code>&#13;
<code class="sd">    Document(page_content='Document loaders\n\nUse document loaders to load data </code>&#13;
<code class="sd">        from a source as `Document`\'s. A `Document` is a piece of text\nand </code>&#13;
<code class="sd">        associated metadata. For example, there are document loaders for </code>&#13;
<code class="sd">        loading a simple `.txt` file, for loading the text\ncontents of any web </code>&#13;
<code class="sd">        page, or even for loading a transcript of a YouTube video.\n\nEvery </code>&#13;
<code class="sd">        document loader exposes two methods:\n1. "Load": load documents from </code>&#13;
<code class="sd">        the configured source\n2. "Load and split": load documents from the </code>&#13;
<code class="sd">        configured source and split them using the passed in text </code>&#13;
<code class="sd">        splitter\n\nThey optionally implement:\n\n3. "Lazy load": load </code>&#13;
<code class="sd">        documents into memory lazily\n', metadata={'source': 'test.txt'})</code>&#13;
<code class="sd">]</code>&#13;
<code class="sd">"""</code>&#13;
&#13;
<code class="c1">## Split the document</code>&#13;
&#13;
<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="p">(</code>&#13;
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code>&#13;
    <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
<code class="n">chunks</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">doc</code><code class="p">)</code>&#13;
&#13;
<code class="c1">## Generate embeddings</code>&#13;
&#13;
<code class="n">embeddings_model</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>&#13;
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">embeddings_model</code><code class="o">.</code><code class="n">embed_documents</code><code class="p">(</code>&#13;
    <code class="p">[</code><code class="n">chunk</code><code class="o">.</code><code class="n">page_content</code> <code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">chunks</code><code class="p">]</code>&#13;
<code class="p">)</code>&#13;
<code class="sd">"""</code>&#13;
<code class="sd">[[0.0053587136790156364,</code>&#13;
<code class="sd"> -0.0004999046213924885,</code>&#13;
<code class="sd">  0.038883671164512634,</code>&#13;
<code class="sd"> -0.003001077566295862,</code>&#13;
<code class="sd"> -0.00900818221271038, ...], ...]</code>&#13;
<code class="sd">"""</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="data indexing" data-tertiary="end-to-end example" data-type="indexterm" id="id511"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">TextLoader</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/document_loaders/fs/text"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RecursiveCharacterTextSplitter</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/textsplitters"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAIEmbeddings</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
&#13;
<code class="c1">// Load the document </code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">loader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">);</code>&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code>&#13;
&#13;
<code class="c1">// Split the document</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">splitter</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">RecursiveCharacterTextSplitter</code><code class="p">({</code>&#13;
  <code class="nx">chunkSize</code><code class="o">:</code> <code class="mi">1000</code><code class="p">,</code>&#13;
  <code class="nx">chunkOverlap</code><code class="o">:</code> <code class="mi">200</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
<code class="kr">const</code> <code class="nx">chunks</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">splitter</code><code class="p">.</code><code class="nx">splitDocuments</code><code class="p">(</code><code class="nx">docs</code><code class="p">)</code>&#13;
&#13;
<code class="c1">// Generate embeddings</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">();</code>&#13;
<code class="nx">await</code> <code class="nx">embeddings</code><code class="p">.</code><code class="nx">embedDocuments</code><code class="p">(</code><code class="nx">chunks</code><code class="p">.</code><code class="nx">map</code><code class="p">(</code><code class="nx">c</code> <code class="o">=&gt;</code> <code class="nx">c</code><code class="p">.</code><code class="nx">pageContent</code><code class="p">));</code></pre>&#13;
&#13;
<p>Once you’ve generated embeddings from your documents, the next step is to store them in a special database known as a vector store.<a contenteditable="false" data-primary="" data-startref="DIend02" data-type="indexterm" id="id512"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Storing Embeddings in a Vector Store" data-type="sect1"><div class="sect1" id="ch02_storing_embeddings_in_a_vector_store_1736545662501907">&#13;
<h1>Storing Embeddings in a Vector Store</h1>&#13;
&#13;
<p class="fix_tracking">Earlier<a contenteditable="false" data-primary="data indexing" data-secondary="storing embeddings in vector stores" data-type="indexterm" id="DIstore02"/><a contenteditable="false" data-primary="vector stores" data-secondary="storing embeddings in vector stores" data-type="indexterm" id="VSstore02"/><a contenteditable="false" data-primary="embeddings" data-secondary="storing embeddings in vector stores" data-type="indexterm" id="Estore02"/> in this chapter, we discussed the cosine similarity calculation to measure the similarity between vectors in a vector space. A vector store is a database designed to store vectors and perform complex calculations, like cosine similarity, efficiently and quickly.</p>&#13;
&#13;
<p class="fix_tracking">Unlike traditional databases that specialize in storing structured data (such as JSON documents or data conforming to the schema of a relational database), vector stores handle unstructured data, including text and images. Like traditional databases, vector stores are capable of performing<a contenteditable="false" data-primary="create, read, update, delete (CRUD)" data-type="indexterm" id="id513"/> create, read, update, delete (CRUD), and search operations.</p>&#13;
&#13;
<p>Vector stores unlock a wide variety of use cases, including scalable applications that utilize AI to answer questions about large documents, as illustrated in <a data-type="xref" href="#ch02_figure_4_1736545662484192">Figure 2-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch02_figure_4_1736545662484192"><img alt="A diagram of a store  Description automatically generated" src="assets/lelc_0204.png"/>&#13;
<h6><span class="label">Figure 2-4. </span>Loading, embedding, storing, and retrieving relevant docs from a vector store</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#ch02_figure_4_1736545662484192">Figure 2-4</a> illustrates how document embeddings are inserted into the vector store and how later, when a query is sent, similar embeddings are retrieved from the vector store.</p>&#13;
&#13;
<p>Currently, there is an abundance of vector store providers to choose from, each specializing in different capabilities. Your selection should depend on the critical requirements of your application, including multitenancy, metadata filtering capabilities, performance, cost, and scalability.</p>&#13;
&#13;
<p>Although vector stores are niche databases built to manage vector data, there are a few disadvantages working with them:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Most vector stores are relatively new and may not stand the test of time.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Managing and optimizing vector stores can present a relatively steep learning curve.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Managing a separate database adds complexity to your application and may drain valuable resources.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Fortunately, vector store capabilities have recently been extended to PostgreSQL (a popular open source relational database) via the <code>pgvector</code> extension. This enables you to use the same database you’re already familiar with and to power both your transactional tables (for instance your users table) as well as your vector search tables.</p>&#13;
&#13;
<section data-pdf-bookmark="Getting Set Up with PGVector" data-type="sect2"><div class="sect2" id="ch02_getting_set_up_with_pgvector_1736545662501989">&#13;
<h2>Getting Set Up with PGVector</h2>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="PGVector" data-type="indexterm" id="id514"/><a contenteditable="false" data-primary="Postgres" data-type="indexterm" id="id515"/> use Postgres and PGVector you’ll need to follow a few setup steps:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Ensure you have Docker installed on your computer, following the <a href="https://oreil.ly/Gn28O">instructions for your operating system</a>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Run the following command in your terminal; it will launch a Postgres instance in your computer running on port 6024:</p>&#13;
&#13;
	<pre data-type="programlisting">&#13;
docker run \&#13;
    --name pgvector-container \&#13;
    -e POSTGRES_USER=langchain \&#13;
    -e POSTGRES_PASSWORD=langchain \&#13;
    -e POSTGRES_DB=langchain \&#13;
    -p 6024:5432 \&#13;
    -d pgvector/pgvector:pg16</pre>&#13;
&#13;
	<p>Open your docker dashboard containers and you should see a green running status next to <code>pgvector-container</code>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Save the connection string to use in your code; we’ll need it later:</p>&#13;
&#13;
	<pre data-type="programlisting">&#13;
postgresql+psycopg://langchain:langchain@localhost:6024/langchain</pre>&#13;
	</li>&#13;
</ol>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Working with Vector Stores" data-type="sect2"><div class="sect2" id="ch02_working_with_vector_stores_1736545662502051">&#13;
<h2>Working with Vector Stores</h2>&#13;
&#13;
<p>Picking up where we left off in the previous section on embeddings, now let’s see an example of loading, splitting, embedding, and storing a document in PGVector:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="vector stores" data-tertiary="working with" data-type="indexterm" id="id516"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># first, pip install langchain-postgres</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">TextLoader</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_postgres.vectorstores</code> <code class="kn">import</code> <code class="n">PGVector</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.documents</code> <code class="kn">import</code> <code class="n">Document</code>&#13;
<code class="kn">import</code> <code class="nn">uuid</code>&#13;
&#13;
<code class="c1"># Load the document, split it into chunks</code>&#13;
<code class="n">raw_documents</code> <code class="o">=</code> <code class="n">TextLoader</code><code class="p">(</code><code class="s1">'./test.txt'</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">()</code>&#13;
<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="p">(</code><code class="n">chunk_size</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> &#13;
    <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code>&#13;
<code class="n">documents</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">raw_documents</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># embed each chunk and insert it into the vector store</code>&#13;
<code class="n">embeddings_model</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>&#13;
<code class="n">connection</code> <code class="o">=</code> <code class="s1">'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'</code>&#13;
<code class="n">db</code> <code class="o">=</code> <code class="n">PGVector</code><code class="o">.</code><code class="n">from_documents</code><code class="p">(</code><code class="n">documents</code><code class="p">,</code> <code class="n">embeddings_model</code><code class="p">,</code> <code class="n">connection</code><code class="o">=</code><code class="n">connection</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="vector stores and" data-type="indexterm" id="id517"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">TextLoader</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/document_loaders/fs/text"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RecursiveCharacterTextSplitter</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/textsplitters"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAIEmbeddings</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PGVectorStore</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/community/vectorstores/pgvector"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">v4</code> <code class="nx">as</code> <code class="nx">uuidv4</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'uuid'</code><code class="p">;</code>&#13;
&#13;
<code class="c1">// Load the document, split it into chunks</code>&#13;
<code class="kr">const</code> <code class="nx">loader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">);</code>&#13;
<code class="kr">const</code> <code class="nx">raw_docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code>&#13;
<code class="kr">const</code> <code class="nx">splitter</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">RecursiveCharacterTextSplitter</code><code class="p">({</code>&#13;
  <code class="nx">chunkSize</code><code class="o">:</code> <code class="mi">1000</code><code class="p">,</code>&#13;
  <code class="nx">chunkOverlap</code><code class="o">:</code> <code class="mi">200</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">splitter</code><code class="p">.</code><code class="nx">splitDocuments</code><code class="p">(</code><code class="nx">docs</code><code class="p">)</code>&#13;
&#13;
<code class="c1">// embed each chunk and insert it into the vector store</code>&#13;
<code class="kr">const</code> <code class="nx">embeddings_model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">();</code>&#13;
<code class="kr">const</code> <code class="nx">db</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">PGVectorStore</code><code class="p">.</code><code class="nx">fromDocuments</code><code class="p">(</code><code class="nx">docs</code><code class="p">,</code> <code class="nx">embeddings_model</code><code class="p">,</code> <code class="p">{</code>&#13;
  <code class="nx">postgresConnectionOptions</code><code class="o">:</code> <code class="p">{</code>&#13;
    <code class="nx">connectionString</code><code class="o">:</code> <code class="s1">'postgresql://langchain:langchain@localhost:6024/langchain'</code>&#13;
  <code class="p">}</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p>Notice how we reuse the code from the previous sections to first load the documents with the loader and then split them into smaller chunks. Then, we instantiate the embeddings model we want to use—in this case, OpenAI’s. Note that you could use any other embeddings model supported by LangChain here.</p>&#13;
&#13;
<p>Next, we have a new line of code, which creates a vector store given documents, the embeddings model, and a connection string. This will do a few things:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Establish a connection to the Postgres instance running in your computer (see <a data-type="xref" href="#ch02_getting_set_up_with_pgvector_1736545662501989">“Getting Set Up with PGVector”</a>.)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Run any setup necessary, such as creating tables to hold your documents and vectors, if this is the first time you’re running it.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Create embeddings for each document you passed in, using the model you chose.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Store the embeddings, the document’s metadata, and the document’s text content in Postgres, ready to be searched.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Let’s see what it looks like to search documents:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">db</code><code class="o">.</code><code class="n">similarity_search</code><code class="p">(</code><code class="s2">"query"</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="nx">await</code> <code class="nx">pgvectorStore</code><code class="p">.</code><code class="nx">similaritySearch</code><code class="p">(</code><code class="s2">"query"</code><code class="p">,</code> <code class="mi">4</code><code class="p">);</code></pre>&#13;
&#13;
<p>This method will find the most relevant documents (which you previously indexed), by following this process:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The search query—in this case, the word <code>query</code>—will be sent to the embeddings model to retrieve its embedding.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Then, it will run a query on Postgres to find the N (in this case 4) previously stored embeddings that are most similar to your query.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Finally, it will fetch the text content and metadata that relates to each of those embeddings.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The model can now return a list of <code>Document</code> sorted by how similar they are to the query—the most similar first, the second most similar after, and so on.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>You can also add more documents to an existing database. Let’s see an example:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">ids</code> <code class="o">=</code> <code class="p">[</code><code class="nb">str</code><code class="p">(</code><code class="n">uuid</code><code class="o">.</code><code class="n">uuid4</code><code class="p">()),</code> <code class="nb">str</code><code class="p">(</code><code class="n">uuid</code><code class="o">.</code><code class="n">uuid4</code><code class="p">())]</code>&#13;
<code class="n">db</code><code class="o">.</code><code class="n">add_documents</code><code class="p">(</code>&#13;
    <code class="p">[</code>&#13;
        <code class="n">Document</code><code class="p">(</code>&#13;
            <code class="n">page_content</code><code class="o">=</code><code class="s2">"there are cats in the pond"</code><code class="p">,</code>&#13;
            <code class="n">metadata</code><code class="o">=</code><code class="p">{</code><code class="s2">"location"</code><code class="p">:</code> <code class="s2">"pond"</code><code class="p">,</code> <code class="s2">"topic"</code><code class="p">:</code> <code class="s2">"animals"</code><code class="p">},</code>&#13;
        <code class="p">),</code>&#13;
        <code class="n">Document</code><code class="p">(</code>&#13;
            <code class="n">page_content</code><code class="o">=</code><code class="s2">"ducks are also found in the pond"</code><code class="p">,</code>&#13;
            <code class="n">metadata</code><code class="o">=</code><code class="p">{</code><code class="s2">"location"</code><code class="p">:</code> <code class="s2">"pond"</code><code class="p">,</code> <code class="s2">"topic"</code><code class="p">:</code> <code class="s2">"animals"</code><code class="p">},</code>&#13;
        <code class="p">),</code>&#13;
    <code class="p">],</code>&#13;
    <code class="n">ids</code><code class="o">=</code><code class="n">ids</code><code class="p">,</code>&#13;
<code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">ids</code> <code class="o">=</code> <code class="p">[</code><code class="nx">uuidv4</code><code class="p">(),</code> <code class="nx">uuidv4</code><code class="p">()];</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">db</code><code class="p">.</code><code class="nx">addDocuments</code><code class="p">(</code>&#13;
  <code class="p">[</code>&#13;
    <code class="p">{</code>&#13;
      <code class="nx">pageContent</code><code class="o">:</code> <code class="s2">"there are cats in the pond"</code><code class="p">,</code>&#13;
      <code class="nx">metadata</code><code class="o">:</code> <code class="p">{</code><code class="nx">location</code><code class="o">:</code> <code class="s2">"pond"</code><code class="p">,</code> <code class="nx">topic</code><code class="o">:</code> <code class="s2">"animals"</code><code class="p">}</code>&#13;
    <code class="p">},</code> &#13;
    <code class="p">{</code>&#13;
      <code class="nx">pageContent</code><code class="o">:</code> <code class="s2">"ducks are also found in the pond"</code><code class="p">,</code>&#13;
      <code class="nx">metadata</code><code class="o">:</code> <code class="p">{</code><code class="nx">location</code><code class="o">:</code> <code class="s2">"pond"</code><code class="p">,</code> <code class="nx">topic</code><code class="o">:</code> <code class="s2">"animals"</code><code class="p">}</code>&#13;
    <code class="p">},</code>&#13;
  <code class="p">],</code> &#13;
  <code class="p">{</code><code class="nx">ids</code><code class="p">}</code>&#13;
<code class="p">);</code></pre>&#13;
&#13;
<p>The <code>add_documents</code> method we’re using here will follow a similar process to <span class="keep-together"><code>fromDocuments</code>:</span></p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Create embeddings for each document you passed in, using the model you chose.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Store the embeddings, the document’s metadata, and the document’s text content in Postgres, ready to be searched.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In this example, we are using the optional <code>ids</code> argument to assign identifiers to each document, which allows us to update or delete them later.</p>&#13;
&#13;
<p class="pagebreak-before less_space">Here’s an example of the delete operation:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">db</code><code class="o">.</code><code class="n">delete</code><code class="p">(</code><code class="n">ids</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="nx">await</code> <code class="nx">db</code><code class="p">.</code><code class="k">delete</code><code class="p">({</code> <code class="nx">ids</code><code class="o">:</code> <code class="p">[</code><code class="nx">ids</code><code class="p">[</code><code class="mi">1</code><code class="p">]]</code> <code class="p">})</code></pre>&#13;
&#13;
<p>This removes the second document inserted by using its Universally Unique Identifier (UUID). Now let’s see how to do this in a more systematic way.<a contenteditable="false" data-primary="" data-startref="Estore02" data-type="indexterm" id="id518"/><a contenteditable="false" data-primary="" data-startref="VSstore02" data-type="indexterm" id="id519"/><a contenteditable="false" data-primary="" data-startref="DIstore02" data-type="indexterm" id="id520"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Tracking Changes to Your Documents" data-type="sect1"><div class="sect1" id="ch02_tracking_changes_to_your_documents_1736545662502114">&#13;
<h1>Tracking Changes to Your Documents</h1>&#13;
&#13;
<p>One<a contenteditable="false" data-primary="data indexing" data-secondary="tracking document changes" data-type="indexterm" id="DItrack02"/><a contenteditable="false" data-primary="documents" data-secondary="tracking document changes" data-type="indexterm" id="Dtrack02"/> of the key challenges with working with vector stores is working with data that regularly changes, because changes mean re-indexing. And re-indexing can lead to costly recomputations of embeddings and duplications of preexisting content.</p>&#13;
&#13;
<p>Fortunately, LangChain<a contenteditable="false" data-primary="record management" data-type="indexterm" id="recmang02"/> provides an indexing API to make it easy to keep your documents in sync with your vector store. The API utilizes a class (<code>RecordManager</code>) to keep track of document writes into the vector store. When indexing content, hashes are computed for each document and the following information is stored in <code>RecordManager</code>:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The document hash (hash of both page content and metadata)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Write time</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The source ID (each document should include information in its metadata to determine the ultimate source of this document).</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In addition, the indexing API provides<a contenteditable="false" data-primary="documents" data-secondary="deleting" data-type="indexterm" id="id521"/> cleanup modes to help you decide how to delete existing documents in the vector store. For example, If you’ve made changes to how documents are processed before insertion or if source documents have changed, you may want to remove any existing documents that come from the same source as the new documents being indexed. If some source documents have been deleted, you’ll want to delete all existing documents in the vector store and replace them with the re-indexed documents.</p>&#13;
&#13;
<p>The modes are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p><code>None</code> mode does not do any automatic cleanup, allowing the user to manually do cleanup of old content.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><code>Incremental</code> and <code>full</code> modes delete previous versions of the content if the content of the source document or derived documents has changed.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><code>Full</code> mode will additionally delete any documents not included in documents currently being indexed.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Here’s an example of the use of the indexing API with Postgres database set up as a record manager:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="record management in" data-type="indexterm" id="id522"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain.indexes</code> <code class="kn">import</code> <code class="n">SQLRecordManager</code><code class="p">,</code> <code class="n">index</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_postgres.vectorstores</code> <code class="kn">import</code> <code class="n">PGVector</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>&#13;
<code class="kn">from</code> <code class="nn">langchain.docstore.document</code> <code class="kn">import</code> <code class="n">Document</code>&#13;
	&#13;
<code class="n">connection</code> <code class="o">=</code> <code class="s2">"postgresql+psycopg://langchain:langchain@localhost:6024/langchain"</code>&#13;
<code class="n">collection_name</code> <code class="o">=</code> <code class="s2">"my_docs"</code>&#13;
<code class="n">embeddings_model</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"text-embedding-3-small"</code><code class="p">)</code>&#13;
<code class="n">namespace</code> <code class="o">=</code> <code class="s2">"my_docs_namespace"</code>&#13;
	&#13;
<code class="n">vectorstore</code> <code class="o">=</code> <code class="n">PGVector</code><code class="p">(</code>&#13;
    <code class="n">embeddings</code><code class="o">=</code><code class="n">embeddings_model</code><code class="p">,</code>&#13;
    <code class="n">collection_name</code><code class="o">=</code><code class="n">collection_name</code><code class="p">,</code>&#13;
    <code class="n">connection</code><code class="o">=</code><code class="n">connection</code><code class="p">,</code>&#13;
    <code class="n">use_jsonb</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
	&#13;
<code class="n">record_manager</code> <code class="o">=</code> <code class="n">SQLRecordManager</code><code class="p">(</code>&#13;
    <code class="n">namespace</code><code class="p">,</code>&#13;
    <code class="n">db_url</code><code class="o">=</code><code class="s2">"postgresql+psycopg://langchain:langchain@localhost:6024/langchain"</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
	&#13;
<code class="c1"># Create the schema if it doesn't exist</code>&#13;
<code class="n">record_manager</code><code class="o">.</code><code class="n">create_schema</code><code class="p">()</code>&#13;
	&#13;
<code class="c1"># Create documents</code>&#13;
<code class="n">docs</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="n">Document</code><code class="p">(</code><code class="n">page_content</code><code class="o">=</code><code class="s1">'there are cats in the pond'</code><code class="p">,</code> <code class="n">metadata</code><code class="o">=</code><code class="p">{</code>&#13;
        <code class="s2">"id"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"source"</code><code class="p">:</code> <code class="s2">"cats.txt"</code><code class="p">}),</code>&#13;
    <code class="n">Document</code><code class="p">(</code><code class="n">page_content</code><code class="o">=</code><code class="s1">'ducks are also found in the pond'</code><code class="p">,</code> <code class="n">metadata</code><code class="o">=</code><code class="p">{</code>&#13;
        <code class="s2">"id"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"source"</code><code class="p">:</code> <code class="s2">"ducks.txt"</code><code class="p">}),</code>&#13;
<code class="p">]</code>&#13;
	&#13;
<code class="c1"># Index the documents</code>&#13;
<code class="n">index_1</code> <code class="o">=</code> <code class="n">index</code><code class="p">(</code>&#13;
    <code class="n">docs</code><code class="p">,</code>&#13;
    <code class="n">record_manager</code><code class="p">,</code>&#13;
    <code class="n">vectorstore</code><code class="p">,</code>&#13;
    <code class="n">cleanup</code><code class="o">=</code><code class="s2">"incremental"</code><code class="p">,</code>  <code class="c1"># prevent duplicate documents</code>&#13;
    <code class="n">source_id_key</code><code class="o">=</code><code class="s2">"source"</code><code class="p">,</code>  <code class="c1"># use the source field as the source_id</code>&#13;
<code class="p">)</code>&#13;
	&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Index attempt 1:"</code><code class="p">,</code> <code class="n">index_1</code><code class="p">)</code>&#13;
	&#13;
<code class="c1"># second time you attempt to index, it will not add the documents again</code>&#13;
<code class="n">index_2</code> <code class="o">=</code> <code class="n">index</code><code class="p">(</code>&#13;
    <code class="n">docs</code><code class="p">,</code>&#13;
    <code class="n">record_manager</code><code class="p">,</code>&#13;
    <code class="n">vectorstore</code><code class="p">,</code>&#13;
    <code class="n">cleanup</code><code class="o">=</code><code class="s2">"incremental"</code><code class="p">,</code>&#13;
    <code class="n">source_id_key</code><code class="o">=</code><code class="s2">"source"</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
	&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Index attempt 2:"</code><code class="p">,</code> <code class="n">index_2</code><code class="p">)</code>&#13;
	&#13;
<code class="c1"># If we mutate a document, the new version will be written and all old </code>&#13;
<code class="c1"># versions sharing the same source will be deleted.</code>&#13;
	&#13;
<code class="n">docs</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">page_content</code> <code class="o">=</code> <code class="s2">"I just modified this document!"</code>&#13;
	&#13;
<code class="n">index_3</code> <code class="o">=</code> <code class="n">index</code><code class="p">(</code>&#13;
    <code class="n">docs</code><code class="p">,</code>&#13;
    <code class="n">record_manager</code><code class="p">,</code>&#13;
    <code class="n">vectorstore</code><code class="p">,</code>&#13;
    <code class="n">cleanup</code><code class="o">=</code><code class="s2">"incremental"</code><code class="p">,</code>&#13;
    <code class="n">source_id_key</code><code class="o">=</code><code class="s2">"source"</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
	&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Index attempt 3:"</code><code class="p">,</code> <code class="n">index_3</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="record management in" data-type="indexterm" id="id523"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="cm">/** </code>&#13;
<code class="cm">1. Ensure docker is installed and running (https://docs.docker.com/get-docker/)</code>&#13;
<code class="cm">2. Run the following command to start the postgres container:</code>&#13;
<code class="cm">	   </code>&#13;
<code class="cm">docker run \</code>&#13;
<code class="cm">  --name pgvector-container \</code>&#13;
<code class="cm">  -e POSTGRES_USER=langchain \</code>&#13;
<code class="cm">  -e POSTGRES_PASSWORD=langchain \</code>&#13;
<code class="cm">  -e POSTGRES_DB=langchain \</code>&#13;
<code class="cm">  -p 6024:5432 \</code>&#13;
<code class="cm">  -d pgvector/pgvector:pg16</code>&#13;
<code class="cm">3. Use the connection string below for the postgres container</code>&#13;
<code class="cm">*/</code>&#13;
	&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PostgresRecordManager</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/community/indexes/postgres'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">index</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'langchain/indexes'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAIEmbeddings</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PGVectorStore</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/community/vectorstores/pgvector'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">v4</code> <code class="nx">as</code> <code class="nx">uuidv4</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'uuid'</code><code class="p">;</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">tableName</code> <code class="o">=</code> <code class="s1">'test_langchain'</code><code class="p">;</code>&#13;
<code class="kr">const</code> <code class="nx">connectionString</code> <code class="o">=</code>&#13;
  <code class="s1">'postgresql://langchain:langchain@localhost:6024/langchain'</code><code class="p">;</code>&#13;
<code class="c1">// Load the document, split it into chunks</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">config</code> <code class="o">=</code> <code class="p">{</code>&#13;
  <code class="nx">postgresConnectionOptions</code><code class="o">:</code> <code class="p">{</code>&#13;
    <code class="nx">connectionString</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
  <code class="nx">tableName</code><code class="o">:</code> <code class="nx">tableName</code><code class="p">,</code>&#13;
  <code class="nx">columns</code><code class="o">:</code> <code class="p">{</code>&#13;
    <code class="nx">idColumnName</code><code class="o">:</code> <code class="s1">'id'</code><code class="p">,</code>&#13;
    <code class="nx">vectorColumnName</code><code class="o">:</code> <code class="s1">'vector'</code><code class="p">,</code>&#13;
    <code class="nx">contentColumnName</code><code class="o">:</code> <code class="s1">'content'</code><code class="p">,</code>&#13;
    <code class="nx">metadataColumnName</code><code class="o">:</code> <code class="s1">'metadata'</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
<code class="p">};</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">vectorStore</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">PGVectorStore</code><code class="p">.</code><code class="nx">initialize</code><code class="p">(</code>&#13;
  <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">(),</code>&#13;
  <code class="nx">config</code>&#13;
<code class="p">);</code>&#13;
	&#13;
<code class="c1">// Create a new record manager</code>&#13;
<code class="kr">const</code> <code class="nx">recordManagerConfig</code> <code class="o">=</code> <code class="p">{</code>&#13;
  <code class="nx">postgresConnectionOptions</code><code class="o">:</code> <code class="p">{</code>&#13;
    <code class="nx">connectionString</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
  <code class="nx">tableName</code><code class="o">:</code> <code class="s1">'upsertion_records'</code><code class="p">,</code>&#13;
<code class="p">};</code>&#13;
<code class="kr">const</code> <code class="nx">recordManager</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">PostgresRecordManager</code><code class="p">(</code>&#13;
  <code class="s1">'test_namespace'</code><code class="p">,</code>&#13;
  <code class="nx">recordManagerConfig</code>&#13;
<code class="p">);</code>&#13;
	&#13;
<code class="c1">// Create the schema if it doesn't exist</code>&#13;
<code class="nx">await</code> <code class="nx">recordManager</code><code class="p">.</code><code class="nx">createSchema</code><code class="p">();</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="p">[</code>&#13;
  <code class="p">{</code>&#13;
    <code class="nx">pageContent</code><code class="o">:</code> <code class="s1">'there are cats in the pond'</code><code class="p">,</code>&#13;
    <code class="nx">metadata</code><code class="o">:</code> <code class="p">{</code> <code class="nx">id</code><code class="o">:</code> <code class="nx">uuidv4</code><code class="p">(),</code> <code class="nx">source</code><code class="o">:</code> <code class="s1">'cats.txt'</code> <code class="p">},</code>&#13;
  <code class="p">},</code>&#13;
  <code class="p">{</code>&#13;
    <code class="nx">pageContent</code><code class="o">:</code> <code class="s1">'ducks are also found in the pond'</code><code class="p">,</code>&#13;
    <code class="nx">metadata</code><code class="o">:</code> <code class="p">{</code> <code class="nx">id</code><code class="o">:</code> <code class="nx">uuidv4</code><code class="p">(),</code> <code class="nx">source</code><code class="o">:</code> <code class="s1">'ducks.txt'</code> <code class="p">},</code>&#13;
  <code class="p">},</code>&#13;
<code class="p">];</code>&#13;
	&#13;
<code class="c1">// the first attempt will index both documents</code>&#13;
<code class="kr">const</code> <code class="nx">index_attempt_1</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">index</code><code class="p">({</code>&#13;
  <code class="nx">docsSource</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code>&#13;
  <code class="nx">recordManager</code><code class="p">,</code>&#13;
  <code class="nx">vectorStore</code><code class="p">,</code>&#13;
  <code class="nx">options</code><code class="o">:</code> <code class="p">{</code>&#13;
    <code class="c1">// prevent duplicate documents by id from being indexed</code>&#13;
    <code class="nx">cleanup</code><code class="o">:</code> <code class="s1">'incremental'</code><code class="p">,</code>&#13;
    <code class="c1">// the key in the metadata that will be used to identify the document </code>&#13;
    <code class="nx">sourceIdKey</code><code class="o">:</code> <code class="s1">'source'</code><code class="p">,</code> &#13;
  <code class="p">},</code>&#13;
<code class="p">});</code>&#13;
	&#13;
<code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">index_attempt_1</code><code class="p">);</code>&#13;
	&#13;
<code class="c1">// the second attempt will skip indexing because the identical documents </code>&#13;
<code class="c1">// already exist</code>&#13;
<code class="kr">const</code> <code class="nx">index_attempt_2</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">index</code><code class="p">({</code>&#13;
  <code class="nx">docsSource</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code>&#13;
  <code class="nx">recordManager</code><code class="p">,</code>&#13;
  <code class="nx">vectorStore</code><code class="p">,</code>&#13;
  <code class="nx">options</code><code class="o">:</code> <code class="p">{</code>&#13;
    <code class="nx">cleanup</code><code class="o">:</code> <code class="s1">'incremental'</code><code class="p">,</code>&#13;
    <code class="nx">sourceIdKey</code><code class="o">:</code> <code class="s1">'source'</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
<code class="p">});</code>&#13;
	&#13;
<code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">index_attempt_2</code><code class="p">);</code>&#13;
	&#13;
<code class="c1">// If we mutate a document, the new version will be written and all old </code>&#13;
<code class="c1">// versions sharing the same source will be deleted.</code>&#13;
<code class="nx">docs</code><code class="p">[</code><code class="mi">0</code><code class="p">].</code><code class="nx">pageContent</code> <code class="o">=</code> <code class="s1">'I modified the first document content'</code><code class="p">;</code>&#13;
<code class="kr">const</code> <code class="nx">index_attempt_3</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">index</code><code class="p">({</code>&#13;
  <code class="nx">docsSource</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code>&#13;
  <code class="nx">recordManager</code><code class="p">,</code>&#13;
  <code class="nx">vectorStore</code><code class="p">,</code>&#13;
  <code class="nx">options</code><code class="o">:</code> <code class="p">{</code>&#13;
    <code class="nx">cleanup</code><code class="o">:</code> <code class="s1">'incremental'</code><code class="p">,</code>&#13;
    <code class="nx">sourceIdKey</code><code class="o">:</code> <code class="s1">'source'</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
<code class="p">});</code>&#13;
	&#13;
<code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">index_attempt_3</code><code class="p">);</code></pre>&#13;
&#13;
<p>First, you create a record manager, which keeps track of which documents have been indexed before. Then you use the <code>index</code> function to synchronize your vector store with the new list of documents. In this example, we’re using the incremental mode, so any documents that have the same ID as previous ones will be replaced with the new version.<a contenteditable="false" data-primary="" data-startref="DItrack02" data-type="indexterm" id="id524"/><a contenteditable="false" data-primary="" data-startref="Dtrack02" data-type="indexterm" id="id525"/><a contenteditable="false" data-primary="" data-startref="recmang02" data-type="indexterm" id="id526"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Indexing Optimization" data-type="sect1"><div class="sect1" id="ch02_indexing_optimization_1736545662502179">&#13;
<h1>Indexing Optimization</h1>&#13;
&#13;
<p>A<a contenteditable="false" data-primary="data indexing" data-secondary="optimizing" data-type="indexterm" id="DIoptimiz02"/> basic RAG indexing stage involves naive text splitting and embedding of chunks of a given document. However, this basic approach leads to inconsistent retrieval results and a relatively high occurrence of hallucinations, especially when the data source contains images and tables.</p>&#13;
&#13;
<p>There are various strategies to enhance the accuracy and performance of the indexing stage. We will cover three of them in the next sections: MultiVectorRetriever, RAPTOR, and ColBERT.</p>&#13;
&#13;
<section data-pdf-bookmark="MultiVectorRetriever" data-type="sect2"><div class="sect2" id="ch02_multivectorretriever_1736545662502238">&#13;
<h2>MultiVectorRetriever</h2>&#13;
&#13;
<p>A<a contenteditable="false" data-primary="MultiVectorRetriever" data-type="indexterm" id="multivector02"/><a contenteditable="false" data-primary="tables" data-type="indexterm" id="id527"/><a contenteditable="false" data-primary="documents" data-secondary="decoupling" data-type="indexterm" id="Ddecoupling02"/><a contenteditable="false" data-primary="text" data-secondary="mixed with tables" data-type="indexterm" id="id528"/> document that contains a mixture of text and tables cannot be simply split by text into chunks and embedded as context: the entire table can be easily lost. To solve this problem, we can decouple documents that we want to use for answer synthesis, from a reference that we want to use for the retriever. <a data-type="xref" href="#ch02_figure_5_1736545662484212">Figure 2-5</a> illustrates how.</p>&#13;
&#13;
<figure><div class="figure" id="ch02_figure_5_1736545662484212"><img alt="Screenshot 2024-03-16 at 5.54.55 PM.png" src="assets/lelc_0205.png"/>&#13;
<h6><span class="label">Figure 2-5. </span>Indexing multiple representations of a single document</h6>&#13;
</div></figure>&#13;
&#13;
<p>For example, in the case of a document that contains tables, we can first generate and embed summaries of table elements, ensuring each summary contains an <code>id</code> reference to the full raw table. Next, we store the raw referenced tables in a separate docstore. Finally, when a user’s query retrieves a table summary, we pass the entire referenced raw table as context to the final prompt sent to the LLM for answer synthesis. This approach enables us to provide the model with the full context of information required to answer the question.</p>&#13;
&#13;
<p>Here’s an example. First, let’s use the LLM to generate summaries of the documents:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="document summaries, generating" data-type="indexterm" id="id529"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">TextLoader</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_postgres.vectorstores</code> <code class="kn">import</code> <code class="n">PGVector</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.output_parsers</code> <code class="kn">import</code> <code class="n">StrOutputParser</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
<code class="kn">from</code> <code class="nn">pydantic</code> <code class="kn">import</code> <code class="n">BaseModel</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">RunnablePassthrough</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.documents</code> <code class="kn">import</code> <code class="n">Document</code>&#13;
<code class="kn">from</code> <code class="nn">langchain.retrievers.multi_vector</code> <code class="kn">import</code> <code class="n">MultiVectorRetriever</code>&#13;
<code class="kn">from</code> <code class="nn">langchain.storage</code> <code class="kn">import</code> <code class="n">InMemoryStore</code>&#13;
<code class="kn">import</code> <code class="nn">uuid</code>&#13;
	&#13;
<code class="n">connection</code> <code class="o">=</code> <code class="s2">"postgresql+psycopg://langchain:langchain@localhost:6024/langchain"</code>&#13;
<code class="n">collection_name</code> <code class="o">=</code> <code class="s2">"summaries"</code>&#13;
<code class="n">embeddings_model</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>&#13;
<code class="c1"># Load the document</code>&#13;
<code class="n">loader</code> <code class="o">=</code> <code class="n">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s2">"utf-8"</code><code class="p">)</code>&#13;
<code class="n">docs</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code>&#13;
	&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"length of loaded docs: "</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">docs</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">page_content</code><code class="p">))</code>&#13;
<code class="c1"># Split the document</code>&#13;
<code class="n">splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="p">(</code><code class="n">chunk_size</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code>&#13;
<code class="n">chunks</code> <code class="o">=</code> <code class="n">splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">docs</code><code class="p">)</code>&#13;
	&#13;
<code class="c1"># The rest of your code remains the same, starting from:</code>&#13;
<code class="n">prompt_text</code> <code class="o">=</code> <code class="s2">"Summarize the following document:</code><code class="se">\n\n</code><code class="si">{doc}</code><code class="s2">"</code>&#13;
	&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="n">prompt_text</code><code class="p">)</code>&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">)</code>&#13;
<code class="n">summarize_chain</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s2">"doc"</code><code class="p">:</code> <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="o">.</code><code class="n">page_content</code><code class="p">}</code> <code class="o">|</code> <code class="n">prompt</code> <code class="o">|</code> <code class="n">llm</code> <code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code>&#13;
	&#13;
<code class="c1"># batch the chain across the chunks</code>&#13;
<code class="n">summaries</code> <code class="o">=</code> <code class="n">summarize_chain</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">chunks</code><code class="p">,</code> <code class="p">{</code><code class="s2">"max_concurrency"</code><code class="p">:</code> <code class="mi">5</code><code class="p">})</code></pre>&#13;
&#13;
<p>Next, let’s define the vector store and docstore to store the raw summaries and their embeddings:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="vector stores" data-tertiary="storing summaries in" data-type="indexterm" id="id530"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># The vectorstore to use to index the child chunks</code>&#13;
<code class="n">vectorstore</code> <code class="o">=</code> <code class="n">PGVector</code><code class="p">(</code>&#13;
    <code class="n">embeddings</code><code class="o">=</code><code class="n">embeddings_model</code><code class="p">,</code>&#13;
    <code class="n">collection_name</code><code class="o">=</code><code class="n">collection_name</code><code class="p">,</code>&#13;
    <code class="n">connection</code><code class="o">=</code><code class="n">connection</code><code class="p">,</code>&#13;
    <code class="n">use_jsonb</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
<code class="c1"># The storage layer for the parent documents</code>&#13;
<code class="n">store</code> <code class="o">=</code> <code class="n">InMemoryStore</code><code class="p">()</code>&#13;
<code class="n">id_key</code> <code class="o">=</code> <code class="s2">"doc_id"</code>&#13;
	&#13;
<code class="c1"># indexing the summaries in our vector store, whilst retaining the original </code>&#13;
<code class="c1"># documents in our document store:</code>&#13;
<code class="n">retriever</code> <code class="o">=</code> <code class="n">MultiVectorRetriever</code><code class="p">(</code>&#13;
    <code class="n">vectorstore</code><code class="o">=</code><code class="n">vectorstore</code><code class="p">,</code>&#13;
    <code class="n">docstore</code><code class="o">=</code><code class="n">store</code><code class="p">,</code>&#13;
    <code class="n">id_key</code><code class="o">=</code><code class="n">id_key</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
	&#13;
<code class="c1"># Changed from summaries to chunks since we need same length as docs</code>&#13;
<code class="n">doc_ids</code> <code class="o">=</code> <code class="p">[</code><code class="nb">str</code><code class="p">(</code><code class="n">uuid</code><code class="o">.</code><code class="n">uuid4</code><code class="p">())</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">chunks</code><code class="p">]</code>&#13;
	&#13;
<code class="c1"># Each summary is linked to the original document by the doc_id</code>&#13;
<code class="n">summary_docs</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="n">Document</code><code class="p">(</code><code class="n">page_content</code><code class="o">=</code><code class="n">s</code><code class="p">,</code> <code class="n">metadata</code><code class="o">=</code><code class="p">{</code><code class="n">id_key</code><code class="p">:</code> <code class="n">doc_ids</code><code class="p">[</code><code class="n">i</code><code class="p">]})</code>&#13;
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">s</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">summaries</code><code class="p">)</code>&#13;
<code class="p">]</code>&#13;
	&#13;
<code class="c1"># Add the document summaries to the vector store for similarity search</code>&#13;
<code class="n">retriever</code><code class="o">.</code><code class="n">vectorstore</code><code class="o">.</code><code class="n">add_documents</code><code class="p">(</code><code class="n">summary_docs</code><code class="p">)</code>&#13;
	&#13;
<code class="c1"># Store the original documents in the document store, linked to their summaries </code>&#13;
<code class="c1"># via doc_ids</code>&#13;
<code class="c1"># This allows us to first search summaries efficiently, then fetch the full </code>&#13;
<code class="c1"># docs when needed</code>&#13;
<code class="n">retriever</code><code class="o">.</code><code class="n">docstore</code><code class="o">.</code><code class="n">mset</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">doc_ids</code><code class="p">,</code> <code class="n">chunks</code><code class="p">)))</code>&#13;
	&#13;
<code class="c1"># vector store retrieves the summaries</code>&#13;
<code class="n">sub_docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">vectorstore</code><code class="o">.</code><code class="n">similarity_search</code><code class="p">(</code>&#13;
    <code class="s2">"chapter on philosophy"</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code></pre>&#13;
&#13;
<p>Finally, let’s retrieve the relevant full context document based on a query:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="documents, retrieving full context" data-type="indexterm" id="id531"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># Whereas the retriever will return the larger source document chunks:</code>&#13;
<code class="n">retrieved_docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"chapter on philosophy"</code><code class="p">)</code>&#13;
	</pre>&#13;
&#13;
<p>Here’s the full implementation in JavaScript:</p>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="documents, retrieving full context" data-type="indexterm" id="JSretriev02"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="o">*</code> <code class="nx">as</code> <code class="nx">uuid</code> <code class="nx">from</code> <code class="s1">'uuid'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">MultiVectorRetriever</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'langchain/retrievers/multi_vector'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAIEmbeddings</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RecursiveCharacterTextSplitter</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/textsplitters'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">InMemoryStore</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/stores'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">TextLoader</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'langchain/document_loaders/fs/text'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">Document</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/documents'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PGVectorStore</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/community/vectorstores/pgvector'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PromptTemplate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RunnableSequence</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/runnables'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">StringOutputParser</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/output_parsers'</code><code class="p">;</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">connectionString</code> <code class="o">=</code>&#13;
  <code class="s1">'postgresql://langchain:langchain@localhost:6024/langchain'</code><code class="p">;</code>&#13;
<code class="kr">const</code> <code class="nx">collectionName</code> <code class="o">=</code> <code class="s1">'summaries'</code><code class="p">;</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">textLoader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">TextLoader</code><code class="p">(</code><code class="s1">'./test.txt'</code><code class="p">);</code>&#13;
<code class="kr">const</code> <code class="nx">parentDocuments</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">textLoader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code>&#13;
<code class="kr">const</code> <code class="nx">splitter</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">RecursiveCharacterTextSplitter</code><code class="p">({</code>&#13;
  <code class="nx">chunkSize</code><code class="o">:</code> <code class="mi">10000</code><code class="p">,</code>&#13;
  <code class="nx">chunkOverlap</code><code class="o">:</code> <code class="mi">20</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">splitter</code><code class="p">.</code><code class="nx">splitDocuments</code><code class="p">(</code><code class="nx">parentDocuments</code><code class="p">);</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">PromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code>&#13;
  <code class="sb">`Summarize the following document:</code><code class="err">\</code><code class="sb">n</code><code class="err">\</code><code class="sb">n{doc}`</code>&#13;
<code class="p">);</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code> <code class="nx">modelName</code><code class="o">:</code> <code class="s1">'gpt-3.5-turbo'</code> <code class="p">});</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">chain</code> <code class="o">=</code> <code class="nx">RunnableSequence</code><code class="p">.</code><code class="nx">from</code><code class="p">([</code>&#13;
  <code class="p">{</code> <code class="nx">doc</code><code class="o">:</code> <code class="p">(</code><code class="nx">doc</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="nx">doc</code><code class="p">.</code><code class="nx">pageContent</code> <code class="p">},</code>&#13;
  <code class="nx">prompt</code><code class="p">,</code>&#13;
  <code class="nx">llm</code><code class="p">,</code>&#13;
  <code class="k">new</code> <code class="nx">StringOutputParser</code><code class="p">(),</code>&#13;
<code class="p">]);</code>&#13;
	&#13;
<code class="c1">// batch summarization chain across the chunks</code>&#13;
<code class="kr">const</code> <code class="nx">summaries</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">chain</code><code class="p">.</code><code class="nx">batch</code><code class="p">(</code><code class="nx">docs</code><code class="p">,</code> <code class="p">{</code>&#13;
  <code class="nx">maxConcurrency</code><code class="o">:</code> <code class="mi">5</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">idKey</code> <code class="o">=</code> <code class="s1">'doc_id'</code><code class="p">;</code>&#13;
<code class="kr">const</code> <code class="nx">docIds</code> <code class="o">=</code> <code class="nx">docs</code><code class="p">.</code><code class="nx">map</code><code class="p">((</code><code class="nx">_</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="nx">uuid</code><code class="p">.</code><code class="nx">v4</code><code class="p">());</code>&#13;
<code class="c1">// create summary docs with metadata linking to the original docs</code>&#13;
<code class="kr">const</code> <code class="nx">summaryDocs</code> <code class="o">=</code> <code class="nx">summaries</code><code class="p">.</code><code class="nx">map</code><code class="p">((</code><code class="nx">summary</code><code class="p">,</code> <code class="nx">i</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="kr">const</code> <code class="nx">summaryDoc</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Document</code><code class="p">({</code>&#13;
    <code class="nx">pageContent</code><code class="o">:</code> <code class="nx">summary</code><code class="p">,</code>&#13;
    <code class="nx">metadata</code><code class="o">:</code> <code class="p">{</code>&#13;
      <code class="p">[</code><code class="nx">idKey</code><code class="p">]</code><code class="o">:</code> <code class="nx">docIds</code><code class="p">[</code><code class="nx">i</code><code class="p">],</code>&#13;
    <code class="p">},</code>&#13;
  <code class="p">});</code>&#13;
  <code class="k">return</code> <code class="nx">summaryDoc</code><code class="p">;</code>&#13;
<code class="p">});</code>&#13;
	&#13;
<code class="c1">// The byteStore to use to store the original chunks</code>&#13;
<code class="kr">const</code> <code class="nx">byteStore</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">InMemoryStore</code><code class="p">();</code>&#13;
	&#13;
<code class="c1">// vector store for the summaries</code>&#13;
<code class="kr">const</code> <code class="nx">vectorStore</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">PGVectorStore</code><code class="p">.</code><code class="nx">fromDocuments</code><code class="p">(</code>&#13;
  <code class="nx">docs</code><code class="p">,</code>&#13;
  <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">(),</code>&#13;
  <code class="p">{</code>&#13;
    <code class="nx">postgresConnectionOptions</code><code class="o">:</code> <code class="p">{</code>&#13;
      <code class="nx">connectionString</code><code class="p">,</code>&#13;
    <code class="p">},</code>&#13;
  <code class="p">}</code>&#13;
<code class="p">);</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">retriever</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">MultiVectorRetriever</code><code class="p">({</code>&#13;
  <code class="nx">vectorstore</code><code class="o">:</code> <code class="nx">vectorStore</code><code class="p">,</code>&#13;
  <code class="nx">byteStore</code><code class="p">,</code>&#13;
  <code class="nx">idKey</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
	&#13;
<code class="kr">const</code> <code class="nx">keyValuePairs</code> <code class="o">=</code> <code class="nx">docs</code><code class="p">.</code><code class="nx">map</code><code class="p">((</code><code class="nx">originalDoc</code><code class="p">,</code> <code class="nx">i</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">[</code><code class="nx">docIds</code><code class="p">[</code><code class="nx">i</code><code class="p">],</code> <code class="nx">originalDoc</code><code class="p">]);</code>&#13;
	&#13;
<code class="c1">// Use the retriever to add the original chunks to the document store</code>&#13;
<code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">docstore</code><code class="p">.</code><code class="nx">mset</code><code class="p">(</code><code class="nx">keyValuePairs</code><code class="p">);</code>&#13;
	&#13;
<code class="c1">// Vectorstore alone retrieves the small chunks</code>&#13;
<code class="kr">const</code> <code class="nx">vectorstoreResult</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">vectorstore</code><code class="p">.</code><code class="nx">similaritySearch</code><code class="p">(</code>&#13;
  <code class="s1">'chapter on philosophy'</code><code class="p">,</code>&#13;
  <code class="mi">2</code>&#13;
<code class="p">);</code>&#13;
<code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="sb">`summary: </code><code class="si">${</code><code class="nx">vectorstoreResult</code><code class="p">[</code><code class="mi">0</code><code class="p">].</code><code class="nx">pageContent</code><code class="si">}</code><code class="sb">`</code><code class="p">);</code>&#13;
<code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code>&#13;
  <code class="sb">`summary retrieved length: </code><code class="si">${</code><code class="nx">vectorstoreResult</code><code class="p">[</code><code class="mi">0</code><code class="p">].</code><code class="nx">pageContent</code><code class="p">.</code><code class="nx">length</code><code class="si">}</code><code class="sb">`</code>&#13;
<code class="p">);</code>&#13;
	&#13;
<code class="c1">// Retriever returns larger chunk result</code>&#13;
<code class="kr">const</code> <code class="nx">retrieverResult</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="s1">'chapter on philosophy'</code><code class="p">);</code>&#13;
<code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code>&#13;
  <code class="sb">`multi-vector retrieved chunk length: </code><code class="si">${</code><code class="nx">retrieverResult</code><code class="p">[</code><code class="mi">0</code><code class="p">].</code><code class="nx">pageContent</code><code class="p">.</code><code class="nx">length</code><code class="si">}</code><code class="sb">`</code>&#13;
<code class="p">);</code></pre>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval" data-type="sect2"><div class="sect2" id="ch02_raptor_recursive_abstractive_processing_for_tree_1736545662502309">&#13;
<h2>RAPTOR: Recursive Abstractive Processing for <span class="keep-together">Tree-Organized Retrieval</span></h2>&#13;
&#13;
<p>RAG systems<a contenteditable="false" data-startref="multivector02" data-type="indexterm" id="id532"/><a contenteditable="false" data-startref="Ddecoupling02" data-type="indexterm" id="id533"/><a contenteditable="false" data-startref="JSretriev02" data-type="indexterm" id="id534"/> need to handle lower-level questions that reference specific facts found in a single document or higher-level questions that distill ideas that span many documents. Handling both types of questions can be a challenge with<a contenteditable="false" data-primary="k-nearest neighbors (k-NN)" data-type="indexterm" id="id535"/> typical k-nearest neighbors (k-NN) retrieval over document chunks.</p>&#13;
&#13;
<p><em>Recursive abstractive processing for tree-organized retrieval</em> (RAPTOR) is<a contenteditable="false" data-primary="recursive abstractive processing for tree-organized retrieval (RAPTOR)" data-type="indexterm" id="id536"/><a contenteditable="false" data-primary="RAPTOR (recursive abstractive processing for tree-organized retrieval)" data-type="indexterm" id="id537"/> an effective strategy that involves creating document summaries that capture higher-level concepts, embedding and clustering those documents, and then <a href="https://oreil.ly/VdIpJ">summarizing each cluster</a>.<sup><a data-type="noteref" href="ch02.html#id538" id="id538-marker">2</a></sup> This is done recursively, producing a tree of summaries with increasingly high-level concepts. The summaries and initial documents are then indexed together, giving coverage across lower-to-higher-level user questions. <a data-type="xref" href="#ch02_figure_6_1736545662484232">Figure 2-6</a> illustrates.</p>&#13;
&#13;
<figure><div class="figure" id="ch02_figure_6_1736545662484232"><img alt="Screenshot 2024-03-16 at 6.16.21 PM.png" src="assets/lelc_0206.png"/>&#13;
<h6><span class="label">Figure 2-6. </span>Recursively summarizing documents</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="ColBERT: Optimizing Embeddings" data-type="sect2"><div class="sect2" id="ch02_colbert_optimizing_embeddings_1736545662502384">&#13;
<h2>ColBERT: Optimizing Embeddings</h2>&#13;
&#13;
<p>One<a contenteditable="false" data-primary="ColBERT model" data-type="indexterm" id="id539"/><a contenteditable="false" data-primary="embeddings" data-secondary="optimizing" data-type="indexterm" id="id540"/> of the challenges of using embeddings models during the indexing stage is that they compress text into fixed-length (vector) representations that capture the semantic content of the document. Although this compression is useful for retrieval, embedding irrelevant or redundant content may lead to<a contenteditable="false" data-primary="hallucinations" data-type="indexterm" id="id541"/> hallucinations in the final LLM output.</p>&#13;
&#13;
<p>One solution to this problem is to do the following:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Generate contextual embeddings for each token in the document and query.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Calculate and score similarity between each query token and all document tokens.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Sum the maximum similarity score of each query embedding to any of the document embeddings to get a score for each document.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>This results in a granular and effective embedding approach for better retrieval. Fortunately, the embedding model called ColBERT<strong> </strong>embodies the solution to this problem.<sup><a data-type="noteref" href="ch02.html#id542" id="id542-marker">3</a></sup></p>&#13;
&#13;
<p>Here’s how we can utilize ColBERT for optimal embedding of our data:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="ColBERT model" data-type="indexterm" id="id543"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># RAGatouille is a library that makes it simple to use ColBERT</code>&#13;
<code class="c1">#! pip install -U ragatouille</code>&#13;
&#13;
<code class="kn">from</code> <code class="nn">ragatouille</code> <code class="kn">import</code> <code class="n">RAGPretrainedModel</code>&#13;
<code class="n">RAG</code> <code class="o">=</code> <code class="n">RAGPretrainedModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"colbert-ir/colbertv2.0"</code><code class="p">)</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">get_wikipedia_page</code><code class="p">(</code><code class="n">title</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Retrieve the full text content of a Wikipedia page.</code>&#13;
&#13;
<code class="sd">    :param title: str - Title of the Wikipedia page.</code>&#13;
<code class="sd">    :return: str - Full text content of the page as raw string.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="c1"># Wikipedia API endpoint</code>&#13;
    <code class="n">URL</code> <code class="o">=</code> <code class="s2">"https://en.wikipedia.org/w/api.php"</code>&#13;
&#13;
    <code class="c1"># Parameters for the API request</code>&#13;
    <code class="n">params</code> <code class="o">=</code> <code class="p">{</code>&#13;
        <code class="s2">"action"</code><code class="p">:</code> <code class="s2">"query"</code><code class="p">,</code>&#13;
        <code class="s2">"format"</code><code class="p">:</code> <code class="s2">"json"</code><code class="p">,</code>&#13;
        <code class="s2">"titles"</code><code class="p">:</code> <code class="n">title</code><code class="p">,</code>&#13;
        <code class="s2">"prop"</code><code class="p">:</code> <code class="s2">"extracts"</code><code class="p">,</code>&#13;
        <code class="s2">"explaintext"</code><code class="p">:</code> <code class="kc">True</code><code class="p">,</code>&#13;
    <code class="p">}</code>&#13;
&#13;
    <code class="c1"># Custom User-Agent header to comply with Wikipedia's best practices</code>&#13;
    <code class="n">headers</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"User-Agent"</code><code class="p">:</code> <code class="s2">"RAGatouille_tutorial/0.0.1"</code><code class="p">}</code>&#13;
&#13;
    <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">URL</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">)</code>&#13;
    <code class="n">data</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()</code>&#13;
&#13;
    <code class="c1"># Extracting page content</code>&#13;
    <code class="n">page</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="nb">iter</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="s2">"query"</code><code class="p">][</code><code class="s2">"pages"</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">()))</code>&#13;
    <code class="k">return</code> <code class="n">page</code><code class="p">[</code><code class="s2">"extract"</code><code class="p">]</code> <code class="k">if</code> <code class="s2">"extract"</code> <code class="ow">in</code> <code class="n">page</code> <code class="k">else</code> <code class="kc">None</code>&#13;
&#13;
<code class="n">full_document</code> <code class="o">=</code> <code class="n">get_wikipedia_page</code><code class="p">(</code><code class="s2">"Hayao_Miyazaki"</code><code class="p">)</code>&#13;
&#13;
<code class="c1">## Create an index</code>&#13;
<code class="n">RAG</code><code class="o">.</code><code class="n">index</code><code class="p">(</code>&#13;
    <code class="n">collection</code><code class="o">=</code><code class="p">[</code><code class="n">full_document</code><code class="p">],</code>&#13;
    <code class="n">index_name</code><code class="o">=</code><code class="s2">"Miyazaki-123"</code><code class="p">,</code>&#13;
    <code class="n">max_document_length</code><code class="o">=</code><code class="mi">180</code><code class="p">,</code>&#13;
    <code class="n">split_documents</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="c1">#query</code>&#13;
<code class="n">results</code> <code class="o">=</code> <code class="n">RAG</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">query</code><code class="o">=</code><code class="s2">"What animation studio did Miyazaki found?"</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>&#13;
<code class="n">results</code>&#13;
&#13;
<code class="c1">#utilize langchain retriever</code>&#13;
<code class="n">retriever</code> <code class="o">=</code> <code class="n">RAG</code><code class="o">.</code><code class="n">as_langchain_retriever</code><code class="p">(</code><code class="n">k</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>&#13;
<code class="n">retriever</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"What animation studio did Miyazaki found?"</code><code class="p">)</code></pre>&#13;
&#13;
<p>By using ColBERT, you can improve the relevancy of retrieved documents used as context by the LLM.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch02_summary_1736545662502441">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In<a contenteditable="false" data-primary="" data-startref="DIoptimiz02" data-type="indexterm" id="id544"/> this chapter, you’ve learned how to prepare and preprocess your documents for your LLM application using various LangChain’s modules. The document loaders enable you to extract text from your data source, the text splitters help you split your document into semantically similar chunks, and the embeddings models convert your text into vector representations of their meaning.</p>&#13;
&#13;
<p>Separately, vector stores allow you to perform CRUD operations on these embeddings alongside complex calculations to compute semantically similar chunks of text. Finally, indexing optimization strategies enable your AI app to improve the quality of embeddings and perform accurate retrieval of documents that contain semistructured data including tables.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580">Chapter 3</a>, you’ll learn how to efficiently retrieve the most similar chunks of documents from your vector store based on your query, provide it as context<em> </em>the model can see, and then generate an accurate output.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id478"><sup><a href="ch02.html#id478-marker">1</a></sup> Arvind Neelakantan et al., <a href="https://oreil.ly/YOVmh">“Text and Code Embeddings by Contrastive Pre-Training”</a>, arXiv, January 21, 2022. </p><p data-type="footnote" id="id538"><sup><a href="ch02.html#id538-marker">2</a></sup> Parth Sarthi et al., <a href="https://oreil.ly/hS4NB">“RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval”</a>, arXiv, January 31, 2024. Paper published at ICLR 2024. </p><p data-type="footnote" id="id542"><sup><a href="ch02.html#id542-marker">3</a></sup> Keshav Santhanam et al., <a href="https://oreil.ly/9spW2">“ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction”</a>, arXiv, December 2, 2021.</p></div></div></section></body></html>