<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">5 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/>Decision trees and gradient boosting</h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-134"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Decision trees and their ensembles</li>

    <li class="co-summary-bullet">Gradient boosting decision trees</li>

    <li class="co-summary-bullet">Scikit-learn’s gradient boosting decision trees options</li>

    <li class="co-summary-bullet">XGBoost algorithm and its innovations</li>

    <li class="co-summary-bullet">How LightGBM algorithm works</li>
  </ul>

  <p class="body">So far, we have explored machine learning algorithms based on linear models because they can handle tabular problems from datasets consisting of a few rows and columns and find a way to scale to problems of millions of rows and many columns. In addition, linear models are fast to train and get predictions from. Moreover, they are relatively easy to understand, explain, and tweak. Linear models are also helpful because they present many concepts we will keep building on in the book, such as L1 and L2 regularization and gradient descent.</p>

  <p class="body">This chapter will discuss a different classical machine learning algorithm: decision trees. Decision trees are the foundations of ensemble models such as random forests and boosting. We will especially focus on a machine learning ensemble algorithm, gradient boosting, and its implementations eXtreme Gradient Boosting (XGBoost) and Light Gradient Boosted Machines (LightGBM), which are considered state-of-the-art solutions for tabular data. <a id="idTextAnchor003"/><a id="idIndexMarker001"/><a id="idIndexMarker002"/></p>

  <h2 class="fm-head" id="heading_id_3">5.1 Introduction to tree-based methods</h2>

  <p class="body"><a id="marker-135"/>Tree-based models are a family of ensemble algorithms of different kinds and the favored methods for handling tabular data because of their performance and low re<a id="idIndexMarker003"/>quirements in data preprocessing. Ensemble algorithms are sets of machine learning models that contribute together toward a single prediction. All tree-based ensemble models are based on decision trees, a popular algorithm dating to the 1960s. The basic idea behind decision trees, no matter whether they are used for classification or regression, is that you can split your training set to have subsets where your prediction is more favorable because there is a predominant output class (in a classification problem) or there is a decreased variability of the target values (i.e., they are all very near; this refers to a regression problem instead).<a id="idIndexMarker004"/><a id="idIndexMarker005"/></p>

  <p class="body">Figure 5.1 shows a scheme of the key elements that make up a decision tree. The problem the decision tree is trying to solve is classifying an animal based on the number of legs and eyes. You start from the root of the tree, which corresponds to the entire dataset you have available, and set a condition for split. The condition is usually true<a id="idTextAnchor004"/>/false—a so-called binary split. Still, some variants of decision trees allow for multiple conditions applied at the same node, resulting in multiple splits, each decided based on a different value or label from the feature. Each branch leads to another node, where a new condition may be applied, or to a terminal node, which is used for predictions based on the instances that terminate ther<a id="idTextAnchor005"/>e.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F01_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.1 The key elements, such as roots, branches, and leaves, constituting a decision tree classifying animals based on the number of legs and eyes</p>
  </div>

  <p class="body">A split happens based on a deliberate search of the algorithm among features and, inside the feature, its observed values. As a splitting criterion in a classification problem, the decision trees algorithm searches for the best feature and feature value combination that splits the data into subsets with a homogeneous target. The homogeneity of a target in a subset is typically measured in a classification problem using criteria such as entropy, information gain, or Gini impurity:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Entropy</i> measures the degree of disorder or randomness in the distribution of labels in the subset. <a id="idIndexMarker006"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Information gain</i><b class="fm-bold">,</b> derived from entropy, measures the reduction in uncertainty about the class labels of the data, which is achieved by splitting the data based on a particular feature. <a id="idIndexMarker007"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Gini impurity</i> measures the probability of misclassifying a randomly chosen element in the subset if labeled randomly according to the distribution of labels in the subset. <a id="idIndexMarker008"/><a id="marker-136"/></p>
    </li>
  </ul>

  <p class="body">If the decision tree is being used for regression, it resorts to different splitting criteria than classification. In regression, the goal is to split the data into subsets to minimize the resulting mean squared error, the mean absolute error, or simply the variance of the target variable within each subset. In the training process, there’s an automatic selection of the best features, and most of the computations for a decision tree are to determine the best feature splits. However, once the tree is constructed, predicting the class label or target value for new data is relatively fast and straightforward, involving traversing the tree starting from the root and ending at the leaf based on the values of a limited set of features.</p>

  <p class="body">Decision trees are simple to compute and are also relatively easy to visualize. They don’t require scaling or modeling nonlinearities or otherwise transforming your features or output target because they can approximate any nonlinear relationship between target and predictors since they can consider separately single parts of their distribution. Basically, they are cutting a curve into parts so that each part looks like a line. On the other hand, decision trees are prone to overfitting and end up with an excessive number of splits that can fit the training data you are working on. Over time, different strategies have been devised to avoid overfitting:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Limiting the number of splits in the tree</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Pruning the splitting nodes backward after they have been built to reduce their overfitting</p>
    </li>
  </ul>

  <p class="body">Figure 5.2 shows a different perspective on a decision tree. Figure 5.1 visualizes a tree as a graph based on two features, while figure 5.2 visualizes a decision tree in terms of partitions of the data itself. Each split of the tree is a line in the chart, and there are seven vertical lines (thus a result of binary conditions on the feature on the x-axis) and three horizontal ones (thus on the feature on the y-axis) for a total of 10 splits. You can consider the decision tree a success because each class is well separated into its partitions (each partition is a terminal node in the end). However, by observation, it also becomes evident that specific partitions have been carved out just to fit examples in a certain position in the space. There are a few partitions containing only one single case. Any new example risks being incorrectly classified if it doesn’t perfectly fit the training distribution (an overfitting situati<a id="idTextAnchor006"/>on).</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F02_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.2 How a fully grown decision tree branching can also be interpreted as a series of dataset splits</p>
  </div>

  <p class="body">Figure 5.3 shows the same problem visualized with fewer splits—two for every feature. You can achieve this by pruning the previous tree splits backward, removing the ones enclosing too few training examples, or you accomplish that simply by limiting in the first place the tree’s growth—for instance, by imposing a maximum number of splits to be created. If you use fewer partitions, the tree may not fit the training data as perfectly as before. However, a simpler approach provides more confidence that new instances will likely be classified correctly, as the solution depends explicitly on single points in the training set.<a id="marker-137"/><a id="idTextAnchor007"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F03_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.3 The problem handled by a simpler decision tree, obtained by pruning or by limiting its growth</p>
  </div>

  <p class="body">Thinking of this algorithm in terms of underfitting and overfitting, it is a high-variance algorithm because its complexity always tends to exceed what should be given the problem and the data. It is not easy to find its sweet spot by tuning. In truth, the best way to use decision trees to achieve more accurate predictions is not as single models but as part of an ensemble of models. In subsequent subsections, we will explore ensemble methods such as bagging, random forests, and gradient boosting, an advanced method based on decision trees.</p>

  <p class="body">In this chapter, we’ll return to the Airbnb NYC dataset to illustrate the core gradient-boosted decision tree implementations and how the technique works. The code in the following listing reprises the data and some key functions and classes we previously used to illustrate other classical machine learning algori<a id="idTextAnchor008"/>thms.<a id="marker-138"/></p>

  <p class="fm-code-listing-caption">Listing 5.1 Reprising the Airbnb NYC dataset</p>
  <pre class="programlisting">import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
 
data = pd.read_csv("./AB_NYC_2019.csv")
excluding_list = ['price', 'id', 'latitude', 'longitude', 'host_id', 
                  'last_review', 'name', 'host_name']              <span class="fm-combinumeral">①</span>
low_card_categorical = ['neighbourhood_group',
   _    _               'room_type']                               <span class="fm-combinumeral">②</span>
high_card_categorical = ['neighbourhood']                          <span class="fm-combinumeral">③</span>
continuous = ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 
              'calculated_host_listings_count', 'availability_365']
target_mean = (
    (data["price"] &gt; data["price"].mean())
    .astype(int))                                                  <span class="fm-combinumeral">④</span>
target_median = (
    (data["price"] &gt; data["price"].median())
    .astype(int))                                                  <span class="fm-combinumeral">⑤</span>
target_multiclass = pd.qcut(
    data["price"], q=5, labels=False)                              <span class="fm-combinumeral">⑥</span>
target_regression = data["price"]                                  <span class="fm-combinumeral">⑦</span>
categorical_onehot_encoding = OneHotEncoder(handle_unknown='ignore')
categorical_ord_encoding = 
OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=np.nan)
numeric_passthrough = SimpleImputer(strategy=”constant", fill_value=0)
 
column_transform = ColumnTransformer(
    [('low_card_categories', 
      categorical_onehot_encoding, 
      low_card_categorical),
     ('high_card_categories', 
      categorical_ord_encoding, 
      high_card_categorical),
     ('numeric', 
      numeric_passthrough, 
      continuous),
    ],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                          <span class="fm-combinumeral">⑧</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> List of features to be excluded from data processing</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> List of low-cardinality categorical features to be one-hot encoded</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> List of high-cardinality categorical features to be ordinally encoded</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates a binary target indicating whether the price is above the mean (unbalanced binary target)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a binary target indicating whether the price is above the median (balanced binary target)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Creates a multiclass target by quantile binning the price into five classes</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Sets the target for regression as the price column</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Creates a column transformer that applies different transformations to different groups of features</p>

  <p class="body"><a id="marker-139"/>The code reads a CSV file containing data related to Airbnb listings in New York City in 2019 using the pandas library. It then defines several lists that categorize the features of the data into different types:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">excluding_list</code>—A list of features that should be excluded from the analysis, such as unique identifiers and text features</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">low_card_categorical</code>—A subset of categorical features that have a low cardinality (few unique values) and will be one-hot encoded</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">high_card_categorical</code>—A subset of categorical features that have a high cardinality (many unique values) and will be encoded using an ordinal encoding</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">continuous</code>—A list of continuous numerical features that will be standardized for analysis</p>
    </li>
  </ul>

  <p class="body">The code then creates several target variables based on the Price feature of the data:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">target_mean</code>—A binary variable indicating whether the price is higher than the mean price of all listings</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">target_median</code>—A binary variable indicating whether the price is higher than the median price of all listings</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">target_multiclass</code>—A variable with five classes based on the quantiles of the price distribution</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">target_regression</code>—The actual price values, which will be used for regression analysis</p>
    </li>
  </ul>

  <p class="body">All these targets allow us to deal with different regression and classification problems and thus test machine learning algorithms. In this chapter, we will always use <code class="fm-code-in-text">target_median</code>, but you can experiment with all the other targets by making small changes in the code.</p>

  <p class="body">Next, the code sets up several transformers to preprocess the data for the analysis in this chapter:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">categorical_onehot_encoding</code>—A one-hot encoding transformer for the low-cardinality categorical features</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">categorical_ord_encoding</code>—An ordinal encoding transformer for the high-cardinality categorical features</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">numeric_passthrough</code>—A transformer that simply passes through the continuous numerical features</p>
    </li>
  </ul>

  <p class="body">Finally, the code sets up a <code class="fm-code-in-text">ColumnTransformer</code> object that will apply the appropriate transformers to each subset of features based on their type. It applies one-hot encoding to the low-cardinality categorical features and passes through the continuous numerical features. The transformer is set to drop any features not explicitly included in the transformation steps and output concise feature names. The <code class="fm-code-in-text">sparse_threshold</code> parameter is set to zero to ensure that the transformer always returns dense arrays.<a id="idIndexMarker009"/><a id="idIndexMarker010"/></p>

  <p class="body">Listing 5.2 shows how a standard decision trees model is applied to our example problem. As in the examples seen in the previous chapter, we import the necessary modules from the Scikit-learn library, define a custom scoring metric based on accuracy, and set up a five-fold cross-validation strategy. Then we define a ColumnTransformer named <code class="fm-code-in-text">column_transform</code>, which orchestrates data preprocessing. It involves<a id="idIndexMarker011"/><a id="marker-140"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Transforming categorical variables using a function <code class="fm-code-in-text">categorical_onehot_encoding</code> for specific low-cardinality categorical columns<a id="idIndexMarker012"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Passing through numeric features with a function <code class="fm-code-in-text">numeric_passthrough</code> for continuous variables<a id="idIndexMarker013"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Dropping any remaining unprocessed columns (<code class="fm-code-in-text">remainder='drop'</code>)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Setting some options like suppressing verbose feature names and not applying sparse matrix representation</p>
    </li>
  </ul>

  <p class="body">At this point, a pipeline combining the ColumnTransformer with a decision tree classifier model is tested using cross-validation, which returns accuracy scores along with the average fit time and score time.</p>

  <p class="body">Under the hood of the cross-validation procedure and the data pipelining, the dataset is separated multiple times by the decision tree classifier during the training based on a splitting value from a feature. The procedure can be algorithmically explained as “greedy” because the decision tree picks the feature with the best split at each step without questioning whether alternatives could lead to a better result. Despite such a simple approach, decision trees are effective machine learning algorithms. The process goes on until there are no more splits that improve the training, as shown in the followin<a id="idTextAnchor009"/>g listing.</p>

  <p class="fm-code-listing-caption">Listing 5.2 A decision tree classifier</p>
  <pre class="programlisting">from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import make_scorer, accuracy_score
from sklearn.model_selection import KFold, cross_validate
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_passthrough, continuous)],              <span class="fm-combinumeral">①</span>
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)
 
model = DecisionTreeClassifier(random_state=0)                   <span class="fm-combinumeral">②</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                       <span class="fm-combinumeral">③</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)                <span class="fm-combinumeral">④</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                      <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a column transformer that applies different transformations to categorical and numeric features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> An instance of a decision tree classifier</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> A pipeline that sequentially applies column transformation and the decision tree model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A five-fold cross-validation using the defined pipeline, calculating accuracy scores, and returning additional information</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the mean and standard deviation of the accuracy scores from cross-validation</p>

  <p class="body">The result we obtain in terms of accuracy is</p>
  <pre class="programlisting">0.761 (0.005) fit: 0.22 secs pred: 0.01 secs</pre>

  <p class="body">The result could be better after comparing the results of our previous experiments with other machine learning algorithms. We can determine this because the decision tree has overfitted, and it ended up building too many ramifications. We can get better performance by limiting its growth by trial and error (you have to state the <code class="fm-code-in-text">max_depth</code> parameter to do so). However, there are even better ways to obtain improved results from this algorithm. In the next subsection, we will examine the first of such methods, which is based on multiple decision trees based on variations of the examples and the employed<a id="idTextAnchor010"/> features.<a id="idIndexMarker014"/><a id="marker-141"/></p>

  <h3 class="fm-head1" id="heading_id_4">5.1.1 Bagging and sampling</h3>

  <p class="body">We have examined all the single learning algorithms with decision trees. Ensembling algorithms of the same type is the next step that can help you achieve more predictive power on your problem. The idea is intuitive: if a single algorithm can perform at a certain level, using the insights of multiple models or chaining them together (so that one can learn from the results and errors of the other) should render even better results. There are two core ensemble strategies:<a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Averaging</i>—Predictions are obtained by averaging the predictions of multiple models. Differences in how the models are built, for instance by pasting, bagging, random subspaces, and random patches as we will see in this section, lead to different results. The best example of ensemble models of this kind is the random forests algorithm, which is built on an approach similar to random patches.<a id="idIndexMarker018"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Boosting</i>—Predictions are built as a weighted average of chained models, which are models sequentially built on the results of previous ones. The best example of a boosting algorithm is gradient boosting machines such as XGBoost and LightGBM.<a id="idIndexMarker019"/></p>
    </li>
  </ul>

  <p class="body">In the following subsection, we will look at random forests. Before delving into the random forests algorithm, it is necessary to spend some time on the other averaging approaches, not only because the random patches approach is built upon them but also because they point out solutions that are always worth applying to tabular data when you need to reduce the variance of the estimates, hence obtaining more reliable predictions, of any machine learning model you may want to use on your data.</p>

  <p class="body"><i class="fm-italics">Pasting</i> is the first approach to consider. Leo Breiman, the creator of the random forests algorithm, suggests that pasting consists of creating a set of different models trained on subsamples, obtained by sampling without replacement, of your training data. The models’ predictions are pooled together by averaging in the case of a regression problem or by majority voting in the case of a classification task.<a id="idIndexMarker020"/><a id="marker-142"/></p>

  <p class="body">The pros of pasting are</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Improvement of the results by reducing the variance of the predictions by only partially increasing their bias, which is a measure of how far the predictions of a model are from the true values</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Predictions that are more robust and less affected by outliers</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Reduction of the amount of data to learn at training time, thus reducing memory requirements</p>
    </li>
  </ul>

  <p class="body">The cons are</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Reduction of the amount of data available, which increases the bias because there is the chance of excluding important parts of the data distribution by sampling</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Very computationally intensive with complex algorithms</p>
    </li>
  </ul>

  <p class="body">The last con depends on your time constraints or available resources. Historically, averaging methods have been suggested to be applied using weak models (i.e., machine learning models that are very fast to be trained because of their simplicity, such as a linear regression or a k-nearest neighbors model). Practitioners observed that combining multiple weak models could beat the results of a single, more complex algorithm. However, weak models usually have a high bias problem, and by subsampling, you induce only some variance in their estimates, but their bias problem remains mostly untouched. Using an averaging approach has the main advantage in that it reduces the variance of the estimates by trading it with a bit more bias. Since weak models inherently carry a substantial bias, they might not achieve comparable results to the same approach applied to more complex models. In situations where more significant improvements are needed by reducing the variance of the estimates, employing an averaging strategy can be more effective with more complex models.</p>

  <p class="body"><i class="fm-italics">Bagging</i>, also suggested by Leo Breimar as a better solution, differs from pasting because you switch from subsampling to bootstrapping. Bootstrapping consists of sampling with replacement multiple times from a data sample to approximate the population distribution of a statistic. Bootstrapping is a frequently employed statistical technique that allows us to estimate the variability and uncertainty of statistics relative to the underlying data population from which our sample has been drawn. By using the information from the available sample through multiple resamples that mimic the original population’s behavior, bootstrapping emulates the population’s behavior without requiring explicit knowledge about its statistical distribution.<a id="idIndexMarker021"/></p>

  <p class="body">The reason for using bootstrapping in machine learning is to estimate the uncertainty of a model’s performance or to assess the distribution of a statistic. In addition, bootstrapping helps create more diverse variations of the original dataset for training and ensembling purposes. This is based on the observation that averaging multiple models reduces variance more if the predictions of the models being used are less correlated (i.e., more diverse). Subsampling creates diverse datasets to train. However, it has limitations because if you subsample aggressively—for instance, picking up less than 50% of the original data—you tend to introduce bias.</p>

  <p class="body">In contrast, if you subsample in a more limited way, such as using 90% of the data, the resulting subsamples will tend to be correlated. Instead, bootstrapping is more efficient because, on average, you use about 63.2% of the original data at each bootstrap. For a detailed statistical explanation of such calculated proportions, see the detailed cross-v<a id="idTextAnchor011"/>alidated answer at <a class="url" href="https://mng.bz/zZ0w">https://mng.bz/zZ0w</a>. Moreover, sampling with replacement tends to give results that mimic the original distribution of the data. Bootstrapping creates a set of more different datasets to learn from and, consequently, a set of more different predictions that can ensemble, reducing variance.</p>

  <p class="body">In fact, since in averaging we are building a distribution of predictions and getting the center of the distribution as our prediction, the more the averaged predictions resemble a random distribution, the less the center of the distribution will be biased by problems in the data picked up by the model (such as overfitting).</p>

  <p class="body"><a id="marker-143"/>By contrast, with <i class="fm-italics">random subspaces</i>, introduced by T. Ho [“The Random Subspace Method for Constructing Decision Forests,” <i class="fm-italics">Pattern Analysis and Machine Intelligence</i>, 20(8), 832-844, 1998], the sampling is limited only to features. This works because the model used for the ensemble is the decision tree, a model whose high variance of the estimates is greatly reduced in the ensemble by using only a part of the features for every model that is a part of it. The improved result is because the models trained on a subsample of the features tend to produce uncorrelated predictions—all the decision trees overfit the data but in a different way with respect to each other.<a id="idIndexMarker022"/></p>

  <p class="body">Finally, with <i class="fm-italics">random patches</i> [G. Louppe and P. Geurts, “Ensembles on Random Patches,” in <i class="fm-italics">Machine Learning and Knowledge Discovery in Databases</i> (2012): <span class="times">346–361</span>], sampling of both samples and features are used together to achieve even more uncorrelated predictions that can be averaged even more profitably.<a id="idIndexMarker023"/></p>

  <p class="body">Pasting, bagging, random subspaces, and random patches can all be implemented using Scikit-learn functions for bagging. The behavior of <code class="fm-code-in-text">BaggingClassifier</code> for classification tasks and <code class="fm-code-in-text">BaggingRegressor</code> for regression tasks can be controlled in regards to training data thanks to the following parameters:<a id="idIndexMarker024"/><a id="idIndexMarker025"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">bootstrap</code> </p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">max_sample</code> </p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">max_features</code> </p>
    </li>
  </ul>

  <p class="body">By combining them according to each averaging method’s specifications, you can obtain all four of the averaging strategies we have desc<a id="idTextAnchor012"/>ribed (see table 5.1).<a id="marker-144"/></p>

  <p class="fm-table-caption">Table 5.1 Bagging and sampling strategies</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Averaging strategy</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">What happens to data</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Parameters for BaggingClassifier/BaggingRegressor</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Pasting</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Training examples are sampled without replacement</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">bootstrap = False</code> </p>

          <p class="fm-table-body"><code class="fm-code-in-text">max_samples &lt; 1.0</code> </p>

          <p class="fm-table-body"><code class="fm-code-in-text">max_features = 1.0</code> </p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Bagging</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Training examples are sampled with replacement (bootstrapping)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">bootstrap = True</code> </p>

          <p class="fm-table-body"><code class="fm-code-in-text">max_samples = 1.0</code> </p>

          <p class="fm-table-body"><code class="fm-code-in-text">max_features = 1.0</code> </p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Random subspaces</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Features are sampled (without replacement)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">bootstrap = False</code> </p>

          <p class="fm-table-body"><code class="fm-code-in-text">max_samples = 1.0</code> </p>

          <p class="fm-table-body"><code class="fm-code-in-text">max_features &lt; 1.0</code> </p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Random patches</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Training examples and features are sampled without replacement</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">bootstrap = False</code> </p>

          <p class="fm-table-body"><code class="fm-code-in-text">max_samples &lt; 1.0</code> </p>

          <p class="fm-table-body"><code class="fm-code-in-text">max_features &lt; 1.0</code> </p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">By inputting the desired Scikit-learn model class in the parameter estimator, you can instead decide what algorithm to use for building the ensemble. A decision tree is the default, but you can decide what weak or strong models you prefer. In the following example, we apply a bagged classifier, setting the number of decision tree models to 300. The following listing shows all the models contributing together to improve the low performances that, as we have seen from listing 5.2, a decision tree tends to produce in this problem.</p>

  <p class="fm-code-listing-caption"><a id="idTextAnchor013"/>Listing 5.3 Bagged tree-based classifier</p>
  <pre class="programlisting">from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
model = BaggingClassifier(
    estimator=DecisionTreeClassifier(),                     <span class="fm-combinumeral">①</span>
    n_estimators=300, 
    bootstrap=True,                                         <span class="fm-combinumeral">②</span>
    max_samples=1.0,                                        <span class="fm-combinumeral">③</span>
    max_features=1.0,                                       <span class="fm-combinumeral">④</span>
    random_state=0)
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_passthrough, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                   <span class="fm-combinumeral">⑤</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                  <span class="fm-combinumeral">⑥</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)           <span class="fm-combinumeral">⑦</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}", 
      f"secs pred: {score_time:0.2f} secs")                 <span class="fm-combinumeral">⑧</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a BaggingClassifier ensemble model based on decision trees</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets bootstrap sampling for the BaggingClassifier</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Sets no sampling of features for the BaggingClassifier</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sets no sampling of data for the BaggingClassifier</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> A column transformer that applies different transformations to categorical and numeric features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> A pipeline that sequentially applies column transformation and the bagging classifier model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Five-fold cross-validation using the defined pipeline and calculating accuracy scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Prints the mean and standard deviation of the accuracy scores from cross-validation</p>

  <p class="body">The results take a little while more and look promising, but they are still not enough to compete with our previous solutions based on support vector machines and logistic regression:<a id="marker-145"/></p>
  <pre class="programlisting">0.809 (0.004) fit: 37.93 secs pred: 0.83 secs</pre>

  <p class="body">In the next subsection, we take the next step in ensembling by revisiting random forests, which make use of the random patches in bagging for a good reason.</p>

  <h3 class="fm-head1" id="heading_id_5">5.1.2 <a id="idTextAnchor014"/>Predicting with random forests</h3>

  <p class="body">Random forests work similarly to bagging. Still, it also simultaneously applies random patches (training examples and features are sampled without replacement): it bootstraps the samples before each model is trained and subsamples the features during modeling. Since the basic algorithm used in a random forests ensemble is a decision tree built by binary splits, the feature sampling happens at every tree split when a set of features is sampled as potential candidates for the split itself.<a id="idIndexMarker026"/><a id="idIndexMarker027"/></p>

  <p class="body">Allowing each decision tree in the ensemble to grow to its extremities may lead to overfitting the data and high variance in the estimates; employing bootstrapping and feature sampling might mitigate these problems. Bootstrapping ensures that the models are trained on slightly different data samples from the same distribution, while feature sampling at each split guarantees diverse tree structures. This combination helps generate a set of models that are quite distinct from one another. Different models produce very different predictions (hence, we can say that their predictions are quite uncorrelated), and that’s a great advantage for the averaging technique because, when ensembling to a single prediction vector, the result is more reliable and accurate predictions.</p>

  <p class="body"><a id="marker-146"/>Figure 5.4 shows how random forests work. The figure illustrates a binary classification problem with a dataset of two classes. The dataset is modeled using multiple decision trees, employing bootstrapping and feature sampling techniques. These techniques result in different partitions of the dataset, represented in the top-most part of the figure by three example results. The trees partition the dataset space in diverse ways, showcasing the variability in their splitting strategies. To simplify the representation, only two features are shown, providing a clearer understanding of the process.</p>

  <p class="body">Finally, when all the results are put together by majority voting, where you pick the more frequent classification as a predicted class, the random forests will provide better predictions derived from the results of all the trees. This is shown in the bottom part of the figure, where different shades indicate the prevalence of one or another class in a specific partition. The definitive boundary between classes is shown as a black polygonal line in the majority voting. The line can be even smoother when using multiple trees, resembling a curve. Ensemble methods can approximately resemble any curve if given enough models to the ensemble.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F04_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.4 How random forests arrives at its results by combining the different data partitioning due to its decision trees thanks to majority voting</p>
  </div>

  <p class="body">Originally devised by Leo Breiman and Adele Cutler <a id="idTextAnchor015"/>(<a class="url" href="https://mng.bz/0Qlp">https://mng.bz/0Qlp</a>), though commercially protected, the algorithm has been open-sourced—hence the many different names of its implementations. Random forests open up even more interesting possibilities, apart from better predictions, because you can use the algorithm to determine feature importance and measure the degree of similarity of the cases in a dataset.</p>

  <p class="body">In the example in listing 5.4, we test how random forests would work on our classification problem with the Airbnb NYC dataset. Apart from the algorithm, there are no differences from our standard data processing when applying decision trees. One-hot encoding turns low-categorical features into binaries, and numeric features are left as they are. <a id="idTextAnchor016"/><a id="marker-147"/></p>

  <p class="fm-code-listing-caption">Listing 5.4 Random forests classifier</p>
  <pre class="programlisting">from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
model = RandomForestClassifier(n_estimators=300, 
                               min_samples_leaf=3,
                               random_state=0)              <span class="fm-combinumeral">①</span>
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_passthrough, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                   <span class="fm-combinumeral">②</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                  <span class="fm-combinumeral">③</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)           <span class="fm-combinumeral">④</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}", 
      f"secs pred: {score_time:0.2f} secs")                 <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A RandomForestClassifier with 300 estimators and a minimum number of samples at a leaf node set to 3</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A column transformer that applies different transformations to categorical and numeric features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> A pipeline that sequentially applies column transformation and the random forests classifier model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A five-fold cross-validation using the defined pipeline and calculating accuracy scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the mean and standard deviation of the accuracy scores from cross-validation</p>

  <p class="body">After running the script, you will obtain the following results, which are actually the best performance you will find in this chapter for this problem:</p>
  <pre class="programlisting">0.826 (0.004) fit: 12.29 secs pred: 0.68 secs</pre>

  <p class="body">The secret to obtaining a good result with random forests is to choose its few hyperparameters wisely. Though the random forests algorithm is a no-brainer since it works fine with default parameters, fine-tuning it will bring better results. First, the purpose of the algorithm is to reduce the variance of the estimates, and that’s done by setting a high enough number of <code class="fm-code-in-text">n_estimators</code>. The principle is that if you have many trees, you have a distribution of results, and if the results are randomly drawn, you have an effect similar to the regression to the mean (the best prediction) due to the law of large numbers. The bootstrapping of examples and sampling of the features to be considered for splitting is usually enough to make the resulting trees of the forests different enough to be considered “random draws.” However, you need enough draws to have a proper regression to the mean.</p>

  <p class="body">You need some tests to fine-tune how many trees you build since there’s always a sweet spot: after a certain number of trees, you won’t obtain any more improvements, and sometimes some decrements in performance will result instead. Also, setting a too-high number of trees to be built by the algorithm will increase its computational costs, and more time will be needed in training and inference. However, no matter how many trees you train for, if your variance starts high using the default settings of a random forests model, you can do little to reduce it. Here the tradeoff between variance and bias comes into play; that is, you can trade some variance, implying you are overfitting the data, for some higher bias.</p>

  <p class="body">You can set a proper bias for random forests by making the following adjustments:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Setting a lower number of features to consider when looking for the best split by setting the <code class="fm-code-in-text">max_features</code> parameter</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Setting a max number of splits per tree, which will limit its growth to a certain predefined extent, by setting the <code class="fm-code-in-text">max_depth</code> parameter</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Setting a minimum number of examples in the terminal leaves of the tree, which will limit its growth, by setting the <code class="fm-code-in-text">min_samples_leaf</code> parameter with a number higher than 1</p>
    </li>
  </ul>

  <p class="body">In the next section, we explore extremely randomized trees (ERT), a variant of random forests that can be quite handy when data is larger and noisy.<a id="idTextAnchor017"/><a id="marker-148"/></p>

  <h3 class="fm-head1" id="heading_id_6">5.1.3 Resorting to extremely randomized trees</h3>

  <p class="body">ERT (also known as extra-trees in Scikit-learn) is a more randomized kind of random forests algorithm. The reason is the choice of candidates for splits in the ensemble’s single trees. In random forests, the algorithm samples its candidates for each split and then decides on the best feature to use among the candidates. Instead, the feature to be split in ERT is not evaluated among the possible candidates but randomly chosen. Afterward, the algorithm evaluates the best split point in the randomly chosen feature. This has some consequences. First, since the resulting trees are even more uncorrelated, there is even less variance in predictions from ERT—but at the price of a higher bias. Randomly split features have an effect on the accuracy of the predictions. Second, ERT is more computationally efficient because it doesn’t test sets of features but a single feature each time for the best split. All these characteristics make ERT best suited for handling<a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">High-dimensional data</i> because it will split features faster than any other decision-tree ensemble algorithm</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Noisy data</i> because the random feature and sample selection process can help reduce the influence of noisy data points, making the model more robust to extreme values</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Imbalanced data</i> because, due to the random feature selection, the signals from the minority subset of the data won’t be systematically excluded in favor of the majority subset of the data</p>
    </li>
  </ul>

  <p class="body">The following listing tests ERT by replacing the random forests in listing 5.4, where you built a model with the Airbnb NYC dataset to figure out if the price of a listing is above or below the median val<a id="idTextAnchor018"/>ue.<a id="marker-149"/></p>

  <p class="fm-code-listing-caption">Listing 5.5 ERTs classifier</p>
  <pre class="programlisting">from sklearn.ensemble import ExtraTreesClassifier
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
model = ExtraTreesClassifier(n_estimators=300, 
                             min_samples_leaf=3,
                             random_state=0)                  <span class="fm-combinumeral">①</span>
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_passthrough, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                     <span class="fm-combinumeral">②</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                    <span class="fm-combinumeral">③</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)             <span class="fm-combinumeral">④</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                   <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> An ExtraTreesClassifier with 300 estimators and a minimum number of samples at a leaf node set to 3</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A column transformer that applies different transformations to categorical and numeric features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> A pipeline that sequentially applies column transformation and the random forests classifier model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A five-fold cross-validation using the defined pipeline and calculating accuracy scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the mean and standard deviation of the accuracy scores from cross-validation</p>

  <p class="body">You obtain a bit better result than using random forests:</p>
  <pre class="programlisting">0.823 (0.004) fit: 4.99 secs pred: 0.42 secs</pre>

  <p class="body">If you run this example, you will see that training by ETR is much faster than by random forests, given that you use the same dataset and build the same number of trees. ETR becomes an interesting alternative when your dataset is larger (more cases) and even more when it is wider (more features) because it saves a lot of time picking the feature to split since it is randomly decided. By contrast, the random forests algorithm has to look for the best feature among a selection.</p>

  <p class="body">The fact that splits are decided randomly is a great advantage when many collinear and noisy features are related to the target. The algorithm avoids picking the same signals as an algorithm driven by searching for the feature that best fits the target. In addition, you can see the working dynamics in feature splitting of an ETR as another way to trade variance with bias. Splitting randomly is a limitation for the algorithm and reduces the variance because the resulting set of trees will be very uncorrelated.</p>

  <p class="body">In the next section, we complete our overview of tree-based ensembles by examining gradient boosting. This slightly different ensembling approach is often more effective for tabular data problems than bagging or random patch<a id="idTextAnchor019"/>es.<a id="marker-150"/></p>

  <h2 class="fm-head" id="heading_id_7">5.2 Gradient boosting</h2>

  <p class="body">In recent years, <i class="fm-italics">gradient boosting decision trees</i> (GBDT) has firmly established itself as a cutting-edge method for tabular data problems. GBDT is generally considered a state-of-the-art machine learning method across a wide range of problems across multiple domains, including multiclass classification, advertising click prediction, and search engine ranking. When applied to standard tabular problems, you can expect GBDT to perform better than neural networks, support vector machines, random forests, and bagging ensembles.<a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/></p>

  <p class="body">Above all, GBDT’s ability to handle heterogeneous features and its flexibility in the choice of the loss function and evaluation metrics make the algorithm most suitable for tabular data predictive modeling tasks. In sum, GBDT offers the following benefits for tabular data problems:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">With the proper hyperparameter tuning, it can achieve the best performance among all other techniques.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">There is no need for scaling or other monotone transformations of the features.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">It automatically captures nonlinear relationships in the data.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">It is robust to outliers and noisy data.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">It automatically handles missing data.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">It automatically selects the best features and can report their importance.</p>
    </li>
  </ul>

  <p class="body">All these characteristics depend on how the algorithm works, combining sequences of decision trees in a gradient descent optimization. In fact, in gradient boosting, starting from a constant value, you add tree models to an ensemble sequentially, each one correcting the errors of the previous models in a fashion similar to gradient descent optimization. Gradient boosting represents an evolution of the original boosting approach. Used in models such as Adaboost, in the original boosting, you just average models built on the residuals of the previous model.</p>

  <p class="body">In Adaboost, the algorithm fits a sequence of weak learners, any machine learning algorithm that consistently beats random guessing, to the data (for an explanation about how to choose weak learner<a id="idTextAnchor020"/>s, see <a class="url" href="https://mng.bz/KG9P">https://mng.bz/KG9P</a>). It then attributes more weight to incorrect predictions and less to correct ones. Weighting helps the algorithm to focus more on the observations that are harder to predict. The process is concluded after multiple corrections by a majority vote in classification or an average of the predictions in regression.</p>

  <p class="body">By contrast, in gradient boosting, you rely on a double optimization: first that of the single trees that strive to reduce the error based on their optimization function and then the general one, involving calculating the error from the summation of the boosted model, in a form that mimics gradient descent, where you gradually correct the model’s predictions. Since you have an optimization also based on a second level, the optimization based on the error of the entire ensemble procedure, gradient boosting is more versatile than the previously seen tree ensembles because it allows arbitrary loss functions to be used when calculating how much the summation of the predictions of models diverges from the expected results.</p>

  <p class="body">Figure 5.5 visually represents how the training error decreases after adding a new tree. Each tree takes part in a gradient descent style optimization, contributing to predicting the correction of the residual errors from the previous trees<a id="marker-151"/><a id="idTextAnchor021"/>.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F05_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.5 How gradient descent works with boosted trees</p>
  </div>

  <p class="body">If gradient descent provides optimal results and flexibility in the optimization, having decision trees as base learning (ensembles, as seen, are not limited to decision trees) offers various advantages. This is because it automatically selects the features it needs. It doesn’t need to specify a functional form (a formula as in regression), scaling, or linear relationships between features and the target.</p>

  <p class="body">In the next section, before seeing specific implementations in action (Scikit-learn, XGBoost, LightGBM), we will try building our simple implementation of gradient boosting to understand how to use this powerful algor<a id="idTextAnchor022"/>ithm.</p>

  <h3 class="fm-head1" id="heading_id_8">5.2.1 How gradient boosting works</h3>

  <p class="body">All implementations of GBDT offer a variety of hyperparameters that need to be set to get the best results on the data problem you are trying to solve. Figuring out what each setting does is a challenge, and being agnostic and leaving the task to an automatic tuning procedure doesn’t help too much since you will have challenges telling the tuning algorithm what to tune and how to tune.<a id="idIndexMarker034"/><a id="marker-152"/><a id="idIndexMarker035"/></p>

  <p class="body">In our experience, writing down a simple implementation is the best way to understand how an algorithm works and to figure out how hyperparameters relate to predictive performances and results. Listing 5.6 shows a <code class="fm-code-in-text">GradientBoosting</code> class capable of addressing any binary classification problem, such as the Airbnb NYC dataset we are tackling as an example, using two parameters for the gradient descent procedure and the parameters from the decision tree model offered by Scikit-learn.<a id="idIndexMarker036"/></p>

  <p class="body">The code creates a GradientBoosting class that comprises methods for fitting, predicting probabilities, and predicting class. Internally, it stores the sequence of fitted decision trees in a list, from where they can be accessed sequentially to reconstruct the following summation formula:</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F05_Ryan2-eqs-0x.png"/></p>
  </div>

  <p class="body">In the formula,</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><span class="times">H(X)</span> is the gradient boosting model applied to predictors <span class="times">X</span></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">M corresponds to the number of tree estimators used</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="cambria">ν</span> represents the learning rate</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times">w<sup class="fm-superscript">m</sup></span> instead represents the corrections from previous trees that have to be predicted</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The <span class="times">h<sup class="fm-superscript">m</sup></span> symbol refers to the mth decision tree used</p>
    </li>
  </ul>

  <p class="body">Interestingly, gradient boosting trees are always regression trees (even for classification problems)—hence our choice of using Scikit-learn’s DecisionTreeRegressor. This also explains why GBDT is better at predicting probabilities than other tree-based ensemble models: gradient boosting trees regress directly on the logit of class probability, thus optimizing in a fashion not too different from logistic regression. On the other hand, algorithms such as random forests are optimized for purity metrics, and they estimate probabilities by counting the fraction of a class in a terminal node, which is not truly a probability estimate. Generally, probabilities outputted by GBDTs are correct, and they rarely require subsequent probabilistic calibration, which is a post-processing step used to adjust predicted probabilities to improve their accuracy and reliability in applications where probability estimates are paramount, such as medical diagnosis (e.g., disease detection), fraud detection, or credit risk assessment. <a id="idIndexMarker037"/></p>

  <p class="body">In our code implementation, we allow passing any parameter for the DecisionTreeRegre<a id="idTextAnchor023"/>ssor (see <a class="url" href="https://mng.bz/9YQx">https://mng.bz/9YQx</a>), though the most useful are the ones related to the complexity of the tree development, such as <code class="fm-code-in-text">max_depth</code>, fixing the maximum depth of the tree, or <code class="fm-code-in-text">min_samples_split</code> and <code class="fm-code-in-text">min_samples_leaf</code>, the minimum number of samples needed to split an internal node or to be at a leaf node, respectively.<a id="idIndexMarker038"/><a id="idIndexMarker039"/><a id="idIndexMarker040"/><a id="idIndexMarker041"/></p>

  <p class="body">The role of each tree regressor is to provide a w vector containing the learned corrections to be summed together with the previous estimates after being weighted by the learning rate. Each w vector depends on the previous one because it is produced by a tree regressor trained on the gradients necessary to correct the estimates to match the true classification labels. The chained vectors w resemble a sequence of gradient corrections—at first large, then finer and finer, converging toward an optimal output prediction. Such gradient descent is perfectly similar to the gradient descent optimization procedure we introduced in chapter 4. In addition, by changing the cost function on which you base your gradients’ computation, you can ask the GBDT to optimize for different lo<a id="marker-153"/><a id="idTextAnchor024"/>ss functions.</p>

  <p class="fm-code-listing-caption">Listing 5.6 Building a gradient boosting classifier</p>
  <pre class="programlisting">from sklearn.tree import DecisionTreeRegressor
import numpy as np
 
class GradientBoosting():
    def __init__(self, learning_rate=0.1, n_estimators=10, **params):
        self.learning_rate = learning_rate
        self.n_estimators = n_estimators
        self.params = params
        self.trees = list()
        
    def sigmoid(self, x):
        x = np.clip(x, -100, 100)
        return 1 / (1 + np.exp(-x))                          <span class="fm-combinumeral">①</span>
 
    def logit(self, x, eps=1e-6):
        xp = np.clip(x, eps, 1-eps)
        return np.log(xp / (1 - xp))                         <span class="fm-combinumeral">②</span>
 
    def gradient(self, y_true, y_pred):
        gradient =  y_pred - y_true                          <span class="fm-combinumeral">③</span>
        return gradient
 
    def fit(self, X, y):
        self.init = self.logit(np.mean(y))                   <span class="fm-combinumeral">④</span>
        y_pred = self.init * np.ones((X.shape[0],))
        for k in range(self.n_estimators):
            gradient = self.gradient(self.logit(y), y_pred)
            tree = DecisionTreeRegressor(**self.params)
            tree.fit(X, -gradient)                           <span class="fm-combinumeral">⑤</span>
            self.trees.append(tree)
            y_pred += (
                self.learning_rate * tree.predict(X)
            )                                                <span class="fm-combinumeral">⑥</span>
 
    def predict_proba(self, X):
        y_pred = self.init * np.ones((X.shape[0],))
        for tree in self.trees:
            y_pred += (
                self.learning_rate * tree.predict(X)
            )                                                <span class="fm-combinumeral">⑦</span>
        return self.sigmoid(y_pred)
 
    def predict(self, X, threshold=0.5):
        proba = self.predict_proba(X)
        return np.where(proba &gt;= threshold, 1, 0)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sigmoid function implementation used for probability transformation that converts logits back into probabilities</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Logit function implementation used to transform probabilities into logits</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates the gradient of the loss function (negative log-likelihood) with respect to the predictions</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Initializes the model with the logit-transformed mean of the target values</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Fits a decision tree regressor to the negative gradient of the log-odds transformed target</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Updates the predicted values using the output of the fitted tree with a learning rate factor</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Predicting back requires cumulating predictions from all the trees.</p>

  <p class="body">As in the gradient descent we have seen applied to linear models, you rely on making the process stochastic to avoid the optimization being struck on a suboptimal solution, which is achieved by sampling rows or columns before the training of each decision tree. In addition, you use early stopping to prevent the GBDT from using too many decision trees in sequence and adapting too much to the training data. We will demonstrate early stopping in the next chapter.</p>

  <p class="body">Now that we have explained the inner workings of our GradientBoosting class, we can now experiment with it. We’ll use the Airbnb NYC dataset and begin by dividing it into a training set and a testing set. This will involve creating two lists of row indexes—one for the training set and one for the testing set—employing the Scikit-learn function <code class="fm-code-in-text">tra<a class="calibre" id="idTextAnchor025"/>in_test_split</code> (<a class="url" href="https://mng.bz/jp1z">https://mng.bz/jp1z</a>). We instantiate our GradientBoosting class, which requires a learning rate of 0.1 and 300 decision trees, with a maximum depth of four branches and terminal leaves with at least three examples. After transforming the training data, by treating numeric and categorical features, we fit the model, predict the test set, and evaluate<a id="idTextAnchor026"/> the results.<a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="marker-154"/></p>

  <p class="fm-code-listing-caption">Listing 5.7 Testing our gradient boosting class</p>
  <pre class="programlisting">from sklearn.model_selection import train_test_split
 
train, test = train_test_split(range(len(data)), test_size=0.2, 
random_state=0)                                              <span class="fm-combinumeral">①</span>
 
cls = GradientBoosting(n_estimators=300, 
                       learning_rate=0.1,
                       max_depth=4,
                       min_samples_leaf=3, 
                       random_state=0)                       <span class="fm-combinumeral">②</span>
 
X = column_transform.fit_transform(data.iloc[train])         <span class="fm-combinumeral">③</span>
y = target_median[train]                                     <span class="fm-combinumeral">④</span>
 
cls.fit(X, y)
 
Xt = column_transform.transform(data.iloc[test])             <span class="fm-combinumeral">⑤</span>
yt = target_median[test]                                     <span class="fm-combinumeral">⑥</span>
 
preds = cls.predict(Xt)
score = accuracy_score(y_true=yt, y_pred=preds)              <span class="fm-combinumeral">⑦</span>
print(f"Accuracy: {score:0.5f}")                             <span class="fm-combinumeral">⑧</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Splits the dataset indices into training and test sets using a fixed random seed</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Initializes a GradientBoosting model with specified hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Applies the column transformations to the training data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Extracts the target values corresponding to the training data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Applies the same column transformations to the test data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Extracts the target values corresponding to the test data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Calculates the accuracy score by comparing the predicted labels with the actual test labels</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Prints the calculated accuracy score</p>

  <p class="body">The evaluated accuracy on our test set is</p>
  <pre class="programlisting">Accuracy: 0.82503</pre>

  <p class="body">This is a very good result, pointing out that even our basic implementation is able to do a very good job on the data we are working with. In the next section, we will investigate the results obtained and observe a key characteristic of GBDT models that distinguishes them from other decision <a id="idTextAnchor027"/>tree ensembles.<a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="marker-155"/></p>

  <h3 class="fm-head1" id="heading_id_9">5.2.2 Extrapolating with gradient boosting</h3>

  <p class="body">In our implementation from scratch of a GBDT, we can visualize how the model fits the data by predicting the same training set. The visualization shown in figure 5.6, created by the small code snippet in listing 5.8, is a normalized density histogram. In a normalized density histogram, the height of each bar represents the relative frequency of data points falling within a specific bin, and the total area under the histogram becomes equal to 1. The result depicts a distribution of values predominantly polarized to the extremities of 0-1 boundaries, showing a model quite decisive in classify<a id="idTextAnchor028"/>ing the examples.<a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>

  <p class="fm-code-listing-caption">Listing 5.8 Plotting gradient boosting predicted probabilities</p>
  <pre class="programlisting">import matplotlib.pyplot as plt
 
proba = cls.predict_proba(Xt)                              <span class="fm-combinumeral">①</span>
plt.figure(figsize=(8, 6))
plt.hist(proba, 
         bins=30,
         density=True,
         color='blue',
         alpha=0.7)                                        <span class="fm-combinumeral">②</span>
plt.xlabel('Predicted Probabilities')
plt.ylabel('Density')
plt.title('Histogram of Predicted Probabilities')
plt.grid(True)
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates predicted probabilities for the test data using the trained model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a histogram of the predicted probabilities with specified bins and no<a id="idTextAnchor029"/>rmalized density</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F06_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.6 A histogram describing the fitted probabilities for a gradient boosting classification, showing how the model has strongly decided for most cases if they are positive or negative</p>
  </div>

  <p class="body">Our implementation, under the hood, uses a regression loss, the squared loss, whose gradient is equal to the residual of the probabilities transformed into logits. For a definit<a id="idTextAnchor030"/>ion of logits, see <a class="url" href="https://mng.bz/W214">https://mng.bz/W214</a>.<a id="marker-156"/></p>

  <p class="body">The logits for a probability p are calculated as</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F06_Ryan2-eqs-1x.png"/></p>
  </div>

  <p class="body">The advantage of this definition is that the logit function maps the probabilities to a log odds scale, which is an unbounded scale that ranges from negative infinity to positive infinity, allowing us to treat our problem as a regression problem.</p>

  <p class="body">This means that at each iteration, the gradient boosting algorithm fits a regression model to the gradient of the loss function with respect to the logit values, which corresponds to the difference between the logit of the true target values and the current predictions expressed in logits. This approach allows the algorithm to iteratively improve the predictions by adjusting them in the direction of the steepest descent of the loss function and to finally have a logit prediction that is bounded in the range between 0 and 1, thanks to the inverse function of the logit, the sigmoid. A sigmoid is a mathematical function that maps its input to a value between zero and one, providing a smooth and continuous curve.</p>

  <p class="body">The formula for the sigmoid is</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F06_Ryan2-eqs-2x.png"/></p>
  </div>

  <p class="body">where<a id="marker-157"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><span class="times"><i class="timestalic">σ</i>(<i class="timestalic">x</i>)</span> represents the sigmoid function applied to the input value <span class="times">x</span>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times">exp(–<i class="timestalic">x</i>)</span> is the exponential function, where exp denotes Euler’s number (approximately 2.71828) rais<a id="idTextAnchor031"/>ed to the power of <span class="times">–x</span>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times">1 + exp(–<i class="timestalic">x</i>)</span> is the denominator, which ensures that the output of the sigmoid function is always positive.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times">1 / (1 + exp(–<i class="timestalic">x</i>))</span> represents the division of 1 by the denominator, resulting in the output value of the sigmoid function.</p>
    </li>
  </ul>

  <p class="body">It is commonly used in machine learning and statistical models to convert logit predictions into probabilities.</p>

  <p class="body">What if, instead, we treat the problem as a regression one? In listing 5.9, we define a <code class="fm-code-in-text">GradientBoostingRegression</code> class by building on our <code class="fm-code-in-text">GradientBoosting</code> class and overwriting the fit and predict methods by removing logit and sigmoi<a id="idTextAnchor032"/>d transformations.<a id="idIndexMarker048"/><a id="idIndexMarker049"/></p>

  <p class="fm-code-listing-caption">Listing 5.9 Testing a gradient boosting regression class</p>
  <pre class="programlisting">class GradientBoostingRegression(GradientBoosting):
 
    def fit(self, X, y):
        self.init = np.mean(y)                              <span class="fm-combinumeral">①</span>
        y_pred = self.init * np.ones((X.shape[0],))
 
        for k in range(self.n_estimators):
            gradient = self.gradient(y, y_pred)
            tree = DecisionTreeRegressor(**self.params)
            tree.fit(X, -gradient)                          <span class="fm-combinumeral">②</span>
            self.trees.append(tree)
            y_pred += (
                self.learning_rate * tree.predict(X)
            )                                               <span class="fm-combinumeral">③</span>
 
    def predict(self, X):
        y_pred = self.init * np.ones((X.shape[0],))
        for tree in self.trees:
            y_pred += (
                self.learning_rate * tree.predict(X)
            )                                               <span class="fm-combinumeral">④</span>
        return y_pred
 
reg = GradientBoostingRegression(n_estimators=300,
                                 learning_rate=0.1,
                                 max_depth=4,
                                 min_samples_leaf=3, 
                                 random_state=0)
 
reg.fit(X, y)
 
proba = reg.predict(Xt)
plt.figure(figsize=(8, 6))
plt.hist(proba, 
         bins=10,
         density=True,
         color='blue',
         alpha=0.7)                                         <span class="fm-combinumeral">⑤</span>
plt.xlabel('Predicted Probabilities')
plt.ylabel('Density')
plt.title('Histogram of Predicted Probabilities')
plt.grid(True)
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initializes predictions with mean of y</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Fits the tree to the negative gradient</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Updates predictions with tree’s predictions scaled by learning rate</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Predicting back requires cumulating predictions from all the trees.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Plots a histogram of regression predicted probabilities</p>

  <p class="body">When the code in listing 5.9 is run, it will produce a histogram of the fit predictions, as shown in figure 5.7. Figure 5.7 shows how fit probabilities exceed the 0-1 boundaries. As with linear regression, which is a weighted combination based on the features, gradient boosting, a weighted combination based on the results of a chained sequence of models, can extrapolate beyond the limits of the learned target. Such extrapolations are impossible with other ensembles based on decision trees, such as random forests. Decision trees in regression cannot predict anything exceeding the values seen in training since predictions are based on means of training subsamples. The extrapolative potentiality of GBDT, based on the fact that they are an additive ensemble, is the basis for their success with time series, where you extrapolate future results that may be very different from past ones.</p>

  <p class="body">As a caveat, however, always consider that the extrapolative capabilities of GBDTs cannot be extended as far as could be achieved using a linear model. In time series predictive situations where the value to be predicted is way too far from the targets you provided for training, for instance in case of an outlier, the extrapolation will be limited, missing a correct estimation. A linear model, which directly associates a linear relationship between the input data and the predictions, could prove more suitable in such situations. Linear models are capable of handling extreme predictions for completely unseen outlying data points. To offer an alternative to decision trees as base learners in such situations, many GBDT implementations offer linear boosting by simply ensembling linear models (such as in the XGBoost implementation) or applying a piecewise linear gradient boosting tree, where linear models are built on the terminal nodes of a decisions trees (such as in LightGBM<a id="idTextAnchor033"/> implementation).<a id="marker-158"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F07_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.7 A histogram describing fitted probabilities with a gradient boosting regression model where some probabilities exceed the 0-1 boundary</p>
  </div>

  <p class="body">Also, the strength of GBDTs in time series problems relies on their being automatic in choosing the information for the predictions, with very few hyperparameters to set. All you need to do is to have enough examples, at least in the range of thousands of data points, and to have some careful engineering of time series features such as lagged values and moving averages at different time horizons. For shorter series, classical time series methods such as ARIMA or exponential smoothing are still the recommended choice. For complex problems such as hierarchically structured series, GBDTs can outperform even the most sophisticated deep learning architectures explicitly designed for time series data. For example, GBDTs excel in solving problems found in supermarket networks where both slow- and fast-moving goods are sold.</p>

  <p class="body">A clear example of the advantage of GBDTs in time series analysis has been demonstrated during the M5 forecasting competition recently held on Kaggle (<a class="url" href="https://github.com/Mcompetitions/M5-methods">https://github.com/Mcompetitions/M5-methods</a>), where solutions made by a LightGBM algorithm proved superior to deep learning architectures designed for the task of predicting in hierarchical structured series, such as DeepAR (<a class="url" href="https://arxiv.org/abs/1704.04110">https://arxiv.org/abs/1704.04110</a>) or NBEATS (<a class="url" href="https://arxiv.org/abs/1905.10437">https://arxiv.org/abs/1905.10437</a>). A lucid and insightful analysis of the competition and the success and ubiquity of tree-based methods in the practice of time series analysis can be found in the paper “Forecasting with Trees” by Tim Januschowski et al. [<i class="fm-italics">International Journal of Forecasting</i> 38.4<a id="idTextAnchor034"/> (2022): <span class="times">1473–1481</span>: <a class="url" href="https://mng.bz/8O4Z">ht<span id="idTextAnchor035">tps://mng.bz/8O4Z</span></a>].<a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="marker-159"/></p>

  <h3 class="fm-head1" id="heading_id_10">5.2.3 Explaining gradient boosting effectiveness</h3>

  <p class="body">Today, despite the remarkable results in image and text recognition and generation, neural networks do not match the performance on tabular data of gradient boosting solutions such as XGBoost and LightGBM. Both practitioners and participants in data science competitions favor these solutions. For instance, see the “State of Competitive Machine Learning” regarding tabular competitions at <a class="url" href="https://mlcontests.com/tabular-data/">https://mlcontests.com/tabular-data/</a>. But where exactly does the advantage that GBDTs have over deep neural networks (DNNs) come from? From our experience building a gradient boosting classifier, we could appreciate how the algorithm combines gradient descent with the flexibility of decision trees with heterogeneous data. Is this enough to explain why GBDTs are so effective with tabular data?<a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="idIndexMarker054"/></p>

  <p class="body">“Why Do Tree-Based Models Still Outperform Deep Learning on Typical Tabular Data?” by Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux (Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022: <a class="url" href="https://hal.science/hal-03723551v2/document">https://hal.science/hal-03723551v2/document</a>) is a recent study that tries to shed some light on the different performances of deep learning architectures and gradient-boosting decision trees. The study shows that tree-based methods outperform deep learning methods (even modern architectures) in achieving good predictions on tabular data. The authors explicitly focus on the heterogeneity of columns that distinguishes tabular data from datasets with exclusively continuous features (we could refer to them as homogenous tabular datasets) and define a standard benchmark using 45 open datasets. They considered only data from about 10,000 samples, consisting of columns of different types, including numerical features with different units and categorical features, because that’s considered the typical situation with tabular datasets.</p>

  <p class="body">Various deep learning models, including multilayer perceptrons (MLPs), ResNets, SAINT, and FTtransformer, were tried, but tree-based methods were found to have better performance with less hyperparameter tuning. Even when considering only numerical features, tree-based methods outperformed deep learning methods. This advantage became more pronounced when considering fitting time, although the hardware used (including GPUs) also influenced the results. The gap between the two methods was narrower on large datasets, which are not typical for tabular data.<a id="marker-160"/><a id="idIndexMarker055"/></p>

  <p class="body">The authors also investigated the features of tabular data that explain the performance difference between tree-based and deep learning methods. They found that smoothing the outcome in feature space narrowed the gap since deep architectures struggle with irregular patterns, whereas smoothness does not affect tree models. Removing uninformative features narrowed the gap for MLP-like neural architectures more. However, it was only after applying random rotations to the data that deep architectures outperformed tree models.</p>

  <p class="body">Random rotations refer to applying a random rotation matrix to the input features of a dataset before feeding them into a machine learning model. This rotation matrix is a square matrix that preserves the length of vectors and the angles between them, ensuring that the rotated data remains equivalent to the original data. Random rotations are used in machine learning for various purposes, including enhancing the diversity of ensembles, improving the robustness of models, and addressing rotation invariance in tasks such as computer vision and machine learning for quantum chemistry. Such a technique, perfectly reversible, however, tends to obscure the relationship between predictors and the target for tree-based algorithms, whereas deep learning models are unaffected thanks to their representative learning capacity to learn also the applied rotation.</p>

  <p class="body">This result doesn’t necessarily indicate an advantage of DNNs but rather a limitation of GBDTs. Deep architectures are rotationally invariant, which means they can detect rotated signals, as in image recognition, where certain images can be recognized regardless of their orientation. In contrast, GBDTs are not rotationally invariant and can only detect signals that are always oriented in the same fashion, since they operate based on splitting rules. Therefore, using any kind of rotation on data, such as a principal component analysis or a singular value decomposition, can be detrimental to GBDTs. DNNs, unaffected by rotation, can catch up in these situations.</p>

  <p class="body">Currently, this study reinforces our experience with GBDTs and their perceived strengths:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Can perform well even on datasets of moderate size (<span class="times">1,000–5,000 cases</span>) but outshines other algorithms from 10,000 to 100,000 samples (based on our experience)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Tend to excel with datasets that are heterogeneous in nature</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Robust against noise and irregularities in the target data</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Filter out noisy or irrelevant features due to their automatic feature selection process</p>
    </li>
  </ul>

  <p class="body">In addition to the strengths mentioned here, it should also be noted that GBDTs are often preferred over DNNs in certain scenarios. One reason is that GBDTs require less data preprocessing, making them more efficient and straightforward to implement. Moreover, GBDTs are as flexible as DNNs in terms of objective functions. In both cases, there are many to choose from, which can be especially useful in domains with complex optimization goals. Another benefit of GBDTs is that they offer more control over how their decision tree rules are built, providing users with some transparency and interpretability. Finally, GBDTs can train faster than DNNs in most cases, and they can also predict in a reasonable inference time, depending on their complexity, which can be a critical factor in real-time applications or time-sensitive tasks.</p>

  <p class="body">Now that you’ve gained an understanding of the fundamental concepts behind gradient boosting and its effectiveness in solving tabular data problems compared to deep learning, the next section will explore some of its implementations, starting with the one <a id="idTextAnchor036"/>provided by Scikit-learn.<a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="marker-161"/><a id="idIndexMarker059"/></p>

  <h2 class="fm-head" id="heading_id_11">5.3 Boosting in Scikit-learn</h2>

  <p class="body">Scikit-learn offers gradient boosting algorithms for both regression and classification tasks. These algorithms can be accessed through th<a id="idTextAnchor037"/>e GradientBoostingClassifier (<a class="url" href="https://mng.bz/Ea9o">https://mng.bz/Ea9o</a>) a<a id="idTextAnchor038"/>nd GradientBoostingRegressor (<a class="url" href="https://mng.bz/N1VN">https://mng.bz/N1VN</a>) classes, respectively. <a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/><a id="idIndexMarker063"/><a id="idIndexMarker064"/></p>

  <p class="body">The Scikit-learn implementation of gradient boosting was one of the earliest options available to Python users in data science. This implementation closely resembles the original proposal of the algorithm by Jerome Friedman in 1999 [“Greedy Function Approximation: A Gradient Boosting Machine,” <i class="fm-italics">Annals of Statistics</i> (2001): <span class="times">1189–1232</span>]. Let’s see the implementation in action in the next code listing, where we cross-validate the classifier performance on the Airbnb NY dataset to predict if a listing i<a id="idTextAnchor039"/>s above or below the median value.<a id="marker-162"/></p>

  <p class="fm-code-listing-caption">Listing 5.10 Scikit-l<a id="idTextAnchor040"/>earn gradient boosting classifier</p>
  <pre class="programlisting">from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
model = GradientBoostingClassifier(
    n_estimators=300,
    learning_rate=0.1,
    max_depth=4,
    min_samples_leaf=3, 
    random_state=0
)                                                            <span class="fm-combinumeral">①</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                   <span class="fm-combinumeral">②</span>
 
cv_scores = cross_validate(
    estimator=model_pipeline, 
    X=data, 
    y=target_median,
    scoring=accuracy,
    cv=cv, 
    return_train_score=True,
    return_estimator=True
)                                                            <span class="fm-combinumeral">③</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                  <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A GradientBoostingClassifier model with specified hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A pipeline that first applies data processing with column_transform and then fits the model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> A five-fold cross-validation using the defined pipeline, calculating accuracy scores, and returning additional information</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints the mean and standard deviation of the accuracy scores from cross-validation</p>

  <p class="body">Using the same set of parameters previously used, the performances we obtain are a bit better than those we obtained in our implementation:</p>
  <pre class="programlisting">0.826 (0.004) fit: 16.48 secs pred: 0.07 secs</pre>

  <p class="body">One notable aspect of this Scikit-learn implementation is that the training process can take a considerable amount of time, and the bottleneck can be attributed to the decision trees, the only supported model for building the sequential ensemble utilized by Scikit-learn itself. In exchange, you have some flexibility in parameters and control over how the GBDT uses single decision trees. For instance, Scikit-learn’s gradient boosting allows you to</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Define the <code class="fm-code-in-text">init</code> function. We used an average as the first estimator in our implementation. Here you can use whatever estimator you want as a starting point. Since gradient boosting is based on gradient descent and the gradient descent optimization process is sensitive to the starting point, this can prove an advantage when solving more complicated data problems. <a id="idIndexMarker065"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Revert the algorithm to Adaboost, the original algorithm that inspired sequential ensembles, by training a <code class="fm-code-in-text">GradientBoostingClassifier</code> with the exponential loss (<code class="fm-code-in-text">loss="exponential"</code>).<a id="idIndexMarker066"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Control in detail the complexity of the decision trees used, implying that you can model more complex data at risk of overfitting by means of parameters such as</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">min_samples_split</code> for the minimum number of samples required to split an internal node<a id="idIndexMarker067"/></p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">min_sample_leaf</code> for the minimum number of samples needed to be at a leaf node<a id="idIndexMarker068"/></p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">min_weight_fraction_leaf</code> for the minimum weighted fraction of the total of weights of all the input samples required at a leaf node<a id="idIndexMarker069"/><a id="marker-163"/></p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">max_depth</code> for the max depth of the tree<a id="idIndexMarker070"/></p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">min_impurity_decrease</code> as the threshold in impurity decrease, used to decide whether to split or stop the growth of the tree<a id="idIndexMarker071"/></p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">max_leaf_nodes</code> as the maximum reachable number of final nodes before stopping growing the tree<a id="idIndexMarker072"/></p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">If controlling the growth of the decision trees is not enough, once they are grown, you can reduce their complexity using the <code class="fm-code-in-text">ccp_alpha</code> parameter. This parameter reduces the trees backward from the final nodes by removing the nod<a id="idTextAnchor041"/>es that do not pass a complexity test (see <a class="url" href="https://mng.bz/DM9n">https://mng.bz/DM9n</a> for details).<a id="idIndexMarker073"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Subsample both rows (<code class="fm-code-in-text">subsample</code>) and columns (<code class="fm-code-in-text">max_features</code>), a particularly effective way to reduce overfitting and increase the training model’s generalizability.<a id="idIndexMarker074"/><a id="idIndexMarker075"/></p>
    </li>
  </ul>

  <p class="body">In addition, the implementation also offers support for sparse data and early stopping, a procedure to prevent overfitting in GBDT and neural networks that will be discussed in detail in the next chapter.</p>

  <p class="body">Also, on the side of the provided outputs, the support offered by this version is quite impressive, making it a perfect tool for understanding and explaining why your GBDT model came to certain predictions. For instance, you can access all the decision trees used and require the raw values predicted from the trees of the ensemble, both as a total (<code class="fm-code-in-text">decision_function</code> method) or as a sequence of steps (<code class="fm-code-in-text">staged_decision_function</code> method). <a id="idIndexMarker076"/><a id="idIndexMarker077"/></p>

  <p class="body">Recently, this implementation has been less used by practitioners because of the faster, more performing solutions offered by XGBoost, LightGBM, and Scikit-learn, with its HistGradientBoosting. However, it remains an interesting choice for smaller datasets if you want to control certain aspects of the gradient boosting procedure. In the next section, we will explore XGBoost and determine how it can be a more powerful<a id="idTextAnchor042"/> choice for solving your tabular data problems.</p>

  <h3 class="fm-head1" id="heading_id_12">5.3.1 Applying early stopping to avoid overfitting</h3>

  <p class="body"><a id="marker-164"/>The presentation of the original Scikit-learn classes for gradient boosting offers a chance to introduce a procedure that can help control overfitting. The procedure is early stopping, a method originally used in gradient descent to restrict the number of iterations when further adjustments to the coefficients under optimization would lead to no enhancements or a poor generalization of the solution. The method has also been used to train neural networks. In gradient boosting, which takes gradient descent as part of its optimization process, the method can help solve the same problem: limiting the number of added decision tree models to reduce the computational burden and avoid possible overfitting.<a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="idIndexMarker082"/><a id="idIndexMarker083"/></p>

  <p class="body">Early stopping works in a few simple steps:</p>

  <ol class="calibre7">
    <li class="fm-list-bullet">
      <p class="list">A fraction of the training dataset is set aside to form a validation set.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">During each iteration of the training process, the resulting partial model is evaluated using the validation set.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The performance of the partial model on the validation set is recorded and compared with previous results.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">If the model’s performance does not improve, the algorithm increases its counting (commonly called <i class="fm-italics">patience</i>) of how many iterations since its last improvement. Otherwise, it resets the counting.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The training process is stopped if there has been no improvement over a certain number of iterations.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Otherwise, the training process continues for another iteration unless all the designated decision trees to be boosted have been completed. At this point, the training process is halted.</p>
    </li>
  </ol>

  <p class="body">Figu<a id="idTextAnchor043"/>re 5.8 shows this process in a process flow chart.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F08_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.8 A process flow chart describing how early stopping works in GBDTs</p>
  </div>

  <p class="body">Figure 5.9 shows the very same process from the point of view of the validation metric, which can be the loss or any other metric for which you want to get the best result. As the iterations progress, it is customary to observe training errors decrease. Validation error, on the other hand, tends to have a sweet point before the algorithm starts overfitting. Early stopping helps to catch this increase in validation error, and monitoring the validation error dynamics allows you to retrace <a id="idTextAnchor044"/>to the iteration before any overfitting happens.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F09_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.9 How validation error and training error generally behave when using GBDTs through multiple iterations</p>
  </div>

  <p class="body"><a id="marker-165"/>Different stopping points may be found for each model trained on different folds in a cross-validation process with early stopping. When training on the entire dataset, you can still rely on using early stopping based on a validation sample. Hence, you just need to set a high number of iterations and see when the training stops. Otherwise, you can use a fixed number of iterations based on the stopping iterations you observed in cross-validation. In this case, you can calculate the average or median of all stopping points seen in cross-validation to determine the number of boosting trees to use. However, there’s no fixed rule, and you may choose to use the second maximum value for an aggressive stopping policy or the second minimum value for a conservative one.</p>

  <p class="body">Additionally, consider increasing the number of boosting trees since training is done on more data than during cross-validation. As a general guideline, increase the number of boosting trees by a percentage equivalent to dividing one by the number of folds you used in cross-validation. However, since no one-size-fits-all solution exists, experimentation may be necessary to find the best approach for your problem.</p>

  <p class="body">As an example, we run the previous code again, this time setting a higher number of base estimators to be used and two parameters, <code class="fm-code-in-text">validation_fraction</code> and <code class="fm-code-in-text">n_iter_no_change</code>, that activate the early stopping procedure. The parameter <code class="fm-code-in-text">validation_fraction</code> determines the fraction of the training data to be used for validation, and it is effective only when <code class="fm-code-in-text">n_iter_no_change</code> is set to an integer indicating how many iterations should pass without improvements when testing the model<a id="idTextAnchor045"/> on the validation set before stopping the process.<a id="idIndexMarker084"/><a id="idIndexMarker085"/><a id="marker-166"/></p>

  <p class="fm-code-listing-caption">Listing 5.11 Applying early stopping with GradientBoostingClassifier</p>
  <pre class="programlisting">model = GradientBoostingClassifier(
    n_estimators=1000,                                      <span class="fm-combinumeral">①</span>
    learning_rate=0.1,
    validation_fraction=0.2,                                <span class="fm-combinumeral">②</span>
    n_iter_no_change=10,                                    <span class="fm-combinumeral">③</span>
    max_depth=4,
    min_samples_leaf=3, 
    random_state=0
)
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f} secs pred: {score_time:0.2f} secs")
iters = [cv_scores["estimator"][i].named_steps["modeling"].n_estimators_ 
         for i in range(5)]                                  <span class="fm-combinumeral">④</span>
print(iters)                                                 <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A GradientBoostingClassifier model whose iterations are raised to 1,000 from the previous 300</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> As a validation fraction, the GradientBoostingClassifier uses 20% of the training data for validation.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The training of the GradientBoostingClassifier will stop after 10 iterations without improvements on the validation.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Extracts the number of estimators used during training for each fold’s estimator</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the list of the number of estimators for each fold’s estimator</p>

  <p class="body">The output is</p>
  <pre class="programlisting">0.826 (0.005) fit: 6.24 secs pred: 0.04 secs
[145, 109, 115, 163, 159]</pre>

  <p class="body">Since the mean number of iterations for cross-validation folds is 268 iterations, based on a rule of thumb, when using all the available data during the training phase, we suggest increasing the number of iterations by 20%, fixing it at 322 iterations.</p>

  <p class="body">In the following sections, we will introduce new implementations of gradient boosting, such as XGBoost and LightGBM. We w<a id="idTextAnchor046"/>ill also present how to use early stopping with them.<a id="idIndexMarker086"/><a id="idIndexMarker087"/><a id="idIndexMarker088"/><a id="idIndexMarker089"/><a id="idIndexMarker090"/><a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="idIndexMarker093"/><a id="idIndexMarker094"/></p>

  <h2 class="fm-head" id="heading_id_13">5.4 Using XGBoost</h2>

  <p class="body">XGBoost gained traction after being successful in a Kaggle competition, the Higgs Boson Machine Learning Challenge (<a class="url" href="https://www.kaggle.com/c/higgs-boson">https://www.kaggle.com/c/higgs-boson</a>), where XGBoost has been proposed in the competition forums as a fast and accurate solution in comparison to Scikit-learn’s gradient boosting. XGBoost has since been adopted and successfully used in many other data science competitions, proving its effectiveness and the usefulness of Kaggle competitions as a good place to introduce innovations that disrupt performance benchmarks. Keras is another example of an innovation that was widely adopted after success in Kaggle competitions. At the time of writing, the XGBoost package had been updated and reached the milestone of version 2.0.3, which is the version we have used in this book.<a id="idIndexMarker095"/><a id="idIndexMarker096"/><a id="marker-167"/></p>

  <p class="body">Initially conceived as a research project by Tianqi Chen, later further developed with the contribution of Carlos Guestrin, XGBoost, is a gradient boosting framework available as open-source software. It is noticeable that, contrary to other initiatives, such as Ligh<a id="idTextAnchor047"/>tGBM, sponsored by Microsoft, and Yggdrasil Decision Forests (see <a class="url" href="https://mng.bz/lYV6">https://mng.bz/lYV6</a>) that Google sponsors, XGBoost remained completely independent, maintained by the Distributed (Deep) Machine Learning Common community (<a class="url" href="https://dmlc.github.io/">dmlc.github.io</a>).</p>

  <p class="body">Over time, the framework has undergone significant enhancements and now provides advanced capabilities for distributed processing and parallelization, which enables it to operate on large-scale datasets. Meanwhile, XGBoost has also gained wide adoption, and it is currently accessible in several programming languages, including C/C++, Python, and R. Moreover, this framework is supported on multiple data-science platforms, such as H2O.ai and Apache Spark.</p>

  <p class="body">As a data science user, you will immediately notice several key characteristics of this framework, including</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The ability to handle various input data types</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Support for customized objective and evaluation functions</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Automatic handling of missing values</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Easy support for GPU training</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Accommodation of monotonicity and feature interaction constraints</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Optimization of multiple cores and cache on standalone computers</p>
    </li>
  </ul>

  <p class="body">From a system performance perspective, notable features include</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Networked parallel training, which allows for distributed computing across a cluster of machines</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Utilization of all available CPU cores during tree construction for parallelization</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Out-of-core computing when working with large datasets that don’t fit into memory</p>
    </li>
  </ul>

  <p class="body">However, what sets the “extreme” XGBoost algorithm apart is its innovative algorithmic details for optimization, including a variant of gradient descent known as Newton Descent and regularization terms, as well as its unique approach to feature splitting and sparse data handling. The following section briefly summarizes these groundbreaking techniques that power XGBoost’s performance.<a id="marker-168"/></p>

  <p class="body">Now let’s try the algorithm with our classification task on the Airbnb NYC dataset. In this case, use the <code class="fm-code-in-text">XGBClassifier</code> from XGBoost. For regression problems, you can use the <code class="fm-code-in-text">XGBRegressor</code> class. First, however, you need to have XGBoost installed on your system. To install XGBoost, you can just pip install it:<a id="idIndexMarker097"/><a id="idIndexMarker098"/></p>
  <pre class="programlisting">pip install XGBoost</pre>

  <p class="body">Or use conda for the job:</p>
  <pre class="programlisting">conda install -c conda-forge py-XGBoost</pre>

  <p class="body">The installation commands should do all the necessary steps for you; also install both CPU and GPU variants of the algorithm, if possible, on your s<a id="idTextAnchor048"/>ystem. See instructions and details on the installation process at <a class="url" href="https://mng.bz/BXd0">https://mng.bz/BXd0</a>.</p>

  <p class="body">In the following listing, we replicate the same approach we previously used with Scikit-learn’s GradientBoostingClassifier: we boost 300 trees, limit the depth of decision t<a id="idTextAnchor049"/>rees to four levels, and accept nodes of at least three examples.</p>

  <p class="fm-code-listing-caption">Listing 5.12 XGBoost classifier</p>
  <pre class="programlisting">from XGBoost import XGBClassifier
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
xgb = XGBClassifier(booster='gbtree',                        <span class="fm-combinumeral">①</span>
                    objective='reg:logistic',                <span class="fm-combinumeral">②</span>
                    n_estimators=300,
                    max_depth=4,
                    min_child_weight=3)                      <span class="fm-combinumeral">③</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('XGBoost', xgb)])
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                  <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates an XGBClassifier model with specified hyperparameters, including the booster type</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The learning objective, equivalent to Scikit-learn’s loss</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> min_child_weight is equivalent to Scikit-learn’s min_samples_leaf.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints the mean and standard deviation of cross-validated test scores</p>

  <p class="body">The obtained result is the best obtained so far, and it is impressive that the training fit is just a fraction of what previously the Scikit-learn implementation took:</p>
  <pre class="programlisting">0.826 (0.004) fit: 0.84 secs pred: 0.05 secs</pre>

  <p class="body">In the next subsection, we explore the key parameters we used for running this example and <a id="idTextAnchor050"/>discuss what parameters, of the many offered by the algorithm (see <a class="url" href="https://mng.bz/dX6N">https://mng.bz/dX6N</a> for a complete list) you nee<a id="idTextAnchor051"/>d for running successfully any tabular data project using XGBoost.</p>

  <h3 class="fm-head1" id="heading_id_14">5.4.1 XGBoost’s key parameters</h3>

  <p class="body"><a id="marker-169"/>Let’s review the specific options we decided on in our previous example shown in listing 5.12, beginning with the <code class="fm-code-in-text">n_estimators</code> parameter, which specifies the number of decision trees involved in building the ensemble and is part of the gradient descent process that we discussed previously in this chapter. <a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="idIndexMarker101"/></p>

  <p class="body">The <code class="fm-code-in-text">n_estimators</code> parameter in XGBoost determines the number of decision trees used to produce the output. In standard tabular problems, the usual values for this parameter range between 10 to 10,000. While increasing this value can improve prediction performance by involving more weak learners, it can also slow down the training time. It’s worth noting that there is an ideal number of trees that maximizes performance on prediction tasks with unseen data, and finding this sweet spot depends on other XGBoost parameters, such as the learning rate. To achieve a high-performing XGBoost model, it’s important to choose the appropriate number of trees based on the problem at hand while setting the other parameters correctly, including the learning rate.<a id="idIndexMarker102"/></p>

  <p class="body">Whereas Scikit-learn is limited to just decision trees, XGBoost proposes more choices using its booster parameter:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">gbtree</code>—Decision trees, as you would expect in gradient boosting</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">gblinear</code>—Linear regression models</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">dart</code>—Decision trees, but the optimization process is more regularized</p>
    </li>
  </ul>

  <p class="body">The <code class="fm-code-in-text">gblinear</code> booster produces a sum of chained linear models. Since a sum of linear combinations is linear, you end up with a coefficient for each feature you have used, similar to a linear model. You can access the coefficients using the <code class="fm-code-in-text">.coef</code> method. It is a different way to fit a GBDT model with an emphasis on interpretability since the model can be reduced to a linear combination, different from the one you could directly fit because of using a different approach for complexity penalization and optimization. The most notable difference is that you cannot interpret the coefficients as you would if a linear regression or a generalized linear model produced them. Furthermore, the interpretation of the intercept generated by the <code class="fm-code-in-text">gblinear</code> booster differs from classical linear models as it is influenced by both the learning rate and the initial estimate employed by the booster.<a id="idIndexMarker103"/><a id="marker-170"/><a id="idIndexMarker104"/></p>

  <p class="body">The <code class="fm-code-in-text">dart</code> booster is different because it combines the optimization based on gradient descent with an approach similar to dropout, a technique used in deep learning. Presented by a UC Berkeley researcher and a Microsoft researcher in the paper “DART: Dropouts Meet Multiple Additive Regression Trees” by Rashmi Korlakai Vinayak and Ran Gilad-Bachrach (Artificial Intelligence and Statistics. PMLR, 2015), DART focuses on overfitting due to the dependence of each decision tree’s estimates on the previous ones. The researchers then take the idea of dropout from deep learning, where a dropout mask randomly and partially blanked a neural network layer. The neural network cannot always rely on certain signals in a particular layer to determine the next layer’s weights. In DART, the gradients are not calculated compared to the sum of the residuals from all previously built trees. Still, instead, the algorithm at each iteration randomly selects a subset of the previous trees and scales their leaves by a factor of 1/k, where k is the number of trees that were dropped.<a id="idIndexMarker105"/></p>

  <p class="body">The <code class="fm-code-in-text">gblinear</code> and the <code class="fm-code-in-text">dart</code> are the only alternative boosters available. For instance, there is no booster to mimic random forests (as there is for another GBDT implementation, LightGBM). However, though a random forests booster is not supported yet by <code class="fm-code-in-text">XGBClassifier</code> and <code class="fm-code-in-text">XGBRegression</code>, you can obtain a similar result by playing with XGBoost parameters and functions:<a id="idIndexMarker106"/><a id="idIndexMarker107"/><a id="idIndexMarker108"/><a id="idIndexMarker109"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Use the <code class="fm-code-in-text">num_parallel_tree</code> parameter and set it to a number above 1. At each step of the optimization, the gradients are estimated not from a single decision tree but from a bagged ensemble of decision trees, thus creating a boosted random forests model. In some instances, this approach may provide better results than the gradient boosting approach because it will reduce the variance of the estimates at the expense of an increased computational cost.<a id="idIndexMarker110"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Use the <code class="fm-code-in-text">XGBRFClassifier</code> or <code class="fm-code-in-text">XGBRFRegressor</code>, two classes from XGBoost that implement a ran<a id="idTextAnchor052"/>dom forests approach. The classes are still experimental. For more details see <a class="url" href="https://mng.bz/rKVB">https://mng.bz/rKVB</a> with the caveat that there are some differences with the random forests algorithm offered by Scikit-learn because XGBoost computes a matrix made up of second derivatives and called the Hessian (see <a class="url" href="https://brilliant.org/wiki/hessian-matrix/">https://brilliant.org/wiki/hessian-matrix/</a> for a mathematical definition) to weight the gradients and it has no bootstrap capability. Hence, your results will be different.<a id="idIndexMarker111"/><a id="idIndexMarker112"/></p>
    </li>
  </ul>

  <p class="body">As for the loss function, controlled by the parameter <code class="fm-code-in-text">objective</code>, we chose <code class="fm-code-in-text">reg:logistic</code>, but we could have also chosen <code class="fm-code-in-text">binary:logistic</code>, both comparable to log-loss for binary classification. In XGBoost, you have loss functions organized into six classes:<a id="idIndexMarker113"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">reg</code> for regression problems (but you also have the logistic regression among its options).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">binary</code> for binary classification problems.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">multi</code> for multiclass classification.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">count</code> for count data—that is, discrete events.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">survival</code> for survival analysis, which is a statistical technique to analyze data on the time it occurs for an event of interest to occur, such as the failure of a part in a machinery. It considers censoring, where the event of interest has not yet happened for some individuals in the study.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">rank</code> for ranking problems, such as estimating what rank a site should have in the results.</p>
    </li>
  </ul>

  <p class="body">Apart from Poisson distribution, useful for modeling the frequency of events, XGBoost also offers <code class="fm-code-in-text">reg:gamma</code> and <code class="fm-code-in-text">reg:tweedie</code>, optimizing for two distributions used in insurance for claim amount modeling as mentioned in chapter 4 when discussing generalized linear models.</p>

  <p class="body"><a id="marker-171"/>The availability of various objective functions demonstrates the multiple possible applications that XGB<a id="idTextAnchor053"/>oost may have in different domains. For a full overview of the loss functions see <a class="url" href="https://mng.bz/dX6N">https://mng.bz/dX6N</a>. Loss functions are essential in gradient boosting, as they define the optimization objective. By contrast, the evaluation metrics are not used to optimize the gradient descent in gradient boosting. However, they play a crucial role in monitoring the training process, optimizing for feature selection, hyper-parameter optimization, and even enabling early stopping to halt the training once it is no longer providing benefit. The equivalent of Scikit-learn’s <code class="fm-code-in-text">min_samples_leaf</code> in XGBoost is <code class="fm-code-in-text">min_child_weight</code>. Both parameters control the minimum number of samples needed to be at a leaf node of the decision tree. Thus, they regularize the decision tree by limiting the depth of the resulting tree. There are differences, however, since <code class="fm-code-in-text">min_child_weight</code> refers to the minimum sum of Hessian weight needed in a child node, while <code class="fm-code-in-text">min_samples_leaf</code> refers to the minimum number of samples required in a leaf. Hence, the two parameters are not completely comparable since their values are used differently in XGBoost and Scikit-learn. <a id="idIndexMarker114"/><a id="idIndexMarker115"/></p>

  <p class="body">As a general rule of thumb, <code class="fm-code-in-text">min_child_weight</code> affects how single decision trees are built, and the larger the value of this parameter, the more conservative the resulting tree will be. The usual values to be tested range from 0 (implying no limit to the size of the leaf nodes) to 10. In his 2015 talk at the NYC Data Science Academy, titled “Winning Data Science Competitions,” Owen Zhang, a former top Kaggle competitor, suggested computing the optimal value of this parameter by dividing 3 by the percentage of the rarest events in the data to be predicted. For instance, following this rule of thumb, since our classes are split 50%/50%, the ideal value should be 3/0.5, resulting in 6.</p>

  <p class="body">Other important XGBoost parameters we didn’t use in our example are as follows:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The <code class="fm-code-in-text">learning_rate</code>, also called <code class="fm-code-in-text">eta</code>, is a parameter in XGBoost that determines the rate at which the model learns. A lower learning rate allows the model to converge more slowly yet more precisely, potentially leading to better predictive accuracy. However, this will result in a greater number of iterations and a longer training time. On the other hand, setting the value too high can speed up the process but result in worse model performance because, as it happens in gradient descent when your learning parameter is too high, the optimization overshoots its target. <a id="idIndexMarker116"/><a id="idIndexMarker117"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">alpha</code> and <code class="fm-code-in-text">lambda</code> are the L1 and L2 regularizers, respectively. They both contribute to avoiding overfitting in the gradient descent optimization part of XGBoost.<a id="idIndexMarker118"/><a id="idIndexMarker119"/><a id="marker-172"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The <code class="fm-code-in-text">max_depth</code> parameter in XGBoost controls the algorithm’s complexity. If this value is set too low, the model may not be able to identify many patterns (known as underfitting). However, if it is set too high, the model may become overly complex and identify patterns that do not generalize well to new data (known as overfitting). Ideally, it is a value between 1 and 16.<a id="idIndexMarker120"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The <code class="fm-code-in-text">gamma</code>, or <code class="fm-code-in-text">min_split_loss</code>, parameter in XGBoost is a regularization parameter ranging from 0 to infinity, and setting this value higher increases the strength of regularization, reducing the risk of overfitting but potentially leading to underfitting if the value is too large. Also, this parameter controls the resulting complexity of the decision trees. We suggest starting with this value at 0 or a low value and then testing increasing it after all the other parameters are set.<a id="idIndexMarker121"/><a id="idIndexMarker122"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The <code class="fm-code-in-text">colsample_bytree</code> parameter in XGBoost controls the fraction of the total number of features or predictors to be used for a given tree during training. Setting this value to less than 1 means that each tree may use a different subset of features for prediction, potentially reducing the risk of overfitting or being too much influenced by single features in data. It also improves training speed by not using all features in every tree. The allowable range of values for this parameter is between 0 and 1.<a id="idIndexMarker123"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The <code class="fm-code-in-text">subsample</code> parameter in XGBoost controls the fraction of the number of instances used for a given tree during training. Like <code class="fm-code-in-text">colsample_bytree</code>, this parameter can help reduce overfitting and improve training time. By using a fraction of the cases for each tree, the model can identify more generalizable patterns in the data. The default value for <code class="fm-code-in-text">subsample</code> is 1.0, which means that all instances are used in each tree.<a id="idIndexMarker124"/></p>
    </li>
  </ul>

  <p class="body">In many cases, you will only require some of the parameters provided by XGBoost or discussed here for your projects. Simply adjusting the <code class="fm-code-in-text">learning_rate</code>, setting the optimization steps, and <code class="fm-code-in-text">min_child_weight</code> to prevent overfitting individual decision trees in the gradient boosting process will be sufficient most of the time. Additionally, you may derive benefits from setting the <code class="fm-code-in-text">objective</code>, <code class="fm-code-in-text">max_depth</code>, <code class="fm-code-in-text">colsample_bytree</code>, and <code class="fm-code-in-text">subsample</code> parameters, but it is unlikely that tweaking the numerous other available parameters will yield significant improvements. This holds not only for XGBoost but also for different implementations of gradient boosting. <a id="idIndexMarker125"/><a id="idIndexMarker126"/><a id="idIndexMarker127"/><a id="idIndexMarker128"/><a id="idIndexMarker129"/><a id="idIndexMarker130"/></p>

  <p class="body">Nex<a id="idTextAnchor054"/>t we explain what makes the XGBoost implementation perform better in computations and predictions.<a id="idIndexMarker131"/><a id="idIndexMarker132"/></p>

  <h3 class="fm-head1" id="heading_id_15">5.4.2 How XGBoost works</h3>

  <p class="body">As explained in the paper “Xgboost: A Scalable Tree Boosting System” by Tianqi Chen and Carlos Guestrin (<i class="fm-italics">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i>, 2016), the great performance of XGBoost boils down to a few innovations that weren’t present in other implementations: <a id="idIndexMarker133"/><a id="marker-173"/><a id="idIndexMarker134"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Column block for parallel learning</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Second-order approximation for quicker optimization</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Improved split-finding algorithms</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Sparsity-aware split finding</p>
    </li>
  </ul>

  <p class="body">Column Block is a technique used in parallel learning that involves dividing a dataset into blocks of columns or subsets of features. This allows for parallel training across multiple processors, significantly reducing the overall training time. You can see it in action when you train an XGBoost model and look for CPU utilization pointing to multiple usages of different cores. XGBoost cannot use multiple cores to train multiple models simultaneously, as it happens in other ensemble models such as random forests. That’s because gradient boosting is a serial model, where each model is trained after the results of another one. Instead, the XGBoost training process of every single model is divided among multiple cores to increase efficiency and speed.</p>

  <p class="body">Currently, XGBoost can be utilized in Python through two distinct APIs: the Native API and the Scikit-learn API. In this book, we will exclusively use the Scikit-learn API due to its benefits in terms of best modeling practices and the added benefit of being able to easily utilize various tools available in the Scikit-learn library, such as model selection and pipelines, as explained in chapter 4.</p>

  <p class="body">When using the Native API, the user is required to convert their data into a <code class="fm-code-in-text">DMatrix<a class="calibre" id="idTextAnchor055"/></code>, which is an internal XGBoost data structure optimized for both memory efficiency and training speed (<a class="url" href="https://mng.bz/VVxP">https://mng.bz/VVxP</a>). The use of the DMatrix format makes the column block technique possible. However, when using the Scikit-learn API, users can input their data as pandas DataFrames or Numpy arrays without requiring explicit conversion into the DMatrix format. This is because XGBoost performs the conversion under the hood, making the process more streamlined. Therefore, it is safe to choose the API that best suits your preferences, as both APIs offer the same performance and differ only in some of the parameters, default values, and options they provide.<a id="idIndexMarker135"/></p>

  <p class="body">The second-order approximation for speeding up the optimization incorporating the second derivative (the gradient derived from the first derivative) is based on a more comprehensive root-finding technique, Newton’s method. In the context of minimization, we often refer to Newton’s method as Newton descent instead of gradient descent. Listing 5.13 shows it implemented as a new class, the <code class="fm-code-in-text">NewtonianGradientBoosting</code> class, that inherits the original GradientBoosting class with some additions and modifications to its existing methods and attributes. In particular, we add the Hessian calculations to balance the gra<a id="idTextAnchor056"/>dient steps in regard to the convergence acceleration and a regularization term to prevent overfitting.<a id="idIndexMarker136"/><a id="marker-174"/></p>

  <p class="fm-code-listing-caption">Listing 5.13 How XGBoost works</p>
  <pre class="programlisting">class NewtonianGradientBoosting(GradientBoosting):              <span class="fm-combinumeral">①</span>
    """the Newton-Raphson method is used to update the predictions"""
    
    reg_lambda = 0.25                                           <span class="fm-combinumeral">②</span>
    
    def hessian(self, y_true, y_pred):
        hessian = np.ones_like(y_true)                          <span class="fm-combinumeral">③</span>
        return hessian
    
    def fit(self, X, y):
        self.init = self.logit(np.mean(y))
        y_pred = self.init * np.ones((X.shape[0],))
        
        for k in range(self.n_estimators):
            gradient = self.gradient(self.logit(y), y_pred)
            hessian = self.hessian(self.logit(y), y_pred)
            tree = DecisionTreeRegressor(**self.params)
            tree.fit(
                X, 
                -gradient / (
                    hessian + self.reg_lambda
                )
            )                                                   <span class="fm-combinumeral">④</span>
            self.trees.append(tree)
            y_pred += self.learning_rate * tree.predict(X)
        
cls = NewtonianGradientBoosting(n_estimators=300,
                                learning_rate=0.1,
                                max_depth=4,
                                min_samples_leaf=3, 
                                random_state=0)                 <span class="fm-combinumeral">⑤</span>
 
cls.fit(X, y)                                                   <span class="fm-combinumeral">⑥</span>
preds = cls.predict(Xt)
score = accuracy_score(y_true=yt, y_pred=preds)
print(f"Accuracy: {score:0.5f}")                                <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a new class NewtonianGradientBoosting as a subclass of GradientBoosting</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets a regularization parameter reg_lambda</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initializes a constant Hessian matrix with ones</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Fits the decision tree by dividing the negative gradient by the sum of the Hessian and the regularization parameter</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates an instance of the NewtonianGradientBoosting class with specified hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Fits the NewtonianGradientBoosting model to the training data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Predicts target values using the fitted model and calculating the accuracy score for evaluation</p>

  <p class="body">The resulting accuracy is a little better than what we obtained from the original GradientBoosting class:</p>
  <pre class="programlisting">Accuracy: 0.82514</pre>

  <p class="body">In our case, the Hessian is probably not particularly helpful because it is the same for all due to the kind of objective function we use: the squared error. However, in the context of optimization with other objective functions, the Hessian matrix provides information about the curvature of a function, which can be used to determine the direction and rate of change of the function. Intuitively, you can figure out that, with larger curvatures, you have larger Hessian values, which reduce the effect of the gradient, acting as a brake for the learning rate. On the contrary, smaller curvatures lead to an acceleration of the learning rate. Using the information of the Hessian, you get an adaptive learning rate for each of your training examples. However, as a side effect, computing the second derivative can often be complicated or intractable, necessitating significant computation. Analytical expressions and numerical methods for determining the second derivative require substantial computational effort. In the next chapter, devoted to more advanced topics, we will provide further information on how to build your customized objective functions by computing gradients and Hessians analytically and numerically.</p>

  <p class="body">A role in the Newton optimization used by XGBoost is also played by the regularization term that is summed up in Hessian and reduces the target further—that is, the adjustment to be estimated by the base learners. Another idea taken from gradient descent that XGBoost uses is regularization in the form of L2 regularization, as implemented in our example, and L1 regularization. The additional regularization terms help to smooth the final learned weights and avoid overfitting by directly modifying the Newtonian descent step. Consequently, it is important to consider how to tune both L1 and L2 values, referred to as lambda and alpha in the XGBoost and LightGBM implementations, as significant hyperparameters for improving optimization results and reducing overfitting. These regularization values ensure that the Newton descent takes smaller steps during optimization.<a id="idIndexMarker137"/><a id="idIndexMarker138"/></p>

  <p class="body">In the next section, we will continue to explore the new capabilities introduced by XGBoost by examining<a id="idTextAnchor057"/> the contribution of split-finding algorithms to the increased speed performance offered by the algorithm.<a id="idIndexMarker139"/><a id="idIndexMarker140"/><a id="marker-175"/></p>

  <h3 class="fm-head1" id="heading_id_16">5.4.3 Accelerating with histogram splitting</h3>

  <p class="body">Gradient boosting is based on binary trees, which work by partitioning the data to have a better-optimized objective metric in the resulting splits than in the original set. Since gradient boosting treats all the features as numeric, it has a unique way of deciding how to partition. To find the feature to use for the split and the rule for the split, a binary tree decision should iterate through all the features, sort each feature, and evaluate every split point. Ultimately, the decision tree should pick the feature and its split point that led to better improvement relative to the objective.<a id="idIndexMarker141"/><a id="idIndexMarker142"/><a id="idIndexMarker143"/></p>

  <p class="body">With the emergence of larger datasets, the splitting procedure in decision trees poses serious scalability and computational problems for the original GBDT architecture based on serial models that continuously scan through data. From a computational point of view, the main cost in GBDT lies in learning the decision trees, and the most time-consuming part of learning a decision tree is finding the best split points.</p>

  <p class="body">Continuously looking for the best splitting point takes quite some time, rendering the algorithm quite demanding when training on a large number of features and instances. Histogram splitting helps reduce the time by replacing each feature’s value with the histogram’s split points to summarize its values. Listing 5.14 simulates a split search on our data problem. In doing so, we define an objective function and a splitting function that<a id="idTextAnchor058"/> can operate both as the original decision tree splitting algorithm or by the faster histogram-based splitting.<a id="marker-176"/></p>

  <p class="fm-code-listing-caption">Listing 5.14 The histogram split</p>
  <pre class="programlisting">import numpy as np
 
def gini_impurity(y):
    _, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    return 1 - np.sum(probs**2)                              <span class="fm-combinumeral">①</span>
 
def histogram_split(x, y, use_histogram, n_bins=256):
    if use_histogram:
        hist, thresholds = np.histogram(
            x, bins=n_bins, density=False
        )                                                    <span class="fm-combinumeral">②</span>
    else:
        thresholds = np.unique(x)                            <span class="fm-combinumeral">③</span>
    best_score = -1
    best_threshold = None                                    <span class="fm-combinumeral">④</span>
    for threshold in thresholds:                             <span class="fm-combinumeral">⑤</span>
        left_mask = x &lt;= threshold
        right_mask = x &gt; threshold
        left_y = y[left_mask]
        right_y = y[right_mask]                              <span class="fm-combinumeral">⑥</span>
        score = (
            gini_impurity(left_y) * len(left_y) 
            + gini_impurity(right_y) * len(right_y)
        )                                                    <span class="fm-combinumeral">⑦</span>
        if score &gt; best_score: q                             <span class="fm-combinumeral">⑧</span>
            best_threshold = threshold
            best_score = score
    return best_threshold, best_score                        <span class="fm-combinumeral">⑨</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Function calculating and returning the Gini impurity of a set of labels y</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> If use_histogram is true, computes the histogram for the selected feature</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> If use_histogram is false, just enumerates all the unique values in the feature</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Initializes the best score and threshold</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Iterates over all possible thresholds</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Splits y based into left and right subsets based on the selected threshol</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Calculates the Gini impurity score for the left and right subsets</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Updates the best score and threshold if the current split has a higher Gini impurity score than the previous best split</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Returns the best threshold and its corresponding Gini impurity score</p>

  <p class="body">In the code in listing 5.14, after defining a scoring function, the Gini impurity, we define a function that picks a feature and enumerates the values of its potential splits to be evaluated. If we use the basic approach, all its unique values are taken into account. Using the histogram approach instead, a 256-bin histogram is computed, and we use the bins delimiting values to be explored as potential split candidates. If our feature has more than 256 unique values, using the histogram will save us a lot of time when we just iterate through all split candidates and evaluate them by the scoring function.</p>

  <p class="body">Now that we have explained the workings of the example functions, we are ready for a test. We decided to optimally split the latitude in the classification task of predicting if the host is in the upper or lower price range. Since the latitude feature has many unique values to be considered as splitting candidates because Manhattan is a long, thin north-south island where property values vary by latitude, it should result in a difficult task because we expect many different latitudes to be evaluated against the target.</p>

  <p class="body">In our first test, we try to find the best split just by evaluating all the unique values that the feature presents:</p>
  <pre class="programlisting">%%time
histogram_split(x=data.latitude, y=target_median, use_histogram=False)
 
CPU times: user 46.9 s, sys: 10.1 ms, total: 46.9 s
Wall time: 46.9 s
(40.91306, 24447.475447387256)</pre>

  <p class="body"><a id="marker-177"/>In our second test, we rely on evaluating the splitting points found out by a histogram with 256 bins built on the feature:</p>
  <pre class="programlisting">%%time
histogram_split(
    x=data.latitude,
    y=target_median,
    use_histogram=True,
    n_bins=256
)
 
CPU times: user 563 ms, sys: 0 ns, total: 563 ms
Wall time: 562 ms
(40.91306, 24447.475447387256)</pre>

  <p class="body">Looking under the hood of histogram splitting, we find binning, where values for a variable are grouped into discrete bins, and each bin is assigned a unique integer to preserve the order between the bins. Binning is also commonly referred to as k-bins, where the k in the name refers to the number of groups into which a numeric variable is rearranged, and it is used in histogram plotting, where you can declare a value for k or automatically have it set to summarize and represent your data distribution.</p>

  <p class="body">The speed-up is due not only to a minor number of split points to evaluate, which can be tested in parallel, thus using multicore architectures, but also to the fact that histograms are integer-based data structures that are much faster to handle than continuous values vectors.</p>

  <p class="body">XGBoost uses an algorithm to compute the best split based on presorting the values and usage of histograms. The presorting splitting works as follows:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">For each node, enumerating the features</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">For every feature, sorting instances by their values</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Using a linear scan and histograms, determining the best split for the feature and computing the information gain</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Picking the best solution among all the features and their best split</p>
    </li>
  </ul>

  <p class="body">XGBoost has other concept improvements: the traditional split finding algorithm is denoted by <code class="fm-code-in-text">exact</code> specified as the value of the <code class="fm-code-in-text">tree_method</code> parameter. The Weighted Quantile Sketch, referred to as <code class="fm-code-in-text">approx</code> in the XGBoost API, is an exclusive feature unique to XGBoost. This split-finding technique utilizes approximations and harnesses information derived from gradient statistics. By employing quantiles, the method defines potential split points among candidates. Notably, the quantiles are weighted t<a id="idTextAnchor059"/>o prioritize selecting candidates capable of mitigating high gradients, reducing significant prediction errors.<a id="idIndexMarker144"/></p>

  <p class="body">Weighted Quantile Sketch using histograms is now available as <code class="fm-code-in-text">tree_method="hist"</code>, which, since the 2.0.0 release, is the default method. In contrast, the <code class="fm-code-in-text">approx</code> tree method generates a new set of bins for each iteration, whereas the <code class="fm-code-in-text">hist</code> method reuses the bins over multiple iterations.<a id="idIndexMarker145"/></p>

  <p class="body">Another feature of the algorithm is related to data storage in DMatrices. The most time-consuming part of tree learning is sorting the data. To reduce the sorting cost, we propose storing the data in in-memory units: a block. This allows linearly scanning over the presorted entries and parallelizing, giving us an efficient parallel algorithm for split finding.</p>

  <p class="body">After the successful histogram aggregation implementation in LightGBM, XGBoost adopted it. Histogram aggregation is also the main fe<a id="idTextAnchor060"/>ature of <code class="fm-code-in-text">HistGradientBoosting</code>, the Scikit-learn histogram-based gradient boosting we will present after LightGBM.<a id="idIndexMarker146"/><a id="idIndexMarker147"/><a id="marker-178"/><a id="idIndexMarker148"/></p>

  <h3 class="fm-head1" id="heading_id_17">5.4.4 Applying early stopping to XGBoost</h3>

  <p class="body">Section 5.3.1 illustrates how early stopping works with Scikit-learn’s gradient boosting. XGBoost also supports early stopping. You can specify early stopping by adding a few arguments when instantiating an XGBClassifier or XGBRegressor model:<a id="idIndexMarker149"/><a id="idIndexMarker150"/><a id="idIndexMarker151"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">early_stopping_rounds</code>—This is the number of rounds to wait patiently without improvement in the validation score before stopping the training. If you set it to a positive integer, the training will stop when the performance on the validation set hasn’t improved in that many rounds. <a id="idIndexMarker152"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">eval_metric</code>—This is the evaluation metric to use for early stopping. By default, XGBoost uses <code class="fm-code-in-text">rmse</code> for regression’s root mean<a id="idTextAnchor061"/> squared error and <code class="fm-code-in-text">error</code> for accuracy in classification. Still, you can specify any other from a long list (available at <a class="url" href="https://mng.bz/xK2W">https://mng.bz/xK2W</a>) as well as specify your own metric (which will be discussed in the next chapter on advanced machine learning topics).<a id="idIndexMarker153"/><a id="idIndexMarker154"/><a id="idIndexMarker155"/></p>
    </li>
  </ul>

  <p class="body">In addition to setting these parameters, you also have to specify, at fitting time, a sample with its target where to monitor the evaluation metric. This is done by the <code class="fm-code-in-text">parameter eval_set</code>, which contains a list of tuples containing all the validation samples and their responses. In our example, we use only a validation set. Still, if there are multiple samples to monitor, XGboost will consider only the last tuple of data and response for stopping purposes.<a id="idIndexMarker156"/></p>

  <p class="body">In listing 5.15, we replicate the same approach we experimented with earlier by splitting our data into a train and a test <a id="idTextAnchor062"/>set. However, to properly monitor the evaluation metric, we further split the train set to extract a validation set from it.<a id="marker-179"/></p>

  <p class="fm-code-listing-caption">Listing 5.15 Applying early stopping to XGBoost</p>
  <pre class="programlisting">train, test = train_test_split(
    range(len(data)),
    test_size=0.2,
    random_state=0
)                                                          <span class="fm-combinumeral">①</span>
train, validation = train_test_split(
    train,
    test_size=0.2,
    random_state=0
)                                                          <span class="fm-combinumeral">②</span>
 
xgb = XGBClassifier(booster='gbtree',
                    objective='reg:logistic',
                    n_estimators=1000,
                    max_depth=4,
                    min_child_weight=3,
                    early_stopping_rounds=100,             <span class="fm-combinumeral">③</span>
                    eval_metric='error')                   <span class="fm-combinumeral">④</span>
 
X = column_transform.fit_transform(data.iloc[train])
y = target_median[train]
 
Xv = column_transform.transform(data.iloc[validation])
yv = target_median[validation]
 
xgb.fit(X, y, eval_set=[(Xv, yv)], verbose=False)          <span class="fm-combinumeral">⑤</span>
 
Xt = column_transform.transform(data.iloc[test])
yt = target_median[test]
 
preds = xgb.predict(Xt)
score = accuracy_score(y_true=yt, y_pred=preds)
print(f"Accuracy: {score:0.5f}")                           <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Splits the indices of the data into training and test sets using a fixed random seed</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Further splits the training set into training and validation sets using the same random seed</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initializes an XGBoost classifier with an early stopping patience for 100 rounds</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Uses the 'error' parameter, equivalent to accuracy, as an evaluation metric</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Fits the XGBoost classifier to the training data X and labels y and performance on the validation data Xv and yv</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Prints the accuracy score after comparing the predicted labels with the true labels</p>

  <p class="body">After completing the training, we managed to get this accuracy measure, which is slightly underperforming in respect of our previous cross-validation results because it is obtained by training on fewer examples—that is, 64% of available data since we reserved 20% for test and 16% for validation:</p>
  <pre class="programlisting">Accuracy: 0.82657</pre>

  <p class="body">During the training, the evaluation metric is constantly checked, and the fit procedure is halted if it doesn’t improve from more iterations than those specified by <code class="fm-code-in-text">early_stopping_rounds</code>. The best iteration is automatically recorded and used at prediction time. Thus, you have nothing more to do with the model. If you need to verify how many iterations it took before stopping, you can<a id="idTextAnchor063"/> obtain it by inquiring about the model using the <code class="fm-code-in-text">best_iteration</code> attribute. In our example, <code class="fm-code-in-text">xgb.best_iteration</code> returns 200.<a id="idIndexMarker157"/><a id="idIndexMarker158"/><a id="idIndexMarker159"/><a id="idIndexMarker160"/><a id="idIndexMarker161"/><a id="idIndexMarker162"/><a id="idIndexMarker163"/></p>

  <h2 class="fm-head" id="heading_id_18">5.5 Introduction to LightGBM</h2>

  <p class="body"><a id="marker-180"/>LightGBM was first intro<a id="idTextAnchor064"/>duced in a 2017 paper titled “LightGBM: A Highly Efficient Gradient Boosting Decision Tree” by Guolin Ke and his team at Microsoft (<a class="url" href="https://mng.bz/AQdz">https://mng.bz/AQdz</a>). Recently, the package reached the 4.3.0 version, which is the version we tested in this book. According to the authors, the term “light” in LightGBM highlights the algorithm’s faster training and lower memory usage than traditional gradient boosting decision trees. The paper demonstrated, through experiments on multiple public datasets, the algorithm’s effectiveness and its ability to speed up the training process of conventional gradient boosting decision trees by over 20 times while maintaining almost the same accuracy. LightGBM was made available as open-source software on GitHub (<a class="url" href="https://github.com/microsoft/LightGBM/">https://github.com/microsoft/LightGBM/</a>), quickly gaining popularity among data scientists and machine learning practitioners.<a id="idIndexMarker164"/><a id="idIndexMarker165"/></p>

  <p class="body">On paper, LightGBM shares many characteristics similar to those of XGBoost, such as support for missing values, native handling of categorical variables, GPU training, networked parallel training, and monotonicity constraints. We will tell you more about all that in the next chapter. In addition, LightGBM also supports sparse data. However, its major advantage lies in its speed, as it is significantly faster than XGBoost on various tasks, which has made it popular in both Kaggle competitions and real-world applications. The Kaggle community quickly took notice of LightGBM and began incorporating it into their competition entries alongside the already-popular XGBoost. In fact, mlcontests.com, a website that tracks the data science competition scene, reported in 2022 that LightGBM had become the preferred tool among competition winners, surpassing XGBoost in popularity. An impressive 25% of reported solutions for tabular problems were based on LightGBM. While LightGBM has seen comparable success among data science practitioners, XGBoost remains more popular overall. For example, the XGBoost repository has many more GitHub stars than the LightGBM repository.<a id="idIndexMarker166"/></p>

  <p class="body">LightGBM is a cross-platform machine learning library available for Windows, Linux, and M<a id="idTextAnchor065"/>acOS. It can be installed using various tools, such as pip or conda, or built from source code (see the complete installation guide at <a class="url" href="https://mng.bz/ZlEP">https://mng.bz/ZlEP</a>). Its usage syntax is similar to Scikit-learn’s, making it easy for users who are familiar with Scikit-learn to transition to LightGBM. When optimizing gradient descent, LightGBM follows in the footsteps of XGBoost by utilizing the Newton-Raphson update, which involves dividing the gradient by the Hessian matrix. Guolin Ke’s answer on GitHub confirms that (see <a class="url" href="https://github.com/microsoft/LightGBM/issues/5233">https://github.com/microsoft/LightGBM/issues/5<span id="idTextAnchor066">233</span></a>).<a id="idIndexMarker167"/><a id="idIndexMarker168"/></p>

  <p class="body">Let’s put the algorithm to the test on the same problem we previously examined with ScikitLearn’s GradientBoosting and XGBoost.</p>

  <p class="fm-code-listing-caption">Listing 5.16 LightGBM classifier</p>
  <pre class="programlisting">from lightgbm import LGBMClassifier
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
lgbm = LGBMClassifier(boosting_type='gbdt',            <span class="fm-combinumeral">①</span>
                      n_estimators=300,
                      max_depth=-1,
                      min_child_samples=3,
                      force_col_wise=True,             <span class="fm-combinumeral">②</span>
                      verbosity=0) 
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('lightgbm', lgbm)])                              <span class="fm-combinumeral">③</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)      <span class="fm-combinumeral">④</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"CV Accuracy {mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")            <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initializes an LGBMClassifier with a number of estimators, maximum tree depth, and minimum number of child samples</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Forces column-wise histogram building</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a model pipeline that includes a column transformation step and an LGBMClassifier step</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Performs five-fold cross-validation using the model pipeline using accuracy scoring</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the mean test score and standard deviation of the test scores obtained during cross-validation</p>

  <p class="body"><a id="marker-181"/>The following are the impressive results in terms of accuracy, training, and prediction times:</p>
  <pre class="programlisting">0.826 (0.004) fit: 1.16 secs pred: 0.16 secs</pre>

  <p class="body">Like XGBoost, LightGBM controls gradient descent with parameters such as <code class="fm-code-in-text">n_estimators</code>, <code class="fm-code-in-text">learning_rate,</code> <code class="fm-code-in-text">lambda_l1,</code> and <code class="fm-code-in-text">lambda_l2</code> (L1 and L2 regularization, respectively). The most important parameters of LightGBM that help control its complexity are as follows:<a id="idIndexMarker169"/><a id="idIndexMarker170"/><a id="idIndexMarker171"/><a id="idIndexMarker172"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">max_depth</code>—This parameter controls the maximum depth of each tree in the ensemble. A higher value increases the complexity of the model, making it more prone to overfitting. If it is set to <span class="times">–1</span> it means that no limit is set to the growth of the trees.<a id="idIndexMarker173"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">num_leaves</code>—This parameter specifies the maximum number of leaves in a tree and, therefore, the complexity of the model. To avoid overfitting, it should be set to less than <code class="fm-code-in-text">2**(max_depth)</code>.<a id="idIndexMarker174"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">min_data_in_leaf</code>—This parameter controls the minimum number of samples required to be present in each leaf node. A higher value can prevent the tree from growing too deep and overfitting but can also lead to underfitting if set too high. The default value is 20. We suggest trying lower values, such as 10, and then testing, increasing the value to 300.<a id="idIndexMarker175"/></p>
    </li>
  </ul>

  <p class="body"><a id="marker-182"/>The parameters <code class="fm-code-in-text">feature_fraction</code> and <code class="fm-code-in-text">bagging_fraction</code> control how LightGBM samples from features and examples:<a id="idIndexMarker176"/><a id="idIndexMarker177"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">feature_fraction</code>—This parameter controls the fraction of features to be considered at each split. Similar to the <code class="fm-code-in-text">colsample_bytree</code> parameter in XGBoost, it can help reduce overfitting by preventing the model from relying too heavily on any feature.<a id="idIndexMarker178"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">bagging_fraction</code>—This parameter controls the fraction of data to be used for each tree. Similar to the subsample parameter in XGBoost, it can help reduce overfitting and improve training speed by randomly sampling from the data.<a id="idIndexMarker179"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">bagging_freq</code>—This parameter, which is not present in XGBoost, determines how frequently bagging should be applied. It turns off bagging examples when set to 0, even if <code class="fm-code-in-text">bagging_fraction</code> is specified. A value of n means bagging at every n iteration. For instance, a value of 2 means you have a bagged iteration every two (half of the time).<a id="idIndexMarker180"/></p>
    </li>
  </ul>

  <p class="body">Related to the way LightGBM executes during the training, <code class="fm-code-in-text">verbosity</code> controls the amount of output information during training, while <code class="fm-code-in-text">force_col_wise</code> indicates histograms for feature splits during tree construction to be built based on columns. LightGBM can build histograms either column-wise or row-wise. Column-wise histogram building is generally faster, but it can require more memory, especially for datasets with a large number of columns. Row-wise histogram building is slower, but it can be more memory-efficient when dealing with datasets with a large number of columns. LightGBM will automatically choose the best method for building histograms for the dataset. However, you can also force LightGBM to use a specific method by setting the <code class="fm-code-in-text">force_col_wise</code> or <code class="fm-code-in-text">force_row_wise</code> parameters.<a id="idIndexMarker181"/><a id="idIndexMarker182"/><a id="idIndexMarker183"/></p>

  <p class="body">As for XGBoost, LightGBM can also use different base learners by specifying the <code class="fm-code-in-text">boosting</code> parameter:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">gbdt</code>—The default option, using decision trees as base learners</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">rf</code>—Implements the random forests algorithm</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">dart</code>—Implements the “Dropouts meet Multiple Additive Regression Trees” algorithm</p>
    </li>
  </ul>

  <p class="body">In addition, setting the parameter <code class="fm-code-in-text">linear_tree</code> to true, as you are using the default <code class="fm-code-in-text">boosting=gbdt</code>, will fit a piecewise linear gradient boosting tree—that is, decision trees having linear models as their terminal nodes. This is a compromise solution that uses both the nonlinear learning capabilities of decision trees and the extrapolative capabilities of l<a id="idTextAnchor067"/>inear models with unseen, outlying cases.<a id="idIndexMarker184"/></p>

  <p class="body">In the following section, we will examine closely all the innovations that distinguish LightGBM from XGBoost.</p>

  <h3 class="fm-head1" id="heading_id_19">5.5.1 How LightGBM grows trees</h3>

  <p class="body">Let’s examine each characteristic that distinguishes LightGBM from XGBoost, starting with how LightGBM grows decision trees. Instead of increasing the tree level-wise (also known as <i class="fm-italics">depth-first</i>) like XGBoost, LightGBM grows the tree leaf-wise (also known as <i class="fm-italics">best first)</i>. This means that the algorithm chooses the leaf node that provides the maximum gain and then splits it further until it is no longer advantageous. In contrast, the level-wise approach simultaneously splits all the nodes at the same depth. <a id="idIndexMarker185"/><a id="idIndexMarker186"/><a id="idIndexMarker187"/><a id="idIndexMarker188"/><a id="marker-183"/><a id="idIndexMarker189"/></p>

  <p class="body">To sum up, in XGBoost’s level-wise growth approach, the algorithm grows all tree leaves to the same level. Then it splits them simultaneously, which may result in many insignificant leaves that don’t contribute much to the final prediction. In contrast, LightGBM’s leaf-wise growth approach splits the leaf with the maximum loss reduction at each step, resulting in fewer leaves but with higher accuracy. The leaf-wise approach allows LightGBM to focus only on the important features with the most significant effect on the target variable. This means that the algorithm can quickly converge to the optimal solution with fewer splits and a smaller number of trees.</p>

  <p class="body">Figure 5.10 shows a representation of the two approaches: on the left, the level-wise approach and, on the right, the leaf-wise approach, both constrained to have, at th<a id="idTextAnchor068"/>e most, four terminal nodes. The two approaches take completely different paths in terms of the rules they decide to apply and how they segment the data.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F10_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.10 How level-wise (on the left) and leaf-wise (on the right) tree growth differ</p>
  </div>

  <p class="body">It is important to point out that if you allow two trees to grow using the same data, one using a leaf-wise approach and one using a level-wise approach fully, they will define the same terminal leaves and predictions. The distinction lies in how they are built, with the leaf-wise approach being more aggressive in splitting nodes that provide the most information gain first.</p>

  <p class="body">This implies that the leaf-wise and level-wise approaches differ when a stopping rule is applied based on reaching a certain number of terminal nodes or a specific depth in tree splitting. In this case, the leaf-wise approach can result in smaller trees, faster training times, and higher accuracy, but it also carries an in<a id="idTextAnchor069"/>creased risk of overfitting. To control the depth of the tree leaf-wise growth and address overfitting, you can control the max-depth parameter in LightGBM.</p>

  <h3 class="fm-head1" id="heading_id_20">5.5.2 Gaining speed with exclusive feature bundling and gradient-based one-side sampling</h3>

  <p class="body">To further reduce the training time, a basic strategy in gradient boosting and many other machine learning algorithms is reducing the number of examples processed. The simplest way to reduce the number of processed values is using stochastic sampling (i.e., row reduction) and/or dimensionality reduction techniques such as column sampling or principal component analysis (i.e., column reduction). Although sampling can improve accuracy in the presence of noise in data, excessive sampling can harm the training process and decrease predictive performance. In LightGBM, the <i class="fm-italics">Gradient-Based One-Side Sampling</i> algorithm (GOSS) determines the manner and extent of sampling. Dimensionality reduction techniques rely on identifying redundancies in the data and combining them using a linear combination, typically a weighted sum. However, linear combinations can destroy nonlinear relationships in the data. Dimensionality reduction by discarding rare signals may lead to a decrease in model accuracy if the successful resolution of the data problem depends on those weak signals. In LightGBM, dimensionality reduction is handled by <i class="fm-italics">Exclusive Feature Bundling</i> (EFB), which is a way to reduce the column dimension without losing information.<a id="idIndexMarker190"/><a id="idIndexMarker191"/><a id="idIndexMarker192"/><a id="idIndexMarker193"/><a id="idIndexMarker194"/><a id="marker-184"/><a id="idIndexMarker195"/></p>

  <p class="body">Let’s start by explaining the two major speed improvements in LightGBM, starting with how EFB works. EFB is a technique that efficiently decreases the number of features without compromising data integrity. When using extensively one-hot encoded and binary features, many features become sparse, with few values and an abundance of zeros. You can retain all non-zero values without loss by summing these features and encoding some values. LightGBM optimizes computations and data dimensionality by grouping these features into <i class="fm-italics">Exclusive Feature Bundles</i>, ensuring that predictive accuracy is maintained.<a id="idIndexMarker196"/></p>

  <p class="body">Figure 5.11 shows how effectively two features can be bundled together. The solution involves adding feature B to feature A only for non-zero values, using the maximum value present in feature A. This combined feature will preserve the original fea<a id="idTextAnchor070"/>tures’ order because the values from feature A are separate from the values from feature B and will be located in different sections of the value distribution.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F11_Ryan2.png"/></p>

    <p class="figurecaption">Figure 5.11 Demonstrating how EFB works when combining two features</p>
  </div>

  <p class="body">Finding the optimal way to bundle exclusive features is a complex problem, classified as NP-hard. However, according to the LightGBM paper by Guolin Ke and his team, a greedy algorithm can provide a good approximation by automatically bundling many features. The feature bundling algorithm works sequentially, selecting the features with the least number of overlapping values and bundling them together. If it finds another feature with minimal overlap, it continues to bundle. Otherwise, it starts a new bundle until no more bundles can be found. The stopping rule is provided by the degree of conflicts two features have. If they have more conflicts than a certain gamma threshold, the bundle cannot be made, and the entire process may stop if there aren’t better candidates. Although the resulting bundles from this greedy process are not guaranteed to be optimal, the algorithm provides an acceptable solution in a reasonable time.</p>

  <p class="body">The other performance improvement presented in the paper is GOSS. As we mentioned, if EFB is aimed at reducing column dimensionality, GOSS works on the rows by sampling them effectively without bias.</p>

  <p class="body">GOSS is based on the observation that certain data instances are unlikely to provide useful information for finding a split point. Searching a carefully selected subset of the training set can save computation time without affecting predictive accuracy. Additionally, in gradient boosted decision trees, the algorithm implicitly specifies a weight for data instances when optimizing for the gradient for each data instance. Determining weights is crucial to compute a correction of the previous estimates, but that could also be used for sampling the data instances that could be more interesting to learn.</p>

  <p class="body">GOSS estimates data examples with larger gradients to contribute more toward information gain. Focusing on examples with larger gradients and ignoring part of those with smaller ones should reduce the number of processed data instances while still optimizing the algorithm for predictions. The procedure for GOSS results in the following:<a id="marker-185"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">GOSS first sorts the data examples according to the absolute value of their gradients.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">It selects the top a × 100% data examples.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">It randomly samples b × 100% data examples from the rest of the data.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">It trains the decision tree on the combined samples using a weight of 1 for the top data examples and a weight of <span class="times">(1 – a) / b</span> for the randomly sampled data examples.</p>
    </li>
  </ul>

  <p class="body">The final weighting is necessary to maintain the original data distribution of the dataset and avoid any unwanted shift in its representation.</p>

  <p class="body">GOSS can accelerate the training of gradient boosted decision trees, especially when dealing with larger datasets and complex trees. The original paper’s authors demonstrate that the error incurred by GOSS’s sampling approximation becomes negligible for larger datasets compared to the traditional method. In our experience using GOSS, at best, you get similar results compared to standard LightGBM training. Still, the speed-up is significant, making GOSS a good choice for faster experimentation when looking for the correct hyperparameters or selecting<a id="idTextAnchor071"/> the most relevant features for your problem.</p>

  <p class="body">Contrary to the other speed-ups we presented, GOSS is not used by default: you must specify that you want to use it.<a id="idIndexMarker197"/><a id="idIndexMarker198"/><a id="idIndexMarker199"/></p>

  <h3 class="fm-head1" id="heading_id_21">5.5.3 Applying early stopping to LightGBM</h3>

  <p class="body">LightGBM supports early stopping, and the parameters to control it are similar to those used in the XGBoost implementation. In the example in listing 5.17, we use LightGBM for training and have the algorithm assess its performance during the training phase using a test set. If there is no improvement in performance on the te<a id="idTextAnchor072"/>st set for 100 iterations, the algorithm halts the training process. It selects the round of iterations that achieved the highest performance on the test set so far.<a id="idIndexMarker200"/><a id="idIndexMarker201"/><a id="idIndexMarker202"/><a id="marker-186"/></p>

  <p class="fm-code-listing-caption">Listing 5.17 Applying early stopping to LightGBM</p>
  <pre class="programlisting">from lightgbm import LGBMClassifier, log_evaluation
  
train, test = train_test_split(range(len(data)), test_size=0.2, 
random_state=0)                                             <span class="fm-combinumeral">①</span>
train, validation = train_test_split(
    train, 
    test_size=0.2,
    random_state=0
)                                                           <span class="fm-combinumeral">②</span>
 
lgbm = LGBMClassifier(boosting_type='gbdt', 
                      early_stopping_round=150,
                      n_estimators=1000, 
                      max_depth=-1,
                      min_child_samples=3,
                      force_col_wise=True,
                      verbosity=0)                          <span class="fm-combinumeral">③</span>
 
X = column_transform.fit_transform(data.iloc[train])
y = target_median[train]
 
Xv = column_transform.transform(data.iloc[validation])
yv = target_median[validation]
 
lgbm.fit(X, y, eval_set=[(Xv, yv)],                         <span class="fm-combinumeral">④</span>
         eval_metric='accuracy',                                <span class="fm-combinumeral">⑤</span>
         callbacks=[log_evaluation(period=0)])                  <span class="fm-combinumeral">⑥</span>
 
Xt = column_transform.transform(data.iloc[test])
yt = target_median[test]
 
preds = lgbm.predict(Xt)
score = accuracy_score(y_true=yt, y_pred=preds)
print(f"Test accuracy: {score:0.5f}")                       <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Splits the indices of the data into training and test sets using a fixed random seed</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Further splits the training set into training and validation sets using the same random seed</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initializes a LightGBM classifier with a number of estimators, max depth, and minimum number of child samples</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Fits the LightGBM classifier to the training data X and labels y and performance on the validation data Xv and yv</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Sets accuracy as an evaluation metric</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sets a callback to suppress the evaluation (period=0)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Prints the accuracy score after comparing the predicted labels with the true labels</p>

  <p class="body">Even in this case, the result is penalized by the fact that we are training only on 64% of the available data:</p>
  <pre class="programlisting">Accuracy: 0.82585</pre>

  <p class="body">H<a id="idTextAnchor073"/>owever, there are some minor differences from the XGBoost implementation that you can notice from the code. The <code class="fm-code-in-text">eval_metric</code> takes different names (that you can check at <a class="url" href="https://mng.bz/RVZK">https://mng.bz/RVZK</a>) and, to suppress the printing of the evaluations during the training, you don’t use the verbose parameters as in XGBoost; rather, you have to specify a callback funct<a id="idTextAnchor074"/>ion (<code class="fm-code-in-text">log_evaluation</code>) that has to be declared at the fitting time in the list of callbacks.<a id="marker-187"/><a id="idIndexMarker203"/><a id="idIndexMarker204"/></p>

  <p class="body">Recently, early stopping has been implemented as a callback function too (see <a class="url" href="https://mng.bz/2yK0">https://mng.bz/2yK0</a>). Keeping the declaration of early stopping rounds during model instantiation stayed just for maintaining API compatibility with XGBoost. In case you use early stopping as a callback, you have more control over the way the LightGBM stops its training:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">first_metric_only</code> allows you to indicate whether to use only the first metric for early stopping or any metric you pointed out using. <a id="idIndexMarker205"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">min_delta</code> signals the minimum improvement in the metric to keep on training, which usually is set to zero (any improvement), but it could be raised to impose more strict control over the growth of the ensemble. <a id="idIndexMarker206"/></p>
    </li>
  </ul>

  <p class="body">In the previous example, you just need to remove the <code class="fm-code-in-text">early_stopping_rounds</code> from the LGBMClassifier instantiation and add the proper call back to the callback list in the fit method to obtain the same result:</p>
  <pre class="programlisting">early_stopping(
    stopping_rounds=150,
    first_metric_only=True,
    verbose=False,
    min_delta=0.0
)</pre>

  <p class="body">Whatever method you use, the iterati<a id="idTextAnchor075"/>on index, resulting in the best validation score, will be stored in the <code class="fm-code-in-text">best_iteration</code> attribute of a model, and that iteration will be automatically used when predicting. <a id="idIndexMarker207"/><a id="idIndexMarker208"/><a id="idIndexMarker209"/><a id="idIndexMarker210"/></p>

  <h3 class="fm-head1" id="heading_id_22">5.5.4 Making XGBoost imitate LightGBM</h3>

  <p class="body">Since the introduction of LightGBM and its impressive usage of unbalanced decision trees, XGBoost also started supporting the leaf-wise strategy in addition to its original level-wise strategy. In XGBoost, the original level-wise approach is called <code class="fm-code-in-text">depthwise,</code> and the leaf-wise strategy is called <code class="fm-code-in-text">lossguide</code>. By setting one or another using the <code class="fm-code-in-text">grow_policy</code> parameter, you can have your XGBoost behave as a LightGBM. In addition, XGBoost authors suggested, when using the lossguide grow policy, to set the following parameters to avoid overfitting:<a id="idIndexMarker211"/><a id="idIndexMarker212"/><a id="idIndexMarker213"/><a id="idIndexMarker214"/><a id="idIndexMarker215"/><a id="idIndexMarker216"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">max_leaves</code>—Sets the maximum number of nodes to be added and is only relevant for the lossguide policy.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">max_depth</code>—Sets the maximum depth of the tree. If <code class="fm-code-in-text">grow_policy</code> is set to <code class="fm-code-in-text">depthwise</code>, <code class="fm-code-in-text">max_depth</code> behaves as usual. However, if <code class="fm-code-in-text">grow_policy</code> is set to <code class="fm-code-in-text">lossguide</code>, <code class="fm-code-in-text">max_depth</code> can be set to zero, indicating no dep<a id="idTextAnchor076"/>th limit.<a id="idIndexMarker217"/></p>
    </li>
  </ul>

  <p class="body">Incidentally, you also have the same parameters in LightGBM to be used for the same purpose (<code class="fm-code-in-text">max_leaves</code> is an alias—i.e., another working name of the parameter <code class="fm-code-in-text">num_leaves</code>).<a id="idIndexMarker218"/></p>

  <h3 class="fm-head1" id="heading_id_23">5.5.5  How LightGBM inspired Scikit-learn</h3>

  <p class="body">In version 0.21 of Scikit-learn, two novel implementations of gradient boosting trees were added: <code class="fm-code-in-text">HistGradientBoostingClassifier</code> and <code class="fm-code-in-text">HistGradientBoostingRegressor</code>, inspired by LightGBM. You may wonder why you would bother with this new implementation if the current LightGBM and XGBoost versions can offer you everything you need to develop the best-performing tabular solutions based on gradient boosting. They also ensure full compatibility with Scikit-learn API. It is worth the time to look at it because the histogram-based implementation, though now a work in progress, is expected to take over the original, offering the same control over the learning process and building of decision trees. Moreover, it has shown even better predictive performances than XGBoost and LightGBM in some specific applications. Hence, it may be worth testing for your specific problems.<a id="idIndexMarker219"/><a id="idIndexMarker220"/><a id="idIndexMarker221"/><a id="idIndexMarker222"/><a id="marker-188"/><a id="idIndexMarker223"/></p>

  <p class="body">In comparison with the original Scikit-learn implementation for gradient boosting, the new histogram-based ones present new characteristics:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Binning</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Multicore (the initial implementation was single-core)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">No sparse data support</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Built-in support for missing values</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Monotonicity and interaction constraints</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Native categorical variables</p>
    </li>
  </ul>

  <p class="body">At the moment, talking about differences, there is no support for sparse data in the new histogram-based implementation. Consequently, if your data is in a sparse matrix, you should first densify the data matrix. Also, some other features typical of <code class="fm-code-in-text">GradientBoostingClassifier</code> and <code class="fm-code-in-text">GradientBoostingRegressor</code> still need to be supported—for instance, some loss functions. <a id="idIndexMarker224"/><a id="idIndexMarker225"/></p>

  <p class="body">On the API side, most parameters are unchanged from <code class="fm-code-in-text">GradientBoosting</code><code class="fm-code-in-text">Classifier</code> and <code class="fm-code-in-text">GradientBoostingRegressor</code>. One exception is the <code class="fm-code-in-text">max_iter</code> parameter that replaces <code class="fm-code-in-text">n_estimators</code>. The<a id="idTextAnchor077"/> following listing shows an example of the <code class="fm-code-in-text">HistGradientBoostingClassifier</code> applied to our classification problem with the Airbnb NYC dataset classifying listings above the median market value.<a id="idIndexMarker226"/><a id="idIndexMarker227"/></p>

  <p class="fm-code-listing-caption">Listing 5.18 The new Scikit-learn’s histogram gradient boosting</p>
  <pre class="programlisting">from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
 
model = HistGradientBoostingClassifier(learning_rate=0.1,
                                       max_iter=300,
                                       max_depth=4,
                                       min_samples_leaf=3,
                                       random_state=0)      <span class="fm-combinumeral">①</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                  <span class="fm-combinumeral">②</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)           <span class="fm-combinumeral">③</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                 <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initializes a HistGradientBoostingClassifier with specific hyperparameters for the boosting algorithm</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a model pipeline combining data preprocessing (column_transform) and the model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Executes five-fold cross-validation on the model pipeline returning scores and trained estimators</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints the mean and standard deviation of the accuracy scores from cross-validation</p>

  <p class="body">The results are</p>
  <pre class="programlisting">0.827 (0.005) fit: 1.71 secs pred: 0.13 secs</pre>

  <p class="body"><a id="marker-189"/>Compared to our previous examples with XGBoost and LightGBM, this one differs from the used command and the <code class="fm-code-in-text">max_iter</code> parameter in exchange for the usual <code class="fm-code-in-text">n_estimators</code>. Also, the Scikit-learn new boosting algorithm is a histogram one. You just set the <code class="fm-code-in-text">max_bins</code> argument to change the initial default value of 255 (it is 256 bins because 1 is reserved for missing cases).<a id="idIndexMarker228"/><a id="idIndexMarker229"/></p>

  <p class="body">The algorithm is still under devel<a id="idTextAnchor078"/>opment and lacks support for sparse data. This implies that it cannot perform as fast as XGBoost or LightGBM in the presence of many one-hot encoded features, no matter how you prepare your data.<a id="idIndexMarker230"/><a id="idIndexMarker231"/><a id="idIndexMarker232"/><a id="idIndexMarker233"/><a id="idIndexMarker234"/></p>

  <h2 class="fm-head" id="heading_id_24">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><a id="marker-190"/>Ensemble algorithms are used to improve the predictive power of a single model by using multiple models or chaining them together:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">Ensemble algorithms are often based on decision trees.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">There are two core ensemble strategies: averaging and boosting.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Averaging strategies, such as random forests, tend to reduce the variance of predictions while only slightly increasing the bias.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Pasting is a type of averaging approach that involves creating a set of different models trained on subsamples of the data and pooling the predictions together.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Bagging is similar to averaging but with bootstrapping instead of subsampling.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Averaging methods can be computationally intensive and increase bias by excluding important parts of the data distribution through sampling.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Random forests is an ensemble learning algorithm that combines decision trees by bootstrapping samples and subsampling features during modeling (random patches):</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">It creates a set of models that are different from each other and produces more reliable and accurate predictions.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">It can be used to determine feature importance and measure cases’ similarity in a dataset.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">The algorithm requires fine-tuning its few hyperparameters, like the number of employed trees, and adjusting bias-variance tradeoffs by setting the maximum number of features used for splits, the maximum depth of trees, and the minimum size of the terminal branches.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">It can be computationally costly if the number of trees is set too high.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">ERT, Extremely Randomized Trees, is a variation of the random forests algorithm:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">It randomly selects the feature for the split at each decision tree node, leading to less variance (because trees are more diverse) but more bias (randomization sacrifices some of the decision trees’ predictive accuracy, resulting in a higher bias).</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">It is more computationally efficient and useful for large datasets with many collinear and noisy features.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">It reduces variance by making the resulting set of trees less correlated.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">GBDT is a highly effective machine learning method for tabular data problems. It has become a leading approach in various domains, including multiclass classification, advertising click prediction, and search engine ranking. Compared to other methods, such as neural networks, support vector machines, random forests, and bagging ensembles, GBDT generally performs better in standard tabular problems.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Gradient boosting is effective because it combines gradient descent, an optimization procedure typical of linear models and neural networks, and decision trees trained on the gradients derived from the sum of the previous decision trees.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Scikit-learn offers one of the earliest options for gradient boosting algorithms for regression and classification tasks. Recently, the original algorithm was replaced by a speedier histogram-based one, which is still under development.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">XGBoost, an algorithm for gradient boosting decision trees, gained popularity after its successful use in the Higgs Boson Machine Learning Challenge on Kaggle. It is based on a more complex optimization based on Newton’s Descent, and it offers the following advantages:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">The Ability to handle various input data types</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Support for customized objective and evaluation functions</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Automatic handling of missing values</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Easy support for GPU training</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Accommodation of monotonicity and feature interaction constraints</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Optimization of multiple cores and cache on standalone computers</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">LightGBM is a highly efficient gradient boosting decision tree algorithm, introduced in a 2017 paper by Guolin Ke and his team at Microsoft. The algorithm was designed to be faster and use less memory than traditional gradient boosting decision trees, as demonstrated in experiments on multiple public datasets. The LightGBM algorithm achieves this thanks to its leaf-wise splitting policy and EFB.<a id="marker-191"/></p>
    </li>
  </ul>
</body></html>