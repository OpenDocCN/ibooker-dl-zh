["```py\nimport py3Dmol\nimport requests\n\ndef fetch_protein_structure(pdb_id: str) -> str:\n  \"\"\"Grab a PDB protein structure from the RCSB Protein Data Bank.\"\"\"\n  url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n  response = requests.get(url)\n  return response.text\n\n# The Protein Data Bank (PDB) is the main database of protein structures.\n# Each structure has a unique 4-character PDB ID. Below are a few examples.\nprotein_to_pdb = {\n  \"insulin\": \"3I40\",  # Human insulin – regulates glucose uptake.\n  \"collagen\": \"1BKV\",  # Human collagen – provides structural support.\n  \"proteasome\": \"1YAR\",  # Archaebacterial proteasome – degrades proteins.\n}\n\nprotein = \"collagen\"  # @param [\"insulin\", \"collagen\", \"proteasome\"]\npdb_structure = fetch_protein_structure(pdb_id=protein_to_pdb[protein])\n\npdbview = py3Dmol.view(width=400, height=300)\npdbview.addModel(pdb_structure, \"pdb\")\npdbview.setStyle({\"cartoon\": {\"color\": \"spectrum\"}})\npdbview.zoomTo()\npdbview.show()\n\n```", "```py\n# Precursor insulin protein sequence (processed into two protein chains).\ninsulin_sequence = (\n  \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n)\nprint(f\"Length of the insulin protein precursor: {len(insulin_sequence)}.\")\n\n```", "```py\nLength of the insulin protein precursor: 110.\n\n```", "```py\nfrom dlfb.utils.display import print_short_dict\n\namino_acids = [\n  \"R\", \"H\", \"K\", \"D\", \"E\", \"S\", \"T\", \"N\", \"Q\", \"G\", \"P\", \"C\", \"A\", \"V\", \"I\",\n  \"L\", \"M\", \"F\", \"Y\", \"W\",\n]\n\namino_acid_to_index = {\n  amino_acid: index for index, amino_acid in enumerate(amino_acids)\n}\n\nprint_short_dict(amino_acid_to_index)\n\n```", "```py\n{'R': 0, 'H': 1, 'K': 2, 'D': 3, 'E': 4, 'S': 5, 'T': 6, 'N': 7, 'Q': 8, 'G': 9}\n…(+10 more entries)\n\n```", "```py\n# Methionine, alanine, leucine, tryptophan, methionine.\ntiny_protein = [\"M\", \"A\", \"L\", \"W\", \"M\"]\n\ntiny_protein_indices = [\n  amino_acid_to_index[amino_acid] for amino_acid in tiny_protein\n]\n\ntiny_protein_indices\n\n```", "```py\n[16, 12, 15, 19, 16]\n\n```", "```py\nimport jax\n\none_hot_encoded_sequence = jax.nn.one_hot(\n  x=tiny_protein_indices, num_classes=len(amino_acids)\n)\n\nprint(one_hot_encoded_sequence)\n\n```", "```py\n[[0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 1\\. 0\\. 0\\. 0.]\n [0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 1\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.]\n [0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 1\\. 0\\. 0\\. 0\\. 0.]\n [0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 1.]\n [0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 1\\. 0\\. 0\\. 0.]]\n\n```", "```py\nimport seaborn as sns\n\nfig = sns.heatmap(\n  one_hot_encoded_sequence, square=True, cbar=False, cmap=\"inferno\"\n)\nfig.set(xlabel=\"Amino Acid Index\", ylabel=\"Protein Sequence\");\n\n```", "```py\nfrom transformers import AutoTokenizer, EsmModel\n\n# Model checkpoint name taken from this GitHub README:\n# https://github.com/facebookresearch/esm#available-models-and-datasets-\nmodel_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = EsmModel.from_pretrained(model_checkpoint)\n\n```", "```py\nvocab_to_index = tokenizer.get_vocab()\nprint_short_dict(vocab_to_index)\n\n```", "```py\n{'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7,\n'S': 8, 'E': 9}\n…(+23 more entries)\n\n```", "```py\ntokenized_tiny_protein = tokenizer(\"MALWM\")[\"input_ids\"]\ntokenized_tiny_protein\n\n```", "```py\n[0, 20, 5, 4, 22, 20, 2]\n\n```", "```py\ntokenized_tiny_protein[1:-1]\n\n```", "```py\n[20, 5, 4, 22, 20]\n\n```", "```py\ntoken_embeddings = model.get_input_embeddings().weight.detach().numpy()\ntoken_embeddings.shape\n\n```", "```py\n(33, 1280)\n\n```", "```py\nimport pandas as pd\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_tsne = tsne.fit_transform(token_embeddings)\nembeddings_tsne_df = pd.DataFrame(\n  embeddings_tsne, columns=[\"first_dim\", \"second_dim\"]\n)\nembeddings_tsne_df.shape\n\n```", "```py\n(33, 2)\n\n```", "```py\nfig = sns.scatterplot(\n  data=embeddings_tsne_df, x=\"first_dim\", y=\"second_dim\", s=50\n)\nfig.set_xlabel(\"First Dimension\")\nfig.set_ylabel(\"Second Dimension\");\n\n```", "```py\nfrom adjustText import adjust_text\n\nembeddings_tsne_df[\"token\"] = list(vocab_to_index.keys())\n\ntoken_annotation = {\n  \"hydrophobic\": [\"A\", \"F\", \"I\", \"L\", \"M\", \"V\", \"W\", \"Y\"],\n  \"polar uncharged\": [\"N\", \"Q\", \"S\", \"T\"],\n  \"negatively charged\": [\"D\", \"E\"],\n  \"positively charged\": [\"H\", \"K\", \"R\"],\n  \"special amino acid\": [\"B\", \"C\", \"G\", \"O\", \"P\", \"U\", \"X\", \"Z\"],\n  \"special token\": [\n    \"-\",\n    \".\",\n    \"<cls>\",\n    \"<eos>\",\n    \"<mask>\",\n    \"<null_1>\",\n    \"<pad>\",\n    \"<unk>\",\n  ],\n}\n\nembeddings_tsne_df[\"label\"] = embeddings_tsne_df[\"token\"].map(\n  {t: label for label, tokens in token_annotation.items() for t in tokens}\n)\n\nfig = sns.scatterplot(\n  data=embeddings_tsne_df,\n  x=\"first_dim\",\n  y=\"second_dim\",\n  hue=\"label\",\n  style=\"label\",\n  s=50,\n)\nfig.set_xlabel(\"First Dimension\")\nfig.set_ylabel(\"Second Dimension\")\ntexts = [\n  fig.text(point[\"first_dim\"], point[\"second_dim\"], point[\"token\"])\n  for _, point in embeddings_tsne_df.iterrows()\n]\nadjust_text(\n  texts, expand=(1.5, 1.5), arrowprops=dict(arrowstyle=\"->\", color=\"grey\")\n);\n\n```", "```py\ninsulin_sequence = (\n  \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n)\n\nmasked_insulin_sequence = (\n  # Let's mask the `L` amino acid in the 29th position (0-based indexing):\n  #       ...LALLALWGPDPAAAFVNQH  L   CGSHLVEALYLVCGERGFF...\n  \"MALWMRLLPLLALLALWGPDPAAAFVNQH<mask>CGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n)\n\n# Tokenize the masked insulin sequence.\nmasked_inputs = tokenizer(masked_insulin_sequence)[\"input_ids\"]\n\n# Check that we indeed have a <mask> token in the place that we expect it. Note\n# that the tokenizer adds a <cls> token to the start of the sequence, so we in\n# fact expect the <mask> token at position 30 (not 29).\nassert masked_inputs[30] == vocab_to_index[\"<mask>\"]\n\n```", "```py\nfrom transformers import EsmForMaskedLM\n\n# Model checkpoint name taken from this GitHub README:\n# https://github.com/facebookresearch/esm#available-models-and-datasets-\nmodel_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmasked_lm_model = EsmForMaskedLM.from_pretrained(model_checkpoint)\n\n```", "```py\nimport matplotlib.pyplot as plt\n\nmodel_outputs = masked_lm_model(\n  **tokenizer(text=masked_insulin_sequence, return_tensors=\"pt\")\n)\nmodel_preds = model_outputs.logits\n\n# Index into the predictions at the <mask> position.\nmask_preds = model_preds[0, 30].detach().numpy()\n\n# Apply softmax to convert the model's predicted logits to probabilities.\nmask_probs = jax.nn.softmax(mask_preds)\n\n# Visualize the predicted probability of each token.\nletters = list(vocab_to_index.keys())\nfig, ax = plt.subplots(figsize=(6, 4))\nplt.bar(letters, mask_probs, color=\"grey\")\nplt.xticks(rotation=90)\nplt.title(\"Model Probabilities for the Masked Amino Acid\");\n\n```", "```py\nclass MaskPredictor:\n  \"\"\"Predict masked amino acids using a protein language model.\"\"\"\n\n  def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n    \"\"\"Initialize with a tokenizer and pretrained model.\"\"\"\n    self.tokenizer = tokenizer\n    self.model = model\n\n  def plot_predictions(self, sequence: str, mask_index: int) -> Figure:\n    \"\"\"Plot predicted probabilities for the masked amino acid.\"\"\"\n    mask_probs = self.predict(sequence, mask_index)\n    fig, _ = plt.subplots(figsize=(6, 4))\n    plt.bar(list(self.tokenizer.get_vocab().keys()), mask_probs, color=\"grey\")\n    plt.xticks(rotation=90)\n    plt.title(\n      \"Model Probabilities for the Masked Amino Acid\\n\"\n      f\"at Index={mask_index} (True Amino Acid = {sequence[mask_index]}).\"\n    )\n    return fig\n\n  def predict(self, sequence: str, mask_index: int) -> jax.Array:\n    \"\"\"Return model probabilities for masked amino acid at a position.\"\"\"\n    masked_sequence = self.mask_sequence(sequence, mask_index)\n    masked_inputs = self.tokenizer(masked_sequence, return_tensors=\"pt\")\n    model_outputs = self.model(**masked_inputs)\n    mask_preds = model_outputs.logits[0, mask_index + 1].detach().numpy()\n    mask_probs = jax.nn.softmax(mask_preds)\n    return mask_probs\n\n  @staticmethod\n  def mask_sequence(sequence: str, mask_index: int) -> str:\n    \"\"\"Insert mask token at specified index in the input sequence.\"\"\"\n    if mask_index < 0 or mask_index > len(sequence):\n      raise ValueError(\"Mask index outside of sequence range.\")\n    return f\"{sequence[0:mask_index]}<mask>{sequence[(mask_index + 1):]}\"\n\n```", "```py\nMaskPredictor(tokenizer, model=masked_lm_model).plot_predictions(\n  sequence=insulin_sequence, mask_index=26\n);\n\n```", "```py\nimport pandas as pd\n\nfrom dlfb.utils.context import assets\n\nprotein_df = pd.read_csv(assets(\"proteins/datasets/sequence_df_cco.csv\"))\nprotein_df = protein_df[~protein_df[\"term\"].isin([\"GO:0005575\", \"GO:0110165\"])]\nnum_proteins = protein_df[\"EntryID\"].nunique()\nprint(protein_df)\n\n```", "```py\n       EntryID             Sequence  taxonomyID        term aspect  Length\n0       O95231  MRLSSSPPRGPQQLSS...        9606  GO:0005622    CCO     258\n1       O95231  MRLSSSPPRGPQQLSS...        9606  GO:0031981    CCO     258\n2       O95231  MRLSSSPPRGPQQLSS...        9606  GO:0043229    CCO     258\n...        ...                  ...         ...         ...    ...     ...\n337551  E7ER32  MPPLKSPAAFHEQRRS...        9606  GO:0031974    CCO     798\n337552  E7ER32  MPPLKSPAAFHEQRRS...        9606  GO:0005634    CCO     798\n337553  E7ER32  MPPLKSPAAFHEQRRS...        9606  GO:0005654    CCO     798\n\n[294731 rows x 6 columns]\n\n```", "```py\n# Filter protein dataframe to proteins with a single location.\nnum_locations = protein_df.groupby(\"EntryID\")[\"term\"].nunique()\nproteins_one_location = num_locations[num_locations == 1].index\nprotein_df = protein_df[protein_df[\"EntryID\"].isin(proteins_one_location)]\n\ngo_function_examples = {\n  \"extracellular\": \"GO:0005576\",\n  \"membrane\": \"GO:0016020\",\n}\n\nsequences_by_function = {}\n\nmin_length = 100\nmax_length = 500  # Cap sequence length for speed and memory.\nnum_samples = 20\n\nfor function, go_term in go_function_examples.items():\n  proteins_with_function = protein_df[\n    (protein_df[\"term\"] == go_term)\n    & (protein_df[\"Length\"] >= min_length)\n    & (protein_df[\"Length\"] <= max_length)\n  ]\n  print(\n    f\"Found {len(proteins_with_function)} human proteins\\n\"\n    f\"with the molecular function '{function}' ({go_term}),\\n\"\n    f\"and {min_length}<=length<={max_length}.\\n\"\n    f\"Sampling {num_samples} proteins at random.\\n\"\n  )\n  sequences = list(\n    proteins_with_function.sample(num_samples, random_state=42)[\"Sequence\"]\n  )\n  sequences_by_function[function] = sequences\n\n```", "```py\nFound 164 human proteins\nwith the molecular function 'extracellular' (GO:0005576),\nand 100<=length<=500.\nSampling 20 proteins at random.\n\nFound 65 human proteins\nwith the molecular function 'membrane' (GO:0016020),\nand 100<=length<=500.\nSampling 20 proteins at random.\n\n```", "```py\ndef get_mean_embeddings(\n  sequences: list[str],\n  tokenizer: PreTrainedTokenizer,\n  model: PreTrainedModel,\n  device: torch.device | None = None,\n) -> np.ndarray:\n  \"\"\"Compute mean embedding for each sequence using a protein LM.\"\"\"\n  if not device:\n    device = get_device()\n\n  # Tokenize input sequences and pad them to equal length.\n  model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n\n  # Move tokenized inputs to the target device (CPU or GPU).\n  model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n\n  # Move model to the target device and set it to evaluation mode.\n  model = model.to(device)\n  model.eval()\n\n  # Forward pass without gradient tracking to obtain embeddings.\n  with torch.no_grad():\n    outputs = model(**model_inputs)\n    mean_embeddings = outputs.last_hidden_state.mean(dim=1)\n\n  return mean_embeddings.detach().cpu().numpy()\n\n```", "```py\nmodel_checkpoint = \"facebook/esm2_t6_8M_UR50D\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = EsmModel.from_pretrained(model_checkpoint)\n\n```", "```py\n# Compute mean protein embeddings for each location.\nprotein_embeddings = {\n  loc: get_mean_embeddings(sequences_by_function[loc], tokenizer, model)\n  for loc in [\"extracellular\", \"membrane\"]\n}\n\n# Reformat data.\nlabels, embeddings = [], []\nfor location, embedding in protein_embeddings.items():\n  labels.extend([location] * embedding.shape[0])\n  embeddings.append(embedding)\n  print(f\"{location}: {embedding.shape}\")\n\n```", "```py\nextracellular: (20, 320)\nmembrane: (20, 320)\n\n```", "```py\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\n\nembeddings_tsne = TSNE(n_components=2, random_state=42).fit_transform(\n  np.vstack(embeddings)\n)\nembeddings_tsne_df = pd.DataFrame(\n  {\n    \"first_dimension\": embeddings_tsne[:, 0],\n    \"second_dimension\": embeddings_tsne[:, 1],\n    \"location\": np.array(labels),\n  }\n)\n\nfig = sns.scatterplot(\n  data=embeddings_tsne_df,\n  x=\"first_dimension\",\n  y=\"second_dimension\",\n  hue=\"location\",\n  style=\"location\",\n  s=50,\n  alpha=0.7,\n)\nplt.title(\"tSNE of Protein Embeddings\")\nfig.set_xlabel(\"First Dimension\")\nfig.set_ylabel(\"Second Dimension\");\n\n```", "```py\nlabels = pd.read_csv(\n  assets(\"proteins/datasets/train_terms.tsv.zip\"), sep=\"\\t\", compression=\"infer\"\n)\nprint(labels)\n\n```", "```py\n            EntryID        term aspect\n0        A0A009IHW8  GO:0008152    BPO\n1        A0A009IHW8  GO:0034655    BPO\n2        A0A009IHW8  GO:0072523    BPO\n...             ...         ...    ...\n5363860      X5M5N0  GO:0005515    MFO\n5363861      X5M5N0  GO:0005488    MFO\n5363862      X5M5N0  GO:0003674    MFO\n\n[5363863 rows x 3 columns]\n\n```", "```py\nimport obonet\n\ndef get_go_term_descriptions(store_path: str) -> pd.DataFrame:\n  \"\"\"Return GO term to description mapping, downloading if needed.\"\"\"\n  if not os.path.exists(store_path):\n    url = \"https://current.geneontology.org/ontology/go-basic.obo\"\n    graph = obonet.read_obo(url)\n\n    # Extract GO term IDs and names from the graph nodes.\n    id_to_name = {id: data.get(\"name\") for id, data in graph.nodes(data=True)}\n    go_term_descriptions = pd.DataFrame(\n      zip(id_to_name.keys(), id_to_name.values()),\n      columns=[\"term\", \"description\"],\n    )\n    go_term_descriptions.to_csv(store_path, index=False)\n\n  else:\n    go_term_descriptions = pd.read_csv(store_path)\n  return go_term_descriptions\n\n```", "```py\ngo_term_descriptions = get_go_term_descriptions(\n  store_path=assets(\"proteins/datasets/go_term_descriptions.csv\")\n)\nprint(go_term_descriptions)\n\n```", "```py\n             term          description\n0      GO:0000001  mitochondrion in...\n1      GO:0000002  mitochondrial ge...\n2      GO:0000006  high-affinity zi...\n...           ...                  ...\n40211  GO:2001315  UDP-4-deoxy-4-fo...\n40212  GO:2001316  kojic acid metab...\n40213  GO:2001317  kojic acid biosy...\n\n[40214 rows x 2 columns]\n\n```", "```py\nlabels = labels.merge(go_term_descriptions, on=\"term\")\nlabels\n\n```", "```py\n            EntryID        term aspect          description\n0        A0A009IHW8  GO:0008152    BPO    metabolic process\n1        A0A009IHW8  GO:0034655    BPO  nucleobase-conta...\n2        A0A009IHW8  GO:0072523    BPO  purine-containin...\n...             ...         ...    ...                  ...\n4933955      X5M5N0  GO:0005515    MFO      protein binding\n4933956      X5M5N0  GO:0005488    MFO              binding\n4933957      X5M5N0  GO:0003674    MFO   molecular_function\n\n[4933958 rows x 4 columns]\n\n```", "```py\nlabels = labels[labels[\"aspect\"] == \"MFO\"]\nprint(labels[\"description\"].value_counts())\n\n```", "```py\ndescription\nmolecular_function                            78637\nbinding                                       57380\nprotein binding                               47987\n                                              ...  \nkaempferide 7-O-methyltransferase activity        1\nprotopine 6-monooxygenase activity                1\ncostunolide 3beta-hydroxylase activity            1\nName: count, Length: 6973, dtype: int64\n\n```", "```py\nfrom Bio import SeqIO\n\nsequences_file = assets(\"proteins/datasets/train_sequences.fasta\")\nfasta_sequences = SeqIO.parse(open(sequences_file), \"fasta\")\n\ndata = []\nfor fasta in fasta_sequences:\n  data.append(\n    {\n      \"EntryID\": fasta.id,\n      \"Sequence\": str(fasta.seq),\n      \"Length\": len(fasta.seq),\n    }\n  )\nsequence_df = pd.DataFrame(data)\nprint(sequence_df)\n\n```", "```py\n           EntryID             Sequence  Length\n0           P20536  MNSVTVSHAPYTITYH...     218\n1           O73864  MTEYRNFLLLFITSLS...     354\n2           O95231  MRLSSSPPRGPQQLSS...     258\n...            ...                  ...     ...\n142243      Q5RGB0  MADKGPILTSVIIFYL...     448\n142244  A0A2R8QMZ5  MGRKKIQITRIMDERN...     459\n142245  A0A8I6GHU0  HCISSLKLTAFFKRSF...     138\n\n[142246 rows x 3 columns]\n\n```", "```py\ntaxonomy_file = assets(\"proteins/datasets/train_taxonomy.tsv.zip\")\ntaxonomy = pd.read_csv(taxonomy_file, sep=\"\\t\", compression=\"infer\")\nprint(taxonomy)\n\n```", "```py\n           EntryID  taxonomyID\n0           Q8IXT2        9606\n1           Q04418      559292\n2           A8DYA3        7227\n...            ...         ...\n142243  A0A2R8QBB1        7955\n142244      P0CT72      284812\n142245      Q9NZ43        9606\n\n[142246 rows x 2 columns]\n\n```", "```py\nsequence_df = sequence_df.merge(taxonomy, on=\"EntryID\")\nsequence_df = sequence_df[sequence_df[\"taxonomyID\"] == 9606]\n\n```", "```py\nsequence_df = sequence_df.merge(labels, on=\"EntryID\")\nprint(\n  f'Dataset contains {sequence_df[\"EntryID\"].nunique()} human proteins '\n  f'with {sequence_df[\"term\"].nunique()} molecular functions.'\n)\n\n```", "```py\nDataset contains 16336 human proteins with 4101 molecular functions.\n\n```", "```py\nprint(sequence_df)\n```", "```py\n       EntryID             Sequence  Length  taxonomyID        term aspect  \\\n0       O95231  MRLSSSPPRGPQQLSS...     258        9606  GO:0003676    MFO   \n1       O95231  MRLSSSPPRGPQQLSS...     258        9606  GO:1990837    MFO   \n2       O95231  MRLSSSPPRGPQQLSS...     258        9606  GO:0001216    MFO   \n...        ...                  ...     ...         ...         ...    ...   \n152523  Q86TI6  MGAAAVRWHLCVLLAL...     347        9606  GO:0005515    MFO   \n152524  Q86TI6  MGAAAVRWHLCVLLAL...     347        9606  GO:0005488    MFO   \n152525  Q86TI6  MGAAAVRWHLCVLLAL...     347        9606  GO:0003674    MFO   \n\n                description  \n0       nucleic acid bin...  \n1       sequence-specifi...  \n2       DNA-binding tran...  \n...                     ...  \n152523      protein binding  \n152524              binding  \n152525   molecular_function  \n\n[152526 rows x 7 columns]\n\n```", "```py\nsequence_df.groupby(\"EntryID\")[\"term\"].nunique().plot.hist(\n  bins=100, figsize=(5, 3), color=\"grey\", log=True\n)\nplt.xlabel(\"Number of Molecular Function Annotations per Protein\")\nplt.ylabel(\"Frequency (log scale)\")\nplt.title(\"Distribution of Function Counts per Protein\")\nplt.tight_layout()\n\n```", "```py\nuninteresting_functions = [\n  \"GO:0003674\",  # \"molecular function\". Applies to 100% of proteins.\n  \"GO:0005488\",  # \"binding\". Applies to 93% of proteins.\n  \"GO:0005515\",  # \"protein binding\". Applies to 89% of proteins.\n]\n\nsequence_df = sequence_df[~sequence_df[\"term\"].isin(uninteresting_functions)]\nsequence_df.shape\n\n```", "```py\n(106501, 7)\n\n```", "```py\ncommon_functions = (\n  sequence_df[\"term\"]\n  .value_counts()[sequence_df[\"term\"].value_counts() >= 50]\n  .index\n)\n\nsequence_df = sequence_df[sequence_df[\"term\"].isin(common_functions)]\nsequence_df[\"term\"].value_counts()\n\n```", "```py\nterm\nGO:0003824    3875\nGO:1901363    2943\nGO:0003676    2469\n              ... \nGO:0031490      51\nGO:0019003      50\nGO:0015179      50\nName: count, Length: 303, dtype: int64\n\n```", "```py\nsequence_df = (\n  sequence_df[[\"EntryID\", \"Sequence\", \"Length\", \"term\"]]\n  .assign(value=1)\n  .pivot(\n    index=[\"EntryID\", \"Sequence\", \"Length\"], columns=\"term\", values=\"value\"\n  )\n  .fillna(0)\n  .astype(int)\n  .reset_index()\n)\nprint(sequence_df)\n\n```", "```py\nterm      EntryID             Sequence  Length  GO:0000166  GO:0000287  ...  \\\n0      A0A024R6B2  MIASCLCYLLLPATRL...     670           0           0  ...   \n1      A0A087WUI6  MSRKISKESKKVNISS...     698           0           0  ...   \n2      A0A087X1C5  MGLEALVPLAMIVAIF...     515           0           0  ...   \n...           ...                  ...     ...         ...         ...  ...   \n10706      Q9Y6Z7  MNGFASLLRRNQFILL...     277           0           0  ...   \n10707      X5D778  MPKGGCPKAPQQEELP...     421           0           0  ...   \n10708      X5D7E3  MLDLTSRGQVGTSRRM...     237           0           0  ...   \n\nterm   GO:1901702  GO:1901981  GO:1902936  GO:1990782  GO:1990837  \n0               0           0           0           0           0  \n1               0           0           0           0           0  \n2               0           0           0           0           0  \n...           ...         ...         ...         ...         ...  \n10706           0           0           0           0           0  \n10707           0           0           0           0           0  \n10708           0           0           0           0           0  \n\n[10709 rows x 306 columns]\n\n```", "```py\nsequence_df[\"EntryID\"].nunique()\n\n```", "```py\n10709\n\n```", "```py\nsequence_df[\"Sequence\"].nunique()\n\n```", "```py\n10698\n\n```", "```py\nprint(sequence_df[sequence_df[\"EntryID\"].isin([\"P0DP23\", \"P0DP24\", \"P0DP25\"])])\n\n```", "```py\nterm EntryID             Sequence  Length  GO:0000166  GO:0000287  ...  \\\n1945  P0DP23  MADQLTEEQIAEFKEA...     149           0           0  ...   \n1946  P0DP24  MADQLTEEQIAEFKEA...     149           0           0  ...   \n1947  P0DP25  MADQLTEEQIAEFKEA...     149           0           0  ...   \n\nterm  GO:1901702  GO:1901981  GO:1902936  GO:1990782  GO:1990837  \n1945           0           0           0           0           0  \n1946           0           0           0           0           0  \n1947           0           0           0           0           0  \n\n[3 rows x 306 columns]\n\n```", "```py\nprint(sequence_df.shape)\nsequence_df = sequence_df[sequence_df[\"Length\"] <= 500]\nprint(sequence_df.shape)\n\n```", "```py\n(10709, 306)\n(5957, 306)\n\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\n# 60% of the proteins will go into the training set.\ntrain_sequence_ids, valid_test_sequence_ids = train_test_split(\n  list(set(sequence_df[\"EntryID\"])), test_size=0.40, random_state=42\n)\n\n# Split the remaining 40% evenly between validation and test sets.\nvalid_sequence_ids, test_sequence_ids = train_test_split(\n  valid_test_sequence_ids, test_size=0.50, random_state=42\n)\n\n```", "```py\nsequence_splits = {\n  \"train\": sequence_df[sequence_df[\"EntryID\"].isin(train_sequence_ids)],\n  \"valid\": sequence_df[sequence_df[\"EntryID\"].isin(valid_sequence_ids)],\n  \"test\": sequence_df[sequence_df[\"EntryID\"].isin(test_sequence_ids)],\n}\n\nfor split, df in sequence_splits.items():\n  print(f\"{split} has {len(df)} entries.\")\n\n```", "```py\ntrain has 3574 entries.\nvalid has 1191 entries.\ntest has 1192 entries.\n\n```", "```py\ndef store_sequence_embeddings(\n  sequence_df: pd.DataFrame,\n  store_prefix: str,\n  tokenizer: PreTrainedTokenizer,\n  model: PreTrainedModel,\n  batch_size: int = 64,\n  force: bool = False,\n) -> None:\n  \"\"\"Extract and store mean embeddings for each protein sequence.\"\"\"\n  model_name = str(model.name_or_path).replace(\"/\", \"_\")\n  store_file = f\"{store_prefix}_{model_name}.feather\"\n\n  if not os.path.exists(store_file) or force:\n    device = get_device()\n\n    # Iterate through protein dataframe in batches, extracting embeddings.\n    n_batches = ceil(sequence_df.shape[0] / batch_size)\n    batches: list[np.ndarray] = []\n    for i in range(n_batches):\n      batch_seqs = list(\n        sequence_df[\"Sequence\"][i * batch_size : (i + 1) * batch_size]\n      )\n      batches.extend(get_mean_embeddings(batch_seqs, tokenizer, model, device))\n\n    # Store each of the embedding values in a separate column in the dataframe.\n    embeddings = pd.DataFrame(np.vstack(batches))\n    embeddings.columns = [f\"ME:{int(i)+1}\" for i in range(embeddings.shape[1])]\n    df = pd.concat([sequence_df.reset_index(drop=True), embeddings], axis=1)\n    df.to_feather(store_file)\n\ndef load_sequence_embeddings(\n  store_file_prefix: str, model_checkpoint: str\n) -> pd.DataFrame:\n  \"\"\"Load stored embedding DataFrame from disk.\"\"\"\n  model_name = model_checkpoint.replace(\"/\", \"_\")\n  store_file = f\"{store_file_prefix}_{model_name}.feather\"\n  return pd.read_feather(store_file)\n\n```", "```py\nmodel_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = EsmModel.from_pretrained(model_checkpoint)\n\nfor split, df in sequence_splits.items():\n  store_sequence_embeddings(\n    sequence_df=df,\n    store_prefix=assets(f\"proteins/datasets/protein_dataset_{split}\"),\n    tokenizer=tokenizer,\n    model=model,\n  )\n\n```", "```py\ntrain_df = load_sequence_embeddings(\n  assets(\"proteins/datasets/protein_dataset_train\"),\n  model_checkpoint=model_checkpoint,\n)\n\nprint(train_df)\n\n```", "```py\n         EntryID             Sequence  Length  GO:0000166  GO:0000287  ...  \\\n0     A0A0C4DG62  MAHVGSRKRSRSRSRS...     218           0           0  ...   \n1     A0A1B0GTB2  MVITSENDEDRGGQEK...      48           0           0  ...   \n2         A0AVI4  MDSPEVTFTLAYLVFA...     362           0           0  ...   \n...          ...                  ...     ...         ...         ...  ...   \n3571      Q9Y6W5  MPLVTRNIEPRHLCRQ...     498           0           0  ...   \n3572      Q9Y6W6  MPPSPLDDRVVVALSR...     482           0           0  ...   \n3573      Q9Y6Y9  MLPFLFFSTLFSSIFT...     160           0           0  ...   \n\n        ME:636    ME:637    ME:638    ME:639    ME:640  \n0     0.062926  0.040286  0.030008 -0.033614  0.023891  \n1     0.129815 -0.044294  0.023842 -0.020635  0.125583  \n2     0.153848 -0.075747  0.024440 -0.123321  0.020945  \n...        ...       ...       ...       ...       ...  \n3571 -0.001535 -0.084161 -0.014317 -0.141801 -0.040719  \n3572  0.120192 -0.086032 -0.016481 -0.108710 -0.077937  \n3573  0.114847 -0.028570  0.084638  0.038610  0.087047  \n\n[3574 rows x 946 columns]\n\n```", "```py\nimport tensorflow as tf\n\ndef convert_to_tfds(\n  df: pd.DataFrame,\n  embeddings_prefix: str = \"ME:\",\n  target_prefix: str = \"GO:\",\n  is_training: bool = False,\n  shuffle_buffer: int = 50,\n) -> tf.data.Dataset:\n  \"\"\"Convert embedding DataFrame into a TensorFlow dataset.\"\"\"\n  dataset = tf.data.Dataset.from_tensor_slices(\n    {\n      \"embedding\": df.filter(regex=f\"^{embeddings_prefix}\").to_numpy(),\n      \"target\": df.filter(regex=f\"^{target_prefix}\").to_numpy(),\n    }\n  )\n  if is_training:\n    dataset = dataset.shuffle(shuffle_buffer).repeat()\n  return dataset\n\n```", "```py\ntrain_ds = convert_to_tfds(train_df, is_training=True)\n```", "```py\nbatch_size = 32\n\nbatch = next(train_ds.batch(batch_size).as_numpy_iterator())\nbatch[\"embedding\"].shape, batch[\"target\"].shape\n\n```", "```py\n((32, 640), (32, 303))\n```", "```py\ndef build_dataset(\n  store_file_prefix: str, model_checkpoint: str\n) -> dict[str, tf.data.Dataset]:\n  \"\"\"Build train/valid/test TensorFlow datasets from stored embeddings.\"\"\"\n  dataset_splits = {}\n\n  for split in [\"train\", \"valid\", \"test\"]:\n    dataset_splits[split] = convert_to_tfds(\n      df=load_sequence_embeddings(\n        store_file_prefix=f\"{store_file_prefix}_{split}\",\n        model_checkpoint=model_checkpoint,\n      ),\n      is_training=(split == \"train\"),\n    )\n  return dataset_splits\n\n```", "```py\ndataset_splits = build_dataset(\n  assets(\"proteins/datasets/protein_dataset\"), model_checkpoint=model_checkpoint\n)\n\n```", "```py\nimport flax.linen as nn\nfrom flax.training import train_state\n\nclass Model(nn.Module):\n  \"\"\"Simple MLP for protein function prediction.\"\"\"\n\n  num_targets: int\n  dim: int = 256\n\n  @nn.compact\n  def __call__(self, x):\n    \"\"\"Apply MLP layers to input features.\"\"\"\n    x = nn.Sequential(\n      [\n        nn.Dense(self.dim * 2),\n        jax.nn.gelu,\n        nn.Dense(self.dim),\n        jax.nn.gelu,\n        nn.Dense(self.num_targets),\n      ]\n    )(x)\n    return x\n\n  def create_train_state(self, rng: jax.Array, dummy_input, tx) -> TrainState:\n    \"\"\"Initialize model parameters and return a training state.\"\"\"\n    variables = self.init(rng, dummy_input)\n    return TrainState.create(\n      apply_fn=self.apply, params=variables[\"params\"], tx=tx\n    )\n\n```", "```py\ntargets = list(train_df.columns[train_df.columns.str.contains(\"GO:\")])\n\nmlp = Model(num_targets=len(targets))\n\n```", "```py\n@jax.jit\ndef train_step(state, batch):\n  \"\"\"Run a single training step and update model parameters.\"\"\"\n\n  def calculate_loss(params):\n    \"\"\"Compute sigmoid cross-entropy loss from logits.\"\"\"\n    logits = state.apply_fn({\"params\": params}, x=batch[\"embedding\"])\n    loss = optax.sigmoid_binary_cross_entropy(logits, batch[\"target\"]).mean()\n    return loss\n\n  grad_fn = jax.value_and_grad(calculate_loss, has_aux=False)\n  loss, grads = grad_fn(state.params)\n  state = state.apply_gradients(grads=grads)\n  return state, loss\n\n```", "```py\nimport sklearn\n\ndef compute_metrics(\n  targets: np.ndarray, probs: np.ndarray, thresh=0.5\n) -> dict[str, float]:\n  \"\"\"Compute accuracy, recall, precision, auPRC, and auROC.\"\"\"\n  if np.sum(targets) == 0:\n    return {\n      m: 0.0 for m in [\"accuracy\", \"recall\", \"precision\", \"auprc\", \"auroc\"]\n    }\n  return {\n    \"accuracy\": metrics.accuracy_score(targets, probs >= thresh),\n    \"recall\": metrics.recall_score(targets, probs >= thresh).item(),\n    \"precision\": metrics.precision_score(\n      targets,\n      probs >= thresh,\n      zero_division=0.0,\n    ).item(),\n    \"auprc\": metrics.average_precision_score(targets, probs).item(),\n    \"auroc\": metrics.roc_auc_score(targets, probs).item(),\n  }\n\n```", "```py\ndef eval_step(state, batch) -> dict[str, float]:\n  \"\"\"Run evaluation step and return mean metrics over targets.\"\"\"\n  logits = state.apply_fn({\"params\": state.params}, x=batch[\"embedding\"])\n  loss = optax.sigmoid_binary_cross_entropy(logits, batch[\"target\"]).mean()\n  target_metrics = calculate_per_target_metrics(logits, batch[\"target\"])\n  metrics = {\n    \"loss\": loss.item(),\n    **pd.DataFrame(target_metrics).mean(axis=0).to_dict(),\n  }\n  return metrics\n\ndef calculate_per_target_metrics(logits, targets):\n  \"\"\"Compute metrics for each target in a multi-label batch.\"\"\"\n  probs = jax.nn.sigmoid(logits)\n  target_metrics = []\n  for target, prob in zip(targets, probs):\n    target_metrics.append(compute_metrics(target, prob))\n  return target_metrics\n\n```", "```py\ndef train(\n  state: TrainState,\n  dataset_splits: dict[str, tf.data.Dataset],\n  batch_size: int,\n  num_steps: int = 300,\n  eval_every: int = 30,\n):\n  \"\"\"Train model using batched TF datasets and track performance metrics.\"\"\"\n  # Create containers to handle calculated during training and evaluation.\n  train_metrics, valid_metrics = [], []\n\n  # Create batched dataset to pluck batches from for each step.\n  train_batches = (\n    dataset_splits[\"train\"]\n    .batch(batch_size, drop_remainder=True)\n    .as_numpy_iterator()\n  )\n\n  steps = tqdm(range(num_steps))  # Steps with progress bar.\n  for step in steps:\n    steps.set_description(f\"Step {step + 1}\")\n\n    # Get batch of training data, convert into a JAX array, and train.\n    state, loss = train_step(state, next(train_batches))\n    train_metrics.append({\"step\": step, \"loss\": loss.item()})\n\n    if step % eval_every == 0:\n      # For all the evaluation batches, calculate metrics.\n      eval_metrics = []\n      for eval_batch in (\n        dataset_splits[\"valid\"].batch(batch_size=batch_size).as_numpy_iterator()\n      ):\n        eval_metrics.append(eval_step(state, eval_batch))\n      valid_metrics.append(\n        {\"step\": step, **pd.DataFrame(eval_metrics).mean(axis=0).to_dict()}\n      )\n\n  return state, {\"train\": train_metrics, \"valid\": valid_metrics}\n\n```", "```py\nimport optax\n\nfrom dlfb.utils.restore import restorable\n\n# Initiate training state with dummy data from a single batch.\nrng = jax.random.PRNGKey(42)\nrng, rng_init = jax.random.split(key=rng, num=2)\n\nstate, metrics = restorable(train)(\n  state=mlp.create_train_state(\n    rng=rng_init, dummy_input=batch[\"embedding\"], tx=optax.adam(0.001)\n  ),\n  dataset_splits=dataset_splits,\n  batch_size=32,\n  num_steps=300,\n  eval_every=30,\n  store_path=assets(\"proteins/models/mlp\"),\n)\n\n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom dlfb.utils.metric_plots import DEFAULT_SPLIT_COLORS\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n\n# Plot training loss curve.\nlearning_data = pd.concat(\n  pd.DataFrame(metrics[split]).melt(\"step\").assign(split=split)\n  for split in [\"train\", \"valid\"]\n)\n\nsns.lineplot(\n  ax=ax[0],\n  x=\"step\",\n  y=\"value\",\n  hue=\"split\",\n  data=learning_data[learning_data[\"variable\"] == \"loss\"],\n  palette=DEFAULT_SPLIT_COLORS,\n)\nax[0].set_title(\"Loss over training steps.\")\n\n# Plot validation metrics curves.\nsns.lineplot(\n  ax=ax[1],\n  x=\"step\",\n  y=\"value\",\n  hue=\"variable\",\n  style=\"variable\",\n  data=learning_data[learning_data[\"variable\"] != \"loss\"],\n  palette=\"Set2\",\n)\nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nax[1].set_title(\"Validation metrics over training steps.\");\n\n```", "```py\nvalid_df = load_sequence_embeddings(\n  store_file_prefix=f\"{assets('proteins/datasets/protein_dataset')}_valid\",\n  model_checkpoint=model_checkpoint,\n)\n\n# Use batch size of 1 to avoid dropping the remainder.\nvalid_probs = []\nfor valid_batch in dataset_splits[\"valid\"].batch(1).as_numpy_iterator():\n  logits = state.apply_fn({\"params\": state.params}, x=valid_batch[\"embedding\"])\n  valid_probs.extend(jax.nn.sigmoid(logits))\n\nvalid_true_df = valid_df[[\"EntryID\"] + targets].set_index(\"EntryID\")\nvalid_prob_df = pd.DataFrame(\n  np.stack(valid_probs), columns=targets, index=valid_true_df.index\n)\n\n```", "```py\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(11, 4))\n\nsns.heatmap(\n  ax=ax[0],\n  data=valid_true_df,\n  yticklabels=False,\n  xticklabels=False,\n  cmap=\"flare\",\n)\nax[0].set_title(\"True functional annotations by protein.\")\nax[0].set_xlabel(\"Functional category\")\n\nsns.heatmap(\n  ax=ax[1],\n  data=valid_prob_df,\n  yticklabels=False,\n  xticklabels=False,\n  cmap=\"flare\",\n)\nax[1].set_title(\"Predicted functional annotations by protein.\")\nax[1].set_xlabel(\"Functional category\");\n\n```", "```py\nmetrics_by_function = {}\nfor function in targets:\n  metrics_by_function[function] = compute_metrics(\n    valid_true_df[function].values, valid_prob_df[function].values\n  )\n\noverview_valid = (\n  pd.DataFrame(metrics_by_function)\n  .T.merge(go_term_descriptions, left_index=True, right_on=\"term\")\n  .set_index(\"term\")\n  .sort_values(\"auprc\", ascending=False)\n)\nprint(overview_valid)\n\n```", "```py\n            accuracy    recall  precision     auprc     auroc  \\\nterm                                                            \nGO:0004930  0.958858  0.000000   0.000000  0.948591  0.982272   \nGO:0004888  0.945424  0.177215   1.000000  0.849885  0.968354   \nGO:0003824  0.848027  0.731591   0.819149  0.849362  0.909372   \n...              ...       ...        ...       ...       ...   \nGO:0003774  0.000000  0.000000   0.000000  0.000000  0.000000   \nGO:0051015  0.000000  0.000000   0.000000  0.000000  0.000000   \nGO:1902936  0.000000  0.000000   0.000000  0.000000  0.000000   \n\n                    description  \nterm                             \nGO:0004930  G protein-couple...  \nGO:0004888  transmembrane si...  \nGO:0003824   catalytic activity  \n...                         ...  \nGO:0003774  cytoskeletal mot...  \nGO:0051015  actin filament b...  \nGO:1902936  phosphatidylinos...  \n\n[303 rows x 6 columns]\n\n```", "```py\n# Compute number of occurrences of each function in the training set.\noverview_valid = overview_valid.merge(\n  pd.DataFrame(train_df[targets].sum(), columns=[\"train_n\"]),\n  left_index=True,\n  right_index=True,\n)\nprint(overview_valid)\n\n```", "```py\n            accuracy    recall  precision     auprc     auroc  \\\nGO:0004930  0.958858  0.000000   0.000000  0.948591  0.982272   \nGO:0004888  0.945424  0.177215   1.000000  0.849885  0.968354   \nGO:0003824  0.848027  0.731591   0.819149  0.849362  0.909372   \n...              ...       ...        ...       ...       ...   \nGO:0003774  0.000000  0.000000   0.000000  0.000000  0.000000   \nGO:0051015  0.000000  0.000000   0.000000  0.000000  0.000000   \nGO:1902936  0.000000  0.000000   0.000000  0.000000  0.000000   \n\n                    description  train_n  \nGO:0004930  G protein-couple...      138  \nGO:0004888  transmembrane si...      228  \nGO:0003824   catalytic activity     1210  \n...                         ...      ...  \nGO:0003774  cytoskeletal mot...        5  \nGO:0051015  actin filament b...       17  \nGO:1902936  phosphatidylinos...       18  \n\n[303 rows x 7 columns]\n\n```", "```py\nfig = sns.scatterplot(\n  x=\"train_n\", y=\"auprc\", data=overview_valid, alpha=0.5, s=50, color=\"grey\"\n)\nfig.set_xlabel(\"# Train instances\")\nfig.set_ylabel(\"Validation auPRC\");\n\n```", "```py\ndef make_coin_flip_predictions(\n  valid_true_df: pd.DataFrame, targets: list[str]\n) -> pd.DataFrame:\n  \"\"\"Make random coin flip predictions for each protein function.\"\"\"\n  predictions = np.random.choice([0.0, 1.0], size=valid_true_df.shape)\n  return pd.DataFrame(predictions, columns=targets, index=valid_true_df.index)\n\ndef make_proportional_predictions(\n  valid_true_df: pd.DataFrame, train_df: pd.DataFrame, targets: list[str]\n) -> pd.DataFrame:\n  \"\"\"Make random protein function predictions proportional to frequency.\"\"\"\n  percent_1_train = dict(train_df[targets].mean())\n  proportional_preds = []\n  for target_column in targets:\n    prob_1 = percent_1_train[target_column]\n    prob_0 = 1 - prob_1\n    proportional_preds.append(\n      np.random.choice([0.0, 1.0], size=len(valid_true_df), p=[prob_0, prob_1])\n    )\n  return pd.DataFrame(\n    np.stack(proportional_preds).T, columns=targets, index=valid_true_df.index\n  )\n\n```", "```py\nprediction_methods = {\n  \"coin_flip_baseline\": make_coin_flip_predictions(valid_true_df, targets),\n  \"proportional_guess_baseline\": make_proportional_predictions(\n    valid_true_df, train_df, targets\n  ),\n  \"model\": valid_prob_df,\n}\n\n```", "```py\nmetrics_by_method = {}\nfor method, preds_df in prediction_methods.items():\n  metrics_by_method[method] = pd.DataFrame(\n    [\n      compute_metrics(valid_true_df.iloc[i], preds_df.iloc[i])\n      for i in range(len(valid_true_df))\n    ]\n  ).mean()\n\nprint(pd.DataFrame(metrics_by_method))\n\n```", "```py\n           coin_flip_baseline  proportional_guess_baseline     model\naccuracy             0.500916             0.956447          0.978569\nrecall               0.499229             0.093555          0.128532\nprecision            0.023883             0.079994          0.424301\nauprc                0.025307             0.039701          0.412350\nauroc                0.500027             0.535605          0.882679\n\n```", "```py\nauprc_by_function = {}\n\nfor method, preds_df in prediction_methods.items():\n  metrics_by_function = {}\n\n  for function in targets:\n    metrics_by_function[function] = compute_metrics(\n      valid_true_df[function], preds_df[function]\n    )\n\n  auprc_by_function[method] = (\n    pd.DataFrame(metrics_by_function)\n    .T.merge(go_term_descriptions, left_index=True, right_on=\"term\")\n    .set_index(\"term\")\n    .sort_values(\"auprc\", ascending=False)\n  )[\"auprc\"].to_dict()\n\n```", "```py\nbest_performing = (\n  pd.DataFrame(auprc_by_function)\n  .merge(go_term_descriptions, left_index=True, right_on=\"term\")\n  .set_index(\"term\")\n  .sort_values(\"model\", ascending=False)\n  .head(20)\n  .melt(\"description\")\n)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.barplot(\n  x=\"description\",\n  y=\"value\",\n  hue=\"variable\",\n  data=best_performing,\n)\nax.set_title(\"The model's 20 best performing protein functions\")\nax.set_ylabel(\"Validation auPRC\")\nplt.xticks(rotation=90);\n\n```", "```py\neval_metrics = []\n\nfor split in [\"valid\", \"test\"]:\n  split_metrics = []\n\n  for eval_batch in dataset_splits[split].batch(32).as_numpy_iterator():\n    split_metrics.append(eval_step(state, eval_batch))\n\n  eval_metrics.append(\n    {\"split\": split, **pd.DataFrame(split_metrics).mean(axis=0).to_dict()}\n  )\nprint(pd.DataFrame(eval_metrics))\n\n```", "```py\n   split      loss  accuracy    recall  precision     auprc     auroc\n0  valid  0.080156  0.978457  0.126869   0.418515  0.411870  0.880883\n1   test  0.080675  0.978032  0.125820   0.435193  0.410439  0.879234\n\n```"]