- en: 6 Beyond natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How transformer layers work on data other than text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helping LLMs to write working software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tweaking LLMs so they understand mathematical notation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How transformers replace the input and output steps to work with images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While modeling natural language was the transformers’ primary purpose, machine
    learning researchers quickly discovered they could predict anything involving
    data sequences. Transformers view a sentence as a sequence of tokens and either
    produce a related sequence of tokens, such as a translation from one language
    to another, or predict the following tokens in a sequence, such as when answering
    questions or acting like a chatbot. While sequence modeling and prediction are
    potent tools for interpreting and generating natural language, natural language
    is the only domain where LLMs can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Many data types, other than human language, can be represented as a sequence
    of tokens. Source code used to implement software is one example. Instead of the
    words and syntax you would expect to see in English, source code is written in
    a computer programming language like Python. Source code has its own structure
    that describes the operations a software developer wants a computer to perform.
    Like human language, the tokens in the source code have meaning according to the
    language used and the context in which they appear. If anything, source code is
    more highly structured and specific than human language. A programming language
    with shades of ambiguity and meaning would be challenging for a computer to interpret
    and harder for others to modify and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source code, or simply “code” (which is how we’ll refer to it from here on),
    is just one example of how LLMs and transformers work with data that is not natural
    language. Almost any data you can recast as a sequence of tokens can use transformers
    and the many lessons we have learned about how LLMs work. This chapter will review
    three examples that become progressively less like natural language: code, mathematics,
    and computer vision.'
  prefs: []
  type: TYPE_NORMAL
- en: Each of these three different types of data, known as data *modalities*, will
    require a new way of looking at a transformer’s inputs or outputs. However, in
    all cases, the transformer itself will remain unchanged. We will still stack multiple
    transformer layers on top of each other to build a model, and we will continue
    to train the transformer layers using gradient descent. Code, being the most similar
    to natural language, does not require too many changes. To make a code LLM work
    well, though, we will change how the outputs of the LLM generate subsequent tokens.
    Next, we will look at mathematics, where we need to change tokenization to get
    an LLM to succeed at basic operations such as addition. Finally, for computer
    vision, which concerns working with images and performing tasks such as object
    detection and identification, we will modify both the inputs and outputs, showing
    how you can convert a very different type of data into a sequence by replacing
    the concept of tokens entirely. We show the parts of LLMs that you must modify
    to work with each data modality in figure [6.1](#fig__codeMathCV).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F01_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 If we break an LLM into three primary components—input (tokenization),
    transformation (transformers), and output generation (unembedding)—we can use
    new data modalities by changing at least one of the input or output components.
    Meanwhile, the transformer does not require modification for most use cases because
    it is general-purpose.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In working with all three new types of data, we must solve a common problem.
    How do we give the LLM or transformer the ability to use knowledge related to
    that specific subject area? We commonly handle this by integrating external software
    into the LLM. You can think of these external software components as tools. Similarly
    to how you need a hammer to drive a nail through a piece of wood, LLMs can benefit
    from using tools to achieve end goals. Tools built for code will help us improve
    coding LLMs. Knowing how humans do math and the tools we use to automate math
    will help us make better math LLMs. Understanding how we represent images as pixels
    (which we ultimately transform into sequences of numbers representing the amount
    of red, green, and blue in one part of an image) will allow us to convert them
    into sequences for the LLM. As you think about the specific knowledge related
    to your work where LLMs have not yet been applied, you will be able to identify
    the unique characteristics of the data you work with to modify an LLM to better
    operate with data from that domain of knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 LLMs for software development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve already briefly discussed that LLMs can write source code for software.
    In chapter [4](../Text/chapter-4.html), we asked ChatGPT to write some Python
    code for calculating the mathematical constant ![equation image](../Images/eq-chapter-6-13-1.png).
    Next, we asked it to convert that code into an obscure language called Modula-3\.
    Software was one of the first things people discovered LLMs could help with as
    a relatively natural consequence of how programming works. Programming languages
    are designed to be read and written by humans like text! Consequently, we can
    generate code without changing the tokenization process. Everything we have discussed
    about constructing LLMs applies equally to code and human languages.
  prefs: []
  type: TYPE_NORMAL
- en: We can see this by looking at ChatGPT’s tokenization of two similar code segments
    for Python and Java in figure [6.2](#fig__pythonJavaTokens). Here, we use shades
    of grey to show the OpenAI tokenizer ([https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)),
    which breaks code into different tokens. While the same token might have a different
    color in each example, we can focus on how the tokenizer breaks code into tokens
    and the similarities between both examples. These include things like
  prefs: []
  type: TYPE_NORMAL
- en: The indentation for each line of code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `x` and `i` variables (in most cases)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function name and return statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operators, such as `+=`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These similarities make it far easier for an LLM to correlate the similarity
    between each piece of code. The similarities also mean that the LLM shares information
    between programming languages with common naming, syntax, and coding practices
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F02_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Two similar samples of code written in the programming languages
    Python (left) and Java (right). These show how byte-pair encoding can identify
    similar tokens across different languages. The boxes show individual tokens. Standard
    tokenization methods for human languages do a reasonable job on code since it
    has many similarities to natural language.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Software developers are encouraged to use meaningful variable names that reflect
    a variable’s role or purpose in the programs they write. Variables named like
    `initValue` are broken up into two tokens for `init` and `Value`, using the same
    tokens to represent natural language text where the prefix “init” of the word
    “Value” occurs. So not only do we share information between programming languages
    with similar syntax, but we also share information about the context and intention
    of code via variable names. LLMs also benefit from the code comments that programmers
    add to describe complex parts of the code for themselves or other programmers.
    In figure [6.3](#fig__javaCommented), we have the Java version repeated with a
    change in the variable name and a descriptive (but unnecessary in real life) comment
    at the top of the function.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F03_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 Code written in Java, including a comment describing what the code
    does. Because (good) code (hopefully) has a lot of comments, there is a natural
    mix of natural language and code for the LLM to use to obtain information. When
    variables have descriptive names, it becomes easier for the model to correlate
    information between the code and the intent described in comments and variable
    names.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In most cases, we get the same tokens between code and comments, linking human
    and programming languages together since they use the same representation. Whether
    we are working with a programming language or natural language, we get the same
    tokens and embeddings. The beauty of this is that an LLM will reuse information
    about natural languages to capture the meaning of the source code, much like human
    programmers do.
  prefs: []
  type: TYPE_NORMAL
- en: In each case, we see that the tokenization is not perfect for the code. There
    are edge cases where the LLM’s tokenizer does not convert the data types in the
    code to the same token. For example, you can see that the token for `(double`
    in the function argument is handled differently from the token for `double` in
    the function body. However, these differences are similar to the problems we already
    see in LLMs for natural language, where different cases of punctuation around
    a word like “hello ”, “hello.”, and “hello!” are interpreted as different tokens.
    Since LLMs can handle these minor differences, it makes sense that they can also
    handle the same problem for code. The problem is, in many ways, easier for an
    LLM to handle in code because code is case sensitive, so we do not need to worry
    about textual situations like “hello” and “Hello” being inappropriately mapped
    to different tokens. In code, “hello” and “Hello” would be separate and distinct
    variable or function names. Treating them as separate tokens is correct because
    the programming language treats them as different elements.
  prefs: []
  type: TYPE_NORMAL
- en: Code generation is particularly interesting from an application perspective
    because of the various opportunities for self-validation. We can apply all the
    lessons on supervised fine tuning (SFT) and reinforcement learning with human
    feedback (RLHF) from chapter [5](../Text/chapter-5.html) to make an LLM an effective
    coding agent.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Improving LLMs to work with code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step to improving an LLM for code is ensuring that code examples
    are present within the initial training data. Due to the nature of the internet,
    most LLM developers have already done this: code examples are frequent online
    and naturally make their way into everyone’s training datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Improving the results then becomes an opportunity to apply SFT, where we collect
    additional code examples and fine-tune our LLM on the given code examples. Open
    source repositories like GitHub, which contain significant volumes of code, make
    obtaining a large amount of code especially easy. Code collected from sources
    such as GitHub forms the basis of a fine-tuning dataset for LLMs that interpret
    and produce code.
  prefs: []
  type: TYPE_NORMAL
- en: The more interesting case is using RLHF to improve a model’s utility for writing
    code. Again, there are many tools and datasets available that make it possible
    to build a decent RLHF dataset for a coding assistant. Sources like Stack Overflow
    allow users to enter questions, provide a facility for other people to give answers
    to these questions, and include a system where other users vote on the best answers.
    Data sources include coding competitions like CodeJam, which provide many example
    solutions to a specific coding problem. Incorporating information from data sources
    like these is shown in figure [6.4](#fig__RLHF_for_code).
  prefs: []
  type: TYPE_NORMAL
- en: 'Like all good machine learning solutions, you get the best results if you create
    and label your own data specific to your task. It is rumored that OpenAI did this
    for generating code, hiring contractors to complete coding tasks as part of creating
    the data for their system [1]. Regardless of how training and fine-tuning data
    is collected, the overall strategy remains the same: use standard tokenizers and
    SFT with RLHF to make an LLM tailored to generate code. This recipe has been used
    successfully to produce LLMs such as Code Llama [2] and StarCoder [3].'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F04_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 Developing an LLM for code applies multiple rounds of fine-tuning.
    Standard training procedures, such as those described in chapter [4](../Text/chapter-4.html),
    produce an initial base LLM. Using a large amount of code, SFT creates an LLM
    that works well with code. Including RLHF as a second fine-tuning step improves
    the LLM’s ability to produce code.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.1.2 Validating code generated by LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLMs are particularly useful for code generation because there is an objective
    and easy-to-run verification step: attempting to compile the code into an executable
    program [4]. When generating natural language, it is challenging to check the
    correctness of the output generated by an LLM because natural language can be
    subjective. There isn’t an automated way to check the truthfulness or veracity
    of the output generated by an LLM. However, when generating code, simply checking
    whether the code compiles successfully into an executable is a good first step
    and catches a large portion of the incorrect code. Some commercial products take
    this a step further and integrate tools such as compilers (software that transforms
    source code into executables) and visualization tools into their backend. For
    example, ChatGPT can check whether the code it writes compiles before returning
    it to the user. If the code doesn’t pass this verification step, ChatGPT will
    try to generate different code for the prompt it received. If the model cannot
    create valid code to compile, it will warn the user of this fact.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond checking whether code can compile, LLMs are increasingly able to create
    methods for validating functional correctness. Many code generation tools utilize
    LLM to generate unit tests, which are tiny programs that provide sample input
    into generated code and validate that it produces the correct result. In some
    cases, these capabilities require the developer to describe the test cases that
    they want the LLM to generate, and the LLM creates an initial implementation as
    a starting point for further testing.
  prefs: []
  type: TYPE_NORMAL
- en: Code is particularly special because multiple ways exist to validate its output
    beyond just compilation. For example, code compilation can’t happen until the
    LLM finishes generating its response.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that LLMs are expensive to run, and we don’t want to keep a user
    waiting too long for output, it would be ideal if the LLM could correct errors
    before completing a large generation. Again, applying the lessons from chapter
    5, we can use a syntax parser to check whether the code is incorrect before completing
    the entire generation process. If portions of the output code fail a basic syntax
    check, we can instruct the LLM to regenerate just that faulty portion of code.
    We show the basic process behind this in figure [6.5](#fig__codeSyntaxCheck),
    where the LLM performs a check on a per-token basis instead of waiting for the
    generation to complete before checking the code using compilation. The syntax
    check is less expensive and can happen faster than compilation, but it does not
    validate that a compiler can turn the code into a working executable program.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F05_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 A Python code example where the current tokens `if(A > B)` have been
    generated. If the next token produced by the LLM is a newline, a syntax error
    will occur because an `if` statement must end in a colon to be valid. Running
    a syntax checker on each new token allows us to catch this error and force the
    LLM to pick an alternative token that doesn’t cause a syntax error.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.1.3 Improving code via formatting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using parsers for syntax checking and compilers to produce working executables
    makes it far easier to adapt LLMs to the new problem domain of generating code.
    However, one additional trick is helpful. We can use tools known as *code formatters*
    (also known by programmers as *linters*) to change tokenization and improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that there can be many ways to write code that performs the same
    functions yet is tokenized differently. Applying a linter to adjust source code
    formatting helps remove differences between two functionally equivalent, yet different
    pieces of code. While reformatting code is not a requirement to make code LLMs
    function well, it helps to avoid unnecessary redundancy that can occur. For example,
    consider the Java programming language that uses brackets to begin and end a new
    scope in a program. Various forms of white space are now nonimportant but would
    be tokenized differently, especially since the brackets are optional for a scope
    that only uses a single line of code! Figure [6.6](#fig__codeForamtter) shows
    how these different legal formats exist for the code that performs the same functions
    and how we could, ideally, convert code to a single canonical representation.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F06_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 A Java code example of how multiple ways to format the same code
    will lead to different tokenizations, even though each is semantically identical.
    Linters are a common tool to force code to follow a specific formatting rule.
    Instead, a linter can be used to create an identical “base” form, thus avoiding
    representing unnecessary information (like spaces versus tabs).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Removing nonfunctional aspects of code is called *canonicalization*, meaning
    we convert code with formatting variations into a standard or “canonical” form.
    Here, we demonstrated a robust method of canonicalization by adding special tokens
    like `<NEW SCOPE>` that capture the fact that a new context exists for the `if`
    statement, regardless of whether it’s a single-line or multiline statement. Instead
    of adding special tokens, we can use formatting that is consistent across the
    code (e.g., always use spaces versus tabs, a newline before `{` or not). Both
    special parsing and formatting will improve the performance of a code LLM. The
    robust method, where we add special tokens, will yield better performance over
    formatting but has the added cost of writing and maintaining a custom parser for
    code that adds those special tokens. The problem of altering the tokenizer will
    be more critical in the next section when we discuss using LLMs for mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 LLMs for formal mathematics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs can also perform mathematical tasks that are usually quite challenging
    for humans to do successfully. These tasks are more than just performing operations
    like addition and subtraction to calculate numbers; they include formal and symbolic
    mathematics. We give an example of the kinds of formal math we are talking about
    in figure [6.7](#fig__minerva_example). You can ask these LLMs to calculate derivatives,
    limits, and integrals and write proofs. They can produce shockingly reasonable
    results.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs for code are practical because we can use parsers and compilers to partially
    validate their outputs. Proper tokenization is paramount for making a helpful
    LLM for mathematics. Using LLMs for math is still a particularly active area of
    research [6], so the best ways to get an LLM to perform math are not yet known.
    However, researchers have identified some problems that cluster around the tokenization
    stage of building and running an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F07_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 A symbolic math problem that the Minerva LLM can solve correctly.
    While this example mixes natural language with mathematical content, the standard
    tokenization used by many LLMs would not allow this kind of mathematical output
    and can cause some surprising problems. (Image Creative Commons licensed from
    [5])
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note In chapter 5, we mentioned that fine-tuning can be applied multiple times,
    and math LLMs are a great example of this. Researchers often create math LLMs
    by fine-tuning code LLMs, which are created by fine-tuning general-purpose text
    LLMs. Between SFT and RLHF at each stage, as many as three to six rounds of fine-tuning
    are applied to the original downstream LLM for math LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Sanitized input
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Math LLMs often suffer from input preparation that may work well for natural
    language text but degrade representations of mathematical concepts. In text, formatted
    mathematics representations often involve symbols like `{}<>;^`. Special symbols
    like these are commonly removed from training data when working with regular text.
    Preserving this information requires rewriting input parsers for tokenization
    to ensure you do not remove the data you are trying to get your model to learn
    from.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple representations for equivalent mathematical equations further complicate
    training LLMs to understand math in a similar way that multiple formatting may
    cause problems when processing programming languages. Several formats like TeX,
    asciimath, and MathML allow mathematical notation to be expressed using plain
    text but provide instructions for a typesetter to render equations correctly.
    These formats offer many different ways to represent the same equation. We show
    an example of this problem in figure [6.8](#fig__mathRepresentation). There are
    problems with the method of typesetting the math (i.e., how to draw the equation
    by picking TeX versus MathML) and the representation of the math (i.e., two mathematically
    equivalent ways of expressing the same thing).
  prefs: []
  type: TYPE_NORMAL
- en: 'These are both forms of a problem that has come up a few times in our discussion
    of LLMs: different ways to represent the same thing. In the case of mathematics,
    the current preference is to keep math formatted using TeX and very similar but
    less-frequent alternatives like asciimath and to discard verbose content like
    MathML. We base this motivation on three factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F08_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 A mathematical equation in the top-left demonstrates two different
    representation problems that occur with math. The nicely formatted math requires
    a typesetting language. TeX and MathML are two different typesetting languages
    that have vastly different text and, thus, tokenization. Separate from the typesetting
    language, there are many ways to represent the same mathematical statement.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: TeX-based formatted math is the most common and available form of math thanks
    to publicly available sources like arXiv, which consistently uses TeX formatting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping all TeX-like representations mitigates the challenge of learning multiple
    formats and, thus, very different token sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The more verbose MathML uses a larger variety of tokens; thus, more computing
    resources are required to store the data associated with each unique token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing TeX as a single preferred representation for math in LLMs doesn’t solve
    the fact that there are multiple ways to write equivalent equations. Determining
    which equations are the same is so difficult that researchers have proven that
    no single algorithm can determine the equivalence of two mathematical expressions.
    (We are being a little loose with our words here, given that this section is on
    *formal* mathematics, so we will point you to the source [7].) So far, the best
    answer for LLMs appears to be “let the model try to figure that out,” which has
    been reasonably successful thus far. But we wouldn’t be surprised if the developers
    of future math LLMs invest heavily in improving preprocessing by creating more
    consistent canonical representations for mathematical equations that reduce the
    variety of possible expressions for equivalent expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Helping LLMs understand numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For most people, numbers are the more accessible part of math. You can put them
    in a calculator and get the result. Although it may be tedious, you can perform
    calculations by hand if you do not have a calculator. One follows a fixed set
    of rules to get the result. Somewhat surprisingly, LLMs have a lot of trouble
    doing that sort of rote calculation, but developers have worked to improve tokenizers’
    ability to work better with numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The first problem is that the standard byte-pair encoding (BPE) algorithm produces
    tokenizers that create inconsistent tokens for numbers. For example, “1812” will
    likely be tokenized as a single token because there are references to the War
    of 1812 in thousands of documents; tokenizers will possibly break up 1811 and
    1813 into smaller numbers. To further explore why this happens, consider the initial
    string `3252+3253` and how GPT-3 and GPT-4 tokenize this string. GPT-4 will do
    a better job because it seems to tokenize numbers by starting with the first three
    digits every time, resulting in a three-digit number followed by a single-digit
    number. GPT-3 appears inconsistent because it changes the order in which it tokenizes
    numbers, as shown in figure [6.9](#fig__additionTokenProblem).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F09_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 LLMs cannot learn to do basic arithmetic unless they tokenize digits
    consistently. In this figure, underlines denote different tokens. The tokenized
    digits might represent the tens, hundreds, or thousands place for any given number.
    GPT-3 (left) is inconsistent in how numbers get tokenized, making adding two numbers
    needlessly complex. GPT-4 is better (but not perfect) at tokenizing numbers in
    a consistent way.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now a significant problem has occurred. The “3” token for GPT-3 occurs two times
    in two different contexts, once in the thousands place (*three-thousand* two hundred
    ...) and once in the tens place (three-thousand two hundred and fifty *three*).
    For GPT-3 to correctly add these numbers, the tokenizer must properly capture
    four different digit locations. In contrast, GPT-4 uses the order for digit representations
    for each number, making it easier to get the correct result.
  prefs: []
  type: TYPE_NORMAL
- en: People are still experimenting with different ways of changing the tokenizer
    to improve LLMs’ ability to work with numbers. If we are going to tokenize digits
    into subcomponents, the current best approach is to separate each number, like
    3252, into individual digits, like “3, 2, 5, 2” [8]. However, other alternatives
    also exist.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting approach for representing numbers is called *xVal* [9],
    with the idea of replacing every number with the same token that represents “a
    number.” We could call this special token `NUM`, which will get mapped to a vector
    of numbers by the embedding layer we learned about in chapter [3](../Text/chapter-3.html).
  prefs: []
  type: TYPE_NORMAL
- en: The clever trick is to include a multiplier with each token, a second number
    multiplied against the embedded vector value. By default, the LLM uses a multiplier
    of 1 for every token. Multiplying anything by 1 does nothing. But for any `NUM`
    token we encounter, it will instead be multiplied by the original number from
    the text! This way, we can represent every possible number that might appear,
    even fractional values, including those that did not appear in the training data.
    Numbers captured in this manner are related in a simple and intuitive way. We
    show this in more detail in figure [6.10](#fig__xVal).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F10_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 xVal uses a trick to help reduce the number of tokens and make them
    less ambiguous. By modifying how the LLM converts numbers to vectors, a single
    vector represents each number, such as the number 1\. By always using the 1 token
    and multiplying it by the number observed, we avoid many edge cases in number
    token representation, such as numbers that never appeared in the training data.
    This conversion method also makes fractional numbers like 3.14 easier to support.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Both the consistent digits and the xVal strategy share one important realization.
    We know how to represent math and simple algorithms like grade-school addition
    and multiplication. If we design the LLM to tokenize mathematics in a way that
    is more consistent with how we, as humans, do mathematical tasks, our LLMs get
    better and more consistent mathematical capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Math LLMs also use tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The astute reader may have noticed that most of the tokenization problems related
    to math involve handling digits and not symbolic math. LLMs cannot do essential
    addition or subtraction without changing the tokenizer and keeping typically “bad”
    symbols like `{}<>;^`. Enabling computation by changing the way the tokenizerhandles
    numbers may seem like a minor problem. Still, it is a significant factor for good
    symbolic performance and often insufficient for handling other forms of symbolic
    math. Obtaining the best possible performance on symbolic math relies on external
    tools and playing clever tricks with LLM output.
  prefs: []
  type: TYPE_NORMAL
- en: If you ever had the TI-89 calculator that could solve derivatives for you, you
    know that computers can automate calculations without LLMs. Functionally, computer
    algebra systems (CAS) can provide this functionality. A CAS implements algorithms
    to perform some (but not all) mathematical steps. Calculating derivatives is one
    of them, so having an LLM use a CAS, like Sympy, helps ensure the LLM always performs
    specific steps correctly. However, the ability to integrate a CAS like Sympy into
    an LLM does not guarantee the entire sequence of steps will be performed correctly.
  prefs: []
  type: TYPE_NORMAL
- en: To validate correctness, math LLMs have begun to use a programming language
    called *Lean*. In Lean, the program is a kind of mathematical proof, and the program
    will not compile if there is an error in the proof. It effectively makes incorrect
    proof steps one type of syntax error that can then be detected. Once detected,
    as we have shown in other examples, the output can be regenerated by the LLM until
    the proof, output as a Lean program, compiles successfully, just like we show
    in section [6.1.2](#sec__code_validation).
  prefs: []
  type: TYPE_NORMAL
- en: Using Lean can guarantee that a returned proof from an LLM is 100% correct,
    but there is no guarantee that the LLM can find the proof. Notably, there may
    also be cases where the LLM might be able to solve the problem correctly but might
    not be able to express the solved problem using Lean. We diagram the logic behind
    this problem in figure [6.11](#fig__leanHard), and it boils down to the fact that
    the effectiveness of tool use in LLMs depends on the variety of examples of the
    tool’s use in training data. Since Lean is relatively new and niche, there are
    few examples of fine-tuning an LLM to use Lean effectively. People like you and
    me will need to generate those examples to produce suitable training data to teach
    an LLM how to use Lean.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F11_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 Given some mathematical goal, getting an LLM to use Lean (right
    path) might not result in a verifiably correct proof because it may not be effective
    at using Lean as a tool. Having the LLM produce a normal proof (left path) may
    yield a correct proof, but not a way for us to verify that it is (in)correct.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: So what can you do if the LLM cannot provide verifiable proof that its math
    is correct? A trick used today is to run the LLM multiple times. Because the next
    token is selected randomly, you can potentially get a different result with a
    different answer each time you run the LLM. Whichever answer appears most frequently
    is most likely correct. This process does not guarantee the proof is correct,
    but it helps.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Transformers and computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of translating code and math to tokens is fairly intuitive. Code
    is fundamentally text used to tell computers what to do in a highly pedantic way.
    Math is difficult to convert into tokens, but we have discussed how it is possible.
    Computer vision is a different story, where the data involved is images or videos
    represented using pixels. The idea of tokens for images seems confusing. How on
    earth could we possibly convert an image into tokens? Images typically contain
    lots of detail, and you cannot just combine a bunch of small images into one coherent
    image like you do when you string words together to form a sentence. Nevertheless,
    we can apply transformers to images if we think about tokenization as a process
    to convert any input into a sequence of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Note There was an approach to representing images as a combination of tiny images
    called *code books*. Code books can be useful, but not the same in the spirit
    of our discussion. Consider this a keyword nugget to explore if you want to learn
    about some older computer vision techniques.
  prefs: []
  type: TYPE_NORMAL
- en: While high-quality image recognition algorithms and image generators existed
    for many years before transformers, transformers have rapidly become one of the
    premier ways to work with images in machine learning. Both vision transformer
    (ViT) architectures that strictly use transformers, as well as mixed architecture
    models such as VQGAN and U-Net transformer that mix transformers with other types
    of data structures, have seen great success in both interpreting image-based data
    and producing amazing computer-generated images from text descriptions. It may
    seem counterintuitive that transformers perform so well in images because images
    do not look like discrete sequences of symbols like natural language, code, or
    amino acid sequences do. Still, transformers fulfill a critical role in computer
    vision by bringing global cohesion to models.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Converting images to patches and back
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Conceptually, we will replace the tokenizer and embedding process with a new
    process that outputs a sequence of vectors similar to the embedding layers we
    discussed in section [3.1.1](../Text/chapter-3.html#sec__transformer_layers).
    The prevailing approach to creating a sequence representing an image is to divide
    the image into a set of *patches*. As a result, we will replace our tokenizer
    with a *patch extractor* that returns a sequence of vectors. The output of an
    LLM uses an unembedding layer to convert vectors back into tokens. Since we have
    no tokens, we need a *patch combiner* to take the outputs of a transformer and
    merge them into one coherent image. We show this process in figure [6.12](#fig__tokensToPatches).
    Please pay special attention to the fact that the central portion of the diagram
    remains the same as it was for text-based LLMs. We reuse the same transformer
    layers and learning algorithm (gradient descent) between text and images.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F12_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 On the left, this simplified diagram shows how text input is tokenized
    and embedded before going to the transformer. An unembedding layer then converts
    the transformer output into the desired text representation. The input and output
    will be images when performing a computer vision task. The transformer stays the
    same, but then we modify the method for breaking the image into a sequence of
    vectors to perform patch extraction instead of tokenization. The LLM produces
    image output using a patch combiner, analogous to the unembedding layer for text
    LLMs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since everything except the input vector sequence generation and output steps
    remains the same, we can focus on how the conversion of images to and from vectors
    works. It will be helpful to focus on the input side first.
  prefs: []
  type: TYPE_NORMAL
- en: As the name *patch* implies, the patch extractor breaks up each image into asequence
    of smaller images. It is common to pick a fixed size for the patch, likea square
    of ![equation image](../Images/eq-chapter-6-82-1.png) pixels. We want a fixed
    size so that it is easy to feed into a neural network, which always processes
    data of a fixed size, and small so that they represent just a piece of the entire
    image. Breaking an image into patches is similar to breaking text into a collection
    of tokens. Each individual token isn’t informative, but when combined with other
    tokens, it makes a coherent sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Once an image is broken into patches, each pixel in that patch is converted
    to three numbers representing the amount of red, green, and blue (RGB) present
    in each pixel. An initial vector is created by combining each pixel’s RGB values
    into a single long vector. So for our square of ![equation image](../Images/eq-chapter-6-83-1.png)
    pixels with three color values for each pixel, we will have a vector that is 768
    values in length (16 height, 16 width, and an RGB value for each pixel). Then,
    a small neural network that might have only one or two layers processes each vector
    separately to make the final outputs. This neural network implements a very light
    feature-extraction process that does not require significant memory or computation
    resources. This design is common in computer vision because the first layer usually
    learns simple patterns like “dark inside, light outside” and does not need a transformer
    layer’s greater expense or power to learn the basic features of an image patch.
    This whole process is summarized in figure [6.13](#fig__patchExtractor).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F13_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 Extracting patches is a straightforward process. The patch extractor
    breaks up an image into square tiles called patches. Images consist of pixel values
    that are already numbers, so we convert each patch into a vector of numbers. Then,
    we use a small neural network as a preprocessor before passing the vectors to
    the full transformer-based neural network.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are many possible ways to design the small neural network used in the
    patch extractor, but all generally work equally well. One option is to use what
    is called a *convolutional neural network* (CNN), which is a type of neural network
    that understands that pixels near each other are related to each other. Others
    have used just the same kind of linear layer that is a component of a transformer
    layer. In this case, the overall model that includes the small neural network
    and a series of transformers is often called a *vision transformer*.
  prefs: []
  type: TYPE_NORMAL
- en: The design of the small network is a minor detail but worth mentioning because
    its existence is relevant to the patch combiner that produces the final output.
    It does not matter whether you pick a CNN or a linear layer for the architecture
    of the small neural network, but it is essential to ensure the output’s shape
    matches the input’s shape. For example, if you have ![equation image](../Images/eq-chapter-6-86-1.png)
    patches, you can use the small network to force the output to have ![equation
    image](../Images/eq-chapter-6-86-2.png) values, regardless of the size of the
    transformer layer itself. To produce image output, you reverse the patch extraction
    process to convert the vectors into patches and then combine the patches into
    an image, as shown in figure [6.14](#fig__patchCombiner).
  prefs: []
  type: TYPE_NORMAL
- en: We have thus successfully replaced the input tokenization and the output embedding
    with new image-centric layers. In many ways, this is much nicer than tokenization.
    There is no need to build/keep track of a vocabulary, no sampling process, etc.
    This is a crucial insight into the general applicability of transformers as the
    general-purpose core of an LLM. If you can find a lot of data and a reasonable
    method of converting that data into a sequence of vectors, you can use transformers
    to solve certain classes of input and output problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F14_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 Compared to figure [6.13](#fig__patchExtractor), the arrows here
    go in the opposite direction. The purpose is to emphasize that the patch combiner
    and extractor do the same thing but operate in different directions. The neural
    network is more important in this stage as a way to force the transformer’s output
    to have the same shape as the original patches because we can control the output
    size of any neural network.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.3.2 Multimodal models using images and text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ability to change the input and output of an LLM to arrive at a vision transformer
    means that we can take an image as input and produce an image as output. It demonstrates
    how a transformer can produce input of different modalities, but we have only
    discussed cases where the input and output are the same modality. We either have
    text as input and text as output or images as input and images as output. However,
    deep learning is flexible! There is nothing that forces us to use the same modality
    as both input and output or even restrict input and output to be a single modality.
    You can combine text as input with image as output, images as input and text as
    output, text and images as input and audio as output, or any other data modality
    combinations you might think of. Figure [6.15](#fig__mixAndMatch) shows how image
    and text give us four total ways we might combine them to handle different kinds
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: By creating a model that uses images as input and text as output, we create
    an *image captioning model*. We can train this model to generate text describing
    the input image’s content. Models such as these help make images more discoverable
    and aid visually impaired users.
  prefs: []
  type: TYPE_NORMAL
- en: 'By creating a model that uses text as the input and an image as the output,
    we create an *image generation model*. You can describe a desired image using
    words, and the model can create a reasonable image based on your input. Famous
    products like MidJourney are models of this flavor. Though their implementation
    involves more than just a vision transformer, the high-level idea is the same:
    by pairing a text-based input with image-based output and a lot of data, we can
    create new multimodal capabilities that span different data types.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F15_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 Four combinations showing different types of model input and output.
    The example at the furthest right represents a normal text-based LLM we have already
    learned about. To the left, we show possibilities like an image-generating model
    that takes text as input (“Draw me a picture of a stop sign in a flood zone”)
    or an image captioning model that generates text that describes an image input
    (“This picture shows a stop sign surrounded by murky water”).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.3.3 Applicability of prior lessons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Other lessons learned throughout this book remain relevant to these vision transformer
    and multimodal models. Ultimately, they learn to do what they are trained for,
    and when you try to bend them in ways beyond what is found in the training data,
    you may get an unusual result. As an example, we might tell an image generation
    model “Draw anything but an adorable cat,” and you will probably end up with a
    cat as shown in figure [6.16](#fig__adorableCat)
  prefs: []
  type: TYPE_NORMAL
- en: These models are (currently) trained with pairs of images and pieces of text
    describing the image. Thus, they learn a strong correlation to produce visualizations
    of anything in the input sentence. For example, the model wants to produce a cat
    since the word *cat* is in the input sentence. More sophisticated abstract drawing
    requests like “Draw anything but” do not appear in such datasets, and so the model
    is not trained to handle such a request.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, as LLMs like ChatGPT have developed prompting as a strategy for devising
    inputs that produce desired outputs, prompting has also been developed for image
    captioning models. It is not uncommon to include unusual information like “Unreal3D,”
    the name of software used to generate 3D imagery for computer games to produce
    output with a particular style and quality. Words like *high resolution* and even
    the names of artists, alive and dead, are used to try to influence the models
    into producing particular styles.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F16_Boozallen.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 This was generated with an old version of Stable Diffusion, a popular
    image generation model. Despite telling the model “Do not draw a cat,” the model
    was trained to generate content. The request is outside what the model was incentivized
    to learn, so it cannot handle it. This is similar to the problems with LLMs regurgitating
    close-but-wrong output because the model saw similar data during training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs benefit when they can use external tools. For example, a code LLM can use
    syntax checkers and compilers to detect erroneous code generation. When the LLM
    finds an error, the output is regenerated, minimizing the risk of giving the user
    unhelpful or broken code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizers must be modified to support math by keeping unusual symbols used
    to express formatted math and changing digit representations. We can improve math
    LLMs further by giving them tools like computer algebra systems to detect and
    avoid errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers can be applied to images by breaking up an image into patches,
    where each patch becomes a vector and makes a sequence of inputs for the transformer
    to process. Patches are conceptually similar to tokens for text LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers can use different data modalities for input and output, allowing
    the creation of multimodal models like those used in image captioning and image
    generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
