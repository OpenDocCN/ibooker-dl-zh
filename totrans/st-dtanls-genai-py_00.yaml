- en: 1 Introduction to the use of generative AI in data analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing key limitations of generative AI models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of generative AI in data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started using LLMs to support data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As the dust over the generative AI hype begins to settle and the notes of disappointment
    mix in with the chorus of praises, it may be a good time to ask yourself a question:
    “If LLMs aren’t the silver bullet to all world problems, what are they really
    good for?” Our experience using these amazing tools to improve various processes
    gave us the answer. They are really good, and we mean *really* *good* in supporting
    improvements for different processes. Throughout this book, we will guide you
    through our methods for utilizing the enormous potential hidden in the matrices
    of generative AI to improve your analytics skills without falling victim to the
    risks inherent in this technology.'
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood  To excel in these goals, you will preferably have in the back
    of your mind what drives the responses you get to your prompts. However, due to
    the architecture-agnostic nature of this book and the rapidly changing technology
    landscape, we have consciously avoided the technological nitty-gritty, focusing
    instead on process implementation. We encourage you, though, to get a good overview
    of what’s what. You can learn it from several Manning books, such as *The Complete
    Obsolete Guide to Generative AI* by David Clinton or *Introduction to Generative
    AI* by Numa Dhamani and Maggie Engler. For the technical details of GPT models,
    see *How GPT Works* by Drew Farris, Edward Raff, and Stella Biderman.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about three important aspects of working with
    generative AI. As we strongly believe that first things should indeed be first,
    we’ll start by presenting the inherent limitations of generative AI. We already
    mentioned the misalignment between the expectations and results of generative
    AI applications. A good understanding of the unavoidable limitations is critical
    to avoid disappointments at your work. The second aspect is related to embedding
    generative AI into the data analytics process. This part of the chapter will help
    you develop your first intuition about when and how to use generative AI when
    trying to solve an analytical problem. We will also manage expectations when it
    comes to automating processes involving generative AI. The last part of the chapter
    will provide you with knowledge about methods of accessing generative AIs. In
    the lion’s share of cases, browser-based access to chat would be sufficient, but
    history teaches us that this may not be an advisable method when working with
    sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: The overall goal of this chapter is not to give you encyclopedic knowledge of
    this technology, but to ensure you have a deep enough understanding to demystify
    generative AI and allow for a more critical interpretation of its abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Inherent limitations of generative AI models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Middle Ages, map edges had the inscription *Hic sunt dracones* (Latin
    for “Here be dragons”). You also may find monsters depicted in areas of uncharted
    waters. Later, the dragons, sirens, and krakens were replaced with depictions
    of reefs, shoals, and ice fields. We would like you to consider our warnings as
    the latter, rather than the former. For any endeavor, knowing the dangers to be
    wary of is at least as important as knowing what benefits to hope for.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list outlines the inherent limitations of any generative AI system.
    Some of them may be reduced or even removed in the future, but reading about a
    potentially obsolete limitation will cost you less than being unaware of even
    one that remains valid. So, here are the treacherous waters we should be aware
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative AIs will always provide an answer (even if it’s a wrong one)*—Like
    a child who finished their “why?” phase, or an overpromoted manager with heavy
    impostor syndrome, generative AIs, when asked a question, know everything and,
    as their first reaction, are unable to admit the limits of their knowledge. One
    of the main reasons you really want to read this book is because generative AIs
    can be convinced otherwise. In chapter 8, we’ll also discuss the model’s sensitivity
    to input phrasing. Slightly rephrasing a question may result in different answers
    with varying degrees of quality and relevance. It’s worth noting that, since prompts
    are usually supplied in natural language, this sensitivity is slightly different
    than for search engines. The latter only react to clusters of keywords, whereas
    generative AI may provide a different response based not just on the keywords,
    but also on the grammatical structure of the prompt, its perceived emotional tone
    of writing, and the context created by recent exchanges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Some of the answers might be entirely made up*—There are instances where generative
    AI provides an answer that appears plausible but is not based on facts or directly
    linked to the training material. This is because the model sometimes fills in
    gaps in its knowledge by generating content that aligns with the patterns observed
    in the training data, even if the information is not accurate or complete. We
    will cover this problem in chapter 7, when we discuss the phenomenon of AI hallucinations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inherent sycophancy*—The larger the model at the base of generative AI, the
    more likely it is to exercise agreeability over reliability and accuracy. If confronted
    or questioned about the provided answer, it’s likely to apologize and present
    the point of view contradicting its previous statement, even if it was correct
    the first time—truth be damned! Generative models can even make up numbers and
    falsify references to support the user’s perspective!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inaccurate or outdated information*—Each generative AI’s knowledge is derived
    from the content corpus it was trained on. The model may provide outdated or incorrect
    information depending on the knowledge cutoff. This will be visible in examples
    in this book, where the model gives answers using API calls or programming language
    structures from obsolete versions. This is not as severe a limitation as it might
    initially seem. First, the majority of the concepts covered don’t evolve that
    quickly, and most people will have a lot of ground to cover in the basics before
    they reach the need to tap the latest developments. Second, many generative AIs
    have access to the internet. However, mixing the time-constrained body of knowledge
    used to train the model with continuous updates may lead to inaccurate results.
    It’s also worth remembering that some generative AIs only check the internet for
    the latest information when directly prompted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Input and output limits*—When using generative AIs, you should be aware that
    the amount of text they can process (the amount of input they can read as a whole
    and, on this basis, generate the output) is limited, although it can vary greatly
    between different models and implementations. The unit of text processed by generative
    AI is called a *token*, and it can be a word, part of a word, or a punctuation
    mark, depending on the tokenization method employed. Tokenization algorithms translate
    text into tokens with an average of 4 tokens per 3 words, or 0.75 words per token
    (this value should be relatively stable). At the time of writing this book, models
    have context windows ranging from several thousand up to millions of tokens, and
    the race to truly limitless context is full-on. For now, however, the available
    tools offer a limited number of input (prompt) and output (response) tokens, and
    you should remember that the context window covers both. You will get no warning
    if some data falls out of the window (no pun intended) and gets forgotten. Such
    truncation will usually manifest itself by the model giving responses inconsistent
    with previous exchanges, showing it has forgotten previous prompts or responses.
    In section 1.3, we present methods for estimating the number of tokens used. As
    a way of dealing with this limitation, you may frequently summarize the conversation
    and its key findings to ensure they are not left out of the context window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Verbosity*—When you try some prompts, it will quickly become apparent that
    generative AIs generate overly verbose responses or overuse certain phrases (e.g.,
    they tend to “unwaveringly delve into vast landscapes of the rich tapestry of
    intricacies of…” anything they encounter). This verbosity can be attributed to
    biases or patterns in the training data, where longer responses, or responses
    of a particular structure, might be more common.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Biased or inappropriate content*—Despite efforts to reduce harmful and biased
    content, generative AIs, especially those fine-tuned on unknown data, may still
    generate responses that exhibit biases or produce content that could be considered
    inappropriate. This can result from some biases still being present in the training
    data, biases that are hidden or purposefully included in the prompts, or a multitude
    of other overlapping factors. However, the developers of most generative AIs have
    gone to great lengths to balance the model’s responses. An example can be found
    in the GPT-4 System Card document (https://cdn.openai.com/papers/gpt-4-system-card.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He said, she said  You can help generative AI provide correct, or at least useful,
    answers by providing underlying LLMs with the ability to search additional data
    sources and require linking answers to sources. You can learn more about that
    form from *Generative AI in Action* by Amit Bahree or *AI-Powered Search* by Trey
    Grainger, Doug Turnbull, and Max Irwin, both available from Manning Publications.
  prefs: []
  type: TYPE_NORMAL
- en: A word to the wise  The prompt/response size limit and the verbosity can often
    lead to incomplete or cut-off responses. When designing a conversation with a
    generative AI, one option is to ensure that the combined length of the prompt
    and expected response doesn’t exceed the token limit.
  prefs: []
  type: TYPE_NORMAL
- en: Awareness of these limitations is crucial when you are interacting with generative
    AIs or incorporating them into various applications. Continued research and development
    aim to address these limitations and improve the performance and safety of generative
    AIs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 The role of generative AIs in data analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On groups and forums focused on generative AIs, there are dozens of questions
    to the effect of “Where can I find a GenAI-based tool that does [very specific
    task description here]?” Even if a requested tool does not exist yet, it probably
    will, and soon. And it’s all good. Data warehouses, lakes, lakehouses, meshes,
    fabrics, and so on replace Excel files, data in emails, and napkins (if not for
    all intended purposes). Dashboards and self-serve business intelligence (BI) platforms
    replace manually created reports and PowerPoint presentations (OK, questions about
    using generative AI to create, modify, or improve PowerPoint slides are the most
    common). However, beware of silver bullets. Only 0.5% of data collected in data
    warehouses, lakes, and so on is ever analyzed, while the remaining 99.5% generates
    costs and big-data hangover to companies that over-eagerly started to collect
    the data without a data utilization plan. BI platforms, in turn, are filled to
    the brim with rogue analytics (for example, data slicing and dicing that serves
    no purpose other than the justification of poor business decisions).
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of generative AI use in data analytics will depend on your,
    the data analyst’s, ability to harness the possibilities and overcome the limitations
    of this new tool. Generative AIs, like any tool or technology, cannot be expected
    to do all the work. Let’s look closely at what we are dealing with and at generative
    AI’s differences from and similarities to other elements of the data analytics
    flow, namely the analytical process and software.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we have leaned toward the doom-and-gloom side of things, giving you
    a lot of warnings about generative AI’s limitations and discouraging you from
    jumping onto every tool labeled as “GenAI-powered” (we have an internal betting
    pool regarding when the first toothbrush labeled as such will hit the market).
    We did this on purpose as we noticed that overinflated expectations are the main
    blocker to the efficient use of this amazing tool. Now let’s get out of this shadow
    of doubt and step into the light of the bright generative AI-supported future
    of data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Generative AI in the data analytics flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Years and years of working with data has convinced us that its value does not
    come from the complexity of the utilized technologies. We’ve seen millions of
    dollars saved with a simple breakdown of costs done by process rather than by
    organizational unit. We’ve seen millions of dollars lost because an overcomplicated
    market analysis involving dozens of tools and teams poorly reflected actual customer
    sentiments. *Data analysis is not about transforming raw data into charts. It’s
    about supporting business decisions using conclusions from relevant business data.*
    Your success in this endeavor will depend on a couple of aspects, of which the
    available tooling is just one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different types of data and different business questions require different
    analytic pipelines. If you work in a retail company, you most likely seek insights
    into customer behavior. Your pipeline may begin with the collection and cleaning
    of data from multiple sources: transaction records, customer feedback, and website
    interactions. Once the data is scrubbed and standardized, you will process it
    through algorithms performing customer segmentation, product affinity analysis,
    and sales forecasting. Working with a healthcare provider, your input data would
    include patient electronic health records, medical imaging, and sensor data from
    wearable devices. Your processing would employ algorithms for disease diagnosis,
    treatment optimization, and patient outcome prediction. If you find yourself in
    a manufacturing firm, you’ll be integrating data from IoT sensors on the factory
    floor, quality control checks, and supply chain logistics, with the analytics
    focused on anomaly detection, predictive maintenance, and supply/demand forecasting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Irrespective of what you’re analyzing and for whom, the essence of the process
    remains the same: collect and clean input data, process it using more or less
    advanced algorithms, and finally, present the results to the desired audience.'
  prefs: []
  type: TYPE_NORMAL
- en: The details differ greatly depending on the business area, data sources, analytical
    methods applied, and expected output format. Each of these topics warrants a book
    (or six) on how to most effectively perform each of these steps, taking into account
    both cost and time, using this or that technology stack.
  prefs: []
  type: TYPE_NORMAL
- en: This book does not aim to answer all possible questions about all the possible
    scenarios you may encounter in your work as a data analyst. We offer you something
    much, much better. We propose a structured method to effectively use generative
    AIs’ unbelievable knowledge repository (from Wikipedia to scientific papers, to
    books and literature, to dialogue data, to the Pile ([https://pile.eleuther.ai/](https://pile.eleuther.ai/)),
    and so on) to prepare an analytical pipeline tailor-made to solve exactly your
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: The missing link  The scale of generative AIs’ abilities is only starting to
    be explored. However, it’s already clear that they can be taught to respond consistently
    and relevantly on a wide range of topics. They have the ability to drill down
    into details, summarize, explain, and associate related concepts to an extraordinary
    degree. These abilities can be used to effectively unblock your own thinking and
    get you out of your rut. You no longer have to trawl through dozens of random
    articles trying to find inspiration or pointers. Just ask a question. Even if
    the answer is imperfect, it may point you to concepts you haven’t thought of before.
    Use this to expand your horizons.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preface, we identified a question you should ask yourself every time
    you encounter a new analytical problem: *where do I star**t*? Searching for the
    right input data might not be the worst possible choice, especially if it’s accompanied
    by an analysis of what input data is actually relevant. Let’s assume you work
    in a healthcare unit and get a question like “What are the average patient waiting
    times on Tuesdays?”, or you work in retail and are asked to analyze, “How do our
    customers use loyalty cards?” Do not be fooled by the simplicity of the former
    question. They can, in fact, both be tricky.'
  prefs: []
  type: TYPE_NORMAL
- en: The flow presented in figure 1.1 will help guide you through the crucial steps
    in going from the question to decision-enabling conclusions, focusing on getting
    the most added value from both the human and generative AI along the way, while
    avoiding the common pitfalls. It can be applied to any analytical task and technology
    stack you may have. All the examples you’ll encounter in this book will follow
    this general structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F01_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 Recommended generative AI–supported data analysis flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You always need to start with a problem statement. Let’s look at our healthcare
    example. We were asked a question about waiting times on Tuesdays; however, issues
    around waiting times on a specific day are more likely to be the symptom of a
    deeper problem. The question you get asked will not often translate directly into
    an effective problem statement. We should ask ourselves, “What real problem are
    we looking at here?” While it’s the job of the final decision maker to define
    the scope of the requested analysis, the guided questions can get you into a much
    better starting position for your analysis and, ultimately, provide more value
    from the analyzed data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the fun part. Even if the request comes from an area of operations you
    don’t have experience with, generative AI can help you put the received request
    into a business context without bothering your stakeholders with unnecessary inquiries!
    Let’s try a couple of generative AI on our Tuesdays-specific question.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I work in a healthcare unit. I’ve been asked
    to answer a question: “What is the average waiting time for patients on Tuesdays?”
    What do you think could be some actual reasons behind such a question?'
  prefs: []
  type: TYPE_NORMAL
- en: The answers are too long to include here, but both OpenAI’s ChatGPTs (3.5 and
    4), Google Gemini and Gemini Pro, and Meta’s Llama 2 (13B) provided lists of possible
    project types where such an analysis could be of importance. The answers generally
    involve planning and budgeting (including resource allocation and staffing optimization),
    patient experience and quality of care, operational efficiency, and staff training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your knowledge of current projects in your environment, you may
    come back with more or less specific questions, which shouldn’t be considered
    a waste of time. For example, if you ask a stakeholder asking for an analysis
    a follow-up question like, “Is it connected to our latest focus on increasing
    patient satisfaction?”, you could get an answer like, “Oh yeah, we’ve often heard
    complaints about Tuesdays, and we want to try to do something about it.” Once
    you understand that patient complaints and waiting times are the root issues behind
    the original question about waiting times on Tuesdays, your final problem statement
    may look something like this: “What is the distribution of waiting times and its
    correlation with patient satisfaction?” This question balances specificity with
    scope, aiming to uncover actionable insights that can directly influence patient
    care protocols and satisfaction.'
  prefs: []
  type: TYPE_NORMAL
- en: The level of detail you disclose about your project in conversations with your
    generative AI advisor should be tailored to the confidentiality requirements of
    your analysis and the specifics of your generative AI setup. It’s advisable to
    share more freely within a locally managed software environment than on public
    platforms. This topic, including the associated risks of employing generative
    AI, is explored further in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: You can then try querying generative AI about the best ways to answer the final
    question.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**And how could I approach analysis leading
    to answering the question: “What is the distribution of waiting times and its
    correlation with patient satisfaction?”'
  prefs: []
  type: TYPE_NORMAL
- en: Yet again, the answers are quite lengthy and detailed. They offer you seven-
    to nine-step approaches, which, with additional iterative inquiries, should allow
    you to construct a robust pipeline. Chapter 3 provides a practical example of
    using generative AI to develop a detailed analysis design from an extremely vague
    request (unfortunately, likely to be encountered in the real world). Here, our
    aim is to show you that the value of generative AIs extends beyond just answering
    highly specific queries. Their utility isn’t really reliant on the craft of “prompt
    engineering.” Instead, it depends on your readiness to present the full scope
    of your problem and to recognize that (as in the case of any meaningful conversation
    with a well-informed colleague) you’re unlikely to receive a flawless answer on
    the first attempt. Instead, expect to engage in an iterative process, refining
    broad concepts to meet your particular requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Inquiring into details of the analysis or code and discussing the received results
    are common for all the steps of your analytical process on all levels of granularity.
    You should use them as elements of the flow of the whole project and when going
    through detailed substeps, such as when cleaning the data or formatting the final
    charts. Generative AI can help you clarify what you want to achieve at any given
    moment, determine how to effectively get there, and test if what you got is what
    you wanted.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, generative AI will not replace analytical tools but help you
    optimize their use. Let’s have a look at the areas where they shine the brightest.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 The complementarity of language models and other data analytics tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On their own, generative AIs are particularly well-suited for tasks involving
    text data, such as sentiment analysis, text classification, summarization, and
    question-answering. However, their potential extends beyond text-based tasks.
    Multimodal AIs like Google’s Gemini or OpenAI’s GPT-4 allow you to upload different
    types of files, with the latter accepting raw data in formats such as CSV. But
    as we mentioned before, your success as a data analyst will depend on your ability
    to utilize generative AIs in a wider analytical environment. Luckily, that’s precisely
    where you can get excellent support from generative AI. It’s like having an expert
    on a speed dial!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, all generative AIs worth their mettle have deep knowledge of most analytical
    frameworks available on the market. They can help you navigate through a vast
    array of technologies to extract, process, analyze, and visualize data. Suppose
    you have data in a bunch of Excel files and are tasked with creating the dashboard
    in Power BI. Try dropping the following question into the generative AI of your
    choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I have data in a bunch of Excel files and
    am tasked with creating the dashboard in Power BI. What shall I do?'
  prefs: []
  type: TYPE_NORMAL
- en: You will get detailed instructions, including where to click to upload your
    data, basic options for modeling it, and, again, where to click to prepare a dashboard.
    And that’s just the first step of the iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Did your company just move from WordPress plugins to Google Analytics, but Google
    Tag Manager–based event tracking is needed for yesterday?
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I’m tasked with enhancing our website’s
    performance and need to leverage Google Tag Manager (GTM) for tracking various
    user interactions. Additionally, I must provide detailed reports in Google Analytics
    4 (GA4). I’m completely new to the Google environment. Can you please help?'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll get a good list of options, and you will be able to choose what’s right
    for your specific case. Again, no “prompt engineering” is needed. Just a good,
    old cry for help.
  prefs: []
  type: TYPE_NORMAL
- en: Second, if you’re more invested in serious data analytics, language models can
    generate code in various programming languages, such as Python, R, Scala, or even,
    for the more adventurous, PHP, Perl, or even Cobol or Intercal. The best-known
    example of this concept implementation is the GitHub Copilot, with software like
    Bito, Tabnine, Codeium, and FauxPilot (and many more) following in its footsteps.
    This capability allows you to obtain ready-to-use code for data processing, analysis,
    and visualization tasks, saving time and effort. The generated code can vary in
    size and complexity, from short snippets and single functions serving as a starting
    point for customizing and refining analyses to whole algorithm implementations
    and modules, limited only by your imagination and patience to coax the model to
    spit it out. Unlike raw code snippets downloaded off the internet, generative
    AI will provide code suited exactly to your needs, and it has the invaluable ability
    to explain the code, as we’ll see in several examples in this book, and optimize
    it to your specifications.
  prefs: []
  type: TYPE_NORMAL
- en: This ability to generate and explain code will be the most helpful feature for
    us throughout the book, but it also comes with the biggest warning, which we’ll
    repeat in many places and cover in depth in chapter 7\. Specifically, never *trust*
    the model to spit out entirely correct answers or perfectly working code on the
    first try. The higher the importance or risk of your project, the more scrupulously
    you should verify any output through review and testing. In subsequent chapters,
    you’ll find examples of model-generated code that doesn’t work as expected or
    that has incorrect explanations attached to it. Caveat emptor!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once the analysis is performed, language models can help interpret
    the results by generating natural language summaries and explanations. This feature
    may help you understand the intricacies of complex analytical results and communicate
    your findings to a broader audience.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an old story of a math professor complaining to his colleague about
    students: *I explained it to them three times, I finally understood it myself,
    and they still had questions*. With generative AI, you can allow yourself to be
    such a student, shamelessly pestering your advisor with questions until you’re
    comfortable with the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: If you wish to practice and feel audacious, start by pasting the following prompt
    into multiple generative AIs of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I got a confidence interval of 0.55-0.9
    using the Wilson Score Interval (95% confidence level). How should I precisely
    communicate what it means for the performed test?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Statistics should be a very important part of your toolbox. Generative AI can
    help you avoid situations where stakeholders will evaluate your analyses on this
    scale: small lies, big lies, and statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: After reading the previous two sections, you should now feel excited about adding
    generative AI to your data analytics practice. We share that feeling every time
    we accomplish our tasks in a third or a quarter of the time they used to take.
    But, as always with generative AI, you need to be aware of some limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 Limits of generative AIs’ ability to automate and streamline data analytics
    processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While you can successfully employ generative AIs in all the applications listed
    previously and in the subsequent sections of this book (and more), their effectiveness
    in automating and streamlining data analytics processes has certain limitations.
    You can incorporate them into the data analytics domain, but their limitations
    make them an amazing supplement, not a replacement, for a data analyst.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of quantitative analysis skills
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generative AIs excel at understanding and generating natural language, but they
    lack the inherent ability to perform complex quantitative analysis (aka math).
    ChatGPT has already included an add-on running Python code on the fly (utilizing
    an external environment, not as a native LLM capability), and other generative
    AIs will probably follow suit, but the success rate of the generated analytical
    runs, as you’ll see in the following sections, is not something we’d wage the
    success of our business on. Data analytics processes often require mathematical
    and statistical methods, such as regression analysis, time-series forecasting,
    and clustering techniques. While generative AIs can suggest such methods and often
    offer the relevant code, the code must be thoroughly tested before it is put into
    the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Limited understanding of domain-specific concepts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While generative AIs can generate human-like text based on the context provided,
    their training data may not include highly specialized domain knowledge. Consequently,
    their ability to accurately generate insights or recommendations in the context
    of specific industries or niche subjects may be limited. This problem is enhanced
    by the already mentioned inherent inability of generative AIs to admit their ignorance.
    Knowledge limitations differ between generative AI providers and can be somewhat
    mitigated by providing GenAI with internet access, but you really don’t want to
    base critical business decisions on an enthusiastic hallucination!
  prefs: []
  type: TYPE_NORMAL
- en: In the case of such specific needs, your best option is to provide the model
    with more general prompts and refine the answer based on your specialist knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Inability to interact with databases and APIs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Your work in data analytics will, more often than not, involve working with
    databases, APIs, or other data sources to extract, clean, and process data. Generative
    AIs lack the built-in capability to interact directly with these sources. While
    it is possible to integrate generative AIs with custom-built solutions to bridge
    this gap, doing so can be resource-intensive and challenging to implement effectively.
    As in the previous cases, the model can still be effectively used to guide your
    analysis and provide solutions or even whole swathes of code, which you can execute
    independently of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Unreliable internet access
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some generative AIs, such as Google’s Gemini, are “internet native.” The internet
    connectivity is a natural part of their operations. For others, not so much. Self-hosted
    models, which you can download and run on your machine, such as Llama 2, need
    access to a search engine API and an implementation of so-called multistage reasoning,
    where in the first answer, the LLM decides what it needs to do to get the proper
    answer, and in the following steps, the architecture runs the internet search
    and provides the answer to the LLM, which on that basis can form the final response.
    ChatGPT 4 implements this in its web-based interface.
  prefs: []
  type: TYPE_NORMAL
- en: It seems complicated, and it is. The connectivity of self-hosted LLMs depends
    on external APIs and either a lot of code or fast-changing libraries. ChatGPT,
    in turn, sometimes forgets it can connect to the internet. When it was trained,
    it couldn’t, and this memory still lingers in its network’s deep layers. If your
    analysis depends on access to the latest data or news, you need to choose your
    tools very carefully.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters of this book, we will show you how to apply this general-to-specific
    problem-solving path for the best results.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Getting started with generative AIs for data analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an old Chinese proverb, “In the forest of algorithms, the path to wisdom
    has many branches.” Actually, there isn’t—ChatGPT generated it for us. We have
    tried to convey that, depending on the situation, you have more than one way to
    access your AI advisor. To utilize generative AI’s potential, you need to get
    comfortable with conversing with it, as most of the tools built upon it strip
    its answers of relevant nuance, but you should know your options here.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Web interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this book, we will mainly use ChatGPT ([https://chat.openai.com](https://chat.openai.com))
    and Gemini([https://gemini.google.com](https://gemini.google.com)) as examples
    of generative AI. These are readily accessible (it’s more true than you’d like,
    as this access can be bidirectional—do not paste your confidential data there!),
    and the underlying language models are in constant, rapid development. There is
    also a chance that your company will have a self-hosted generative AI based on
    either of these or some other model, such as the *n*-th incarnation of Llama or
    Mixtral, but hopefully it will be accompanied by a proper web interface.
  prefs: []
  type: TYPE_NORMAL
- en: And the winner is…  *At the time of writing this book*, GPT 4 was by far the
    most useful of all the tested generative AIs for any analytics-related tasks.
    However, this *will* change (just not with GPT 4o). Keep your eyes open and don’t
    be shy when it comes to engaging new generative AIs and testing their usefulness.
    Each model has its own training dataset and an architecture influencing its interpretation
    of your prompts and the resulting answer. Just remember that the technicalities
    behind them are irrelevant from your perspective (unless they are related to cost-effectiveness,
    of course). What interests you, as a data analyst, is the model’s ability to support
    your process.
  prefs: []
  type: TYPE_NORMAL
- en: If you need specific models, you can search for one of half a million hosted
    on the AI-development website, www.huggingface.co. Some of them require downloading,
    but some can be run there. You can register for free, but extensive model use
    will require creating your own “Space” and purchasing processing power.
  prefs: []
  type: TYPE_NORMAL
- en: The use of generative AI via web interface is as simple as writing a query and
    reading the answer. Sometimes, there will also be a button allowing you to upload
    files to be analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 Beware of tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you plan to connect directly to a model, you must understand the critical
    difference between your and GenAIs’ perceptions of text. As explained earlier,
    generative AIs break down input text into manageable units known as *tokens*.
    This foundational step is critical, as it transforms raw text into a structured
    form that an AI model can efficiently process and learn from.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization involves dissecting the input text into a sequence of tokens. This
    process is not a mere splitting by whitespace; it’s more nuanced, incorporating
    an understanding of the language’s syntax and semantics. For instance, the word
    “don’t” may be tokenized into “do” and “n’t” to better capture its meaning and
    structure. Advanced models leverage subword tokenization schemes to balance the
    tradeoff between representing common words as single tokens and decomposing less
    common words into smaller, meaningful components. This approach enables the model
    to handle a vast vocabulary, including neologisms, with a fixed set of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens of Babel  The method of tokenization—how the words are split before processing—is
    specific for each model. This is a critical point, as using tokenization meant
    for one model as input for another may case the latter model to interpret the
    input as a meaningless, or worse, misleading, soup of semi-words misaligned with
    how the model’s training material has been prepared.
  prefs: []
  type: TYPE_NORMAL
- en: As we warned you in section 1.1, when the input surpasses the context window,
    the earliest tokens are truncated, leaving the model with only the most recent
    tokens within its comprehension horizon. This truncation can lead to a loss of
    crucial context or information necessary for generating coherent and relevant
    responses. Have you ever been in a situation where someone overheard just the
    last couple sentences of a lengthy conversation and offered unsolicited advice?
    Such a response rarely adds to the conversation. Exceeding the context puts generative
    AI in a similar position. It’s “deaf” to parts of the conversation exceeding its
    context window. This is particularly dangerous if you work with a large piece
    of code!
  prefs: []
  type: TYPE_NORMAL
- en: Several strategies can be employed to navigate the constraints of limited context
    windows. One common method is chunking the input text into smaller segments that
    fit within the model’s context window, ensuring that each segment contains enough
    context to stand on its own for generation tasks. Another approach involves using
    techniques like sliding windows or iterative refinement, where the model progressively
    processes text, maintaining as much relevant context as possible across segments.
    For more complex interactions involving longer texts or conversations, strategies
    like creating a summary of previous interactions or leveraging external memory
    mechanisms can help maintain coherence.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3 Accessing and using the API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more advanced use cases and seamless integration with your existing data
    analysis tools, you can access most of the popular models via their application
    programming interface (API). These APIs suit various programming languages, including
    Python, JavaScript, and more. With API access, you can create custom applications,
    integrate generative AIs into your existing data analysis workflows, and even
    build GenAI-powered analytics dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: To further simplify the process of integrating generative AIs into your data
    analysis projects, you can use available SDKs (software development kits) and
    libraries created by model or third-party developers, one of the most prominent
    being LangChain ([www.langchain.com](http://www.langchain.com)). These resources
    can save you time and effort when it comes to working with the API, as they provide
    prebuilt functions and classes that handle common tasks. You can find popular
    SDKs and libraries for various programming languages on platforms like GitHub.
    Make sure to check the compatibility and support status before using them in your
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: An example of programmatic access to ChatGPT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can access different ChatGPT versions through the OpenAI API. This method
    allows you to programmatically send requests and receive responses, giving you
    greater control over the AI’s capabilities. Sign up for an API key on the OpenAI
    website to get started ([https://platform.openai.com/signup](https://platform.openai.com/signup)).
    Then, follow the API documentation to learn how to interact with ChatGPT using
    your preferred programming language.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate programmatic access, let’s look at an example of accessing the
    ChatGPT models from Python. Similar code will be used in some of the discussions
    in chapter 5 on using ChatGPT directly for data analysis. If you haven’t yet set
    up the OpenAI API, follow the instructions to install the library and set up an
    API key at the OpenAI signup page.
  prefs: []
  type: TYPE_NORMAL
- en: A rose by any other name . . . Throughout this book, we’ll work with Python
    (in a Jupyter environment or any Unix environment). In chapter 7, we’ll show that
    generative AIs are also capable of supporting many other programming environments.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the OpenAI API, it is recommended that you assign it to the `OPENAI_API_KEY`
    variable in the environment, either through your shell setup (depending on your
    system) or, preferably, through a .env file in your project. You can then use
    the following simple Python code to interact with ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.1 Interacting with ChatGPT through the API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code sets up an example exchange to be further completed by the
    model. It exemplifies how the model can handle multiturn exchanges (for example,
    multiple question-answer iterations) with context. The main input is the `messages`
    array of message objects, where each object consists of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: A `role`, either `system`, `user`, or `assistant`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A system-role message is typically included first, followed by alternating user
    and assistant messages. The system-role message sets up the background for the
    behavior of the assistant. The example in listing 1.1 modifies the assistant’s
    personality to reflect the style of responses we want it to take. This can be
    omitted, and without any specific tone or audience requirement, the model will
    reply in its usual helpful but flat language.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the messages should consist of alternating user and assistant content,
    providing the model with the exchange context. Initially, you can provide one
    user message to which the model should respond. In subsequent exchanges, you can
    build up the message array with the history of prompts and responses to provide
    the model with a context of the exchange so far, allowing the model to relate
    better to subsequent prompts. By default, the models have no memory of past requests,
    and all the relevant information must be supplied as part of the message array
    in each request.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `model` parameter specifies which instance of LLM we want to access. It’s
    best to refer to the OpenAI models website ([https://platform.openai.com/docs/models/](https://platform.openai.com/docs/models/))
    for the latest list of available models, as it changes quite frequently. To get
    started, a good choice would be to experiment with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: gpt-4—The latest production version of the GPT 4 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gpt-4o—A younger, faster, if less thoughtful brother of GPT 4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gpt-3.5-turbo—Still a very good choice, and it can be more cost-effective in
    some cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dall-e-3—Optimized for image generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tts-1—Designed to generate natural-sounding speech from text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: whisper-1—Can recognize speech and transcribe it as text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New models are being developed continuously; the preceding list illustrates
    the breadth of capabilities already available. Obviously, for the models using
    images or sound as either input or output, more advanced programming techniques
    will be required to interact with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 1.1 used only one of the plethora of other optional parameters: `temperature`.
    As you can guess, this controls randomness in the model responses. Increasing
    `temperature` can generate interesting results, but it has a high risk of causing
    the model to hallucinate. Experimenting with this is very interesting, but please
    use it cautiously in production environments. We’ll touch more on hallucinations
    and related risks in chapter 8.'
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the latest OpenAI documentation for other up-to-date information and
    additional parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Programmatic access to other OpenAI API-compatible models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many models are available in the market (over a million models in HuggingFace
    alone), each with its own interface. However, a lot of them are compatible with
    OpenAI API de facto standards, such as Meta’s Llama models.
  prefs: []
  type: TYPE_NORMAL
- en: To access these models, you can just replace the OpenAI API call code in the
    previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.2 Code to direct the client to a specific model, e.g., Llama
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The base URL parameter specifies the server hosting the model. Obviously, you’ll
    need to provide a specific API key, such as the Llama API token referenced here,
    as each provider will require their own user authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, as you probably noticed, we used the “gpt-4-0125-preview” model
    in listing 1.1\. If you switch to Llama, you’ll need to provide a valid model,
    such as “llama-13b-chat” or one of the other available Llama variants. The rest
    of the code can remain unchanged most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Example of programmatic access to Google Vertex AI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A slightly different API worth mentioning is the one defined by Google to access
    its powerful Gemini AI models, as well as Codey, which is optimized for code generation
    and completion, and Imagen, designed for image generation, editing, captioning,
    and visual question answering. Given the power of Google in the market, this API
    may also be a good contender for a standard in the future.
  prefs: []
  type: TYPE_NORMAL
- en: The quickest way to access these models is through the Cloud Shell ([https://cloud.google.com/shell/docs/launching-cloud-shell](https://cloud.google.com/shell/docs/launching-cloud-shell)),
    which is a terminal, or command line, used to access cloud services. Once you
    activate the shell, you need to install the API.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.3 Command to install the Google AI package on Google Cloud
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then you can use the following script to generate completions from the selected
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.4 Example code to call Vertex AI on Google Cloud
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code follows a similar flow as the OpenAI example. Apart from
    the project ID and location, which Google uses to authenticate access to the API,
    we need to specify which model we want to use. In this case, we chose “gemini-1.0-pro”,
    the basic text-only model. Google’s API also supports multimodal requests, including
    sound and images both in the input and response. A range of examples is available
    on the API web pages. We’d need to specify the “gemini-1.0-pro-vision” model for
    multimodal requests.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.4 also shows how to provide the `temperature` parameter, analogous
    to the one discussed in the OpenAI example, which is used to control the randomness
    in the responses.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth mentioning that Google provides an interface for explicitly setting
    the safety parameters of the model to block unsafe content, based on a list of
    defined blocking thresholds (table 1.1). The safety parameters can limit the model
    when it comes to generating content containing harassment, hate speech, explicit
    sexuality, or that may otherwise be dangerous. The full list for the newest Google
    models is provided on Google’s AI website([https://ai.google.dev/gemini-api/docs/safety-settings](https://ai.google.dev/gemini-api/docs/safety-settings)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.1 Blocking thresholds for configuring Google’s model safety parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Threshold name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `BLOCK_NONE`  | Always show, regardless of the probability of unsafe content.  |'
  prefs: []
  type: TYPE_TB
- en: '| `BLOCK_ONLY_HIGH`  | Block when there is a high probability of unsafe content.  |'
  prefs: []
  type: TYPE_TB
- en: '| `BLOCK_MEDIUM_AND_ABOVE` (default)  | Block when there is a medium or high
    probability of unsafe content.  |'
  prefs: []
  type: TYPE_TB
- en: '| `BLOCK_LOW_AND_ABOVE`  | Block when there is a low, medium, or high probability
    of unsafe content.  |'
  prefs: []
  type: TYPE_TB
- en: '| `HARM_BLOCK_THRESHOLD_UNSPECIFIED`  | The threshold is unspecified, so block
    using the default threshold.  |'
  prefs: []
  type: TYPE_TB
- en: In listing 1.4, we set very conservative parameters, `safety_config` `=` `{...}`,
    so the model would apply quite stringent filters to the output. This may result
    in a lower risk while the model is used, but at the cost of returning less useful
    responses to some prompts. Chapter 8 will offer a broader discussion of model
    risk considerations.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.4 Third-party integrations of generative AI models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the methods mentioned in previous sections, you may also find
    a number of generative AI models integrated into various third-party applications
    and plugins. These integrations typically focus on specific use cases, such as
    code generation and completion, data visualization, natural language processing,
    or predictive analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some examples of such integrated models:'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot, designed to assist with code generation, completion, and explanation,
    with integrations available for the most common integrated development environments
    (IDEs), such as VSCode, Visual Studio, and the JetBrains suite of IDEs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packages within the RStudio IDE, like *air*, provide integration of LLM models
    into this popular R and Python environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantage of such integrations is that they usually have direct access to
    the data or code inside the host environment and are able to directly insert and
    modify the code, which saves the user the effort of copying each code snippet
    from the IDE to the model chat window and back again.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.5 Running LLMs locally
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running any downloadable model on your personal computer isn’t rocket science.
    You don’t need NASA-grade equipment either; a decent PC with enough RAM should
    suffice (with a definition of “decent” being somewhat dynamic). A solid GPU would
    speed things up, but that is not an absolute requirement. You will need some familiarity
    with the command line and a couple of libraries to bridge the gap between ambition
    and reality.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we can use Python to connect to the Llama 2 model. Implementing
    it as advertised on Meta’s page is a bit daunting, so we’ll cheat a little. First,
    we’ll use a quantized model to save on RAM requirements. In essence, quantization
    of the model means “shaving off,” or rounding, the model weights, sacrificing
    a little accuracy in exchange for computational efficiency. We’ll also utilize
    models transitioned into an easy-to-use GGUF file format. If you requested and
    got a proper license from Meta, you can download the Llama of your choice from
    the HuggingFace portal.
  prefs: []
  type: TYPE_NORMAL
- en: Two libraries we propose to make your task easier are LangChain and llama-cpp-python.
    Overall, the environment setup is as simple as presented in listings 1.1 and 1.2\.
    It’s worth mentioning that LangChain could also be used to streamline connecting
    to ChatGPT or Gemini as well.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.5 Command to install the LangChain and Llama libraries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A word of fair warning. The llama-cpp-python library has some non-negotiable
    requirements, especially when installed in a Windows environment. However, as
    it’s a fast-developing tool, check the library site for the latest details.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our model downloaded and the proper libraries installed, all
    we need to do is send a prompt to a model and capture the result.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.6 Example of sending a prompt to Llama
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! Depending on your hardware and the chosen model size, you should
    get your answer in anywhere from a few seconds to a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Context is not only about tokens  The minimalistic implementation we have presented
    here does not have any conversation memory. It’s a shoot-and-forget method of
    getting a specific answer to a specific question. Implementing chat memory using
    LangChain is, however, relatively simple, and you should not be afraid to check
    the website for current instructions. Funnily enough, it even has an LLM-powered
    chatbot to answer your questions about the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many options for both model instantiation and prompt development,
    which you may find useful. You’ll find detailed instructions on the LangChain
    library website (www.langchain.com). We will not dig into them deeper, as this
    simple setup is sufficient for our purposes: asking Llama a question and getting
    an answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.6 Best practices and tips for successful generative AI implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although this book won’t cover advanced topics related to the direct integration
    of generative AI into applications using APIs, we encourage you to follow some
    best practices and consider the following tips to successfully integrate generative
    AIs with your data analytics solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Define clear objectives*—Start by clearly identifying the goals and expectations
    of integrating generative AI into your data analytics solution. Determine the
    tasks you want generative AI to perform, such as data preprocessing, generating
    insights, or creating visualizations, and tailor your integration accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Familiarize yourself with the API or SDK you are planning to use*—Thoroughly
    read and understand the OpenAI API documentation, including details about the
    API’s or SDK’s features, limitations, and best practices. This knowledge will
    help you design efficient and reliable interactions between generative AI and
    your data analytics tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use appropriate data formats*—Ensure that you are using compatible data formats
    when sending requests to and receiving responses from generative AI. Transform
    your data, if necessary, to ensure seamless integration and prevent data loss
    or misinterpretation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monitor usage and costs*—Keep track of your API usage to prevent unexpected
    costs, especially when working with large datasets or complex analytics tasks.
    Implement rate limiting, caching, or other optimizations to manage your API calls
    and stay within your plan’s limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Handle errors and timeouts*—Implement proper error handling and retry mechanisms
    to deal with potential issues, such as timeouts or rate-limit errors. This will
    help ensure the stability and reliability of your integration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Simplify and optimize your prompts*—Craft your prompts carefully to obtain
    the most accurate and relevant results from generative AI. Use clear, concise
    language, and provide enough context to help the AI understand your requirements.
    You may need to experiment with different prompt structures to find the best approach
    for your specific use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Maintain the prompt cheatsheet after optimizing (and verifying the output)*—Regularly
    update and refine your prompt notes as you continue to work with generative AI.
    As your application evolves and your understanding of AI capabilities deepens,
    your cheatsheet should evolve to include new findings, common errors to avoid,
    and updated best practices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Provide source material and context*—Alongside the problem or question, give
    the model context data examples or explicitly ask it to follow a certain reasoning
    path. You can also suggest steps in the prompt for the model to follow in its
    reasoning or ask it to explain certain solution steps. All of this will ensure
    a greater probability of a correct response and increased transparency regarding
    how it was generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Evaluate the AI’s output*—Generative AI’s output may not always be accurate
    or relevant. Always double-check the results provided by the AI, and consider
    implementing human review or validation processes, especially for critical or
    high-stakes decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Test and iterate*—Before fully integrating generative AI into your data analytics
    solution, thoroughly test its performance with various tasks and datasets. This
    will help you identify any issues, limitations, or inaccuracies. Continuously
    iterate on your prompts, data formats, and integration methods to improve the
    overall effectiveness of the AI in your analytics workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ensure data security and privacy*—When working with sensitive data, make sure
    you comply with data protection regulations and follow security best practices.
    If you’re working with cloud-based generative AI, encrypt data when transmitting
    it to and from the provider, and consider using data anonymization techniques
    to protect the privacy of your users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stay updated on generative AI developments*—Keep track of updates and new
    features developed in the field, as these may impact your integration or offer
    additional capabilities. Regularly review the API documentation, and subscribe
    to relevant newsletters or forums to stay informed about any changes or improvements
    to generative AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Leverage community resources*—Take advantage of resources provided by the
    AI community, such as sample code, tutorials, and forums. These resources can
    help you learn from others’ experiences and discover best practices for integrating
    generative AIs with various data analytics solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By following these best practices and tips, you can successfully integrate generative
    AI into your data analytics workflows and harness its full potential to enhance
    your decision-making, automate tasks, and uncover valuable insights.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, after this introduction, generative AI no longer appears to be a
    mysterious and, possibly, useless invention. Subsequent chapters will demonstrate
    specific exchanges between a human and a generative AI and will show us using
    the responses in all aspects of data analytical work. We’ll also comment on the
    shortcomings and pitfalls that need to be looked out for to make this cooperation
    between humans and AI as painless and productive as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Things to ask generative AI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: What are your limitations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the knowledge base you’ve been built upon?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the latest version of <insert favorite analytics tool> that you know
    about?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI and derivative tools have taken great strides in recent years
    and can be used as invaluable support in many fields, including data analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the progress, these tools won’t (yet!) replace a competent data analyst,
    and there are many limitations that users should be aware of.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, we encourage you to take full advantage of the immense possibilities
    of supporting your data analytical work with these language models, which can
    be done safely by following a few common-sense guidelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The easiest way to access generative AIs is via their web interfaces, although
    APIs and SDKs can be used in more advanced applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
