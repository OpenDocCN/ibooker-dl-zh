<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">1</span></span> <span class="chapter-title-text">Discovering graph neural networks</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Defining graphs and graph neural networks</li>
<li class="readable-text" id="p3">Understanding why people are excited about graph neural networks</li>
<li class="readable-text" id="p4">Recognizing when to use graph neural networks</li>
<li class="readable-text" id="p5">Taking a big picture look at solving a problem with a graph neural network</li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>For data practitioners, the fields of machine learning and data science initially excite us because of the potential to draw nonintuitive and useful insights from data. In particular, the insights from machine learning and deep learning promise to enhance our understanding of the world. For the working engineer, these tools promise to deliver business value in unprecedented ways. </p>
</div>
<div class="readable-text intended-text" id="p7">
<p>Experience deviates from this ideal. Real-world data is usually messy, dirty and biased. Furthermore, statistical methods and learning systems come with their own set of limitations. An essential role of the practitioner is to comprehend these limitations and bridge the gap between real data and a feasible solution. For example, we may want to predict fraudulent activity in a bank, but we first need to make sure that our training data has been correctly labeled. Even more importantly, we’ll need to check that our models won’t incorrectly assign fraudulent activity to normal behaviors, possibly due to some hidden confounders in the data. </p>
</div>
<div class="readable-text intended-text" id="p8">
<p>For graph data, until recently, bridging this gap has been particularly challenging. Graphs are a data structure that is rich with information and especially adept at capturing the intricacies of data where relationships play a crucial role. Graphs are omnipresent, with relational data appearing in different forms such as atoms in molecules (nature), social networks (society), and even models the connection of web pages on the internet (technology) [1]. It’s important to note that the term <em>relational</em> here doesn’t refer to <em>relational databases</em>, but rather to data where relationships are of significance.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>Previously, if you wanted to incorporate relational features from a graph into a deep learning model, it had to be done in an indirect way, with different models used to process, analyze, and then use the graph data. These separate models often couldn’t be easily scaled and had trouble taking into account all the node and edge properties of graph data. To make the best use of this rich and ubiquitous data type for machine learning, we needed a specialized machine learning technique specifically designed for the distinct qualities of graphs and relational data. This is the gap that graph neural networks (GNNs) fill.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>The deep learning field often contains a lot of hype around new technologies and methods. However, GNNs are widely recognized as a genuine leap forward for graph-based learning [2]. This doesn’t mean that GNNs are a silver bullet. Careful comparisons should be done between predictive results derived from GNNs and other machine learning and deep learning methods. </p>
</div>
<div class="readable-text intended-text" id="p11">
<p>The key thing to remember is that if your data science problem involves data that can be structured as a graph—that is, the data is connected or relational—then GNNs could offer a valuable approach, even if you weren’t aware that something was missing in your approach. GNNs can be designed to handle very large data, to scale, and to adapt to graphs of different sizes and shapes. This can make working with relationship-centric data easier and more efficient, as well as yield richer results.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>The standout advantages of GNNs are why data scientists and engineers are increasingly recognizing the importance of mastering them. GNNs have the ability to unveil unique insights from relational data—from identifying new drug candidates to optimizing ETA prediction accuracy in your Google Maps app—acting as a catalyst for discovery and innovation, and empowering professionals to push the boundaries of conventional data analysis. Their diverse applicability spans various fields, offering professionals a versatile tool that is as relevant in e-commerce (e.g., recommendation engines) as it is in bioinformatics (e.g., drug toxicity prediction). Proficiency in GNNs equips data professionals with a multifaceted tool for enhanced, accurate, and innovative data analysis of graphs.</p>
</div>
<div class="readable-text intended-text" id="p13">
<p>For all these reasons, GNNs are now the popular choice for recommender engines, analyzing social networks, detecting fraud, understanding how biomolecules behave, and many other practical examples that we’ll meet over the course of this book. </p>
</div>
<div class="readable-text" id="p14">
<h2 class="readable-text-h2"><span class="num-string">1.1</span> Goals of this book</h2>
</div>
<div class="readable-text" id="p15">
<p><em>Graph Neural Networks in Action</em> is aimed at practitioners who want to begin to deploy GNNs to solve real problems. This could be a machine learning engineer not familiar with graph data structures, a data scientist who hasn’t yet tried GNNs, or even a software engineer who may be unfamiliar with either. Throughout this book, we’ll be covering topics from the basics of graphs all the way to more complex GNN models. We’ll be building up the architecture of a GNN, step-by-step. This includes the overall architecture of a GNN and the critical aspect of message passing. We then go on to add different features and extensions to these basic aspects, such as introducing convolution and sampling, attention mechanisms, a generative model, and operating on dynamic graphs. When building our GNNs, we’ll be working in Python and using some standard libraries. GNN libraries are either standalone or use TensorFlow or PyTorch as a backend. In this text, the focus will be on PyTorch Geometric (PyG). Other popular libraries include Deep Graph Library (DGL, a standalone library) and Spektral (which uses Keras and TensorFlow as a backend). There is also Jraph for JAX users.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>Our aim throughout this book is to enable you to </p>
</div>
<ul>
<li class="readable-text" id="p17"> assess the suitability of a GNN solution for your problem. </li>
<li class="readable-text" id="p18"> understand when traditional neural networks won’t perform as well as a GNN for graph structured data and when GNNs may not be the best tool for tabular data. </li>
<li class="readable-text" id="p19"> design and implement a GNN architecture to solve problems specific to you. </li>
<li class="readable-text" id="p20"> make clear the limitations of GNNs. </li>
</ul>
<div class="readable-text" id="p21">
<p>This book is weighted toward implementation using programming. We also devote some time on essential theory and concepts, so that the techniques covered can be sufficiently understood. These are covered in an “Under the Hood” section at the end of most chapters to separate the technical reasons from the actual implementation. There are many different models and packages that build on the key concepts we introduce in this book. So, this book shouldn’t be seen as a comprehensive review of all GNN methods and models, which could run to several thousands of pages, but rather the starting point for the curious and eager-to-learn practitioner. </p>
</div>
<div class="readable-text intended-text" id="p22">
<p>The book is divided into three parts. Part 1 covers the basics of GNNs, especially the ways in which they differ from other neural networks, such as <em>message passing</em> and <em>embeddings</em>, which have specific meaning for GNNs. Part 2, the heart of the book, goes over the models themselves, where we cover a handful of key model types. Then, in part 3, we’ll go into more detail with some of the harder models and concepts, including how to scale graphs and deal with temporal data. </p>
</div>
<div class="readable-text intended-text" id="p23">
<p><em>Graph Neural Networks in Action</em> is designed for people to jump quickly into this new field and start building applications. Our aim for this book is to reduce the friction of implementing new technologies by filling in the gaps and answering key development questions whose answers may not be easy to find or may not be covered elsewhere at all. Each method is introduced through an example application so you can understand how GNNs are applied in practice. We strongly advise you to try out the code for yourself along the way. </p>
</div>
<div class="readable-text" id="p24">
<h3 class="readable-text-h3"><span class="num-string">1.1.1</span> Catching up on graph fundamentals</h3>
</div>
<div class="readable-text" id="p25">
<p>Yes, you do need to understand the basics of graphs before you can understand GNNs. Yet our goal for this book is to teach GNNs to deep learning practitioners and builders of traditional neural networks who may not know much about graphs. At the same time, we also recognize that readers of this book may vary enormously in their knowledge of graphs. How to address these differences and make sure everyone has what they need to make the most of this book? In this chapter, we provide an introduction to the fundamental graph concepts that are most essential to understanding GNNs. If you’re well-versed in graphs, you may choose to skip this section, although we recommend skimming through as we cover some specific terminology and use-cases that will be helpful to understand for the remainder of the book. For those of you who have more questions about graphs, we’ve also included a full tutorial on basic graph concepts and terminology in appendix A. This primer should also serve as a reference for looking up specific concepts. </p>
</div>
<div class="readable-text intended-text" id="p26">
<p>After the refresher on key concepts in graphs and graph learning, we’ll look into some case studies in several fields where GNNs are being successfully applied. Then, we’ll break down those specific cases to see what makes a good case for using a GNN, as well as how to know if you have a GNN problem on your hands. At the end of the chapter, we introduce the mechanics of GNNs, the barebone skeleton that the rest of the book will add to.</p>
</div>
<div class="readable-text" id="p27">
<h2 class="readable-text-h2"><span class="num-string">1.2</span> Graph-based learning</h2>
</div>
<div class="readable-text" id="p28">
<p>This section defines graphs, graph-based learning, and some fundamentals of GNNs, including the basic structure of a graph and a taxonomy of different types of graphs. Then, we’ll review graph-based learning, putting GNNs in context with other learning methods. Finally, we’ll explain the value of graphs, ending with an example of data derived from the Titanic dataset.</p>
</div>
<div class="readable-text" id="p29">
<h3 class="readable-text-h3"><span class="num-string">1.2.1</span> What are graphs?</h3>
</div>
<div class="readable-text" id="p30">
<p>Graphs are data structures with elements, expressed as <em>nodes or vertices</em>, and relationships between elements, expressed as <em>edges or links</em>, as shown in figure 1.1. All nodes in the graph will have additional <em>feature data</em>. This is node-specific data, relating to things such as the names or ages of individuals in a social network. The links are key to the power of relational data, as they allow us to learn more about the system, give new tools for analyzing data, and predict new properties from it. This is in contrast to tabular data such as a database table, dataframe, or spreadsheet, where the data is fixed in rows and columns.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p31">
<img alt="figure" height="492" src="../Images/1-1.png" width="896"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.1</span> A graph. Individual elements, represented here by letters A through E, are nodes, also called vertices, and their relationships are described by edges, also known as links.</h5>
</div>
<div class="readable-text" id="p32">
<p>To describe and learn from the edges between the nodes, we need a way to write them down. This can be done explicitly, stating that the A node is connected to B and E, and that the B node is connected to A, C, D, and E. Quickly, we can see that describing things in this way becomes unwieldy and that we might be repeating redundant information (that A is connected to B and that B is connected to A). Luckily, there are many mathematical formalisms for describing relations in graphs. One of the most common is to describe the <em>adjacency matrix</em>, which we write out in table 1.1. Notice that the adjacency matrix is symmetric across the diagonal and that all values are ones or zero. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p33">
<h5 class="browsable-container-h5"><span class="num-string">Table 1.1</span> The adjacency matrix for the simple graph in figure 1.1 </h5>
<table>
<thead>
<tr>
<th/>
<th>
<div>
         A 
       </div></th>
<th>
<div>
         B 
       </div></th>
<th>
<div>
         C 
       </div></th>
<th>
<div>
         D 
       </div></th>
<th>
<div>
         E 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> <strong>A</strong> <br/></td>
<td>  0 <br/></td>
<td>  1 <br/></td>
<td>  0 <br/></td>
<td>  0 <br/></td>
<td>  1 <br/></td>
</tr>
<tr>
<td> <strong>B</strong> <br/></td>
<td>  1 <br/></td>
<td>  0 <br/></td>
<td>  1 <br/></td>
<td>  1 <br/></td>
<td>  1 <br/></td>
</tr>
<tr>
<td> <strong>C</strong> <br/></td>
<td>  0 <br/></td>
<td>  1 <br/></td>
<td>  0 <br/></td>
<td>  1 <br/></td>
<td>  0 <br/></td>
</tr>
<tr>
<td> <strong>D</strong> <br/></td>
<td>  0 <br/></td>
<td>  1 <br/></td>
<td>  1 <br/></td>
<td>  0 <br/></td>
<td>  1 <br/></td>
</tr>
<tr>
<td> <strong>E</strong> <br/></td>
<td>  1 <br/></td>
<td>  1 <br/></td>
<td>  0 <br/></td>
<td>  1 <br/></td>
<td>  0 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p34">
<p>The adjacency matrix of a graph is an important concept that makes it easy to observe all the connections of a graph in a single table [3]. Here, we assumed that there is no directionality in our graph; that is, if 0 is connected to 1, then 1 is also connected to 0. This is known as an <em>undirected graph</em>. Undirected graphs can be easily inferred from an adjacency matrix because, in this case, the matrix is symmetric across the diagonal (e.g., in table 1.1, the upper-right triangle is reflected onto the bottom left). </p>
</div>
<div class="readable-text intended-text" id="p35">
<p>We also assume here that all the relations between nodes are identical. If we wanted the relation of nodes B–E to mean more than the relation of nodes B–A, then we could increase the weight of this edge. This translates to increasing the value in the adjacency matrix, making the entry for the B–A edge in table 1.1 equal to 10 instead of 1, for example. </p>
</div>
<div class="readable-text intended-text" id="p36">
<p>Graphs where all relations are of equal importance are known as <em>unweighted graphs</em> and can also be easily observed from the adjacency matrix because all graph entries are either 1s or 0s. Graphs where edges have multiple values are known as <em>weighted.</em></p>
</div>
<div class="readable-text intended-text" id="p37">
<p>If any of the nodes in the graph don’t have an edge that connects to itself, then the nodes will also have 0s at their own value in the adjacency matrix (0s along the diagonal). This means a graph doesn’t have self-loops. A <em>self-loop</em> occurs when a node has an edge that connects to that same node. To add a self-loop, we just make the value for that node nonzero at its position in the diagonal.</p>
</div>
<div class="readable-text intended-text" id="p38">
<p>In practice, an adjacency matrix is only one of many ways to describe relations in a graph. Others include adjacency lists, edge lists, or an incidence matrix. Understanding these types of data structures well is vital to graph-based learning. If you’re unfamiliar with these terms, or need a refresher, we recommend looking through appendix A, which has additional details and explanations. </p>
</div>
<div class="readable-text" id="p39">
<h3 class="readable-text-h3"><span class="num-string">1.2.2</span> Different types of graphs</h3>
</div>
<div class="readable-text" id="p40">
<p>Understanding the many different types of graphs can help us work out what methods to use to analyze and transform the graph, and what machine learning methods to apply. In the following, we give a very quick overview of some of the most common properties for graphs to have. As before, we recommend you look through appendix A for further information. </p>
</div>
<div class="readable-text" id="p41">
<h4 class="readable-text-h4">Homogeneous and heterogeneous graphs</h4>
</div>
<div class="readable-text" id="p42">
<p>The most basic graphs are <em>homogenous graphs</em>, which are made up of one type of node and one type of edge. Consider a homogeneous graph that describes a recruitment network. In this type of graph, the nodes would represent job candidates, and the edges would represent relationships between the candidates.</p>
</div>
<div class="readable-text intended-text" id="p43">
<p>If we want to expand the power of our graph to describe our recruitment network, we could give it more types of nodes and edges, making it a <em>heterogeneous graph</em>. With this expansion, some nodes may be candidates and others may be companies. Edges could now consist of relationships between candidates and current or past employment of job candidates at the companies. See figure 1.2 <span class="aframe-location"/>for a comparison of a homogeneous graph (all nodes or edges have the same shade) with a heterogeneous graph (nodes and edges have a variety of shades). </p>
</div>
<div class="browsable-container figure-container" id="p44">
<img alt="figure" height="279" src="../Images/1-2.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.2</span> A homogeneous graph and a heterogeneous graph. Here, the shade of a node or edge represents its type or class. For the homogeneous graph, all nodes are of the same type, and all edges are of the same type. For the heterogeneous graph, nodes and edges have multiple types. </h5>
</div>
<div class="readable-text" id="p45">
<h4 class="readable-text-h4">Bipartite graphs</h4>
</div>
<div class="readable-text" id="p46">
<p>Similar to heterogeneous graphs, <em>bipartite graphs</em> also can be separated or partitioned into different subsets. However, bipartite graphs (figure 1.3) have a very specific network structure such that nodes in each subset connect to nodes outside of their subset and not inside. Later, we’ll be discussing recommendation systems and the Pinterest graph. This graph is bipartite because one set of nodes (pins) connects another set of nodes (boards) but not to nodes within their set (pins). <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p47">
<img alt="figure" height="279" src="../Images/1-3.png" width="542"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.3</span> A bipartite graph. There are two types of nodes (two shades of circles). In a bipartite graph, nodes can’t be connected to nodes of the same type. This is also an example of a heterogeneous graph.</h5>
</div>
<div class="readable-text" id="p48">
<h4 class="readable-text-h4">Cyclic graphs, acyclic graphs, and directed acyclic graphs</h4>
</div>
<div class="readable-text" id="p49">
<p>A graph is <em>cyclic</em> if it allows you to start at a node, travel along its edges, and return to the starting node without retracing any steps, creating a circular path within the graph. In contrast, in an <em>acyclic</em> graph, no matter which path you take from any starting node, you can’t return to the starting point without backtracking. These graphs, as shown in figure 1.4, often resemble tree-like structures or paths that don’t loop back on themselves.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>While both cyclic and acyclic graphs can be either undirected or directed, a <em>directed acyclic graph (DAG)</em> is a specific type of acyclic graph that is exclusively directed. In a DAG, all edges have a direction, and no cycles are allowed. DAGs represent one-way relationships where you can’t follow the arrows and end up back at the starting point. This characteristic makes DAGs essential in causal analysis, as they reflect causal structures where causality is assumed to be unidirectional. For example, A can cause B, but B can’t simultaneously cause A. This unidirectional nature aligns perfectly with the structure of DAGs, making them ideal for modeling workflow processes, dependency chains, and causal relationships in various fields. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p51">
<img alt="figure" height="844" src="../Images/1-4.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.4</span> A cyclic graph (left), an acyclic graph (right), and a DAG (bottom). In the cyclic graph, the cycle is shown by the arrows (directed edges) connecting nodes A-E-D-C-B-A. Note that two nodes, G and F are part of the graph, but not part of its defining cycle. The acyclic graph is composed of undirected edges, and no cycle is possible. In the DAG, all directed edges flow in one direction, from A to F.</h5>
</div>
<div class="readable-text" id="p52">
<h4 class="readable-text-h4">Knowledge graphs</h4>
</div>
<div class="readable-text" id="p53">
<p>A <em>knowledge graph</em> is a specialized type of heterogeneous graph that represents data with enriched semantic meaning, capturing not only the relationships between different entities but also the context and nature of these relationships. Unlike conventional graphs, which primarily emphasize structure and connectivity, a knowledge graph incorporates metadata and follows specific schemas to provide deeper contextual information. This allows for advanced reasoning and querying capabilities, such as identifying patterns, uncovering specific types of connections, or inferring new relationships.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>In the example of an academic research network at a university, a knowledge graph might represent various entities such as Professors, Students, Papers, and Research Topics, and explicitly define the relationships between them. For instance, Professors and Students could be associated with Papers through an Authorship relationship, while Professors might also Supervise Students. Furthermore, the graph would reflect hierarchical structures, such as Professors and Students being categorized under Departments. You can see this knowledge graph depicted in figure 1.5.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p55">
<img alt="figure" height="739" src="../Images/1-5.png" width="1009"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.5</span> A knowledge graph representing an academic research network within a university’s physics department. The graph illustrates both hierarchical relationships, such as professors and students as members of the department, and behavioral relationships, such as professors supervising students and authoring papers. Entities such as Professors, Students, Papers, and Topics are connected through semantically meaningful relationships (Supervises, Wrote, Inspires). Entities also have detailed features (Name, Department, Type) providing further context. The semantic connections and features enable advanced querying and analysis of complex academic interactions. </h5>
</div>
<div class="readable-text" id="p56">
<p>A key feature of knowledge graphs is their ability to provide explicit context. Unlike conventional heterogeneous graphs, which display different types of entities and their basic connections without detailed semantic meaning, knowledge graphs go further by defining the specific types and meanings of relationships. For example, while a traditional graph might show that Professors are connected to Departments or that Students are linked to Papers, a knowledge graph would specify that Professors supervise Students or that Students and Professors Wrote Papers. This added layer of meaning enables more powerful querying and analysis, making knowledge graphs particularly valuable in fields such as natural language processing, recommendation systems, and academic research analysis.</p>
</div>
<div class="readable-text" id="p57">
<h4 class="readable-text-h4">Hypergraphs</h4>
</div>
<div class="readable-text" id="p58">
<p>One of the more complex and difficult graphs to work with is the hypergraph. <em>Hypergraphs</em> are those where a single edge can be connected to multiple different nodes. For graphs that aren’t hypergraphs, edges are used to connect exactly two nodes (or a node to itself for self-loops). As shown in figure 1.6, edges in a hypergraph can connect between any number of nodes. The complexity of a hypergraph is reflected in its adjacency data. For typical graphs, network connectivity is represented by a two-dimensional adjacency matrix. For hypergraphs, the adjacency matrix extends to a higher dimensional tensor, referred to as an <em>incidence tensor</em>. This tensor is N-dimensional, where N is the maximum number of nodes connected by a single edge. An example of a hypergraph might be a communication platform that allows for group chats as well as single person conversations.<span class="aframe-location"/> In an ordinary graph, edges would only connect two people. In a hypergraph, one hyperedge could connect multiple people, representing a group chat.</p>
</div>
<div class="browsable-container figure-container" id="p59">
<img alt="figure" height="694" src="../Images/1-6.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.6</span> One undirected hypergraph, illustrated in two ways. On the left, we have a graph whose edges are represented by shaded areas, marked by letters, and whose vertices are dots, marked by numbers. On the right, we have a graph whose edge lines (marked by letters) connect up to 3 nodes (circles marked by numbers). Node 8 has no edge. Node 7 has a self-loop.</h5>
</div>
<div class="readable-text" id="p60">
<h3 class="readable-text-h3"><span class="num-string">1.2.3</span> Graph-based learning</h3>
</div>
<div class="readable-text" id="p61">
<p>As we’ll see in the rest of this chapter, graphs are ubiquitous in our everyday life. <em>Graph-based learning</em> takes graphs as input data to build models that give insight into questions about this data. Later in this chapter, we look at different examples of graph data as well as at the sort of questions and tasks we can use graph-based learning to answer. </p>
</div>
<div class="readable-text intended-text" id="p62">
<p>Graph-based learning uses a variety of machine learning methods to build <em>representations</em> of graphs. These representations are then used for downstream tasks such as node or link prediction or graph classification. In chapter 2, you’ll learn about one of the essential tools in graph-based learning, building embeddings. Briefly, embeddings are <em>low-dimensional </em>vector representations. We can build an embedding of different nodes, edges, or entire graphs, and there are a number of different ways to do this such as the Node2Vec (N2V) or DeepWalk algorithms. </p>
</div>
<div class="readable-text intended-text" id="p63">
<p>Methods for analysis on graph data have been around for a long time, at least as early as the 1950s when <em>clique methods</em> used certain features of a graph to identify subsets or communities in the graph data [4]. </p>
</div>
<div class="readable-text intended-text" id="p64">
<p>One of the most famous graph-based algorithms is PageRank, which was developed by Larry Page and Sergey Brin in 1996 and formed the basis for Google’s search algorithms. Some believe that this algorithm was a key element in the company’s meteoric rise in the following years. This highlights that a successful graph-based learning algorithm can have a huge effect. </p>
</div>
<div class="readable-text intended-text" id="p65">
<p>These methods are only a small subset of graph-based learning and analysis techniques. Others include belief propagation [5], graph kernel methods [6], label propagation [7], and isomaps [8]. However, in this book, we’ll focus on one of the newest and most exciting additions to the family of graph-based learning techniques: GNNs. </p>
</div>
<div class="readable-text" id="p66">
<h3 class="readable-text-h3"><span class="num-string">1.2.4</span> What is a GNN?</h3>
</div>
<div class="readable-text" id="p67">
<p>GNNs combine graph-based learning with deep learning. This means that neural networks are used to build embeddings and process the relational data. An overview of the inner workings of a GNN is shown in figure 1.7. </p>
</div>
<div class="readable-text intended-text" id="p68">
<p>GNNs allows you to represent and learn from graphs, including their constituent nodes, edges, and features. In particular, many methods of GNNs are built specifically to scale effectively with the size and complexity of a graph. This means that GNNs can operate on huge graphs, as we’ll discuss. In this sense, GNNs provide analogous advantages to relational data as convolutional neural networks have given for image-based data and computer vision. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p69">
<img alt="figure" height="634" src="../Images/1-7.png" width="927"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.7</span> An overview of how GNNs work. An input graph is passed to a GNN. The GNN then uses neural networks to transform graph features such as nodes or edges into nonlinear embeddings through a process known as message passing. These embeddings are then tuned to specific unknown properties using training data. After the GNN is trained, it can predict unknown features of a graph.</h5>
</div>
<div class="readable-text intended-text" id="p70">
<p>Historically, applying traditional machine learning methods to graph data structures has been challenging because graph data, when represented in grid-like formats and data structures, can lead to massive repetitions of data. To address this, graph-based learning focuses on approaches that are <em>permutation invariant</em>. This means that the machine learning method is uninfluenced by the ordering of the graph representation. In concrete terms, it means that we can shuffle the rows and columns of the adjacency matrix without affecting our algorithm’s performance. Whenever we’re working with data that contains relational data, that is, has an adjacency matrix, then we want to use a machine learning method that is permutation invariant to make our method more general and efficient. Although GNNs can be applied to all graph data, GNNs are especially useful because they can deal with huge graph datasets and typically perform better than other machine learning methods. </p>
</div>
<div class="readable-text intended-text" id="p71">
<p>Permutation invariances are a type of <em>inductive bias</em>, or an algorithm’s learning bias, and are powerful tools for designing machine learning algorithms [1]. The need for permutation-invariant approaches is one of the central reasons that graph-based learning has increased in popularity in recent years.</p>
</div>
<div class="readable-text intended-text" id="p72">
<p>Being designed for permutation-invariant data comes with some drawbacks along with its advantages. GNNs aren’t as well suited for other data, such as images or tables. While this might seem obvious, images and tables are <em>not </em>permutation invariant and therefore not a good fit for GNNs. If we shuffle the rows and columns of an image, then we scramble the input. Instead, machine learning algorithms for images seek <em>translational invariance</em>, which means that we can translate (shift) the object in an image, and it won’t affect the performance of the algorithm. Other neural networks, such as convolutional neural networks (CNNs) typically perform much better on images. </p>
</div>
<div class="readable-text" id="p73">
<h3 class="readable-text-h3"><span class="num-string">1.2.5</span> Differences between tabular and graph data</h3>
</div>
<div class="readable-text" id="p74">
<p>Graph data includes all data with some relational content, making it a powerful way to represent complex connections. While graph data might initially seem distinct from traditional tabular data, many datasets that are typically represented in tables can be re-created as graphs with some data engineering and imagination. Let’s take a closer look at the Titanic dataset, a classic example in machine learning, and explore how it can be transformed from a table format to a graph format.</p>
</div>
<div class="readable-text intended-text" id="p75">
<p>The Titanic dataset describes passengers on the Titanic, a ship that famously met an untimely end when it collided with an iceberg. Historically, this dataset has been analyzed in tabular format, containing rows for each passenger with columns representing features such as age, gender, fare, class, and survival status. However, the dataset also contains rich, unexplored relationships that aren’t immediately visible in a table format, as shown in figure 1.8.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p76">
<img alt="figure" height="210" src="../Images/1-8.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.8</span> The Titanic Dataset is usually displayed and analyzed using a table format.</h5>
</div>
<div class="readable-text" id="p77">
<h4 class="readable-text-h4">Recasting the Titanic dataset as a graph</h4>
</div>
<div class="readable-text" id="p78">
<p>To transform the Titanic dataset into a graph, we need to consider how to represent the underlying relationships between passengers as nodes and edges:</p>
</div>
<ul>
<li class="readable-text" id="p79"> <em>Nodes</em>—In the graph, each passenger can be represented as a node. We can also introduce nodes for other entities, such as cabins, families, or even groups such as “third-class passengers.” </li>
<li class="readable-text" id="p80"> <em>Edges</em>—Edges represent the relationships or connections between these nodes. For example: 
    <ul>
<li> Passengers who are family members (siblings, spouses, parents, or children) based on the available data </li>
<li> Passengers who share a cabin or were traveling together </li>
<li> Social or business relationships that might be inferred from shared ticket numbers, last names, or other identifying features </li>
</ul></li>
</ul>
<div class="readable-text" id="p81">
<p>To construct this graph, we need to use the existing information in the table and potentially enrich it with secondary data sources or assumptions (e.g., linking last names to create family groups). This process converts the tabular data into a graph-based structure, shown in figure 1.9, where each edge and node encapsulates meaningful relational data.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p82">
<img alt="figure" height="439" src="../Images/1-9.png" width="542"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.9</span> The Titanic dataset, showing the family relationships of the people on the Titanic visualized as a graph (Source: Matt Hagy). Here, we can see that there was a rich social network as well as many passengers with unknown family ties. </h5>
</div>
<div class="readable-text" id="p83">
<h4 class="readable-text-h4">How graph data adds depth and meaning</h4>
</div>
<div class="readable-text" id="p84">
<p>Once the dataset is represented as a graph, it provides a much deeper view of the social and familial connections between the passengers. For example:</p>
</div>
<ul>
<li class="readable-text" id="p85"> <em>Family relationships</em>—The graph clearly shows how certain passengers were related (e.g., as parents, children, or siblings). This could help us understand survival patterns, as family members might have behaved differently in a crisis than individuals traveling alone. </li>
<li class="readable-text" id="p86"> <em>Social networks</em>—Beyond families, the graph could reveal broader social networks (e.g., friendships or business connections), which could be important factors in analyzing behavior and outcomes. </li>
<li class="readable-text" id="p87"> <em>Community insights</em>—The graph structure also allows for community detection algorithms to identify clusters of related or connected passengers, which may reveal new insights into survival rates, rescue patterns, or other behaviors. </li>
</ul>
<div class="readable-text" id="p88">
<p>Graph representations add depth by specifying connections that might not be obvious in a tabular format. For example, understanding who traveled together, who shared a cabin, or who had social or family ties can provide more context on survival rates and passenger behavior. This is crucial for tasks such as node prediction, where we want to predict attributes or outcomes based on the relationships represented in the graph.</p>
</div>
<div class="readable-text intended-text" id="p89">
<p>By creating an adjacency matrix or defining graph edges and nodes based on the relationships in the dataset, we can transition from simple data analysis to more sophisticated graph-based learning methods.</p>
</div>
<div class="readable-text" id="p90">
<h2 class="readable-text-h2"><span class="num-string">1.3</span> GNN applications: Case studies</h2>
</div>
<div class="readable-text" id="p91">
<p>As we’ve seen, GNNs are neural networks designed to work on relational data. They give new ways for relational data to be transformed and manipulated, by being easier to scale and more accurate than previous graph-based learning methods. In the following, we discuss some exciting applications of GNNs, to see, at a high level, how this class of models are solving real-world problems. Links to source papers are listed at the end of the book if you want to learn more about these particular projects.</p>
</div>
<div class="readable-text" id="p92">
<h3 class="readable-text-h3"><span class="num-string">1.3.1</span> Recommendation engines</h3>
</div>
<div class="readable-text" id="p93">
<p>Enterprise graphs can exceed billions of nodes and many billions of edges. On the other hand, many GNNs are benchmarked on datasets that consist of fewer than a million nodes. When applying GNNs to large graphs, adjustments of the training and inference algorithms and storage techniques all have to be made. (You can learn more about the specifics of scaling GNNs in chapter 7.)</p>
</div>
<div class="readable-text intended-text" id="p94">
<p>One of the most well-known industry examples of GNNs is their use as recommendation engines. For instance, Pinterest is a social media platform for finding and sharing images and ideas. There are two major concepts to Pinterest’s users: collections or categories of ideas, called <em>boards</em> (like a bulletin board); and objects a user wants to bookmark called <em>pins</em>. Pins include images, videos, and website URLs. A user board focused on dogs might then include pins of pet photos, puppy videos, or dog-related website links. A board’s pins aren’t exclusive to it; a pet drawing that was pinned to the Dogs board could also be pinned to a Puppies board, as shown in figure 1.10.</p>
</div>
<div class="browsable-container figure-container" id="p97">
<img alt="figure" height="686" src="../Images/1-10.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.10</span> A bipartite graph that is like the Pinterest graph. Nodes in this case are the pins and boards. </h5>
</div>
<div class="readable-text intended-text" id="p95">
<p>As of this writing, Pinterest has 400 million active users who have likely pinned tens if not hundreds of items per user. One imperative of Pinterest is to help their users find content of interest via recommendations. Such recommendations should not only take into account image data and user tags but also draw insights from the relationships between pins and boards.</p>
</div>
<div class="readable-text intended-text" id="p96">
<p>One way to interpret the relationships between pins and boards is as a <em>bipartite graph</em>, which we discussed earlier. For the Pinterest graph, all the pins are connected to boards, but no pin is connected to another pin, and no board is connected to another board. Pins and boards are two classes of nodes. Members of these classes can be linked to members of the other class, but not to members of the same class. The Pinterest graph was reported to have 3 billion nodes and 18 billion edges.<span class="aframe-location"/></p>
</div>
<div class="readable-text intended-text" id="p98">
<p>PinSage, a graph convolutional network (GCN), was one of the first documented highly scaled GNNs used in an enterprise system [9]. This was used in Pinterest’s recommendation systems to overcome past challenges of applying graph-learning models to massive graphs. Compared to baseline methods, tests on this system showed it improved user engagement by 30%. Specifically, PinSage was used to predict which objects should be recommended to be included in a user’s graph. However, GNNs can also be used to predict what an object is, such as whether it contains a dog or mountain, based on the rest of the nodes in the graph and how they are connected. We’ll be doing a deep dive on GCNs, of which PinSage is an extension, in chapter 3.</p>
</div>
<div class="readable-text" id="p99">
<h3 class="readable-text-h3"><span class="num-string">1.3.2</span> Drug discovery and molecular science</h3>
</div>
<div class="readable-text" id="p100">
<p>In chemistry and molecular sciences, a prominent problem has been representing molecules in a general, application-agnostic way, and inferring possible interfaces between molecules, such as proteins. For molecule representation, we can see that the drawings of molecules that are common in high school chemistry classes bear resemblance to a graph structure, consisting of nodes (atoms) and edges (atomic bonds), as shown in figure 1.11.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p101">
<img alt="figure" height="292" src="../Images/1-11.png" width="422"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.11</span> In this molecule, we can see individual atoms as nodes and the atomic bonds as edges.</h5>
</div>
<div class="readable-text intended-text" id="p102">
<p>Applying GNNs to these structures can, in certain circumstances, outperform traditional “fingerprint” methods for determining the properties of a molecule. These traditional methods involve the creation of features by domain experts to capture a molecule’s properties, such as interpreting the presence or absence of certain molecules or atoms [10]. GNNs learn new data-driven features that can be used to group certain molecules together in new and unexpected ways or even to propose new molecules for synthesis. This is extremely important for predicting whether a chemical is toxic or safe for use or whether it has some downstream effects that can affect disease progression. Therefore, GNNs have shown themselves to be incredibly useful in the field of drug discovery. </p>
</div>
<div class="readable-text intended-text" id="p103">
<p>Drug discovery, especially for GNNs, can be understood as a graph prediction problem. <em>Graph prediction</em> tasks are those that require learning and predicting properties about the entire graph. For drug discovery, the aim is to predict properties such as toxicity or treatment effectiveness (discriminative) or to suggest entirely new graphs that should be synthesized and tested (generative). To suggest these new graphs, drug discovery methods often combine GNNs with other generative models such as variational graph autoencoders (VGAEs), as shown, for example, in figure 1.12. We’ll describe VGAEs in more detail in chapter 5 and show how we can use these to predict molecules.<span class="aframe-location"/> </p>
</div>
<div class="browsable-container figure-container" id="p104">
<img alt="figure" height="419" src="../Images/1-12.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.12</span> A GNN system used to predict new molecules [11]. The workflow here starts on the left with a representation of a molecule as a graph. In the middle parts of the figure, this graph representation is transformed via a GNN into a latent representation. The latent representation is then transformed back to the molecule to ensure that the latent space can be decoded (right).</h5>
</div>
<div class="readable-text" id="p105">
<h3 class="readable-text-h3"><span class="num-string">1.3.3</span> Mechanical reasoning</h3>
</div>
<div class="readable-text" id="p106">
<p>We develop rudimentary intuition about mechanics and physics of the world around us at a remarkably young age and without any formal training in the subject. We don’t need to write down a set of equations to know how to catch a bouncing ball. We don’t even have to be in the presence of a physical ball. Given a series of snapshots of a bouncing ball, we can predict reasonably well where the ball is going to end up. </p>
</div>
<div class="readable-text intended-text" id="p107">
<p>While these problems might seem trivial for us, they are critical for many physical industries, including manufacturing and autonomous driving. For example, autonomous driving systems need to anticipate what will happen in a traffic scene consisting of many moving objects. Until recently, this task was typically treated as a problem of computer vision. However, more recent approaches have begun to use GNNs [12]. These GNN-based methods demonstrate that including relational information, such as how limbs are connected, can enable algorithms to develop physical intuition about how a person or animal moves with higher accuracy and less data. </p>
</div>
<div class="readable-text intended-text" id="p108">
<p>In figure 1.13, we give an example of how a body can be thought of as a “mechanical” graph. The input graphs for these physical reasoning systems have elements that reflect the problem. For instance, when reasoning about a human or animal body, a graph could consist of nodes that represent points on the body where limbs connect. For systems of free bodies, the nodes of a graph could be individual objects such as bouncing balls. The edges of the graph then represent the physical relationship (e.g., gravitational forces, elastic springs, or rigid connections) between the nodes. Given these inputs, GNNs learn to predict future states of a set of objects without explicitly calling on physical/mechanical laws [13]. These methods are a form of <em>edge prediction</em>; that is, they predict how the nodes connect over time. Furthermore, these models have to be dynamic to account for the temporal evolution of the system. We consider these problems in detail in chapter 6.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p109">
<img alt="figure" height="254" src="../Images/1-13.png" width="362"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.13</span> A graph representation of a mechanical body, taken from Sanchez-Gonzalez [13]. The body’s segments are represented as nodes, and the mechanical forces binding them are edges.</h5>
</div>
<div class="readable-text" id="p110">
<h2 class="readable-text-h2"><span class="num-string">1.4</span> When to use a GNN?</h2>
</div>
<div class="readable-text" id="p111">
<p>Now that we’ve explored real-world applications of GNNs, let’s identify some underlying characteristics that make problems suitable for graph-based solutions. While the cases of the previous section clearly involved data that was naturally modeled as a graph, it’s crucial to recognize that GNNs can also be effectively applied to problems where the graph-like nature may not be immediately obvious.</p>
</div>
<div class="readable-text intended-text" id="p112">
<p>So, instead of simply stating that GNNs are useful for graph problems, this section will help you recognize patterns and relationships within your data that could benefit from graph-based modeling, even if those relationships aren’t immediately apparent. Essentially, there are three types of criteria for identifying GNN problems: implicit relationships and interdependencies; high dimensionality and sparsity; and complex nonlocal interactions. </p>
</div>
<div class="readable-text" id="p113">
<h3 class="readable-text-h3"><span class="num-string">1.4.1</span> Implicit relationships and interdependencies</h3>
</div>
<div class="readable-text" id="p114">
<p>Graphs are versatile data structures that can model a wide range of relationships. Even when a problem doesn’t initially appear to be graph-like, even if your dataset is tabular, it’s beneficial to explore whether implicit relationships or interdependencies might exist that could be represented explicitly. Implicit relationships are connections that aren’t immediately documented or obvious within the data but can still play a significant role in understanding the underlying patterns and behaviors.</p>
</div>
<div class="readable-text" id="p115">
<h4 class="readable-text-h4">Key indicators</h4>
</div>
<div class="readable-text" id="p116">
<p>To determine if your problem might benefit from modeling implicit relationships with graphs, consider whether there are hidden or indirect connections between entities in your dataset. For example, in customer behavior analysis, customers may appear as independent entities in a tabular dataset containing their purchases, demographics, and other details. However, they could be connected through social media influence, peer recommendations, or shared purchasing patterns, forming an underlying network of interactions.</p>
</div>
<div class="readable-text intended-text" id="p117">
<p>Another indicator is the presence of entities that share common attributes or activities without a direct or documented relationship. In the case of investors, for example, two or more investors may not have any formal connection but might frequently co-invest in the same companies under similar conditions. Such patterns of co-investment could indicate a shared strategy or influence. In this scenario, a graph representation can be created where nodes represent individual investors, and edges are formed between nodes when two or more investors co-invest in the same company. Additional attributes, such as investment size, timing, or the types of companies invested in can be added to nodes or edges, allowing GNNs to identify patterns, trends, or even potential collaboration opportunities.</p>
</div>
<div class="readable-text intended-text" id="p118">
<p>Additionally, consider whether the data involves entities that are interconnected through shared references or co-occurrence patterns. Document and text data may not immediately suggest a graph structure, but if documents cite each other or share common topics or authors, they can be represented as nodes in a graph, with edges reflecting these relationships. Similarly, terms within documents can form co-occurrence networks, which are useful for tasks such as keyword extraction, document classification, or topic modeling.</p>
</div>
<div class="readable-text intended-text" id="p119">
<p>By identifying these key indicators in your data, you can uncover hidden or implicit relationships that can be represented explicitly through graphs. Such representations allow for more advanced analyses using GNNs, which can effectively capture and model these relationships, leading to more accurate predictions and deeper insights into the data.</p>
</div>
<div class="readable-text" id="p120">
<h3 class="readable-text-h3"><span class="num-string">1.4.2</span> High dimensionality and sparsity</h3>
</div>
<div class="readable-text" id="p121">
<p>Graph-based models are particularly effective in handling high-dimensional data where many features may be sparse or missing. These models excel in situations where there are underlying structures connecting sparse entities, allowing for more meaningful analysis and improved performance.</p>
</div>
<div class="readable-text" id="p122">
<h4 class="readable-text-h4">Key indicators</h4>
</div>
<div class="readable-text" id="p123">
<p>To determine if your problem involves high-dimensional and sparse data suitable for GNNs, consider whether your dataset contains numerous entities with limited direct interactions or relationships. For example, in recommender systems, user-item interaction data may appear tabular, but it’s inherently sparse—most users only interact with a small subset of the available items. By representing users and items as nodes and representing their interactions (e.g., purchases or clicks) as edges, GNNs can exploit network effects to make more accurate recommendations. These models can also address the cold-start problem by uncovering both explicit and implicit relationships, leading to better performance in recommending new items to users or engaging new users with existing items.</p>
</div>
<div class="readable-text intended-text" id="p124">
<p>Another indicator that your problem may be suitable for graph-based models is when the data represents entities that are sparsely connected but share significant characteristics. In drug discovery, for example, molecules are represented as graphs, with atoms as nodes and chemical bonds as edges. This representation captures the inherent sparsity of molecular structures, where most atoms form only a few bonds, and large portions of the molecule may be distant from each other in the graph. Traditional machine learning methods often struggle to predict properties of new molecules due to this sparsity, as they don’t account for the full structural context.</p>
</div>
<div class="readable-text intended-text" id="p125">
<p>Graph-based models, particularly GNNs, overcome these challenges by capturing both local atomic environments and global molecular structures. GNNs learn hierarchical features from fine-grained atomic interactions to broader molecular properties, and their ability to remain invariant to the ordering of atoms ensures consistent predictions. By using the graph structure of molecules, GNNs make accurate predictions from sparse, connected data, thereby accelerating the drug discovery process.</p>
</div>
<div class="readable-text intended-text" id="p126">
<p>By recognizing these key indicators in your data, you can identify situations where graph-based models can effectively handle high-dimensional and sparse datasets. Representing such data as graphs allows GNNs to capture and use underlying structures, resulting in more accurate predictions and deeper insights across various applications.</p>
</div>
<div class="readable-text" id="p127">
<h3 class="readable-text-h3"><span class="num-string">1.4.3</span> Complex, nonlocal interactions</h3>
</div>
<div class="readable-text" id="p128">
<p>Certain problems require understanding how distant elements in a dataset influence each other. In these cases, GNNs provide a framework to capture these complex interactions, where the predicted value or label of a particular data point depends not just on the features of its immediate neighbors but also on those of other related data points. This capability is especially useful when relationships extend beyond direct connections to involve multiple levels or degrees of separation.</p>
</div>
<div class="readable-text intended-text" id="p129">
<p>However, some standard GNNs, which rely primarily on local message passing, may struggle to capture long-range dependencies effectively. Advanced architectures or modifications, such as those incorporating global attention, nonlocal aggregation, or hierarchical message-passing, can better address these challenges [14].</p>
</div>
<div class="readable-text" id="p130">
<h4 class="readable-text-h4">Key indicators</h4>
</div>
<div class="readable-text" id="p131">
<p>To determine if your problem involves complex, nonlocal interactions suitable for GNNs, consider whether the outcome or behavior of one entity depends on the attributes or actions of entities that aren’t directly connected to it but may be indirectly connected through other entities. For example, in supply chain optimization, a delay in one supplier may not only affect its immediate downstream customers but could cascade through multiple levels of the network, influencing distributors and final consumers.</p>
</div>
<div class="readable-text intended-text" id="p132">
<p>Another indicator is whether the problem involves scenarios where information, influence, or effects propagate through a network over time. In healthcare and epidemiology, for instance, a disease outbreak might spread from a small cluster of patients through their interactions with shared healthcare providers, common environments, or overlapping social networks. Such propagation requires an approach that captures the indirect transmission pathways of information or effects.</p>
</div>
<div class="readable-text intended-text" id="p133">
<p>To close this section, in determining whether your problem is a good candidate for a GNN, ask yourself these questions:</p>
</div>
<ul>
<li class="readable-text" id="p134"> Are there implicit relationships or interdependencies in my data that I could model? </li>
<li class="readable-text" id="p135"> Do the interactions between entities exhibit complex, nonlocal dependencies that go beyond immediate connections? </li>
<li class="readable-text" id="p136"> Is the data high-dimensional and sparse, with a need to capture underlying relational structures? </li>
</ul>
<div class="readable-text" id="p137">
<p>If the answer to any of these questions is yes, consider framing your problem as a graph and applying GNNs to unlock new insights and predictive capabilities.</p>
</div>
<div class="readable-text" id="p138">
<h2 class="readable-text-h2"><span class="num-string">1.5</span> Understanding how GNNs operate</h2>
</div>
<div class="readable-text" id="p139">
<p>In this section, we’ll explore how GNNs work, starting from the initial collection of raw data to the final deployment of trained models. We’ll examine each step, highlighting the processes of data handling, model building, and the unique message-passing technique that sets GNNs apart from traditional deep learning models. </p>
</div>
<div class="readable-text" id="p140">
<h3 class="readable-text-h3"><span class="num-string">1.5.1</span> Mental model for training a GNN</h3>
</div>
<div class="readable-text" id="p141">
<p>Our mental model covers the data sourcing, graph representation, preprocessing, and model development workflow. We start with raw data and end up with a trained GNN model and its outputs. Figure 1.14 illustrates and visualizes topics related to these stages, annotated with the chapters in which these topics appear.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p142">
<img alt="figure" height="539" src="../Images/1-14.png" width="1009"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.14</span> Mental model of the GNN project. We start with raw data, which is transformed into a graph data model that can be stored in a graph database or used in a graph processing system. From the graph processing system (and some graph databases), exploratory data analysis and visualization can be done. Finally, for graph machine learning, data is preprocessed into a form that can be submitted for training.</h5>
</div>
<div class="readable-text" id="p143">
<p>While not all workflows include every step or stage of this process, most will incorporate at least some elements. At different stages of a model development project, different parts of this process will typically be used. For example, when <em>training</em> a model, data analysis and visualization may be needed to make design decisions, but when <em>deploying</em> a model, it may only be necessary to stream raw data and quickly preprocess it for ingestion into a model. Though this book touches on the earlier stages in this mental model, the bulk of the book is focused on how to train different types of GNNs. When the other topics are discussed, they serve to support this main focus.</p>
</div>
<div class="readable-text intended-text" id="p144">
<p>The mental model shows the core tasks of applying GNNs to machine learning problems, and we’ll be returning to this process repeatedly throughout the rest of the book. Let’s examine this diagram from end to end. </p>
</div>
<div class="readable-text intended-text" id="p145">
<p>The first step in training a GNN is structuring this raw data into a graph format, if it isn’t already. This requires deciding which entities in the data to represent as nodes and edges, as well as determining the features to assign to them. Decisions must also be made about data storage—whether to use a graph database, processing system, or other formats.</p>
</div>
<div class="readable-text intended-text" id="p146">
<p>For machine learning, the data must be preprocessed for training and inference, involving tasks such as sampling, batching, and splitting the data into training, validation, and test sets. Throughout this book, we use PyTorch Geometric (PyG), which offers specialized classes for preprocessing and data splitting while preserving the graph’s structure. Preprocessing is covered in most chapters, with more in-depth explanations available in appendix B.</p>
</div>
<div class="readable-text intended-text" id="p147">
<p>After processing the data, we can then move on to the model training. In this book, we cover several architectures and training types:</p>
</div>
<ul>
<li class="readable-text" id="p148"> Chapters 2 and 3 discuss convolutional GNNs, where we first use a GCN layer to produce graph embeddings (chapter 2) and then train a full GCN and GraphSAGE models (chapter 3). </li>
<li class="readable-text" id="p149"> Chapter 4 explains graph attention networks (GATs), which adds attention to our GNNs. </li>
<li class="readable-text" id="p150"> Chapter 5 introduces GNNs for unsupervised and generative problems, where we train and use a variational graph autoencoder (VGAE). </li>
<li class="readable-text" id="p151"> Chapter 6 then explores the advanced concept of spatiotemporal GNNs, based on graphs that evolve over time. We train a neural relational inference (NRI) model, which combines an autoencoder structure with a recurrent neural network. </li>
</ul>
<div class="readable-text list-body-item" id="p152">
<p>Most of the examples provided for the GNNs mentioned so far are illustrated with code examples which use small-scale graphs that can fit into memory on a laptop or desktop computer. </p>
</div>
<ul>
<li class="readable-text" id="p153"> In chapter 7, we delve into strategies for handling data that exceeds the processing capacity of a single machine. </li>
<li class="readable-text" id="p154"> In chapter 8, we close with some considerations for graph and GNN projects, such as practical aspects of working with graph data, as well as how to convert nongraph data into a graph format. </li>
</ul>
<div class="readable-text" id="p155">
<h3 class="readable-text-h3"><span class="num-string">1.5.2</span> Unique mechanisms of a GNN model</h3>
</div>
<div class="readable-text" id="p156">
<p>Although there are a variety of GNN architectures at this point, they all tackle the same problem of dealing with graph data in a way that is permutation invariant. They do this via encoding and exchanging information across the graph structure during the learning process.</p>
</div>
<div class="readable-text intended-text" id="p157">
<p>In a conventional neural network, we first need to initialize a set of parameters and functions. These include the number of layers, the size of the layers, the learning rate, the loss function, the batch size, and other hyperparameters. (These are all treated in detail in other books on deep learning, so we assume you’re familiar with these terms.) Once we’ve defined these features, we then train our network by iteratively updating the weights of the network, as shown in figure 1.15.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p158">
<img alt="figure" height="432" src="../Images/1-15.png" width="922"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.15</span> Process for training a GNN, which is similar to training most other deep learning models</h5>
</div>
<div class="readable-text" id="p159">
<p>Explicitly, we perform the following steps:</p>
</div>
<ol>
<li class="readable-text" id="p160"> Input our data. </li>
<li class="readable-text" id="p161"> Pass the data through neural network layers that transform the data according to the parameters of the layer and an activation rule. </li>
<li class="readable-text" id="p162"> Output a representation from the final layer of the network. </li>
<li class="readable-text" id="p163"> Backpropagate the error, and adjust the parameters accordingly. </li>
<li class="readable-text" id="p164"> Repeat these steps a fixed number of <em>epochs</em> (the process by which data is passed forward and backward to train a neural network). </li>
</ol>
<div class="readable-text" id="p165">
<p>For tabular data, these steps are exactly as listed, as shown in figure 1.16. For graph-based or relational data, these steps are similar except that each epoch relates to one iteration of message passing, which is described in the next subsection. </p>
</div>
<div class="browsable-container figure-container" id="p169">
<img alt="figure" height="819" src="../Images/1-16.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.16</span> Comparison of (simple) non-GNN (above) and GNN (below). GNNs have a layer that distributes data among its vertices.</h5>
</div>
<div class="readable-text" id="p166">
<h3 class="readable-text-h3"><span class="num-string">1.5.3</span> Message passing</h3>
</div>
<div class="readable-text" id="p167">
<p><em>Message passing</em>, which is touched on throughout the book, is a central mechanism in GNNs that enables nodes to communicate and share information across a graph [15]. This process allows GNNs to learn rich, informative representations of graph-structured data, which is essential for tasks such as node classification, link prediction, and graph-level prediction. Figure 1.17 illustrates the steps involved in a typical message-passing layer.</p>
</div>
<div class="browsable-container figure-container" id="p170">
<img alt="figure" height="235" src="../Images/1-17.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 1.17</span> Elements of our message passing layer. Each message passing layer consists of an aggregation, a transformation, and an update step.</h5>
</div>
<div class="readable-text intended-text" id="p168">
<p>The message-passing process begins with the Input (step 1) of the initial graph, where every node and edge have their own features. In the Collect step (step 2), each node gathers information from its immediate neighbors—these pieces of information are referred to as “messages.” This step ensures that each node has access to the features of its neighbors, which are crucial for understanding the local graph structure.<span class="aframe-location"/><span class="aframe-location"/></p>
</div>
<div class="readable-text" id="p171">
<p>Next, in the Aggregate step (step 3), the collected messages from neighboring nodes are combined using an invariant function, such as sum, mean, or max. This aggregation consolidates the information from a node’s neighborhood into a single vector, capturing the most relevant details about its local environment.</p>
</div>
<div class="readable-text intended-text" id="p172">
<p>In the Transform step (step 4), the aggregated messages are processed by a neural network to produce a new representation for each node. This transformation allows the GNN to learn complex interactions and patterns within the graph by applying nonlinear functions to the aggregated information.</p>
</div>
<div class="readable-text intended-text" id="p173">
<p>Finally, during the Update step (step 5), the features of each node in the graph are replaced or updated with these new representations. This completes one round of message passing, incorporating information from neighboring nodes to refine each node’s features.</p>
</div>
<div class="readable-text intended-text" id="p174">
<p>Each message-passing layer in a GNN allows nodes to gather information from nodes that are further away, or more “hops” away, in the graph. Repeating these steps over multiple layers enables the GNN to capture more complex dependencies and long-range interactions within the graph.</p>
</div>
<div class="readable-text intended-text" id="p175">
<p>By using message passing, GNNs efficiently encode the graph structure and data into useful representations for a variety of downstream tasks. Advanced architectures, such as those incorporating global attention or hierarchical message passing, further enhance the model’s ability to capture long-range dependencies across the graph, enabling more robust performance on diverse applications.</p>
</div>
<div class="readable-text" id="p176">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p177"> Graph neural networks (GNNs) are specialized tools for handling relational, or relationship-centric, data, particularly in scenarios where traditional neural networks struggle due to the complexity and diversity of graph structures. </li>
<li class="readable-text" id="p178"> GNNs have found significant applications in areas such as recommendation engines, drug discovery, and mechanical reasoning, showcasing their versatility in handling large and complex relational data for enhanced insights and predictions. </li>
<li class="readable-text" id="p179"> Specific GNN tasks include node prediction, edge prediction, graph prediction, and graph representation through embedding techniques. </li>
<li class="readable-text" id="p180"> GNNs are best used when data is represented as a graph, indicating a strong emphasis on relationships and connections between data points. They aren’t ideal for individual, standalone data entries where relational information is insignificant. </li>
<li class="readable-text" id="p181"> When deciding if a GNN solution is a good fit for your problem, consider cases that have characteristics such as implicit relationships, high-dimensionality, sparsity, and complex nonlocal interactions. By understanding these fundamentals, practitioners can evaluate the suitability of GNNs for their specific problems, implement them effectively, and recognize their tradeoffs and limitations in real-world applications. </li>
<li class="readable-text" id="p182"> Message passing is a core mechanism of GNNs, which enables them to encode and exchange information across a graph’s structure, allowing for meaningful node, edge, and graph-level predictions. Each layer of a GNN represents one step of message passing, with various aggregation functions to combine messages effectively, providing insights and representations useful for machine learning tasks. </li>
</ul>
</div></body></html>