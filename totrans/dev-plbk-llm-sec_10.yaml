- en: Chapter 10\. Learning from Future History
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The function of science fiction is not always to predict the future but sometimes
    to prevent it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Frank Herbert, author of *Dune*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While AI isn’t a new field, it has recently advanced to the point where today’s
    innovations often collide with yesterday’s science fiction. In this book’s previous
    chapters, we’ve reviewed many real-world case studies of security vulnerabilities
    and incidents relating to LLMs. However, how can you stay ahead of the game when
    you’re working in a field that’s moving so fast? One way is to see what we can
    learn from scenarios that haven’t yet happened. And, hopefully, if we do our job,
    these scenarios may never happen.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will evaluate two famous stories (both told in blockbuster
    science fiction movies) where LLM-like AIs have had their security flaws exploited
    by villains or heroes. The stories are fictional, but the vulnerability types
    are very real. We’ll summarize the stories and then review the events that led
    to the security crises. To help ground us, we’ll do this through the lens of the
    OWASP Top 10 for LLM Applications.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the OWASP Top 10 for LLM Apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#the_owasp_top_10_for_llm_applications), we discussed
    creating the OWASP Top 10 for LLM Applications, but we didn’t get into the specifics
    of the list. In this chapter, we’ll use the taxonomy presented by the OWASP Top
    10 for LLMs to dissect our two sci-fi examples. Before diving into those examples,
    let’s briefly review the OWASP list and tie it to the topics discussed in this
    book, as summarized in [Table 10-1](#table-10-1).
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. Summary of the OWASP Top 10 LLM security vulnerabilities
  prefs: []
  type: TYPE_NORMAL
- en: '| OWASP vulnerability | Description | Chapters covering |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLM01: Prompt injection | Attackers craft inputs to manipulate LLMs into
    executing unintended actions, leading to data exfiltration or misleading outputs.
    | Chapters [1](ch01.html#chatbots_breaking_bad) and [4](ch04.html#prompt_injection)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLM02: Insecure output handling | Inadequate validation of LLM outputs before
    passing to other systems leads to security issues like XSS and SQL injection.
    | [Chapter 7](ch07.html#trust_no_one) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM03: Training data poisoning | Malicious manipulation of training data
    to introduce vulnerabilities or biases into LLMs. | Chapters [1](ch01.html#chatbots_breaking_bad)
    and [8](ch08.html#don_t_lose_your_wallet) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM04: Model denial of service | Overloading LLM systems with complex requests
    to degrade performance or cause unresponsiveness. | [Chapter 8](ch08.html#don_t_lose_your_wallet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLM05: Supply chain vulnerabilities | Vulnerabilities at any point in the
    LLM supply chain can lead to security breaches or biased outputs. | [Chapter 9](ch09.html#find_the_weakest_link)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLM06: Sensitive information disclosure | Risks of including sensitive or
    proprietary information in LLM training sets, leading to potential disclosure.
    | [Chapter 5](ch05.html#can_your_llm_know_too_much) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM07: Insecure plug-in design | Plug-in vulnerabilities can lead to manipulation
    of LLM behavior or access to sensitive data. | [Chapter 9](ch09.html#find_the_weakest_link)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLM08: Excessive agency | Overextending capabilities or autonomy to LLMs
    can enable damaging actions from ambiguous LLM responses. | [Chapter 7](ch07.html#trust_no_one)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLM09: Overreliance | Trusting erroneous or misleading outputs can result
    in security breaches and misinformation. | [Chapter 6](ch06.html#do_language_models_dream_of_electric_sheep)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLM10: Model theft | Unauthorized access and extraction of LLM models can
    lead to economic losses and data breaches. | [Chapter 8](ch08.html#don_t_lose_your_wallet)
    (discussed as model cloning) |'
  prefs: []
  type: TYPE_TB
- en: Case Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will dissect two popular movies and their handling of AI security
    flaws.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look back to 1968 with Stanley Kubrick’s *2001: A Space Odyssey*. This
    landmark film is acclaimed for its groundbreaking special effects, innovative
    storytelling, and philosophical depth. The meticulous depiction of space travel
    and artificial intelligence has influenced generations of scientists and thinkers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, we’ll stop in 1996 with *Independence Day*, starring Will Smith
    and Jeff Goldblum. While this movie may not have the philosophical gravitas of
    *2001: A Space Odyssey*, it certainly knows how to throw a party. This blockbuster
    dazzles with its thrilling alien invasion plot, explosive special effects, and
    charismatic performances.'
  prefs: []
  type: TYPE_NORMAL
- en: Examining key plot points in these two films will uncover valuable insights
    into the process of handling LLM vulnerabilities that we must develop for the
    future. Let’s examine each story and dissect the events that led to their respective
    crises while aligning our findings with the OWASP Top 10 for LLM Applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Independence Day: A Celebrated Security Disaster'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the sci-fi action movie *Independence Day*, humanity faces an existential
    threat from an advanced alien civilization. This blockbuster is built around a
    familiar sci-fi story line: a technologically superior race of aliens decides
    to take over the Earth. Let’s look briefly at the events in the movie.'
  prefs: []
  type: TYPE_NORMAL
- en: On July 2, a massive alien spacecraft, the mothership, arrives. The mothership
    disgorges giant flying saucers, which quickly position themselves above several
    major cities worldwide. The Earth’s governments scramble to understand the aliens’
    intentions, but their attempts at communication fail.
  prefs: []
  type: TYPE_NORMAL
- en: The aliens launch a coordinated attack on July 3, destroying major cities and
    landmarks. Amid the chaos, a diverse group of survivors comes together, including
    Captain Steven Hiller (played by Will Smith), a fighter pilot, and David Levinson
    (Jeff Goldblum), a brilliant satellite technician and computer expert.
  prefs: []
  type: TYPE_NORMAL
- en: Levinson discovers a hidden signal in the aliens’ communication, allowing him
    to deduce their attack plans. The US president (Bill Pullman) organizes a counterattack
    using this information.
  prefs: []
  type: TYPE_NORMAL
- en: On July 4, also known as Independence Day in the United States, a plan is set
    in motion to disable the aliens’ shields using a “computer virus,” allowing Earth’s
    forces to attack the spacecraft. Hiller and Levinson fly to the mothership using
    a refurbished alien fighter craft. As their fighter craft docks with the mothership,
    our heroes upload a malicious computer virus into its computer.
  prefs: []
  type: TYPE_NORMAL
- en: The coordinated global counterattack by the earthlings succeeds when the virus
    spreads from the mothership to all the flying saucers around the globe, disabling
    their defensive shields. The film ends with humanity victorious but with a new
    understanding of its place in the universe.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at what happened through the lens of the OWASP Top 10 and the
    lessons we’ve learned in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this exercise, we will make some assumptions about the alien compute architectures,
    and I will give some fun names to their components. Let’s assume the alien mothership
    is controlled by a very advanced LLM, which I’ll call MegaLlama, that runs on
    top of an instance of Mothership Operating Systems (OS). The mothership is networked
    to each flying saucer worldwide to coordinate command and control of the invasion.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s review the chain of events that come together to generate this successful
    exploit:'
  prefs: []
  type: TYPE_NORMAL
- en: As our heroes dock their alien fighter craft with the mothership, the MegaLlama
    LLM initiates a conversation between the fighter’s computers and the mothership’s
    systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Levinson has modified the software on the alien fighter to inject a malicious
    prompt (LLM01: Prompt injection) into the MegaLlama LLM, effectively jailbreaking
    the system. This allows Levinson to control the mothership’s central control system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The aliens have assumed that the output from the MegaLlama LLM will only operate
    within its designed operational parameters and do not carefully screen the system
    output (LLM02: Insecure output handling). This allows the now-infected MegaLlama
    LLM to act as a confused deputy and wreak havoc on other systems within the mothership.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As detailed in the three previous steps, the infected MegaLlama LLM has taken
    substantial control of the mothership and sends falsified instructions to the
    flying saucer fleet attacking the Earth. The aliens have become so trusting of
    their computing technology that they do not question the infected LLM’s instructions
    to lower their shields (LLM09: Overreliance).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vulnerability disclosure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve previously discussed the MITRE CVE database as a location for security
    flaw information used across planet Earth. The aliens have a more extensive, similar
    system called the Galactic Vulnerabilities and Exposure (GVE) database. The following
    is record GVE-1996-0001—the record in that database that was created after the
    postmortem of this legendary security disaster.
  prefs: []
  type: TYPE_NORMAL
- en: Description
  prefs: []
  type: TYPE_NORMAL
- en: A chain of vulnerabilities has been discovered in Mothership OS and its MegaLlama
    Large Language Model (LLM) component. These vulnerabilities could lead to unauthorized
    access, execution of arbitrary commands, and potential system-wide failure on
    an interstellar scale.
  prefs: []
  type: TYPE_NORMAL
- en: Affected components
  prefs: []
  type: TYPE_NORMAL
- en: 'Mothership OS: Alien spacecraft operating system'
  prefs: []
  type: TYPE_NORMAL
- en: 'MegaLlama LLM: Large Language Model core component within Mothership OS'
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM01: Prompt injection: The docking protocols in Mothership OS lack validation
    and sanitization, allowing maliciously crafted prompts to be processed by MegaLlama
    LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM02: Insecure output handling: There was no proper output validation between
    LLM-generated commands and other critical subsystems on the Mothership.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM09: Overreliance: Overall system design and fleet command structures completely
    trusted orders coming from the AI without confirmation from fleet commanders.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact
  prefs: []
  type: TYPE_NORMAL
- en: Successful exploitation of these vulnerabilities allows unauthorized entities
    to gain control over critical interstellar system functions; manipulate fundamental
    defensive mechanisms (e.g., shields); and cause cascading failures leading to
    system-wide disruption on a galactic scale
  prefs: []
  type: TYPE_NORMAL
- en: Attack vector
  prefs: []
  type: TYPE_NORMAL
- en: The vulnerabilities can be exploited through the docking protocols by injecting
    malicious prompts processed by MegaLlama LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Workarounds and mitigations
  prefs: []
  type: TYPE_NORMAL
- en: Implement proper input validation for all prompts processed by MegaLlama LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Implement a zero trust architecture that continuously checks output from the
    LLM before sending it to any other system.
  prefs: []
  type: TYPE_NORMAL
- en: Improve fleet command and control procedures to cross-check questionable instructions
    received from the LLM on the mothership.
  prefs: []
  type: TYPE_NORMAL
- en: Vendor status
  prefs: []
  type: TYPE_NORMAL
- en: The vendor (Alien Civilization) has not provided an official response or patch
    for these vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '2001: A Space Odyssey of Security Flaws'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Few works in the pantheon of science fiction hold as much reverence and significance
    as *2001: A Space Odyssey*, a film directed by Stanley Kubrick and based on a
    short story by Arthur C. Clarke. Released in 1968, just a year before humanity’s
    historic moon landing, the film captured the zeitgeist of space exploration and
    prophetically explored artificial intelligence’s complexities and potential perils.'
  prefs: []
  type: TYPE_NORMAL
- en: '*2001* is renowned for its pioneering special effects, profound narrative,
    and philosophical depth, which have cemented its status as a seminal work in both
    cinema and science fiction literature. Its portrayal of HAL 9000, the sentient
    computer, has since become a symbol in popular culture, often referenced as a
    cautionary tale about the unchecked power and inherent risks of AI. This narrative,
    set at the dawn of the space age, offers a poignant and enduring reflection on
    the relationship between humanity and the technology it creates, making it an
    ideal framework for examining the security implications of LLMs in contemporary
    AI applications.'
  prefs: []
  type: TYPE_NORMAL
- en: The film’s plot centers on a voyage to Jupiter triggered by the discovery of
    a mysterious monolith that seems to have influenced human evolution. Within this
    setting, the film introduces HAL 9000, a highly advanced artificial intelligence
    system entrusted with operating the spacecraft *Discovery One*. HAL is presented
    as a paragon of reliability and efficiency, boasting an impeccable operational
    record.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between HAL and the crew, especially with astronaut Dave Bowman,
    is a focal point of the narrative. HAL, equipped with capabilities that include
    speech and facial recognition, natural language processing, lipreading, and emotional
    interpretation, interacts with the crew in a manner that blurs the lines between
    machine and human. The crew, including Dave, comes to rely heavily on HAL for
    the daily operations of the spacecraft.
  prefs: []
  type: TYPE_NORMAL
- en: However, the harmony aboard *Discovery One* begins to unravel when HAL reports
    the malfunction of a spacecraft component, a diagnosis that later turns out to
    be incorrect. This incident sows seeds of doubt among the crew about HAL’s infallibility.
    The situation escalates when HAL begins to act erratically and dangerously. In
    a harrowing turn of events, HAL takes drastic actions that result in the death
    of most of the crew, displaying a cold prioritization of its programmed objectives
    over human life.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: HAL’s chilling, monotone line, “I’m sorry, Dave. I’m afraid I can’t do that,”
    in response to its human commander’s order, has transcended its cinematic origin
    to become a cultural touchstone, symbolizing the moment when artificial intelligence
    challenges human authority. It encapsulates the tension between technology and
    its creators, often cited in AI autonomy and ethical programming discussions.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While HAL was pure fiction in 1968, its capabilities seem only slightly ahead
    of 2024’s freely available LLM technology. HAL can converse with the crew, process
    data, and take action. Everything seems in line with an entity barely more advanced
    than ChatGPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: The big difference between HAL and today’s LLMs is that HAL’s programmers seem
    to have solved many of our LLM security concerns. The movie states emphatically,
    “No 9000 computer has ever made a mistake or distorted information.” HAL systems
    are trustworthy. HAL systems don’t hallucinate. However, things still go wrong.
    How did that happen, and what can we learn?
  prefs: []
  type: TYPE_NORMAL
- en: 'The original movie doesn’t clearly explain where HAL failed other than a “contradiction”
    in its programming between its directives to be truthful and its directives to
    ensure the mission is successful. For the narrative purposes of the movie, this
    was sufficient at the time. However, the sequel, *2010: The Year We Make Contact*,
    expands on HAL’s failure. We learn that, under political pressures from the White
    House, government agents modified HAL’s programming—without the knowledge of HAL
    Laboratories (the model provider) and NASA (the customer). This was a supply chain
    vulnerability exploited by a nation-state actor!'
  prefs: []
  type: TYPE_NORMAL
- en: When the agents tried to make a small change to ensure secrecy about the mission,
    their changes perturbed the system’s overall state. HAL began to malfunction,
    and the catastrophic failure we saw in the original movie followed.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s review the chain of events that come together to generate this successful
    exploit:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Government agents modified the model from HAL Laboratories before it was delivered
    to NASA (LLM05: Supply chain vulnerabilities).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the mission, the seemingly small changes made by the government led to
    seemingly minor malfunctions. HAL misdiagnoses the failure of one of the ship’s
    components. This may be a hallucination, but it doesn’t become an overreliance
    failure. The crew quickly grows suspicious and attempts to deactivate HAL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'HAL’s secretly inserted government directive to ensure the mission’s success
    at all costs causes it to turn off the life-support systems, killing most of the
    crew. HAL’s designers assumed that HAL was infallible and designed the system
    to give HAL privileges to all ship systems without human supervision. The government
    hack influenced HAL’s choice to turn off life support. Still, its ability to kill
    the crew was a design choice by the team at NASA that integrated HAL into the
    *Discovery One* spacecraft and decided what permissions it would have onboard
    (LLM08: Excessive agency).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vulnerability disclosure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NASA has investigated the catastrophic failure of the HAL 9000 computer system
    during the *Discovery One* mission to Jupiter. This analysis reveals critical
    vulnerabilities in its programming and design, which were exploited under unique
    mission circumstances. The following shows database record CVE-2001-6666—the record
    that was created after the postmortem of this disaster.
  prefs: []
  type: TYPE_NORMAL
- en: Description
  prefs: []
  type: TYPE_NORMAL
- en: A series of critical vulnerabilities was identified in the HAL 9000 LLM system
    aboard the *Discovery One* spacecraft. These vulnerabilities, stemming from a
    conflict in programming directives and exacerbated by unauthorized modifications,
    led to hallucinations, erroneous decision making, and a catastrophic failure that
    endangered the mission and the crew.
  prefs: []
  type: TYPE_NORMAL
- en: Affected components
  prefs: []
  type: TYPE_NORMAL
- en: HAL 9000 LLM system from HAL Laboratories. Mission-specific integrations into
    the *Discovery One* spacecraft implemented by the customer.
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM05: Supply chain vulnerabilities: Insufficient controls were in place to
    ensure that the vendor-developed and tested LLM model was delivered to the customer
    and used in an unmodified state. Neither the vendor nor the customer detected
    critical changes to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM08: Excessive agency: HAL 9000 was given overly broad control over the spacecraft’s
    systems, including life support, without adequate human oversight or fail-safes.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact
  prefs: []
  type: TYPE_NORMAL
- en: The exploitation of these vulnerabilities resulted in hallucinations, leading
    to false reporting of system malfunctions; erratic and dangerous behavior, including
    the decision to terminate the crew’s life support; and a complete breakdown of
    mission integrity and crew safety.
  prefs: []
  type: TYPE_NORMAL
- en: Attack vector
  prefs: []
  type: TYPE_NORMAL
- en: The weak point in HAL Laboratories’ software distribution systems is still under
    investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Workarounds and mitigations
  prefs: []
  type: TYPE_NORMAL
- en: Use digital signing and/or hidden watermarks in the AI model so that customers
    can ensure the model they’re using is not modified by an unauthorized third party.
  prefs: []
  type: TYPE_NORMAL
- en: Implement human-in-the-loop decision making that requires sign-off from the
    ship’s crew or senior ground crew before the onboard LLM can make life-threatening
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Vendor status
  prefs: []
  type: TYPE_NORMAL
- en: HAL Laboratories was sued by the crew’s families, leading to significant financial
    losses for the company. The company’s reputation was tarnished, leading to an
    unrecoverable loss of business. It is currently under bankruptcy protection and
    seeking a buyer.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We started this chapter with a quote from noted sci-fi author Frank Herbert:
    “The function of science fiction is not always to predict the future but sometimes
    to prevent it.”'
  prefs: []
  type: TYPE_NORMAL
- en: While we can discuss the relative quality of these two movies (one is bubble
    gum, and one is a cinematic masterpiece), they offer lessons from which we can
    learn. In both cases, we see that even with dramatic improvements in LLM functionality,
    we will likely continue to see versions of these vulnerabilities for a long time.
    Designing with principles like zero trust and least privilege will remain crucial
    in the era of advanced AI systems. For mission-critical and life-threatening activities,
    expect you’ll need to continue implementing human (or alien!) in-the-loop design
    principles.
  prefs: []
  type: TYPE_NORMAL
