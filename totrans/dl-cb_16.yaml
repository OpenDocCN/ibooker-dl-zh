- en: Chapter 16\. Productionizing Machine Learning Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building and training a model is one thing; deploying your model in a production
    system is a different and often overlooked story. Running code in a Python notebook
    is nice, but not a great way to serve web clients. In this chapter we’ll look
    at how to get up and running for real.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with embeddings. Embeddings have played a role in many of the recipes
    in this book. In [Chapter 3](ch03.html#word_embeddings), we looked at the interesting
    things we can do with word embeddings, like finding similar words by looking at
    their nearest neighbors or finding analogues by adding and subtracting embedding
    vectors. In [Chapter 4](ch04.html#movie_recommender), we used embeddings of Wikipedia
    articles to build a simple movie recommender system. In [Chapter 10](ch10.html#image_search),
    we saw how we can treat the output of the final layer of a pretrained image classification
    network as embeddings for the input image and use this to build a reverse image
    search service.
  prefs: []
  type: TYPE_NORMAL
- en: Just as with these examples, we find that real-world cases often end with embeddings
    for certain entities that we then want to query from a production-quality application.
    In other words, we have a set of images, texts, or words and an algorithm that
    for each produces a vector in a high-dimensional space. For a concrete application,
    we want to be able to query this space.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with a simple approach: we’ll build a nearest neighbor model and
    save it to disk, so we can load it when we need it. We’ll then look at using Postgres
    for the same purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also explore using microservices as a way to expose machine learning models
    using Flask as a web server and Keras’s ability to save and load models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following notebooks are available for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 16.1 Using Scikit-Learn’s Nearest Neighbors for Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you quickly serve up the closest matches from an embedding model?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s nearest neighbor’s algorithm and save the model into a file.
    We’ll continue the code from [Chapter 4](ch04.html#movie_recommender), where we
    created a movie prediction model. After we’ve run everything, we normalize the
    values and fit a nearest neighbor model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then later load the model again with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest way to productionize a machine learning model is to save it to
    disk after the training is done and then to load it up when it is needed. All
    major machine learning frameworks support this, including the ones we’ve used
    throughout this book, Keras and scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: This solution is great if you are in control of memory management. In a production
    web server this is often not the case, however, and when you have to load a large
    model into memory when a web request comes in, latency obviously suffers.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2 Use Postgres to Store Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to use Postgres to store embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the Postgres `Cube` extension.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Cube` extension allows for the handling of high-dimensional data, but
    it needs to be enabled first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once that is done, we can create a table and corresponding index. To make it
    also possible to search on movie names, we’ll create a text index on the `movie_name`
    field, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Postgres is a free database that is remarkably powerful, not least because of
    the large number of extensions that are available. One of those modules is the
    `cube` module. As the name suggests, it was originally meant to make 3-dimensional
    coordinates available as a primitive, but it has since been extended to index
    arrays up to 100 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Postgres has many extensions that are well worth exploring for anybody handling
    sizeable amounts of data. In particular, the ability to store less-structured
    data in the form of arrays and JSON documents inside of classical SQL tables comes
    in handy when prototyping.
  prefs: []
  type: TYPE_NORMAL
- en: 16.3 Populating and Querying Embeddings Stored in Postgres
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Can you store our model and query results in Postgres?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use `psycopg2` to connect to Postgres from Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a username/password/database/host combination we can easily connect to
    Postgres using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Inserting our previously built model works like any other SQL operation in
    Python, except that we need to cast our `numpy` array to a Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, we can query the values. In this case we take (part of)
    a title of a movie, find the best match for that movie, and return the most similar
    movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storing an embedding model in a Postgres database allows us to query it directly,
    without having to load the model up on every request, and is therefore a good
    solution when we want to use such a model from a web server—especially when our
    web setup was Postgres-based to begin with, of course.
  prefs: []
  type: TYPE_NORMAL
- en: Running a model or the results of a model on the database server that is powering
    your website has the added advantage that you can seamlessly mix ranking components.
    We could easily extend the code of this recipe to include the Rotten Tomatoes
    ratings in our movies table, from which point on we could use this information
    to help sort the returned movies. However, if the ratings and similarity distance
    come from a different source, we would either have to do an in-memory join by
    hand or return incomplete results.
  prefs: []
  type: TYPE_NORMAL
- en: 16.4 Storing High-Dimensional Models in Postgres
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you store a model with more than 100 dimensions in Postgres?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a dimension reduction technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we wanted to load Google’s pretrained Word2vec model that we used
    in [Chapter 3](ch03.html#word_embeddings) into Postgres. Since the Postgres `cube`
    extension (see [Recipe 16.2](#use_postgres_to_store_embeddings)) limits the number
    of dimensions it will index to 100, we need to do something to make this fit.
    Reducing the dimensionality using singular value decomposition (SVD)—a technique
    we met in [Recipe 10.4](ch10.html#exploring_local_neighborhoods_in_embeddings)—is
    a good option. Let’s load up the Word2vec model as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The normalized vectors per word are stored in the `syn0norm` property, so we
    can run the SVD over that. This does take a little while:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to renormalize the vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can look at the similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The results still look reasonable, but they are not exactly the same. The last
    entry, martini, is somewhat unexpected in a list of caffeinated pick-me-ups.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Postgres `cube` extension is great, but comes with the caveat that it only
    works for vectors that have 100 or fewer elements. The documentation helpfully
    explains this limitation with: “To make it harder for people to break things,
    there is a limit of 100 on the number of dimensions of cubes.” One way around
    this restriction is to recompile Postgres, but that’s only an option if you directly
    control your setup. Also, it requires you to keep doing this as new versions of
    the database come out.'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the dimensionality before inserting our vectors into the database can
    easily be done using the `TruncatedSVD` class. In this recipe we used the entire
    set of words from the Word2vec dataset, which led to the loss of some precision.
    If we not only reduce the dimensionality of the output but also cut down the number
    of terms, we can do better. SVD can then find the most important dimensions for
    the data that we provide, rather than for all the data. This can even help by
    generalizing a bit and papering over a lack of data in our original input.
  prefs: []
  type: TYPE_NORMAL
- en: 16.5 Writing Microservices in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to write and deploy a simple Python microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Build a minimal web app using Flask, returning a JSON document based on a REST
    request.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we need a Flask web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define the service we want to offer. As an example, we’ll take in an
    image and return the size of the image. We expect the image to be part of a `POST`
    request. If we don’t get a `POST` request, we’ll return a simple HTML form so
    we can test the service without a client. The `@app.route` decoration specifies
    that the `return_size` handles any requests at the root, supporting both `GET`
    and `POST`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now all we have to do is run the server at a port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: REST was originally meant as a full-blown resource management framework that
    assigns URLs to all resources in a system and then lets clients interact with
    the whole spectrum of HTTP verbs, from `PUT` to `DELETE`. Like many APIs out there,
    we forego all that in this example and just have a `GET` method defined on one
    handler that triggers our API and returns a JSON document.
  prefs: []
  type: TYPE_NORMAL
- en: The service we developed here is of course rather trivial; having a microservice
    for just getting the size of an image is probably taking the concept a little
    too far. In the next recipe we’ll explore how we can use this approach to serve
    up the results of a previously developed machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 16.6 Deploying a Keras Model Using a Microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to deploy a Keras model as a standalone service.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Expand your Flask server to forward requests to a pretrained Keras model.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe builds on the recipes in [Chapter 10](ch10.html#image_search), where
    we downloaded thousands of images from Wikipedia and fed them into a pretrained
    image recognition network, getting back a 2,048-dimensional vector describing
    each image. We’ll fit a nearest neighbor model on these vectors so that we can
    quickly find the most similar image, given a vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load the pickled image names and nearest neighbor model
    and instantiate the pretrained model for image recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now modify how we handle the incoming image by changing the bit of code
    after `if file:`. We’ll resize the image to the target size of the model, normalize
    the data, run the prediction, and find the nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Feed it an image of a cat, and you should see a large number of cats sampled
    from the Wikipedia images—with one photo of kids playing with a home computer
    thrown in.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By loading the model on startup and then feeding in the images as they come
    in, we can cut down on the latency that we would get if we followed the approach
    of the first recipe in this section. We’re effectively chaining two models here,
    the pretrained image recognition network and the nearest neighbor classifier,
    and exporting the combination as one service.
  prefs: []
  type: TYPE_NORMAL
- en: 16.7 Calling a Microservice from a Web Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to call a microservice from Django.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use `requests` to call the microservice while handling the Django request.
    We can do this along the lines of the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code here is from a Django request handler, but things should look really
    similar in other web frameworks, even ones based on a different language than
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: The key thing here is that we separate the session management of the web framework
    from the session management of our microservice. This way we know that at any
    given time there is exactly one instance of our model, which makes latency and
    memory use predictable.
  prefs: []
  type: TYPE_NORMAL
- en: '`Requests` is a straightforward module for making `HTTP` calls. It doesn’t
    support making async calls, though. In the code for this recipe that isn’t important,
    but if we need to call multiple services, we’d want to do that in parallel. There
    are a number of options for this, but they all fall into the pattern where we
    fire off calls to the backends at the beginning of our request, do the processing
    we need to, and then, when we need the results, wait on the outstanding requests.
    This is a good setup for building high-performance systems using Python.'
  prefs: []
  type: TYPE_NORMAL
- en: 16.8 TensorFlow seq2seq models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you productionize a seq2seq chat model?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Run a TensorFlow session with an output-capturing hook.
  prefs: []
  type: TYPE_NORMAL
- en: The `seq2seq` model that Google published is a very nice way to quickly develop
    sequence-to-sequence models, but out of the box the inference phase can only be
    run using `stdin` and `stdout`. It’s entirely possible to call out from our microservice
    this way, but that means we’ll incur the latency cost of loading the model up
    on every call.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better way is to instantiate the model manually and capture the output using
    a hook. The first step is to reinstate the model from the checkpoint directory.
    We need to load both the model and the model configuration. The model feeds in
    the `source_tokens` (i.e., the chat prompt) and we’ll use a batch size of 1, since
    we’ll do this in an interactive fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to set up the TensorFlow session that allows us to feed data
    into the model. This is all fairly boilerplate stuff (and should make us appreciate
    frameworks like Keras even more):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We’ve now configured a TensorFlow session with a hook to `DecodeOnce`, which
    is a class that implements the basic functionality of the inference task but then,
    when it is done, calls the provided `callback` function to return the actual results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code for *seq2seq_server.py* we can then use this to handle an HTTP
    request as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This will let us handle seq2seq calls from a simple web server.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The way we feed data into the seq2seq TensorFlow model in this recipe is not
    very pretty, but it is effective and in terms of performance a much better option
    than using `stdin` and `stdout`. Hopefully an upcoming version of this library
    will provide us with a nicer way to use these models in production, but for now
    this will have to do.
  prefs: []
  type: TYPE_NORMAL
- en: 16.9 Running Deep Learning Models in the Browser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you run a deep learning web app without a server?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use Keras.js to run the model in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: Running a deep learning model in the browser sounds crazy. Deep learning needs
    lots of processing power, and we all know that JavaScript is slow. But it turns
    out that you can run models in the browser at a decent speed with GPU acceleration.
    [Keras.js](https://transcranial.github.io/keras-js/#) has a tool to convert Keras
    models to something that the JavaScript runtime can work with, and it uses WebGL
    to get the GPU to help with this. It’s an amazing bit of engineering and it comes
    with some impressive demos. Let’s try this on one of our own models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook `16.1 Simple Text Generation` is taken from the Keras example
    directory and trains a simple text generation model based on the writings of Nietzsche.
    After training we save the model with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to convert the Keras model to the Keras.js format. First get the
    conversion code using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now open a shell and, in the directory where you saved the model, execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This should give you a *nietzsche.bin* file.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to use this file from a web page.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll do this in the file *nietzsche.html*, which you’ll find in the *keras_js*
    directory of the *deep_learning_cookbook* repository. Let’s take a look. It starts
    with code to load the Keras.js library and the variables we saved from Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'At the bottom we have a very simple bit of HTML that lets the user enter some
    text and then press a button to run the model to extend the text in a Nietzschean
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s load the model and, when it’s done, enable the currently disabled
    button `buttonGo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In `runModel` we first need to one-hot encode the text data using the `char_indices`
    we imported before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run the model with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `outputData` variable will contain a probability distribution for each
    of the characters in our vocabulary. The easiest way to make sense of that is
    to pick just the character with the highest probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we just add that character to what we had so far and do the same thing
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being able to run models straight in the browser creates entirely new possibilities
    for productionalizing. It means you don’t need a server to do the actual calculations,
    and with WebGL you even get GPU acceleration for free. Check out the fun demos
    at [*https://transcranial.github.io/keras-js*](https://transcranial.github.io/keras-js).
  prefs: []
  type: TYPE_NORMAL
- en: There are limitations to this approach. To use the GPU, Keras.js uses WebGL
    2.0\. Unfortunately, not all browsers support this at the moment. Moreover, tensors
    are encoded as WebGL textures, which are limited in size. The actual limit depends
    on your browser and hardware. You can of course fall back to CPU only, but that
    means running in pure JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: A second limitation is the size of the models. Production-quality models often
    have sizes of tens of megabytes, which isn’t a problem at all when they are loaded
    up once on the server but might create issues when they need to be sent to a client.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *encoder.py* script has a flag called `--quantize` that will encode the
    weights of the model as 8-bit integers. This reduces the size of the model by
    75%, but it means the weights will be less precise, which might hurt prediction
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 16.10 Running a Keras Model Using TensorFlow Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you run a Keras model using Google’s state-of-the art server?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convert the model and invoke the TensorFlow Serving toolkit to write out the
    model spec so you can run it using TensorFlow Serving.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Serving is part of the TensorFlow platform; according to Google it’s
    a flexible, high-performance serving system for machine learning models, designed
    for production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Writing out a TensorFlow model in a way that TensorFlow Serving will work with
    is somewhat involved. Keras models need even more massaging in order for this
    to work. In principle, any model can be used as long as the model has only one
    input and only one output—a restriction that comes with TensorFlow Serving. Another
    is that TensorFlow Serving only supports Python 2.7.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is recreate the model as a testing-only model. Models
    behave differently during training and testing. For example, the `Dropout` layer
    only randomly drops neurons while training—during testing everything is used.
    Keras hides this from the user, passing the learning phase in as an extra variable.
    If you see errors stating that something is missing from your input, this is probably
    it. We’ll set the learning phase to `0` (false) and extract the config and the
    weights from our character CNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point it might be useful to run a prediction on the model so we can
    later see that it still works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then rebuild the model with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In order for the model to run, we need to provide TensorFlow Serving with the
    input and output spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then construct the `builder` object to define our handler and write
    out the definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we run the server with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: You can either get the binaries directly from Google or build them from source—see
    [the installation instructions](https://www.tensorflow.org/serving/setup) for
    details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if we can call the model from Python. We’ll instantiate a prediction
    request and use `grpc` to make a call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the actual predicted emojis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow Serving is the way to productionize models blessed by Google but
    using it with a Keras model is somewhat involved compared to bringing up a custom
    Flask server and handling the input and output ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: It does have advantages, though. For one thing, since is not custom, these servers
    all behave the same. Furthermore, it is an industrial-strength server that supports
    versioning and can load models straight from a number of cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: 16.11 Using a Keras Model from iOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to use a model trained on the desktop from a mobile app on iOS.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use CoreML to convert your model and talk to it directly from Swift.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This recipe describes how to build an app for iOS, so you’ll need a Mac with
    Xcode installed to run the example. Moreover, since the example uses the camera
    for detection, you’ll also need an iOS device with a camera to try it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is to convert the model. Unfortunately Apple’s code only
    supports Python 2.7 and also seems to lag a bit when it comes to supporting the
    latest versions of `tensorflow` and `keras`, so we’ll set specific versions. Open
    a shell to set up Python 2.7 with the right requirements and type in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then start Python and enter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the previously saved model and the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then convert the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You could also skip this and work with the *.mlmodel* file in the *zoo* directory.
  prefs: []
  type: TYPE_NORMAL
- en: Now start Xcode, create a new project, and drag the *PetRecognizer.mlmodel*
    file to the project. Xcode automatically imports the model and makes it callable.
    Let’s recognize some pets!
  prefs: []
  type: TYPE_NORMAL
- en: Apple has an example project [on its website](https://apple.co/2HPUHOW) that
    uses a standard image recognition network. Download this project, unzip it, and
    then open it with Xcode.
  prefs: []
  type: TYPE_NORMAL
- en: In the project overview, you should see a file called *MobileNet.mlmodel*. Delete
    that and then drag the *PetRecognizer.mlmodel* file to where *MobileNet.mlmodel*
    used to be. Now open *ImageClassificationViewController.swift* and replace any
    occurences of `MobileNet` with `PetRecognizer`.
  prefs: []
  type: TYPE_NORMAL
- en: You should now be able to run the app as before, but with the new model and
    output classes.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using a Keras model from an iOS app is surprisingly simple, at least if we stick
    to the examples that Apple’s SDK ships with. The technology is quite recent though,
    and there are not a lot of working examples out there that are radically different
    from Apple’s examples. Moreover, CoreML only works on Apple operating systems,
    and then only on iOS 11 or higher or macOS 10.13 or higher.
  prefs: []
  type: TYPE_NORMAL
