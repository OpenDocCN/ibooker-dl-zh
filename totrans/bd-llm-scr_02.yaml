- en: 3 Coding attention mechanisms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 编码注意力机制
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The reasons for using attention mechanisms in neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在神经网络中使用注意力机制的原因
- en: A basic self-attention framework, progressing to an enhanced self-attention
    mechanism
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基本的自注意力框架，逐步发展到增强的自注意力机制
- en: A causal attention module that allows LLMs to generate one token at a time
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个因果注意力模块，允许LLM一次生成一个标记
- en: Masking randomly selected attention weights with dropout to reduce overfitting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用dropout随机屏蔽选定的注意力权重以减少过拟合
- en: Stacking multiple causal attention modules into a multi-head attention module
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个因果注意力模块堆叠成多头注意力模块
- en: At this point, you know how to prepare the input text for training LLMs by splitting
    text into individual word and subword tokens, which can be encoded into vector
    representations, embeddings, for the LLM.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经知道如何通过将文本分割成单个单词和子词标记来准备训练LLM的输入文本，这些标记可以编码成LLM的向量表示，即嵌入。
- en: Now, we will look at an integral part of the LLM architecture itself, attention
    mechanisms, as illustrated in figure 3.1\. We will largely look at attention mechanisms
    in isolation and focus on them at a mechanistic level. Then we will code the remaining
    parts of the LLM surrounding the self-attention mechanism to see it in action
    and to create a model to generate text.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看LLM架构本身的组成部分，即注意力机制，如图3.1所示。我们将主要关注注意力机制本身，并在机制层面上进行关注。然后我们将编码LLM围绕自注意力机制的其余部分，以观察其实际应用并创建一个生成文本的模型。
- en: '![figure](../Images/3-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-1.png)'
- en: 'Figure 3.1 The three main stages of coding an LLM. This chapter focuses on
    step 2 of stage 1: implementing attention mechanisms, which are an integral part
    of the LLM architecture.'
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1 编码LLM的三个主要阶段。本章重点介绍第一阶段第二步：实现注意力机制，这是LLM架构的组成部分。
- en: We will implement four different variants of attention mechanisms, as illustrated
    in figure 3.2. These different attention variants build on each other, and the
    goal is to arrive at a compact and efficient implementation of multi-head attention
    that we can then plug into the LLM architecture we will code in the next chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现如图3.2所示的四种不同的注意力机制变体。这些不同的注意力变体相互构建，目标是达到一个紧凑且高效的多头注意力实现，然后我们可以将其插入到我们在下一章中编码的LLM架构中。
- en: '![figure](../Images/3-2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-2.png)'
- en: Figure 3.2 The figure depicts different attention mechanisms we will code in
    this chapter, starting with a simplified version of self-attention before adding
    the trainable weights. The causal attention mechanism adds a mask to self-attention
    that allows the LLM to generate one word at a time. Finally, multi-head attention
    organizes the attention mechanism into multiple heads, allowing the model to capture
    various aspects of the input data in parallel.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2 该图展示了本章我们将编码的不同注意力机制，从简化的自注意力版本开始，然后添加可训练的权重。因果注意力机制为自注意力添加了一个掩码，允许LLM一次生成一个单词。最后，多头注意力将注意力机制组织成多个头，允许模型并行捕获输入数据的各个方面。
- en: 3.1 The problem with modeling long sequences
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 模型长序列的问题
- en: Before we dive into the *self-attention* mechanism at the heart of LLMs, let’s
    consider the problem with pre-LLM architectures that do not include attention
    mechanisms. Suppose we want to develop a language translation model that translates
    text from one language into another. As shown in figure 3.3, we can’t simply translate
    a text word by word due to the grammatical structures in the source and target
    language.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨LLM核心的*自注意力*机制之前，让我们考虑一下不包含注意力机制的预LLM架构的问题。假设我们想要开发一个语言翻译模型，将文本从一种语言翻译成另一种语言。如图3.3所示，由于源语言和目标语言的语法结构，我们不能简单地逐词翻译文本。
- en: '![figure](../Images/3-3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-3.png)'
- en: Figure 3.3 When translating text from one language to another, such as German
    to English, it’s not possible to merely translate word by word. Instead, the translation
    process requires contextual understanding and grammatical alignment.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3 当将文本从一种语言翻译成另一种语言，例如从德语翻译成英语时，不能仅仅逐词翻译。相反，翻译过程需要上下文理解和语法对齐。
- en: To address this problem, it is common to use a deep neural network with two
    submodules, an *encoder* and a *decoder*. The job of the encoder is to first read
    in and process the entire text, and the decoder then produces the translated text.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，通常使用一个具有两个子模块的深度神经网络，一个*编码器*和一个*解码器*。编码器的任务是首先读取并处理整个文本，然后解码器生成翻译后的文本。
- en: Before the advent of transformers, *recurrent neural networks* (RNNs) were the
    most popular encoder–decoder architecture for language translation. An RNN is
    a type of neural network where outputs from previous steps are fed as inputs to
    the current step, making them well-suited for sequential data like text. If you
    are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings
    of RNNs to follow this discussion; our focus here is more on the general concept
    of the encoder–decoder setup.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在变压器出现之前，*循环神经网络*（RNNs）是语言翻译中最受欢迎的编码器-解码器架构。RNN是一种神经网络，其中前一步的输出被作为当前步骤的输入，这使得它们非常适合像文本这样的顺序数据。如果你对RNN不熟悉，不用担心——你不需要了解RNN的详细工作原理来跟随这次讨论；我们在这里更关注编码器-解码器设置的一般概念。
- en: In an encoder–decoder RNN, the input text is fed into the encoder, which processes
    it sequentially. The encoder updates its hidden state (the internal values at
    the hidden layers) at each step, trying to capture the entire meaning of the input
    sentence in the final hidden state, as illustrated in figure 3.4\. The decoder
    then takes this final hidden state to start generating the translated sentence,
    one word at a time. It also updates its hidden state at each step, which is supposed
    to carry the context necessary for the next-word prediction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器-解码器RNN中，输入文本被输入到编码器中，编码器按顺序处理它。编码器在每一步更新其隐藏状态（隐藏层的内部值），试图在最终的隐藏状态中捕获输入句子的整个含义，如图3.4所示。然后，解码器使用这个最终的隐藏状态开始生成翻译句子，一次一个单词。它也在每一步更新其隐藏状态，这个状态应该携带进行下一个单词预测所需的上下文。
- en: '![figure](../Images/3-4.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-4.png)'
- en: Figure 3.4 Before the advent of transformer models, encoder–decoder RNNs were
    a popular choice for machine translation. The encoder takes a sequence of tokens
    from the source language as input, where a hidden state (an intermediate neural
    network layer) of the encoder encodes a compressed representation of the entire
    input sequence. Then, the decoder uses its current hidden state to begin the translation,
    token by token.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4 在变压器模型出现之前，编码器-解码器RNN是机器翻译的一个流行选择。编码器接收源语言的一序列标记作为输入，其中编码器的隐藏状态（一个中间神经网络层）编码了整个输入序列的压缩表示。然后，解码器使用其当前的隐藏状态开始逐个标记进行翻译。
- en: While we don’t need to know the inner workings of these encoder–decoder RNNs,
    the key idea here is that the encoder part processes the entire input text into
    a hidden state (memory cell). The decoder then takes in this hidden state to produce
    the output. You can think of this hidden state as an embedding vector, a concept
    we discussed in chapter 2.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不需要了解这些编码器-解码器RNN的内部工作原理，但这里的关键思想是，编码器部分将整个输入文本处理成一个隐藏状态（记忆单元）。然后，解码器接收这个隐藏状态以生成输出。你可以将这个隐藏状态视为一个嵌入向量，这是我们第2章讨论的概念。
- en: The big limitation of encoder–decoder RNNs is that the RNN can’t directly access
    earlier hidden states from the encoder during the decoding phase. Consequently,
    it relies solely on the current hidden state, which encapsulates all relevant
    information. This can lead to a loss of context, especially in complex sentences
    where dependencies might span long distances.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器RNN的一个重大限制是，RNN在解码阶段不能直接访问编码器中的早期隐藏状态。因此，它完全依赖于当前的隐藏状态，该状态封装了所有相关信息。这可能导致上下文丢失，尤其是在复杂句子中，其中依赖关系可能跨越很长的距离。
- en: Fortunately, it is not essential to understand RNNs to build an LLM. Just remember
    that encoder–decoder RNNs had a shortcoming that motivated the design of attention
    mechanisms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，构建一个LLM并不需要理解RNN。只需记住，编码器-解码器RNN有一个不足之处，这促使了注意力机制的设计。
- en: 3.2 Capturing data dependencies with attention mechanisms
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 使用注意力机制捕获数据依赖
- en: Although RNNs work fine for translating short sentences, they don’t work well
    for longer texts as they don’t have direct access to previous words in the input.
    One major shortcoming in this approach is that the RNN must remember the entire
    encoded input in a single hidden state before passing it to the decoder (figure
    3.4).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RNN对于翻译短句效果不错，但它们对于较长的文本效果不佳，因为它们无法直接访问输入中的先前单词。这种方法的重大不足在于，RNN必须在传递给解码器之前，在单个隐藏状态中记住整个编码输入（图3.4）。
- en: Hence, researchers developed the *Bahdanau attention* mechanism for RNNs in
    2014 (named after the first author of the respective paper; for more information,
    see appendix B), which modifies the encoder–decoder RNN such that the decoder
    can selectively access different parts of the input sequence at each decoding
    step as illustrated in figure 3.5.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员在2014年为RNN开发了*Bahdanau注意力*机制（以该论文的第一作者命名；更多信息，见附录B），该机制修改了编码器-解码器RNN，使得解码器可以在每个解码步骤中选择性访问输入序列的不同部分，如图3.5所示。
- en: '![figure](../Images/3-5.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: 自注意力是一种机制，允许输入序列中的每个位置在计算序列表示时考虑同一序列中所有其他位置的相关性或“关注”。自注意力是当代基于transformer架构的LLM（如GPT系列）的关键组件。
- en: Figure 3.5 Using an attention mechanism, the text-generating decoder part of
    the network can access all input tokens selectively. This means that some input
    tokens are more important than others for generating a given output token. The
    importance is determined by the attention weights, which we will compute later.
    Note that this figure shows the general idea behind attention and does not depict
    the exact implementation of the Bahdanau mechanism, which is an RNN method outside
    this book’s scope.
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用注意力机制，网络中生成文本的解码器部分可以选择性访问所有输入标记。这意味着对于生成给定输出标记，某些输入标记比其他标记更重要。重要性由注意力权重决定，我们将在后面计算。请注意，此图展示了注意力背后的基本思想，并不描绘Bahdanau机制的确切实现，该机制是本书范围之外的RNN方法。
- en: Interestingly, only three years later, researchers found that RNN architectures
    are not required for building deep neural networks for natural language processing
    and proposed the original *transformer* architecture (discussed in chapter 1)
    including a self-attention mechanism inspired by the Bahdanau attention mechanism.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure](../Images/3-5.png)'
- en: Self-attention is a mechanism that allows each position in the input sequence
    to consider the relevancy of, or “attend to,” all other positions in the same
    sequence when computing the representation of a sequence. Self-attention is a
    key component of contemporary LLMs based on the transformer architecture, such
    as the GPT series.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure](../Images/3-6.png)'
- en: This chapter focuses on coding and understanding this self-attention mechanism
    used in GPT-like models, as illustrated in figure 3.6\. In the next chapter, we
    will code the remaining parts of the LLM.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍编码和理解GPT类模型中使用的自注意力机制，如图3.6所示。在下一章中，我们将编写LLM的剩余部分。
- en: '![figure](../Images/3-6.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: 在自注意力中，“self”指的是该机制通过关联单个输入序列内的不同位置来计算注意力权重的能力。它评估和学习输入本身各部分之间的关系和依赖性，例如句子中的单词或图像中的像素。
- en: Figure 3.6 Self-attention is a mechanism in transformers used to compute more
    efficient input representations by allowing each position in a sequence to interact
    with and weigh the importance of all other positions within the same sequence.
    In this chapter, we will code this self-attention mechanism from the ground up
    before we code the remaining parts of the GPT-like LLM in the following chapter.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '![figure](../Images/3-5.png)'
- en: 3.3 Attending to different parts of the input with self-attention
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 使用自注意力关注输入的不同部分
- en: We’ll now cover the inner workings of the self-attention mechanism and learn
    how to code it from the ground up. Self-attention serves as the cornerstone of
    every LLM based on the transformer architecture. This topic may require a lot
    of focus and attention (no pun intended), but once you grasp its fundamentals,
    you will have conquered one of the toughest aspects of this book and LLM implementation
    in general.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨自注意力机制的内部工作原理，并学习如何从头开始编写它的代码。自注意力是每个基于transformer架构的LLM的基石。这个主题可能需要大量的关注和注意（无意中打趣），但一旦你掌握了它的基础，你将征服这本书和LLM实现的一般性中最具挑战性的方面之一。
- en: The “self” in self-attention
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 有趣的是，仅仅三年后，研究人员发现构建用于自然语言处理的深度神经网络不需要RNN架构，并提出了原始的*transformer*架构（在第1章中讨论），包括一个受Bahdanau注意力机制启发的自注意力机制。
- en: In self-attention, the “self” refers to the mechanism’s ability to compute attention
    weights by relating different positions within a single input sequence. It assesses
    and learns the relationships and dependencies between various parts of the input
    itself, such as words in a sentence or pixels in an image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力中的“self”
- en: This is in contrast to traditional attention mechanisms, where the focus is
    on the relationships between elements of two different sequences, such as in sequence-to-sequence
    models where the attention might be between an input sequence and an output sequence,
    such as the example depicted in figure 3.5.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这与传统注意力机制形成对比，传统注意力机制关注的是两个不同序列元素之间的关系，例如在序列到序列模型中，注意力可能是在输入序列和输出序列之间，如图3.5所示。
- en: Since self-attention can appear complex, especially if you are encountering
    it for the first time, we will begin by examining a simplified version of it.
    Then we will implement the self-attention mechanism with trainable weights used
    in LLMs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自注意力可能看起来很复杂，尤其是如果你第一次遇到它，我们将首先检查它的简化版本。然后我们将实现LLMs中使用的带有可训练权重的自注意力机制。
- en: 3.3.1 A simple self-attention mechanism without trainable weights
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 无可训练权重的简单自注意力机制
- en: Let’s begin by implementing a simplified variant of self-attention, free from
    any trainable weights, as summarized in figure 3.7\. The goal is to illustrate
    a few key concepts in self-attention before adding trainable weights.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从实现一个简化的自注意力变体开始，如图3.7所示，这个变体不包含任何可训练的权重。目标是先展示自注意力的一些关键概念，然后再添加可训练的权重。
- en: '![figure](../Images/3-7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-7.png)'
- en: Figure 3.7 The goal of self-attention is to compute a context vector for each
    input element that combines information from all other input elements. In this
    example, we compute the context vector z^((2)). The importance or contribution
    of each input element for computing z^((2)) is determined by the attention weights
    a[21] to a[2T]. When computing z^((2)), the attention weights are calculated with
    respect to input element x^((2)) and all other inputs.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7 自注意力的目标是计算每个输入元素的一个上下文向量，该向量结合了所有其他输入元素的信息。在这个例子中，我们计算上下文向量z^((2))。每个输入元素对于计算z^((2))的重要性或贡献由注意力权重a[21]到a[2T]决定。在计算z^((2))时，注意力权重是根据输入元素x^((2))和所有其他输入计算的。
- en: Figure 3.7 shows an input sequence, denoted as *x*, consisting of *T* elements
    represented as *x*(1) to *x*(T). This sequence typically represents text, such
    as a sentence, that has already been transformed into token embeddings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7展示了输入序列，用*x*表示，由*T*个元素组成，表示为*x*(1)到*x*(T)。这个序列通常代表文本，例如一个句子，它已经被转换成标记嵌入。
- en: For example, consider an input text like “Your journey starts with one step.”
    In this case, each element of the sequence, such as *x*(1), corresponds to a *d*-dimensional
    embedding vector representing a specific token, like “Your.” Figure 3.7 shows
    these input vectors as three-dimensional embeddings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个输入文本如“Your journey starts with one step.”在这种情况下，序列中的每个元素，如*x*(1)，对应一个*d*-维嵌入向量，代表一个特定的标记，如“Your。”图3.7展示了这些输入向量作为三维嵌入。
- en: In self-attention, our goal is to calculate context vectors *z*(i) for each
    element *x*(i) in the input sequence. A *context vector* can be interpreted as
    an enriched embedding vector.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力中，我们的目标是计算输入序列中每个元素*x*(i)的上下文向量*z*(i)。上下文向量可以解释为一个丰富的嵌入向量。
- en: To illustrate this concept, let’s focus on the embedding vector of the second
    input element, *x*(2) (which corresponds to the token “journey”), and the corresponding
    context vector, *z*(2), shown at the bottom of figure 3.7\. This enhanced context
    vector, *z*(2), is an embedding that contains information about *x*(2) and all
    other input elements, *x*(1) to *x*(T).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，让我们关注第二个输入元素*x*(2)（对应标记“journey”）及其相应的上下文向量*z*(2)，如图3.7底部所示。这个增强的上下文向量*z*(2)是一个包含关于*x*(2)和所有其他输入元素*x*(1)到*x*(T)信息的嵌入。
- en: Context vectors play a crucial role in self-attention. Their purpose is to create
    enriched representations of each element in an input sequence (like a sentence)
    by incorporating information from all other elements in the sequence (figure 3.7).
    This is essential in LLMs, which need to understand the relationship and relevance
    of words in a sentence to each other. Later, we will add trainable weights that
    help an LLM learn to construct these context vectors so that they are relevant
    for the LLM to generate the next token. But first, let’s implement a simplified
    self-attention mechanism to compute these weights and the resulting context vector
    one step at a time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文向量在自注意力机制中起着至关重要的作用。它们的目的是通过结合序列（如句子）中所有其他元素的信息，为输入序列中的每个元素（如单词）创建丰富的表示（如图3.7所示）。这对于LLMs（大型语言模型）至关重要，因为LLMs需要理解句子中单词之间的关系和相关性。稍后，我们将添加可训练的权重，帮助LLM学习构建这些上下文向量，以便它们对LLM生成下一个标记相关。但首先，让我们实现一个简化的自注意力机制，逐步计算这些权重和结果上下文向量。
- en: 'Consider the following input sentence, which has already been embedded into
    three-dimensional vectors (see chapter 2). I’ve chosen a small embedding dimension
    to ensure it fits on the page without line breaks:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下已经嵌入为三维向量的输入句子（参见第2章）。我选择了一个小的嵌入维度，以确保它可以在不换行的情况下适应页面：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first step of implementing self-attention is to compute the intermediate
    values w, referred to as attention scores, as illustrated in figure 3.8\. Due
    to spatial constraints, the figure displays the values of the preceding `inputs`
    tensor in a truncated version; for example, 0.87 is truncated to 0.8\. In this
    truncated version, the embeddings of the words “journey” and “starts” may appear
    similar by random chance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自注意力的第一步是计算中间值w，称为注意力分数，如图3.8所示。由于空间限制，图中的值以截断版本显示前一个`inputs`张量的值；例如，0.87被截断为0.8。在这个截断版本中，单词“journey”和“starts”的嵌入可能由于随机机会而看起来相似。
- en: '![figure](../Images/3-8.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-8.png)'
- en: Figure 3.8 The overall goal is to illustrate the computation of the context
    vector z^((2)) using the second input element, x^((2)) as a query. This figure
    shows the first intermediate step, computing the attention scores w between the
    query x^((2)) and all other input elements as a dot product. (Note that the numbers
    are truncated to one digit after the decimal point to reduce visual clutter.)
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8 整体目标是说明使用第二个输入元素x^((2))作为查询计算上下文向量z^((2))的过程。此图显示了第一个中间步骤，即计算查询x^((2))与所有其他输入元素之间的注意力分数w，作为点积。
    （注意，数字被截断到小数点后一位，以减少视觉混乱。）
- en: 'Figure 3.8 illustrates how we calculate the intermediate attention scores between
    the query token and each input token. We determine these scores by computing the
    dot product of the query, *x*(2), with every other input token:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8说明了我们如何计算查询标记和每个输入标记之间的中间注意力分数。我们通过计算查询*x*(2)与每个其他输入标记的点积来确定这些分数：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 The second input token serves as the query.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第二个输入标记作为查询。'
- en: The computed attention scores are
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的注意力分数是
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Understanding dot products
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 理解点积
- en: 'A dot product is essentially a concise way of multiplying two vectors element-wise
    and then summing the products, which can be demonstrated as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 点积本质上是一种简洁地逐元素乘以两个向量并求和其乘积的方法，如下所示：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output confirms that the sum of the element-wise multiplication gives the
    same results as the dot product:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确认了逐元素乘积的总和与点积给出相同的结果：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Beyond viewing the dot product operation as a mathematical tool that combines
    two vectors to yield a scalar value, the dot product is a measure of similarity
    because it quantifies how closely two vectors are aligned: a higher dot product
    indicates a greater degree of alignment or similarity between the vectors. In
    the context of self-attention mechanisms, the dot product determines the extent
    to which each element in a sequence focuses on, or “attends to,” any other element:
    the higher the dot product, the higher the similarity and attention score between
    two elements.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将点积操作视为结合两个向量以产生标量值的数学工具之外，点积是相似度的度量，因为它量化了两个向量对齐的紧密程度：点积越高，向量之间的对齐或相似度就越高。在自注意力机制的情况下，点积决定了序列中每个元素对其他元素的关注程度或“注意”程度：点积越高，两个元素之间的相似度和注意力分数就越高。
- en: 'In the next step, as shown in figure 3.9, we normalize each of the attention
    scores we computed previously. The main goal behind the normalization is to obtain
    attention weights that sum up to 1\. This normalization is a convention that is
    useful for interpretation and maintaining training stability in an LLM. Here’s
    a straightforward method for achieving this normalization step:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，如图3.9所示，我们对之前计算的所有注意力分数进行归一化。归一化的主要目标是获得总和为1的注意力权重。这种归一化是一种对解释和维持LLM训练稳定性的有用惯例。以下是一种实现这一归一化步骤的简单方法：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![figure](../Images/3-9.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-9.png)'
- en: Figure 3.9 After computing the attention scores w[21] to w[2T] with respect
    to the input query x^((2)), the next step is to obtain the attention weights a[21]
    to a[2T] by normalizing the attention scores.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9 在根据输入查询x^((2))计算注意力分数w[21]到w[2T]之后，下一步是通过对注意力分数进行归一化来获得注意力权重a[21]到a[2T]。
- en: 'As the output shows, the attention weights now sum to 1:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，注意力权重现在总和为1：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In practice, it’s more common and advisable to use the softmax function for
    normalization. This approach is better at managing extreme values and offers more
    favorable gradient properties during training. The following is a basic implementation
    of the softmax function for normalizing the attention scores:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，更常见且建议使用softmax函数进行归一化。这种方法在处理极端值时表现更好，并且在训练期间提供了更有利的梯度属性。以下是对注意力分数进行归一化的softmax函数的基本实现：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As the output shows, the softmax function also meets the objective and normalizes
    the attention weights such that they sum to 1:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，softmax函数也达到了目标，并归一化了注意力权重，使得它们的总和为1：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In addition, the softmax function ensures that the attention weights are always
    positive. This makes the output interpretable as probabilities or relative importance,
    where higher weights indicate greater importance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，softmax函数确保注意力权重始终为正。这使得输出可解释为概率或相对重要性，其中较高的权重表示更大的重要性。
- en: 'Note that this naive softmax implementation (`softmax_naive`) may encounter
    numerical instability problems, such as overflow and underflow, when dealing with
    large or small input values. Therefore, in practice, it’s advisable to use the
    PyTorch implementation of softmax, which has been extensively optimized for performance:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种原始的softmax实现（`softmax_naive`）在处理大或小输入值时可能会遇到数值不稳定性问题，如溢出和下溢。因此，在实践中，建议使用经过广泛优化的PyTorch
    softmax实现：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this case, it yields the same results as our previous `softmax_naive` function:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，它产生了与我们的先前`softmax_naive`函数相同的结果：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we have computed the normalized attention weights, we are ready for
    the final step, as shown in figure 3.10: calculating the context vector *z*(2)
    by multiplying the embedded input tokens, *x*(i), with the corresponding attention
    weights and then summing the resulting vectors. Thus, context vector *z*(2) is
    the weighted sum of all input vectors, obtained by multiplying each input vector
    by its corresponding attention weight:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了归一化的注意力权重，我们准备进行最终步骤，如图3.10所示：通过将嵌入输入标记*x*(i)与相应的注意力权重相乘，然后求和得到的向量来计算上下文向量*z*(2)。因此，上下文向量*z*(2)是所有输入向量的加权总和，通过将每个输入向量乘以其相应的注意力权重得到：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 The second input token is the query.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第二个输入标记是查询。'
- en: '![figure](../Images/3-10.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-10.png)'
- en: Figure 3.10 The final step, after calculating and normalizing the attention
    scores to obtain the attention weights for query x^((2)), is to compute the context
    vector z^((2)). This context vector is a combination of all input vectors x^((1))
    to x^(^(*T *)^) weighted by the attention weights.
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10 最终步骤，在计算并归一化注意力分数以获得查询x^((2))的注意力权重之后，是计算上下文向量z^((2))。这个上下文向量是所有输入向量x^((1))到x^(^(*T *)^)的加权组合，权重由注意力权重决定。
- en: The results of this computation are
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此计算的结果是
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, we will generalize this procedure for computing context vectors to calculate
    all context vectors simultaneously.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将将计算上下文向量的这一过程推广，以同时计算所有上下文向量。
- en: 3.3.2 Computing attention weights for all input tokens
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 计算所有输入标记的注意力权重
- en: So far, we have computed attention weights and the context vector for input
    2, as shown in the highlighted row in figure 3.11\. Now let’s extend this computation
    to calculate attention weights and context vectors for all inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经计算了输入2的注意力权重和上下文向量，如图3.11中高亮显示的行所示。现在让我们扩展这个计算，以计算所有输入的注意力权重和上下文向量。
- en: '![figure](../Images/3-11.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-11.png)'
- en: Figure 3.11 The highlighted row shows the attention weights for the second input
    element as a query. Now we will generalize the computation to obtain all other
    attention weights. (Please note that the numbers in this figure are truncated
    to two digits after the decimal point to reduce visual clutter. The values in
    each row should add up to 1.0 or 100%.)
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11 高亮行显示了作为查询的第二输入元素的注意力权重。现在我们将计算推广到获得所有其他注意力权重。（请注意，此图中的数字被截断到小数点后两位，以减少视觉杂乱。每行的值应加起来为1.0或100%。）
- en: 'We follow the same three steps as before (see figure 3.12), except that we
    make a few modifications in the code to compute all context vectors instead of
    only the second one, *z*(2):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循之前相同的三个步骤（见图3.12），只是在代码中做了一些修改，以计算所有上下文向量而不是仅计算第二个向量 *z*(2)：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![figure](../Images/3-12.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-12.png)'
- en: Figure 3.12 In step 1, we add an additional `for` loop to compute the dot products
    for all pairs of inputs.
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12 在第1步中，我们添加了一个额外的 `for` 循环来计算所有输入对之间的点积。
- en: 'The resulting attention scores are as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果注意力分数如下：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Each element in the tensor represents an attention score between each pair of
    inputs, as we saw in figure 3.11\. Note that the values in that figure are normalized,
    which is why they differ from the unnormalized attention scores in the preceding
    tensor. We will take care of the normalization later.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 张量中的每个元素代表每对输入之间的注意力分数，正如我们在图3.11中看到的。请注意，图中的值是归一化的，这就是为什么它们与前面张量中的未归一化注意力分数不同。我们将在稍后处理归一化。
- en: 'When computing the preceding attention score tensor, we used `for` loops in
    Python. However, `for` loops are generally slow, and we can achieve the same results
    using matrix multiplication:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算前面的注意力分数张量时，我们使用了Python中的 `for` 循环。然而，`for` 循环通常较慢，我们可以通过矩阵乘法达到相同的结果：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can visually confirm that the results are the same as before:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地确认结果与之前相同：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In step 2 of figure 3.12, we normalize each row so that the values in each
    row sum to 1:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.12的第2步中，我们将每一行归一化，使得每行的值加起来为1：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This returns the following attention weight tensor that matches the values
    shown in figure 3.10:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回以下与图3.10中显示的值相匹配的注意力权重张量：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the context of using PyTorch, the dim parameter in functions like `torch.softmax`
    specifies the dimension of the input tensor along which the function will be computed.
    By setting `dim=-1`, we are instructing the `softmax` function to apply the normalization
    along the last dimension of the `attn_scores` tensor. If `attn_scores` is a two-dimensional
    tensor (for example, with a shape of [rows, columns]), it will normalize across
    the columns so that the values in each row (summing over the column dimension)
    sum up to 1.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用PyTorch的上下文中，函数如 `torch.softmax` 中的 dim 参数指定了函数将在哪个维度上对输入张量进行计算。通过设置 `dim=-1`，我们指示
    `softmax` 函数在 `attn_scores` 张量的最后一个维度上应用归一化。如果 `attn_scores` 是一个二维张量（例如，形状为 [rows,
    columns]），它将在列上归一化，使得每行的值（在列维度上求和）加起来为1。
- en: 'We can verify that the rows indeed all sum to 1:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证行确实都加起来为1：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The result is
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the third and final step of figure 3.12, we use these attention weights
    to compute all context vectors via matrix multiplication:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.12的第三和最后一步中，我们使用这些注意力权重通过矩阵乘法计算所有上下文向量：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the resulting output tensor, each row contains a three-dimensional context
    vector:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果输出张量中，每一行包含一个三维上下文向量：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can double-check that the code is correct by comparing the second row with
    the context vector *z*^((2)) that we computed in section 3.3.1:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将第二行与我们在3.3.1节中计算的上下文向量 *z*^((2)) 进行比较来双重检查代码的正确性：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Based on the result, we can see that the previously calculated `context_vec_2`
    matches the second row in the previous tensor exactly:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，我们可以看到之前计算的 `context_vec_2` 与前面张量的第二行完全匹配：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This concludes the code walkthrough of a simple self-attention mechanism. Next,
    we will add trainable weights, enabling the LLM to learn from data and improve
    its performance on specific tasks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了简单自注意力机制的代码讲解。接下来，我们将添加可训练权重，使LLM能够从数据中学习并提高其在特定任务上的性能。
- en: 3.4 Implementing self-attention with trainable weights
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 使用可训练权重实现自注意力
- en: Our next step will be to implement the self-attention mechanism used in the
    original transformer architecture, the GPT models, and most other popular LLMs.
    This self-attention mechanism is also called *scaled dot-product attention*. Figure
    3.13 shows how this self-attention mechanism fits into the broader context of
    implementing an LLM.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的步骤将是实现原始变压器架构、GPT模型以及大多数其他流行的LLMs中使用的自注意力机制。这种自注意力机制也称为*缩放点积注意力*。图3.13展示了这种自注意力机制如何融入实现LLM的更广泛背景中。
- en: '![figure](../Images/3-13.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-13.png)'
- en: Figure 3.13 Previously, we coded a simplified attention mechanism to understand
    the basic mechanism behind attention mechanisms. Now, we add trainable weights
    to this attention mechanism. Later, we will extend this self-attention mechanism
    by adding a causal mask and multiple heads.
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13 之前，我们编写了一个简化的注意力机制来理解注意力机制背后的基本机制。现在，我们向这个注意力机制添加可训练权重。稍后，我们将通过添加因果掩码和多个头扩展这种自注意力机制。
- en: 'As illustrated in figure 3.13, the self-attention mechanism with trainable
    weights builds on the previous concepts: we want to compute context vectors as
    weighted sums over the input vectors specific to a certain input element. As you
    will see, there are only slight differences compared to the basic self-attention
    mechanism we coded earlier.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.13所示，具有可训练权重的自注意力机制建立在先前概念之上：我们希望计算特定输入元素的输入向量上的加权求和作为上下文向量。您将看到，与之前编写的简单自注意力机制相比，只有细微的差异。
- en: The most notable difference is the introduction of weight matrices that are
    updated during model training. These trainable weight matrices are crucial so
    that the model (specifically, the attention module inside the model) can learn
    to produce “good” context vectors. (We will train the LLM in chapter 5.)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最显著的区别是引入了在模型训练期间更新的权重矩阵。这些可训练权重矩阵对于模型（特别是模型内的注意力模块）能够学会产生“良好”的上下文向量至关重要。（我们将在第5章训练LLM。）
- en: We will tackle this self-attention mechanism in the two subsections. First,
    we will code it step by step as before. Second, we will organize the code into
    a compact Python class that can be imported into the LLM architecture.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在两个小节中处理这种自注意力机制。首先，我们将像以前一样逐步编码它。其次，我们将代码组织成一个紧凑的Python类，可以导入到LLM架构中。
- en: 3.4.1 Computing the attention weights step by step
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 逐步计算注意力权重
- en: We will implement the self-attention mechanism step by step by introducing the
    three trainable weight matrices *W*[q], *W*[k], and *W*[v]. These three matrices
    are used to project the embedded input tokens, *x*^((i)), into query, key, and
    value vectors, respectively, as illustrated in figure 3.14.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过引入三个可训练权重矩阵*W*[q]、*W*[k]和*W*[v]逐步实现自注意力机制。这三个矩阵用于将嵌入输入标记*x*^((i))分别投影到查询、键和值向量，如图3.14所示。
- en: '![figure](../Images/3-14.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-14.png)'
- en: Figure 3.14 In the first step of the self-attention mechanism with trainable
    weight matrices, we compute query (q), key (k), and value (v) vectors for input
    elements x. Similar to previous sections, we designate the second input, x^((2)),
    as the query input. The query vector q^((2)) is obtained via matrix multiplication
    between the input x^((2)) and the weight matrix W[q]. Similarly, we obtain the
    key and value vectors via matrix multiplication involving the weight matrices
    W[k] and W[v].
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.14 在具有可训练权重矩阵的自注意力机制的第一步中，我们为输入元素x计算查询(q)、键(k)和值(v)向量。与前面的章节类似，我们将第二个输入x^((2))指定为查询输入。查询向量q^((2))是通过输入x^((2))与权重矩阵W[q]之间的矩阵乘法获得的。同样，我们通过涉及权重矩阵W[k]和W[v]的矩阵乘法获得键和值向量。
- en: Earlier, we defined the second input element *x*^((2)) as the query when we
    computed the simplified attention weights to compute the context vector *z*^((2)).
    Then we generalized this to compute all context vectors *z*^((1)) *... z*^((T))
    for the six-word input sentence “Your journey starts with one step.”
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，当我们计算简化注意力权重以计算上下文向量*z*^((2))时，我们将第二个输入元素*x*^((2))定义为查询。然后我们将其推广到计算六个单词输入句子“Your
    journey starts with one step.”的所有上下文向量*z*^((1)) *... z*^((T))。
- en: Similarly, we start here by computing only one context vector, *z*^((2)), for
    illustration purposes. We will then modify this code to calculate all context
    vectors.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们在这里仅为了说明目的计算一个上下文向量*z*^((2))。然后我们将修改此代码以计算所有上下文向量。
- en: 'Let’s begin by defining a few variables:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义几个变量开始：
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 The second input element'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第二个输入元素'
- en: '#2 The input embedding size, d=3'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 输入嵌入大小，d=3'
- en: '#3 The output embedding size, d_out=2'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 输出嵌入大小，d_out=2'
- en: Note that in GPT-like models, the input and output dimensions are usually the
    same, but to better follow the computation, we’ll use different input (`d_in=3`)
    and output (`d_out=2`) dimensions here.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在GPT-like模型中，输入和输出维度通常是相同的，但为了更好地跟踪计算，我们在这里使用不同的输入（`d_in=3`）和输出（`d_out=2`）维度。
- en: 'Next, we initialize the three weight matrices *W*[q], *W*[k], and *W*[v] shown
    in figure 3.14:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化图3.14中显示的三个权重矩阵 *W*[q]、*W*[k] 和 *W*[v]：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We set `requires_grad=False` to reduce clutter in the outputs, but if we were
    to use the weight matrices for model training, we would set `requires_grad=True`
    to update these matrices during model training.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `requires_grad=False` 设置为减少输出中的杂乱，但如果我们要使用权重矩阵进行模型训练，我们将在模型训练期间将这些矩阵设置为
    `requires_grad=True` 以更新这些矩阵。
- en: 'Next, we compute the query, key, and value vectors:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算查询、键和值向量：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output for the query results in a two-dimensional vector since we set the
    number of columns of the corresponding weight matrix, via `d_out`, to 2:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通过`d_out`设置了相应权重矩阵的列数为2，查询结果输出为一个二维向量：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Weight parameters vs. attention weights
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重参数与注意力权重对比
- en: In the weight matrices *W*, the term “weight” is short for “weight parameters,”
    the values of a neural network that are optimized during training. This is not
    to be confused with the attention weights. As we already saw, attention weights
    determine the extent to which a context vector depends on the different parts
    of the input (i.e., to what extent the network focuses on different parts of the
    input).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重矩阵 *W* 中，“权重”一词是“权重参数”的简称，这些参数是神经网络在训练过程中优化的值。这不同于注意力权重。正如我们之前看到的，注意力权重决定了上下文向量依赖于输入的不同部分的程度（即网络在多大程度上关注输入的不同部分）。
- en: In summary, weight parameters are the fundamental, learned coefficients that
    define the network’s connections, while attention weights are dynamic, context-specific
    values.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，权重参数是定义网络连接的基本、学习系数，而注意力权重是动态的、上下文特定的值。
- en: Even though our temporary goal is only to compute the one context vector, *z*^((2)),
    we still require the key and value vectors for all input elements as they are
    involved in computing the attention weights with respect to the query *q *^((2))
    (see figure 3.14).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的临时目标只是计算一个上下文向量，*z*^((2))，但我们仍然需要所有输入元素的键和值向量，因为它们涉及到计算与查询 *q*^((2)) 相关的注意力权重（见图3.14）。
- en: 'We can obtain all keys and values via matrix multiplication:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过矩阵乘法获得所有键和值：
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As we can tell from the outputs, we successfully projected the six input tokens
    from a three-dimensional onto a two-dimensional embedding space:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看出，我们成功地将六个输入标记从三维投影到二维嵌入空间：
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The second step is to compute the attention scores, as shown in figure 3.15.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是计算注意力分数，如图3.15所示。
- en: '![figure](../Images/3-15.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-15.png)'
- en: Figure 3.15 The attention score computation is a dot-product computation similar
    to what we used in the simplified self-attention mechanism in section 3.3\. The
    new aspect here is that we are not directly computing the dot-product between
    the input elements but using the query and key obtained by transforming the inputs
    via the respective weight matrices.
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.15 注意力分数的计算是一种点积计算，类似于我们在3.3节中使用的简化自注意力机制。这里的新特点是，我们不是直接计算输入元素之间的点积，而是使用通过各自的权重矩阵变换得到的查询和键。
- en: 'First, let’s compute the attention score ω[22]:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算注意力分数 ω[22]：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 Remember that Python starts indexing at 0.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 记住Python从0开始索引。'
- en: The result for the unnormalized attention score is
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 未归一化的注意力分数结果为
- en: '[PRE32]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Again, we can generalize this computation to all attention scores via matrix
    multiplication:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以通过矩阵乘法将此计算推广到所有注意力分数：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#1 All attention scores for given query'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 给定查询的所有注意力分数'
- en: 'As we can see, as a quick check, the second element in the output matches the
    `attn_score_22` we computed previously:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，作为一个快速检查，输出中的第二个元素与我们之前计算的`attn_score_22`相匹配：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we want to go from the attention scores to the attention weights, as illustrated
    in figure 3.16\. We compute the attention weights by scaling the attention scores
    and using the softmax function. However, now we scale the attention scores by
    dividing them by the square root of the embedding dimension of the keys (taking
    the square root is mathematically the same as exponentiating by 0.5):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要从注意力分数转换到注意力权重，如图3.16所示。我们通过缩放注意力分数并使用softmax函数来计算注意力权重。然而，现在我们通过将注意力分数除以键的嵌入维度的平方根来缩放注意力分数（取平方根在数学上等同于0.5次幂）：
- en: '[PRE35]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![figure](../Images/3-16.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-16.png)'
- en: Figure 3.16 After computing the attention scores ω, the next step is to normalize
    these scores using the softmax function to obtain the attention weights 𝛼.
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.16 计算完注意力分数 ω 后，下一步是使用softmax函数对这些分数进行归一化，以获得注意力权重 𝛼。
- en: The resulting attention weights are
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的注意力权重如下：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The rationale behind scaled-dot product attention
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缩放点积注意力的原理
- en: The reason for the normalization by the embedding dimension size is to improve
    the training performance by avoiding small gradients. For instance, when scaling
    up the embedding dimension, which is typically greater than 1,000 for GPT-like
    LLMs, large dot products can result in very small gradients during backpropagation
    due to the softmax function applied to them. As dot products increase, the softmax
    function behaves more like a step function, resulting in gradients nearing zero.
    These small gradients can drastically slow down learning or cause training to
    stagnate.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通过嵌入维度大小进行归一化的原因是通过避免小的梯度来提高训练性能。例如，当放大嵌入维度时，对于类似GPT的LLM，这通常大于1,000，由于应用了softmax函数，大点积在反向传播期间可能导致非常小的梯度。随着点积的增加，softmax函数表现得越来越像阶跃函数，导致梯度接近零。这些小的梯度可以极大地减慢学习速度或导致训练停滞。
- en: The scaling by the square root of the embedding dimension is the reason why
    this self-attention mechanism is also called scaled-dot product attention.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过嵌入维度的平方根进行缩放是为什么这种自注意力机制也被称为缩放点积注意力。
- en: Now, the final step is to compute the context vectors, as illustrated in figure
    3.17.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最终步骤是计算上下文向量，如图3.17所示。
- en: '![figure](../Images/3-17.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3-17.png)'
- en: Figure 3.17 In the final step of the self-attention computation, we compute
    the context vector by combining all value vectors via the attention weights.
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.17 在自注意力计算的最终步骤中，我们通过结合所有值向量并通过注意力权重来计算上下文向量。
- en: 'Similar to when we computed the context vector as a weighted sum over the input
    vectors (see section 3.3), we now compute the context vector as a weighted sum
    over the value vectors. Here, the attention weights serve as a weighting factor
    that weighs the respective importance of each value vector. Also as before, we
    can use matrix multiplication to obtain the output in one step:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们计算上下文向量作为输入向量的加权求和（参见第3.3节），我们现在将上下文向量计算为值向量的加权求和。在这里，注意力权重作为加权因子，衡量每个值向量的重要性。同样，正如之前一样，我们可以使用矩阵乘法一步获得输出：
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The contents of the resulting vector are as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 结果向量的内容如下：
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: So far, we’ve only computed a single context vector, *z*^((2)). Next, we will
    generalize the code to compute all context vectors in the input sequence, *z*^((1))
    to *z*^((T)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只计算了一个上下文向量，*z*^((2))。接下来，我们将通用代码来计算输入序列中的所有上下文向量，*z*^((1)) 到 *z*^((T))。
- en: Why query, key, and value?
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么是查询、键和值？
- en: The terms “key,” “query,” and “value” in the context of attention mechanisms
    are borrowed from the domain of information retrieval and databases, where similar
    concepts are used to store, search, and retrieve information.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力机制中，“键”、“查询”和“值”这些术语是从信息检索和数据库领域借用的，在这些领域中，类似的概念用于存储、搜索和检索信息。
- en: A *query* is analogous to a search query in a database. It represents the current
    item (e.g., a word or token in a sentence) the model focuses on or tries to understand.
    The query is used to probe the other parts of the input sequence to determine
    how much attention to pay to them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *查询* 类似于数据库中的搜索查询。它代表模型当前关注或试图理解的项目（例如，句子中的一个词或标记）。查询用于探测输入序列的其他部分，以确定对它们的注意力程度。
- en: The *key* is like a database key used for indexing and searching. In the attention
    mechanism, each item in the input sequence (e.g., each word in a sentence) has
    an associated key. These keys are used to match the query.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*键*类似于数据库键，用于索引和搜索。在注意力机制中，输入序列中的每个项目（例如，句子中的每个单词）都有一个相关的键。这些键用于匹配查询。'
- en: The *value* in this context is similar to the value in a key-value pair in a
    database. It represents the actual content or representation of the input items.
    Once the model determines which keys (and thus which parts of the input) are most
    relevant to the query (the current focus item), it retrieves the corresponding
    values.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，*值*类似于数据库中的键值对中的值。它表示输入项目的实际内容或表示。一旦模型确定哪些键（以及因此哪些输入部分）与查询（当前焦点项）最相关，它就会检索相应的值。
- en: 3.4.2 Implementing a compact self-attention Python class
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 实现紧凑的自注意力Python类
- en: At this point, we have gone through a lot of steps to compute the self-attention
    outputs. We did so mainly for illustration purposes so we could go through one
    step at a time. In practice, with the LLM implementation in the next chapter in
    mind, it is helpful to organize this code into a Python class, as shown in the
    following listing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经走过了许多步骤来计算自注意力输出。我们这样做主要是为了说明目的，以便我们可以一步一步地进行。在实践中，考虑到下一章中LLM的实现，将此代码组织成一个Python类是有帮助的，如下所示。
- en: Listing 3.1 A compact self-attention class
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1 紧凑的自注意力类
- en: '[PRE39]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this PyTorch code, `SelfAttention_v1` is a class derived from `nn.Module`,
    which is a fundamental building block of PyTorch models that provides necessary
    functionalities for model layer creation and management.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在此PyTorch代码中，`SelfAttention_v1`是一个从`nn.Module`派生的类，它是PyTorch模型的基本构建块，为模型层创建和管理提供必要的功能。
- en: The `__init__` method initializes trainable weight matrices (`W_query`, `W_key`,
    and `W_value`) for queries, keys, and values, each transforming the input dimension
    `d_in` to an output dimension `d_out`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__`方法初始化查询、键和值的可训练权重矩阵（`W_query`、`W_key`和`W_value`），每个矩阵将输入维度`d_in`转换为输出维度`d_out`。'
- en: During the forward pass, using the forward method, we compute the attention
    scores (`attn_scores`) by multiplying queries and keys, normalizing these scores
    using softmax. Finally, we create a context vector by weighting the values with
    these normalized attention scores.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播过程中，使用前向方法，我们通过乘以查询和键来计算注意力分数（`attn_scores`），使用softmax对这些分数进行归一化。最后，我们通过使用这些归一化的注意力分数来加权值，创建一个上下文向量。
- en: 'We can use this class as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此类如下：
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Since `inputs` contains six embedding vectors, this results in a matrix storing
    the six context vectors:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`inputs`包含六个嵌入向量，这导致存储六个上下文向量的矩阵：
- en: '[PRE41]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As a quick check, notice that the second row (`[0.3061,` `0.8210]`) matches
    the contents of `context_vec_2` in the previous section. Figure 3.18 summarizes
    the self-attention mechanism we just implemented.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 作为快速检查，请注意，第二行（`[0.3061,` `0.8210]`）与上一节中的`context_vec_2`的内容相匹配。图3.18总结了我们刚刚实现的自注意力机制。
- en: '![figure](../Images/3-18.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-18.png)'
- en: Figure 3.18 In self-attention, we transform the input vectors in the input matrix
    X with the three weight matrices, W[q], W[k], and W[v]. Then we compute the attention
    weight matrix based on the resulting queries (Q) and keys (K). Using the attention
    weights and values (V), we then compute the context vectors (Z). For visual clarity,
    we focus on a single input text with n tokens, not a batch of multiple inputs.
    Consequently, the three-dimensional input tensor is simplified to a two-dimensional
    matrix in this context. This approach allows for a more straightforward visualization
    and understanding of the processes involved. For consistency with later figures,
    the values in the attention matrix do not depict the real attention weights. (The
    numbers in this figure are truncated to two digits after the decimal point to
    reduce visual clutter. The values in each row should add up to 1.0 or 100%.)
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.18 在自注意力中，我们使用三个权重矩阵W[q]、W[k]和W[v]将输入矩阵X中的输入向量进行转换。然后我们根据生成的查询（Q）和键（K）计算注意力权重矩阵。使用注意力权重和值（V），然后计算上下文向量（Z）。为了视觉清晰，我们关注单个输入文本n个标记，而不是多个输入的批次。因此，在这个上下文中，三维输入张量简化为二维矩阵。这种方法允许更直观地可视化和理解涉及的过程。为了与后续的图保持一致，注意力矩阵中的值不表示真实的注意力权重。（此图中的数字被截断到小数点后两位以减少视觉杂乱。每行的值应加起来为1.0或100%。）
- en: Self-attention involves the trainable weight matrices *W*[q], *W*[k], and *W*[v].
    These matrices transform input data into queries, keys, and values, respectively,
    which are crucial components of the attention mechanism. As the model is exposed
    to more data during training, it adjusts these trainable weights, as we will see
    in upcoming chapters.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制涉及可训练的权重矩阵 *W*[q]、*W*[k] 和 *W*[v]。这些矩阵将输入数据转换为查询、键和值，分别是注意力机制的关键组成部分。随着模型在训练过程中接触到更多数据，它会调整这些可训练的权重，我们将在接下来的章节中看到这一点。
- en: We can improve the `SelfAttention_v1` implementation further by utilizing PyTorch’s
    `nn.Linear` layers, which effectively perform matrix multiplication when the bias
    units are disabled. Additionally, a significant advantage of using `nn.Linear`
    instead of manually implementing `nn.Parameter(torch.rand(...))` is that `nn.Linear`
    has an optimized weight initialization scheme, contributing to more stable and
    effective model training.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过利用 PyTorch 的 `nn.Linear` 层进一步改进 `SelfAttention_v1` 的实现，这些层在禁用偏置单元时有效地执行矩阵乘法。此外，使用
    `nn.Linear` 而不是手动实现 `nn.Parameter(torch.rand(...))` 的一个显著优点是 `nn.Linear` 具有优化的权重初始化方案，有助于更稳定和有效的模型训练。
- en: Listing 3.2 A self-attention class using PyTorch’s Linear layers
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.2 使用 PyTorch 线性层的自注意力类
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You can use the `SelfAttention_v2` similar to `SelfAttention_v1`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `SelfAttention_v2` 类似于 `SelfAttention_v1`：
- en: '[PRE43]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The output is
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that `SelfAttention_v1` and `SelfAttention_v2` give different outputs because
    they use different initial weights for the weight matrices since `nn.Linear` uses
    a more sophisticated weight initialization scheme.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`SelfAttention_v1` 和 `SelfAttention_v2` 给出不同的输出，因为它们使用不同的初始权重进行权重矩阵的初始化，而
    `nn.Linear` 使用了更复杂的权重初始化方案。
- en: Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 3.1 比较SelfAttention_v1和SelfAttention_v2
- en: Note that `nn.Linear` in `SelfAttention_v2` uses a different weight initialization
    scheme as `nn.Parameter(torch.rand(d_in,` `d_out))` used in `SelfAttention_v1`,
    which causes both mechanisms to produce different results. To check that both
    implementations, `SelfAttention_v1` and `SelfAttention_v2`, are otherwise similar,
    we can transfer the weight matrices from a `SelfAttention_v2` object to a `SelfAttention_v1`,
    such that both objects then produce the same results.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`SelfAttention_v2` 中的 `nn.Linear` 使用了与 `SelfAttention_v1` 中使用的 `nn.Parameter(torch.rand(d_in,
    d_out))` 不同的权重初始化方案，这导致两种机制产生不同的结果。为了验证 `SelfAttention_v1` 和 `SelfAttention_v2`
    这两种实现方式在其他方面是相似的，我们可以将 `SelfAttention_v2` 对象的权重矩阵转移到 `SelfAttention_v1` 对象中，这样两个对象就会产生相同的结果。
- en: 'Your task is to correctly assign the weights from an instance of `SelfAttention_v2`
    to an instance of `SelfAttention_v1`. To do this, you need to understand the relationship
    between the weights in both versions. (Hint: `nn.Linear` stores the weight matrix
    in a transposed form.) After the assignment, you should observe that both instances
    produce the same outputs.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是正确地将 `SelfAttention_v2` 实例的权重分配给 `SelfAttention_v1` 实例。为此，你需要理解两个版本中权重之间的关系。（提示：`nn.Linear`
    以转置形式存储权重矩阵。）分配后，你应该观察到两个实例产生相同的输出。
- en: Next, we will make enhancements to the self-attention mechanism, focusing specifically
    on incorporating causal and multi-head elements. The causal aspect involves modifying
    the attention mechanism to prevent the model from accessing future information
    in the sequence, which is crucial for tasks like language modeling, where each
    word prediction should only depend on previous words.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对自注意力机制进行改进，重点关注引入因果和多头元素。因果方面涉及修改注意力机制，以防止模型访问序列中的未来信息，这对于像语言模型这样的任务至关重要，其中每个单词预测应该只依赖于前面的单词。
- en: The multi-head component involves splitting the attention mechanism into multiple
    “heads.” Each head learns different aspects of the data, allowing the model to
    simultaneously attend to information from different representation subspaces at
    different positions. This improves the model’s performance in complex tasks.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 多头组件涉及将注意力机制分割成多个“头”。每个头学习数据的不同方面，允许模型在不同的位置同时关注来自不同表示子空间的信息。这提高了模型在复杂任务中的性能。
- en: 3.5 Hiding future words with causal attention
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 使用因果注意力隐藏未来单词
- en: For many LLM tasks, you will want the self-attention mechanism to consider only
    the tokens that appear prior to the current position when predicting the next
    token in a sequence. Causal attention, also known as *masked attention*, is a
    specialized form of self-attention. It restricts a model to only consider previous
    and current inputs in a sequence when processing any given token when computing
    attention scores. This is in contrast to the standard self-attention mechanism,
    which allows access to the entire input sequence at once.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多LLM任务，你可能希望自注意力机制只考虑当前位置之前出现的标记，当预测序列中的下一个标记时。因果注意力，也称为*屏蔽注意力*，是一种特殊形式的自注意力。它限制模型在计算注意力得分时只考虑序列中的先前和当前输入。这与标准的自注意力机制形成对比，后者允许一次访问整个输入序列。
- en: Now, we will modify the standard self-attention mechanism to create a *causal
    attention* mechanism, which is essential for developing an LLM in the subsequent
    chapters. To achieve this in GPT-like LLMs, for each token processed, we mask
    out the future tokens, which come after the current token in the input text, as
    illustrated in figure 3.19\. We mask out the attention weights above the diagonal,
    and we normalize the nonmasked attention weights such that the attention weights
    sum to 1 in each row. Later, we will implement this masking and normalization
    procedure in code.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将修改标准的自注意力机制，以创建一个*因果注意力*机制，这对于在后续章节中开发LLM至关重要。为了在类似GPT的LLM中实现这一点，对于每个处理的标记，我们将屏蔽掉输入文本中当前标记之后的未来标记，如图3.19所示。我们屏蔽掉对角线以上的注意力权重，并将非屏蔽的注意力权重归一化，使得每行的注意力权重总和为1。稍后，我们将在代码中实现这个屏蔽和归一化过程。
- en: '![figure](../Images/3-19.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/3-19.png)'
- en: Figure 3.19 In causal attention, we mask out the attention weights above the
    diagonal such that for a given input, the LLM can’t access future tokens when
    computing the context vectors using the attention weights. For example, for the
    word “journey” in the second row, we only keep the attention weights for the words
    before (“Your”) and in the current position (“journey”).
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.19 在因果注意力中，我们屏蔽掉对角线以上的注意力权重，这样对于给定的输入，LLM在计算上下文向量时无法访问未来的标记。例如，对于第二行中的单词“journey”，我们只保留“Your”和当前位置“journey”的注意力权重。
- en: 3.5.1 Applying a causal attention mask
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 应用因果注意力掩码
- en: Our next step is to implement the causal attention mask in code. To implement
    the steps to apply a causal attention mask to obtain the masked attention weights,
    as summarized in figure 3.20, let’s work with the attention scores and weights
    from the previous section to code the causal attention mechanism.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的步骤是在代码中实现因果注意力掩码。为了实现将因果注意力掩码应用于获取掩码注意力权重，如图3.20所示，让我们使用上一节中的注意力得分和权重来编写因果注意力机制。
- en: '![figure](../Images/3-20.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/3-20.png)'
- en: Figure 3.20 One way to obtain the masked attention weight matrix in causal attention
    is to apply the softmax function to the attention scores, zeroing out the elements
    above the diagonal and normalizing the resulting matrix.
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.20 在因果注意力中，通过应用softmax函数到注意力得分，将对角线以上的元素置零，并对结果矩阵进行归一化，这是一种获得掩码注意力权重矩阵的方法。
- en: 'In the first step, we compute the attention weights using the softmax function
    as we have done previously:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们使用softmax函数计算注意力权重，就像我们之前做的那样：
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '#1 Reuses the query and key weight matrices of the SelfAttention_v2 object
    from the previous section for convenience'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 重用上一节中SelfAttention_v2对象的查询和键权重矩阵以方便起见'
- en: 'This results in the following attention weights:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下注意力权重：
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can implement the second step using PyTorch’s `tril` function to create
    a mask where the values above the diagonal are zero:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PyTorch的`tril`函数来实现第二步，创建一个对角线上方值为零的掩码：
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The resulting mask is
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最终得到的掩码是
- en: '[PRE48]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, we can multiply this mask with the attention weights to zero-out the values
    above the diagonal:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这个掩码与注意力权重相乘，以屏蔽对角线以上的值：
- en: '[PRE49]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As we can see, the elements above the diagonal are successfully zeroed out:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，对角线以上的元素已被成功置零：
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The third step is to renormalize the attention weights to sum up to 1 again
    in each row. We can achieve this by dividing each element in each row by the sum
    in each row:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是将注意力权重重新归一化，使每行的总和再次为1。我们可以通过将每行中的每个元素除以该行的总和来实现这一点：
- en: '[PRE51]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The result is an attention weight matrix where the attention weights above
    the diagonal are zeroed-out, and the rows sum to 1:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个注意力权重矩阵，其中对角线以上的注意力权重被置零，且每行的和为1：
- en: '[PRE52]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Information leakage
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 信息泄露
- en: When we apply a mask and then renormalize the attention weights, it might initially
    appear that information from future tokens (which we intend to mask) could still
    influence the current token because their values are part of the softmax calculation.
    However, the key insight is that when we renormalize the attention weights after
    masking, what we’re essentially doing is recalculating the softmax over a smaller
    subset (since masked positions don’t contribute to the softmax value).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用掩码并重新归一化注意力权重时，最初可能看起来未来标记（我们打算掩码）的信息仍然可能影响当前标记，因为它们的值是softmax计算的一部分。然而，关键洞察是，当我们对掩码后的注意力权重进行重新归一化时，我们实际上是在对更小的子集重新计算softmax（因为掩码位置不贡献softmax值）。
- en: The mathematical elegance of softmax is that despite initially including all
    positions in the denominator, after masking and renormalizing, the effect of the
    masked positions is nullified—they don’t contribute to the softmax score in any
    meaningful way.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax的数学优雅之处在于，尽管最初在分母中包含所有位置，但在掩码和重新归一化之后，掩码位置的影响被消除——它们不会以任何有意义的方式对softmax分数做出贡献。
- en: In simpler terms, after masking and renormalization, the distribution of attention
    weights is as if it was calculated only among the unmasked positions to begin
    with. This ensures there’s no information leakage from future (or otherwise masked)
    tokens as we intended.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 用更简单的话说，在掩码和重新归一化之后，注意力权重的分布就像一开始只在对角线以上的位置计算一样。这确保了没有信息泄露到未来（或被掩码的）标记，正如我们打算的那样。
- en: While we could wrap up our implementation of causal attention at this point,
    we can still improve it. Let’s take a mathematical property of the softmax function
    and implement the computation of the masked attention weights more efficiently
    in fewer steps, as shown in figure 3.21.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以在这一点上完成因果注意力的实现，但我们仍然可以改进它。让我们利用softmax函数的一个数学特性，并在图3.21中展示的更少的步骤中更高效地计算掩码注意力权重。
- en: '![figure](../Images/3-21.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-21.png)'
- en: Figure 3.21 A more efficient way to obtain the masked attention weight matrix
    in causal attention is to mask the attention scores with negative infinity values
    before applying the softmax function.
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.21 在因果注意力中，获取掩码注意力权重矩阵的一种更高效的方法是在应用softmax函数之前用负无穷大值掩码注意力分数。
- en: The softmax function converts its inputs into a probability distribution. When
    negative infinity values (`-`∞) are present in a row, the softmax function treats
    them as zero probability. (Mathematically, this is because *e  *^–^∞ approaches
    0.)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax函数将其输入转换为概率分布。当一行中存在负无穷大值（`-∞`）时，softmax函数将其视为零概率。（从数学上讲，这是因为 *e*^(-∞)
    趋近于0。）
- en: 'We can implement this more efficient masking “trick” by creating a mask with
    1s above the diagonal and then replacing these 1s with negative infinity (`-inf`)
    values:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过创建一个对角线以上的1s掩码，然后将这些1s替换为负无穷大（`-inf`）值来实现这个更高效的掩码“技巧”：
- en: '[PRE53]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This results in the following mask:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下掩码：
- en: '[PRE54]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now all we need to do is apply the softmax function to these masked results,
    and we are done:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要做的就是将这些掩码结果应用softmax函数，任务就完成了：
- en: '[PRE55]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'As we can see based on the output, the values in each row sum to 1, and no
    further normalization is necessary:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，每行的值之和为1，因此不需要进一步归一化：
- en: '[PRE56]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We could now use the modified attention weights to compute the context vectors
    via `context_vec` `=` `attn_weights` `@` `values`, as in section 3.4\. However,
    we will first cover another minor tweak to the causal attention mechanism that
    is useful for reducing overfitting when training LLMs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用修改后的注意力权重通过`context_vec` `=` `attn_weights` `@` `values`来计算上下文向量，正如3.4节中所述。然而，我们首先将介绍对因果注意力机制的一个小调整，这对于在训练LLMs时减少过拟合是有用的。
- en: 3.5.2 Masking additional attention weights with dropout
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 使用dropout掩码额外的注意力权重
- en: '*Dropout* in deep learning is a technique where randomly selected hidden layer
    units are ignored during training, effectively “dropping” them out. This method
    helps prevent overfitting by ensuring that a model does not become overly reliant
    on any specific set of hidden layer units. It’s important to emphasize that dropout
    is only used during training and is disabled afterward.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的*Dropout*是一种技术，在训练期间随机选择隐藏层单元被忽略，有效地“丢弃”它们。这种方法通过确保模型不会过度依赖任何特定的隐藏层单元集来帮助防止过拟合。重要的是要强调，dropout仅在训练期间使用，并在之后被禁用。
- en: 'In the transformer architecture, including models like GPT, dropout in the
    attention mechanism is typically applied at two specific times: after calculating
    the attention weights or after applying the attention weights to the value vectors.
    Here we will apply the dropout mask after computing the attention weights, as
    illustrated in figure 3.22, because it’s the more common variant in practice.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在transformer架构中，包括GPT等模型，注意力机制中的dropout通常在两个特定时间应用：在计算注意力权重之后或应用注意力权重到值向量之后。在这里，我们将像图3.22所示的那样在计算注意力权重后应用dropout掩码，因为在实践中这是更常见的变体。
- en: '![figure](../Images/3-22.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-22.png)'
- en: Figure 3.22 Using the causal attention mask (upper left), we apply an additional
    dropout mask (upper right) to zero out additional attention weights to reduce
    overfitting during training.
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.22 使用因果注意力掩码（左上角），我们应用一个额外的dropout掩码（右上角）来置零更多的注意力权重，以减少训练过程中的过拟合。
- en: 'In the following code example, we use a dropout rate of 50%, which means masking
    out half of the attention weights. (When we train the GPT model in later chapters,
    we will use a lower dropout rate, such as 0.1 or 0.2.) We apply PyTorch’s dropout
    implementation first to a 6 × 6 tensor consisting of 1s for simplicity:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们使用50%的dropout率，这意味着屏蔽掉一半的注意力权重。（当我们后续章节中训练GPT模型时，我们将使用较低的dropout率，如0.1或0.2。）我们首先应用PyTorch的dropout实现到一个由1组成的6×6张量，以简化操作：
- en: '[PRE57]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#1 We choose a dropout rate of 50%.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们选择50%的dropout率。'
- en: '#2 Here, we create a matrix of 1s.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在这里，我们创建一个由1组成的矩阵。'
- en: 'As we can see, approximately half of the values are zeroed out:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，大约一半的值被置零：
- en: '[PRE58]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: When applying dropout to an attention weight matrix with a rate of 50%, half
    of the elements in the matrix are randomly set to zero. To compensate for the
    reduction in active elements, the values of the remaining elements in the matrix
    are scaled up by a factor of 1/0.5 = 2\. This scaling is crucial to maintain the
    overall balance of the attention weights, ensuring that the average influence
    of the attention mechanism remains consistent during both the training and inference
    phases.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 当以50%的比率对注意力权重矩阵应用dropout时，矩阵中一半的元素被随机设置为零。为了补偿活动元素数量的减少，矩阵中剩余元素的价值以1/0.5 =
    2的因子放大。这种缩放对于保持注意力权重的整体平衡至关重要，确保在训练和推理阶段注意力机制的平均影响保持一致。
- en: 'Now let’s apply dropout to the attention weight matrix itself:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将dropout应用于注意力权重矩阵本身：
- en: '[PRE59]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The resulting attention weight matrix now has additional elements zeroed out
    and the remaining 1s rescaled:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的注意力权重矩阵现在有额外的元素被置零，剩余的1被重新缩放：
- en: '[PRE60]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note that the resulting dropout outputs may look different depending on your
    operating system; you can read more about this inconsistency here on the PyTorch
    issue tracker at [https://github.com/pytorch/pytorch/issues/121595](https://github.com/pytorch/pytorch/issues/121595).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于操作系统的不同，生成的dropout输出可能会有所不同；你可以在PyTorch问题跟踪器上了解更多关于这种不一致性的信息，链接为[https://github.com/pytorch/pytorch/issues/121595](https://github.com/pytorch/pytorch/issues/121595)。
- en: Having gained an understanding of causal attention and dropout masking, we can
    now develop a concise Python class. This class is designed to facilitate the efficient
    application of these two techniques.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了因果注意力和dropout掩码之后，我们现在可以开发一个简洁的Python类。这个类旨在促进这两种技术的有效应用。
- en: 3.5.3 Implementing a compact causal attention class
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 实现紧凑的因果注意力类
- en: We will now incorporate the causal attention and dropout modifications into
    the `SelfAttention` Python class we developed in section 3.4\. This class will
    then serve as a template for developing *multi-head attention*, which is the final
    attention class we will implement.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将因果注意力和dropout修改纳入我们在3.4节中开发的`SelfAttention`Python类。这个类将作为开发*多头注意力*的模板，这是我们最终要实现的注意力类。
- en: But before we begin, let’s ensure that the code can handle batches consisting
    of more than one input so that the `CausalAttention` class supports the batch
    outputs produced by the data loader we implemented in chapter 2.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们开始之前，让我们确保代码可以处理包含多个输入的批次，这样`CausalAttention`类就能支持我们在第2章中实现的数据加载器产生的批输出。
- en: 'For simplicity, to simulate such batch inputs, we duplicate the input text
    example:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，为了模拟这样的批输入，我们复制了输入文本示例：
- en: '[PRE61]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#1 Two inputs with six tokens each; each token has embedding dimension 3.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 两个输入，每个输入有六个标记；每个标记的嵌入维度为3。'
- en: 'This results in a three-dimensional tensor consisting of two input texts with
    six tokens each, where each token is a three-dimensional embedding vector:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个三维张量，由两个输入文本组成，每个输入文本有六个标记，其中每个标记是一个三维嵌入向量：
- en: '[PRE62]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The following `CausalAttention` class is similar to the `SelfAttention` class
    we implemented earlier, except that we added the dropout and causal mask components.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`CausalAttention`类与我们之前实现的`SelfAttention`类相似，除了我们添加了dropout和因果掩码组件。
- en: Listing 3.3 A compact causal attention class
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3 一个紧凑的因果注意力类
- en: '[PRE63]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '#1 Compared to the previous SelfAttention_v1 class, we added a dropout layer.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 与之前的SelfAttention_v1类相比，我们添加了一个dropout层。'
- en: '#2 The register_buffer call is also a new addition (more information is provided
    in the following text).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 `register_buffer`调用也是一个新增功能（更多信息将在下文提供）。'
- en: '#3 We transpose dimensions 1 and 2, keeping the batch dimension at the first
    position (0).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们交换维度1和2，保持批维度在第一个位置（0）。'
- en: '#4 In PyTorch, operations with a trailing underscore are performed in-place,
    avoiding unnecessary memory copies.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 在PyTorch中，带有尾随下划线的操作是在原地执行的，避免了不必要的内存复制。'
- en: While all added code lines should be familiar at this point, we now added a
    `self .register_buffer()` call in the `__init__` method. The use of `register_buffer`
    in PyTorch is not strictly necessary for all use cases but offers several advantages
    here. For instance, when we use the `CausalAttention` class in our LLM, buffers
    are automatically moved to the appropriate device (CPU or GPU) along with our
    model, which will be relevant when training our LLM. This means we don’t need
    to manually ensure these tensors are on the same device as your model parameters,
    avoiding device mismatch errors.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此时所有添加的代码行都应该熟悉，但我们现在在`__init__`方法中添加了一个`self.register_buffer()`调用。在PyTorch中，`register_buffer`的使用对于所有用例并非严格必要，但在这里提供了几个优点。例如，当我们在我们的大型语言模型（LLM）中使用`CausalAttention`类时，缓冲区会自动移动到与我们的模型相同的设备（CPU或GPU）上，这在训练我们的LLM时将是相关的。这意味着我们不需要手动确保这些张量与我们的模型参数位于同一设备上，从而避免了设备不匹配错误。
- en: 'We can use the `CausalAttention` class as follows, similar to `SelfAttention`
    previously:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`CausalAttention`类如下，类似于之前实现的`SelfAttention`：
- en: '[PRE64]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The resulting context vector is a three-dimensional tensor where each token
    is now represented by a two-dimensional embedding:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 结果上下文向量是一个三维张量，其中每个标记现在由一个二维嵌入表示：
- en: '[PRE65]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Figure 3.23 summarizes what we have accomplished so far. We have focused on
    the concept and implementation of causal attention in neural networks. Next, we
    will expand on this concept and implement a multi-head attention module that implements
    several causal attention mechanisms in parallel.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23总结了我们迄今为止所取得的成果。我们专注于神经网络中因果注意力的概念和实现。接下来，我们将在此基础上扩展这个概念，并实现一个多头注意力模块，该模块并行实现多个因果注意力机制。
- en: '![figure](../Images/3-23.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-23.png)'
- en: Figure 3.23 Here’s what we’ve done so far. We began with a simplified attention
    mechanism, added trainable weights, and then added a causal attention mask. Next,
    we will extend the causal attention mechanism and code multi-head attention, which
    we will use in our LLM.
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.23 这是我们迄今为止所做的工作。我们从简化的注意力机制开始，添加了可训练的权重，然后添加了因果注意力掩码。接下来，我们将扩展因果注意力机制，并实现多头注意力，我们将在我们的LLM中使用它。
- en: 3.6 Extending single-head attention to multi-head attention
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 将单头注意力扩展到多头注意力
- en: Our final step will be to extend the previously implemented causal attention
    class over multiple heads. This is also called *multi-head attention*.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步将是将之前实现的因果注意力类扩展到多个头。这也被称为*多头注意力*。
- en: The term “multi-head” refers to dividing the attention mechanism into multiple
    “heads,” each operating independently. In this context, a single causal attention
    module can be considered single-head attention, where there is only one set of
    attention weights processing the input sequentially.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: “多头”一词指的是将注意力机制分成多个“头”，每个头独立操作。在这个上下文中，一个单因果注意力模块可以被认为是单头注意力，其中只有一个注意力权重集按顺序处理输入。
- en: We will tackle this expansion from causal attention to multi-head attention.
    First, we will intuitively build a multi-head attention module by stacking multiple
    `CausalAttention` modules. Then we will then implement the same multi-head attention
    module in a more complicated but more computationally efficient way.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个因果注意力扩展到多头注意力。首先，我们将通过堆叠多个`CausalAttention`模块直观地构建一个多头注意力模块。然后，我们将以更复杂但更计算高效的方式实现相同的多头注意力模块。
- en: 3.6.1 Stacking multiple single-head attention layers
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 堆叠多个单头注意力层
- en: In practical terms, implementing multi-head attention involves creating multiple
    instances of the self-attention mechanism (see figure 3.18), each with its own
    weights, and then combining their outputs. Using multiple instances of the self-attention
    mechanism can be computationally intensive, but it’s crucial for the kind of complex
    pattern recognition that models like transformer-based LLMs are known for.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，实现多头注意力机制涉及创建多个自注意力机制的实例（见图3.18），每个实例都有自己的权重，然后将它们的输出组合起来。使用多个自注意力机制的实例可能会计算量较大，但对于像基于transformer的LLM模型所擅长的复杂模式识别来说，这是至关重要的。
- en: Figure 3.24 illustrates the structure of a multi-head attention module, which
    consists of multiple single-head attention modules, as previously depicted in
    figure 3.18, stacked on top of each other.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24展示了多头注意力模块的结构，它由多个单头注意力模块组成，如图3.18中所示，它们相互堆叠。
- en: '![figure](../Images/3-24.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-24.png)'
- en: 'Figure 3.24 The multi-head attention module includes two single-head attention
    modules stacked on top of each other. So, instead of using a single matrix W[v]
    for computing the value matrices, in a multi-head attention module with two heads,
    we now have two value weight matrices: W[v1] and W[v2]. The same applies to the
    other weight matrices, W[Q] and W[k]. We obtain two sets of context vectors Z[1]
    and Z[2] that we can combine into a single context vector matrix Z.'
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.24 多头注意力模块包括两个单头注意力模块，它们相互堆叠。因此，在具有两个头的多头注意力模块中，我们不再使用单个矩阵W[v]来计算值矩阵，而是有两个值权重矩阵：W[v1]和W[v2]。其他权重矩阵，如W[Q]和W[k]，也是如此。我们获得两组上下文向量Z[1]和Z[2]，我们可以将它们组合成一个单一的上下文向量矩阵Z。
- en: As mentioned before, the main idea behind multi-head attention is to run the
    attention mechanism multiple times (in parallel) with different, learned linear
    projections—the results of multiplying the input data (like the query, key, and
    value vectors in attention mechanisms) by a weight matrix. In code, we can achieve
    this by implementing a simple `MultiHeadAttentionWrapper` class that stacks multiple
    instances of our previously implemented `CausalAttention` module.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，多头注意力的主要思想是多次（并行）运行注意力机制，使用不同的、学习到的线性投影——通过权重矩阵乘以输入数据（如注意力机制中的查询、键和值向量）的结果。在代码中，我们可以通过实现一个简单的`MultiHeadAttentionWrapper`类来实现这一点，该类堆叠了我们之前实现的`CausalAttention`模块的多个实例。
- en: Listing 3.4 A wrapper class to implement multi-head attention
  id: totrans-314
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.4 实现多头注意力的包装类
- en: '[PRE66]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: For example, if we use this `MultiHeadAttentionWrapper` class with two attention
    heads (via `num_heads=2`) and `CausalAttention` output dimension `d_out=2`, we
    get a four-dimensional context vector (`d_out*num_heads=4`), as depicted in figure
    3.25.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们使用这个`MultiHeadAttentionWrapper`类，具有两个注意力头（通过`num_heads=2`）和`CausalAttention`输出维度`d_out=2`，我们得到一个四维的上下文向量（`d_out*num_heads=4`），如图3.25所示。
- en: '![figure](../Images/3-25.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-25.png)'
- en: Figure 3.25 Using the `MultiHeadAttentionWrapper`, we specified the number of
    attention heads (`num_heads`). If we set `num_heads=2`, as in this example, we
    obtain a tensor with two sets of context vector matrices. In each context vector
    matrix, the rows represent the context vectors corresponding to the tokens, and
    the columns correspond to the embedding dimension specified via `d_out=4`. We
    concatenate these context vector matrices along the column dimension. Since we
    have two attention heads and an embedding dimension of 2, the final embedding
    dimension is 2 × 2 = 4.
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.25 使用`MultiHeadAttentionWrapper`，我们指定了注意力头部的数量(`num_heads`)。如果我们设置`num_heads=2`，如本例所示，我们得到一个包含两套上下文向量矩阵的张量。在每个上下文向量矩阵中，行表示与标记对应的上下文向量，列对应通过`d_out=4`指定的嵌入维度。我们沿着列维度连接这些上下文向量矩阵。由于我们有两个注意力头部和一个嵌入维度为2，最终的嵌入维度是2
    × 2 = 4。
- en: 'To illustrate this further with a concrete example, we can use the `MultiHeadAttentionWrapper`
    class similar to the `CausalAttention` class before:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步用具体例子说明这一点，我们可以使用与之前的`CausalAttention`类相似的`MultiHeadAttentionWrapper`类：
- en: '[PRE67]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This results in the following tensor representing the context vectors:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下张量表示上下文向量：
- en: '[PRE68]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The first dimension of the resulting `context_vecs` tensor is 2 since we have
    two input texts (the input texts are duplicated, which is why the context vectors
    are exactly the same for those). The second dimension refers to the 6 tokens in
    each input. The third dimension refers to the four-dimensional embedding of each
    token.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的`context_vecs`张量的第一维是2，因为我们有两个输入文本（输入文本被复制了，这就是为什么那些上下文向量完全相同）。第二维指的是每个输入中的6个标记。第三维指的是每个标记的四维嵌入。
- en: Exercise 3.2 Returning two-dimensional embedding vectors
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习3.2 返回二维嵌入向量
- en: 'Change the input arguments for the `MultiHeadAttentionWrapper(...,` `num_ heads=2)`
    call such that the output context vectors are two-dimensional instead of four
    dimensional while keeping the setting `num_heads=2`. Hint: You don’t have to modify
    the class implementation; you just have to change one of the other input arguments.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 修改`MultiHeadAttentionWrapper(...,` `num_heads=2)`调用的输入参数，使得输出上下文向量是二维的，而不是四维的，同时保持`num_heads=2`的设置。提示：您不需要修改类实现；您只需更改其他输入参数之一。
- en: Up to this point, we have implemented a `MultiHeadAttentionWrapper` that combined
    multiple single-head attention modules. However, these are processed sequentially
    via `[head(x)` `for` `head` `in` `self.heads]` in the forward method. We can improve
    this implementation by processing the heads in parallel. One way to achieve this
    is by computing the outputs for all attention heads simultaneously via matrix
    multiplication.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了一个`MultiHeadAttentionWrapper`，它结合了多个单头注意力模块。然而，这些模块在正向方法中是顺序处理的，通过`[head(x)`
    `for` `head` `in` `self.heads]`。我们可以通过并行处理头部来改进这个实现。实现这一目标的一种方法是通过矩阵乘法同时计算所有注意力头部的输出。
- en: 3.6.2 Implementing multi-head attention with weight splits
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.2 使用权重拆分实现多头注意力
- en: So far, we have created a `MultiHeadAttentionWrapper` to implement multi-head
    attention by stacking multiple single-head attention modules. This was done by
    instantiating and combining several `CausalAttention` objects.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了一个`MultiHeadAttentionWrapper`，通过堆叠多个单头注意力模块来实现多头注意力。这是通过实例化和组合几个`CausalAttention`对象来完成的。
- en: Instead of maintaining two separate classes, `MultiHeadAttentionWrapper` and
    `CausalAttention`, we can combine these concepts into a single `MultiHeadAttention`
    class. Also, in addition to merging the `MultiHeadAttentionWrapper` with the `CausalAttention`
    code, we will make some other modifications to implement multi-head attention
    more efficiently.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不是维护两个独立的类，`MultiHeadAttentionWrapper`和`CausalAttention`，而是将这些概念合并成一个`MultiHeadAttention`类。此外，除了将`MultiHeadAttentionWrapper`与`CausalAttention`代码合并之外，我们还将进行一些其他修改，以更有效地实现多头注意力。
- en: In the `MultiHeadAttentionWrapper`, multiple heads are implemented by creating
    a list of `CausalAttention` objects (`self.heads`), each representing a separate
    attention head. The `CausalAttention` class independently performs the attention
    mechanism, and the results from each head are concatenated. In contrast, the following
    `MultiHeadAttention` class integrates the multi-head functionality within a single
    class. It splits the input into multiple heads by reshaping the projected query,
    key, and value tensors and then combines the results from these heads after computing
    attention.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `MultiHeadAttentionWrapper` 中，通过创建一个 `CausalAttention` 对象列表（`self.heads`），每个对象代表一个单独的注意力头，实现了多个头。`CausalAttention`
    类独立执行注意力机制，并将每个头的输出连接起来。相比之下，下面的 `MultiHeadAttention` 类在单个类中集成了多头功能。它通过重塑投影查询、键和值张量来分割输入，然后在计算注意力后结合这些头的输出。
- en: Let’s take a look at the `MultiHeadAttention` class before we discuss it further.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论之前，让我们看看 `MultiHeadAttention` 类。
- en: Listing 3.5 An efficient multi-head attention class
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5 一个高效的多头注意力类
- en: '[PRE69]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '#1 Reduces the projection dim to match the desired output dim'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将投影维度减少以匹配所需的输出维度'
- en: '#2 Uses a Linear layer to combine head outputs'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用线性层来组合头输出'
- en: '#3 Tensor shape: (b, num_tokens, d_out)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 张量形状：(b, num_tokens, d_out)'
- en: '#4 We implicitly split the matrix by adding a num_heads dimension. Then we
    unroll the last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 我们通过添加一个 num_heads 维度隐式地分割了矩阵。然后我们展开最后一个维度：(b, num_tokens, d_out) -> (b,
    num_tokens, num_heads, head_dim)。'
- en: '#5 Transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads,
    num_tokens, head_dim)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从形状 (b, num_tokens, num_heads, head_dim) 转置到 (b, num_heads, num_tokens,
    head_dim)'
- en: '#6 Computes dot product for each head'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 为每个头计算点积'
- en: '#7 Masks truncated to the number of tokens'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 掩码截断到令牌数量'
- en: '#8 Uses the mask to fill attention scores'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 使用掩码填充注意力分数'
- en: '#9 Tensor shape: (b, num_tokens, n_heads, head_dim)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 张量形状：(b, num_tokens, n_heads, head_dim)'
- en: '#10 Combines heads, where self.d_out = self.num_heads * self.head_dim'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 合并头，其中 self.d_out = self.num_heads * self.head_dim'
- en: '#11 Adds an optional linear projection'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 添加一个可选的线性投影'
- en: Even though the reshaping (`.view`) and transposing (`.transpose`) of tensors
    inside the `MultiHeadAttention` class looks very mathematically complicated, the
    `MultiHeadAttention` class implements the same concept as the `MultiHeadAttentionWrapper`
    earlier.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 `MultiHeadAttention` 类内部张量的重塑（`.view`）和转置（`.transpose`）看起来非常数学复杂，但 `MultiHeadAttention`
    类实现了与之前 `MultiHeadAttentionWrapper` 相同的概念。
- en: On a big-picture level, in the previous `MultiHeadAttentionWrapper`, we stacked
    multiple single-head attention layers that we combined into a multi-head attention
    layer. The `MultiHeadAttention` class takes an integrated approach. It starts
    with a multi-head layer and then internally splits this layer into individual
    attention heads, as illustrated in figure 3.26.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 从宏观角度来看，在之前的 `MultiHeadAttentionWrapper` 中，我们堆叠了多个单头注意力层，并将它们组合成一个多头注意力层。`MultiHeadAttention`
    类采用了一种综合方法。它从一个多头层开始，然后内部将这个层分割成单独的注意力头，如图3.26所示。
- en: '![figure](../Images/3-26.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/3-26.png)'
- en: Figure 3.26 In the `MultiHeadAttentionWrapper` class with two attention heads,
    we initialized two weight matrices, W[q1] and W[q2], and computed two query matrices,
    Q[1] and Q[2] (top). In the `MultiheadAttention` class, we initialize one larger
    weight matrix W[q], only perform one matrix multiplication with the inputs to
    obtain a query matrix Q, and then split the query matrix into Q[1] and Q[2] (bottom).
    We do the same for the keys and values, which are not shown to reduce visual clutter.
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.26 在具有两个注意力头的 `MultiHeadAttentionWrapper` 类中，我们初始化了两个权重矩阵 W[q1] 和 W[q2]，并计算了两个查询矩阵
    Q[1] 和 Q[2]（顶部）。在 `MultiheadAttention` 类中，我们初始化了一个更大的权重矩阵 W[q]，只与输入进行一次矩阵乘法以获得查询矩阵
    Q，然后将查询矩阵分割成 Q[1] 和 Q[2]（底部）。我们对键和值也做了同样的处理，这里没有展示以减少视觉混乱。
- en: The splitting of the query, key, and value tensors is achieved through tensor
    reshaping and transposing operations using PyTorch’s `.view` and `.transpose`
    methods. The input is first transformed (via linear layers for queries, keys,
    and values) and then reshaped to represent multiple heads.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 PyTorch 的 `.view` 和 `.transpose` 方法进行张量重塑和转置操作，实现了查询、键和值张量的分割。输入首先通过查询、键和值的线性层进行转换，然后重塑以表示多个头。
- en: 'The key operation is to split the `d_out` dimension into `num_heads` and `head_dim`,
    where `head_dim` `=` `d_out` `/` `num_heads`. This splitting is then achieved
    using the `.view` method: a tensor of dimensions `(b,` `num_tokens,` `d_out)`
    is reshaped to dimension `(b,` `num_tokens,` `num_heads,` `head_dim)`.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 关键操作是将`d_out`维度分割成`num_heads`和`head_dim`，其中`head_dim` `=` `d_out` `/` `num_heads`。然后使用`.view`方法实现这种分割：将维度为`(b,
    num_tokens, d_out)`的张量重塑为维度`(b, num_tokens, num_heads, head_dim)`。
- en: The tensors are then transposed to bring the `num_heads` dimension before the
    `num_ tokens` dimension, resulting in a shape of `(b,` `num_heads,` `num_tokens,`
    `head_dim)`. This transposition is crucial for correctly aligning the queries,
    keys, and values across the different heads and performing batched matrix multiplications
    efficiently.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将张量转置，以便将`num_heads`维度放在`num_tokens`维度之前，从而得到形状为`(b, num_heads, num_tokens,
    head_dim)`。这种转置对于正确地对齐不同头部的查询、键和值以及高效地执行批量矩阵乘法至关重要。
- en: 'To illustrate this batched matrix multiplication, suppose we have the following
    tensor:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种批量矩阵乘法，假设我们有一个以下张量：
- en: '[PRE70]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '#1 The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2,
    3, 4).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 这个张量的形状是(b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)。'
- en: 'Now we perform a batched matrix multiplication between the tensor itself and
    a view of the tensor where we transposed the last two dimensions, `num_tokens`
    and `head_dim`:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对张量本身及其一个视图执行批量矩阵乘法，其中我们转置了最后两个维度，即`num_tokens`和`head_dim`：
- en: '[PRE71]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The result is
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE72]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: In this case, the matrix multiplication implementation in PyTorch handles the
    four-dimensional input tensor so that the matrix multiplication is carried out
    between the two last dimensions `(num_tokens,` `head_dim)` and then repeated for
    the individual heads.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，PyTorch中的矩阵乘法实现处理了四维输入张量，使得矩阵乘法在最后两个维度`(num_tokens, head_dim)`之间进行，然后为每个单独的头重复执行。
- en: 'For instance, the preceding becomes a more compact way to compute the matrix
    multiplication for each head separately:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，前面的方法成为了一种更紧凑的方式来分别计算每个头的矩阵乘法：
- en: '[PRE73]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The results are exactly the same results as those we obtained when using the
    batched matrix multiplication `print(a` `@` `a.transpose(2,` `3))`:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与使用批量矩阵乘法`print(a @ a.transpose(2, 3))`得到的结果完全相同：
- en: '[PRE74]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Continuing with `MultiHeadAttention`, after computing the attention weights
    and context vectors, the context vectors from all heads are transposed back to
    the shape `(b,` `num_tokens,` `num_heads,` `head_dim)`. These vectors are then
    reshaped (flattened) into the shape `(b,` `num_tokens,` `d_out)`, effectively
    combining the outputs from all heads.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用`MultiHeadAttention`，在计算注意力权重和上下文向量之后，所有头部的上下文向量被转置回形状`(b, num_tokens, num_heads,
    head_dim)`。然后这些向量被重塑（展平）为形状`(b, num_tokens, d_out)`，有效地结合了所有头部的输出。
- en: Additionally, we added an output projection layer (`self.out_proj`) to `MultiHeadAttention`
    after combining the heads, which is not present in the `CausalAttention` class.
    This output projection layer is not strictly necessary (see appendix B for more
    details), but it is commonly used in many LLM architectures, which is why I added
    it here for completeness.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在将头部合并后，向`MultiHeadAttention`添加了一个输出投影层(`self.out_proj`)，这在`CausalAttention`类中不存在。这个输出投影层不是严格必要的（更多细节请见附录B），但它被广泛应用于许多LLM架构中，因此我为了完整性在此添加了它。
- en: Even though the `MultiHeadAttention` class looks more complicated than the `MultiHeadAttentionWrapper`
    due to the additional reshaping and transposition of tensors, it is more efficient.
    The reason is that we only need one matrix multiplication to compute the keys,
    for instance, `keys` `=` `self.W_key(x)` (the same is true for the queries and
    values). In the `MultiHeadAttentionWrapper`, we needed to repeat this matrix multiplication,
    which is computationally one of the most expensive steps, for each attention head.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于额外的张量重塑和转置，`MultiHeadAttention`类看起来比`MultiHeadAttentionWrapper`更复杂，但它更高效。原因是，我们只需要一次矩阵乘法来计算键，例如，`keys`
    `=` `self.W_key(x)`（对于查询和值也是如此）。在`MultiHeadAttentionWrapper`中，我们需要为每个注意力头重复这个矩阵乘法，这是计算上最昂贵的步骤之一。
- en: 'The `MultiHeadAttention` class can be used similar to the `SelfAttention` and
    `CausalAttention` classes we implemented earlier:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultiHeadAttention`类可以像我们之前实现的`SelfAttention`和`CausalAttention`类一样使用：'
- en: '[PRE75]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The results show that the output dimension is directly controlled by the `d_out`
    argument:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明输出维度直接受`d_out`参数控制：
- en: '[PRE76]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: We have now implemented the `MultiHeadAttention` class that we will use when
    we implement and train the LLM. Note that while the code is fully functional,
    I used relatively small embedding sizes and numbers of attention heads to keep
    the outputs readable.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经实现了`MultiHeadAttention`类，当我们实现和训练LLM时将使用这个类。请注意，虽然代码完全可用，但我使用了相对较小的嵌入大小和注意力头数，以保持输出可读。
- en: For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention
    heads and a context vector embedding size of 768\. The largest GPT-2 model (1.5
    billion parameters) has 25 attention heads and a context vector embedding size
    of 1,600\. The embedding sizes of the token inputs and context embeddings are
    the same in GPT models (`d_in` `=` `d_out`).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，最小的GPT-2模型（1.17亿参数）有12个注意力头和768维的上下文向量嵌入大小。最大的GPT-2模型（15亿参数）有25个注意力头和1,600维的上下文向量嵌入大小。在GPT模型中，标记输入和上下文嵌入的嵌入大小是相同的（`d_in`
    `=` `d_out`）。
- en: Exercise 3.3 Initializing GPT-2 size attention modules
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习3.3 初始化GPT-2大小注意力模块
- en: Using the `MultiHeadAttention` class, initialize a multi-head attention module
    that has the same number of attention heads as the smallest GPT-2 model (12 attention
    heads). Also ensure that you use the respective input and output embedding sizes
    similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports
    a context length of 1,024 tokens.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`MultiHeadAttention`类，初始化一个具有与最小的GPT-2模型（12个注意力头）相同数量的注意力头的多头注意力模块。同时确保您使用与GPT-2类似的相应输入和输出嵌入大小（768维）。请注意，最小的GPT-2模型支持1,024个标记的上下文长度。
- en: Summary
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Attention mechanisms transform input elements into enhanced context vector representations
    that incorporate information about all inputs.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制将输入元素转换为增强的上下文向量表示，该表示包含有关所有输入的信息。
- en: A self-attention mechanism computes the context vector representation as a weighted
    sum over the inputs.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力机制通过输入的加权和来计算上下文向量表示。
- en: In a simplified attention mechanism, the attention weights are computed via
    dot products.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在简化的注意力机制中，注意力权重通过点积来计算。
- en: A dot product is a concise way of multiplying two vectors element-wise and then
    summing the products.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点积是一种简洁的方式，通过逐元素相乘然后求和来乘以两个向量。
- en: Matrix multiplications, while not strictly required, help us implement computations
    more efficiently and compactly by replacing nested `for` loops.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法，虽然不是严格必需的，但通过替换嵌套的`for`循环，有助于我们更高效和紧凑地实现计算。
- en: 'In self-attention mechanisms used in LLMs, also called scaled-dot product attention,
    we include trainable weight matrices to compute intermediate transformations of
    the inputs: queries, values, and keys.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM中使用的自注意力机制，也称为缩放点积注意力，我们包括可训练的权重矩阵来计算输入的中间变换：查询、值和键。
- en: When working with LLMs that read and generate text from left to right, we add
    a causal attention mask to prevent the LLM from accessing future tokens.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与从左到右读取和生成文本的LLM一起工作时，我们添加一个因果注意力掩码以防止LLM访问未来的标记。
- en: In addition to causal attention masks to zero-out attention weights, we can
    add a dropout mask to reduce overfitting in LLMs.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了用于将注意力权重置零的因果注意力掩码外，我们还可以添加一个dropout掩码以减少LLM中的过拟合。
- en: The attention modules in transformer-based LLMs involve multiple instances of
    causal attention, which is called multi-head attention.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于transformer的LLM中的注意力模块涉及多个因果注意力实例，这被称为多头注意力。
- en: We can create a multi-head attention module by stacking multiple instances of
    causal attention modules.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过堆叠多个因果注意力模块的实例来创建一个多头注意力模块。
- en: A more efficient way of creating multi-head attention modules involves batched
    matrix multiplications.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建多头注意力模块的更有效的方法涉及批量矩阵乘法。
