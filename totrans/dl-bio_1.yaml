- en: Chapter 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Biology is increasingly becoming a data-driven science, and deep learning—a
    powerful subfield of machine learning—is opening new ways to uncover patterns
    in complex, high-dimensional datasets. As these two fields converge, new opportunities
    are emerging to extract meaningful insights using modern computational tools.
    This book is a practical introduction to working at that intersection, focused
    on developing the skills and mindset needed to apply deep learning effectively
    in biological contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This opening chapter helps you get oriented. Before jumping into code, we walk
    through how to frame a project, evaluate your data, and avoid common pitfalls.
    A bit of structure and planning up front will make your work more reproducible,
    more flexible, and ultimately more useful and impactful.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding What Your Model Will Replace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The success of a deep learning project in biology often hinges on what happens
    before you write a single line of code. It’s easy to get lost in technical details
    or spend weeks exploring data and architecture variants that don’t lead to meaningful
    outcomes. Especially in a field as interesting as this one, the temptation to
    tinker is strong. To stay focused, it helps to ask a few grounding questions up
    front.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most useful is: *What existing process will my model replace or
    improve?* The most impactful projects in this field often (though not always)
    have a clear answer. Here are some examples across different domains:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In healthcare and drug discovery:'
  prefs: []
  type: TYPE_NORMAL
- en: Skin cancer classification models
  prefs: []
  type: TYPE_NORMAL
- en: Aim to replicate a dermatologist’s ability to visually diagnose melanoma or
    other lesions from clinical images, offering faster, more scalable screening for
    at-risk populations
  prefs: []
  type: TYPE_NORMAL
- en: Pathogen detection systems
  prefs: []
  type: TYPE_NORMAL
- en: Trained on sequencing data or imaging to detect bacterial or viral infections
    directly from raw clinical samples (e.g., blood, saliva, or tissue), potentially
    replacing slower, culture-based diagnostics
  prefs: []
  type: TYPE_NORMAL
- en: Brain tumor segmentation models
  prefs: []
  type: TYPE_NORMAL
- en: Automate or accelerate the process of outlining tumors on MRI scans, a task
    that radiologists typically perform manually and with great time investment
  prefs: []
  type: TYPE_NORMAL
- en: Drug-target interaction prediction tools
  prefs: []
  type: TYPE_NORMAL
- en: Aim to prioritize the most promising compound-target pairs, reducing the need
    for costly wet-lab screening of massive chemical libraries
  prefs: []
  type: TYPE_NORMAL
- en: Antibiotic resistance prediction models
  prefs: []
  type: TYPE_NORMAL
- en: Forecast whether a given bacterial strain will resist certain treatments, helping
    clinicians select effective antibiotics more quickly
  prefs: []
  type: TYPE_NORMAL
- en: 'In molecular biology:'
  prefs: []
  type: TYPE_NORMAL
- en: AlphaFold
  prefs: []
  type: TYPE_NORMAL
- en: This protein structure prediction model, in many cases, replaces the need for
    experimentally determining 3D protein shapes via expensive lab-based techniques
    like X-ray crystallography, cryo-EM, or NMR
  prefs: []
  type: TYPE_NORMAL
- en: Gene expression prediction models
  prefs: []
  type: TYPE_NORMAL
- en: Forecast gene activity from raw genomic sequences, offering a computational
    alternative to RNA sequencing (RNA-seq) experiments
  prefs: []
  type: TYPE_NORMAL
- en: Variant effect prediction models
  prefs: []
  type: TYPE_NORMAL
- en: Help automate the interpretation of genetic mutations, supporting clinical decision
    making by prioritizing likely pathogenic variants for follow-up analysis or experimental
    validation
  prefs: []
  type: TYPE_NORMAL
- en: 'And in ecology and environmental science:'
  prefs: []
  type: TYPE_NORMAL
- en: Acoustic species classification systems
  prefs: []
  type: TYPE_NORMAL
- en: Use forest sound recordings to identify animal species present, offering a scalable
    and less labor-intensive alternative to in-person biodiversity surveys
  prefs: []
  type: TYPE_NORMAL
- en: Crop disease detection
  prefs: []
  type: TYPE_NORMAL
- en: Via drone or satellite imagery, enables early identification of plant stress,
    reducing the need for manual scouting across large fields
  prefs: []
  type: TYPE_NORMAL
- en: Animal facial recognition tools
  prefs: []
  type: TYPE_NORMAL
- en: Track individual animals, reptiles, birds, and mammals over time without the
    need for tagging, collars, or other invasive methods
  prefs: []
  type: TYPE_NORMAL
- en: Poaching detection systems
  prefs: []
  type: TYPE_NORMAL
- en: Trained on infrared or motion sensor data, can automatically flag human activity
    in protected wildlife zones, assisting conservation efforts
  prefs: []
  type: TYPE_NORMAL
- en: Ideally this gives you a flavor of the kinds of workflows deep learning can
    improve or even replace. Where possible, try to estimate the potential impact
    of your model—how much time, cost, or manual labor it could save, or what new
    insights it might enable. This will help you stay grounded in the real-world utility
    of your work and communicate its value to collaborators, stakeholders, or the
    public.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That said, not every valuable model needs to replace an existing process. Some
    open up entirely new capabilities—like generating novel biological sequences,
    uncovering hidden patterns in large datasets, or linking data types that were
    never connected before. These models might not streamline a lab task, but they
    can enable new kinds of discovery, expand what questions we can ask, or offer
    fresh ways to interpret complex systems. If your model creates something new,
    just be clear about what that is and why it matters—and be thoughtful about how
    you evaluate success when no established benchmark exists.
  prefs: []
  type: TYPE_NORMAL
- en: Determining Your Criteria for Success
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s important to define, as early and explicitly as possible, what success
    looks like for your project. Research can be time-consuming and open-ended, so
    clear goals help you stay focused and avoid endless tweaking—repeatedly changing
    models, architectures, or training settings without a clear hypothesis or evaluation
    plan. This kind of trial-and-error loop is common in deep learning due to the
    large number of design choices and hyperparameters. Without structure, it can
    waste time and produce results that are hard to interpret or reproduce.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of success criteria include:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance metric (e.g., accuracy, AUC, F1)
  prefs: []
  type: TYPE_NORMAL
- en: You might aim to match the performance of a human expert, achieve a correlation
    with experimental results comparable to a technical replicate, or keep the false-positive
    rate below a certain number.
  prefs: []
  type: TYPE_NORMAL
- en: Level of interpretability
  prefs: []
  type: TYPE_NORMAL
- en: In many applications, it’s important not only that a model performs well, but
    also that its decisions can be understood by domain experts. For instance, you
    may prioritize well-calibrated uncertainty estimates or interpretable feature
    attributions, especially when trust and explainability are critical.
  prefs: []
  type: TYPE_NORMAL
- en: Model size or inference latency
  prefs: []
  type: TYPE_NORMAL
- en: If your model needs to operate in a resource-constrained environment (e.g.,
    smartphones or embedded devices) or meet real-time throughput targets (e.g., process
    20 frames per second), your success criterion might focus on efficiency—such as
    achieving high performance per floating point operation (FLOP), which measures
    how effectively the model uses computational resources. In such cases, metrics
    like inference time, memory usage, or energy consumption may matter more than
    raw accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Training time and efficiency
  prefs: []
  type: TYPE_NORMAL
- en: When compute is limited—or for educational contexts—you may prioritize fast
    training or minimal hardware requirements. Since training deep learning models
    typically involves large matrix operations, they are often accelerated using graphics
    processing units (GPUs). In low-resource settings, developing a simpler model
    that trains quickly on a CPU may be a more practical goal than maximizing performance.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizability
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the goal is to build a model that works well across many datasets
    or tasks, rather than one that is finely tuned to a single benchmark. For example,
    *foundational models*—large models trained on broad datasets that can be adapted
    to many downstream applications—prioritize flexibility and reuse. In such settings,
    broad applicability may be more valuable than squeezing out the best possible
    performance on a specific task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining these goals up front helps you answer the key question: *When is the
    project done?* You’ll likely need to balance multiple criteria—but having them
    laid out early will keep your efforts aligned and your scope realistic.'
  prefs: []
  type: TYPE_NORMAL
- en: Invest Heavily in Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you’ve defined your criteria for success, it’s time to prioritize *evaluation*.
    This means thinking carefully about precisely how you’ll measure progress—including
    what metrics you’ll use, how you’ll validate results, and which baselines you’ll
    compare against. Without a clear, well-designed evaluation strategy, even a technically
    impressive model can fail to produce meaningful conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strong evaluations don’t just help you measure progress. They also help you
    detect bugs, estimate task difficulty, and build intuition. The key idea is simple:
    you need a known point of comparison to understand if your model is doing anything
    meaningful.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While no general rule exists, it wouldn’t be surprising if successful machine
    learning projects spent 50% of their time designing evaluation strategies and
    running baselines, 25% curating or processing data, and only 25% on model architecture.
    Without good evaluations, you’re flying blind: you won’t know whether your model
    is actually improving, what trade-offs you’re making, or even whether it’s learning
    anything meaningful at all.'
  prefs: []
  type: TYPE_NORMAL
- en: Spend time here. Evaluation isn’t just something you do at the end. It’s something
    you design at the beginning, and it guides the entire project.
  prefs: []
  type: TYPE_NORMAL
- en: Designing Baselines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most practical evaluation tools is a strong *baseline*—a simple method
    that gives you something to beat. Good baselines help you measure progress, catch
    bugs early, and understand the difficulty of your task. Sometimes they can even
    be surprisingly hard to beat.
  prefs: []
  type: TYPE_NORMAL
- en: 'Designing good baselines requires thinking carefully about the task. Here are
    a few common baseline strategies for classification tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Random prediction
  prefs: []
  type: TYPE_NORMAL
- en: Assign labels completely at random, with equal probability for each class. This
    tells you what performance looks like with no information at all.
  prefs: []
  type: TYPE_NORMAL
- en: Random prediction weighted by class frequencies
  prefs: []
  type: TYPE_NORMAL
- en: Sample labels randomly, but in proportion to how often they occur in the training
    data. This is useful for imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Majority class
  prefs: []
  type: TYPE_NORMAL
- en: Always predict the most common class. This can be a surprisingly hard baseline
    to beat in highly class imbalanced settings.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbor
  prefs: []
  type: TYPE_NORMAL
- en: Predict the label of the most similar example in the training data (e.g., 1-nearest
    neighbor using Euclidean distance). This is often effective when inputs are low
    dimensional or well structured.
  prefs: []
  type: TYPE_NORMAL
- en: 'And for regression tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean or median of the target
  prefs: []
  type: TYPE_NORMAL
- en: Always predict the average or median target value from the training set. This
    often matches what a model would do if it’s not learning anything meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with a single feature
  prefs: []
  type: TYPE_NORMAL
- en: Fit a line using just the strongest individual predictor (e.g., one biomarker).
    This helps gauge how much a more complex model improves over a simple signal.
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbor regression
  prefs: []
  type: TYPE_NORMAL
- en: Predict the target as the average (or weighted average) of the *k* most similar
    data points. This is simple to implement and often surprisingly competitive on
    structured datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'And for both:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple heuristics
  prefs: []
  type: TYPE_NORMAL
- en: Use straightforward rules based on domain knowledge. For example, in diagnostics,
    classify a patient as positive if a single biomarker or measurement exceeds a
    threshold. For skin cancer images, rank lesions by average pixel intensity. In
    genomics, if the task is to predict which gene a mutation affects, a simple baseline
    is to assume it affects the nearest gene in the genome.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your model can’t beat a basic baseline, something is likely off—and that’s
    useful to know. It’s a key signal to revisit your data, features, or modeling
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Time-Boxing Your Project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s important to time-box your project—that is, set a fixed amount of time
    to work on it, after which you pause or stop regardless of the outcome. Many research
    ideas “fail” in the sense that they don’t achieve the desired metrics. That’s
    normal. All projects, even the unsuccessful ones, generate insights that inform
    future work. Time-boxing helps ensure that failed experiments still move you forward—without
    consuming unlimited time and energy.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Time-boxing doesn’t mean giving up easily—it means setting boundaries to maintain
    focus, avoid burnout, and keep making progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some tips for time-boxing effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Set a clear deadline
  prefs: []
  type: TYPE_NORMAL
- en: Choose a realistic time frame (e.g., two weeks, three months) and stick to it.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Define checkpoints
  prefs: []
  type: TYPE_NORMAL
- en: Identify intermediate milestones—like completing dataset preprocessing, training
    a baseline model, or hitting a certain accuracy—to track progress.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Reflect at the end
  prefs: []
  type: TYPE_NORMAL
- en: Take time to evaluate what worked, what didn’t, and what you learned.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Time-boxing is also useful *within* a project. For example: “I’ll experiment
    with this new processing or modeling idea for one week, and if it doesn’t help,
    I’ll move on.”'
  prefs: []
  type: TYPE_NORMAL
- en: The biggest risk with time-boxing is yourself. It’s easy to justify extensions,
    add new ideas, or convince yourself you’ll strike gold if you just try 10 more
    things. Scope creep and perfectionism are common traps. In these cases, it can
    help to talk to someone else—a collaborator, mentor, or friend—to get perspective
    and avoid spinning your wheels. A quick conversation can often cut through indecision
    or obsessiveness and help you refocus on the broader context.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding Whether You Really Need Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This might seem like strange advice in a deep learning book, but before diving
    in, take a moment to ask yourself: *Do I actually need deep learning for this
    problem?* We’ll say it again—seriously consider simpler approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning models are powerful (and undeniably interesting), but they’re
    also resource intensive, complex to train, and difficult to debug. In many cases,
    traditional methods—like linear regression, decision trees, or basic statistical
    techniques—can achieve your goals with far less effort. These approaches are often:'
  prefs: []
  type: TYPE_NORMAL
- en: Easier to implement
  prefs: []
  type: TYPE_NORMAL
- en: Quicker to set up and require less expertise
  prefs: []
  type: TYPE_NORMAL
- en: Less computationally demanding
  prefs: []
  type: TYPE_NORMAL
- en: Can run on standard hardware with minimal training time
  prefs: []
  type: TYPE_NORMAL
- en: More interpretable
  prefs: []
  type: TYPE_NORMAL
- en: Easier to explain, troubleshoot, and validate
  prefs: []
  type: TYPE_NORMAL
- en: Carefully weigh the trade-offs. If a simpler method delivers the insights or
    performance you need, it’s often the smarter and more efficient path.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring That You Have Enough Good Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning models don’t just need a lot of data—they also generally need
    high-quality data. Models trained on poor data can often fail catastrophically,
    regardless of their sophistication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you have:'
  prefs: []
  type: TYPE_NORMAL
- en: Sufficient quantity
  prefs: []
  type: TYPE_NORMAL
- en: Deep models typically need thousands of examples or more. What counts as “enough”
    data depends on your problem and architecture. Check relevant literature for benchmarks.
    If you’re working with a small dataset, consider *transfer learning*, where you
    start from a model trained on a related task and fine-tune it on your own data.
    This approach can dramatically reduce the amount of data needed to achieve good
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Sufficient quality
  prefs: []
  type: TYPE_NORMAL
- en: Clean, consistent data is critical. Label errors, noise, or inconsistencies
    can seriously degrade performance. Even large language models—like those powering
    modern chat-based assistant systems—can benefit substantially from training on
    carefully curated, high-quality data. Prioritize quality checks and thoughtful
    curation.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling a Team
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Working alone is totally fine—but teaming up can accelerate progress, improve
    your ideas, and make the process more enjoyable. Here are some tips for finding
    great collaborators:'
  prefs: []
  type: TYPE_NORMAL
- en: Engage with the community
  prefs: []
  type: TYPE_NORMAL
- en: Join relevant forums, online groups, or webinars to connect with others, share
    ideas, and discover potential collaborators. Communities like Reddit, Discord
    servers, X, and specialized Slack groups can be great starting points.
  prefs: []
  type: TYPE_NORMAL
- en: Participate in hackathons and competitions
  prefs: []
  type: TYPE_NORMAL
- en: Platforms like Kaggle, Zindi, or local university events offer structured challenges,
    feedback, and opportunities to meet people with similar interests.
  prefs: []
  type: TYPE_NORMAL
- en: Form an interdisciplinary team
  prefs: []
  type: TYPE_NORMAL
- en: Combining expertise from different areas often leads to stronger projects. If
    you’re a biologist, team up with someone in machine learning, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborate with experts
  prefs: []
  type: TYPE_NORMAL
- en: Domain experts can help shape your approach and identify blind spots early.
    Look for collaborators at conferences or workshops, or reach out to authors of
    relevant papers. Strangers are surprisingly responsive to genuine cold requests
    from interested people.
  prefs: []
  type: TYPE_NORMAL
- en: 'And here are some tips for once you find people to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: Define clear goals and roles
  prefs: []
  type: TYPE_NORMAL
- en: When working with others, it helps to clarify responsibilities early—who’s doing
    what, what success looks like, and how decisions will be made. This avoids misunderstandings
    and keeps the project moving.
  prefs: []
  type: TYPE_NORMAL
- en: Use shared tools for collaboration
  prefs: []
  type: TYPE_NORMAL
- en: Version control (like Git), shared notebooks (e.g., Google Colab), and simple
    task trackers (like Notion or Trello, or just a shared Google Doc with some lists)
    can make it much easier to coordinate and stay organized.
  prefs: []
  type: TYPE_NORMAL
- en: Support specialization
  prefs: []
  type: TYPE_NORMAL
- en: Let people lean into the parts of the project they enjoy most—some may focus
    on infrastructure and software engineering, others on data curation, modeling,
    or biological interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Start small
  prefs: []
  type: TYPE_NORMAL
- en: If you’re unsure about long-term compatibility, try a short project or exploration
    together first. A small, low-pressure collaboration is a great way to test the
    waters.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you’re working solo or as part of a team, the most important thing is
    to stay curious, keep learning, and take that first step.
  prefs: []
  type: TYPE_NORMAL
- en: You Don’t Need a Supercomputer or a PhD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few common misconceptions about working in deep learning for biology
    that are worth challenging:'
  prefs: []
  type: TYPE_NORMAL
- en: You need huge budgets and compute power.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an age of multimillion-dollar training runs for massive language models,
    it’s easy to assume you need vast resources. But that’s not always the case:'
  prefs: []
  type: TYPE_NORMAL
- en: Prototype with small models
  prefs: []
  type: TYPE_NORMAL
- en: Start small to iterate quickly. You might find that lightweight models are more
    than enough for your goals.
  prefs: []
  type: TYPE_NORMAL
- en: Use free or affordable compute
  prefs: []
  type: TYPE_NORMAL
- en: Platforms like Google Colab and Kaggle offer free GPU access for smaller projects.
    For more demanding workloads, cloud providers such as Amazon Web Services (AWS),
    Microsoft Azure, and Google Cloud Platform (GCP) offer scalable paid instances.
  prefs: []
  type: TYPE_NORMAL
- en: Not everything is about scale
  prefs: []
  type: TYPE_NORMAL
- en: Many valuable projects focus on analyzing existing models rather than training
    new ones. These often require modest compute but can yield deep insight. There’s
    still a lot we don’t understand about how deep models behave.
  prefs: []
  type: TYPE_NORMAL
- en: You need deep expertise in machine learning or biology (or both).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another myth is that only highly trained experts can contribute meaningfully.
    In reality:'
  prefs: []
  type: TYPE_NORMAL
- en: Better tools
  prefs: []
  type: TYPE_NORMAL
- en: Modern frameworks make it easier than ever to build and experiment with powerful
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Open source culture
  prefs: []
  type: TYPE_NORMAL
- en: Freely available code and pretrained models let you learn from and build on
    existing work.
  prefs: []
  type: TYPE_NORMAL
- en: Educational resources
  prefs: []
  type: TYPE_NORMAL
- en: There’s now no shortage of tutorials, videos, and walkthroughs online to help
    you get started.
  prefs: []
  type: TYPE_NORMAL
- en: Plenty of untapped problems
  prefs: []
  type: TYPE_NORMAL
- en: Many important biological questions remain unexplored by machine learning. You
    don’t need a PhD or a Kaggle medal to work on them.
  prefs: []
  type: TYPE_NORMAL
- en: While the bleeding edge of research may require specialized knowledge and high-end
    infrastructure, there’s plenty of room in this field for curiosity, creativity,
    and new perspectives—no supercomputer required.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you explore this field, you’ll almost certainly come across academic papers—whether
    you’re digging into a specific method, reading related work, or looking for project
    ideas. Both biology and machine learning papers can feel impenetrable at first:
    the language is dense, the ideas are highly condensed, and there’s often a lot
    of jargon. But remember:'
  prefs: []
  type: TYPE_NORMAL
- en: You’re seeing the result of months or years of work by a team of researchers—and
    encountering it for the first time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading papers is a skill, and like any skill, it improves with practice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blog posts, YouTube videos, and open source projects can also be great, more
    accessible ways to learn the same concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that background in place, let’s dive into the technical foundations of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll be using Python-based deep learning frameworks, in particular, JAX and
    Flax. JAX is a system for high-performance numerical computing and machine learning,
    and Flax is a flexible neural network library built on top of JAX. We’ll motivate
    by explaining why we’re using JAX and reviewing a few Python features that tend
    to come up in a lot of machine learning (ML) code. Then, we’ll introduce some
    foundational machine learning concepts recurring throughout, with the main one
    being how a training loop is structured.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As mentioned in [“Prerequisites”](preface01.html#id109), this book assumes basic
    knowledge of Python. If you’re new to Python, check out the recommended resources
    listed there first.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to avoid repetition across chapters, we’ve created a small companion
    library called `dlfb` (Deep Learning for Biology), which wraps up common utilities
    and components. We’ll reference it throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t worry if some parts of this technical introduction feel unfamiliar or
    challenging at first. You’re welcome to skim or skip ahead. Many of the concepts
    will become clearer when you see them in action later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Why JAX and Flax?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book uses the JAX and Flax ecosystem. But why did we make this choice,
    when other options like PyTorch or Keras are more common?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, some honesty: there is no one “best” framework. All of them can be used
    to build effective biological models, and many of the concepts in this book will
    carry over easily if you’re using PyTorch or Keras.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve chosen JAX/Flax primarily because:'
  prefs: []
  type: TYPE_NORMAL
- en: Familiar NumPy API
  prefs: []
  type: TYPE_NORMAL
- en: JAX’s `jax.numpy` module (commonly imported as `jnp`) offers an API that so
    closely mirrors standard NumPy for array manipulation and mathematical operations
    that NumPy `np` calls can often be directly substituted with `jnp`. This means
    users already proficient with NumPy can transition to JAX with a significantly
    reduced learning curve, leveraging their existing knowledge to quickly build and
    adapt code while gaining JAX’s powerful transformations and accelerator support.
  prefs: []
  type: TYPE_NORMAL
- en: Functional programming encourages clarity
  prefs: []
  type: TYPE_NORMAL
- en: JAX’s pure function style can reduce hidden state and make training logic more
    transparent. This fits well with the educational goals of the book—explicit is
    better than implicit.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations are first-class
  prefs: []
  type: TYPE_NORMAL
- en: JAX provides powerful, composable transformations like `jit` (just-in-time compilation),
    `grad` (automatic differentiation), and `vmap` (vectorization) that work cleanly
    on Python functions. These tools simplify and unify many aspects of model training
    and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: JAX aligns with cutting-edge research
  prefs: []
  type: TYPE_NORMAL
- en: JAX has gained traction in recent machine learning research, particularly for
    biology, physics, and large-scale models. Using it here helps you align with newer
    toolchains and experiment with modern practices.
  prefs: []
  type: TYPE_NORMAL
- en: Speed
  prefs: []
  type: TYPE_NORMAL
- en: JAX uses a compiler that can yield significant performance gains on specialized
    hardware such as GPUs (e.g., from NVIDIA or AMD) and TPUs (made by Google), making
    it well-suited for large-scale deep learning workloads. This compiler is based
    on XLA, a low-level system for optimizing numerical computations on accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, JAX and Flax come with trade-offs: a smaller ecosystem and APIs
    that evolve quickly (sometimes breaking things along the way). And while JAX can
    offer impressive speedups, that speed isn’t exclusive to JAX/Flax. For example,
    [Keras](https://keras.io) now supports a JAX backend, offering another option
    for users who prefer a higher-level API. If you’re already comfortable with PyTorch,
    Keras, or TensorFlow, you’re welcome to implement the ideas in this book using
    those tools—and even contribute your own version to this book’s repository.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s not necessary when you’re just starting out, but over time it can be helpful
    to become familiar with more than one deep learning framework. Each has strengths
    in different ecosystems—for example, we use PyTorch in Chapter 2 to extract pretrained
    embeddings from a Hugging Face model, since many models on Hugging Face are primarily
    released and maintained in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The deep learning field moves fast. While we use the `linen` API in Flax throughout
    this book, a newer API called `nnx` has recently emerged as the recommended way
    to build models. `linen` remains fully supported, but be aware that you may come
    across other tutorials or examples that use `nnx`, which has a slightly different
    syntax.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll introduce key JAX concepts as needed throughout the book, but we won’t
    cover the entire library in detail. For more detailed hands-on learning, check
    out the official [JAX tutorials](https://oreil.ly/Jcqtu). And if you run into
    unexpected behavior, the JAX [“sharp bits” notebook](https://oreil.ly/TcOzQ) is
    an excellent reference for common gotchas and how to avoid them.
  prefs: []
  type: TYPE_NORMAL
- en: A note on performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As this is an educational book, our focus is on clarity rather than peak performance.
    That means we won’t cover things like precision tuning, advanced hardware strategies,
    or distributed training. But these things can matter a lot in real-world setups.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re comfortable with the basics and want to go deeper, here are a few
    areas worth exploring:'
  prefs: []
  type: TYPE_NORMAL
- en: Numerical precision and tuning
  prefs: []
  type: TYPE_NORMAL
- en: Many machine learning operations, especially matrix multiplications (matmuls),
    benefit from reduced-precision formats like `bfloat16`, which can significantly
    improve speed and memory usage with minimal impact on model accuracy. JAX lets
    you control the precision used for matmuls via `jax.default_matmul_precision`,
    helping you take advantage of specialized hardware like Tensor Cores (on NVIDIA
    GPUs) or matrix units (on TPUs). Lower-precision training is widely used in large-scale
    setups because it enables training larger models more efficiently and cost-effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling tools like `jax.profiler` or TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: Profiling helps you identify where your code is spending time and memory so
    you can spot bottlenecks in training and optimize the most expensive operations.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-efficient training techniques
  prefs: []
  type: TYPE_NORMAL
- en: Methods like gradient checkpointing (remat in JAX) let you trade off computation
    for memory, allowing you to train deeper models without running out of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Multihost/multidevice training
  prefs: []
  type: TYPE_NORMAL
- en: Training across multiple GPUs, TPUs, or even machines allows you to scale up
    models and datasets that wouldn’t fit on a single device.
  prefs: []
  type: TYPE_NORMAL
- en: You won’t need any of these to follow this book, but they are good to be aware
    of and are worth exploring as you grow more comfortable with the JAX ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Python Tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this book doesn’t cover Python background knowledge in depth, this section
    highlights a few helpful Python concepts you’re likely to encounter when working
    with machine learning code in general—and with JAX and Flax in particular.
  prefs: []
  type: TYPE_NORMAL
- en: Type annotations and docstrings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python is a dynamically typed language, meaning you don’t need to declare variable
    types (such as strings or integers) explicitly. Instead, types are determined
    at runtime, which makes the language flexible—but this flexibility can also make
    bugs harder to catch, especially in larger codebases. Adding *type annotations*
    helps mitigate this by improving readability, enabling static type checks with
    tools like `mypy`, and making debugging easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple function that computes the mean squared error (MSE) between
    two NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of its use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can improve this function by adding type hints to specify that the inputs
    are `np.ndarray` objects and the return type is a `float`, along with a docstring
    to explain what the function does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'These changes don’t affect the function’s behavior, but they offer several
    benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Clarify input and output types
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s immediately clear that `y_true` and `y_pred` should be NumPy arrays and
    the return value is a `float`. Note that some machine learning code goes further
    by specifying the data type within the array (e.g., `arr: NDArray[np.float64]`),
    but we will not do this in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Enhance documentation
  prefs: []
  type: TYPE_NORMAL
- en: IDEs and documentation tools can provide better inline help and autocompletion.
    This can really improve productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Improve readability
  prefs: []
  type: TYPE_NORMAL
- en: The function is easier to understand for others (or your future self).
  prefs: []
  type: TYPE_NORMAL
- en: Enable static checking
  prefs: []
  type: TYPE_NORMAL
- en: Tools like `mypy` can catch type-related errors.
  prefs: []
  type: TYPE_NORMAL
- en: The MSE example is very simple, so adding type hints and a full docstring is
    arguably overkill—but the principle is important.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t necessarily always use typing and docstrings throughout this book due
    to space constraints, but they’re good habits to adopt in your own projects. When
    we do include docstrings in the main book text, we’ll usually keep them to a single
    line, which will save a few trees in printing.
  prefs: []
  type: TYPE_NORMAL
- en: Decorators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Decorators* are functions that modify the behavior of other functions or methods.
    In machine learning and data science, decorators are often used to enhance performance,
    cache results, or log function behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Just-in-time (JIT) compilation with JAX
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most common decorators when working in JAX is `jax.jit`, which performs
    JIT compilation to accelerate code execution. The first run of a JIT-compiled
    function is slower because it is compiled to XLA (Accelerated Linear Algebra)
    machine code. However, subsequent calls run significantly faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a function that takes a JAX array, raises all values to the
    10th power, and sums them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can speed up this function in one of two ways. Either directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, via the Python syntactic sugar, we can apply it when defining the function
    with the `@jax.jit` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output remains the same for all, but the jitted functions run much faster.
    If you are working in a Jupyter notebook, you can use `%timeit` to measure the
    execution time for a line of code (or `%%timeit` for entire cells). Try timing
    the function with and without `@jax.jit`. On a GPU, you may see an ~20× speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How does `@jax.jit` work? Briefly, when you apply `@jax.jit`, JAX doesn’t run
    your function like regular Python. Instead, it first traces the function—it runs
    through it once using special tracer objects (not real data) to build a computation
    graph. This graph is a static representation of all the numerical operations performed,
    with control flow unrolled and variable shapes and types fixed.
  prefs: []
  type: TYPE_NORMAL
- en: Once the graph is built, JAX compiles it using XLA (Accelerated Linear Algebra),
    a backend that generates highly optimized machine code. This compiled version
    is cached and reused whenever the function is called again with the same input
    shapes and types—leading to significant speedups.
  prefs: []
  type: TYPE_NORMAL
- en: 'JIT compilation is powerful, but it comes with trade-offs, especially for debugging.
    This is because:'
  prefs: []
  type: TYPE_NORMAL
- en: Python debugging tools like `print()` statements or `pdb` don’t behave as expected
    inside jitted functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Side effects (e.g., `print()`, logging, or modifying a list) are not actually
    executed during tracing, since JAX skips anything that doesn’t affect the computation
    graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error messages can refer to internal JAX or XLA code instead of your original
    function and can be quite cryptic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While you can temporarily disable jitting by commenting out `@jax.jit`, this
    becomes impractical if many functions rely on JIT. Fortunately, you can globally
    disable JIT by setting the environment variable `JAX_DISABLE_JIT=True`, which
    forces all jitted functions to run normally. This is a convenient way to debug
    without rewriting your code. See the [JAX debugging documentation](https://oreil.ly/oXI97)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Preconfiguring JAX jit with partial
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is a common source of confusion in machine learning code regarding usage
    of `partial`, especially with JAX code. The use of `functools.partial` is to prefill
    (or “bind”) some arguments of a function, returning a new function with those
    values fixed. This is a general Python utility and is not specific to JAX or ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we adapt the `scale` function and create a new function, `scale_by_10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, `` `scale_by_10` `` is a new function that behaves like `scale(x, 10)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of `JAX`, `partial` is often used to customize a decorator before
    applying it, like this: `@partial(jax.jit, static_argnums=...)`. This is a way
    to configure the `jax.jit` decorator itself. As mentioned previously, `jax.jit`
    compiles your Python function for speed. However, JAX needs to know if certain
    arguments are static. Static arguments are typically non-JAX array types (like
    integers, strings, or booleans) that control the structure of the computation
    (e.g., in if/else statements). If a static argument changes, JAX may need to recompile
    the function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we want to compute a summary statistic over an array, choosing either
    the mean or the median based on a string argument `average_method`. Since this
    choice affects the control flow, JAX needs to know the value of `average_method`
    at compile time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If we didn’t mark `average` as static with `static_argnums=(0,)`, JAX would
    throw an error, because it can’t trace control flow that depends on strings unless
    it knows their value ahead of time. Marking arguments as static tells JAX to compile
    a separate, specialized version of the function for each unique value of that
    static argument it encounters.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A bit of a clarification on the meaning of “static” versus “dynamic”: JAX treats
    most numerical inputs (like `jax.Array`, `float`, or `int`) as dynamic, meaning
    they can vary between calls without requiring recompilation, as long as their
    shapes and types stay the same.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other inputs, like strings, Python objects, or functions, are static: they
    affect control flow or can’t be traced as part of the computation graph. If you
    pass them into a jitted function, you must mark them as static using `static_argnums`
    or close over them using a closure instead (see the next section).'
  prefs: []
  type: TYPE_NORMAL
- en: Closures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A closure is a function that “remembers” the environment in which it was created.
    This means it can access variables from its enclosing (outer) function’s scope,
    even after that scope has finished executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `add_five` is a closure. It “remembers” that `x` was 5 when
    `outer_function` was called.
  prefs: []
  type: TYPE_NORMAL
- en: Closures are used extensively in JAX-based machine learning code. Many components—like
    loss functions, regularizers, and augmentation pipelines—are parameterized by
    configuration values. Instead of passing these values as arguments (which might
    require `static_argnums` if used inside control flow), they’re often closed over.
    A little later we will see this in action when defining the JAX training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Generators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Generators* are functions that allow you to iterate over data lazily, yielding
    one item at a time. They are especially useful for working with large datasets
    where loading everything into memory (RAM) at once is impractical or impossible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple generator function that simulates streaming batches of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be working with TensorFlow datasets (TFDSs) in some chapters. Because
    JAX doesn’t include a native data-loading library, it’s common to see hybrid setups
    using TFDS. If you have your data in NumPy arrays, you can easily create a TensorFlow
    dataset using `tf.data.Dataset.from_tensor_slices`. This allows you to integrate
    NumPy data into TensorFlow pipelines for efficient training and preprocessing.
    It provides a clean API for batching, shuffling, and prefetching (preloading data
    before it’s needed to increase training speed), which is helpful when getting
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In later chapters, we will also be writing custom data pipelines that offer
    more control.
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of a Training Loop with JAX/Flax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the details of machine learning projects may vary, the core structure
    of training a model remains fairly consistent. This core structure will serve
    as a foundation throughout the chapters of this book. Let’s walk through the basic
    layout of training a model using JAX and Flax—a pattern you can build upon as
    you explore more complex examples.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s create some toy data where the target `y` values are a linear transformation
    of the `x` values with a bit of random noise added. We’ll use the relationship
    `y = 2x + 1` with Gaussian noise, as you can see in [Figure 1-1](#fig1-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. This scatterplot visualizes the underlying relationship that we
    want our model to learn.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Defining a model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Flax, we define a model by inheriting from `nn.Module`. The `@nn.compact`
    decorator allows us to define layers directly inside the `__call__` method, rather
    than in the class’s `setup()` method. This is especially useful for simple, sequential
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a minimal example, a single linear (dense) layer with one output unit
    and no activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can instantiate the model like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To initialize the model’s parameters, use the `.init` method with a random
    key and a sample input. This allows Flax to infer the input and output shapes.
    Here, we pass in a dummy input with shape `[1, 1]`—one example (batch size of
    1) with one input feature, matching the shape of our toy data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This initializes the model’s parameters. The result is a dictionary containing
    the weights and bias for the dense layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel` is the learned weight matrix (shape `[1, 1]`, since our input and
    output dimensions are both 1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` is the learned bias term added after the matrix multiplication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that while the Flax API has evolved over time, the core ideas remain stable:
    defining model layers, initializing parameters, and inferring shapes from inputs.
    Moreover, even if the syntax changes, these fundamentals will carry over to nearly
    any deep learning framework.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Why infer shapes from input? In Flax, the shapes of the model’s weight and
    bias parameters are actually unknown until you run data through the model. That’s
    because Flax follows a *functional* style: layers don’t store input shape information
    when you define them. Instead, you provide a sample input during initialization,
    and Flax infers the necessary parameter shapes on the fly.'
  prefs: []
  type: TYPE_NORMAL
- en: Other libraries, like PyTorch or Keras, use an object-oriented style, where
    layers often remember input shapes internally. This can make model construction
    feel more automatic, but Flax’s approach gives you more control and makes model
    behavior easier to inspect and debug, especially when working with JAX’s just-in-time
    (JIT) compilation.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a training state
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A *training state* in Flax is a container that packages together everything
    you need for training: the model’s parameters, the optimizer, and the function
    used to apply the model. Let’s build one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `TrainState` object is designed to make training in Flax cleaner and more
    manageable. It holds everything needed for model updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '`params`'
  prefs: []
  type: TYPE_NORMAL
- en: The current model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '`tx`'
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer (in this case, Adam). “Tx” is short for transformation. In Optax
    (JAX’s optimization library), optimizers are defined as transformations of gradients.
    For example, Adam transforms raw gradients using momentum and adaptive scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '`apply_fn`'
  prefs: []
  type: TYPE_NORMAL
- en: The function that runs the model’s forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, the training state is immutable—rather than modifying it in place,
    each update returns a new `TrainState` with the updated parameters. This functional
    style is consistent with JAX’s overall design and helps keep computations pure
    and traceable.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although `TrainState` is immutable and each update returns a new object, this
    doesn’t lead to memory issues. JAX reuses memory efficiently, especially inside
    `@jit`-compiled functions.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The loss function measures how close the model’s predictions are to the true
    targets. Here, we use MSE, which is common for regression tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note that `model.apply` works here because `model` was defined earlier and is
    available in the current scope (e.g., the same notebook or script). We don’t need
    to pass it as an argument, because the function is still pure—all variable model
    state comes from the `params` we pass in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you would call this function with your current model and data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This computes the current loss by applying the model to the data using the (random)
    parameters stored in `state`. Since the model hasn’t learned from the data yet,
    the loss is expected to be relatively high. As training progresses, this number
    should steadily decrease—which would tell us that the model is improving at its
    task of predicting the targets from the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the training step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training step performs a forward pass, computes the loss and gradients,
    and updates the model parameters. We use `jax.jit` to compile the entire step
    for efficiency. While JAX can run on GPUs and TPUs without it, using `jit` ensures
    that the code is compiled into a single optimized graph—which runs significantly
    faster and takes full advantage of accelerator hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Often, you don’t just want the gradients (to update the parameters). You also
    want to log the loss explicitly (e.g., to plot it over time). But computing gradients
    requires computing the loss anyway. Instead of doing this twice, `jax.value_and_grad`
    is a convenient utility that does both at once:'
  prefs: []
  type: TYPE_NORMAL
- en: It evaluates the function you give it (in this case, `calculate_loss`) to get
    the loss value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It computes the gradients of that loss with respect to the parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This avoids redundant computation.
  prefs: []
  type: TYPE_NORMAL
- en: The result of our `train_step` function is a new `TrainState` with updated parameters,
    along with the current loss, which we can use to monitor training progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that often you will encounter the training step defined with a closure
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, `state`, `x`, and `y` are closed over—they’re not part of the `calculate_loss`
    function’s input signature, making the code more compact and easier to read and
    reason about.
  prefs: []
  type: TYPE_NORMAL
- en: Handling auxiliary outputs in the loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a slight aside (but one that comes up often), sometimes we want to return
    extra information from the loss function—for example, predictions or metrics for
    logging—without affecting the gradient computation. JAX makes this easy with the
    `has_aux=True` flag. It tells `value_and_grad` to treat everything after the loss
    as “auxiliary” and to exclude it from gradient computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s modify our loss function to also return predictions and
    accommodate this by using `has_aux=True` in `jax.value_and_grad` in our `train_step`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Without `has_aux=True`, JAX expects the loss function to return a single scalar.
    Returning anything else (like predictions) will raise an error. By setting `has_aux=True`,
    you’re telling JAX: “only differentiate with respect to the loss; ignore any extra
    outputs, like predictions.”'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that all the components are in place, we can define the training loop that
    actually updates the model parameters based on the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most machine learning workflows, training happens over either *steps* or
    *epochs*:'
  prefs: []
  type: TYPE_NORMAL
- en: A *step* refers to one update of the model using a single batch of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *epoch* is a full pass through the entire training dataset—typically consisting
    of many steps. In our toy example, we feed the entire dataset (100 input–output
    pairs) into the model at once, without batching. This means each step is equivalent
    to a full epoch. In more realistic settings, you’d typically break the data into
    batches, resulting in many steps per epoch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The model converges quickly—its loss decreases rapidly to a stable, low value.
    After training, we can test how well the model has learned the underlying pattern
    by comparing its predictions to the true target values, as shown in [Figure 1-2](#fig1-2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Scatterplot comparing the linear model predictions with the true
    values.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that after training, the model has learned to approximate the true
    function y=2x+1 very closely. The predicted outputs are nearly identical to the
    expected values, which is exactly what we want in this toy regression task.
  prefs: []
  type: TYPE_NORMAL
- en: 'This may be a simple example, but it captures the core structure of almost
    every deep learning workflow: define a model, compute a loss, update parameters,
    and repeat. While real projects involve more complexity—batching, data pipelines,
    regularization, metrics, logging, and so on—the basic loop you’ve built here is
    the foundation of it all. You now have a solid mental model for how training works
    in JAX and Flax—and the foundation to build much more powerful systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Where to go from here? Once you’ve built a working training loop (an important
    milestone), it’s common to expand in several directions: add metrics that capture
    key aspects of model performance, split off a validation set to check generalization,
    and track progress over training time. These are all building blocks of real-world
    deep learning workflows, and we’ll walk through them in upcoming chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Tips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here’s a brief reminder of some machine learning concepts, focusing on what
    we use throughout the book. We explain these ideas in more detail as necessary
    when they come up.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *classification*, the model predicts a label or a probability distribution
    over labels. There are three types of classification:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs: []
  type: TYPE_NORMAL
- en: Choose between two options; for example, whether a cell is healthy or not.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification
  prefs: []
  type: TYPE_NORMAL
- en: Choose one label out of several; for example, based on patterns in a biological
    sample, a model might predict which part of the body it came from—like the brain,
    liver, or skin. Each sample belongs to exactly one class.
  prefs: []
  type: TYPE_NORMAL
- en: Multilabel classification
  prefs: []
  type: TYPE_NORMAL
- en: Choose all labels that apply, not just one. For example, when analyzing a cell
    image, the model might predict which structures are visible—such as the nucleus,
    membrane, and mitochondria. In this case, multiple labels can be true for the
    same image.
  prefs: []
  type: TYPE_NORMAL
- en: '*Regression* involves predicting a continuous value, such as the binding strength
    between two molecules.'
  prefs: []
  type: TYPE_NORMAL
- en: We also use *representation learning*—learning useful embeddings or feature
    representations without direct supervision. These embeddings capture patterns
    or structure in the data, which can then be used for tasks like clustering, visualization,
    or as inputs to downstream models.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this book, we primarily use the following model architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear models and multilayer perceptrons (MLPs)
  prefs: []
  type: TYPE_NORMAL
- en: These are fully connected networks that transform input vectors into output
    vectors through stacked dense layers. They are simple and widely used.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks (CNNs)
  prefs: []
  type: TYPE_NORMAL
- en: CNNs apply spatial filters to model local structure, typically mapping images
    to images or feature maps. They are especially effective for image and sequence
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs: []
  type: TYPE_NORMAL
- en: Model long-range dependencies in sequences using attention. They operate on
    sets or sequences and are now state of the art in many areas of biology and language
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Graph neural networks (GNNs)
  prefs: []
  type: TYPE_NORMAL
- en: Operate on graph-structured data, passing messages between nodes and their neighbors.
    They are useful when data has relational or interaction-based structure.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs: []
  type: TYPE_NORMAL
- en: Learn compact representations by encoding inputs into a latent space and reconstructing
    them. They are common in unsupervised learning and denoising tasks..
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explain each of these as they appear throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning models are typically trained using three dataset partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: Training set
  prefs: []
  type: TYPE_NORMAL
- en: Used to fit the model’s parameters (its weights) by minimizing a loss function
  prefs: []
  type: TYPE_NORMAL
- en: Validation set
  prefs: []
  type: TYPE_NORMAL
- en: Used to tune *hyperparameters* (discussed next) and evaluate performance during
    development
  prefs: []
  type: TYPE_NORMAL
- en: Test set
  prefs: []
  type: TYPE_NORMAL
- en: Held out until the end to assess the model’s final performance on truly unseen
    data, providing an estimate of how well it generalizes
  prefs: []
  type: TYPE_NORMAL
- en: This split structure is good practice because it helps ensure your model generalizes
    well to new data and gives you a reliable estimate of its real-world performance.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hyperparameters control how a model is trained. They’re set before training
    begins and are not updated by the optimizer. Common examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate
  prefs: []
  type: TYPE_NORMAL
- en: How quickly the model updates its weights
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs: []
  type: TYPE_NORMAL
- en: How many examples are processed together during one update
  prefs: []
  type: TYPE_NORMAL
- en: Model size
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers, hidden units, or attention heads
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs: []
  type: TYPE_NORMAL
- en: The dropout rate or weight decay, to prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate different hyperparameter settings using performance on the validation
    set. If we tune hyperparameters based on training performance alone, we risk *overfitting*—the
    model may simply memorize the training data, including noise or outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting leads to poor performance on new data. In contrast, *generalization*
    means the model has learned patterns that apply beyond the training examples.
    This is a key requirement in biological applications, where test data can often
    come from different experiments or conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Activation functions introduce nonlinearity into neural networks, enabling them
    to model complex relationships between inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: As data flows through a model, it’s transformed into intermediate tensors known
    as *activations*. These activations pass through a series of linear projections
    and nonlinear activation functions at each layer. The output of the final hidden
    layer is then often passed through a final activation function, which produces
    the model’s ultimate prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the right final activation is important—it should reflect the kind
    of data you’re trying to predict. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `sigmoid` for binary classification, where outputs should lie between 0
    and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `softmax` for multiclass classification, where outputs represent probabilities
    across categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use *no* final activation if your loss function expects raw logits (e.g., `sigmoid_cross_entropy`
    or `softmax_cross_entropy`). These loss functions apply the activation internally,
    so applying it yourself would be redundant or even harmful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid ReLU or GELU as final activations when predicting real-valued outputs
    that can be negative—they will clip or distort negative values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For regression tasks, no activation is often best—or use tanh or sigmoid only
    if your target values are known to be bounded (e.g., between -1 and 1 or 0 and
    1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When in doubt, check the range of your target values, and choose an activation
    (or none) that allows the model to produce outputs in that range. The term *logits*
    is often used to describe the raw, unnormalized outputs of the final layer, especially
    before applying softmax or sigmoid. But it’s a somewhat overloaded term that is
    sometimes used more loosely—for example, to describe intermediate values passed
    into a softmax, such as attention scores in a transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the most commonly used activation functions, with their shapes
    shown in [Figure 1-3](#fig1-3):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Common activation functions used in deep learning.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A bit more detail on these common activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU (rectified linear unit)
  prefs: []
  type: TYPE_NORMAL
- en: Outputs zero for negative inputs and the input itself for positive inputs. It
    is simple and effective, especially in deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: GELU (Gaussian error linear unit)
  prefs: []
  type: TYPE_NORMAL
- en: A smoother alternative to ReLU, often used in transformer models. It can yield
    slightly better results.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid and tanh
  prefs: []
  type: TYPE_NORMAL
- en: Older activation functions that squash values into fixed ranges. Sigmoid maps
    inputs to a range between 0 and 1, while tanh maps to a range between –1 and 1\.
    They’re useful in certain settings, such as output layers for binary classification,
    but can suffer from gradient issues in deeper models.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax
  prefs: []
  type: TYPE_NORMAL
- en: Converts a vector of values into a probability distribution that sums to 1\.
    Used in the final layer of multiclass classification models. Not applied element-wise;
    it operates across the whole vector.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ll usually stick to ReLU or GELU for hidden layers, and choose
    the final activation based on the prediction task.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimizers are algorithms that adjust a model’s parameters (its weights and
    biases) to reduce the error (loss) during training. They do this using *gradient
    descent*, which computes how each parameter affects the loss and updates it to
    reduce the error.
  prefs: []
  type: TYPE_NORMAL
- en: We mostly use *Adam*, a widely used optimizer that adapts the learning rate
    for each parameter and combines ideas from momentum and RMSProp. It usually trains
    faster and more reliably than plain gradient descent, especially in noisy or sparse
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: Initialization Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before training starts, we need to set initial values for model parameters.
    This step is more important than it sounds. Poor initialization can cause gradients
    to vanish or explode, making training unstable.
  prefs: []
  type: TYPE_NORMAL
- en: We typically use Xavier (Glorot) initialization, which is designed to keep the
    scale of activations and gradients roughly stable across layers. This helps training
    proceed smoothly. By default, Flax uses Xavier initialization for most layers
    like `Dense` and `Conv`, so you don’t usually need to specify it manually.
  prefs: []
  type: TYPE_NORMAL
- en: Model Checkpointing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During training, it’s often useful to periodically save your model parameters
    so that you can resume later or avoid losing progress. While production training
    pipelines often use robust checkpointing strategies (e.g., saving best-performing
    checkpoints, versioned histories, etc.), that level of complexity isn’t necessary
    for the teaching examples in this book. Instead, we provide a lightweight utility
    that saves and restores just the most recent checkpoint. This is enough to pause
    and resume training or to store the final model output for later use. You’ll see
    it used in several chapters to simplify experimentation and reduce boilerplate.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re building your own training loop for production or research, consider
    upgrading to a more complete checkpointing solution—for example, using [Orbax](https://oreil.ly/aazC8),
    Flax’s newer checkpointing system.
  prefs: []
  type: TYPE_NORMAL
- en: Early Stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real-world training, it’s often a good idea to stop training when your validation
    performance stops improving. This is known as *early stopping*, which helps prevent
    overfitting and saves compute.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we often show longer training runs to visualize how the loss evolves
    over time. But in practice, you’ll likely want to use early stopping when training
    your own models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flax includes a simple utility for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You can track validation metrics and stop training when they stop improving
    for a certain number of steps (the `patience` parameter).
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a Working Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training large neural networks often benefits from *GPU acceleration*. There
    are a few ways to access a GPU, depending on your budget, goals, and technical
    comfort level.
  prefs: []
  type: TYPE_NORMAL
- en: One option is to use a local machine with a GPU, such as a gaming desktop or
    a workstation you’ve set up yourself. This gives you full control and avoids ongoing
    cloud costs, but it requires up-front hardware investment and setup. Another option
    is to rent GPU access from cloud providers like AWS, GCP, or Azure. These offer
    flexibility and scalability but can get expensive over time, especially if you’re
    training large models.
  prefs: []
  type: TYPE_NORMAL
- en: For many beginners and small projects, Google Colab is a great place to start.
    It provides free, cloud-based Jupyter notebooks with GPU or TPU support and minimal
    setup.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we’ll briefly walk through your options, from interactive
    notebooks to fully customized GPU development environments.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an Interactive Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jupyter notebooks are a popular and powerful tool for interactive coding, making
    them ideal for running the code examples in this book. They let you write code,
    execute it in cells, visualize results, and document your work, all in one place.
    This interactivity makes it easy to experiment, debug, and iterate quickly. Common
    notebook environments include JupyterLab, VSCode with Jupyter extensions, and
    Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab in particular provides cloud-based notebooks with free access to
    GPUs and TPUs, making it a great option if you don’t have a powerful local machine.
    It runs entirely in your browser and requires no setup beyond a Google account.
    You can install libraries with commands like `!pip install jax flax optax` and
    save notebooks directly to Google Drive. To enable hardware acceleration, navigate
    to the Runtime menu. From the drop-down menu, select “Change runtime type” and
    then select a GPU or TPU.
  prefs: []
  type: TYPE_NORMAL
- en: We provide all code in this book as Google Colab notebooks that you can open,
    run, and modify interactively.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Colab sessions can time out after periods of inactivity, so remember to save
    your work frequently.
  prefs: []
  type: TYPE_NORMAL
- en: While notebooks are excellent for exploration and prototyping, they can make
    version control, debugging, and maintaining complex projects more difficult. For
    longer-term work, setting up a dedicated development environment with GPU support
    offers more control, scalability, and reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring Your Code for Reuse and Debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Notebooks are great for interactive use, but as you begin building your own
    projects, it’s worth thinking about how you will structure it early. Organize
    your code into clear, reusable Python modules: one for datasets, one for models,
    one for metrics and evaluation. Keeping these pieces separate makes your code
    easier to debug, scale, and reuse later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This modular approach mirrors how this book is structured. Each subsection
    of each chapter generally focuses on a specific building block: dataset, model,
    training loop, and so on. As you follow along, you might try using this structure
    in your own code. Build clean components you can plug into future projects, and
    you’ll move faster each time.'
  prefs: []
  type: TYPE_NORMAL
- en: A good dataset class in particular will help you catch issues early. There are
    plenty of reasons you might get confused during a project. If your dataset and
    metrics are cleanly separated and easy to inspect, you can at least rule them
    out as the source of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Add frequent sanity checks. Can your model overfit a tiny dataset (like 10 examples)?
    Do loss and accuracy behave as expected when you shuffle labels?
  prefs: []
  type: TYPE_NORMAL
- en: Plot everything. Visualizing predictions, losses, or inputs will often reveal
    issues you wouldn’t notice in the final numbers alone—like shifted targets, mislabeled
    data, empty tensors, or models that haven’t learned anything at all.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up a GPU Development Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As your models grow more complex, you may want to move beyond notebooks into
    a more powerful GPU-enabled setup, either locally or in the cloud. A robust development
    environment can speed up experimentation, simplify debugging, and make your workflow
    more reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: 'For local development, we recommend the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Docker with NVIDIA Docker to create containerized environments that seamlessly
    access your GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pair it with an editor like VSCode, which supports Docker and remote development
    for a smooth workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Git for version control to track changes, collaborate, and back up your
    work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mac users with Apple Silicon can try [`jax-metal`](https://oreil.ly/_FYSk) to
    enable GPU acceleration via Apple’s Metal backend. While it’s improving rapidly,
    not all features are fully supported yet, so expect occasional compatibility issues.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Paid cloud services like AWS, GCP, or Azure offer on-demand GPU instances if
    you don’t have a local GPU. Alternatives like Paperspace, Lambda Labs, or RunPod
    can offer simpler setup and better value for smaller-scale projects.
  prefs: []
  type: TYPE_NORMAL
- en: Version Conflicts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning and scientific computing libraries evolve quickly—and not
    always in sync. You’ll inevitably hit version conflicts between tools like NumPy,
    JAX, Flax, PyTorch, or Hugging Face Transformers. Strive for a balance: use versions
    recent enough to benefit from improvements, but not so new that they break compatibility
    with everything else.'
  prefs: []
  type: TYPE_NORMAL
- en: Tools like [`uv`](https://oreil.ly/_aezy)—a faster, more flexible alternative
    to `pip`—can help override incompatibilities and install packages even when their
    metadata says they’re not compatible. It’s not a permanent solution, but it can
    get you unstuck.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working outside a notebook, always use a virtual environment (`venv`,
    `conda`, or `uv venv`) to keep dependencies isolated. Another option is to define
    your environment in a Docker container, which guarantees full reproducibility
    across machines. This setup is especially useful when working on remote GPU instances
    or in the cloud—something we’ll explain later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: When in doubt, check GitHub issues or forums—version mismatches are a common
    pain point for everyone, and at the very least, you can commiserate with others.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working in a notebook, version issues can be especially tricky. Google
    Colab, for example, comes with many preinstalled packages—but these can be outdated
    or incompatible with the latest JAX/Flax stack. You can install or override versions
    directly in a cell using `!pip install`, but you may need to restart the runtime
    afterward for changes to take effect (Runtime > Restart runtime).
  prefs: []
  type: TYPE_NORMAL
- en: A Living Document
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this book endeavors to capture the state of the field at the time of writing,
    deep learning and biology remain fast-moving frontiers. A printed book is a static
    artifact; in contrast, frameworks evolve, APIs break, and new ideas emerge constantly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ve done our best to future-proof examples and flag potential breaking changes
    (like Flax’s transition from `linen` to `nnx`). Still, some discrepancies may
    arise over time. If you spot anything out of date—or have corrections, improvements,
    suggestions, or extensions—please let us know. The online repository can evolve
    even if the book is in a fixed state.
  prefs: []
  type: TYPE_NORMAL
- en: We also encourage you to look beyond these pages. Resources like [D2L.ai](http://D2L.ai),
    fast.ai, the JAX ecosystem, and preprints on bioRxiv and arXiv are excellent places
    to go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Most of all, experiment, build, break things, train bad models, and learn. The
    best way to grow in deep learning for biology is to get your hands dirty.
  prefs: []
  type: TYPE_NORMAL
