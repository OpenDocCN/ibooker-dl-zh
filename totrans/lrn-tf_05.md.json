["```py\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\n# Define some parameters\nelement_size = 28\ntime_steps = 28\nnum_classes = 10\nbatch_size = 128\nhidden_layer_size = 128\n\n# Where to save TensorBoard model summaries\nLOG_DIR = \"logs/RNN_with_summaries\"\n\n# Create placeholders for inputs, labels\n_inputs = tf.placeholder(tf.float32,shape=[None, time_steps,\n                        element_size],\n                                              name='inputs')\ny = tf.placeholder(tf.float32, shape=[None, num_classes],\n                                              name='labels')\n\n```", "```py\nbatch_x, batch_y = mnist.train.next_batch(batch_size)\n# Reshape data to get 28 sequences of 28 pixels\nbatch_x = batch_x.reshape((batch_size, time_steps, element_size))\n```", "```py\n# This helper function, taken from the official TensorFlow documentation,\n# simply adds some ops that take care of logging summaries\ndef variable_summaries(var):\n  with tf.name_scope('summaries'):\n   mean = tf.reduce_mean(var)\n   tf.summary.scalar('mean', mean)\n   with tf.name_scope('stddev'):\n    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n   tf.summary.scalar('stddev', stddev)\n   tf.summary.scalar('max', tf.reduce_max(var))\n   tf.summary.scalar('min', tf.reduce_min(var))\n   tf.summary.histogram('histogram', var)\n```", "```py\n# Weights and bias for input and hidden layer\nwith tf.name_scope('rnn_weights'):\n  with tf.name_scope(\"W_x\"):\n    Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n    variable_summaries(Wx)\n  with tf.name_scope(\"W_h\"):\n    Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n    variable_summaries(Wh)\n  with tf.name_scope(\"Bias\"):\n    b_rnn = tf.Variable(tf.zeros([hidden_layer_size]))\n    variable_summaries(b_rnn)\n\n```", "```py\ndef rnn_step(previous_hidden_state,x):\n\n    current_hidden_state = tf.tanh(\n      tf.matmul(previous_hidden_state, Wh) +\n      tf.matmul(x, Wx) + b_rnn)\n\n    return current_hidden_state\n\n```", "```py\n# Processing inputs to work with scan function\n# Current input shape: (batch_size, time_steps, element_size)\nprocessed_input = tf.transpose(_inputs, perm=[1, 0, 2])\n# Current input shape now: (time_steps, batch_size, element_size)\n\ninitial_hidden = tf.zeros([batch_size,hidden_layer_size])\n# Getting all state vectors across time\nall_hidden_states = tf.scan(rnn_step,\n              processed_input,\n              initializer=initial_hidden,\n              name='states')\n\n```", "```py\nimport numpy as np\nimport tensorflow as tf\n\nelems = np.array([\"T\",\"e\",\"n\",\"s\",\"o\",\"r\", \" \", \"F\",\"l\",\"o\",\"w\"])\nscan_sum = tf.scan(lambda a, x: a + x, elems)\n\nsess=tf.InteractiveSession()\nsess.run(scan_sum)\n\n```", "```py\narray([b'T', b'Te', b'Ten', b'Tens', b'Tenso', b'Tensor', b'Tensor ',\n\u00a0 \u00a0 \u00a0 \u00a0b'Tensor F', b'Tensor Fl', b'Tensor Flo', b'Tensor Flow'],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dtype=object)\n\n```", "```py\n# Weights for output layers\nwith tf.name_scope('linear_layer_weights') as scope:\n  with tf.name_scope(\"W_linear\"):\n    Wl = tf.Variable(tf.truncated_normal([hidden_layer_size,\n                       num_classes],\n                       mean=0,stddev=.01))\n    variable_summaries(Wl)\n  with tf.name_scope(\"Bias_linear\"):\n    bl = tf.Variable(tf.truncated_normal([num_classes],\n                      mean=0,stddev=.01))\n    variable_summaries(bl)\n\n# Apply linear layer to state vector \u00a0 \u00a0\ndef get_linear_layer(hidden_state):\n\n  return tf.matmul(hidden_state, Wl) + bl\n\nwith tf.name_scope('linear_layer_weights') as scope:\n  # Iterate across time, apply linear layer to all RNN outputs\n  all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n  # Get last output\n  output = all_outputs[-1]\n  tf.summary.histogram('outputs', output)\n\n```", "```py\nwith tf.name_scope('cross_entropy'):\n  cross_entropy = tf.reduce_mean(\n  tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n  tf.summary.scalar('cross_entropy', cross_entropy)\n\nwith tf.name_scope('train'):\n  # Using RMSPropOptimizer\n  train_step = tf.train.RMSPropOptimizer(0.001, 0.9)\\\n                                  .minimize(cross_entropy)\n\nwith tf.name_scope('accuracy'):\n  correct_prediction = tf.equal(\n                               tf.argmax(y,1), tf.argmax(output,1))\n\n  accuracy = (tf.reduce_mean(\n                      tf.cast(correct_prediction, tf.float32)))*100\n  tf.summary.scalar('accuracy', accuracy)\n\n# Merge all the summaries\nmerged = tf.summary.merge_all()\n\n```", "```py\n# Get a small test set \u00a0\ntest_data = mnist.test.images[:batch_size].reshape((-1, time_steps,\n                          element_size))\ntest_label = mnist.test.labels[:batch_size]\n\nwith tf.Session() as sess:\n  # Write summaries to LOG_DIR -- used by TensorBoard\n  train_writer = tf.summary.FileWriter(LOG_DIR + '/train',\n                    graph=tf.get_default_graph())\n  test_writer = tf.summary.FileWriter(LOG_DIR + '/test',\n                    graph=tf.get_default_graph())\n\n  sess.run(tf.global_variables_initializer())\n\n  for i in range(10000):\n\n      batch_x, batch_y = mnist.train.next_batch(batch_size)\n      # Reshape data to get 28 sequences of 28 pixels\n      batch_x = batch_x.reshape((batch_size, time_steps,\n                   element_size))\n      summary,_ = sess.run([merged,train_step],\n                feed_dict={_inputs:batch_x, y:batch_y})\n      # Add to summaries\n      train_writer.add_summary(summary, i)\n\n      if i % 1000 == 0:\n        acc,loss, = sess.run([accuracy,cross_entropy],\n                  feed_dict={_inputs: batch_x,\n                        y: batch_y})\n        print (\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n           \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n           \"{:.5f}\".format(acc)) \n      if i % 10:\n        # Calculate accuracy for 128 MNIST test images and\n        # add to summaries\n        summary, acc = sess.run([merged, accuracy],\n                    feed_dict={_inputs: test_data,\n                         y: test_label})\n        test_writer.add_summary(summary, i)\n\n  test_acc = sess.run(accuracy, feed_dict={_inputs: test_data,\n                      y: test_label})\n  print (\"Test Accuracy:\", test_acc)\n\n```", "```py\nIter 0, Minibatch Loss= 2.303386, Training Accuracy= 7.03125\nIter 1000, Minibatch Loss= 1.238117, Training Accuracy= 52.34375\nIter 2000, Minibatch Loss= 0.614925, Training Accuracy= 85.15625\nIter 3000, Minibatch Loss= 0.439684, Training Accuracy= 82.81250\nIter 4000, Minibatch Loss= 0.077756, Training Accuracy= 98.43750\nIter 5000, Minibatch Loss= 0.220726, Training Accuracy= 89.84375\nIter 6000, Minibatch Loss= 0.015013, Training Accuracy= 100.00000\nIter 7000, Minibatch Loss= 0.017689, Training Accuracy= 100.00000\nIter 8000, Minibatch Loss= 0.065443, Training Accuracy= 99.21875\nIter 9000, Minibatch Loss= 0.071438, Training Accuracy= 98.43750\nTesting Accuracy: 97.6563\n\n```", "```py\ntensorboard--logdir=*`LOG_DIR`*\n```", "```py\ntensorboard--logdir=rnn_demo:*`LOG_DIR`*\n```", "```py\ntensorboard--logdir=rnn_demo1:*`LOG_DIR1`*,rnn_demo2:*`LOG_DIR2`*\n```", "```py\nStarting TensorBoard b'39' on port 6006\n(You can navigate to http://10.100.102.4:6006)\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nelement_size = 28;time_steps = 28;num_classes = 10\nbatch_size = 128;hidden_layer_size = 128\n\n_inputs = tf.placeholder(tf.float32,shape=[None, time_steps,\n                     element_size],\n                     name='inputs')\ny = tf.placeholder(tf.float32, shape=[None, num_classes],name='inputs')\n\n# TensorFlow built-in functions\nrnn_cell = tf.contrib.rnn.BasicRNNCell(hidden_layer_size)\noutputs, _ = tf.nn.dynamic_rnn(rnn_cell, _inputs, dtype=tf.float32)\n\nWl = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes],\n                  mean=0,stddev=.01))\nbl = tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))\n\ndef get_linear_layer(vector):\n  return tf.matmul(vector, Wl) + bl\n\nlast_rnn_output = outputs[:,-1,:]\nfinal_output = get_linear_layer(last_rnn_output)\n\nsoftmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output,\n                         labels=y)\ncross_entropy = tf.reduce_mean(softmax)\ntrain_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(final_output,1))\naccuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n\nsess=tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\ntest_data = mnist.test.images[:batch_size].reshape((-1,\n                      time_steps, element_size))\ntest_label = mnist.test.labels[:batch_size]\n\nfor i in range(3001):\n\n   batch_x, batch_y = mnist.train.next_batch(batch_size)\n   batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n   sess.run(train_step,feed_dict={_inputs:batch_x,\n                   y:batch_y})\n   if i % 1000 == 0:\n      acc = sess.run(accuracy, feed_dict={_inputs: batch_x,\n                        y: batch_y})\n      loss = sess.run(cross_entropy,feed_dict={_inputs:batch_x,\n                          y:batch_y})\n      print (\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n         \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n         \"{:.5f}\".format(acc)) \n\nprint (\"Testing Accuracy:\", \n  sess.run(accuracy, feed_dict={_inputs: test_data, y: test_label}))\n\n```", "```py\nimport numpy as np\nimport tensorflow as tf\n\nbatch_size = 128;embedding_dimension = 64;num_classes = 2\nhidden_layer_size = 32;times_steps = 6;element_size = 1\n```", "```py\ndigit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n          6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\ndigit_to_word_map[0]=\"PAD\"\n\neven_sentences = []\nodd_sentences = []\nseqlens = []\nfor i in range(10000):\n  rand_seq_len = np.random.choice(range(3,7))\n  seqlens.append(rand_seq_len)\n  rand_odd_ints = np.random.choice(range(1,10,2),\n                  rand_seq_len)\n  rand_even_ints = np.random.choice(range(2,10,2),\n                   rand_seq_len)\n\n    # Padding\n  if rand_seq_len<6:\n    rand_odd_ints = np.append(rand_odd_ints,\n                 [0]*(6-rand_seq_len))\n    rand_even_ints = np.append(rand_even_ints,\n                 [0]*(6-rand_seq_len))\n\n  even_sentences.append(\" \".join([digit_to_word_map[r] for\n               r in rand_odd_ints]))\n  odd_sentences.append(\" \".join([digit_to_word_map[r] for\n               r in rand_even_ints])) \n\ndata = even_sentences+odd_sentences\n# Same seq lengths for even, odd sentences\nseqlens*=2\n\n```", "```py\neven_sentences[0:6]\n\nOut:\n['Four Four Two Four Two PAD',\n'Eight Six Four PAD PAD PAD',\n'Eight Two Six Two PAD PAD',\n'Eight Four Four Eight PAD PAD',\n'Eight Eight Four PAD PAD PAD',\n'Two Two Eight Six Eight Four']\n\n```", "```py\nodd_sentences[0:6]\n\nOut:\n['One Seven Nine Three One PAD',\n'Three Nine One PAD PAD PAD',\n'Seven Five Three Three PAD PAD',\n'Five Five Three One PAD PAD',\n'Three Three Five PAD PAD PAD',\n'Nine Three Nine Five Five Three']\n\n```", "```py\nseqlens[0:6]\n\nOut: \n[5, 3, 4, 4, 3, 6]\n```", "```py\n# Map from words to indices\nword2index_map ={}\nindex=0\nfor sent in data:\n  for word in sent.lower().split():\n    if word not in word2index_map:\n      word2index_map[word] = index\n      index+=1\n# Inverse map\u00a0 \u00a0\u00a0\nindex2word_map = {index: word for word, index in word2index_map.items()}      \nvocabulary_size = len(index2word_map)\n\n```", "```py\nlabels = [1]*10000 + [0]*10000\nfor i in range(len(labels)):\n  label = labels[i]\n  one_hot_encoding = [0]*2\n  one_hot_encoding[label] = 1\n  labels[i] = one_hot_encoding\n\ndata_indices = list(range(len(data)))\nnp.random.shuffle(data_indices)\ndata = np.array(data)[data_indices]\n\nlabels = np.array(labels)[data_indices]\nseqlens = np.array(seqlens)[data_indices]\ntrain_x = data[:10000]\ntrain_y = labels[:10000]\ntrain_seqlens = seqlens[:10000]\n\ntest_x = data[10000:]\ntest_y = labels[10000:]\ntest_seqlens = seqlens[10000:]\n\n```", "```py\ndef get_sentence_batch(batch_size,data_x,\n           data_y,data_seqlens):\n  instance_indices = list(range(len(data_x)))\n  np.random.shuffle(instance_indices)\n  batch = instance_indices[:batch_size]\n  x = [[word2index_map[word] for word in data_x[i].lower().split()]\n    for i in batch]\n  y = [data_y[i] for i in batch]\n  seqlens = [data_seqlens[i] for i in batch]\n  return x,y,seqlens\n\n```", "```py\n_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n\n# seqlens for dynamic calculation\n_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n```", "```py\nwith tf.name_scope(\"embeddings\"):\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size,\n             embedding_dimension],\n             -1.0, 1.0),name='embedding')\n  embed = tf.nn.embedding_lookup(embeddings, _inputs)\n\n```", "```py\nwith tf.variable_scope(\"lstm\"):\n\n  lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,\n                      forget_bias=1.0)\n  outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed,\n                    sequence_length = _seqlens,\n                    dtype=tf.float32)\n\nweights = {\n  'linear_layer': tf.Variable(tf.truncated_normal([hidden_layer_size,\n                          num_classes],\n                          mean=0,stddev=.01))\n}\nbiases = {\n  'linear_layer':tf.Variable(tf.truncated_normal([num_classes],\n                         mean=0,stddev=.01))\n}\n\n# Extract the last relevant output and use in a linear layer\nfinal_output = tf.matmul(states[1],\n            weights[\"linear_layer\"]) + biases[\"linear_layer\"]\nsoftmax = tf.nn.softmax_cross_entropy_with_logits(logits = final_output,\n                         labels = _labels)            \ncross_entropy = tf.reduce_mean(softmax)\n\n```", "```py\ntrain_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(_labels,1),\n               tf.argmax(final_output,1))\naccuracy = (tf.reduce_mean(tf.cast(correct_prediction,\n                 tf.float32)))*100\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n\n  for step in range(1000):\n    x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n                             train_x,train_y,\n                             train_seqlens)\n    sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n                   _seqlens:seqlen_batch})\n\n    if step % 100 == 0:\n      acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n                       _labels:y_batch,\n                       _seqlens:seqlen_batch})\n      print(\"Accuracy at %d: %.5f\" % (step, acc))\n\n  for test_batch in range(5):\n    x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n                            test_x,test_y,\n                            test_seqlens)\n    batch_pred,batch_acc = sess.run([tf.argmax(final_output,1),\n                    accuracy],\n                    feed_dict={_inputs:x_test,\n                         _labels:y_test,\n                         _seqlens:seqlen_test})\n    print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n\n   output_example = sess.run([outputs],feed_dict={_inputs:x_test,\n                         _labels:y_test,\n                         _seqlens:seqlen_test})\n  states_example = sess.run([states[1]],feed_dict={_inputs:x_test,\n                         _labels:y_test,\n                         _seqlens:seqlen_test})\n\n```", "```py\nAccuracy at 0: 32.81250\nAccuracy at 100: 100.00000\nAccuracy at 200: 100.00000\nAccuracy at 300: 100.00000\nAccuracy at 400: 100.00000\nAccuracy at 500: 100.00000\nAccuracy at 600: 100.00000\nAccuracy at 700: 100.00000\nAccuracy at 800: 100.00000\nAccuracy at 900: 100.00000\nTest batch accuracy 0: 100.00000\nTest batch accuracy 1: 100.00000\nTest batch accuracy 2: 100.00000\nTest batch accuracy 3: 100.00000\nTest batch accuracy 4: 100.00000\n```", "```py\nseqlen_test[1]\n\nOut:\n4\n```", "```py\noutput_example[0][1].shape\n\nOut: \n(6, 32)\n\n```", "```py\noutput_example[0][1][:6,0:3]\n\nOut:\narray([[-0.44493711, -0.51363373, -0.49310589],\n   [-0.72036862, -0.68590945, -0.73340571],\n   [-0.83176643, -0.78206956, -0.87831545],\n   [-0.87982416, -0.82784462, -0.91132098],\n   [ 0.    , 0.    , 0.    ],\n   [ 0.    , 0.    , 0.    ]], dtype=float32)\n```", "```py\nstates_example[0][1][0:3]\n\nOut:\narray([-0.87982416, -0.82784462, -0.91132098], dtype=float32)\n```", "```py\nnum_LSTM_layers = 2\nwith tf.variable_scope(\"lstm\"):\n\n  lstm_cell_list = \n  [tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,forget_bias=1.0) \n    for ii in range(num_LSTM_layers)]\n  cell = tf.contrib.rnn.MultiRNNCell(cells=lstm_cell_list, \n    state_is_tuple=True)\n\n  outputs, states = tf.nn.dynamic_rnn(cell, embed,\n                    sequence_length = _seqlens,\n                    dtype=tf.float32)\n\n```", "```py\n# Extract the final state and use in a linear layer\nfinal_output = tf.matmul(states[num_LSTM_layers-1][1],\n            weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n\n```"]