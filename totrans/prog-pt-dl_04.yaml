- en: Chapter 4\. Transfer Learning and Other Tricks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。迁移学习和其他技巧
- en: Having looked over the architectures in the previous chapter, you might wonder
    whether you could download an already trained model and train it even further.
    And the answer is yes! It’s an incredibly powerful technique in deep learning
    circles called *transfer learning*, whereby a network trained for one task (e.g.,
    ImageNet) is adapted to another (fish versus cats).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看了上一章的架构之后，您可能会想知道是否可以下载一个已经训练好的模型，然后进一步训练它。答案是肯定的！这是深度学习领域中一种非常强大的技术，称为*迁移学习*，即将一个任务（例如ImageNet）训练的网络适应到另一个任务（鱼与猫）。
- en: Why would you do this? It turns out that an architecture trained on ImageNet
    already knows an awful lot about images, and in particular, quite a bit about
    whether something is a cat or a fish (or a dog or a whale). Because you’re no
    longer starting from an essentially blank neural network, with transfer learning
    you’re likely to spend much less time in training, *and* you can get away with
    a vastly smaller training dataset. Traditional deep learning approaches take huge
    amounts of data to generate good results. With transfer learning, you can build
    human-level classifiers with a few hundred images.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要这样做呢？事实证明，一个在ImageNet上训练过的架构已经对图像有了很多了解，特别是对于是否是猫或鱼（或狗或鲸鱼）有相当多的了解。因为您不再从一个基本上空白的神经网络开始，使用迁移学习，您可能会花费更少的时间进行训练，*而且*您可以通过一个远远较小的训练数据集来完成。传统的深度学习方法需要大量数据才能产生良好的结果。使用迁移学习，您可以用几百张图像构建人类级别的分类器。
- en: Transfer Learning with ResNet
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ResNet进行迁移学习
- en: Now, the obvious thing to do is to create a ResNet model as we did in [Chapter 3](ch03.html#convolutional-neural-networks)
    and just slot it into our existing training loop. And you can do that! There’s
    nothing magical in the ResNet model; it’s built up from the same building blocks
    that you’ve already seen. However, it’s a big model, and although you will see
    some improvement over a baseline ResNet with your data, you will need a lot of
    data to make sure that the training *signal* gets to all parts of the architecture
    and trains them significantly toward your new classification task. We’re trying
    to avoid using a lot of data in this approach.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，显而易见的事情是创建一个ResNet模型，就像我们在[第3章](ch03.html#convolutional-neural-networks)中所做的那样，并将其插入到我们现有的训练循环中。您可以这样做！ResNet模型中没有什么神奇的东西；它是由您已经看到的相同构建块构建而成。然而，这是一个庞大的模型，尽管您将看到一些改进，但您需要大量数据来确保训练*信号*到达架构的所有部分，并显著训练它们以适应您的新分类任务。我们试图避免在这种方法中使用大量数据。
- en: 'Here’s the thing, though: we’re not dealing with an architecture that has been
    initialized with random parameters, as we have done in the past. Our pretrained
    ResNet model already has a bunch of information encoded into it for image recognition
    and classification needs, so why bother attempting to retrain it? Instead, we
    *fine-tune* the network. We alter the architecture slightly to include a new network
    block at the end, replacing the standard 1,000-category linear layers that normally
    perform ImageNet classification. We then *freeze* all the existing ResNet layers,
    and when we train, we update only the parameters in our new layers, but still
    take the activations from our frozen layers. This allows us to quickly train our
    new layers while preserving the information that the pretrained layers already
    contain.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一点需要注意：我们不再处理一个使用随机参数初始化的架构，就像我们过去所做的那样。我们的预训练的ResNet模型已经编码了大量信息，用于图像识别和分类需求，那么为什么要尝试重新训练它呢？相反，我们*微调*网络。我们稍微改变架构，以在末尾包含一个新的网络块，替换通常执行ImageNet分类的标准1,000个类别的线性层。然后，我们*冻结*所有现有的ResNet层，当我们训练时，我们只更新我们新层中的参数，但仍然从我们冻结的层中获取激活。这样可以快速训练我们的新层，同时保留预训练层已经包含的信息。
- en: 'First, let’s create a pretrained ResNet-50 model:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个预训练的ResNet-50模型：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we need to freeze the layers. The way we do this is simple: we stop them
    from accumulating gradients by using `requires_grad()`. We need to do this for
    every parameter in the network, but helpfully, PyTorch provides a `parameters()`
    method that makes this rather easy:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要冻结层。我们这样做的方法很简单：通过使用`requires_grad()`来阻止它们累积梯度。我们需要为网络中的每个参数执行此操作，但幸运的是，PyTorch提供了一个`parameters()`方法，使这变得相当容易：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tip
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'You might not want to freeze the `BatchNorm` layers in a model, as they will
    be trained to approximate the mean and standard deviation of the dataset that
    the model was originally trained on, not the dataset that you want to fine-tune
    on. Some of the *signal* from your data may end up being lost as `BatchNorm` *corrects*
    your input. You can look at the model structure and freeze only layers that aren’t
    `BatchNorm` like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能不想冻结模型中的`BatchNorm`层，因为它们将被训练来逼近模型最初训练的数据集的均值和标准差，而不是您想要微调的数据集。由于`BatchNorm`会*校正*您的输入，您的数据中的一些*信号*可能会丢失。您可以查看模型结构，并仅冻结不是`BatchNorm`的层，就像这样：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we need to replace the final classification block with a new one that
    we will train for detecting cats or fish. In this example, we replace it with
    a couple of `Linear` layers, a `ReLU`, and `Dropout`, but you could have extra
    CNN layers here too. Happily, the definition of PyTorch’s implementation of ResNet
    stores the final classifier block as an instance variable, `fc`, so all we need
    to do is replace that with our new structure (other models supplied with PyTorch
    use either `fc` or `classifier`, so you’ll probably want to check the definition
    in the source if you’re trying this with a different model type):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要用一个新的分类块替换最终的分类块，用于检测猫或鱼。在这个例子中，我们用几个`Linear`层、一个`ReLU`和`Dropout`来替换它，但您也可以在这里添加额外的CNN层。令人高兴的是，PyTorch对ResNet的实现定义了最终分类器块作为一个实例变量`fc`，所以我们只需要用我们的新结构替换它（PyTorch提供的其他模型使用`fc`或`classifier`，所以如果您尝试使用不同的模型类型，您可能需要检查源代码中的定义）：
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code, we take advantage of the `in_features` variable that
    allows us to grab the number of activations coming into a layer (2,048 in this
    case). You can also use `out_features` to discover the activations coming out.
    These are handy functions for when you’re snapping together networks like building
    bricks; if the incoming features on a layer don’t match the outgoing features
    of the previous layer, you’ll get an error at runtime.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们利用了`in_features`变量，它允许我们获取传入层的激活数量（在本例中为2,048）。你也可以使用`out_features`来发现传出的激活数量。当你像搭积木一样组合网络时，这些都是很方便的函数；如果一层的传入特征与前一层的传出特征不匹配，你会在运行时得到一个错误。
- en: Finally, we go back to our training loop and then train the model as per usual.
    You should see some large jumps in accuracy even within a few epochs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回到我们的训练循环，然后像往常一样训练模型。你应该在几个epochs内看到一些准确度的大幅提升。
- en: Transfer learning is a key technique for improving the accuracy of your deep
    learning application, but we can employ a bunch of other tricks in order to boost
    the performance of our model. Let’s take a look at some of them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是提高深度学习应用准确性的关键技术，但我们可以采用一堆其他技巧来提升我们模型的性能。让我们看看其中一些。
- en: Finding That Learning Rate
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 找到那个学习率
- en: 'You might remember from [Chapter 2](ch02.html#image-classification-with-pytorch)
    that I introduced the concept of a *learning rate* for training neural networks,
    mentioned that it was one of the most important hyperparameters you can alter,
    and then waved away what you should use for it, suggesting a rather small number
    and for you to experiment with different values. Well…the bad news is, that really
    is how a lot of people discover the optimum learning rate for their architectures,
    usually with a technique called *grid search*, exhaustively searching their way
    through a subset of learning rate values, comparing the results against a validation
    dataset. This is incredibly time-consuming, and although people do it, many others
    err on the side of the practioner’s lore. For example, a learning rate value that
    has empirically been observed to work with the Adam optimizer is 3e-4\. This is
    known as Karpathy’s constant, after Andrej Karpathy (currently director of AI
    at Tesla) [tweeted about it](https://oreil.ly/WLw3q) in 2016\. Unfortunately,
    fewer people read his next tweet: “I just wanted to make sure that people understand
    that this is a joke.” The funny thing is that 3e-4 tends to be a value that can
    often provide good results, so it’s a joke with a hint of reality about it.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我在[第2章](ch02.html#image-classification-with-pytorch)中介绍了训练神经网络的*学习率*的概念，提到它是你可以改变的最重要的超参数之一，然后又提到了你应该使用什么值，建议使用一个相对较小的数字，让你尝试不同的值。不过...坏消息是，很多人确实是这样发现他们架构的最佳学习率的，通常使用一种称为*网格搜索*的技术，通过穷举搜索一部分学习率值，将结果与验证数据集进行比较。这是非常耗时的，尽管有人这样做，但许多其他人更倾向于从实践者的传统中获得经验。例如，一个已经被观察到与Adam优化器一起工作的学习率值是3e-4。这被称为Karpathy的常数，以安德烈·卡帕西（目前是特斯拉AI主管）在2016年[发推文](https://oreil.ly/WLw3q)后得名。不幸的是，更少的人读到了他的下一条推文：“我只是想确保人们明白这是一个笑话。”有趣的是，3e-4往往是一个可以提供良好结果的值，所以这是一个带有现实意味的笑话。
- en: On the one hand, you have slow and cumbersome searching, and on the other, obscure
    and arcane knowledge gained from working on countless architectures until you
    get a *feel* for what a good learning rate would be—artisanal neural networks,
    even. Is there a better way than these two extremes?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，你可以进行缓慢而繁琐的搜索，另一方面，通过在无数架构上工作直到对一个好的学习率有了*感觉*来获得的晦涩和神秘的知识——甚至可以说是手工制作的神经网络。除了这两个极端，还有更好的方法吗？
- en: 'Thankfully, the answer is yes, although you’ll be surprised by how many people
    don’t use this better method. A somewhat obscure paper by Leslie Smith, a research
    scientist at the US Naval Research Laboratory, contained an approach for finding
    an appropriate learning rate.^([1](ch04.html#idm45762364996360)) But it wasn’t
    until Jeremy Howard brought the technique to the fore in his fast.ai course that
    it started to catch on in the deep learning community. The idea is quite simple:
    over the course of an epoch, start out with a small learning rate and increase
    to a higher learning rate over each mini-batch, resulting in a high rate at the
    end of the epoch. Calculate the loss for each rate and then, looking at a plot,
    pick the learning rate that gives the greatest decline. For example, look at the
    graph in [Figure 4-1](#learning-rate-loss-graph).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，答案是肯定的，尽管你会对有多少人没有使用这种更好的方法感到惊讶。美国海军研究实验室的研究科学家莱斯利·史密斯撰写的一篇有些晦涩的论文包含了一种寻找适当学习率的方法。但直到杰里米·霍华德在他的fast.ai课程中将这种技术推广开来，深度学习社区才开始关注。这个想法非常简单：在一个epoch的过程中，从一个小的学习率开始，逐渐增加到一个更高的学习率，每个小批次结束时都会有一个较高的学习率。计算每个速率的损失，然后查看绘图，选择使下降最大的学习率。例如，查看[图4-1](#learning-rate-loss-graph)中的图表。
- en: '![Learning rate loss graph](assets/ppdl_0401.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![学习率损失图](assets/ppdl_0401.png)'
- en: Figure 4-1\. Learning rate against loss
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1。学习率与损失
- en: In this case, we should look at using a learning rate of around 1e-2 (marked
    within the circle), as that is roughly the point where the gradient of the descent
    is steepest.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们应该考虑使用大约1e-2的学习率（在圆圈内标记），因为这大致是梯度下降最陡峭的点。
- en: Note
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that you’re not looking for the bottom of the curve, which might be the
    more intuitive place; you’re looking for the point that is getting to the bottom
    the fastest.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你不是在寻找曲线的底部，这可能是更直观的地方；你要找的是最快到达底部的点。
- en: 'Here’s a simplified version of what the fast.ai library does under the covers:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是fast.ai库在幕后执行的简化版本：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'What’s going on here is that we iterate through the batches, training almost
    as usual; we pass our inputs through the model and then we get the loss from that
    batch. We record what our `best_loss` is so far, and compare the new loss against
    it. If our new loss is more than four times the `best_loss`, we crash out of the
    function, returning what we have so far (as the loss is probably tending to infinity).
    Otherwise, we keep appending the loss and logs of the current learning rate, and
    update the learning rate with the next step along the way to the maximal rate
    at the end of the loop. The plot can then be shown using the `matplotlib` `plt`
    function:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的情况是，我们遍历批次，几乎像往常一样训练；我们通过模型传递我们的输入，然后从该批次获取损失。我们记录到目前为止的`best_loss`是多少，并将新的损失与其进行比较。如果我们的新损失是`best_loss`的四倍以上，我们就会退出函数，返回到目前为止的内容（因为损失可能趋向无穷大）。否则，我们会继续附加当前学习率的损失和日志，并在循环结束时更新学习率到最大速率的下一步。然后可以使用`matplotlib`的`plt`函数显示绘图：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that we return slices of the `lr` logs and losses. We do that simply because
    the first bits of training and the last few (especially if the learning rate becomes
    very large quite quickly) tend not to tell us much information.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们返回`lr`日志和损失的切片。我们这样做只是因为训练的最初部分和最后几部分（特别是如果学习率变得非常快地变大）往往不会告诉我们太多信息。
- en: The implementation in fast.ai’s library also includes weighted smoothing, so
    you get smooth lines in your plot, whereas this snippet produces spiky output.
    Finally, remember that because this function does actually train the model and
    messes with the optimizer’s learning rate settings, you should save and reload
    your model beforehand to get back to the state it was in before you called `find_lr()`
    and also reinitialize the optimizer you’ve chosen, which you can do now, passing
    in the learning rate you’ve determined from looking at the graph!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: fast.ai库中的实现还包括加权平滑，因此您在绘图中会得到平滑的线条，而此代码段会产生尖锐的输出。最后，请记住，因为这个函数实际上确实训练了模型并干扰了优化器的学习率设置，所以您应该在调用`find_lr()`之前保存和重新加载您的模型，以恢复到调用该函数之前的状态，并重新初始化您选择的优化器，您现在可以这样做，传入您从图表中确定的学习率！
- en: That gets us a good value for our learning rate, but we can do even better with
    *differential learning rates*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个良好的学习率值，但我们可以通过*差异学习率*做得更好。
- en: Differential Learning Rates
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 差异学习率
- en: 'In our training so far, we have applied one learning rate to the entire model.
    When training a model from scratch, that probably makes sense, but when it comes
    to transfer learning, we can normally get a little better accuracy if we try something
    different: training different groups of layers at different rates. Earlier in
    the chapter, we froze all the pretrained layers in our model and trained just
    our new classifier, but we may want to fine-tune some of the layers of, say, the
    ResNet model we’re using. Perhaps adding some training to the layers just preceding
    our classifier will make our model just a little more accurate. But as those preceding
    layers have already been trained on the ImageNet dataset, maybe they need only
    a little bit of training as compared to our newer layers? PyTorch offers a simple
    way of making this happen. Let’s modify our optimizer for the ResNet-50 model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对整个模型应用了一个学习率。从头开始训练模型时，这可能是有道理的，但是在迁移学习时，如果我们尝试一些不同的东西，通常可以获得更好的准确性：以不同速率训练不同组层。在本章的前面，我们冻结了模型中的所有预训练层，并只训练了我们的新分类器，但是我们可能想要微调我们正在使用的ResNet模型的一些层。也许给我们的分类器之前的层添加一些训练会使我们的模型更准确一点。但是由于这些前面的层已经在ImageNet数据集上进行了训练，也许与我们的新层相比，它们只需要一点点训练？PyTorch提供了一种简单的方法来实现这一点。让我们修改ResNet-50模型的优化器：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'That sets the learning rate for `layer4` (just before our classifier) to a
    third of the *found* learning rate and a ninth for `layer3`. That combination
    has empirically worked out quite well in my work, but obviously feel free to experiment.
    There’s one more thing, though. As you may remember from the beginning of this
    chapter, we *froze* all these pretrained layers. It’s all very well to give them
    a different learning rate, but as of right now, the model training won’t touch
    them at all because they don’t accumulate gradients. Let’s change that:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把`layer4`（就在我们的分类器之前）的学习率设置为*找到的*学习率的三分之一，`layer3`的学习率的九分之一。这种组合在我的工作中经验上表现得非常好，但显然您可以随意尝试。不过还有一件事。正如您可能还记得本章开头所说的，我们*冻结*了所有这些预训练层。给它们一个不同的学习率是很好的，但是目前，模型训练不会触及它们，因为它们不会累积梯度。让我们改变这一点：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that the parameters in these layers take gradients once more, the differential
    learning rates will be applied when you fine-tine the model. Note that you can
    freeze and unfreeze parts of the model at will and do further fine-tuning on every
    layer separately if you’d like!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些层的参数再次接受梯度，当您微调模型时将应用差异学习率。请注意，您可以随意冻结和解冻模型的部分，并对每个层进行进一步的微调，如果您愿意的话！
- en: 'Now that we’ve looked at the learning rates, let’s investigate a different
    aspect of training our models: the data that we feed into them.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过学习率了，让我们来研究训练模型的另一个方面：我们输入的数据。
- en: Data Augmentation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: One of the dreaded phrases in data science is, *Oh no, my model has overfit
    on the data!* As I mentioned in [Chapter 2](ch02.html#image-classification-with-pytorch),
    *overfitting* occurs when the model decides to reflect the data presented in the
    training set rather than produce a generalized solution. You’ll often hear people
    talking about how a particular model *memorized the dataset*, meaning the model
    learned the answers and went on to perform poorly on production data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中令人恐惧的短语之一是，“哦，不，我的模型在数据上过拟合了！”正如我在[第2章](ch02.html#image-classification-with-pytorch)中提到的，*过拟合*发生在模型决定反映训练集中呈现的数据而不是产生一个泛化解决方案时。你经常会听到人们谈论特定模型*记住了数据集*，意味着模型学习了答案，然后在生产数据上表现不佳。
- en: The traditional way of guarding against this is to amass large quantities of
    data. By seeing more data, the model gets a more general idea of the problem it
    is trying to solve. If you view the situation as a compression problem, then if
    you prevent the model from simply being able to store all the answers (by overwhelming
    its storage capacity with so much data), it’s forced to *compress* the input and
    therefore produce a solution that cannot simply be storing the answers within
    itself. This is fine, and works well, but say we have only a thousand images and
    we’re doing transfer learning. What can we do?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的防范方法是积累大量数据。通过观察更多数据，模型对它试图解决的问题有一个更一般的概念。如果你把这种情况看作是一个压缩问题，那么如果你阻止模型简单地能够存储所有答案（通过用大量数据压倒性地超出其存储容量），它被迫*压缩*输入，因此产生一个不能简单地在自身内部存储答案的解决方案。这是可以的，而且效果很好，但是假设我们只有一千张图片，我们正在进行迁移学习。我们能做什么呢？
- en: One approach that we can use is *data augmentation*. If we have an image, we
    can do a number of things to that image that should prevent overfitting and make
    the model more general. Consider the images of Helvetica the cat in Figures [4-2](#normal-cat-in-box)
    and [4-3](#flipped-cat-in-box).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用的一种方法是*数据增强*。如果我们有一张图像，我们可以对该图像做一些事情，应该可以防止过拟合，并使模型更加通用。考虑图4-2(#normal-cat-in-box)和图4-3(#flipped-cat-in-box)中的Helvetica猫的图像。
- en: '![Cat In A Box](assets/ppdl_0402.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![盒子里的猫](assets/ppdl_0402.png)'
- en: Figure 4-2\. Our original image
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. 我们的原始图像
- en: '![Flipped Cat In A Box](assets/ppdl_0403.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![翻转的盒子里的猫](assets/ppdl_0403.png)'
- en: Figure 4-3\. A flipped Helvetica
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 翻转的Helvetica
- en: 'Obviously to us, they’re the same image. The second one is just a mirrored
    copy of the first. The tensor representation is going to be different, as the
    RGB values will be in different places in the 3D image. But it’s still a cat,
    so the model training on this image will hopefully learn to recognize a cat shape
    on the left or right side of the frame, rather than simply associating the entire
    image with *cat*. Doing this in PyTorch is simple. You may remember this snippet
    of code from [Chapter 2](ch02.html#image-classification-with-pytorch):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 显然对我们来说，它们是相同的图像。第二个只是第一个的镜像副本。张量表示将会不同，因为RGB值将在3D图像中的不同位置。但它仍然是一只猫，所以训练在这张图像上的模型希望能够学会识别左侧或右侧帧上的猫形状，而不仅仅是将整个图像与*猫*关联起来。在PyTorch中做到这一点很简单。你可能还记得这段代码片段来自[第2章](ch02.html#image-classification-with-pytorch)：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This forms a transformation pipeline that all images go through as they enter
    the model for training. But the `torchivision.transforms` library contains many
    other transformation functions that can be used to augment training data. Let’s
    have a look at some of the more useful ones and see what happens to Helvetica
    with some of the less obvious transforms as well.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这形成了一个转换管道，所有图像在进入模型进行训练时都会经过。但是`torchivision.transforms`库包含许多其他可以用于增强训练数据的转换函数。让我们看一下一些更有用的转换，并查看Helvetica在一些不太明显的转换中会发生什么。
- en: Torchvision Transforms
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Torchvision转换
- en: '`torchvision` comes complete with a large collection of potential transforms
    that can be used for data augmentation, plus two ways of constructing new transformations.
    In this section, we look at the most useful ones that come supplied as well as
    a couple of custom transformations that you can use in your own applications.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision`包含了一个大量的潜在转换集合，可以用于数据增强，以及构建新转换的两种方式。在本节中，我们将看一下提供的最有用的转换，以及一些你可以在自己的应用中使用的自定义转换。'
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`ColorJitter` randomly changes the brightness, contrast, saturation, and hue
    of an image. For brightness, contrast, and saturation, you can supply either a
    float or a tuple of floats, all nonnegative in the range 0 to 1, and the randomness
    will either be between 0 and the supplied float or it will use the tuple to generate
    randomness between the supplied pair of floats. For hue, a float or float tuple
    between –0.5 and 0.5 is required, and it will generate random hue adjustments
    between [-*hue*,*hue*] or [*min*, *max*]. See [Figure 4-4](#colorjitter) for an
    example.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColorJitter`会随机改变图像的亮度、对比度、饱和度和色调。对于亮度、对比度和饱和度，你可以提供一个浮点数或一个浮点数元组，所有非负数在0到1的范围内，随机性将在0和提供的浮点数之间，或者它将使用元组生成在提供的一对浮点数之间的随机性。对于色调，需要一个在-0.5到0.5之间的浮点数或浮点数元组，它将在[-*hue*,*hue*]或[*min*,
    *max*]之间生成随机色调调整。参见[图4-4](#colorjitter)作为示例。'
- en: '![ColorJitter applied at 0.5 for all parameters](assets/ppdl_0404.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![ColorJitter应用于所有参数为0.5](assets/ppdl_0404.png)'
- en: Figure 4-4\. ColorJitter applied at 0.5 for all parameters
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. ColorJitter应用于所有参数为0.5
- en: 'If you want to flip your image, these two transforms randomly reflect an image
    on either the horizontal or vertical axis:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想翻转你的图像，这两个转换会随机地在水平或垂直轴上反射图像：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Either supply a float from 0 to 1 for the probability of the reflection to occur
    or accept the default of a 50% chance of reflection. A vertically flipped cat
    is shown in [Figure 4-5](#randomverticalflip).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要么提供一个从0到1的概率来发生反射，要么接受默认的50%反射几率。在[图4-5](#randomverticalflip)中展示了一个垂直翻转的猫。
- en: '![RandomVerticalFlip](assets/ppdl_0405.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![RandomVerticalFlip](assets/ppdl_0405.png)'
- en: Figure 4-5\. Vertical flip
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5\. 垂直翻转
- en: '`RandomGrayscale` is a similar type of transformation, except that it randomly
    turns the image grayscale, depending on the parameter *p* (the default is 10%):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomGrayscale`是一种类似的转换类型，不同之处在于它会随机将图像变为灰度，取决于参数*p*（默认为10%）：'
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`RandomCrop` and `RandomResizeCrop`, as you might expect, perform random crops
    on the image of `size`, which can either be an int for height and width, or a
    tuple containing different heights and widths. [Figure 4-6](#randomcrop) shows
    an example of a `RandomCrop` in action.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomCrop`和`RandomResizeCrop`，正如你所期望的那样，在图像上执行随机裁剪，`size`可以是一个整数，表示高度和宽度，或包含不同高度和宽度的元组。[图4-6](#randomcrop)展示了`RandomCrop`的示例。'
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you need to be a little careful here, because if your crops are too small,
    you run the risk of cutting out important parts of the image and making the model
    train on the wrong thing. For instance, if a cat is playing on a table in an image,
    and the crop takes out the cat and just leaves part of the table to be classified
    as *cat*, that’s not great. While the `RandomResizeCrop` will resize the crop
    to fill the given size, `RandomCrop` may take a crop close to the edge and into
    the darkness beyond the image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在这里需要小心一点，因为如果您的裁剪区域太小，就有可能剪掉图像的重要部分，使模型训练错误的内容。例如，如果图像中有一只猫在桌子上玩耍，而裁剪掉了猫，只留下部分桌子被分类为*猫*，那就不太好。虽然`RandomResizeCrop`会调整裁剪大小以填充给定大小，但`RandomCrop`可能会取得靠近边缘并进入图像之外黑暗区域的裁剪。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`RandomResizeCrop` is using Bilinear interpolation, but you can also select
    nearest neighbor or bicubic interpolation by changing the `interpolation` parameter.
    See the [PIL filters page](https://oreil.ly/rNOtN) for further details.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomResizeCrop`使用双线性插值，但您也可以通过更改`interpolation`参数选择最近邻或双三次插值。有关更多详细信息，请参阅[PIL滤镜页面](https://oreil.ly/rNOtN)。'
- en: As you saw in [Chapter 3](ch03.html#convolutional-neural-networks), we can add
    padding to maintain the required size of the image. By default, this is `constant`
    padding, which fills out the otherwise empty pixels beyond the image with the
    value given in `fill`. However, I recommend that you use the `reflect` padding
    instead, as empirically it seems to work a little better than just throwing in
    empty constant space.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第3章](ch03.html#convolutional-neural-networks)中看到的，我们可以添加填充以保持图像所需的大小。默认情况下，这是`constant`填充，它用`fill`中给定的值填充图像之外的空白像素。然而，我建议您改用`reflect`填充，因为经验上它似乎比只是填充空白常数空间要好一点。
- en: '![RandomCrop with size=100](assets/ppdl_0406.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![尺寸为100的RandomCrop](assets/ppdl_0406.png)'
- en: Figure 4-6\. RandomCrop with size=100
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6。尺寸为100的RandomCrop
- en: 'If you’d like to randomly rotate an image, `RandomRotation` will vary between
    `[-degrees, degrees]` if `degrees` is a single float or int, or `(min,max)` if
    it is a tuple:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要随机旋转图像，`RandomRotation`将在`[-degrees, degrees]`之间变化，如果`degrees`是一个单个浮点数或整数，或者在元组中是`(min,max)`：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If `expand` is set to `True`, this function will expand the output image so
    that it can include the entire rotation; by default, it’s set to crop to within
    the input dimensions. You can specify a PIL resampling filter, and optionally
    provide an `(x,y)` tuple for the center of rotation; otherwise the transform will
    rotate about the center of the image. [Figure 4-7](#randomrotation) is a `RandomRotation`
    transformation with `degrees` set to 45.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`expand`设置为`True`，此函数将扩展输出图像，以便包含整个旋转；默认情况下，它设置为在输入尺寸内裁剪。您可以指定PIL重采样滤镜，并可选择提供一个`(x,y)`元组作为旋转中心；否则，变换将围绕图像中心旋转。[图4-7](#randomrotation)是一个`RandomRotation`变换，其中`degrees`设置为45。
- en: '![RandomRotation with degrees=45](assets/ppdl_0407.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![旋转角度为45度的RandomRotation](assets/ppdl_0407.png)'
- en: Figure 4-7\. RandomRotation with degrees = 45
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7。旋转角度为45度的RandomRotation
- en: '`Pad` is a general-purpose padding transform that adds padding (extra height
    and width) onto the borders of an image:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pad`是一个通用的填充变换，它在图像的边界上添加填充（额外的高度和宽度）：'
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: A single value in `padding` will apply padding on that length in all directions.
    A two-tuple `padding` will produce padding in the length of (left/right, top/bottom),
    and a four-tuple will produce padding for (left, top, right, bottom). By default,
    padding is set to `constant` mode, which copies the value of `fill` into the padding
    slots. The other choices are `edge`, which pads the last values of the edge of
    the image into the padding length; `reflect`, which reflects the values of the
    image (except the edge) into the border; and `symmetric`, which is `reflection`
    but includes the last value of the image at the edge. [Figure 4-8](#padding) shows
    `padding` set to 25 and `padding_mode` set to `reflect`. See how the box repeats
    at the edges.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`padding`中的单个值将在所有方向上应用填充。两元组`padding`将在长度为（左/右，上/下）的方向上产生填充，四元组将在（左，上，右，下）的方向上产生填充。默认情况下，填充设置为`constant`模式，它将`fill`的值复制到填充槽中。其他选择是`edge`，它将图像边缘的最后值填充到填充长度；`reflect`，它将图像的值（除边缘外）反射到边界；以及`symmetric`，它是`reflection`，但包括图像边缘的最后值。[图4-8](#padding)展示了`padding`设置为25和`padding_mode`设置为`reflect`。看看盒子如何在边缘重复。'
- en: '![Pad with padding=25 and padding_mode=reflect](assets/ppdl_0408.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![填充为25和填充模式为reflect的Pad](assets/ppdl_0408.png)'
- en: Figure 4-8\. Pad with padding = 25 and padding_mode = reflect
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8。使用填充为25和填充模式为reflect的填充
- en: '`RandomAffine` allows you to specify random affine translations of the image
    (scaling, rotations, translations, and/or shearing, or any combination). [Figure 4-9](#randomaffine)
    shows an example of an affine transformation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomAffine`允许您指定图像的随机仿射变换（缩放、旋转、平移和/或剪切，或任何组合）。[图4-9](#randomaffine)展示了仿射变换的一个示例。'
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![RandomAffine with degrees=10 and shear=50](assets/ppdl_0409.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![旋转角度为10度和剪切为50的RandomAffine](assets/ppdl_0409.png)'
- en: Figure 4-9\. RandomAffine with degrees = 10 and shear = 50
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9。旋转角度为10度，剪切为50的RandomAffine
- en: The `degrees` parameter is either a single float or int or a tuple. In single
    form, it produces random rotations between (–`*degrees*`, `*degrees*`). With a
    tuple, it will produce random rotations between (`*min*`, `*max*`). `degrees`
    has to be explicitly set to prevent rotations from occurring—there’s no default
    setting. `translate` is a tuple of two multipliers (`*horizontal_multipler*`,
    `*vertical_multiplier*`). At transform time, a horizontal shift, `dx`, is sampled
    in the range –`*image_width × horizontal_multiplier < dx < img_width × horizontal_width*`,
    and a vertical shift is sampled in the same way with respect to the image height
    and the vertical multiplier.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`degrees`参数可以是单个浮点数或整数，也可以是一个元组。以单个形式，它会产生在（–`*degrees*`，`*degrees*`）之间的随机旋转。使用元组时，它会产生在（`*min*`，`*max*`）之间的随机旋转。必须明确设置`degrees`以防止旋转发生——没有默认设置。`translate`是一个包含两个乘数（`*horizontal_multipler*`，`*vertical_multiplier*`）的元组。在变换时，水平偏移`dx`在范围内取样，即（–`*image_width
    × horizontal_multiplier < dx < img_width × horizontal_width*`），垂直偏移也以相同的方式相对于图像高度和垂直乘数进行取样。'
- en: Scaling is handled by another tuple, (`*min*`, `*max*`), and a uniform scaling
    factor is randomly sampled from those. Shearing can be either a single float/int
    or a tuple, and randomly samples in the same manner as the `degrees` parameter.
    Finally, `resample` allows you to optionally provide a PIL resampling filter,
    and `fillcolor` is an optional int specifying a fill color for areas inside the
    final image that lie outside the final transform.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放由另一个元组（`*min*`，`*max*`）处理，从中随机抽取一个均匀缩放因子。剪切可以是单个浮点数/整数或一个元组，并以与`degrees`参数相同的方式随机取样。最后，`resample`允许您可选地提供一个PIL重采样滤波器，`fillcolor`是一个可选的整数，指定最终图像中位于最终变换之外的区域的填充颜色。
- en: As for what transforms you should use in a data augmentation pipeline, I definitely
    recommend using the various random flips, color jittering, rotation, and crops
    to start.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 至于在数据增强流水线中应该使用哪些转换，我强烈建议开始使用各种随机翻转、颜色抖动、旋转和裁剪。
- en: Other transformations are available in `torchvision`; check the [documentation](https://oreil.ly/b0Q0A)
    for more details. But of course you may find yourself wanting to create a transformation
    that is particular to your data domain that isn’t included by default, so PyTorch
    provides various ways of defining custom transformations, as you’ll see next.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision`中还提供其他转换；查看[文档](https://oreil.ly/b0Q0A)以获取更多详细信息。但当然，您可能会发现自己想要创建一个特定于您的数据领域的转换，而这并不是默认包含的，因此PyTorch提供了各种定义自定义转换的方式，接下来您将看到。'
- en: Color Spaces and Lambda Transforms
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 颜色空间和Lambda转换
- en: This may seem a little odd to even bring up, but so far all our image work has
    been in the fairly standard 24-bit RGB color space, where every pixel has an 8-bit
    red, green, and blue value to indicate the color of that pixel. However, other
    color spaces are available!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 即使提到这似乎有点奇怪，但到目前为止，我们所有的图像工作都是在相当标准的24位RGB颜色空间中进行的，其中每个像素都有一个8位的红色、绿色和蓝色值来指示该像素的颜色。然而，其他颜色空间也是可用的！
- en: A popular alternative is HSV, which has three 8-bit values for *hue*, *saturation*,
    and *value*. Some people feel this system more accurately models human vision
    than the traditional RGB color space. But why does this matter? A mountain in
    RGB is a mountain in HSV, right?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: HSV是一种受欢迎的替代方案，它具有三个8位值，分别用于*色调*、*饱和度*和*值*。一些人认为这种系统比传统的RGB颜色空间更准确地模拟了人类视觉。但为什么这很重要呢？在RGB中的一座山在HSV中也是一座山，对吧？
- en: Well, there’s some evidence from recent deep learning work in colorization that
    other color spaces can produce slightly higher accuracy than RGB. A mountain may
    be a mountain, but the tensor that gets formed in each space’s representation
    will be different, and one space may capture something about your data better
    than another.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在着色方面的深度学习工作中有一些证据表明，其他颜色空间可能比RGB产生稍微更高的准确性。一座山可能是一座山，但在每个空间的表示中形成的张量将是不同的，一个空间可能比另一个更好地捕捉到您的数据的某些特征。
- en: When combined with ensembles, you could easily create a series of models that
    combines the results of training on RGB, HSV, YUV, and LAB color spaces to wring
    out a few more percentage points of accuracy from your prediction pipeline.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与集成结合使用时，您可以轻松地创建一系列模型，将RGB、HSV、YUV和LAB颜色空间的训练结果结合起来，从而从您的预测流水线中挤出更多的准确性百分点。
- en: 'One slight problem is that PyTorch doesn’t offer a transform that can do this.
    But it does provide a couple of tools that we can use to randomly change an image
    from standard RGB into HSV (or another color space). First, if we look in the
    PIL documentation, we see that we can use `Image.convert()` to translate a PIL
    image from one color space to another. We could write a custom `transform` class
    to carry out this conversion, but PyTorch adds a `transforms.Lambda` class so
    that we can easily wrap any function and make it available to the transform pipeline.
    Here’s our custom function:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小问题是PyTorch没有提供可以执行此操作的转换。但它提供了一些工具，我们可以使用这些工具将标准RGB图像随机转换为HSV（或其他颜色空间）。首先，如果我们查看PIL文档，我们会发现可以使用`Image.convert()`将PIL图像从一种颜色空间转换为另一种。我们可以编写一个自定义的`transform`类来执行这种转换，但PyTorch添加了一个`transforms.Lambda`类，以便我们可以轻松地包装任何函数并使其可用于转换流水线。这是我们的自定义函数：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is then wrapped in a `transforms.Lambda` class and can be used in any
    standard transformation pipeline like we’ve seen before:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其包装在`transforms.Lambda`类中，并可以在任何标准转换流水线中使用，就像我们以前看到的那样：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'That’s fine if we want to convert *every* image into HSV, but really we don’t
    want that. We’d like it to randomly change images in each batch, so it’s probable
    that the image will be presented in different color spaces in different epochs.
    We could update our original function to generate a random number and use that
    to generate a random probability of changing the image, but instead we’re even
    lazier and use `RandomApply`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将*每张*图像都转换为HSV，那倒没什么问题，但实际上我们并不想这样。我们希望它在每个批次中随机更改图像，因此很可能图像在不同的时期以不同的颜色空间呈现。我们可以更新我们的原始函数以生成一个随机数，并使用该随机数生成更改图像的随机概率，但相反，我们更懒惰，使用`RandomApply`：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: By default, `RandomApply` fills in a parameter `p` with a value of `0.5`, so
    there’s a 50/50 chance of the transform being applied. Experiment with adding
    more color spaces and the probability of applying the transformation to see what
    effect it has on our cat and fish problem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`RandomApply`会用值`0.5`填充参数`p`，所以转换被应用的概率是50/50。尝试添加更多的颜色空间和应用转换的概率，看看它对我们的猫和鱼问题有什么影响。
- en: Let’s look at another custom transform that is a little more complicated.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个稍微复杂一些的自定义转换。
- en: Custom Transform Classes
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义转换类
- en: 'Sometimes a simple lambda isn’t enough; maybe we have some initialization or
    state that we want to keep track of, for example. In these cases, we can create
    a custom transform that operates on either PIL image data or a tensor. Such a
    class has to implement two methods: `__call__`, which the transform pipeline will
    invoke during the transformation process; and `__repr__`, which should return
    a string representation of the transform, along with any state that may be useful
    for diagnostic purposes.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有时一个简单的lambda不够；也许我们有一些初始化或状态要跟踪，例如。在这些情况下，我们可以创建一个自定义转换，它可以操作PIL图像数据或张量。这样的类必须实现两个方法：`__call__`，转换管道在转换过程中将调用该方法；和`__repr__`，它应该返回一个字符串表示转换，以及可能对诊断目的有用的任何状态。
- en: 'In the following code, we implement a transform class that adds random Gaussian
    noise to a tensor. When the class is initialized, we pass in the mean and standard
    distribution of the noise we require, and during the `__call__` method, we sample
    from this distribution and add it to the incoming tensor:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们实现了一个转换类，它向张量添加随机高斯噪声。当类被初始化时，我们传入所需噪声的均值和标准分布，在`__call__`方法中，我们从这个分布中采样并将其添加到传入的张量中：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If we add this to a pipeline, we can see the results of the `__repr__` method
    being called:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个添加到管道中，我们可以看到`__repr__`方法被调用的结果：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Because transforms don’t have any restrictions and just inherit from the base
    Python object class, you can do anything. Want to completely replace an image
    at runtime with something from Google image search? Run the image through a completely
    different neural network and pass that result down the pipeline? Apply a series
    of image transforms that turn the image into a crazed reflective shadow of its
    former self? All possible, if not entirely recommended. Although it would be interesting
    to see whether Photoshop’s *Twirl* transformation effect would make accuracy worse
    or better! Why not give it a go?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因为转换没有任何限制，只是继承自基本的Python对象类，你可以做任何事情。想在运行时完全用来自Google图像搜索的东西替换图像？通过完全不同的神经网络运行图像并将结果传递到管道中？应用一系列图像转换，将图像变成其以前的疯狂反射阴影？所有这些都是可能的，尽管不完全推荐。尽管看到Photoshop的*Twirl*变换效果会使准确性变得更糟还是更好会很有趣！为什么不试试呢？
- en: Aside from transformations, there are a few more ways of squeezing as much performance
    from a model as possible. Let’s look at more examples.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 除了转换，还有一些其他方法可以尽可能地从模型中挤出更多性能。让我们看更多例子。
- en: Start Small and Get Bigger!
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从小开始，变得更大！
- en: 'Here’s a tip that seems odd, but obtains real results: start small and get
    bigger. What I mean is if you’re training on 256 × 256 images, create a few more
    datasets in which the images have been scaled to 64 × 64 and 128 × 128\. Create
    your model with the 64 × 64 dataset, fine-tune as normal, and then train the *exact
    same model* with the 128 × 128 dataset. Not from scratch, but using the parameters
    that have already been trained. Once it looks like you’ve squeezed the most out
    of the 128 × 128 data, move on to your target 256 × 256 data. You’ll probably
    find a percentage point or two improvement in accuracy.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个看起来奇怪但确实能获得真实结果的提示：从小开始，变得更大。我的意思是，如果你在256×256图像上训练，创建几个更多的数据集，其中图像已经缩放到64×64和128×128。使用64×64数据集创建你的模型，像平常一样微调，然后使用完全相同的模型在128×128数据集上训练。不是从头开始，而是使用已经训练过的参数。一旦看起来你已经从128×128数据中挤出了最大的价值，转移到目标256×256数据。你可能会发现准确性提高了一个或两个百分点。
- en: While we don’t know exactly why this works, the working theory is that by training
    at the lower resolutions, the model learns about the overall structure of the
    image and can refine that knowledge as the incoming images expand. But that’s
    just a theory. However, that doesn’t stop it from being a good little trick to
    have up your sleeve when you need to squeeze every last bit of performance from
    a model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不知道为什么这样做有效，但工作理论是通过在较低分辨率训练，模型学习图像的整体结构，并随着传入图像的扩展来完善这些知识。但这只是一个理论。然而，这并不能阻止它成为一个很好的小技巧，当你需要从模型中挤出每一点性能时。
- en: 'If you don’t want to have multiple copies of a dataset hanging around in storage,
    you can use `torchvision` transforms to do this on the fly using the `Resize`
    function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想在存储中留下数据集的多个副本，你可以使用`torchvision`转换来使用`Resize`函数实时进行操作：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The penalty you pay here is that you end up spending more time in training,
    as PyTorch has to apply the resize every time. If you resized all the images beforehand,
    you’d likely get a quicker training run, at the expense of filling up your hard
    drive. But isn’t that trade-off always the way?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你要付出的代价是你最终会花更多时间在训练上，因为PyTorch必须每次应用调整大小。如果你事先调整了所有图像，你可能会得到更快的训练运行，但这会填满你的硬盘。但这种权衡难道不是一直存在的吗？
- en: The concept of starting small and then getting bigger also applies to architectures.
    Using a ResNet architecture like ResNet-18 or ResNet-34 to test out approaches
    to transforms and get a feel for how training is working provides a much tighter
    feedback loop than if you start out using a ResNet-101 or ResNet-152 model. Start
    small, build upward, and you can potentially reuse the smaller model runs at prediction
    time by adding them to an ensemble model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从小开始然后变得更大的概念也适用于架构。使用像ResNet-18或ResNet-34这样的ResNet架构来测试转换方法并了解训练的工作方式，比起一开始就使用ResNet-101或ResNet-152模型，提供了一个更紧密的反馈循环。从小开始，逐步建立，你可以在预测时通过将它们添加到集成模型中来潜在地重复使用较小的模型运行。
- en: Ensembles
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成
- en: What’s better than one model making predictions? Well, how about a bunch of
    them? *Ensembling* is a technique that is fairly common in more traditional machine
    learning methods, and it works rather well in deep learning too. The idea is to
    obtain a prediction from a series of models, and combine those predictions to
    produce a final answer. Because different models will have different strengths
    in different areas, hopefully a combination of all their predictions will produce
    a more accurate result than one model alone.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有什么比一个模型做出预测更好？那么，多个模型怎么样？*集成*是一种在传统机器学习方法中相当常见的技术，在深度学习中也非常有效。其思想是从一系列模型中获得预测，并将这些预测组合起来产生最终答案。由于不同模型在不同领域具有不同的优势，希望所有预测的组合将产生比单个模型更准确的结果。
- en: 'There are plenty of approaches to ensembles, and we won’t go into all of them
    here. Instead, here’s a simple way of getting started with ensembles, one that
    has eeked out another 1% of accuracy in my experience; simply average the predictions:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多集成方法，我们不会在这里详细介绍所有方法。相反，这里有一种简单的集成方法，可以在我的经验中再增加1%的准确率；简单地平均预测结果：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `stack` method concatenates the array of tensors together, so if we were
    working on the cat/fish problem and had four models in our ensemble, we’d end
    up with a 4 × 2 tensor constructed from the four 1 × 2 tensors. And `mean` does
    what you’d expect, taking the average, although we have to pass in a dimension
    of 0 to ensure that it takes an average across the first dimension instead of
    simply adding up all the tensor elements and producing a scalar output. Finally,
    `argmax` picks out the tensor index with the highest element, as you’ve seen before.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`stack`方法将张量数组连接在一起，因此，如果我们在猫/鱼问题上工作，并且在我们的集成中有四个模型，我们将得到一个由四个1×2张量构成的4×2张量。`mean`执行您所期望的操作，取平均值，尽管我们必须传入维度0以确保它在第一维上取平均值，而不仅仅是将所有张量元素相加并产生标量输出。最后，`argmax`选择具有最高元素的张量索引，就像您以前看到的那样。'
- en: It’s easy to imagine more complex approaches. Perhaps weights could be added
    to each individual model’s prediction, and those weights adjusted if a model gets
    an answer right or wrong. What models should you use? I’ve found that a combination
    of ResNets (e.g., 34, 50, 101) work quite well, and there’s nothing to stop you
    from saving your model regularly and using different snapshots of the model across
    time in your ensemble!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易想象更复杂的方法。也许可以为每个单独模型的预测结果添加权重，并且如果模型回答正确或错误，则调整这些权重。您应该使用哪些模型？我发现ResNets（例如34、50、101）的组合效果非常好，没有什么能阻止您定期保存模型并在集成中使用模型的不同快照！
- en: Conclusion
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As we come to the end of [Chapter 4](#transfer-learning-and-other-tricks), we’re
    leaving images behind to move on to text. Hopefully you not only understand how
    convolutional neural networks work on images, but also have a deep bag of tricks
    in hand, including transfer learning, learning rate finding, data augmentation,
    and ensembling, which you can bring to bear on your particular application domain.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束[第4章](#transfer-learning-and-other-tricks)时，我们将离开图像，转向文本。希望您不仅了解卷积神经网络在图像上的工作原理，还掌握了一系列技巧，包括迁移学习、学习率查找、数据增强和集成，这些技巧可以应用于您特定的应用领域。
- en: Further Reading
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: If you’re interested in learning more in the image realm, check out the [fast.ai](https://fast.ai)
    course by Jeremy Howard, Rachel Thomas, and Sylvain Gugger. This chapter’s learning
    rate finder is, as I mentioned, a simplified version of the one they use, but
    the course goes into further detail about many of the techniques in this chapter.
    The fast.ai library, built on PyTorch, allows you to bring them to bear on your
    image (and text!) domains easily.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对图像领域的更多信息感兴趣，请查看Jeremy Howard，Rachel Thomas和Sylvain Gugger的[fast.ai](https://fast.ai)课程。正如我所提到的，本章的学习率查找器是他们使用的简化版本，但该课程详细介绍了本章中许多技术。建立在PyTorch上的fast.ai库使您可以轻松将它们应用于您的图像（和文本！）领域。
- en: '[Torchvision documentation](https://oreil.ly/vNnST)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Torchvision文档](https://oreil.ly/vNnST)'
- en: '[PIL/Pillow documentation](https://oreil.ly/Jlisb)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PIL/Pillow文档](https://oreil.ly/Jlisb)'
- en: '[“Cyclical Learning Rates for Training Neural Networks”](https://arxiv.org/abs/1506.01186)
    by Leslie N. Smith (2015)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leslie N. Smith（2015）的[“用于训练神经网络的循环学习率”](https://arxiv.org/abs/1506.01186)
- en: '[“ColorNet: Investigating the Importance of Color Spaces for Image Classification”](https://arxiv.org/abs/1902.00267)
    by Shreyank N. Gowda and Chun Yuan (2019)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shreyank N. Gowda和Chun Yuan（2019）的[“ColorNet：研究颜色空间对图像分类的重要性”](https://arxiv.org/abs/1902.00267)
- en: ^([1](ch04.html#idm45762364996360-marker)) See [“Cyclical Learning Rates for
    Training Neural Networks”](https://arxiv.org/abs/1506.01186) by Leslie Smith (2015).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#idm45762364996360-marker)) 请参阅Leslie Smith（2015）的[“用于训练神经网络的循环学习率”](https://arxiv.org/abs/1506.01186)。
