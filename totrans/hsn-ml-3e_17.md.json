["```py\nimport pandas as pd\nfrom pathlib import Path\n\npath = Path(\"datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv\")\ndf = pd.read_csv(path, parse_dates=[\"service_date\"])\ndf.columns = [\"date\", \"day_type\", \"bus\", \"rail\", \"total\"]  # shorter names\ndf = df.sort_values(\"date\").set_index(\"date\")\ndf = df.drop(\"total\", axis=1)  # no need for total, it's just bus + rail\ndf = df.drop_duplicates()  # remove duplicated months (2011-10 and 2014-07)\n```", "```py\n>>> df.head()\n day_type     bus    rail\ndate\n2001-01-01        U  297192  126455\n2001-01-02        W  780827  501952\n2001-01-03        W  824923  536432\n2001-01-04        W  870021  550011\n2001-01-05        W  890426  557917\n```", "```py\nimport matplotlib.pyplot as plt\n\ndf[\"2019-03\":\"2019-05\"].plot(grid=True, marker=\".\", figsize=(8, 3.5))\nplt.show()\n```", "```py\ndiff_7 = df[[\"bus\", \"rail\"]].diff(7)[\"2019-03\":\"2019-05\"]\n\nfig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 5))\ndf.plot(ax=axs[0], legend=False, marker=\".\")  # original time series\ndf.shift(7).plot(ax=axs[0], grid=True, legend=False, linestyle=\":\")  # lagged\ndiff_7.plot(ax=axs[1], grid=True, marker=\".\")  # 7-day difference time series\nplt.show()\n```", "```py\n>>> list(df.loc[\"2019-05-25\":\"2019-05-27\"][\"day_type\"])\n['A', 'U', 'U']\n```", "```py\n>>> diff_7.abs().mean()\nbus     43915.608696\nrail    42143.271739\ndtype: float64\n```", "```py\n>>> targets = df[[\"bus\", \"rail\"]][\"2019-03\":\"2019-05\"]\n>>> (diff_7 / targets).abs().mean()\nbus     0.082938\nrail    0.089948\ndtype: float64\n```", "```py\nperiod = slice(\"2001\", \"2019\")\ndf_monthly = df.resample('M').mean()  # compute the mean for each month\nrolling_average_12_months = df_monthly[period].rolling(window=12).mean()\n\nfig, ax = plt.subplots(figsize=(8, 4))\ndf_monthly[period].plot(ax=ax, marker=\".\")\nrolling_average_12_months.plot(ax=ax, grid=True, legend=False)\nplt.show()\n```", "```py\ndf_monthly.diff(12)[period].plot(grid=True, marker=\".\", figsize=(8, 3))\nplt.show()\n```", "```py\nfrom statsmodels.tsa.arima.model import ARIMA\n\norigin, today = \"2019-01-01\", \"2019-05-31\"\nrail_series = df.loc[origin:today][\"rail\"].asfreq(\"D\")\nmodel = ARIMA(rail_series,\n              order=(1, 0, 0),\n              seasonal_order=(0, 1, 1, 7))\nmodel = model.fit()\ny_pred = model.forecast()  # returns 427,758.6\n```", "```py\norigin, start_date, end_date = \"2019-01-01\", \"2019-03-01\", \"2019-05-31\"\ntime_period = pd.date_range(start_date, end_date)\nrail_series = df.loc[origin:end_date][\"rail\"].asfreq(\"D\")\ny_preds = []\nfor today in time_period.shift(-1):\n    model = ARIMA(rail_series[origin:today],  # train on data up to \"today\"\n                  order=(1, 0, 0),\n                  seasonal_order=(0, 1, 1, 7))\n    model = model.fit()  # note that we retrain the model every day!\n    y_pred = model.forecast()[0]\n    y_preds.append(y_pred)\n\ny_preds = pd.Series(y_preds, index=time_period)\nmae = (y_preds - rail_series[time_period]).abs().mean()  # returns 32,040.7\n```", "```py\nimport tensorflow as tf\n\nmy_series = [0, 1, 2, 3, 4, 5]\nmy_dataset = tf.keras.utils.timeseries_dataset_from_array(\n    my_series,\n    targets=my_series[3:],  # the targets are 3 steps into the future\n    sequence_length=3,\n    batch_size=2\n)\n```", "```py\n>>> list(my_dataset)\n[(<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n array([[0, 1, 2],\n [1, 2, 3]], dtype=int32)>,\n <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>),\n (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[2, 3, 4]], dtype=int32)>,\n <tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>)]\n```", "```py\n>>> for window_dataset in tf.data.Dataset.range(6).window(4, shift=1):\n...     for element in window_dataset:\n...         print(f\"{element}\", end=\" \")\n...     print()\n...\n0 1 2 3\n1 2 3 4\n2 3 4 5\n3 4 5\n4 5\n5\n```", "```py\n>>> dataset = tf.data.Dataset.range(6).window(4, shift=1, drop_remainder=True)\n>>> dataset = dataset.flat_map(lambda window_dataset: window_dataset.batch(4))\n>>> for window_tensor in dataset:\n...     print(f\"{window_tensor}\")\n...\n[0 1 2 3]\n[1 2 3 4]\n[2 3 4 5]\n```", "```py\ndef to_windows(dataset, length):\n    dataset = dataset.window(length, shift=1, drop_remainder=True)\n    return dataset.flat_map(lambda window_ds: window_ds.batch(length))\n```", "```py\n>>> dataset = to_windows(tf.data.Dataset.range(6), 4)  # 3 inputs + 1 target = 4\n>>> dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n>>> list(dataset.batch(2))\n[(<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n array([[0, 1, 2],\n [1, 2, 3]])>,\n <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 4])>),\n (<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2, 3, 4]])>,\n <tf.Tensor: shape=(1,), dtype=int64, numpy=array([5])>)]\n```", "```py\nrail_train = df[\"rail\"][\"2016-01\":\"2018-12\"] / 1e6\nrail_valid = df[\"rail\"][\"2019-01\":\"2019-05\"] / 1e6\nrail_test = df[\"rail\"][\"2019-06\":] / 1e6\n```", "```py\nseq_length = 56\ntrain_ds = tf.keras.utils.timeseries_dataset_from_array(\n    rail_train.to_numpy(),\n    targets=rail_train[seq_length:],\n    sequence_length=seq_length,\n    batch_size=32,\n    shuffle=True,\n    seed=42\n)\nvalid_ds = tf.keras.utils.timeseries_dataset_from_array(\n    rail_valid.to_numpy(),\n    targets=rail_valid[seq_length:],\n    sequence_length=seq_length,\n    batch_size=32\n)\n```", "```py\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(1, input_shape=[seq_length])\n])\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_mae\", patience=50, restore_best_weights=True)\nopt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=[\"mae\"])\nhistory = model.fit(train_ds, validation_data=valid_ds, epochs=500,\n                    callbacks=[early_stopping_cb])\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])\n])\n```", "```py\nunivar_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),\n    tf.keras.layers.Dense(1)  # no activation function by default\n])\n```", "```py\ndeep_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),\n    tf.keras.layers.SimpleRNN(32, return_sequences=True),\n    tf.keras.layers.SimpleRNN(32),\n    tf.keras.layers.Dense(1)\n])\n```", "```py\ndf_mulvar = df[[\"bus\", \"rail\"]] / 1e6  # use both bus & rail series as input\ndf_mulvar[\"next_day_type\"] = df[\"day_type\"].shift(-1)  # we know tomorrow's type\ndf_mulvar = pd.get_dummies(df_mulvar)  # one-hot encode the day type\n```", "```py\nmulvar_train = df_mulvar[\"2016-01\":\"2018-12\"]\nmulvar_valid = df_mulvar[\"2019-01\":\"2019-05\"]\nmulvar_test = df_mulvar[\"2019-06\":]\n```", "```py\ntrain_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_train.to_numpy(),  # use all 5 columns as input\n    targets=mulvar_train[\"rail\"][seq_length:],  # forecast only the rail series\n    [...]  # the other 4 arguments are the same as earlier\n)\nvalid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_valid.to_numpy(),\n    targets=mulvar_valid[\"rail\"][seq_length:],\n    [...]  # the other 2 arguments are the same as earlier\n)\n```", "```py\nmulvar_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n    tf.keras.layers.Dense(1)\n])\n```", "```py\nimport numpy as np\n\nX = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]\nfor step_ahead in range(14):\n    y_pred_one = univar_model.predict(X)\n    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)\n```", "```py\ndef split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):\n    return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]\n\nahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_train.to_numpy(),\n    targets=None,\n    sequence_length=seq_length + 14,\n    [...]  # the other 3 arguments are the same as earlier\n).map(split_inputs_and_targets)\nahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_valid.to_numpy(),\n    targets=None,\n    sequence_length=seq_length + 14,\n    batch_size=32\n).map(split_inputs_and_targets)\n```", "```py\nahead_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n    tf.keras.layers.Dense(14)\n])\n```", "```py\nX = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]\nY_pred = ahead_model.predict(X)  # shape [1, 14]\n```", "```py\n>>> my_series = tf.data.Dataset.range(7)\n>>> dataset = to_windows(to_windows(my_series, 3), 4)\n>>> list(dataset)\n[<tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n array([[0, 1, 2],\n [1, 2, 3],\n [2, 3, 4],\n [3, 4, 5]])>,\n <tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n array([[1, 2, 3],\n [2, 3, 4],\n [3, 4, 5],\n [4, 5, 6]])>]\n```", "```py\n>>> dataset = dataset.map(lambda S: (S[:, 0], S[:, 1:]))\n>>> list(dataset)\n[(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3])>,\n <tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n array([[1, 2],\n [2, 3],\n [3, 4],\n [4, 5]])>),\n (<tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 3, 4])>,\n <tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n array([[2, 3],\n [3, 4],\n [4, 5],\n [5, 6]])>)]\n```", "```py\ndef to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1,\n                       batch_size=32, shuffle=False, seed=None):\n    ds = to_windows(tf.data.Dataset.from_tensor_slices(series), ahead + 1)\n    ds = to_windows(ds, seq_length).map(lambda S: (S[:, 0], S[:, 1:, 1]))\n    if shuffle:\n        ds = ds.shuffle(8 * batch_size, seed=seed)\n    return ds.batch(batch_size)\n```", "```py\nseq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle=True, seed=42)\nseq2seq_valid = to_seq2seq_dataset(mulvar_valid)\n```", "```py\nseq2seq_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),\n    tf.keras.layers.Dense(14)\n])\n```", "```py\nX = mulvar_valid.to_numpy()[np.newaxis, :seq_length]\ny_pred_14 = seq2seq_model.predict(X)[0, -1]  # only the last time step's output\n```", "```py\nclass LNSimpleRNNCell(tf.keras.layers.Layer):\n    def __init__(self, units, activation=\"tanh\", **kwargs):\n        super().__init__(**kwargs)\n        self.state_size = units\n        self.output_size = units\n        self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units,\n                                                             activation=None)\n        self.layer_norm = tf.keras.layers.LayerNormalization()\n        self.activation = tf.keras.activations.get(activation)\n\n    def call(self, inputs, states):\n        outputs, new_states = self.simple_rnn_cell(inputs, states)\n        norm_outputs = self.activation(self.layer_norm(outputs))\n        return norm_outputs, [norm_outputs]\n```", "```py\ncustom_ln_model = tf.keras.Sequential([\n    tf.keras.layers.RNN(LNSimpleRNNCell(32), return_sequences=True,\n                        input_shape=[None, 5]),\n    tf.keras.layers.Dense(14)\n])\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 5]),\n    tf.keras.layers.Dense(14)\n])\n```", "```py\nconv_rnn_model = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=4, strides=2,\n                           activation=\"relu\", input_shape=[None, 5]),\n    tf.keras.layers.GRU(32, return_sequences=True),\n    tf.keras.layers.Dense(14)\n])\n\nlonger_train = to_seq2seq_dataset(mulvar_train, seq_length=112,\n                                       shuffle=True, seed=42)\nlonger_valid = to_seq2seq_dataset(mulvar_valid, seq_length=112)\ndownsampled_train = longer_train.map(lambda X, Y: (X, Y[:, 3::2]))\ndownsampled_valid = longer_valid.map(lambda X, Y: (X, Y[:, 3::2]))\n[...]  # compile and fit the model using the downsampled datasets\n```", "```py\nwavenet_model = tf.keras.Sequential()\nwavenet_model.add(tf.keras.layers.Input(shape=[None, 5]))\nfor rate in (1, 2, 4, 8) * 2:\n    wavenet_model.add(tf.keras.layers.Conv1D(\n        filters=32, kernel_size=2, padding=\"causal\", activation=\"relu\",\n        dilation_rate=rate))\nwavenet_model.add(tf.keras.layers.Conv1D(filters=14, kernel_size=1))\n```"]