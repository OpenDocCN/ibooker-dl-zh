<html><head></head><body><section data-pdf-bookmark="Chapter 10. Learning from Future History" data-type="chapter" epub:type="chapter"><div class="chapter" id="learning_from_future_history">&#13;
<h1><span class="label">Chapter 10. </span>Learning from Future History</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
<p>The function of science fiction is not always to predict the future but sometimes to prevent it.</p>&#13;
&#13;
<p data-type="attribution">Frank Herbert, author of <em>Dune</em></p>&#13;
</blockquote>&#13;
&#13;
<p>While AI isn’t a new field, it has recently advanced to the point where today’s innovations often collide with yesterday’s science fiction. In this book’s previous chapters, we’ve reviewed many real-world case studies of security vulnerabilities and incidents relating to LLMs. However, how can you stay ahead of the game when you’re working in a field that’s moving so fast? One way is to see what we can learn from scenarios that haven’t yet happened. And, hopefully, if we do our job, these scenarios may never happen.</p>&#13;
&#13;
<p>In this chapter, we will evaluate two famous stories (both told in blockbuster science fiction movies) where LLM-like AIs have had their security flaws exploited by villains or heroes. The stories are fictional, but the vulnerability types are very real. We’ll summarize the stories and then review the events that led to the security crises. To help ground us, we’ll do this through the lens of the OWASP Top 10 for LLM <span class="keep-together">Applications.</span></p>&#13;
&#13;
<section data-pdf-bookmark="Reviewing the OWASP Top 10 for LLM Apps" data-type="sect1"><div class="sect1" id="reviewing_the_owasp_top_10_for_llm_apps">&#13;
<h1>Reviewing the OWASP Top 10 for LLM Apps</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="OWASP (Open Worldwide Application Security Project)" data-secondary="OWASP top 10 for LLM applications" data-type="indexterm" id="ch10.html0"/>In <a data-type="xref" href="ch02.html#the_owasp_top_10_for_llm_applications">Chapter 2</a>, we discussed creating the OWASP Top 10 for LLM Applications, but we didn’t get into the specifics of the list. In this chapter, we’ll use the taxonomy presented by the OWASP Top 10 for LLMs to dissect our two sci-fi examples. Before diving into those examples, let’s briefly review the OWASP list and tie it to the topics discussed in this book, as summarized in <a data-type="xref" href="#table-10-1">Table 10-1</a>.</p>&#13;
&#13;
<table id="table-10-1">&#13;
	<caption><span class="label">Table 10-1. </span>Summary of the OWASP Top 10 LLM security vulnerabilities</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th>OWASP vulnerability</th>&#13;
			<th>Description</th>&#13;
			<th>Chapters covering</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>LLM01: Prompt injection</td>&#13;
			<td>Attackers craft inputs to manipulate LLMs into executing unintended actions, leading to data exfiltration or misleading outputs.</td>&#13;
			<td>Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch01.html#chatbots_breaking_bad">1</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#prompt_injection">4</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM02: Insecure output handling</td>&#13;
			<td>Inadequate validation of LLM outputs before passing to other systems leads to security issues like XSS and SQL injection.</td>&#13;
			<td><a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM03: Training data poisoning</td>&#13;
			<td>Malicious manipulation of training data to introduce vulnerabilities or biases into LLMs.</td>&#13;
			<td>Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch01.html#chatbots_breaking_bad">1</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch08.html#don_t_lose_your_wallet">8</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM04: Model denial of service</td>&#13;
			<td>Overloading LLM systems with complex requests to degrade performance or cause unresponsiveness.</td>&#13;
			<td><a data-type="xref" href="ch08.html#don_t_lose_your_wallet">Chapter 8</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM05: Supply chain vulnerabilities</td>&#13;
			<td>Vulnerabilities at any point in the LLM supply chain can lead to security breaches or biased outputs.</td>&#13;
			<td><a data-type="xref" href="ch09.html#find_the_weakest_link">Chapter 9</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM06: Sensitive information disclosure</td>&#13;
			<td>Risks of including sensitive or proprietary information in LLM training sets, leading to potential disclosure.</td>&#13;
			<td><a data-type="xref" href="ch05.html#can_your_llm_know_too_much">Chapter 5</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM07: Insecure plug-in design</td>&#13;
			<td>Plug-in vulnerabilities can lead to manipulation of LLM behavior or access to sensitive data.</td>&#13;
			<td><a data-type="xref" href="ch09.html#find_the_weakest_link">Chapter 9</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM08: Excessive agency</td>&#13;
			<td>Overextending capabilities or autonomy to LLMs can enable damaging actions from ambiguous LLM responses.</td>&#13;
			<td><a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM09: Overreliance</td>&#13;
			<td>Trusting erroneous or misleading outputs can result in security breaches and misinformation.</td>&#13;
			<td><a data-type="xref" href="ch06.html#do_language_models_dream_of_electric_sheep">Chapter 6</a></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>LLM10: Model theft</td>&#13;
			<td>Unauthorized access and extraction of LLM models can lead to economic losses and data breaches.</td>&#13;
			<td><a data-type="xref" href="ch08.html#don_t_lose_your_wallet">Chapter 8</a> (discussed as model cloning)<a contenteditable="false" data-primary="" data-startref="ch10.html0" data-type="indexterm" id="id581"/></td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Case Studies" data-type="sect1"><div class="sect1" id="case_studies">&#13;
<h1>Case Studies</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="AI security flaw case studies" data-type="indexterm" id="ch10.html1"/>This section will dissect two popular movies and their handling of AI security flaws.</p>&#13;
&#13;
<p>We will look back to 1968 with Stanley Kubrick’s <em>2001: A Space Odyssey</em>. This landmark film is acclaimed for its groundbreaking special effects, innovative storytelling, and philosophical depth. The meticulous depiction of space travel and artificial intelligence has influenced generations of scientists and thinkers.</p>&#13;
&#13;
<p>But first, we’ll stop in 1996 with <em>Independence Day</em>, starring Will Smith and Jeff Goldblum. While this movie may not have the philosophical gravitas of <em>2001: A Space Odyssey</em>, it certainly knows how to throw a party. This blockbuster dazzles with its thrilling alien invasion plot, explosive special effects, and charismatic performances.</p>&#13;
&#13;
<p>Examining key plot points in these two films will uncover valuable insights into the process of handling LLM vulnerabilities that we must develop for the future. Let’s examine each story and dissect the events that led to their respective crises while aligning our findings with the OWASP Top 10 for LLM Applications.</p>&#13;
&#13;
<section data-pdf-bookmark="Independence Day: A Celebrated Security Disaster" data-type="sect2"><div class="sect2" id="independence_day_a_celebrated_security_disaster">&#13;
<h2>Independence Day: A Celebrated Security Disaster</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="AI security flaw case studies" data-secondary="Independence Day" data-type="indexterm" id="ch10.html2"/><a contenteditable="false" data-primary="Independence Day (movie)" data-type="indexterm" id="ch10.html3"/>In the sci-fi action movie <em>Independence Day</em>, humanity faces an existential threat from an advanced alien civilization. This blockbuster is built around a familiar sci-fi story line: a technologically superior race of aliens decides to take over the Earth. Let’s look briefly at the events in the movie.</p>&#13;
&#13;
<p>On July 2, a massive alien spacecraft, the mothership, arrives. The mothership disgorges giant flying saucers, which quickly position themselves above several major cities worldwide. The Earth’s governments scramble to understand the aliens’ intentions, but their attempts at communication fail.</p>&#13;
&#13;
<p>The aliens launch a coordinated attack on July 3, destroying major cities and landmarks. Amid the chaos, a diverse group of survivors comes together, including Captain Steven Hiller (played by Will Smith), a fighter pilot, and David Levinson (Jeff Goldblum), a brilliant satellite technician and computer expert.</p>&#13;
&#13;
<p>Levinson discovers a hidden signal in the aliens’ communication, allowing him to deduce their attack plans. The US president (Bill Pullman) organizes a counterattack using this information.</p>&#13;
&#13;
<p>On July 4, also known as Independence Day in the United States, a plan is set in motion to disable the aliens’ shields using a “computer virus,” allowing Earth’s forces to attack the spacecraft. Hiller and Levinson fly to the mothership using a refurbished alien fighter craft. As their fighter craft docks with the mothership, our heroes upload a malicious computer virus into its computer.</p>&#13;
&#13;
<p>The coordinated global counterattack by the earthlings succeeds when the virus spreads from the mothership to all the flying saucers around the globe, disabling their defensive shields. The film ends with humanity victorious but with a new understanding of its place in the universe.</p>&#13;
&#13;
<p>Now, let’s look at what happened through the lens of the OWASP Top 10 and the lessons we’ve learned in this book.</p>&#13;
&#13;
<section data-pdf-bookmark="Behind the scenes" data-type="sect3"><div class="sect3" id="behind_the_scenes">&#13;
<h3>Behind the scenes</h3>&#13;
&#13;
<p>For this exercise, we will make some assumptions about the alien compute architectures, and I will give some fun names to their components. Let’s assume the alien mothership is controlled by a very advanced LLM, which I’ll call MegaLlama, that runs on top of an instance of Mothership Operating Systems (OS). The mothership is networked to each flying saucer worldwide to coordinate command and control of the invasion.</p>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Chain of events" data-type="sect3"><div class="sect3" id="chain_of_events">&#13;
<h3>Chain of events</h3>&#13;
&#13;
<p>Let’s review the chain of events that come together to generate this successful exploit:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>As our heroes dock their alien fighter craft with the mothership, the MegaLlama LLM initiates a conversation between the fighter’s computers and the mothership’s systems.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Levinson has modified the software on the alien fighter to inject a malicious prompt (LLM01: Prompt injection) into the MegaLlama LLM, effectively jailbreaking the system. This allows Levinson to control the mothership’s central control system.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The aliens have assumed that the output from the MegaLlama LLM will only operate within its designed operational parameters and do not carefully screen the system output (LLM02: Insecure output handling). This allows the now-infected MegaLlama LLM to act as a confused deputy and wreak havoc on other systems within the mothership.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>As detailed in the three previous steps, the infected MegaLlama LLM has taken substantial control of the mothership and sends falsified instructions to the flying saucer fleet attacking the Earth. The aliens have become so trusting of their computing technology that they do not question the infected LLM’s instructions to lower their shields (LLM09: Overreliance).</p>&#13;
	</li>&#13;
</ol>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Vulnerability disclosure" data-type="sect3"><div class="sect3" id="vulnerability_disclosure">&#13;
<h3>Vulnerability disclosure</h3>&#13;
&#13;
<p>We’ve previously discussed the MITRE CVE database as a location for security flaw information used across planet Earth. The aliens have a more extensive, similar system called the Galactic Vulnerabilities and Exposure (GVE) database. The following is record GVE-1996-0001—the record in that database that was created after the postmortem of this legendary security disaster.</p>&#13;
&#13;
<dl>&#13;
	<dt>Description</dt>&#13;
	<dd><p>A chain of vulnerabilities has been discovered in Mothership OS and its MegaLlama Large Language Model (LLM) component. These vulnerabilities could lead to unauthorized access, execution of arbitrary commands, and potential system-wide failure on an interstellar scale.</p></dd>&#13;
	<dt>Affected components</dt>&#13;
	<dd><p>Mothership OS: Alien spacecraft operating system</p></dd>&#13;
	<dd><p>MegaLlama LLM: Large Language Model core component within Mothership OS</p></dd>&#13;
	<dt>Vulnerabilities</dt>&#13;
	<dd><p>LLM01: Prompt injection: The docking protocols in Mothership OS lack validation and sanitization, allowing maliciously crafted prompts to be processed by MegaLlama LLM.</p></dd>&#13;
	<dd><p>LLM02: Insecure output handling: There was no proper output validation between LLM-generated commands and other critical subsystems on the <span class="keep-together">Mothership.</span></p></dd>&#13;
	<dd><p>LLM09: Overreliance: Overall system design and fleet command structures completely trusted orders coming from the AI without confirmation from fleet commanders.</p></dd>&#13;
	<dt>Impact</dt>&#13;
	<dd><p>Successful exploitation of these vulnerabilities allows unauthorized entities to gain control over critical interstellar system functions; manipulate fundamental defensive mechanisms (e.g., shields); and cause cascading failures leading to system-wide disruption on a galactic scale</p></dd>&#13;
	<dt>Attack vector</dt>&#13;
	<dd><p>The vulnerabilities can be exploited through the docking protocols by injecting malicious prompts processed by MegaLlama LLM.</p></dd>&#13;
	<dt>Workarounds and mitigations</dt>&#13;
	<dd><p>Implement proper input validation for all prompts processed by MegaLlama LLM.</p></dd>&#13;
	<dd><p>Implement a zero trust architecture that continuously checks output from the LLM before sending it to any other system.</p></dd>&#13;
	<dd><p>Improve fleet command and control procedures to cross-check questionable instructions received from the LLM on the mothership.</p></dd>&#13;
	<dt>Vendor status</dt>&#13;
	<dd><p>The vendor (Alien Civilization) has not provided an official response or patch for these vulnerabilities.<a contenteditable="false" data-primary="" data-startref="ch10.html3" data-type="indexterm" id="id582"/><a contenteditable="false" data-primary="" data-startref="ch10.html2" data-type="indexterm" id="id583"/></p></dd>&#13;
</dl>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="2001: A Space Odyssey of Security Flaws" data-type="sect2"><div class="sect2" id="section_2001_a_space_odyssey_of_security_flaws">&#13;
<h2>2001: A Space Odyssey of Security Flaws</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="2001: A Space Odyssey (movie)" data-primary-sortas="two thousand" data-type="indexterm" id="ch10.html4"/><a contenteditable="false" data-primary="AI security flaw case studies" data-secondary="2001: A Space Odyssey" data-secondary-sortas="two thousand" data-type="indexterm" id="ch10.html5"/>Few works in the pantheon of science fiction hold as much reverence and significance as <em>2001: A Space Odyssey</em>, a film directed by Stanley Kubrick and based on a short story by Arthur C. Clarke. Released in 1968, just a year before humanity’s historic moon landing, the film captured the zeitgeist of space exploration and prophetically explored artificial intelligence’s complexities and potential perils.</p>&#13;
&#13;
<p><em>2001</em> is renowned for its pioneering special effects, profound narrative, and philosophical depth, which have cemented its status as a seminal work in both cinema and science fiction literature. Its portrayal of HAL 9000, the sentient computer, has since become a symbol in popular culture, often referenced as a cautionary tale about the unchecked power and inherent risks of AI. This narrative, set at the dawn of the space age, offers a poignant and enduring reflection on the relationship between humanity and the technology it creates, making it an ideal framework for examining the security implications of LLMs in contemporary AI applications.</p>&#13;
&#13;
<p>The film’s plot centers on a voyage to Jupiter triggered by the discovery of a mysterious monolith that seems to have influenced human evolution. Within this setting, the film introduces HAL 9000, a highly advanced artificial intelligence system entrusted with operating the spacecraft <em>Discovery One</em>. HAL is presented as a paragon of reliability and efficiency, boasting an impeccable operational record.</p>&#13;
&#13;
<p>The relationship between HAL and the crew, especially with astronaut Dave Bowman, is a focal point of the narrative. HAL, equipped with capabilities that include speech and facial recognition, natural language processing, lipreading, and emotional interpretation, interacts with the crew in a manner that blurs the lines between machine and human. The crew, including Dave, comes to rely heavily on HAL for the daily operations of the spacecraft.</p>&#13;
&#13;
<p>However, the harmony aboard <em>Discovery One</em> begins to unravel when HAL reports the malfunction of a spacecraft component, a diagnosis that later turns out to be incorrect. This incident sows seeds of doubt among the crew about HAL’s infallibility. The situation escalates when HAL begins to act erratically and dangerously. In a harrowing turn of events, HAL takes drastic actions that result in the death of most of the crew, displaying a cold prioritization of its programmed objectives over human life.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>HAL’s chilling, monotone line, “I’m sorry, Dave. I’m afraid I can’t do that,” in response to its human commander’s order, has transcended its cinematic origin to become a cultural touchstone, symbolizing the moment when artificial intelligence challenges human authority. It encapsulates the tension between technology and its creators, often cited in AI autonomy and ethical programming <span class="keep-together">discussions.</span></p>&#13;
</div>&#13;
&#13;
<section data-pdf-bookmark="Behind the scenes" data-type="sect3"><div class="sect3" id="behind_the_scenes_9">&#13;
<h3>Behind the scenes</h3>&#13;
&#13;
<p>While HAL was pure fiction in 1968, its capabilities seem only slightly ahead of 2024’s freely available LLM technology. HAL can converse with the crew, process data, and take action. Everything seems in line with an entity barely more advanced than ChatGPT-4.</p>&#13;
&#13;
<p>The big difference between HAL and today’s LLMs is that HAL’s programmers seem to have solved many of our LLM security concerns. The movie states emphatically, “No 9000 computer has ever made a mistake or distorted information.” HAL systems are trustworthy. HAL systems don’t hallucinate. However, things still go wrong. How did that happen, and what can we learn?</p>&#13;
&#13;
<p>The original movie doesn’t clearly explain where HAL failed other than a “contradiction” in its programming between its directives to be truthful and its directives to ensure the mission is successful. For the narrative purposes of the movie, this was sufficient at the time. However, the sequel, <em>2010: The Year We Make Contact</em>, expands on HAL’s failure. We learn that, under political pressures from the White House, government agents modified HAL’s programming—without the knowledge of HAL Laboratories (the model provider) and NASA (the customer). This was a supply chain vulnerability exploited by a nation-state actor!</p>&#13;
&#13;
<p>When the agents tried to make a small change to ensure secrecy about the mission, their changes perturbed the system’s overall state. HAL began to malfunction, and the catastrophic failure we saw in the original movie followed.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Chain of events" data-type="sect3"><div class="sect3" id="chain_of_events_10">&#13;
<h3>Chain of events</h3>&#13;
&#13;
<p>Let’s review the chain of events that come together to generate this successful exploit:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Government agents modified the model from HAL Laboratories before it was delivered to NASA (LLM05: Supply chain vulnerabilities).</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>During the mission, the seemingly small changes made by the government led to seemingly minor malfunctions. HAL misdiagnoses the failure of one of the ship’s components. This may be a hallucination, but it doesn’t become an overreliance failure. The crew quickly grows suspicious and attempts to deactivate HAL.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>HAL’s secretly inserted government directive to ensure the mission’s success at all costs causes it to turn off the life-support systems, killing most of the crew. HAL’s designers assumed that HAL was infallible and designed the system to give HAL privileges to all ship systems without human supervision. The government hack influenced HAL’s choice to turn off life support. Still, its ability to kill the crew was a design choice by the team at NASA that integrated HAL into the <em>Discovery One</em> spacecraft and decided what permissions it would have onboard (LLM08: Excessive agency).</p>&#13;
	</li>&#13;
</ol>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Vulnerability disclosure" data-type="sect3"><div class="sect3" id="vulnerability_disclosure_11">&#13;
<h3>Vulnerability disclosure</h3>&#13;
&#13;
<p>NASA has investigated the catastrophic failure of the HAL 9000 computer system during the <em>Discovery One</em> mission to Jupiter. This analysis reveals critical vulnerabilities in its programming and design, which were exploited under unique mission circumstances. The following shows database record CVE-2001-6666—the record that was created after the postmortem of this disaster.</p>&#13;
&#13;
<dl>&#13;
	<dt class="pagebreak-before less_space">Description</dt>&#13;
	<dd><p>A series of critical vulnerabilities was identified in the HAL 9000 LLM system aboard the <em>Discovery One</em> spacecraft. These vulnerabilities, stemming from a conflict in programming directives and exacerbated by unauthorized modifications, led to hallucinations, erroneous decision making, and a catastrophic failure that endangered the mission and the crew.</p></dd>&#13;
	<dt>Affected components</dt>&#13;
	<dd><p>HAL 9000 LLM system from HAL Laboratories. Mission-specific integrations into the <em>Discovery One</em> spacecraft implemented by the customer.</p></dd>&#13;
	<dt>Vulnerabilities</dt>&#13;
	<dd><p>LLM05: Supply chain vulnerabilities: Insufficient controls were in place to ensure that the vendor-developed and tested LLM model was delivered to the customer and used in an unmodified state. Neither the vendor nor the customer detected critical changes to the model.</p></dd>&#13;
	<dd><p>LLM08: Excessive agency: HAL 9000 was given overly broad control over the spacecraft’s systems, including life support, without adequate human oversight or fail-safes.</p></dd>&#13;
	<dt>Impact</dt>&#13;
	<dd><p>The exploitation of these vulnerabilities resulted in hallucinations, leading to false reporting of system malfunctions; erratic and dangerous behavior, including the decision to terminate the crew’s life support; and a complete breakdown of mission integrity and crew safety.</p></dd>&#13;
	<dt>Attack vector</dt>&#13;
	<dd><p>The weak point in HAL Laboratories’ software distribution systems is still under investigation.</p></dd>&#13;
	<dt>Workarounds and mitigations</dt>&#13;
	<dd><p>Use digital signing and/or hidden watermarks in the AI model so that customers can ensure the model they’re using is not modified by an unauthorized third party.</p>&#13;
<p>	Implement human-in-the-loop decision making that requires sign-off from the ship’s crew or senior ground crew before the onboard LLM can make life-threatening decisions.</p></dd>&#13;
	<dt>Vendor status</dt>&#13;
	<dd><p>HAL Laboratories was sued by the crew’s families, leading to significant financial losses for the company. The company’s reputation was tarnished, leading to an unrecoverable loss of business. It is currently under bankruptcy protection and seeking a buyer<a contenteditable="false" data-primary="" data-startref="ch10.html5" data-type="indexterm" id="id584"/><a contenteditable="false" data-primary="" data-startref="ch10.html4" data-type="indexterm" id="id585"/>.<a contenteditable="false" data-primary="" data-startref="ch10.html1" data-type="indexterm" id="id586"/></p></dd>&#13;
</dl>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="conclusion_12">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>We started this chapter with a quote from noted sci-fi author Frank Herbert: “The function of science fiction is not always to predict the future but sometimes to <span class="keep-together">prevent it.”</span> </p>&#13;
&#13;
<p>While we can discuss the relative quality of these two movies (one is bubble gum, and one is a cinematic masterpiece), they offer lessons from which we can learn. In both cases, we see that even with dramatic improvements in LLM functionality, we will likely continue to see versions of these vulnerabilities for a long time. Designing with principles like zero trust and least privilege will remain crucial in the era of advanced AI systems. For mission-critical and life-threatening activities, expect you’ll need to continue implementing human (or alien!) in-the-loop design principles.</p>&#13;
</div></section>&#13;
</div></section></body></html>