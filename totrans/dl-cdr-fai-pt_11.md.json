["```py\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n```", "```py\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=['user','movie','rating','timestamp'])\nratings.head()\n```", "```py\nlast_skywalker = np.array([0.98,0.9,-0.9])\n```", "```py\nuser1 = np.array([0.9,0.8,-0.6])\n```", "```py\n(user1*last_skywalker).sum()\n```", "```py\n2.1420000000000003\n```", "```py\ncasablanca = np.array([-0.99,-0.3,0.8])\n```", "```py\n(user1*casablanca).sum()\n```", "```py\n-1.611\n```", "```py\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n```", "```py\nratings = ratings.merge(movies)\nratings.head()\n```", "```py\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n```", "```py\nn_users  = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n```", "```py\none_hot_3 = one_hot(3, n_users).float()\nuser_factors.t() @ one_hot_3\n```", "```py\ntensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])\n```", "```py\nuser_factors[3]\n```", "```py\ntensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])\n```", "```py\nclass Example:\n    def __init__(self, a): self.a = a\n    def say(self,x): return f'Hello {self.a}, {x}.'\n```", "```py\nex = Example('Sylvain')\nex.say('nice to meet you')\n```", "```py\n'Hello Sylvain, nice to meet you.'\n```", "```py\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n\n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return (users * movies).sum(dim=1)\n```", "```py\nx,y = dls.one_batch()\nx.shape\n```", "```py\ntorch.Size([64, 2])\n```", "```py\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n```", "```py\nlearn.fit_one_cycle(5, 5e-3)\n```", "```py\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n```", "```py\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n```", "```py\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n```", "```py\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n```", "```py\nloss_with_wd = loss + wd * (parameters**2).sum()\n```", "```py\nparameters.grad += wd * 2 * parameters\n```", "```py\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n```", "```py\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n```", "```py\n(#0) []\n```", "```py\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n```", "```py\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n```", "```py\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n```", "```py\n(#1) [Parameter containing:\ntensor([[-0.9595],\n        [-0.8490],\n        [ 0.8159]], requires_grad=True)]\n```", "```py\ntype(t.a.weight)\n```", "```py\ntorch.nn.parameter.Parameter\n```", "```py\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n```", "```py\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n```", "```py\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n```", "```py\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n```", "```py\n['Children of the Corn: The Gathering (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Beautician and the Beast, The (1997)',\n 'Crow: City of Angels, The (1996)',\n 'Home Alone 3 (1997)']\n```", "```py\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n```", "```py\n['L.A. Confidential (1997)',\n 'Titanic (1997)',\n 'Silence of the Lambs, The (1991)',\n 'Shawshank Redemption, The (1994)',\n 'Star Wars (1977)']\n```", "```py\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n```", "```py\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n```", "```py\nlearn.model\n```", "```py\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1635, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1635, 1)\n)\n```", "```py\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n```", "```py\n['Titanic (1997)',\n \"Schindler's List (1993)\",\n 'Shawshank Redemption, The (1994)',\n 'L.A. Confidential (1997)',\n 'Silence of the Lambs, The (1991)']\n```", "```py\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n```", "```py\n'Dial M for Murder (1954)'\n```", "```py\nembs = get_emb_sz(dls)\nembs\n```", "```py\n[(944, 74), (1635, 101)]\n```", "```py\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n\n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n```", "```py\nmodel = CollabNN(*embs)\n```", "```py\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n```", "```py\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n```", "```py\n@delegates(TabularModel)\nclass EmbeddingNN(TabularModel):\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)\n```"]