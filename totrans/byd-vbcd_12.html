<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. The Ethical Implications of Vibe Coding"><div class="chapter" id="ch09_the_ethical_implications_of_vibe_coding_1752630044848930">
<h1><span class="label">Chapter 9. </span>The Ethical Implications of Vibe Coding</h1>

<p>As AI-assisted development becomes increasingly commonplace, it’s critical to address the ethical and societal implications of this new paradigm.<a contenteditable="false" data-primary="ethical implications of vibe coding" data-type="indexterm" id="ix_ethi"/> This chapter steps back from the technical details to examine vibe coding through an ethical lens: these new development methods can be effective, but they also need to be implemented responsibly and to benefit individuals and society at large.</p>

<p>I begin with questions of intellectual property (IP). Who owns the code that AI generates, and is it permissible to use AI outputs that may be derived from open source code without attribution? From there, I consider bias and fairness. Transparency is another focus: should developers disclose which parts of a codebase were AI-generated, and how can teams ensure accountability for code quality and bugs?</p>

<p>I outline responsible development practices in AI usage, from establishing transparency and accountability to avoiding sensitive data in prompts to ensuring accessibility and inclusivity. The chapter finishes with a set of guidelines for using AI tools responsibly.</p>

<div data-type="warning" epub:type="warning">
<h1>Legal Disclaimer</h1>
<p>The following section <a contenteditable="false" data-primary="intellectual property, considerations with AI-generated code" data-type="indexterm" id="ix_intelprp"/>touches on<a contenteditable="false" data-primary="ethical implications of vibe coding" data-secondary="intellectual property considerations" data-type="indexterm" id="ix_ethiintel"/> complex legal topics, particularly concerning copyright and intellectual property law, from a primarily US perspective. Legal systems and interpretations are evolving worldwide, especially concerning artificial intelligence. This information is for educational purposes only and does not constitute legal advice. You should consult with a qualified intellectual property lawyer before making any decisions based on this information, especially if you have concerns about the ownership or licensing of code you or an AI tool generates.</p>
</div>

<section data-type="sect1" data-pdf-bookmark="Intellectual Property Considerations"><div class="sect1" id="ch09_intellectual_property_considerations_1752630044849075">
<h1>Intellectual Property Considerations</h1>

<p>Who owns AI-generated code? And does using it respect the licenses and copyrights of the source material on which the AI was trained?<a contenteditable="false" data-primary="intellectual property, considerations with AI-generated code" data-secondary="copyrights, licenses, and ownership of code" data-type="indexterm" id="ix_intelprpcpy"/><a contenteditable="false" data-primary="copyrights, AI-generated code and" data-type="indexterm" id="id1012"/><a contenteditable="false" data-primary="licensing considerations in AI code" data-type="indexterm" id="id1013"/> AI models like GPT have been trained on huge swaths of code from the internet, including open source repositories with various licenses (MIT, GPL, Apache, etc.). If the AI generates a snippet that is <a href="https://oreil.ly/I3HxT">very similar</a> (or identical) to something from a GPL-licensed project, using that snippet in a proprietary codebase could inadvertently violate the GPL, which generally requires <a href="https://oreil.ly/8inJc">sharing derivative code</a>.</p>

<p>According to open source norms and general copyright principles, small snippets of a few lines <em>might not</em> be copyrightable if they lack sufficient originality to be considered an independent creative work, or their use <em>could potentially</em> be considered de minimis (too trivial to warrant legal concern).<a contenteditable="false" data-primary="de minimis" data-type="indexterm" id="id1014"/> However, anything substantial or expressing a unique creative choice is more likely to be protected by copyright. It’s crucial to understand that “open source” does not mean “public domain.” By default, creative work, including code, is under exclusive copyright by its author.<a contenteditable="false" data-primary="open source" data-secondary="not synonymous with public domain" data-type="indexterm" id="id1015"/> Open source licenses explicitly grant permissions that would otherwise be restricted by copyright law.</p>

<p>If you want to know more about open <a contenteditable="false" data-primary="open source" data-secondary="information about norms, sources of" data-type="indexterm" id="id1016"/>source norms, good places to start include the following:</p>

<dl>
	<dt>The Open Source Initiative</dt>
	<dd>
	<p>The <a href="https://oreil.ly/hmJVN">OSI</a> defines and promotes open source software, maintains the Open Source Definition, and approves licenses that meet its <span class="keep-together">criteria</span>.</p>
	</dd>
	<dt>The Free Software Foundation (FSF)</dt>
	<dd>
	<p>The <a href="https://fsf.org">FSF</a> advocates for “free software” (which has a strong overlap with open source principles) and is the steward of licenses like the GNU General Public License (GPL).</p>
	</dd>
	<dt>Project-specific documentation</dt>
	<dd>
	<p>Individual open source projects typically include <em>LICENSE</em> files, <em>README</em> files, and <em>CONTRIBUTING</em> guidelines that detail the terms of use and contribution for that specific project.</p>
	</dd>
	<dt>Community and legal resources</dt>
	<dd>
	<p>Websites like GitHub offer extensive documentation and discussions on open source practices. Organizations like the Linux Foundation and legal information sites also provide valuable resources on open source compliance and legal aspects.</p>
	</dd>
</dl>

<p>The question of whether using small code snippets overlaps with the <a href="https://oreil.ly/d0ZK8">fair use doctrine</a> (in the US; “fair dealing” in many other jurisdictions) is complex and highly fact-dependent. <a href="https://oreil.ly/EwrJ2">Fair use</a> permits limited use of copyrighted material without permission for purposes such as criticism, comment, news reporting, teaching, scholarship, or research. <a contenteditable="false" data-primary="fair use" data-type="indexterm" id="id1017"/>US courts typically consider four factors to determine fair use:</p>

<ul>
	<li>
	<p>The <a href="https://oreil.ly/1TE5B">purpose</a> and character of the use (commercial versus nonprofit, transformative versus duplicative)</p>
	</li>
	<li>
	<p>The nature of the copyrighted work (highly creative versus factual)</p>
	</li>
	<li>
	<p>The amount and substantiality of the portion used in relation to the copyrighted work as a whole</p>
	</li>
	<li>
	<p>The effect of the use upon the potential market for or value of the copyrighted work</p>
	</li>
</ul>

<p>While some might argue that copying very small, functional code snippets for interoperability or to access uncopyrightable ideas could fall under fair use, especially if the use is transformative, this is not a clearly settled area of law for code, and there’s no universally agreed-upon number of lines that is definitively “fair use” or de minimis. The safest course is often to get permission or to understand the underlying idea and rewrite the code in your own way.<a contenteditable="false" data-primary="Google LLC v. Oracle America, Inc. (court case)" data-type="indexterm" id="id1018"/> The U.S. Supreme Court case <em>Google LLC v. Oracle America, Inc.</em> addressed fair use in the context of software APIs, finding Google’s reimplementation of Java API declaring code to be fair use, but this was a specific and complex ruling focused on API declarations, not all code. It’s generally understood that copyright protects the specific expression of an idea, not the idea, procedure, or method of operation itself.<a contenteditable="false" data-primary="copyrights, AI-generated code and" data-type="indexterm" id="id1019"/></p>

<p>Typically, the developer <em>using</em> the AI is considered the “author” in the sense that the AI is a tool, similar to a compiler or a word processor. <a contenteditable="false" data-primary="developers" data-secondary="considered as authors of AI-generated code" data-type="indexterm" id="id1020"/><a contenteditable="false" data-primary="author of AI-generated code" data-type="indexterm" id="id1021"/>Thus, if code is generated in a work context, the developer’s company would likely own the code produced by the developer using the tool, subject to the AI tool’s terms of service and underlying IP issues. However, the terms of service (ToS) of AI tools are critical.<a contenteditable="false" data-primary="OpenAI’s ToS (terms of service)" data-type="indexterm" id="id1022"/><a contenteditable="false" data-primary="terms of service (ToS) of AI tools" data-type="indexterm" id="id1023"/> Most ToS grant the user rights to the output they generate.<a contenteditable="false" data-primary="GPT-4, OpenAI's ToS for code" data-type="indexterm" id="id1024"/><a contenteditable="false" data-primary="ChatGPT" data-secondary="OpenAI’s ToS for" data-type="indexterm" id="id1025"/> OpenAI’s ToS, for instance, states, “You own the outputs you create with GPT-4, including code.”</p>

<p>This “ownership,” however, needs careful consideration. <a contenteditable="false" data-primary="ownership of code" data-type="indexterm" id="id1026"/>It generally means that the AI provider isn’t claiming ownership of what <em>you</em> create <em>with their tool</em>. But this assumes you have the rights to the <em>inputs</em> you provide, and it doesn’t automatically mean the output is itself eligible for copyright protection or that it’s free from third-party intellectual property claims. If you input your own original code to the tool for modification or extension, the output is most likely yours (or your employer’s), again, subject to how the AI processes it and what it incorporates from its training data. But if you input someone else’s copyrighted code to fix or transform, the output <em>might</em> be considered a <a href="https://oreil.ly/mBPyq">derivative work of that third-party code</a>.</p>

<p>In the US and many other jurisdictions, whether AI-generated output that is substantially similar to training data, or output based on copyrighted input, constitutes a derivative work is a subject of ongoing legal debate and lacks full clarity. Don’t feed large chunks of copyrighted code that isn’t yours (or licensed appropriately) into an AI tool, because the output could be deemed a <a href="https://oreil.ly/O4ktq">derivative work</a> and thus fall under the license of that original copyrighted code.</p>

<p>Given these uncertainties, to be safe, treat AI-generated code as if it’s under an ambiguous license, and only use it if you are comfortable that it doesn’t infringe on existing copyrights and that you can comply with any potential open source license obligations. Regarding the copyright status of the AI output itself, the <a href="https://oreil.ly/Y0PYG">US Copyright Office has stated</a> that works generated solely by AI without sufficient human authorship are not copyrightable. If a human significantly modifies or arranges <a contenteditable="false" data-primary="copyrights, AI-generated code and" data-secondary="possible copyrightable material" data-type="indexterm" id="id1027"/>AI-generated material in a creative way, that human contribution might be <a href="https://oreil.ly/NV3Gl">copyrightable</a> but not the AI-generated elements standing alone. Thus, it’s often wise to assume that purely AI-generated outputs might not be copyrightable by anyone or that copyright would extend only to the human’s creative contributions.</p>

<p>This is not a hypothetical worry. <a contenteditable="false" data-primary="open source" data-secondary="and class action lawsuit Doe v. GitHub, Inc." data-secondary-sortas="class" data-type="indexterm" id="id1028"/>In fact, there’s ongoing legal debate.<a contenteditable="false" data-primary="Doe v. GitHub, Inc. class-action lawsuit" data-type="indexterm" id="id1029"/> A <a href="https://githubcopilotlitigation.com">prominent class-action lawsuit, <em>Doe v. GitHub, Inc.</em></a>, was filed against GitHub, Microsoft, and OpenAI, claiming that GitHub Copilot produces code that is too similar to licensed open source code without proper attribution or adherence to license terms.<a contenteditable="false" data-primary="GitHub Copilot" data-secondary="lawsuit against" data-type="indexterm" id="id1030"/> <a contenteditable="false" data-primary="Copilot" data-secondary="lawsuit against" data-type="indexterm" id="id1031"/> While some claims in this case have been dismissed or are under appeal (as of mid-2025, the case involves ongoing proceedings, including an appeal to the Ninth Circuit regarding DMCA claims and remaining breach of contract claims), it highlights a genuine concern: AI can and sometimes does regurgitate or closely paraphrase copyrighted code from its training data.<sup><a data-type="noteref" id="id1032-marker" href="ch09.html#id1032">1</a></sup></p>

<p>An older (but still relevant and later substantiated) <a href="https://oreil.ly/fFUUd">study by GitHub itself</a> noted that, in some cases, Copilot’s output included suggestions that matched training data, including rare instances of longer verbatim snippets. While most AI tools are designed to avoid direct, extensive copying of identifiable code unless specifically prompted or dealing with very standard algorithms, the risk exists. Furthermore, it’s not just open source code that’s a concern; numerous lawsuits have been filed by authors, artists, and media companies alleging that their fully copyrighted, privately owned intellectual property was used without permission or compensation to train large language models and other generative AI systems. The challenge with proprietary code is that, unlike open source, it’s often not publicly visible, <span class="keep-together">making</span> it harder for an end user to confirm if an AI’s output is inadvertently similar to such private code.</p>

<p>Nevertheless, the ethical and prudent practice is to <em>act as if any code you accept from an AI tool is your responsibility</em>. Thoroughly review, test, and understand any AI-generated code before incorporating it into your projects, and ensure its use complies with all applicable licenses and copyright laws.<a contenteditable="false" data-primary="intellectual property, considerations with AI-generated code" data-secondary="copyrights, licenses, and ownership of code" data-startref="ix_intelprpcpy" data-type="indexterm" id="id1033"/></p>

<section data-type="sect2" data-pdf-bookmark="What to Do If You Get Suspicious Output"><div class="sect2" id="ch09_what_to_do_if_you_get_suspicious_output_1752630044849140">
<h2>What to Do If You Get Suspicious Output</h2>

<p>If an AI output seems like a verbatim or near-verbatim copy of known code (especially if it includes distinctive comments or author names), treat it carefully.<a contenteditable="false" data-primary="intellectual property, considerations with AI-generated code" data-secondary="dealing with suspicious AI output" data-type="indexterm" id="id1034"/> Consider running a similarity check using a plagiarism detector tool, or do a web search for unique strings to see if you find any matches that could indicate copying.</p>

<p>Another principle to follow is <em>When in doubt, leave it out</em>. Either avoid using the output or make sure it’s under a compatible license and give attribution if required. For example, if Copilot spits out a well-known algorithm implementation that you recognize from Stack Overflow or an open source project, cite the source or rewrite it in your own way, using the AI’s answer as a guide but not quoting it verbatim.</p>

<p>If you suspect the output matches an existing library solution, consider including the library itself instead (with proper license). You can also prompt the AI:</p>

<blockquote>
<p>Please provide an original implementation rather than one copied from a library.</p>
</blockquote>

<p>It might then synthesize a more unique solution. (There’s no guarantee it won’t be influenced by its training code, but at least it will try to not copy outright).</p>

<p>The ethics here also touch on not using AI to willfully strip attribution. For example, it would be unethical to copy code from Stack Overflow via AI without attribution to circumvent a policy that you should credit the answer. That erodes trust in the open knowledge ecosystem. It’s better to incorporate the material with proper credit. Depending on the circumstances, that might mean the following:</p>

<ul>
	<li>
	<p>If an AI writes a code comment from some source that has an author’s name (like copying a snippet with “John Doe 2018” in a comment), you should keep that or move it to a proper attribution section with a full citation rather than deleting it. That respects the original author’s credit.</p>
	</li>
	<li>
	<p>If an AI provided a solution that you know comes from a known algorithm or code snippet, cite that source as you normally would if you had looked it up yourself.</p>
	</li>
	<li>
	<p>If an AI tool creates something arguably creative (like a unique approach or text for documentation), acknowledge its contribution. Though it doesn’t have rights, it’s about transparency (and maybe a nod to the tech).</p>
	</li>
</ul>

<p class="pagebreak-before less_space">Some open source licenses (like MIT) are permissive enough that including copied code with attribution would satisfy the license. Others, like GPL or AGPL, would “infect” your whole codebase if you include that code, which is undesirable for closed projects.</p>

<p>In short: if you suspect the AI has given you something that might cause IP issues, either avoid using it or transform it sufficiently to ensure you’re complying with any possible license.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Gray Areas"><div class="sect2" id="ch09_grey_areas_1752630044849191">
<h2>Gray Areas</h2>

<p>Even as I write this, AI tools continue to raise new questions about IP, copyright, and ethics.<a contenteditable="false" data-primary="gray areas in intellectual property issues with AI-generated code" data-type="indexterm" id="id1035"/><a contenteditable="false" data-primary="intellectual property, considerations with AI-generated code" data-secondary="gray areas" data-type="indexterm" id="id1036"/> For instance:</p>

<ul>
	<li>
	<p>If your vibe coding includes using AI to generate noncode assets like documentation text, config files, or images, similar IP questions arise. For instance, if you generate an icon image via an AI tool that was trained on copyrighted images, who owns that new image?</p>
	</li>
	<li>
	<p>If an AI writes a significant part of a software product, should the original authors of the code on which the AI was trained get credit?</p>
	</li>
	<li>
	<p>Could someone claim that your AI-generated code infringes on their copyright because it looks similar to theirs? If sections of nontrivial lengths are possibly identical, this is where similarity checking comes in.</p>
	</li>
</ul>

<p>There’s an emerging notion that AI companies might need to implement license-respecting filters or allow teams to opt out of their code being included in AI training data. It’s evolving, but developers on the ground should act conservatively to not violate rights.</p>

<p>It will take time for courts to settle all of the legal issues, but in the meantime, intellectual honesty and respect should guide us. If AI uses a known algorithm from a published paper, cite the paper in a comment. If it uses a common open source helper code, credit the project. It’s about respect for authorship. If you recognize where something came from, err on the side of giving credit. It’s a good practice that fosters transparency.</p>

<p>Remember that under the hood, the AI’s knowledge comes from thousands of developers who shared their code publicly. Ethically, the software industry owes that community the respect of upholding open source licenses and norms. Give credit where <a contenteditable="false" data-primary="ethical implications of vibe coding" data-secondary="intellectual property considerations" data-startref="ix_ethiintel" data-type="indexterm" id="id1037"/>it’s due and don’t abuse others’ work under <a contenteditable="false" data-primary="intellectual property, considerations with AI-generated code" data-startref="ix_intelprp" data-type="indexterm" id="id1038"/>the guise of “the AI wrote it, not me.”</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Transparency and Attribution"><div class="sect1" id="ch09_transparency_and_attribution_1752630044849242">
<h1>Transparency and Attribution</h1>

<p><em>Transparency</em> refers to being open about the use of AI in your development process and outputs, and <em>attribution</em> refers to giving proper credit when AI-derived code comes from identifiable sources.<a contenteditable="false" data-primary="ethical implications of vibe coding" data-secondary="transparency and attribution" data-type="indexterm" id="ix_ethitrnsatt"/><a contenteditable="false" data-primary="transparency about use of AI in development" data-type="indexterm" id="id1039"/></p>

<p>Transparency is important for the sake of accountability. For example, if AI-generated code introduces a bug or security flaw, being transparent that “this code was AI-suggested” might help you analyze the root cause—perhaps an ambiguous prompt should be rewritten. In code comments or a project’s README or documentation, you might mention generally that “this project was built with assistance from AI tools like ChatGPT.” Or get more specific: “Added a function to parse CSV (generated with ChatGPT’s help, then modified).” It’s a bit like acknowledging your use of frameworks or libraries.</p>

<p>Transparency is also key to trust: stakeholders (your team, clients, end users, or industry regulators) might want to know how your software was developed and validated. If an AI was involved in code generation, some stakeholders might wrongly trust it too much or too little. Transparency allows a conversation about reliability: “Yes, we used AI, but we tested it thoroughly” or “This part was tricky—we had AI generate the initial code, but we’ve since verified it.”</p>

<p>Attributions are also expected or required in many academic venues.<a contenteditable="false" data-primary="attributions for AI-generated code" data-type="indexterm" id="id1040"/> Some open source projects restrict or even forbid AI contributions due to IP concerns, so check the contributor guidelines before using AI. Being transparent with maintainers if a patch was AI-generated helps them evaluate it, especially if licensing is a worry.</p>

<p>In fact, some highly regulated industries require software vendors to disclose any AI use for auditing purposes. <a href="https://oreil.ly/wDNKs">The EU’s AI Act</a> mandates transparency <a contenteditable="false" data-primary="EU's AI Act" data-type="indexterm" id="id1041"/>for automated decision making that affects individuals (such as credit-scoring algorithms). If vibe coding leads to such systems, it becomes a legal/ethical necessity to inform users that “recommendations are generated automatically and may reflect patterns in data.”</p>

<p>Similarly, if your product feeds user data or proprietary data like user-provided code examples into an AI model to fine-tune it and help program its analysis, you might need to say in the privacy policy that user data may be used with permission to improve AI models (as always, do consult a lawyer for legal matters). Transparency intersects with privacy here.</p>

<p>It’s also just generally ethical to acknowledge the tools and sources you use.<a contenteditable="false" data-primary="sources, acknowledging for AI-generated code" data-type="indexterm" id="id1042"/><a contenteditable="false" data-primary="tools" data-secondary="acknowledging use of in AI-generated code" data-type="indexterm" id="id1043"/> If 30% of your code was generated by Copilot, it’s fair to mention that in your documentation or internal communication—not to diminish your own role but to be honest about the process.</p>

<p class="pagebreak-before less_space">Some developers might fear admitting that AI helped, worried that it could undermine their perceived contribution or skill or be seen as “cheating.” As vibe coding becomes more normalized, this stigma should decrease; eventually, you might be seen as behind the times if you’re <em>not</em> using the AI available to you. We need to normalize AI as a tool—it’s no more “cheating” than using Stack Overflow or an IDE.</p>

<p>On the flip side, providing too many disclaimers could cause undue worry. If you tell a client, “We used AI to code this product,” they might question its safety (even if that’s due to misconceptions). It’s important how you phrase it. Emphasize quality measures in the same breath: “We utilized advanced coding assistants to speed up development, and all AI-generated code was rigorously reviewed and tested to meet our quality standards.”</p>

<p>In sum, transparency and attribution foster trust and community values. They ensure that credit flows to human creators and that we remain honest about how our software is built. It’s akin to an artist listing their tools or inspirations; it doesn’t diminish the art; it contextualizes it. If, like me, you want vibe coding to be accepted widely, being open about using AI and how you mitigate its risks is important.<a contenteditable="false" data-primary="ethical implications of vibe coding" data-secondary="transparency and attribution" data-startref="ix_ethitrnsatt" data-type="indexterm" id="id1044"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Bias and Fairness"><div class="sect1" id="ch09_bias_and_fairness_1752630044849293">
<h1>Bias and Fairness</h1>

<p>As you know well by this point in the book, AI models’ output reflects the data they’re trained on. <a contenteditable="false" data-primary="ethical implications of vibe coding" data-secondary="bias and fairness" data-type="indexterm" id="ix_ethibifa"/><a contenteditable="false" data-primary="bias in AI output" data-type="indexterm" id="ix_bias"/>If that data contains biases or exclusionary patterns, the models can produce outputs that are biased or unfair.<a contenteditable="false" data-primary="fairness in AI-generated output" data-type="indexterm" id="ix_fair"/></p>

<p>You might ask: “How can code be biased? It’s not like an LLM is making hiring decisions or something.” But bias can creep into your coding in subtle ways:</p>

<ul>
	<li>
	<p>Code often reflects assumptions on its creators’ part. User-facing text or content the AI generates might reflect cultural biases or insensitive language present in its training data.<a contenteditable="false" data-primary="cultural biases" data-type="indexterm" id="id1045"/> For instance, <a href="https://oreil.ly/d8wxO">Microsoft’s Tay</a>, an early chatbot in 2016, infamously learned to parrot racist and misogynistic slurs from Twitter interactions within hours of launch.</p>
	</li>
	<li>
	<p>Assumptions can also be geared toward specific cultural norms, like a middle-class North American lifestyle (such as assuming car ownership or universal access to certain technologies). A notable example of unexamined assumptions leading to exclusionary products was the initial <a href="https://oreil.ly/67sZG">2014 release of Apple’s Health app</a>, which lacked a period tracker—a significant oversight likely stemming from a lack of diversity and perspective on the design team. Even in example code, comments, or synthetic data, the model might always use <em>he/him</em> pronouns, reinforcing gender bias.</p>
	</li>
	<li>
	<p>It is well known that code repositories and the broader software development landscape predominantly reflect Western perspectives and English speakers. As a result, an AI trained <a contenteditable="false" data-primary="internationalization, AI model training and" data-type="indexterm" id="id1046"/>on these repositories might overlook crucial internationalization aspects, such as proper support for Unicode and multibyte characters (essential for languages like Chinese, Japanese, Korean, Arabic, Hindi, and many others using non-Latin or syllabary scripts), or it might default to English-centric examples for things like type names. Developers must bring awareness and design and code for internationalization, even if the AI doesn’t spontaneously <span class="keep-together">do so.</span></p>
	</li>
	<li>
	<p>If writing algorithms, be wary <a contenteditable="false" data-primary="race, gender, and age, variables in algorithms" data-type="indexterm" id="id1047"/>of certain variables like race, gender, age, etc. The AI might not spontaneously include them unless asked, but if it hallucinates some criteria or if you’re using an AI like Code Assistant on a dataset, apply fairness constraints; the AI won’t inherently know the moral or legal context.</p>
	</li>
</ul>

<p>Beyond just coding, models can mirror <em>data bias</em> in their content domain: the historical biases present in their training data.<a contenteditable="false" data-primary="data bias" data-type="indexterm" id="id1048"/> For example, consider an AI tasked with writing code for a credit-scoring algorithm for loan approvals. In the United States, credit scoring systems have a documented history of reflecting and perpetuating racial biases.<a contenteditable="false" data-primary="racial biases" data-type="indexterm" id="id1049"/> These biases stem from historical practices like redlining and other forms of systemic discrimination that have had lasting financial repercussions, particularly for Black communities and other marginalized groups. (See Richard Rothstein’s <em>The Color of Law</em> [Economic Policy Institute, 2017] for a comprehensive history of how government policies segregated America.)</p>

<p>If the training data reflects these historical biases, the AI might incorporate discriminatory variables, such as using zip codes (which can be a proxy for racial demographics due to segregated housing patterns) or other seemingly neutral data points that correlate with protected characteristics. If not properly guided, the AI might produce code that leads banks to make unfair lending decisions, thus perpetuating historical inequalities and affecting real people’s lives. Similar issues arise in areas like predictive policing algorithms, where historical arrest data (itself potentially biased) can lead to AI systems that <a href="https://oreil.ly/H4rmr">disproportionately target certain communities</a>.</p>

<p>Similarly, if you’re using specialized models (like an AI code assistant fine-tuned for, say, medical software), ensure the model isn’t locked into biases from that domain’s data. <a contenteditable="false" data-primary="gender bias" data-type="indexterm" id="id1050"/>For example, historically, some medical guidelines were biased by research studies that predominantly used male subjects, leading to misdiagnoses or less effective treatments for other genders. If AI is recommending code or solutions for medical diagnostics, you need to double-check that it doesn’t inadvertently encode those <span class="keep-together">biases</span>.</p>

<p>There are tools emerging to detect bias in AI outputs, though these are more common in GPT models used to generate content, and AI providers themselves attempt to filter overtly biased or toxic outputs. Code-oriented AIs rarely produce hate speech spontaneously, but it’s good that they have content filters for it. Building in ethical constraints means, in many AI tools, that if a user tries to get the AI to create malware or discriminatory algorithms, it will refuse. Don’t try to break those filters to get unethical outputs.</p>

<p>There are lots of other ways to recognize and mitigate bias at different stages of the development process, though. These include:</p>

<dl>
	<dt>Testing with diverse examples</dt>
	<dd>
	<p>If your AI generates user-facing components or logic that deals with human-related data, test it with diverse inputs.<a contenteditable="false" data-primary="human-related data, testing with diverse inputs" data-type="indexterm" id="id1051"/> For example, if an AI-generated form validation expects “First Name” and “Last Name,” does it allow single names, which are common in some cultures? If not, that’s a bias in assumption. If it generates sample usernames, are they all like “JohnDoe”? If so, consider incorporating more diversity in the examples.</p>
	</dd>
	<dt>Prompting for inclusivity</dt>
	<dd>
	<p>You can explicitly instruct the<a contenteditable="false" data-primary="inclusivity, prompting for" data-type="indexterm" id="id1052"/> AI to be neutral or inclusive: “Generate examples using a variety of names from different cultures.” If it always refers to the user as “he,” you might prompt:</p>
	</dd>
	<dd>
	<blockquote>
	<p>Avoid gendered language in this code comment; use neutral phrasing or they/them pronouns.</p>
	</blockquote>
	</dd>
	<dd>
	<p>Also, be cautious about jokes or examples the AI might produce that could be culturally insensitive; you can prompt it to use a professional tone to avoid that. The AI will usually comply. It doesn’t have an agenda; it just outputs what seems normal to it, unless told otherwise. We shape that “normal.”</p>
	</dd>
	<dt>Hiring diverse teams</dt>
	<dd>
	<p>Having a diverse team review outputs can catch issues.<a contenteditable="false" data-primary="diverse teams, hiring" data-type="indexterm" id="id1053"/> For example, someone might say, “Hey, our AI always picks variable names like foo/bar, which is fine, but in documentation, all of its personas are male-typed.” Then you can correct that systematically. If all developers are from similar backgrounds, they might not catch a subtle bias. If possible, involve people from underrepresented groups—or at least consider their perspectives—when reviewing AI usage guidelines.</p>
	</dd>
</dl>

<p>In summary, bias and fairness are about using vibe-coding tools to produce code that is fair to users of all backgrounds and that doesn’t reflect—or, worse, perpetuate—historical discrimination. The way we use these tools in teams should also be fair to developers and other colleagues of varying levels and backgrounds. See <a data-type="xref" href="ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362">Chapter 4</a> for a discussion of the ethical implications of how AI tools are changing workplaces, especially for junior developers.</p>
</div></section>

<section class="pagebreak-before" data-type="sect1" data-pdf-bookmark="Golden Rules for Responsible AI Use"><div class="sect1" id="ch09_golden_rules_for_responsible_ai_use_1752630044849350">
<h1 class="less_space">Golden Rules for Responsible AI Use</h1>

<p>Bringing <a contenteditable="false" data-primary="fairness in AI-generated output" data-startref="ix_fair" data-type="indexterm" id="id1054"/>together a lot of<a contenteditable="false" data-primary="bias in AI output" data-startref="ix_bias" data-type="indexterm" id="id1055"/> what <a contenteditable="false" data-primary="ethical implications of vibe coding" data-secondary="bias and fairness" data-startref="ix_ethibifa" data-type="indexterm" id="id1056"/>we’ve covered, it’s worth <a contenteditable="false" data-primary="responsible AI use, golden rules for" data-type="indexterm" id="ix_respAI"/>articulating a set of responsible practices for vibe coding:</p>

<ol>
	<li>
	<p><em>Always keep a human in the loop.</em><a contenteditable="false" data-primary="human oversight of AI-assisted coding" data-type="indexterm" id="id1057"/></p>
	<p>Again: never let the AI work unsupervised. Responsible AI-assisted dev means you, the developer, are reviewing every line and making decisions, not deploying raw AI output without human validation.</p>
	</li>
	<li>
	<p><em>Take responsibility for your code.</em></p>
	<p>If something goes wrong, it’s not the AI’s fault—it’s the development team’s responsibility. Keeping that mindset avoids complacency. Be prepared to justify your code, whether you wrote it from scratch or accepted AI code. If someone asks you, “Why does the code do this?” don’t say, “I don’t know; Copilot did that.” That’s why one of <a data-type="xref" href="ch03.html#ch03_the_70_problem_ai_assisted_workflows_that_actual_1752630043200933">Chapter 3</a>’s golden rules is “Never commit code you don’t fully understand.” That’s responsible engineering.</p>
	</li>
	<li>
	<p><em>Protect users’ privacy and ask for their consent.</em></p>
	<p>Ethically, you owe it <a contenteditable="false" data-primary="secret data, keeping secret" data-type="indexterm" id="id1058"/>to users and your company to keep their secret data secret. When using AI tools, especially cloud-based ones, be careful not to expose sensitive data in your prompts or conversations. For instance, if you’re debugging an issue with a user database, don’t feed actual user records to ChatGPT. Use sanitized or synthetic data instead.</p>
	<p>Many tools now allow users (or at least business users) to opt out of having their input data used for training. <a contenteditable="false" data-primary="privacy concerns with user data" data-type="indexterm" id="id1059"/>If you’re an enterprise user, use those settings or use on-prem solutions for sensitive code. If you do feed any user data to a model, or if any AI functionality directly touches users (like a chatbot in your app that uses an LLM), get users’ consent and allow them to opt out if appropriate. A warning like “This feature uses an AI service; your input will be sent to it for processing” is transparent and lets privacy-conscious users decide for themselves.</p>
	</li>
	<li>
	<p><em>Comply with laws and regulations.</em></p>
	<p>Keep an eye on legal requirements around AI, which <a contenteditable="false" data-primary="legal requirements around AI" data-type="indexterm" id="id1060"/>are constantly evolving. For instance, data protection laws <a contenteditable="false" data-primary="data protection laws" data-type="indexterm" id="id1061"/>like the EU’s General Data Protection Regulation (GDPR) and AI Act consider some AI outputs as personal data if they include any personal data. Training a model on users’ data might require those users’ consent. Regulatory bodies may classify code generation as “general AI” and impose transparency or risk management obligations. Stay informed and work closely with your legal and compliance professionals to avoid breaking any <span class="keep-together">regulations</span>.</p>
	<p>While this should go without saying, do not use AI to generate malware, exploit code without ethical justification, or automate unethical or illegal practices.<sup><a data-type="noteref" id="id1062-marker" href="ch09.html#id1062">2</a></sup> While an AI could probably write a very effective phishing email or code injection attack, using it for that purpose violates ethics, the laws of most countries, and likely the AI’s terms of service. Focus on constructive use.</p>
	</li>
	<li>
	<p><em>Foster a responsible AI culture in your organization.</em></p>
	<p>If your team adopts vibe coding, encourage discussions about ethics and provide relevant ethics training. Consider having developers and code reviewers use a brief checklist like the one in <a data-type="xref" href="#ch09_figure_1_1752630044839745">Figure 9-1</a>.</p>
<figure><div id="ch09_figure_1_1752630044839745" class="figure"><img src="assets/bevc_0901.png" width="626" height="495"/>
<h6><span class="label">Figure 9-1. </span>Responsible AI development checklist: essential validation steps including intellectual property review, bias assessment, and security audits before integrating AI-generated code into production systems.</h6>
</div></figure>

	<p>Everyone should feel responsible for ethical AI use; it’s a collective effort, not just the burden of the individual using the tool at any given moment. To formalize this, consider designating an “ethics champion” or a small ethics committee within your team or organization. This individual or group wouldn’t be the sole owner of ethics (as that responsibility remains shared), but they would take the lead on:</p>
		<ul>
		<li>
		<p>Staying abreast of the latest developments in AI ethics, emerging best practices, and new regulatory landscapes</p>
		</li>
		<li>
		<p>Facilitating discussions about ethical considerations in specific projects</p>
		</li>
		<li>
		<p>Championing the integration of ethical principles into the development <span class="keep-together">lifecycle</span></p>
		</li>
		<li>
		<p>Helping to curate and disseminate relevant resources and training materials to the broader team</p>
		</li>
		<li>
		<p>Acting as a point of contact for team members who have ethical questions or concerns</p>
		</li>
	</ul>

	<p>Since this field is moving incredibly fast, it’s crucial to work as a team to stay updated on new versions of AI tools and their capabilities, limitations, and evolving best practices for responsible use.</p>

	<p>Since this field is moving fast, work as a team to stay updated on new versions of AI tools and best practices.<a contenteditable="false" data-primary="model cards" data-type="indexterm" id="id1063"/> One important concept to integrate into your workflows is the use of model cards. <em>Model cards</em> are essentially standardized documents that provide transparency about a machine learning model. Think of them as nutrition labels for AI models. They typically include details about:</p>

	<ul>
		<li>
		<p>What the model is, its version, and when it was developed</p>
		</li>
		<li>
		<p>The specific use cases the model was designed and tested for</p>
		</li>
		<li>
		<p>Scenarios where the model should not be used, due to limitations or potential for harm</p>
		</li>
		<li>
		<p>How well the model performs on various benchmarks, including evaluations for fairness and bias across different demographic groups</p>
		</li>
		<li>
		<p>Information about the datasets used to train the model, including any known limitations or biases in the data</p>
		</li>
		<li>
		<p>Potential risks and societal implications and any mitigation strategies employed</p>
		</li>
	</ul>

	<p>Whenever you are using a pretrained model or evaluating a model for use, look for its model card. If you are fine-tuning or developing models, creating your own model cards is a best practice.<a contenteditable="false" data-primary="responsible AI use, golden rules for" data-secondary="responsible AI checklist" data-type="indexterm" id="id1064"/></p>
	</li>
	</ol>

<ol start="6">
	<li>
	<p><em>Create guardrails and safety nets.</em></p>

	<p>Practicing responsible design means that your AI-generated systems should have safety nets. For example, if AI suggests an out-of-bounds index fix that might mask an underlying issue, it’s better for the system to fail safely than to cause silent errors. If an AI-generated recommendation system might be wrong, providing ways for users to correct or override it shows respect for their human agency. Strive to build systems that degrade gracefully if AI components misbehave.</p>
	</li>
	<li>
	<p><em>Document AI usage decisions within your team.</em></p>

	<p>Keep an internal log of why you used certain AI suggestions (or didn’t): “We tried AI for module X, but it tended to produce too much duplicate code, so we wrote that part manually.” This can help you refine your processes, provide context to new team members about AI’s role in the codebase’s history, and augment your team’s collective memory. It can also be useful during audits.</p>
	</li>
	<li>
	<p><em>Proactively work to avoid bias, discrimination, and <a contenteditable="false" data-primary="unfairness resulting from AI usage" data-type="indexterm" id="id1065"/>unfairness.</em></p>

	<p>Be vigilant for<a contenteditable="false" data-primary="bias in AI output" data-type="indexterm" id="id1066"/> signs that your AI usage could lead to <a contenteditable="false" data-primary="discrimination resulting from AI usage" data-type="indexterm" id="id1067"/>discrimination, and work to avoid such situations before they happen. For example, if your app is global, is your AI multilingual or does it favor those who speak English? Do all of your team members have equal access to AI tools and training?</p>
	</li>
</ol>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch09_responsible_ai_checklist_1752630044849406">
<h1>Responsible AI Checklist</h1>

<ol>
	<li>
	<p>Prompting and code<a contenteditable="false" data-primary="prompts" data-secondary="prompting and code generation, responsible AI" data-type="indexterm" id="id1068"/> generation (developers)</p>

	<ul class="checkbox">
		<li>
		<p>Confirm that your prompts contain no confidential or sensitive data such as client info, PII, or secrets.</p>
		</li>
		<li>
		<p>Check licensing for all output and confirm it includes no proprietary or GPL code, unless allowed. Use tools like FOSSA for scanner checks.</p>
		</li>
		<li>
		<p>Test output for bias to ensure code and comments don’t reinforce stereotypes or discrimination.</p>
		</li>
		<li>
		<p>Confirm security hygiene by prompting for safe defaults. Confirm the code avoids insecure patterns (eval, unsanitized input).</p>
		</li>
		<li>
		<p>Specify any constraints in prompts, including style, framework, performance needs, and compatibility guidelines.</p>
		</li>
	</ul>
	</li>
	<li>
	<p>Code review <a contenteditable="false" data-primary="code reviews" data-secondary="in responsible AI" data-secondary-sortas="responsible" data-type="indexterm" id="id1069"/>checks (developers and code reviewers)</p>

	<ul class="checkbox">
		<li>
		<p>Verify that no embedded copyrighted material is used in the code unless licensed.</p>
		</li>
		<li>
		<p>Confirm that attribution and credit are given when due.</p>
		</li>
		<li>
		<p>Audit the logic, language, and naming for bias and fairness—especially in user/UI-facing layers.</p>
		</li>
		<li>
		<p>Ensure that the code doesn’t facilitate harm, misuse, manipulation, or discrimination.</p>
		</li>
		<li>
		<p>Validate your input sanitization, data handling, and logging, and check for secret leaks.</p>
		</li>
		<li>
		<p>Confirm the code’s functionality and correctness via unit tests, edge cases, error handling, and test coverage.</p>
		</li>
		<li>
		<p>Check for inefficient or power-hungry patterns.</p>
		</li>
		<li>
		<p>Check dependencies to ensure they include no unvetted libraries or hidden license risks.<a contenteditable="false" data-primary="dependencies" data-secondary="checking for unvetted libraries or license risks" data-type="indexterm" id="id1070"/></p>
		</li>
		<li>
		<p>Check for readability and maintainability: the code should follow style guides and use clear naming conventions.</p>
		</li>
		<li>
		<p>Check that any unused code has been removed.</p>
		</li>
		<li>
		<p>Confirm that code comments explain the code’s intent, especially for AI-generated logic.</p>
		</li>
		<li>
		<p>Confirm that your code-review feedback is respectful, specific, and empathetic.</p>
		</li>
	</ul>
	</li>
	<li>
	<p>Governance and process (organization)</p>

	<ul class="checkbox">
		<li>
		<p>Confirm that integrated license scanners, audit logs, and provenance tracking are in place.<a contenteditable="false" data-primary="governance and process in responsible AI" data-type="indexterm" id="id1071"/></p>
		</li>
		<li>
		<p>Provide training in ethics and AI-assisted coding, and share updates regularly.</p>
		</li>
		<li>
		<p>Maintain a vetted list of AI tools; prohibit unapproved or high-risk ones.</p>
		</li>
		<li>
		<p>Put an incident process in place, with escalation channels and whistleblower options for anyone who discovers unethical code.</p>
		</li>
		<li>
		<p>Monitor responsible AI metrics, such as bias incidents, security findings, and license violations. Maintain a checklist of these metrics and revise it periodically.</p>
		</li>
		<li>
		<p>Solicit and listen to community feedback. Include diverse perspectives via retrospective meetings or external audits.</p>
		</li>
	</ul>
	</li>
</ol>

<h1>How to Use This Checklist</h1>

<ul>
	<li>
	<p>Customize this list to <a contenteditable="false" data-primary="responsible AI use, golden rules for" data-secondary="using the responsible AI checklist" data-type="indexterm" id="id1072"/>include questions specific to your organization and business domain, as well as your team’s tech, risk tolerance, and values.</p>
	</li>
	<li>
	<p>Start small: begin with key questions like “Did we avoid sensitive data?” and “Did we scan for licenses?”</p>
	</li>
	<li>
	<p>Integrate checks and checklists into your workflow via PR templates, CI pipelines, and code-review tools.</p>
	</li>
	<li>
	<p>Schedule reviews of this checklist every quarter or after major incidents. Use these reviews to iterate on the list, adding new items or deleting unneeded ones.</p>
	</li>
	<li>
	<p>Treat this checklist not as a rigid rulebook but as a conversation starter, just as pilots and surgeons do with their checklists.</p>
	</li>
</ul>
</div></aside>

<p>As the AI landscape continues changing and growing, the software industry is likely to introduce AI standards or certifications. It’s early, but your company could even help shape those guidelines by engaging in standardization efforts, like IEEE or ISO working groups on AI software engineering. Ethically, it’s better for the dev community to help set the rules than to leave it solely to regulators or the courts.<a contenteditable="false" data-primary="responsible AI use, golden rules for" data-startref="ix_respAI" data-type="indexterm" id="id1073"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Summary and Next Steps"><div class="sect1" id="ch09_summary_and_next_steps_1752630044849471">
<h1>Summary and Next Steps</h1>

<p>Responsible vibe coding means integrating AI into the software development lifecycle in a way that respects all stakeholders: original creators (by respecting their IP), colleagues (through transparency and fairness), users (through privacy, security, and fairness in outcomes), and society (by not letting misuse cause harm). It’s about leveraging AI’s strengths while diligently guarding against its weaknesses.</p>

<p>I’ve often said that vibe coding is not an excuse for low-quality work. It’s not an excuse for ethical shortcuts either. As the humans in charge, developers must ensure that speed doesn’t compromise values.<a contenteditable="false" data-primary="ethical implications of vibe coding" data-startref="ix_ethi" data-type="indexterm" id="id1074"/></p>

<p>Next, <a data-type="xref" href="ch10.html#ch10_autonomous_background_coding_agents_1752630045087844">Chapter 10</a> looks at a new technology that’s changing the way we work with AI models: autonomous coding agents.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1032"><sup><a href="ch09.html#id1032-marker">1</a></sup> Case information can often be found on <a href="https://oreil.ly/BdDiV">court dockets</a>, like those for the US District Court for the Northern District of California and the Ninth Circuit Court of Appeals, or through <a href="https://oreil.ly/AZrc-">legal news outlets and case trackers</a>.</p><p data-type="footnote" id="id1062"><sup><a href="ch09.html#id1062-marker">2</a></sup> There are some ethically justified exceptions. Penetration testers and security researchers can ethically use AI to find vulnerabilities that should be fixed, as long as they work under responsible disclosure protocols.</p></div></div></section></div></div></body></html>