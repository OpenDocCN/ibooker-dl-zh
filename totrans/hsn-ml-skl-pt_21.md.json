["```py\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=1000)\n```", "```py\n>>> obs, info = env.reset(seed=42)\n>>> obs\narray([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)\n>>> info\n{}\n```", "```py\n>>> img = env.render()\n>>> img.shape  # height, width, channels (3 = Red, Green, Blue)\n(400, 600, 3)\n```", "```py\n>>> env.action_space\nDiscrete(2)\n```", "```py\n>>> action = 1  # accelerate right\n>>> obs, reward, done, truncated, info = env.step(action)\n>>> obs\narray([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)\n>>> reward, done, truncated, info\n(1.0, False, False, {})\n```", "```py\ndef basic_policy(obs):\n    angle = obs[2]\n    return 0 if angle < 0 else 1  # go left if leaning left, otherwise go right\n\ntotals = []\nfor episode in range(500):\n    total_rewards = 0\n    obs, info = env.reset(seed=episode)\n    while True:  # no risk of infinite loop: will be truncated after 1000 steps\n        action = basic_policy(obs)\n        obs, reward, done, truncated, info = env.step(action)\n        total_rewards += reward\n        if done or truncated:\n            break\n\n    totals.append(total_rewards)\n```", "```py\n>>> import numpy as np\n>>> np.mean(totals), np.std(totals), min(totals), max(totals)\n(np.float64(41.698), np.float64(8.389445512070509), 24.0, 63.0)\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(4, 5), nn.ReLU(), nn.Linear(5, 1))\n\n    def forward(self, state):\n        return self.net(state)\n```", "```py\ndef choose_action(model, obs):\n    state = torch.as_tensor(obs)\n    logit = model(state)\n    dist = torch.distributions.Bernoulli(logits=logit)\n    action = dist.sample()\n    log_prob = dist.log_prob(action)\n    return int(action.item()), log_prob\n```", "```py\ndef compute_returns(rewards, discount_factor):\n    returns = rewards[:]  # copy the rewards\n    for step in range(len(returns) - 1, 0, -1):\n        returns[step - 1] += returns[step] * discount_factor\n\n    return torch.tensor(returns)\n```", "```py\n>>> compute_returns([10, 0, -50], discount_factor=0.8)\ntensor([-22., -40., -50.])\n```", "```py\ndef run_episode(model, env, seed=None):\n    log_probs, rewards = [], []\n    obs, info = env.reset(seed=seed)\n    while True:  # the environment will truncate the episode if it is too long\n        action, log_prob = choose_action(model, obs)\n        obs, reward, done, truncated, _info = env.step(action)\n        log_probs.append(log_prob)\n        rewards.append(reward)\n        if done or truncated:\n            return log_probs, rewards\n```", "```py\ndef train_reinforce(model, optimizer, env, n_episodes, discount_factor):\n    for episode in range(n_episodes):\n        seed = torch.randint(0, 2**32, size=()).item()\n        log_probs, rewards = run_episode(model, env, seed=seed)\n        returns = compute_returns(rewards, discount_factor)\n        std_returns = (returns - returns.mean()) / (returns.std() + 1e-7)\n        losses = [-logp * rt for logp, rt in zip(log_probs, std_returns)]\n        loss = torch.cat(losses).sum()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"\\rEpisode {episode + 1}, Reward: {sum(rewards):.2f}\", end=\" \")\n```", "```py\ntorch.manual_seed(42)\nmodel = PolicyNetwork()\noptimizer = torch.optim.NAdam(model.parameters(), lr=0.06)\ntrain_reinforce(model, optimizer, env, n_episodes=200, discount_factor=0.95)\n```", "```py\ntransition_probabilities = [  # shape=[s, a, s']\n    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n    [None, [0.8, 0.1, 0.1], None]\n]\nrewards = [  # shape=[s, a, s']\n    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n]\npossible_actions = [[0, 1, 2], [0, 2], [1]]\n```", "```py\nQ_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\nfor state, actions in enumerate(possible_actions):\n    Q_values[state, actions] = 0.0  # for all possible actions\n```", "```py\ngamma = 0.90  # the discount factor\n\nfor iteration in range(50):\n    Q_prev = Q_values.copy()\n    for s in range(3):\n        for a in possible_actions[s]:\n            Q_values[s, a] = np.sum([\n                    transition_probabilities[s][a][sp]\n                    * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n                for sp in range(3)])\n```", "```py\n>>> Q_values\narray([[18.91891892, 17.02702702, 13.62162162],\n [ 0\\.        ,        -inf, -4.87971488],\n [       -inf, 50.13365013,        -inf]])\n```", "```py\n>>> Q_values.argmax(axis=1)  # optimal action for each state\narray([0, 0, 1])\n```", "```py\ndef step(state, action):\n    probas = transition_probabilities[state][action]\n    next_state = np.random.choice([0, 1, 2], p=probas)\n    reward = rewards[state][action][next_state]\n    return next_state, reward\n```", "```py\ndef exploration_policy(state):\n    return np.random.choice(possible_actions[state])\n```", "```py\nalpha0 = 0.05  # initial learning rate\ndecay = 0.005  # learning rate decay\ngamma = 0.90  # discount factor\nstate = 0  # initial state\n\nfor iteration in range(10_000):\n    action = exploration_policy(state)\n    next_state, reward = step(state, action)\n    next_value = Q_values[next_state].max()  # greedy policy at the next step\n    alpha = alpha0 / (1 + iteration * decay)\n    Q_values[state, action] *= 1 - alpha\n    Q_values[state, action] += alpha * (reward + gamma * next_value)\n    state = next_state\n```", "```py\nclass DQN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(),\n                                 nn.Linear(32, 32), nn.ReLU(),\n                                 nn.Linear(32, 2))\n\n    def forward(self, state):\n        return self.net(state)\n```", "```py\ndef choose_dqn_action(model, obs, epsilon=0.0):\n        if torch.rand(()) < epsilon:  # epsilon greedy policy\n            return torch.randint(2, size=()).item()\n        else:\n            state = torch.as_tensor(obs)\n            Q_values = model(state)\n            return Q_values.argmax().item()  # optimal according to the DQN\n```", "```py\ndef sample_experiences(replay_buffer, batch_size):\n    indices = torch.randint(len(replay_buffer), size=[batch_size])\n    batch = [replay_buffer[index] for index in indices.tolist()]\n    return [to_tensor([exp[index] for exp in batch]) for index in range(6)]\n\ndef to_tensor(data):\n    array = np.stack(data)\n    dtype = torch.float32 if array.dtype == np.float64 else None\n    return torch.as_tensor(array, dtype=dtype)\n```", "```py\ndef play_and_record_episode(model, env, replay_buffer, epsilon, seed=None):\n    obs, _info = env.reset(seed=seed)\n    total_rewards = 0\n    model.eval()\n    with torch.no_grad():\n        while True:\n            action = choose_dqn_action(model, obs, epsilon)\n            next_obs, reward, done, truncated, _info = env.step(action)\n            experience = (obs, action, reward, next_obs, done, truncated)\n            replay_buffer.append(experience)\n            total_rewards += reward\n            if done or truncated:\n                return total_rewards\n            obs = next_obs\n```", "```py\ndef dqn_training_step(model, optimizer, criterion, replay_buffer, batch_size,\n                      discount_factor):\n    experiences = sample_experiences(replay_buffer, batch_size)\n    state, action, reward, next_state, done, truncated = experiences\n    with torch.inference_mode():\n        next_Q_value = model(next_state)\n\n    max_next_Q_value, _ = next_Q_value.max(dim=1)\n    running = (~(done | truncated)).float()  # 0 if s' is over, 1 if running\n    target_Q_value = reward + running * discount_factor * max_next_Q_value\n    all_Q_values = model(state)\n    Q_value = all_Q_values.gather(dim=1, index=action.unsqueeze(1))\n    loss = criterion(Q_value, target_Q_value.unsqueeze(1))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```", "```py\nfrom collections import deque\n\ndef train_dqn(model, env, replay_buffer, optimizer, criterion, n_episodes=800,\n              warmup=30, batch_size=32, discount_factor=0.95):\n    totals = []\n    for episode in range(n_episodes):\n        epsilon = max(1 - episode / 500, 0.01)\n        seed = torch.randint(0, 2**32, size=()).item()\n        total_rewards = play_and_record_episode(model, env, replay_buffer,\n                                                epsilon, seed=seed)\n        print(f\"\\rEpisode: {episode + 1}, Rewards: {total_rewards}\", end=\" \")\n        totals.append(total_rewards)\n        if episode >= warmup:\n            dqn_training_step(model, optimizer, criterion, replay_buffer,\n                              batch_size, discount_factor)\n    return totals\n\ntorch.manual_seed(42)\ndqn = DQN()\noptimizer = torch.optim.NAdam(dqn.parameters(), lr=0.03)\nmse = nn.MSELoss()\nreplay_buffer = deque(maxlen=100_000)\ntotals = train_dqn(dqn, env, replay_buffer, optimizer, mse)\n```", "```py\nclass ActorCritic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.body = nn.Sequential(nn.Linear(4, 32), nn.ReLU(),\n                                  nn.Linear(32, 32), nn.ReLU())\n        self.actor_head = nn.Linear(32, 1)  # outputs action logits\n        self.critic_head = nn.Linear(32, 1)  # outputs state values\n\n    def forward(self, state):\n        features = self.body(state)\n        return self.actor_head(features), self.critic_head(features)\n```", "```py\ndef choose_action_and_evaluate(model, obs):\n    state = torch.as_tensor(obs)\n    logit, state_value = model(state)\n    dist = torch.distributions.Bernoulli(logits=logit)\n    action = dist.sample()\n    log_prob = dist.log_prob(action)\n    return int(action.item()), log_prob, state_value\n```", "```py\ndef ac_training_step(optimizer, criterion, state_value, target_value, log_prob,\n                     critic_weight):\n    td_error = target_value - state_value\n    actor_loss = -log_prob * td_error.detach()\n    critic_loss = criterion(state_value, target_value)\n    loss = actor_loss + critic_weight * critic_loss\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```", "```py\ndef get_target_value(model, next_obs, reward, done, truncated, discount_factor):\n    with torch.inference_mode():\n        _, _, next_state_value = choose_action_and_evaluate(model, next_obs)\n\n    running = 0.0 if (done or truncated) else 1.0\n    target_value = reward + running * discount_factor * next_state_value\n    return target_value\n```", "```py\ndef run_episode_and_train(model, optimizer, criterion, env, discount_factor,\n                          critic_weight, seed=None):\n    obs, _info = env.reset(seed=seed)\n    total_rewards = 0\n    while True:\n        action, log_prob, state_value = choose_action_and_evaluate(model, obs)\n        next_obs, reward, done, truncated, _info = env.step(action)\n        target_value = get_target_value(model, next_obs, reward, done,\n                                        truncated, discount_factor)\n        ac_training_step(optimizer, criterion, state_value, target_value,\n                         log_prob, critic_weight)\n        total_rewards += reward\n        if done or truncated:\n            return total_rewards\n        obs = next_obs\n```", "```py\ndef train_actor_critic(model, optimizer, criterion, env, n_episodes=400,\n                       discount_factor=0.95, critic_weight=0.3):\n    totals = []\n    model.train()\n    for episode in range(n_episodes):\n        seed = torch.randint(0, 2**32, size=()).item()\n        total_rewards = run_episode_and_train(model, optimizer, criterion, env,\n                                              discount_factor, critic_weight,\n                                              seed=seed)\n        totals.append(total_rewards)\n        print(f\"\\rEpisode: {episode + 1}, Rewards: {total_rewards}\", end=\" \")\n\n    return totals\n```", "```py\ntorch.manual_seed(42)\nac_model = ActorCritic()\noptimizer = torch.optim.NAdam(ac_model.parameters(), lr=1.1e-3)\ncriterion = nn.MSELoss()\ntotals = train_actor_critic(ac_model, optimizer, criterion, env)\n```", "```py\nimport ale_py\n\nale = ale_py.ALEInterface()\n```", "```py\nfrom stable_baselines3.common.env_util import make_atari_env\n\nenvs = make_atari_env(\"BreakoutNoFrameskip-v4\", n_envs=4)\nobs = envs.reset()  # a 4 × 84 × 84 × 1 NumPy array (note: no info dict)\n```", "```py\nfrom stable_baselines3.common.vec_env import VecFrameStack\n\nenvs_stacked = VecFrameStack(envs, n_stack=4)\nobs = envs_stacked.reset()  # returns a 4 × 84 × 84 × 4 NumPy array\n```", "```py\nfrom stable_baselines3 import PPO\n\nppo_model = PPO(\"CnnPolicy\", envs_stacked, device=device, learning_rate=2.5e-4,\n                batch_size=256, n_steps=256, n_epochs=4, clip_range=0.1,\n                vf_coef=0.5, ent_coef=0.01, gamma=0.99, verbose=0)\n```", "```py\nfrom stable_baselines3.common.callbacks import CheckpointCallback\n\ncb = CheckpointCallback(save_freq=100_000, save_path=\"my_ppo_breakout.ckpt\")\nppo_model.learn(total_timesteps=30_000_000, progress_bar=True, callback=cb)\nppo_model.save(\"my_ppo_breakout\")  # save the final model\n```", "```py\ntensorboard_logdir = \"my_ppo_breakout_tensorboard\"  # path to the log directory\n%tensorboard --logdir={tensorboard_logdir} --port 6006\n```", "```py\nppo_model = PPO(\"CnnPolicy\", [...], tensorboard_log=tensorboard_logdir)\n```", "```py\nppo_model = PPO.load(\"my_ppo_agent_breakout\")  # or load the best checkpoint\neval_env = make_atari_env(\"BreakoutNoFrameskip-v4\", n_envs=1, seed=42)\neval_stacked = VecFrameStack(eval_env, n_stack=4)\nframes = []\nobs = eval_stacked.reset()\nfor _ in range(5000):  # some limit in case the agent never loses\n    frames.append(eval_stacked.render())\n    action, _ = ppo_model.predict(obs, deterministic=True) # for reproducibility\n    obs, reward, done, info = eval_stacked.step(action)\n    if done[0]:  # note: there's no `truncated`\n        break\n\neval_stacked.close()\n```"]