- en: 9 A line-by-line implementation of attention and Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: The architecture and functionalities of encoders and decoders in Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the attention mechanism uses query, key, and value to assign weights to
    elements in a sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Transformer from scratch for language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers are advanced deep learning models that excel in handling sequence-to-sequence
    prediction challenges, outperforming older models like recurrent neural networks
    (RNNs) and convolutional neural networks (CNNs). Their strength lies in effectively
    understanding the relationships between elements in input and output sequences
    over long distances, such as two words far apart in the text. Unlike RNNs, Transformers
    are capable of parallel training, significantly cutting down training times and
    enabling the handling of vast datasets. This transformative architecture has been
    pivotal in the development of large language models (LLMs) like ChatGPT, BERT,
    and T5, marking a significant milestone in AI progress.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the introduction of Transformers in the groundbreaking 2017 paper “Attention
    Is All You Need” by a group of Google researchers,^([1](#footnote-001)) natural
    language processing (NLP) and similar tasks primarily relied on RNNs, including
    long short-term memory (LSTM) models. RNNs, however, process information sequentially,
    limiting their speed due to the inability to train in parallel and struggling
    with maintaining information about earlier parts of a sequence, thus failing to
    capture long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The revolutionary aspect of the Transformer architecture is its attention mechanism.
    This mechanism assesses the relationship between words in a sequence by assigning
    weights, determining the degree of relatedness in meaning among words based on
    the training data. This enables models like ChatGPT to comprehend relationships
    between words, thus understanding human language more effectively. The nonsequential
    processing of inputs allows for parallel training, reducing training time and
    facilitating the use of large datasets, thereby powering the rise of knowledgeable
    LLMs and the current surge in AI advancements.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will implement, line by line, the creation of a Transformer
    from the ground up, based on the paper “Attention Is All You Need.” The Transformer,
    once trained, can handle translations between any two languages (such as German
    to English or English to Chinese). In the next chapter, we’ll focus on training
    the Transformer developed here to perform English to French translations.
  prefs: []
  type: TYPE_NORMAL
- en: To build the Transformer from scratch, we’ll explore the inner workings of the
    self-attention mechanism, including the roles of query, key, and value vectors,
    and the computation of scaled dot product attention (SDPA). We’ll construct an
    encoder layer by integrating layer normalization and residual connection into
    a multihead attention layer and combining it with a feed-forward layer. We’ll
    then stack six of these encoder layers to form the encoder. Similarly, we’ll develop
    a decoder in the Transformer that is capable of generating translation one token
    at a time, based on previous tokens in the translation and the encoder’s output.
  prefs: []
  type: TYPE_NORMAL
- en: This groundwork will equip you to train the Transformer for translations between
    any two languages. In the next chapter, you’ll learn to train the Transformer
    using a dataset containing more than 47,000 English-to-French translations. You’ll
    witness the trained model translating common English phrases to French with an
    accuracy comparable to using Google Translate.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Introduction to attention and Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To grasp the concept of Transformers in machine learning, it’s essential to
    first understand the attention mechanism. This mechanism allows Transformers to
    recognize long-range dependencies between sequence elements, a feature that sets
    them apart from earlier sequence prediction models like RNNs. With this mechanism,
    Transformers can simultaneously focus on every element in a sequence, comprehending
    the context of each word.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the word “bank” to illustrate how the attention mechanism interprets
    words based on context. In the sentence “I went fishing by the river yesterday,
    remaining near the bank the whole afternoon,” the word ”bank“ is linked to “fishing”
    because it refers to the area beside a river. Here, a Transformer understands
    “bank” as part of the river’s terrain.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, in “Kate went to the bank after work yesterday and deposited a
    check there,” “bank” is connected to “check,” leading the Transformer to identify
    “bank” as a financial institution. This example showcases how Transformers discern
    word meanings based on their surrounding context.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’ll dive deeper into the attention mechanism, exploring
    how it works. This process is crucial for determining the importance, or weights,
    of various words within a sentence. After that, we’ll examine the structure of
    different Transformer models, including one that can translate between any two
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1 The attention mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attention mechanism is a method used to determine the interconnections between
    elements in a sequence. It calculates scores to indicate how one element relates
    to others in the sequence, with higher scores denoting a stronger relationship.
    In NLP, this mechanism is instrumental in linking words within a sentence meaningfully.
    This chapter will guide you through implementing the attention mechanism for language
    translation.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll construct a Transformer composed of an encoder and a decoder for that
    purpose. We’ll then train the Transformer to translate English to French in the
    next chapter. The encoder transforms an English sentence, such as “How are you?”,
    into vector representations that capture its meaning. The decoder then uses these
    vector representations to generate the French translation.
  prefs: []
  type: TYPE_NORMAL
- en: To transform the phrase “How are you?” into vector representations, the model
    first breaks it down into tokens `[how, are, you, ?]`, a process similar to what
    you have done in chapter 8\. These tokens are each represented by a 256-dimensional
    vector known as word embeddings, which capture the meaning of each token. The
    encoder also employs positional encoding, a method to determine the positions
    of tokens in the sequence. This positional encoding is added to the word embeddings
    to create input embeddings, which are then used to calculate self-attention. The
    input embedding for “How are you?” forms a tensor with dimensions (4, 256), where
    4 represents the number of tokens and 256 is the dimensionality of each embedding.
  prefs: []
  type: TYPE_NORMAL
- en: While there are different ways to calculate attention, we’ll use the most common
    method, SDPA. This mechanism is also called self-attention because the algorithm
    calculates how a word attends to all words in the sequence, including the word
    itself. Figure 9.1 provides a diagram of how to calculate SDPA.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 A diagram of the self-attention mechanism. To calculate attention,
    the input embedding X is first passed through three neural layers with weights,
    W^Q, W^K, and W^V, respectively. The outputs are query Q, key K, and value V.
    The scaled attention score is the product of Q and K divided by the square root
    of the dimension of K, d[k]. We apply the softmax function on the scaled attention
    score to obtain the attention weight. The attention is the product of the attention
    weight and value V.
  prefs: []
  type: TYPE_NORMAL
- en: The utilization of query, key, and value in calculating attention is inspired
    by retrieval systems. Consider visiting a public library to find a book. If you
    search for “machine learning in finance” in the library’s search engine, this
    phrase becomes your query. The book titles and descriptions in the library serve
    as the keys. Based on the similarity between your query and these keys, the library’s
    retrieval system suggests a list of books (values). Books containing “machine
    learning,” “finance,” or both in their titles or descriptions are likely to rank
    higher. In contrast, books unrelated to these terms will have a lower matching
    score and thus are less likely to be recommended.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate SDPA, the input embedding X is processed through three distinct
    neural network layers. The corresponding weights for these layers are W^Q, W^K,
    and W^V; each has a dimension of 256 × 256. These weights are learned from data
    during the training phase. Thus, we can calculate query Q, key K, and value V
    as Q = X * W^Q, K = X * Q^K, and V = X * W^V. The dimensions of Q, K, and V match
    those of the input embedding X, which are 4 × 256.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the retrieval system example we mentioned earlier, in the attention
    mechanism, we assess the similarities between the query and key vectors using
    the SDPA approach. SDPA involves calculating the dot product of the query (Q)
    and key (K) vectors. A high dot product indicates a strong similarity between
    the two vectors and vice versa. For instance, in the sentence “How are you?”,
    the scaled attention score is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F01_Liu_EQ01.png)'
  prefs: []
  type: TYPE_IMG
- en: '| (9.1) |'
  prefs: []
  type: TYPE_TB
- en: where d[k] represents the dimension of the key vector K, which in our case is
    256\. We scale the dot product of Q and K by the square root of d[k] to stabilize
    training. This scaling is done to prevent the dot product from growing too large
    in magnitude. The dot product between the query and key vectors can become very
    large when the dimension of these vectors (i.e., the depth of the embedding) is
    high. This is because each element of the query vector is multiplied by each element
    of the key vector, and these products are then summed up.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to apply the softmax function to these attention scores, converting
    them into attention weights. This ensures that the total attention a word gives
    to all words in the sentence sums to 100%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 Steps to calculate attention weights. The input embedding is passed
    through two neural networks to obtain query Q and key K. The scaled attention
    scores are calculated as the dot product of Q and K divided by the square root
    of the dimension of K. Finally, we apply the softmax function on the scaled attention
    scores to obtain attention weights, which demonstrate how each element is related
    to all other elements in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 shows how this is done. For the sentence “How are you?”, the attention
    weights form a 4 × 4 matrix, which shows how each token in `["How", "are," "you,"
    "?"]` is related to all other tokens (including itself). The numbers in figure
    9.2 are made-up numbers to illustrate the point. For example, the first row in
    the attention weights shows that the token `"How"` gives 10% of its attention
    to itself and 40%, 40%, and 10% to the other three tokens, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final attention is then calculated as the dot product of these attention
    weights and the value vector V (also illustrated in figure 9.3):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F02_Liu_EQ02.png)'
  prefs: []
  type: TYPE_IMG
- en: '| (9.2) |'
  prefs: []
  type: TYPE_TB
- en: '![](../../OEBPS/Images/CH09_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Use attention weights and the value vector to calculate the attention
    vector. The input embedding is passed through a neural network to obtain value
    V. The final attention is the dot product of the attention weights that we calculated
    earlier and the value vector V.
  prefs: []
  type: TYPE_NORMAL
- en: This output also maintains a dimension of 4 × 256, consistent with our input
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the process begins with the input embedding X of the sentence
    “How are you?”, which has a dimension of 4 × 256. This embedding captures the
    meanings of the four individual tokens but lacks contextualized understanding.
    The attention mechanism ends with the output `attention(Q,K,V)`, which maintains
    the same dimension of 4 × 256. This output can be viewed as a contextually enriched
    combination of the original four tokens. The weighting of the original tokens
    varies based on the contextual relevance of each token, granting more significance
    to words that are more important within the sentence’s context. Through this procedure,
    the attention mechanism transforms vectors representing isolated tokens into vectors
    imbued with contextualized meanings, thereby extracting a richer, more nuanced
    understanding from the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Further, instead of using one set of query, key, and value vectors, Transformer
    models use a concept called multihead attention. For example, the 256-dimensional
    query, key, and value vectors can be split into say, 8, heads, and each head has
    a set of query, key, and value vectors with dimensions of 32 (because 256/8 =
    32). Each head pays attention to different parts or aspects of the input, enabling
    the model to capture a broader range of information and form a more detailed and
    contextual understanding of the input data. Multihead attention is especially
    useful when a word has multiple meanings in a sentence, such as in a pun. Let’s
    continue the “bank” example we mentioned earlier. Consider the pun joke, “Why
    is the river so rich? Because it has two banks.” In the project of translating
    English to French in the next chapter, you’ll implement first-hand splitting Q,
    K, and V into multiple heads to calculate attention in each head before concatenating
    them back into one single attention vector.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 The Transformer architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of the attention mechanism was introduced by Bahdanau, Cho, and
    Bengio in 2014.^([2](#footnote-000)) It became widely used after the groundbreaking
    paper “Attention Is All You Need,” which focused on creating a model for machine
    language translation. The architecture of this model, known as the Transformer,
    is depicted in figure 9.4\. It features an encoder-decoder structure that relies
    heavily on the attention mechanism. In this chapter, you’ll build this model from
    scratch, coding it line by line, intending to train it for translation between
    any two languages.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 The Transformer architecture. The encoder in the Transformer (left
    side of the diagram), which consists of N identical encoder layers, learns the
    meaning of the input sequence and converts it into vectors that represent its
    meaning. It then passes these vectors to the decoder (right side of the diagram),
    which consists of N identical decoder layers. The decoder constructs the output
    (e.g., the French translation of an English phrase) by predicting one token at
    a time, based on previous tokens in the sequence and vector representations from
    the encoder. The generator on the top right is the head attached to the output
    from the decoder so that the output is the probability distribution over all tokens
    in the target language (e.g., the French vocabulary).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use English-to-French translation as our example. The Transformer’s encoder
    transforms an English sentence like “I don’t speak French” into vector representations
    that store its meaning. The Transformer’s decoder then processes them to produce
    the French translation “Je ne parle pas français.” The encoder’s role is to capture
    the essence of the original English sentence. For instance, if the encoder is
    effective, it should translate both “I don’t speak French” and “I do not speak
    French” into similar vector representations. Consequently, the decoder will interpret
    these vectors and generate similar translations. Interestingly, when using ChatGPT,
    these two English phrases indeed result in the same French translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder in the Transformer approaches the task by first tokenizing both
    the English and French sentences. This is similar to the process described in
    chapter 8 but with a key difference: it employs subword tokenization. Subword
    tokenization is a technique used in NLP to break words into smaller components,
    or subwords, allowing for more efficient and nuanced processing. For example,
    as you’ll see in the next chapter, the English phrase “I do not speak French”
    is divided into six tokens: (`i, do, not, speak, fr, ench`). Similarly, its French
    counterpart “Je ne parle pas français” is tokenized into six parts: (`je, ne,
    parle, pas, franc, ais`). This method of tokenization enhances the Transformer’s
    ability to handle language variations and complexities.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning models, including Transformers, can’t directly process text, so
    tokens are indexed using integers before being fed to the model. These tokens
    are typically first represented using one-hot encoding, as we discussed in chapter
    8\. We then pass them through a word embedding layer to compress them into vectors
    with continuous values of a much smaller size, such as a length of 256\. Thus,
    after applying word embedding, the sentence “I do not speak French” is represented
    by a 6 × 256 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers process input data such as sentences in parallel, unlike RNNs,
    which handle data sequentially. This parallelism enhances their efficiency but
    doesn’t inherently allow them to recognize the sequence order of the input. To
    address this, Transformers add positional encodings to the input embeddings. These
    positional encodings are unique vectors assigned to each position in the input
    sequence and align in dimension with the input embeddings. The vector values are
    determined by a specific positional function, particularly involving sine and
    cosine functions of varying frequencies, defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F04_Liu_EQ03.png)'
  prefs: []
  type: TYPE_IMG
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F04_Liu_EQ04.png)'
  prefs: []
  type: TYPE_IMG
- en: '| (9.3) |'
  prefs: []
  type: TYPE_TB
- en: In these equations, vectors are calculated using the sine function for even
    indexes and the cosine function for odd indexes. The two parameters *pos* and
    *i* represent the position of a token within the sequence and the index within
    the vector, respectively. As an illustration, consider the positional encoding
    for the phrase “I do not speak French.” This is depicted as a 6 × 256 matrix,
    the same size as the word embedding for the sentence. Here, *pos* ranges from
    0 to 5, and the indexes 2i and 2i + 1 collectively span 256 distinct values (from
    0 to 255). A beneficial aspect of this positional encoding approach is that all
    values are constrained within the range of –1 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that each token position is uniquely identified by a
    256-dimensional vector, and these vector values remain constant throughout training.
    Before being input to the attention layers, these positional encodings are added
    to the word embeddings of the sequence. In the example of the sentence “I do not
    speak French,” the encoder generates both word embedding and positional encoding,
    each having dimensions of 6 × 256, before combining them into a single 6 × 256-dimensional
    representation. Subsequently, the encoder applies the attention mechanism to refine
    this embedding into more sophisticated vector representations that capture the
    overall meaning of the phrase, before passing them to the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer’s encoder, as depicted in figure 9.5, is made up of six identical
    layers (N = 6). Each of these layers comprises two distinct sublayers. The first
    sublayer is a multihead self-attention layer, similar to what was discussed earlier.
    The second sublayer is a basic, position-wise, fully connected feed-forward network.
    This network treats each position in the sequence independently rather than as
    sequential elements. In the model’s architecture, each sublayer incorporates layer
    normalization and a residual connection. Layer normalization normalizes observations
    to have zero mean and unit standard deviation. Such normalization helps stabilize
    the training process. After the normalization layer, we perform the residual connection.
    This means the input to each sublayer is added to its output, enhancing the flow
    of information through the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 The structure of the encoder in the Transformer. The encoder consists
    of N = 6 identical encoder layers. Each encoder layer contains two sublayers.
    The first sublayer is a multihead self-attention layer and the second is a feed-forward
    network. Each sublayer uses layer normalization and residual connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder of the Transformer model, as seen in figure 9.6, is comprised of
    six identical decoder layers (N = 6). Each of these decoder layers features three
    sublayers: a multihead self-attention sublayer, a sublayer that performs multihead
    cross attention between the output from the first sublayer and the encoder’s output,
    and a feed-forward sublayer. Note that the input to each sublayer is the output
    from the previous sublayer. Further, the second sublayer in the decoder layer
    also takes the output from the encoder as input. This design is crucial for integrating
    information from the encoder: this is how the decode generates translations based
    on the output from the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 The structure of the decoder in the Transformer. The decoder consists
    of N = 6 identical decoder layers. Each decoder layer contains three sublayers.
    The first sublayer is a masked multihead self-attention layer. The second is a
    multihead cross-attention layer to calculate the cross attention between the output
    from the first sublayer and the output from the encoder. The third sublayer is
    a feed-forward network. Each sublayer uses layer normalization and residual connection.
  prefs: []
  type: TYPE_NORMAL
- en: A key aspect of the decoder’s self-attention sublayer is the masking mechanism.
    This mask prevents the model from accessing future positions in the sequence,
    ensuring that predictions for a particular position can only depend on previously
    known elements. This sequential dependency is vital for tasks like language translation
    or text generation.
  prefs: []
  type: TYPE_NORMAL
- en: The decoding process begins with the decoder receiving an input phrase in French.
    The decoder transforms the French tokens into word embeddings and positional encodings
    before combining them into a single embedding. This step ensures that the model
    not only understands the semantic content of the phrase but also maintains the
    sequential context, which is crucial for accurate translation or generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder operates in an autoregressive manner, generating the output sequence
    one token at a time. At the first time step, it starts with the `"BOS"` token,
    which indicates the beginning of a sentence. Using this start token as its initial
    input, the decoder examines vector representations of the English phrase “I do
    not speak French” and attempts to predict the first token following `"BOS"`. Suppose
    the decoder’s first prediction is `"Je"`. In the next time step, it then uses
    the sequence `"BOS Je"` as its new input to predict the following token. This
    process continues iteratively, with the decoder adding each newly predicted token
    to its input sequence for the subsequent prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The translation process is designed to conclude when the decoder predicts the
    `"EOS"` token, signifying the end of the sentence. When preparing for the training
    data, we add EOS to the end of each phrase, so the model has learned that it means
    the end of a sentence. Upon reaching this token, the decoder recognizes the completion
    of the translation task and ceases its operation. This autoregressive approach
    ensures that each step in the decoding process is informed by all previously predicted
    tokens, allowing for coherent and contextually appropriate translations.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.3 Different types of Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are three types of Transformers: encoder-only Transformers, decoder-only
    Transformers, and encoder-decoder Transformers. We are using an encoder-decoder
    Transformer in this chapter and the next, but you’ll get a chance to explore firsthand
    decoder-only Transformers later in the book.'
  prefs: []
  type: TYPE_NORMAL
- en: An encoder-only Transformer consists of N identical encoder layers as shown
    on the left side of figure 9.4 and is capable of converting a sequence into abstract
    continuous vector representations. For example, BERT is an encoder-only Transformer
    that contains 12 encoder layers. An encoder-only Transformer can be used for text
    classification, for example. If two sentences have similar vector representations,
    we can classify the two sentences into one category. On the other hand, if two
    sequences have very different vector representations, we can put them in different
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: A decoder-only Transformer also consists of N identical layers, and each layer
    is a decoder layer as shown on the right side of figure 9.4\. For example, ChatGPT
    is a decoder-only Transformer that contains many decoder layers. The decoder-only
    Transformer can generate text based on a prompt, for example. It extracts the
    semantic meaning of the words in the prompt and predicts the most likely next
    token. It then adds the token to the end of the prompt and repeats the process
    until the text reaches a certain length.
  prefs: []
  type: TYPE_NORMAL
- en: The machine language translation Transformer we discussed earlier is an example
    of an encoder-decoder Transformer. They are needed for handling complicated tasks,
    such as text-to-image generation or speech recognition. Encoder-decoder Transformers
    combine the strengths of both encoders and decoders. Encoders are efficient in
    processing and understanding input data, while decoders excel in generating output.
    This combination allows the model to effectively understand complex inputs (like
    text or speech) and generate intricate outputs (like images or transcribed text).
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Building an encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll develop and train an encoder-decoder Transformer designed for machine
    language translation. The coding in this project is adapted from the work of Chris
    Cui in translating Chinese to English ([https://mng.bz/9o1o](https://mng.bz/9o1o))
    and Alexander Rush’s German-to-English translation project ([https://mng.bz/j0mp](https://mng.bz/j0mp)).
  prefs: []
  type: TYPE_NORMAL
- en: This section discusses how to construct an encoder in the Transformer. Specifically,
    we’ll dive into the process of building various sublayers within each encoder
    layer and implementing the multihead self-attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 The attention mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While there are different attention mechanisms, we’ll use the SDPA because it’s
    widely used and effective. The SDPA attention mechanism uses query, key, and value
    to calculate the relationships among elements in a sequence. It assigns scores
    to show how an element is related to all elements in a sequence (including the
    element itself).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using one set of query, key, and value vectors, the Transformer model
    uses a concept called multihead attention. Our 256-dimensional query, key, and
    value vectors are split into 8 heads, and each head has a set of query, key, and
    value vectors with dimensions of 32 (because 256/8 = 32). Each head pays attention
    to different parts or aspects of the input, enabling the model to capture a broader
    range of information and form a more detailed and contextual understanding of
    the input data. For example, multihead attention allows the model to capture the
    multiple meanings of the word “bank” in the pun joke, “Why is the river so rich?
    Because it has two banks.”
  prefs: []
  type: TYPE_NORMAL
- en: To implement this, we define an `attention()` function in the local module ch09util.
    Download the file ch09util.py from the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and store it in the /utils/ directory on your computer. The attention() function
    is defined as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 Calculating attention based on query, key, and value
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Scaled attention score is the dot product of query and key, scaled by the
    square root of d[k].
  prefs: []
  type: TYPE_NORMAL
- en: ② If there is a mask, hides future elements in the sequence
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates attention weights
  prefs: []
  type: TYPE_NORMAL
- en: ④ Returns both attention and attention weights
  prefs: []
  type: TYPE_NORMAL
- en: The `attention()` function takes query, key, and value as inputs and calculates
    attention and attention weights as we discussed earlier in this chapter. The scaled
    attention score is the dot product of query and key, scaled by the square root
    of the dimension of the key, *d[k]*. We apply the softmax function on the scaled
    attention score to obtain attention weights. Finally, attention is calculated
    as the dot product of attention weights and value.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use our running example to show how multihead attention works (see figure
    9.7). The embedding for “How are you?” is a tensor with a size of (1, 6, 256),
    as we explained in the last section (after we add positional encoding to word
    embedding). Note that 1 means there is one sentence in the batch, and there are
    six tokens in the sentence instead of four because we add BOS and EOS to the beginning
    and the end of the sequence. This embedding is passed through three linear layers
    to obtain query Q, key K, and value V, each of the same size (1, 6, 256). These
    are divided into eight heads, resulting in eight distinct sets of Q, K, and V,
    now sized (1, 6, 256/8 = 32) each. The attention function, as defined earlier,
    is applied to each of these sets, yielding eight attention outputs, each also
    sized (1, 6, 32). We then concatenate the eight attention outputs into one single
    attention, and the result is a tensor with a size of (1, 6, 32 × 8 = 256). Finally,
    this combined attention passes through another linear layer sized 256 × 256, leading
    to the output from the `MultiHeadAttention()` class. This output maintains the
    original input’s dimensions, which are (1, 6, 256).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F07_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 An example of multihead attention. This diagram uses the calculation
    of the multihead self attention for the phrase “How are you?” as an example. We
    first pass the embedding through three neural networks to obtain query Q, key
    K, and value V, each with a size of (1, 6, 256). We split them into eight heads,
    each with a set of Q, k, and V, with a size of (1, 6, 32). We calculate the attention
    in each head. The attention vectors from the eight heads are then joined back
    into one single attention vector, with a size of (1, 6, 256).
  prefs: []
  type: TYPE_NORMAL
- en: This is implemented in the following code listing in the local module.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 Calculating multihead attention
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Passes input through three linear layers to obtain Q, K, V, and splits them
    into multiheads
  prefs: []
  type: TYPE_NORMAL
- en: ② Calculates attention and attention weights for each head
  prefs: []
  type: TYPE_NORMAL
- en: ③ Concatenates attention vectors from multiheads into one single attention vector
  prefs: []
  type: TYPE_NORMAL
- en: ④ Passes the output through a linear layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Each encoder layer and decoder layer also contain a feed-forward sublayer,
    which is a two-layer fully connected neural network, with the purpose of enhancing
    the model’s ability to capture and learn intricate features in the training dataset.
    Further, the neural network processes each embedding independently. It doesn’t
    treat the sequence of embeddings as a single vector. Therefore, we often call
    it a position-wide feed-forward network (or a 1D convolutional network). For that
    purpose, we define a `PositionwiseFeedForward()` class in the local module as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `PositionwiseFeedForward()` class is defined with two key parameters: `d_ff`,
    the dimensionality of the feed-forward layer, and `d_model`, representing the
    model’s dimension size. Typically, `d_ff` is chosen to be four times the size
    of `d_model`. In our example, `d_model` is 256, and we therefore set `d_ff` to
    256 * 4 = 1024. This practice of enlarging the hidden layer in comparison to the
    model size is a standard approach in Transformer architectures. It enhances the
    network’s ability to capture and learn intricate features in the training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Creating an encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create an encoder layer, we first define the following `EncoderLayer()` class
    and `SublayerConnection()` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 A class to define an encoder layer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① The first sublayer in each encoder layer is a multihead self-attention network.
  prefs: []
  type: TYPE_NORMAL
- en: ② The second sublayer in each encoder layer is a feed-forward network.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Each sublayer goes through residual connection and layer normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each encoder layer is composed of two distinct sublayers: one is a multihead
    self-attention layer, as outlined in the `MultiHeadAttention()` class, and the
    other is a straightforward, position-wise, fully connected feed-forward network,
    as specified in the `PositionwiseFeedForward()` class. Additionally, both of these
    sublayers incorporate layer normalization and residual connections. As explained
    in chapter 6, a residual connection involves passing the input through a sequence
    of transformations (either the attention or the feed-forward layer in this context)
    and then adding the input back to these transformations’ output. The method of
    residual connection is employed to combat the problem of vanishing gradients,
    which is a common challenge in very deep networks. Another benefit of residual
    connections in Transformers is to provide a passage to pass the positional encodings
    (which are calculated only before the first layer) to subsequent layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer normalization is somewhat similar to the batch normalization we implemented
    in chapter 4\. It standardizes the observations in a layer to have a zero mean
    and a unit standard deviation. To achieve this within the local module, we define
    the `LayerNorm()` class, which executes layer normalization as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `mean` and `std` values in the preceding `LayerNorm()` class are the mean
    and standard deviation of the inputs in each layer. The `a_2` and `b_2` layers
    in the `LayerNorm()` class expand `x_zscore` back to the shape of the input `x`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create an encoder by stacking six encoder layers together. For that
    purpose, we define the `Encoder()` class in the local module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `Encoder()` class is defined with two arguments: `layer`, which is
    an encoder layer as defined in the `EncoderLayer()` class in listing 9.3, and
    `N`, the number of encoder layers in the encoder. The `Encoder()` class takes
    input `x` (for example, a batch of English phrases) and the mask (to mask out
    sequence padding, as I’ll explain in chapter 10) to generate output (vector representations
    that capture the meanings of the English phrases).'
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have created an encoder. Next, you’ll learn how to create a decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Building an encoder-decoder Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you understand how to create an encoder in the Transformer, let’s move
    on to the decoder. You’ll first learn how to create a decoder layer in this section.
    You’ll then stack N = 6 identical decoder layers to form a decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create an encoder-decoder transformer with five components: `encoder`,
    `decoder`, `src_embed, tgt_embed`, and `generator`, which I’ll explain in this
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Creating a decoder layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each decoder layer consists of three sublayers: (1) a multihead self-attention
    layer, (2) the cross attention between the output from the first sublayer and
    the encoder’s output, and (3) a feed-forward network. Each of these three sublayers
    incorporates a layer normalization and the residual connection, similar to what
    we have done in encoder layers. Furthermore, the decoder stack’s multihead self-attention
    sublayer is masked to prevent positions from attending to subsequent positions.
    The mask forces the model to use previous elements in a sequence to predict later
    elements. I’ll explain how masked multihead self-attention works in a moment.
    To implement this, we define the `DecoderLayer()` class in the local module.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.4 Creating a decoder layer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① The first sublayer is a masked multihead self-attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: ② The second sublayer is a cross-attention layer between the target language
    and the source language.
  prefs: []
  type: TYPE_NORMAL
- en: ③ The third sublayer is a feed-forward network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the operation of a decoder layer, let’s consider our ongoing
    example. The decoder takes in tokens `[''BOS'', ''comment'', ''et'', ''es-vous'',
    ''?'']`, along with the output from the encoder (referred to as `memory` in the
    preceding code block), to predict the sequence `[''comment'', ''et'', ''es-vous'',
    ''?'', ''EOS'']`. The embedding of `[''BOS'', ''comment'', ''et'', ''es-vous'',
    ''?'']` is a tensor of size (1, 5, 256): 1 is the number of sequences in the batch,
    5 is the number of tokens in the sequence, and 256 means each token is represented
    by a 256-value vector. We pass this embedding through the first sublayer, a masked
    multihead self-attention layer. This process is similar to the multihead self-attention
    calculation you saw earlier in the encoder layer. However, the process utilizes
    a mask, designated as `tgt_mask` in the preceding code block, which is a 5 × 5
    tensor with the following values in the ongoing example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you may have noticed, the lower half of the mask (values below the main diagonal
    in the tensor) is turned on as `True`, and the upper half of the mask (values
    above the main diagonal) is turned off as `False`. When this mask is applied to
    the attention scores, it results in the first token attending only to itself during
    the first time step. In the second time step, attention scores are calculated
    exclusively between the first two tokens. As the process continues, for example,
    in the third time step, the decoder uses tokens `['BOS', 'comment', 'et']` to
    predict the token `'es-vous'`, and the attention scores are computed only among
    these three tokens, effectively hiding the future tokens `['es-vous', '?']`
  prefs: []
  type: TYPE_NORMAL
- en: Following this process, the output generated from the first sublayer, which
    is a tensor of size (1, 5, 256), matches the input’s size. This output, which
    we can refer to as x, is then fed into the second sublayer. Here, cross attention
    is computed between x and the output of the encoder stack, termed `memory`. You
    may remember that `memory` has a dimension of (1, 6, 256) since the English phrase
    “How are you?” is converted to six tokens `['BOS', 'how', 'are', 'you', '?', 'EOS']`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.8 shows how cross-attention weights are calculated. To calculate the
    cross attention between x and `memory`, we first pass x through a neural network
    to obtain query, which has a dimension of (1, 5, 256). We then pass `memory` through
    two neural networks to obtain key and value, each having a dimension of (1, 6,
    256). The scaled attention score is calculated using the formula as specified
    in equation 9.1\. This scaled attention score has a dimension of (1, 5, 6): the
    query Q has a dimension of (1, 5, 256) and the transposed key K has a dimension
    of (1, 256, 6). Therefore, the scaled attention score, which is the dot product
    of the two, scaled by √*d[k]*, has a size of (1, 5, 6). After applying the softmax
    function to the scaled attention score, we obtain attention weights, which is
    a 5 × 6 matrix. This matrix tells us how the five tokens in the French input `[''BOS'',
    ''comment'', ''et'', ''es-vous'', ''?'']` attend to the six tokens in the English
    phrase `[''BOS'', ''how'', ''are'', ''you'', ''?'', ''EOS'']`. This is how the
    decoder captures the meaning of the English phrase when translating.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F08_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 An example of how cross-attention weights are calculated between
    the input to the decoder and the output from the encoder. The input to the decoder
    is passed through a neural network to obtain query Q. The output from the encoder
    is passed through a different neural network to obtain key K. The scaled cross-attention
    scores are calculated as the dot product of Q and K divided by the square root
    of the dimension of K. Finally, we apply the softmax function on the scaled cross-attention
    scores to obtain the cross-attention weights, which demonstrate how each element
    in Q is related to all elements in K.
  prefs: []
  type: TYPE_NORMAL
- en: The final cross attention in the second sublayer is then calculated as the dot
    product of attention weights and the value vector V. The attention weights have
    a dimension of (1, 5, 6) and the value vector has a dimension of (1, 6, 256),
    so the final cross attention, which is the dot product of the two, has a size
    of (1, 5, 256). Therefore, the input and output of the second sublayer have the
    same dimension of (1, 5, 256). After processing through this second sublayer,
    the output is then directed through the third sublayer, which is a feed-forward
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 Creating an encoder-decoder Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decoder consists of N = 6 identical decoder layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Decoder()` class is defined in the local module as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To create an encoder-decoder transformer, we first define a `Transformer()`
    class in the local module. Open the file ch09util.py, and you’ll see the definition
    of the class as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.5 A class to represent an encoder-decoder Transformer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines the encoder in the Transformer
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the decoder in the Transformer
  prefs: []
  type: TYPE_NORMAL
- en: ③ Source language is encoded into abstract vector representations.
  prefs: []
  type: TYPE_NORMAL
- en: ④ The decoder uses these vector representations to generate translation in the
    target language.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Transformer()` class is constructed with five key components: `encoder`,
    `decoder`, `src_embed`, `tgt_embed`, and `generator`. The encoder and decoder
    are represented by the `Encoder()` and `Decoder()` classes defined previously.
    In the next chapter, you’ll learn to generate the source language embedding: we’ll
    process numerical representations of English phrases using word embedding and
    positional encoding, combining the results to form the `src_embed` component.
    Similarly, for the target language, we process numerical representations of French
    phrases in the same manner, using the combined output as the `tgt_embed` component.
    The generator produces predicted probabilities for each index that corresponds
    to the tokens in the target language. We’ll define a `Generator()` class in the
    next section for this purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Putting all the pieces together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll put all the pieces together to create a model that can
    translate between any two languages.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.1 Defining a generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we define a `Generator()` class in the local module to generate the probability
    distribution of the next token (see figure 9.9). The idea is to attach a head
    to the decoder for downstream tasks. In our example in the next chapter, the downstream
    task is to predict the next token in the French translation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F09_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 The structure of the generator in the Transformer. The generator
    converts the output from the decoder stack to a probability distribution over
    the target language’s vocabulary, so that the Transformer can use the distribution
    to predict the next token in the French translation of an English phrase. The
    generator contains a linear layer so that the number of outputs is the same as
    the number of tokens in the French vocabulary. The generator also applies a softmax
    activation to the output so that the output is a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `Generator()` class produces predicted probabilities for each index that
    corresponds to the tokens in the target language. This enables the model to sequentially
    predict tokens in an autoregressive manner, utilizing previously generated tokens
    and the encoder’s output.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.2 Creating a model to translate between two languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we are ready to create a Transformer model to translate between any two
    languages (e.g., English to French or Chinese to English). The `create_model()`
    function defined in the local module accomplishes that.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 Creating a Transformer to translate between two languages
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates an encoder by instantiating the Encoder() class
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a decoder by instantiating the Decoder() class
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates src_embed by passing source language through word embedding and positional
    encoding
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates tgt_embed by passing target language through word embedding and positional
    encoding
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a generator by instantiating the Generator() class
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary element of the `create_model()` function is the `Transformer()`
    class, which was previously defined. Recall that the `Transformer()` class is
    built with five essential elements: `encoder`, `decoder`, `src_embed`, `tgt_embed`,
    and `generator`. Within the create_model() function, we sequentially construct
    these five components, using the recently defined `Encoder()`, `Decoder()`, and
    `Generator()` classes. In the next chapter, we’ll discuss in detail how to generate
    the source and target language embeddings, `src_embed` and `tgt_embed`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you’ll apply the Transformer you created here to English-to-French
    translation. You’ll train the model using more than 47,000 pairs of English-to-French
    translations. You’ll then use the trained model to translate common English phrases
    into French.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers are advanced deep-learning models that excel in handling sequence-to-sequence
    prediction challenges. Their strength lies in effectively understanding the relationships
    between elements in input and output sequences over long distances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The revolutionary aspect of the Transformer architecture is its attention mechanism.
    This mechanism assesses the relationship between words in a sequence by assigning
    weights, determining how closely words are related based on the training data.
    This enables Transformer models like ChatGPT to comprehend relationships between
    words, thus understanding human language more effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate SDPA, the input embedding X is processed through three distinct
    neural network layers, query (Q), key (K), and value (V). The corresponding weights
    for these layers are W^Q, W^K, and W^V. We can calculate Q, K, and V as Q = X
    * W^Q, K = X * Q^K, and V = X * W^V. SDPA is calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F09_Liu_EQ05.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: where d[k] represents the dimension of the key vector K. The softmax function
    is applied to the attention scores, converting them into attention weights. This
    ensures that the total attention a word gives to all words in the sentence sums
    to 100%. The final attention is the dot product of these attention weights and
    the value vector V.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using one set of query, key, and value vectors, Transformer models
    use a concept called multihead attention. The query, key, and value vectors are
    split into multiple heads. Each head pays attention to different parts or aspects
    of the input, enabling the model to capture a broader range of information and
    form a more detailed and contextual understanding of the input data. Multihead
    attention is especially useful when a word has multiple meanings in a sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-001-backlink))  Vaswani et al., 2017, “Attention Is All You
    Need.” [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](#footnote-000-backlink))  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
    Bengio, 2014, “Neural Machine Translation by Jointly Learning to Align and Translate.”
    h[ttps://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).
  prefs: []
  type: TYPE_NORMAL
