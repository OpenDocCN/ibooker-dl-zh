- en: '3 Indexing pipeline: Creating a knowledge base for RAG'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data loading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text splitting or chunking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting text to embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing embeddings in vector databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples in Python using LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 2, we discussed the main components of retrieval-augmented generation
    (RAG) systems. You may recall that the indexing pipeline creates the knowledge
    base or the non-parametric memory of RAG applications. An indexing pipeline needs
    to be set up before the real-time user interaction with the large language model
    (LLM) can begin.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter elaborates on the four components of the indexing pipeline. We
    begin by discussing data loading, which involves connecting to the source, extracting
    files, and parsing text. At this stage, we introduce a framework called LangChain,
    which has become increasingly popular in the LLM app developer community. Next,
    we elaborate on the need for data splitting or chunking and discuss chunking strategies.
    Embeddings is an important design pattern in the world of AI and ML. We explore
    embeddings in detail and how they are relevant in the RAG context. Finally, we
    look at a new storage technique called vector storage and the databases that facilitate
    it.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have a solid understanding of how a knowledge
    base, or the non-parametric memory of a RAG application, is created. We also embellish
    this chapter with snippets of Python code, so those of you who are so inclined
    can try a hands-on development of the indexing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should
  prefs: []
  type: TYPE_NORMAL
- en: Know how to extract data from sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a deeper understanding of text-chunking strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn what embeddings are and how they are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gain knowledge of vector storage and vector databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have an end-to-end knowledge of setting up the indexing pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.1 Data loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section focuses on the first stage of the indexing pipeline. You will read
    about data loaders, metadata information, and data transformers.
  prefs: []
  type: TYPE_NORMAL
- en: The first step toward building a knowledge base (or non-parametric memory) of
    a RAG system is to source data from its original location. This data may be in
    the form of Word documents, PDF files, CSV, HTML, and similar. Furthermore, the
    data may be stored in file, block, or object stores, in data lakes, data warehouses,
    or even in third-party sources that can be accessed via the open internet. This
    process of sourcing data from its original location is called *data loading*.
    Loading documents from a list of sources may turn out to be a complicated process.
    Therefore, it is advisable to document all the sources and the file formats in
    advance.
  prefs: []
  type: TYPE_NORMAL
- en: Before going too deep, let’s begin with a simple example. If you recall, in
    chapter 1, we used Wikipedia as a source of information about the 2023 Cricket
    World Cup. At that time, we copied the opening paragraph of the article and pasted
    it in the ChatGPT prompt window. Instead of doing it manually, we will now *connect*
    to Wikipedia and *extract* the data programmatically, using a very popular framework
    called LangChain. The code in this chapter and the book can be run on Python notebooks
    and is available in the GitHub repository of this book ([https://mng.bz/a9DJ](https://mng.bz/a9DJ)).
  prefs: []
  type: TYPE_NORMAL
- en: Note LangChain is an open source framework developed by Harrison Chase and launched
    in October 2022\. It was written in Python and JavaScript and designed for building
    applications using LLMs. Apart from being suitable for RAG, LangChain is also
    suitable for building application use cases such as chatbots, document summarizers,
    synthetic data generation, and more. Over time, LangChain has built integrations
    with LLM providers such as OpenAI, Anthropic, and Hugging Face; a variety of vector
    store providers; cloud storage systems such as AWS, Google, Azure, and SQL and
    NoSQL databases; and APIs for news, weather, and similar. Although LangChain has
    received some criticism, it is still a good starting point for developers.
  prefs: []
  type: TYPE_NORMAL
- en: Installing LangChain
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To install LangChain (we’ll use the version 0.3.19 in this chapter) using `pip`,
    run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `langchain-community` package contains third-party integrations. It is
    automatically installed by LangChain, but in case it does not work, you can also
    install it separately using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have installed LangChain, we will use it to connect to Wikipedia
    and extract data from the page about the 2023 Cricket World Cup. For this task,
    we will use the `AsyncHtmlLoader` function from the `document_loaders`library
    in the `langchain-community` package. To run `AsyncHtmlLoader`, we will have to
    install another Python package called bs4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `data` variable in the code now stores the information from the Wikipedia
    page.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here is the output (A large section of the text is replaced with periods to
    save space.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable `data` is a list of documents that includes two elements: `page_content`
    and `metadata`. `page_content` contains the text sourced from the URL. You will
    notice that the text along with the relevant information also has newline characters
    (`\n`) and other HTML tags; however, `metadata` contains another important data
    aspect.'
  prefs: []
  type: TYPE_NORMAL
- en: Metadata is information about the data (e.g., type, origin, and purpose). This
    can include a data summary; the way the data was created; who created it and why;
    when it was created; and the size, quality, and condition of the data. Metadata
    information comes in extremely handy in the retrieval stage. Also, it can be used
    to resolve conflicting information that can arise due to chronology or origin.
    In the previous example, while extracting the data from the URL, Wikipedia has
    already provided the source, title, and language in the metadata information.
    For many data sources, you will have to add metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Often, a *cleaning* of the source data is required. The data in our example
    has a lot of new line characters and HTML tags, which requires a certain level
    of cleanup. We will attempt to clean up the webpage data that we extracted using
    the `Html2Text­Transformer` function from the `document_transformers` library
    in the `langchain-community` package. For `Html2TextTransformer`*,* we will also
    have to install another package called `html2text`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `page_content` is now free of any HTML tags and contains
    only the text from the webpage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The text is more coherent now since we have removed the HTML part of the data.
    There can be further cleanup, such as removing special characters and other unnecessary
    information. Data cleaning also removes duplication. Yet another step to include
    in the data-loading stage can be masking of sensitive information such as PII
    (Personally Identifiable Information) or company secrets. In some cases, a fact
    check may also be required.
  prefs: []
  type: TYPE_NORMAL
- en: The source for our data was Wikipedia (more precisely, a web address pointing
    to a Wikipedia page), and the format was HTML. The source can also be other storage
    locations such as AWS S3, SQL/NoSQL databases, Google Drive, GitHub, even WhatsApp,
    YouTube, and other social media sites. Likewise, the data formats can be .doc,
    .pdf, .csv, .ppt, .eml, and the like. Most of the time, you will be able to use
    frameworks such as LangChain that have integrations for the sources and the formats
    already built in. Sometimes, you may have to build custom connectors and loaders.
  prefs: []
  type: TYPE_NORMAL
- en: Although data loading may seem simple (after all, it’s just connecting to a
    source and extracting data), the nuances of adding metadata, document transformation,
    masking, and similar add complexity to this step. Advanced planning of the sources,
    a review of the formats, and curation of metadata information are advised for
    best results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now taken the first step toward building our RAG system. The data-loading
    process can be further broken down into four sub-steps, as shown in figure 3.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a data conversion process'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH03_F01_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1  Four sub-steps of the data-loading component of the indexing pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Connect to the source of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract text from the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review and update metadata information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean or transform the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have now obtained data from the source and cleaned it to an extent. This
    Wikipedia page that we have loaded has more than 8,000 words, alone. Imagine the
    number of words if we had multiple documents. For efficient management of information,
    we employ something called data splitting, which will be discussed in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Data splitting (chunking)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Breaking down long pieces of text to manageable segments is called *data splitting*
    or *chunking*. This section discusses why chunking is necessary and the different
    chunking strategies. We also use functions from LangChain to illustrate a few
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Advantages of chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we loaded the data from a URL (a Wikipedia page) and
    extracted the text. It was a long piece of text of approximately 8,000 words.
    When it comes to overcoming the major limitations of using long pieces of text
    in LLM applications, chunking offers the following three advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Context window of LLM**s*—Due to the inherent nature of the technology, the
    number of tokens (loosely, words) LLMs can work with at a time is limited. This
    includes both the number of tokens in the prompt (or the input) and in the completion
    (or the output). The limit on the total number of tokens that an LLM can process
    in one go is called “the context window size.” If we pass an input that is longer
    than the context window size, the LLM chooses to ignore all text beyond the size.
    It is thus very important to be careful with the amount of text being passed to
    the LLM. In our example, a text of 50,000 words will not work well with LLMs that
    have a smaller context window. The way to address this problem is to break the
    text down into smaller chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lost-in-the-middle proble**m*—Even in those LLMs that have a long context
    window (e.g., Claude 3 by Anthropic has a context window of up to 200,000 tokens),
    a problem with accurately reading the information has been observed. It has been
    noticed that accuracy declines dramatically if the relevant information is somewhere
    in the middle of the prompt. This problem can be addressed by passing only the
    relevant information to the LLM instead of the entire document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ease of searc**h*—This is not a problem with the LLM per se, but it has been
    observed that large chunks of text are harder to search over. When we use a retriever
    (recall the generation pipeline introduced in chapter 2), it is more efficient
    to search over smaller pieces of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DEFINITION Tokens are the fundamental semantic units used in natural language
    processing (NLP) tasks. Tokens can be assumed to be words, but sometimes, a single
    word can be made up of more than one token. OpenAI suggests one token to be made
    of four characters or 0.75 words. Tokens are important as most proprietary LLMs
    are priced based on token usage.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Chunking process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The chunking process can be divided into three steps, as illustrated in figure
    3.2:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the longer text into compact, meaningful units (e.g., sentences or paragraphs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the smaller units into larger chunks until a specific size is achieved.
    After that, this chunk is treated as an independent segment of text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When creating a new chunk, include a part of the previous chunk at the start
    of the new chunk. This overlap is necessary to maintain contextual continuity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is also known as “small to big” chunking.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of a diagram of a large chunking unit'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH03_F02_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2  Data-chunking process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3.2.3 Chunking methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While splitting documents into chunks might sound like a simple concept, multiple
    methods can be employed to execute chunking. The following two aspects vary across
    the chunking methodologies:'
  prefs: []
  type: TYPE_NORMAL
- en: The manner of text splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring of the chunk size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixed-Size Chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A very common approach is to predetermine the size of the chunk and the amount
    of overlap between the chunks. The following two methods fall under the *fixed-size
    chunking* category:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Split by characte**r*—Here, we specify a certain character, such as a newline
    character `\n` or a special character `*`, to determine how the text should be
    split. The text is split into a unit whenever this character is encountered. The
    chunk size is measured in the number of characters. We must choose the chunk size
    or the number of characters we need in each chunk. We can also choose the number
    of characters we need to overlap between two chunks. We will look at an example
    and demonstrate this method using `CharacterTextSplitter` from `langchain.text_splitters`.
    For this, we will take the same document that we loaded and transformed in the
    previous section from Wikipedia and store it in the variable `html_data_transformed.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This method created 64 chunks. But what about the overlap? Let’s check two chunks
    at random, say, chunks 4 and 5\. We will compare the last 200 characters
  prefs: []
  type: TYPE_NORMAL
- en: 'of chunk 4 with the first 200 characters of chunk 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Comparing the two outputs, we can observe that there is an overlap between the
    two consecutive chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting by character is a simple and effective way to create chunks. It is
    the first chunking method that anyone should try. However, sometimes, it may not
    be feasible to create chunks within the specified length. This is because the
    sequential occurrence of the character on which the text needs to be split is
    far apart. To address this problem, a recursive approach is employed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Recursively split by character—*This method is quite like the split by character
    but instead of specifying a single character for splitting, we specify a list
    of characters. The approach initially tries creating chunks based on the first
    character. In case it is not able to create a chunk of the specified size using
    the first character, it then uses the next character to further break down chunks
    to the required size. This method ensures that chunks are largely created within
    the specified size. This method is recommended for generic texts. You can use
    `RecursiveCharacter­TextSplitter` from LangChain to use this method. The only
    difference in `RecursiveCharacterTextSplitter`is that instead of passing a single
    character in the separator parameter `separator=``"``\n``"`, we will need to pass
    a list `separators= [``"``\n\n``"``,``"``\n``"``,` `"``.``"``,` `"` `"``]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another perspective to consider with fixed-sized chunking is the use of tokens.
    As shown at the beginning of this section, tokens are the fundamental units of
    NLP. They can be understood loosely as a proxy for words. All LLMs process text
    in the form of tokens. So, it would also make sense to use tokens to determine
    the size of the chunks. This method is called the *split by token method*. Here,
    the splitting still happens based on a character, but the size of the chunk and
    the overlap are determined by the number of tokens instead of the number of characters.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Tokenizers are used to create tokens from a piece of text. Tokens are slightly
    different from words. A phrase such as “I’d like that!” has three words; however,
    in NLP, this text may be parsed as five tokens, that is, “I”, “‘d”, “like”, “that”,
    “!”. Different LLMs use different methods for creating tokens. OpenAI uses a tokenizer
    called tiktoken for GPT3.5 and GPT4 models;Llama2 by Meta uses LLamaTokenizer,
    available in the transformers library by Hugging Face. You can also explore other
    tokenizers on Hugging Face. NLTK and spaCy are some other popular libraries that
    can be used as tokenizers.
  prefs: []
  type: TYPE_NORMAL
- en: To use the split by token method, you can use specific methods within the `Recursive­CharacterTextSplitter`  and  `CharacterTextSplitter`
    classes, such as `Recursive­CharacterTextSplitter.from_tiktoken_encoder` `(encoding=``"``cl100k_base``"``,
    chunk_size=100,` `chunk_overlap=10)` for creating chunks of 100 tokens with an
    overlap of 10 tokens using OpenAI’s tiktoken tokenizer or `CharacterTextSplitter.from_``huggingface_tokenizer(tokenizer,`
    `chunk_size=100,` `chunk_overlap=10)` for creating the same sized chunk using
    another tokenizer from Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: The limitation of fixed-size chunking is that it doesn’t consider the semantic
    integrity of the text. In other words, the meaning of the text is ignored. It
    works best in scenarios where data is inherently uniform, such as genetic sequences
    and service manuals, or uniformly structured reports such as survey responses.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chunking aims to keep meaningful data together. If we are dealing with data
    in the form of HTML, Markdown, JSON, or even computer code, it makes more sense
    to split the data based on the structure rather than a fixed size. Another approach
    to chunking is to consider the format of the extracted and loaded data. A markdown
    file, for example, is organized by headers, a code written in a programming language
    such as Python or Java is organized by classes and functions, and likewise, HTML
    is organized in headers and sections. For such formats, a specialized chunking
    approach can be employed. LangChain offers classes such as `MarkdownHeaderTextSplitter`,
    `HTMLHeader­TextSplitter`, and `RecursiveJsonSplitter`, among others, for these
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example of a code that splits an HTML document using `HTML­SectionSplitter`*.*
    We are using the same Wikipedia article to source the HTML page. We first split
    the input data based on the sections. Sections in HTML are tagged as `<h1>`, `<h2>`,
    `<table>`, and so on. It can be assumed that a well-structured HTML document will
    have similar information. This helps us in creating chunks that have similar information.
    To use the `HTMLSectionSplitter`library, we must install another Python package
    called `lxml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The advantage of specialized chunking is that chunk sizes are no longer limited
    by a fixed width. This feature helps in preserving the inherent structure of the
    data. Because the size of the chunks changes depending on the structure, this
    method is also sometimes called *adaptive chunking*. Specialized chunking works
    well in structured scenarios such as customer reviews or patient records where
    data can be of different lengths but should ideally be in the same chunk.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, let’s see how many chunks have been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This method has given us 231 chunks from the URL. Chunking methods do not have
    to be exclusive. We can further chunk these 231 chunks using a fixed-size chunking
    method such as `RecursiveCharacterTextSplitter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at how many chunks were created by this combination of techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: A total of 285 chunks were created by splitting the HTML data from the URL first,
    using a specialized chunking method followed by a fixed size method. This gave
    us more chunks than using the fixed size method alone that we saw in the previous
    section (“split by character” gave us 67 chunks).
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering about the advantages of having more chunks and the optimal
    number. Unfortunately, there’s no straightforward answer to that. Having many
    chunks (consequently smaller-sized chunks) means that the information in the chunks
    is precise. This is advantageous when it comes to providing the LLM with accurate
    information. In contrast, by chunking into small sizes, you may lose the overall
    themes, ideas, and coherence of the larger document. The task here is to strike
    a balance. We will discuss more chunking strategies after we take a cursory look
    at a novel method that considers the meaning of the text to perform chunking and
    aims to create chunks that are super-contextual.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This idea, proposed by Greg Kamradt, questions two aspects of the previous chunking
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Why should we have a predefined fixed size of chunks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why don’t chunking methods take into consideration the actual meaning of content?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address these problems, a method that looks at semantic similarity (or similarity
    in the meaning) between sentences is called semantic chunking. It first creates
    groups of three sentences and then merges groups that are similar in meaning.
    To find out the similarity in meaning, this method uses embeddings. (We will discuss
    embeddings in the next section.) This is still an experimental chunking technique.
    In LangChain, you can use the class `SemanticChunker` from the `langchain_experimental.text_splitter`
    library. See figure 3.3 for examples of different chunking methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Several different types of diagrams'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH03_F03_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3  Chunking methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As the LLM and the generative AI space are evolving fast, chunking methods are
    also becoming more sophisticated. Simple chunking methods predetermine the size
    of the chunks and a split by characters. A slightly more nuanced technique is
    to split the data by tokens. Specialized methods are more suitable for different
    data formats. Experimental techniques such as semantic chunking and agentic chunking
    are spearheading the advancements in the chunking space. Now, let’s consider the
    important question of how to select a chunking method.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Choosing a chunking strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have seen that there are many chunking methods available. Which chunking
    method to use (i.e., whether to use a single method or multiple methods) is a
    question that comes up during the creation of the indexing pipeline. There are
    no guidelines or rules to answer this question. However, certain features of the
    application that you’re developing can guide you toward an effective strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Nature of the content
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The type of data that you’re dealing with can be a guide for the chunking strategy.
    If your application uses data in a specific format such as code or HTML, a specialized
    chunking method is recommended. Not only that, whether you’re working with long
    documents such as whitepapers and reports or short-form content such as social
    media posts, tweets, and so on, can also guide the chunk size and overlap limits.
    If you’re using a diverse set of information sources, then you might have to use
    different methods for different sources.
  prefs: []
  type: TYPE_NORMAL
- en: Expected length and complexity of user query
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The nature of the query that your RAG system is likely to receive also determines
    the chunking strategy. If your system expects a short and straightforward query,
    then the size of your chunks should be different when compared to a long and complex
    query. Matching long queries to short chunks may prove inefficient in certain
    cases. Similarly, short queries matching large chunks may yield partially irrelevant
  prefs: []
  type: TYPE_NORMAL
- en: results.
  prefs: []
  type: TYPE_NORMAL
- en: Application and use case requirements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The nature of the use case you’re addressing may also determine the optimal
    chunking strategy. For a direct question-answering system, shorter chunks are
    likely used for precise results, while for summarization tasks, longer chunks
    may make more sense. If the results of your system need to serve as an input to
    another downstream application, that may also influence the choice of the chunking
    strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We are going to discuss embeddings in the next section. For now, you can make
    a note that certain embeddings models perform better with chunks of specific sizes.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed chunking at length in this section. From understanding the
    need and advantages of chunking to different chunking methods and the choice of
    chunking strategies, you are now equipped to load data from different sources
    and split them into optimal sizes. Remember, chunking is not an overcomplicated
    task, and most chunking methods will work. You will, however, have to evaluate
    and improve your chunking strategy depending on the results you observe.
  prefs: []
  type: TYPE_NORMAL
- en: Now that data has been split into manageable sizes, we need to store it so that
    it can be fetched later to be used in the generation pipeline. We need to ensure
    that these chunks can be effectively searched over to match the user query. Turns
    out that one data pattern is the most efficient for such tasks. This pattern is
    called “embeddings.” Let’s explore embeddings and their use in RAG systems in
    the next
  prefs: []
  type: TYPE_NORMAL
- en: section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Data conversion (embeddings)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computers, at their very core, do mathematical calculations. Mathematical calculations
    are done on numbers. Therefore, for a computer to process any kind of nonnumeric
    data such as text or image, it must be first converted into a numerical form.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 What are embeddings?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Embeddings* is a design pattern that is extremely helpful in the fields of
    data science, machine learning, and AI. Embeddings are vector representations
    of data. As a general definition, embeddings are data that has been transformed
    into *n*-dimensional matrixes. The word *embedding* is a vector representation
    of words. I explain embeddings by using three words as an example: *dog, bark,*
    and *fly*.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE In physics and mathematics, the vector is an object that has a magnitude
    and a direction, like an arrow in space. The length of the arrow is the magnitude
    of the quantity and the direction that the arrow points to is the direction of
    the quantity. Examples of such quantities in physics are velocity, force, acceleration,
    and so forth. In computer science and machine learning, the idea of a vector is
    an abstract representation of data, and the representation is an array or list
    of numbers. These numbers represent the data features or attributes. In NLP, a
    vector can represent a document, a sentence, or even a word. The length of the
    array or list is the number of dimensions in the vector. A 2D vector will have
    two numbers, a 3D vector will have three numbers, and an *n*-dimensional vector
    will have *n* numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand embeddings by assigning a number to the three words: Dog =
    1, Bark = 2 and Fly = 6, as shown in figure 3.4\. We chose these numbers because
    the word *dog* is closer to the word *bark* and farther from the word *fly*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A blue line with black text'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH03_F04_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4  Words in a unidimensional vector
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unidimensional vectors are not great representations because we can’t plot unrelated
    words accurately. In our example, we can plot that the words *fly* and *bark*,
    which are verbs, are far from each other, and bark is closer to a dog because
    dogs can bark. But how do we plot words such as *love* or *re**d*? To accurately
    represent all the words, we need to increase the number of dimensions. See figure
    3.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![A blue line with white text'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH03_F05_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5  Words in a 2D vector space
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The goal of an embedding model is to convert words (or sentences/paragraphs)
    into *n*-dimensional vectors so that the words (or sentences/paragraphs) that
    are like each other in meaning lie close to each other in the vector space. See
    figure 3.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a model'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH03_F06_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6  The process of embedding transforms data (such as text) into vectors
    and compresses the input information, which results in an embedding space specific
    to the training data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'An embeddings model can be trained on a corpus of preprocessed text data using
    an embedding algorithm such as Word2Vec, GloVe, FastText, or BERT:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Word2Ve**c*—Word2Vec is a shallow-neural-network-based model for learning
    word embeddings, developed by researchers at Google. It is one of the earliest
    embedding techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GloV**e*—Global Vectors for Word Representations is an unsupervised learning
    technique developed by researchers at Stanford University.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*FastTex**t*—FastText is an extension of Word2Vec developed by Facebook AI
    Research. It is particularly useful for handling misspellings and rare words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ELM**o*—Embeddings from Language Models was developed by researchers at Allen
    Institute for AI. ELMo embeddings have been shown to improve performance on question
    answering and sentiment analysis tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BERT*—Bidirectional Encoder Representations from Transformers, developed by
    researchers at Google, is a transformers-architecture-based model. It provides
    contextualized word embeddings by considering bidirectional context, achieving
    state-of-the-art performance on various NLP tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a custom embeddings model can prove to be beneficial in some use cases
    where the scope is limited. Training an embeddings model that generalizes well
    can be a laborious exercise. Collection and preprocessing text data can be cumbersome.
    The training process can turn out to be computationally expensive too.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Common pre-trained embeddings models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The good news for anyone building RAG systems is that embeddings once created
    can also generalize across tasks and domains. There are a variety of proprietary
    and open source pre-trained embeddings models available to use. This is also one
    of the reasons why the usage of embeddings has exploded in popularity across machine
    learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: '*Embeddings models by OpenAI*—OpenAI, the company behind ChatGPT and the GPT
    series of LLMs, also provides three embeddings models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*text-embedding-ada-002* was released in December 2022\. It has a dimension
    of 1536, meaning that it converts text into a vector of 1536 dimensions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*text-embedding-3-small*is the latest small embedding model of 1536 dimensions
    released in January 2024\. The flexibility it provides over the ada-002 model
    is that users can adjust the size of the dimensions according to their needs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*text-embedding-3-large* is a large embedding model of 3072 dimensions, released
    together with the text-embedding-3-small model. It is the best performing model
    released by OpenAI yet.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI models are closed source and can be accessed using the OpenAI API. They
    are priced based on the number of input tokens for which embeddings are desired.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Gemini Embeddings Model by Googl**e*—*text-embedding-004* (last updated in
    April 2024)is the model offered by Google Gemini. It offers elastic embeddings
    size up to 768 dimensions and can be accessed via the Gemini API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Voyage A**I*—These embeddings models are recommended by Anthropic, the provider
    of the Claude series of LLMs. Voyage offers several embedding models such as'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*voyage-large-2-instruct*is a 1024-dimensional embeddings model that has become
    a leader in embeddings models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*voyage-law-2*is a 1024-dimension model optimized for legal documents.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*voyage-code-2* is a 1536-dimension model optimized for code retrieval.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*voyage-large-2* is a 1536-dimension general-purpose model optimized for retrieval.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Voyage AI offers several free tokens before charging for using the embeddings
    models.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Mistral AI embedding**s*—Mistral is the company behind LLMs such as Mistral
    and Mixtral. They offer a 1024-dimensional embeddings model known as *mistral-embed*.
    This is an open source embeddings model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cohere embedding**s*—Cohere, the developers of Command, Command R, and Command
    R + LLMs also offer a variety of embeddings models, which can be accessed via
    the Cohere API. Some of these are'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*embed-english-v3.0* is a 1024-dimension model that works on embeddings for
    English only.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*embed-english-light-v3.0* is a lighter version of the embed-english model,
    which has 384 dimensions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*embed-multilingual-v3.0* offers multilingual support for over 100 languages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These five models are in no way recommendations but just a list of the popular
    embeddings models. Apart from these providers, almost all LLM developers such
    Meta, TII, and LMSYS also offer pre-trained embeddings models. One place to check
    out all the popular embeddings models is the MTEB (Massive Text Embedding Benchmark)
    Leaderboard on Hugging Face ([https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).
    The MTEB benchmark compares the embeddings models on tasks such as classification,
    retrieval, clustering, and more. You now know what embeddings are, but why are
    they useful? Let’s discuss that next with some examples of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Embeddings use cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The reason why embeddings are so popular is because they help in establishing
    semantic relationships between words, phrases, and documents. In the simplest
    methods of searching or text matching, we use keywords, and if the keywords match,
    we can show the matching documents as results of the search. However, this approach
    fails to consider the semantic relationships or the meanings of the words while
    searching. This challenge is overcome by using embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: How is similarity calculated
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We discussed that embeddings are vector representations of words or sentences.
    Similar pieces of text lie close to each other. Closeness to each other is calculated
    by the distance between the points in the vector space. One of the most common
    measures of similarity is *cosine similarity*. Cosine similarity is calculated
    as the cosine value of the angle between the two vectors. Recall from trigonometry
    that the cosine of parallel lines (i.e., angle = 0o) is 1, and the cosine of a
    right angle (i.e., 90o) is 0\. The cosine of the opposite lines (i.e., angle =
    180o) is −1\. Therefore, the cosine similarity lies between −1 and 1, where unrelated
    terms have a value close to 0, and related terms have a value close to 1\. Terms
    that are opposite in meaning have a value of −1\. See figure 3.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a trigonometry'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH03_F07_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7  Cosine similarity of vectors in 2D vector space
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Yet another measure of similarity is the *Euclidean distance* between two vectors.
    Close vectors have a small Euclidean distance. It can be calculated using the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Distance (A, B) = sqrt((A[i]-B[i])²),
  prefs: []
  type: TYPE_NORMAL
- en: where i is the i-th dimension of the *n*-dimensional vectors
  prefs: []
  type: TYPE_NORMAL
- en: Different use cases of embeddings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here are some different use cases of embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Text search*—Searching through the knowledge base for the right document chunk
    is a key component of RAG systems. Embeddings are used to calculate similarity
    between the user query and the stored documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clustering*—Categorizing similar data together to find themes and groups in
    the data can result in valuable insights. Embeddings are used to group similar
    pieces of text together to find out, for example, the common themes in customer
    reviews.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine learning*—Advanced machine learning techniques can be used for different
    problems such as classification and regression. To convert text data into numerical
    features, embeddings prove to be a valuable technique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recommendation engines*—Shorter distances between product features mean greater
    similarity. Using embeddings for product and user features can be used to recommend
    similar products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we are focusing on RAG systems, here we examine using embeddings for text
    search— to find the document chunks that are closest to the user’s query. Let’s
    continue with our example of the Wikipedia page on the 2023 Cricket World Cup.
    In the last section, we created 67 chunks using a combination of specialized and
    fixed-width chunking. Now we will see how to create embeddings for each chunk.
    We will see how to use an open source as well as a proprietary embeddings model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code example for creating embeddings using an open source embeddings
    model all-MPnet-base-v2 via Hugging Face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This model creates embeddings of dimension 768\. The list `hf_embeddings` is
    made up of 285 lists, each containing 768 numbers for each chunk. Figure 3.8 shows
    the embeddings space of all the chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '![A blank lined paper with numbers'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH03_F08_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8  Embeddings created for chunks of Wikipedia page using the all-MiniLM-l6-v2
    model.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, we can use a proprietary model such as the text-embedding-3-small
    model, hosted by OpenAI. The only prerequisite is obtaining an API key and setting
    up a billing account with OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This text-embedding-3-small model creates embeddings for the same chunks of
    dimension 1536.
  prefs: []
  type: TYPE_NORMAL
- en: There are several embeddings models available, and new ones are being added
    every day. The choice of embeddings can be dictated by certain factors. Let’s
    look at a few factors.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4 How to choose embeddings?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a few major factors that will affect your choice of embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Use case
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Your application use case may determine your choice of embeddings. The MTEB
    leaderboard scores each of the embeddings models across seven use cases: classification,
    clustering, pair classification, reranking, retrieval, semantic text similarity,
    and summarization. At the time of writing this book, the `SFR-Embedding-Mistral`
    model developed by Salesforce performs the best for retrieval tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cost is another important factor to consider. To create the knowledge base,
    you may have to create embeddings for thousands of documents, thus running into
    millions of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are powerful data patterns that are most effective in finding similarities
    between texts. In RAG systems, embeddings play a critical role in search and retrieval
    of data relevant to the user query. Once the embeddings have been created, they
    need to be stored in persistent memory for real-time access. To store embeddings,
    a new kind of database called a *vector database* have become increasingly popular.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Storage (vector databases)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we are at the last step of the indexing pipeline. The data has been loaded,
    split, and converted to embeddings. To use this information repeatedly, we need
    to store it in memory so that it can be accessed on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 What are vector databases?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evolution of databases can be traced back to the early days of computing.
    Databases are organized collections of data, designed to be easily accessed, managed,
    and updated. Relational databases such as MySQL organize structured data into
    rows and columns. NoSQL databases such as MongoDB specialize in handling unstructured
    and semi-structured data. Graph databases such as Neo4j are optimized for querying
    graph data. In the same manner, vector databases are built to handle high-dimensional
    vectors. These databases specialize in indexing and storing vector embeddings
    for fast semantic search and retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from efficiently storing high-dimensional vector data, modern vector databases
    offer traditional features such as scalability, security, multi-tenancy, versioning
    and management, and similar. However, vector databases are unique in offering
    similarity searches based on Euclidean distance or cosine similarity. They also
    employ specialized indexing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Types of vector databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vector databases started as a specialized database offering, but propelled by
    the growth in demand for storing vector data, all major database providers have
    added the vector indexing capability. We can categorize the popular vector databases
    available today into six broad categories.
  prefs: []
  type: TYPE_NORMAL
- en: '*Vector indexe**s*—These are libraries that focus on the core features of indexing
    and search. They do not support data management, query processing, or interfaces.
    They can be considered a bare-bones vector database. Examples of vector indexes
    are Facebook AI Similarity Search (FAISS), Non-Metric Space Library (NMSLIB),
    Approximate Nearest Neighbors Oh Yeah (ANNOY), Scalable Nearest Neighbors (ScaNN),
    and similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Specialized vector DB**s*—These databases focus on the core feature of high-dimensional
    vector support, indexing, search, and retrieval such as vector indexes, but also
    offer database features such as data management, extensibility, security, scalability,
    non-vector data support, and similar. Examples of specialized vector DBs are Pinecone,
    ChromaDB, Milvus, Qdrant, Weaviate, Vald, LanceDB, Vespa, and Marqo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Search platform**s*—Solr, Elastic Search, Open Search, and Apache Lucene are
    traditional text search platforms and engines built for full text search. They
    have now added vector similarity search capabilities to their existing search
    capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vector capabilities for SQL database**s*—Azure SQL, Postgres SQL(pgvector),
    SingleStore, and CloudSQL are traditional SQL databases that have now added vector
    data-handling capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vector capabilities for NoSQL database**s*—Like SQL DBs, NoSQL DBs such as
    MongoDB have also added vector search capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graph databases with vector capabilitie**s*—Graph DBs such as Neo4j, have
    also opened new possibilities by adding vector capabilities, .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using a vector index such as FAISS is supported by LangChain. To use FAISS,
    we first must install the `faiss-cpu` library. We will use the chunks already
    created in section 3.2 and the OpenAI embeddings that we used in section 3.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With this code, the 285 chunks of data have been converted to vector embeddings,
    and these embeddings are stored in a FAISS vector index. The FAISS vector index
    can also be saved to memory using the `vector_store.save_local(folder_path,index_name)`
    and `FAISS.load_local(folder_path,index_name)` functions. Let’s now take a cursory
    look at how a vector store can be used. We will take the original question that
    we have been asking from the beginning of this book: “Who won the 2023 Cricket
    World Cup?”'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Similarity search orders the chunks in descending order of similarity, meaning
    that the most similar chunks to the query are ranked on top. In the previous example,
    we can observe that the chunk that speaks about the world cup final has been ranked
  prefs: []
  type: TYPE_NORMAL
- en: on top.
  prefs: []
  type: TYPE_NORMAL
- en: FAISS is a stripped-down high-performance vector index that works for many applications.
    ChromaDB is another user-friendly vector DB that has gained popularity. Pinecone
    offers managed services and customization. Milvus claims higher performance on
    similarity search, while Qdrant provides an advanced filtering system. We will
    now discuss some points on how to choose a vector database that works best for
    your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Choosing a vector database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All vector databases offer the same basic capabilities, but each one of them
    also claims a differentiated value. Your choice should be influenced by the nuance
    of your use case matching with the value proposition of the database. Here are
    a few things to consider while evaluating and implementing a vector database:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy vs. spee**d*—Certain algorithms are more accurate but slower. A balance
    between search accuracy and query speed must be achieved based on application
    needs. It will become important to evaluate vector DBs on these parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flexibility vs. performanc**e*—Vector DBs provide customizations to the user.
    While it may help you in tailoring the DB to your specific requirements, more
    customizations can add overhead and slow systems down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Local vs. cloud storag**e*—Assess tradeoffs between local storage speed and
    access versus cloud storage benefits like security, redundancy, and scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Direct access vs. AP**I*—Determine if tight integration control via direct
    libraries is required or if ease-of-use abstractions like APIs better suit your
    use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Simplicity vs. advanced feature**s*—Compare advanced algorithm optimizations,
    query features, and indexing versus how much complexity your use case necessitates
    versus needs for simplicity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cos**t*—While you may incur regular costs in a fully managed solution, a self-hosted
    one might prove costlier if not managed well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have now completed an end-to-end indexing of a document. We continued with
    the same question (“Who won the 2023 Cricket World Cup?”) and the same external
    source—the Wikipedia page of the 2023 Cricket World Cup ([https://mng.bz/yN4J](https://mng.bz/yN4J)).
    In this chapter, we started with the programmatic loading of this Wikipedia page
    extracting the HTML document and then parsing the HTML document to extract. Thereafter,
    we divided the text into small-sized chunks using a specialized and fixed-width
    chunking method. We converted these chunks into embeddings using OpenAI’s text-embedding-003-large
    model. Finally, we stored the embeddings into a FAISS vector index. We also saw
    how using similarity search on this vector index helped us retrieve relevant chunks.
  prefs: []
  type: TYPE_NORMAL
- en: When several such documents in different formats from different sources are
    indexed using a combination of methods and strategies, we can store all the information
    in the form of vector embeddings creating a non-parametric knowledge base for
    our RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on the indexing pipeline. By now, you must have
    built a solid understanding of the four components of the indexing pipeline and
    should be ready to build a knowledge base for a RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will use this knowledge base to generate real-time responses
    to user queries through the generation pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data loading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process of sourcing data from its original location is called *data loading*,
    and it includes the following four steps: connecting to the source, extracting
    and parsing text, reviewing and updating metadata, and cleaning and transforming
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading documents from a list of sources may turn out to be a complicated process.
    Make sure to plan for all the sources and loaders in advance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A variety of data loaders from LangChain can be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking down long pieces of text into manageable sizes is called *data splitting*
    or *chunking*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking addresses context window limits of LLMs, mitigates the lost-in-the-middle
    problem for long prompts, and enables easier search and retrieval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chunking process involves dividing longer texts into small units, merging
    small units into chunks, and including an overlap between chunks to preserve contextual
    continuity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking can be fixed size, specialized (or adaptive), or semantic. Newer chunking
    methods are constantly being introduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your choice of the chunking strategy should be based on the nature of the content,
    expected length and complexity of user query, application use case, and the embeddings
    model being used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A chunking strategy can include multiple methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data conversion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For processing, text needs to be converted into a numerical format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings are vector representations of data (words, sentences, documents,
    etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of an embedding algorithm is to position similar data points close
    to each other in a vector space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several pre-trained, open source and proprietary, embedding models are available
    for use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings models enable similarity search. Embeddings can be used for text
    search, clustering, ML models, and recommendation engines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of embeddings is largely based on the use case and the cost implications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector databases are designed to efficiently store and retrieve high-dimensional
    vector data such as embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector databases provide similarity searches based on distance metrics such
    as cosine similarity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from the similarity search, vector databases offer traditional services
    such as scalability, security, versioning, and the like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector capabilities can be offered by standalone vector indexes, specialized
    vector databases, or legacy offerings such as search platforms, SQL, and NoSQL
    databases with added vector capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy, speed, flexibility, storage, performance, simplicity, access, and
    cost are some of the factors that can influence the choice of a vector database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
