["```py\n\"Ġoffice\": 3906\n\"Ġofficer\": 5908\n\"Ġofficers\": 6251\n\"ĠOffice\": 7454\n\"ĠOfficer\": 12743\n\"Ġoffices\": 14145\n\"office\": 30496\n\"Office\": 33577\n\"ĠOfficers\": 37209\n```", "```py\n\"93\": 4590\n\"937\": 47508\n\"930\": 48180\n```", "```py\n\"]);\": 9259\n```", "```py\n'As a language model, I am trained to <UNK> sequences, and output <UNK> text'.\n```", "```py\n!pip install transformers accelerate sentencepiece\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-largel\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\",\n    device_map=\"auto\")\n\ninput_text = \"what is 937 + 934?\"\nencoded_text = tokenizer.encode(input_text)\ntokens = tokenizer.convert_ids_to_tokens(encoded_text)\nprint(encoded_text)\nprint(tokens)\n```", "```py\n[125, 19, 668, 4118, 1768, 668, 3710, 58, 1]\n['▁what', '▁is', '▁9', '37', '▁+', '▁9', '34', '?', '</s>']\n```", "```py\ninput_text = \"Insuffienct adoption of corduroy pants is the reason this\n\neconomy is in the dumps!!!\"\nencoded_text = tokenizer.encode(input_text)\ntokens = tokenizer.convert_ids_to_tokens(encoded_text)\nprint(tokens)\n```", "```py\n['▁In', 's', 'uff', 'i', 'en', 'c', 't', '▁adoption', '▁of', '▁cord', 'u',\n'roy', '▁pants', '▁is', '▁the', '▁reason', '▁this', '▁economy', '▁is', '▁in',\n'▁the', '▁dump', 's', '!!!', '</s>']\n```", "```py\n!pip install tiktoken\n\nimport tiktoken\ntiktoken.list_encoding_names()\n```", "```py\n['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']\n```", "```py\nencoding = tiktoken.encoding_for_model(\"gpt-4\")\ninput_ids = encoding.encode(\"Insuffienct adoption of corduroy pants is the\n\nreason this economy is in the dumps!!!\")\ntokens = [encoding.decode_single_token_bytes(token) for token in input_ids]\n```", "```py\n[b'Ins', b'uff', b'ien', b'ct', b' adoption', b' of', b' cord', b'uro', b'y',\nb' pants', b' is', b' the', b' reason', b' this', b' economy', b' is', b' in',\nb' the', b' dumps', b'!!!']\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nprint(tokenizer.backend_tokenizer.normalizer.normalize_str(\n    'Pédrò pôntificated at üs:-)')\n```", "```py\npedro pontificated at us:-)\n```", "```py\n\\w+|[^\\w\\s]+\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"I'm starting to\n\nsuspect - I am 55 years old!   Time to vist New York?\")\n```", "```py\n[(\"▁I'm\", (0, 3)),\n ('▁starting', (3, 12)),\n ('▁to', (12, 15)),\n ('▁suspect', (15, 23)),\n ('▁-', (23, 25)),\n ('▁I', (25, 27)),\n ('▁am', (27, 30)),\n ('▁55', (30, 33)),\n ('▁years', (33, 39)),\n ('▁old!', (39, 44)),\n ('▁', (44, 45)),\n ('▁', (45, 46)),\n ('▁Time', (46, 51)),\n ('▁to', (51, 54)),\n ('▁vist', (54, 59)),\n ('▁New', (59, 63)),\n ('▁York?', (63, 69))]\n```", "```py\n'bat', 'cat', 'cap', 'sap', 'map', 'fan'\n```", "```py\n'b', 'a', 't', 'c', 'p', 's', 'm', 'f', 'n'\n```", "```py\n'ba' - 1, 'at' - 2, 'ca' - 2, 'ap' - 3, 'sa' - 1, 'ma' - 1, 'fa' - 1, 'an' - 1\n```", "```py\n'b', 'a', 't', 'c', 'p', 's', 'm', 'f', 'n', 'ap'\n```", "```py\n'ba' - 1, 'at' - 2, 'cap' - 1, 'sap' - 1, 'map' - 1, 'fa' - 1, 'an' - 1\n```", "```py\nscore = freq(a,b)/(freq(a) * freq(b))\n```", "```py\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\",\n    device_map=\"auto\")\n\ntokenizer.add_tokens([\"CAR-T\", \"antisense\", \"oligonucleotides\"])\nmodel.resize_token_embeddings(len(tokenizer))\n```"]