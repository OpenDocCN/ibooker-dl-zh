["```py\n\"Ġoffice\": 3906\n\"Ġofficer\": 5908\n\"Ġofficers\": 6251\n\"ĠOffice\": 7454\n\"ĠOfficer\": 12743\n\"Ġoffices\": 14145\n\"office\": 30496\n\"Office\": 33577\n\"ĠOfficers\": 37209\n```", "```py\n\"93\": 4590\n\"937\": 47508\n\"930\": 48180\n```", "```py\n\"]);\": 9259\n```", "```py\n'As a language model, I am trained to <UNK> sequences, and output <UNK> text'.\n```", "```py\n!pip install transformers accelerate sentencepiece\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-largel\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\",\n    device_map=\"auto\")\n\ninput_text = \"what is 937 + 934?\"\nencoded_text = tokenizer.encode(input_text)\ntokens = tokenizer.convert_ids_to_tokens(encoded_text)\nprint(encoded_text)\nprint(tokens)\n```", "```py\n[125, 19, 668, 4118, 1768, 668, 3710, 58, 1]\n['▁what', '▁is', '▁9', '37', '▁+', '▁9', '34', '?', '</s>']\n```", "```py\ninput_text = \"Insuffienct adoption of corduroy pants is the reason this `economy` `is` `in` `the` `dumps``!!!``\"` ```", "```py\n```", "```py```", "```py```", "``` ['▁In', 's', 'uff', 'i', 'en', 'c', 't', '▁adoption', '▁of', '▁cord', 'u', 'roy', '▁pants', '▁is', '▁the', '▁reason', '▁this', '▁economy', '▁is', '▁in', '▁the', '▁dump', 's', '!!!', '</s>'] ```", "``` !pip install tiktoken  import tiktoken tiktoken.list_encoding_names() ```", "``` ['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base'] ```", "``` encoding = tiktoken.encoding_for_model(\"gpt-4\") input_ids = encoding.encode(\"Insuffienct adoption of corduroy pants is the `reason` `this` `economy` `is` `in` `the` `dumps``!!!``\")` ```", "``` ```", "````` ```py`The output is:    ``` [b'Ins', b'uff', b'ien', b'ct', b' adoption', b' of', b' cord', b'uro', b'y', b' pants', b' is', b' the', b' reason', b' this', b' economy', b' is', b' in', b' the', b' dumps', b'!!!'] ```py    As you can see there is not much difference between the tokenization used by GPT-4 and FLAN-T5.    ###### Tip    For a given task, if you observe strange behavior from LLMs on only a subset of your inputs, it is worthwhile to check how they have been tokenized. While you cannot definitively diagnose your problem just by analyzing the tokenization, it is often helpful in analysis. In my experience, a non-negligible number of LLM failures can be attributed to the way the text was tokenized. This is especially true if your target domain is different from the pre-training domain.```` ```py`` `````", "``````py`  ``````", "``````py` ``````", "``` tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") print(tokenizer.backend_tokenizer.normalizer.normalize_str(     'Pédrò pôntificated at üs:-)') ```", "``` pedro pontificated at us:-) ```", "``` \\w+|[^\\w\\s]+ ```", "``` tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\") tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"I'm starting to `suspect` `-` `I` `am` `55` `years` `old``!`   `Time` `to` `vist` `New` `York``?``\")` ```", "````` ```py`The output is:    ``` [(\"▁I'm\", (0, 3)),  ('▁starting', (3, 12)),  ('▁to', (12, 15)),  ('▁suspect', (15, 23)),  ('▁-', (23, 25)),  ('▁I', (25, 27)),  ('▁am', (27, 30)),  ('▁55', (30, 33)),  ('▁years', (33, 39)),  ('▁old!', (39, 44)),  ('▁', (44, 45)),  ('▁', (45, 46)),  ('▁Time', (46, 51)),  ('▁to', (51, 54)),  ('▁vist', (54, 59)),  ('▁New', (59, 63)),  ('▁York?', (63, 69))] ```py    Along with the pre-tokens (or word tokens), the character offsets are returned.    The T5 pre-tokenizer splits only on whitespace, doesn’t collapse multiple spaces into one, and doesn’t split on punctuation or numbers. The behavior can be vastly different for other tokenizers.```` ```py``  `````", "```py`## Tokenization    After the optional pre-tokenization step, the actual tokenization step is performed. Some of the important algorithms in this space are byte pair encoding (BPE), byte-level BPE, WordPiece, and Unigram LM. The tokenizer comprises a set of rules that is learned during a pre-training phase over a pre-training dataset. Now let’s go through these algorithms in detail.    ## Byte Pair Encoding    This algorithm is the simplest and most widely used tokenization algorithm.    ### Training stage    We take a training dataset, run it through the normalization and pre-tokenization steps discussed earlier, and record the unique tokens in the resulting output and their frequencies. We then construct an initial vocabulary consisting of the unique characters that make up these tokens. Starting from this initial vocabulary, we continue adding new tokens using *merge* rules. The merge rule is simple; we create a new token using the most frequent consecutive pairs of tokens. The merges continue until we reach the desired vocabulary size.    Let’s explore this with an example. Imagine our training dataset is composed of six words, each appearing just once:    ```", "```py    The initial vocabulary is then made up of:    ```", "```py    The frequencies of contiguous token pairs are:    ```", "```py    The most frequent pair is “ap,” so the first merge rule is to merge “a” and “p.” The vocabulary now is:    ```", "```py    The new frequencies are:    ```", "```py    Now, the most frequent pair is “at,” so the next merge rule is to merge “a” and “t.” This process continues until we reach the vocabulary size.    ### Inference stage    After the tokenizer has been trained, it can be used to divide the text into appropriate subword tokens and feed the text into the model. This happens in a similar fashion as the training step. After normalization and pre-tokenization of the input text, the resulting tokens are broken into individual characters, and all the merge rules are applied in order. The tokens standing after all merge rules have been applied are the final tokens, which are then fed to the model.    You can open the [vocabulary file](https://oreil.ly/7JAyY) for GPT-NeoX again, and Ctrl+F “merges” to see the merge rules. As expected, the initial merge rules join single characters with each other. At the end of the merge list, you can see larger subwords like “out” and “comes” being merged into a single token.    ###### Note    Since all unique individual characters in the tokenizer training set will get their own token, it is guaranteed that there will be no OOV tokens as long as all tokens seen during inference in the future are made up of characters that were present in the training set. But Unicode consists of over a million code points and around 150,000 valid characters, which would not fit in a vocabulary of size 30,000\\. This means that if your input text contained a character that wasn’t in the training set, that character would be assigned an <UNK> token. To resolve this, a variant of BPE called byte-level BPE is used. Byte-level BPE starts with 256 tokens, representing all the characters that can be represented by a byte. This ensures that every Unicode character can be encoded just by the concatenation of the constituent byte tokens. Hence, it also ensures that we will never encounter an <UNK> token. The GPT family of models use this tokenizer.    ## WordPiece    WordPiece is similar to BPE, so we will highlight only the differences.    Instead of the frequency approach used by BPE, WordPiece uses the maximum likelihood approach. The frequency of the token pairs in the dataset is normalized by the product of the frequency of the individual tokens. The pairs with the resulting highest score are then merged:    ```", "```py    This means that if a token pair is made up of tokens that individually have low frequency, they will be merged first.    [Figure 3-4](#WordPiece) shows the merge priority and how the normalization by individual frequencies affects the order of merging.  ![WordPiece tokenization](assets/dllm_0304.png)  ###### Figure 3-4\\. WordPiece tokenization    During inference, merge rules are not used. Instead, for each pre-tokenized token in the input text, the tokenizer finds the longest subword from the vocabulary in the token and splits on it. For example, if the token is “understanding” and the longest subword in the dictionary within this token is “understand,” then it will be split into “understand” and “ing.”    ### Postprocessing    Now that we have looked at a couple of tokenizer algorithms, let’s move on to the next stage of the pipeline, the postprocessing stage. This is where model-specific special tokens are added. Common tokens include [CLS], the classification token used in many language models, and [SEP], a separator token used to separate parts of the input.    ## Special Tokens    Depending on the model, a few special tokens are added to the vocabulary to facilitate processing. These tokens can include:    <PAD>      To indicate padding, in case the size of the input is less than the maximum sequence length.      <EOS>      To indicate the end of the sequence. Generative models stop generating after outputting this token.      <UNK>      To indicate an OOV term.      <TOOL_CALL>, </TOOL_CALL>      Content between these tokens is used as input to an external tool, like an API call or a query to a database.      <TOOL_RESULT>, </TOOL_RESULT>      Content between these tokens is used to represent the results from calling the aforementioned tools.      As we have seen, if our data is domain-specific like healthcare, scientific literature, etc., tokenization from a general-purpose tokenizer will be unsatisfactory. GALACTICA by Meta introduced several domain-specific tokens in their model and special tokenization rules:    *   [START_REF] and [END_REF] for wrapping citations.           *   <WORK> to wrap tokens that make up an internal working memory, used for reasoning and code generation.           *   Numbers are handled by assigning each digit in the number its own token.           *   [START_SMILES], [START_DNA], [START_AMINO], [END_SMILES], [END_DNA], [END_AMINO] for protein sequences, DNA sequences, and amino acid sequences, respectively.              If you are using a model on domain-specific data like healthcare, finance, law, biomedical, etc., with a tokenizer that was trained on general-purpose data, the compression ratio will be relatively lower because domain-specific words do not have their own tokens and will be split into multiple tokens. One way to adapt models to specialized domains is for models to learn good vector representations for domain-specific terms.    To this end, we can add new tokens to existing tokenizers and continue pre-training the model on domain-specific data so that those new domain-specific tokens learn effective representations. We will learn more about continued pre-training in [Chapter 7](ch07.html#ch07).    For now, let’s see how we can add new tokens to a vocabulary using Hugging Face.    Consider the sentence, “The addition of CAR-T cells and antisense oligonucleotides drove down incidence rates.” The FLAN-T5 tokenizer splits this text as follows:    *   ['▁The', '▁addition', '▁of', '▁C', ' AR', '-', ' T', '▁cells', '▁and', '▁anti', ' s', ' ense', '▁', ' oli', ' gon', ' u', ' cle', ' o', ' t', ' ides', '▁drove', '▁down', '▁incidence', '▁rates', ' .', '</s>']    Let’s add the domain-specific terms to the vocabulary:    ```", "```py    Now, tokenizing the string again gives the following tokens, with the domain-specific tokens being added:    *   ['▁The', '▁addition', '▁of', ' CAR-T', '▁cells', '▁and', ' antisense', ' oligonucleotides', '▁drove', '▁down', '▁incidence', '▁rates', ' .', '</s>']    We are only halfway done here. The embedding vectors corresponding to these new tokens do not contain any information about these tokens. We will need to learn the right representations for these tokens, which we can do using fine-tuning or continued pre-training, which we will discuss in [Chapter 7](ch07.html#ch07).```", "```py``  `` `# Summary    In this chapter, we focused on a key ingredient of language models: their vocabulary. We discussed how vocabularies are defined and constructed in the realm of language models. We introduced the concept of tokenization and presented tokenization algorithms like BPE and WordPiece that are used to construct vocabularies and break down raw input text into a sequence of tokens that can be consumed by the language model. We also explored the vocabularies of popular language models and noted how tokens can differ from human conceptions of a word.    In the next chapter, we will continue exploring the remaining ingredients of a language model, including its architecture and the learning objectives on which models are trained.` `` ```", "```py ```", "```py` ```"]