- en: Chapter 9\. Classification Models and Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Forethought spares afterthought.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Amelia Barr
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There’s a reason you don’t just dump data into a model. Neural networks operate
    at intense speeds and perform complex calculations the same way humans can have
    an instantaneous reaction. However, for both humans and machine learning models,
    a reaction rarely contains a reasoned context. Dealing with dirty and confusing
    data creates subpar models, if anything at all. In this chapter, you’ll explore
    the process of identifying, loading, cleaning, and refining data to improve the
    training accuracy of a model in TensorFlow.js.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify how to make a classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to handle CSV data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn about Danfo.js and DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify how to get messy data into training (wrangle your data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice graphing and analyzing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn about machine learning notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expose core concepts of feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you finish this chapter, you’ll feel confident in gathering large amounts
    of data, analyzing it, and testing your intuitions by using context to create
    features that help models train.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll build a *Titanic* life-or-death classifier. Will Miss
    Kate Connolly, a 30-year-old woman with a third-class ticket, survive? Let’s train
    a model to take that information and give us a likelihood of survival.
  prefs: []
  type: TYPE_NORMAL
- en: Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, you’ve trained a model that outputs numbers. Most of the models you’ve
    consumed behave a bit differently from the ones you’ve created. In [Chapter 8](ch08.html#the_chapter_8)
    you implemented linear regression, but in this chapter, you will implement a classification
    model (sometimes called *logistic regression*).
  prefs: []
  type: TYPE_NORMAL
- en: The Toxicity, MobileNet, and even Tic-Tac-Toe models output a single choice
    among a collection of options. They do so with a group of numbers that sum to
    one, rather than a single number that has no range. This is a common structure
    for classification models. A model that is made to identify three different options
    will give us numbers that correspond with each option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Models that attempt to predict classifications require some kind of mapping
    from output values to their associated classes. This is most commonly done with
    outputting their probability, like you’ve seen in classification models so far.
    To create a model that does this, you only need to implement special activation
    functions on the final layer:'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that activation functions help your neural network behave in a nonlinear
    fashion. Each activation function causes a layer to behave nonlinearly in a desired
    way, and the final layer’s activation translates directly to the output. It’s
    important to make sure you learn what activation will give you the model output
    you’re seeking.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function you’ve seen over and over in models used in this book
    is called a *softmax* activation. That’s the group of values that sum to one.
    For example, if your model would have a True/False output, you’d expect a model
    to output two values, with one identifying probability `true` and the other for
    `false`. For example, a softmax for this model could output `[0.66, 0.34]` with
    some rounding.
  prefs: []
  type: TYPE_NORMAL
- en: This can scale to N values for N classifications *as long as classes are mutually
    exclusive*. When designing the model, you would enforce softmax in the final layer,
    and the number of outputs would be the number of categories you’re looking to
    support. To achieve the True or False result, your model architecture would have
    two outputs with a softmax activation on the final layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'What if you were trying to detect several things from your input? For example,
    a chest X-ray could be positive for both pneumonia and emphysema. Softmax wouldn’t
    work in that case, as the outputs have to sum to one, and confidence in one would
    have to fight against another. In this case, there’s an activation that enforces
    each node to be a value between zero and one, so you can achieve probability per
    node. The activation is called the *sigmoid* activation. This can scale to N values
    for N classifications that are not exclusive. That means you could achieve a True/False
    model (binary classification) by having a single output with `sigmoid` where close
    to zero is false, and close to one is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Yes, these activation names are strange, but they aren’t complicated. You could
    easily lose a day in a YouTube rabbit hole by researching the math behind how
    these activation functions work. But most importantly, understand how they are
    used in classification. Here in [Table 9-1](#binary_classification_example_table)
    you’ll see some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-1\. Binary classification examples
  prefs: []
  type: TYPE_NORMAL
- en: '| Activation | Output | Analysis of results |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| sigmoid | `[0.999999]` | 99% sure it is True |'
  prefs: []
  type: TYPE_TB
- en: '| softmax | `[0.99, 0.01]` | 99% sure it is True |'
  prefs: []
  type: TYPE_TB
- en: '| sigmoid | `[0.100000]` | 10% sure of True (so 90% False) |'
  prefs: []
  type: TYPE_TB
- en: '| softmax | `[0.10, 0.90]` | 90% sure it is False |'
  prefs: []
  type: TYPE_TB
- en: The difference between when you would use `softmax` versus `sigmoid` goes away
    when you are handling True/False. There’s no real difference in which activation
    you choose for your final layer because there’s nothing that one could exclude.
    In this chapter, we’ll be using sigmoid in the last layer for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: If you were trying to classify multiple things, you’d need to make an intelligent
    choice between `sigmoid` or `softmax`. This book will reiterate and clarify the
    use of these activation functions where applicable.
  prefs: []
  type: TYPE_NORMAL
- en: The Titanic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On April 15, 1912, the “unsinkable” RMS *Titanic* (see [Figure 9-1](#titanic))
    sank. This tragedy is popularized throughout history books, tales of hubris, and
    even a feature film starring Leonardo DiCaprio and Kate Winslet. This tragic event
    is awe-inspiring with a hint of morbid curiosity. If you visit the *Titanic* exhibit
    at the Luxor in Las Vegas, your ticket assigns you the name of a passenger, and
    tells you your ticket price, your cabin class, and several other things about
    your life. As you peruse the ship and the accommodations, you can experience it
    through the eyes of the person on your ticket. At the end of the exhibit, you
    find out if the person printed on your ticket survived.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Titanic profile](assets/ltjs_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. The RMS Titanic
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Was it 100% random who lived and who didn’t? Anyone familiar with the history
    or who’s watched the movie knows it was no coin flip. Maybe you can train a model
    to find patterns in the data. Thankfully, the guest log and the survivor list
    are available for us to use.
  prefs: []
  type: TYPE_NORMAL
- en: Titanic Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with most things these days, the data has been transcribed to a digital format.
    The *Titanic* manifest is available in comma-separated values (CSV) form. This
    tabular data can be read by any spreadsheet software. There are lots of copies
    of the *Titanic* dataset available, and they generally have the same information.
    The CSV files that we’ll be using can be found in the associated code for this
    chapter in the [extra folder](https://oreil.ly/ry4Pf).
  prefs: []
  type: TYPE_NORMAL
- en: This *Titanic* dataset contains column data shown in [Table 9-2](#titanic_data_table).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-2\. Titanic data
  prefs: []
  type: TYPE_NORMAL
- en: '| Column | Definition | Legend |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| survival | Survived | 0 = No, 1 = Yes |'
  prefs: []
  type: TYPE_TB
- en: '| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |'
  prefs: []
  type: TYPE_TB
- en: '| sex | Sex |  |'
  prefs: []
  type: TYPE_TB
- en: '| Age | Age in years |  |'
  prefs: []
  type: TYPE_TB
- en: '| sibsp | Number of siblings or spouses aboard |  |'
  prefs: []
  type: TYPE_TB
- en: '| parch | Number of parents or children aboard |  |'
  prefs: []
  type: TYPE_TB
- en: '| ticket | Ticket number |  |'
  prefs: []
  type: TYPE_TB
- en: '| fare | Passenger fare |  |'
  prefs: []
  type: TYPE_TB
- en: '| cabin | Cabin number |  |'
  prefs: []
  type: TYPE_TB
- en: '| embarked | Port of embarkation | C = Cherbourg, Q = Queenstown, S = Southampton
    |'
  prefs: []
  type: TYPE_TB
- en: So how do you get this CSV data into tensor form? One way would be to read the
    CSV file and convert each of the inputs into a tensor representation for training.
    This sounds like quite a significant task, especially when you’re looking to experiment
    with what columns and formats would be most useful for training your model.
  prefs: []
  type: TYPE_NORMAL
- en: In the Python community, a popular way to load, modify, and train with data
    is to use a library called [Pandas](https://pandas.pydata.org). This open source
    library is prevalent for data analysis. While this is quite useful for Python
    developers, there is a significant need for a similar tool in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Danfo.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Danfo.js](https://danfo.jsdata.org) is an open source JavaScript alternative
    to Pandas. The API of Danfo.js is purposefully kept close to Pandas to capitalize
    on informational experience sharing. Even the function names in Danfo.js are `snake_case`
    instead of the standard JavaScript `camelCase` format. This means that you can
    utilize years of tutorials for Pandas in Danfo.js with minimal translation.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be using Danfo.js to read the *Titanic* CSV and modify it into TensorFlow.js
    tensors. To get started, you will need to add Danfo.js to a project.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the Node version of Danfo.js, you will run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then `require` Danfo.js if you’re using simple Node.js, or you can
    `import` if you’ve configured your code to use ES6+:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Danfo.js can run in the browser, too. This chapter depends on printing information
    more than usual, so it makes sense to utilize the full terminal window and rely
    on the simplicity of Node.js for access to local files.
  prefs: []
  type: TYPE_NORMAL
- en: Danfo.js is powered by TensorFlow.js behind the scenes, but it provides common
    data reading and processing utilities.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing for the Titanic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common criticisms of machine learning is that it comes off as
    a golden goose. You may think the next steps are to hook a model up to the CSV
    files, click Train, and then take the day off to enjoy a walk in the park. While
    efforts are being made daily to improve automation in machine learning, data is
    rarely in a format that is “ready to go.”
  prefs: []
  type: TYPE_NORMAL
- en: The *Titanic* data in this chapter contains alluring Train and Test CSV files.
    However, using Danfo.js, we’ll quickly see the provided data is far from ready
    to be loaded into tensors. It’s the goal of this chapter for you to identify data
    in this shape and prepare it appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the CSV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The CSV file is loaded into a construct called a DataFrame. The DataFrame is
    like a spreadsheet with columns of potentially different types and rows of individual
    entries that fit those types, like a series of objects.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames have the ability to print their contents to the console, as well
    as plenty of other helper functions to review and edit the contents programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s review the following code, which reads the CSV into a DataFrame and then
    prints a few rows to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_classification_models_and_data_analysis_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `read_csv` method can read from a URL or a local file URI.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_classification_models_and_data_analysis_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame can be limited to the head of five rows and then printed.
  prefs: []
  type: TYPE_NORMAL
- en: The CSV being loaded is the training data, and the `print()` command logs the
    contents of a DataFrame to the console. The results are displayed in the console,
    as shown in [Figure 9-2](#danfo_table).
  prefs: []
  type: TYPE_NORMAL
- en: '![Head printout](assets/ltjs_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Printing the CSV DataFrame head
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Upon examining the content of the data, you might notice some strange entries,
    especially in the `Cabin` column, that say `NaN`. These represent missing data
    in the dataset. This is one of the reasons you can’t hook the CSV directly to
    a model: it’s important to identify how to handle the missing information. We’ll
    assess this issue shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Danfo.js and Pandas have many useful commands to help you familiarize yourself
    with the data you’ve loaded. One popular method is to call `.describe()`, which
    attempts to analyze the contents of each column as a report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you print the DataFrame’s `describe` data, you’ll see that the CSV you’ve
    loaded has 891 entries, as well as a printout of their max, min, median, etc.,
    so you can validate the information. The printed table looks like [Figure 9-3](#danfo_describe).
  prefs: []
  type: TYPE_NORMAL
- en: '![Describe printout](assets/ltjs_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Describing the DataFrame
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some columns have been removed from [Figure 9-3](#danfo_describe) because they
    contain non-numeric data. This is something you will solve in Danfo.js easily.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating the CSV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This CSV reflects the real world of data, where there’s often missing information.
    Before training, you’ll need to handle this.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all missing fields with `isna()`, which will return `true` or
    `false` for each missing field. You can then sum or count these values to get
    results. The following is the code that will report empty cells or properties
    of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With the results you can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Empty `Age` values: 177 (20%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Empty `Cabin` values: 687 (77%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Empty `Embarked` values: 2 (0.002%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a small glance at how much data is missing, you can see you’re not getting
    around cleaning this data. It’s going to be critical to solve the missing-values
    problem, removing useless columns like `PassengerId` and ultimately encoding the
    non-numeric columns you want to keep.
  prefs: []
  type: TYPE_NORMAL
- en: So you don’t have to do it twice, you might as well combine the CSV files, clean
    them, and then create two new CSV files that are ready for training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, these are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Combine the CSV files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean the DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-create the CSV files from the DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combining CSVs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To combine the CSVs, you’ll create two DataFrames and then concatenate them
    along an axis like you would for tensors. You may feel your tensor training guiding
    you on the path with managing and cleaning data, and that’s no mistake. While
    the terminology can differ slightly, the concepts and intuition you’ve accumulated
    from the previous chapters will serve you well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_classification_models_and_data_analysis_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Prints “Train Size 891”
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_classification_models_and_data_analysis_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Prints “Test Size 418”
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_classification_models_and_data_analysis_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Displays a table with the count 1,309
  prefs: []
  type: TYPE_NORMAL
- en: With familiar syntax, you’ve loaded two CSV files and combined them into a singular
    DataFrame named `mega`, which you can now clean.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning CSVs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is where you’ll handle blanks and identify what data is actually useful.
    There are three operations that you need to do to properly prepare the CSV data
    for training:'
  prefs: []
  type: TYPE_NORMAL
- en: Prune the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Handle the blanks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Migrate to numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pruning features means removing features that have little to no influence on
    the outcome of the result. For this, you can experiment, graph the data, or simply
    use your personal intuition. To prune the features, you can use the DataFrame’s
    `.drop` function. The `.drop` function can remove entire columns or specified
    rows from a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: For this dataset, we will be dropping the columns that have little influence,
    such as the passenger’s name, ID, ticket, and cabin. You might argue that many
    of those features could be quite significant, and you’d be right. However, we’ll
    leave you to research these features outside the confines of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To handle blanks, you can fill or remove rows. Filling empty rows is a craft
    called *imputation*. While this is a great skill to read up on, it can get complicated.
    We’ll be taking the easy road in this chapter and merely removing any row that
    has missing values. To remove any rows with empty data, we can use the `dropna()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s critical that this is done *after* dropping columns. Otherwise, the 77%
    missing data from the `Cabin` column will destroy the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can drop all empty rows with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The result of this code drops the dataset from 1,309 to 1,043 rows. Consider
    this an experiment in laziness.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you are left with two columns that have strings instead of numbers (`Embarked`
    and `Sex`). These will need to be converted to numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Embarked` values, for review, are: C = Cherbourg, Q = Queenstown, S =
    Southampton. There are several ways this can be encoded. One is to encode them
    with a numeric equivalent. Danfo.js has a `LabelEncoder`, which can read an entire
    column and then transform the values to a numeric encoded equivalent. `LabelEncoder`
    encodes labels with values between `0` and `n-1` classes. To encode the `Embarked`
    column, you can use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_classification_models_and_data_analysis_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new `LabelEncoder` instance.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_classification_models_and_data_analysis_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Fit that instance to encode the contents of the `Embarked` column.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_classification_models_and_data_analysis_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Transform the column to values and immediately overwrite the current column
    with the generated one.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_classification_models_and_data_analysis_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Print the top five rows to verify the replacement occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Your intuition might be surprised with the ability to overwrite columns of a
    DataFrame like in step 3\. This is one of the many benefits of dealing with DataFrames
    over tensors, even though TensorFlow.js tensors power Danfo.js behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Now you can do the same thing to encode the `male` / `female` strings with the
    same trick. (Note that we’re simplifying sex to a binary for the purposes of the
    model and based on the data available in the passenger manifest.) Once done, your
    entire dataset is now numeric. If you call `describe` on the DataFrame, it will
    present all the columns, rather than just a few.
  prefs: []
  type: TYPE_NORMAL
- en: Saving new CSVs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve created a usable dataset for training, you’ll need to return
    the two CSV files, which had a friendly test-and-train split.
  prefs: []
  type: TYPE_NORMAL
- en: You can resplit the DataFrame using Danfo.js’s `.sample`. The `.sample` method
    randomly selects N rows from a DataFrame. From there, you can create the test
    set as the remaining unselected values. To remove the sampled values, you can
    drop rows by index rather than an entire column.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DataFrame object has a `to_csv` converter, which optionally takes a parameter
    of what file to write. The `to_csv` command writes the parameter file and returns
    a promise, which resolves to the CSV contents. The entire code to resplit the
    DataFrame and write two files could go like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now you have two files, one with 800 rows and the other with 243 for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Training on Titanic Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s one last step you’ll need to handle before training on the data, and
    that’s the classic machine learning labeled input and expected output (X and Y,
    respectively). This means you’ll need to separate the answers (the `Survived`
    column) from the other inputs. For this, you can use `iloc` to declare the index
    of columns to make new DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Since the first column is the `Survived` column, you’ll make your X skip that
    column and grab all the rest. You’ll identify from index one to the end of the
    DataFrame. This is written as `1:`. You could write `1:9`, which would grab the
    same set, but the `1:` means “everything after index zero.” The `iloc` index format
    represents the range you’re selecting for your DataFrame subset.
  prefs: []
  type: TYPE_NORMAL
- en: The Y values, or *answers,* are selected by grabbing the `Survived` column.
    Since this is a single column, there’s no need to use `iloc`. *Don’t forget to
    do the same for the test dataset*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning models expect tensors, and since Danfo.js is built on TensorFlow.js,
    it’s trivial to convert a DataFrame to a tensor. When all is said and done, you
    can convert a DataFrame by accessing the `.tensor` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The values are ready to be fed into a model for training.
  prefs: []
  type: TYPE_NORMAL
- en: The model I used for this problem after very little research was a sequential
    Layers model with three hidden layers and an output of one tensor with sigmoid
    activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is composed like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_classification_models_and_data_analysis_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Each layer is utilizing ReLU activation up until the final layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_classification_models_and_data_analysis_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This line tells the model to initialize weights based on an algorithm rather
    than simply setting the model’s initial weights to complete randomness. This sometimes
    helps a model start much closer to the answer. It’s not critical in this case,
    but it’s a useful feature of TensorFlow.js.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_classification_models_and_data_analysis_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The final layer uses sigmoid activation to print a number between zero and one
    (survived or did not survive).
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_classification_models_and_data_analysis_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: When training a binary classifier, it’s prudent to evaluate loss with a fancy
    named function that works with binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_classification_models_and_data_analysis_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: This displays accuracy in logs, not just loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you `fit` the model to the data, you can identify the testing data and
    get results on data the model has never seen before. This helps you stop from
    overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_classification_models_and_data_analysis_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Provide the data that the model should use to validate on each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The training configuration displayed in the previous `fit` method does not take
    advantage of callbacks. If you’re training on `tfjs-node`, you will automatically
    see training results printed to the console. If you use `tfjs`, you’ll need to
    add an `onEpochEnd` callback to print the training and validation accuracy. Examples
    of both are provided in the associated [source code for this chapter](https://oreil.ly/39p7V).
  prefs: []
  type: TYPE_NORMAL
- en: 'After training for 100 epochs, this model was 83% accurate with the training
    data and 83% accurate with the validation from the test set. Technically, the
    results will vary in each training, but they should be nearly the same: `acc=0.827
    loss=0.404 val_acc=0.831 val_loss=0.406`.'
  prefs: []
  type: TYPE_NORMAL
- en: The model has identified some patterns and beaten pure chance (50% accuracy).
    Lots of people stop here and celebrate creating a model that works 83% of the
    time with little or no effort. However, this is also a great opportunity to identify
    the benefits of Danfo.js and feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you glance around the internet, 80% is a common accuracy score for the *Titanic*
    dataset. We’ve beaten that score with no real effort. However, there’s still room
    for improving the model, and that comes directly from improving the data.
  prefs: []
  type: TYPE_NORMAL
- en: Was throwing blank data a good choice? Are there correlations that exist that
    could be better emphasized? Were the patterns properly organized for the model?
    The better you can prechew and organize the data, the better the model will be
    at finding and emphasizing patterns. Lots of breakthroughs in machine learning
    have come from techniques that simplify patterns before they are passed to the
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the “just dump the data” flatlines, and feature engineering grows.
    Danfo.js lets you level up your features by analyzing patterns and emphasizing
    key features. You can do this in your interactive Node.js read evaluate print
    loop (REPL), or you can even utilize web pages that have been constructed for
    evaluation and feedback loops.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to improve the model above 83% by determining and adding features
    to the data using a Danfo.js Notebook, called Dnotebook.
  prefs: []
  type: TYPE_NORMAL
- en: Dnotebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Danfo Notebook, or [Dnotebook](https://dnotebook.jsdata.org), is an interactive
    web page for experimenting, prototyping, and customizing data with Danfo.js. The
    Python equivalent is called a Jupyter Notebook. The data science you can achieve
    with this notebook will significantly help your models.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be using a Dnotebook to create and share live code, as well as take advantage
    of the built-in charting capabilities to find critical features and correlations
    in the *Titanic* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Dnotebook by creating a global command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When you run `$ dnotebook`, you’ll automatically run a local server and open
    a page to the local notebook site, which looks a bit like [Figure 9-4](#dnotebook).
  prefs: []
  type: TYPE_NORMAL
- en: '![Dnotebook fresh screenshot](assets/ltjs_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Fresh Dnotebook running
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each Dnotebook cell can be code or text. The text is Markdown-formatted. The
    code can print output, and variables that are initialized with no `const` or `let`
    can survive across cells. See the example illustrated in [Figure 9-5](#dnotebook_vars).
  prefs: []
  type: TYPE_NORMAL
- en: '![Dnotebook demo screenshot](assets/ltjs_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Using Dnotebook cells
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The notebook in [Figure 9-5](#dnotebook_vars) can be downloaded and loaded from
    the *explaining_vars.json* file in this chapter’s [*extra/dnotebooks*](https://oreil.ly/pPvQu)
    folder. This makes it friendly for experimenting, saving, and sharing.
  prefs: []
  type: TYPE_NORMAL
- en: Titanic Visuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you can find correlations in the data, you can emphasize them as additional
    features in the training data and ideally improve the model’s accuracy. Using
    the Dnotebook, you can visualize your data and add comments along the way. This
    is an excellent resource for analyzing the dataset. We’ll load the two CSV files
    and combine them, and then we’ll print the results directly in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: You can create your own notebook, or you can load the JSON for the displayed
    notebook from the associated source code. Any method is fine as long as you’re
    able to follow along with what is displayed in [Figure 9-6](#dnotebook_load).
  prefs: []
  type: TYPE_NORMAL
- en: '![instructional code screenshot](assets/ltjs_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. Loading the CSVs and combining them in the Dnotebook
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `load_csv` command is similar to the `read_csv` command, but it shows a
    friendly spinner in the web page while loading the CSV content. You may also notice
    the use of a `table` command. The `table` command is similar to the DataFrame’s
    `print()` except that it generates an HTML table of the output for the notebook,
    as you see in [Figure 9-6](#dnotebook_load).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have the data, let’s look for essential distinctions that we can
    emphasize for our model. In the movie *Titanic*, they were shouting “Women and
    children first” when loading the lifeboats. Was that what really happened? One
    idea is to check the survival rate of men versus women. You can do this by using
    `groupby`. And then you can print the average (mean) of each group.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: And *voila!* You can see that 83% of females survived, whereas only 14% of males
    survived, as illustrated in [Figure 9-7](#dnotebook_male_v_female).
  prefs: []
  type: TYPE_NORMAL
- en: '![screenshot of survival rates](assets/ltjs_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Females were more likely to survive
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You might wonder if there were perhaps just more females aboard the *Titanic*
    and whether that accounts for the skewed results, so you can quickly check that
    using `count()` instead of using `mean()` like you did a moment ago:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: By the printed results, you can see there were far more men who survived despite
    the survival ratio leaning toward female. This means sex was an excellent indicator
    of chance of survival, so it would be a good feature to emphasize.
  prefs: []
  type: TYPE_NORMAL
- en: The real advantage of using Dnotebook is that it leverages Danfo.js charts.
    For instance, what if we’d like to see a histogram of the survivors? Rather than
    grouping users, you can query for all survivors and then plot the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To query for survivors, you can use the DataFrame’s query method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Then, to print a chart in Dnotebooks, you can use the built-in `viz` command,
    which requires an ID and callback for populating the generated DIV in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The histogram can be created with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The notebook will then display the resulting graph, as shown in [Figure 9-8](#dnotebook_age_hist).
  prefs: []
  type: TYPE_NORMAL
- en: '![screenshot of survival histogram](assets/ltjs_0908.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-8\. Survivor age histogram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here you can see significant survival rates of children over the elderly. Again,
    it might be worth determining the quantities and percentages of each, but it appears
    that specific buckets or bins of age groups fared better than others. This gives
    us a second way to possibly improve our model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the information we now have and take another shot at beating our record
    of 83% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Features (aka Preprocessing)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Growing up, I was told the more neurons you can activate for a memory, the stronger
    that memory will be, so remember the smell, the colors, and the facts together.
    Let’s see if the same goes for neural networks. We’ll move passenger sex to two
    inputs, and we’ll create a grouping of ages often called *bucketing* or *binning*.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we’ll do is move sex from one column to two columns. This is
    often called *one-hot encoding*. Currently, the `Sex` has a numeric encoding.
    A one-hot encoded version of the sex of a passenger would convert `0` to `[1,
    0]` and `1` to `[0, 1]`, successfully moving the value to two columns/units. Once
    converted, you remove the `Sex` column and insert two columns that look like [Figure 9-9](#danfo_one_hot_encoded).
  prefs: []
  type: TYPE_NORMAL
- en: '![Danfo One-Hot Coded](assets/ltjs_0909.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-9\. Describing sex one-hot encoded
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To one-hot encode, Danfo.js and Pandas have a `get_dummies` method that turns
    one column into several where only one of them has the value of 1\. In TensorFlow.js,
    the method for one-hot encoding is called `oneHot`, but here in Danfo.js, `get_dummies`
    is paying homage to the binary variables, which are often called *dummy variables*
    in statistics. Once you have the result encoded, you then use `drop` and `addColumn`
    to do the switch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_classification_models_and_data_analysis_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Using `get_dummies` to encode the column
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_classification_models_and_data_analysis_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Using an `inplace` drop on the `Sex` column
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_classification_models_and_data_analysis_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Adding the new column, switching title to male/female
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can create buckets for ages using the `apply` method. The `apply`
    method lets you run conditional code on the entire column. For our needs, we’ll
    define a function of significant age groups we saw in our charts, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can create and add a whole new column for these buckets using the
    `ageToBucket` function you defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This adds a whole column of values ranging from zero to two.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can normalize our data to be numbers between zero and one. Scaling
    the values normalizes the differences between values so the model can identify
    patterns and scale differences that were warped in the original numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Think of normalization as a feature. If you were working with 10 different currencies
    from various countries, it could be confusing to comprehend. Normalizing scales
    inputs so they all have relative magnitudes of influence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: From here, you can write out two CSV files for training and get started! Another
    option is that you could write a single CSV file, and rather than setting `validationData`
    with specific X and Y values, you can set a property called `validationSplit`,
    which will break off a percentage of the data for validation. This saves us a
    bit of time and headache, so let’s train the model using `validationSplit` instead
    of explicitly passing in `validationData`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting `fit` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The model trains with the new data for 100 epochs, and if you’re using `tfjs-node`,
    you can see the results printed even though there’s no callback defined.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineered Training Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last time, the model accuracy revolved around 83%. Now, using the same model
    structure but adding a few features, we reached 87% for training accuracy and
    87% for validation accuracy. Specifically, my results were `acc=0.867 loss=0.304
    val_acc=0.871 val_loss=0.370`.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy increased, and the loss values are lower than before. What’s really
    great is that both the accuracy and the validation accuracy are aligned, so it
    is unlikely that the model is overfitting. This is generally one of the better
    *Titanic* dataset scores for a neural network. For such a strange problem, creating
    a fairly accurate model has served the purpose of explaining what it’s like to
    pull useful information out of data.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solving the *Titanic* problem to achieve 87% accuracy took some finesse. You
    might still be wondering if the result could be improved, and the answer is most
    assuredly “yes” because others have posted more impressive scores to leaderboards.
    In situations without leaderboards, a common method for evaluating if there’s
    room for growth is to compare against what an educated human could score if given
    the same problem.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re a high-score junkie, the Chapter Challenge will be useful in improving
    the already impressive model we’ve created. Be sure to practice engineering features
    rather than overtraining and thus overfitting the model to essentially memorize
    the answers.
  prefs: []
  type: TYPE_NORMAL
- en: Finding important values, normalizing features, and emphasizing significant
    correlations is a useful skill in machine learning training, and now you can do
    so with Danfo.js.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So what happened to the individual we identified at the start of this chapter?
    Miss Kate Connolly, a 30-year-old woman with a third-class ticket, *did* survive
    the *Titanic*, and the model agreed.
  prefs: []
  type: TYPE_NORMAL
- en: Did we pass up some epic opportunity to increase the accuracy of the machine
    learning model? Perhaps we should have filled empty values with `-1` instead of
    deleting them? Maybe we should have examined the cabin structure of the *Titanic*?
    Or perhaps we should have looked at `parch`, `sibsp`, and `pclass` to create a
    new column for people who were traveling alone in third class? “I’ll never let
    go!”
  prefs: []
  type: TYPE_NORMAL
- en: Not all data can be cleaned and featured like this *Titanic* dataset was, but
    it was a useful adventure in data science for machine learning. There are plenty
    of CSVs available out there, and being confident in loading, understanding, and
    processing them is key to building novel models. Tools like Danfo.js enable you
    to process these mountains of data, and you can now add this to your machine learning
    tool chest.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re already a fan of other JavaScript notebooks like [ObservableHQ.com](https://observablehq.com),
    Danfo.js can be imported and easily integrated with those as well.
  prefs: []
  type: TYPE_NORMAL
- en: Working with data is a mixed bag. Some problems are more clear-cut and don’t
    require any adjustment to the features at all. If you’re interested, you should
    take a look at a simpler dataset like [the Palmer Penguins](https://oreil.ly/CiNv5).
    These penguins are significantly distinguishable into their species based on the
    shape and size of their bill. Another easy win is the Iris dataset mentioned in
    [Chapter 7](ch07.html#the_chapter_7).
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter Challenge: Ship Happens'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Did you know that not a single reverend survived the sinking of the *Titanic*?
    A bucket/bin of titles like Mr., Mrs., Ms., Rev., etc. might be useful to the
    learning of the model. These *honorifics*—yes, that’s what they are called—could
    be collected and analyzed from the `Name` column that was discarded.
  prefs: []
  type: TYPE_NORMAL
- en: In this Chapter Challenge, use Danfo.js to identify the honorifics used on the
    *Titanic* and their associated survival rates. This is an excellent opportunity
    for you to get comfortable with Dnotebooks.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  prefs: []
  type: TYPE_NORMAL
- en: Review Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What kind of activation function would you use for a rock-paper-scissors classifier?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many nodes would you put in the final layer of a sigmoid “Dog or Not” model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the command to load an interactive, locally hosted notebook that has
    Danfo.js built in?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you combine the data of two CSVs with the same columns?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What command would you use to one-hot encode a single column into multiple columns?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can you use to scale all the values of a DataFrame between 0 and 1?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  prefs: []
  type: TYPE_NORMAL
