["```py\n>>> import torch\n```", "```py\n>>> X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])\n>>> X\ntensor([[1., 4., 7.],\n [2., 3., 6.]])\n```", "```py\n>>> X.shape\ntorch.Size([2, 3])\n>>> X.dtype\ntorch.float32\n```", "```py\n>>> X[0, 1]\ntensor(4.)\n>>> X[:, 1]\ntensor([4., 3.])\n```", "```py\n>>> 10 * (X + 1.0)  # itemwise addition and multiplication\ntensor([[20., 50., 80.],\n [30., 40., 70.]])\n>>> X.exp()  # itemwise exponential\ntensor([[   2.7183,   54.5981, 1096.6332],\n [   7.3891,   20.0855,  403.4288]])\n>>> X.mean()\ntensor(3.8333)\n>>> X.max(dim=0)  # max values along dimension 0 (i.e., max value per column)\ntorch.return_types.max(values=tensor([2., 4., 7.]), indices=tensor([1, 0, 0]))\n>>> X @ X.T  # matrix transpose and matrix multiplication\ntensor([[66., 56.],\n [56., 49.]])\n```", "```py\n>>> import numpy as np\n>>> X.numpy()\narray([[1., 4., 7.],\n [2., 3., 6.]], dtype=float32)\n>>> torch.tensor(np.array([[1., 4., 7.], [2., 3., 6.]]))\ntensor([[1., 4., 7.],\n [2., 3., 6.]], dtype=torch.float64)\n```", "```py\n>>> torch.FloatTensor(np.array([[1., 4., 7.], [2., 3., 6]]))\ntensor([[1., 4., 7.],\n [2., 3., 6.]])\n```", "```py\n>>> X[:, 1] = -99\n>>> X\ntensor([[  1., -99.,   7.],\n [  2., -99.,   6.]])\n```", "```py\n>>> X.relu_()\n>>> X\ntensor([[1., 0., 7.],\n [2., 0., 6.]])\n```", "```py\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n```", "```py\n>>> M = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n>>> M = M.to(device)\n```", "```py\n>>> M.device\ndevice(type='cuda', index=0)\n----\n```", "```py\n>>> M = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device=device)\n```", "```py\n>>> R = M @ M.T  # run some operations on the GPU\n>>> R\ntensor([[14., 32.],\n [32., 77.]], device='cuda:0')\n```", "```py\n>>> M = torch.rand((1000, 1000))  # on the CPU\n>>> %timeit M @ M.T\n16.1 ms ± 2.17 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n>>> M = torch.rand((1000, 1000), device=\"cuda\")  # on the GPU\n>>> %timeit M @ M.T\n549 µs ± 3.99 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n```", "```py\n>>> x = torch.tensor(5.0, requires_grad=True)\n>>> f = x ** 2\n>>> f\ntensor(25., grad_fn=<PowBackward0>)\n>>> f.backward()\n>>> x.grad\ntensor(10.)\n```", "```py\nlearning_rate = 0.1\nwith torch.no_grad():\n    x -= learning_rate * x.grad  # gradient descent step\n```", "```py\nx_detached = x.detach()\nx_detached -= learning_rate * x.grad\n```", "```py\nx.grad.zero_()\n```", "```py\nlearning_rate = 0.1\nx = torch.tensor(5.0, requires_grad=True)\nfor iteration in range(100):\n    f = x ** 2  # forward pass\n    f.backward()  # backward pass\n    with torch.no_grad():\n        x -= learning_rate * x.grad  # gradient descent step\n\n    x.grad.zero_()  # reset the gradients\n```", "```py\nt = torch.tensor(2.0, requires_grad=True)\nz = t.exp()  # this is an intermediate result\nz += 1  # this is an in-place operation\nz.backward()  #  RuntimeError!\n\n```", "```py\nX_train = torch.FloatTensor(X_train)\nX_valid = torch.FloatTensor(X_valid)\nX_test = torch.FloatTensor(X_test)\nmeans = X_train.mean(dim=0, keepdims=True)\nstds = X_train.std(dim=0, keepdims=True)\nX_train = (X_train - means) / stds\nX_valid = (X_valid - means) / stds\nX_test = (X_test - means) / stds\n```", "```py\ny_train = torch.FloatTensor(y_train).reshape(-1, 1)\ny_valid = torch.FloatTensor(y_valid).reshape(-1, 1)\ny_test = torch.FloatTensor(y_test).reshape(-1, 1)\n```", "```py\ntorch.manual_seed(42)\nn_features = X_train.shape[1]  # there are 8 input features\nw = torch.randn((n_features, 1), requires_grad=True)\nb = torch.tensor(0., requires_grad=True)\n```", "```py\nlearning_rate = 0.4\nn_epochs = 20\nfor epoch in range(n_epochs):\n    y_pred = X_train @ w + b\n    loss = ((y_pred - y_train) ** 2).mean()\n    loss.backward()\n    with torch.no_grad():\n        b -= learning_rate * b.grad\n        w -= learning_rate * w.grad\n        b.grad.zero_()\n        w.grad.zero_()\n    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")\n```", "```py\nEpoch 1/20, Loss: 16.158456802368164\nEpoch 2/20, Loss: 4.8793745040893555\nEpoch 3/20, Loss: 2.255225419998169\n[...]\nEpoch 20/20, Loss: 0.5684100389480591\n```", "```py\n>>> X_new = X_test[:3]  # pretend these are new instances\n>>> with torch.no_grad():\n...     y_pred = X_new @ w + b  # use the trained parameters to make predictions\n...\n>>> y_pred\ntensor([[0.8916],\n [1.6480],\n [2.6577]])\n```", "```py\nimport torch.nn as nn  # by convention, this module is usually imported this way\n\ntorch.manual_seed(42)  # to get reproducible results\nmodel = nn.Linear(in_features=n_features, out_features=1)\n```", "```py\n>>> model.bias\nParameter containing:\ntensor([0.3117], requires_grad=True)\n>>> model.weight\nParameter containing:\ntensor([[ 0.2703, 0.2935, -0.0828, 0.3248, -0.0775, 0.0713, -0.1721, 0.2076]],\n requires_grad=True)\n```", "```py\n>>> for param in model.parameters():\n...     [...]  # do something with each parameter\n```", "```py\n>>> model(X_train[:2])\ntensor([[-0.4718],\n [ 0.1131]], grad_fn=<AddmmBackward0>)\n```", "```py\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\nmse = nn.MSELoss()\n```", "```py\ndef train_bgd(model, optimizer, criterion, X_train, y_train, n_epochs):\n    for epoch in range(n_epochs):\n        y_pred = model(X_train)\n        loss = criterion(y_pred, y_train)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")\n```", "```py\n>>> train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)\nEpoch 1/20, Loss: 4.3378496170043945\nEpoch 2/20, Loss: 0.780293345451355\n[...]\nEpoch 20/20, Loss: 0.5374288558959961\n```", "```py\n>>> X_new = X_test[:3]  # pretend these are new instances\n>>> with torch.no_grad():\n...     y_pred = model(X_new)  # use the trained model to make predictions\n...\n>>> y_pred\ntensor([[0.8061],\n [1.7116],\n [2.6973]])\n```", "```py\ntorch.manual_seed(42)\nmodel = nn.Sequential(\n    nn.Linear(n_features, 50),\n    nn.ReLU(),\n    nn.Linear(50, 40),\n    nn.ReLU(),\n    nn.Linear(40, 1)\n)\n```", "```py\n>>> learning_rate = 0.1\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n>>> mse = nn.MSELoss()\n>>> train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)\nEpoch 1/20, Loss: 5.045480251312256\nEpoch 2/20, Loss: 2.0523128509521484\n[...]\nEpoch 20/20, Loss: 0.565444827079773\n```", "```py\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n```", "```py\ntorch.manual_seed(42)\nmodel = nn.Sequential([...])  # create the model just like earlier\nmodel = model.to(device)\n```", "```py\ndef train(model, optimizer, criterion, train_loader, n_epochs):\n    model.train()\n    for epoch in range(n_epochs):\n        total_loss = 0.\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        mean_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {mean_loss:.4f}\")\n```", "```py\n>>> train(model, optimizer, mse, train_loader, n_epochs)\nEpoch 1/20, Loss: 0.6958\nEpoch 2/20, Loss: 0.4480\n[...]\nEpoch 20/20, Loss: 0.3227\n```", "```py\ndef evaluate(model, data_loader, metric_fn, aggregate_fn=torch.mean):\n    model.eval()\n    metrics = []\n    with torch.no_grad():\n        for X_batch, y_batch in data_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            metric = metric_fn(y_pred, y_batch)\n            metrics.append(metric)\n    return aggregate_fn(torch.stack(metrics))\n```", "```py\n>>> valid_dataset = TensorDataset(X_valid, y_valid)\n>>> valid_loader = DataLoader(valid_dataset, batch_size=32)\n>>> valid_mse = evaluate(model, valid_loader, mse)\n>>> valid_mse\ntensor(0.4080, device='cuda:0')\n```", "```py\n>>> def rmse(y_pred, y_true):\n...     return ((y_pred - y_true) ** 2).mean().sqrt()\n...\n>>> evaluate(model, valid_loader, rmse)\ntensor(0.5668, device='cuda:0')\n```", "```py\n>>> valid_mse.sqrt()\ntensor(0.6388, device='cuda:0')\n```", "```py\n>>> evaluate(model, valid_loader, mse,\n...          aggregate_fn=lambda metrics: torch.sqrt(torch.mean(metrics)))\n...\ntensor(0.6388, device='cuda:0')\n```", "```py\nimport torchmetrics\n\ndef evaluate_tm(model, data_loader, metric):\n    model.eval()\n    metric.reset()  # reset the metric at the beginning\n    with torch.no_grad():\n        for X_batch, y_batch in data_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            metric.update(y_pred, y_batch)  # update it at each iteration\n    return metric.compute()  # compute the final result at the end\n```", "```py\n>>> rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n>>> evaluate_tm(model, valid_loader, rmse)\ntensor(0.6388, device='cuda:0')\n```", "```py\nclass WideAndDeep(nn.Module):\n    def __init__(self, n_features):\n        super().__init__()\n        self.deep_stack = nn.Sequential(\n            nn.Linear(n_features, 50), nn.ReLU(),\n            nn.Linear(50, 40), nn.ReLU(),\n        )\n        self.output_layer = nn.Linear(40 + n_features, 1)\n\n    def forward(self, X):\n        deep_output = self.deep_stack(X)\n        wide_and_deep = torch.concat([X, deep_output], dim=1)\n        return self.output_layer(wide_and_deep)\n```", "```py\ntorch.manual_seed(42)\nmodel = WideAndDeep(n_features).to(device)\nlearning_rate = 0.002  # the model changed, so did the optimal learning rate\n[...]  # train, evaluate, and use the model, exactly like earlier\n```", "```py\nclass WideAndDeepV2(nn.Module):\n    [...]  # same constructor as earlier, except with adjusted input sizes\n\n    def forward(self, X):\n        X_wide = X[:, :5]\n        X_deep = X[:, 2:]\n        deep_output = self.deep_stack(X_deep)\n        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n        return self.output_layer(wide_and_deep)\n```", "```py\nclass WideAndDeepV3(nn.Module):\n    [...]  # same as WideAndDeepV2\n\n    def forward(self, X_wide, X_deep):\n        deep_output = self.deep_stack(X_deep)\n        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n        return self.output_layer(wide_and_deep)\n```", "```py\ntrain_data_wd = TensorDataset(X_train[:, :5], X_train[:, 2:], y_train)\ntrain_loader_wd = DataLoader(train_data_wd, batch_size=32, shuffle=True)\n[...]  # same for the validation set and test set\n```", "```py\nfor X_batch_wide, X_batch_deep, y_batch in data_loader:\n    X_batch_wide = X_batch_wide.to(device)\n    X_batch_deep = X_batch_deep.to(device)\n    y_batch = y_batch.to(device)\n    y_pred = model(X_batch_wide, X_batch_deep)\n    [...]  # the rest of the function is unchanged\n```", "```py\nfor *X_batch_inputs, y_batch in data_loader:\n    X_batch_inputs = [X.to(device) for X in X_batch_inputs]\n    y_batch = y_batch.to(device)\n    y_pred = model(*X_batch_inputs)\n    [...]\n```", "```py\nclass WideAndDeepDataset(torch.utils.data.Dataset):\n    def __init__(self, X_wide, X_deep, y):\n        self.X_wide = X_wide\n        self.X_deep = X_deep\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        input_dict = {\"X_wide\": self.X_wide[idx], \"X_deep\": self.X_deep[idx]}\n        return input_dict, self.y[idx]\n```", "```py\ntrain_data_named = WideAndDeepDataset(\n    X_wide=X_train[:, :5], X_deep=X_train[:, 2:], y=y_train)\ntrain_loader_named = DataLoader(train_data_named, batch_size=32, shuffle=True)\n[...]  # same for the validation set and test set\n```", "```py\nfor inputs, y_batch in data_loader:\n    inputs = {name: X.to(device) for name, X in inputs.items()}\n    y_batch = y_batch.to(device)\n    y_pred = model(X_wide=inputs[\"X_wide\"], X_deep=inputs[\"X_deep\"])\n    [...]  # the rest of the function is unchanged\n```", "```py\nclass WideAndDeepV4(nn.Module):\n    def __init__(self, n_features):\n        [...]  # same as earlier\n        self.aux_output_layer = nn.Linear(40, 1)\n\n    def forward(self, X_wide, X_deep):\n        deep_output = self.deep_stack(X_deep)\n        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n        main_output = self.output_layer(wide_and_deep)\n        aux_output = self.aux_output_layer(deep_output)\n        return main_output, aux_output\n```", "```py\nfor inputs, y_batch in train_loader:\n    y_pred, y_pred_aux = model(**inputs)\n    main_loss = criterion(y_pred, y_batch)\n    aux_loss = criterion(y_pred_aux, y_batch)\n    loss = 0.8 * main_loss + 0.2 * aux_loss\n    [...]  # the rest is unchanged\n```", "```py\nfor inputs, y_batch in data_loader:\n    y_pred, _ = model(**inputs)\n    metric.update(y_pred, y_batch)\n    [...]  # the rest is unchanged\n```", "```py\nimport torchvision\nimport torchvision.transforms.v2 as T\n\ntoTensor = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale=True)])\n\ntrain_and_valid_data = torchvision.datasets.FashionMNIST(\n    root=\"datasets\", train=True, download=True, transform=toTensor)\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"datasets\", train=False, download=True, transform=toTensor)\n\ntorch.manual_seed(42)\ntrain_data, valid_data = torch.utils.data.random_split(\n    train_and_valid_data, [55_000, 5_000])\n```", "```py\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_data, batch_size=32)\ntest_loader = DataLoader(test_data, batch_size=32)\n```", "```py\n>>> X_sample, y_sample = train_data[0]\n>>> X_sample.shape\ntorch.Size([1, 28, 28])\n>>> X_sample.dtype\ntorch.float32\n```", "```py\n>>> train_and_valid_data.classes[y_sample]\n'Ankle boot'\n```", "```py\nclass ImageClassifier(nn.Module):\n    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_classes):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(n_inputs, n_hidden1),\n            nn.ReLU(),\n            nn.Linear(n_hidden1, n_hidden2),\n            nn.ReLU(),\n            nn.Linear(n_hidden2, n_classes)\n        )\n\n    def forward(self, X):\n        return self.mlp(X)\n\ntorch.manual_seed(42)\nmodel = ImageClassifier(n_inputs=28 * 28, n_hidden1=300, n_hidden2=100,\n                        n_classes=10)\nxentropy = nn.CrossEntropyLoss()\n```", "```py\naccuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n```", "```py\n>>> model.eval()\n>>> X_new, y_new = next(iter(valid_loader))\n>>> X_new = X_new[:3].to(device)\n>>> with torch.no_grad():\n...     y_pred_logits = model(X_new)\n...\n>>> y_pred = y_pred_logits.argmax(dim=1)  # index of the largest logit\n>>> y_pred\ntensor([7, 4, 2], device='cuda:0')\n>>> [train_and_valid_data.classes[index] for index in y_pred]\n['Sneaker', 'Coat', 'Pullover']\n```", "```py\n>>> import torch.nn.functional as F\n>>> y_proba = F.softmax(y_pred_logits, dim=1)\n>>> y_proba.round(decimals=3)\ntensor([[0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000, 0.911, 0.000, 0.088],\n [0.000, 0.000, 0.004, 0.000, 0.996, 0.000, 0.000, 0.000, 0.000, 0.000],\n [0.000, 0.000, 0.625, 0.000, 0.335, 0.000, 0.039, 0.000, 0.000, 0.000]],\n device='cuda:0')\n```", "```py\n>>> y_top4_logits, y_top4_indices = torch.topk(y_pred_logits, k=4, dim=1)\n>>> y_top4_probas = F.softmax(y_top4_logits, dim=1)\n>>> y_top4_probas.round(decimals=3)\ntensor([[0.9110, 0.0880, 0.0010, 0.0000],\n [0.9960, 0.0040, 0.0000, 0.0000],\n [0.6250, 0.3350, 0.0390, 0.0000]], device='cuda:0')\n>>> y_top4_indices\ntensor([[7, 9, 5, 8],\n [4, 2, 6, 0],\n [2, 4, 6, 0]], device='cuda:0')\n```", "```py\nimport optuna\n\ndef objective(trial):\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n    n_hidden = trial.suggest_int(\"n_hidden\", 20, 300)\n    model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=n_hidden,\n                            n_hidden2=n_hidden, n_classes=10).to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    [...]  # train the model, then evaluate it on the validation set\n    return validation_accuracy\n```", "```py\ntorch.manual_seed(42)\nsampler = optuna.samplers.TPESampler(seed=42)\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=5)\n```", "```py\n>>> study.best_params\n{'learning_rate': 0.08525846269447772, 'n_hidden': 116}\n>>> study.best_value\n0.8867999911308289\n```", "```py\ndef objective(trial, train_loader, valid_loader):\n    [...]  # the rest of the function remains the same as above\n\nobjective_with_data = lambda trial: objective(\n    trial, train_loader=train_loader, valid_loader=valid_loader)\nstudy.optimize(objective_with_data, n_trials=5)\n```", "```py\nfrom functools import partial\n\nobjective_with_data = partial(objective, train_loader=train_loader,\n                              valid_loader=valid_loader)\n```", "```py\npruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0,\n                                     interval_steps=1)\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler,\n                            pruner=pruner)\n```", "```py\nfor epoch in range(n_epochs):\n    [...]  # train the model for one epoch\n    validation_accuracy = [...]  # evaluate the model's validation accuracy\n    trial.report(validation_accuracy, epoch)\n    if trial.should_prune():\n        raise optuna.TrialPruned()\n```", "```py\ntorch.save(model, \"my_fashion_mnist.pt\")\n```", "```py\nloaded_model = torch.load(\"my_fashion_mnist.pt\", weights_only=False)\n```", "```py\nloaded_model.eval()\ny_pred_logits = loaded_model(X_new)\n```", "```py\ntorch.save(model.state_dict(), \"my_fashion_mnist_weights.pt\")\n```", "```py\nnew_model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=300, n_hidden2=100,\n                            n_classes=10)\nloaded_weights = torch.load(\"my_fashion_mnist_weights.pt\", weights_only=True)\nnew_model.load_state_dict(loaded_weights)\nnew_model.eval()\n```", "```py\nmodel_data = {\n    \"model_state_dict\": model.state_dict(),\n    \"model_hyperparameters\": {\"n_inputs\": 1 * 28 * 28, \"n_hidden1\": 300, [...]}\n}\ntorch.save(model_data, \"my_fashion_mnist_model.pt\")\n```", "```py\nloaded_data = torch.load(\"my_fashion_mnist_model.pt\", weights_only=True)\nnew_model = ImageClassifier(**loaded_data[\"model_hyperparameters\"])\nnew_model.load_state_dict(loaded_data[\"model_state_dict\"])\nnew_model.eval()\n```", "```py\ntorchscript_model = torch.jit.trace(model, X_new)\n```", "```py\ntorchscript_model = torch.jit.script(model)\n```", "```py\noptimized_model = torch.jit.optimize_for_inference(torchscript_model)\n```", "```py\ntorchscript_model.save('my_fashion_mnist_torchscript.pt')\n```", "```py\nloaded_torchscript_model = torch.jit.load(\"my_fashion_mnist_torchscript.pt\")\n```", "```py\ncompiled_model = torch.compile(model)\n```", "```py\n    t = torch.tensor(2.0, requires_grad=True)\n    z = t.cos().exp_()\n    z.backward()\n    ```", "```py\n    u = torch.tensor(2.0, requires_grad=True)\n    v = u + 1\n    w = v.cos() * v.sin_()\n    w.backward()\n    ```"]