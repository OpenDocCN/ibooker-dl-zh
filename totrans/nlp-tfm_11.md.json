["```py\nfrom transformers import pipeline, set_seed\n\ngeneration_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\ngeneration_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")\n```", "```py\ndef model_size(model):\n    return sum(t.numel() for t in model.parameters())\n\nprint(f\"GPT  size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\nprint(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")\n```", "```py\nGPT  size: 116.5M parameters\nGPT2 size: 124.4M parameters\n```", "```py\ndef enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n    out = pipe(prompt, num_return_sequences=num_return_sequences,\n               clean_up_tokenization_spaces=True)\n    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n\nprompt = \"\\nWhen they came back\"\nprint(\"GPT completions:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\nprint(\"\")\nprint(\"GPT-2 completions:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))\n```", "```py\nGPT completions:\n1.\nWhen they came back.\n \" we need all we can get, \" jason said once they had settled into the back of\nthe truck without anyone stopping them. \" after getting out here, it 'll be up\nto us what to find. for now\n2.\nWhen they came back.\n his gaze swept over her body. he 'd dressed her, too, in the borrowed clothes\nthat she 'd worn for the journey.\n \" i thought it would be easier to just leave you there. \" a woman like\n3.\nWhen they came back to the house and she was sitting there with the little boy.\n \" don't be afraid, \" he told her. she nodded slowly, her eyes wide. she was so\nlost in whatever she discovered that tom knew her mistake\n\nGPT-2 completions:\n1.\nWhen they came back we had a big dinner and the other guys went to see what\ntheir opinion was on her. I did an hour and they were happy with it.\n2.\nWhen they came back to this island there had been another massacre, but he could\nnot help but feel pity for the helpless victim who had been left to die, and\nthat they had failed that day. And so was very, very grateful indeed.\n3.\nWhen they came back to our house after the morning, I asked if she was sure. She\nsaid, \"Nope.\" The two kids were gone that morning. I thought they were back to\nbeing a good friend.\n\nWhen Dost\n```", "```py\n    SELECT\n      f.repo_name, f.path, c.copies, c.size, c.content, l.license\n    FROM\n      `bigquery-public-data.github_repos.files` AS f\n    JOIN\n      `bigquery-public-data.github_repos.contents` AS c\n    ON\n      f.id = c.id\n    JOIN\n      `bigquery-public-data.github_repos.licenses` AS l\n    ON\n      f.repo_name = l.repo_name\n    WHERE\n      NOT c.binary\n      AND ((f.path LIKE '%.py')\n        AND (c.size BETWEEN 1024\n          AND 1048575))\n    ```", "```py\n        $ gsutil -m -o\n        \"GSUtil:parallel_process_count=1\" cp -r gs://*<name_of_bucket>*\n\n        ```", "```py\n$ git clone https://huggingface.co/datasets/transformersbook/codeparrot\n\n```", "```py\nfrom datasets import load_dataset, DownloadConfig\n\ndownload_config = DownloadConfig(delete_extracted=True)\ndataset = load_dataset(\"./codeparrot\", split=\"train\",\n                       download_config=download_config)\n```", "```py\nimport psutil\n\nprint(f\"Number of python files code in dataset : {len(dataset)}\")\nds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n# os.stat.st_size is expressed in bytes, so we convert to GB\nprint(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n# Process.memory_info is expressed in bytes, so we convert to MB\nprint(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")\n```", "```py\nNumber of python files code in dataset : 18695559\nDataset size (cache file) : 183.68 GB\nRAM memory used: 4924 MB\n```", "```py\nstreamed_dataset = load_dataset('./codeparrot', split=\"train\", streaming=True)\n```", "```py\niterator = iter(streamed_dataset)\n\nprint(dataset[0] == next(iterator))\nprint(dataset[1] == next(iterator))\n```", "```py\nTrue\nTrue\n```", "```py\nremote_dataset = load_dataset('transformersbook/codeparrot', split=\"train\",\n                              streaming=True)\n```", "```py\n$ huggingface-cli login\n\n```", "```py\n$ huggingface-cli repo create --type dataset --organization transformersbook \\\ncodeparrot-train\n$ huggingface-cli repo create --type dataset --organization transformersbook \\\ncodeparrot-valid\n\n```", "```py\n$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-train\n$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-valid\n\n```", "```py\n$ cd codeparrot-train\n$ cp ../codeparrot/*.json.gz .\n$ rm ./file-000000000183.json.gz\n\n```", "```py\n$ git add .\n$ git commit -m \"Adding dataset files\"\n$ git push\n\n```", "```py\n$ cd ../codeparrot-valid\n$ cp ../codeparrot/file-000000000183.json.gz .\n$ mv ./file-000000000183.json.gz ./file-000000000183_validation.json.gz\n$ git add .\n$ git commit -m \"Adding dataset files\"\n$ git push\n\n```", "```py\nfrom transformers import AutoTokenizer\n\ndef tok_list(tokenizer, string):\n    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n    return [tokenizer.decode(tok) for tok in input_ids]\n\ntokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\ntokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\n```", "```py\nprint(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\nprint(f'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}')\n```", "```py\nT5 tokens for \"sex\": ['', 's', 'ex']\nCamemBERT tokens for \"being\": ['be', 'ing']\n```", "```py\nfrom transformers import AutoTokenizer\n\npython_code = r\"\"\"def say_hello():\n print(\"Hello, World!\")\n\n# Print it\nsay_hello()\n\"\"\"\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nprint(tokenizer(python_code).tokens())\n```", "```py\n['def', '\u0120say', '_', 'hello', '():', '\u010a', '\u0120', '\u0120', '\u0120', '\u0120print', '(\"',\n'Hello', ',', '\u0120World', '!\"', ')', '\u0120#', '\u0120Print', '\u0120it', '\u010a', '\u010a', 'say', '_',\n'hello', '()', '\u010a']\n```", "```py\nprint(tokenizer.backend_tokenizer.normalizer)\n```", "```py\nNone\n```", "```py\nprint(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))\n```", "```py\n[('def', (0, 3)), ('\u0120say', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():',\n(13, 16)), ('\u010a\u0120\u0120\u0120', (16, 20)), ('\u0120print', (20, 26)), ('(\"', (26, 28)), ('Hello',\n(28, 33)), (',', (33, 34)), ('\u0120World', (34, 40)), ('!\")', (40, 43)), ('\u0120#', (43,\n45)), ('\u0120Print', (45, 51)), ('\u0120it', (51, 54)), ('\u010a', (54, 55)), ('\u010a', (55, 56)),\n('say', (56, 59)), ('_', (59, 60)), ('hello', (60, 65)), ('()', (65, 67)), ('\u010a',\n(67, 68))]\n```", "```py\na, e = u\"a\", u\"\u20ac\"\nbyte = ord(a.encode(\"utf-8\"))\nprint(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\nbyte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\nprint(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')\n```", "```py\n`a` is encoded as `b'a'` with a single byte: 97\n`\u20ac` is encoded as `b'\\xe2\\x82\\xac'` with three bytes: [226, 130, 172]\n```", "```py\nfrom transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n\nbyte_to_unicode_map = bytes_to_unicode()\nunicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\nbase_vocab = list(unicode_to_byte_map.keys())\n\nprint(f'Size of our base vocabulary: {len(base_vocab)}')\nprint(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')\n```", "```py\nSize of our base vocabulary: 256\nFirst element: `!`, last element: `\u0143`\n```", "```py\nprint(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))\n```", "```py\n[('def', (0, 3)), ('\u0120say', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():',\n(13, 16)), ('\u010a\u0120\u0120\u0120', (16, 20)), ('\u0120print', (20, 26)), ('(\"', (26, 28)), ('Hello',\n(28, 33)), (',', (33, 34)), ('\u0120World', (34, 40)), ('!\")', (40, 43)), ('\u0120#', (43,\n45)), ('\u0120Print', (45, 51)), ('\u0120it', (51, 54)), ('\u010a', (54, 55)), ('\u010a', (55, 56)),\n('say', (56, 59)), ('_', (59, 60)), ('hello', (60, 65)), ('()', (65, 67)), ('\u010a',\n(67, 68))]\n```", "```py\nprint(f\"Size of the vocabulary: {len(tokenizer)}\")\n```", "```py\nSize of the vocabulary: 50257\n```", "```py\nprint(tokenizer(python_code).tokens())\n```", "```py\n['def', '\u0120say', '_', 'hello', '():', '\u010a', '\u0120', '\u0120', '\u0120', '\u0120print', '(\"',\n'Hello', ',', '\u0120World', '!\"', ')', '\u0120#', '\u0120Print', '\u0120it', '\u010a', '\u010a', 'say', '_',\n'hello', '()', '\u010a']\n```", "```py\ntokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\nprint([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:8]]);\n```", "```py\n['\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2', '\n=================================================================', '\n----------------------------------------------------------------\n',\n'................................................................',\n'\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2',\n'\n----------------------------------------------------------------\n',\n'================================================================',\n'________________________________________________________________']\n```", "```py\ntokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\nprint([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:12]]);\n```", "```py\n['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated',\n' amplification', 'Compar', '...\"', ' (/', 'Commission', ' Hitman']\n```", "```py\nfrom tqdm.auto import tqdm\n\nlength = 100000\ndataset_name = 'transformersbook/codeparrot-train'\ndataset = load_dataset(dataset_name, split=\"train\", streaming=True)\niter_dataset = iter(dataset)\n\ndef batch_iterator(batch_size=10):\n    for _ in tqdm(range(0, length, batch_size)):\n        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n\nnew_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(),\n                                                  vocab_size=12500,\n                                                  initial_alphabet=base_vocab)\n```", "```py\ntokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\nprint([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[257:280]]);\n```", "```py\n['  ', '    ', '   ', '        ', 'se', 'in', '       ', 're', 'on', 'te', '\\n\n', '\\n        ', 'or', 'st', 'de', '\\n   ', 'th', 'le', ' =', 'lf', 'self',\n'me', 'al']\n```", "```py\nprint([f'{new_tokenizer.convert_tokens_to_string(t)}' for t,_ in tokens[-12:]]);\n```", "```py\n[' capt', ' embedded', ' regarding', 'Bundle', '355', ' recv', ' dmp', ' vault',\n' Mongo', ' possibly', 'implementation', 'Matches']\n```", "```py\nprint(new_tokenizer(python_code).tokens())\n```", "```py\n['def', '\u0120s', 'ay', '_', 'hello', '():', '\u010a\u0120\u0120\u0120', '\u0120print', '(\"', 'Hello', ',',\n'\u0120Wor', 'ld', '!\")', '\u0120#', '\u0120Print', '\u0120it', '\u010a', '\u010a', 's', 'ay', '_', 'hello',\n'()', '\u010a']\n```", "```py\nimport keyword\n\nprint(f'There are in total {len(keyword.kwlist)} Python keywords.')\nfor keyw in keyword.kwlist:\n    if keyw not in new_tokenizer.vocab:\n        print(f'No, keyword `{keyw}` is not in the vocabulary')\n```", "```py\nThere are in total 35 Python keywords.\nNo, keyword `await` is not in the vocabulary\nNo, keyword `finally` is not in the vocabulary\nNo, keyword `nonlocal` is not in the vocabulary\n```", "```py\nlength = 200000\nnew_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n    vocab_size=32768, initial_alphabet=base_vocab)\n```", "```py\ntokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\n                reverse=False)\nprint([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[-12:]]);\n```", "```py\n['lineEdit', 'spik', ' BC', 'pective', 'OTA', 'theus', 'FLUSH', ' excutils',\n'00000002', ' DIVISION', 'CursorPosition', ' InfoBar']\n```", "```py\nprint(new_tokenizer_larger(python_code).tokens())\n```", "```py\n['def', '\u0120say', '_', 'hello', '():', '\u010a\u0120\u0120\u0120', '\u0120print', '(\"', 'Hello', ',',\n'\u0120World', '!\")', '\u0120#', '\u0120Print', '\u0120it', '\u010a', '\u010a', 'say', '_', 'hello', '()',\n'\u010a']\n```", "```py\nfor keyw in keyword.kwlist:\n    if keyw not in new_tokenizer_larger.vocab:\n        print(f'No, keyword `{keyw}` is not in the vocabulary')\n```", "```py\nNo, keyword `nonlocal` is not in the vocabulary\n```", "```py\nmodel_ckpt = \"codeparrot\"\norg = \"transformersbook\"\nnew_tokenizer_larger.push_to_hub(model_ckpt, organization=org)\n```", "```py\nreloaded_tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\nprint(reloaded_tokenizer(python_code).tokens())\n```", "```py\n['def', '\u0120say', '_', 'hello', '():', '\u010a\u0120\u0120\u0120', '\u0120print', '(\"', 'Hello', ',',\n'\u0120World', '!\")', '\u0120#', '\u0120Print', '\u0120it', '\u010a', '\u010a', 'say', '_', 'hello', '()',\n'\u010a']\n```", "```py\nnew_tokenizer.push_to_hub(model_ckpt+ \"-small-vocabulary\", organization=org)\n```", "```py\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nconfig = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\nmodel = AutoModelForCausalLM.from_config(config)\n```", "```py\nprint(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')\n```", "```py\nGPT-2 (xl) size: 1529.6M parameters\n```", "```py\nmodel.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True,\n                      organization=org)\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nconfig_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\nmodel_small = AutoModelForCausalLM.from_config(config_small)\n```", "```py\nprint(f'GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters')\n```", "```py\nGPT-2 size: 111.0M parameters\n```", "```py\nmodel_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True,\n                            organization=org)\n```", "```py\ninput_characters = number_of_sequences * sequence_length * characters_per_token\n```", "```py\nexamples, total_characters, total_tokens = 500, 0, 0\ndataset = load_dataset('transformersbook/codeparrot-train', split='train',\n                       streaming=True)\n\nfor _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n    total_characters += len(example['content'])\n    total_tokens += len(tokenizer(example['content']).tokens())\n\ncharacters_per_token = total_characters / total_tokens\n```", "```py\nprint(characters_per_token)\n```", "```py\n3.6233025034779565\n```", "```py\nimport torch\nfrom torch.utils.data import IterableDataset\n\nclass ConstantLengthDataset(IterableDataset):\n\n    def __init__(self, tokenizer, dataset, seq_length=1024,\n                 num_of_sequences=1024, chars_per_token=3.6):\n        self.tokenizer = tokenizer\n        self.concat_token_id = tokenizer.eos_token_id\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.input_characters = seq_length * chars_per_token * num_of_sequences\n\n    def __iter__(self):\n        iterator = iter(self.dataset)\n        more_examples = True\n        while more_examples:\n            buffer, buffer_len = [], 0\n            while True:\n                if buffer_len >= self.input_characters:\n                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n                    print(m)\n                    break\n                try:\n                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n                    print(m)\n                    buffer.append(next(iterator)[\"content\"])\n                    buffer_len += len(buffer[-1])\n                except StopIteration:\n                    iterator = iter(self.dataset)\n\n            all_token_ids = []\n            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n            for tokenized_input in tokenized_inputs[\"input_ids'\"]:\n            for tokenized_input in tokenized_inputs:\n                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n\n            for i in range(0, len(all_token_ids), self.seq_length):\n                input_ids = all_token_ids[i : i + self.seq_length]\n                if len(input_ids) == self.seq_length:\n                    yield torch.tensor(input_ids)\n```", "```py\nshuffled_dataset = dataset.shuffle(buffer_size=100)\nconstant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n                                                num_of_sequences=10)\ndataset_iterator = iter(constant_length_dataset)\n\nlengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\nprint(f\"Lengths of the sequences: {lengths}\")\n```", "```py\nFill buffer: 0<36864\nFill buffer: 3311<36864\nFill buffer: 9590<36864\nFill buffer: 22177<36864\nFill buffer: 25530<36864\nFill buffer: 31098<36864\nFill buffer: 32232<36864\nFill buffer: 33867<36864\nBuffer full: 41172>=36864\nLengths of the sequences: [1024, 1024, 1024, 1024, 1024]\n```", "```py\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n- device = 'cpu'\n+ accelerator = Accelerator()\n\n- model = torch.nn.Transformer().to(device)\n+ model = torch.nn.Transformer()\n  optimizer = torch.optim.Adam(model.parameters())\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ model, optimizer, data = accelerator.prepare(model, optimizer, data)\n\n  model.train()\n  for epoch in range(10):\n      for source, targets in data:\n-         source = source.to(device)\n-         targets = targets.to(device)\n          optimizer.zero_grad()\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n-         loss.backward()\n+         accelerator.backward(loss)\n          optimizer.step()\n```", "```py\nfrom argparse import Namespace\n\n# Commented parameters correspond to the small model\nconfig = {\"train_batch_size\": 2, # 12\n          \"valid_batch_size\": 2, # 12\n          \"weight_decay\": 0.1,\n          \"shuffle_buffer\": 1000,\n          \"learning_rate\": 2e-4, # 5e-4\n          \"lr_scheduler_type\": \"cosine\",\n          \"num_warmup_steps\": 750, # 2000\n          \"gradient_accumulation_steps\": 16, # 1\n          \"max_train_steps\": 50000, # 150000\n          \"max_eval_steps\": -1,\n          \"seq_length\": 1024,\n          \"seed\": 1,\n          \"save_checkpoint_steps\": 50000} # 15000\n\nargs = Namespace(**config)\n```", "```py\nfrom torch.utils.tensorboard import SummaryWriter\nimport logging\nimport wandb\n\ndef setup_logging(project_name):\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y%H:%M:%S\", level=logging.INFO, handlers=[\n        logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n        logging.StreamHandler()])\n    if accelerator.is_main_process: # We only want to set up logging once\n        wandb.init(project=project_name, config=args)\n        run_name = wandb.run.name\n        tb_writer = SummaryWriter()\n        tb_writer.add_hparams(vars(args), {'0': 0})\n        logger.setLevel(logging.INFO)\n        datasets.utils.logging.set_verbosity_debug()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        tb_writer = None\n        run_name = ''\n        logger.setLevel(logging.ERROR)\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    return logger, tb_writer, run_name\n```", "```py\ndef log_metrics(step, metrics):\n    logger.info(f\"Step {step}: {metrics}\")\n    if accelerator.is_main_process:\n        wandb.log(metrics)\n        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]\n```", "```py\nfrom torch.utils.data.dataloader import DataLoader\n\ndef create_dataloaders(dataset_name):\n    train_data = load_dataset(dataset_name+'-train', split=\"train\",\n                              streaming=True)\n    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,\n                                    seed=args.seed)\n    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\",\n                              streaming=True)\n\n    train_dataset = ConstantLengthDataset(tokenizer, train_data,\n                                          seq_length=args.seq_length)\n    valid_dataset = ConstantLengthDataset(tokenizer, valid_data,\n                                          seq_length=args.seq_length)\n\n    train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)\n    eval_dataloader=DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n    return train_dataloader, eval_dataloader\n```", "```py\ndef get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n    params_with_wd, params_without_wd = [], []\n    for n, p in model.named_parameters():\n        if any(nd in n for nd in no_decay):\n            params_without_wd.append(p)\n        else:\n            params_with_wd.append(p)\n    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n            {'params': params_without_wd, 'weight_decay': 0.0}]\n```", "```py\ndef evaluate():\n    model.eval()\n    losses = []\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(batch, labels=batch)\n        loss = outputs.loss.repeat(args.valid_batch_size)\n        losses.append(accelerator.gather(loss))\n        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n    loss = torch.mean(torch.cat(losses))\n    try:\n\tperplexity = torch.exp(loss)\n    except OverflowError:\n\tperplexity = torch.tensor(float(\"inf\"))\n    return loss.item(), perplexity.item()\n```", "```py\nset_seed(args.seed)\n\n# Accelerator\naccelerator = Accelerator()\nsamples_per_step = accelerator.state.num_processes * args.train_batch_size\n\n# Logging\nlogger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\nlogger.info(accelerator.state)\n\n# Load model and tokenizer\nif accelerator.is_main_process:\n    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\nmodel = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\ntokenizer = AutoTokenizer.from_pretrained(\"./\")\n\n# Load dataset and dataloader\ntrain_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n\n# Prepare the optimizer and learning rate scheduler\noptimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\nlr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n                             num_warmup_steps=args.num_warmup_steps,\n                             num_training_steps=args.max_train_steps,)\ndef get_lr():\n    return optimizer.param_groups[0]['lr']\n\n# Prepare everything with our `accelerator` (order of args is not important)\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader)\n\n# Train model\nmodel.train()\ncompleted_steps = 0\nfor step, batch in enumerate(train_dataloader, start=1):\n    loss = model(batch, labels=batch).loss\n    log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,\n                       'steps': completed_steps, 'loss/train': loss.item()})\n    loss = loss / args.gradient_accumulation_steps\n    accelerator.backward(loss)\n    if step % args.gradient_accumulation_steps == 0:\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        completed_steps += 1\n    if step % args.save_checkpoint_steps == 0:\n        logger.info('Evaluating and saving model checkpoint')\n        eval_loss, perplexity = evaluate()\n        log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        if accelerator.is_main_process:\n            unwrapped_model.save_pretrained(\"./\")\n            hf_repo.push_to_hub(commit_message=f'step {step}')\n        model.train()\n    if completed_steps >= args.max_train_steps:\n        break\n\n# Evaluate and save the last checkpoint\nlogger.info('Evaluating and saving model after training')\neval_loss, perplexity = evaluate()\nlog_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nif accelerator.is_main_process:\n    unwrapped_model.save_pretrained(\"./\")\n    hf_repo.push_to_hub(commit_message=f'final model')\n```", "```py\n$ git clone https://huggingface.co/transformersbook/codeparrot\n$ cd codeparrot\n$ pip install -r requirements.txt\n$ wandb login\n$ accelerate config\n$ accelerate launch codeparrot_training.py\n\n```", "```py\n$ git checkout main\n$ git merge <RUN_NAME>\n$ git push\n\n```", "```py\nfrom transformers import pipeline, set_seed\n\nmodel_ckpt = 'transformersbook/codeparrot-small'\ngeneration = pipeline('text-generation', model=model_ckpt, device=0)\n```", "```py\nimport re\nfrom transformers import set_seed\n\ndef first_block(string):\n    return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n\ndef complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n    set_seed(seed)\n    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1,\n                  \"do_sample\":True,}\n    code_gens = generation(prompt, num_return_sequences=num_completions,\n                            max_length=max_length, **gen_kwargs)\n    code_strings = []\n    for code_gen in code_gens:\n        generated_code = first_block(code_gen['generated_text'][len(prompt):])\n        code_strings.append(generated_code)\n    print(('\\n'+'='*80 + '\\n').join(code_strings))\n```", "```py\nprompt = '''def area_of_rectangle(a: float, b: float):\n \"\"\"Return the area of the rectangle.\"\"\"'''\ncomplete_code(generation, prompt)\n```", "```py\n    return math.sqrt(a * b)\n================================================================================\n\n    return a * b / 2.0\n================================================================================\n\n    return a * b\n================================================================================\n\n    return a * b / a\n```", "```py\nprompt = '''def get_urls_from_html(html):\n \"\"\"Get all embedded URLs in a HTML string.\"\"\"'''\ncomplete_code(generation, prompt)\n```", "```py\n    if not html:\n        return []\n    return [url for url in re.findall(r'<a href=\"(/[^/]+/[^\"]+?)\">', html)]\n================================================================================\n\n    return [url for url in re.findall(r'<a href=\"(.*?)\"', html)\n            if url]\n================================================================================\n\n    return [url for url in re.findall(r'<a href=\"(/.*)\",', html)]\n================================================================================\n\n    return re.findall(r'<a href=\"(.*?)\" class=\"url\"[^>]*>', html)\n```", "```py\nimport requests\n\ndef get_urls_from_html(html):\n    return [url for url in re.findall(r'<a href=\"(.*?)\"', html) if url]\n\nprint(\" | \".join(get_urls_from_html(requests.get('https://hf.co/').text)))\n```", "```py\nhttps://github.com/huggingface/transformers | /allenai | /facebook |\n/asteroid-team | /google | /amazon | /speechbrain | /microsoft | /grammarly |\n/models | /inference-api | /distilbert-base-uncased |\n/dbmdz/bert-large-cased-finetuned-conll03-english |\nhttps://huggingface.co/transformers | https://arxiv.org/abs/1811.06031 |\nhttps://arxiv.org/abs/1803.10631 | https://transformer.huggingface.co/ | /coref\n| https://medium.com/huggingface/distilbert-8cf3380435b5\n```", "```py\nmodel_ckpt = 'transformersbook/codeparrot'\ngeneration = pipeline('text-generation', model=model_ckpt, device=0)\n\nprompt = '''# a function in native python:\ndef mean(a):\n return sum(a)/len(a)\n\n# the same function using numpy:\nimport numpy as np\ndef mean(a):'''\ncomplete_code(generation, prompt, max_length=64)\n```", "```py\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n    return np.mean(a)\n================================================================================\n\n    return np.mean(a)\n================================================================================\n\n    return np.mean(a)\n================================================================================\n\n    return np.mean(a)\n```", "```py\nprompt = '''X = np.random.randn(100, 100)\ny = np.random.randint(0, 1, 100)\n\n# fit random forest classifier with 20 estimators'''\ncomplete_code(generation, prompt, max_length=96)\n```", "```py\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\nreg = DummyRegressor()\n\nforest = RandomForestClassifier(n_estimators=20)\n\nforest.fit(X, y)\n================================================================================\n\nclf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt')\nclf.fit(X, y)\n================================================================================\n\nclf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)\nclf.fit(X, y)\n================================================================================\n\nclf = RandomForestClassifier(n_estimators=20)\nclf.fit(X, y)\n```"]