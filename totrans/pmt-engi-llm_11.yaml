- en: Chapter 8\. Conversational Agency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 对话代理
- en: In [Chapter 3](ch03.html#ch03a_moving_toward_chat_1728432131625250), we covered
    the departure from text completion models to chat models. A chat model by itself
    is aware of only the information covered in training and whatever information
    the user has just told it. The chat model is unable to reach out into the world
    and learn about information that was unavailable during training, and it’s unable
    to interact with the world and take external actions on behalf of the user.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#ch03a_moving_toward_chat_1728432131625250)中，我们讨论了从文本生成模型转向聊天模型的过程。聊天模型本身只能意识到训练过程中覆盖的信息以及用户刚刚告诉它的信息。聊天模型无法接触外部世界并学习训练期间不可用的信息，也无法代表用户与世界互动并采取外部行动。
- en: The LLM community is making great headway in overcoming these limitations through
    conversational agency. *Agency* is the ability of an entity to complete tasks
    and achieve goals in a self-directed and autonomous manner. The conversational
    agents that we discuss in this chapter provide an experience similar to chat—a
    back-and-forth dialogue between a user and an assistant—but add in the ability
    for the assistant to reach out to the real world, learn new information, and interact
    with real-world assets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对话代理，LLM社区正在取得重大进展以克服这些限制。**代理**是指一个实体以自我指导和自主的方式完成任务和实现目标的能力。本章中我们讨论的对话代理提供了一种类似于聊天的体验——用户和助手之间的来回对话——但增加了助手能够接触现实世界、学习新信息以及与现实世界资产互动的能力。
- en: In this chapter, we’ll introduce several state-of-the-art approaches to building
    an LLM-based conversational agent. We’ll explore how models can use tools to reach
    out into the external world, how they can be conditioned to better reason through
    their problem space, and how we can gather the best context to facilitate long
    or complex interactions. By the end of this chapter, you’ll be able to build your
    own conversational agent that’s capable of going out into the world and performing
    guided tasks on your behalf.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍几种基于LLM构建对话代理的最新方法。我们将探讨模型如何使用工具接触外部世界，如何被训练以更好地通过其问题空间进行推理，以及我们如何收集最佳上下文以促进长或复杂的交互。到本章结束时，你将能够构建自己的对话代理，它能够代表你走出世界并执行指导性任务。
- en: Tool Usage
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具使用
- en: Working in isolation, language models are limited in what they can accomplish.
    Certainly, a chat assistant is fascinating to talk to because, in some ways, it’s
    the digital zeitgeist of the world. You can learn anything you want from a broad
    range of topics, and the model can draw on diverse schools of thought and help
    you brainstorm. The model is a fantastic tutor—if you don’t mind some hallucinations.
    But one thing it can’t do is access “hidden” knowledge—any bit of information
    that was unavailable to the model during training.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在孤立状态下工作，语言模型在它能完成的事情上有限。当然，聊天助手很有趣，因为它在某种程度上是世界的数字时代精神。你可以从广泛的主题中学习任何你想要的东西，模型可以借鉴不同的思想流派并帮助你进行头脑风暴。模型是一个出色的导师——如果你不介意一些幻觉的话。但它无法做到的是访问“隐藏”的知识——任何在训练期间对模型不可用的信息片段。
- en: When you’re at work, you regularly make use of private information in the form
    of corporate documentation, internal memos, chat messages, and code—information
    that the model has no access to. You also work in the present, not the past, and
    therefore, older information may be less relevant or even incorrect. If the model
    isn’t aware of the most recent API changes for the library you’re using or of
    recent news events, then the completions will be misleading and incorrect. At
    an extreme, you may even require up-to-the-moment information. For instance, if
    you’re planning travel arrangements, you need to know what flights are available
    *now*. A bare chat model has access to none of this.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在工作时，你经常使用私人信息，这些信息以公司文档、内部备忘录、聊天信息和代码的形式存在——模型无法访问这些信息。你也处于现在，而不是过去，因此，旧信息可能不那么相关，甚至可能是错误的。如果模型不知道你使用的库的最新API更改或最近的新闻事件，那么生成的文本将是误导性的和不正确的。在极端情况下，你可能甚至需要最新的信息。例如，如果你在计划旅行安排，你需要知道现在有哪些航班可用。一个简单的聊天模型无法访问这些信息。
- en: Besides missing important information, language models just aren’t good at certain
    tasks—most prominently, math. If you ask ChatGPT to evaluate any simple arithmetic
    problem, then it will often get the correct answer because it has effectively
    memorized all the simple problems. But as the numbers get larger or the computation
    becomes more complicated, the model will make poorer and poorer estimates. What’s
    even worse, these mistakes are often presented confidently as truth.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除了遗漏重要信息外，语言模型在执行某些任务方面并不擅长——最突出的是数学。如果你要求ChatGPT评估任何简单的算术问题，那么它通常会给出正确的答案，因为它实际上记住了所有简单的问题。但是，随着数字的增大或计算的复杂性增加，模型的估计将越来越差。更糟糕的是，这些错误通常被自信地作为事实呈现。
- en: Finally, by themselves, chat models don’t *do* anything at all—they just talk!
    The only way they can make a change in the real world is by asking the user to
    do something for them. Language models can’t buy plane tickets, send emails, or
    change the temperature on the thermostat.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，单就聊天模型本身而言，它们什么也不做——它们只是聊天！它们在现实世界中产生变化的唯一方式是通过请求用户为他们做某事。语言模型无法购买机票、发送电子邮件或改变恒温器的温度。
- en: 'To address all of these issues, the LLM community is turning to tool usage
    to give language models access to up-to-date information, help them perform nonlanguage
    tasks, and help them interact with the world around them. The idea is simple:
    tell the model about tools it has access to and when and how to use them, and
    the model will then use the tools to execute external APIs. It’s the job of the
    application to parse the tool invocation from the model completion, relay the
    request to a real-world API, and then incorporate that information into future
    prompts sent to the model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决所有这些问题，LLM社区正在转向工具使用，以使语言模型能够访问最新信息，帮助他们执行非语言任务，并帮助他们与世界互动。这个想法很简单：告诉模型它能够访问的工具以及何时以及如何使用它们，然后模型将使用这些工具来执行外部API。应用程序的任务是从模型完成中解析工具调用，将请求传递给现实世界的API，然后将这些信息纳入发送给模型的未来提示中。
- en: LLMs Trained for Tool Usage
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为工具使用而训练的LLMs
- en: In June of 2023, OpenAI introduced a new model that was fine-tuned for tool
    invocation, and several other competing LLMs have since followed suit. Let’s take
    a look at OpenAI’s take on tools.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年6月，OpenAI推出了一种针对工具调用的新模型，此后，其他几个竞争性的大型语言模型也相继效仿。让我们来看看OpenAI对工具的看法。
- en: Defining and using tools
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义和使用工具
- en: 'First, we set up the actual functions that reach out into the real world, gather
    information, and make changes to the environment. The implementation is mocked
    out, but if you’re so inclined, it would not be difficult to find a Python library
    that allows you to interact with a real thermostat:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置实际的功能，这些功能可以进入现实世界，收集信息，并对环境进行更改。实现是模拟的，但如果你有兴趣，找到允许你与真实恒温器交互的Python库并不困难：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next up, we represent both of these functions as [JSON schema](https://oreil.ly/rZsdN)
    so that OpenAI can represent them in the prompt:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将这两个功能表示为[JSON模式](https://oreil.ly/rZsdN)，以便OpenAI可以在提示中代表它们：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The JSON schema declares both functions, including their arguments. The functions
    and arguments also have description text that tells the model how the functions
    and arguments are intended to be used.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: JSON模式声明了这两个函数，包括它们的参数。这些函数和参数还有描述性文本，告诉模型如何使用这些函数和参数。
- en: 'Next, we create a look-up dictionary so that our tools can be retrieved by
    name when necessary:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个查找字典，以便在必要时可以通过名称检索我们的工具：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With all that in place, we are ready to make the actual message handling functionality.
    The `process_messages` function in [Example 8-1](#ex-8-1) is similar to what you’ll
    find in the OpenAI function calling documentation, but it’s improved in that this
    implementation allows for tools to be easily swapped—just modify the `tools` and
    `available_functions` definitions described previously.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些准备就绪之后，我们就可以开始制作实际的消息处理功能了。[示例 8-1](#ex-8-1) 中的 `process_messages` 函数与你在OpenAI函数调用文档中找到的类似，但它得到了改进，这种实现允许轻松地交换工具——只需修改之前描述的
    `tools` 和 `available_functions` 定义。
- en: Example 8-1\. Algorithm for processing messages and invoking and evaluating
    tools
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-1\. 处理消息和调用及评估工具的算法
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `process_messages` function in the example takes a list of messages and
    passes them to the model (in Step 1). The model will always return a response
    in the voice of the assistant, and this message is added to the list of messages
    passed in (in Step 2). It’s possible that the assistant message contains prose
    content for the user, tool invocation requests, or both. If tools are requested
    (as in Step 3), then for each tool invocation request, we extract the function
    name and arguments, call the actual function (in Step 4), and then add the function
    output to a new message appended to the end of the list of messages (in Step 5).
    After the function completes, the provided messages have been extended by the
    new messages derived from the model input.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的`process_messages`函数接收一条消息列表，并将其传递给模型（在步骤1）。模型将始终以助手的语音返回响应，并将此消息添加到传入的消息列表中（在步骤2）。可能助手的消息包含用户的内容、工具调用请求或两者兼有。如果请求工具（如步骤3所示），那么对于每个工具调用请求，我们将提取函数名称和参数，调用实际函数（在步骤4），然后将函数输出添加到消息列表末尾的新消息中（在步骤5）。函数完成后，提供的消息已通过从模型输入中派生的新消息进行了扩展。
- en: 'Let’s take a look at how `process_messages` works when provided with a user
    request to modify the temperature:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当提供修改温度的用户请求时`process_messages`是如何工作的：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When this code is run, we can examine the messages and see that two new messages
    have been created:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行此代码时，我们可以检查消息并看到已创建了两个新的消息：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As expected, the first message, which comes from the model, is a call to the
    `get_room_temp` tool. The subsequent message, provided by the application, injects
    the room temperature (74ºF) that was retrieved from calling the actual `get_room_temp`
    function. (Notice that there can be more than one tool call at a time. The IDs
    are required to make sure the correct tool response is associated with its corresponding
    tool request.)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，第一条消息来自模型，是调用`get_room_temp`工具。随后的消息由应用程序提供，注入了从实际调用`get_room_temp`函数中检索到的室温（74华氏度）。（请注意，一次可以有多个工具调用。需要ID以确保正确的工具响应与其相应的工具请求相关联。）
- en: 'We’re not done yet. The application knows the current room temperature, but
    it still has to set the new temperature. Notice that `process_messages` has appended
    both of the new messages to the messages array, so we can progress one more turn
    in the conversation simply by calling `process_messages` once more:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。应用程序知道当前室温，但仍需要设置新的温度。请注意，`process_messages`已将这两条新消息附加到消息数组中，因此我们只需再次调用`process_messages`即可进行一次更多的对话轮次：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This leads to the following new messages:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下新的消息：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Appropriately, the model calls `set_room_temp` with the arguments `{"temp":76}`,
    which is 2 degrees warmer than the current room temperature—this is just what
    the user wanted!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 适当地，模型使用参数`{"temp":76}`调用`set_room_temp`，这比当前室温高出2度——这正是用户所希望的！
- en: 'But it’s rude not to let the user know what just happened, so we make one more
    request:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但不通知用户刚刚发生的事情是不礼貌的，所以我们再做一个请求：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This generates a single new message—a response in the voice of the assistant:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了一条新的消息——助手的响应：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: At this point, we don’t quite have conversation agency because we are manually
    calling `process_messages`. But I expect you can see that we’re basically one
    while loop away from full autonomy. Don’t worry, we’ll wrap it all up by the end
    of this chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们还没有完全获得对话代理权，因为我们正在手动调用`process_messages`。但我预计你能够看到，我们基本上只需要一个while循环就能实现完全自主。别担心，我们将在本章结束时将其全部完成。
- en: Take a look under the hood
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 看看内部结构
- en: Tool calling feels fundamentally different from document completion. How does
    the model accomplish this? It must surely be something special and different from
    plain-old document completion, right? *Wrong!* Remember how chat seemed special
    and different? In [Chapter 3](ch03.html#ch03a_moving_toward_chat_1728432131625250),
    we showed that under the hood, the OpenAI chat API converts system, user, and
    assistant messages to ChatML-formatted transcripts, and then, the model simply
    completes those documents. In just the same way that chat is a fine-tuned model
    plus syntactic sugar at the API level, tool calling is *also* a fine-tuned model
    plus syntactic sugar at the API level. Let’s look under the hood!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 工具调用在本质上与文档补全不同。模型是如何完成这个任务的？它肯定一定是某种特殊且与普通的文档补全不同的东西，对吧？*错误！* 记得聊天看起来很特别，不同吗？在[第3章](ch03.html#ch03a_moving_toward_chat_1728432131625250)中，我们展示了OpenAI聊天API在底层将系统、用户和助手消息转换为ChatML格式的记录，然后模型简单地完成这些文档。就像聊天是一个经过微调的模型加上API级别的语法糖一样，工具调用*也是*一个经过微调的模型加上API级别的语法糖。让我们看看底层！
- en: First, let’s look at how tools are represented in the internal prompt. It’s
    important to understand what tools look like in the prompt because this informs
    how you should describe the tools and interact with them at the API level. Also,
    we need to account for the size of the tools’ representation in the prompt because
    it counts against your token budget. Unfortunately, OpenAI provides no documentation
    for the internal representation, so what follows is our best attempt to reconstruct
    the internal prompt format based on our interrogations of the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看工具在内部提示中的表示方式。了解提示中工具的外观非常重要，因为这会告知您应该如何描述工具并在API级别与之交互。此外，我们还需要考虑工具表示在提示中的大小，因为它会抵扣您的令牌预算。不幸的是，OpenAI没有提供内部表示的文档，所以以下内容是我们根据对模型的询问，尽我们所能重建内部提示格式的最佳尝试。
- en: 'Let’s consider the `set_room_temp` function defined earlier in this section.
    In the internal prompt, it looks like this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑本节中定义的`set_room_temp`函数。在内部提示中，它看起来像这样：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: First, notice that the tool definitions are placed in the system message just
    after the message that you provide. Function definitions are just part of the
    document, formatted, again, as ChatML.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意工具定义放置在您提供的消息之后的系统消息中。函数定义只是文档的一部分，再次格式化为ChatML。
- en: Next, see how the prompt makes use of markdown to organize and format the response?
    This is a good example of the Little Red Riding Hood principle—markdown is a motif
    that occurs often in training data, and the model readily understands the structure
    it implies. (This is also a hint that *you* should use markdown when organizing
    your own prompts.)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，看看提示如何使用Markdown来组织和格式化响应？这是一个很好的Little Red Riding Hood原则的例子——Markdown是一个在训练数据中经常出现的主题，模型很容易理解它所暗示的结构。（这也是一个提示，*您*在组织自己的提示时应该使用Markdown。）
- en: 'The final thing to notice here is that the snippet represents tools as if they
    were TypeScript functions. This is clever for several reasons:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要注意的最后一件事是，该片段将工具表示为TypeScript函数。这有几个原因很聪明：
- en: TypeScript allows for a much richer vocabulary for type definitions. This helps
    ensure that the model will format the arguments using the correct types.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TypeScript允许类型定义有更丰富的词汇。这有助于确保模型将使用正确的类型格式化参数。
- en: It’s easy to incorporate the documentation into the function definition. Notice
    that not only is the function documented, but the individual arguments are documented
    as well.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文档集成到函数定义中很容易。请注意，不仅函数有文档说明，各个参数也有文档说明。
- en: The way in which the function is defined *requires* the function to be invoked
    with a JSON object that lists out the argument names. This ensures that functions
    are called very consistently—which makes them easier to parse. Also, because of
    the requirement to specify each argument by name, as opposed to possibly using
    positional arguments, the model is much more “thoughtful” about the function call
    and much less likely to make mistakes. The model literally says temp right before
    it specifies the value, making it difficult to accidentally specify the wrong
    value.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数的定义方式*要求*使用列出参数名称的JSON对象来调用函数。这确保了函数调用非常一致——这使得它们更容易解析。此外，由于要求按名称指定每个参数，而不是可能使用位置参数，模型在函数调用方面更加“深思熟虑”，出错的可能性也小得多。模型在指定值之前确实说了temp，这使得意外指定错误值变得困难。
- en: 'Now, since we know how the tool definitions are represented, let’s take a look
    at their invocation and evaluation. This is what it looks like internally:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们知道工具定义是如何表示的，让我们看看它们的调用和评估。这是其内部的样子：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, the assistant uses special syntax to invoke functions—using the `name`
    field of the OpenAI message to specify the function name and the `content` field
    to specify the arguments as a JSON object. Let’s dwell upon this for a moment.
    Remember from [Chapter 2](ch02.html#ch02_understanding_llms_1728407258904677)
    that, at its very core, the model is just predicting the next token? Well, this
    is used to great effect here, because just about every single token in the tool
    invocation serves a purpose in narrowing down the tool invocation problem. Just
    look at this single message:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，助手使用特殊语法调用函数——使用 OpenAI 消息的 `name` 字段指定函数名称，使用 `content` 字段将参数指定为 JSON 对象。让我们稍作停留。记得从
    [第二章](ch02.html#ch02_understanding_llms_1728407258904677)，模型在最核心的任务只是预测下一个标记？嗯，这里正是这样使用的，因为几乎每个工具调用中的标记都在缩小工具调用问题中发挥作用。只需看看这条单独的消息：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Take a look at each step of the completion and notice how at every point, the
    model is effectively acting as a classification algorithm, deciding what should
    happen next:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 查看完成过程的每一步，注意模型在每一个点上都有效地充当一个分类算法，决定接下来应该发生什么：
- en: '*Who should speak?* The OpenAI API, rather than the model, inserts `<|im_start|>assistant`
    at the beginning of the completion text. This conditions the model to generate
    the subsequent text in the voice of the assistant. The API forces this text into
    the prompt. If it had not, then it’s plausible that the model could have generated
    another message from the user. Forcing the speaker is safer.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*谁应该发言？* OpenAI API，而不是模型，在完成文本的开头插入 `<|im_start|>assistant`。这使模型生成随后的文本以助手的语气。API
    强制将此文本放入提示中。如果没有这样做，那么模型可能已经生成了另一条用户消息。强制发言者更安全。'
- en: '*Should a tool be called?* The next tokens, `to=functions.`, are generated
    by the model. They indicate that a tool is to be called. But the model could have
    also generated `\n`, conditioning the model to generate a message from the assistant.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*是否需要调用工具？* 下一个标记 `to=functions.` 是由模型生成的。这表明需要调用一个工具。但模型也可能生成 `\n`，使模型生成一条来自助手的消息。'
- en: '*Which tool should be called?* The next tokens the model generates represent
    the name of the function: in this case, `set_room_temp\n`.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*应该调用哪个工具？* 模型接下来生成的标记代表函数的名称：在这种情况下，是 `set_room_temp\n`。'
- en: '*Which argument should be specified?* The next text generated from the model
    infers the argument that should be specified. In this case, there is only one
    option `{"temp":`, but in more complicated tools with multiple, possibly nonrequired
    arguments, the model can use this opportunity to select from several options.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*应该指定哪个参数？* 模型接下来生成的文本推断出应该指定的参数。在这种情况下，只有一个选项 `{"temp":`，但在更复杂的工具中，可能有多个参数，可能不是必需的，模型可以利用这个机会从几个选项中进行选择。'
- en: '*What value will the argument have?* The model next predicts the value that
    the current argument is going to take: in this case, 77\. If there are multiple
    arguments, then the model loops through steps 4 and 5 several times.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*参数将具有什么值？* 模型接下来预测当前参数将要取的值：在这种情况下，是 77。如果有多个参数，那么模型将多次循环步骤 4 和 5。'
- en: '*Are we done?* Once all the arguments have been specified, the model predicts
    that it’s time to wrap up. It predicts `}<|im_end|>`, which closes the JSON and
    the assistant message.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我们完成了吗？* 一旦所有参数都已指定，模型预测现在是时候结束。它预测 `}<|im_end|>`，这关闭了 JSON 和助手消息。'
- en: How awesomely flexible these models are! In the span of 10 to 20 tokens, the
    same, generic underlying neural network has effectively implemented 5 different,
    highly specialized inference algorithms. (Recall that step 1 was specified at
    the API rather than being inferred.) Wow...just wow. Also, see that at each step,
    the problem is broken down hierarchically. Do we need a tool? Which tool? Which
    arguments are required? What are the values for those arguments?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型是多么地灵活！在 10 到 20 个标记的范围内，相同的通用底层神经网络有效地实现了 5 个不同、高度专业化的推理算法。（回想一下，步骤 1 是在
    API 中指定的，而不是推断出来的。）哇...真是令人惊叹。此外，注意在每一步，问题都是按层次分解的。我们需要工具吗？哪个工具？需要哪些参数？这些参数的值是什么？
- en: 'After tool invocation comes an evaluation message. Here, OpenAI has introduced
    a new `tool` role for the purpose of incorporating evaluation data back into the
    prompt. The output of the `set_room_temp` function is just `DONE` (indicating
    success), so the response message looks like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在工具调用之后是一个评估消息。在这里，OpenAI引入了一个新的`tool`角色，用于将评估数据重新整合到提示中。`set_room_temp`函数的输出只是`DONE`（表示成功），因此响应消息看起来像这样：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note: The ID of the tool call and response that was present at the API level
    is no longer required because the API used the IDs to assemble the corresponding
    tool calls and responses together in the correct order.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：API级别存在的工具调用和响应的ID不再需要，因为API使用了ID将相应的工具调用和响应按正确的顺序组装在一起。
- en: Guidelines for Tool Definitions
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具定义的指南
- en: 'This section provides general guidelines for you to follow when you’re designing
    and describing tools associated with conversational agents. Primarily, these guidelines
    rely upon two bits of intuition:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了在设计和管理与对话代理相关的工具时你应该遵循的一般性指南。主要来说，这些指南依赖于以下两点直觉：
- en: Whatever is easier for a human to understand is also easier for an LLM to understand.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对人类来说更容易理解的东西，对大型语言模型（LLM）来说也更容易理解。
- en: The best results are derived by patterning prompts after training data (a.k.a.
    the Little Red Riding Hood principle).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最好的结果是通过模仿训练数据（即小红帽原则）来构建提示。
- en: Selecting the right tools
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的工具
- en: Limit the number of tools the model has access to at once. The more tools available
    to the model, the greater the chance that the model will get confused. To the
    extent possible, the tools should partition the domain activity—that is, they
    should cover as much of the domain as possible but avoid tools that perform similar
    actions. Simpler tools are better. *Do not* copy your web API into the prompt!
    Web APIs often have tons of parameters and complex responses. Describing the API
    will take up tons of space, and the model will be less successful at invoking
    such a complex tool.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 限制模型一次可访问的工具数量。模型可用的工具越多，模型混淆的可能性就越大。尽可能让工具划分领域活动——也就是说，它们应该尽可能覆盖领域，但避免执行类似动作的工具。更简单的工具更好。*不要*将你的Web
    API复制到提示中！Web API通常有大量的参数和复杂的响应。描述API会占用大量空间，并且模型在调用如此复杂的工具时不太可能成功。
- en: Naming tools and arguments
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名工具和参数
- en: Names should be meaningful and self-documenting because, like a human reading
    an API specification, the model will read the names and build some expectations
    about the purpose of the tools and arguments. For OpenAI, the tools are presented
    as TypeScript in the prompt; it’s a good idea to follow suit and use camel case
    naming conventions. In any case, avoid names that are lowercase concatenations
    of words (e.g., `retrieveemail`) because these are more difficult to parse.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 名称应该有意义且自解释，因为就像人类阅读API规范一样，模型会读取名称并构建一些关于工具和参数目的的预期。对于OpenAI，工具在提示中以TypeScript的形式呈现；遵循这一做法并使用驼峰命名约定是个好主意。无论如何，避免使用单词的小写连接（例如，`retrieveemail`），因为这些名称更难解析。
- en: Defining tools
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义工具
- en: Generally, you should make the definitions as simple as possible while capturing
    enough details about the tool so that the model (or a human) would understand
    how to use it. If your definitions sound like legalese, then you may be introducing
    too many concepts for the model to process with its limited attention mechanism.
    Simplify it if you can, but if your tool legitimately requires a detailed explanation,
    then make sure the definition doesn’t leave any ambiguity that the model will
    trip over.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你应该尽可能简化定义，同时捕捉足够关于工具的细节，以便模型（或人类）能够理解如何使用它。如果你的定义听起来像法律术语，那么你可能为模型引入了太多概念，而模型有限的注意力机制难以处理。如果可能，简化它，但如果你的工具确实需要详细说明，那么确保定义没有留下任何模型可能会踩到的歧义。
- en: If you’re working with a public API that the model is familiar with, then lean
    into the model’s training by creating a simplified version of that API that retains
    the naming, concepts, and style of the original API. For instance, when working
    on GitHub Copilot, we found out that the OpenAI model that we were using was well
    aware of GitHub’s code search syntax. (How did we know this? We asked it. The
    model could basically recite our documentation back to us.) We found that it was
    less confusing for the model if we named the arguments as they were in documentation
    and also expected the format of the argument values to be the same as in documentation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用模型熟悉的公共API，那么可以通过创建一个简化版的API来利用模型的训练，这个简化版API保留了原始API的命名、概念和风格。例如，在开发GitHub
    Copilot时，我们发现我们使用的OpenAI模型对GitHub的代码搜索语法非常熟悉。（我们是如何知道的呢？我们问了它。模型几乎能背诵我们的文档。）我们发现，如果我们将参数命名为文档中的名称，并期望参数值的格式与文档中相同，对模型来说会更少混淆。
- en: Dealing with arguments
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理参数
- en: 'Keep the arguments few and simple if possible. Naturally, the OpenAI models
    do fine with all of the JSON schema types: string, number, integer, and boolean.
    You can additionally modify properties with `enum` and `default` to better condition
    the model’s usage of the arguments. However, as of the OpenAI 1106 models (released
    in November 2023), it appears that some JSON schema property modifiers—such as
    `minItems`, `uniqueItems`, `minimum`, `maximum`, `pattern`, and `format`—are not
    represented in the prompt. Similarly, if you have any nested parameters, their
    descriptions are not presented in the prompt.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，尽量保持参数少而简单。自然地，OpenAI模型可以很好地处理所有的JSON模式类型：字符串、数字、整数和布尔值。你可以通过`enum`和`default`属性来修改属性，以更好地控制模型对参数的使用。然而，截至2023年11月发布的OpenAI
    1106模型，一些JSON模式属性修饰符（如`minItems`、`uniqueItems`、`minimum`、`maximum`、`pattern`和`format`）在提示中并未表示。同样，如果你有任何嵌套参数，它们的描述也不会在提示中呈现。
- en: For OpenAI models especially, be cautious of long-form text input for arguments.
    Since the arguments get stuffed into JSON, the values must be newline and quotation
    mark escaped, and the more text there is, the more likely the model is to forget
    an escape. This problem is exacerbated for code that is full of newlines and quotation
    marks. Anthropic, it turns out, encodes their function calls using XML tags rather
    than JSON, so the arguments don’t have to be escaped. In principle, this should
    mean that Claude is more amenable to long-form arguments.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于OpenAI模型尤其如此，对于参数的长时间文本输入要小心。由于参数会被填充到JSON中，因此值必须转义换行符和引号，文本越多，模型忘记转义的可能性就越大。对于充满换行符和引号的代码，这个问题会加剧。事实上，Anthropic使用XML标签而不是JSON来编码他们的函数调用，因此参数不需要转义。原则上，这意味着Claude更愿意接受长时间参数。
- en: 'Finally, watch out for argument hallucination. For example, several tools that
    we’re building at GitHub have org and repo arguments, but if the values for these
    arguments have not been mentioned in the conversation, then the model is liable
    to assume placeholder values like `"my-org"` and `"my-repo"`. There’s no silver
    bullet to solve this, but you can try the following options:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要注意参数幻觉。例如，我们在GitHub上构建的几个工具都有org和repo参数，但如果这些参数的值在对话中没有提到，那么模型可能会假设占位符值如`"my-org"`和`"my-repo"`。没有银弹可以解决这个问题，但你可以尝试以下选项：
- en: When the desired value is known in the application, remove the arguments from
    the function definition so that the model has nothing to be confused about. Alternatively,
    you can provide a default—that way, if the model specifies the default value,
    then you can make appropriate accommodations in the application.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当应用程序中已知所需值时，从函数定义中删除参数，这样模型就没有任何可以混淆的东西了。或者，你可以提供一个默认值——这样，如果模型指定了默认值，你就可以在应用程序中进行适当的调整。
- en: Instruct the model to ask if it’s unsure about an argument—and then pray it
    does, because it often won’t. Don’t worry, though—models are quickly getting better
    at this type of thing.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示模型在不确定参数时询问——然后祈祷它真的会这样做，因为它通常不会。不过，别担心——模型在这方面正在迅速变得更好。
- en: Dealing with tool outputs
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理工具输出
- en: In the tool definitions, make sure the model can anticipate what it will find
    in the output. The outputs can be free-form, natural-language text or a structured
    JSON object. The model should do fine with either. Do not include too much extra
    “just-in-case-it’s-helpful” content in the output because models can be distracted
    by spurious content.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在工具定义中，确保模型可以预测它在输出中会发现什么。输出可以是自由形式的自然语言文本或结构化的JSON对象。模型应该都能处理。不要在输出中包含太多额外的“以防万一有帮助”的内容，因为模型可能会被虚假内容分散注意力。
- en: Dealing with tool errors
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理工具错误
- en: When a tool makes an error, this information is valuable to the model because
    it can look at errors and make corrections. But don’t just spit out the text of
    your internal error message into a tool response—make sure it makes sense in the
    context of the *model’s* definition of the tool. If it’s a validation error, then
    tell the model what it did wrong so that it can try again. If it’s some other
    error that the model should be able to deal with, then make sure the error message
    contains helpful information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个工具出错时，这个信息对模型来说很有价值，因为它可以查看错误并进行纠正。但不要只是将你的内部错误信息文本直接输出到工具响应中——确保它在工具的*模型*定义的上下文中是有意义的。如果是验证错误，那么告诉模型它做错了什么，这样它就可以再次尝试。如果是模型应该能够处理的某些其他错误，那么确保错误信息包含有用的信息。
- en: Executing “dangerous” tools
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行“危险”的工具
- en: When you’re allowing the model to execute tools that make changes in the real
    world, you must protect your users from unintended side effects. *Do not* allow
    the model to execute any tool that could negatively impact a user unless the user
    has *explicitly* signed off first. Naively, you might say to yourself, “No problem,
    in the tool description, I’ll just say, ‘Make sure to double-check with the user
    before you run this.’ and then, we’ll be fine.” *Not so!* Models are inherently
    undependable, and with a strategy like this, we *guarantee* that a small portion
    of the time, the model will do exactly the thing you told it not to do.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当你允许模型执行在现实世界中做出改变的工具时，你必须保护你的用户免受意外副作用的影响。*不要*允许模型执行任何可能对用户产生负面影响的功能，除非用户*明确地*签署了同意。天真地，你可能会对自己说，“没问题，在工具描述中，我只需说‘在运行此操作之前请务必与用户确认。’然后，我们就会没事。”*并非如此!*
    模型本质上是不可靠的，采用这种策略，我们*保证*在很小一部分时间里，模型会做你告诉它不要做的事情。
- en: Instead, don’t prevent the model from calling whatever tool it wants to call.
    That’s right—let it make the request to send all of Bill’s money to his ex-wife’s
    bank account. Just make sure that in the application layer, you intercept all
    such dangerous requests and *explicitly* get sign-off before the application calls
    the actual API and makes a boneheaded mistake.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，不要阻止模型调用它想要的任何工具。没错——让它发出请求，将所有比尔的钱转到他前妻的银行账户。只是确保在应用层中，你拦截所有这样的危险请求，并在应用实际调用API并犯愚蠢错误之前*明确地*获得同意。
- en: Reasoning
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: LLMs select tokens, one by one, to provide a statistically likely completion
    of the prompt (see [Chapter 2](ch02.html#ch02_understanding_llms_1728407258904677)).
    In doing so, LLMs, in a sense, demonstrate a sort of reasoning capability—but
    it’s a very superficial form of reasoning. The model’s only goal—enforced by layers
    of training—is to make text that just, well, *sounds* right. As covered in [Chapter 2](ch02.html#ch02_understanding_llms_1728407258904677),
    the model doesn’t have any sort of internal monologue—so there’s no mental review
    of a problem statement, no consideration of how it maps to known facts, and no
    comparison of several competing ideas. Rather, one by one, the model predicts
    tokens that just fit best after the text being processed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs逐个选择标记，以提供对提示的统计上可能的完成（见[第2章](ch02.html#ch02_understanding_llms_1728407258904677)）。在这个过程中，LLMs在某种程度上展示了某种推理能力——但它是一种非常肤浅的推理形式。模型的唯一目标——由多层训练强制执行——是生成听起来完全正确的文本。如[第2章](ch02.html#ch02_understanding_llms_1728407258904677)所述，模型没有任何形式的内部独白——因此没有对问题陈述的心理审查，没有考虑它如何映射到已知事实，也没有比较几个竞争性想法。相反，模型一个接一个地预测与正在处理的文本最匹配的标记。
- en: So, let’s fix that! There are several tricks that you can use to make the model
    more thoughtful in its response, and all of them have to do with giving the model
    an internal monologue that allows it to reason more carefully through a problem
    before providing a final response.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来解决这个问题！你可以使用几个技巧来使模型在回答时更加深思熟虑，所有这些技巧都与给模型一个内部独白有关，这允许它在提供最终回答之前更仔细地通过问题进行推理。
- en: Chain of Thought
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链
- en: In the January 2022 paper titled [“Chain-of-Thought Prompting Elicits Reasoning
    in Large Language Models”](https://arxiv.org/abs/2201.11903), the authors demonstrated
    that few-shot examples can be used to condition a model to be more thoughtful—and
    therefore more accurate—in its responses. Normally, a model would answer a commonsense
    question like “Will *The Exorcist* stimulate the limbic system?” with a yes or
    no, followed by an explanation. That’s how humans speak and therefore how models
    have learned to respond. But since the model has no internal monologue, then the
    initial yes or no will be an intuitive guess and the explanation will actually
    be a rationalization to justify that guess.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年1月发表的题为[“Chain-of-Thought Prompting Elicits Reasoning in Large Language
    Models”](https://arxiv.org/abs/2201.11903)的论文中，作者们展示了少量示例可以用来条件化模型，使其回答更加周到——因此更加准确。通常，模型会以是或否回答一个常识问题，如“*《驱魔人》*会刺激边缘系统吗？”，然后给出解释。这就是人类说话的方式，因此模型也学会了这样回答。但由于模型没有内部独白，所以最初的“是”或“否”将是一个直观的猜测，而解释实际上是对这个猜测的合理化。
- en: 'The authors of the chain-of-thought paper demonstrated that if you could have
    the model reason about the question first and *then* give the answer, it was more
    likely to arrive at the correct answer. They achieved this by providing the model
    with few-shot examples to condition subsequent model responses toward thinking
    and then answering. Here are a couple of the few-shot examples:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 链式思维论文的作者们展示了，如果你能让模型先对问题进行推理，然后**再**给出答案，那么更有可能得出正确答案。他们通过向模型提供少量示例来条件化后续模型响应，使其倾向于思考和回答，从而实现了这一点。以下是一些少量示例：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Provided several such examples, the subsequent answer to the question about
    *The Exorcist* now looks like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了几个这样的例子后，关于*《驱魔人》*的问题的后续回答现在看起来是这样的：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Using the StrategyQA dataset and the PaLM 540B model, the paper indicated that
    this style of chain-of-thought reasoning increased accuracy when answering commonsense
    questions from the prior state-of-the-art rate of 69.4% to 75.6%.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用StrategyQA数据集和PaLM 540B模型，论文指出这种链式思维推理风格在回答常识问题时提高了准确性，从之前的最先进率69.4%提升到了75.6%。
- en: But the domain of answering commonsense questions was not the only domain that
    benefitted. As a matter of fact, answers to math problems showed significant improvements.
    When applying the PaLM 540B model against a battery of math word problems from
    the GSM8K dataset, the authors demonstrated a solve rate increase from roughly
    20% with standard prompting to 60% with chain-of-thought reasoning. The chain-of-thought
    paper demonstrated similar benefits with several other datasets and other domains
    such as symbolic reasoning.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但受益的领域不仅仅是回答常识问题。事实上，数学问题的答案也显示出显著的改进。当将PaLM 540B模型应用于GSM8K数据集中的一系列数学词汇问题时，作者们展示了从标准提示的约20%的解决率提高到链式思维推理的60%。链式思维论文在几个其他数据集和其他领域，如符号推理，也展示了类似的好处。
- en: In May of 2022, a subsequent paper titled [“Large Language Models are Zero-Shot
    Reasoners”](https://arxiv.org/abs/2205.11916) one-upped the chain-of-thought paper
    with a clever trick. Rather than curating sets of relevant few-shot examples to
    get the model into a pattern of thinking out loud, this paper showed that you
    can simply start the answer with the phrase, “Let’s think step-by-step,” and that
    cue would cause the model to generate chain-of-thought reasoning followed by a
    more accurate response.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年5月，一篇题为[“大型语言模型是零样本推理者”](https://arxiv.org/abs/2205.11916)的后续论文通过一个巧妙的技巧超越了链式思维论文。这篇论文不是通过精心挑选相关少量示例来让模型进入大声思考的模式，而是展示了你可以简单地以“让我们一步步思考”的短语开始回答，这个提示会导致模型生成链式思维推理，然后给出更准确的回答。
- en: 'Another paper from October 2023 titled [“Think Before you Speak: Training Language
    Models With Pause Tokens”](https://arxiv.org/abs/2310.02226) took chain of thought
    to a somewhat bizarre extreme. The authors fine-tuned a language model to use
    a “pause” token, and after asking a question, they would inject some number, say
    10, of these meaningless tokens into the prompt. The effect was that the model
    had additional timesteps to reason about the answer. The information from previous
    tokens got more thoroughly incorporated into the model state so that it produced
    a better answer. This is analogous to what humans do—we have our own “pause” tokens
    called “Uh,” and “Um,” and we use them when we are stalling for more time to think
    about what we’re going to say.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '另一篇2023年10月发表的论文题为“Think Before you Speak: Training Language Models With Pause
    Tokens”（[“Think Before you Speak: Training Language Models With Pause Tokens”](https://arxiv.org/abs/2310.02226)），将思维链推进到一个相当奇特的程度。作者微调了一个语言模型来使用“暂停”标记，在提问后，他们会将一些无意义的标记，比如10个，注入提示中。结果是模型有额外的时间步来推理答案。来自先前标记的信息被更彻底地纳入模型状态，从而产生了更好的答案。这与人类的行为类似——我们有自己的“暂停”标记，称为“嗯”和“啊”，我们在需要更多时间思考将要说什么时使用它们。'
- en: The main point to understand in this section is the point we made at the beginning—language
    models have no internal monologue and therefore no way to think about something
    before blurting out an answer. If you can condition a model to spend some time
    thinking about the problem—be it through few-shot examples or simply by requesting
    it—then the model will be much more likely to generate a good completion.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中需要理解的主要观点是我们一开始就提出的观点——语言模型没有内部独白，因此无法在脱口而出答案之前思考某事。如果你能训练模型花一些时间去思考问题——无论是通过少量示例，还是简单地请求它——那么模型更有可能生成一个良好的补充。
- en: 'ReAct: Iterative Reasoning and Action'
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ReAct: 迭代推理和行动'
- en: 'The October 2022 paper titled [“ReAct: Synergizing Reasoning and Acting in
    Language Models”](https://arxiv.org/abs/2210.03629) took reasoning one level deeper
    by looking at situations that require information retrieval and multistep problem
    solving. Also, for a little extra fun, this paper was one of the first to make
    use of the external tools.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '2022年10月发表的论文题为“ReAct: Synergizing Reasoning and Acting in Language Models”（[“ReAct:
    Synergizing Reasoning and Acting in Language Models”](https://arxiv.org/abs/2210.03629)），通过研究需要信息检索和多步问题解决的情况，将推理推进了一个层次。此外，为了增加一些乐趣，这篇论文是第一批使用外部工具的论文之一。'
- en: Of the domains investigated in the paper, the most interesting for our purposes
    is the HotpotQA, a dataset that contains questions like “Which magazine was started
    first, *Arthur’s Magazine* or *First for Women*?” As a human, think about how
    you would answer this question. You would probably look up both of these magazines,
    find the date they were first published, compare the dates, and then declare the
    answer. This is the type of multistep reasoning that the ReAct authors intended
    to demonstrate.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中调查的领域里，对我们来说最有兴趣的是HotpotQA，这是一个包含像“哪本杂志先开始，*《亚瑟杂志》*还是*《女性第一》？”这样的问题的数据集。作为一个人类，想想你会如何回答这个问题。你可能会上网查找这两本杂志，找到它们首次出版的日期，比较日期，然后宣布答案。这正是ReAct作者想要展示的多步推理类型。
- en: 'The authors of this paper introduced the notion of three different tools to
    aid the model in finding the answer:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的作者引入了三种不同工具的概念，以帮助模型找到答案：
- en: Search[entity]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Search[实体]
- en: This returns the first five sentences from the corresponding Wikipedia page
    if it exists or otherwise returns the top five most similar entities based on
    a Wikipedia search.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在相应的维基百科页面，则返回前五句话，否则根据维基百科搜索返回前五个最相似实体。
- en: Lookup[string]
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Lookup[字符串]
- en: This searches the most recent entity (from Search) and returns the next sentence
    that contains the provided string.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这会搜索最新的实体（来自Search）并返回包含提供字符串的下一句。
- en: Finish[answer]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Finish[答案]
- en: This signals that the work is complete and indicates the final answer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明工作已完成，并指示最终答案。
- en: The expectation is for the model to address the question by iteratively thinking
    about what needs to be done; acting by using the `Search` or `Lookup` tool to
    gather information; and observing the answers from the tools. After several think-act-observe
    loops, the model will have the information it needs and will end the session by
    selecting the `Finish` tool and declaring the final answer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 预期模型通过迭代思考需要做什么来回答问题；通过使用`Search`或`Lookup`工具来收集信息来行动；并观察工具的答案。经过几次思考-行动-观察循环后，模型将拥有所需的信息，并通过选择`Finish`工具并宣布最终答案来结束会话。
- en: 'Here’s an example (drawn from the paper) of how this would work for the preceding
    question:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子（来自论文），说明了这如何适用于前面的问题：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To condition the model to make use of `Search`, `Lookup`, and `Finish` tools,
    the ReAct authors injected the following preamble into the prompt:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型能够使用`Search`、`Lookup`和`Finish`工具，ReAct的作者们在提示中注入了以下前言：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is then followed by six examples of the think-act-observe pattern similar
    to the one shown. Finally, this is followed by the actual question. (The ReAct
    authors put together a short and really well organized [Jupyter notebook](https://oreil.ly/_N_K3)
    if you want to see exactly how this all works.)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是六个与展示的类似思考-行动-观察模式的例子。最后，紧随其后的是实际的问题。（如果你想确切地看到这一切是如何工作的，ReAct的作者们整理了一个简短且非常组织良好的[Jupyter笔记本](https://oreil.ly/_N_K3)。）
- en: So, how well does ReAct perform? Well, initially, the answer was poorly. As
    shown on the left side of [Figure 8-1](#ch08_01_figure_1_1728429579251484), on
    the HotpotQA dataset for every size of model, ReAct was actually *worse* than
    both “standard” prompting (just presenting the model with the question) and chain-of-thought
    prompting. This is because the in-prompt examples were not sufficient to teach
    the model how the tools worked and how to reason.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，ReAct的表现如何呢？起初，答案是表现不佳。如图8-1的左侧所示，在HotpotQA数据集上，对于每个尺寸的模型，ReAct实际上比“标准”提示（只是向模型展示问题）和思维链提示都要差。这是因为提示中的例子不足以教会模型工具的工作方式和如何进行推理。
- en: But after fine-tuning the two smaller models with just three thousand examples,
    ReAct suddenly shoots into the lead. As the right side of [Figure 8-1](#ch08_01_figure_1_1728429579251484)
    shows, not only does ReAct outperform standard and chain-of-thought prompting
    on same-size models, but now, ReAct on the fine-tuned 8B model outperforms the
    standard prompting approaches on the original 62B model. And similarly, ReAct
    on the fine-tuned 62B model outperforms the other prompting approaches on the
    original 540B model. So with proper reasoning on a *slightly* fine-tuned model,
    we can achieve much higher quality than is available on a much larger vanilla
    model without the reasoning steps.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但在仅用三千个例子微调了两个较小的模型之后，ReAct突然跃居领先。如图8-1的右侧所示，ReAct不仅在同一尺寸的模型上优于标准提示和思维链提示，而且现在，微调后的8B模型在原始的62B模型上也优于标准提示方法。同样，微调后的62B模型在原始的540B模型上也优于其他提示方法。因此，在略微微调的模型上进行适当的推理，我们可以实现比在大型的未微调模型上更高的质量，后者没有推理步骤。
- en: '![A graph of different colored bars  Description automatically generated with
    medium confidence](assets/pefl_0801.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![不同颜色条形图的图表，描述自动生成，置信度中等](assets/pefl_0801.png)'
- en: Figure 8-1\. Performance of the ReAct prompt strategy, before and after fine-tuning
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. ReAct提示策略在微调和未微调前的性能
- en: Part of the success of ReAct with HotpotQA tasks is due to the fact that ReAct
    can use search tools to look up facts that the model is missing. If you skip the
    reasoning step, then the performance is still pretty good; this is represented
    as the Act data in [Figure 8-1](#ch08_01_figure_1_1728429579251484).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct在HotpotQA任务中的部分成功归因于ReAct可以使用搜索工具查找模型缺失的事实。如果你跳过推理步骤，那么性能仍然相当不错；这如图8-1中的Act数据所示。
- en: 'Reasoning becomes critical in decision-making tasks such as ALFWorld. For the
    ALFWorld benchmark, the model is required to act as an agent navigating and performing
    tasks in a simulated house (reminiscent of old-school word-based role-playing
    games). In this domain, the importance of the *thinking* step is clear. The paper
    enumerates several features of thinking that lead to improved success rates:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在ALFWorld等决策任务中，推理变得至关重要。对于ALFWorld基准，模型需要作为一个代理在模拟房屋中导航和执行任务（类似于老式的基于文字的角色扮演游戏）。在这个领域，思考步骤的重要性是显而易见的。论文列举了几个导致成功率提高的思考特征：
- en: Decomposing task goals and creating plans of action
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分解任务目标和制定行动计划
- en: Injecting commonsense knowledge relevant to solving the task
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注入与解决任务相关的常识性知识
- en: Extracting helpful details from observations
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从观察中提取有用的细节
- en: Tracking progress and pushing action plans forward
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪进度并推进行动计划
- en: Handling exceptions and adjusting the course of action
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理异常情况并调整行动方案
- en: Compared to thinking and then acting (*ReAct*), acting alone (*Act*) is worse
    at breaking down goals into subgoals, and it tends to lose track of the environmental
    state. ReAct demonstrates a success rate of 71% in ALFWorld tasks, whereas Act
    leads to a mere 45% success rate. That’s a big difference!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与先思考后行动（*ReAct*）相比，单独行动（*Act*）在将目标分解为子目标方面表现更差，并且容易失去对环境状态的跟踪。ReAct在ALFWorld任务中的成功率达到了71%，而Act的成功率仅为45%。这是一个很大的差距！
- en: Beyond ReAct
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越ReAct
- en: 'While ReAct has been a very important step in improving reasoning capabilities
    in LLM applications, it’s not the last improvement we’ll see. In this short section,
    we present a couple of related approaches that show promise. The first is [plan-and-solve
    prompting](https://arxiv.org/abs/2305.04091). Whereas ReAct jumps right in with
    the think-act-observe loop, the plan-and-solve approach prompts the model to first
    devise an overarching plan. It uses the following prompt:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ReAct在提高LLM应用中的推理能力方面是一个非常重要的步骤，但它不是我们将看到的最后一个改进。在本节中，我们介绍了几种有希望的相关方法。第一种是[计划并解决提示](https://arxiv.org/abs/2305.04091)。与ReAct直接进入思考-行动-观察循环不同，计划并解决方法提示模型首先制定一个总体计划。它使用以下提示：
- en: Let’s first understand the problem and devise a plan to solve the problem. Then,
    let’s carry out the plan and solve the problem step-by-step.
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先，让我们理解问题并制定一个解决问题的计划。然后，让我们执行这个计划，逐步解决问题。
- en: Unlike ReAct, the plan-and-solve prompting paper doesn’t involve any tool usage;
    it’s purely focused on improving reasoning without reaching for data from the
    outside world. So really, plan-and-solve prompting is more closely analogous to
    the approach in the chain-of-thought section, which used the prompt, “Let’s think
    step-by-step.” The key point here is that the model may perform better in certain
    domains if we ask it to holistically understand the problem and make a plan before
    jumping directly into the actual step-by-step problem-solving. Combining this
    preplanning approach with ReAct’s think-act-observe steps might lead to further
    reasoning improvements.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 与ReAct不同，计划并解决提示文档不涉及任何工具的使用；它纯粹关注于提高推理能力，而不从外部世界获取数据。因此，实际上，计划并解决提示与思维链部分的方法更为相似，该方法使用了提示“让我们一步步思考”。关键点在于，如果我们要求模型在直接进入实际步骤解决问题之前，全面理解问题并制定计划，那么模型在某些领域可能会表现得更好。将这种预先规划方法与ReAct的思考-行动-观察步骤相结合，可能会进一步改善推理能力。
- en: 'If plan-and-solve prompting augments ReAct with preemptive planning, then *Reflexion*,
    introduced in the [widely cited 2023 paper “Reflexion: Language Agents with Verbal
    Reinforcement Learning](https://arxiv.org/abs/2303.11366)”, does the opposite—it
    allows the model to review its work after the fact, identify problems, and make
    better plans next time. Naturally, if the model has made a mistake that isn’t
    undoable, then this is of little help. (“I’m sorry for transferring your assets
    to your ex-husband’s account. I won’t do *that* again!”) But there are plenty
    of domains where you get a do-over. A great example near to our work at GitHub
    is in writing software that passes a suite of unit tests. With Reflexion, you
    can create pieces of the software using whatever approach you’d like (ReAct is
    cited in the paper), and then, once the work is finished, if the unit tests don’t
    pass, the failure messages can be inserted into the prompt so that the model can
    try again and this time avoid making the same mistakes.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '如果计划并解决提示增强了ReAct的预先规划，那么在[广受引用的2023年论文“Reflexion: Language Agents with Verbal
    Reinforcement Learning](https://arxiv.org/abs/2303.11366)】中引入的*Reflexion*则相反——它允许模型在事后回顾其工作，识别问题，并为下一次制定更好的计划。当然，如果模型犯了一个无法撤销的错误，那么这几乎没有什么帮助。（“很抱歉把你的资产转到了你前夫的账户上。我不会再*那样*做了！”）但有很多领域可以重新开始。GitHub附近的一个与我们工作相关的好例子是在编写通过一系列单元测试的软件。使用Reflexion，你可以使用你喜欢的任何方法（论文中引用了ReAct），一旦工作完成，如果单元测试未通过，可以将失败信息插入到提示中，以便模型可以再次尝试，这次避免犯同样的错误。'
- en: '[Branch-solve-merge](https://arxiv.org/abs/2310.15123) is an approach that
    you might be able to guess from its name. Given a problem, you branch to *N* different
    *solvers*—independent LLM conversations—each of which tackles the problem in isolation.
    You could just have them make three independent attempts to solve the problem
    (and depend upon a relatively high temperature to ensure that their solution techniques
    are distinct), or better yet, you could prompt each solver to tackle the problem
    from a different perspective. Once all the solvers are complete, then the content
    they have produced is combined together and placed before a merging agent that
    combines the information from all three solvers into a better or more complete
    solution.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[分支解决合并](https://arxiv.org/abs/2310.15123)是一种你可能可以从其名称中猜出其方法的方法。给定一个问题，你将其分支到*N*个不同的*求解器*——独立的LLM对话——每个求解器都独立地处理问题。你可以让他们独立尝试解决该问题三次（并依赖于相对较高的温度以确保他们的解决方案技术是不同的），或者更好的是，你可以提示每个求解器从不同的角度处理问题。一旦所有求解器都完成了，然后他们产生的内容将被合并在一起，并放置在一个合并代理面前，该代理将所有三个求解器的信息合并成一个更好或更完整的解决方案。'
- en: As we close this section, hopefully, you’ve noticed some converging ideas in
    our conversation. For instance, this section makes use of the tools introduced
    in the first part of this chapter but also introduces new techniques that improve
    the model’s reasoning capabilities. In all the cases in this section, we do this
    by giving the model its own internal monologue so that it can process the situation,
    break down goals, and make better decisions on how to accomplish a task. We now
    have almost all the ingredients necessary for building our own autonomous agents;
    there’s just one more—context.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束这一节时，希望你能注意到我们对话中的一些汇聚思想。例如，本节利用了本章第一部分介绍的工具，同时也引入了新的技术，这些技术可以提升模型的推理能力。在本节的所有情况下，我们都是通过给模型提供它自己的内部独白，以便它能够处理情况，分解目标，并就如何完成任务做出更好的决策来做到这一点的。我们现在几乎拥有了构建我们自己的自主代理所需的所有成分；只是还缺一个——上下文。
- en: Context for Task-Based Interactions
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于任务交互的上下文
- en: In Chapters [5](ch05.html#ch05_prompt_content_1728435524680844) and [6](ch06.html#ch06a_assembling_the_prompt_1728442733857948),
    we discussed in great detail how to find and organize context when building a
    prompt for a document completion model. All of those ideas still hold true, but
    in regard to the task-based interactions that agents perform, there are some new
    things to consider. In this section, we’ll talk about where to retrieve context
    from, how to prioritize it, and how to organize it and represent it in the prompt.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5](ch05.html#ch05_prompt_content_1728435524680844)章和第[6](ch06.html#ch06a_assembling_the_prompt_1728442733857948)章中，我们详细讨论了在构建文档完成模型提示时如何寻找和组织上下文。所有这些想法仍然适用，但关于代理执行的基于任务的交互，有一些新事物需要考虑。在本节中，我们将讨论从哪里检索上下文，如何优先考虑它，以及如何在提示中组织它和表示它。
- en: Sources for Context
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文来源
- en: In a moment, we’re going to build a general-purpose conversational agent. Such
    an agent will carry a variety of context drawn from several sources and couch
    it in the form of a conversational transcript.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在不久的将来，我们将构建一个通用对话代理。这样的代理将携带从多个来源收集的各种上下文，并以对话记录的形式呈现。
- en: First, there is a *preamble*, which sets up agent behavior and makes sure the
    agent understands what tools are at its disposal. If necessary, the preamble can
    include few-shot examples to demonstrate the behavior that the agent should exhibit
    during the conversation. The preamble typically goes in the system message when
    building an OpenAI chat prompt.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有一个*序言*，它设定了代理的行为并确保代理理解它可以使用哪些工具。如果需要，序言可以包括一些示例来展示代理在对话中应该表现出的行为。在构建OpenAI聊天提示时，序言通常放在系统消息中。
- en: The *prior conversation* is composed of all recent back-and-forth messages between
    the user and the assistant, up until the user’s current message. The prior conversation
    contains the broader context of this conversation, including information that
    will be important for the model to consider when handling the user’s current request.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*先前的对话*由用户和助手之间最近的来回消息组成，直到用户当前的消息。先前的对话包含了这次对话的更广泛背景，包括在处理用户当前请求时对模型来说重要的信息。'
- en: Both user and assistant messages may have attached artifacts, and an *artifact*
    is any piece of data that is relevant to the conversation. For instance, a user
    might ask an LLM-based airline assistant about available flights. The artifact
    attached to this conversation would be a representation of the flights available,
    including details that might be helpful later in the conversation—dates, times,
    origin and destination airports, etc.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 用户和助手的消息都可能附加有工件，而**工件**是任何与对话相关的数据。例如，用户可能会询问基于LLM的航空公司助手关于可用航班的信息。附加到这次对话的工件将包括航班可用性的表示，包括可能在对话后期有用的详细信息——日期、时间、出发和目的地机场等。
- en: The *current exchange* begins with the user’s request as well as any artifacts
    they have attached to the conversation. For example, in the application interface,
    the user might indicate that they are talking about something on the screen (for
    instance, by highlighting text or clicking on a component). Rather than forcing
    the user to copy/paste details into the conversation, the application should be
    aware of what the user is referring to and should incorporate the relevant information
    into the prompt as an artifact.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**当前交流**从用户的请求以及他们附加到对话中的任何工件开始。例如，在应用程序界面中，用户可能会表明他们正在谈论屏幕上的某物（例如，通过突出文本或点击组件）。而不是强迫用户将详细信息复制/粘贴到对话中，应用程序应该知道用户在指什么，并将相关信息作为工件纳入提示。'
- en: After the user message, in the remainder of the current exchange, the model
    will make tool calls when necessary and the application will incorporate both
    the call and the response into the prompt (as we described at the start of this
    chapter). In subsequent exchanges, data from the tool evaluations can be presented
    as artifacts attached to assistant messages. The current exchange is finished
    when the model returns a message from the assistant back to the user. This message
    does not become part of this prompt, but it will be included in the *prior conversation*
    at the time of the next exchange.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户消息之后，在当前交流的剩余部分，模型将在必要时进行工具调用，并将调用和响应都纳入提示（正如我们在本章开头所描述的）。在随后的交流中，工具评估的数据可以作为附加到助手消息的工件来展示。当前交流在模型将助手的消息返回给用户时结束。这条消息不会成为本提示的一部分，但将在下一次交流时被包含在**先前的对话**中。
- en: '[Table 8-1](#ch08_01_table_1_1728429579262866) demonstrates what the full context
    of a conversational agent would look like, including the preamble, the prior conversation,
    and the current exchange.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8-1](#ch08_01_table_1_1728429579262866)展示了对话代理的完整上下文看起来会是什么样子，包括前言、先前的对话和当前交流。'
- en: Table 8-1\. Anatomy of a conversational agent’s context
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-1\. 对话代理上下文的结构
- en: '| **Preamble**: text that conditions general agent behavior'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '| **前言**：影响一般代理行为的文本'
- en: Rules, instructions, and expectations
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则、指示和期望
- en: Relevant tool definitions
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关工具定义
- en: Few-shot examples if necessary
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如有必要，提供少量示例
- en: (Tool definitions are typically incorporated into the system message behind
    the model API.) |
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: （工具定义通常被整合到模型API背后的系统消息中。）|
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Prior conversation**: captures the context of the conversation to this
    point'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '| **先前的对话**：捕捉到目前为止对话的上下文'
- en: Previous user and agent messages, excluding the current exchange
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前的用户和代理消息，不包括当前交流
- en: 'Artifacts: pieces of data attached to user or agent messages'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工件：附加到用户或代理消息的数据
- en: '|'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Current exchange**: the current user request'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '| **当前交流**：当前用户请求'
- en: The most recent user message
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近用户的消息
- en: Any artifacts attached by the user
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户附加的任何工件
- en: Tool calls and responses generated while servicing the user’s request
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在服务用户请求时生成的工具调用和响应
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Agent response**: summarizes this exchange; will be part of the prior conversation
    in the next exchange |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| **代理响应**：总结这次交流；将成为下一次交流中的先前的对话部分'
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Selecting and Organizing Context
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择和组织上下文
- en: In the preceding discussion, we presented a variety of contexts that might be
    included in a conversational LLM application. In this section, we’ll look at several
    techniques and ideas for assembling this context into a prompt. There is no one-size-fits-all
    approach; the effectiveness of a particular prompt engineering approach is dependent
    upon the domain, model, data, and many other factors. The key is to constantly
    try new ideas and then evaluate, evaluate, evaluate (more on that in [Chapter 10](ch10.html#ch10_evaluating_llm_applications_1728407085475721)).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的讨论中，我们展示了可能包含在对话型LLM应用中的各种上下文。在本节中，我们将探讨将此上下文组装成提示的几种技术和想法。没有一种适合所有情况的解决方案；特定提示工程方法的有效性取决于领域、模型、数据以及许多其他因素。关键是不断尝试新想法，然后评估、评估、评估（更多内容请见[第10章](ch10.html#ch10_evaluating_llm_applications_1728407085475721))。
- en: 'Here’s a list of the things you might consider when selecting and organizing
    context for your prompt:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择和组织提示上下文时，你可能需要考虑以下事项：
- en: What tools do you need? During parts of the conversation, you might know that
    the agent has no use for particular tools. Drop them from consideration and your
    agent will have one less distraction when using other tools.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要哪些工具？在对话的部分时间里，你可能会知道代理不需要某些工具。将它们从考虑中排除，你的代理在使用其他工具时将少一个干扰。
- en: 'What artifacts should you present? Your options are as follows:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该展示哪些文物？你的选择如下：
- en: Include all of them. While you can be sure the model will have the best information
    available, irrelevant content and lots of it are sure to confuse the model.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含所有内容。虽然你可以确信模型将拥有最佳信息，但大量无关内容很可能会让模型困惑。
- en: Ask the model to select which artifacts it thinks are relevant. This requires
    substantial additional complexity in the application because you must set up the
    side request to have the model choose which artifacts it thinks are important.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求模型选择它认为相关的文物。这需要在应用中设置大量的额外复杂性，因为你必须设置一个侧请求，让模型选择它认为重要的文物。
- en: 'How should artifacts be presented? Your options are as follows:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文物应该如何展示？你的选择如下：
- en: Add artifact data directly to user and assistant content by sticking it in an
    XML tag, like the `<artifact>` tag in [Table 8-1](#ch08_01_table_1_1728429579262866),
    or in a markdown section, like `## Attached Data`.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将数据直接粘贴到XML标签中，如[表8-1](#ch08_01_table_1_1728429579262866)中的`<artifact>`标签，或将数据添加到Markdown部分，如`##
    附带数据`，将文物数据直接添加到用户和助手内容中。
- en: The format of the artifact can be JSON, plain text, or anything else. Anecdotally,
    it doesn’t seem to matter much (but test this for yourself).
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文物的格式可以是JSON、纯文本或其他任何格式。据传闻，这似乎并不重要（但你可以亲自测试一下）。
- en: Alternatively, if all of your artifacts come from function calls, then don’t
    treat artifacts in a special way at all. Just preserve the function calls from
    the current exchange into the prior conversation. The benefit is that this provides
    more examples of tool invocation that can help the model make better use of tools
    during the current exchange.
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，如果你的所有文物都来自函数调用，那么根本不需要特别对待文物。只需将当前交换中的函数调用保留到之前的对话中。好处是这提供了更多工具调用的例子，可以帮助模型在当前交换中更好地使用工具。
- en: 'How much content do you include in each artifact? If the user refers to a book,
    then certainly, you wouldn’t include the full text in the prompt. You wouldn’t
    be able to include the full content in the prompt, and even if you could fit it
    in, it would confuse the model. So, drawing on the “elastic snippet” conversation
    in [Chapter 6](ch06.html#ch06a_assembling_the_prompt_1728442733857948), you need
    to find a way to extract information from artifacts and present only the most
    relevant data for the task at hand. Here are some possible ways to do that:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个文物中应包含多少内容？如果用户提到一本书，那么当然，你不会在提示中包含全文。你不可能在提示中包含全部内容，即使你能放进去，也会让模型困惑。因此，借鉴“弹性片段”对话中的[第6章](ch06.html#ch06a_assembling_the_prompt_1728442733857948)，你需要找到一种方法从文物中提取信息，并仅展示与当前任务最相关的数据。以下是一些可能的实现方式：
- en: 'One clever idea (though one that we haven’t yet tried) is to present the artifact
    as a bulleted summary and then also include this text for each bullet: `` for
    more information, call `details(''section 5'')` `` where details is a tool used
    to retrieve more details about the referenced argument. Then, if the application
    calls `details(''section 5'')`, you can unfurl that portion of the artifact, possibly
    revealing more subsections that can be unfurled.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个巧妙的想法（尽管我们还没有尝试过）是将文物以项目符号摘要的形式呈现，然后为每个项目符号也包含以下文本：“``了解更多信息，请拨打`details('section
    5')` ``”，其中details是一个用于检索有关引用论点的更多详细信息的工具。然后，如果应用程序调用`details('section 5')`，你可以展开文物的该部分，可能揭示更多可以展开的子部分。
- en: Alternatively, just provide a retrieval for searching through the large artifact
    (a.k.a. a traditional RAG).
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，只需提供一个检索，以便在大型文物中搜索（即传统的RAG）。
- en: How far back should the prior conversation go? If the conversation has shifted
    to a new topic, then you can drop it. How do you know if the conversation has
    moved on? That’s a good question. One option is to automatically drop all content
    from prior user sessions (e.g., after the user has been inactive for some predetermined
    amount of time). Alternatively, you can ask a model to decide what content is
    relevant. This is probably overkill for a large model (too expensive and high
    latency), but you can train a smaller model to do this.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该回溯到先前的对话有多远？如果对话已经转移到新的主题，那么你可以放弃它。如何知道对话是否已经转移？这是一个好问题。一个选项是自动删除先前用户会话中的所有内容（例如，在用户不活跃一段时间后）。或者，你可以要求模型决定哪些内容是相关的。这可能对于大型模型来说可能有些过度（成本太高且延迟高），但你可以训练一个较小的模型来完成这项工作。
- en: We wish that we could be more prescriptive with our advice here. It’s tricky.
    If you include too much information, then you’ll confuse the model, run out of
    space in the prompt, and drive up latency and cost. If you include too little,
    then the model will not have the information it needs to address the task at hand.
    But LLM technology is moving quickly. Models are getting smarter and faster, and
    their prompt capacity is increasing. Perhaps the questions in this section will
    get easier in the future, when we’ll be able to just say, “When in doubt, add
    it to the prompt and let the model figure it out!” Until then—evaluate, evaluate,
    evaluate!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们在这里能给出更具体的建议。这很棘手。如果你包含太多信息，那么你会使模型困惑，在提示中耗尽空间，并增加延迟和成本。如果你包含太少，那么模型将没有处理当前任务所需的信息。但LLM技术正在快速发展。模型变得更聪明、更快，它们的提示容量在增加。也许在将来，当我们可以简单地说“当不确定时，将其添加到提示中，让模型解决它！”的时候，这个问题会变得更容易。在此之前——评估，评估，再评估！
- en: Building a Conversational Agent
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建对话代理
- en: Now, it’s time for you to assimilate all that we’ve discussed in this chapter
    and build your own conversational agent. By the end of the tool usage discussion
    at the beginning of the chapter, we were actually quite close. Turn back and look
    at [Example 8-1](#ex-8-1). There, we defined `process_messages`, which takes all
    of the messages in a conversation, optionally calls one or more tools, and finally
    provides a response in the voice of the assistant to answer the user and summarize
    any behind-the-scenes tool-calling activity. The only two things remaining are
    (1) providing a way to allow your user to interact with the agent (here, we’re
    just using a Python input statement) and (2) tossing a loop around the `process_messages`
    function so that you can facilitate a full back-and-forth conversation between
    the user and the assistant.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候将本章中讨论的所有内容吸收并构建你自己的对话代理了。到本章开头工具使用讨论结束时，我们实际上已经很接近了。回顾一下[示例8-1](#ex-8-1)。在那里，我们定义了`process_messages`，它接受对话中的所有消息，可选地调用一个或多个工具，并最终以助手的语气提供响应，回答用户并总结任何幕后工具调用活动。剩下的事情只有两件：（1）提供一个让用户与代理交互的方式（在这里，我们只是使用Python输入语句），以及（2）围绕`process_messages`函数添加循环，以便你可以促进用户和助手之间的完整双向对话。
- en: Managing Conversations
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理对话
- en: Referring to [Example 8-2](#ch08_01_example_2_1728429579274312), the `process_messages`
    function takes a set of messages and then appends new messages corresponding to
    tool invocation and evaluations. It might do this several times. Finally, `process_messages`
    appends a response from the assistant, which incorporates any information discovered
    from tool usage. The `run_conversation` function wraps the `process_messages`
    function. It initializes the messages list, iteratively requests user input, appends
    the user message, and sends the messages to the `process_messages` function. The
    `run_conversation` function also prints out user and assistant messages, giving
    us a reasonable text-only user experience. The result is a natural flowing conversation
    that can make use of tools if necessary.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 参考 [示例8-2](#ch08_01_example_2_1728429579274312)，`process_messages` 函数接收一组消息，然后添加与工具调用和评估相对应的新消息。它可能这样做几次。最后，`process_messages`
    添加来自助手的响应，该响应包含从工具使用中发现的任何信息。`run_conversation` 函数封装了 `process_messages` 函数。它初始化消息列表，迭代请求用户输入，添加用户消息，并将消息发送到
    `process_messages` 函数。`run_conversation` 函数还打印出用户和助手的消息，为我们提供了一个合理的纯文本用户体验。结果是自然流畅的对话，如果需要，可以利用工具。
- en: Example 8-2\. The `run_conversation` function manages the full conversation
    state, including user input and agent output
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-2\. `run_conversation` 函数管理完整的对话状态，包括用户输入和代理输出
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To make this just a bit clearer, [Figure 8-2](#ch08_01_figure_2_1728429579251516)
    presents a sequence diagram illustrating the design of the conversational agent.
    Here, we’ve generalized the agent as a sequence of interactions between the user,
    the application, the model, and the tool. The sequence diagram details the information
    transferred between actors and highlights the actions taken by the application.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这更加清晰，[图8-2](#ch08_01_figure_2_1728429579251516) 展示了一个序列图，说明了对话代理的设计。在这里，我们将代理概括为用户、应用程序、模型和工具之间的一系列交互。序列图详细说明了参与者之间的信息传递，并突出了应用程序采取的动作。
- en: At the beginning of the chapter, in the tools discussion, we ran `process_messages`
    several times in a row. This gave us a glimpse of the conversational agent in
    action. However, it’s not truly a conversational agent until it’s running inside
    the `run_conversation` loop defined in [Example 8-2](#ch08_01_example_2_1728429579274312).
    In [Table 8-2](#ch08_01_table_2_1728429579262901), we demonstrate how the conversational
    agent interacts with a user when `run_conversation` is executed.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，在工具讨论中，我们连续多次运行了 `process_messages`。这让我们对对话代理的实际操作有了一个初步的了解。然而，直到它运行在
    [示例8-2](#ch08_01_example_2_1728429579274312) 中定义的 `run_conversation` 循环内，它才真正成为一个对话代理。在
    [表8-2](#ch08_01_table_2_1728429579262901) 中，我们展示了当执行 `run_conversation` 时，对话代理如何与用户交互。
- en: '![A diagram of a software application  Description automatically generated
    with medium confidence](assets/pefl_0802.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![软件应用程序的图表 描述自动生成，置信度中等](assets/pefl_0802.png)'
- en: Figure 8-2\. A sequence diagram representing the design of the conversational
    agent
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 表示对话代理设计的序列图
- en: Table 8-2\. Executing `run_conversation`
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-2\. 执行 `run_conversation`
- en: '| Role | Dialogue/[tools] | Commentary |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 角色 | 对话/[工具] | 评论 |'
- en: '| --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| User | `Golly gee, it''s hot in here. Let''s cool this place down.` | LLMs
    agents can interpret “colorful” language. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | `哎呀，这里真热。让我们把这地方冷却下来。` | LLMs代理可以解释“丰富多彩”的语言。|'
- en: '| Agent |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 代理 |'
- en: '[PRE23]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`The current room temperature is 64ºF, which is actually quite cool. If you
    find it too warm, it might be due to other factors. Would you still like to adjust
    the temperature?` | The agent checks the temperature and then *uses common sense*
    to tell the user that their statement contradicts normal expectations. The agent
    still offers to comply with the request. |'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`当前室温为64ºF，实际上已经很凉快了。如果你觉得太热，可能是因为其他因素。你还想调整温度吗？` | 代理检查了温度，然后 *运用常识* 告诉用户他们的陈述与正常预期相矛盾。代理仍然提出遵守请求。|'
- en: '| User | `Make it LOTS cooler.` | The user proceeds. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | `让它变得更冷一些。` | 用户继续。 |'
- en: '| Agent |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 代理 |'
- en: '[PRE24]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`I''ve set the room temperature to 50ºF. It should feel much cooler soon!`
    | The agent complies. (Hey, the guy said, “LOTS!”) |'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`我已经将室温设置为50ºF。很快就会感觉凉快多了！` | 代理遵守。 (嘿，那个人说，“很多！”) |'
- en: '| User | `Whoa...that''s a bit extreme, isn''t it? Put it back to where it
    started.` | The user realizes that the result is unexpected. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | `哇...这有点极端，不是吗？把它放回原来的位置。` | 用户意识到结果并不如预期。|'
- en: '| Agent |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 代理 |'
- en: '[PRE25]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`I''ve reset the room temperature to 64ºF. It should start heating up again
    shortly.` | The agent, *equipped with the prior conversation*, correctly sets
    the temperature back to the starting point. |'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`我已经将房间温度重置为64°F。它应该很快就会再次开始加热。` | 配备了先前对话的代理，正确地将温度恢复到起始点。|'
- en: There are several things to note here. For one, it’s still hard to not be in
    awe of how flexible these models are. The user’s opening comment isn’t formal
    at all—it’s even kinda weird—but the model correctly interprets the intent. Also
    impressive is how you get common sense reasoning for free. We see this in the
    agent’s remark about 64ºF being “actually quite cool”—you have to know a lot about
    humans to get that right. We also see this later—and take it for granted—when
    the model makes the temperature “LOTS cooler” by setting it to 50ºF instead of
    0ºF or―1,000ºF. And we see this when the agent talks about how the temperature
    will change soon rather than immediately—clearly, the agent understands thermostats
    at some level.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个需要注意的地方。首先，仍然很难不对这些模型的灵活性感到敬畏。用户的开场评论一点也不正式——甚至有点奇怪——但模型正确地解读了意图。同样令人印象深刻的是，你可以免费获得常识推理。我们在代理关于64°F“实际上相当凉爽”的评论中看到了这一点——你必须对人类有相当多的了解才能做到这一点。我们还在后来——并理所当然地接受——当模型将温度“降低很多”设置为50°F而不是0°F或-1,000°F时看到了这一点。当代理谈论温度将很快而不是立即变化时，我们也能看到这一点——显然，代理在某种程度上理解了恒温器。
- en: The most important new behavior for the agent is seen in the last exchange,
    when it correctly converts the temperature back to its starting point of 64ºF.
    It can accomplish this step because we are now correctly tracking not only the
    current exchange but the prior conversation as well. This allows the agent to
    refer to the start of the conversation, where it first learned that the temperature
    was 64ºF.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 代理最重要的新行为体现在最后的交流中，当时它正确地将温度转换回64°F的起始点。它能完成这一步是因为我们现在不仅正确跟踪当前的交流，还跟踪先前的对话。这使得代理能够引用对话的开始，在那里它第一次了解到温度是64°F。
- en: With `run_conversation` (see [Table 8-2](#ch08_01_table_2_1728429579262901))
    wrapping `process_messages` (see [Example 8-1](#ex-8-1)), we have a simple but
    complete conversational agent. All of the code is generic, and you can modify
    the system message and the tools to easily create whatever behavior you please.
    As the agent becomes more complex, you might need to spend time thinking about
    how to deal with other concerns we’ve discussed in this chapter—such as providing
    the agent with the appropriate tools for a request, retrieving earlier conversations,
    and incorporating information in the form of artifacts. And naturally, you’ll
    probably want more than a text-based tool, so you’ll have to place the agent behind
    an API, handle errors, and add logging. But, at this point, the sky’s the limit.
    What will you make first?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`run_conversation`（见[表8-2](#ch08_01_table_2_1728429579262901)）包装`process_messages`（见[示例8-1](#ex-8-1)），我们得到了一个简单但完整的对话代理。所有代码都是通用的，你可以修改系统消息和工具，轻松地创建你想要的任何行为。随着代理变得更加复杂，你可能需要花时间思考如何处理本章中讨论的其他关注点——例如，为代理提供适当的工具来处理请求，检索之前的对话，以及以工件的形式整合信息。当然，你可能还希望拥有一个不仅仅是基于文本的工具，因此你将不得不将代理放在API后面，处理错误，并添加日志记录。但，到目前为止，一切皆有可能。你首先会做什么？
- en: User Experience
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户体验
- en: In the preceding examples, we’ve been looking at blobs of text. But your users
    will likely be engaging with the agent through a much richer visual interface.
    In this section, we talk about some of the basic affordances that you should consider
    when implementing the UI.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们一直在查看文本块。但你的用户很可能会通过一个更加丰富的视觉界面与代理互动。在本节中，我们将讨论在实现用户界面时应考虑的一些基本功能。
- en: The chat UI is ubiquitous—from AOL Instant Messenger, released in the 1990s,
    up until Slack, it’s been the same. It’s people taking turns typing in little
    rectangles on the screen. This format is the same for ChatGPT, and it will be
    the same for your application as well. One simple affordance not to forget is
    a spinner that indicates the agent is processing and will return soon with a new
    interaction. In [Figure 8-3](#fig-8-3), we see that the user, Dave, has asked
    the assistant, HAL, a question, and the spinner (labeled as item 1) indicates
    that HAL is taking time to process the next response.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天用户界面无处不在——从20世纪90年代发布的AOL即时消息，到现在的Slack，它一直如此。这是人们在屏幕上的小矩形中轮流输入。这种格式对ChatGPT也是一样的，它也将适用于您的应用。不要忘记的一个简单功能是一个旋转器，它表明代理正在处理并将很快返回新的交互。在[图8-3](#fig-8-3)中，我们看到用户Dave向助手HAL提出了一个问题，旋转器（标记为项目1）表明HAL正在花费时间处理下一个响应。
- en: One thing that’s new and special for most conversational agents is the use of
    tools. Your UI should indicate when the agent is using tools, for instance, with
    a pill button inside of the agent message (item 2). This lets the user know that
    the agent is engaging in background work before returning with a final response.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数对话代理来说，新且特别的一点是使用工具。您的用户界面应该表明代理何时使用工具，例如，在代理消息中的药丸按钮（项目2）。这使用户知道代理在进行背景工作，在返回最终响应之前。
- en: For more complex chat applications, you should allow the user to have visibility
    into the processing that is taking place. In [Figure 8-3](#fig-8-3), Dave becomes
    puzzled about the unexpected response from HAL, so Dave clicks the “Tool calls”
    button (item 3). Once clicked, the button reveals full details about the tool
    calls. This includes the name of the tool, the arguments presented as a webform,
    and the results that the agent will be working with. Dave can inspect this form
    and understand the rationale behind HAL’s response.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的聊天应用，你应该允许用户了解正在进行的处理过程。在[图8-3](#fig-8-3)中，Dave对HAL的意外响应感到困惑，因此Dave点击了“工具调用”按钮（项目3）。一旦点击，按钮就会显示关于工具调用的全部详细信息。这包括工具的名称、作为网页表单呈现的参数，以及代理将与之工作的结果。Dave可以检查这个表单并理解HAL响应背后的逻辑。
- en: Even though LLMs are increasing in intelligence, they still need a fair amount
    of course correction from users. Let your users interact with the agent’s tool
    calls. Allow users to modify arguments from the webform (item 4) and then resubmit
    the corrected request. Once the user resubmits the tool request, the conversation
    can be regenerated from that point onward (item 5), hopefully leading to a more
    desirable outcome. As you can see, Dave uses this change of tool arguments to
    change the course of the conversation. Silly HAL.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）的智能在不断提高，但它们仍然需要用户进行相当程度的课程纠正。让您的用户与代理的工具调用进行交互。允许用户修改网页表单（项目4）中的参数，然后重新提交修正后的请求。一旦用户重新提交工具请求，对话就可以从那个点开始重新生成（项目5），希望得到更满意的结果。正如您所看到的，Dave使用这种工具参数的变化来改变对话的方向。愚蠢的HAL。
- en: '![](assets/pefl_0803.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pefl_0803.png)'
- en: Figure 8-3\. Interacting with a tool-equipped conversational agent
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3\. 与配备工具的对话代理交互
- en: As mentioned earlier, once you introduce tool calls that modify real-world assets,
    you introduce a new level of risk into your application. Therefore, you should
    always allow your user to authorize any request that has a remote chance of being
    dangerous (see [Figure 8-4](#fig-8-4)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一旦你引入了修改现实世界资产的工具调用，你就在你的应用中引入了新的风险级别。因此，你应该始终允许你的用户授权任何有远程危险可能性的请求（参见[图8-4](#fig-8-4)）。
- en: Note that if a tool call modifies real-world assets, then you should make sure
    to allow the user to authorize the request before executing it.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果工具调用修改了现实世界的资产，那么你应该确保在执行之前允许用户授权请求。
- en: Finally, though it’s not pictured here, lots of chat experiences implicitly
    attach artifacts to the conversation (for instance, if the user is looking at
    a document on their screen, then the application might include its text in the
    prompt). To help your users understand what the agent is thinking about, give
    them some way to see into the agent’s “mind” and see the same artifacts that the
    agent is looking at. If the user has an understanding of where the agent’s attention
    is focused, then they will be able to ask more pointed questions and resolve problems
    more quickly. Similarly, if the agent is looking at the wrong thing, then giving
    the user the ability to dismiss an artifact might help keep the conversation on
    track.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管这里没有展示图片，但许多聊天体验会隐式地将物品附加到对话中（例如，如果用户正在屏幕上查看文档，那么应用程序可能会将其文本包含在提示中）。为了帮助用户理解代理正在思考的内容，给他们一种方式可以看到代理的“思维”并看到代理正在查看的相同物品。如果用户了解代理的注意力集中在何处，那么他们就能提出更有针对性的问题，并更快地解决问题。同样，如果代理正在查看错误的事物，那么给予用户取消物品的能力可能有助于使对话保持正轨。
- en: '![](assets/pefl_0804.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pefl_0804.png)'
- en: Figure 8-4\. A possible UI implementation of an authorization request
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4\. 授权请求的可能UI实现
- en: Conclusion
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'You’ve come a long way in this chapter. You’ve learned that agency is the ability
    of an entity to accomplish tasks in a self-directed manner. You’ve also learned
    that *conversational* agency is a form of assisted agency in which a human and
    an assistant work together to accomplish tasks through back-and-forth dialogue.
    In this chapter, we talked about core aspects of conversational agency: using
    tools to gather information and make changes to assets in the real world, improved
    reasoning about the task at hand, and the requirements for collecting and organizing
    context information relevant to the task. In the last section, we built the complete
    conversational agent and discussed UX concerns.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经取得了很大的进步。你了解到代理是实体以自我指导的方式完成任务的能力。你还了解到*对话*代理是一种辅助代理的形式，其中人类和助手通过来回对话共同完成任务。在本章中，我们讨论了对话代理的核心方面：使用工具收集信息并对现实世界中的资产进行更改，提高对当前任务的推理能力，以及收集和组织与任务相关的上下文信息的要求。在最后一节中，我们构建了完整的对话代理并讨论了用户体验问题。
- en: Conversational agents have their limits, though—they often need the corrective
    influence of a human to keep them on track and pushing toward the goal. In the
    next chapter, we’ll show you how to use LLM-based workflows to accomplish goals.
    Rather than making you rely on humans to keep the agent on track, we’ll show you
    how to break down complex problems into tasks that can be executed in a directed
    workflow. Each task is simple and does not require human intervention, but the
    workflow as a whole will be able to accomplish tasks that haven’t been technologically
    feasible until now.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对话代理有其局限性——它们通常需要人类的纠正影响来保持其方向并推动目标。在下一章中，我们将向您展示如何使用基于LLM的工作流程来实现目标。而不是让您依赖人类来保持代理的方向，我们将向您展示如何将复杂问题分解为可以在定向工作流程中执行的任务。每个任务都很简单，不需要人工干预，但整个工作流程将能够完成以前技术上不可行的任务。
