- en: Chapter 8\. Conversational Agency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#ch03a_moving_toward_chat_1728432131625250), we covered
    the departure from text completion models to chat models. A chat model by itself
    is aware of only the information covered in training and whatever information
    the user has just told it. The chat model is unable to reach out into the world
    and learn about information that was unavailable during training, and it’s unable
    to interact with the world and take external actions on behalf of the user.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM community is making great headway in overcoming these limitations through
    conversational agency. *Agency* is the ability of an entity to complete tasks
    and achieve goals in a self-directed and autonomous manner. The conversational
    agents that we discuss in this chapter provide an experience similar to chat—a
    back-and-forth dialogue between a user and an assistant—but add in the ability
    for the assistant to reach out to the real world, learn new information, and interact
    with real-world assets.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll introduce several state-of-the-art approaches to building
    an LLM-based conversational agent. We’ll explore how models can use tools to reach
    out into the external world, how they can be conditioned to better reason through
    their problem space, and how we can gather the best context to facilitate long
    or complex interactions. By the end of this chapter, you’ll be able to build your
    own conversational agent that’s capable of going out into the world and performing
    guided tasks on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: Tool Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working in isolation, language models are limited in what they can accomplish.
    Certainly, a chat assistant is fascinating to talk to because, in some ways, it’s
    the digital zeitgeist of the world. You can learn anything you want from a broad
    range of topics, and the model can draw on diverse schools of thought and help
    you brainstorm. The model is a fantastic tutor—if you don’t mind some hallucinations.
    But one thing it can’t do is access “hidden” knowledge—any bit of information
    that was unavailable to the model during training.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re at work, you regularly make use of private information in the form
    of corporate documentation, internal memos, chat messages, and code—information
    that the model has no access to. You also work in the present, not the past, and
    therefore, older information may be less relevant or even incorrect. If the model
    isn’t aware of the most recent API changes for the library you’re using or of
    recent news events, then the completions will be misleading and incorrect. At
    an extreme, you may even require up-to-the-moment information. For instance, if
    you’re planning travel arrangements, you need to know what flights are available
    *now*. A bare chat model has access to none of this.
  prefs: []
  type: TYPE_NORMAL
- en: Besides missing important information, language models just aren’t good at certain
    tasks—most prominently, math. If you ask ChatGPT to evaluate any simple arithmetic
    problem, then it will often get the correct answer because it has effectively
    memorized all the simple problems. But as the numbers get larger or the computation
    becomes more complicated, the model will make poorer and poorer estimates. What’s
    even worse, these mistakes are often presented confidently as truth.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, by themselves, chat models don’t *do* anything at all—they just talk!
    The only way they can make a change in the real world is by asking the user to
    do something for them. Language models can’t buy plane tickets, send emails, or
    change the temperature on the thermostat.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address all of these issues, the LLM community is turning to tool usage
    to give language models access to up-to-date information, help them perform nonlanguage
    tasks, and help them interact with the world around them. The idea is simple:
    tell the model about tools it has access to and when and how to use them, and
    the model will then use the tools to execute external APIs. It’s the job of the
    application to parse the tool invocation from the model completion, relay the
    request to a real-world API, and then incorporate that information into future
    prompts sent to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs Trained for Tool Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In June of 2023, OpenAI introduced a new model that was fine-tuned for tool
    invocation, and several other competing LLMs have since followed suit. Let’s take
    a look at OpenAI’s take on tools.
  prefs: []
  type: TYPE_NORMAL
- en: Defining and using tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we set up the actual functions that reach out into the real world, gather
    information, and make changes to the environment. The implementation is mocked
    out, but if you’re so inclined, it would not be difficult to find a Python library
    that allows you to interact with a real thermostat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we represent both of these functions as [JSON schema](https://oreil.ly/rZsdN)
    so that OpenAI can represent them in the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The JSON schema declares both functions, including their arguments. The functions
    and arguments also have description text that tells the model how the functions
    and arguments are intended to be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a look-up dictionary so that our tools can be retrieved by
    name when necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With all that in place, we are ready to make the actual message handling functionality.
    The `process_messages` function in [Example 8-1](#ex-8-1) is similar to what you’ll
    find in the OpenAI function calling documentation, but it’s improved in that this
    implementation allows for tools to be easily swapped—just modify the `tools` and
    `available_functions` definitions described previously.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-1\. Algorithm for processing messages and invoking and evaluating
    tools
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `process_messages` function in the example takes a list of messages and
    passes them to the model (in Step 1). The model will always return a response
    in the voice of the assistant, and this message is added to the list of messages
    passed in (in Step 2). It’s possible that the assistant message contains prose
    content for the user, tool invocation requests, or both. If tools are requested
    (as in Step 3), then for each tool invocation request, we extract the function
    name and arguments, call the actual function (in Step 4), and then add the function
    output to a new message appended to the end of the list of messages (in Step 5).
    After the function completes, the provided messages have been extended by the
    new messages derived from the model input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at how `process_messages` works when provided with a user
    request to modify the temperature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When this code is run, we can examine the messages and see that two new messages
    have been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the first message, which comes from the model, is a call to the
    `get_room_temp` tool. The subsequent message, provided by the application, injects
    the room temperature (74ºF) that was retrieved from calling the actual `get_room_temp`
    function. (Notice that there can be more than one tool call at a time. The IDs
    are required to make sure the correct tool response is associated with its corresponding
    tool request.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re not done yet. The application knows the current room temperature, but
    it still has to set the new temperature. Notice that `process_messages` has appended
    both of the new messages to the messages array, so we can progress one more turn
    in the conversation simply by calling `process_messages` once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to the following new messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Appropriately, the model calls `set_room_temp` with the arguments `{"temp":76}`,
    which is 2 degrees warmer than the current room temperature—this is just what
    the user wanted!
  prefs: []
  type: TYPE_NORMAL
- en: 'But it’s rude not to let the user know what just happened, so we make one more
    request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates a single new message—a response in the voice of the assistant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we don’t quite have conversation agency because we are manually
    calling `process_messages`. But I expect you can see that we’re basically one
    while loop away from full autonomy. Don’t worry, we’ll wrap it all up by the end
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look under the hood
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tool calling feels fundamentally different from document completion. How does
    the model accomplish this? It must surely be something special and different from
    plain-old document completion, right? *Wrong!* Remember how chat seemed special
    and different? In [Chapter 3](ch03.html#ch03a_moving_toward_chat_1728432131625250),
    we showed that under the hood, the OpenAI chat API converts system, user, and
    assistant messages to ChatML-formatted transcripts, and then, the model simply
    completes those documents. In just the same way that chat is a fine-tuned model
    plus syntactic sugar at the API level, tool calling is *also* a fine-tuned model
    plus syntactic sugar at the API level. Let’s look under the hood!
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s look at how tools are represented in the internal prompt. It’s
    important to understand what tools look like in the prompt because this informs
    how you should describe the tools and interact with them at the API level. Also,
    we need to account for the size of the tools’ representation in the prompt because
    it counts against your token budget. Unfortunately, OpenAI provides no documentation
    for the internal representation, so what follows is our best attempt to reconstruct
    the internal prompt format based on our interrogations of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the `set_room_temp` function defined earlier in this section.
    In the internal prompt, it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: First, notice that the tool definitions are placed in the system message just
    after the message that you provide. Function definitions are just part of the
    document, formatted, again, as ChatML.
  prefs: []
  type: TYPE_NORMAL
- en: Next, see how the prompt makes use of markdown to organize and format the response?
    This is a good example of the Little Red Riding Hood principle—markdown is a motif
    that occurs often in training data, and the model readily understands the structure
    it implies. (This is also a hint that *you* should use markdown when organizing
    your own prompts.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The final thing to notice here is that the snippet represents tools as if they
    were TypeScript functions. This is clever for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: TypeScript allows for a much richer vocabulary for type definitions. This helps
    ensure that the model will format the arguments using the correct types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s easy to incorporate the documentation into the function definition. Notice
    that not only is the function documented, but the individual arguments are documented
    as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way in which the function is defined *requires* the function to be invoked
    with a JSON object that lists out the argument names. This ensures that functions
    are called very consistently—which makes them easier to parse. Also, because of
    the requirement to specify each argument by name, as opposed to possibly using
    positional arguments, the model is much more “thoughtful” about the function call
    and much less likely to make mistakes. The model literally says temp right before
    it specifies the value, making it difficult to accidentally specify the wrong
    value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, since we know how the tool definitions are represented, let’s take a look
    at their invocation and evaluation. This is what it looks like internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the assistant uses special syntax to invoke functions—using the `name`
    field of the OpenAI message to specify the function name and the `content` field
    to specify the arguments as a JSON object. Let’s dwell upon this for a moment.
    Remember from [Chapter 2](ch02.html#ch02_understanding_llms_1728407258904677)
    that, at its very core, the model is just predicting the next token? Well, this
    is used to great effect here, because just about every single token in the tool
    invocation serves a purpose in narrowing down the tool invocation problem. Just
    look at this single message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at each step of the completion and notice how at every point, the
    model is effectively acting as a classification algorithm, deciding what should
    happen next:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Who should speak?* The OpenAI API, rather than the model, inserts `<|im_start|>assistant`
    at the beginning of the completion text. This conditions the model to generate
    the subsequent text in the voice of the assistant. The API forces this text into
    the prompt. If it had not, then it’s plausible that the model could have generated
    another message from the user. Forcing the speaker is safer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Should a tool be called?* The next tokens, `to=functions.`, are generated
    by the model. They indicate that a tool is to be called. But the model could have
    also generated `\n`, conditioning the model to generate a message from the assistant.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Which tool should be called?* The next tokens the model generates represent
    the name of the function: in this case, `set_room_temp\n`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Which argument should be specified?* The next text generated from the model
    infers the argument that should be specified. In this case, there is only one
    option `{"temp":`, but in more complicated tools with multiple, possibly nonrequired
    arguments, the model can use this opportunity to select from several options.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What value will the argument have?* The model next predicts the value that
    the current argument is going to take: in this case, 77\. If there are multiple
    arguments, then the model loops through steps 4 and 5 several times.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Are we done?* Once all the arguments have been specified, the model predicts
    that it’s time to wrap up. It predicts `}<|im_end|>`, which closes the JSON and
    the assistant message.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How awesomely flexible these models are! In the span of 10 to 20 tokens, the
    same, generic underlying neural network has effectively implemented 5 different,
    highly specialized inference algorithms. (Recall that step 1 was specified at
    the API rather than being inferred.) Wow...just wow. Also, see that at each step,
    the problem is broken down hierarchically. Do we need a tool? Which tool? Which
    arguments are required? What are the values for those arguments?
  prefs: []
  type: TYPE_NORMAL
- en: 'After tool invocation comes an evaluation message. Here, OpenAI has introduced
    a new `tool` role for the purpose of incorporating evaluation data back into the
    prompt. The output of the `set_room_temp` function is just `DONE` (indicating
    success), so the response message looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: The ID of the tool call and response that was present at the API level
    is no longer required because the API used the IDs to assemble the corresponding
    tool calls and responses together in the correct order.'
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines for Tool Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides general guidelines for you to follow when you’re designing
    and describing tools associated with conversational agents. Primarily, these guidelines
    rely upon two bits of intuition:'
  prefs: []
  type: TYPE_NORMAL
- en: Whatever is easier for a human to understand is also easier for an LLM to understand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The best results are derived by patterning prompts after training data (a.k.a.
    the Little Red Riding Hood principle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting the right tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Limit the number of tools the model has access to at once. The more tools available
    to the model, the greater the chance that the model will get confused. To the
    extent possible, the tools should partition the domain activity—that is, they
    should cover as much of the domain as possible but avoid tools that perform similar
    actions. Simpler tools are better. *Do not* copy your web API into the prompt!
    Web APIs often have tons of parameters and complex responses. Describing the API
    will take up tons of space, and the model will be less successful at invoking
    such a complex tool.
  prefs: []
  type: TYPE_NORMAL
- en: Naming tools and arguments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Names should be meaningful and self-documenting because, like a human reading
    an API specification, the model will read the names and build some expectations
    about the purpose of the tools and arguments. For OpenAI, the tools are presented
    as TypeScript in the prompt; it’s a good idea to follow suit and use camel case
    naming conventions. In any case, avoid names that are lowercase concatenations
    of words (e.g., `retrieveemail`) because these are more difficult to parse.
  prefs: []
  type: TYPE_NORMAL
- en: Defining tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally, you should make the definitions as simple as possible while capturing
    enough details about the tool so that the model (or a human) would understand
    how to use it. If your definitions sound like legalese, then you may be introducing
    too many concepts for the model to process with its limited attention mechanism.
    Simplify it if you can, but if your tool legitimately requires a detailed explanation,
    then make sure the definition doesn’t leave any ambiguity that the model will
    trip over.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working with a public API that the model is familiar with, then lean
    into the model’s training by creating a simplified version of that API that retains
    the naming, concepts, and style of the original API. For instance, when working
    on GitHub Copilot, we found out that the OpenAI model that we were using was well
    aware of GitHub’s code search syntax. (How did we know this? We asked it. The
    model could basically recite our documentation back to us.) We found that it was
    less confusing for the model if we named the arguments as they were in documentation
    and also expected the format of the argument values to be the same as in documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with arguments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Keep the arguments few and simple if possible. Naturally, the OpenAI models
    do fine with all of the JSON schema types: string, number, integer, and boolean.
    You can additionally modify properties with `enum` and `default` to better condition
    the model’s usage of the arguments. However, as of the OpenAI 1106 models (released
    in November 2023), it appears that some JSON schema property modifiers—such as
    `minItems`, `uniqueItems`, `minimum`, `maximum`, `pattern`, and `format`—are not
    represented in the prompt. Similarly, if you have any nested parameters, their
    descriptions are not presented in the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: For OpenAI models especially, be cautious of long-form text input for arguments.
    Since the arguments get stuffed into JSON, the values must be newline and quotation
    mark escaped, and the more text there is, the more likely the model is to forget
    an escape. This problem is exacerbated for code that is full of newlines and quotation
    marks. Anthropic, it turns out, encodes their function calls using XML tags rather
    than JSON, so the arguments don’t have to be escaped. In principle, this should
    mean that Claude is more amenable to long-form arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, watch out for argument hallucination. For example, several tools that
    we’re building at GitHub have org and repo arguments, but if the values for these
    arguments have not been mentioned in the conversation, then the model is liable
    to assume placeholder values like `"my-org"` and `"my-repo"`. There’s no silver
    bullet to solve this, but you can try the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: When the desired value is known in the application, remove the arguments from
    the function definition so that the model has nothing to be confused about. Alternatively,
    you can provide a default—that way, if the model specifies the default value,
    then you can make appropriate accommodations in the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instruct the model to ask if it’s unsure about an argument—and then pray it
    does, because it often won’t. Don’t worry, though—models are quickly getting better
    at this type of thing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dealing with tool outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the tool definitions, make sure the model can anticipate what it will find
    in the output. The outputs can be free-form, natural-language text or a structured
    JSON object. The model should do fine with either. Do not include too much extra
    “just-in-case-it’s-helpful” content in the output because models can be distracted
    by spurious content.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with tool errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a tool makes an error, this information is valuable to the model because
    it can look at errors and make corrections. But don’t just spit out the text of
    your internal error message into a tool response—make sure it makes sense in the
    context of the *model’s* definition of the tool. If it’s a validation error, then
    tell the model what it did wrong so that it can try again. If it’s some other
    error that the model should be able to deal with, then make sure the error message
    contains helpful information.
  prefs: []
  type: TYPE_NORMAL
- en: Executing “dangerous” tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you’re allowing the model to execute tools that make changes in the real
    world, you must protect your users from unintended side effects. *Do not* allow
    the model to execute any tool that could negatively impact a user unless the user
    has *explicitly* signed off first. Naively, you might say to yourself, “No problem,
    in the tool description, I’ll just say, ‘Make sure to double-check with the user
    before you run this.’ and then, we’ll be fine.” *Not so!* Models are inherently
    undependable, and with a strategy like this, we *guarantee* that a small portion
    of the time, the model will do exactly the thing you told it not to do.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, don’t prevent the model from calling whatever tool it wants to call.
    That’s right—let it make the request to send all of Bill’s money to his ex-wife’s
    bank account. Just make sure that in the application layer, you intercept all
    such dangerous requests and *explicitly* get sign-off before the application calls
    the actual API and makes a boneheaded mistake.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs select tokens, one by one, to provide a statistically likely completion
    of the prompt (see [Chapter 2](ch02.html#ch02_understanding_llms_1728407258904677)).
    In doing so, LLMs, in a sense, demonstrate a sort of reasoning capability—but
    it’s a very superficial form of reasoning. The model’s only goal—enforced by layers
    of training—is to make text that just, well, *sounds* right. As covered in [Chapter 2](ch02.html#ch02_understanding_llms_1728407258904677),
    the model doesn’t have any sort of internal monologue—so there’s no mental review
    of a problem statement, no consideration of how it maps to known facts, and no
    comparison of several competing ideas. Rather, one by one, the model predicts
    tokens that just fit best after the text being processed.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s fix that! There are several tricks that you can use to make the model
    more thoughtful in its response, and all of them have to do with giving the model
    an internal monologue that allows it to reason more carefully through a problem
    before providing a final response.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of Thought
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the January 2022 paper titled [“Chain-of-Thought Prompting Elicits Reasoning
    in Large Language Models”](https://arxiv.org/abs/2201.11903), the authors demonstrated
    that few-shot examples can be used to condition a model to be more thoughtful—and
    therefore more accurate—in its responses. Normally, a model would answer a commonsense
    question like “Will *The Exorcist* stimulate the limbic system?” with a yes or
    no, followed by an explanation. That’s how humans speak and therefore how models
    have learned to respond. But since the model has no internal monologue, then the
    initial yes or no will be an intuitive guess and the explanation will actually
    be a rationalization to justify that guess.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the chain-of-thought paper demonstrated that if you could have
    the model reason about the question first and *then* give the answer, it was more
    likely to arrive at the correct answer. They achieved this by providing the model
    with few-shot examples to condition subsequent model responses toward thinking
    and then answering. Here are a couple of the few-shot examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Provided several such examples, the subsequent answer to the question about
    *The Exorcist* now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using the StrategyQA dataset and the PaLM 540B model, the paper indicated that
    this style of chain-of-thought reasoning increased accuracy when answering commonsense
    questions from the prior state-of-the-art rate of 69.4% to 75.6%.
  prefs: []
  type: TYPE_NORMAL
- en: But the domain of answering commonsense questions was not the only domain that
    benefitted. As a matter of fact, answers to math problems showed significant improvements.
    When applying the PaLM 540B model against a battery of math word problems from
    the GSM8K dataset, the authors demonstrated a solve rate increase from roughly
    20% with standard prompting to 60% with chain-of-thought reasoning. The chain-of-thought
    paper demonstrated similar benefits with several other datasets and other domains
    such as symbolic reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: In May of 2022, a subsequent paper titled [“Large Language Models are Zero-Shot
    Reasoners”](https://arxiv.org/abs/2205.11916) one-upped the chain-of-thought paper
    with a clever trick. Rather than curating sets of relevant few-shot examples to
    get the model into a pattern of thinking out loud, this paper showed that you
    can simply start the answer with the phrase, “Let’s think step-by-step,” and that
    cue would cause the model to generate chain-of-thought reasoning followed by a
    more accurate response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another paper from October 2023 titled [“Think Before you Speak: Training Language
    Models With Pause Tokens”](https://arxiv.org/abs/2310.02226) took chain of thought
    to a somewhat bizarre extreme. The authors fine-tuned a language model to use
    a “pause” token, and after asking a question, they would inject some number, say
    10, of these meaningless tokens into the prompt. The effect was that the model
    had additional timesteps to reason about the answer. The information from previous
    tokens got more thoroughly incorporated into the model state so that it produced
    a better answer. This is analogous to what humans do—we have our own “pause” tokens
    called “Uh,” and “Um,” and we use them when we are stalling for more time to think
    about what we’re going to say.'
  prefs: []
  type: TYPE_NORMAL
- en: The main point to understand in this section is the point we made at the beginning—language
    models have no internal monologue and therefore no way to think about something
    before blurting out an answer. If you can condition a model to spend some time
    thinking about the problem—be it through few-shot examples or simply by requesting
    it—then the model will be much more likely to generate a good completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'ReAct: Iterative Reasoning and Action'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The October 2022 paper titled [“ReAct: Synergizing Reasoning and Acting in
    Language Models”](https://arxiv.org/abs/2210.03629) took reasoning one level deeper
    by looking at situations that require information retrieval and multistep problem
    solving. Also, for a little extra fun, this paper was one of the first to make
    use of the external tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Of the domains investigated in the paper, the most interesting for our purposes
    is the HotpotQA, a dataset that contains questions like “Which magazine was started
    first, *Arthur’s Magazine* or *First for Women*?” As a human, think about how
    you would answer this question. You would probably look up both of these magazines,
    find the date they were first published, compare the dates, and then declare the
    answer. This is the type of multistep reasoning that the ReAct authors intended
    to demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of this paper introduced the notion of three different tools to
    aid the model in finding the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Search[entity]
  prefs: []
  type: TYPE_NORMAL
- en: This returns the first five sentences from the corresponding Wikipedia page
    if it exists or otherwise returns the top five most similar entities based on
    a Wikipedia search.
  prefs: []
  type: TYPE_NORMAL
- en: Lookup[string]
  prefs: []
  type: TYPE_NORMAL
- en: This searches the most recent entity (from Search) and returns the next sentence
    that contains the provided string.
  prefs: []
  type: TYPE_NORMAL
- en: Finish[answer]
  prefs: []
  type: TYPE_NORMAL
- en: This signals that the work is complete and indicates the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: The expectation is for the model to address the question by iteratively thinking
    about what needs to be done; acting by using the `Search` or `Lookup` tool to
    gather information; and observing the answers from the tools. After several think-act-observe
    loops, the model will have the information it needs and will end the session by
    selecting the `Finish` tool and declaring the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example (drawn from the paper) of how this would work for the preceding
    question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To condition the model to make use of `Search`, `Lookup`, and `Finish` tools,
    the ReAct authors injected the following preamble into the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is then followed by six examples of the think-act-observe pattern similar
    to the one shown. Finally, this is followed by the actual question. (The ReAct
    authors put together a short and really well organized [Jupyter notebook](https://oreil.ly/_N_K3)
    if you want to see exactly how this all works.)
  prefs: []
  type: TYPE_NORMAL
- en: So, how well does ReAct perform? Well, initially, the answer was poorly. As
    shown on the left side of [Figure 8-1](#ch08_01_figure_1_1728429579251484), on
    the HotpotQA dataset for every size of model, ReAct was actually *worse* than
    both “standard” prompting (just presenting the model with the question) and chain-of-thought
    prompting. This is because the in-prompt examples were not sufficient to teach
    the model how the tools worked and how to reason.
  prefs: []
  type: TYPE_NORMAL
- en: But after fine-tuning the two smaller models with just three thousand examples,
    ReAct suddenly shoots into the lead. As the right side of [Figure 8-1](#ch08_01_figure_1_1728429579251484)
    shows, not only does ReAct outperform standard and chain-of-thought prompting
    on same-size models, but now, ReAct on the fine-tuned 8B model outperforms the
    standard prompting approaches on the original 62B model. And similarly, ReAct
    on the fine-tuned 62B model outperforms the other prompting approaches on the
    original 540B model. So with proper reasoning on a *slightly* fine-tuned model,
    we can achieve much higher quality than is available on a much larger vanilla
    model without the reasoning steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of different colored bars  Description automatically generated with
    medium confidence](assets/pefl_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Performance of the ReAct prompt strategy, before and after fine-tuning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Part of the success of ReAct with HotpotQA tasks is due to the fact that ReAct
    can use search tools to look up facts that the model is missing. If you skip the
    reasoning step, then the performance is still pretty good; this is represented
    as the Act data in [Figure 8-1](#ch08_01_figure_1_1728429579251484).
  prefs: []
  type: TYPE_NORMAL
- en: 'Reasoning becomes critical in decision-making tasks such as ALFWorld. For the
    ALFWorld benchmark, the model is required to act as an agent navigating and performing
    tasks in a simulated house (reminiscent of old-school word-based role-playing
    games). In this domain, the importance of the *thinking* step is clear. The paper
    enumerates several features of thinking that lead to improved success rates:'
  prefs: []
  type: TYPE_NORMAL
- en: Decomposing task goals and creating plans of action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Injecting commonsense knowledge relevant to solving the task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting helpful details from observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking progress and pushing action plans forward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling exceptions and adjusting the course of action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to thinking and then acting (*ReAct*), acting alone (*Act*) is worse
    at breaking down goals into subgoals, and it tends to lose track of the environmental
    state. ReAct demonstrates a success rate of 71% in ALFWorld tasks, whereas Act
    leads to a mere 45% success rate. That’s a big difference!
  prefs: []
  type: TYPE_NORMAL
- en: Beyond ReAct
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While ReAct has been a very important step in improving reasoning capabilities
    in LLM applications, it’s not the last improvement we’ll see. In this short section,
    we present a couple of related approaches that show promise. The first is [plan-and-solve
    prompting](https://arxiv.org/abs/2305.04091). Whereas ReAct jumps right in with
    the think-act-observe loop, the plan-and-solve approach prompts the model to first
    devise an overarching plan. It uses the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first understand the problem and devise a plan to solve the problem. Then,
    let’s carry out the plan and solve the problem step-by-step.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlike ReAct, the plan-and-solve prompting paper doesn’t involve any tool usage;
    it’s purely focused on improving reasoning without reaching for data from the
    outside world. So really, plan-and-solve prompting is more closely analogous to
    the approach in the chain-of-thought section, which used the prompt, “Let’s think
    step-by-step.” The key point here is that the model may perform better in certain
    domains if we ask it to holistically understand the problem and make a plan before
    jumping directly into the actual step-by-step problem-solving. Combining this
    preplanning approach with ReAct’s think-act-observe steps might lead to further
    reasoning improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'If plan-and-solve prompting augments ReAct with preemptive planning, then *Reflexion*,
    introduced in the [widely cited 2023 paper “Reflexion: Language Agents with Verbal
    Reinforcement Learning](https://arxiv.org/abs/2303.11366)”, does the opposite—it
    allows the model to review its work after the fact, identify problems, and make
    better plans next time. Naturally, if the model has made a mistake that isn’t
    undoable, then this is of little help. (“I’m sorry for transferring your assets
    to your ex-husband’s account. I won’t do *that* again!”) But there are plenty
    of domains where you get a do-over. A great example near to our work at GitHub
    is in writing software that passes a suite of unit tests. With Reflexion, you
    can create pieces of the software using whatever approach you’d like (ReAct is
    cited in the paper), and then, once the work is finished, if the unit tests don’t
    pass, the failure messages can be inserted into the prompt so that the model can
    try again and this time avoid making the same mistakes.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Branch-solve-merge](https://arxiv.org/abs/2310.15123) is an approach that
    you might be able to guess from its name. Given a problem, you branch to *N* different
    *solvers*—independent LLM conversations—each of which tackles the problem in isolation.
    You could just have them make three independent attempts to solve the problem
    (and depend upon a relatively high temperature to ensure that their solution techniques
    are distinct), or better yet, you could prompt each solver to tackle the problem
    from a different perspective. Once all the solvers are complete, then the content
    they have produced is combined together and placed before a merging agent that
    combines the information from all three solvers into a better or more complete
    solution.'
  prefs: []
  type: TYPE_NORMAL
- en: As we close this section, hopefully, you’ve noticed some converging ideas in
    our conversation. For instance, this section makes use of the tools introduced
    in the first part of this chapter but also introduces new techniques that improve
    the model’s reasoning capabilities. In all the cases in this section, we do this
    by giving the model its own internal monologue so that it can process the situation,
    break down goals, and make better decisions on how to accomplish a task. We now
    have almost all the ingredients necessary for building our own autonomous agents;
    there’s just one more—context.
  prefs: []
  type: TYPE_NORMAL
- en: Context for Task-Based Interactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Chapters [5](ch05.html#ch05_prompt_content_1728435524680844) and [6](ch06.html#ch06a_assembling_the_prompt_1728442733857948),
    we discussed in great detail how to find and organize context when building a
    prompt for a document completion model. All of those ideas still hold true, but
    in regard to the task-based interactions that agents perform, there are some new
    things to consider. In this section, we’ll talk about where to retrieve context
    from, how to prioritize it, and how to organize it and represent it in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Sources for Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a moment, we’re going to build a general-purpose conversational agent. Such
    an agent will carry a variety of context drawn from several sources and couch
    it in the form of a conversational transcript.
  prefs: []
  type: TYPE_NORMAL
- en: First, there is a *preamble*, which sets up agent behavior and makes sure the
    agent understands what tools are at its disposal. If necessary, the preamble can
    include few-shot examples to demonstrate the behavior that the agent should exhibit
    during the conversation. The preamble typically goes in the system message when
    building an OpenAI chat prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The *prior conversation* is composed of all recent back-and-forth messages between
    the user and the assistant, up until the user’s current message. The prior conversation
    contains the broader context of this conversation, including information that
    will be important for the model to consider when handling the user’s current request.
  prefs: []
  type: TYPE_NORMAL
- en: Both user and assistant messages may have attached artifacts, and an *artifact*
    is any piece of data that is relevant to the conversation. For instance, a user
    might ask an LLM-based airline assistant about available flights. The artifact
    attached to this conversation would be a representation of the flights available,
    including details that might be helpful later in the conversation—dates, times,
    origin and destination airports, etc.
  prefs: []
  type: TYPE_NORMAL
- en: The *current exchange* begins with the user’s request as well as any artifacts
    they have attached to the conversation. For example, in the application interface,
    the user might indicate that they are talking about something on the screen (for
    instance, by highlighting text or clicking on a component). Rather than forcing
    the user to copy/paste details into the conversation, the application should be
    aware of what the user is referring to and should incorporate the relevant information
    into the prompt as an artifact.
  prefs: []
  type: TYPE_NORMAL
- en: After the user message, in the remainder of the current exchange, the model
    will make tool calls when necessary and the application will incorporate both
    the call and the response into the prompt (as we described at the start of this
    chapter). In subsequent exchanges, data from the tool evaluations can be presented
    as artifacts attached to assistant messages. The current exchange is finished
    when the model returns a message from the assistant back to the user. This message
    does not become part of this prompt, but it will be included in the *prior conversation*
    at the time of the next exchange.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-1](#ch08_01_table_1_1728429579262866) demonstrates what the full context
    of a conversational agent would look like, including the preamble, the prior conversation,
    and the current exchange.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Anatomy of a conversational agent’s context
  prefs: []
  type: TYPE_NORMAL
- en: '| **Preamble**: text that conditions general agent behavior'
  prefs: []
  type: TYPE_NORMAL
- en: Rules, instructions, and expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevant tool definitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot examples if necessary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Tool definitions are typically incorporated into the system message behind
    the model API.) |
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Prior conversation**: captures the context of the conversation to this
    point'
  prefs: []
  type: TYPE_NORMAL
- en: Previous user and agent messages, excluding the current exchange
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Artifacts: pieces of data attached to user or agent messages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Current exchange**: the current user request'
  prefs: []
  type: TYPE_NORMAL
- en: The most recent user message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any artifacts attached by the user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tool calls and responses generated while servicing the user’s request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Agent response**: summarizes this exchange; will be part of the prior conversation
    in the next exchange |'
  prefs: []
  type: TYPE_TB
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting and Organizing Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding discussion, we presented a variety of contexts that might be
    included in a conversational LLM application. In this section, we’ll look at several
    techniques and ideas for assembling this context into a prompt. There is no one-size-fits-all
    approach; the effectiveness of a particular prompt engineering approach is dependent
    upon the domain, model, data, and many other factors. The key is to constantly
    try new ideas and then evaluate, evaluate, evaluate (more on that in [Chapter 10](ch10.html#ch10_evaluating_llm_applications_1728407085475721)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a list of the things you might consider when selecting and organizing
    context for your prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: What tools do you need? During parts of the conversation, you might know that
    the agent has no use for particular tools. Drop them from consideration and your
    agent will have one less distraction when using other tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What artifacts should you present? Your options are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include all of them. While you can be sure the model will have the best information
    available, irrelevant content and lots of it are sure to confuse the model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask the model to select which artifacts it thinks are relevant. This requires
    substantial additional complexity in the application because you must set up the
    side request to have the model choose which artifacts it thinks are important.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How should artifacts be presented? Your options are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add artifact data directly to user and assistant content by sticking it in an
    XML tag, like the `<artifact>` tag in [Table 8-1](#ch08_01_table_1_1728429579262866),
    or in a markdown section, like `## Attached Data`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the artifact can be JSON, plain text, or anything else. Anecdotally,
    it doesn’t seem to matter much (but test this for yourself).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, if all of your artifacts come from function calls, then don’t
    treat artifacts in a special way at all. Just preserve the function calls from
    the current exchange into the prior conversation. The benefit is that this provides
    more examples of tool invocation that can help the model make better use of tools
    during the current exchange.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How much content do you include in each artifact? If the user refers to a book,
    then certainly, you wouldn’t include the full text in the prompt. You wouldn’t
    be able to include the full content in the prompt, and even if you could fit it
    in, it would confuse the model. So, drawing on the “elastic snippet” conversation
    in [Chapter 6](ch06.html#ch06a_assembling_the_prompt_1728442733857948), you need
    to find a way to extract information from artifacts and present only the most
    relevant data for the task at hand. Here are some possible ways to do that:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One clever idea (though one that we haven’t yet tried) is to present the artifact
    as a bulleted summary and then also include this text for each bullet: `` for
    more information, call `details(''section 5'')` `` where details is a tool used
    to retrieve more details about the referenced argument. Then, if the application
    calls `details(''section 5'')`, you can unfurl that portion of the artifact, possibly
    revealing more subsections that can be unfurled.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, just provide a retrieval for searching through the large artifact
    (a.k.a. a traditional RAG).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How far back should the prior conversation go? If the conversation has shifted
    to a new topic, then you can drop it. How do you know if the conversation has
    moved on? That’s a good question. One option is to automatically drop all content
    from prior user sessions (e.g., after the user has been inactive for some predetermined
    amount of time). Alternatively, you can ask a model to decide what content is
    relevant. This is probably overkill for a large model (too expensive and high
    latency), but you can train a smaller model to do this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We wish that we could be more prescriptive with our advice here. It’s tricky.
    If you include too much information, then you’ll confuse the model, run out of
    space in the prompt, and drive up latency and cost. If you include too little,
    then the model will not have the information it needs to address the task at hand.
    But LLM technology is moving quickly. Models are getting smarter and faster, and
    their prompt capacity is increasing. Perhaps the questions in this section will
    get easier in the future, when we’ll be able to just say, “When in doubt, add
    it to the prompt and let the model figure it out!” Until then—evaluate, evaluate,
    evaluate!
  prefs: []
  type: TYPE_NORMAL
- en: Building a Conversational Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it’s time for you to assimilate all that we’ve discussed in this chapter
    and build your own conversational agent. By the end of the tool usage discussion
    at the beginning of the chapter, we were actually quite close. Turn back and look
    at [Example 8-1](#ex-8-1). There, we defined `process_messages`, which takes all
    of the messages in a conversation, optionally calls one or more tools, and finally
    provides a response in the voice of the assistant to answer the user and summarize
    any behind-the-scenes tool-calling activity. The only two things remaining are
    (1) providing a way to allow your user to interact with the agent (here, we’re
    just using a Python input statement) and (2) tossing a loop around the `process_messages`
    function so that you can facilitate a full back-and-forth conversation between
    the user and the assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Conversations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Referring to [Example 8-2](#ch08_01_example_2_1728429579274312), the `process_messages`
    function takes a set of messages and then appends new messages corresponding to
    tool invocation and evaluations. It might do this several times. Finally, `process_messages`
    appends a response from the assistant, which incorporates any information discovered
    from tool usage. The `run_conversation` function wraps the `process_messages`
    function. It initializes the messages list, iteratively requests user input, appends
    the user message, and sends the messages to the `process_messages` function. The
    `run_conversation` function also prints out user and assistant messages, giving
    us a reasonable text-only user experience. The result is a natural flowing conversation
    that can make use of tools if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-2\. The `run_conversation` function manages the full conversation
    state, including user input and agent output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: To make this just a bit clearer, [Figure 8-2](#ch08_01_figure_2_1728429579251516)
    presents a sequence diagram illustrating the design of the conversational agent.
    Here, we’ve generalized the agent as a sequence of interactions between the user,
    the application, the model, and the tool. The sequence diagram details the information
    transferred between actors and highlights the actions taken by the application.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of the chapter, in the tools discussion, we ran `process_messages`
    several times in a row. This gave us a glimpse of the conversational agent in
    action. However, it’s not truly a conversational agent until it’s running inside
    the `run_conversation` loop defined in [Example 8-2](#ch08_01_example_2_1728429579274312).
    In [Table 8-2](#ch08_01_table_2_1728429579262901), we demonstrate how the conversational
    agent interacts with a user when `run_conversation` is executed.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a software application  Description automatically generated
    with medium confidence](assets/pefl_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. A sequence diagram representing the design of the conversational
    agent
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Table 8-2\. Executing `run_conversation`
  prefs: []
  type: TYPE_NORMAL
- en: '| Role | Dialogue/[tools] | Commentary |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| User | `Golly gee, it''s hot in here. Let''s cool this place down.` | LLMs
    agents can interpret “colorful” language. |'
  prefs: []
  type: TYPE_TB
- en: '| Agent |'
  prefs: []
  type: TYPE_TB
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`The current room temperature is 64ºF, which is actually quite cool. If you
    find it too warm, it might be due to other factors. Would you still like to adjust
    the temperature?` | The agent checks the temperature and then *uses common sense*
    to tell the user that their statement contradicts normal expectations. The agent
    still offers to comply with the request. |'
  prefs: []
  type: TYPE_NORMAL
- en: '| User | `Make it LOTS cooler.` | The user proceeds. |'
  prefs: []
  type: TYPE_TB
- en: '| Agent |'
  prefs: []
  type: TYPE_TB
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`I''ve set the room temperature to 50ºF. It should feel much cooler soon!`
    | The agent complies. (Hey, the guy said, “LOTS!”) |'
  prefs: []
  type: TYPE_NORMAL
- en: '| User | `Whoa...that''s a bit extreme, isn''t it? Put it back to where it
    started.` | The user realizes that the result is unexpected. |'
  prefs: []
  type: TYPE_TB
- en: '| Agent |'
  prefs: []
  type: TYPE_TB
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`I''ve reset the room temperature to 64ºF. It should start heating up again
    shortly.` | The agent, *equipped with the prior conversation*, correctly sets
    the temperature back to the starting point. |'
  prefs: []
  type: TYPE_NORMAL
- en: There are several things to note here. For one, it’s still hard to not be in
    awe of how flexible these models are. The user’s opening comment isn’t formal
    at all—it’s even kinda weird—but the model correctly interprets the intent. Also
    impressive is how you get common sense reasoning for free. We see this in the
    agent’s remark about 64ºF being “actually quite cool”—you have to know a lot about
    humans to get that right. We also see this later—and take it for granted—when
    the model makes the temperature “LOTS cooler” by setting it to 50ºF instead of
    0ºF or―1,000ºF. And we see this when the agent talks about how the temperature
    will change soon rather than immediately—clearly, the agent understands thermostats
    at some level.
  prefs: []
  type: TYPE_NORMAL
- en: The most important new behavior for the agent is seen in the last exchange,
    when it correctly converts the temperature back to its starting point of 64ºF.
    It can accomplish this step because we are now correctly tracking not only the
    current exchange but the prior conversation as well. This allows the agent to
    refer to the start of the conversation, where it first learned that the temperature
    was 64ºF.
  prefs: []
  type: TYPE_NORMAL
- en: With `run_conversation` (see [Table 8-2](#ch08_01_table_2_1728429579262901))
    wrapping `process_messages` (see [Example 8-1](#ex-8-1)), we have a simple but
    complete conversational agent. All of the code is generic, and you can modify
    the system message and the tools to easily create whatever behavior you please.
    As the agent becomes more complex, you might need to spend time thinking about
    how to deal with other concerns we’ve discussed in this chapter—such as providing
    the agent with the appropriate tools for a request, retrieving earlier conversations,
    and incorporating information in the form of artifacts. And naturally, you’ll
    probably want more than a text-based tool, so you’ll have to place the agent behind
    an API, handle errors, and add logging. But, at this point, the sky’s the limit.
    What will you make first?
  prefs: []
  type: TYPE_NORMAL
- en: User Experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding examples, we’ve been looking at blobs of text. But your users
    will likely be engaging with the agent through a much richer visual interface.
    In this section, we talk about some of the basic affordances that you should consider
    when implementing the UI.
  prefs: []
  type: TYPE_NORMAL
- en: The chat UI is ubiquitous—from AOL Instant Messenger, released in the 1990s,
    up until Slack, it’s been the same. It’s people taking turns typing in little
    rectangles on the screen. This format is the same for ChatGPT, and it will be
    the same for your application as well. One simple affordance not to forget is
    a spinner that indicates the agent is processing and will return soon with a new
    interaction. In [Figure 8-3](#fig-8-3), we see that the user, Dave, has asked
    the assistant, HAL, a question, and the spinner (labeled as item 1) indicates
    that HAL is taking time to process the next response.
  prefs: []
  type: TYPE_NORMAL
- en: One thing that’s new and special for most conversational agents is the use of
    tools. Your UI should indicate when the agent is using tools, for instance, with
    a pill button inside of the agent message (item 2). This lets the user know that
    the agent is engaging in background work before returning with a final response.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex chat applications, you should allow the user to have visibility
    into the processing that is taking place. In [Figure 8-3](#fig-8-3), Dave becomes
    puzzled about the unexpected response from HAL, so Dave clicks the “Tool calls”
    button (item 3). Once clicked, the button reveals full details about the tool
    calls. This includes the name of the tool, the arguments presented as a webform,
    and the results that the agent will be working with. Dave can inspect this form
    and understand the rationale behind HAL’s response.
  prefs: []
  type: TYPE_NORMAL
- en: Even though LLMs are increasing in intelligence, they still need a fair amount
    of course correction from users. Let your users interact with the agent’s tool
    calls. Allow users to modify arguments from the webform (item 4) and then resubmit
    the corrected request. Once the user resubmits the tool request, the conversation
    can be regenerated from that point onward (item 5), hopefully leading to a more
    desirable outcome. As you can see, Dave uses this change of tool arguments to
    change the course of the conversation. Silly HAL.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/pefl_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Interacting with a tool-equipped conversational agent
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As mentioned earlier, once you introduce tool calls that modify real-world assets,
    you introduce a new level of risk into your application. Therefore, you should
    always allow your user to authorize any request that has a remote chance of being
    dangerous (see [Figure 8-4](#fig-8-4)).
  prefs: []
  type: TYPE_NORMAL
- en: Note that if a tool call modifies real-world assets, then you should make sure
    to allow the user to authorize the request before executing it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, though it’s not pictured here, lots of chat experiences implicitly
    attach artifacts to the conversation (for instance, if the user is looking at
    a document on their screen, then the application might include its text in the
    prompt). To help your users understand what the agent is thinking about, give
    them some way to see into the agent’s “mind” and see the same artifacts that the
    agent is looking at. If the user has an understanding of where the agent’s attention
    is focused, then they will be able to ask more pointed questions and resolve problems
    more quickly. Similarly, if the agent is looking at the wrong thing, then giving
    the user the ability to dismiss an artifact might help keep the conversation on
    track.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/pefl_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. A possible UI implementation of an authorization request
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You’ve come a long way in this chapter. You’ve learned that agency is the ability
    of an entity to accomplish tasks in a self-directed manner. You’ve also learned
    that *conversational* agency is a form of assisted agency in which a human and
    an assistant work together to accomplish tasks through back-and-forth dialogue.
    In this chapter, we talked about core aspects of conversational agency: using
    tools to gather information and make changes to assets in the real world, improved
    reasoning about the task at hand, and the requirements for collecting and organizing
    context information relevant to the task. In the last section, we built the complete
    conversational agent and discussed UX concerns.'
  prefs: []
  type: TYPE_NORMAL
- en: Conversational agents have their limits, though—they often need the corrective
    influence of a human to keep them on track and pushing toward the goal. In the
    next chapter, we’ll show you how to use LLM-based workflows to accomplish goals.
    Rather than making you rely on humans to keep the agent on track, we’ll show you
    how to break down complex problems into tasks that can be executed in a directed
    workflow. Each task is simple and does not require human intervention, but the
    workflow as a whole will be able to accomplish tasks that haven’t been technologically
    feasible until now.
  prefs: []
  type: TYPE_NORMAL
