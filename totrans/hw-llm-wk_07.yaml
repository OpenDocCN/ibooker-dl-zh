- en: 8 Designing solutions with large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using retrieval augmented generation to reduce errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How LLMs can supervise humans to mitigate automation bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling classic machine learning tools with embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ways to present LLMs that are mutually beneficial to companies and users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By now you should have a strong understanding of LLMs and their capabilities.
    They produce text that is very similar to human text because they are trained
    on hundreds of millions of human text documents. The content they produce is valuable
    but also subject to errors. And, as you know, you can mitigate these errors by
    incorporating domain knowledge or tools like parsers for computer source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you are ready to design a solution using an LLM. How do you consider everything
    we have discussed thus far and convert it into an effective implementation plan?
    This chapter will walk you through the process, trade-offs, and considerations
    in designing that plan. To do so, we will use a running example that we can all
    relate to: contacting tech support when help is needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will consider the obvious path: building a chatbot. Chatbots are
    the vehicle that introduced many people to LLMs because generally, they can do
    an excellent job of generating output interactively. We’ll evaluate the risks
    of deploying an LLM-powered chatbot in a customer service scenario. Through this
    discussion, you’ll see that using an LLM can increase risk compared to other options.
    However, a simple chatbot may be a valid option if the risks are sufficiently
    minimal.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore ways to manage the risks by using application designs
    that improve how customers interact with the LLM. We’ll discuss how having a person
    check each output produced by an LLM is fraught with problems due to a phenomenon
    known as automation bias. We’ll discuss how automation bias can be somewhat counterintuitively
    avoided by having the LLM supervise the person instead. We’ll explore how an LLM’s
    embeddings, the semantic representation of text encoded as numbers, can be combined
    with classical machine learning algorithms to address this risk and handle tasks
    that an LLM can’t perform independently.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll investigate how technology is presented to users and plays a
    vital role in establishing trust and conveying an understanding of its inner workings.
    We’ll discuss the area of “explainable AI,” where a machine learning algorithm
    produces output that describes or explains how it arrived at a specific output.
    Explainable AI is often the approach adopted to handle situations where people
    need to understand how an LLM works, but studies show that although explainability
    may shed some light on the inner workings of LLMs by describing the behavior of
    these models in human terms, it does not tend to help for its own sake. Instead,
    we’ll describe the benefits of focusing on transparency, aligning incentives with
    customers, and creating feedback cycles to design solutions that better meet the
    needs of both the companies that employ them and the customers that interact with
    them by providing accurate output and creating efficiencies in business processes.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Just make a chatbot?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsurprisingly, many people are building chatbots using LLMs based on transformer
    architectures, the same technology that underpins ChatGPT. It’s an obvious and
    seemingly reasonable first step. ChatGPT’s fantastic ability to interact with
    people, adapt to conversations, and retrieve and present information demonstrates
    how well LLM technology supports customer interaction applications. With the advent
    and availability of LLMs, it would likely be short-sighted to attempt to implement
    a customer service agent using any other approach, such as using an expert system
    trained to use a decision tree of canned responses. When an unhappy customer has
    some technical problem, instead of searching an online Frequently Asked Questions
    (FAQ) document, sending an email into the black hole of a trouble ticket system,
    or calling a phone number with an automated interactive voice response system,
    they can start directly interacting with an AI-powered tool and make progress
    on getting their problems solved. This sounds wonderful on paper, and if you draw
    a little diagram like figure [8.1](#fig__hci_chatbot), it sure looks like we are
    simplifying life.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F01_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 When looking at the process diagram, it would seem like replacing
    FAQs, email tickets, and support numbers could be simplified and streamlined with
    an LLM-based chatbot. However, the folly of this view is that the process is incomplete.
    The potential errors and remediation processes required to ensure an LLM will
    perform accurately are hidden and create more complexity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are certainly cases where a chatbot is a good idea. But surprisingly,
    an online LLM-based chatbot that handles support probably is not at the top of
    the list of customer support tools for most companies because of the effort required
    to build a system that will be accurate and reliable in many cases and not create
    unexpected output when confronted with unexpected input. Ultimately, the decision
    to use an LLM to implement a customer support chatbot comes down to our ongoing
    discussion of the errors an LLM might make when generating customer responses.
    We know that LLMs are not error-free, and while machine learning is sometimes
    practical, the expense of those potential errors is the primary decision criterion
    when considering deploying this technology. Fundamentally, using an LLM potentially
    increases the cost of those errors. The bottom line is that in their current form,
    LLMs can provide incorrect answers, and the liability for these falls on the shoulders
    of the companies or individuals who deploy and maintain them.
  prefs: []
  type: TYPE_NORMAL
- en: Executives or product managers might consider the cost of errors in the context
    of a few classic business key performance indicators. For example, customer retention
    rates might decrease if they entrust support to chatbots. Perhaps the retention
    rate would be higher than if the customer relations functions were outsourced
    to a call center in another country. Indeed, these considerations are important
    to evaluate, and you should probably do a trial deployment to see what customers
    think before replacing your customer support function with an LLM wholesale.
  prefs: []
  type: TYPE_NORMAL
- en: Note We almost always recommend trial deployments of any machine learning system.
    The investing adage “Past performance is not a guarantee of future returns” is
    true of any AI. One way to do this is through *phantom deployments*, where you
    run your new AI system alongside the existing process for some weeks or months.
    You may choose to ignore its outcomes while the existing business processes are
    in place. This gives you time to observe the discrepancies between your current
    and new processes, identify and address problems, and determine whether the performance
    of the machine learning system degrades over time.
  prefs: []
  type: TYPE_NORMAL
- en: Most critically, your LLM can give advice that causes harm to your users. Since
    an LLM is not a person who can be held legally liable for their actions, you and
    your company will be held liable instead. This has already happened with an airline
    that deployed a chatbot that gave errant policy statements. A court decided that
    the company had to abide by the policy incorrectly generated and shared by their
    chatbot [1].
  prefs: []
  type: TYPE_NORMAL
- en: We recommend always considering an *adversarial* mindset when deploying an LLM.
    Asking “What could a motivated bad actor do if they knew how this worked?” will
    help you identify and mitigate significant risks and is often the best way to
    determine whether your intended LLM application is a good or bad idea. For example,
    a car company integrated an LLM into their website to help sell cars and answer
    questions. After realizing this, it took less than a day for users to convince
    the website to sell them a car for just $1 [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'If the potential cost or risk of errors is low, you can feel comfortable deploying
    an LLM chatbot if you so choose. But for the sake of this chapter, let us assume
    that this technical support agent we are hypothesizing is very important, and
    the mistakes it makes could cost the company a lot of money. The question now
    becomes: How do we design a solution that gives us benefits in productivity and
    efficiency yet limits users’ direct access to an LLM? If you are new to AI/ML
    and a chatbot is your primary exposure to the field, this might sound like a contradiction,
    but there are some easy, repeatable design patterns you can apply to do this.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Automation bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common approach to addressing the risk of using LLMs for direct customer interactions
    is to have the LLM interact with support staff or technicians instead. This is
    often referred to as “human in the loop” because there’s a person who is reviewing
    the feedback loop between the LLM and the customer, providing a critical assessment
    of the automated system’s output, and intervening and adjusting the output when
    they detect an error. The technician will still be employed, but we will increase
    their efficiency by having the LLM generate an initial response to each question
    from a user and a technician curating those responses to ensure that they are
    accurate and relevant. If the LLM generates a potentially costly or incorrect
    response, our trusty technicians will intervene and reply with something more
    appropriate. In this context, it is ultimately up to the technician to choose
    the proper authoritative response.
  prefs: []
  type: TYPE_NORMAL
- en: The clever reader who remembers our discussion about retrieval augmentedgeneration
    (RAG) from chapter [5](../Text/chapter-5.html) might even identify ways to improve
    upon this idea. You’ll say, “Ah, we can put all our training manuals and documentation
    inside a database, and then we can use RAG so that the LLM can retrieve the most
    relevant information to a user’s question.”. This approach is outlined in figure
    [8.2](#fig__human_in_loop_naive_rag), which shows a process where a user’s questions
    are first sent to the LLM to focus output generation using a collection of known
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F02_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 A naive approach toward implementing a “human in the loop” system
    that uses an LLM paired with a database of relevant information to produce output
    that is ultimately reviewed by and possibly corrected by a human worker
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The RAG approach will likely mitigate a lot of risk, but it also has the potential
    to hit the pitfall of *automation bias*. Automation bias refers to the fact that
    people, in general, tend to pick automated or default choices presented by a system
    because it is easier than applying critical thinking to determine which choice
    is most appropriate to the situation at hand. If a system works well and does
    not need you to intervene often, it becomes incredibly challenging to remain hypervigilant
    and detect the occasional error. The paradox is that if the system is so inaccurate
    in its suggestions that you can maintain your vigilance, the chances are good
    that the system is slowing you down when compared to directly answering questions
    using no automation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where trial or phantom deployments become incredibly important. If
    your system is so accurate that automation bias is the real source of risk, you
    have two options that do not require deviating from the “human in the loop” design:'
  prefs: []
  type: TYPE_NORMAL
- en: Add an “escape to a human” path to the pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigate the risk of errors externally via process changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first point is pretty straightforward. Eventually, a novel situation will
    occur that the LLM cannot answer. In this case, it would be best to provide a
    way for a customer to “escape” from an infinite loop with a computer to get to
    a higher tier of support. This could be a maximum conversation length measured
    in the number of messages exchanged or the amount of time spent chatting, an option
    to contact a human representative that appears based on multiple failed attempts
    to communicate, or other possible designs.
  prefs: []
  type: TYPE_NORMAL
- en: Note Suppose you are going to do the work to create an RLHF or SFT dataset to
    fine-tune your LLM to your situation as we discussed in chapter [5](../Text/chapter-5.html).
    In that case, you can even add training examples where the LLM’s expected response
    is “I’m sorry, this situation sounds more complex than what I can assist with;
    allow me to get a human to help.”
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Changing the process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second suggestion, changing the process, is not as difficult as it may sound.
    If one of your bosses has an MBA, they are (allegedly) trained to think in these
    terms. (One of the authors has an MBA, so it is OK for us to say that.) For example,
    interactions with the chatbot could include a caveat about any outcome requiring
    “a human’s final approval.” In this case, having the entire conversation reviewed
    by a person is far less of an automation bias risk than requiring someone to maintain
    constant vigilance throughout a continuous conversation. Ultimately, adversarial
    users know a human is going to check and so are demotivated from trying to game
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the context, preventing adversarial use of an LLM can be achieved
    by requiring the user to provide collateral to ensure they act in good faith.
    For example, you could take actions equivalent to putting a hold on the user’s
    credit card as a kind of insurance against bad-faith interactions. Such a hold
    would be released when the transaction is completed successfully. You could also
    limit how much of the process is automated, require authentication, or randomize
    how often people are routed to a human versus an AI so that it becomes unpredictable
    when a situation that could be exploited will arise.
  prefs: []
  type: TYPE_NORMAL
- en: All of these actions will depend on your specific application, the risks, the
    tolerance of those risks, and the nature of your users. Some customers might be
    turned off by a credit hold and be upset. Or maybe you frame it as an optional
    method in which the user gets $2 off their bill if an AI system successfully helped
    them with their problem, presuming that it is less than what the old system would
    have cost per call. Either way, it is case by case and will depend on your creativity
    to manage the risk.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 When things are too risky for autonomous LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So now you have done a trial deployment, evaluated the risks and your users’
    adversarial proclivities, and concluded that it is too risky for LLMs to provide
    the initial answers. How could an LLM still provide some level of efficiency?
  prefs: []
  type: TYPE_NORMAL
- en: An unintuitive approach is to have the LLM check the person rather than the
    person check the LLM. This may sound strange. Why would we let the LLM supervise
    if we cannot trust it to act alone? To consider this further, imagine you have
    an LLM system in this supervisory role, checking each response, as shown in figure
    [8.3](#fig__HCI_LLM_supervise_human).
  prefs: []
  type: TYPE_NORMAL
- en: If the LLM and the person are correct, action will be taken, and the message
    will be relayed to the customer. It will be as if the user is chatting with the
    technician. But if the technician and the LLM disagree on the answer, we can prompt
    the technician to double-check their response before sending it to the user.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F03_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Notice that the direction of the arrows in this diagram has changed
    from figure [8.2](#fig__human_in_loop_naive_rag). Everything goes to a human first,
    and we use LLMs to catch mistakes before they happen.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This double-check could be as simple as telling the technician, “Hey, this looks
    like it may be abnormal for a solution; please confirm before sending.” You could
    try having the LLM produce its own suggested alternative. Or you could keep the
    LLM out of the process and use it to notify a more experienced technician to join
    the process and assist. Regardless of how this is structured, the purpose is to
    signal that there may be a risk of a negative customer interaction, such as an
    incorrect answer. While this risk existed previously, we now have a chance to
    mitigate it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, because we are considering human-initiated customer support errors,
    we are generally not taking on any new risk because a support representative acting
    alone could just as easily make a mistake. So if the LLM and human are both wrong
    simultaneously, you were already doomed to make that process error anyway. Such
    is life. Technically, we could argue that technicians could question their responses
    too much based on an LLM’s assessment of their interactions, thus reducing efficiency.
    Additionally, an overly sensitive LLM may ask technicians to double-check their
    work too often, which would cause alert fatigue that could lead to technicians
    ignoring the LLM suggestions entirely. If your use case is prone to these sorts
    of problems, that fact will be uncovered during trial deployments that provide
    context-specific feedback on how an LLM should be tuned to address this problem.
    The general caveat that applies to all machine learning is especially important
    here: always test; do not assume.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Employing an LLM to double-check human performance can reduce errors in the
    process as a whole. It may not seem like this approach makes anything faster because
    humans are still generating the initial response. However, this approach still
    creates opportunities for increased efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: It can reduce the conversation length by helping to catch errors and reach a
    solution faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can identify staff who need more training or information to answer customer
    questions or recognize when specific error situations occur.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may help avoid escalation to more costly levels of support or managers,reducing
    the frequency and cost of troublesome customers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.3 Using more than LLMs to reduce risk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Everything we have discussed has involved a “fight fire with fire” approach
    in which, although there are risks to using LLMs, we have considered different
    ways to use LLMs to mitigate those risks. While we’ve changed how we use the LLM,
    the LLM is still the primary component. Alternatively, we can consider using tools
    other than LLMs to address our design challenges. Other approaches in the scope
    of generative AI, such as text-to-speech and speech-to-text, can be used to build
    more accessible or simply convenient user experiences. For example, users with
    arthritis or low vision may greatly prefer a phone call over typing responses
    into a chatbot prompt window.
  prefs: []
  type: TYPE_NORMAL
- en: If we think about our customer service problem and when LLMs work well, we will
    discover that the ingredients for a broader class of tools are also available.
    LLMs work best when there is repetition in scenarios where problems reoccur and
    formulaic solutions and responses can be given. LLMs are very flexible in recognizing
    broad patterns in the fuzzy nature of language. If the LLM can correctly interpret
    a user’s problem, and there is a known solution, it can potentially walk a user
    through that solution. This might sound much like an unsupervised chatbot, but
    the critical distinction is that in the cases where the LLM takes a subordinate
    role in the solution, the output was ultimately generated by customer support
    technicians, as described in figure [8.3](#fig__HCI_LLM_supervise_human).
  prefs: []
  type: TYPE_NORMAL
- en: This section will also discuss how we can use classic machine learning techniques,
    such as classification, to tackle existing problems. We can do this by using the
    knowl-edge within LLMs to enable machine learning techniques by producing embeddings
    of the user’s text.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Combining LLM embeddings with other tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter [3](../Text/chapter-3.html), we described how an LLM transforms tokens
    into embeddings, which are vectors that encode a semantic representation of the
    meaning of each token as a series of numbers. These vector embeddings are useful
    in other ways outside the context of LLM’s transformer architecture. While vector
    embeddings are essential for making the LLM operate, they are themselves an extraordinarily
    useful tool.
  prefs: []
  type: TYPE_NORMAL
- en: The semantic nature of the vectors produced by LLMs is important because hundreds
    of other practical machine learning algorithms operate on vector representations.
    LLMs are essentially a very powerful way of converting complex human language
    text into a form compatible with the rest of the machine learning field. Utilizing
    the vector outputs of LLMs with other algorithms has been such an extraordinarily
    useful strategy that practitioners will describe it as “creating embeddings.”
    The description comes from the idea that the LLM is taking one representation
    (human text) and embedding it into another representation (a mathematical vector).
    Because these numbers encode information about the original text, you can plot
    them like numbers and see that similar texts end up in similar locations on the
    plot, as shown in figure [8.4](#fig__embeddings_are_useful).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F04_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 LLMs produce numeric vectors known as embeddings as an intrinsic
    part of their function-ing. The utility of these embeddings is dependent on the
    fact that these numbers only change a little bit when given similar text. The
    two example tests here will have similar embeddings, and thus, their plots look
    similar, even though they don’t share any of the same words. This is a powerful
    feature that was present in older machine learning techniques.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s look at a quick description of four types of machine learning algorithms
    you can use once you have embeddings. We consider each type of machine learning
    to be particularly useful for most real-world use with LLMs; we will also note
    some popular algorithms you can find that are relatively reliable and easy to
    use. The critical takeaway is that if you break out of the mindset that only an
    LLM can solve a problem, a more extensive set of tools becomes available to you.
    This list is your starting map for some of those tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Clustering algorithms*—Grouping texts by similarity to each other that are
    distinct from the larger amount of text available (e.g., used for market segment
    analysis). Popular algorithms include k-means and HDBSCAN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Outlier detection*—Finding texts that are dissimilar from essentially all
    other texts available (i.e., finding contrarian customers or novel problems).
    Popular algorithms include Isolation Forests and Local Outlier Factor (LoF).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Information visualization*—Creating a 2D plot of your data to allow visual
    inspection/exploration, especially when combined with interactive tools (i.e.,
    data exploration). Popular algorithms include UMAP and PCA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Classification and regression*—If you label your old texts with known outcomes
    (e.g., net promoter score rating), you can use classification (i.e., pick one
    of A, B, or C) or regression (i.e., predict a continuous number like 3.14 or 42)
    to predict what the score would be on a new text (i.e., data categorization and
    value prediction). Using embeddings as input for simple algorithms like logistic
    regression and linear regression works well for classification or regression,
    respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note Embeddings are not something new that was invented as a part of LLMs. An
    algorithm known as Word2Vec, which could embed single words, popularized embeddings
    as a go-to strategy for representing the meaning in text back in 2013\. Despite
    this, LLMs tend to produce embeddings with greater utility than other older algorithms.
    However, an LLM is far more computationally demanding than older algorithms like
    Word2Vec. For this reason, you may want to use an older or faster algorithm for
    this task. The existence of generative AI methods in images, video, and speech
    means you can also use embeddings for domains such as images, video, and speech
    in addition to text.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Designing a solution that uses embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have described the concept of embeddings and how they offer us more
    tools, let’s build an enhanced tech-support call center solution. We will continue
    to use LLMs for their text-generating capability and their embeddings and incorporate
    other machine learning techniques to enable the voice interaction that people
    are accustomed to, reduce wait times, and increase efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: First, to support voice interaction, we will use speech-to-text to convert the
    words spoken by a user into text that is used as input into an LLM. It would be
    reasonable to think, “I’ve used some really horrible voice-controlled systems
    before,” and yes, you likely have. This is why adding a “bail-out” mechanism is
    essential to escape the automated system (e.g., max tries, times, or opt-out)
    for cases where the system can’t understand a user’s speech. In addition to speech-to-text,
    we will also use text-to-speech to give the LLM a way to convert the text output
    it produces into something that a user should be able to hear and understand.
  prefs: []
  type: TYPE_NORMAL
- en: Second, to reduce wait times, we can implement a system where, if a queue of
    callers has formed due to the number of support requests incoming, we will ask
    the customer to describe their problem so that they can be routed to the most
    appropriate analyst. Assuming that customers may have novel problems, we do not
    attempt to use the LLM to address their problems outright. Instead, we will use
    a customer’s problem description to call the LLM’s embedding API to produce a
    representation of their problem. Once we have that problem description embedding,
    we can use clustering to group the users in the queue. Users with similar problems
    can be assigned to the same team of analysts to help analysts solve problems faster.
    That alone is a win.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this problem grouping to gain further efficiency. Say an analyst
    has identified that a user has a common problem for which there is a consistent,
    predefined solution. Instead of relying on the LLM to dynamically generate a hypothetical
    solution, your human analyst can share the predefined solution that has already
    been vetted by real users. Additionally, you can push that solution out to the
    users who are waiting in the queue via the LLM. You will be able to inform the
    users: “An automated solution has been developed that we believe will solve your
    problem. While you wait, let us try to solve this with our automated AI.” This
    approach is summarized in figure [8.5](#fig__HCI_MoreApproaches).'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F05_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 This diagram describes our “better solution” to customer support
    requests, where customers describe their problem while waiting to talk to someone.
    The LLM uses an embedding representation of the problem to compare similar problems
    with known solutions. While the user waits, an automated system can provide information
    that may help them solve their problem without support personnel intervention.
    If that fails, there’s always the possibility to “bail out” and talk to a real
    person. The model used to generate the embeddings does not necessarily have to
    be the same as the LLM that walks the user through the solution.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It’s entirely possible to combine the solutions we have described so far. For
    example, the analyst-to-customer interaction loop in the top-right of figure [8.5](#fig__HCI_MoreApproaches)
    could involve two people talking through the problem, or it could be the LLM-supervised
    validation solution we designed in figure [8.3](#fig__HCI_LLM_supervise_human).
    Depending on what problems need to be solved, there are many opportunities to
    extend these solutions now that we have embeddings. For example, if analysts saved
    information about how angry or upset a customer is, you could train a regression
    model to predict how angry a customer may be from their embedding. Then, you could
    distribute the angry customers evenly amongst analysts to avoid someone being
    overwhelmed or try to route angry customers away from new analysts who are still
    learning how to help customers solve their problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be clear, we are not saying that all customer service tech support systems
    will be better if they use this approach. The goal is to show you that there are
    ways to build solutions with LLMs that work around their shortcomings, such as
    their tendency to hallucinate and their inability to incorporate new knowledge
    dynamically. In summary, we present two basic strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: Use LLMs as a second set of eyes on what is happening. If the LLM agrees, all
    is good. If it disagrees, you perform a double-check that could be simple or complex,
    depending on the nature of the problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use embeddings to apply classic machine learning to the problem. Clustering
    (grouping similar things) and outlier detection (finding unique or unusual things)
    will be particularly useful for many real-world applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don’t solely rely on the LLM to create output at any point in these solutions
    because LLMs can generate incorrect or inappropriate output. However, we can still
    use the LLMs to reduce workload, errors, and the time to resolution by being careful
    in how we design the system as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Technology presentation matters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of you may be incredulous after reading through this example of how we
    would design a tech support system that uses LLMs. We often hear folks who fully
    believe in LLM technology say, “If you have the LLM explain its reasoning, the
    user or analyst can figure out if it makes sense, and all of the problems related
    to hallucinations and errors will be solved.” We often receive similar requests
    to create “explainable AI” from those on the more skeptical end of the spectrum
    who are concerned about the errors LLMs produce and who don’t understand what
    is happening. Thus, there is a perception on both sides that explanations will
    provide the means to establish trust in the technology and believe that the LLM
    (or any machine learning algorithm) is working properly and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we want to discuss some points that support the notion that
    explain-ability is not the solution to these problems. Explainability is not the
    single solution that will help catch errors or make a system more transparent
    and trustworthy. The unfortunate truth is that our assumptions about how an LLM
    will work with people are often wrong and must be carefully evaluated. In fact,
    recent research has shown that when explainable AI techniques are employed by
    a system, people erroneously trust the AI to be correct solely based on the fact
    that an explanation is present, regardless of its accuracy. This is true even
    when the user could perform the task independently without an AI’s support, and
    the user has been taught about how the AI systems actually work [3]. The bottom
    line is that explanations can be harmful to the very goals that they attempt to
    advance.
  prefs: []
  type: TYPE_NORMAL
- en: Why use explainable AI at all?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our professional experience, many requests for explainable AI come from a
    place of fear or anxiety. Ideally, explainable AI would not be the way to calm
    such fears because it is counterproductive to the actual goals being solved. So
    why would anyone do any explainable AI of any form?
  prefs: []
  type: TYPE_NORMAL
- en: 'Two key things make explainable AI useful from a practical perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: Answering the question, explainable *to whom*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reaching explainable AI from the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, a real-world problem statement may describe the need to develop
    a scientific understanding of a physical or chemical process. With this goal,
    a useful explanation from the algorithm may be to generate an equation that produces
    the answers rather than producing the answers directly. With the equation, a physicist
    or chemist can inspect it for logical consistency and use it as a starting point
    for further scientific exploration.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the solution is explainable only to someone with significant expertise,
    but that is the only person who needs the explanation. The explanation in the
    form of an equation also directly tackles the problem of scientific understanding
    rather than merely understanding the inner workings of the AI algorithm. We do
    not have any explanation of how the AI came up with the equation itself, and the
    equation is (hopefully) a logically consistent form that explains the physical
    or chemical process.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example reflects the general situation in which we find explainable AI
    the most helpful: when it is used to aid a narrow and specific audience of potentially
    expert users in performing a very specific goal. For example, it is indeed common
    for data scientists to use explainable AI to help them figure out why a particular
    model is making a particular set of errors, even if the tools they use are not
    comprehensible to a nondata scientist audience.'
  prefs: []
  type: TYPE_NORMAL
- en: So if explainable AI is not a solution for building trust in an AI system or
    solution, what is? Unfortunately, there is no agreed-upon generic and rigorously
    evaluated way to build trust in AI. Our unoriginal suggestion is to focus on transparency,
    user evaluation, and the specifics of the use cases involved.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 How can you be transparent?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transparency can be as simple as informing users about the AI system that is
    being used: Which model was it designed with and, at a high level, how was it
    modified? If the system is meant to mimic a specific person (“Get tutored by Albert
    A.I. Einstein”) or a type of credentialed person (“Ask Dr. GPT about that mole
    on your back”), has that person or similarly credentialed person consented to
    this or approved its efficacy? How can the consumer verify this information?'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, enumerating these kinds of reasonable questions and their answers
    that an auditor or skeptical user might want to know will put you far ahead of
    the average in making your system more transparent. These do not need to be presented
    in detail to every user, but having a way for users to discover this information
    is helpful. It not only helps sophisticated users understand what is happening
    but also helps set the expectations of users in general about what is and is not
    possible with a given system. Furthermore, it is essential to inform users when
    they are interacting with a system that is generating automated responses. There
    is a big difference between trying to pretend a human is in control and thus should
    be able to solve any reasonable challenge versus an automated AI that you inform
    the customer has limited capability.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Aligning incentives with users
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Part of transparency and system presentation involves aligning the incentives
    involved. This isn’t just a feel-good statement about management practices but
    a practical unit of advice. Remember from chapter [4](../Text/chapter-4.html)
    that AI algorithms are greedy machines that optimize for what you ask, not what
    you intend. If you start building an LLM system where the incentives of the system
    are not well aligned with your broader goals, you risk overfitting to what you
    asked, not what both you and your users need.
  prefs: []
  type: TYPE_NORMAL
- en: With aligned incentives (e.g., our example of “try out the LLM and get $2 off
    your bill if it worked”), you are much more likely to have a positive outcome.
    They also give you more ways to advertise using an LLM as a mechanism for providing
    value to your customers instead of coming across as the evil people trying to
    outsource all the jobs. Presenting and discussing the aligned incentives between
    a business and its customers and how you are using LLMs to achieve those goals
    describes what needs to be said without any need for hiding the information.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.3 Incorporating feedback cycles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The world is not a static place. Things change, and what works today may not
    work tomorrow. This is one reason why you should have regular and continuous auditing
    of any automated AI/ML system: because they do not improve or adapt independently
    with experience.'
  prefs: []
  type: TYPE_NORMAL
- en: But it will also help you catch potentially negative feedback cycles, something
    you want to try to think about in advance. Negative feedback cycles are not always
    possible to predict. To help you catch these, try to think about which users will
    or won’t find the most benefit with a new system and what happens as that repeats
    over and over again.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we mentioned that speech-to-text and text-to-speech can be helpful
    for older customers or any hearing or movement-impaired customer. If we did not
    include such an option, we might alienate those customers over time, because every
    time they have a problem, they must use a physically difficult system.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you were a cell phone company that relied on family plans for some of
    your revenue. Your previously middle-aged customers who first bought your family
    plans are getting frustrated with your support system, so they move their entire
    family plan over to a new provider who puts in the extra work to ensure that the
    customer support process is accurate and efficient. Now you’re losing both your
    older and younger customers at once!
  prefs: []
  type: TYPE_NORMAL
- en: The point here is to think things through and train yourself to do these thought
    experiments. You will not catch every case, but you will improve. Regular auditing
    and testing then help you catch the failure cases, document them, and improve
    how you think about future situations and repeat problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs will have errors, and you first need to determine the risk and potential
    cost of errors to design an appropriate solution. If the risk and cost of errors
    are low, you can potentially use a normal chatbot-style LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to control the risk of using an LLM by changing how users interact
    with the system or shifting automation to a different part of the business process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Including a “human in the loop” to supervise an LLM creates automation bias
    risk, even when using techniques such as RAG to reduce the risk of errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can convert text into embeddings, numeric representations where similar
    sentences receive similar values. This allows you to use additional machine learning
    approaches, including classic techniques like clustering and outlier detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While LLMs can explain their decisions, their explanations are often ineffective
    because people become dependent on them. Instead, focus on producing explanations
    to satisfy a specific need or use case rather than generic “needing to explain.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design your system’s incentives to align with your user’s incentives. This is
    both a good way to avoid mistakes from an LLM optimizing for what you asked instead
    of what you intended and a good way to communicate and present your LLM to users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
