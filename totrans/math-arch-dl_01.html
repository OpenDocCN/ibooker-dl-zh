<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="ch-fVectors-matrices-tensors-in-ml">2 Vectors, matrices, and tensors in machine learning</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Vectors and matrices and their role in datascience</li>
<li class="co-summary-bullet">Working with eigenvalues and eigenvectors</li>
<li class="co-summary-bullet">Finding the axes of a hyper-ellipse</li>
</ul>
<p class="body"><a id="marker-18"/>At its core, machine learning, and indeed all computer software, is about number crunching. We input a set of numbers into the machine and get back a different set of numbers as output. However, this cannot be done randomly. It is important to organize these numbers appropriately and group them into meaningful objects that go into and come out of the machine. This is where vectors and matrices come in. These are concepts that mathematicians have been using for centuries—we are simply reusing them in machine learning.</p>
<p class="body">In this chapter, we will study vectors and matrices, primarily from a machine learning point of view. Starting from the basics, we will quickly graduate to advanced concepts, restricting ourselves to topics relevant to machine learning.</p>
<p class="body">We provide Jupyter Notebook-based Python implementations for most of the concepts discussed in this and other chapters. Complete, fully functional code that can be downloaded and executed (after installing Python and Jupyter Notebook) can be found at <a class="url" href="http://mng.bz/KMQ4">http://mng.bz/KMQ4</a>. The code relevant to this chapter can be found at <a class="url" href="http://mng.bz/d4nz">http://mng.bz/d4nz</a>.</p>
<h2 class="fm-head" id="sec-vectors">2.1 Vectors and their role in machine learning</h2>
<p class="body">Let’s revisit the machine learning model for a cat brain introduced in section <a class="url" href="../Text/01.xhtml#sec-cat_brain">1.3</a>. It takes two numbers as input, representing the hardness and sharpness of the object in front of the cat. The cat brain processes the input and generates an output threat score that leads to a decision to <i class="fm-italics">run away</i> or <i class="fm-italics">ignore</i> or <i class="fm-italics">approach and purr</i>. The two input numbers usually appear together, and it will be handy to group them into a single object. This object will be an ordered sequence of two numbers, the first representing hardness and the second representing sharpness. Such an object is a perfect example of a vector.<a id="marker-19"/></p>
<p class="body">Thus, a <i class="fm-italics">vector</i> can be thought of as an ordered sequence of two or more numbers, also known as an <i class="fm-italics">array</i> of numbers.<a class="url" href="02.xhtml#fn4" id="fnref4"><sup class="fm-superscript">1</sup></a> Vectors constitute a compact way of denoting a set of numbers that together represent some entity. In this book, vectors are represented by lowercase letters with an overhead arrow and arrays by square brackets. For instance, the input to the cat brain model in section <a class="url" href="../Text/01.xhtml#sec-cat_brain">1.3</a> was a vector <span class="infigure"><img alt="" class="calibre5" height="75" src="../../OEBPS/Images/eq_02-00-a2.png" width="65"/></span>, where <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub></span> represented hardness and <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span> represented sharpness.</p>
<p class="body">Outputs to machine learning models are also often represented as vectors. For instance, consider an object recognition model that takes an image as input and emits a set of numbers indicating the probabilities that the image contains a dog, human, or cat, respectively. The output of such a model is a three element vector <span class="infigure"><img alt="" class="calibre5" height="109" src="../../OEBPS/Images/eq_02-00-b2.png" width="65"/></span>, where the number <span class="math"><i class="timesitalic">y</i><sub class="fm-subscript">0</sub></span> denotes the probability that the image contains a dog, <span class="math"><i class="timesitalic">y</i><sub class="fm-subscript">1</sub></span> denotes the probability that the image contains a human, and <span class="math"><i class="timesitalic">y</i><sub class="fm-subscript">2</sub></span> denotes the probability that the image contains a cat. Figure <a class="url" href="02.xhtml#fig-vec_out">2.1</a> shows some possible input images and corresponding output vectors.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="515" id="fig-vec_out" src="../../OEBPS/Images/CH02_F01_Chaudhury.png" width="513"/></p>
<p class="figurecaption">Figure 2.1 Input images and corresponding output vectors denoting probabilities that the image contains a dog and/or human and/or cat, respectively. Example output vectors are shown.</p>
</div>
<p class="body">In multilayered machines like neural networks, the input and output to a layer can be vectors. We also typically represent the parameters of the model function (see section <a class="url" href="../Text/01.xhtml#sec-cat_brain">1.3</a>) as vectors. This is illustrated in section <a class="url" href="02.xhtml#sec-matrices">2.3</a>.</p>
<p class="fm-table-caption">Table 2.1 Toy documents and corresponding feature vectors describing them. Words eligible for the feature vector are bold. The first element of the feature vector indicates the number of occurrences of the word <i class="fm-italics">gun</i> and the second <i class="fm-italics">violence</i>.<a id="marker-20"/></p>
<table border="1" class="contenttable-1-table" id="tab-doc-vec" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="10%"/>
<col class="contenttable-0-col" span="1" width="70%"/>
<col class="contenttable-0-col" span="1" width="20%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Docid</p>
</th>
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Document</p>
</th>
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Feature vector</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math"><i class="timesitalic">d</i><sub class="fm-subscript">0</sub></span></p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">Roses are lovely. Nobody hates roses.</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">[0 0]</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math"><i class="timesitalic">d</i><sub class="fm-subscript">1</sub></span></p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><b class="fm-bold">Gun violence</b> has reached an epidemic proportion in America.</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">[1 1]</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math"><i class="timesitalic">d</i><sub class="fm-subscript">2</sub></span></p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">The issue of <b class="fm-bold">gun violence</b> is really over-hyped. One can find many instances of <b class="fm-bold">violence</b>, where no <b class="fm-bold">guns</b> were involved.</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">[2 2]</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math"><i class="timesitalic">d</i><sub class="fm-subscript">3</sub></span></p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><b class="fm-bold">Guns</b> are for <b class="fm-bold">violence</b> prone people. <b class="fm-bold">Violence</b> begets <b class="fm-bold">guns</b>. <b class="fm-bold">Guns</b> beget <b class="fm-bold">violence</b>.</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">[3 3]</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math"><i class="timesitalic">d</i><sub class="fm-subscript">4</sub></span></p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">I like <b class="fm-bold">guns</b> but I hate <b class="fm-bold">violence</b>. I have never been involved in <b class="fm-bold">violence</b>. But I own many <b class="fm-bold">guns</b>. <b class="fm-bold">Gun violence</b> is incomprehensible to me. I do believe <b class="fm-bold">gun</b> owners are the most anti <b class="fm-bold">violence</b> people on the planet. He who never uses a <b class="fm-bold">gun</b> will be prone to senseless <b class="fm-bold">violence</b>.</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">[5 5]</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math"><i class="timesitalic">d</i><sub class="fm-subscript">5</sub></span></p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><b class="fm-bold">Guns</b> were used in a armed robbery in San Francisco last night.</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">[1 0]</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math"><i class="timesitalic">d</i><sub class="fm-subscript">6</sub></span></p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">Acts of <b class="fm-bold">violence</b> usually involves a weapon.</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">[0 1]</span></p>
</td>
</tr>
</tbody>
</table>
<p class="body">One particularly significant notion in machine learning and data science is the idea of a <i class="fm-italics">feature vector</i>. This is essentially a vector that describes various properties of the object being dealt with in a particular machine learning problem. We will illustrate the idea with an example from the world of natural language processing (NLP). Suppose we have a set of documents. We want to create a document retrieval system where, given a new document, we have to retrieve similar documents in the system. This essentially boils down to estimating the similarity between documents in a quantitative fashion. We will study this problem in detail later, but for now, we want to note that the most natural way to approach this is to create feature vectors for each document that quantitatively describe the document. In section <a class="url" href="02.xhtml#subsection-dot_product">2.5.6</a>, we will see how to measure the similarity between these vectors; here, let’s focus on simply creating descriptor vectors for the documents. A popular way to do this is to choose a set of interesting words (we typically exclude words like “and,” “if,” and “to” that are present in all documents from this list), count the number of occurrences of those interesting words in each document, and make a vector of those values. Table <a class="url" href="02.xhtml#tab-doc-vec">2.1</a> shows a toy example with six documents and corresponding feature vectors. For simplicity, we have considered only two of the possible set of words: <i class="fm-italics">gun</i> and <i class="fm-italics">violence</i>, plural or singular, uppercase or lowercase.</p>
<p class="body">As a different example, the sequence of pixels in an image can also be viewed as a feature vector. Neural networks in computer vision tasks usually expect this feature vector.</p>
<h3 class="fm-head1" id="subsec-vec-geom-ml">2.1.1 The geometric view of vectors and its significance in machine learning</h3>
<p class="body"><a id="marker-21"/>Vectors can also be viewed geometrically. The simplest example is a two-element vector <span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_02-00-c2.png" width="120"/></span>. Its two elements can be taken to be <i class="timesitalic">x</i> and <i class="timesitalic">y</i>, Cartesian coordinates in a two-dimensional space, in which case the vector corresponds to a point in that space. <i class="fm-italics">Vectors with n elements represent points in an n-dimensional space</i>. The ability to see inputs and outputs of a machine learning model as points allows us to view the model itself as a geometric transformation that maps input points to output points in some high-dimensional space. We have already seen this in section <a class="url" href="../Text/01.xhtml#sec-geom-view-ml">1.4</a>. It is an enormously powerful concept we will use throughout the book.</p>
<p class="body">A vector represents a point in space. Also, an array of coordinate values like <span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_02-00-d2.png" width="21"/></span> describes the position of one point <i class="fm-italics">in a given coordinate system</i>. Hence, an array (of coordinate values) can be viewed as the quantitative representation of a vector. See figure <a class="url" href="02.xhtml#fig-vector_diagram">2.2</a> to get an intuitive understanding of this.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="252" id="fig-vector_diagram" src="../../OEBPS/Images/CH02_F02_Chaudhury.png" width="491"/></p>
<p class="figurecaption">Figure 2.2 A vector describing the position of point P with respect to point O. The basic mental picture is an arrowed line. This agrees with the definition of a vector that you may have learned in high school: a vector has a magnitude (length of the arrowed line) and direction (indicated by the arrow). On a plane, this is equivalent to the ordered pair of numbers <i class="timesitalic">x</i>, <i class="timesitalic">y</i>, where the geometric interpretations of <i class="timesitalic">x</i> and <i class="timesitalic">y</i> are as shown in the figure. In this context, it is worthwhile to note that only the relative positions of the points O and P matter. If both the points are moved, keeping their relationship intact, the vector does not change.</p>
</div>
<p class="body">For a real life example, consider the plane of a page of this book. Suppose we want to reach the top-right corner point of the page from the bottom-left corner. Let’s call the bottom-left corner <i class="timesitalic">O</i> and the top-right corner <i class="timesitalic">P</i>. We can travel the width (8.5 inches) to the right to reach the bottom-left corner and then travel the height (11 inches) upward to reach the top-right corner. Thus, if we choose a coordinate system with the bottom-left corner as the origin and the <i class="timesitalic">X</i>-axis along the width, and the <i class="timesitalic">Y</i>-axis along the height, point <i class="timesitalic">P</i> corresponds to the array representation <span class="infigure"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_02-00-e2.png" width="39"/></span>. But we could also travel along the diagonal from the bottom-left to the top-right corner to reach <i class="timesitalic">P</i> from <i class="timesitalic">O</i>. Either way, we end up at the same point <i class="timesitalic">P</i>.</p>
<p class="body">This leads to a conundrum. The vector <span class="infigure"><img alt="" class="calibre15" height="24" src="../../OEBPS/Images/AR_OP.png" width="32"/></span> represents the abstract geometric notion “position of <i class="timesitalic">P</i> with respect to <i class="timesitalic">O</i>” independent of our choice of coordinate axes. On the other hand, the array representation depends on the choice of a coordinate system. For example, the array <span class="infigure"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_02-00-e2.png" width="39"/></span> represents the top-right corner point <i class="timesitalic">P</i> only under a specific choice of coordinate axes (parallel to the sides of the page) and a reference point (bottom-left corner). Ideally, to be unambiguous, we should specify the coordinate system along with the array representation. Why don’t we ever do this in machine learning? Because in machine learning, it doesn’t exactly matter what the coordinate system is as long as we stick to any fixed coordinate system. Machine learning is about minimizing loss functions (which we will study later). As such, absolute positions of point are immaterial, only relative positions matter.</p>
<p class="body">There are explicit rules (which we will study later) that state how the vector transforms when the coordinate system changes. We will invoke them when necessary. All vectors used in a machine learning computation must consistently use the same coordinate system or be transformed appropriately.<a id="marker-22"/></p>
<p class="body">One other point: planar spaces, such as the plane of the paper on which this book is written, are two-dimensional (2D). The mechanical world we live in is three-dimensional (3D). Human imagination usually fails to see higher dimensions. In machine learning and data science, we often talk of spaces with thousands of dimensions. You may not be able to see those spaces in your mind, but that is not a crippling limitation. You can use 3D analogues in your head. They work in a surprisingly large variety of cases. However, it is important to bear in mind that this is not always true. Some examples where the lower-dimensional intuitions fail at higher dimensions will be shown later.</p>
<h2 class="fm-head" id="python-vector-intro">2.2 PyTorch code for vector manipulations</h2>
<p class="body">PyTorch is an open source machine learning library developed by Facebook’s artificial intelligence group. It is one of the most elegant practical tools for developing deep learning applications at present. In this book, we aim to familiarize you with PyTorch and similar programming paradigms alongside the relevant mathematics. Knowledge of Python basics will be assumed. You are strongly encouraged to try out all the code snippets in this book (after installing the appropriate packages like PyTorch, that is).</p>
<p class="body">All the Python code in this book is produced via Jupyter Notebook. A summary of the theoretical material presented in the code is provided before the code snippet.</p>
<h3 class="fm-head1" id="pytorch-code-for-the-introduction-to-vectors">2.2.1 PyTorch code for the introduction to vectors</h3>
<p class="body">Listing 2.1 shows how to create and access vectors and subvectors and slice and dice vectors using PyTorch.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code demonstrating how to create a vector and access its elements, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/xm8q">http://mng.bz/xm8q</a>.</p>
<p class="fm-code-listing-caption" id="listing-2.1-introduction-to-vectors-via-pytorch">Listing 2.1 Introduction to vectors via PyTorch</p>
<pre class="programlisting">v = torch.tensor([0.11, 0.01, 0.98, 0.12, 0.98,  <span class="fm-combinumeral">①</span>
                ,0.85, 0.03, 0.55, 0.49, 0.99,
                0.02, 0.31, 0.55, 0.87, 0.63],
                dtype=torch.float64)             <span class="fm-combinumeral">②</span>

first_element = v[0]
third_element = v[2]                             <span class="fm-combinumeral">③</span>

last_element = v[-1]
second_last_element = v[-2]                      <span class="fm-combinumeral">④</span>

second_to_fifth_elements = v[1:4]                <span class="fm-combinumeral">⑤</span>

first_to_third_elements = v[:2]
last_two_elements = v[-2:]                       <span class="fm-combinumeral">⑥</span>

num_elements_in_v = len(v)

u = np.array([0.11, 0.01, 0.98, 0.12, 0.98, 0.85, 0.03,
              0.55, 0.49, 0.99, 0.02, 0.31, 0.55, 0.87,
              0.63])

u = torch.from_numpy(u)                          <span class="fm-combinumeral">⑦</span>

diff = v.sub(u)                                  <span class="fm-combinumeral">⑧</span>

u1 = u.numpy()                                   <span class="fm-combinumeral">⑨</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> torch.tensor represents a multidimensional array. The vector is a 1D tensor that can be initialized by directly specifying values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Tensor elements are floats by default. We can force tensors to be other types such as float64 (double).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The square bracket operator lets us access individual vector elements.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Negative indices count from the end of the array. –1 denotes the last element. -2 denotes the second-to-last element.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The colon operator slices off a range of elements from the vector.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Nothing before a colon denotes the beginning of~the~array. Nothing after a colon denotes the end~of~the array.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Torch tensors can be initialized from NumPy arrays.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> The difference between the Torch tensor and its NumPy version is zero.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Torch tensors can be converted to NumPy arrays.</p>
<h2 class="fm-head" id="sec-matrices">2.3 Matrices and their role in machine learning</h2>
<p class="body"><a id="marker-23"/>Sometimes it is not sufficient to group a set of numbers into a vector. We have to collect several vectors into another group. For instance, consider the input to training a machine learning model. Here we have several input instances, each consisting of a sequence of numbers. As seen in section <a class="url" href="02.xhtml#sec-vectors">2.1</a>, the sequence of numbers belonging to a single input instance can be grouped into a vector. How do we represent the entire collection of input instances? This is where the concept of matrices comes in handy from the world of mathematics. A <i class="fm-italics">matrix</i> can be viewed as a rectangular array of numbers arranged in a fixed count of rows and columns. Each row of a matrix is a vector, and so is each column. Thus a matrix can be thought of as a collection of row vectors. It can also be viewed as a collection of column vectors. We can represent the entire set of numbers that constitute the training input to a machine learning model as a matrix, with each row vector corresponding to a single training instance.</p>
<p class="body">Consider our familiar cat-brain problem again. As stated earlier, a single input instance to the machine is a vector <span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_02-00-f2.png" width="63"/></span>, where <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub></span> describes the hardness of the object in front of the cat. Now consider a training dataset with many such input instances, each with a known output threat score. You might recall from section <a class="url" href="../Text/01.xhtml#sec-paradigm-shift">1.1</a> that the goal in machine learning is to create a function that maps these inputs to their respective outputs with as little overall error as possible. Our training data may look as shown in table <a class="url" href="02.xhtml#tab-cat-brain-training-data">2.2</a> (note that in real-life problems, the training dataset is usually large—often millions of input-output pairs—but in this toy problem, we will have <span class="math">8</span> training data instances).</p>
<p class="fm-table-caption">Table 2.2 Example training dataset for our toy machine learning–based cat brain</p>
<table border="1" class="contenttable-1-table" id="tab-cat-brain-training-data" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="10%"/>
<col class="contenttable-0-col" span="1" width="30%"/>
<col class="contenttable-0-col" span="1" width="30%"/>
<col class="contenttable-0-col" span="1" width="30%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th" colspan="1" rowspan="1"/>
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Input value: Hardness</p>
</th>
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Input value: Sharpness</p>
</th>
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Output: Threat score</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.11</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.09</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">−</span>0.8</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.01</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.02</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">−</span>0.97</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">2</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.98</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.91</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.89</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">3</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.12</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.21</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">−</span>0.68</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">4</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.98</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.99</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.95</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">5</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.85</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.87</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.74</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">6</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.03</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.14</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">−</span>0.88</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">7</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.55</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.45</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.00</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-24"/>From table <a class="url" href="02.xhtml#tab-cat-brain-training-data">2.2</a>, we can collect the columns corresponding to hardness and sharpness into a matrix, as shown in equation <a class="url" href="02.xhtml#eq-cat-brain-toy-training-dataset">2.1</a>—this is a compact representation of the training dataset for this problem. <a class="url" href="02.xhtml#fn5" id="fnref5"><sup class="fm-superscript">2</sup></a></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="265" src="../../OEBPS/Images/eq_02-01.png" width="432"/></p>
</div>
<p class="fm-equation-caption">Equation 2.1 <span class="calibre" id="eq-cat-brain-toy-training-dataset"/></p>
<p class="body">Each row of matrix <i class="timesitalic">X</i> is a particular input instance. Different rows represent different input instances. On the other hand, different columns represent different feature elements. For example, the 0th row of matrix <i class="timesitalic">X</i> is the vector <span class="math">[<i class="fm-italics">x</i><sub class="fm-subscript">00</sub>    <i class="fm-italics">x</i><sub class="fm-subscript">01</sub>]</span> representing the 0th input instance. Its elements, <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">00</sub></span> and <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">01</sub></span> represent different feature elements, hardness and sharpness respectively of the 0th training input instance.</p>
<h3 class="fm-head1" id="subsubsec-mat-rep-img">2.3.1 Matrix representation of digital images</h3>
<p class="body"><a id="marker-25"/>Digital images are also often represented as matrices. Here, each element represents the brightness at a specific pixel position (<span class="math"><i class="fm-italics">x</i>, <i class="fm-italics">y</i></span> coordinate) of the image. Typically, the brightness value is normalized to an integer in the range <span class="math">0</span> to <span class="math">255</span>. <span class="math">0</span> is black, <span class="math">255</span> is white, and <span class="math">128</span> is gray.<a class="url" href="02.xhtml#fn6" id="fnref6"><sup class="fm-superscript">3</sup></a> Following is an example of a tiny image, <span class="math">9</span> pixels wide and <span class="math">4</span> pixels high:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="122" src="../../OEBPS/Images/eq_02-02.png" width="499"/></p>
</div>
<p class="fm-equation-caption">Equation 2.2 <span class="calibre" id="eq-tiny_im"/></p>
<p class="body">The brightness increases gradually from left to right and also from top to bottom. <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">00</sub></span> represents the top-left pixel, which is black. <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">3, 8</sub></span> represents the bottom-right pixel, which is white. The intermediate pixels are various shades of gray between black and white. The actual image is shown in figure <a class="url" href="02.xhtml#fig-tiny_im">2.2</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="215" src="../../OEBPS/Images/CH02_F03_Chaudhury.png" width="483"/></p>
<p class="figurecaption" id="fig-tiny_im">Figure 2.3 Image corresponding to matrix <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">4, 9</sub></span> in equation <a class="url" href="02.xhtml#eq-tiny_im">2.2</a></p>
</div>
<h2 class="fm-head" id="python-matrix-intro">2.4 Python code: Introducing matrices, tensors, and images via PyTorch</h2>
<p class="body">For programming purposes, you can think of tensors as multidimensional arrays. Scalars are zero-dimensional tensors. Vectors are one-dimensional tensors. Matrices are two-dimensional tensors. RGB images are three-dimensional tensors (<span class="math"><i class="fm-italics">colorchannels</i> × <i class="fm-italics">height</i> × <i class="fm-italics">width</i></span>). A batch of <span class="math">64</span> images is a four-dimensional tensor (<span class="math">64 × <i class="fm-italics">colorchannels</i> × <i class="fm-italics">height</i> × <i class="fm-italics">width</i></span>).</p>
<p class="fm-code-listing-caption" id="listing-2.2-introducing-matrices-via-pytorch">Listing 2.2 Introducing matrices via PyTorch</p>
<pre class="programlisting">X = torch.tensor(                                      <span class="fm-combinumeral">①</span>
     [
        [0.11, 0.09], [0.01, 0.02], [0.98, 0.91],
        [0.12, 0.21], [0.98, 0.99], [0.85, 0.87],
        [0.03, 0.14], [0.55, 0.45]                     <span class="fm-combinumeral">②</span>
     ]
 )

print("Shape of the matrix is: {}".format(X.shape))    <span class="fm-combinumeral">③</span>

first_element = X[0, 0]                                <span class="fm-combinumeral">④</span>

row_0 = X[0, :]                                        <span class="fm-combinumeral">⑤</span>
row_1 = X[1, 0:2]                                      <span class="fm-combinumeral">⑥</span>

column_0 = X[:, 0]                                     <span class="fm-combinumeral">⑦</span>
column_1 = X[:, 1]                                     <span class="fm-combinumeral">⑧</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A matrix is a 2D array of numbers: i.e., a 2D tensor. The entire training data input set for a machine-learning model can be viewed as a matrix. Each input instance is one row. Row count <span class="math">≡</span> number of training examples, column count <span class="math">≡</span> training instance size</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Cat-brain training data input: 8 examples, each with two values (hardness, sharpness). An <span class="math">8 × 2</span> tensor is created by specifying values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The shape of a tensor is a list. For a matrix, the first list element is num rows; the second list element is num columns.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Square brackets extract individual matrix elements.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> A standalone colon operator denotes all possible indices.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> The colon operator denotes the range of indices.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> 0th column</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> 1st column</p>
<p class="fm-code-listing-caption" id="listing-2.3-slicing-and-dicing-matrices">Listing 2.3 Slicing and dicing matrices<a id="marker-26"/></p>
<pre class="programlisting">first_3_training_examples = X[:3, ]               <span class="fm-combinumeral">①</span>
                                                  <span class="fm-combinumeral">②</span>
print("Sharpness of 5-7 training examples is: {}"
       .format(X[5:8, 1]))                        <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Ranges of rows and columns can be specified via the colon operator to slice off (extract) submatrices.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Extracts the first three training examples (rows)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Extracts the sharpness feature for the 5th to 7th training examples</p>
<p class="fm-code-listing-caption" id="listing-2.4-tensors-and-images-in-pytorch">Listing 2.4 Tensors and images in PyTorch<a id="marker-27"/></p>
<pre class="programlisting">tensor = torch.rand((5, 5, 3))                                  <span class="fm-combinumeral">①</span>
                                                                <span class="fm-combinumeral">②</span>
I49 = torch.tensor([[0, 8, 16, 24, 32, 40, 48,  56,  64],       <span class="fm-combinumeral">③</span>
               [64,   72,  80,  88,  96, 104, 112, 120, 128],
                [128, 136, 144, 152, 160, 168, 176, 184, 192],
                [192, 200, 208, 216, 224, 232, 240, 248, 255]],
               )                                                <span class="fm-combinumeral">④</span>

img = torch.tensor(cv2.imread('../../Figures/dog3.jpg'))        <span class="fm-combinumeral">⑤</span>
img_b = img[:, :, 0]                                            <span class="fm-combinumeral">⑥</span>
img_g = img[:, :, 1]                                            <span class="fm-combinumeral">⑥</span>
img_r = img[:, :, 2]                                            <span class="fm-combinumeral">⑥</span>
img_cropped = img[0:100, 0:100, :]                              <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> PyTorch tensors can be used to represent tensors. A vector is a 1-tensor, a matrix is a 2-tensor, and a scalar is a 0-tensor.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a random tensor of specified dimensions</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> All images are tensors. An RGB image of height H, width W is a 3-tensor of shape <span class="math">[3, H, W]</span>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> <span class="math">4 × 9</span> single-channel image shown in figure <a class="url" href="02.xhtml#fig-tiny_im">2.3</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Reads a <span class="math">199 × 256 × 3</span> image from disk</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Usual slicing dicing operators work. Extracts the red, green, and blue channels of the image as shown in figure <a class="url" href="02.xhtml#fig-numpy-dog-grid">2.4</a>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Crops out a <span class="math">100 × 100</span> subimage as shown in figure <a class="url" href="02.xhtml#fig-numpy-crop-dog">2.5</a></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="868" id="fig-numpy-dog-grid" src="../../OEBPS/Images/CH02_F04_Chaudhury.png" width="977"/></p>
<p class="figurecaption">Figure 2.4 Tensors and images in PyTorch</p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre16" height="439" id="fig-numpy-crop-dog" src="../../OEBPS/Images/CH02_F05_Chaudhury.png" width="430"/></p>
<p class="figurecaption">Figure 2.5 Cropped image of dog</p>
</div>
<h2 class="fm-head" id="sec-misc_mat-vec">2.5 Basic vector and matrix operations in machine learning</h2>
<p class="body">In this section, we introduce several basic vector and matrix operations along with examples to demonstrate their significance in image processing, computer vision, and machine learning. It is meant to be an application-centric introduction to linear algebra. But it is <i class="fm-italics">not</i> meant to be a comprehensive review of matrix and vector operations, for which you are referred to a textbook on linear algebra.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="531" src="../../OEBPS/Images/CH02_F06_Chaudhury.png" width="237"/></p>
<p class="figurecaption" id="fig-tiny_im-transpose">Figure 2.6 Image corresponding to the transpose of matrix <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">4, 9</sub></span> shown in equation <a class="url" href="02.xhtml#eq-tiny_im_transpose">2.3</a>. This is equivalent to rotating the image by 90°.</p>
</div>
<h3 class="fm-head1" id="matrix-and-vector-transpose">2.5.1 Matrix and vector transpose</h3>
<p class="body"><a id="marker-28"/>In equation <a class="url" href="02.xhtml#eq-tiny_im">2.2</a>, we encountered the matrix <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">4, 9</sub></span> depicting a tiny image. Suppose we want to rotate the image by <span class="math">90°</span> so it looks like figure <a class="url" href="02.xhtml#fig-tiny_im-transpose">2.5</a>. The original matrix <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">4, 9</sub></span> and its transpose <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">4,</sub><i class="fm-italics"><sup class="fm-superscript">T</sup></i><sub class="fm-subscript">9</sub> = <i class="fm-italics">I</i><sub class="fm-subscript">9, 4</sub></span> are shown here:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="460" src="../../OEBPS/Images/eq_02-03.png" width="547"/></p>
</div>
<p class="fm-equation-caption">Equation 2.3 <span class="calibre" id="eq-tiny_im_transpose"/></p>
<p class="body">By comparing equation <a class="url" href="02.xhtml#eq-tiny_im">2.2</a> and equation <a class="url" href="02.xhtml#eq-tiny_im_transpose">2.3</a>, you can easily see that one can be obtained from the other by interchanging the row and column indices. This operation is generally known as <i class="fm-italics">matrix transposition</i>.</p>
<p class="body">Formally, the transpose of a matrix <i class="timesitalic">A<sub class="fm-subscript">m, n</sub></i> with <i class="timesitalic">m</i> rows and <i class="timesitalic">n</i> columns is another matrix with <i class="timesitalic">n</i> rows and <i class="timesitalic">m</i> columns. This transposed matrix, denoted <span class="math"><i class="fm-italics">A<sub class="fm-subscript">n,</sub><sup class="fm-superscript">T</sup><sub class="fm-subscript">m</sub></i></span>, is such that <span class="math"><i class="fm-italics">A<sup class="fm-superscript">T</sup></i>[<i class="fm-italics">i</i>, <i class="fm-italics">j</i>] = <i class="fm-italics">A</i>[<i class="fm-italics">j</i>, <i class="fm-italics">i</i>]</span>. For instance, the value at row <span class="math">0</span> column <span class="math">6</span> in matrix <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">4, 9</sub></span> is <span class="math">48</span>; in the transposed matrix, the same value appears in row <span class="math">6</span> and column <span class="math">0</span>. In matrix parlance, <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">4, 9</sub>[0,6] = <i class="fm-italics">I</i><sub class="fm-subscript">9,</sub><i class="fm-italics"><sup class="fm-superscript">T</sup></i><sub class="fm-subscript">4</sub>[6,0] = 48</span>.</p>
<p class="body">Vector transposition is a special case of matrix transposition (since all vectors are matrices—a column vector with <i class="timesitalic">n</i> elements is an <span class="math"><i class="fm-italics">n</i> × 1</span> matrix). For instance, an arbitrary vector and its transpose are shown next:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="110" src="../../OEBPS/Images/eq_02-04.png" width="59"/></p>
</div>
<p class="fm-equation-caption">Equation 2.4</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="40" src="../../OEBPS/Images/eq_02-05.png" width="127"/></p>
</div>
<p class="fm-equation-caption">Equation 2.5</p>
<h3 class="fm-head1" id="subsec-dotprod-ml">2.5.2 Dot product of two vectors and its role in machine learning</h3>
<p class="body"><a id="marker-29"/>In section <a class="url" href="../Text/01.xhtml#sec-cat_brain">1.3</a>, we saw the simplest of machine learning models where the output is generated by taking a weighted sum of the inputs (and then adding a constant bias value). This model/machine is characterized by the weights <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>, and bias <i class="timesitalic">b</i>. Take the rows of table <a class="url" href="02.xhtml#tab-cat-brain-training-data">2.2</a>. For example, for row <span class="math">0</span>, the input values are the hardness of the approaching object = <span class="math">0.11</span> and softness = <span class="math">0.09</span>. The corresponding model output will be <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> × 0.11 + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> × 0.09 + <i class="fm-italics">b</i></span>. In fact, the goal of training is to choose <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>, and <i class="timesitalic">b</i> such that model outputs are as close as possible to the known outputs; that is, <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> × 0.11 + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> × 0.09 + <i class="fm-italics">b</i></span> should be as close to <span class="math">−0.8</span> as possible, <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> × 0.01 + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> × 0.02 + <i class="fm-italics">b</i></span> should be as close to <span class="math">−0.97</span> as possible, that is,  in general, given an input instance <span class="infigure"><img alt="" class="calibre5" height="40" src="../../OEBPS/Images/eq_02-05-a.png" width="62"/></span>, the model output is <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">x</i><sub class="fm-subscript">0</sub><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">1</sub><i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">b</i></span>.</p>
<p class="body">We will keep returning to this model throughout the chapter. But first, let’s consider a different question. In this toy example, we have only three model parameters: two weights, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>, and one bias <i class="timesitalic">b</i>. Hence it is not very messy to write the model output flat out as <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">x</i><sub class="fm-subscript">0</sub><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">1</sub><i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">b</i></span>. But, with longer feature vectors (that is, more weights) it will become unwieldy. Is there a compact way to represent the model output for a specific input instance, irrespective of the size of the input?</p>
<p class="body">Turns out the answer is yes—we can use an operation called <i class="fm-italics">dot product</i> from the world of mathematics. We have already seen in section <a class="url" href="02.xhtml#sec-vectors">2.1</a> that an individual instance of model input can be compactly represented by a vector, say <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> (it can have any number of input values). We can also represent the set of weights as vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>—it will have the same number of items as the input vector. The dot product is simply the element-wise multiplication of the two vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>. Formally, given two vectors and <span class="infigure"><img alt="" class="calibre5" height="94" src="../../OEBPS/Images/eq_02-05-b2.png" width="64"/></span> and <span class="infigure"><img alt="" class="calibre5" height="94" src="../../OEBPS/Images/eq_02-05-c2.png" width="69"/></span>, the dot product of the two vectors is defined as</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="25" src="../../OEBPS/Images/eq_02-06.png" width="239"/></p>
</div>
<p class="fm-equation-caption">Equation 2.6 <span class="calibre" id="eq-dot-product"/></p>
<p class="body"><a id="marker-30"/>In other words, the sum of the products of corresponding elements of the two vectors is the dot product of the two vectors, denoted <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> ⋅ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The dot product notation can compactly represent the model output as <span class="math"><i class="fm-italics">y</i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> ⋅ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i></span>. The representation does not increase in size even when the number of inputs and weights is large.</p>
<p class="body">Consider our (by now familiar) cat-brain example again. Suppose the weight vector is <span class="infigure"><img alt="" class="calibre5" height="50" src="../../OEBPS/Images/eq_02-06-a2.png" width="57"/></span> and the bias value <span class="math"><i class="fm-italics">b</i> = 5</span>. Then the model output for the 0th input instance from table <a class="url" href="02.xhtml#tab-cat-brain-training-data">2.2</a> will be <span class="infigure"><img alt="" class="calibre5" height="50" src="../../OEBPS/Images/eq_02-06-b2.png" width="341"/></span>. It is another matter that these are bad choices for weight and bias parameters, since the model output <span class="math">5.51</span> is a far cry from the desired output <span class="math">−0.89</span>. We will soon see how to obtain better parameter values. For now, we just need to note that the dot product offers a neat way to represent the simple weighted sum model output.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The dot product is defined only if the vectors have the same dimensions.</p>
<p class="body">Sometimes the dot product is also referred to as <i class="fm-italics">inner product</i>, denoted <span class="math"><span class="segoe">⟨</span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><span class="segoe">⟩</span></span>. Strictly speaking, the phrase <i class="fm-italics">inner product</i> is a bit more general; it applies to infinite-dimensional vectors as well. In this book, we will often use the terms interchangeably, sacrificing mathematical rigor for enhanced understanding.</p>
<h3 class="fm-head1" id="subsec-matmul-ml">2.5.3 Matrix multiplication and machine learning</h3>
<p class="body">Vectors are special cases of matrices. Hence, matrix-vector multiplication is a special case of matrix-matrix multiplication. We will start with that.</p>
<p class="fm-head2" id="matrix-vector-multiplication">Matrix-vector multiplication</p>
<p class="body">In section <a class="url" href="02.xhtml#subsec-dotprod-ml">2.5.2</a>, we saw that given a weight vector, say <span class="infigure"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_02-06-c2.png" width="59"/></span>, and the bias value <span class="math"><i class="fm-italics">b</i> = 5</span>, the weighted sum model output upon a single input instance, say <span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-06-d2.png" width="48"/></span>, can be represented using a vector-vector dot product <span class="infigure"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_02-06-e2.png" width="206"/></span>. As depicted in equation <a class="url" href="02.xhtml#eq-cat-brain-toy-training-dataset">2.1</a>, during training, we are dealing with many training data instances at the same time. In real life, we typically deal with hundreds of thousands of input instances, each having hundreds of values. Is there a way to represent the model output for the entire training dataset compactly, such that it is independent of the count of input instances and their sizes?<a id="marker-31"/></p>
<p class="body">The answer turns out to be yes. We can use the idea of matrix-vector multiplication from the world of mathematics. The product of a matrix <i class="timesitalic">X</i> and column vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> is another vector, denoted <span class="math"><i class="fm-italics">X</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span></span>. Its elements are the dot products between the row vectors of <i class="timesitalic">X</i> and the column vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>. For example, given the model weight vector <span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_02-06-f2.png" width="58"/></span> and the bias value <span class="math"><i class="fm-italics">b</i> = 5</span>, the outputs on the toy training dataset of our familiar cat-brain model (equation <a class="url" href="02.xhtml#eq-cat-brain-toy-training-dataset">2.1</a>) can be obtained via the following steps:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="252" src="../../OEBPS/Images/eq_02-07.png" width="379"/></p>
</div>
<p class="fm-equation-caption">Equation 2.7 <span class="calibre" id="eq-cat-brain-nobiased"/></p>
<p class="body">Adding the bias value of <span class="math">5</span>, the model output on the toy training dataset is</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="255" src="../../OEBPS/Images/eq_02-08.png" width="135"/></p>
</div>
<p class="fm-equation-caption">Equation 2.8 <span class="calibre" id="eq-cat-brain-output"/></p>
<p class="body">In general, the output of our simple model (biased weighted sum of input elements) can be expressed compactly as <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="timesitalic">X</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>.</p>
<p class="fm-head2" id="matrix-matrix-multiplication">Matrix-matrix multiplication</p>
<p class="body"><a id="marker-32"/>Generalizing the notion of matrix times vector, we can define matrix times matrix. A matrix with <i class="timesitalic">m</i> rows and <i class="timesitalic">p</i> columns, say <i class="timesitalic">A<sub class="fm-subscript">m, p</sub></i>, can be multiplied with another matrix with <i class="timesitalic">p</i> rows and <i class="timesitalic">n</i> columns, say <i class="timesitalic">B<sub class="fm-subscript">p, n</sub></i>, to generate a matrix with <i class="timesitalic">m</i> rows and <i class="timesitalic">n</i> columns, say <i class="timesitalic">C<sub class="fm-subscript">m, n</sub></i>: for example, <span class="math"><i class="fm-italics">C<sub class="fm-subscript">m, n</sub></i> = <i class="fm-italics">A<sub class="fm-subscript">m, p</sub></i> <i class="fm-italics">B<sub class="fm-subscript">p, n</sub></i></span>. Note that the number of columns in the left matrix must match the number of rows in the right matrix. Element <i class="timesitalic">i, j</i> of the result matrix, <i class="timesitalic">C<sub class="fm-subscript">i, j</sub></i>, is obtained by point-wise multiplication of the elements of the <i class="timesitalic">i</i>th row vector of <i class="timesitalic">A</i> and the <i class="timesitalic">j</i>th column vector of <i class="timesitalic">B</i>. The following example illustrates the idea:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="472" src="../../OEBPS/Images/eq_02-08-a.png" width="452"/></p>
</div>
<p class="body">The computation for <span class="math"><i class="fm-italics">C</i><sub class="fm-subscript">2, 1</sub></span> is shown via bolding by way of example.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Matrix multiplication is not commutative. In general, <span class="math"><i class="fm-italics">AB</i> ≠ <i class="fm-italics">BA</i></span>.</p>
<p class="body">At this point, the astute reader may already have noted that the dot product is a special case of matrix multiplication. For instance, the dot product between two vectors <span class="infigure"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_02-08-b2.png" width="69"/></span> and <span class="infigure"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_02-08-c2.png" width="63"/></span> is equivalent to transposing either of the two vectors and then doing a matrix multiplication with the other. In other words,</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="78" src="../../OEBPS/Images/eq_02-08-d.png" width="516"/></p>
</div>
<p class="body">The idea works in higher dimensions, too. In general, given two vectors <span class="infigure"><img alt="" class="calibre5" height="83" src="../../OEBPS/Images/eq_02-08-e2.png" width="64"/></span> and <span class="infigure"><img alt="" class="calibre5" height="83" src="../../OEBPS/Images/eq_02-08-f2.png" width="70"/></span>, the dot product of the two vectors is defined as</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="344" src="../../OEBPS/Images/eq_02-09.png" width="304"/></p>
</div>
<p class="fm-equation-caption">Equation 2.9 <span class="calibre" id="eq-dotprod-matmul"/></p>
<p class="body">Another special case of matrix multiplication is row-vector matrix multiplication. For example, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span></span> or</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="110" src="../../OEBPS/Images/eq_02-09-a.png" width="678"/></p>
</div>
<p class="fm-head2" id="transpose-of-matrix-products">Transpose of matrix products</p>
<p class="body"><a id="marker-33"/>Given two matrices <i class="timesitalic">A</i> and <i class="timesitalic">B</i>, where the number of columns in <i class="timesitalic">A</i> matches the number of rows in <i class="timesitalic">B</i> (that is, it is possible to multiply them), the transpose of the product is the product of the individual transposes, in reversed order. The rule also applies to matrix-vector multiplication. The following equations capture this rule:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="111" src="../../OEBPS/Images/eq_02-10.png" width="133"/></p>
</div>
<p class="fm-equation-caption">Equation 2.10 <span class="calibre" id="eq-mat-prod-transpose"/></p>
<h3 class="fm-head1" id="subsection-vector_length">2.5.4 Length of a vector (L2 norm): Model error</h3>
<p class="body">Imagine that a machine learning model is supposed to output a target value <i class="timesitalic">ȳ</i>, but it outputs <i class="timesitalic">y</i> instead. We are interested in the <i class="fm-italics">error</i> made by the model. The error is the difference between the target and the actual outputs.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Squared error</p>
<p class="fm-sidebar-text">When a computing error occurs, we are only interested in how far the computed value is from ideal. We do not care whether the computed value is bigger or smaller than ideal. For instance, if the target (ideal) value is 2, the computed values 1.5 and 2.5 are equally in error—we are equally happy or unhappy with either of them. Hence, it is common practice to <i class="fm-italics">square</i> error values. Thus for instance, if the target value is 2 and the computed value is 1.5, the error is <span class="math">(1.5 − 2)<sup class="fm-superscript">2</sup> = 0.25</span>. If the target value is 2.5, the error is <span class="math">(2.5 − 2)<sup class="fm-superscript">2</sup> = 0.25</span>. The squaring operation essentially eliminates the sign of the error value. We can then follow it up with a square root, but it is OK not to.</p>
<p class="fm-sidebar-text">You might ask, “But wait: squaring alters the value of the quantity. Don’t we care about the exact value of the error?” The answer is, we usually don’t; we only care about <i class="fm-italics">relative</i> values of errors. If the target is 2, we want the error for an output value of, say, 2.1 to be less than the error for an output value of 2.5; the exact values of the errors do not matter.</p>
</div>
<p class="body"><a id="marker-34"/>Let’s apply this idea of squaring to machine learning model error. As seen earlier in section <a class="url" href="02.xhtml#subsec-matmul-ml">2.5.3</a>, given a model weight vector, say <span class="infigure"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_02-10-a2.png" width="58"/></span>, and the bias value <span class="math"><i class="fm-italics">b</i> = 5</span>, the weighted sum model output upon a single input instance, say <span class="infigure"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_02-10-b2.png" width="48"/></span>, is <span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_02-10-c2.png" width="173"/></span>. The corresponding target (ideal) output, from table <a class="url" href="02.xhtml#tab-cat-brain-training-data">2.2</a>, is <span class="math">−0.8</span>. The squared error <span class="math"><i class="fm-italics">e</i><sup class="fm-superscript">2</sup> = (−0.8−5.51)<sup class="fm-superscript">2</sup> = 39.82</span> gives us an idea of how good or bad the model parameters <span class="math">3</span>, <span class="math">2</span>, <span class="math">5</span> are. For instance, if we instead use a weight vector <span class="infigure"><img alt="" class="calibre5" height="49" src="../../OEBPS/Images/eq_02-10-d2.png" width="58"/></span> and bias value <span class="math">−1</span>, we get model output <span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_02-10-e2.png" width="264"/></span>. The output is exactly the same as the target. The corresponding squared error <span class="math"><i class="fm-italics">e</i><sup class="fm-superscript">2</sup> = (−0.8−(−0.8))<sup class="fm-superscript">2</sup> = 0</span>. This (zero error) immediately tells us that <span class="math">1</span>, <span class="math">1</span>, <span class="math">−1</span> are much better choices of model parameters than <span class="math">3</span>, <span class="math">2</span>, <span class="math">5</span>.</p>
<p class="body">In general, the error made by a biased weighted sum model can be expressed as follows. If <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> denotes the weight vector and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> denotes the bias, the output corresponding to an input instance <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> can be expressed as <span class="math"><i class="fm-italics">y</i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> ⋅ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i></span>. Let <i class="timesitalic">ȳ</i> denote the corresponding target (ground truth). Then the error is defined as <span class="math"><i class="fm-italics">e</i> = (<i class="fm-italics">y</i>−<i class="fm-italics">ȳ</i>)<sup class="fm-superscript">2</sup></span>.</p>
<p class="body">Thus we see that we can compute the error on a single training instance by taking the difference between the model output and the ground truth and squaring it. How do we extend this concept over the entire training dataset? The set of outputs corresponding to the entire set of training inputs can be expressed as the output vector <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">X</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>. The corresponding target output vector, consisting of the entire set of ground truths can be expressed as <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span>. The differences between the target and model output over the entire training set can be expressed as another vector <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span> - <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span></span>. In our particular example:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="263" src="../../OEBPS/Images/eq_02-10-f.png" width="574"/></p>
</div>
<p class="body"><a id="marker-35"/>Thus the total error over the entire training dataset is obtained by taking the difference between the output and the ground truth vector, squaring its elements and adding them up. Recalling equation <a class="url" href="02.xhtml#eq-dotprod-matmul">2.9</a>, this is exactly what will happen if we take the <i class="fm-italics">dot product of the difference vector with itself</i>. That happens to be the definition of the <i class="fm-italics">squared magnitude</i> or <i class="fm-italics">length</i> or <i class="fm-italics">L2 norm</i> of a vector: the dot product of the vector with itself. In the previous example, the overall training (squared) error is:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="282" src="../../OEBPS/Images/eq_02-10-g.png" width="645"/></p>
</div>
<p class="body">Formally, the length of a vector <span class="infigure"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_02-10-h2.png" width="61"/></span>, denoted <span class="math">||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span>||</span>, is defined as <span class="infigure"><img alt="" class="calibre5" height="38" src="../../OEBPS/Images/eq_02-10-i2.png" width="250"/></span>. This quantity is sometimes called the L2 norm of the vector.</p>
<p class="body">In particular, given a machine learning model with output vector <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> and a target vector <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span>, the error is the same as the magnitude or L2 norm of the difference vector</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_02-10-j.png" width="413"/></p>
</div>
<h3 class="fm-head1" id="geometric-intuitions-for-vector-length">2.5.5 Geometric intuitions for vector length</h3>
<p class="body"><a id="marker-36"/>For a 2D vector <!--<span class="times">$\vec{v}=\begin{bmatrix}x\\y\\ \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="35" src="../../OEBPS/Images/eq_02-10-k2.png" width="54"/></span>, as seen in figure <a class="url" href="02.xhtml#fig-vector_diagram">2.2</a>, the L2 norm <span class="infigure"><img alt="" class="calibre5" height="27" src="../../OEBPS/Images/eq_02-10-l2.png" width="122"/></span> is nothing but the hypotenuse of the right-angled triangle whose sides are elements of the vector. The same intuition holds in higher dimensions.</p>
<p class="body">A <i class="fm-italics">unit vector</i> is a vector whose length is 1. Given any vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span>, the corresponding unit vector can be obtained by dividing every element by the length of that vector. For example, given <span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_02-10-m2.png" width="53"/></span>, length <span class="infigure"><img alt="" class="calibre5" height="28" src="../../OEBPS/Images/eq_02-10-n2.png" width="162"/></span> and the corresponding unit vector <span class="infigure"><img alt="" class="calibre5" height="67" src="../../OEBPS/Images/eq_02-10-o2.png" width="66"/></span>. Unit vectors typically represent a direction.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Unit vectors are conventionally depicted with the hat symbol as opposed to the little overhead arrow, as in <span class="math"><i class="fm-italics">û<sup class="fm-superscript">T</sup>û</i> = 1</span>.</p>
<p class="body">In machine learning, the goal of training is often to minimize the length of the error vector (the difference between the model output vector and the target ground truth vector).</p>
<h3 class="fm-head1" id="subsection-dot_product">2.5.6 Geometric intuitions for the dot product: Feature similarity</h3>
<p class="body">Consider the document retrieval problem depicted in table <a class="url" href="02.xhtml#tab-doc-vec">2.1</a> one more time. We have a set of documents, each described by its own feature vector. Given a pair of such documents, we must find their similarity. This essentially boils down to estimating the similarity between two feature vectors. In this section, we will see that the dot product between a pair of vectors can be used as a measure of similarity between them.</p>
<p class="body">For instance, consider the feature vectors corresponding to <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">5</sub></span> and <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">6</sub></span> in table <a class="url" href="02.xhtml#tab-doc-vec">2.1</a>. They are <span class="infigure"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_02-10-p2.png" width="22"/></span> and <span class="infigure"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_02-10-q2.png" width="22"/></span>. The dot product between them is <span class="math">1 × 0 + 0 × 1 = 0</span>. This is low and agrees with our intuition that there is no common word of interest between them, so the documents are very dissimilar. On the other hand, the dot product between feature vectors of <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">3</sub></span> and <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">4</sub></span> is <span class="infigure"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_02-10-r2.png" width="219"/></span>. This is high and agrees with our intuition that they have many commonalities in words of interest and are similar documents. Thus, we get the first glimpse of an important concept. Loosely speaking, <i class="fm-italics">similar vectors have larger dot products, and dissimilar vectors have near-zero dot products.</i></p>
<p class="body">We will keep revisiting this problem of estimating similarity between feature vectors and solve it with more and more finesse. As a first attempt, we will now study in greater detail how dot products measure similarities between vectors. First we will show that the component of a vector along another is yielded by the dot product. Using this, we will show that the “similarity/agreement” between a pair of vectors can be estimated using the dot product between them. In particular, we will see that if the vectors point in more or less the same direction, their dot products are higher than when the vectors are perpendicular to each other. If the vectors point in opposite directions, their dot product is negative.</p>
<p class="fm-head2" id="subsubsec-dotproduct_as_component">Dot product measures the component of one vector along another</p>
<p class="body"><a id="marker-37"/>Let’s examine a special case first: the component of a vector along a coordinate axis. This can be obtained by multiplying the length of the vector with the cosine of the angle between the vector and the relevant coordinate axis. As shown for 2D in figure <a class="url" href="02.xhtml#ch2fig-vec_component">2.7a</a>, a vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span> can be broken into two components along the <i class="timesitalic">X</i> and <i class="timesitalic">Y</i> axes as</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_02-10-s.png" width="288"/></p>
</div>
<p class="body">Note how the length of the vector is preserved:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_02-10-t.png" width="325"/></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="363" src="../../OEBPS/Images/CH02_F07a_Chaudhury.png" width="412"/></p>
<p class="figurecaption">(a) Components of a 2D vector along coordinate axes. Note that <span class="math">||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>||</span> is the length of hypotenuse.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="416" id="fig-dotproduct_as_component" src="../../OEBPS/Images/CH02_F07b_Chaudhury.png" width="460"/></p>
<p class="figurecaption">(b) Dot product as a component of one vector along another <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> ⋅ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> = <i class="fm-italics">a<sub class="fm-subscript">x</sub>b<sub class="fm-subscript">x</sub></i> + <i class="fm-italics">a<sub class="fm-subscript">y</sub>b<sub class="fm-subscript">y</sub></i> = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>|| ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||<i class="fm-italics">cos</i>(<i class="fm-italics">θ</i>)</span>.</p>
</div>
<p class="figurecaption" id="ch2fig-vec_component">Figure 2.7 Vector components and dot product</p>
<p class="body">Now let’s study the more general case of the component of one vector in the direction of another arbitrary vector (figure <a class="url" href="02.xhtml#ch2fig-vec_component">2.7b</a>). The component of a vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> along another vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> is <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> ⋅ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. This is equivalent to <span class="math">||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>|| ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||<i class="fm-italics">cos</i>(<i class="fm-italics">θ</i>)</span>, where <i class="timesitalic">θ</i> is the angle between the vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. (This has been proven for the two-dimension case discussed in section <a class="url" href="../Text/A.xhtml#sec-dotprod-cosine-proof">A.1</a> of the appendix. You can read it if you would like deeper intuition.)</p>
<p class="fm-head2" id="subsubsec-dotproduct_as_agreement">Dot product measures the agreement between two vectors</p>
<p class="body"><a id="marker-38"/>The dot product can be expressed using the cosine of the angle between the vectors. Given two vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>, if <i class="timesitalic">θ</i> is the angle between them, we have see figure <a class="url" href="02.xhtml#ch2fig-vec_component">2.7b</a>)</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="68" src="../../OEBPS/Images/eq_02-11.png" width="394"/></p>
</div>
<p class="fm-equation-caption">Equation 2.11 <span class="calibre" id="eq-dot-prod-with-cosine"/></p>
<p class="body">Expressing the dot product using cosines makes it easier to see that it measures the <i class="fm-italics">agreement</i> (aka <i class="fm-italics">correlation</i>) between two vectors. If the vectors have the same direction, the angle between them is <span class="math">0</span> and the cosine is <span class="math">1</span>, implying maximum agreement. The cosine becomes progressively smaller as the angle between the vectors increases, until the two vectors become perpendicular to each other and the cosine is zero, implying no correlation—the vectors are independent of each other. If the angle between them is <span class="math">180°</span>, the cosine is <span class="math">−1</span>, implying that the vectors are anti-correlated. Thus, the dot product of two vectors is proportional to their directional agreement.</p>
<p class="body">What role do the vector lengths play in all this? The dot product between two vectors is also proportional to the lengths of the vectors. This means agreement scores between bigger vectors are higher (an agreement between the US president and the German chancellor counts more than an agreement between you and me).</p>
<p class="body">If you want the agreement score to be neutral to the vector length, you can use a normalized dot product between unit-length vectors along the same directions:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_02-11-a.png" width="353"/></p>
</div>
<p class="fm-head2" id="dot-product-and-the-difference-between-two-unit-vectors">Dot product and the difference between two unit vectors</p>
<p class="body">To obtain further insight into how the dot product indicates agreement or correlation between two directions, consider the two unit vectors <span class="infigure"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_02-11-b2.png" width="62"/></span> and <span class="infigure"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_02-11-c2.png" width="59"/></span>. The difference between them is <span class="infigure"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_02-11-d2.png" width="127"/></span>.</p>
<p class="body">Note that since they are unit vectors, <span class="infigure"><img alt="" class="calibre5" height="40" src="../../OEBPS/Images/eq_02-11-e2.png" width="355"/></span>. The length of the difference vector</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="119" src="../../OEBPS/Images/eq_02-11-f.png" width="478"/></p>
</div>
<p class="body">From the last equality, it is evident that a larger dot product implies a smaller difference: that is, more agreement between the vectors.</p>
<h2 class="fm-head" id="section-orthogonality">2.6 Orthogonality of vectors and its physical significance</h2>
<p class="body"><a id="marker-39"/>Try moving an object at right angles to the direction in which you are pushing it. You will find it impossible. The larger the angle, the less effective your force vector becomes (finally becoming totally ineffective at a 90° angle). This is why it is easy to walk on a horizontal surface (you are moving at right angles to the direction of gravitational pull, so the gravity vector is ineffective) but harder on an upward incline (the gravity vector is having some effect against you).</p>
<p class="body">These physical notions are captured mathematically in the notion of a dot product. The dot product between two vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> (say, the push vector) and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> (say, the displacement of the pushed object vector) is <span class="math">||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>|| ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||<i class="fm-italics">cosθ</i></span>, where <i class="timesitalic">θ</i> is the angle between the two vectors. When <i class="timesitalic">θ</i> is <span class="math">0</span> (the two vectors are aligned), <span class="math"><i class="fm-italics">cosθ</i> = 1</span>, the maximum possible value of <i class="timesitalic">cosθ</i>, so push is maximally effective. As <i class="timesitalic">θ</i> increases, <i class="timesitalic">cosθ</i> decreases, and push becomes less and less effective. Finally, at <span class="math"><i class="fm-italics">θ</i> = 90°</span>, <span class="math"><i class="fm-italics">cosθ</i> = 0</span>, and push becomes completely ineffective.</p>
<p class="body">Two vectors are orthogonal if their dot product is zero. Geometrically, this means the vectors are perpendicular to each other. Physically, this means the two vectors are independent: one cannot influence the other. You can say there is nothing in common between orthogonal vectors. For instance, the feature vector for <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">5</sub></span> is <span class="infigure"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_02-10-p2.png" width="22"/></span> and that for <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">6</sub></span> is <span class="infigure"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_02-10-q2.png" width="22"/></span> in table <a class="url" href="02.xhtml#tab-doc-vec">2.1</a>. These are orthogonal (dot product is zero), and you can easily see that none of the feature words (<i class="fm-italics">gun</i>, <i class="fm-italics">violence</i>) are common to both documents.</p>
<h2 class="fm-head" id="python-basic-vector-matrix-ops">2.7 Python code: Basic vector and matrix operations via PyTorch</h2>
<p class="body">In this section, we use Python PyTorch code to illustrate many of the concepts discussed earlier.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this section, executable via Jupyter Notebook, can be found at <a class="url" href="https://github.com/krishnonwork/mathematical-methods-in-deep-learning-ipython/blob/master/python/ch2/2.7-transpose-dot-matmul.ipynb">http://mng.bz/ryzE</a>.</p>
<h3 class="fm-head1" id="pytorch-code-for-a-matrix-transpose">2.7.1 PyTorch code for a matrix transpose</h3>
<p class="body">The following listing shows the PyTorch code for a matrix transpose.</p>
<p class="fm-code-listing-caption" id="listing-2.5-transpose">Listing 2.5 Transpose</p>
<pre class="programlisting">I49 = torch.stack([torch.arange(0, 72, 8), torch.arange(64, 136, 8),   <span class="fm-combinumeral">①</span>
                torch.arange(128, 200, 8), torch.arange(192, 264, 8)])

I49_t = torch.transpose(I49, 0, 1)                                     <span class="fm-combinumeral">②</span>

for i in range(0, I49.shape[0]):
    for j in range(0, I49.shape[1]):
        assert I49[i][j] == I49_t[j][i]                                <span class="fm-combinumeral">③</span>


assert torch.allclose(I49_t, I49.T, 1e-5)                              <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The torch.arange function creates a vector whose elements go from <i class="fm-italics">start</i> to <i class="fm-italics">stop</i> in increments of <i class="fm-italics">step</i>. Here we create a <span class="math">4 × 9</span> image corresponding to <span class="math"><i class="fm-italics">I</i><sub class="fm-subscript">4,9</sub></span> in equation <a class="url" href="02.xhtml#eq-tiny_im">2.2</a>, shown in figure <a class="url" href="02.xhtml#fig-tiny_im">2.3</a>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The transpose operator interchanges rows and columns. The <span class="math">4 × 9</span> image becomes a <span class="math">9 × 4</span> image (see figure <a class="url" href="02.xhtml#fig-tiny_im-transpose">2.6</a>. The element at position <span class="math">(<i class="fm-italics">i, j</i>)</span> is interchanged with the element at position <span class="math">(<i class="fm-italics">j, i</i>)</span>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Interchanged elements of the original and transposed matrix are equal.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The .T operator retrieves the transpose of an array.</p>
<h3 class="fm-head1" id="pytorch-code-for-a-dot-product">2.7.2 PyTorch code for a dot product</h3>
<p class="body"><a id="marker-40"/>The dot product of two vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> represents the components of one vector along the other. Consider two vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> = [<i class="fm-italics">a</i><sub class="fm-subscript">1</sub> <i class="fm-italics">a</i><sub class="fm-subscript">2</sub> <i class="fm-italics">a</i><sub class="fm-subscript">3</sub>]</span> and <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> = [<i class="fm-italics">b</i><sub class="fm-subscript">1</sub> <i class="fm-italics">b</i><sub class="fm-subscript">2</sub> <i class="fm-italics">b</i><sub class="fm-subscript">3</sub>]</span>. Then <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>.<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> = <i class="fm-italics">a</i><sub class="fm-subscript">1</sub><i class="fm-italics">b</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">a</i><sub class="fm-subscript">2</sub><i class="fm-italics">b</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">a</i><sub class="fm-subscript">3</sub><i class="fm-italics">b</i><sub class="fm-subscript">3</sub></span>.</p>
<p class="fm-code-listing-caption" id="listing-2.6-dot-product">Listing 2.6 Dot product</p>
<pre class="programlisting">a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
a_dot_b = torch.dot(a, b)
print("Dot product of these two vectors is: "
              "{}".format(a_dot_b))               <span class="fm-combinumeral">①</span>

# Dot product of perpendicular vectors is zero
vx = torch.tensor([1, 0]) # a vector along X-axis
vy = torch.tensor([0, 1]) # a vector along Y-axis
print("Example dot product of orthogonal vectors:"
              " {}".format(torch.dot(vx, vy)))    <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Outputs <span class="math">32</span>: <span class="math">1 ∗ 4 + 2 ∗ 5 + 3 ∗ 6</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Outputs <span class="math">0</span>: <span class="math">1 ∗ 0 + 0 ∗ 1</span></p>
<h3 class="fm-head1" id="pytorch-code-for-matrix-vector-multiplication">2.7.3 PyTorch code for matrix vector multiplication</h3>
<p class="body"><a id="marker-41"/>Consider a matrix <i class="timesitalic">A<sub class="fm-subscript">m, n</sub></i> with <i class="timesitalic">m</i> rows and <i class="timesitalic">n</i> columns that is multiplied with a vector <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><sub class="fm-subscript">n</sub></i> with <i class="timesitalic">n</i> elements. The result is a <i class="timesitalic">m</i> element column vector <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span><sub class="fm-subscript">m</sub></i> . In the following example, <span class="math"><i class="fm-italics">m</i> = 3</span> and <span class="math"><i class="fm-italics">n</i> = 2</span>.</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="109" src="../../OEBPS/Images/eq_02-11-g.png" width="295"/></p>
</div>
<p class="body">In general,</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="21" src="../../OEBPS/Images/eq_02-11-h.png" width="295"/></p>
</div>
<p class="fm-code-listing-caption" id="listing-2.7-matrix-vector-multiplication">Listing 2.7 Matrix vector multiplication</p>
<pre class="programlisting">X = torch.tensor([[0.11, 0.09], [0.01, 0.02], [0.98, 0.91], [0.12, 0.21], <span class="fm-combinumeral">①</span>
              [0.98, 0.99], [0.85, 0.87], [0.03, 0.14], [0.55, 0.45],
              [0.49, 0.51], [0.99, 0.01], [0.02, 0.89], [0.31, 0.47],
              [0.55, 0.29], [0.87, 0.76], [0.63, 0.24]])                  <span class="fm-combinumeral">②</span>

w = torch.rand((2, 1))                                                    <span class="fm-combinumeral">③</span>
b = 5.0
  = torch.matmul(X, w) + b                                                <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A linear model comprises a weight vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and bias <i class="timesitalic">b</i>. For each training data instance <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">i</sub></i>, the model outputs <span class="math"><i class="fm-italics">y</i><i class="fm-italics"><sub class="fm-subscript">i</sub></i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <i class="fm-italics">b</i></span>. For the training data matrix <i class="timesitalic">X</i> (whose rows are training data instances), the model outputs <span class="math"><i class="fm-italics">X</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Cat-brain <span class="math">15 × 2</span> training data matrix (equation <a class="url" href="02.xhtml#eq-cat-brain-nobiased">2.7</a>)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Random initialization of weight vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Model training output: <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="fm-italics">X</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <i class="fm-italics">b</i></span>. The scalar <i class="timesitalic">b</i> is automatically replicated to create a vector.</p>
<h3 class="fm-head1" id="pytorch-code-for-matrix-matrix-multiplication">2.7.4 PyTorch code for matrix-matrix multiplication</h3>
<p class="body">Consider a matrix <i class="timesitalic">A<sub class="fm-subscript">m, p</sub></i> with <i class="timesitalic">m</i> rows and <i class="timesitalic">p</i> columns. Let’s multiply it with another matrix <i class="timesitalic">B<sub class="fm-subscript">p, n</sub></i> with <i class="timesitalic">p</i> rows and <i class="timesitalic">n</i> columns. The resultant matrix <i class="timesitalic">C<sub class="fm-subscript">m, n</sub></i> contains <i class="timesitalic">m</i> rows and <i class="timesitalic">n</i> columns. Note that the number of columns in the left matrix <i class="timesitalic">A</i> should match the number of rows in the right matrix <i class="timesitalic">B</i>:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="245" src="../../OEBPS/Images/eq_02-11-i.png" width="280"/></p>
</div>
<p class="body">In general,</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="65" src="../../OEBPS/Images/eq_02-11-j.png" width="118"/></p>
</div>
<p class="fm-code-listing-caption" id="listing-2.8-matrix-matrix-multiplication">Listing 2.8 Matrix-matrix multiplication</p>
<pre class="programlisting">A = torch.tensor([[1, 2], [3, 4], [5, 6]])
B = torch.tensor([[7, 8], [9, 10]])

C = torch.matmul(A, B)                              <span class="fm-combinumeral">①</span>
                                                    <span class="fm-combinumeral">②</span>
  w = torch.tensor([1, 2, 3])
  x = torch.tensor([4, 5, 6])
  assert torch.dot(w, x) == torch.matmul(w.T, x)    <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="math"><i class="fm-italics">C</i> = <i class="fm-italics">AB</i></span> <span class="cambria">⟹</span> <span class="math"><i class="fm-italics">C</i>[<i class="fm-italics">i</i>, <i class="fm-italics">j</i>]</span> is the dot product of the <i class="timesitalic">i</i>th row of <i class="timesitalic">A</i> and <i class="timesitalic">j</i>th column of B.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="infigure"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_02-11-k.png" width="197"/></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The dot product can be viewed as a row matrix multiplied by a column matrix.</p>
<h3 class="fm-head1" id="pytorch-code-for-the-transpose-of-a-matrix-product">2.7.5 PyTorch code for the transpose of a matrix product</h3>
<p class="body">Given two matrices <i class="timesitalic">A</i> and <i class="timesitalic">B</i>, where the number of columns in <i class="timesitalic">A</i> matches the number of rows in <i class="timesitalic">B</i>, the transpose of their product is the product of the individual transposes <i class="fm-italics">in reversed order</i>: <span class="math">(<i class="fm-italics">AB</i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> = <i class="fm-italics">B<sup class="fm-superscript">T</sup>A<sup class="fm-superscript">T</sup></i></span>.</p>
<p class="fm-code-listing-caption" id="listing-2.9-transpose-of-a-matrix-product">Listing 2.9 Transpose of a matrix product</p>
<pre class="programlisting">assert torch.all(torch.matmul(A, B).T == torch.matmul(B.T, A.T))  <span class="fm-combinumeral">①</span>

assert torch.all(torch.matmul(A.T, x).T == torch.matmul(x.T, A))  <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Asserts equality between <span class="math">(<i class="fm-italics">AB</i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i></span> and <i class="timesitalic">B<sup class="fm-superscript">T</sup>A<sup class="fm-superscript">T</sup></i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Applies to matrix-vector multiplication, too: <span class="math">(<i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i></span></p>
<h2 class="fm-head" id="sec-hyperplanes">2.8 Multidimensional line and plane equations and machine learning</h2>
<p class="body"><a id="marker-42"/>Geometrically speaking, what does a machine learning classifier really do? We provided the outline of an answer in section <a class="url" href="../Text/01.xhtml#sec-geom-view-ml">1.4</a>. You are invited to review that and especially figures <a class="url" href="../Text/01.xhtml#fig-geometrical_view">1.2</a> and <a class="url" href="../Text/01.xhtml#fig-ml-as-mapping">1.3</a>. We will briefly summarize here.</p>
<p class="body">Inputs to a classifier are feature vectors. These vectors can be viewed as points in some multidimensional feature space. The task of classification then boils down to separating the points belonging to different classes. The points may be all jumbled up in the input space. It is the model’s job to transform them into a different (output) space where it is easier to separate the classes. A visual example of this was provided in figure <a class="url" href="../Text/01.xhtml#fig-ml-as-mapping">1.3</a>.</p>
<p class="body">What is the geometrical nature of the separator? In a very simple situation, such as the one depicted in figure <a class="url" href="../Text/01.xhtml#fig-geometrical_view">1.2</a>, the separator is a line in 2D space. In real-life situations, the separator is often a line or a plane in a high-dimensional space. In more complicated situations, the separator is a curved surface, as depicted in figure <a class="url" href="../Text/01.xhtml#fig-non_linear_separator">1.4</a>.</p>
<p class="body">In this section, we will study the mathematics and geometry behind two types of separators, lines, and planes in high-dimensional spaces, aka hyperlines and hyperplanes.</p>
<h3 class="fm-head1" id="sec-multidim-line-eq">2.8.1 Multidimensional line equation</h3>
<p class="body">In high school geometry, we learned <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">mx</i> + <i class="fm-italics">c</i></span> as the equation of a line. But this does not lend itself readily to higher dimensions. Here we will study a better representation of a straight line that works equally well for any finite-dimensional space.</p>
<p class="body">As shown in figure <a class="url" href="02.xhtml#fig-multi-dim-lineeq">2.8</a>, a line joining vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> can be viewed as the set of points we will encounter if we</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Start at point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span></p>
</li>
<li class="fm-list-bullet">
<p class="list">Travel along the direction <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span></p>
</li>
</ul>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="462" id="fig-multi-dim-lineeq" src="../../OEBPS/Images/CH02_F08_Chaudhury.png" width="445"/></p>
<p class="figurecaption">Figure 2.8 Any point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> on the line joining two vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> is given by <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> + <i class="fm-italics">α</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>−<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>)</span>.</p>
</div>
<p class="body">Different points on the line are obtained by traveling different distances. Denoting this arbitrary distance by <i class="timesitalic">α</i>, the equation of the line joining vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> can be expressed as</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_02-12.png" width="300"/></p>
</div>
<p class="fm-equation-caption">Equation 2.12 <span class="calibre" id="eq-collinearity"/></p>
<p class="body"><a id="marker-43"/>Equation <a class="url" href="02.xhtml#eq-collinearity">2.12</a> says that any point on the line joining <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> can be obtained as a weighted combination of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>, the weights being <i class="timesitalic">α</i> and <span class="math">1 − <i class="fm-italics">α</i></span>. By varying <i class="timesitalic">α</i>, we obtain different points on the line. Also, different ranges of <i class="timesitalic">α</i> values yield different segments on the line. As shown in figure <a class="url" href="02.xhtml#fig-multi-dim-lineeq">2.8</a>, values of <i class="timesitalic">α</i> between <span class="math">0</span> and <span class="math">1</span> yield points between <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. Negative values of <i class="timesitalic">α</i> yield points to the left of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>. Values of <i class="timesitalic">α</i> greater than <span class="math">1</span> yield points to the right of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. This equation for a line works for any dimensions, not just two.</p>
<h3 class="fm-head1" id="subsec-hyper-planes-classifiers">2.8.2 Multidimensional planes and their role in machine learning</h3>
<p class="body">In section <a class="url" href="../Text/01.xhtml#sec-regression-vs-classification">1.5</a>, we encountered classifiers. Let’s take another look at them. Suppose we want to create a classifier that helps us make <i class="fm-italics">buy</i> or <i class="fm-italics">no-buy</i> decisions on stocks based on only three input variables: 1) <i class="fm-italics">momentum</i>, or the rate at which the stock price is changing positive momentum means the stock price is increasing and vice versa); 2) the <i class="fm-italics">dividend</i> paid last quarter; and (3) <i class="fm-italics">volatility</i>, or how much the price has fluctuated in the last quarter. Let’s plot all training points in the feature space with coordinate axes corresponding to the variables <i class="fm-italics">momentum</i>, <i class="fm-italics">dividend</i>, <i class="fm-italics">volatility</i>. Figure <a class="url" href="02.xhtml#fig-planar-classifier">2.9</a> shows that the classes can be separated by a plane in the three-dimensional feature space.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="431" id="fig-planar-classifier" src="../../OEBPS/Images/CH02_F09_Chaudhury.png" width="738"/></p>
<p class="figurecaption">Figure 2.9 A toy machine learning classifier for stock buy vs. no-buy decision-making. A plus (+) indicates no-buy, and a dash (-) indicates buy. The decision is made based on three input variables: momentum, dividend, and volatility.</p>
</div>
<p class="body"><a id="marker-44"/>Geometrically speaking, our model simply corresponds to this plane. Input points above the plane indicate buy decisions (dashes [-]), and input points indicate no-buy decisions (pluses [+]). In general, you want to buy high-positive-momentum stocks, so points at the higher end of the momentum axis are likelier to be <i class="fm-italics">buy</i>. However, this is not the only indicator. For more volatile stocks, we demand higher <i class="fm-italics">momentum</i> to switch from <i class="fm-italics">no-buy</i> to <i class="fm-italics">buy</i>. This is why the plane slopes upward (higher <i class="fm-italics">momentum</i>) as we move rightward (higher <i class="fm-italics">volatility</i>). Also, we demand less <i class="fm-italics">momentum</i> for stocks with higher <i class="fm-italics">dividends</i>. This is why the plane slopes downward (lower <i class="fm-italics">momentum</i>) as we go toward higher <i class="fm-italics">dividends</i>.</p>
<p class="body">Real problems have more dimensions, of course (since many more inputs are involved in the decision), and the separator becomes a hyperplane. Also, in real-life problems, the points are often too intertwined in the input space for any separator to work. We first have to apply a transformation that maps the point to an output space where it is easier to separate. Given their significance as class separators in machine learning, we will study hyperplanes in this section.</p>
<p class="body">In high school 3D geometry, we learned <span class="math"><i class="fm-italics">ax</i> + <i class="fm-italics">by</i> + <i class="fm-italics">cz</i> + <i class="fm-italics">d</i> = 0</span> as the equation of a plane. Now we will study a version of it that works in higher dimensions.</p>
<p class="body">Geometrically speaking, given a plane (in any dimension), we can find a direction called the <i class="fm-italics">normal direction</i>, denoted <i class="timesitalic">n̂</i>, such that</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">If we take any pair of points on the plane, say <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub></span> and <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, …</p>
</li>
<li class="fm-list-bullet">
<p class="list">The line joining <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub></span>—i.e., the vector <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub></span>—is orthogonal to <i class="timesitalic">n̂</i>.</p>
</li>
</ul>
<p class="body">Thus, if we know a fixed point on the plane, say <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub></span>, then all points on the plane will satisfy</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">n̂</i> · (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub>) = 0</span></p>
<p class="body">or</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">n̂<sup class="fm-superscript">T</sup></i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub>) = 0</span></p>
<p class="body">Thus we can express the equation of a plane as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">n̂<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <i class="fm-italics">n̂<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub> = 0</span></p><!--<div class="figure">
<p class="figure2"><img src="imgs/equations/eq_02-13.png" alt="" /></p>
</div>-->
<p class="fm-equation-caption">Equation 2.13 <span class="calibre" id="eq-plane-0"/></p>
<p class="body">Equation <a class="url" href="02.xhtml#eq-plane-0">2.13</a> is depicted pictorially in figure <a class="url" href="02.xhtml#fig-multi-dim-planeeq">2.10</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="327" id="fig-multi-dim-planeeq" src="../../OEBPS/Images/CH02_F10_Chaudhury.png" width="473"/></p>
<p class="figurecaption">Figure 2.10 The normal to the plane is the same at all points on the plane. This is the fundamental property of a plane. <i class="timesitalic">n̂</i> depicts that normal direction. Let <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub></span> be a point on the plane. All other points on the plane, depicted as <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, will satisfy the equation <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub>) ⋅ <i class="fm-italics">n̂</i> = 0</span>. This physically says that the line joining a known point <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">0</sub></span> on the plane and any other arbitrary point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> on the plane is at right angles to the normal <i class="timesitalic">n̂</i>. This formulation works for any dimension.</p>
</div>
<p class="body"><a id="marker-45"/>In section <a class="url" href="../Text/01.xhtml#sec-cat_brain">1.3</a>, equation <a class="url" href="../Text/01.xhtml#eq-linear-predictor">1.3</a>, we encountered the simplest machine learning model: a weighted sum of inputs along with a bias. Denoting the inputs as <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, the weights as <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, and the bias as <i class="timesitalic">b</i>, this model was depicted as</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i> = 0</span></p>
<p class="fm-equation-caption">Equation 2.14 <span class="calibre" id="eq-plane-1"/></p>
<p class="body">Comparing equations <a class="url" href="02.xhtml#eq-plane-0">2.13</a> and <a class="url" href="02.xhtml#eq-plane-1">2.14</a> , we get the geometric significance: the simple model of equation <a class="url" href="../Text/01.xhtml#eq-linear-predictor">1.3</a> is nothing but a planar separator. Its weight vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> corresponds to the plane’s orientation (normal). The bias <i class="timesitalic">b</i> corresponds to the plane’s location (a fixed point on the plane). During training, we are learning the weights and biases—this is essentially learning the orientation and position of the optimal plane that will separate the training inputs. To be consistent with the machine learning paradigm, henceforth we will write the equation of a hyperplane as equation <a class="url" href="02.xhtml#eq-plane-1">2.14</a> for some constant <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and <i class="timesitalic">b</i>.</p>
<p class="body">Note that <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> need not be a unit-length vector. Since the right-hand side is zero, if necessary, we can divide both sides by <span class="math">||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>||</span> to convert to a form like equation <a class="url" href="02.xhtml#eq-plane-0">2.13</a>.</p>
<p class="body">The sign of the expression <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i></span> has special significance. All points <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> for which <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i> &lt; 0</span> lie on the same side of the hyperplane. All points <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> for which <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i> &gt; 0</span> lie on the other side of the hyperplane. And of course, all points <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> for which <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i> = 0</span> lie on the hyperplane.</p>
<p class="body">It should be noted that the 3D equation <span class="math"><i class="fm-italics">ax</i> + <i class="fm-italics">by</i> + <i class="fm-italics">cz</i> + <i class="fm-italics">d</i> = 0</span> is a special case of equation <a class="url" href="02.xhtml#eq-plane-1">2.14</a> because <span class="math"><i class="fm-italics">ax</i> + <i class="fm-italics">by</i> + <i class="fm-italics">cz</i> + <i class="fm-italics">d</i> = 0</span> can be rewritten as</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="68" src="../../OEBPS/Images/eq_02-14-a.png" width="171"/></p>
</div>
<p class="body">which is same as <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i> = 0</span> with <span class="infigure"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_02-14-b2.png" width="59"/></span> and <span class="infigure"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_02-14-c2.png" width="56"/></span>. Incidentally, this tells us that in 3D, the normal to the plane <span class="math"><i class="fm-italics">ax</i> + <i class="fm-italics">by</i> + <i class="fm-italics">cz</i> + <i class="fm-italics">d</i> = 0</span> is <span class="infigure"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_02-14-d2.png" width="128"/></span> .</p>
<h2 class="fm-head" id="sec-lin-dep">2.9 Linear combinations, vector spans, basis vectors, and collinearity preservation</h2>
<p class="body"><a id="marker-46"/>by now, it should be clear that machine learning and data science are all about points in high-dimensional spaces. Consequently, it behooves us to have a decent understanding of these spaces. For instance, given a space, we may need to ask, “Would it be possible to express all points in the space in terms of a set of a few vectors? What is the smallest set of vectors we really need for that purpose?” This section is devoted to the study of these questions.</p>
<h3 class="fm-head1" id="linear-dependence">2.9.1 Linear dependence</h3>
<p class="body">Consider the vectors (points) shown in figure <a class="url" href="02.xhtml#fig-lin-dep">2.11</a>. The corresponding vectors in 2D are</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="151" src="../../OEBPS/Images/eq_02-14-e.png" width="230"/></p>
</div>
<p class="body">We can find four scalars <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">0</sub> = 2</span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub> = 2</span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">2</sub> = 2</span>, and <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">3</sub> = −3</span> such that</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="78" src="../../OEBPS/Images/eq_02-14-f.png" width="251"/></p>
</div>
<p class="body">If we can find such scalars, not all zero, we say the vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">0</sub></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub></span>, and <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">3</sub></span> are <i class="fm-italics">linearly dependent</i>. The geometric picture to keep in mind is that points corresponding to linearly dependent vectors lie on a single straight line in the space containing them.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="442" id="fig-lin-dep" src="../../OEBPS/Images/CH02_F11_Chaudhury.png" width="445"/></p>
<p class="figurecaption">Figure 2.11 Linearly dependent points in a 2D plane</p>
</div>
<p class="fm-head2" id="collinearity-implies-linear-dependence">Collinearity implies linear dependence</p>
<p class="body"><a id="marker-47"/>Proof: Let <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span> be three collinear vectors. From equation <a class="url" href="02.xhtml#eq-collinearity">2.12</a>, there exists some <span class="math"><i class="fm-italics">α</i> <span class="cambria">∈</span> ℝ</span> such that</p>
<p class="fm-equation"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span> = (1−<i class="fm-italics">α</i>)<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> + <i class="fm-italics">α</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></p>
<p class="body">This equation can be rewritten as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> + <i class="fm-italics">α</i><sub class="fm-subscript">3</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span> = 0</span></p>
<p class="body">where <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub> = (1−<i class="fm-italics">α</i>)</span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">2</sub> = <i class="fm-italics">α</i></span> and <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">3</sub> = −1</span>. Thus we have proven that three collinear vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>, and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span> must also be linearly dependent.</p>
<p class="fm-head2" id="linear-combination">Linear combination</p>
<p class="body">Given a set of vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub></span>, …. <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span> and a set of scalar weights <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">2</sub></span>, …<i class="timesitalic">α<sub class="fm-subscript">n</sub></i>, the weighted sum <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub> + + … <i class="fm-italics">α</i><i class="fm-italics"><sub class="fm-subscript">n</sub></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span> is called a <i class="fm-italics">linear combination</i>.</p>
<p class="fm-head2" id="generic-multidimensional-definition-of-linear-dependence">Generic multidimensional definition of linear dependence</p>
<p class="body">A set of vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub></span>, …. <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span> are <i class="fm-italics">linearly dependent</i> if there exists a set of weights <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">2</sub></span>, …<i class="timesitalic">α<sub class="fm-subscript">n</sub></i>, not all zeros, such that <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub> + + … <i class="fm-italics">α</i><i class="fm-italics"><sub class="fm-subscript">n</sub></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i> = 0</span>. For example, the row vectors <span class="math">[1   1]</span> and <span class="math">[2   2]</span> are linearly dependent, since <span class="math">–2[1   1] + [2   2] = 0</span>.</p>
<h3 class="fm-head1" id="span-of-a-set-of-vectors">2.9.2 Span of a set of vectors</h3>
<p class="body">Given a set of vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">1</sub></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript">2</sub></span>, …. <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span>, their <i class="fm-italics">span</i> is defined as the set of all vectors that are linear combinations of the original set . This includes the original vectors.</p>
<p class="body">For example, consider the two vectors <span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-14-i1b.png" width="71"/></span> and <span class="infigure"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_02-14-i2b.png" width="73"/></span>. The span of these two vectors is the entire plane containing the two vectors. Any vector, for instance, the vector <span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_02-14-j2.png" width="33"/></span> can be expressed as a weighted sum <span class="math">18<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript"><i class="fm-italics1">x</i>⊥</sub> + 97<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><sub class="fm-subscript"><i class="fm-italics1">y</i>⊥</sub></span>.</p>
<p class="body">You can probably recognize that <span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-14-k1b.png" width="21"/></span> and <span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-14-k2b.png" width="22"/></span> are the familiar Cartesian coordinate axes (<i class="timesitalic">X</i>-axis and <i class="timesitalic">Y</i>-axis, respectively) in the 2D plane.</p>
<h3 class="fm-head1" id="vector-spaces-basis-vectors-and-closure">2.9.3 Vector spaces, basis vectors, and closure</h3>
<p class="body"><a id="marker-48"/>We have been talking informally about vector spaces. It is time to define them more precisely.</p>
<p class="fm-head2" id="vector-spaces">Vector spaces</p>
<p class="body">A set of vectors (points) in <i class="timesitalic">n</i> dimensions form a <i class="fm-italics">vector space</i> if and only if the operations of <i class="fm-italics">addition</i> and <i class="fm-italics">scalar multiplication</i> are defined on the set. In particular, this implies that it is possible to take linear combinations of members of a vector space.</p>
<p class="fm-head2" id="basis-vectors">Basis vectors</p>
<p class="body">Given a vector space, a set of vectors that span the space is called a <i class="fm-italics">basis</i> for the space. For instance, for the space <span class="math">ℝ<sup class="fm-superscript">2</sup></span>, the two vectors <span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-14-k1.png" width="21"/></span> and <span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-14-k2.png" width="22"/></span> are basis vectors. This essentially means any vector in <span class="math">ℝ<sup class="fm-superscript">2</sup></span> can be expressed as a linear combination of these two. The notion can be extended to higher dimensions. For <span class="math">ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span>, the vectors <span class="infigure"><img alt="" class="calibre5" height="87" src="../../OEBPS/Images/eq_02-14-l.png" width="134"/></span> form a basis.</p>
<p class="body">The alert reader has probably guessed by now that the basis vectors are related to coordinate axes. In fact, the basis vectors just described constitute the Cartesian coordinate axes.</p>
<p class="body">So far, we have only seen examples of basis vectors that are mutually orthogonal, such as the dot product of the two basis vectors in <span class="math">ℝ<sup class="fm-superscript">2</sup></span> shown earlier: <span class="infigure"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_02-14-m.png" width="203"/></span>. However, basis vectors do not have to be orthogonal. Any pair of linearly independent vectors forms a basis in <span class="math">ℝ<sup class="fm-superscript">2</sup></span>. Basis vectors, then, are by no means unique. That said, orthogonal vectors are most convenient, as we shall see later.</p>
<p class="fm-head2" id="minimal-and-complete-basis">Minimal and complete basis</p>
<p class="body">Exactly <i class="timesitalic">n</i> vectors are needed to span a space with dimensionality <i class="timesitalic">n</i>. This means the basis set for a space will have at least as many elements as the dimensionality of the space. That many basis vectors are also sufficient to form a basis. For instance, exactly <i class="timesitalic">n</i> vectors are needed to form a basis in (that is, span) <span class="math">ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span>.</p>
<p class="body">A related fact is that in <span class="math">ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span>, any set of <i class="timesitalic">m</i> vectors with <span class="math"><i class="fm-italics">m</i> &gt; <i class="fm-italics">n</i></span> will be linearly dependent. In other words, the largest size of a set of linearly independent vectors in an <i class="timesitalic">n</i>-dimensional space is <i class="timesitalic">n</i>.</p>
<p class="fm-head2" id="closure">Closure</p>
<p class="body">A set of vectors is said to be <i class="fm-italics">closed</i> under linear combination if and only if the linear combination of any pair of vectors in the set also belongs to the same set. Consider the set of points <span class="math">ℝ<sup class="fm-superscript">2</sup></span>. Recall that this is the set of vectors with two real elements. Take any pair of vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> in <span class="math">ℝ<sup class="fm-superscript">2</sup></span>: for instance, <span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_02-14-n1.png" width="101"/></span> and <span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_02-14-n2.png" width="111"/></span>. Any linear combination of these two vectors will also comprise two real numbers—that is, will belong to <span class="math">ℝ<sup class="fm-superscript">2</sup></span>. We say <span class="math">ℝ<sup class="fm-superscript">2</sup></span> is a <i class="fm-italics">vector space</i> since it is <i class="fm-italics">closed</i> under linear combination.</p>
<p class="body">Consider the space <span class="math">ℝ<sup class="fm-superscript">2</sup></span>. Geometrically speaking, this represents a two dimensional plane. Let’s take two points on this plane, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. Linear combinations of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> geometrically correspond to points on the line joining them. We know that if two points lie on a plane, the entire line will also lie on the plane. Thus, in two dimensions, a plane is closed under linear combinations. This is the geometrical intuition behind the notion of closure on vector spaces. It can be extended to arbitrary dimensions.</p>
<p class="body">On the other hand, the set of points on the surface of a sphere is <i class="fm-italics">not</i> closed under linear combination because the line joining an arbitrary pair of points on this set will not wholly lie on the surface of that sphere.</p>
<h2 class="fm-head" id="sec-linmap-geom">2.10 Linear transforms: Geometric and algebraic interpretations</h2>
<p class="body"><a id="marker-49"/>Inputs to a machine learning or data science system are typically feature vectors (introduced in section <a class="url" href="02.xhtml#sec-vectors">2.1</a>) in high-dimensional spaces. Each individual dimension of the feature vector corresponds to a particular property of the input. Thus, the feature vector is a descriptor for the particular input instance. It can be viewed as a point in the feature space. We usually transform the points to a friendlier space where it is easier to perform the analysis we are trying to do. For instance, if we are building a classifier, we try to transform the input into a space where the points belonging to different classes are more segregated (see section <a class="url" href="../Text/01.xhtml#fig-ml-as-mapping">1.3</a> in general and figure <a class="url" href="../Text/01.xhtml#fig-ml-as-mapping">1.3</a> in particular for simple examples). Sometimes we transform to simplify the data, eliminating axes along which there is scant variation in the data. Given their significance in machine learning, in this section we will study the basics of transforms.</p>
<p class="body">Informally, a transform is an operation that maps a set of points vectors) to another. Given a set <i class="timesitalic">S</i> of <span class="math"><i class="fm-italics">n</i> × 1</span> vectors, any <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">T</i> can be viewed as a transform. If <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span> belongs to the set <i class="timesitalic">S</i>, multiplication with the matrix <i class="timesitalic">T</i> will map (transform) <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span> to a vector <span class="math"><i class="fm-italics">T</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span></span>. We will later see that matrix multiplication is a subclass of transforms that preserve collinearity—points that lie on a straight line before the transformation will continue to lie on a (possibly different) straight line post the transformation. For instance, consider the matrix</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="73" src="../../OEBPS/Images/eq_02-14-o.png" width="126"/></p>
</div>
<p class="body">In section <a class="url" href="02.xhtml#sec-rotation-matrices-eigen">2.14</a>, we will see that this is a special kind of matrix called a rotation matrix; for now, simply consider it an example of a matrix. <i class="timesitalic">R</i> is a transformation operator that maps a point in a 2D plane to another point in the same plane. In mathematical notation, <span class="math"><i class="fm-italics">R</i> : ℝ<sup class="fm-superscript">2</sup> → ℝ<sup class="fm-superscript">2</sup></span>. In fact, as depicted in figure <a class="url" href="02.xhtml#fig-rotation_matrix_diagram">2.14</a>, this transformation (multiplication by matrix <i class="timesitalic">R</i>) rotates the position vector of a point in the 2D plane by an angle of <span class="math">45°</span>.</p>
<p class="body"><a id="marker-50"/>The output and input points may belong to different spaces in such transforms. For instance, consider the matrix</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_02-14-p.png" width="120"/></p>
</div>
<p class="body">It is not hard to see that this matrix projects 3D points to the 2D X-Y plane:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="108" src="../../OEBPS/Images/eq_02-14-q.png" width="107"/></p>
</div>
<p class="body">Hence, this transformation (multiplication by matrix <i class="timesitalic">P</i>) projects points from three to two dimensions. In mathematical parlance, <span class="math"><i class="fm-italics">P</i> : ℝ<sup class="fm-superscript">3</sup> → ℝ<sup class="fm-superscript">2</sup></span>.</p>
<p class="body">The transforms <i class="timesitalic">R</i> and <i class="timesitalic">P</i> share a common property: <i class="fm-italics">they preserve collinearity</i>. This means a set of vectors (points) <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span>, <span class="math">⋯</span> that originally lay on a straight line remain so after the transformation.</p>
<p class="body">Let’s check this out for the rotation transformation in the example from section <a class="url" href="02.xhtml#sec-lin-dep">2.9</a>. There we saw four vectors:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="165" src="../../OEBPS/Images/eq_02-14-r.png" width="190"/></p>
</div>
<p class="body">These vectors all lie on a straight <span class="math"><i class="fm-italics">L</i> : <i class="fm-italics">x</i> = <i class="fm-italics">y</i></span>. The rotation transformed versions of these vectors are</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="169" src="../../OEBPS/Images/eq_02-14-s.png" width="316"/></p>
</div>
<p class="body">It is trivial to see that the transformed vectors also lie on a (different) straight line. In fact, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_osm.png" width="14"/></span><sup class="fm-superscript">′</sup></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">′</sup></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><sup class="fm-superscript">′</sup></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span><sup class="fm-superscript">′</sup></span> lie on the <i class="timesitalic">Y</i>-axis, which is the <span class="math">45°</span> rotated version of the original line <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">x</i></span>.</p>
<p class="body">It is trivial to see that the transformed vectors also lie on a (different) straight line. In fact, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_osm.png" width="14"/></span><sup class="fm-superscript">′</sup>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">′</sup></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><sup class="fm-superscript">′</sup></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_c.png" width="14"/></span><sup class="fm-superscript">′</sup></span> lie on the <i class="timesitalic">Y</i>-axis, which is the <span class="math">45°</span> rotated version of the original line <span class="math"><i class="fm-italics">y</i> = <i class="fm-italics">x</i></span>.</p>
<p class="body">The projection transform represented by matrix <i class="timesitalic">P</i> also preserves collinearity. Consider four collinear vectors in 3D:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="240" src="../../OEBPS/Images/eq_02-14-t.png" width="217"/></p>
</div>
<p class="body">The corresponding transformed vectors</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="169" src="../../OEBPS/Images/eq_02-14-u.png" width="284"/></p>
</div>
<p class="body">also lie on a straight line in 2D.</p>
<p class="body"><a id="marker-51"/>The class of transforms that preserves collinearity are known as <i class="fm-italics">linear transforms</i>. They can always be represented as a matrix multiplication. Conversely, all matrix multiplications represent a linear transformation. A more formal definition is provided later.</p>
<h3 class="fm-head1" id="generic-multidimensional-definition-of-linear-transforms">2.10.1 Generic multidimensional definition of linear transforms</h3>
<p class="body">A function <i class="timesitalic">ϕ</i> is a linear transform if and only if it satisfies</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">ϕ</i>(<i class="fm-italics">α</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> + <i class="fm-italics">β</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>) = <i class="fm-italics">αϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>) + <i class="fm-italics">βϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>) <span class="cambria">∀</span> <i class="fm-italics">α</i>, <i class="fm-italics">β</i> <span class="cambria">∈</span> ℝ</span></p>
<p class="fm-equation-caption">Equation 2.15 <span class="calibre" id="eq-lin-map"/></p>
<p class="body">In other words, <i class="fm-italics">a transform is linear if and only if the transform of the linear combination of two vectors is the same as the linear combination (with the same weights) of the transforms of individual vectors</i>. (This can be remembered as: <i class="fm-italics">Linear transform means transforms of linear combinations are same as linear combinations of transforms</i>.) Multiplication with a rotation or projection matrix (shown earlier) is a linear transform.</p>
<h3 class="fm-head1" id="all-matrix-vector-multiplications-are-linear-transforms">2.10.2 All matrix-vector multiplications are linear transforms</h3>
<p class="body">Let’s verify that matrix multiplication satisfies the definition of linear mapping (equation <a class="url" href="02.xhtml#eq-lin-map">2.15</a>). Let <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> <span class="cambria">∈</span> ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span> be two arbitrary <i class="timesitalic">n</i>-dimensional vectors and <i class="timesitalic">A<sub class="fm-subscript">m, n</sub></i> be an arbitrary matrix with <i class="timesitalic">n</i> columns. Then following the standard rules of matrix-vector multiplication,<a id="marker-52"/></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i>(<i class="fm-italics">α</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> + <i class="fm-italics">β</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>) = <i class="fm-italics">α</i>(<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>) + <i class="fm-italics">β</i>(<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span></p>
<p class="body">which mimics equation <a class="url" href="02.xhtml#eq-lin-map">2.15</a> with <i class="timesitalic">ϕ</i> replaced with matrix <i class="timesitalic">A</i>. Thus we have proven that all matrix multiplications are linear transforms. The reverse is not true. In particular, linear transforms that operate on infinite-dimensional vectors are not matrices. But all linear transforms that operate on finite-dimensional vectors can be expressed as matrices. (The proof is a bit more complicated and will be skipped.)</p>
<p class="body">Thus, in finite dimensions, multiplication with a matrix and linear transformation are one and the same thing. In section <a class="url" href="02.xhtml#sec-matrices">2.3</a>, we saw the array view of matrices. The corresponding geometric view, that all matrices represent linear transformation, was presented in this section.</p>
<p class="body">Let’s finish this section by studying an example of a transform that is <i class="fm-italics">not</i> linear. Consider the function</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">ϕ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>||</span></p>
<p class="body">for <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="cambria">∈</span> ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span>. This function <i class="timesitalic">ϕ</i> maps <i class="timesitalic">n</i>-dimensional vectors to a scalar that is the length of the vector, <span class="math"><i class="fm-italics">ϕ</i> : ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i> → ℝ</span>. We will examine if it satisfies equation <a class="url" href="02.xhtml#eq-lin-map">2.15</a> with <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">α</i><sub class="fm-subscript">2</sub> = 1</span>. For two specific vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> <span class="cambria">∈</span> ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span>,</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="277" src="../../OEBPS/Images/eq_02-15-a1.png" width="574"/></p>
</div>
<p class="body">Now</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_02-15-b.png" width="410"/></p>
</div>
<p class="body">and</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="134" src="../../OEBPS/Images/eq_02-15-c.png" width="574"/></p>
</div>
<p class="body">Clearly, these two are not equal; hence, we have violated equation <a class="url" href="02.xhtml#eq-lin-map">2.15</a>: <i class="timesitalic">ϕ</i> is a nonlinear mapping.</p>
<h2 class="fm-head" id="multidimensional-arrays-multilinear-transforms-and-tensors">2.11 Multidimensional arrays, multilinear transforms, and tensors</h2>
<p class="body"><a id="marker-53"/>We often hear the term <i class="fm-italics">tensor</i> in connection with machine learning. Google’s famous machine learning platform is named <i class="fm-italics">TensorFlow</i>. In this section, we will introduce you to the concept of a tensor.</p>
<h3 class="fm-head1" id="multidim-array">2.11.1 Array view: Multidimensional arrays of numbers</h3>
<p class="body">A tensor may be viewed as a generalized <i class="timesitalic">n</i>-dimensional array—although, strictly speaking, not all multidimensional arrays are tensors. We will learn more about the distinction between multidimensional arrays and tensors when we study multilinear transforms. For now, we will not worry too much about the distinction. A vector can be viewed as a <span class="math">1</span> tensor, a matrix is a <span class="math">2</span> tensor, and a scalar is a <span class="math">0</span> tensor.</p>
<p class="body">In section <a class="url" href="02.xhtml#sec-matrices">2.3</a>, we saw that digital images are represented as 2D arrays (matrices). A color image—where each pixel is represented by three colors, R, G, and B (red, green, and blue)—is an example of a multidimensional array or tensor. This is because it can be viewed as a combination of three images: the R, G, and B images, respectively.</p>
<p class="body">The inputs and outputs to each layer in a neural network are also tensors.</p>
<h2 class="fm-head" id="sec-lin_systems">2.12 Linear systems and matrix inverse</h2>
<p class="body">Machine learning today is usually an iterative process. Given a set of training data, you want to estimate a set of machine parameters that will yield target values (or close approximations to them) on training inputs. The number of training inputs and the size of the parameter set are often very large. This makes it impossible to have a closed-form solution where we solve for the unknown parameters in a single step. Solutions are usually iterative. We start with a guessed set of values for the parameters and iteratively improve the guess by processing training data.</p>
<p class="body">Having said that, we often encounter smaller problems in real life. We are better off using more traditional closed-form techniques here since they are much faster and more accurate. This section is devoted to gaining some insights into these techniques.</p>
<p class="body">Let’s go back to our familiar cat-brain problem and refer to its training data in table <a class="url" href="02.xhtml#tab-cat-brain-training-data">2.2</a>. As before, we are still talking about a weighted sum model with three parameters: weights <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span> and bias <i class="timesitalic">b</i>. Let’s focus on the top three rows from the table, repeated here in table <a class="url" href="02.xhtml#tab-cat-brain-training-data-trunc">2.2</a> for convenience.</p>
<p class="fm-table-caption">Table 2.3 Example training dataset for our toy machine learning–based cat brain</p>
<table border="1" class="contenttable-1-table" id="tab-cat-brain-training-data-trunc" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="10%"/>
<col class="contenttable-0-col" span="1" width="30%"/>
<col class="contenttable-0-col" span="1" width="30%"/>
<col class="contenttable-0-col" span="1" width="30%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th" colspan="1" rowspan="1"/>
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Input value: Hardness</p>
</th>
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Input value: Sharpness</p>
</th>
<th class="contenttable-1-th" colspan="1" rowspan="1">
<p class="fm-table-head">Output: Threat score</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.11</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.09</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">−</span>0.8</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.01</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.02</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body"><span class="math">−</span>0.97</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">2</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.98</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.91</p>
</td>
<td class="contenttable-1-td" colspan="1" rowspan="1">
<p class="fm-table-body">0.89</p>
</td>
</tr>
</tbody>
</table>
<p class="body">The training data says that with a hardness value <span class="math">0.11</span> and a sharpness value <span class="math">0.09</span>, we expect the system’s output to match or closely approximate) the target value <span class="math">−0.8</span>, and so on. In other words, our estimated values for parameters <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>, <i class="timesitalic">b</i> should ideally satisfy</p>
<p class="fm-equation"><span class="math">0.11<i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.09<i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b = –0.8</span><br class="calibre20"/>
<span class="math">0.01<i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.02<i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b = –0.97</span><br class="calibre20"/>
<span class="math">0.98<i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.91 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b = 0.89</span></p>
<p class="body">We can express this via matrix multiplication as the following equation:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="113" src="../../OEBPS/Images/eq_02-15-d.png" width="263"/></p>
</div>
<p class="body"><a id="marker-54"/>How do we obtain the values of <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>, <i class="timesitalic">b</i> that make this equation true? That is, how do we solve this equation? There are formal methods (discussed later) to directly solve such equations for <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>, and <i class="timesitalic">b</i> (in this very simple example, you might just “see” that <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> = 1, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> = 1, <i class="fm-italics">b</i> = −1</span> solves the equation, but we need a general method).</p>
<p class="body">This equation is an example of a class of equations called a <i class="fm-italics">linear system</i>. A linear system in <i class="timesitalic">n</i> unknowns <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">2</sub></span>, <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">3</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">x<sub class="fm-subscript">n</sub></i>,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">11</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">a</i><sub class="fm-subscript">12</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">a</i><sub class="fm-subscript">13</sub><i class="fm-italics">x</i><sub class="fm-subscript">3</sub> + … + <i class="fm-italics">a</i><sub class="fm-subscript">1<i class="fm-italics1">n</i></sub><i class="fm-italics">x<sub class="fm-subscript">n</sub></i> = <i class="fm-italics">b</i><sub class="fm-subscript">1</sub></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">21</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">a</i><sub class="fm-subscript">22</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">a</i><sub class="fm-subscript">23</sub><i class="fm-italics">x</i><sub class="fm-subscript">3</sub> + … + <i class="fm-italics">a</i><sub class="fm-subscript">2<i class="fm-italics1">n</i></sub><i class="fm-italics">x<sub class="fm-subscript">n</sub></i> = <i class="fm-italics">b</i><sub class="fm-subscript">2</sub></span><br class="calibre20"/>
<span class="math">                                               ⁞</span><br class="calibre20"/>
<span class="math"><i class="fm-italics">a</i><sub class="fm-subscript"><i class="fm-italics1">n</i>1</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">a</i><sub class="fm-subscript"><i class="fm-italics1">n</i>2</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">a</i><sub class="fm-subscript"><i class="fm-italics1">n</i>3</sub><i class="fm-italics">x</i><sub class="fm-subscript">3</sub> + … + <i class="fm-italics">a<sub class="fm-subscript">nn</sub></i><i class="fm-italics">x<sub class="fm-subscript">n</sub></i> = <i class="fm-italics">b<sub class="fm-subscript">n</sub></i></span></p><!--<div class="figure">
<p class="figure2"><img src="imgs/equations/eq_02-15-f.png" alt="" /></p>
</div>-->
<p class="body">can be expressed via matrix and vectors as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">where</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="156" src="../../OEBPS/Images/eq_02-15-e.png" width="586"/></p>
</div>
<p class="body">Although equivalent, the matrix depiction is more compact and dimension-independent. In machine learning, we usually have many variables (thousands), so this compactness makes a significant difference. Also, <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> looks similar to the one-variable equation we know so well: <span class="math"><i class="fm-italics">ax</i> = <i class="fm-italics">b</i></span>. In fact, many intuitions can be transferred from 1D to higher dimensions.</p>
<p class="body">What is the solution to the 1D equation? You may have learned it in fifth grade: The solution of <span class="math"><i class="fm-italics">ax</i> = <i class="fm-italics">b</i></span> is <span class="math"><i class="fm-italics">x</i> = <i class="fm-italics">a</i><sup class="fm-superscript">−1</sup><i class="fm-italics">b</i></span> where <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">−1</sup> = 1/<i class="fm-italics">a</i>, <i class="fm-italics">a</i> ≠ 0</span>.</p>
<p class="body">We can use the same notation in all dimensions. The solution of <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> is <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="math">= <i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>, where <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span> is the matrix inverse. The inverse matrix <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span> has the determinant of the matrix, <span class="math">1/<i class="fm-italics">det</i>(<i class="fm-italics">A</i>)</span>, as a factor. We will not discuss determinant and inverse matrix computation here—you can obtain that in any standard linear algebra textbook—but will state some facts that lend insights into determinants and inverse matrices:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The inverse matrix <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span> is related to matrix <i class="timesitalic">A</i> in the same way the scalar <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">−1</sup></span> is related to the scalar <i class="timesitalic">a</i>. <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">−1</sup></span> exists if and only if <span class="math"><i class="fm-italics">a</i> ≠ 0</span>. Analogously, <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span> exists if <span class="math"><i class="fm-italics">det</i>(<i class="fm-italics">A</i>) ≠ 0</span>, where <span class="math"><i class="fm-italics">det</i>(<i class="fm-italics">A</i>)</span> refers to the determinant of a matrix.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The product of a scalar <i class="timesitalic">a</i> and its inverse <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">−1</sup></span> is <span class="math">1</span>. Analogously, <span class="math"><i class="fm-italics">AA</i><sup class="fm-superscript">−1</sup> = <i class="fm-italics">A</i><sup class="fm-superscript">−1</sup><i class="fm-italics">A</i> = <b class="fm-bold">I</b></span>, where <b class="timesbold">I</b> denotes the identity matrix that is the higher-dimension analog for <span class="math">1</span> in scalar arithmetic. It is a matrix in which the diagonal terms are <span class="math">1</span> and all other terms are <span class="math">0</span>. The <i class="timesitalic">n</i>-dimensional identity matrix is as follows:</p>
</li>
</ul>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="206" src="../../OEBPS/Images/eq_02-15-f.png" width="212"/></p>
</div>
<p class="body-ind">When there is no subscript, the dimensionality can be inferred from the context. For any matrix <i class="timesitalic">A</i>, <span class="math"><b class="fm-bold">I</b><i class="fm-italics">A</i> = <i class="fm-italics">A</i><b class="fm-bold">I</b> = <i class="fm-italics">A</i></span>. For any vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, <span class="math"><b class="fm-bold">I</b><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><b class="fm-bold">I</b> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span></span>. These can be easily verified using the rules of matrix multiplication.</p>
<p class="body"><a id="marker-55"/>There are completely precise but tedious rules for computing determinants and matrix inverses. Despite the importance of the concept, we rarely need to compute them in life as all linear algebra software packages provide routines to do this. Furthermore, computing matrix inverses is not good programming practice because it is numerically unstable. We will not discuss the direct computation of determinant or matrix inverse here (except that in section <a class="url" href="../Text/A.xhtml#sec-det2x2">A.2</a> of the appendix, we show how to compute the determinant of a <span class="math">2 × 2</span> matrix). We will discuss pseudo-inverses, which have more significance in machine learning.</p>
<h3 class="fm-head1" id="linear-systems-with-zero-or-near-zero-determinantsand-ill-conditioned-systems">2.12.1 Linear systems with zero or near-zero determinants,and ill-conditioned systems</h3>
<p class="body">We saw earlier that a linear system <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> has the solution <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">A</i><sup class="fm-superscript">−1</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. But <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span> has <span class="math">1/<i class="fm-italics">det</i>(<i class="fm-italics">A</i>)</span> as a factor. What if the determinant is zero?</p>
<p class="body">The short answer: when the determinant is zero, the linear system cannot be exactly solved. We may still attempt to come up with an approximate answer (see section <a class="url" href="02.xhtml#subsec-over-under-determined-linsys">2.12.3</a>), but an exact solution is not possible.</p>
<p class="body">Let’s examine the situation a bit more closely with the aid of an example. Consider the following system of equations:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">  x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">2</sub>   = 2</span><br class="calibre20"/>
<span class="math">2<i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + 2<i class="fm-italics">x</i><sub class="fm-subscript">2</sub>  = 4</span></p>
<p class="body">It can be rewritten as a linear system with a square matrix:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_02-15-g.png" width="138"/></p>
</div>
<p class="body"><a id="marker-56"/>But you can quickly see that the system of equations cannot be solved. The second equation is really the same as the first. In fact, we can obtain the second by multiplying the first by a scalar, <span class="math">2</span>. Hence, we don’t really have two equations: we have only one, so the system cannot be solved. Now examine the row vectors of matrix <i class="timesitalic">A</i>. They are <span class="math">[1   1]</span> and <span class="math">[2   2]</span>. They are linearly dependent because <span class="math">−2[1   1] + [2   2] = 0</span>. Now examine the determinant of matrix <i class="timesitalic">A</i> (section <a class="url" href="../Text/A.xhtml#sec-det2x2">A.2</a> of the appendix shows how to compute the determinant of a <span class="math">2 × 2</span> matrix). It is <span class="math">2 × 1 − 1 × 2 = 0</span>. These results are not coincidences. Any one of them implies the other. In fact, the following statements about the linear system <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> (with a square matrix) are equivalent:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Matrix <i class="timesitalic">A</i> has a row/column that can be expressed as a weighted sum of the others.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Matrix <i class="timesitalic">A</i> has linearly dependent rows or columns.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Matrix <i class="timesitalic">A</i> has zero determinant (such matrices are called <i class="fm-italics">singular</i> matrices).</p>
</li>
<li class="fm-list-bullet">
<p class="list">The inverse of matrix <i class="timesitalic">A</i> (i.e., <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span>) does not exist. <i class="timesitalic">A</i> is called <i class="fm-italics">singular</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The linear system cannot be solved.</p>
</li>
</ul>
<p class="body">The system is trying to tell you that you have fewer equations than you think you have, and you cannot solve the system of equations.</p>
<p class="body">Sometimes the determinant is not exactly zero but close to zero. Although solvable in theory, such systems are <i class="fm-italics">numerically unstable</i>. Small changes in input cause the result to change drastically. For instance, consider this nearly singular matrix:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="75" src="../../OEBPS/Images/eq_02-16.png" width="129"/></p>
</div>
<p class="fm-equation-caption">Equation 2.16</p>
<p class="body">Its determinant is <span class="math">0.002</span>, close to zero. Let <!--<span class="times">$\vec{b}=\begin{bmatrix} 3\\[-2pt] 6\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-16-a.png" width="52"/></span> be a vector.</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_02-17.png" width="207"/></p>
</div>
<p class="fm-equation-caption">Equation 2.17</p>
<p class="body">(Note how large the elements of <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span> are. This is due to division by an extremely small determinant and, in turn, causes the instability illustrated next.) The solution to the equation <span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_02-17-a.png" width="209"/></span>. But if we change <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> just a little and make <span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-17-b.png" width="81"/></span>, the solution changes to a drastically different <span class="infigure"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_02-17-c.png" width="144"/></span>. This is inherently unstable and arises from the near singularity of the matrix <i class="timesitalic">A</i>. Such linear systems are called <i class="fm-italics">ill-conditioned</i>.</p>
<h3 class="fm-head1" id="pytorch-code-for-inverse-determinant-and-singularity-testing-of-matrices">2.12.2 PyTorch code for inverse, determinant, and singularity testing of matrices</h3>
<p class="body"><a id="marker-57"/>Inverting a matrix and computing its determinant can be done with a single function call from the linear algebra package linalg.</p>
<p class="fm-code-listing-caption" id="listing-2.10-matrix-inverse-for-an-invertible-matrix-nonzero-determinant">Listing 2.10 Matrix inverse for an invertible matrix (nonzero determinant)</p>
<pre class="programlisting">def determinant(A):
    return torch.linalg.det(A)

def inverse(A):
    return torch.linalg.inv(A)

A = torch.tensor([[2, 3], [2, 2]], dtype=torch.float    <span class="fm-combinumeral">①</span>

A_inv = inverse(A)                                      <span class="fm-combinumeral">②</span>

I = torch.eye(2)                                        <span class="fm-combinumeral">③</span>

assert torch.all(torch.matmul(A, A_inv) == I)           <span class="fm-combinumeral">④</span>
assert torch.all(torch.matmul(A_inv, A) == I)


assert torch.all(torch.matmul(I, A) ==  A)
assert torch.all(A == torch.matmul(A,I))                <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="infigure"><img alt="" class="calibre5" height="37" src="../../OEBPS/Images/eq_02-17-d.png" width="81"/></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="infigure"><img alt="" class="calibre5" height="35" src="../../OEBPS/Images/eq_02-17-e.png" width="106"/></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The PyTorch function <span class="math"><i class="fm-italics">torch</i>.<i class="fm-italics">eye</i>(<i class="fm-italics">n</i>)</span> generates an identity matrix <i class="timesitalic">I</i> of size <i class="timesitalic">n</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Verify <span class="infigure"><img alt="" class="calibre5" height="34" src="../../OEBPS/Images/eq_02-17-f.png" width="195"/></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> <b class="timesbold">I</b> is like <span class="math">1</span>. Verify <span class="math"><i class="fm-italics">A</i><b class="fm-bold">I</b> = <b class="fm-bold">I</b><i class="fm-italics">A</i> = <i class="fm-italics">A</i></span></p>
<p class="body">A singular matrix is a matrix whose determinant is zero. Such matrices are non-invertible. Linear systems of equations with singular matrices cannot be solved.</p>
<p class="fm-code-listing-caption" id="listing-2.11-singular-matrix">Listing 2.11 Singular matrix</p>
<pre class="programlisting">B = torch.tensor([[1, 1], [2, 2]], dtype=torch.float)   <span class="fm-combinumeral">①</span>
try:

    B_inv = inverse(B)                                  <span class="fm-combinumeral">②</span>

except RuntimeError as e:
    print("B cannot be inverted: {}".format(B, e))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="infigure"><img alt="" class="calibre5" height="36" src="../../OEBPS/Images/eq_02-17-g.png" width="79"/></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Determinant = <span class="math">1 × 2 − 2 × 1 = 0</span>. Singular matrix; attempting to compute the inverse causes a runtime error.</p>
<h3 class="fm-head1" id="subsec-over-under-determined-linsys">2.12.3 Over- and under-determined linear systems in machine learning</h3>
<p class="body">What if the matrix <i class="timesitalic">A</i> is <i class="fm-italics">not</i> square? This implies that the number of equations does not match the number of unknowns. Does such a system even make sense? Surprisingly, it does. As a rule, machine learning systems fall in this category: the number of equations corresponds to the number of training data instances collected, while the number of unknowns is a function of the number of weights in the model which is a function of the particular model family chosen to represent the system. These are independent of each other. As stated earlier, we often solve these systems iteratively. Nonetheless, it is important to understand linear systems with nonsquare matrices <i class="timesitalic">A</i>, to gain insight.</p>
<p class="body">There are two possible cases, assuming that the matrix <i class="timesitalic">A</i> is <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span> (<i class="timesitalic">m</i> rows and <i class="timesitalic">n</i> columns):</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Case 1: <span class="math"><i class="fm-italics">m</i> &gt; <i class="fm-italics">n</i></span> (more equations than unknowns; overdetermined system)</p>
</li>
<li class="fm-list-bullet">
<p class="list">Case 2: <span class="math"><i class="fm-italics">m</i> &lt; <i class="fm-italics">n</i></span> (fewer equations than unknown; underdetermined system)</p>
</li>
</ul>
<p class="body"><a id="marker-58"/>For instance, table <a class="url" href="02.xhtml#tab-cat-brain-training-data">2.2</a> leads to an overdetermined linear system. Let’s write the system of equations:</p>
<p class="fm-equation"><span class="math">0.11 <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.09 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b = –0.8</span><br class="calibre20"/>
<span class="math">0.01 <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.02 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b = –0.97</span><br class="calibre20"/>
<span class="math">0.98 <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.91 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b =   0.89</span><br class="calibre20"/>
<span class="math">0.12 <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.21 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b = –0.68</span><br class="calibre20"/>
<span class="math">0.98 <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.99 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b =   0.95</span><br class="calibre20"/>
<span class="math">0.85 <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.87 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b =   0.74</span><br class="calibre20"/>
<span class="math">0.03 <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.14 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b = –0.88</span><br class="calibre20"/>
<span class="math">0.55 <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> + 0.45 <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> + b =   0.00</span></p>
<p class="body">These yield the following overdetermined linear system:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="287" src="../../OEBPS/Images/eq_02-18.png" width="268"/></p>
</div>
<p class="fm-equation-caption">Equation 2.18 <span class="calibre" id="eq-overdetemined_system_example"/></p>
<p class="body">This is a nonsquare <span class="math">15 × 3</span> linear system. There are only <span class="math">3</span> unknowns to solve for (<span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">b</i></span>), and there are <span class="math">15</span> equations. This is highly redundant: we needed only three equations and could have solved it via linear system solution techniques (section <a class="url" href="02.xhtml#sec-lin_systems">2.12</a>). But the important thing to note is this: <i class="fm-italics">the equations are not fully consistent</i>. There is no single set of values for the unknown that will satisfy all of them. In other words, the training data is noisy—an almost universal occurrence in real-life machine learning systems. Consequently, we have to find a solution that is optimal (causes as little error as possible) over all the equations.</p>
<p class="body">We want to solve it such that the overall error <span class="math">||<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||</span> is minimized. In other words, we are looking for <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> such that <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> is as close to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> as possible. This closed-form that is, non-iterative) method is an extremely important precursor to machine learning and data science. We will revisit this multiple times, most notably in sections <a class="url" href="02.xhtml#subsec-moore-penrose-pseudoinverse">2.12.4</a> and <a class="url" href="../Text/04.xhtml#sec-svd">4.5</a>.</p>
<h3 class="fm-head1" id="subsec-moore-penrose-pseudoinverse">2.12.4 Moore Penrose pseudo-inverse of a matrix</h3>
<p class="body"><a id="marker-59"/>The pseudo-inverse is a handy technique to solve over- or under-determined linear systems. Suppose we have an overdetermined system with the not-necessarily square <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">A</i>:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">Since <i class="timesitalic">A</i> is not guaranteed to be square, we can take neither the determinant nor the inverse in general. So the usual <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> does not work. At this point, we observe that although the inverse cannot be taken, transposing the matrix is always possible. Let’s multiply both sides of the equation with <i class="timesitalic">A<sup class="fm-superscript">T</sup></i>:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> <span class="cambria">⇔</span> <i class="fm-italics">A<sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">Notice that <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> is a square matrix: its dimensions are <span class="math">(<i class="fm-italics">m</i>×<i class="fm-italics">n</i>) × (<i class="fm-italics">n</i>×<i class="fm-italics">m</i>) = <i class="fm-italics">m</i> × <i class="fm-italics">m</i></span>. Let’s assume, without proof for the moment, that it is invertible. Then</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> <span class="cambria">⇔</span> <i class="fm-italics">A<sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> <span class="cambria">⇔</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = (<i class="fm-italics">A<sup class="fm-superscript">T</sup>A</i>)<sup class="fm-superscript">−1</sup><i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">Hmmm, not bad; we seem to be onto something. In fact, we just derived the <i class="fm-italics">pseudo-inverse</i> of matrix <i class="timesitalic">A</i>, denoted <span class="math"><i class="fm-italics">A</i> <sup class="fm-superscript">+</sup> = (<i class="fm-italics">A<sup class="fm-superscript">T</sup>A</i>)<sup class="fm-superscript">−1</sup><i class="fm-italics">A<sup class="fm-superscript">T</sup></i></span>. Unlike the inverse, the pseudo-inverse does not need the matrix to be square with linearly independent rows. Much like the regular linear system, we get the solution of the (possibly nonsquare) system of equations as <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> <span class="cambria">⇔</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">A</i> <sup class="fm-superscript">+</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>.</p>
<p class="body">The pseudo-inverse-based solution actually minimizes the error <span class="math">||<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||</span>. We will provide an intuitive proof of that in section <a class="url" href="02.xhtml#subsec-pseudo-inv-geometric-intuition">2.12.5</a>. Meanwhile, you are encouraged to write the Python code to evaluate <span class="math">(<i class="fm-italics">A<sup class="fm-superscript">T</sup>A</i>)<sup class="fm-superscript">−1</sup><i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> and verify that it approximately yields the expected answer <!--<span class="times">$\begin{bmatrix} 1\\[-3pt]1\\[-3pt]-1\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="58" src="../../OEBPS/Images/eq_02-18-a.png" width="35"/></span> for equation <a class="url" href="02.xhtml#eq-overdetemined_system_example">2.18</a>.</p>
<h3 class="fm-head1" id="subsec-pseudo-inv-geometric-intuition">2.12.5 Pseudo-inverse of a matrix: A beautiful geometric intuition</h3>
<p class="body">A matrix <span class="math"><i class="fm-italics">A</i><sub class="fm-subscript"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></sub></span> can be rewritten in terms of its column vectors as <span class="math">[<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sub class="fm-subscript">1</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sub class="fm-subscript">2</sub>, … <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i>]</span>, where <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sub class="fm-subscript">1</sub> … <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span> are all <i class="timesitalic">m</i>-dimensional vectors. Then if <!--<span class="times">$\vec{x}=\begin{bmatrix}x_{1}\\[-3pt]x_{2}\\[-3pt]\ldots\\[-3pt]x_{n}\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_02-18-b.png" width="71"/></span>, we get <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">x</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sub class="fm-subscript">2</sub> + ⋯ <i class="fm-italics">x<sub class="fm-subscript">n</sub></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span>. In other words, <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> is just a linear combination of the column vectors of <i class="timesitalic">A</i> with the elements of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> as the weights (you are encouraged to write out a small <span class="math">3 × 3</span> system and verify this). The space of all vectors of the form <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> (that is, the linear span of the column vectors of <i class="timesitalic">A</i>) is known as the <i class="fm-italics">column space</i> of <i class="timesitalic">A</i>.</p>
<p class="body">The solution to the linear system of equations <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> can be viewed as finding the <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> that minimizes the difference of <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>: that is, minimizes <span class="math">||<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||</span>. This means we are trying to find a point in the column space of <i class="timesitalic">A</i> that is closest to the point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. Note that this interpretation does not assume a square matrix <i class="timesitalic">A</i>. Nor does it assume a nonzero determinant. In the friendly case where the matrix <i class="timesitalic">A</i> is square and invertible, we can find a vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> such that <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> becomes exactly equal to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>, which makes <span class="math">||<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>|| = 0</span>. If <i class="timesitalic">A</i> is not square, we will try to find <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> such that <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> is closer to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> than any other vector in the column space of <i class="timesitalic">A</i>. Mathematically speaking, <a class="url" href="02.xhtml#fn7" id="fnref7"><sup class="fm-superscript">4</sup></a></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_02-19.png" width="250"/></p>
</div>
<p class="fm-equation-caption">Equation 2.19 <span class="calibre" id="eq-linear_system_minimization_problem"/></p>
<p class="body"><a id="marker-60"/>From geometry, we intuitively know that the closest point to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> in the column space of <i class="timesitalic">A</i> is obtained by dropping a perpendicular from <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> to the column space of <i class="timesitalic">A</i> (see figure <a class="url" href="02.xhtml#fig-nonsquare_linear_system_diagram">2.12</a>). The point where this perpendicular meets the column space is called the <i class="fm-italics">projection</i> of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> on the column space of <i class="timesitalic">A</i>. The solution vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> to equation <a class="url" href="02.xhtml#eq-linear_system_minimization_problem">2.19</a> that we are looking for should correspond to the projection of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> on the column space of <i class="timesitalic">A</i>. This in turn means <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> − <i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is orthogonal (perpendicular) to all vectors in the column space of <i class="timesitalic">A</i> (see figure <a class="url" href="02.xhtml#fig-nonsquare_linear_system_diagram">2.12</a>). We represent arbitrary vectors in the column space of <i class="timesitalic">A</i> as <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span></span> for arbitrary <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>. Hence, for all such <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>,</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="86" src="../../OEBPS/Images/eq_02-19-a.png" width="313"/></p>
</div>
<p class="body">For the previous equation to be true for all vectors <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>, we must have <span class="math"><i class="fm-italics">A<sup class="fm-superscript">T</sup></i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>−<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = 0</span>.</p>
<p class="body">Thus, we have</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="116" src="../../OEBPS/Images/eq_02-19-b.png" width="172"/></p>
</div>
<p class="body">which is exactly the Moore-Penrose pseudo-inverse.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="655" id="fig-nonsquare_linear_system_diagram" src="../../OEBPS/Images/CH02_F12_Chaudhury.png" width="754"/></p>
<p class="figurecaption">Figure 2.12 Solving a linear system <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> is equivalent to finding the point on the column space of <i class="timesitalic">A</i> that is closest to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. This means we have to drop a perpendicular from <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> to column space of <i class="timesitalic">A</i>. If <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> represents the point where that perpendicular meets the column space (aka projection), the difference vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> − <i class="timesitalic">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> corresponds to the line joining <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> and its projection <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>. This line will be perpendicular to all vectors in the column space of <i class="timesitalic">A</i>. Equivalently, it is perpendicular to <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span></span> for any arbitrary <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>.</p>
</div>
<p class="body">For a machine learning-centric example, consider the overdetermined system corresponding to the cat brain earlier in the chapter. There are 15 training examples, each with input and desired outputs specified.</p>
<p class="body">Our goal is to determine three unknowns <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span>, and <i class="timesitalic">b</i> such that for each training input <span class="infigure"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_02-19-c.png" width="79"/></span>, the model output</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="112" src="../../OEBPS/Images/eq_02-20.png" width="547"/></p>
</div>
<p class="fm-equation-caption">Equation 2.20</p>
<p class="body">matches the desired output (aka ground truth) <i class="timesitalic">ȳ<sub class="fm-subscript">i</sub></i> as closely as possible.<a id="marker-61"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> We employed a neat trick here: we added a <span class="math">1</span> to the right of the input, which allows us to depict the entire system (including the bias) in a single compact matrix-vector multiplication. We call this <i class="fm-italics">augmentation</i>—we augment the input row vector with an extra <span class="math">1</span> on the right.</p>
<p class="body">Collating all the training examples together, we get</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="146" src="../../OEBPS/Images/eq_02-21.png" width="238"/></p>
</div>
<p class="fm-equation-caption">Equation 2.21</p>
<p class="body">which can be expressed compactly as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">X</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span></span></p>
<p class="body">where <i class="timesitalic">X</i> is the augmented input matrix with a rightmost column of all <span class="math">1</span>s. The goal is to minimize <span class="math">||<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> – <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span>||</span>. To this end, we formulate the over-determined linear system</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">X</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> = <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span></span></p>
<p class="body">Note that <i class="fm-italics">this is not a classic system of equations—it has more equations than unknowns</i>. We cannot solve this via matrix inversion. We <i class="fm-italics">can</i>, however, use the pseudo-inverse mechanism to solve it. The resulting solution yields the “best fit” or “best effort” solution, which minimizes the total error over all the training examples.</p>
<p class="body">The exact numerical system (repeated here for ease of reference) is</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="266" src="../../OEBPS/Images/eq_02-22.png" width="422"/></p>
</div>
<p class="fm-equation-caption">Equation 2.22 <span class="calibre" id="eq-lin-model"/></p>
<p class="body">We solve for <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> using the pseudo-inverse formula <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> = (<i class="fm-italics">X<sup class="fm-superscript">T</sup>X</i>)<sup class="fm-superscript">–1</sup><i class="fm-italics">X<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y2.png" width="15"/></span></span></p>
<h3 class="fm-head1" id="python-overdet">2.12.6 PyTorch code to solve overdetermined systems<a id="marker-62"/></h3>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this section, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/PPJ2">http://mng.bz/PPJ2</a>.</p>
<p class="fm-code-listing-caption" id="listing-2.12-solving-an-overdetermined-system-using-the-pseudo-inverse">Listing 2.12 Solving an overdetermined system using the pseudo-inverse</p>
<pre class="programlisting">def pseudo_inverse(A):
    return torch.matmul(torch.linalg.inv(torch.matmul(A.T, A)), A.T)

<span class="fm-combinumeral">①</span>
X = torch.column_stack((X, torch.ones(15))) <span class="fm-combinumeral">②</span>

<span class="fm-combinumeral">③</span>
w = torch.matmul(pseudo_inverse(X), y)      <span class="fm-combinumeral">④</span>

print("The solution is {}".format(w))       <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> X is the augmented data matrix from equation <a class="url" href="02.xhtml#eq-lin-model">2.22</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The Pytorch column stack operator<br class="calibre20"/>
  adds a column to a matrix. Here, the added column is all <span class="math">1</span>s</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> It is easy to verify that the solution to equation <a class="url" href="02.xhtml#eq-lin-model">2.22</a> is roughly <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub> = 1, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> = 1, <i class="fm-italics">b</i> = −1</span>. But the equations are not consistent: no one solution perfectly fits all of them. The pseudo-inverse finds the “best fit” solution: it minimizes total error for all the equations.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Expect the solution to be close to <span class="math">[1, 1, −1]</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The solution is <span class="math">[1.08, 0.90, −0.96]</span></p>
<h2 class="fm-head" id="sec-eigen-values-vectors">2.13 Eigenvalues and eigenvectors: Swiss Army knives of machine learning</h2>
<p class="body">Machine learning and data science are all about finding patterns in large volumes of high-dimensional data. The inputs are feature vectors introduced in section <a class="url" href="02.xhtml#sec-vectors">2.1</a>) in high-dimensional spaces. Each feature vector can be viewed as a point in the feature space descriptor for an input instance. Sometimes we transform these feature vectors—map the feature points to a friendlier space—to simplify the data by reducing dimensionality. This is done by eliminating axes along which there is scant variation in the data. Eigenvalues and eigenvectors are invaluable tools in the arsenal of a machine learning engineer or a data scientist for this purpose. In chapter <a class="url" href="../Text/04.xhtml#chap-linalg-tools-ml">4</a>, we will study how to use these tools to simplify and find broad patterns in a large volume of multidimensional data.</p>
<p class="body">Let’s take an informal look at eigenvectors first. They are properties of square matrices. As seen earlier, matrices can be viewed as linear transforms which map vectors (points) in one space to different vectors (points) in the same or a different space. But a typical linear transform leaves a few points in the space (almost) unaffected. These points are called <i class="fm-italics">eigenvectors</i>. They are important physical aspects of the transform. Let’s look at a simple example. Suppose we are <i class="fm-italics">rotating</i> points in 3D space about the <i class="timesitalic">Z</i>-axis (see figure <a class="url" href="02.xhtml#fig-rotation_about_z_diagram">2.13</a>). The points on the <i class="timesitalic">Z</i>-axis will stay where they were despite the rotation. In general, points on the axis of rotation (<i class="timesitalic">Z</i> in this case) do not go anywhere after rotation. The axis of rotation is an eigenvector of the rotation transformation.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="464" id="fig-rotation_about_z_diagram" src="../../OEBPS/Images/CH02_F13_Chaudhury.png" width="482"/></p>
<p class="figurecaption">Figure 2.13 During rotation, points on the axis of rotation do not change position.</p>
</div>
<p class="body"><a id="marker-63"/>Extending this idea, when <i class="fm-italics">transforming</i> vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> with a matrix <i class="timesitalic">A</i>, are there vectors that do not change, at least in direction? Turns out the answer is yes. These are the so-called <i class="fm-italics">eigenvectors</i>—they do not change direction when undergoing linear transformation by a matrix <i class="timesitalic">A</i>. To be precise, if <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> is an eigenvector of the square matrix <i class="timesitalic">A</i>, <a class="url" href="02.xhtml#fn8" id="fnref8"><sup class="fm-superscript">5</sup></a> then</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i> = <i class="fm-italics">λ<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i></span></p>
<p class="body">Thus the linear transformation (that is, multiplication by matrix <i class="timesitalic">A</i>) has changed the length but not the direction of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> because <i class="timesitalic">λ</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> is parallel to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span>.</p>
<p class="body">How do we obtain <i class="timesitalic">λ</i> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span>? Well,</p>
<p class="fm-equation"><span class="math">      <i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> = <i class="fm-italics">λ</i> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></span><br class="calibre20"/>
<span class="math"><span class="cambria">⇔</span> <i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> - <i class="fm-italics">λ</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span></span><br class="calibre20"/>
<span class="math"><span class="cambria">⇔</span> (<i class="fm-italics">A</i> - <i class="fm-italics">λ</i> <b class="fm-bold">I</b>)<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span></span></p>
<p class="body">where <b class="timesbold">I</b> denotes the identity matrix.</p>
<p class="body">Of course, we are only interested in nontrivial solutions, where <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> ≠ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span></span>. In that case, <span class="math"><i class="fm-italics">A</i> – <i class="fm-italics">λ</i><b class="fm-bold">I</b></span> cannot be invertible, because if it were, we could obtain the contradictory solution <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> = (<i class="fm-italics">A</i> – <i class="fm-italics">λ</i> <b class="fm-bold">I</b>)<sup class="fm-superscript">–1</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_0.png" width="14"/></span></span>. Thus, <span class="math">(<i class="fm-italics">A</i> − <i class="fm-italics">λ</i><b class="fm-bold">I</b>)</span> is non-invertible, implying the determinant</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">det</i>(<i class="fm-italics">A</i> − <i class="fm-italics">λ</i><b class="fm-bold">I</b>) = 0</span></p>
<p class="body">For an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">A</i>, this yields an <i class="timesitalic">n</i>th-degree polynomial equation with <i class="timesitalic">n</i> solutions for the unknown <i class="timesitalic">λ</i>. <i class="fm-italics">Thus, an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> matrix has n eigenvalues, not necessarily all distinct</i>.</p>
<p class="body">Let’s compute eigenvalues and eigenvectors of a <span class="math">3 × 3</span> matrix, just for kicks. The matrix we use is carefully chosen, as will be evident soon. But for now, think of it as an arbitrary matrix:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="112" src="../../OEBPS/Images/eq_02-23.png" width="159"/></p>
</div>
<p class="fm-equation-caption">Equation 2.23 <span class="calibre" id="eq-eigen-example-matrix"/></p>
<p class="body"><a id="marker-64"/>We will compute the eigenvalues and eigenvectors of <i class="timesitalic">A</i>:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="110" src="../../OEBPS/Images/eq_02-23-a.png" width="555"/></p>
</div>
<p class="body">Thus,</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="273" src="../../OEBPS/Images/eq_02-23-b.png" width="454"/></p>
</div>
<p class="body">Here, <span class="math"><i class="fm-italics">i</i> = √–1</span>. If necessary, you are encouraged to refresh your memory of imaginary and complex numbers from high school algebra.</p>
<p class="body">Thus, we have found (as expected) three eigenvalues: <span class="math">1</span>, <span class="math"><i class="fm-italics">e</i><sup class="fm-superscript"><i class="fm-italics1">i</i> <i class="fm-italics1">π</i>/4</sup></span>, and <span class="math"><i class="fm-italics">e</i><sup class="fm-superscript">–<i class="fm-italics1">i</i> <i class="fm-italics1">π</i>/4</sup></span>. Each of them will yield one eigenvector. Let’s compute the eigenvector corresponding to the eigenvalue of <span class="math">1</span> by way of example:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="325" src="../../OEBPS/Images/eq_02-23-c.png" width="221"/></p>
</div>
<p class="body">Thus, <span class="infigure"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_02-23-d.png" width="22"/></span> is an eigenvector for the eigenvalue 1 for matrix A. So is <!--<span class="times">$\begin{bmatrix} 0\\[-2pt]0\\[-2pt]k\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_02-23-e.png" width="22"/></span> for any real <i class="timesitalic">k</i>. In fact, if <span class="math"><i class="fm-italics">λ</i>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></span> is an eigenvalue, eigenvector pair for matrix <i class="timesitalic">A</i>, then</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i> = <i class="fm-italics">λ<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i> <span class="cambria">⇔</span> <i class="fm-italics">A</i>(<i class="fm-italics">k<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i>) = <i class="fm-italics">λ</i>(<i class="fm-italics">k<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i>)</span></p>
<p class="body">That is, <span class="math"><i class="fm-italics">λ</i>, (<i class="fm-italics">k<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i>)</span> is also an eigenvalue, eigenvector pair of <i class="timesitalic">A</i>. In other words, we can only determine the eigenvector up to a fixed scale factor. We take the eigenvector to be of unit length (<span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i> = 1</span>) without loss of generality.</p>
<p class="body">The eigenvector for our example matrix turns out to be the <i class="timesitalic">Z</i>-axis. This is not an accident. Our matrix <i class="timesitalic">A</i> was, in fact, a rotation about the <i class="timesitalic">Z</i>-axis. <i class="fm-italics">A rotation matrix will always have <span class="math">1</span> as an eigenvalue. The corresponding eigenvector will be the axis of rotation. In 3D, the other two eigenvalues will be complex numbers yielding the angle of rotation.</i> This is detailed in section <a class="url" href="02.xhtml#sec-rotation-matrices-eigen">2.14</a>.<a id="marker-65"/></p>
<h3 class="fm-head1" id="eigenvectors-and-linear-independence">2.13.1 Eigenvectors and linear independence</h3>
<p class="body">Two eigenvectors of a matrix corresponding to unequal eigenvalues are linearly independent. Let’s prove this to get some insights. Let <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">2</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub></span> be eigenvalue, eigenvector pairs for a matrix <i class="timesitalic">A</i> with <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> ≠ <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub></span>. Then</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub></span></p>
<p class="body">If possible, let there be two constants <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">2</sub></span> such that</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = 0</span></p>
<p class="fm-equation-caption">Equation 2.24 <span class="calibre" id="eq-linear-dep-vectors"/></p>
<p class="body">In other words, suppose the two eigenvectors are linearly dependent. We will show that this assumption leads to an impossibility.</p>
<p class="body">Multiplying equation <a class="url" href="02.xhtml#eq-linear-dep-vectors">2.24</a> by <i class="timesitalic">A</i>, we get</p>
<p class="fm-equation"><span class="math">     <i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub>  = 0</span><br class="calibre20"/>
<span class="math"><span class="cambria">⇔</span> <i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub>  = 0</span></p>
<p class="body">Also, we can multiply equation <a class="url" href="02.xhtml#eq-linear-dep-vectors">2.24</a> by <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">2</sub></span>. Thus we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = 0</span><br class="calibre20"/>
<span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub><i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">α</i><sub class="fm-subscript">2</sub><i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = 0</span></p>
<p class="body">Subtracting, we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub>(<i class="fm-italics">λ</i><sub class="fm-subscript">1</sub>-<i class="fm-italics">λ</i><sub class="fm-subscript">2</sub>)<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> = 0</span></p>
<p class="body">By assumption, <span class="math"><i class="fm-italics">α</i><sub class="fm-subscript">1</sub> ≠ 0</span>, <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> ≠ <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub></span> and <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub></span> is not all zeros. Thus it is impossible for their product to be zero. Our original assumption (the two eigenvectors are linearly dependent) must have been wrong.</p>
<h3 class="fm-head1" id="symmetric-matrices-and-orthogonal-eigenvectors">2.13.2 Symmetric matrices and orthogonal eigenvectors</h3>
<p class="body"><a id="marker-66"/>Two eigenvectors of a symmetric matrix that correspond to different eigenvalues are mutually orthogonal. Let’s prove this to get additional insight. A matrix <i class="timesitalic">A</i> is symmetric if <span class="math"><i class="fm-italics">A<sup class="fm-superscript">T</sup></i> = <i class="fm-italics">A</i></span>. If <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub></span> and <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">2</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub></span> are eigenvalue, eigenvector pairs for a symmetric matrix <i class="timesitalic">A</i>, then</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub></span></p>
<p class="fm-equation-caption">Equation 2.25 <span class="calibre" id="eq-eigen-vec1"/></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub></span></p>
<p class="fm-equation-caption">Equation 2.26 <span class="calibre" id="eq-eigen-vec2"/></p>
<p class="body">Transposing equation <a class="url" href="02.xhtml#eq-eigen-vec1">2.25</a>,</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> A<sup class="fm-superscript">T</sup></i> = <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><sup class="fm-superscript">T</sup></span></p>
<p class="body">Right-multiplying by <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub></span>, we get</p>
<p class="fm-equation"><span class="math">     <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><sup class="fm-superscript">T</sup> <i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i><sub class="fm-subscript">2</sub></span><br class="calibre20"/>
<span class="math"><span class="cambria">⇔</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><sup class="fm-superscript">T</sup> <i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2   </sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i><sub class="fm-subscript">2</sub></span></p>
<p class="body">where the last equation follows from the matrix symmetry. Also, left-multiplying equation <a class="url" href="02.xhtml#eq-eigen-vec2">2.26</a> by <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup></i></span>, we get</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i><sub class="fm-subscript">2</sub></span></p>
<p class="body">Thus</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i><sub class="fm-subscript">2</sub></span><br class="calibre20"/>
<span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> = <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><sup class="fm-superscript">T</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub></span></p>
<p class="body">Subtracting the equations, we get</p>
<p class="fm-equation"><span class="math">0 = (<i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> - <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub>) <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i><sub class="fm-subscript">2</sub></span></p>
<p class="body">Since <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> ≠ <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub></span>, we must have <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i><sub class="fm-subscript">2</sub> = 0</span>, which means the two eigenvectors are orthogonal. Thus, if <i class="timesitalic">A</i> is an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> symmetric matrix with eigenvectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub>, … <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span>, then <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub><sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">j</sub></i> = 0</span> for all <span class="math"><i class="fm-italics">i</i>, <i class="fm-italics">j</i></span> satisfying <span class="math"><i class="fm-italics">λ<sub class="fm-subscript">i</sub></i> ≠ <i class="fm-italics">λ<sub class="fm-subscript">j</sub></i></span>.</p>
<h3 class="fm-head1" id="python-eig">2.13.3 PyTorch code to compute eigenvectors and eigenvalues</h3>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this section, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/1rEZ">http://mng.bz/1rEZ</a>.</p>
<p class="fm-code-listing-caption" id="listing-2.13-eigenvalues-and-vectors">Listing 2.13 Eigenvalues and vectors</p>
<pre class="programlisting">from torch import linalg as LA

A = torch.tensor([[0.707, 0.707, 0],
       [-0.707, 0.707, 0], [0, 0, 1]])    <span class="fm-combinumeral">①</span>

l, e = LA.eig(A)                          <span class="fm-combinumeral">②</span>

print("Eigen values are {}".format(l))
print("Eigen vectors are {}".format(e.T)) <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="infigure"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_02-23-f.png" width="233"/></span> Rotates points in 3D space around the <i class="timesitalic">Z</i>-axis.<br class="calibre20"/>
  The axis of rotation is the <i class="timesitalic">Z</i>-axis: <span class="math">[0  0  1]<i class="fm-italics"><sup class="fm-superscript">T</sup></i></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Function eig() in the torch linalg package computes eigenvalues and vectors.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Eigenvalues or vectors can contain complex numbers involving <span class="math"><i class="fm-italics">j</i> = √-1</span></p>
<h2 class="fm-head" id="sec-rotation-matrices-eigen">2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors</h2>
<p class="body">Of all the transforms, rotation transforms have a special intuitive appeal because of their highly observable behavior in the mechanical world. Furthermore, they play a significant role in developing and analyzing several machine learning tools. In this section, we overview rotation (aka orthogonal) matrices. (Fully functional code for the Jupyter notebook for this section can be found at <a class="url" href="http://mng.bz/2eNN">http://mng.bz/2eNN</a>.)</p>
<h3 class="fm-head1" id="rotation-matrices">2.14.1 Rotation matrices</h3>
<p class="body"><a id="marker-67"/>Figure <a class="url" href="02.xhtml#fig-rotation_matrix_diagram">2.14</a> shows a point <span class="math">(<i class="fm-italics">x</i>, <i class="fm-italics">y</i>)</span> rotated about the origin by an angle <i class="timesitalic">θ</i>. The original point’s position vector made an angle <i class="timesitalic">α</i> with the <i class="timesitalic">X</i>-axis. Post-rotation, the point’s new coordinates are <span class="math">(<i class="fm-italics">x</i><sup class="fm-superscript">′</sup>, <i class="fm-italics">y</i><sup class="fm-superscript">′</sup>)</span>. Note that by definition, rotation does not change the distance from the center of rotation; that is what the circle indicates.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="753" id="fig-rotation_matrix_diagram" src="../../OEBPS/Images/CH02_F14_Chaudhury.png" width="635"/></p>
<p class="figurecaption">Figure 2.14 Rotation in a plane about the origin. By definition, rotation does not change the distance from the center of rotation (indicated by the circle).</p>
</div>
<p class="body">Some well-known rotation matrices are as follows:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><b class="fm-bold">Planar rotation by angle <i class="timesitalic">θ</i> about the origin</b> (see figure <a class="url" href="02.xhtml#fig-rotation_matrix_diagram">2.14</a>):</p>
</li>
</ul>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_02-27.png" width="180"/></p>
</div>
<p class="fm-equation-caption">Equation 2.27</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><b class="fm-bold">Rotation by angle <i class="timesitalic">θ</i> in 3D space about the <i class="timesitalic">Z</i>-axis</b>:</p>
</li>
</ul>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="111" src="../../OEBPS/Images/eq_02-28.png" width="217"/></p>
</div>
<p class="fm-equation-caption">Equation 2.28 <span class="calibre" id="eq-rot3d-z"/></p>
<p class="body-ind">Note that the <i class="timesitalic">z</i> coordinate remains unaffected by this rotation:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="112" src="../../OEBPS/Images/eq_02-28-a.png" width="238"/></p>
</div>
<p class="body-ind"><a id="marker-68"/>This rotation matrix has an eigenvalue of <span class="math">1</span>, and the corresponding eigenvector is the <i class="timesitalic">Z</i>-axis—you should verify this. This implies that a point on the <i class="timesitalic">Z</i>-axis maps to itself when transformed (rotated) by the previous matrix, which is in keeping with the property that the <i class="timesitalic">z</i> coordinate remains unchanged by this rotation.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><b class="fm-bold">Rotation by angle <i class="timesitalic">θ</i> in 3D space about the <i class="fm-italics">X</i>-axis</b>:</p>
</li>
</ul>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="109" src="../../OEBPS/Images/eq_02-29.png" width="220"/></p>
</div>
<p class="fm-equation-caption">Equation 2.29 <span class="calibre" id="eq-rot3d-x"/></p>
<p class="body-ind">Note that the <i class="timesitalic">X</i> coordinate remains unaffected by this rotation and the <i class="timesitalic">X</i>-axis is an eigenvector of this matrix:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="111" src="../../OEBPS/Images/eq_02-29-a.png" width="239"/></p>
</div>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><b class="fm-bold">Rotation by angle <i class="timesitalic">θ</i> in 3D space about the <i class="fm-italics">Y</i>-axis</b>:</p>
</li>
</ul>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="110" src="../../OEBPS/Images/eq_02-30.png" width="217"/></p>
</div>
<p class="fm-equation-caption">Equation 2.30 <span class="calibre" id="eq-rot3d-y"/></p>
<p class="body-ind">Note that the <i class="timesitalic">Y</i> coordinate remains unaffected by this rotation and the <i class="timesitalic">Y</i>-axis is an eigenvector of this matrix:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="112" src="../../OEBPS/Images/eq_02-30-a.png" width="238"/></p>
</div>
<p class="fm-code-listing-caption" id="listing-2.14-rotation-matrices">Listing 2.14 Rotation matrices<a id="marker-69"/></p>
<pre class="programlisting">def rotation_matrix_2d(theta):                                            <span class="fm-combinumeral">①</span>
     return torch.tensor([[cos(radians(theta)), -sin(radians(theta))],
                      [sin(radians(theta)), cos(radians(theta))]])

def rotation_matrix_3d(theta, axis):                                      <span class="fm-combinumeral">②</span>
     if axis == 0:                                                        <span class="fm-combinumeral">③</span>
       return torch.tensor([[1, 0, 0],
                        [0, cos(radians(theta)),-sin(radians(theta))],
                        [0, sin(radians(theta)),cos(radians(theta))]])
     elif axis == 1:                                                      <span class="fm-combinumeral">④</span>
       return torch.tensor([[cos(radians(theta)),0,-sin(radians(theta))],
                        [0, 1, 0],
                        [sin(radians(theta)),0,cos(radians(theta))]])
     elif axis == 2:                                                      <span class="fm-combinumeral">⑤</span>
       return torch.tensor([[cos(radians(theta)),-sin(radians(theta)),0],
                        [sin(radians(theta)),cos(radians(theta)),0],
                        [0, 0, 1]])</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Returns the matrix that performs in-plane 2D rotation by angle theta about the origin. Thus, multiplication with this matrix moves a point to a new location. The angle between the position vectors of the original and new points is theta (figure <a class="url" href="02.xhtml#fig-rotation_matrix_diagram">2.14</a>).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Returns the matrix that rotates a point in 3D space about the chosen axis by angle theta degrees. The axis of rotation can be 0, 1, or 2, corresponding to the X-, Y-, or Z-axis, respectively.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">3<i class="fm-italics1">dx</i></sub></span> from equation <a class="url" href="02.xhtml#eq-rot3d-x">2.29</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">3<i class="fm-italics1">dy</i></sub></span> from equation <a class="url" href="02.xhtml#eq-rot3d-y">2.30</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">3<i class="fm-italics1">dz</i></sub></span> from equation <a class="url" href="02.xhtml#eq-rot3d-z">2.28</a></p>
<p class="fm-code-listing-caption" id="listing-2.15-applying-rotation-matrices">Listing 2.15 Applying rotation matrices</p>
<pre class="programlisting">u = torch.tensor.array([1, 1, 1], dtype=torch.float)  <span class="fm-combinumeral">①</span>

R3dz = rotation_matrix_3d(45, 2)                      <span class="fm-combinumeral">②</span>
v = torch.matmul(R3dz, u_row)                         <span class="fm-combinumeral">③</span>

R3dx = rotation_matrix_3d(45, 0)                      <span class="fm-combinumeral">④</span>
w = torch.matmul(R3dx, u_row)                         <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_u.png" width="14"/></span> (see figure <a class="url" href="02.xhtml#fig-numpy-rotations">2.15</a>)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">3<i class="fm-italics1">dz</i></sub></span> from equation <a class="url" href="02.xhtml#eq-rot3d-z">2.28</a>, <span class="math">45°</span> about <i class="timesitalic">Z</i>-axis</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span> (see figure <a class="url" href="02.xhtml#fig-numpy-rotations">2.15</a>) is <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_u.png" width="14"/></span> rotated by <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">3<i class="fm-italics1">dz</i></sub></span>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">3<i class="fm-italics1">dx</i></sub></span> from equation <a class="url" href="02.xhtml#eq-rot3d-z">2.28</a>, <span class="math">45°</span> about <i class="timesitalic">X</i>-axis</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> (see figure <a class="url" href="02.xhtml#fig-numpy-rotations">2.15</a>) is <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span> rotated by <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">3<i class="fm-italics1">dx</i></sub></span>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="425" id="fig-numpy-rotations" src="../../OEBPS/Images/CH02_F15_Chaudhury.png" width="602"/></p>
<p class="figurecaption">Figure 2.15 Rotation visualized. Here the original vector u is first rotated by 45 degrees around the Z-axis to get vector v, which is subsequently rotated again by 45 degrees around the X-axis to get vector w.</p>
</div>
<h3 class="fm-head1" id="orthogonality-of-rotation-matrices">2.14.2 Orthogonality of rotation matrices</h3>
<p class="body"><a id="marker-70"/>A matrix <i class="timesitalic">R</i> is <i class="fm-italics">orthogonal</i> if and only if it its transpose is also its inverse: that is, <span class="math"><i class="fm-italics">R<sup class="fm-superscript">T</sup>R</i> = <i class="fm-italics">RR<sup class="fm-superscript">T</sup></i> = <b class="fm-bold">I</b></span>. <i class="fm-italics">All rotations matrices are orthogonal matrices. All orthogonal matrices represent some rotation.</i> For instance:</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="83" src="../../OEBPS/Images/eq_02-30-b1.png" width="878"/></p>
</div>
<p class="body">You are encouraged to verify, likewise, that all the rotation matrices shown here are orthogonal.</p>
<p class="fm-head2" id="orth-len-preserv">Orthogonality and length-preservation</p>
<p class="body">Orthogonality implies that rotation is length-preserving. Given any vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and rotation matrix <i class="timesitalic">R</i>, let <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="fm-italics">R</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> be the rotated vector. The lengths (magnitudes) of the two vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> are equal since it is easy to see that</p>
<p class="fm-equation"><span class="math">||<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>|| = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = (<i class="fm-italics">R</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i>(<i class="fm-italics">R</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>R<sup class="fm-superscript">T</sup>R</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><b class="fm-bold">I</b><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>||</span></p>
<p class="body">From elementary matrix theory, we know that</p>
<p class="fm-equation"><span class="math">(<i class="fm-italics">AB</i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> = <i class="fm-italics">B<sup class="fm-superscript">T</sup>A<sup class="fm-superscript">T</sup></i></span></p>
<p class="fm-head2" id="negating-the-angle-of-rotation">Negating the angle of rotation</p>
<p class="body">Negating the angle of rotation is equivalent to inverting the rotation matrix, which is equivalent to transposing the rotation matrix. For instance, consider in-plane rotation. Say a point <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is rotated about the origin to vector <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> via matrix <i class="timesitalic">R</i>. Thus</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="110" src="../../OEBPS/Images/eq_02-30-c.png" width="168"/></p>
</div>
<p class="body">Now we can go back from <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> by rotating by <span class="math">−<i class="fm-italics">θ</i></span>. The corresponding rotation matrix is</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_02-30-d.png" width="388"/></p>
</div>
<p class="body">In other words, <i class="timesitalic">R<sup class="fm-superscript">T</sup></i> inverts the rotation: that is, rotates by the negative angle.</p>
<h3 class="fm-head1" id="pytorch-code-for-orthogonality-of-rotation-matrices">2.14.3 PyTorch code for orthogonality of rotation matrices</h3>
<p class="body"><a id="marker-71"/>Let’s verify the orthogonality of the rotation matrix by creating one in PyTorch, imparting a transpose to it, and verifying that the product of the original matrix and the transpose is the identity matrix.</p>
<p class="fm-code-listing-caption" id="listing-2.16-orthogonality-of-rotation-matrices">Listing 2.16 Orthogonality of rotation matrices</p>
<pre class="programlisting">R_30 = rotation_matrix_2d(30)                      <span class="fm-combinumeral">①</span>

assert torch.allclose(
    torch.linalg.inv(R_30),
    R_30.T

    )                                              <span class="fm-combinumeral">②</span>

assert torch.allclose(
    torch.matmul(R_30, R_30.T),
    torch.eye(2)                                   <span class="fm-combinumeral">③</span>
    )

u = torch.tensor([[4],[0]], dtype=torch.float)

v = torch.matmul(R_30, u)                          <span class="fm-combinumeral">④</span>

assert torch.linalg.norm(u) ==torch.linalg.norm(v)  <span class="fm-combinumeral">⑤</span>

R_neg30 = rotation_matrix_2d(-30)
w = torch.matmul(R_neg30, v)                       <span class="fm-combinumeral">⑥</span>
assert torch.all(w == u)

assert torch.allclose(R_30, R_neg30.T)
assert torch.allclose(
    torch.matmul(R_30, R_neg30),
    torch.eye(2))                                  <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a rotation matrix, <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">30</sub></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The inverse of a rotation matrix is the same as its transpose.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Multiplying a rotation matrix and its inverse yields the identity matrix.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_u.png" width="14"/></span> rotated by matrix <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">30</sub></span> to yield vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span>, <span class="math"><i class="fm-italics">R</i><sub class="fm-subscript">30</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_u.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span></span>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The norm of a vector is the same as its length. Rotation preserves the length of a vector <span class="math">||<i class="fm-italics">R<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_u.png" width="14"/></span></i>|| = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_u.png" width="14"/></span>||</span>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Rotation by an angle followed by rotation by the negative of that angle takes the vector back to its original position. Rotation by a negative angle is equivalent to inverse rotation.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> A matrix that rotates by an angle is the inverse of the matrix that rotates by the negative of the same angle.</p>
<h3 class="fm-head1" id="eigenvalues-and-eigenvectors-of-a-rotation-matrixfinding-the-axis-of-rotation">2.14.4 Eigenvalues and eigenvectors of a rotation matrix:Finding the axis of rotation</h3>
<p class="body"><a id="marker-72"/>Let <span class="math"><i class="fm-italics">λ</i>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></span> be an eigenvalue, eigenvector pair of a rotation matrix <i class="timesitalic">R</i>. Then <span class="math"><i class="fm-italics">R<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i> = <i class="fm-italics">λ<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i></span></p>
<p class="body">Transposing both sides,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sup class="fm-superscript">T</sup>R<sup class="fm-superscript">T</sup></i> = <i class="fm-italics">λ<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sup class="fm-superscript">T</sup></i></span></p>
<p class="body">Multiplying the left and right sides, respectively, with the equivalent entities <i class="timesitalic">R<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i> and <i class="timesitalic">λ<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i>, we get</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>R<sup class="fm-superscript">T</sup></i>(<i class="fm-italics">R</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span>) = <i class="fm-italics">λ<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sup class="fm-superscript">T</sup></i>(<i class="fm-italics">λ</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span>) <span class="cambria">⇔</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>(R<sup class="fm-superscript">T</sup>R</i>)<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> = <i class="fm-italics">λ</i><sup class="fm-superscript">2</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> <span class="cambria">⇔</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sup class="fm-superscript">T</sup></i>(<b class="fm-bold">I</b>)<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> = <i class="fm-italics">λ</i><sup class="fm-superscript">2</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></span></p>
<p class="fm-equation"><span class="math"><span class="cambria">⇔</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> = <i class="fm-italics">λ</i><sup class="fm-superscript">2</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> <span class="cambria">⇔</span> <i class="fm-italics">λ</i><sup class="fm-superscript">2</sup> = 1 <span class="cambria">⇔</span> <i class="fm-italics">λ</i> = 1</span></p>
<p class="body">(The negative solution <span class="math"><i class="fm-italics">λ</i> = −1</span> corresponds to reflection.) Thus, all rotation matrices have <span class="math">1</span> as one of their eigenvalues. The corresponding eigenvector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span> satisfies <span class="math"><i class="fm-italics">R<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></span>. This is the axis of rotation: the set of points that stay where they were post-rotation.</p>
<h3 class="fm-head1" id="pytorch-code-for-eigenvalues-and-vectors-of-rotation-matrices">2.14.5 PyTorch code for eigenvalues and vectors of rotation matrices</h3>
<p class="body">The following listing shows the code for the axis of rotation.</p>
<p class="fm-code-listing-caption" id="listing-2.17-axis-of-rotation">Listing 2.17 Axis of rotation</p>
<pre class="programlisting">R = torch.tensor([[0.7071, 0.7071, 0],
             [-0.7071, 0.7071, 0],                      <span class="fm-combinumeral">①</span>
             [0, 0, 1]])

l, e = LA.eig(R)                                        <span class="fm-combinumeral">②</span>

<span class="fm-combinumeral">③</span>
axis_of_rotation = e[:, torch.where(l == 1.0)]          <span class="fm-combinumeral">④</span>

axis_of_rotation = torch.squeeze(axis_of_rotation)

assert torch.allclose(
    axis_of_rotation.real,
    torch.tensor([0, 0, 1], dtype=torch.float)          <span class="fm-combinumeral">⑤</span>
)

p = torch.randint(0, 10, (1,)) * axis_of_rotation
assert torch.allclose(torch.matmul(R, p.real), p.real)  <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="infigure"><img alt="" class="calibre5" height="69" src="../../OEBPS/Images/eq_02-30-e.png" width="362"/></span><br class="calibre20"/>
  about the <i class="timesitalic">Z</i>-axis. All rotation matrices will havean eigenvalue <span class="math">1</span>. The corresponding eigenvector<br class="calibre20"/>
  is the axis of rotation (here, the <i class="timesitalic">Z</i>-axis).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The PyTorch function eig() computes eigenvalues and eigenvectors.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The PyTorch function where() returns the indices at which the specified condition is true.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Obtains the eigenvector for eigenvalue 1</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The axis of rotation is the <i class="timesitalic">Z</i>-axis.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Takes a random point<br class="calibre20"/>
  on this axis and applies the rotation to this point; its position does not change.</p>
<h2 class="fm-head" id="sec-mat-diagonalization">2.15 Matrix diagonalization</h2>
<p class="body"><a id="marker-73"/>In section <a class="url" href="02.xhtml#sec-lin_systems">2.12</a>, we studied linear systems and their importance in machine learning. We also remarked that the standard mathematical process of solving linear systems via matrix inversion is not very desirable from a machine learning point of view. In this section, we will see one method of solving linear systems without matrix inversion. In addition, this section will help us develop the insights necessary to understand quadratic forms and, eventually, principal component analysis (PCA), one of the most important tools in data science.</p>
<p class="body">Consider an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">A</i> with <i class="timesitalic">n</i> linearly independent eigenvectors. Let <i class="timesitalic">S</i> be a matrix with these eigenvectors as its columns. That is,</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="137" src="../../OEBPS/Images/eq_02-30-f.png" width="99"/></p>
</div>
<p class="body">and</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_02-30-g.png" width="168"/></p>
</div>
<p class="body">Then</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="185" src="../../OEBPS/Images/eq_02-30-h.png" width="641"/></p>
</div>
<p class="body">where</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="139" src="../../OEBPS/Images/eq_02-30-i.png" width="199"/></p>
</div>
<p class="body">is a diagonal matrix with the eigenvalues of <i class="timesitalic">A</i> on the diagonal and <span class="math">0</span> everywhere else.</p>
<p class="body">Thus, we have</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">AS</i> = <i class="fm-italics">S</i>Λ</span></p>
<p class="body">which leads to</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">S</i>Λ<i class="fm-italics">S</i><sup class="fm-superscript">−1</sup></span></p>
<p class="body">and</p>
<p class="fm-equation"><span class="math">Λ = <i class="fm-italics">S</i><sup class="fm-superscript">−1</sup><i class="fm-italics">AS</i></span></p>
<p class="body">If <i class="timesitalic">A</i> is symmetric, then its eigenvectors are orthogonal. Then <span class="math"><i class="fm-italics">S<sup class="fm-superscript">T</sup>S</i> = <i class="fm-italics">SS<sup class="fm-superscript">T</sup></i> = <b class="fm-bold">I</b> <span class="cambria">⇔</span> <i class="fm-italics">S</i><sup class="fm-superscript">−1</sup> = <i class="fm-italics">S<sup class="fm-superscript">T</sup></i></span>, and we get the diagonalization of <i class="timesitalic">A</i>: <span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">S</i>Λ<i class="fm-italics">S<sup class="fm-superscript">T</sup></i></span> Note that diagonalization is not unique: a given matrix may be diagonalized in multiple ways.</p>
<h3 class="fm-head1" id="python-matdiag">2.15.1 PyTorch code for matrix diagonalization</h3>
<p class="body"><a id="marker-74"/>Now we will study the PyTorch implementation of the math we learned in section <a class="url" href="02.xhtml#sec-mat-diagonalization">2.15</a>. As usual, we will only show the directly relevant bit of code here.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this section, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/RXJn">http://mng.bz/RXJn</a>.</p>
<p class="fm-code-listing-caption" id="listing-2.18-diagonalization-of-a-matrix">Listing 2.18 Diagonalization of a matrix</p>
<pre class="programlisting">def diagonalize(matrix):                              <span class="fm-combinumeral">①</span>

    try:
        l, e = torch.linalg.eig(matrix)               <span class="fm-combinumeral">②</span>


        sigma = torch.diag(l)                         <span class="fm-combinumeral">③</span>

        return e, torch.diag(l), torch.linalg.inv(e)  <span class="fm-combinumeral">④</span>
    except np.linalg.LinAlgError:
        print("Cannot diagonalize matrix!")

A = torch.tensor([[0.7071, 0.7071, 0],
              [-0.7071, 0.7071, 0],                   <span class="fm-combinumeral">⑤</span>
              [0, 0, 1]])

S, sigma, S_inv = diagonalize(A)

A1 = torch.matmul(S, torch.matmul(sigma, S_inv))      <span class="fm-combinumeral">⑥</span>

assert torch.allclose(A, A1.real)                     <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Diagonalization is factorizing a matrix <span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">SΣS</i><sup class="fm-superscript">−1</sup></span>. <i class="timesitalic">S</i> is a matrix with eigenvectors of <i class="timesitalic">A</i> as columns. <span class="math">Σ</span> is a diagonal matrix with eigenvalues of <i class="timesitalic">A</i> in the diagonal.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The PyTorch function eig() returns eigenvalues and vectors.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The PyTorch function diag() creates a diagonal matrix of given values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Returns the three factors</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a matrix <i class="timesitalic">A</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Reconstructs <i class="timesitalic">A</i> from its factors</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Verifies that the reconstructed matrix is the same as the original</p>
<h3 class="fm-head1" id="subsec-lin-sys-solve-diag">2.15.2 Solving linear systems without inversion via diagonalization</h3>
<p class="body">Diagonalization has many practical applications. Let’s study one now. In general, matrix inversion (that is, computing <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span>) is a very complex process that is numerically unstable. Hence, solving <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> via <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">A</i><sup class="fm-superscript">−1</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> is to be avoided when possible. In the case of a square symmetric matrix with <i class="timesitalic">n</i> distinct eigenvalues, diagonalization can come to the rescue. We can solve this in multiple steps. We first diagonalize <i class="timesitalic">A</i>:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">S</i>Λ<i class="fm-italics">S<sup class="fm-superscript">T</sup></i></span></p>
<p class="body">Then</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">can be written as:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">S</i>Λ<i class="fm-italics">S<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">where <i class="timesitalic">S</i> is the matrix with eigenvectors of <i class="timesitalic">A</i> as its columns:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">S</i> = [<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub>   <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub>   … <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i>]</span></p>
<p class="body">(Since <i class="timesitalic">A</i> is symmetric, these eigenvectors are orthogonal. Hence <span class="math"><i class="fm-italics">S<sup class="fm-superscript">T</sup>S</i> = <i class="fm-italics">SS<sup class="fm-superscript">T</sup></i> = <b class="fm-bold">I</b></span>.) The solution can be obtained in a series of very simple steps:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="100" src="../../OEBPS/Images/eq_02-30-j.png" width="113"/></p>
</div>
<p class="body">First solve</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">S</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">as</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub> = <i class="fm-italics">S<sup class="fm-superscript">T</sup></i> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body"><a id="marker-75"/>Notice that both the transpose and matrix-vector multiplications are simple and numerically stable operations, unlike matrix inversion. Then we get</p>
<p class="fm-equation"><span class="math">Λ(<i class="fm-italics">S<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span></p>
<p class="body">Now solve</p>
<p class="fm-equation"><span class="math">Λ<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span></p>
<p class="body">as</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub> = Λ<sup class="fm-superscript">–1</sup><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span></p>
<p class="body">Note that since <span class="math">Λ</span> is a diagonal matrix, inverting it is trivial:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="145" src="../../OEBPS/Images/eq_02-31.png" width="366"/></p>
</div>
<p class="fm-equation-caption">Equation 2.31 <span class="calibre" id="eq-diag-inv"/></p>
<p class="body">As a final step, solve</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">S<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub></span></p>
<p class="body">as</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">S</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub></span></p>
<p class="body">Thus we have obtained <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> without a single complex or unstable step.</p>
<h3 class="fm-head1" id="pytorch-code-for-solving-linear-systems-via-diagonalization">2.15.3 PyTorch code for solving linear systems via diagonalization</h3>
<p class="body"><a id="marker-76"/>Let’s try solving the following set of equations:</p>
<p class="fm-equation"><span class="math">  <i class="fm-italics">x</i> +   <i class="fm-italics">y</i> +  <i class="fm-italics">z</i> = 8</span><br class="calibre20"/>
<span class="math">2<i class="fm-italics">x</i> + 2<i class="fm-italics">y</i> + 3<i class="fm-italics">z</i> = 15</span><br class="calibre20"/>
<span class="math">  <i class="fm-italics">x</i> + 3<i class="fm-italics">y</i> + 3<i class="fm-italics">z</i> = 16</span></p>
<p class="body">This can be written using matrices and vectors as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">where</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="92" src="../../OEBPS/Images/eq_02-31-a.png" width="290"/></p>
</div>
<p class="body">Note that <i class="timesitalic">A</i> is a symmetric matrix. It has orthogonal eigenvectors. The matrix with eigenvectors of <i class="timesitalic">A</i> in columns is orthogonal. Its transpose and inverse are the same.</p>
<p class="fm-code-listing-caption" id="listing-2.19-solving-linear-systems-using-diagonalization">Listing 2.19 Solving linear systems using diagonalization</p>
<pre class="programlisting">A = torch.tensor([[1, 2, 1], [2, 2, 3], [1, 3, 3]],
                        dtype=torch.float)         <span class="fm-combinumeral">①</span>
assert torch.all(A == A.T)                         <span class="fm-combinumeral">②</span>
b = torch.tensor([8, 15, 16], dtype=torch.cfloat)  <span class="fm-combinumeral">③</span>

x_0 = torch.matmul(torch.linalg.inv(A),
                                b.real)            <span class="fm-combinumeral">④</span>

w, S = torch.linalg.eig(A)                         <span class="fm-combinumeral">⑤</span>

 1 = torch.matmul(S.T, b)                          <span class="fm-combinumeral">⑥</span>

 2 = torch.matmul(torch.diag(1/ w), y1)            <span class="fm-combinumeral">⑦</span>

x_1 = torch.matmul(S, y2)                          <span class="fm-combinumeral">⑧</span>

assert torch.allclose(x_0, x_1.real)               <span class="fm-combinumeral">⑨</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a symmetric matrix <i class="timesitalic">A</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Asserts that <i class="timesitalic">A</i> may be symmetric</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Solves <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> using matrix inversion, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="math">= <i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>.<br class="calibre20"/>
  Note: matrix inversion is numerically unstable.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Solves <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> via diagonalization. <span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">S</i>Σ<i class="fm-italics">S<sup class="fm-superscript">T</sup></i></span>.<br class="calibre20"/>
<span class="infigure"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_02-31-b.png" width="93"/></span>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> 1. Solve: <span class="math"><i class="fm-italics">S</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> as <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub> = <i class="fm-italics">S<sup class="fm-superscript">T</sup></i> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> (no matrix inversion)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> 2. Solve: <span class="math">Λ <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span> as <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub> = Λ<sup class="fm-superscript">-1</sup><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span> (inverting a diagonal matrix is easy; see equation <a class="url" href="02.xhtml#eq-diag-inv">2.31</a>.)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> 3. Solve: <span class="math"><i class="fm-italics">S<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub></span> as <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">S</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub></span> (no matrix inversion)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Verifies that the two solutions are the same</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Verifies that the two solutions are the same</p>
<h3 class="fm-head1" id="matrix-powers-using-diagonalization">2.15.4 Matrix powers using diagonalization</h3>
<p class="body">If matrix <i class="timesitalic">A</i> can be diagonalized, then</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="134" src="../../OEBPS/Images/eq_02-31-c.png" width="342"/></p>
</div>
<p class="body">For a diagonal matrix</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="147" src="../../OEBPS/Images/eq_02-31-d.png" width="201"/></p>
</div>
<p class="body">the <i class="timesitalic">n</i>th power is simply</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="148" src="../../OEBPS/Images/eq_02-31-e.png" width="211"/></p>
</div>
<p class="body">If we need to compute various powers of an <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">m</i></span> matrix <span class="times1">A</span> at different times, we should precompute the matrix <span class="times1">S</span> and compute any power with only <span class="math"><i class="fm-italics">O</i>(<i class="fm-italics">m</i>)</span> operations—compared to the <span class="math">(<i class="fm-italics">nm</i><sup class="fm-superscript">3</sup>)</span> operations necessary for naive computations.</p>
<h2 class="fm-head" id="spectral-decomposition-of-a-symmetric-matrix">2.16 Spectral decomposition of a symmetric matrix</h2>
<p class="body"><a id="marker-77"/>We have seen in section <a class="url" href="02.xhtml#sec-mat-diagonalization">2.15</a> that a square symmetric matrix with distinct eigenvalues can be decomposed as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">S</i>Λ<i class="fm-italics">S<sup class="fm-superscript">T</sup></i></span></p>
<p class="body">where</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i> = [<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub> … <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">n</sub>]</span></p>
<p class="body">Thus,</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="144" src="../../OEBPS/Images/eq_02-31-f.png" width="396"/></p>
</div>
<p class="body">This equation can be rewritten as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><sup class="fm-superscript">T</sup> + <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub><i class="fm-italics"><sup class="fm-superscript">T</sup></i> + … + <i class="fm-italics">λ<sub class="fm-subscript">n</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">n</sub></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub><sup class="fm-superscript">T</sup></i></span></p>
<p class="fm-equation-caption">Equation 2.32 <span class="calibre" id="eq-spectral-decomp"/></p>
<p class="body">Thus a square symmetric matrix can be written in terms of its eigenvalues and eigenvectors. This is the spectral resolution theorem.</p>
<h3 class="fm-head1" id="pytorch-code-for-the-spectral-decomposition-of-a-matrix">2.16.1 PyTorch code for the spectral decomposition of a matrix</h3>
<p class="body">The following listing shows the relevant code for this section.</p>
<p class="fm-code-listing-caption" id="listing-2.20-spectral-decomposition-of-a-matrix">Listing 2.20 Spectral decomposition of a matrix</p>
<pre class="programlisting">def spectral_decomposition(A):
  assert len(A.shape) == 2                                    <span class="fm-combinumeral">①</span>

     and A.shape[0] == A.shape[1]                             <span class="fm-combinumeral">②</span>

     and torch.all(A == A.T)                                  <span class="fm-combinumeral">③</span>

  l, e = torch.linalg.eig(A)                                  <span class="fm-combinumeral">④</span>

  assert len(torch.unique(l.real)) == A.shape[0],
         ``Eigen values are not distinct!"

  C = torch.zeros((A.shape[0],
                          A.shape[0],                         <span class="fm-combinumeral">⑤</span>
                          A.shape[0]))

  for i, lambda_i in enumerate(l):
      e_i = e[:, i]
      e_i = e_i.reshape((3, 1))
      C[i, :, :] = (lambda_i * torch.matmul(e_i, e_i.T)).real <span class="fm-combinumeral">⑥</span>
  return C

A = torch.tensor([[1, 2, 1], [2, 2, 3], [1, 3, 3]]).float()
C = spectral_decomposition(A)

A1 = C.sum(axis=0)                                            <span class="fm-combinumeral">⑦</span>

assert torch.allclose(A, A1)                                  <span class="fm-combinumeral">⑧</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Asserts that A is a 2D tensor (i.e., matrix)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A is square: i.e., <span class="math"><i class="fm-italics">A</i>.<i class="fm-italics">shape</i>[0]</span> (num rows) <span class="math">≜</span> <span class="math"><i class="fm-italics">A</i>.<i class="fm-italics">shape</i>[1]</span> (num columns)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Asserts that A is symmetric: i.e., <span class="math"><i class="fm-italics">A</i> = = <i class="fm-italics">A<sup class="fm-superscript">T</sup></i></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The PyTorch function eig() returns eigenvectors and values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Defines a 3D tensor <i class="timesitalic">C</i> of shape <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> to hold the <i class="timesitalic">n</i> components from equation <a class="url" href="02.xhtml#eq-spectral-decomp">2.32</a>. Each term <span class="math"><i class="fm-italics">λ<sub class="fm-subscript">i</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">i</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i><i class="fm-italics"><sup class="fm-superscript">T</sup></i></span> is an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> matrix. There are <i class="timesitalic">n</i> such terms, all compactly held in tensor <i class="timesitalic">C</i>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Computes <span class="math"><i class="fm-italics">C</i>[<i class="fm-italics">i</i>] = <i class="fm-italics">λ<sub class="fm-subscript">i</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">i</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span></i><i class="fm-italics"><sup class="fm-superscript">T</sup></i></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Reconstructs <i class="timesitalic">A</i> by adding its components stored in <i class="timesitalic">C</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Verifies that the matrix reconstructed from spectral components matches the original</p>
<h2 class="fm-head" id="sec-hyper-ellipse">2.17 An application relevant to machine learning: Finding the axes of a hyperellipse</h2>
<p class="body"><a id="marker-78"/>The notion of an ellipse in high-dimensional space (aka hyperellipse) keeps coming back in various forms in machine learning. Here we will make a preliminary review of them. We will revisit these concepts later.</p>
<p class="body">Recall the equation of an ellipse from high school math:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_02-32-a.png" width="96"/></p>
</div>
<p class="body">This is a rather simple ellipse: it is two-dimensional and centered at the origin, and its major and minor axes are aligned with the coordinate axes. Denoting <span class="infigure"><img alt="" class="calibre5" height="35" src="../../OEBPS/Images/eq_02-32-b.png" width="55"/></span> as the position vector, the same equation can be written as</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>Λ</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = 1</span></p>
<p class="body">where <span class="infigure"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_02-32-c.png" width="105"/></span> is a diagonal matrix. Written in this form, the equation can be extended beyond 2D to an <i class="timesitalic">n</i>-dimensional axis-aligned ellipse centered at the origin. Now let’s apply a rotation <i class="timesitalic">R</i> to the axis. Then every vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> transforms to <span class="math"><i class="fm-italics">R</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>. The equation of the ellipse in the new (rotated) coordinate system is</p>
<p class="fm-equation"><span class="math">     (<i class="fm-italics">R<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">T</sup> Λ</i> (<i class="fm-italics">R<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></i>) = 1</span><br class="calibre20"/>
<span class="math"><span class="cambria">⇔</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">T</sup> (<i class="fm-italics">R<sup class="fm-superscript">T</sup> ΛR</i>) <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = 1</span></p>
<p class="body">where</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i> = (<i class="fm-italics">R<sup class="fm-superscript">T</sup>ΛR</i>)</span>.</p>
<p class="body">The generalized equation of the ellipse is</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = 1</span></p>
<p class="body">Note the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The ellipse is no longer axis aligned.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The matrix <i class="timesitalic">A</i> is no longer diagonal.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">A</i> is symmetric. We can easily verify that <span class="math"><i class="fm-italics">A<sup class="fm-superscript">T</sup></i> = (<i class="fm-italics">R<sup class="fm-superscript">T</sup></i>Λ<i class="fm-italics">R</i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> = <i class="fm-italics">R<sup class="fm-superscript">T</sup></i>Λ<i class="fm-italics"><sup class="fm-superscript">T</sup>R</i> = <i class="fm-italics">R<sup class="fm-superscript">T</sup></i>Λ<i class="fm-italics">R</i></span> (remember, the transpose of a diagonal matrix is itself).</p>
</li>
</ul>
<p class="body">If, in addition, we want to get rid of the “centered at the origin” assumption, we get</p>
<p class="fm-equation"><span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<i class="fm-italics">μ</i>)<i class="fm-italics"><sup class="fm-superscript">T</sup>A</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<i class="fm-italics">μ</i>) = 1</span></p>
<p class="fm-equation-caption">Equation 2.33 <span class="calibre" id="eq-hyper-ellipse"/></p>
<p class="body">Now let’s flip the problem around. Suppose we have a generic <i class="timesitalic">n</i>-dimensional ellipse. How do we compute its axes’ directions?</p>
<p class="body">Clearly, if we can rotate the coordinate system so that the matrix in the middle is diagonal, we are done. Diagonalization (see section <a class="url" href="02.xhtml#sec-mat-diagonalization">2.15</a>) is the answer. Specifically, we find the matrix <i class="timesitalic">S</i> with eigenvectors of <i class="timesitalic">A</i> in its columns. This is a rotation matrix (being orthogonal, since <i class="timesitalic">A</i> is symmetric). We transform rotate) the coordinate system by applying this matrix. In this new coordinate system, the ellipse is axis aligned. Stated another way, the new coordinate axes—these are the eigenvectors of <i class="timesitalic">A</i>—yield the axes of the ellipse.<a id="marker-79"/></p>
<h3 class="fm-head1" id="python-hyper-ellipse">2.17.1 PyTorch code for hyperellipses</h3>
<p class="body">Let’s try finding the axes of the hyperellipse described by the equation <span class="math">5<i class="fm-italics">x</i><sup class="fm-superscript">2</sup> + 6<i class="fm-italics">xy</i> + 5<i class="timesitalic">y</i><sup class="fm-superscript">2</sup> = 20</span>. Note that the actual ellipse we use as an example is 2D (to facilitate visualization), but the code we develop will be general and extensible to multiple dimensions.</p>
<p class="body">The ellipse equation can be written using matrices and vectors as <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = 1</span>, where</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_02-33-a.png" width="150"/></p>
</div>
<p class="body">To find the axes of the hyperellipse, we need to transform the coordinate system so that the matrix in the middle becomes diagonal. Here is how this can be done: if we diagonalize <i class="timesitalic">A</i> into <span class="math"><i class="fm-italics">S</i>Σ<i class="fm-italics">S</i><sup class="fm-superscript">−1</sup></span>, then the ellipse equation becomes <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><span class="math"><i class="fm-italics"><sup class="fm-superscript">T</sup>S</i>Σ<i class="fm-italics">S</i><sup class="fm-superscript">−1</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = 1</span>, where <span class="math">Σ</span> is a diagonal matrix. Since <i class="timesitalic">A</i> is symmetric, its eigenvectors are orthogonal. Hence, the matrix containing these eigenvectors as columns is orthogonal: i.e., <span class="math"><i class="fm-italics">S</i><sup class="fm-superscript">−1</sup> = <i class="fm-italics">S<sup class="fm-superscript">T</sup></i></span>. In other words, <i class="timesitalic">S</i> is a rotation matrix. So the ellipse equation becomes <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>S</i>Σ<i class="fm-italics">S<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = 1</span> or <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>S</i>)Σ(<i class="fm-italics">S<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = 1</span> or <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i>Σ<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = 1</span> where <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="fm-italics">S<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>. This is of the desired form since <span class="math">Σ</span> is a diagonal matrix. Remember, <i class="timesitalic">S</i> is a rotation matrix. Thus, rotating the coordinate system by <i class="timesitalic">S</i> aligns the coordinate axes with the ellipse axes.</p>
<p class="fm-code-listing-caption" id="listing-2.21-axes-of-a-hyperellipse">Listing 2.21 Axes of a hyperellipse<a id="marker-80"/></p>
<pre class="programlisting">ellipse_eq = sy.Eq(5*x**2 + 5*y**2 +
                      6*x*y, 20)                   <span class="fm-combinumeral">①</span>

A = torch.tensor([[5, 3], [3, 5]]).float()

l, S = torch.linalg.eig(A)

x_axis_vec = torch.zeros((A.shape[0]))             <span class="fm-combinumeral">②</span>

first_eigen_vec = S[:, 0]                          <span class="fm-combinumeral">③</span>

dot_prod = torch.dot(x_axis_vec, first_eigen_vec)  <span class="fm-combinumeral">④</span>

theta = math.acos(dot_prod)
theta = math.degrees(theta)                        <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Equation of the ellipse: <span class="math">5<i class="fm-italics">x</i><sup class="fm-superscript">2</sup> + 6<i class="fm-italics">x</i><i class="fm-italics">y</i> + 5<i class="fm-italics">y</i><sup class="fm-superscript">2</sup> = 20</span> or <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = 20</span>, where <span class="infigure"><img alt="" class="calibre5" height="38" src="../../OEBPS/Images/eq_02-33-b.png" width="138"/></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> X-axis vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Major axis of the ellipse</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The dot product between two vectors is the cosine of the angle between them.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The angle between the ellipse’s major axis and the X-axis: <span class="math">45°</span> (see figure <a class="url" href="02.xhtml#fig-numpy-hyper-ellipse">2.16</a>)</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="471" id="fig-numpy-hyper-ellipse" src="../../OEBPS/Images/CH02_F16_Chaudhury.png" width="499"/></p>
<p class="figurecaption">Figure 2.16 Note that the ellipse’s major axis forms an angle of 45 degrees with the X-axis. Rotating the coordinate system by this angle will align the ellipse axes with the coordinate axes. Subsequently, the first principal vector will also lie along this direction.</p>
</div>
<h2 class="fm-head" id="summary-1">Summary</h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">In machine learning, a vector is a one-dimensional array of numbers and a matrix is a two-dimensional array of numbers. Inputs and outputs of machine learning models are typically represented as vectors or matrices. In multilayered models, inputs and outputs of each individual layer are also represented as vectors or matrices. Images are two-dimensional arrays of numbers corresponding to pixel color values. As such, they are represented as matrices.</p>
</li>
<li class="fm-list-bullet">
<p class="list">An <i class="timesitalic">n</i>-dimensional vector can be viewed as point in <span class="math">ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span> space. All models can be viewed as functions that map points from input to output space. The model is designed so that it is easier to solve the problem of interest in the output space.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A dot product between a pair of vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = [<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>    <i class="fm-italics">x</i><sub class="fm-subscript">2</sub>   …   <i class="fm-italics">x<sub class="fm-subscript">n</sub></i>]</span> and <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = [<i class="fm-italics">y</i><sub class="fm-subscript">1</sub>    <i class="fm-italics">y</i><sub class="fm-subscript">2</sub>   …   <i class="fm-italics">y<sub class="fm-subscript">n</sub></i>]</span> is the scalar quantity <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> ⋅ <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="fm-italics">x</i><sub class="fm-subscript">1</sub><i class="fm-italics">y</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">2</sub><i class="fm-italics">y</i><sub class="fm-subscript">2</sub> + ⋯ + <i class="fm-italics">x<sub class="fm-subscript">n</sub></i><i class="fm-italics">y</i><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span>. It is a measure of how similar the vectors are. Dot products are widely used in machine learning. For instance, in supervised machine learning, we train the model so that its output is as similar as possible to the known output for a sample set of input points known as training data. Here, some variant of the dot product is often used to measure the similarity of the model output and the known output. Two vectors are orthogonal if their dot product is zero. This means the vectors have no similarity and are independent of each other.<br class="calibre20"/>
      A vector’s dot product with itself is the square of the magnitude or length of the vector <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> ⋅ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>||<sup class="fm-superscript">2</sup> = <i class="fm-italics">x</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">x</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub> + ⋯ + <i class="fm-italics">x<sub class="fm-subscript">n</sub>x<sub class="fm-subscript">n</sub></i></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><a id="marker-81"/>Given a set of vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span>, the weighted sum <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub> + <i class="fm-italics">a</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre19" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub> + ⋯ + <i class="fm-italics">a<sub class="fm-subscript">n</sub></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span> where <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">a<sub class="fm-subscript">n</sub></i> are arbitrary scalars) is known as a linear combination. In particular, if the coefficients <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">a<sub class="fm-subscript">n</sub></i> are non-negative and they sum to <span class="math">1</span>, the linear combination is called a <i class="fm-italics">convex</i> combination.<br class="calibre20"/>
      If it is possible to find a set of coefficients <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">a<sub class="fm-subscript">n</sub></i>, not all zero, such that the linear combination is a null vector (meaning all its elements are zeros), then the vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub></i></span> are said to <i class="fm-italics">linearly dependent</i>. On the other hand, if the only way to obtain a linear combination that is a null vector is to make every coefficient zero, the vectors are said to be <i class="fm-italics">linearly independent</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">One important application of matrices and vectors is to solve a system of linear equations. Such a system can be expressed in matrix vector terms as <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>, where we solve for an unknown vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> satisfying the equation. This system has an exact solution if and only if <i class="timesitalic">A</i> is invertible. This means <i class="timesitalic">A</i> is a square matrix (the number of rows equals the number of columns) and the row vectors are linearly independent. If the row vectors are linearly independent, so are the column vectors, and vice versa. If the rows and columns are linearly independent, the determinant of <i class="timesitalic">A</i> is guaranteed to be nonzero. Hence, linear independence of rows/columns and nonzero determinant are equivalent conditions. If any one of them is satisfied, the linear system has an exact and unique solution.<br class="calibre20"/>
      In practice, this requirement is often not met, and we have an over- or under-determined system. In such situations, the Moore-Penrose inverse leads to a form of best approximation. Geometrically, the Moore-Penrose method yields the point that is closest to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> in the space of vectors spanned by columns of <i class="timesitalic">A</i>. Equivalently, the Moore-Penrose solution <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">*</sub></span> yields the point closest to <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> on the space of vectors spanned by the columns of <i class="timesitalic">A</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">For a square matrix <i class="timesitalic">A</i>, if and only if <span class="math"><i class="fm-italics">Aê</i> = <i class="fm-italics">λê</i></span>, we say <i class="timesitalic">λ</i> is an eigenvalue (a scalar) and <i class="timesitalic">ê</i> is an eigenvector (a unit vector) of <i class="timesitalic">A</i>. Physically, the eigenvector <i class="timesitalic">ê</i> is a unit vector whose direction does not change when transformed by the matrix <i class="timesitalic">A</i>. The transform can magnify its length by the scalar scale factor <i class="timesitalic">λ</i>, which is the eigenvalue.<br class="calibre20"/>
      An <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">A</i> has <i class="timesitalic">n</i> eigenvalue/eigenvector pairs. The eigenvalues need not all be unique. The eigenvectors corresponding to different eigenvalues are linearly independent. If the matrix <i class="timesitalic">A</i> is symmetric, satisfying <span class="math"><i class="fm-italics">A<sup class="fm-superscript">T</sup></i> = <i class="fm-italics">A</i></span>, the eigenvectors corresponding to different eigenvalues are orthogonal. A rotation matrix is a matrix in which the rows are orthogonal to each other and so are the columns. Such a matrix is also known as an orthogonal matrix. An orthogonal matrix <i class="timesitalic">R</i> satisfies the equation <span class="math"><i class="fm-italics">R<sup class="fm-superscript">T</sup>R</i> = <b class="fm-bold">I</b></span>, where <b class="timesbold">I</b> is the identity matrix. In the special case when the matrix <i class="timesitalic">A</i> is a rotation matrix <i class="timesitalic">R</i>, one of the eigenvalues is always <span class="math">1</span>. The corresponding eigenvector is the axis of rotation. A matrix <i class="timesitalic">A</i> with <i class="timesitalic">n</i> linearly independent eigenvectors can be decomposed as <span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">S</i>Λ<i class="fm-italics">S</i><sup class="fm-superscript">−1</sup></span>, where <span class="math"><i class="fm-italics">S</i> =[<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub>    <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub>   …   <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">n</sub></i>]</span> is the matrix with eigenvectors of <i class="timesitalic">A</i> as its columns and <span class="math">Λ</span> is a diagonal matrix with the eigenvalues of <i class="timesitalic">A</i> as its diagonal. This decomposition is called matrix diagonalization and leads to a numerically stable way to solve linear systems.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A square symmetric matrix <i class="timesitalic">A</i> can be expressed in terms of its eigenvectors and eigenvalues as <span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup></i> + <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub><i class="fm-italics"><sup class="fm-superscript">T</sup></i> +   …  + <i class="fm-italics">λ<sub class="fm-subscript">n</sub><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">n</sub></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">n</sub><sup class="fm-superscript">T</sup></i></span>. This is known as the spectral decomposition of the matrix <i class="timesitalic">A</i>.<a id="marker-82"/></p>
</li>
</ul>
<hr class="calibre14"/>
<p class="fm-footnote" id="fn4"><sup class="footnotenumber">1</sup>  In mathematics, vectors can have an infinite number of elements. Such vectors cannot be expressed as arrays—but we will mostly ignore them in this book. <a class="url" href="02.xhtml#fnref4">↩</a></p>
<p class="fm-footnote" id="fn5"><sup class="footnotenumber">2</sup>  We usually use uppercase letters to symbolize matrices. <a class="url" href="02.xhtml#fnref5">↩</a></p>
<p class="fm-footnote" id="fn6"><sup class="footnotenumber">3</sup>  In digital computers, numbers in the range <span class="math">0..255</span> can be represented with a single byte of storage; hence this choice. <a class="url" href="02.xhtml#fnref6">↩</a></p>
<p class="fm-footnote" id="fn7"><sup class="footnotenumber">4</sup>  The mathematical symbol <span class="cambria">∀</span> stands for “for all.” Thus, <span class="math">∀<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> <span class="cambria">∈</span> <span class="segoe">ℜ</span><i class="fm-italics"><sup class="fm-superscript">n</sup></i></span> means “all vectors y in the <i class="timesitalic">n</i>-dimensional space.” <a class="url" href="02.xhtml#fnref7">↩</a></p>
<p class="fm-footnote" id="fn8"><sup class="footnotenumber">5</sup>  You can compute eigenvectors and eigenvalues only of square matrices. <a class="url" href="02.xhtml#fnref8">↩</a></p>
</div></body></html>