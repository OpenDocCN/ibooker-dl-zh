# 第二章。图片中有什么：使用 Keras 进行图像分类

如果您浏览过深度学习文献，可能会看到一大堆充斥着令人生畏的数学的学术解释。不用担心。我们将通过一个简单的例子来引导您进入实际的深度学习，只需几行代码就可以对图像进行分类。

在本章中，我们将更仔细地研究 Keras 框架，讨论它在深度学习领域的地位，然后使用它来使用现有的最先进分类器对一些图像进行分类。我们通过使用*热图*来直观地研究这些分类器的运作方式。通过这些热图，我们将在视频中对对象进行分类，做一个有趣的项目。

回想一下“完美深度学习解决方案的配方”，我们需要四个要素来创建我们的深度学习配方：硬件、数据集、框架和模型。让我们看看这些在本章中是如何发挥作用的：

+   我们从简单的*硬件*开始。即使是一台廉价的笔记本电脑也足以满足本章的需求。或者，您可以通过在 Colab 中打开 GitHub 笔记本（参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）来运行本章中的代码。这只是几次鼠标点击的事情。

+   因为我们暂时不会训练神经网络，所以我们不需要一个*数据集*（除了一些样本照片用于测试）。

+   接下来，我们来看*框架*。本章的标题中包含了 Keras，所以我们暂时会使用它。事实上，在本书的很大一部分中，我们都会使用 Keras 来满足我们的训练需求。

+   解决深度学习问题的一种方法是获取一个数据集，编写训练代码，花费大量时间和精力（包括人力和电力）来训练模型，然后用它进行预测。但我们不是自寻烦恼的人。因此，我们将使用一个*预训练模型*。毕竟，研究界已经花费了大量心血和泪水来训练和发布许多现在公开可用的标准模型。我们将重复使用其中一个更著名的模型，名为 ResNet-50，这是 ResNet-152 的小兄弟，后者在 2015 年赢得了 ILSVRC 比赛。

在本章中，您将亲自动手编写一些代码。众所周知，学习的最佳方式是通过实践。不过，您可能会想知道，这背后的理论是什么？这将在后续章节中介绍，我们将通过本章作为基础，更深入地探讨 CNN 的细节。

# 介绍 Keras

正如第一章所讨论的，Keras 于 2015 年作为一个易于使用的抽象层出现，使快速原型设计成为可能。这使得深度学习的初学者的学习曲线陡峭度降低了很多。同时，它通过帮助他们快速迭代实验，使深度学习专家更加高效。事实上，[Kaggle.com](http://Kaggle.com)上的大多数获胜团队都使用了 Keras。最终，在 2017 年，Keras 的完整实现直接集成到了 TensorFlow 中，将 TensorFlow 的高可扩展性、性能和庞大的生态系统与 Keras 的易用性结合在一起。在网络上，我们经常看到将 TensorFlow 版本的 Keras 称为`tf.keras`。

在本章和第三章中，我们完全使用 Keras 编写所有代码。这包括文件读取、图像处理（增强）等样板函数。我们主要出于学习的便利性。从第五章开始，我们逐渐开始直接使用更多原生高性能的 TensorFlow 函数，以获得更多的可配置性和控制。

# 预测图像的类别

用通俗的语言来说，图像分类回答了一个问题：“这张图像包含什么对象？”更具体地说，“这张图像包含* X *对象的概率是多少”，其中* X *来自预定义的对象类别列表。如果概率高于最小阈值，则图像很可能包含一个或多个* X *实例。

一个简单的图像分类流程包括以下步骤：

1.  加载一张图像。

1.  将其调整为预定义大小，如 224 x 224 像素。

1.  将像素值缩放到范围[0,1]或[–1,1]，也就是归一化。

1.  选择一个预训练模型。

1.  在图像上运行预训练模型，以获取类别预测列表及其相应的概率。

1.  显示几个最高概率类别。

###### 提示

GitHub 链接在网站[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)上提供。导航到`code/chapter-2`，您将找到详细步骤的 Jupyter 笔记本`1-predict-class.ipynb`。

我们首先从 Keras 和 Python 包中导入所有必要的模块：

```py
import tensorflow as tf
from tf.keras.applications.resnet50 import preprocess_input, decode_predictions
from tf.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
```

接下来，我们加载并显示要分类的图像（参见图 2-1）：

```py
img_path = "../../sample-images/cat.jpg"
img = image.load_img(img_path, target_size=(224, 224))
plt.imshow(img)
plt.show()
```

![显示输入文件内容的图表](img/00032.jpeg)

###### 图 2-1。显示输入文件内容的图表

是的，这是一只猫（尽管文件名有点暴露了）。这就是我们的模型理想情况下应该预测的内容。

在将任何图像传递给 Keras 之前，我们希望将其转换为标准格式。这是因为预训练模型期望输入具有特定大小。在我们的情况下，标准化涉及将图像调整为 224 x 224 像素。

大多数深度学习模型期望输入一批图像。但是当我们只有一张图像时该怎么办？当然，我们创建一个包含一张图像的批次！这实质上涉及制作一个由该对象组成的数组。另一种看待这个问题的方式是将维度的数量从三（表示图像的三个通道）扩展到四（额外的一个用于数组本身长度）。

如果这不清楚，考虑这种情况：对于一个包含 64 张尺寸为 224 x 224 像素的图像的批次，每张图像包含三个通道（RGB），表示该批次的对象将具有形状 64 x 224 x 224 x 3。在接下来的代码中，我们将只使用一张尺寸为 224 x 224 x 3 的图像，我们将通过将维度从三扩展到四来创建一个只包含该图像的批次。这个新创建的批次的形状将是 1 x 224 x 224 x 3：

```py
img_array = image.img_to_array(img)
img_batch = np.expand_dims(img_array, axis=0) # Increase the number of dimensions
```

在机器学习中，模型在接收到一致范围内的数据时表现最佳。范围通常包括[0,1]和[–1,1]。鉴于图像像素值在 0 到 255 之间，运行 Keras 的`preprocess_input`函数对输入图像进行归一化，将每个像素归一化到一个标准范围。*归一化*或*特征缩放*是图像预处理的核心步骤之一，使其适用于深度学习。

现在是模型的时间。我们将使用一个名为 ResNet-50 的*卷积神经网络*（CNN）。我们应该问的第一个问题是：“我在哪里找到这个模型？”当然，我们可以在互联网上搜索，找到与我们的深度学习框架（Keras）兼容的内容。*但是没人有时间这样做！*幸运的是，Keras 喜欢简化事情，并通过一个函数调用将其提供给我们。在第一次调用此函数后，模型将从远程服务器下载并在本地缓存：

```py
model = tf.keras.applications.resnet50.ResNet50()
```

当使用这个模型进行预测时，结果包括每个类别的概率预测。Keras 还提供了`decode_predictions`函数，告诉我们图像中包含的每个对象类别的概率。

现在，让我们看看一个方便的函数中的整个代码：

```py
def classify(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    model = tf.keras.applications.resnet50.ResNet50()
    img_array = image.img_to_array(img)
    img_batch = np.expand_dims(img_array, axis=0)
    img_preprocessed = preprocess_input(img_batch)
    prediction = model.predict(img_preprocessed)
    print(decode_predictions(prediction, top=3)[0])

classify("../../sample-images/cat.jpg")
```

```py
[('n02123045', 'tabby', 0.50009364),
 ('n02124075', 'Egyptian_cat', 0.21690978),
 ('n02123159', 'tiger_cat', 0.2061722)]
```

这幅图像的预测类别是各种类型的猫科动物。为什么它不简单地预测“猫”这个词呢？简短的答案是，ResNet-50 模型是在一个包含许多类别的细粒度数据集上训练的，不包括更一般的“猫”。我们稍后会更详细地调查这个数据集，但首先让我们加载另一张样本图像（参见图 2-2）：

```py
img_path = '../../sample-images/dog.jpg'
img = image.load_img(img_path, target_size=(224, 224))
plt.imshow(img)
plt.show()
```

![显示文件 dog.jpg 内容的图](img/00236.jpeg)

###### 图 2-2\. 显示文件 dog.jpg 内容的图

然后，我们再次运行之前的便捷函数：

```py
classify("../../sample-images/dog.jpg")
```

```py
[(u'n02113186', u'Cardigan', 0.809839),
 (u'n02113023', u'Pembroke', 0.17665945),
 (u'n02110806', u'basenji', 0.0042166105)]
```

正如预期的那样，我们得到了不同品种的犬类（不仅仅是“狗”类别）。如果你对柯基品种的狗不熟悉，那么“corgi”这个词在威尔士语中的意思就是“侏儒犬”。卡迪根和彭布罗克是柯基家族的亚品种，它们看起来非常相似。我们的模型也认为是这样，这一点也不奇怪。

注意每个类别的预测概率。通常，具有最高概率的预测被认为是答案。或者，任何高于预定义阈值的值也可以被视为答案。在狗的例子中，如果我们设置阈值为 0.5，那么卡迪根将是我们的答案。

![在浏览器中使用 Google Colab 运行笔记本](img/00183.jpeg)

###### 图 2-3\. 在浏览器中使用 Google Colab 运行笔记本

###### 提示

您可以在本章中跟随代码并在浏览器中交互式地执行，而无需进行任何安装，只需使用 Google Colab。只需在 GitHub 上每个您想要尝试的笔记本顶部找到“在 Colab 上运行”链接。然后，点击“运行单元格”按钮；这应该执行该单元格中的代码，如图 2-3 所示。

# 调查模型

我们从模型中得到了预测，太棒了！但是是什么因素导致了这些预测？这里有一些问题我们需要问：

+   模型是在哪个数据集上训练的？

+   我可以使用其他模型吗？它们有多好？我可以在哪里获取它们？

+   为什么我的模型会做出这样的预测？

我们将在本节中探讨这些问题的答案。

## ImageNet 数据集

让我们调查 ResNet-50 训练的 ImageNet 数据集。正如其名称所示，[ImageNet](http://www.image-net.org/)是一个图像网络；也就是说，这是一个以网络形式组织的图像数据集，如图 2-4 所示。它以分层方式排列（类似于 WordNet 层次结构），使得父节点包含该父节点内所有可能的各种图像的集合。例如，在“动物”父节点内，有鱼类、鸟类、哺乳动物、无脊椎动物等。每个类别都有多个子类别，这些子类别又有子子类别，依此类推。例如，“美国水猎犬”类别距离根节点有八个级别。狗类别包含了总共五个层次中的 189 个子类别。

从视觉上看，我们制作了图 2-5 中显示的树状图，以帮助您了解 ImageNet 数据集包含的各种高级实体。这个树状图还显示了构成 ImageNet 数据集的不同类别的相对百分比。

![ImageNet 数据集中的类别和子类别](img/00258.jpeg)

###### 图 2-4\. ImageNet 数据集中的类别和子类别

![ImageNet 及其类别的树状图](img/00319.jpeg)

###### 图 2-5\. ImageNet 及其类别的树状图

ImageNet 数据集是著名的 ILSVRC 的基础，该比赛始于 2010 年，旨在评估计算机视觉的进展并挑战研究人员在包括对象分类在内的任务上进行创新。回想一下第一章中提到的，ImageNet 挑战看到每年提交的结果在准确性上有了显著提高。当它刚开始时，错误率接近 30%。现在，它是 2.2%，已经优于普通人在这项任务上的表现。这个数据集和挑战被认为是计算机视觉最近进展的最重要原因。

等等，人工智能的准确率比人类还高？如果数据集是由人类创建的，那么人类不应该有 100%的准确率吗？事实上，数据集是由专家创建的，每个图像都经过多人验证。然后，斯坦福研究人员（现在是特斯拉的著名人物）Andrej Karpathy 尝试了解一个普通人在 ImageNet-1000 上的表现。结果他取得了 94.9%的准确率，远低于我们所有人期望的 100%。Andrej 费了一周的时间查看了 1500 张图像，每张图像花费大约一分钟的时间进行标记。他是如何将 5.1%的图像分类错误的呢？原因有点微妙：

细粒度识别

对于许多人来说，很难区分西伯利亚哈士奇和阿拉斯加雪橇犬。真正熟悉狗品种的人能够区分它们，因为他们寻找区分这两种品种的更细节的细节。事实证明，神经网络能够更容易地学习这些更细节的细节，而不是人类。

类别无知

并非每个人都知道所有 120 种狗的品种，当然也不会知道其中的每一种。但人工智能知道。毕竟，它是经过训练的。

###### 注

与 ImageNet 类似，像 Switchboard 这样的语音数据集报告了 5.1%的语音转录错误率（巧合地与 ImageNet 相同）。很明显，人类有极限，而人工智能正在逐渐超越我们。

这种快速改进的另一个关键原因之一是研究人员公开分享了在像 ImageNet 这样的数据集上训练的模型。在下一节中，我们将更详细地了解模型重用。

## 模型动物园

模型动物园是一个组织或个人可以公开上传他们构建的模型供他人重用和改进的地方。这些模型可以使用任何框架（例如 Keras、TensorFlow、MXNet），用于任何任务（分类、检测等），或者在任何数据集上进行训练（例如 ImageNet、街景房屋号码（SVHN））。

模型动物园的传统始于 Caffe，这是第一个深度学习框架之一，由加州大学伯克利分校开发。从头开始在一个拥有数百万图像的数据库上训练深度学习模型需要数周的训练时间和大量的 GPU 计算能量，这使得这项任务变得困难。研究界认识到这是一个瓶颈，参加 ImageNet 比赛的组织在 Caffe 的网站上开源了他们训练过的模型。其他框架很快也效仿。

在开始新的深度学习项目时，首先探索是否已经有一个执行类似任务并在类似数据集上训练的模型是一个好主意。

Keras 中的[模型动物园](https://keras.io/applications/)是使用 Keras 框架在 ImageNet 数据集上训练的各种架构的集合。我们在表 2-1 中列出它们的详细信息。

表 2-1. 选择预训练的 ImageNet 模型的架构细节

| **模型** | **大小** | **Top-1 准确率** | **Top-5 准确率** | **参数** | **深度** |
| --- | --- | --- | --- | --- | --- |
| VGG16 | 528 MB | 0.713 | 0.901 | 138,357,544 | 23 |
| VGG19 | 549 MB | 0.713 | 0.9 | 143,667,240 | 26 |
| ResNet-50 | 98 MB | 0.749 | 0.921 | 25,636,712 | 50 |
| ResNet-101 | 171 MB | 0.764 | 0.928 | 44,707,176 | 101 |
| ResNet-152 | 232 MB | 0.766 | 0.931 | 60,419,944 | 152 |
| InceptionV3 | 92 MB | 0.779 | 0.937 | 23,851,784 | 159 |
| InceptionResNetV2 | 215 MB | 0.803 | 0.953 | 55,873,736 | 572 |
| NASNetMobile | 23 MB | 0.744 | 0.919 | 5,326,716 | — |
| NASNetLarge | 343 MB | 0.825 | 0.96 | 88,949,818 | — |
| MobileNet | 16 MB | 0.704 | 0.895 | 4,253,864 | 88 |
| MobileNetV2 | 14 MB | 0.713 | 0.901 | 3,538,984 | 88 |

“Top-1 准确率”列指示最佳猜测正确答案的次数，“Top-5 准确率”列指示五次猜测中至少有一次正确的次数。网络的“深度”指示网络中存在多少层。“参数”列指示模型的大小；也就是说，模型有多少个独立的权重：参数越多，模型越“重”，预测速度越慢。在本书中，我们经常使用 ResNet-50（在研究论文中引用最多的常见架构，以获得高准确性）和 MobileNet（在速度、大小和准确性之间取得良好平衡）。

## 类激活图

图像显著性，在 UX 研究中通常很有名，试图回答“用户注意力集中在图像的哪个部分？”这是通过眼动研究来实现的，并以热图表示。例如，大号、粗体字或人脸通常比背景更受关注。可以猜想这些热图对设计师和广告商会有多有用，他们可以根据最大化用户注意力来调整内容。受到这种人类显著性版本的启发，了解神经网络关注图像的哪个部分会很有趣，这正是我们将要进行的实验。

在我们的实验中，我们将在视频上叠加一个*类激活图*（或俗称的*热图*），以便了解网络关注的内容。热图告诉我们类似于“在这张图片中，这些像素负责预测类别`dog`，其中`dog`是概率最高的类别。 “热”像素用红色、橙色和黄色等暖色表示，而“冷”像素则用蓝色表示。像素越“热”，提供的信号越高，指向预测的方向。图 2-6 给我们一个更清晰的图像。（如果您正在阅读印刷版本，请参考书的 GitHub 获取原始彩色图片。）

![狗的原始图像及其生成的热图](img/00148.jpeg)

###### 图 2-6\. 狗的原始图像及其生成的热图

在 GitHub 存储库（参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)），导航至*code/chapter-2*。在那里，您会找到一个方便的 Jupyter 笔记本，*2-class-activation-map-on-video.ipynb*，描述以下步骤：

首先，我们需要使用`pip`安装`keras-vis`：

```py
$ pip install keras-vis --user
```

然后，我们在单个图像上运行可视化脚本，生成其热图：

```py
$ python visualization.py --process image --path ../sample-images/dog.jpg
```

我们应该看到一个名为*dog-output.jpg*的新创建的文件，显示原始图像及其热图的并排视图。正如我们从图 2-6 中看到的，图像的右半部分指示了“热区”，以及对“Cardigan”（即威尔士柯基）的正确预测。

接下来，我们想要可视化视频中帧的热图。为此，我们需要`FFmpeg`，这是一个开源的多媒体框架。您可以在[*https://www.ffmpeg.org*](https://www.ffmpeg.org)找到下载二进制文件以及您操作系统的安装说明。

我们使用`ffmpeg`将视频拆分为单独的帧（每秒 25 帧），然后在每个帧上运行我们的可视化脚本。我们必须首先创建一个目录来存储这些帧，并将其名称作为`ffmpeg`命令的一部分传递：

```py
$ mkdir kitchen
$ ffmpeg -i video/kitchen-input.mov -vf fps=25 kitchen/thumb%04d.jpg -hide_banner
```

然后我们使用包含上一步帧的目录路径运行可视化脚本：

```py
$ python visualization.py --process video --path kitchen/
```

我们应该看到一个新创建的*kitchen-output*目录，其中包含来自输入目录的所有帧的热图。

最后，使用`ffmpeg`从这些帧编译一个视频：

```py
$ ffmpeg -framerate 25 -i kitchen-output/result-%04d.jpg kitchen-output.mp4
```

完美！结果是原始视频与覆盖在其上的热图的副本并排显示。这是一个有用的工具，特别是用来发现模型是否学习了正确的特征，或者在训练过程中是否捕捉到了杂散的人工制品。

想象一下生成热图来分析我们训练模型或预训练模型的优点和不足。

您可以通过使用智能手机摄像头拍摄视频并在文件上运行上述脚本来自己尝试这个实验。别忘了在 Twitter 上发布您的视频，标记[@PracticalDLBook](https://www.twitter.com/PracticalDLBook)！

###### 提示

热图是一种在数据中可视化检测偏见的好方法。模型预测的质量严重依赖于其训练的数据。如果数据存在偏见，那将反映在预测中。一个很好的例子是（尽管可能是一个都市传说），美国军方想要使用神经网络来检测伪装在树木中的敌方坦克。^(1) 构建模型的研究人员拍摄了照片——50%包含伪装的坦克，50%只有树木。模型训练得到了 100%的准确率。值得庆祝吗？遗憾的是，当美国军方进行测试时情况并非如此。该模型表现得非常糟糕——不比随机猜测好。调查发现，带有坦克的照片是在多云（阴天）拍摄的，而没有坦克的照片是在晴朗的天气拍摄的。神经网络模型开始寻找天空而不是坦克。如果研究人员使用热图来可视化模型，他们会很早就发现这个问题。

在收集数据时，我们必须警惕潜在的偏见，这可能会影响我们模型的学习。例如，当收集图像来构建食物分类器时，我们应该验证其他人工制品（如盘子和餐具）是否被学习为食物。否则，筷子的存在可能会导致我们的食物被分类为炒面。另一个术语来定义这个问题是*共现性*。食物经常与餐具共同出现。因此要注意这些人工制品是否渗入到分类器的训练中。

# 总结

在本章中，我们通过 Keras 对深度学习宇宙有了一瞥。这是一个易于使用但功能强大的框架，我们将在接下来的几章中使用。我们观察到通常不需要收集数百万张图片并使用强大的 GPU 来训练自定义模型，因为我们可以使用预训练模型来预测图像的类别。通过深入研究像 ImageNet 这样的数据集，我们了解了这些预训练模型可以预测的类别。我们还了解到在大多数框架中存在的模型动物园中可以找到这些模型。

在第三章中，我们探讨了如何调整现有的预训练模型，以便对其原始意图之外的输入类别进行预测。与当前章节一样，我们的方法旨在获得输出，而无需数百万张图片和大量硬件资源来训练分类器。

^(1) [“人工智能作为全球风险中的积极和消极因素”](https://oreil.ly/-svD0) 作者 Eliezer Yudkowsky 在《全球灾难性风险》（牛津大学出版社）中。
