- en: 7 Analyzing audio data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transcribing audio data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating audio data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating speech
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Watch any credible science fiction TV show or movie, and you won’t see people
    typing to interact with their computers! Whether it’s *Star Trek* or *2001: A
    Space Odyssey* (both released in the 1960s), people speak to (not type into) their
    machines. And there are good reasons for that! For most users, voice is the most
    natural form of communication (because that’s the one they start with). No wonder
    people imagined speaking with computers long before that was technically feasible.'
  prefs: []
  type: TYPE_NORMAL
- en: Reality has now caught up with science fiction, and voice assistants, including
    the likes of Amazon’s Alexa, Google’s Assistant, and Microsoft’s Cortana (among
    many others), are ubiquitous. The newest generation of speech recognition (and
    speech generation) models have reached near-human levels of proficiency. And voice-based
    interaction with computers is, of course, only one use case for this amazing technology.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use OpenAI’s latest models for speech transcription,
    translation, and speech generation for several mini-projects. First, we will see
    that transcribing voice recordings to text takes just a few lines of Python code.
    After that, we’ll look at more complex applications, starting with a voice-based
    version of our natural language database query interface from chapter 5\. Whereas
    we previously had to type in questions, we can now simply speak them, and the
    system will produce an answer. Finally, we will see how to build a simultaneous
    translator that turns our voice input into voice output in a different language.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can start with all of those cool projects, we need to perform a few
    setup steps. First, you will need to record voice input via your computer. For
    that to work, you first need some kind of microphone. Most laptops nowadays have
    a built-in microphone. It doesn’t have to be a professional microphone; any way
    of recording sound on your computer will do. But beyond the microphone, you also
    need software that can be activated from Python to turn audio recordings into
    files. For that, we will use Python’s `sounddevice` library. Run the following
    command in the terminal to install this library in the correct version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This library interacts with Python’s `scipy` library, which you should also
    install. Run the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Together, those libraries will enable you to record voice input (which you can
    then transcribe, translate, or summarize using OpenAI’s models).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have covered the input side, but what about the output? For some of the
    following projects, we not only want to listen to audio but also generate it!
    To generate speech, we will use OpenAI’s generative AI models again. But after
    generating speech stored in an audio file, we still need suitable libraries to
    play speech on our computer from Python. We will use the `playsound` library for
    that. Run this command in the terminal to install this library in the correct
    version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On certain operating systems (in particular, macOS), you additionally have
    to install the `PyObjC` library using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you haven’t done so already when working through the last chapter, install
    the `requests` library (enabling you to send requests directly to OpenAI’s API):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Well done! If you didn’t encounter any error messages running these commands,
    your system is now configured to process audio data with OpenAI’s Transformer
    models. Let’s start with our first project in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Transcribing audio files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having recently started your job at Banana, you are overwhelmed by the number
    of meetings. There are just too many meetings to attend, but you don’t want to
    miss anything important! Fortunately, Banana has the good sense to create audio
    recordings of all employee meetings as a general rule (with the consent of all
    participants). But listening to all the recordings of those meetings is still
    too time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: It would be great to have transcripts of meetings, enabling you to quickly search
    for anything relevant to your unit via a simple text search. Unfortunately, Banana
    doesn’t offer such transcripts out of the box, and none of your colleagues are
    willing to take notes during those meetings. Would it be possible to create such
    transcripts automatically? In this section, we will see that it’s not only possible
    but actually easy to create such an automated transcription service.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Transcribing speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For transcribing speech to text, we will use OpenAI’s Whisper model. Unlike
    the models we have used so far (in particular, GPT models), Whisper is specifically
    targeted at audio transcriptions.
  prefs: []
  type: TYPE_NORMAL
- en: What is the Whisper model?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Whisper is a Transformer model trained on large numbers of audio recordings
    (more than 680,000 hours of recordings, to be precise!). Whisper was trained on
    a multilingual audio corpus and therefore supports a broad range of input languages
    that it transcribes to English (i.e., you get speech transcription and translation
    in a single step).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the GPT variants, we will access Whisper via OpenAI’s Python library.
    This means no additional setup is required on your local machine (assuming that
    you have installed OpenAI’s Python library, as described in chapter 3).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will use the Whisper model to transcribe an audio file
    to disk. Let’s assume that our audio file is initially stored on disk. Whisper
    supports a wide range of file formats: MP3, MP4, MPEG, MPGA, M4A, WAV, and WEBM.
    At the time of writing, the file size is limited to 25 MB. Given such a file,
    let’s assume that its file path is stored in the variable `audio_path`. Now all
    it takes to transcribe its content to text are the following few lines of Python
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Opens the audio file'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Transcribes the content'
  prefs: []
  type: TYPE_NORMAL
- en: As a first step (**1**), we need to open our audio file. For that, we can use
    Python’s `open` command. Note the use of the `rb` flag as a parameter of the `open`
    command. This flag indicates to Python that we want to read the file (`r`) and
    that we are opening a binary file (`b`). A binary file is a file that does not
    contain readable characters. Sound files, such as the one we are trying to open
    here, generally qualify as binary files. After processing the first line, the
    file content is accessible via the variable `audio_file`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a second step (**2**), we perform the actual transcription. We now use a
    different endpoint, specialized for audio data processing. From that endpoint,
    we invoke the transcription service (`transcriptions.create`) using two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`file`—A reference to the file to transcribe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`—The name of the model for transcription'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We refer to the previously opened file (`audio_file`) and select `whisper-1`
    as our transcription model. The result of transcription is an object containing
    the transcribed text and metadata about the transcription process. We can access
    the transcribed text via the `text` field (i.e., via `transcription.text`).ents.
    After decompression, you should see three subdirectories in the resulting folder:'
  prefs: []
  type: TYPE_NORMAL
- en: As you see, transcribing text takes just a few lines of Python code! In the
    next subsection, we will use this code to build a simple transcription service.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 End-to-end code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Listing [7.1](#code__transcription) shows the code for a simple transcription
    program. The actual transcription happens in the `transcribe` function (**1**).
    This is essentially the code we discussed in the previous section. Given the path
    to an audio file as input, it returns the transcribed text.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.1 Transcribing audio files to text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Transcribes audio to text'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Main function'
  prefs: []
  type: TYPE_NORMAL
- en: The main function (**2**) reads the path to an audio file (which should contain
    speech) as input. After invoking the `transcriptions.create` function, it prints
    the transcribed text on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3 Trying it out
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To try it, we first need an audio file with recorded speech. You can use any
    such file (including a recording of your company meetings, if available) as long
    as it complies with the format and size restrictions outlined in section [7.2.1](#sub__transcriptionBasics).
    However, keep in mind that you pay per minute of audio data processed! At the
    time of writing, using Whisper via the OpenAI library costs $0.006 per minute
    (you can find more up-to-date information about pricing at [https://openai.com/pricing](https://openai.com/pricing)).
    Processing long recordings can therefore be expensive.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t want to use your own recording, have a look at the book’s companion
    website. You can find a short recording in the Audio item in this chapter’s section.
    Download this recording to use it for transcription (by default, the filename
    should be QuoteFromTheAlchemist.mp3).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing [7.1](#code__transcription) is also available on the book’s companion
    website (item listing1.py in the chapter 7 section). After downloading it, switch
    to the corresponding repository in the terminal. Assuming that you downloaded
    the audio file into the current directory, run the following command in the terminal
    to transcribe the sample file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything goes well, you should see the following output in the terminal
    (for the sample file from the website, that is):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Click the sample file to listen to it yourself; you will find the transcript
    to be accurate! Next, we will integrate speech transcription into more complex
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Querying relational data via voice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analyzing tabular data is fun! A significant part of your job at Banana consists
    of poring over data tables, extracting insights, and preparing corresponding reports
    and visualizations. You’re using the text-to-SQL interface from chapter 5 to automatically
    translate text questions to formal queries (written in SQL), execute them, and
    present the query results. This makes analyzing data easier and is faster than
    writing complex SQL queries from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a problem: you think better when pacing back and forth in
    your office while analyzing data. But typing queries forces you back to your desk
    every time. Can’t we modify our query interface to accept spoken, as opposed to
    typed, input? It turns out that indeed, we can! In this section, we will see how
    to use OpenAI’s models to enable a simple voice query interface for tabular data.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Preliminaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will build a voice query interface that processes spoken questions on tabular
    data. It is an extension of the query interface discussed in chapter 5\. We assume
    that spoken questions refer to data stored in SQLite, a popular system for processing
    queries on relational data. See chapter 5 for a short introduction to SQLite and
    installation instructions. To try the following code, you will first need to install
    the SQLite database system.
  prefs: []
  type: TYPE_NORMAL
- en: The SQLite system processes queries formulated in SQL, the structured query
    language. Fortunately, you won’t need to write SQL queries yourself (we will use
    a language model to write those SQL queries for us). However, language models
    are not perfect and may occasionally produce incorrect queries. To recognize those
    cases, it is useful to have a certain degree of SQL background. You will find
    a short introduction to SQL in chapter 5\. For more details, have a look at [www.databaselecture.com](http://www.databaselecture.com).
  prefs: []
  type: TYPE_NORMAL
- en: Our voice query interface processes spoken questions, so you need to ensure
    that your microphone is working. Also, to execute the following code, make sure
    your voice query interface has all the required permissions to access the microphone.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our voice query interface processes spoken questions on tabular data stored
    in an SQLite database. For instance, having loaded a database with data about
    computer game sales, we can ask questions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: “How many games did Activision sell in 2023?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “How many action games were released between 2019 and 2021?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On receiving a spoken question, the voice query interface performs the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Transcribes the spoken question into text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translates the text question into an SQL query
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processes the SQL query on the data using SQLite
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Displays the query result to the user
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure [7.1](#fig__VQIoverview) illustrates the different processing steps in
    more detail. The process is executed for each spoken question.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F01_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 Our voice query interface transcribes spoken questions into text,
    translates text questions into SQL queries, and finally processes those queries
    and displays the query result.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7.3.3 Recording audio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For our transcription application, we assumed that an audio recording was already
    available. For our new project, we want to issue voice queries repeatedly. That
    means we have to record them ourselves. How can we do that in Python? First, we
    need to import two libraries for precisely that purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Records audio'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Stores .wav files'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sounddevice` library (**1**) contains many useful functions to record
    audio input from a microphone. What will we do with our recordings? We will store
    them as .wav files on disk. In the previous section, we saw how to transcribe
    audio data stored in that format. This is where the second library (**2**), `scipy`,
    comes into play: it enables us to store the recordings in .wav format on disk.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When recording, we need to make two important choices:'
  prefs: []
  type: TYPE_NORMAL
- en: At what sample rate should we read input from the microphone?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many seconds of speech should we record?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will record for a duration of 5 seconds. Five seconds should suffice for
    most voice queries. You can try different settings if the recording tends to terminate
    too soon or if you find yourself waiting often after finishing your voice queries.
    A more sophisticated implementation would record continuously or stop recording
    after a speaking pause is detected. To keep things simple in terms of the recording
    mechanism, we will just record for a predetermined amount of time for each voice
    query.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sampling rate—that is, the number of audio data points stored per second—we
    will choose 44,100 Hertz. This is the standard for CD-quality recordings. The
    total number of *frames*—the number of audio data points received in total—is
    then 44,100 times the number of seconds we want to record (in our case, that’s
    5 seconds). We store the number of frames and the sampling rate in auxiliary variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’re ready to record using the `rec` function of the `sounddevice` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets up recording'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Waits for recording to finish'
  prefs: []
  type: TYPE_NORMAL
- en: The first command (**1**) starts a recording from the input microphone, providing
    as input the total number of frames to record as well as the sampling rate. The
    number of channels (the third parameter in our invocation) depends on the microphone
    used for the recording. If your microphone has more than one channel, try a higher
    value here. After starting the recording, we just need to wait until the predetermined
    recording time has passed. We accomplish that via the `wait` command (**2**).
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the previous code, the variable `recording` contains the recorded
    audio data. As discussed earlier, we want to store the recording as a .wav file
    on disk. All it takes is a single command from the `scipy` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! We have recorded a few seconds of audio input and stored it in a
    file on disk.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.4 End-to-end code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Listing [7.2](#code__voicequeries) shows the code for our voice query interface.
    Beyond our default libraries, `openai` and `argparse`, we import (**1**) the libraries
    for audio processing (`sounddevice` and `scipy`), as well as the `sqlite3` library
    (which we will need for processing SQL queries) and the `time` library. The latter
    library is required to wait for a specified amount of time (for voice input).
    Next, we will discuss the functions introduced in listing [7.2](#code__voicequeries).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2 Querying an SQLite database using voice commands
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Extracts the database schema'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Records audio'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Transcribes audio'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Creates a text-to-SQL prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Translates to SQL'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Processes the SQL query'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Processes the voice queries'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Main loop'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Transcribes voice input'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 SQL translation'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Executes the SQL query'
  prefs: []
  type: TYPE_NORMAL
- en: We process voice queries that refer to data in a relational database. To translate
    voice commands into formal queries formulated in SQL, we need to know a little
    about the database structure. In particular, we need to know the names of the
    data tables and their columns (i.e., we need to know the database schema). The
    function `get_structure` (**2**) retrieves the commands used to create the database
    schema. These commands contain the names of tables and columns, as well as the
    data types associated with the table columns. We will use those commands as part
    of a prompt, instructing the language model to translate questions into SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can translate questions, we first need to record them from the microphone.
    This is where the function `record` (**3**) comes into play. It uses the `sounddevice`
    library to record 5 consecutive seconds of audio input from the microphone. The
    resulting audio recording is stored as a .wav file on disk at a path specified
    as function input (parameter `output_path`). Strictly speaking, storing the audio
    input as a file is not necessary (we can process it directly in memory). However,
    storing audio input on disk can be useful for debugging purposes. If our system
    fails to translate voice input to appropriate queries, we can listen to the audio
    file ourselves to assess the level of background noise and overall audio quality.
    If the microphone is not set up properly (a common problem), our audio files will
    contain nothing but silence.
  prefs: []
  type: TYPE_NORMAL
- en: After recording input from the microphone, we first want to transcribe voice
    input to text. We use the `transcribe` function (**4**) for that. Given a path
    to an audio file (in this case, recorded audio input from the microphone), it
    returns a transcript generated using OpenAI’s Whisper model (the same one we used
    previously).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we want to translate questions into formal SQL queries. Of course, we
    will use language models for that task. The `create_prompt` function (**5**) generates
    a suitable prompt. The prompt contains the previously extracted description of
    the database, the transcribed question, and the task description. The `call_llm`
    function (**6**) calls GPT-4o to translate questions, given the previously mentioned
    prompt as input. Finally, the `process_query` function (**7**) processes the resulting
    queries on the database and returns the query result.
  prefs: []
  type: TYPE_NORMAL
- en: Time to put it all together! Our voice query interface takes the path to an
    SQLite database file as input (**8**). After extracting the database schema, we
    enter the main loop (**9**). Each iteration processes one voice query (unless
    the user enters `quit`, in which case the program terminates). To keep things
    simple, we wait for the user to press the Enter key before recording voice input
    (a more sophisticated version would record continuously). After that, we record
    voice input from the microphone. We print out the transcribed question and store
    the recording itself as question.wav on disk (**10**). Next, we translate the
    transcribed text into a query (**11**), execute it (**12**) (we need exception
    handling here in case of incorrect queries!), and show the result to users.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.5 Trying it out
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Listing [7.2](#code__voicequeries) is listing 2 in the chapter 7 section on
    the book’s website. Download the code, and switch to the containing folder in
    your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the code, we also need an SQLite database to try our voice query interface.
    We discuss in chapter 5 how to set up an example database containing information
    about computer game sales. We assume this database is stored in the same folder
    as your code and named games.db (of course, you are free to use any SQLite database
    you like to try the voice query interface). Now enter the following command in
    the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the path to the database file to the one you want to access. Depending
    on your operating system and security settings, you may be asked to enable microphone
    access for your application. After enabling microphone access, press Enter, and
    ask a question! For instance, using the games database, you may ask “How many
    games were sold in 2007?” or “How many games were released for each genre?” You
    should see output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This output includes the transcribed question, the translated SQL query, and
    the query result (or an error message if the query cannot be executed). Clearly,
    it’s a long way from a voice question to a query result! A mistake in recording,
    transcription, or translation will lead to incorrect results. Before trusting
    the query result, be sure to check the additional output to verify that the system
    did not make any mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip If your voice interface only produces nonsense, check the recordings in
    question.wav. If you don’t hear anything, make sure your application has access
    to your microphone. By default, applications typically have no access to the microphone
    (making it harder to spy on you with malicious software). You need to update your
    security settings to enable access.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Speech-to-speech translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Banana branch in Paris has started looking into language models and potential
    applications for data science tasks. You have grown your reputation as the local
    expert on the topic, and your manager asks you to advise the French team on how
    to get started. There is just one tiny problem: you don’t speak any French. On
    hearing that Banana Paris conducts most staff meetings in French, you are about
    to decline the assignment. But after thinking about it, you realize that this
    may not be a dealbreaker after all. Although you don’t speak any French, GPT-4o
    certainly does! Would it be possible to use language models to translate for you?'
  prefs: []
  type: TYPE_NORMAL
- en: You can indeed use language models to translate between various languages. In
    this section, we will create a translator tool that takes spoken input in a first
    language and produces spoken output in a second language. Because the tool produces
    spoken output, you don’t even need to learn the French pronunciation. Simply speak
    English and wait for the tool to produce a spoken translation. That way, you can
    collaborate with your French colleagues while simultaneously demonstrating the
    capabilities of state-of-the-art language models!
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our translator tool processes spoken input. As before, we will use OpenAI’s
    Whisper model to transcribe input speech to text. Then, we will use the GPT-4o
    model to translate the text to a different language. For our example scenarios,
    we use French as the target language. However, due to the amazing flexibility
    of models like GPT-4o, our tool won’t be restricted to that! Our tool will enable
    users to specify the target language as input, to be used as a text snippet in
    the prompt instructing the language model for the translation.
  prefs: []
  type: TYPE_NORMAL
- en: After generating a text translation, we still want to generate a spoken version.
    It turns out that we can use yet another OpenAI model to transform text into spoken
    output in various languages. Figure [7.2](#fig__TranslatorOverview) shows the
    complete processing pipeline, starting with spoken input in a first language and
    ending with spoken output in a second.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F02_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 Our translator tool records spoken input in a first language, transcribes
    input to text, trans- lates that text into a second language, and finally generates
    spoken output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7.4.2 Generating speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pipeline in figure [7.2](#fig__TranslatorOverview) requires several transformations.
    We already saw how to transcribe spoken input to text in the previous sections.
    Translating text via language models is relatively straightforward (ask GPT-4o
    to translate from one language to another, and it will do so). We are still missing
    a way to transform written text (e.g., in French) into spoken output. We discuss
    how to do that next.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI (as well as other providers) offers several text-to-speech (TTS) models.
    Such models take written text as input and produce a spoken version as output.
    The following piece of code generates speech for a text string (stored in the
    variable `speech_text`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re using a new endpoint in this instance (`audio.speech`) and configuring
    the `create` method using three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model`—The name of the model used to generate spoken output. We use OpenAI’s
    `tts-1` text-to-speech model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input`—We generate spoken output for this text. Submit text in any of the
    various languages supported by the model ([https://github.com/openai/whisper](https://github.com/openai/whisper)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`voice`—We can choose between different voices for speech. Here, we use `alloy`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s all we need to generate speech output via OpenAI! We already know how
    to transcribe speech and how to translate text between different languages, so
    we now have all we need to code our translator tool.
  prefs: []
  type: TYPE_NORMAL
- en: What about pricing?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At the time of writing, OpenAI charges 1.5 cents per 1,000 tokens for text generation
    using the TTS model and twice that for the high-quality version (TTS HD). These
    prices are likely to change over time, so be sure to look at OpenAI’s pricing
    website ([https://openai.com/pricing](https://openai.com/pricing)) for updated
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.3 End-to-end code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Listing [7.3](#code__translator) shows the complete code for our translator
    tool. Let’s start by discussing the libraries it imports (**1**). Besides the
    `openai` and `argparse` libraries included in each project so far, we import `sounddevice`
    and `scipy` to record and store audio files, along with the `time` library to
    limit recording time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.3 Translating spoken input into a different language
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Records audio'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Transcribes audio'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Generates a prompt for translation'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Uses the language model'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Generates speech'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Translates speech to speech'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Main loop'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Transcribes the input'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Translates to the target language'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Generates speech output'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Plays the generated speech'
  prefs: []
  type: TYPE_NORMAL
- en: The `playsound` library is used to play audio files generated by OpenAI’s models.
    Because we generate speech via OpenAI’s HTTP interface, we import the `requests`
    library to create HTTP requests. Next, we will discuss the functions used in listing
    [7.3](#code__translator).
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous project, we record audio data from the microphone. The `record`
    function (**2**) records 5 seconds of audio input and stores it into a .wav file
    on disk. The `transcribe` function (**3**) transcribes that audio input to text.
    Both functions have been discussed in more detail in the prior projects in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The `create_prompt` function (**4**) generates a prompt for translation. As
    in prior projects, the prompt contains a task description, together with all relevant
    input data. In this case, we want to translate from the initial language (English)
    to the target language (French). Note that the target language is specified as
    an input parameter (`to_language`). This input parameter corresponds to a text
    snippet describing the desired output language. In the simplest case, this can
    be the name of a language (e.g., “French”). On the other hand, users can request
    a specific dialect (e.g., “German with Swabian dialect”) or style (e.g., “English
    in the style of Shakespeare”). The target language is integrated into the task
    description that appears in the prompt along with the text to translate.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we do not need to specify the input language. We assume that the language
    model is able to recognize the language of the input text (otherwise, we cannot
    expect the model to translate either).
  prefs: []
  type: TYPE_NORMAL
- en: After invoking the `call_llm` function (**5**) with the prompt, we should obtain
    translated text. The `generate_speech` function (**6**) generates the corresponding
    speech using the approach we discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: The translator application (**7**) expects as input a text describing the target
    language. This parameter is a string that can contain arbitrary text. It simply
    replaces a placeholder in the prompt used for translation. In the main loop (**8**),
    users press Enter to speak or enter `quit` to terminate the application.
  prefs: []
  type: TYPE_NORMAL
- en: When recording user input, we first store 5 seconds of audio recording in a
    file named to_translate.wav before transcribing the input via the `transcribe`
    function (**9**). After that, we use GPT-4o to translate the input to the target
    language (**10**) and then generate speech from the translation (**11**). We store
    the generated speech as an .mp3 file on disk (this means we can easily hear the
    last output again) and, finally, use the `playsound` library to—you guessed it—play
    the generated sound file.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.4 Trying it out
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Time to try our translator! You can find the code on the companion website
    as listing 3 in the chapter 7 section. Download the code, and switch to the containing
    folder in the terminal. We can choose our target language for translation. Of
    course, the quality of the translation and sound output may vary, depending on
    that choice. In particular, the model we use for transcription, as well as the
    model we use for speech generation, support a set of about 60 common languages.
    Transcribing audio input or generating audio output in less common languages may
    fail. Look online to see the current list of supported languages for transcription
    ([https://help.openai.com/en/articles/7031512-whisper-api-faq](https://help.openai.com/en/articles/7031512-whisper-api-faq))
    as well as speech generation ([https://platform.openai.com/docs/guides/text-to-speech](https://platform.openai.com/docs/guides/text-to-speech)).
    For now, consistent with our scenario at the beginning of this section, we will
    go with French as the target language. In the terminal, enter the following command
    to start our translator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Strictly speaking, the quotes around the word “French” are unnecessary. However,
    as we can enter multiword descriptions of the desired target language, we will
    need quotes in the following examples to avoid errors if the console misinterprets
    our input as values for multiple parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in our previous project, we need to give our application access to the microphone.
    Click Yes if you are asked for microphone access; if not, be sure the security
    settings allow it. The following is an extract from a conversation with our translator
    tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You see transcribed input and the generated translation. You should also hear
    the spoken version of the translation (if not, check your settings for audio output).
    Not bad for a few lines of Python code!
  prefs: []
  type: TYPE_NORMAL
- en: 'Translating to French seems like a reasonable use case for our translator tool.
    However, it may not be the one with the highest “fun factor.” Let’s try something
    different to show the flexibility of language models: let’s see if we can “translate”
    our audio input to a highly polished version. In the terminal, enter the following
    instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we get when translating our simple greeting into a much more refined
    version (perhaps a nice intro to a course on language models for our U.S. colleagues
    at Banana):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Try a few more target languages! The possibilities are (almost) unlimited.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI’s Whisper model transcribes speech input to text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access transcription via the audio transcriptions endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pricing for transcription is based on the number of minutes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI offers several models for generating speech from text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can choose the voice and quality for generated speech.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech generation pricing depends on the number of tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
