<html><head></head><body><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. LLM Fundamentals with LangChain" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch01_llm_fundamentals_with_langchain_1736545659776004">&#13;
<h1><span class="label">Chapter 1. </span>LLM Fundamentals with LangChain</h1>&#13;
&#13;
<p>The <a data-type="xref" href="preface01.html#pr01_preface_1736545679069216">Preface</a> gave you a taste of the power of LLM prompting, where we saw firsthand the impact that different prompting techniques can have on what you get out of LLMs, especially when judiciously combined. The<a contenteditable="false" data-primary="LLM applications" data-secondary="challenge in building good applications" data-type="indexterm" id="id339"/> challenge in building good LLM applications is, in fact, in how to effectively construct the prompt sent to the model and process the model’s prediction to return an accurate output (see <a data-type="xref" href="#ch01_figure_1_1736545659763063">Figure 1-1</a>).</p>&#13;
&#13;
<figure><div class="figure" id="ch01_figure_1_1736545659763063"><img src="assets/lelc_0101.png"/>&#13;
<h6><span class="label">Figure 1-1. </span>The challenge in making LLMs a useful part of your application</h6>&#13;
</div></figure>&#13;
&#13;
<p>If you can solve this problem, you are well on your way to building LLM applications, simple and complex alike. In this chapter, you’ll learn more about how LangChain’s building blocks map to LLM concepts and how, when combined effectively, they enable you to build LLM applications. But first, the sidebar <a href="#ch01_why_langchain_1736545659776355">“Why LangChain?”</a> is a brief primer on why we think it useful to use LangChain to build LLM applications.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch01_why_langchain_1736545659776355">&#13;
<h1>Why LangChain?</h1>&#13;
&#13;
<p>You<a contenteditable="false" data-primary="LangChain" data-secondary="benefits of" data-type="indexterm" id="LCbene01"/><a contenteditable="false" data-primary="LLM applications" data-secondary="benefits of LangChain for building" data-type="indexterm" id="LLMbene01"/> can of course build LLM applications without LangChain. The most obvious alternative is to use the software development kit (SDK)—the package exposing the methods of their HTTP API as functions in the programming language of your choice—of the LLM provider you tried first (for example, OpenAI). We think learning LangChain will pay off in the short term and over the long run because of the following factors:</p>&#13;
&#13;
<dl>&#13;
	<dt>Prebuilt common patterns</dt>&#13;
	<dd>&#13;
	<p>LangChain comes with reference implementations of the most common LLM application patterns (we mentioned some of these in the <a data-type="xref" href="preface01.html#pr01_preface_1736545679069216">Preface</a>: chain-of-thought, tool calling, and others). This is the quickest way to get started with LLMs and might often be all you need. We’d suggest starting any new application from these and checking whether the results out of the box are good enough for your use case. If not, then see the next item for the other half of the LangChain libraries.</p>&#13;
	</dd>&#13;
	<dt>Interchangeable building blocks</dt>&#13;
	<dd>&#13;
	<p>These are components that can be easily swapped out for alternatives. Every component (an LLM, chat model, output parser, and so on—more on these shortly) follows a shared specification, which makes your application future-proof. As new capabilities are released by model providers and as your needs change, you can evolve your application without rewriting it each time.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Throughout this book we make use of the following major components in the code examples:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>LLM/chat model: OpenAI</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Embeddings: OpenAI</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Vector store: PGVector</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>You can swap out each of these for any of the<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="alternative providers for" data-type="indexterm" id="id340"/> alternatives listed on the following pages:</p>&#13;
&#13;
<dl>&#13;
	<dt>Chat models</dt>&#13;
	<dd>&#13;
	<p>See<a contenteditable="false" data-primary="chat models" data-secondary="alternative LLM providers for" data-type="indexterm" id="id341"/> the <a href="https://oreil.ly/8Qlnb">LangChain documentation</a>. If you don’t want to use OpenAI (a commercial API) we suggest<a contenteditable="false" data-primary="Anthropic" data-type="indexterm" id="id342"/> <a href="https://oreil.ly/XdGfD">Anthropic</a> as a commercial alternative<a contenteditable="false" data-primary="Ollama" data-type="indexterm" id="id343"/> or <a href="https://oreil.ly/eKy6-">Ollama</a> as an open source one.</p>&#13;
	</dd>&#13;
	<dt>Embeddings</dt>&#13;
	<dd>&#13;
	<p>See the<a contenteditable="false" data-primary="embedding models" data-type="indexterm" id="id344"/><a contenteditable="false" data-primary="LangChain" data-secondary="documentation for" data-type="indexterm" id="id345"/> <a href="https://oreil.ly/sKpfM">LangChain documentation</a>. If you don’t want to use OpenAI (a commercial API) we suggest<a contenteditable="false" data-primary="Cohere" data-type="indexterm" id="id346"/> <a href="https://oreil.ly/o1D0C">Cohere</a> as a commercial alternative or <a href="https://oreil.ly/FarfL">Ollama</a> as an open source one.</p>&#13;
	</dd>&#13;
	<dt>Vector stores</dt>&#13;
	<dd>&#13;
	<p>See<a contenteditable="false" data-primary="vector stores" data-secondary="alternative providers for" data-type="indexterm" id="id347"/> the <a href="https://oreil.ly/q3RF1">LangChain documentation</a>. If you don’t want to use PGVector (an open source extension to the popular SQL database Postgres) we suggest using either<a contenteditable="false" data-primary="Weaviate" data-type="indexterm" id="id348"/> <a href="https://oreil.ly/XqlYa">Weaviate</a> (a dedicated vector store) or<a contenteditable="false" data-primary="OpenSearch" data-type="indexterm" id="id349"/> <a href="https://oreil.ly/1s357">OpenSearch</a> (vector search features that are part of a popular search database).</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>This effort goes beyond, for instance, all LLMs having the same methods, with similar arguments and return values. Let’s look at the example of chat models and two popular LLM providers, OpenAI and Anthropic. Both have a chat API which receives <em>chat messages</em> (loosely defined as objects with a type string and a content string) and returns a new message generated by the model. But if you try to use both models in the same conversation, you’ll immediately run into issues, as their chat message formats are subtly incompatible. LangChain abstracts away these differences to enable building applications that are truly independent of a particular provider. For instance, with LangChain, a chatbot conversation where you use both OpenAI and Anthropic models works.</p>&#13;
&#13;
<p>Finally, as you build out your LLM applications with several of these components, we’ve found it useful to have the<a contenteditable="false" data-primary="orchestration capabilities" data-type="indexterm" id="id350"/> <em>orchestration</em> capabilities of LangChain:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>All major components are instrumented by the callbacks system for observability (more on this in <a data-type="xref" href="ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600">Chapter 8</a>).</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>All major components implement the same interface (more on this toward the end of this chapter).</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Long-running LLM applications can be interrupted, resumed, or retried (more on this in <a data-type="xref" href="ch06.html#ch06_agent_architecture_1736545671750341">Chapter 6</a>).<a contenteditable="false" data-primary="" data-startref="LCbene01" data-type="indexterm" id="id351"/><a contenteditable="false" data-primary="" data-startref="LLMbene01" data-type="indexterm" id="id352"/></p>&#13;
	</li>&#13;
</ul>&#13;
</div></aside>&#13;
&#13;
<section data-pdf-bookmark="Getting Set Up with LangChain" data-type="sect1"><div class="sect1" id="ch01_getting_set_up_with_langchain_1736545659776600">&#13;
<h1>Getting Set Up with LangChain</h1>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="LangChain" data-secondary="setting up" data-type="indexterm" id="id353"/> follow along with the rest of the chapter, and the chapters to come, we recommend setting up LangChain on your computer first.</p>&#13;
&#13;
<p>See the instructions in the <a data-type="xref" href="preface01.html#pr01_preface_1736545679069216">Preface</a> regarding setting up an<a contenteditable="false" data-primary="OpenAI API" data-secondary="account creation" data-type="indexterm" id="id354"/> OpenAI account and complete these if you haven’t yet. If you prefer using a different LLM provider, see <a data-type="xref" href="#ch01_why_langchain_1736545659776355">“Why LangChain?”</a> for alternatives.</p>&#13;
&#13;
<p>Then head over to the <a href="https://oreil.ly/BKrtV">API Keys page</a> on the OpenAI website (after logging in to your OpenAI account), create an API key, and save it—you’ll need it soon.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>In this book, we’ll show code examples in both Python and JavaScript (JS). LangChain offers the same functionality in both languages, so just pick the one you’re most comfortable with and follow the respective code snippets throughout the book (the code examples for each language are equivalent).</p>&#13;
</div>&#13;
&#13;
<p>First, some setup instructions for readers using<a contenteditable="false" data-primary="Python" data-secondary="LangChain set up using" data-type="indexterm" id="id355"/> Python:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Ensure that you have Python installed. See the <a href="https://oreil.ly/20K9l">instructions for your operating system</a>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Install Jupyter if you want to run the examples in a notebook environment. You can do this by running <code>pip install notebook</code> in your terminal.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p class="pagebreak-before less_space">Install the LangChain library by running the following commands in your <span class="keep-together">terminal:</span></p>&#13;
&#13;
	<pre data-type="programlisting">&#13;
pip install langchain langchain-openai langchain-community &#13;
pip install langchain-text-splitters langchain-postgres</pre>&#13;
	</li>&#13;
	<li>&#13;
	<p>Take the OpenAI API key you generated at the beginning of this section and make it available in your terminal environment. You can do this by running the following:</p>&#13;
&#13;
	<pre data-type="programlisting">&#13;
export OPENAI_API_KEY=your-key</pre>&#13;
	</li>&#13;
	<li>&#13;
	<p>Don’t forget to replace <code>your-key</code> with the API key you generated previously.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Open a Jupyter notebook by running this command:</p>&#13;
&#13;
	<pre data-type="programlisting">&#13;
jupyter notebook</pre>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>You’re now ready to follow along with the Python code examples.</p>&#13;
&#13;
<p>Here are the instructions for readers using<a contenteditable="false" data-primary="JavaScript" data-secondary="LangChain set up using" data-type="indexterm" id="id356"/> JavaScript:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Take the OpenAI API key you generated at the beginning of this section and make it available in your terminal environment. You can do this by running the following:</p>&#13;
&#13;
	<pre data-type="programlisting">&#13;
export OPENAI_API_KEY=your-key</pre>&#13;
	</li>&#13;
	<li>&#13;
	<p>Don’t forget to replace <code>your-key</code> with the API key you generated previously.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>If you want to run the examples as Node.js scripts, install Node by following the <a href="https://oreil.ly/5gjiO">instructions</a>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Install the LangChain libraries by running the following commands in your terminal:</p>&#13;
&#13;
	<pre data-type="programlisting">&#13;
npm install langchain @langchain/openai @langchain/community&#13;
npm install @langchain/core pg</pre>&#13;
	</li>&#13;
	<li>&#13;
	<p>Take each example, save it as a <em>.js</em> file and run it with <code>node ./file.js</code>.</p>&#13;
	</li>&#13;
</ol>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Using LLMs in LangChain" data-type="sect1"><div class="sect1" id="ch01_using_llms_in_langchain_1736545659776671">&#13;
<h1>Using LLMs in LangChain</h1>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="LangChain" data-secondary="using LLMs in" data-type="indexterm" id="LCllms01"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="using in LangChain" data-type="indexterm" id="LLMland01"/> recap, LLMs are the driving engine behind most generative AI applications. LangChain provides two simple<a contenteditable="false" data-primary="LangChain" data-secondary="interfaces provided" data-type="indexterm" id="id357"/> interfaces to interact with any LLM API provider:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Chat models</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>LLMs</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>The LLM interface simply takes a string prompt as input, sends the input to the model provider, and then returns the model prediction as output.</p>&#13;
&#13;
<p>Let’s import LangChain’s OpenAI LLM wrapper to<a contenteditable="false" data-primary="invoke method" data-type="indexterm" id="id358"/> <code>invoke</code> a model prediction using a simple prompt:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="model predictions, invoking" data-type="indexterm" id="id359"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai.llms</code> <code class="kn">import</code> <code class="n">OpenAI</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">)</code>&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"The sky is"</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="model predictions, invoking" data-type="indexterm" id="id360"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">OpenAI</code><code class="p">({</code> <code class="nx">model</code><code class="o">:</code> <code class="s2">"gpt-3.5-turbo"</code> <code class="p">});</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="s2">"The sky is"</code><code class="p">);</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
Blue!</pre>&#13;
&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p class="fix_tracking">Notice the<a contenteditable="false" data-primary="model parameter" data-type="indexterm" id="id361"/><a contenteditable="false" data-primary="parameters" data-secondary="model parameter" data-type="indexterm" id="id362"/> parameter <code>model</code> passed to <code>OpenAI</code>. This is the most common parameter to configure when using an LLM or chat model, the underlying model to use, as most providers offer several models with different trade-offs in capability and cost (usually larger models are more capable, but also more expensive and slower). See<a contenteditable="false" data-primary="OpenAI API" data-secondary="overview of models offered" data-type="indexterm" id="id363"/> <a href="https://oreil.ly/dM886">OpenAI’s overview</a> of the models they offer.</p>&#13;
&#13;
<p>Other useful parameters to configure include the following, offered by most<a contenteditable="false" data-primary="parameters" data-secondary="temperature" data-type="indexterm" id="id364"/><a contenteditable="false" data-primary="temperature parameter" data-type="indexterm" id="id365"/> providers.</p>&#13;
&#13;
<dl>&#13;
<dt>&#13;
&#13;
<code>temperature</code> </dt>&#13;
&#13;
<dd>&#13;
<p>This controls the sampling algorithm used to generate output. Lower values produce more predictable outputs (for example, 0.1), while higher values generate more creative, or unexpected, results (such as 0.9). Different tasks will need different values for this parameter. For instance, producing structured output usually benefits from a lower temperature, whereas creative writing tasks do better with a<a contenteditable="false" data-primary="max_tokens parameter" data-type="indexterm" id="id366"/><a contenteditable="false" data-primary="parameters" data-secondary="max_tokens" data-type="indexterm" id="id367"/> higher value:</p></dd>&#13;
&#13;
<dt><code>&#13;
max_tokens</code></dt>&#13;
<dd>&#13;
<p>This limits the size (and cost) of the output. A lower value may cause the LLM to stop generating the output before getting to a natural end, so it may appear to have been truncated.</p></dd>&#13;
</dl>&#13;
<p>Beyond these, each provider exposes a different set of parameters. We recommend looking at the documentation for the one you choose. For an example, refer to <a href="https://oreil.ly/5O1RW">OpenAI’s platform</a>.</p>&#13;
</div>&#13;
&#13;
<p class="fix_tracking">Alternatively, the chat model<a contenteditable="false" data-primary="chat models" data-secondary="roles in" data-type="indexterm" id="id368"/> interface enables back and forth conversations between the user and model. The reason why it’s a separate interface is because popular LLM providers like OpenAI differentiate messages sent to and from the model into <em>user</em>, <em>assistant</em>, and <em>system</em> roles (here<a contenteditable="false" data-primary="roles" data-type="indexterm" id="id369"/> <em>role</em> denotes the type of content the message contains):</p>&#13;
&#13;
<dl>&#13;
	<dt>System role</dt>&#13;
	<dd>&#13;
	<p>Used<a contenteditable="false" data-primary="system role" data-type="indexterm" id="id370"/> for instructions the model should use to answer a user question</p>&#13;
	</dd>&#13;
	<dt>User role</dt>&#13;
	<dd>&#13;
	<p>Used<a contenteditable="false" data-primary="user role" data-type="indexterm" id="id371"/> for the user’s query and any other content produced by the user</p>&#13;
	</dd>&#13;
	<dt>Assistant role</dt>&#13;
	<dd>&#13;
	<p>Used<a contenteditable="false" data-primary="assistant role" data-type="indexterm" id="id372"/> for content generated by the model</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p class="fix_tracking">The chat model’s interface makes it easier to configure and manage conversions in your AI chatbot application. Here’s an example utilizing LangChain’s ChatOpenAI model:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="chatbot conversation configuration" data-type="indexterm" id="id373"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.messages</code> <code class="kn">import</code> <code class="n">HumanMessage</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="p">[</code><code class="n">HumanMessage</code><code class="p">(</code><code class="s2">"What is the capital of France?"</code><code class="p">)]</code>&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="chatbot conversation configuration" data-type="indexterm" id="id374"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">HumanMessage</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/messages'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">()</code>&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="p">[</code><code class="k">new</code> <code class="nx">HumanMessage</code><code class="p">(</code><code class="s1">'What is the capital of France?'</code><code class="p">)]</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">prompt</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
AIMessage(content='The capital of France is Paris.')</pre>&#13;
&#13;
<p>Instead of a single prompt string, chat models make use of different types of<a contenteditable="false" data-primary="chat models" data-secondary="message interfaces in" data-type="indexterm" id="id375"/> chat message interfaces associated with each role mentioned previously. These include the following:</p>&#13;
&#13;
<dl>&#13;
	<dt><code>HumanMessage</code></dt>&#13;
	<dd>&#13;
	<p>A<a contenteditable="false" data-primary="HumanMessage interface" data-type="indexterm" id="id376"/> message sent from the perspective of the human, with the user role</p>&#13;
	</dd>&#13;
	<dt><code>AIMessage</code></dt>&#13;
	<dd>&#13;
	<p>A<a contenteditable="false" data-primary="AIMessage interface" data-type="indexterm" id="id377"/> message sent from the perspective of the AI that the human is interacting with, with the assistant role</p>&#13;
	</dd>&#13;
	<dt><code>SystemMessage</code></dt>&#13;
	<dd>&#13;
	<p>A<a contenteditable="false" data-primary="SystemMessage interface" data-type="indexterm" id="id378"/> message setting the instructions the AI should follow, with the system role</p>&#13;
	</dd>&#13;
	<dt><code>ChatMessage</code></dt>&#13;
	<dd>&#13;
	<p>A<a contenteditable="false" data-primary="ChatMessage interface" data-type="indexterm" id="id379"/> message allowing for arbitrary setting of role</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Let’s incorporate a <code>SystemMessage</code> instruction in our example:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="SystemMessage instruction" data-type="indexterm" id="id380"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_core.messages</code> <code class="kn">import</code> <code class="n">HumanMessage</code><code class="p">,</code> <code class="n">SystemMessage</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>&#13;
<code class="n">system_msg</code> <code class="o">=</code> <code class="n">SystemMessage</code><code class="p">(</code>&#13;
    <code class="sd">'''You are a helpful assistant that responds to questions with three </code>&#13;
<code class="sd">        exclamation marks.'''</code>&#13;
<code class="p">)</code>&#13;
<code class="n">human_msg</code> <code class="o">=</code> <code class="n">HumanMessage</code><code class="p">(</code><code class="s1">'What is the capital of France?'</code><code class="p">)</code>&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">([</code><code class="n">system_msg</code><code class="p">,</code> <code class="n">human_msg</code><code class="p">])</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="SystemMessage instruction" data-type="indexterm" id="id381"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">HumanMessage</code><code class="p">,</code> <code class="nx">SystemMessage</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/core/messages"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">();</code>&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="p">[</code>&#13;
  <code class="k">new</code> <code class="nx">SystemMessage</code><code class="p">(</code>&#13;
    <code class="sb">`You are a helpful assistant that responds to questions with three </code>&#13;
<code class="sb">      exclamation marks.`</code><code class="p">,</code>&#13;
  <code class="p">),</code>&#13;
  <code class="k">new</code> <code class="nx">HumanMessage</code><code class="p">(</code><code class="s2">"What is the capital of France?"</code><code class="p">),</code>&#13;
<code class="p">];</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">prompt</code><code class="p">);</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
AIMessage('Paris!!!')</pre>&#13;
&#13;
<p class="fix_tracking">As you can see, the model obeyed the instruction provided in the <code>SystemMessage</code> even though it wasn’t present in the user’s question. This enables you to preconfigure your AI application to respond in a relatively predictable manner based on the user’s input.<a contenteditable="false" data-primary="" data-startref="LCllms01" data-type="indexterm" id="id382"/><a contenteditable="false" data-primary="" data-startref="LLMland01" data-type="indexterm" id="id383"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Making LLM Prompts Reusable" data-type="sect1"><div class="sect1" id="ch01_making_llm_prompts_reusable_1736545659776740">&#13;
<h1>Making LLM Prompts Reusable</h1>&#13;
&#13;
<p>The<a contenteditable="false" data-primary="LangChain" data-secondary="making LLM prompts reusable" data-type="indexterm" id="LCreusable01"/><a contenteditable="false" data-primary="prompts" data-secondary="reusing" data-type="indexterm" id="Preuse01"/><a contenteditable="false" data-primary="prompts" data-secondary="purpose of" data-type="indexterm" id="id384"/> previous section showed how the <code>prompt</code> instruction significantly influences the model’s output. Prompts help the model understand context and generate relevant answers to queries.</p>&#13;
&#13;
<p>Here<a contenteditable="false" data-primary="prompts" data-secondary="example of detailed" data-type="indexterm" id="id385"/> is an example of a detailed prompt:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
Answer the question based on the context below. If the question cannot be&#13;
answered using the information provided, answer with "I don't know".&#13;
&#13;
Context: The most recent advancements in NLP are being driven by Large Language &#13;
Models (LLMs). These models outperform their smaller counterparts and have&#13;
become invaluable for developers who are creating applications with NLP &#13;
capabilities. Developers can tap into these models through Hugging Face's&#13;
`transformers` library, or by utilizing OpenAI and Cohere's offerings through&#13;
the `openai` and `cohere` libraries, respectively.&#13;
&#13;
Question: Which model providers offer LLMs?&#13;
&#13;
Answer:</pre>&#13;
&#13;
<p>Although the prompt looks like a simple string, the challenge is figuring out what the text should contain and how it should vary based on the user’s input. In this example, the Context and Question values are hardcoded, but what if we wanted to pass these in dynamically?</p>&#13;
&#13;
<p>Fortunately, LangChain provides prompt template interfaces that make it easy to construct prompts with<a contenteditable="false" data-primary="dynamic inputs" data-type="indexterm" id="id386"/><a contenteditable="false" data-primary="input" data-secondary="dynamic inputs" data-type="indexterm" id="id387"/> dynamic inputs:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="dynamic inputs" data-tertiary="constructing" data-type="indexterm" id="id388"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">PromptTemplate</code>&#13;
&#13;
<code class="n">template</code> <code class="o">=</code> <code class="n">PromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Answer the question based on the</code>&#13;
<code class="s2">    context below. If the question cannot be answered using the information </code>&#13;
<code class="s2">    provided, answer with "I don't know".</code>&#13;
&#13;
<code class="s2">Context: </code><code class="si">{context}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Question: </code><code class="si">{question}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Answer: """</code><code class="p">)</code>&#13;
&#13;
<code class="n">template</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code>&#13;
    <code class="s2">"context"</code><code class="p">:</code> <code class="s2">"""The most recent advancements in NLP are being driven by Large </code>&#13;
<code class="s2">        Language Models (LLMs). These models outperform their smaller </code>&#13;
<code class="s2">        counterparts and have become invaluable for developers who are creating </code>&#13;
<code class="s2">        applications with NLP capabilities. Developers can tap into these </code>&#13;
<code class="s2">        models through Hugging Face's `transformers` library, or by utilizing </code>&#13;
<code class="s2">        OpenAI and Cohere's offerings through the `openai` and `cohere` </code>&#13;
<code class="s2">        libraries, respectively."""</code><code class="p">,</code>&#13;
    <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p class="pagebreak-before less_space"><em>JavaS</em><em>cript<a contenteditable="false" data-primary="JavaScript" data-secondary="dynamic inputs" data-tertiary="constructing" data-type="indexterm" id="id389"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PromptTemplate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">template</code> <code class="o">=</code> <code class="nx">PromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Answer the question based on the </code>&#13;
<code class="sb">  context below. If the question cannot be answered using the information </code>&#13;
<code class="sb">  provided, answer with "I don't know".</code>&#13;
&#13;
<code class="sb">Context: {context}</code>&#13;
&#13;
<code class="sb">Question: {question}</code>&#13;
&#13;
<code class="sb">Answer: `</code><code class="p">)</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">template</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code>&#13;
  <code class="nx">context</code><code class="o">:</code> <code class="sb">`The most recent advancements in NLP are being driven by Large </code>&#13;
<code class="sb">    Language Models (LLMs). These models outperform their smaller </code>&#13;
<code class="sb">    counterparts and have become invaluable for developers who are creating </code>&#13;
<code class="sb">    applications with NLP capabilities. Developers can tap into these models </code>&#13;
<code class="sb">    through Hugging Face's \`transformers\` library, or by utilizing OpenAI </code>&#13;
<code class="sb">    and Cohere's offerings through the \`openai\` and \`cohere\` libraries, </code>&#13;
<code class="sb">    respectively.`</code><code class="p">,</code>&#13;
  <code class="nx">question</code><code class="o">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
StringPromptValue(text='Answer the question based on the context below. If the &#13;
    question cannot be answered using the information provided, answer with "I&#13;
    don\'t know".\n\nContext: The most recent advancements in NLP are being &#13;
    driven by Large Language Models (LLMs). These models outperform their &#13;
    smaller counterparts and have become invaluable for developers who are &#13;
    creating applications with NLP capabilities. Developers can tap into these &#13;
    models through Hugging Face\'s `transformers` library, or by utilizing &#13;
    OpenAI and Cohere\'s offerings through the `openai` and `cohere` libraries, &#13;
    respectively.\n\nQuestion: Which model providers offer LLMs?\n\nAnswer: ')</pre>&#13;
&#13;
<p>This example takes the static prompt from the previous block and makes it dynamic. The <code>template</code> contains the structure of the final prompt alongside the definition of where the dynamic inputs will be inserted.</p>&#13;
&#13;
<p>As such, the template can be used as a recipe to build multiple static, specific prompts. When you format the prompt with some specific values—in this case, <code>context</code> and <code>question</code>—you get a static prompt ready to be passed in to an LLM.</p>&#13;
&#13;
<p>As you can see, the <code>question</code> argument is passed dynamically via the<a contenteditable="false" data-primary="invoke method" data-type="indexterm" id="id390"/> <code>invoke</code> function. By default, LangChain prompts follow Python’s<a contenteditable="false" data-primary="Python" data-secondary="f-string syntax" data-type="indexterm" id="id391"/> <code>f-string</code> syntax for defining dynamic parameters—any word surrounded by<a contenteditable="false" data-primary="curly braces ({})" data-type="indexterm" id="id392"/><a contenteditable="false" data-primary="{} (curly braces)" data-type="indexterm" id="id393"/><a contenteditable="false" data-primary="braces, curly ({})" data-type="indexterm" id="id394"/> curly braces, such as <code>{question}</code>, are placeholders for values passed in at runtime. In the previous example, <code>{question}</code> was replaced by <code>“Which model providers offer LLMs?”</code></p>&#13;
&#13;
<p class="pagebreak-before less_space">Let’s see how we’d feed this into an LLM OpenAI model using LangChain:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="dynamic inputs" data-tertiary="feeding into LLMs" data-type="indexterm" id="id395"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai.llms</code> <code class="kn">import</code> <code class="n">OpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">PromptTemplate</code>&#13;
&#13;
<code class="c1"># both `template` and `model` can be reused many times</code>&#13;
&#13;
<code class="n">template</code> <code class="o">=</code> <code class="n">PromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Answer the question based on the </code>&#13;
<code class="s2">    context below. If the question cannot be answered using the information </code>&#13;
<code class="s2">    provided, answer with "I don't know".</code>&#13;
&#13;
<code class="s2">Context: </code><code class="si">{context}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Question: </code><code class="si">{question}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Answer: """</code><code class="p">)</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># `prompt` and `completion` are the results of using template and model once</code>&#13;
&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="n">template</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code>&#13;
    <code class="s2">"context"</code><code class="p">:</code> <code class="s2">"""The most recent advancements in NLP are being driven by Large</code>&#13;
<code class="s2">        Language Models (LLMs). These models outperform their smaller </code>&#13;
<code class="s2">        counterparts and have become invaluable for developers who are creating </code>&#13;
<code class="s2">        applications with NLP capabilities. Developers can tap into these </code>&#13;
<code class="s2">        models through Hugging Face's `transformers` library, or by utilizing </code>&#13;
<code class="s2">        OpenAI and Cohere's offerings through the `openai` and `cohere` </code>&#13;
<code class="s2">        libraries, respectively."""</code><code class="p">,</code>&#13;
    <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="n">completion</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="dynamic inputs" data-tertiary="feeding into LLMs" data-type="indexterm" id="id396"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PromptTemplate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">OpenAI</code><code class="p">()</code>&#13;
<code class="kr">const</code> <code class="nx">template</code> <code class="o">=</code> <code class="nx">PromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Answer the question based on the  </code>&#13;
<code class="sb">  context below. If the question cannot be answered using the information </code>&#13;
<code class="sb">  provided, answer with "I don't know".</code>&#13;
&#13;
<code class="sb">Context: {context}</code>&#13;
&#13;
<code class="sb">Question: {question}</code>&#13;
&#13;
<code class="sb">Answer: `</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">template</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code>&#13;
  <code class="nx">context</code><code class="o">:</code> <code class="sb">`The most recent advancements in NLP are being driven by Large </code>&#13;
<code class="sb">    Language Models (LLMs). These models outperform their smaller </code>&#13;
<code class="sb">    counterparts and have become invaluable for developers who are creating </code>&#13;
<code class="sb">    applications with NLP capabilities. Developers can tap into these models </code>&#13;
<code class="sb">    through Hugging Face's \`transformers\` library, or by utilizing OpenAI </code>&#13;
<code class="sb">    and Cohere's offerings through the \`openai\` and \`cohere\` libraries, </code>&#13;
<code class="sb">    respectively.`</code><code class="p">,</code>&#13;
  <code class="nx">question</code><code class="o">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">prompt</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
Hugging Face's `transformers` library, OpenAI using the `openai` library, and &#13;
Cohere using the `cohere` library offer LLMs.</pre>&#13;
&#13;
<p>If you’re looking to build an AI chat application, the<a contenteditable="false" data-primary="ChatPromptTemplate" data-type="indexterm" id="chatprompttemp01"/> <code>ChatPromptTemplate</code> can be used instead to provide dynamic inputs based on the role of the chat message:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="ChatPromptTemplate" data-type="indexterm" id="Pchatprompt01"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
<code class="n">template</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">([</code>&#13;
    <code class="p">(</code><code class="s1">'system'</code><code class="p">,</code> <code class="s1">'''Answer the question based on the context below. If the </code>&#13;
<code class="s1">        question cannot be answered using the information provided, answer with </code>&#13;
<code class="s1">        "I don</code><code class="se">\'</code><code class="s1">t know".'''</code><code class="p">),</code>&#13;
    <code class="p">(</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'Context: </code><code class="si">{context}</code><code class="s1">'</code><code class="p">),</code>&#13;
    <code class="p">(</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'Question: </code><code class="si">{question}</code><code class="s1">'</code><code class="p">),</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="n">template</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code>&#13;
    <code class="s2">"context"</code><code class="p">:</code> <code class="s2">"""The most recent advancements in NLP are being driven by Large </code>&#13;
<code class="s2">        Language Models (LLMs). These models outperform their smaller </code>&#13;
<code class="s2">        counterparts and have become invaluable for developers who are creating </code>&#13;
<code class="s2">        applications with NLP capabilities. Developers can tap into these </code>&#13;
<code class="s2">        models through Hugging Face's `transformers` library, or by utilizing </code>&#13;
<code class="s2">        OpenAI and Cohere's offerings through the `openai` and `cohere` </code>&#13;
<code class="s2">        libraries, respectively."""</code><code class="p">,</code>&#13;
    <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="ChatPromptTemplate" data-type="indexterm" id="JSchatprompt01"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatPromptTemplate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">template</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromMessages</code><code class="p">([</code>&#13;
  <code class="p">[</code><code class="s1">'system'</code><code class="p">,</code> <code class="sb">`Answer the question based on the context below. If the question </code>&#13;
<code class="sb">    cannot be answered using the information provided, answer with "I </code>&#13;
<code class="sb">    don</code><code class="err">\</code><code class="sb">'t know".`</code><code class="p">],</code>&#13;
  <code class="p">[</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'Context: {context}'</code><code class="p">],</code>&#13;
  <code class="p">[</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'Question: {question}'</code><code class="p">],</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">template</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code>&#13;
  <code class="nx">context</code><code class="o">:</code> <code class="sb">`The most recent advancements in NLP are being driven by Large </code>&#13;
<code class="sb">    Language Models (LLMs). These models outperform their smaller </code>&#13;
<code class="sb">    counterparts and have become invaluable for developers who are creating </code>&#13;
<code class="sb">    applications with NLP capabilities. Developers can tap into these models </code>&#13;
<code class="sb">    through Hugging Face's \`transformers\` library, or by utilizing OpenAI </code>&#13;
<code class="sb">    and Cohere's offerings through the \`openai\` and \`cohere\` libraries, </code>&#13;
<code class="sb">    respectively.`</code><code class="p">,</code>&#13;
  <code class="nx">question</code><code class="o">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
ChatPromptValue(messages=[SystemMessage(content='Answer the question based on &#13;
    the context below. If the question cannot be answered using the information &#13;
    provided, answer with "I don\'t know".'), HumanMessage(content="Context: &#13;
    The most recent advancements in NLP are being driven by Large Language &#13;
    Models (LLMs). These models outperform their smaller counterparts and have &#13;
    become invaluable for developers who are creating applications with NLP &#13;
    capabilities. Developers can tap into these models through Hugging Face\'s &#13;
    `transformers` library, or by utilizing OpenAI and Cohere\'s offerings &#13;
    through the `openai` and `cohere` libraries, respectively."), HumanMessage&#13;
    (content='Question: Which model providers offer LLMs?')])</pre>&#13;
&#13;
<p>Notice how the prompt contains instructions in a <code>SystemMessage</code> and two instances of <code>HumanMessage</code> that contain dynamic <code>context</code> and <code>question</code> variables. You can still format the template in the same way and get back a static prompt that you can pass to a large language model for a prediction output:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
&#13;
<code class="c1"># both `template` and `model` can be reused many times</code>&#13;
&#13;
<code class="n">template</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">([</code>&#13;
    <code class="p">(</code><code class="s1">'system'</code><code class="p">,</code> <code class="s1">'''Answer the question based on the context below. If the 		</code>&#13;
<code class="s1">        question cannot be answered using the information provided, answer</code>&#13;
<code class="s1">        with "I don</code><code class="se">\'</code><code class="s1">t know".'''</code><code class="p">),</code>&#13;
    <code class="p">(</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'Context: </code><code class="si">{context}</code><code class="s1">'</code><code class="p">),</code>&#13;
    <code class="p">(</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'Question: </code><code class="si">{question}</code><code class="s1">'</code><code class="p">),</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># `prompt` and `completion` are the results of using template and model once</code>&#13;
&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="n">template</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code>&#13;
    <code class="s2">"context"</code><code class="p">:</code> <code class="s2">"""The most recent advancements in NLP are being driven by </code>&#13;
<code class="s2">        Large Language Models (LLMs). These models outperform their smaller </code>&#13;
<code class="s2">        counterparts and have become invaluable for developers who are creating </code>&#13;
<code class="s2">        applications with NLP capabilities. Developers can tap into these </code>&#13;
<code class="s2">        models through Hugging Face's `transformers` library, or by utilizing </code>&#13;
<code class="s2">        OpenAI and Cohere's offerings through the `openai` and `cohere` </code>&#13;
<code class="s2">        libraries, respectively."""</code><code class="p">,</code>&#13;
    <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatPromptTemplate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">()</code>&#13;
<code class="kr">const</code> <code class="nx">template</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromMessages</code><code class="p">([</code>&#13;
  <code class="p">[</code><code class="s1">'system'</code><code class="p">,</code> <code class="sb">`Answer the question based on the context below. If the question </code>&#13;
<code class="sb">    cannot be answered using the information provided, answer with "I </code>&#13;
<code class="sb">    don</code><code class="err">\</code><code class="sb">'t know".`</code><code class="p">],</code>&#13;
  <code class="p">[</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'Context: {context}'</code><code class="p">],</code>&#13;
  <code class="p">[</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'Question: {question}'</code><code class="p">],</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">template</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code>&#13;
  <code class="nx">context</code><code class="o">:</code> <code class="sb">`The most recent advancements in NLP are being driven by Large </code>&#13;
<code class="sb">    Language Models (LLMs). These models outperform their smaller </code>&#13;
<code class="sb">    counterparts and have become invaluable for developers who are creating </code>&#13;
<code class="sb">    applications with NLP capabilities. Developers can tap into these models </code>&#13;
<code class="sb">    through Hugging Face's \`transformers\` library, or by utilizing OpenAI </code>&#13;
<code class="sb">    and Cohere's offerings through the \`openai\` and \`cohere\` libraries, </code>&#13;
<code class="sb">    respectively.`</code><code class="p">,</code>&#13;
  <code class="nx">question</code><code class="o">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">prompt</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
AIMessage(content="Hugging Face's `transformers` library, OpenAI using the &#13;
    `openai` library, and Cohere using the `cohere` library offer LLMs.")</pre>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Getting Specific Formats out of LLMs" data-type="sect1"><div class="sect1" id="ch01_getting_specific_formats_out_of_llms_1736545659776807">&#13;
<h1>Getting Specific Formats out of LLMs</h1>&#13;
&#13;
<p>Plain<a contenteditable="false" data-primary="" data-startref="Preuse01" data-type="indexterm" id="id397"/><a contenteditable="false" data-primary="" data-startref="LCreusable01" data-type="indexterm" id="id398"/><a contenteditable="false" data-primary="" data-startref="JSchatprompt01" data-type="indexterm" id="id399"/><a contenteditable="false" data-primary="" data-startref="Pchatprompt01" data-type="indexterm" id="id400"/><a contenteditable="false" data-primary="" data-startref="chatprompttemp01" data-type="indexterm" id="id401"/> text outputs<a contenteditable="false" data-primary="LangChain" data-secondary="getting specific formats out of LLMs" data-type="indexterm" id="LCformat01"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="getting specific formats out of LLMs" data-type="indexterm" id="LLMformat01"/><a contenteditable="false" data-primary="formats" data-secondary="specifying from LLMs" data-type="indexterm" id="Fspecific01"/> are useful, but there may be use cases where you need the LLM to generate a <em>structured</em> output—that is, output in a machine-readable format, such as JSON, XML, CSV, or even in a programming language such as Python or JavaScript. This is very useful when you intend to hand that output off to some other piece of code, making an LLM play a part in your larger application.</p>&#13;
&#13;
<section data-pdf-bookmark="JSON Output" data-type="sect2"><div class="sect2" id="ch01_json_output_1736545659776868">&#13;
<h2>JSON Output</h2>&#13;
&#13;
<p>The<a contenteditable="false" data-primary="JSON output" data-type="indexterm" id="id402"/><a contenteditable="false" data-primary="output" data-secondary="JSON output" data-type="indexterm" id="id403"/> most common format to generate with LLMs is JSON. JSON outputs can (for example) be sent over the wire to your frontend code or be saved to a database.</p>&#13;
&#13;
<p>When generating JSON, the first task is to define the schema you want the LLM to respect when producing the output. Then, you should include that schema in the prompt, along with the text you want to use as the source. Let’s see an example:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="output" data-tertiary="JSON" data-type="indexterm" id="id404"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.pydantic_v1</code> <code class="kn">import</code> <code class="n">BaseModel</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">AnswerWithJustification</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>&#13;
    <code class="sd">'''An answer to the user's question along with justification for the </code>&#13;
<code class="sd">        answer.'''</code>&#13;
    <code class="n">answer</code><code class="p">:</code> <code class="nb">str</code>&#13;
    <code class="sd">'''The answer to the user's question'''</code>&#13;
    <code class="n">justification</code><code class="p">:</code> <code class="nb">str</code>&#13;
    <code class="sd">'''Justification for the answer'''</code>&#13;
&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">structured_llm</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">with_structured_output</code><code class="p">(</code><code class="n">AnswerWithJustification</code><code class="p">)</code>&#13;
&#13;
<code class="n">structured_llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"""What weighs more, a pound of bricks or a pound </code>&#13;
<code class="s2">    of feathers"""</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">z</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"zod"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">answerSchema</code> <code class="o">=</code> <code class="nx">z</code>&#13;
  <code class="p">.</code><code class="nx">object</code><code class="p">({</code>&#13;
    <code class="nx">answer</code><code class="o">:</code> <code class="nx">z</code><code class="p">.</code><code class="nx">string</code><code class="p">().</code><code class="nx">describe</code><code class="p">(</code><code class="s2">"The answer to the user's question"</code><code class="p">),</code>&#13;
    <code class="nx">justification</code><code class="o">:</code> <code class="nx">z</code><code class="p">.</code><code class="nx">string</code><code class="p">().</code><code class="nx">describe</code><code class="p">(</code><code class="sb">`Justification for the </code>&#13;
<code class="sb">      answer`</code><code class="p">),</code>&#13;
  <code class="p">})</code>&#13;
  <code class="p">.</code><code class="nx">describe</code><code class="p">(</code><code class="sb">`An answer to the user's question along with justification for </code>&#13;
<code class="sb">    the answer.`</code><code class="p">);</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code>&#13;
  <code class="nx">model</code><code class="o">:</code> <code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code>&#13;
  <code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code><code class="p">,</code>&#13;
<code class="p">}).</code><code class="nx">withStructuredOutput</code><code class="p">(</code><code class="nx">answerSchema</code><code class="p">)</code>&#13;
<code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="s2">"What weighs more, a pound of bricks or a pound of feathers"</code><code class="p">)</code></pre>&#13;
&#13;
<p class="pagebreak-before less_space"><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
{&#13;
  answer: "They weigh the same",&#13;
  justification: "Both a pound of bricks and a pound of feathers weigh one pound. &#13;
    The weight is the same, but the volu"... 42 more characters&#13;
}</pre>&#13;
&#13;
<p>So, first define a schema. In Python, this is easiest to do with<a contenteditable="false" data-primary="Pydantic" data-type="indexterm" id="id405"/> Pydantic (a library used for validating data against schemas). In JS, this is easiest to do with<a contenteditable="false" data-primary="Zod" data-type="indexterm" id="id406"/> Zod (an equivalent library). The method <code>with_structured_output</code> will use that schema for two things:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The schema will be converted to a <code>JSONSchema</code> object (a JSON format used to describe the shape [types, names, descriptions] of JSON data), which will be sent to the LLM. For each LLM, LangChain picks the best method to do this, usually function calling or prompting.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The schema will also be used to validate the output returned by the LLM before returning it; this ensures the output produced respects the schema you passed in exactly.</p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Other Machine-Readable Formats with Output Parsers" data-type="sect2"><div class="sect2" id="ch01_other_machine_readable_formats_with_output_parsers_1736545659776930">&#13;
<h2>Other Machine-Readable Formats with Output Parsers</h2>&#13;
&#13;
<p>You<a contenteditable="false" data-primary="CSV format" data-type="indexterm" id="id407"/><a contenteditable="false" data-primary="formats" data-secondary="machine-readable" data-type="indexterm" id="id408"/> can also use an LLM or chat model to produce output in other formats, such as<a contenteditable="false" data-primary="XML format" data-type="indexterm" id="id409"/> CSV or XML. This is where<a contenteditable="false" data-primary="output parsers" data-type="indexterm" id="id410"/> output parsers come in handy. <em>Output parsers</em> are classes that help you structure large language model responses. They serve two functions:</p>&#13;
&#13;
<dl>&#13;
	<dt>Providing format instructions</dt>&#13;
	<dd>&#13;
	<p>Output<a contenteditable="false" data-primary="formats" data-secondary="providing format instructions" data-type="indexterm" id="id411"/> parsers can be used to inject some additional instructions in the prompt that will help guide the LLM to output text in the format it knows how to parse.</p>&#13;
	</dd>&#13;
	<dt>Validating and parsing output</dt>&#13;
	<dd>&#13;
	<p>The main function is to take the textual output of the LLM or chat model and render it to a more structured format, such as a list, XML, or other format. This can include removing extraneous information, correcting incomplete output, and validating the parsed values.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Here’s an example of how an output parser works:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="output parsers in" data-type="indexterm" id="id412"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_core.output_parsers</code> <code class="kn">import</code> <code class="n">CommaSeparatedListOutputParser</code>&#13;
<code class="n">parser</code> <code class="o">=</code> <code class="n">CommaSeparatedListOutputParser</code><code class="p">()</code>&#13;
<code class="n">items</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"apple, banana, cherry"</code><code class="p">)</code></pre>&#13;
&#13;
<p class="pagebreak-before less_space"><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="output parsers in" data-type="indexterm" id="id413"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">CommaSeparatedListOutputParser</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/output_parsers'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">parser</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">CommaSeparatedListOutputParser</code><code class="p">()</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">parser</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="s2">"apple, banana, cherry"</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
['apple', 'banana', 'cherry']</pre>&#13;
&#13;
<p>LangChain provides a variety of output parsers for various use cases, including CSV, XML, and more. We’ll see how to combine output parsers with models and prompts in the next section.<a contenteditable="false" data-primary="" data-startref="Fspecific01" data-type="indexterm" id="id414"/><a contenteditable="false" data-primary="" data-startref="LLMformat01" data-type="indexterm" id="id415"/><a contenteditable="false" data-primary="" data-startref="LCformat01" data-type="indexterm" id="id416"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Assembling the Many Pieces of an LLM Application" data-type="sect1"><div class="sect1" id="ch01_assembling_the_many_pieces_of_an_llm_application_1736545659777015">&#13;
<h1>Assembling the Many Pieces of an LLM Application</h1>&#13;
&#13;
<p>The<a contenteditable="false" data-primary="LLM applications" data-secondary="assembling" data-type="indexterm" id="LLMAassem01"/><a contenteditable="false" data-primary="LangChain" data-secondary="assembling LLM applications" data-type="indexterm" id="LCassemgling01"/> key components you’ve learned about so far are essential building blocks of the LangChain framework. Which brings us to the critical question: How do you combine them effectively to build your LLM application?</p>&#13;
&#13;
<section data-pdf-bookmark="Using the Runnable Interface" data-type="sect2"><div class="sect2" id="ch01_using_the_runnable_interface_1736545659777090">&#13;
<h2>Using the Runnable Interface</h2>&#13;
&#13;
<p>As<a contenteditable="false" data-primary="runnable interface" data-type="indexterm" id="id417"/> you may have noticed, all the code examples used so far utilize a similar interface and the<a contenteditable="false" data-primary="invoke method" data-type="indexterm" id="id418"/> <code>invoke()</code> method to generate outputs from the model (or prompt template, or output parser). All components have the following:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>There is a common interface with these<a contenteditable="false" data-primary="batch method" data-type="indexterm" id="id419"/><a contenteditable="false" data-primary="stream method" data-type="indexterm" id="id420"/> methods:</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p><code>invoke</code>: transforms a single input into an output</p>&#13;
		</li>&#13;
		<li>&#13;
		<p><code>batch</code>: efficiently transforms multiple inputs into multiple outputs</p>&#13;
		</li>&#13;
		<li>&#13;
		<p><code>stream</code>: streams output from a single input as it’s produced</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>There are built-in utilities for retries, fallbacks, schemas, and runtime <span class="keep-together">configurability.</span></p>&#13;
	</li>&#13;
	<li>&#13;
	<p>In Python, each of the three methods have <code>asyncio</code> equivalents.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>As such, all components behave the same way, and the interface learned for one of them applies to all:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="runnable interface" data-type="indexterm" id="id421"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai.llms</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>&#13;
&#13;
<code class="n">completion</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s1">'Hi there!'</code><code class="p">)</code> &#13;
<code class="c1"># Hi!</code>&#13;
&#13;
<code class="n">completions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">batch</code><code class="p">([</code><code class="s1">'Hi there!'</code><code class="p">,</code> <code class="s1">'Bye!'</code><code class="p">])</code>&#13;
<code class="c1"># ['Hi!', 'See you!']</code>&#13;
&#13;
<code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">stream</code><code class="p">(</code><code class="s1">'Bye!'</code><code class="p">):</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="n">token</code><code class="p">)</code>&#13;
    <code class="c1"># Good</code>&#13;
    <code class="c1"># bye</code>&#13;
    <code class="c1"># !</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="runnable interface" data-type="indexterm" id="id422"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">()</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">completion</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="s1">'Hi there!'</code><code class="p">)</code> &#13;
<code class="c1">// Hi!</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">completions</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">batch</code><code class="p">([</code><code class="s1">'Hi there!'</code><code class="p">,</code> <code class="s1">'Bye!'</code><code class="p">])</code>&#13;
<code class="c1">// ['Hi!', 'See you!']</code>&#13;
&#13;
<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">token</code> <code class="k">of</code> <code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">stream</code><code class="p">(</code><code class="s1">'Bye!'</code><code class="p">))</code> <code class="p">{</code>&#13;
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">token</code><code class="p">)</code>&#13;
  <code class="c1">// Good</code>&#13;
  <code class="c1">// bye</code>&#13;
  <code class="c1">// !</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>In this example, you see how the three main methods work:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p><code>invoke()</code> takes a single input and returns a single output.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><code>batch()</code> takes a list of outputs and returns a list of outputs.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><code>stream()</code> takes a single input and returns an iterator of parts of the output as they become available.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In some cases, where the underlying component doesn’t support iterative output, there will be a single part containing all output.</p>&#13;
&#13;
<p>You can combine these components in two ways:</p>&#13;
&#13;
<dl>&#13;
	<dt>Imperative</dt>&#13;
	<dd>&#13;
	<p>Call<a contenteditable="false" data-primary="imperative composition" data-type="indexterm" id="impcomp01"/> your components directly, for example, with <code>model.invoke(...)</code></p>&#13;
	</dd>&#13;
	<dt>Declarative</dt>&#13;
	<dd>&#13;
	<p>Use<a contenteditable="false" data-primary="declarative composition" data-type="indexterm" id="id423"/> LangChain Expression Language (LCEL), as covered in an upcoming section</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#ch01_table_1_1736545659767905">Table 1-1</a> summarizes their differences, and we’ll see each in action<a contenteditable="false" data-primary="parallel execution" data-type="indexterm" id="id424"/><a contenteditable="false" data-primary="async execution" data-type="indexterm" id="id425"/><a contenteditable="false" data-primary="LangChain Expression Language (LCEL)" data-secondary="imperative versus declarative composition" data-type="indexterm" id="id426"/> next.</p>&#13;
&#13;
<table id="ch01_table_1_1736545659767905">&#13;
	<caption><span class="label">Table 1-1. </span>The main differences between imperative and declarative composition.</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>Imperative</th>&#13;
			<th>Declarative</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>Syntax</td>&#13;
			<td>All of Python or JavaScript</td>&#13;
			<td>LCEL</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Parallel execution</td>&#13;
			<td>&#13;
			<p>Python: with threads or coroutines</p>&#13;
&#13;
			<p>JavaScript: with <code>Promise.all</code></p>&#13;
			</td>&#13;
			<td>Automatic</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Streaming</td>&#13;
			<td>With yield keyword</td>&#13;
			<td>Automatic</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Async execution</td>&#13;
			<td>With async functions</td>&#13;
			<td>Automatic</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Imperative Composition" data-type="sect2"><div class="sect2" id="ch01_imperative_composition_1736545659777153">&#13;
<h2>Imperative Composition</h2>&#13;
&#13;
<p><em>Imperative composition</em> is just a fancy name for writing the code you’re used to writing, composing these components into functions and classes. Here’s an example combining<a contenteditable="false" data-primary="chatbots" data-secondary="prompt and chat model example of" data-type="indexterm" id="id427"/> prompts, models, and output parsers:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="composition" data-tertiary="imperative" data-type="indexterm" id="id428"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">chain</code>&#13;
&#13;
<code class="c1"># the building blocks</code>&#13;
&#13;
<code class="n">template</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">([</code>&#13;
    <code class="p">(</code><code class="s1">'system'</code><code class="p">,</code> <code class="s1">'You are a helpful assistant.'</code><code class="p">),</code>&#13;
    <code class="p">(</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'</code><code class="si">{question}</code><code class="s1">'</code><code class="p">),</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># combine them in a function</code>&#13;
<code class="c1"># @chain decorator adds the same Runnable interface for any function you write</code>&#13;
&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">chatbot</code><code class="p">(</code><code class="n">values</code><code class="p">):</code>&#13;
    <code class="n">prompt</code> <code class="o">=</code> <code class="n">template</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">values</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use it</code>&#13;
&#13;
<code class="n">chatbot</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code><code class="p">})</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="composition" data-tertiary="imperative" data-type="indexterm" id="id429"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatOpenAI</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatPromptTemplate</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">RunnableLambda</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/runnables'</code>&#13;
&#13;
<code class="c1">// the building blocks</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">template</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromMessages</code><code class="p">([</code>&#13;
  <code class="p">[</code><code class="s1">'system'</code><code class="p">,</code> <code class="s1">'You are a helpful assistant.'</code><code class="p">],</code>&#13;
  <code class="p">[</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'{question}'</code><code class="p">],</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">()</code>&#13;
&#13;
<code class="c1">// combine them in a function</code>&#13;
<code class="c1">// RunnableLambda adds the same Runnable interface for any function you write</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">chatbot</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="nx">values</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">template</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">values</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">prompt</code><code class="p">)</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="c1">// use it</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">chatbot</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code>&#13;
  <code class="s2">"question"</code><code class="o">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
AIMessage(content="Hugging Face's `transformers` library, OpenAI using the &#13;
    `openai` library, and Cohere using the `cohere` library offer LLMs.")</pre>&#13;
&#13;
<p>The preceding is a complete example of a chatbot, using a prompt and chat model. As you can see, it uses familiar Python syntax and supports any custom logic you might want to add in that function.</p>&#13;
&#13;
<p>On the other hand, if you want to enable streaming or async support, you’d have to modify your function to support it. For example, streaming support can be added as follows:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="chatbot streaming support" data-type="indexterm" id="id430"/><a contenteditable="false" data-primary="chatbots" data-secondary="streaming support with Python or JavaScript" data-type="indexterm" id="id431"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">chatbot</code><code class="p">(</code><code class="n">values</code><code class="p">):</code>&#13;
    <code class="n">prompt</code> <code class="o">=</code> <code class="n">template</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">values</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">stream</code><code class="p">(</code><code class="n">prompt</code><code class="p">):</code>&#13;
        <code class="k">yield</code> <code class="n">token</code>&#13;
&#13;
<code class="k">for</code> <code class="n">part</code> <code class="ow">in</code> <code class="n">chatbot</code><code class="o">.</code><code class="n">stream</code><code class="p">({</code>&#13;
    <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">}):</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="n">part</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="chatbot streaming support" data-type="indexterm" id="id432"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">chatbot</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="kd">function</code><code class="o">*</code> <code class="p">(</code><code class="nx">values</code><code class="p">)</code> <code class="p">{</code>&#13;
  <code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">template</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">values</code><code class="p">)</code>&#13;
  <code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">token</code> <code class="k">of</code> <code class="nx">await</code> <code class="nx">model</code><code class="p">.</code><code class="nx">stream</code><code class="p">(</code><code class="nx">prompt</code><code class="p">))</code> <code class="p">{</code>&#13;
    <code class="k">yield</code> <code class="nx">token</code>&#13;
  <code class="p">}</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">token</code> <code class="k">of</code> <code class="nx">await</code> <code class="nx">chatbot</code><code class="p">.</code><code class="nx">stream</code><code class="p">({</code>&#13;
  <code class="s2">"question"</code><code class="o">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">}))</code> <code class="p">{</code>&#13;
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">token</code><code class="p">)</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
AIMessageChunk(content="Hugging")&#13;
AIMessageChunk(content=" Face's")&#13;
AIMessageChunk(content=" `transformers`")&#13;
...</pre>&#13;
&#13;
<p>So, either in JS or Python, you can enable streaming for your custom function by yielding the values you want to stream and then calling it with <code>stream</code>.</p>&#13;
&#13;
<p>For<a contenteditable="false" data-primary="chatbots" data-secondary="asynchronous execution of" data-type="indexterm" id="id433"/> asynchronous execution, you’d rewrite your function like this:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="asynchronous execution with" data-type="indexterm" id="id434"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="nd">@chain</code>&#13;
<code class="k">async</code> <code class="k">def</code> <code class="nf">chatbot</code><code class="p">(</code><code class="n">values</code><code class="p">):</code>&#13;
    <code class="n">prompt</code> <code class="o">=</code> <code class="k">await</code> <code class="n">template</code><code class="o">.</code><code class="n">ainvoke</code><code class="p">(</code><code class="n">values</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="k">await</code> <code class="n">model</code><code class="o">.</code><code class="n">ainvoke</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code>&#13;
&#13;
<code class="k">await</code> <code class="n">chatbot</code><code class="o">.</code><code class="n">ainvoke</code><code class="p">({</code><code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code><code class="p">})</code>&#13;
<code class="c1"># &gt; AIMessage(content="""Hugging Face's `transformers` library, OpenAI using</code>&#13;
    <code class="n">the</code> <code class="err">`</code><code class="n">openai</code><code class="err">`</code> <code class="n">library</code><code class="p">,</code> <code class="ow">and</code> <code class="n">Cohere</code> <code class="n">using</code> <code class="n">the</code> <code class="err">`</code><code class="n">cohere</code><code class="err">`</code> <code class="n">library</code> <code class="n">offer</code> <code class="n">LLMs</code><code class="o">.</code><code class="s2">""")</code></pre>&#13;
&#13;
<p>This<a contenteditable="false" data-primary="JavaScript" data-secondary="asynchronous execution with" data-type="indexterm" id="id435"/> one applies to Python only, as asynchronous execution is the only option in JavaScript.<a contenteditable="false" data-primary="" data-startref="impcomp01" data-type="indexterm" id="id436"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Declarative Composition" data-type="sect2"><div class="sect2" id="ch01_declarative_composition_1736545659777212">&#13;
<h2>Declarative Composition</h2>&#13;
&#13;
<p>LCEL<a contenteditable="false" data-primary="declarative composition" data-type="indexterm" id="deccomp01"/><a contenteditable="false" data-primary="LangChain Expression Language (LCEL)" data-secondary="optimized execution plan using" data-type="indexterm" id="id437"/> is a <em>declarative language</em> for composing LangChain components. LangChain compiles LCEL compositions to an<a contenteditable="false" data-primary="optimized execution plan" data-type="indexterm" id="id438"/> <em>optimized execution plan</em>, with automatic parallelization, streaming, tracing, and async support.</p>&#13;
&#13;
<p>Let’s see the same example using LCEL:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="composition" data-tertiary="declarative" data-type="indexterm" id="id439"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai.chat_models</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
&#13;
<code class="c1"># the building blocks</code>&#13;
&#13;
<code class="n">template</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">([</code>&#13;
    <code class="p">(</code><code class="s1">'system'</code><code class="p">,</code> <code class="s1">'You are a helpful assistant.'</code><code class="p">),</code>&#13;
    <code class="p">(</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'</code><code class="si">{question}</code><code class="s1">'</code><code class="p">),</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># combine them with the | operator</code>&#13;
&#13;
<code class="n">chatbot</code> <code class="o">=</code> <code class="n">template</code> <code class="o">|</code> <code class="n">model</code>&#13;
&#13;
<code class="c1"># use it</code>&#13;
&#13;
<code class="n">chatbot</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code><code class="p">})</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="composition" data-tertiary="declarative" data-type="indexterm" id="id440"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatPromptTemplate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RunnableLambda</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/runnables'</code>&#13;
&#13;
<code class="c1">// the building blocks</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">template</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromMessages</code><code class="p">([</code>&#13;
  <code class="p">[</code><code class="s1">'system'</code><code class="p">,</code> <code class="s1">'You are a helpful assistant.'</code><code class="p">],</code>&#13;
  <code class="p">[</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'{question}'</code><code class="p">],</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">()</code>&#13;
&#13;
<code class="c1">// combine them in a function</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">chatbot</code> <code class="o">=</code> <code class="nx">template</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">model</code><code class="p">)</code>&#13;
&#13;
<code class="c1">// use it</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">chatbot</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code>&#13;
  <code class="s2">"question"</code><code class="o">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
AIMessage(content="Hugging Face's `transformers` library, OpenAI using the &#13;
    `openai` library, and Cohere using the `cohere` library offer LLMs.")</pre>&#13;
&#13;
<p>Crucially, the last line is the same between the two examples—that is, you use the function and the LCEL sequence in the same way, with <code>invoke/stream/batch</code>. And in this version, you don’t need to do anything else to use streaming:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="chatbot streaming support" data-type="indexterm" id="id441"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">chatbot</code> <code class="o">=</code> <code class="n">template</code> <code class="o">|</code> <code class="n">model</code>&#13;
&#13;
<code class="k">for</code> <code class="n">part</code> <code class="ow">in</code> <code class="n">chatbot</code><code class="o">.</code><code class="n">stream</code><code class="p">({</code>&#13;
    <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">}):</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="n">part</code><code class="p">)</code>&#13;
    <code class="c1"># &gt; AIMessageChunk(content="Hugging")</code>&#13;
    <code class="c1"># &gt; AIMessageChunk(content=" Face's")</code>&#13;
    <code class="c1"># &gt; AIMessageChunk(content=" `transformers`")</code>&#13;
    <code class="c1"># ...</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="chatbot streaming support" data-type="indexterm" id="id442"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">chatbot</code> <code class="o">=</code> <code class="nx">template</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">model</code><code class="p">)</code>&#13;
&#13;
<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">token</code> <code class="k">of</code> <code class="nx">await</code> <code class="nx">chatbot</code><code class="p">.</code><code class="nx">stream</code><code class="p">({</code>&#13;
  <code class="s2">"question"</code><code class="o">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">}))</code> <code class="p">{</code>&#13;
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">token</code><code class="p">)</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>And, for Python only, it’s the same for using asynchronous methods:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">chatbot</code> <code class="o">=</code> <code class="n">template</code> <code class="o">|</code> <code class="n">model</code>&#13;
&#13;
<code class="k">await</code> <code class="n">chatbot</code><code class="o">.</code><code class="n">ainvoke</code><code class="p">({</code>&#13;
    <code class="s2">"question"</code><code class="p">:</code> <code class="s2">"Which model providers offer LLMs?"</code>&#13;
<code class="p">})</code></pre>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch01_summary_1736545659777270">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In<a contenteditable="false" data-primary="" data-startref="LLMAassem01" data-type="indexterm" id="id443"/><a contenteditable="false" data-primary="" data-startref="deccomp01" data-type="indexterm" id="id444"/><a contenteditable="false" data-primary="" data-startref="LCassemgling01" data-type="indexterm" id="id445"/> this chapter, you’ve learned about the building blocks and key components necessary to build LLM applications using LangChain. LLM applications are essentially a chain consisting of the large language model to make predictions, the prompt instruction(s) to guide the model toward a desired output, and an optional output parser to transform the format of the model’s output.</p>&#13;
&#13;
<p>All LangChain components share the same interface with<a contenteditable="false" data-primary="invoke method" data-type="indexterm" id="id446"/><a contenteditable="false" data-primary="stream method" data-type="indexterm" id="id447"/><a contenteditable="false" data-primary="batch method" data-type="indexterm" id="id448"/> <code>invoke</code>, <code>stream</code>, and <code>batch</code> methods to handle various inputs and outputs. They can either be combined and executed imperatively by calling them directly or declaratively using LCEL.</p>&#13;
&#13;
<p>The<a contenteditable="false" data-primary="imperative composition" data-type="indexterm" id="id449"/> imperative approach is useful if you intend to write a lot of custom logic, whereas the declarative approach is useful for simply assembling existing components with limited customization.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927">Chapter 2</a>, you’ll learn how to provide external data to your AI chatbot as <em>context</em> so that you can build an LLM application that enables you to “chat” with your data.</p>&#13;
</div></section>&#13;
</div></section></body></html>