<html><head></head><body><section data-pdf-bookmark="Chapter 11. Testing AI Services" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch11">
<h1><span class="label">Chapter 11. </span>Testing AI Services</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1142">
<h1>Chapter Goals</h1>
<p><a data-primary="testing AI services" data-type="indexterm" id="ix_ch11-asciidoc0"/>In this chapter, you will learn about:</p>

<ul>
<li>
<p>How to plan and structure test suites for comprehensive test coverage, including unit, integration, end-to-end, and behavioral tests</p>
</li>
<li>
<p>The concepts of testing boundaries, code coverage, and idempotency in designing tests</p>
</li>
<li>
<p>How to identify and avoid common testing pitfalls to improve test quality</p>
</li>
<li>
<p>How to leverage test fixtures and implement parameterization to run tests with multiple inputs for checking code robustness</p>
</li>
<li>
<p>How to efficiently set up and tear down testing environments using <code>pytest</code></p>
</li>
<li>
<p>How to maintain idempotency in testing processes by correctly handling asynchronous flaky tests</p>
</li>
<li>
<p>How to use mocking and patching to isolate components from external dependencies in unit tests</p>
</li>
<li>
<p>How to test GenAI services that use probabilistic models using behavioral testing and auto-evaluation techniques</p>
</li>
<li>
<p>How to leverage several testing metrics for GenAI services</p>
</li>
</ul>
</div></aside>

<p>In this chapter, you’ll learn the importance of testing and its challenges when building GenAI services.
You’ll also learn about key concepts such as test plans, the verification and validation models, the testing pyramid, and the role of testing data, environments, and boundaries.</p>

<p>To practice testing, you will use <code>pytest</code>, a popular testing framework with features such as test fixtures, scopes, markers, and fixture parameterization.
You’ll also learn about the <code>pytest-mock</code> plug-in for patching functions and using stubs, mocks, and spy objects to simulate and control external dependencies during tests.</p>

<p>Since mocking can make tests brittle, we’ll also explore dependency injection, allowing you to inject mock or stub dependencies directly into the components being tested, avoiding runtime code modifications.</p>

<p>We’ll discuss the role of isolation and idempotency in tests, when to use mocks, and how to test both deterministic and probabilistic GenAI code.
By the end of this chapter, you’ll be confident in writing comprehensive test suites including unit, integration, end-to-end, and behavioral tests for your own services.</p>

<p>Before we dive into writing tests, let’s explore the foundational concepts of traditional software testing and how to approach testing GenAI services, which can prove challenging due to the probabilistic nature of AI models.</p>






<section data-pdf-bookmark="The Importance of Testing" data-type="sect1"><div class="sect1" id="id172">
<h1>The Importance of Testing</h1>

<p><a data-primary="testing AI services" data-secondary="importance of testing" data-type="indexterm" id="ix_ch11-asciidoc1"/>In theory, everyone agrees that testing is necessary when building software.
You write tests to give you confidence in the functionality and performance of your systems, especially when they interact with one another.
But realistically, projects may skip implementing manual or automated tests due to various constraints including budget, time, or associated labor costs related to maintaining tests.</p>
<p class="fix_tracking">
The projects that skip testing, partially or entirely, end up approaching software problems reactively instead of proactively.
This is when <em>technical debt</em> builds up, which you’ll then have to pay back in labor and server costs, with interest, to settle up.</p>

<p>The problem of when to test is challenging to solve.
If you’re just experimenting and hacking a prototype together in fast iterations, you won’t need to worry about testing as much, realistically.
However, as soon as you have a minimum sellable product, a system that interfaces with sensitive data and processes user payments, then you must seriously consider testing plans.</p>

<p>Earlier in my career, I was building a learning management system for a client.
I wrote a webhook endpoint to interface with Stripe’s payment systems and my own home-brewed authentication solution that would only register users on successful first payment.
The system had to charge and process subscription payments of both new and existing customers and send confirmation emails while tracking user records, subscriptions, payments, checkout sessions, and invoices.
The logic of that webhook ended up so convoluted and complex that it led to a monstrosity that became a 1,000-line function.
The function was checking unordered received events of various types, with multiple round-trips to the database.</p>

<p>The whole solution had to be scrapped at the end since the webhook’s behavior was so <em>flaky</em>, returning nonconsistent responses to the same set of inputs.
Users couldn’t register even after successful payments.
This flakiness made it unbearable to debug that webhook, which forced me to rewrite the payment system integration from scratch.
If I had only slowed down to plan and modularized the logic and wrote tests early on, I could have saved myself from so much headache.</p>

<p>When you slow down to plan and test your services, you’re trading off time and effort in exchange for confidence in your code.</p>

<p>A few other times you should consider implementing tests are when:</p>

<ul>
<li>
<p>Multiple contributors add changes over time</p>
</li>
<li>
<p>Maintainers change external dependencies</p>
</li>
<li>
<p>You increase the number of components and dependencies in your services</p>
</li>
<li>
<p>You suddenly spot too many bugs appearing</p>
</li>
<li>
<p>There is too much at stake if things go wrong—my experience fell into this bucket</p>
</li>
</ul>

<p>You should now understand how testing will benefit your project.<a data-startref="ix_ch11-asciidoc1" data-type="indexterm" id="id1143"/></p>
</div></section>






<section data-pdf-bookmark="Software Testing" data-type="sect1"><div class="sect1" id="id173">
<h1>Software Testing</h1>

<p><a data-primary="software testing" data-type="indexterm" id="ix_ch11-asciidoc2"/><a data-primary="testing AI services" data-secondary="software testing" data-type="indexterm" id="ix_ch11-asciidoc3"/>Now that you’re familiar with the challenges and potential approaches to testing GenAI services, let’s review software testing concepts to understand their relevance to GenAI use cases and common pitfalls to avoid.</p>








<section data-pdf-bookmark="Types of Tests" data-type="sect2"><div class="sect2" id="id174">
<h2>Types of Tests</h2>

<p><a data-primary="software testing" data-secondary="types of tests" data-type="indexterm" id="ix_ch11-asciidoc4"/><a data-primary="testing AI services" data-secondary="software testing" data-tertiary="types of tests" data-type="indexterm" id="ix_ch11-asciidoc5"/>There are three common types of tests in software testing, which, ordered by increasing size and complexity, are as follows:</p>
<dl>
<dt>Unit tests</dt>
<dd>
<p><a data-primary="unit tests" data-secondary="overview" data-type="indexterm" id="id1144"/>Focus on testing individual components or functions in isolation across a discrete set of inputs and edge cases to validate functionality at singular component level.
Unit tests are atomic with the smallest scope and often don’t rely on external systems or dependencies.</p>
</dd>
<dt>Integration tests</dt>
<dd>
<p><a data-primary="integration tests" data-secondary="overview" data-type="indexterm" id="id1145"/>Check the interaction between various components or systems to verify they function together as intended.
Integration tests often capture issues with application behavior at a subsystem level, validating data flows and interface contracts, (i.e., specifications) between various components.</p>
</dd>
<dt>End-to-end (E2E) tests</dt>
<dd>
<p><a data-primary="end-to-end testing" data-secondary="overview" data-type="indexterm" id="id1146"/>Verify the functionalities of the application at the highest system level from start to finish by simulating real usage scenarios.
E2E tests give you the highest levels of confidence in your application functionality and performance but are the most challenging tests to design, develop, and maintain.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>E2E tests and integration tests share similarities that make them hard to distinguish from one another.
If a test is big and sometimes flaky, you may be working on an E2E test.</p>

<p>Integration tests normally check a subset of systems and interactions, not the whole system or a long chain of subsystems.</p>
</div>

<p><a data-type="xref" href="#test_types">Figure 11-1</a> demonstrates the scope of each test type.
Unit tests shown on the left focus on isolated components, while integration tests check pairwise interactions of multiple components, including with external services.
Lastly, E2E tests cover the entire user journey and data flow within the application to confirm the intended functionality.</p>

<figure><div class="figure" id="test_types">
<img alt="bgai 1101" src="assets/bgai_1101.png"/>
<h6><span class="label">Figure 11-1. </span>Types of tests in software testing</h6>
</div></figure>

<p>Before implementing any of the aforementioned tests, you can also use <em>static code checks</em> with tools like <code>mypy</code> to catch syntax and type errors.
As you write code, static checks can also help you catch code style violations, misuse of functions and 
<span class="keep-together">dependencies,</span> security vulnerabilities, dead or unused code, data flow issues, and potential bugs in your system components.</p>

<p>As you progress from static checks and unit tests to integration and then to E2E tests, your test cases become more valuable but also more complex, expensive, and slower.
<a data-primary="end-to-end testing" data-secondary="drawbacks" data-type="indexterm" id="id1147"/>More important, since E2E tests have broader scope with multiple components interacting, they’ll become more brittle and likely to fail, requiring frequent updates to stay aligned with changes in your code.</p>

<p>E2E tests are also complex and flaky/nondeterministic.
According to Martin Fowler,<sup><a data-type="noteref" href="ch11.html#id1148" id="id1148-marker">1</a></sup> these are the main reasons for the nondeterminism:</p>

<ul>
<li>
<p><em>Lack of isolation</em> causes components to interfere with each other, leading to unpredictable results.</p>
</li>
<li>
<p><em>Asynchronous behavior</em> with operations that occur out of sequence or at unpredictable times can lead to nondeterministic outcomes.</p>
</li>
<li>
<p><em>Remote services</em> can introduce variability due to network latency, service availability, or differing responses.</p>
</li>
<li>
<p><em>Resource leaks</em>, if not properly managed, can lead to inconsistent system behavior.
Affected resources include memory, file handles, or connections to databases, clients, etc.</p>
</li>
</ul>

<p>Finally, due to the brittleness of E2E tests, refactors and changes in functionality can cause them to fail.
Therefore, there is a trade-off between the level of confidence you gain from E2E tests and the flexibility to make changes to your systems.<a data-startref="ix_ch11-asciidoc5" data-type="indexterm" id="id1149"/><a data-startref="ix_ch11-asciidoc4" data-type="indexterm" id="id1150"/></p>
</div></section>








<section data-pdf-bookmark="The Biggest Challenge in Testing Software" data-type="sect2"><div class="sect2" id="id258">
<h2>The Biggest Challenge in Testing Software</h2>

<p><a data-primary="software testing" data-secondary="biggest challenges" data-type="indexterm" id="id1151"/><a data-primary="testing AI services" data-secondary="software testing" data-tertiary="biggest challenges" data-type="indexterm" id="id1152"/>The biggest challenge in testing services is identifying what to test and to what detail.
As part of this, you need to decide what to mock, fake, or keep real alongside configuring a host of testing tools and environments.</p>

<p>To overcome this challenge, you can plan your tests in advance, identifying breaking points in your system and narrowing down issues to individual components and interactions.
Then, imagine who the user is and list the steps they’ll take when interacting with the problematic systems.
Finally, you can translate that lists of steps into individual tests and automate them.</p>

<p>Another challenge with testing that causes so much frustration is having to rewrite your tests whenever you refactor the code they’re testing.</p>

<p>Since code refactors don’t change the functional behavior but implementation details, it can be a sign that you’re not testing the right things.
For example, if you’re testing the internal string processing logic of the <code>count_tokens(text)</code> function rather than just its final output (i.e., the token count), using an external library to replace the string manipulation logic can cause your tests to fail.</p>

<p>A telltale sign that you might be testing implementation details is when your tests fail as you refactor the code (i.e., false positives), or pass even when you introduce breaking changes to your code (i.e., false negatives).
You can use techniques such as <em>black-box testing</em> to test your system by providing inputs and observing outputs, without considering the implementation details.</p>

<p>If you plan your tests in advance, you can avoid these testing challenges.</p>
</div></section>








<section data-pdf-bookmark="Planning Tests" data-type="sect2"><div class="sect2" id="id175">
<h2>Planning Tests</h2>

<p><a data-primary="software testing" data-secondary="planning tests" data-type="indexterm" id="ix_ch11-asciidoc6"/><a data-primary="testing AI services" data-secondary="software testing" data-tertiary="planning tests" data-type="indexterm" id="ix_ch11-asciidoc7"/>To identify the tests you need during planning, <a data-primary="verification and validation (V&amp;V) process" data-type="indexterm" id="id1153"/>you can use the <em>verification and validation</em> (V&amp;V) process.</p>

<p>Following this process, you first confirm possession of the right requirements (validation) and then leverage tests to verify all requirements are met (verification).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Having 100% code coverage with passing tests will complete only the verification process, not validation.
You still need to make sure that your services implement the right functions (i.e., validation).</p>
</div>

<p>The V&amp;V process can be visualized as a V-shaped model as per <a data-type="xref" href="#vv">Figure 11-2</a>.</p>

<p>When you go down the V model, you define software requirements and design the solution before implementing it as code.
Afterward, you come back up the “V” by running progressive tests (unit, integration, E2E, etc.) to validate that your solution meets the business needs.</p>

<figure><div class="figure" id="vv">
<img alt="bgai 1102" src="assets/bgai_1102.png"/>
<h6><span class="label">Figure 11-2. </span>Verification and validation model</h6>
</div></figure>

<p>As discussed earlier, validation tests can be challenging to identify.
Exactly what progressive tests should you implement?</p>

<p>When you struggle with identifying what to test, you can write tests based on issues and bugs that you find in your application.
This approach is <em>reactive</em> since you’re writing tests only when issues arise, which can help you overcome the challenges of not knowing what to test.
However, testing to resolve issues later in the <em>software development lifecycle</em> (SDLC) requires significant testing efforts as you’ll be dealing with a more complex system with many moving parts to test.</p>

<p><a data-primary="shift-left testing" data-type="indexterm" id="ix_ch11-asciidoc8"/>That’s why movements such as <em>shift-left testing</em> are advocating for <em>preventative</em> testing practices that are planned in advance and written during development to reduce the testing efforts.
<a data-type="xref" href="#shift_left">Figure 11-3</a> demonstrates how moving testing efforts earlier on the SLDC can reduce the overall burden.</p>

<figure><div class="figure" id="shift_left">
<img alt="bgai 1103" src="assets/bgai_1103.png"/>
<h6><span class="label">Figure 11-3. </span>Shift-left software testing</h6>
</div></figure>

<p><a data-primary="Test-Driven Development (TDD)" data-type="indexterm" id="id1154"/><a data-primary="TDD (Test-Driven Development)" data-type="indexterm" id="id1155"/>A common approach in shift-left testing is <em>test-driven development</em> (TDD), as shown in <a data-type="xref" href="#tdd">Figure 11-4</a>.</p>

<figure><div class="figure" id="tdd">
<img alt="bgai 1104" src="assets/bgai_1104.png"/>
<h6><span class="label">Figure 11-4. </span>TDD</h6>
</div></figure>

<p>In the TDD approach, you write the tests before the actual code.
This is an iterative process where written tests will fail at first, but your aim will be to write the minimal amount of code for tests to pass.
Once passing, you refactor the code to optimize the system while keeping the tests passing to finish the iterative TDD process.</p>
<div data-type="tip"><h6>Tip</h6>
<p><a data-primary="prompt engineering" data-secondary="TDD in" data-type="indexterm" id="id1156"/>A great example of where TDD practices are useful in testing GenAI services is during <em>prompt engineering</em>.
Write a set of tests first, and then keep iterating over the prompt design until all your tests pass.</p>

<p>Using the same test cases, you can check for any signs of regression and whether switching models will reduce the performance of your services.</p>
</div>

<p>As you can see, the overall aim of TDD is to reduce the testing efforts by improving your code quality, solution design, and early detection of issues<a data-startref="ix_ch11-asciidoc8" data-type="indexterm" id="id1157"/>.<a data-startref="ix_ch11-asciidoc7" data-type="indexterm" id="id1158"/><a data-startref="ix_ch11-asciidoc6" data-type="indexterm" id="id1159"/></p>
</div></section>








<section data-pdf-bookmark="Test Dimensions" data-type="sect2"><div class="sect2" id="id259">
<h2>Test Dimensions</h2>

<p><a data-primary="software testing" data-secondary="test dimensions" data-type="indexterm" id="id1160"/><a data-primary="testing AI services" data-secondary="software testing" data-tertiary="test dimensions" data-type="indexterm" id="id1161"/>Additionally, during planning, you normally have to decide on various test dimensions including testing <em>scope</em>, <em>coverage</em>, and <em>comprehensiveness</em>:</p>
<dl>
<dt>Scope</dt>
<dd>
<p>Defines what components, systems, and usage scenarios you’ll be testing.
As part of scope definition, you’ll also be drawing <em>testing boundaries</em> to clarify what will and won’t be tested.</p>
</dd>
<dt>Coverage or testing surface area</dt>
<dd>
<p>Measures how much of the system or codebase you’ll be testing.</p>
</dd>
<dt>Comprehensiveness</dt>
<dd>
<p>Indicates how detailed, in-depth, and complete your tests will be within the defined scope and coverage.
For example, you’ll decide whether you’ll be testing every potential usage, success and failure scenarios, and edge cases for every component, system, and interaction.</p>
</dd>
</dl>

<p>You can also think of testing scope, coverage, and comprehensiveness as testing <em>space/volume</em>, <em>surface area</em>, and <em>depth</em>.
Volume/space defines the boundaries and dimensions of what is being tested; surface area measures the spread of tests; and depth implies detail, depth, and completeness of test cases.</p>
</div></section>








<section data-pdf-bookmark="Test Data" data-type="sect2"><div class="sect2" id="id260">
<h2>Test Data</h2>

<p><a data-primary="software testing" data-secondary="test data" data-type="indexterm" id="id1162"/><a data-primary="testing AI services" data-secondary="software testing" data-tertiary="test data" data-type="indexterm" id="id1163"/>To achieve higher test coverage and comprehensiveness, you can leverage four different types of test data:</p>
<dl>
<dt>Valid data</dt>
<dd>
<p>Inputs to the system within the valid and expected range under normal 
<span class="keep-together">conditions.</span></p>
</dd>
<dt>Invalid data</dt>
<dd>
<p>Inputs that are unexpected, wrong, NULL, or outside the valid range.
You can use negative data to test how the system behaves when it’s misused.</p>
</dd>
<dt>Boundary data</dt>
<dd>
<p>Test data at boundaries of acceptable input ranges, whether at the upper or lower limit.</p>
</dd>
<dt>Huge data</dt>
<dd>
<p>Used for performance and stress testing the system to measure its limits.</p>
</dd>
</dl>
</div></section>








<section data-pdf-bookmark="Test Phases" data-type="sect2"><div class="sect2" id="id176">
<h2>Test Phases</h2>

<p><a data-primary="given-when-then (GWT) model" data-type="indexterm" id="id1164"/><a data-primary="GWT (given-when-then) model" data-type="indexterm" id="id1165"/><a data-primary="software testing" data-secondary="test phases" data-type="indexterm" id="id1166"/><a data-primary="testing AI services" data-secondary="software testing" data-tertiary="test phases" data-type="indexterm" id="id1167"/>It doesn’t matter if you’re implementing a unit, integration, or E2E test, you can structure tests in several distinct phases using the <em>given-when-then</em> (GWT) model, as outlined here:</p>
<ol>
<li>
<p><em>Given (preconditions)</em>: Before any given test, you can set up test conditions and predefined states or data (i.e., <em>fixtures</em>).</p>
</li>
<li>
<p><em>When (test steps)</em>:
During the test, you perform a set of action steps that you’ll like to test.
This is where you’ll pass your test fixtures to the <em>system under test</em> (SUT), which, depending on the test scope, can be a singular function or your whole 
<span class="keep-together">service.</span></p>
</li>
<li>
<p><em>Then (expected results)</em>:
After executing the SUT with fixtures, you’ll check the outputs against your expectations in this phase using a set of assert statements.</p>
</li>
<li>
<p><em>Cleanup</em>: Once done, you can clean up test artifacts within an optional
<em>cleanup/tear-down</em> phase.</p>
</li>

</ol>

<p><a data-primary="arrange-act-assert-cleanup testing model" data-type="indexterm" id="id1168"/><a data-primary="pytest" data-secondary="arrange-act-assert-cleanup testing model" data-type="indexterm" id="id1169"/><code>pytest</code> recommends structuring tests using the <em>arrange-act-assert-cleanup</em> model, which directly corresponds to the GWT model with an optional cleanup phase.</p>
</div></section>








<section data-pdf-bookmark="Test Environments" data-type="sect2"><div class="sect2" id="id177">
<h2>Test Environments</h2>

<p><a data-primary="software testing" data-secondary="test environments" data-type="indexterm" id="id1170"/><a data-primary="testing AI services" data-secondary="software testing" data-tertiary="test environments" data-type="indexterm" id="id1171"/>When planning tests, you should also consider various <em>testing environments</em> covering compile time, build time, and runtime environments.</p>

<p><a data-primary="compile time" data-type="indexterm" id="id1172"/>In many programming languages, <em>compile time</em> is when the source code is translated into executable code.
For instance, if you’re writing code in C++, the compilation process involves a comprehensive type check across the entire codebase.
If type errors are found, the compilation process will fail.
Once all checks pass, the C++ compiler translates the code into an executable binaries.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The strong typing in C++ was designed to improve error detection, code robustness, and support developer tooling in larger and complex codebases that change frequently.</p>
</div>

<p>Since Python is an interpreted language, it doesn’t have a traditional compile time like C++.
Instead, Python converts code into bytecode for execution.</p>

<p>During inspections, static code checkers like <code>mypy</code> can identify basic issues in your code, serving as an initial verification layer in your testing efforts.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Since Python is a dynamically typed language, it doesn’t enforce typing by default.
However, static type checkers like <code>mypy</code> can provide significant value if you use type hints in your Python code.</p>
</div>

<p>While static checks are great for catching basic code issues at compile time, unit, integration, and E2E tests can verify the system functionality at <em>runtime</em> when you execute application code.
During runtime, tools like Pydantic can perform data validation checks to catch unexpected data structures.</p>

<p>Your GenAI services may also require additional setup and build steps such as downloading weights and preloading models, before executing any application code.
The environment in which build steps, setups, and dependency installations are completed is referred to as <em>build time</em>, which you can also test.</p>
</div></section>








<section data-pdf-bookmark="Testing Strategies" data-type="sect2"><div class="sect2" id="id178">
<h2>Testing Strategies</h2>

<p><a data-primary="software testing" data-secondary="testing strategies" data-type="indexterm" id="ix_ch11-asciidoc9"/><a data-primary="testing AI services" data-secondary="software testing" data-tertiary="testing strategies" data-type="indexterm" id="ix_ch11-asciidoc10"/>Within the software landscape, various experts have developed strategies for balancing the distribution of tests in projects, which are based on years of software testing experience from the developers that popularized them.</p>

<p><a data-primary="testing pyramid" data-type="indexterm" id="id1173"/>The most widely adopted strategy is the <em>testing pyramid</em>, as shown in
<a data-type="xref" href="#testing_pyramid">Figure 11-5</a>, which promotes writing more unit tests.</p>

<figure><div class="figure" id="testing_pyramid">
<img alt="bgai 1105" src="assets/bgai_1105.png"/>
<h6><span class="label">Figure 11-5. </span>Testing pyramid</h6>
</div></figure>

<p><a data-type="xref" href="#testing_pyramid_example">Table 11-1</a> outlines the purpose of each layer in the testing pyramid alongside a concrete example in the context of a GenAI service.</p>
<table class="striped" id="testing_pyramid_example">
<caption><span class="label">Table 11-1. </span>Testing pyramid in real world</caption>
<thead>
<tr>
<th>Layer</th>
<th>Purpose</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>End-to-end tests</p></td>
<td><p>Validate the entire application flow from start to finish</p></td>
<td><p>Testing user login, generating text based on a prompt, and saving the generated content</p></td>
</tr>
<tr>
<td><p>Integration tests</p></td>
<td><p>Verify various modules or services work together correctly</p></td>
<td><p>Testing the interactions between the text generation API and the database storing user prompts and generated texts</p></td>
</tr>
<tr>
<td><p>Unit tests</p></td>
<td><p>Verify individual components or functions in isolation</p></td>
<td><p>Testing various utility functions that process inputs to the model, for instance, to remove inappropriate content</p></td>
</tr>
</tbody>
</table>

<p>The issue with the pyramid model is that while unit tests improve code coverage, they do not necessarily enhance “business coverage” since project requirements and use cases might not be thoroughly tested.
As a result, relying only on unit tests can create a false sense of security, potentially overlooking testing essential business logic and user workflows.
On the other hand, integration tests allow you to cover more ground and business-driven tests.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Software testing experts have also identified a few strategies as <em>anti-patterns</em> that are counterproductive.</p>

<p>If you follow them, you’ll spend an excessive amount of time setting up the tests, implement overly specific and tightly coupled tests, and end up with tests that exhibit nondeterministic flaky behavior.</p>
</div>

<p><a data-primary="software testing" data-secondary="anti-patterns" data-type="indexterm" id="id1174"/><a data-type="xref" href="#testing_antipatterns">Table 11-2</a> and <a data-type="xref" href="#testing_antipatterns_viz">Figure 11-6</a> show a list of software testing anti-patterns.</p>
<table class="striped" id="testing_antipatterns">
<caption><span class="label">Table 11-2. </span>Software testing anti-patterns</caption>
<thead>
<tr>
<th>Strategy</th>
<th>Test distribution</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Testing ice-cream cone</p></td>
<td><p>Small number of unit tests with a large number of integration and E2E tests, followed by manual testing.</p></td>
<td><p>Avoid. Considered an anti-pattern due to inefficiency of implementing manual tests and high costs of maintaining integration and E2E tests.</p></td>
</tr>
<tr>
<td><p>Testing cupcake</p></td>
<td><p>Similar to the ice-cream cone; has a small number of automated unit and integration tests, a moderate number of automated E2E/GUI tests, and a large number of manual tests.</p><p>Each test type is performed by a different team.</p></td>
<td><p>Avoid. Considered an anti-pattern because it can lead to slow feedback cycles, communication overheads between teams, and brittle tests with high maintenance costs.</p></td>
</tr>
<tr>
<td><p>Testing hourglass</p></td>
<td><p>Large number of unit tests at the base and E2E tests at the top, but significantly fewer integration tests in the middle.</p></td>
<td><p>Avoid. Considered an anti-pattern. Not as bad as the ice-cream cone, but still results in too many test failures, that medium-scope tests could’ve covered.</p></td>
</tr>
</tbody>
</table>

<figure><div class="figure" id="testing_antipatterns_viz">
<img alt="bgai 1106" src="assets/bgai_1106.png"/>
<h6><span class="label">Figure 11-6. </span>Visualization of testing anti-patterns</h6>
</div></figure>

<p><a data-type="xref" href="#testing_strategies">Table 11-3</a> and <a data-type="xref" href="#testing_strategies_viz">Figure 11-7</a> compare software testing strategies.</p>
<table class="striped" id="testing_strategies">
<caption><span class="label">Table 11-3. </span>Comparison of testing strategies</caption>
<thead>
<tr>
<th>Strategy</th>
<th>Test distribution</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><strong>Testing pyramid</strong></p><p>(Mike Cohn)</p></td>
<td><p>Large number of unit tests at the base, fewer integration tests in the middle, and even fewer E2E tests at the top.</p></td>
<td><p>It’s a widely accepted strategy.
However, the pyramid can be perceived as a physical concept to promote building the bottom layer of unit tests first, then constructing the next layer, and so on, until reaching the top. This approach is ineffective when applied to legacy applications with a large codebase.</p></td>
</tr>
<tr>
<td><p><strong>Testing trophy</strong></p><p>(Kent C. Dodds)</p></td>
<td><p>Focuses on having a strong foundation of static checks, then unit tests, followed by integration tests, and a smaller number of E2E tests at the top.</p></td>
<td><p>The rationale for this is that E2E and integration tests are the most valuable. However, E2E tests are slow and expensive. Integration tests strike a balance between both worlds.</p></td>
</tr>
<tr>
<td><p><strong>Testing honeycomb</strong></p><p>(Stephen H. Fishman)</p></td>
<td><p>Represents a balanced approach with equal emphasis on unit, integration, E2E, and other types of tests (performance, security, etc.).</p></td>
<td><p>It can be less efficient if not managed properly and may not be optimal for every project.</p></td>
</tr>
</tbody>
</table>

<figure><div class="figure" id="testing_strategies_viz">
<img alt="bgai 1107" src="assets/bgai_1107.png"/>
<h6><span class="label">Figure 11-7. </span>Visualization of testing strategies</h6>
</div></figure>

<p>For GenAI services, which often involve complex integrations and performance considerations, the trophy testing strategy might be the most suitable.
The trophy strategy consists of a strong foundation of static checks, powered by tools such as <code>mypy</code> and Pydantic, alongside mostly integration tests that strike a balance between value, confidence, and testing costs.</p>

<p>If your GenAI services must be comprehensively tested with various test types, including performance and exploratory tests, then the honeycomb model may be more suitable for your project since it can equalize testing efforts.</p>

<p>You should now feel more comfortable in identifying what tests you need and how to plan your tests<a data-startref="ix_ch11-asciidoc10" data-type="indexterm" id="id1175"/><a data-startref="ix_ch11-asciidoc9" data-type="indexterm" id="id1176"/>.<a data-startref="ix_ch11-asciidoc3" data-type="indexterm" id="id1177"/><a data-startref="ix_ch11-asciidoc2" data-type="indexterm" id="id1178"/></p>

<p>Now that you’re familiar with software testing concepts, let’s review the challenges and potential approaches to testing GenAI services.</p>
</div></section>
</div></section>






<section data-pdf-bookmark="Challenges of Testing GenAI Services" data-type="sect1"><div class="sect1" id="id261">
<h1>Challenges of Testing GenAI Services</h1>

<p><a data-primary="testing AI services" data-secondary="challenges of testing GenAI services" data-type="indexterm" id="ix_ch11-asciidoc11"/>If you’ve decided to test your GenAI services, you will face several challenges.
Testing services that leverage probabilistic GenAI models require a more comprehensive approach than traditional software.</p>

<p>Let’s take a look at a few reasons why testing GenAI services will be challenging.</p>








<section data-pdf-bookmark="Variability of Outputs (Flakiness)" data-type="sect2"><div class="sect2" id="id273">
<h2>Variability of Outputs (Flakiness)</h2>

<p><a data-primary="testing AI services" data-secondary="challenges of testing GenAI services" data-tertiary="variability of outputs (flakiness)" data-type="indexterm" id="id1179"/>Given the same set of inputs and implementation code, GenAI services often produce different outputs.
The outputs are varied because these models use probabilistic techniques such as sampling from a distribution rather than relying on deterministic functions.
Of course, the variability you experience on outputs can be model dependent, and adjusting configurations, such as temperature values, can reduce this 
<span class="keep-together">variance.</span></p>

<p>The variability of outputs from GenAI models can also explode the number of potential test cases you can write (i.e., the <em>testing area/scope</em>) to cover every possibility.
Because of this, you can’t fully rely on deterministic tests.
Your tests will perform inconsistently and will be too <em>flaky</em> to run reliably within a CI/CD pipeline.</p>

<p>Instead, you should approach the GenAI testing problem from a statistical and probabilistic perspective.
Take several samples based on valid assumptions from a <em>legitimate distribution of inputs</em> to verify the quality of your model’s product outputs.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>By <em>legitimate distribution of inputs</em>, I mean selecting inputs that are aligned with the model’s purpose, representative of real-world scenarios, and relevant to the problem you’re trying to solve with the model.</p>
</div>

<p>A more involved approach is to use discriminator models to score your service’s variable outputs as long as you set expectations of a certain tolerance or threshold.</p>

<p>You’ll see examples of how to do this later in the chapter.</p>
</div></section>








<section data-pdf-bookmark="Performance and Resource Constraints (Slow and Expensive)" data-type="sect2"><div class="sect2" id="id274">
<h2>Performance and Resource Constraints (Slow and Expensive)</h2>

<p><a data-primary="testing AI services" data-secondary="challenges of testing GenAI services" data-tertiary="performance/resource constraints (slow and expensive)" data-type="indexterm" id="id1180"/>Since testing GenAI services requires a more statistical and/or multimodel approach, you will also face latency, usage, and hosting problems.</p>

<p>Your tests can’t run fast enough and reliably to run continually within a traditional CI/CD pipeline.
You will end up with excessive token usage costs with multiple model API calls and slow-running and complex multimodel tests.
These challenges remain unless you make several assumptions to simplify the testing scope, reduce model testing frequency, and use efficient testing techniques such as mocking and patching, dependency injection, and statistical hypothesis tests.
You can also investigate the use of small fine-tuned discriminator models to reduce latency and improve performance.</p>
</div></section>








<section data-pdf-bookmark="Regression" data-type="sect2"><div class="sect2" id="id179">
<h2>Regression</h2>

<p><a data-primary="regression testing" data-type="indexterm" id="ix_ch11-asciidoc12"/><a data-primary="testing AI services" data-secondary="challenges of testing GenAI services" data-tertiary="regression" data-type="indexterm" id="ix_ch11-asciidoc13"/><em>Regression testing</em> is another type of test to plan for when working with GenAI 
<span class="keep-together">models.</span></p>

<p>A <a href="https://oreil.ly/1oLQG">research paper published in 2023</a> that compared ChatGPT’s behavior over time found that:</p>
<blockquote>
<p>The performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.
For example, GPT-4 (March 2023) was reasonable at identifying prime vs.
composite numbers (84% accuracy), but GPT-4 (June 2023) was poor on these same questions (51% accuracy).
GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March.
In addition, both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March.</p></blockquote>

<p>According to this study, the behavior of the “same” LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs (and any GenAI services).
Based on this finding, you can assume the performance of your GenAI services may degrade over time due to model fine-tuning or retraining, shifts in user interaction patterns, and the changes in its training data or operating environment.</p>

<p><a data-primary="model drift" data-type="indexterm" id="id1181"/>To elaborate further, probabilistic AI models can experience <em>model drift</em>, a performance degradation over time attributable to their underlying training data.
Fine-tuning on additional data can have unexpected side effects on model’s behavior in other tasks.</p>

<p>As time passes, the original training data can drift away from reality.
Trends change, new historical events happen, languages evolve, and human knowledge expands or mutates to take new forms that won’t be captured if the training data is not continually updated.
<a data-primary="concept drift" data-type="indexterm" id="id1182"/>This phenomenon that causes model drift is referred to as <em>concept drift</em>, which occurs when the statistical properties of the target variable that the model is trying to predict change over time.
Concept drift can lead to a model’s performance degrading because the relationships and patterns the model learned during training no longer apply.</p>

<p><a data-primary="data drift" data-type="indexterm" id="id1183"/>Furthermore, <em>data drift</em>, which involves changes in the distribution of the input features within the training data, can also lead to model drift.</p>

<p>This type of drift is often due to changes in the sampling methods, population distribution and data collection, seasonality changes and temporal effects in the data, external changes in data sources, or quality issues in the processing pipelines.</p>

<p>Regression testing and monitoring (in particular if you rely on external model providers such as OpenAI) can help you detect model drift issues on your specific tasks and use cases.
Any potential drifts can then be addressed at the application layer via data validation or by using techniques such as RAG or at the model layer via retraining and model fine-tuning to reduce regression issues.<a data-startref="ix_ch11-asciidoc13" data-type="indexterm" id="id1184"/><a data-startref="ix_ch11-asciidoc12" data-type="indexterm" id="id1185"/></p>
</div></section>








<section data-pdf-bookmark="Bias" data-type="sect2"><div class="sect2" id="id180">
<h2>Bias</h2>

<p><a data-primary="bias, testing for" data-type="indexterm" id="id1186"/><a data-primary="testing AI services" data-secondary="challenges of testing GenAI services" data-tertiary="bias" data-type="indexterm" id="id1187"/>Another grand challenge in testing GenAI services is detecting model bias before going to production.
Often, bias is investigated during the data exploration process by data scientists and ML engineers responsible for producing the models.
However, with large foundation GenAI models, there’s always a possibility that some form of bias may be introduced through incorrect evaluation, sampling methods, data processing, training algorithms, or hidden bias in the data itself.</p>

<p>For example, if a language model is trained on a dataset that mostly contains text related to a specific demographic, it may generate outputs biased toward that demographic, potentially excluding other groups.
Similarly, if an image recognition model is trained mostly on images of men in professions like doctors and engineers, it may learn to generate images with a gender bias.</p>

<p>The bias becomes particularly serious in scenarios where you want to use LLMs as a judge, such as AI marking tools or interview assessors.</p>

<p>Bias can manifest in various forms such as gender bias, racial bias, age bias, etc., and each type requires specific tests and metrics to detect.
Without knowing what to test for, you can’t confidently verify if your GenAI services are 100% bias-free.</p>

<p>A possible solution to this problem is leveraging model self-checks and AI discriminators where a secondary model identifies or measures the presence of any bias.
There is often a trade-off between latency and usage quotas when detecting bias during service runtime.</p>
</div></section>








<section data-pdf-bookmark="Adversarial Attacks" data-type="sect2"><div class="sect2" id="id181">
<h2>Adversarial Attacks</h2>

<p><a data-primary="adversarial testing" data-type="indexterm" id="id1188"/><a data-primary="securing AI services" data-secondary="adversarial tests" data-type="indexterm" id="id1189"/><a data-primary="testing AI services" data-secondary="challenges of testing GenAI services" data-tertiary="adversarial attacks" data-type="indexterm" id="id1190"/>Public-facing GenAI services can be vulnerable to adversarial attacks such as token manipulation, insecure data handling, jailbreak prompting or prompt injection, sensitive information disclosure, data poisoning, model theft, denial of service, excessive agency, and general misuse and abuse.
Therefore, any GenAI services exposed to the internet will need to include safeguarding layers.</p>

<p>Resources and checklists such as <a href="https://genai.owasp.org">OWASP’s top 10 for LLM application</a> provide a starting point for adding safeguards to your GenAI 
<span class="keep-together">services.</span></p>

<p class="less_space pagebreak-before">However, building safeguarding mechanisms can be challenging because current methods, at the time of writing, rely on classification and discriminatory models to detect adversarial attacks and harmful content.
These safeguarding models often require hundreds of megabytes of dependencies, which can bloat your application, significantly slow down your service’s throughput, and still may not catch every potential attack scenario.</p>

<p>Adversarial tests ensure you have enough safeguarding in place to protect your services and reputation.
As part of adversarial tests, you should also verify the performance of your authentication and authorization guards.</p>

<p><a data-type="xref" href="ch09.html#ch09">Chapter 9</a> goes into more detail on implementing these safeguarding layers and evaluation techniques to protect your models against such attacks.</p>
</div></section>








<section data-pdf-bookmark="Unbound Testing Coverage" data-type="sect2"><div class="sect2" id="id182">
<h2>Unbound Testing Coverage</h2>

<p><a data-primary="testing AI services" data-secondary="challenges of testing GenAI services" data-tertiary="unbound testing coverage" data-type="indexterm" id="id1191"/>The latent space of GenAI models is so vast that you can’t rely on unit tests to achieve 100% coverage of every usage scenario.</p>

<p>Since there are an infinite number of inputs and responses, no matter how much you test your models, there will be hidden edge cases that slip through your tests.
Therefore, instead of relying on predefining every scenario, you can implement <em>behavioral testing</em>, which focuses on properties of responses instead of the exact outputs.</p>

<p>Examples of behavioral properties you can measure include <em>coherence</em> of generated data structures, <em>relevance</em> of outputs to inputs, <em>toxicity</em>, <em>correctness</em>, and <em>faithfulness</em> (i.e., faithful adherence to your policies and ethical guidelines).
You can also add a human in the loop as an additional testing layer for catching unexpected responses.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If you’re building a RAG or agentic application with multiple models and external dependencies, behavioral testing becomes even more practical.
Testing a fixed set of examples may miss edge cases, unexpected interactions between components, and variability in responses due to external factors.</p>
</div>

<p>In the next section, you’ll learn how to implement your own unit, integration, E2E, and behavioral tests by following a hands-on project.<a data-startref="ix_ch11-asciidoc11" data-type="indexterm" id="id1192"/></p>
</div></section>
</div></section>






<section data-pdf-bookmark="Project: Implementing Tests for a RAG System" data-type="sect1"><div class="sect1" id="id262">
<h1>Project: Implementing Tests for a RAG System</h1>

<p><a data-primary="RAG (retrieval augmented generation) module" data-secondary="test implementation project" data-type="indexterm" id="ix_ch11-asciidoc14"/><a data-primary="testing AI services" data-secondary="project: implementing tests for a RAG system" data-type="indexterm" id="ix_ch11-asciidoc15"/>In the hands-on project, you will be writing a test suite for the RAG module that you implemented in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a>.
The RAG system you will be testing has interfaces with an LLM, a vector database, and the server’s filesystem via asynchronous methods, so it provides a perfect opportunity to understand the testing principles discussed so far.</p>

<p>By following along with the code examples, you’ll learn the best practices for implementing unit, integration, and E2E tests for your GenAI services, as well as their 
<span class="keep-together">differences.</span></p>








<section data-pdf-bookmark="Unit Tests" data-type="sect2"><div class="sect2" id="id183">
<h2>Unit Tests</h2>

<p><a data-primary="testing AI services" data-secondary="project: implementing tests for a RAG system" data-tertiary="unit tests" data-type="indexterm" id="ix_ch11-asciidoc16"/><a data-primary="unit tests" data-type="indexterm" id="ix_ch11-asciidoc17"/>You can start testing your GenAI services with unit tests.
The purpose of a unit test is to verify that an isolated part of your code, usually a single function or method, performs as intended.</p>

<p>Prior to writing tests, it’s important to plan the test cases you will be implementing.
For a typical RAG system, you can write unit tests on the data loading, transformation, retrieval, and generation pipelines.</p>

<p><a data-type="xref" href="#unit_boundaries">Figure 11-8</a> visualizes the testing boundaries of these potential unit tests on a data pipeline diagram.</p>

<figure><div class="figure" id="unit_boundaries">
<img alt="bgai 1108" src="assets/bgai_1108.png"/>
<h6><span class="label">Figure 11-8. </span>Unit test boundaries visualized on the RAG data pipeline diagram</h6>
</div></figure>

<p>Notice how the testing boundaries end at the start and end of each data processing pipeline function.
This is because the purpose of these unit tests is to test only the data processing pipeline code and not the database, filesystem, LLM model, or any associated interfaces.</p>

<p>In these unit tests, you’ll be assuming that these external systems will return what you expect and focus your unit tests only on what your data processing code will be doing.</p>

<p>For brevity, we won’t be testing every component of the system, but by following a handful of upcoming examples, you should feel comfortable implementing follow-on tests to achieve full coverage.</p>










<section data-pdf-bookmark="Installing and configuring pytest" data-type="sect3"><div class="sect3" id="id184">
<h3>Installing and configuring pytest</h3>

<p><a data-primary="pytest" data-secondary="installing/configuring" data-type="indexterm" id="ix_ch11-asciidoc18"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="unit tests" data-tertiary="installing/configuring pytest" data-type="indexterm" id="ix_ch11-asciidoc19"/><a data-primary="unit tests" data-secondary="installing/configuring pytest" data-type="indexterm" id="ix_ch11-asciidoc20"/>For this project, you’ll be using the <code>pytest</code> package, which has built-in components for handling test fixtures, parameters, async code, test collections, and a rich ecosystem of plug-ins.
<code>pytest</code> is more powerful, flexible, and extensible compared to 
<span class="keep-together"><code>unittest</code>,</span> Python’s built-in testing package often used for simple testing scenarios.</p>

<p>You can install <code>pytest</code> using the following:</p>

<pre data-type="programlisting">$ pip install pytest</pre>

<p>Next, create a <code>tests</code> directory at the root of your project where you can create Python modules following the <em>test_xxx.py</em> pattern.</p>

<p><code>pytest</code>’s test collector can then traverse your <code>tests</code> directory and find every test module, class, and function contained within:</p>

<pre data-code-language="text" data-type="programlisting">project
|-- main.py
...
|-- tests
    |-- test_rag_loader.py
    |-- test_rag_transform.py
    |-- test_rag_retrieval.py
...</pre>

<p>Inside each test file, you can add your test functions that always contain at least one <code>assert</code> statement.
If no exception is raised in these <code>assert</code> statements, then your tests will get <code>PASSED</code>.
Otherwise, <code>pytest</code> will mark them as <code>FAILED</code> alongside a reason/trace of why <code>assert</code> statements have failed:</p>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># tests/rag/transform.py</code>

<code class="k">def</code> <code class="nf">test_chunk_text</code><code class="p">():</code>
    <code class="n">text</code> <code class="o">=</code> <code class="s2">"Testing GenAI services"</code>
    <code class="n">results</code> <code class="o">=</code> <code class="n">chunk</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
    <code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">results</code><code class="p">)</code> <code class="o">=</code> <code class="mi">2</code></pre>

<p>Let’s assume you’ve written two test functions in each test module:</p>

<p>You can then execute your tests via the <code>pytest &lt;test_dirt_path&gt;</code> command.</p>

<pre data-type="programlisting">$ pytest tests
=========================== test session starts ============================
platform linux -- Python 3.11, pytest-8.0.0

Collected 6 items

tests/rag/loader.py ..                                                 [100%]
tests/rag/transform.py F.                                              [100%]
tests/rag/retrieval.py ..                                              [100%]

================================= FAILURES =================================
______________________________ test_chunk_text _____________________________

def test_chunk_text():
&gt;     assert len(results) == 2
E       assert 3 == 2

tests/rag/transform.py:6: AssertionError
========================= short test summary info ==========================
PASSED 5
FAILED tests/rag/transform.py::test_chunk_text - assert 3 == 2
============================ 1 failed in 0.12s =============================</pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Avoid writing large tests, as they become increasingly difficult to understand and implement correctly.</p>
</div>

<p>When writing unit tests, you care only about an isolated component of your system such as a single function in your code.
Other components fall outside the boundary of unit test.
Thus, you’ll be testing only whether a single component behaves as you expect given a set of test data as inputs.</p>

<p><a data-primary="chunking" data-type="indexterm" id="id1193"/>As an example, you can test whether your chunking function is splitting a document into chunks as you’d expect.
Maybe you want to experiment with different or complex chunking strategies for your RAG pipeline and want to make sure that any input text is chunked correctly.
Unit tests with predefined test data or <em>fixtures</em> can give you some confidence in your chunking function.</p>

<p><a data-type="xref" href="#unit_test">Example 11-1</a> demonstrates an example unit test for your chunking function.</p>
<div data-type="example" id="unit_test">
<h5><span class="label">Example 11-1. </span>Example unit test for a token chunking function</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># rag/transform.py</code>

<code class="k">def</code> <code class="nf">chunk</code><code class="p">(</code><code class="n">tokens</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code><code class="p">,</code> <code class="n">chunk_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">list</code><code class="p">[</code><code class="nb">list</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code><code class="p">]</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO1-1" id="co_testing_ai_services_CO1-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">if</code> <code class="n">chunk_size</code> <code class="o">&lt;</code><code class="o">=</code> <code class="mi">0</code><code class="p">:</code>
        <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code><code class="s2">"</code><code class="s2">Chunk size must be greater than 0</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">tokens</code><code class="p">[</code><code class="n">i</code><code class="p">:</code><code class="n">i</code> <code class="o">+</code> <code class="n">chunk_size</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code><code class="p">,</code> <code class="n">chunk_size</code><code class="p">)</code><code class="p">]</code>


<code class="c1"># tests/rag/transform.py</code>

<code class="kn">import</code> <code class="nn">pytest</code>
<code class="kn">from</code> <code class="nn">rag</code><code class="nn">.</code><code class="nn">transform</code> <code class="kn">import</code> <code class="n">chunk</code>

<code class="k">def</code> <code class="nf">test_chunking_success</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="c1"># GIVEN </code><a class="co" href="#callout_testing_ai_services_CO1-2" id="co_testing_ai_services_CO1-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">tokens</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
    <code class="c1"># WHEN </code><a class="co" href="#callout_testing_ai_services_CO1-3" id="co_testing_ai_services_CO1-3"><img alt="3" src="assets/3.png"/></a>
    <code class="n">result</code> <code class="o">=</code> <code class="n">chunk</code><code class="p">(</code><code class="n">token_list</code><code class="p">,</code> <code class="n">chunk_size</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
    <code class="c1"># THEN </code><a class="co" href="#callout_testing_ai_services_CO1-4" id="co_testing_ai_services_CO1-4"><img alt="4" src="assets/4.png"/></a>
    <code class="k">assert</code> <code class="n">result</code> <code class="o">=</code> <code class="p">[</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">]</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code> <code class="c1"># Other relevant asserts here</code></pre>
<dl class="calloutlist pagebreak-before less_space">
<dt><a class="co" href="#co_testing_ai_services_CO1-1" id="callout_testing_ai_services_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Chunk an integer token list into smaller lists of a specified <code>chunk_size</code>.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO1-2" id="callout_testing_ai_services_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Specify the test data in the <em>GIVEN (preconditions)</em> part of the test.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO1-3" id="callout_testing_ai_services_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Run the test steps in the <em>WHEN</em> part of the test that include passing the test data to the system under test.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO1-4" id="callout_testing_ai_services_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Check the results against the expected outputs in the <em>THEN</em> part of the test.<a data-startref="ix_ch11-asciidoc20" data-type="indexterm" id="id1194"/><a data-startref="ix_ch11-asciidoc19" data-type="indexterm" id="id1195"/><a data-startref="ix_ch11-asciidoc18" data-type="indexterm" id="id1196"/></p></dd>
</dl></div>
</div></section>










<section data-pdf-bookmark="Fixtures and scope" data-type="sect3"><div class="sect3" id="id185">
<h3>Fixtures and scope</h3>

<p><a data-primary="RAG (retrieval augmented generation) module" data-secondary="unit tests" data-tertiary="fixtures and scope" data-type="indexterm" id="ix_ch11-asciidoc21"/><a data-primary="unit tests" data-secondary="fixtures and scope" data-type="indexterm" id="ix_ch11-asciidoc22"/>The <a data-primary="fixtures (input data)" data-type="indexterm" id="id1197"/>input data you defined in <a data-type="xref" href="#unit_test">Example 11-1</a> for testing is also called a <em>fixture</em>, as its value remains fixed across each test run. There are two types of fixtures:</p>
<dl>
<dt>Fresh fixture</dt>
<dd>
<p>You define it inside each test, which then Python garbage collects (i.e., discards) after the test.
<a data-type="xref" href="#unit_test">Example 11-1</a> used a fresh fixture.</p>
</dd>
<dt>Shared fixture</dt>
<dd>
<p>You can reuse it across multiple tests to avoid repeating the same fixture over and over for each new test.</p>
</dd>
</dl>
<p class="fix_tracking">
You can declare a shared fixture outside the test functions as a global variable of the test module, but it’s considered an anti-pattern, as you can inadvertently modify them.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Shared fixtures must be <em>immutable</em>.
Otherwise, a test can change the fixture, creating a side effect rippling through other tests.
A major cause of flaky tests is mutable fixtures.</p>
</div>

<p><a data-primary="fixture functions" data-type="indexterm" id="id1198"/>Instead of being responsible for managing the state of shared fixtures yourself, you can rely on <code>pytest</code>’s dependency injection system through the use of <em>fixture functions</em>, as shown in <a data-type="xref" href="#fixture_function">Example 11-2</a>.</p>
<div data-type="example" id="fixture_function">
<h5><span class="label">Example 11-2. </span><code>pytest</code> fixture function</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># tests/rag/transform.py</code>

<code class="kn">import</code> <code class="nn">pytest</code>
<code class="kn">from</code> <code class="nn">rag</code><code class="nn">.</code><code class="nn">transform</code> <code class="kn">import</code> <code class="n">chunk</code>

<code class="c1"># GIVEN</code>
<code class="nd">@pytest</code><code class="o">.</code><code class="n">fixture</code><code class="p">(</code><code class="n">scope</code><code class="o">=</code><code class="s2">"</code><code class="s2">module</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO2-1" id="co_testing_ai_services_CO2-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">def</code> <code class="nf">tokens</code><code class="p">(</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO2-2" id="co_testing_ai_services_CO2-2"><img alt="2" src="assets/2.png"/></a>
    <code class="k">return</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>

<code class="k">def</code> <code class="nf">test_token_chunking_small</code><code class="p">(</code><code class="n">token_list</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO2-2" id="co_testing_ai_services_CO2-3"><img alt="2" src="assets/2.png"/></a>
    <code class="n">result</code> <code class="o">=</code> <code class="n">chunk</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">chunk_size</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
    <code class="k">assert</code> <code class="n">result</code> <code class="o">=</code> <code class="p">[</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">]</code>

<code class="k">def</code> <code class="nf">test_token_chunking_large</code><code class="p">(</code><code class="n">token_list</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO2-2" id="co_testing_ai_services_CO2-4"><img alt="2" src="assets/2.png"/></a>
    <code class="n">result</code> <code class="o">=</code> <code class="n">chunk</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">chunk_size</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
    <code class="k">assert</code> <code class="n">result</code> <code class="o">=</code> <code class="p">[</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code><code class="p">]</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO2-1" id="callout_testing_ai_services_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Declare the <code>input_text</code> function as a <code>pytest</code> fixture that can be shared across the module as specified by <code>scope="module"</code>.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO2-2" id="callout_testing_ai_services_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the <code>pytest</code> dependency injection to inject a shared fixture into different tests.</p></dd>
</dl></div>

<p>You can declare a function as a <code>pytest</code> fixture using the <code>@pytest.fixture(scope)</code> decorator.
The <code>scope</code> parameter specifies the lifespan of the shared fixture within a testing session.</p>

<p>Based on the <code>scope</code>’s value, <code>pytest</code> creates and destroys fixtures once per test function, <code>class</code>, <code>module</code>, <code>package</code>, or the entire testing <code>session</code>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>A scenario where you might need a shared fixture to persist across modules or the entire testing session is when you fetch the fixture from an external API and want to avoid making requests 
<span class="keep-together">repeatedly.</span></p>
</div>

<p>Using fixtures, you can implement several tests with various inputs covering valid, invalid, and boundary values to verify the robustness of each component.
However, you’ll need to separate test functions for each set of inputs and expected outputs.
To avoid rewriting the same test, <code>pytest</code> has a <em>parameterization</em> feature that you can 
<span class="keep-together">leverage.</span><a data-startref="ix_ch11-asciidoc22" data-type="indexterm" id="id1199"/><a data-startref="ix_ch11-asciidoc21" data-type="indexterm" id="id1200"/></p>
</div></section>










<section data-pdf-bookmark="Parameterization" data-type="sect3"><div class="sect3" id="id186">
<h3>Parameterization</h3>

<p><a data-primary="parameterization" data-type="indexterm" id="ix_ch11-asciidoc23"/><a data-primary="pytest" data-secondary="parameterization" data-type="indexterm" id="ix_ch11-asciidoc24"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="unit tests" data-tertiary="parameterization" data-type="indexterm" id="ix_ch11-asciidoc25"/><a data-primary="unit tests" data-secondary="parameterization" data-type="indexterm" id="ix_ch11-asciidoc26"/>With <code>pytest</code> parameterization, you can iterate over various test data and expected outputs to avoid duplicating tests, as you can see in <a data-type="xref" href="#pytest_params">Example 11-3</a>.</p>
<div data-type="example" id="pytest_params">
<h5><span class="label">Example 11-3. </span><code>pytest</code> parameterization</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># tests/rag/transform.py</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">parametrize</code><code class="p">(</code><code class="s2">"</code><code class="s2">tokens, chunk_size, expected</code><code class="s2">"</code><code class="p">,</code> <code class="p">[</code> <a class="co" href="#callout_testing_ai_services_CO3-1" id="co_testing_ai_services_CO3-1"><img alt="1" src="assets/1.png"/></a>
    <code class="p">(</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="p">[</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">]</code><code class="p">)</code><code class="p">,</code> <code class="c1"># valid</code>
    <code class="p">(</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="p">[</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code><code class="p">]</code><code class="p">)</code><code class="p">,</code> <code class="c1"># valid</code>
    <code class="p">(</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="p">[</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">3</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">4</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">]</code><code class="p">)</code><code class="p">,</code>  <code class="c1"># valid</code>
    <code class="p">(</code><code class="p">[</code><code class="p">]</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="p">[</code><code class="p">]</code><code class="p">)</code><code class="p">,</code> <code class="c1"># valid/empty input</code>
    <code class="p">(</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="p">[</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]</code><code class="p">]</code><code class="p">)</code><code class="p">,</code>   <code class="c1"># boundary input</code>
    <code class="p">(</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="s2">"</code><code class="s2">ValueError</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code> <code class="c1"># invalid (chunk_size &lt;= 0)</code>
    <code class="p">(</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"</code><code class="s2">ValueError</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code> <code class="c1"># invalid (chunk_size &lt;= 0)</code>
    <code class="p">(</code>
        <code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code><code class="p">)</code><code class="p">,</code> <code class="mi">1000</code><code class="p">,</code> <code class="p">[</code><code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">i</code><code class="p">,</code> <code class="n">i</code> <code class="o">+</code> <code class="mi">1000</code><code class="p">)</code><code class="p">)</code> <code class="c1"># huge data</code>
        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">10000</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code><code class="p">]</code>
    <code class="p">)</code>
<code class="p">]</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">test_token_chunking</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">chunk_size</code><code class="p">,</code> <code class="n">expected</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO3-2" id="co_testing_ai_services_CO3-2"><img alt="2" src="assets/2.png"/></a>
    <code class="k">if</code> <code class="n">expected</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">ValueError</code><code class="s2">"</code><code class="p">:</code>
        <code class="k">with</code> <code class="n">pytest</code><code class="o">.</code><code class="n">raises</code><code class="p">(</code><code class="ne">ValueError</code><code class="p">)</code><code class="p">:</code>
            <code class="n">chunk</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">chunk_size</code><code class="p">)</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="k">assert</code> <code class="n">chunk</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">chunk_size</code><code class="p">)</code> <code class="o">==</code> <code class="n">expected</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO3-1" id="callout_testing_ai_services_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use the <code>@pytest.mark.parametrize</code> decorator function to specify multiple test arguments and expected outputs.
The test arguments cover valid, empty, invalid, boundary ranges, and large values to verify the robustness of the token chunking function.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO3-2" id="callout_testing_ai_services_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Inject the test parameters into the test function, and if the expected output is a <code>ValueError</code>, use <code>pytest.raises</code> to verify that a <code>ValueError</code> exception has been raise.
Otherwise, run the assertion check instead.</p></dd>
</dl></div>

<p>You can also store the test data inside JSON files and load them as fixtures to inject into parameterized test functions, as shown in <a data-type="xref" href="#pytest_params_json">Example 11-4</a>.</p>
<div data-type="example" id="pytest_params_json">
<h5><span class="label">Example 11-4. </span>JSON fixtures in parameterized <code>pytest</code> tests</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># tests/rag/test_data.json </code><a class="co" href="#callout_testing_ai_services_CO4-1" id="co_testing_ai_services_CO4-1"><img alt="1" src="assets/1.png"/></a>

<code class="p">[</code>
    <code class="p">{</code><code class="s2">"</code><code class="s2">tokens</code><code class="s2">"</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]</code><code class="p">,</code> <code class="s2">"</code><code class="s2">chunk_size</code><code class="s2">"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"</code><code class="s2">expected</code><code class="s2">"</code><code class="p">:</code> <code class="p">[</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">3</code><code class="p">]</code><code class="p">}</code><code class="p">,</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code>
<code class="p">]</code>

<code class="c1"># tests/rag/transform.py</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">fixture</code>
<code class="k">def</code> <code class="nf">test_data</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s1">'</code><code class="s1">test_data.json</code><code class="s1">'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
        <code class="k">return</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">)</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">parametrize</code><code class="p">(</code><code class="s2">"</code><code class="s2">case</code><code class="s2">"</code><code class="p">,</code> <code class="n">test_data</code><code class="p">(</code><code class="p">)</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">test_token_chunking</code><code class="p">(</code><code class="n">case</code><code class="p">)</code><code class="p">:</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO4-1" id="callout_testing_ai_services_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>The JSON file contains a list of test cases as dictionaries.</p></dd>
</dl></div>

<p>As you can see, fixtures and the parameterization technique are extremely powerful tools to help you verify the robustness of each function in your code.</p>

<p>When writing tests, you’ll probably want to specify setup code, configurations, and global fixtures to be shared across test files.
Luckily, you can achieve this in the 
<span class="keep-together"><code>pytest</code>’s</span> global configuration file called <em>conftest.py</em>.<a data-startref="ix_ch11-asciidoc26" data-type="indexterm" id="id1201"/><a data-startref="ix_ch11-asciidoc25" data-type="indexterm" id="id1202"/><a data-startref="ix_ch11-asciidoc24" data-type="indexterm" id="id1203"/><a data-startref="ix_ch11-asciidoc23" data-type="indexterm" id="id1204"/></p>
</div></section>










<section data-pdf-bookmark="Conftest module" data-type="sect3"><div class="sect3" id="id263">
<h3>Conftest module</h3>

<p><a data-primary="RAG (retrieval augmented generation) module" data-secondary="unit tests" data-tertiary="conftest module" data-type="indexterm" id="id1205"/><a data-primary="unit tests" data-secondary="conftest module" data-type="indexterm" id="id1206"/>If you want your entire test modules to have access to fixtures and global configurations, you can add a  <em>conftest.py</em> module to your <code>tests</code> directory.
Any fixtures, setup code, and configurations defined in the conftest module will be shared with other test modules.
See <a data-type="xref" href="#conftest_fixture">Example 11-5</a>.</p>
<div data-type="example" id="conftest_fixture">
<h5><span class="label">Example 11-5. </span>Add a shared fixture across every module</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># tests/conftest.py</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">fixture</code><code class="p">(</code><code class="n">scope</code><code class="o">=</code><code class="s2">"</code><code class="s2">module</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO5-1" id="co_testing_ai_services_CO5-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">def</code> <code class="nf">tokens</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="k">return</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>

<code class="c1"># tests/rag/transform.py</code>

<code class="k">def</code> <code class="nf">test_chunking</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="c1"># tests/rag/retrieval.py</code>

<code class="k">def</code> <code class="nf">test_query</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="o">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO5-1" id="callout_testing_ai_services_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Define a shared fixture in <em>conftest.py</em> to be used across every test module.
Otherwise, the fixture would be scoped only to a single module.</p></dd>
</dl></div>

<p>You’ve now learned about writing basic tests using the <code>pytest</code> framework following the GWT model.
Next, let’s look at how to perform setup and cleanup operations before and after tests.</p>
</div></section>










<section data-pdf-bookmark="Setup and teardown" data-type="sect3"><div class="sect3" id="id187">
<h3>Setup and teardown</h3>

<p><a data-primary="RAG (retrieval augmented generation) module" data-secondary="unit tests" data-tertiary="setup and teardown" data-type="indexterm" id="ix_ch11-asciidoc27"/><a data-primary="unit tests" data-secondary="setup and teardown" data-type="indexterm" id="ix_ch11-asciidoc28"/>When implementing tests, you may also need to configure a testing environment beforehand and perform teardown or cleanup operations afterward.
You can use the <code>yield</code> keyword in shared fixtures to implement setup and teardown operations that must consistently happen for each test.</p>

<p>For instance, you may need to use this feature when setting up and cleaning up a database session, as demonstrated in <a data-type="xref" href="#setup_teardown">Example 11-6</a>.</p>
<div data-type="example" id="setup_teardown">
<h5><span class="label">Example 11-6. </span>Set up and tear down a database session in a shared fixture</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># tests/conftest.py</code>

<code class="kn">from</code> <code class="nn">qdrant_client</code> <code class="kn">import</code> <code class="n">QdrantClient</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">fixture</code><code class="p">(</code><code class="n">scope</code><code class="o">=</code><code class="s2">"</code><code class="s2">function</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO6-1" id="co_testing_ai_services_CO6-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">def</code> <code class="nf">db_client</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="n">client</code> <code class="o">=</code> <code class="n">QdrantClient</code><code class="p">(</code><code class="n">host</code><code class="o">=</code><code class="s2">"</code><code class="s2">localhost</code><code class="s2">"</code><code class="p">,</code> <code class="n">port</code><code class="o">=</code><code class="mi">6333</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO6-2" id="co_testing_ai_services_CO6-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">client</code><code class="o">.</code><code class="n">create_collection</code><code class="p">(</code> <a class="co" href="#callout_testing_ai_services_CO6-2" id="co_testing_ai_services_CO6-3"><img alt="2" src="assets/2.png"/></a>
        <code class="n">collection_name</code><code class="o">=</code><code class="s2">"</code><code class="s2">test</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">vectors_config</code><code class="o">=</code><code class="n">VectorParams</code><code class="p">(</code><code class="n">size</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">distance</code><code class="o">=</code><code class="n">Distance</code><code class="o">.</code><code class="n">DOT</code><code class="p">)</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="n">client</code><code class="o">.</code><code class="n">upsert</code><code class="p">(</code> <a class="co" href="#callout_testing_ai_services_CO6-2" id="co_testing_ai_services_CO6-4"><img alt="2" src="assets/2.png"/></a>
        <code class="n">collection_name</code><code class="o">=</code><code class="s2">"</code><code class="s2">test</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">points</code><code class="o">=</code><code class="p">[</code>
            <code class="n">PointStruct</code><code class="p">(</code>
                <code class="nb">id</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">vector</code><code class="o">=</code><code class="p">[</code><code class="mf">0.05</code><code class="p">,</code> <code class="mf">0.61</code><code class="p">,</code> <code class="mf">0.76</code><code class="p">,</code> <code class="mf">0.74</code><code class="p">]</code><code class="p">,</code> <code class="n">payload</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">doc</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">test.pdf</code><code class="s2">"</code><code class="p">}</code>
            <code class="p">)</code>
        <code class="p">]</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">yield</code> <code class="n">client</code> <a class="co" href="#callout_testing_ai_services_CO6-3" id="co_testing_ai_services_CO6-5"><img alt="3" src="assets/3.png"/></a>
    <code class="n">client</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO6-4" id="co_testing_ai_services_CO6-6"><img alt="4" src="assets/4.png"/></a>

<code class="c1"># tests/rag/retrieve.py</code>

<code class="k">def</code> <code class="nf">test_search_db</code><code class="p">(</code><code class="n">db_client</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO6-5" id="co_testing_ai_services_CO6-7"><img alt="5" src="assets/5.png"/></a>
    <code class="n">result</code> <code class="o">=</code> <code class="n">db_client</code><code class="o">.</code><code class="n">search</code><code class="p">(</code>
        <code class="n">collection_name</code><code class="o">=</code><code class="s2">"</code><code class="s2">test</code><code class="s2">"</code><code class="p">,</code> <code class="n">query_vector</code><code class="o">=</code><code class="p">[</code><code class="mf">0.18</code><code class="p">,</code> <code class="mf">0.81</code><code class="p">,</code> <code class="mf">0.75</code><code class="p">,</code> <code class="mf">0.12</code><code class="p">]</code><code class="p">,</code> <code class="n">limit</code><code class="o">=</code><code class="mi">1</code>
    <code class="p">)</code>
    <code class="k">assert</code> <code class="n">result</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO6-1" id="callout_testing_ai_services_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create a global shared fixture in <em>conftest.py</em> that is created and destroyed once per test function.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO6-2" id="callout_testing_ai_services_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Instantiate the <code>qdrant</code> database client and then create and configure a test collection with an upserted data point as part of the test setup phase.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO6-5" id="callout_testing_ai_services_CO6-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Yield the database client to the test function as part of the main testing phase.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO6-6" id="callout_testing_ai_services_CO6-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Clean up after each test by closing the client connection to the database.
The teardown code after the <code>yield</code> keyword is executed once the yield operation completes.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO6-7" id="callout_testing_ai_services_CO6-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Inject the preconfigured database client to each test function after the test setup is complete.
Query the database for the document inserted and assert that a data point has been fetched.
Once the assertion step is complete, run the teardown process as part of the <code>db_client</code> fixture function.</p></dd>
</dl></div>

<p>Following the example in <a data-type="xref" href="#setup_teardown">Example 11-6</a>, you can also create fixtures with setup and teardown steps for your API test client or any other external services.</p>

<p>Also, you may have noticed that <a data-type="xref" href="#setup_teardown">Example 11-6</a> used a synchronous client instead of an asynchronous one.
This is because handling asynchronous tests can be tricky, flaky, and error-prone and because it requires installation of additional <code>pytest</code> plug-ins for handling test event loops.</p>

<p>To avoid flaky unit tests, you should use mocks to isolate functional components from external services.
We do this because the scope and testing boundary of unit tests don’t include external dependencies and interfaces.
Instead, testing external dependencies and interfaces such as database interactions will fall within the remit of integration tests.<a data-startref="ix_ch11-asciidoc28" data-type="indexterm" id="id1207"/><a data-startref="ix_ch11-asciidoc27" data-type="indexterm" id="id1208"/></p>

<p>You’ll learn about handling asynchronous tests and mocking/patching techniques next.</p>
</div></section>










<section data-pdf-bookmark="Handling asynchronous tests" data-type="sect3"><div class="sect3" id="id188">
<h3>Handling asynchronous tests</h3>

<p><a data-primary="asynchronous tests" data-type="indexterm" id="ix_ch11-asciidoc29"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="unit tests" data-tertiary="handling asynchronous tests" data-type="indexterm" id="ix_ch11-asciidoc30"/><a data-primary="unit tests" data-secondary="handling asynchronous tests" data-type="indexterm" id="ix_ch11-asciidoc31"/>To execute async tests, you can use plug-ins such as <code>pytest-asyncio</code> to integrate 
<span class="keep-together"><code>pytest</code></span> with Python’s <code>asyncio</code>:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install pytest-asyncio</pre>

<p>Once you install the plug-in, you can follow <a data-type="xref" href="#async_tests">Example 11-7</a> to write and execute an async test.</p>
<div data-type="example" id="async_tests">
<h5><span class="label">Example 11-7. </span>Writing asynchronous tests</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># tests/rag/retrieve.py</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">asyncio</code> <a class="co" href="#callout_testing_ai_services_CO7-1" id="co_testing_ai_services_CO7-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">async</code> <code class="k">def</code> <code class="nf">test_search_db</code><code class="p">(</code><code class="n">async_db_client</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO7-2" id="co_testing_ai_services_CO7-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">result</code> <code class="o">=</code> <code class="k">await</code> <code class="n">async_db_client</code><code class="o">.</code><code class="n">search</code><code class="p">(</code>
        <code class="n">collection_name</code><code class="o">=</code><code class="s2">"</code><code class="s2">test</code><code class="s2">"</code><code class="p">,</code> <code class="n">query_vector</code><code class="o">=</code><code class="p">[</code><code class="mf">0.18</code><code class="p">,</code> <code class="mf">0.81</code><code class="p">,</code> <code class="mf">0.75</code><code class="p">,</code> <code class="mf">0.12</code><code class="p">]</code><code class="p">,</code> <code class="n">limit</code><code class="o">=</code><code class="mi">1</code>
    <code class="p">)</code>
    <code class="k">assert</code> <code class="n">result</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO7-1" id="callout_testing_ai_services_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Explicitly mark the async test with an <code>asyncio</code> decorator to run the test within an event loop.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO7-2" id="callout_testing_ai_services_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>This assumes you’ve replaced the synchronous database client with async one in <em>conftest.py</em>.</p></dd>
</dl></div>

<p>When you run the <code>pytest</code> command, it searches the project directory tree to discover tests using <em>collectors</em> for each level of the directory hierarchy: functions, classes, 
<span class="keep-together">modules,</span> packages, or session.
The <code>pytest-asyncio</code> plug-in provides an 
<span class="keep-together">asyncio event</span> loop for each of these collectors.
By default, tests marked with 
<span class="keep-together"><code>@pytest.mark.asyncio</code></span> run in the event loop provided by the <em>function collector</em>, to narrow the event loop scope and maximize the isolation between tests.</p>

<p>But why is this isolation important?</p>

<p>The biggest source of frustration for developers when testing software is flaky tests.
These are tests that randomly fail when you consecutively run the test suite without changing any code or configurations.</p>

<p><a data-primary="test isolation" data-type="indexterm" id="id1209"/>Often when you investigate the cause of the flaky test behavior, you’ll find that there is a central fixture or dependency that is changed by one of the tests, violating a core principle of testing: <em>test isolation</em>.
The purpose of this isolation is to achieve <em>idempotency</em> in tests where repeated execution would produce the same results every time regardless of how many times you run the test suite.</p>

<p>Without isolation, tests can create side effects on each other, invalidating core assumptions, leading to fluctuations in their outcome/behavior, and random failures.
In addition, interdependent and order-dependent tests often fail together, preventing you from getting valuable feedback on failures.</p>

<p>But how is flaky behavior related to asynchronous tests?</p>

<p>As discussed in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a>, asynchronous code is leveraging Python’s built-in scheduler, an event loop, to switch tasks when faced with a blocking I/O operation.
This task switching in a testing environment can make asynchronous tests challenging to implement correctly because async operations may not complete immediately and can be executed out of order.</p>

<p>Async tests often interface with external dependencies like databases or filesystems executing I/O blocking operations that can take a long time to run.
This is a major issue for unit tests that must run very quickly so that you can execute them frequently.</p>

<p>Unlike synchronous code where operations are executed in a predictable and linear sequence, async code also introduces variability in timing, execution order, and fixture state, reducing the consistency of the outcomes across tests.
Additionally, response times from external dependencies can fluctuate, leading to side effects that violate the test isolation principle.</p>

<p>To mitigate the risk of side effects and flaky behavior, you’ll need to correctly handle async tests by:</p>

<ul>
<li>
<p>Awaiting blocking I/O operations</p>
</li>
<li>
<p>Avoiding unintentional use of blocking synchronous I/O operations inside async tests</p>
</li>
<li>
<p>Using correct timeouts for managing delays</p>
</li>
<li>
<p>Explicitly controlling the sequence of operations, especially when running async tests in parallel</p>
</li>
</ul>

<p>Perhaps, the best mitigation is to write synchronous tests by mocking external dependencies, which will decouple your functions from I/O blocking dependencies.
Using mocks, you can then run fast and reliable tests without having to wait for I/O operations to complete in the order you need.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Async tests can still be useful with real dependencies when locally testing a replicated production environment.<a data-startref="ix_ch11-asciidoc31" data-type="indexterm" id="id1210"/><a data-startref="ix_ch11-asciidoc30" data-type="indexterm" id="id1211"/><a data-startref="ix_ch11-asciidoc29" data-type="indexterm" id="id1212"/></p>
</div>

<p>Next, let’s see how to mock external dependencies in unit tests so that you can write synchronous tests in replacement of slow async ones.</p>
</div></section>










<section data-pdf-bookmark="Mocking and patching" data-type="sect3"><div class="sect3" id="id189">
<h3>Mocking and patching</h3>

<p><a data-primary="mocking and patching" data-type="indexterm" id="ix_ch11-asciidoc32"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="unit tests" data-tertiary="mocking and patching" data-type="indexterm" id="ix_ch11-asciidoc33"/><a data-primary="unit tests" data-secondary="mocking and patching" data-type="indexterm" id="ix_ch11-asciidoc34"/>When writing unit tests, you need to isolate your components from external dependencies to avoid slow-running tests and consuming unnecessary resources.
For instance, you don’t want to call your GenAI model every time you run the test suite, which is going to be frequent, as that’ll be compute-intensive and possibly expensive.</p>

<p>Instead, you can use <em>test doubles</em> to simulate real dependencies in your unit tests without having to rely on external dependencies in your tests.
In essence, they pretend to be the real thing, just like stunt doubles in action movies that pretend to be the main actors.
Isolated unit tests that use test doubles can verify the component state changes or behavior as it interacts with external dependencies like an LLM API.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Be careful not to replace any component behavior you’re trying to test with test doubles.</p>

<p>For example, if you have a <code>ChatBot</code> class that uses an LLM API and performs content filtering on the responses, replace only the LLM API calls with test doubles, not the content filtering logic.
Otherwise, you’ll be testing your own test double.</p>
</div>

<p>There are five types of test doubles that you can use in your unit tests, as shown in <a data-type="xref" href="#test_doubles">Figure 11-9</a>.</p>

<figure><div class="figure" id="test_doubles">
<img alt="bgai 1109" src="assets/bgai_1109.png"/>
<h6><span class="label">Figure 11-9. </span>Test doubles</h6>
</div></figure>

<p>These include the following:</p>
<dl>
<dt>Fake</dt>
<dd>
<p>A simplified implementation of a dependency for testing purposes</p>
</dd>
<dt>Dummy</dt>
<dd>
<p>A placeholder used for when an argument needs to be filled in</p>
</dd>
<dt>Stub</dt>
<dd>
<p>Provides fake data to the system under test that is using it</p>
</dd>
<dt>Spy</dt>
<dd>
<p>Keeps track of dependency usage for later verification</p>
</dd>
<dt>Mock</dt>
<dd>
<p>Checks how the dependency will be used and causes failure if the expectation isn’t met</p>
</dd>
</dl>

<p>Except mocks that verify component behavior, the rest of these doubles can be used to verify state changes.
Mocks have an entirely different setup and verification logic but work exactly like the other doubles in making the component being tested believe that it’s interacting with the real dependencies.</p>

<p>Let’s see each double in action to understand their similarities and differences.</p>












<section data-pdf-bookmark="Fakes" data-type="sect4"><div class="sect4" id="id190">
<h4>Fakes</h4>

<p><a data-primary="fakes" data-type="indexterm" id="id1213"/><a data-primary="mocking and patching" data-secondary="fakes" data-type="indexterm" id="id1214"/><a data-primary="test doubles" data-secondary="fakes" data-type="indexterm" id="id1215"/><a data-primary="unit tests" data-secondary="mocking and patching" data-tertiary="fakes" data-type="indexterm" id="id1216"/><em>Fake</em> objects are fully functional but simplified versions of the real dependency, possibly taking shortcuts.
An example would be a database client that uses an in-memory database during tests instead of an actual database server; or an LLM client that fetches cached responses from a local testing server instead of an actual LLM.</p>

<p><a data-type="xref" href="#test_fakes">Example 11-8</a> demonstrates what a fake LLM client looks like.</p>
<div data-type="example" id="test_fakes">
<h5><span class="label">Example 11-8. </span>Fake test double</h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">FakeLLMClient</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO8-1" id="co_testing_ai_services_CO8-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">cache</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">invoke</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query</code><code class="p">)</code><code class="p">:</code>
        <code class="k">if</code> <code class="n">query</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">cache</code><code class="p">:</code>
            <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">cache</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">query</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO8-2" id="co_testing_ai_services_CO8-2"><img alt="2" src="assets/2.png"/></a>

        <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">http://localhost:8001</code><code class="s2">"</code><code class="p">,</code> <code class="n">json</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">query</code><code class="s2">"</code><code class="p">:</code> <code class="n">query</code><code class="p">}</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">response</code><code class="o">.</code><code class="n">status_code</code> <code class="o">!=</code> <code class="mi">200</code><code class="p">:</code>
            <code class="k">return</code> <code class="s2">"</code><code class="s2">Error fetching result</code><code class="s2">"</code>

        <code class="n">result</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">response</code><code class="s2">"</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">cache</code><code class="p">[</code><code class="n">query</code><code class="p">]</code> <code class="o">=</code> <code class="n">result</code>
        <code class="k">return</code> <code class="n">result</code>

<code class="k">def</code> <code class="nf">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">,</code> <code class="n">token</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">token</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO8-1" id="co_testing_ai_services_CO8-3"><img alt="1" src="assets/1.png"/></a>
    <code class="k">return</code> <code class="n">response</code>

<code class="k">def</code> <code class="nf">test_fake_llm_client</code><code class="p">(</code><code class="n">query</code><code class="p">)</code><code class="p">:</code>
    <code class="n">llm_client</code> <code class="o">=</code> <code class="n">FakeLLMClient</code><code class="p">(</code><code class="p">)</code>
    <code class="n">query</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">some query</code><code class="s2">"</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">,</code> <code class="n">token</code><code class="o">=</code><code class="s2">"</code><code class="s2">fake_token</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">assert</code> <code class="n">response</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">some response</code><code class="s2">"</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO8-1" id="callout_testing_ai_services_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>A fully functional and simplified version LLM client that mimics the behavior of the real one by interacting with a local testing server.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO8-2" id="callout_testing_ai_services_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Return cached responses if repeated prompts are used.</p></dd>
</dl></div>
</div></section>












<section data-pdf-bookmark="Dummies" data-type="sect4"><div class="sect4" id="id191">
<h4>Dummies</h4>

<p><a data-primary="dummies" data-type="indexterm" id="id1217"/><a data-primary="mocking and patching" data-secondary="dummies" data-type="indexterm" id="id1218"/><a data-primary="test doubles" data-secondary="dummies" data-type="indexterm" id="id1219"/><a data-primary="unit tests" data-secondary="mocking and patching" data-tertiary="dummies" data-type="indexterm" id="id1220"/><em>Dummies</em> are objects that aren’t used in tests, but you pass around to satisfy parameter requirements of functions.
An example would be passing a fake authentication token to an API client to prevent errors, even though the token isn’t used for authentication during the test.</p>

<p><a data-type="xref" href="#test_dummies">Example 11-9</a> shows how dummies can be used as test doubles.</p>
<div data-type="example" id="test_dummies">
<h5><span class="label">Example 11-9. </span>Dummy test double</h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">DummyLLMClient</code><code class="p">:</code>
    <code class="k">def</code> <code class="nf">invoke</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="n">token</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO9-1" id="co_testing_ai_services_CO9-1"><img alt="1" src="assets/1.png"/></a>
        <code class="k">return</code> <code class="s2">"</code><code class="s2">some response</code><code class="s2">"</code>

<code class="k">def</code> <code class="nf">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">,</code> <code class="n">token</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">token</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO9-1" id="co_testing_ai_services_CO9-2"><img alt="1" src="assets/1.png"/></a>
    <code class="k">return</code> <code class="n">response</code>

<code class="k">def</code> <code class="nf">test_dummy_llm_client</code><code class="p">(</code><code class="n">query</code><code class="p">)</code><code class="p">:</code>
    <code class="n">llm_client</code> <code class="o">=</code> <code class="n">DummyLLMClient</code><code class="p">(</code><code class="p">)</code>
    <code class="n">query</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">some query</code><code class="s2">"</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">,</code> <code class="n">token</code><code class="o">=</code><code class="s2">"</code><code class="s2">fake_token</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">assert</code> <code class="n">response</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">some response</code><code class="s2">"</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO9-1" id="callout_testing_ai_services_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Notice the <code>token</code> is not being used but is required to satisfy the <code>.invoke(query, token)</code> function signature.</p></dd>
</dl></div>
</div></section>












<section data-pdf-bookmark="Stubs" data-type="sect4"><div class="sect4" id="id192">
<h4>Stubs</h4>

<p><a data-primary="mocking and patching" data-secondary="stubs" data-type="indexterm" id="id1221"/><a data-primary="stubs" data-type="indexterm" id="id1222"/><a data-primary="test doubles" data-secondary="stubs" data-type="indexterm" id="id1223"/><a data-primary="unit tests" data-secondary="mocking and patching" data-tertiary="stubs" data-type="indexterm" id="id1224"/><em>Stubs</em> are simplified versions of fakes.
They don’t have fully functional implementations and instead return canned responses to method calls.
As an example, a stub LLM client will return a predefined fixture string when called without making any actual model requests.</p>

<p><a data-type="xref" href="#test_stubs">Example 11-10</a> shows what a stub looks like.
Can you spot the differences when comparing this example with <a data-type="xref" href="#test_fakes">Example 11-8</a>?</p>
<div data-type="example" id="test_stubs">
<h5><span class="label">Example 11-10. </span>Stub test double</h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">StubLLMClient</code><code class="p">:</code>
    <code class="k">def</code> <code class="nf">invoke</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query</code><code class="p">)</code><code class="p">:</code>
        <code class="k">if</code> <code class="n">query</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">specific query</code><code class="s2">"</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO10-1" id="co_testing_ai_services_CO10-1"><img alt="1" src="assets/1.png"/></a>
            <code class="k">return</code> <code class="s2">"</code><code class="s2">specific response</code><code class="s2">"</code>
        <code class="k">return</code> <code class="s2">"</code><code class="s2">default response</code><code class="s2">"</code>

<code class="k">def</code> <code class="nf">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">query</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">response</code>

<code class="k">def</code> <code class="nf">test_stub_llm_client</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="n">llm_client</code> <code class="o">=</code> <code class="n">StubLLMClient</code><code class="p">(</code><code class="p">)</code>
    <code class="n">query</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">specific query</code><code class="s2">"</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code>
    <code class="k">assert</code> <code class="n">response</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">specific response</code><code class="s2">"</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO10-1" id="callout_testing_ai_services_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Return a canned response on a given condition.</p></dd>
</dl></div>
</div></section>












<section data-pdf-bookmark="Spies" data-type="sect4"><div class="sect4" id="id193">
<h4>Spies</h4>

<p><a data-primary="mocking and patching" data-secondary="spies" data-type="indexterm" id="id1225"/><a data-primary="spies" data-type="indexterm" id="id1226"/><a data-primary="test doubles" data-secondary="spies" data-type="indexterm" id="id1227"/><a data-primary="unit tests" data-secondary="mocking and patching" data-tertiary="spies" data-type="indexterm" id="id1228"/><em>Spies</em> are like stubs but also record method calls and interactions.
They’re extremely useful when you need to verify how a complex component interacts with a dependency.
For example, with a spy LLM client, you can verify the number of times it was invoked by the component under test.</p>

<p><a data-type="xref" href="#test_spies">Example 11-11</a> shows a spy test double in action.</p>
<div data-type="example" id="test_spies">
<h5><span class="label">Example 11-11. </span>Spy test double</h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">SpyLLMClient</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">call_count</code> <code class="o">=</code> <code class="mi">0</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">calls</code> <code class="o">=</code> <code class="p">[</code><code class="p">]</code>

    <code class="k">def</code> <code class="nf">invoke</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query</code><code class="p">)</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">call_count</code> <code class="o">+</code><code class="o">=</code> <code class="mi">1</code> <a class="co" href="#callout_testing_ai_services_CO11-1" id="co_testing_ai_services_CO11-1"><img alt="1" src="assets/1.png"/></a>
        <code class="bp">self</code><code class="o">.</code><code class="n">calls</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">(</code><code class="n">query</code><code class="p">)</code><code class="p">)</code>
        <code class="k">return</code> <code class="s2">"</code><code class="s2">some response</code><code class="s2">"</code>

<code class="k">def</code> <code class="nf">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">query</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">response</code>

<code class="k">def</code> <code class="nf">test_process_query_with_spy</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="n">llm_client</code> <code class="o">=</code> <code class="n">SpyLLMClient</code><code class="p">(</code><code class="p">)</code>
    <code class="n">query</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">some query</code><code class="s2">"</code>

    <code class="n">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code>

    <code class="k">assert</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">call_count</code> <code class="o">==</code> <code class="mi">1</code>
    <code class="k">assert</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">calls</code> <code class="o">==</code> <code class="p">[</code><code class="p">(</code><code class="s2">"</code><code class="s2">some query</code><code class="s2">"</code><code class="p">)</code><code class="p">]</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO11-1" id="callout_testing_ai_services_CO11-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Keep track of function calls and arguments passed in.</p></dd>
</dl></div>
</div></section>












<section data-pdf-bookmark="Mocks" data-type="sect4"><div class="sect4" id="id264">
<h4>Mocks</h4>

<p><a data-primary="mocking and patching" data-secondary="mocks" data-type="indexterm" id="id1229"/><a data-primary="test doubles" data-secondary="mocks" data-type="indexterm" id="id1230"/><a data-primary="unit tests" data-secondary="mocking and patching" data-tertiary="mocks" data-type="indexterm" id="id1231"/>A mock is a smarter stub.
If you know in advance how many times a dependency is called and how (i.e., you have expectations of interaction), you can implement a <em>mock</em>.
A mock can verify whether the dependency has been called correctly with the right parameters to confirm the component being tested has behaved correctly.</p>

<p>How you set up mocks and perform checks with them is different to other doubles, as you can see in <a data-type="xref" href="#test_mocks">Example 11-12</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For this example, you need to install the <code>pytest-mocks</code> plug-in, which is a thin wrapper over Python’s built-in mocks library to simplify mocking implementation. Use the following command to install <code>pytest-mocks</code>:</p>

<pre data-type="programlisting">$ pip install pytest-mock</pre>
</div>
<div data-type="example" id="test_mocks">
<h5><span class="label">Example 11-12. </span>Mock test double</h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">query</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">response</code>

<code class="k">def</code> <code class="nf">test_process_query_with_mock</code><code class="p">(</code><code class="n">mocker</code><code class="p">)</code><code class="p">:</code>
    <code class="n">llm_client</code> <code class="o">=</code> <code class="n">mocker</code><code class="o">.</code><code class="n">Mock</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO12-1" id="co_testing_ai_services_CO12-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="o">.</code><code class="n">return_value</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">mock response</code><code class="s2">"</code>
    <code class="n">query</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">some query</code><code class="s2">"</code>

    <code class="n">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code>
    <code class="n">process_query</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code>

    <code class="k">assert</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="o">.</code><code class="n">call_count</code> <code class="o">==</code> <code class="mi">2</code>
    <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="o">.</code><code class="n">assert_any_call</code><code class="p">(</code><code class="s2">"</code><code class="s2">some query</code><code class="s2">"</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO12-1" id="callout_testing_ai_services_CO12-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create and configure a mock to act as an LLM client tracking function calls and arguments passed in.
It will also return any canned responses we want.</p></dd>
</dl></div>

<p>Using mocks, as shown in <a data-type="xref" href="#test_mocks">Example 11-12</a>, will be useful to check the mocked component’s behavior in highly nested and complex application logic such as checking how a mocked function like <code>llm_client.invoke()</code> is called several levels deep within a higher order function such as
<code>process_query()</code>.</p>

<p>Now that you’re more familiar with implementing various test doubles from scratch, let’s see how an external package such as <code>pytest-mock</code> can simplify using test doubles.</p>
</div></section>












<section data-pdf-bookmark="Implementing test doubles with pytest-mock" data-type="sect4"><div class="sect4" id="id194">
<h4>Implementing test doubles with pytest-mock</h4>

<p><a data-primary="mocking and patching" data-secondary="implementing test doubles with pytest-mock" data-type="indexterm" id="ix_ch11-asciidoc35"/><a data-primary="pytest-mock" data-type="indexterm" id="ix_ch11-asciidoc36"/><a data-primary="test doubles" data-secondary="implementing test doubles with pytest-mock" data-type="indexterm" id="ix_ch11-asciidoc37"/><a data-primary="unit tests" data-secondary="mocking and patching" data-tertiary="implementing test doubles with pytest-mock" data-type="indexterm" id="ix_ch11-asciidoc38"/>The five aforementioned test doubles can also be implemented with <code>pytest-mock</code>,
as shown in
<a data-type="xref" href="#test_double_pytest_mock">Example 11-13</a>.</p>
<div data-type="example" id="test_double_pytest_mock">
<h5><span class="label">Example 11-13. </span>Test doubles with <code>pytest-mock</code></h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">LLMClient</code><code class="p">:</code>
    <code class="k">def</code> <code class="nf">invoke</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">query</code><code class="p">)</code><code class="p">:</code>
        <code class="k">return</code> <code class="n">openai</code><code class="o">.</code><code class="n">ChatCompletion</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
            <code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">gpt-4o</code><code class="s2">"</code><code class="p">,</code> <code class="n">messages</code><code class="o">=</code><code class="p">[</code><code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">query</code><code class="p">}</code><code class="p">]</code>
        <code class="p">)</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">fixture</code>
<code class="k">def</code> <code class="nf">llm_client</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">LLMClient</code><code class="p">(</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">test_fake</code><code class="p">(</code><code class="n">mocker</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="k">class</code> <code class="nc">FakeOpenAIClient</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO13-1" id="co_testing_ai_services_CO13-1"><img alt="1" src="assets/1.png"/></a>
        <code class="nd">@staticmethod</code>
        <code class="k">def</code> <code class="nf">invoke</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">query</code><code class="p">)</code><code class="p">:</code>
            <code class="k">return</code> <code class="p">{</code><code class="s2">"</code><code class="s2">choices</code><code class="s2">"</code><code class="p">:</code> <code class="p">[</code><code class="p">{</code><code class="s2">"</code><code class="s2">message</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">fake response</code><code class="s2">"</code><code class="p">}</code><code class="p">}</code><code class="p">]</code><code class="p">}</code>

    <code class="n">mocker</code><code class="o">.</code><code class="n">patch</code><code class="p">(</code><code class="n">openai</code><code class="o">.</code><code class="n">ChatCompletion</code><code class="p">,</code> <code class="n">new</code><code class="o">=</code><code class="n">FakeOpenAIClient</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO13-1" id="co_testing_ai_services_CO13-2"><img alt="1" src="assets/1.png"/></a>
    <code class="n">result</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"</code><code class="s2">test query</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">assert</code> <code class="n">result</code> <code class="o">==</code> <code class="p">{</code><code class="s2">"</code><code class="s2">choices</code><code class="s2">"</code><code class="p">:</code> <code class="p">[</code><code class="p">{</code><code class="s2">"</code><code class="s2">message</code><code class="s2">"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">fake response</code><code class="s2">"</code><code class="p">}</code><code class="p">}</code><code class="p">]</code><code class="p">}</code>

<code class="k">def</code> <code class="nf">test_stub</code><code class="p">(</code><code class="n">mocker</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">stub</code> <code class="o">=</code> <code class="n">mocker</code><code class="o">.</code><code class="n">Mock</code><code class="p">(</code><code class="p">)</code>
    <code class="n">stub</code><code class="o">.</code><code class="n">process</code><code class="o">.</code><code class="n">return_value</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">stubbed response</code><code class="s2">"</code>
    <code class="n">result</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">stub</code><code class="p">)</code>
    <code class="k">assert</code> <code class="n">result</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">stubbed response</code><code class="s2">"</code>  <a class="co" href="#callout_testing_ai_services_CO13-2" id="co_testing_ai_services_CO13-3"><img alt="2" src="assets/2.png"/></a>

<code class="k">def</code> <code class="nf">test_spy</code><code class="p">(</code><code class="n">mocker</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">spy</code> <code class="o">=</code> <code class="n">mocker</code><code class="o">.</code><code class="n">spy</code><code class="p">(</code><code class="n">LLMClient</code><code class="p">,</code> <code class="s1">'</code><code class="s1">send_request</code><code class="s1">'</code><code class="p">)</code>
    <code class="n">spy</code><code class="o">.</code><code class="n">return_value</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">some_value</code><code class="s2">"</code>
    <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"</code><code class="s2">some query</code><code class="s2">"</code><code class="p">)</code>
    <code class="n">spy</code><code class="o">.</code><code class="n">call_count</code> <code class="o">==</code> <code class="mi">1</code>  <a class="co" href="#callout_testing_ai_services_CO13-3" id="co_testing_ai_services_CO13-4"><img alt="3" src="assets/3.png"/></a>

<code class="k">def</code> <code class="nf">test_mock</code><code class="p">(</code><code class="n">mocker</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">mock</code> <code class="o">=</code> <code class="n">mocker</code><code class="o">.</code><code class="n">Mock</code><code class="p">(</code><code class="p">)</code>
    <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">mock</code><code class="p">)</code>
    <code class="n">mock</code><code class="o">.</code><code class="n">process</code><code class="o">.</code><code class="n">assert_called_once_with</code><code class="p">(</code><code class="s2">"</code><code class="s2">some query</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO13-4" id="co_testing_ai_services_CO13-5"><img alt="4" src="assets/4.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO13-1" id="callout_testing_ai_services_CO13-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Fake simulates real behavior.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO13-3" id="callout_testing_ai_services_CO13-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Stub returns a fixed value.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO13-4" id="callout_testing_ai_services_CO13-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Spy tracks calls.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO13-5" id="callout_testing_ai_services_CO13-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Checks dependency behavior and fails if expectation isn’t met.</p></dd>
</dl></div>

<p>You can now use any of the mentioned test doubles to isolate your unit tests, run them faster while covering various hard-to-test edge cases, and bypass dependencies that introduce nondeterminism in your tests.
But bear in mind, having too many mocked tests is an anti-pattern as these tests may give you false confidence and become a maintenance overhead due to their brittleness.
They can also mask integration issues since they’re not testing real dependencies and overusing them can lead to complex test implementations that are hard to understand.</p>

<p>In such cases, you should rely on simple mocks only when necessary and in unit tests.
Furthermore, you should complement any mocked tests with integration tests that use real dependencies to avoid missing any production issues<a data-startref="ix_ch11-asciidoc38" data-type="indexterm" id="id1232"/><a data-startref="ix_ch11-asciidoc37" data-type="indexterm" id="id1233"/><a data-startref="ix_ch11-asciidoc36" data-type="indexterm" id="id1234"/><a data-startref="ix_ch11-asciidoc35" data-type="indexterm" id="id1235"/>.<a data-startref="ix_ch11-asciidoc34" data-type="indexterm" id="id1236"/><a data-startref="ix_ch11-asciidoc33" data-type="indexterm" id="id1237"/><a data-startref="ix_ch11-asciidoc32" data-type="indexterm" id="id1238"/></p>

<p>In the next section, you’ll learn how to implement an integration test for your RAG pipeline.<a data-startref="ix_ch11-asciidoc17" data-type="indexterm" id="id1239"/><a data-startref="ix_ch11-asciidoc16" data-type="indexterm" id="id1240"/></p>
</div></section>
</div></section>
</div></section>








<section data-pdf-bookmark="Integration Testing" data-type="sect2"><div class="sect2" id="id195">
<h2>Integration Testing</h2>

<p><a data-primary="integration tests" data-type="indexterm" id="ix_ch11-asciidoc39"/><a data-primary="testing AI services" data-secondary="project: implementing tests for a RAG system" data-tertiary="integration testing" data-type="indexterm" id="ix_ch11-asciidoc40"/>Up until this point, you’ve been practicing with writing isolated unit tests.
But as soon as you want to test a component interacting with another component or a real dependency, you’ll be implementing integration tests.</p>

<p>The testing boundary of integration tests should include two components, and the scope of these tests must focus on checking the functionality of their interface and the communication contract between them.</p>

<p>As an example, you can see potential integration tests you can implement for your RAG pipeline in <a data-type="xref" href="#integration_boundaries">Figure 11-10</a>.</p>

<figure><div class="figure" id="integration_boundaries">
<img alt="bgai 1110" src="assets/bgai_1110.png"/>
<h6><span class="label">Figure 11-10. </span>Integration test boundaries visualized on the RAG data pipeline diagram</h6>
</div></figure>

<p>Compared to unit test boundaries you saw in <a data-type="xref" href="#unit_boundaries">Figure 11-8</a>, you can see that each integration is only concerned about verifying the interaction behavior between two or maximum three components at a time.</p>

<p>To practice, we’ll implement an integration test for the document retrieval interface with the vector database in the RAG pipeline.
In this test, you’ll be querying the real vector database to verify whether the relevant documents are being fetched in relation to the query.</p>

<p>The tests will leverage RAG-related retrieval metrics such as context precision and context recall to measure the effectiveness of the retrieval system with the real vector database.</p>










<section data-pdf-bookmark="Context precision and recall" data-type="sect3"><div class="sect3" id="id196">
<h3>Context precision and recall</h3>

<p><a data-primary="context precision" data-type="indexterm" id="ix_ch11-asciidoc41"/><a data-primary="context recall" data-type="indexterm" id="ix_ch11-asciidoc42"/><a data-primary="integration tests" data-secondary="context precision and recall" data-type="indexterm" id="ix_ch11-asciidoc43"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="integration testing" data-tertiary="context precision and recall" data-type="indexterm" id="ix_ch11-asciidoc44"/>Both context precision and recall are evaluation metrics specifically designed for measuring the quality of the document retrieval system from the vector database in a RAG pipeline.</p>

<p>While <em>context precision</em> focuses on the signal-to-noise ratio (i.e., quality), of the retrieved information from the vector database, <em>context recall</em> measures whether all information relevant for responding to the user query has been retrieved from the database.</p>

<p class="less_space pagebreak-before">You can calculate context precision and recall using
<a data-type="xref" href="#context_precision_recall">Example 11-14</a>.</p>
<div data-type="example" id="context_precision_recall">
<h5><span class="label">Example 11-14. </span>Calculating context precision and recall</h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">calculate_recall</code><code class="p">(</code><code class="n">expected</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code><code class="p">,</code> <code class="n">retrieved</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">int</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO14-1" id="co_testing_ai_services_CO14-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">true_positives</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="nb">set</code><code class="p">(</code><code class="n">expected</code><code class="p">)</code> <code class="o">&amp;</code> <code class="nb">set</code><code class="p">(</code><code class="n">retrieved</code><code class="p">)</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">true_positives</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">expected</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">calculate_precision</code><code class="p">(</code><code class="n">expected</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code><code class="p">,</code> <code class="n">retrieved</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">int</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO14-2" id="co_testing_ai_services_CO14-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">true_positives</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="nb">set</code><code class="p">(</code><code class="n">expected</code><code class="p">)</code> <code class="o">&amp;</code> <code class="nb">set</code><code class="p">(</code><code class="n">retrieved</code><code class="p">)</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">true_positives</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">retrieved</code><code class="p">)</code>

<code class="n">expected_document_ids</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
<code class="n">retrieved_documents_ids</code> <code class="o">=</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">7</code><code class="p">]</code>

<code class="n">recall</code> <code class="o">=</code> <code class="n">calculate_recall</code><code class="p">(</code><code class="n">expected_document_ids</code><code class="p">,</code> <code class="n">retrieved_documents_ids</code><code class="p">)</code>
<code class="n">precision</code> <code class="o">=</code> <code class="n">calculate_precision</code><code class="p">(</code><code class="n">expected_document_ids</code><code class="p">,</code> <code class="n">retrieved_documents_ids</code><code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Recall: </code><code class="si">{</code><code class="n">recall</code><code class="si">:</code><code class="s2">.2f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code> <code class="c1"># Recall: 0.40</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Precision: </code><code class="si">{</code><code class="n">precision</code><code class="si">:</code><code class="s2">.2f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code> <code class="c1"># Precision: 0.50</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO14-1" id="callout_testing_ai_services_CO14-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Count of correct documents retrieved/count of expected documents.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO14-2" id="callout_testing_ai_services_CO14-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Count of correct documents retrieved/count of retrieved documents.</p></dd>
</dl></div>

<p>Here, we’re assuming that each document will contain all the relevant contextual information to respond to a given query, so asserting that certain documents are fetched will suffice.</p>

<p>Open source libraries such as <code>deep-eval</code> and <code>ragas</code> can help you automate the computation of these metrics considering the relevant context is scattered across documents, but for simplicity, we will be implementing our integration tests without relying on such libraries.</p>

<p>With precision and recall metrics, the integration test for the document retrieval system will look like <a data-type="xref" href="#integration_retrieval">Example 11-15</a>.</p>
<div data-type="example" id="integration_retrieval">
<h5><span class="label">Example 11-15. </span>Document retrieval system integration test</h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">parametrize</code><code class="p">(</code><code class="s2">"</code><code class="s2">query_vector, expected_ids</code><code class="s2">"</code><code class="p">,</code> <code class="p">[</code> <a class="co" href="#callout_testing_ai_services_CO15-1" id="co_testing_ai_services_CO15-1"><img alt="1" src="assets/1.png"/></a>
    <code class="p">(</code><code class="p">[</code><code class="mf">0.1</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">,</code> <code class="mf">0.4</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]</code><code class="p">)</code><code class="p">,</code>
    <code class="p">(</code><code class="p">[</code><code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">,</code> <code class="mf">0.4</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">]</code><code class="p">)</code><code class="p">,</code>
    <code class="p">(</code><code class="p">[</code><code class="mf">0.3</code><code class="p">,</code> <code class="mf">0.4</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.6</code><code class="p">]</code><code class="p">,</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code><code class="p">)</code><code class="p">,</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code>
<code class="p">]</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">test_retrieval_subsystem</code><code class="p">(</code><code class="n">db_client</code><code class="p">,</code> <code class="n">query_vector</code><code class="p">,</code> <code class="n">expected_ids</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO15-2" id="co_testing_ai_services_CO15-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">response</code> <code class="o">=</code> <code class="n">db_client</code><code class="o">.</code><code class="n">search</code><code class="p">(</code> <a class="co" href="#callout_testing_ai_services_CO15-2" id="co_testing_ai_services_CO15-3"><img alt="2" src="assets/2.png"/></a>
        <code class="n">collection_name</code><code class="o">=</code><code class="s2">"</code><code class="s2">test</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">query_vector</code><code class="o">=</code><code class="n">query_vector</code><code class="p">,</code>
        <code class="n">limit</code><code class="o">=</code><code class="mi">3</code>
    <code class="p">)</code>

    <code class="n">retrieved_ids</code> <code class="o">=</code> <code class="p">[</code><code class="n">point</code><code class="o">.</code><code class="n">id</code> <code class="k">for</code> <code class="n">point</code> <code class="ow">in</code> <code class="n">response</code><code class="p">]</code>
    <code class="n">recall</code> <code class="o">=</code> <code class="n">calculate_recall</code><code class="p">(</code><code class="n">expected_ids</code><code class="p">,</code> <code class="n">retrieved_ids</code><code class="p">)</code>
    <code class="n">precision</code> <code class="o">=</code> <code class="n">calculate_precision</code><code class="p">(</code><code class="n">expected_ids</code><code class="p">,</code> <code class="n">retrieved_ids</code><code class="p">)</code>

    <code class="k">assert</code> <code class="n">recall</code> <code class="o">&gt;</code><code class="o">=</code> <code class="mf">0.66</code> <a class="co" href="#callout_testing_ai_services_CO15-3" id="co_testing_ai_services_CO15-4"><img alt="3" src="assets/3.png"/></a>
    <code class="k">assert</code> <code class="n">precision</code> <code class="o">&gt;</code><code class="o">=</code> <code class="mf">0.66</code> <a class="co" href="#callout_testing_ai_services_CO15-3" id="co_testing_ai_services_CO15-5"><img alt="3" src="assets/3.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO15-1" id="callout_testing_ai_services_CO15-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Specify parameterized test data that covers various cases.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO15-2" id="callout_testing_ai_services_CO15-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Set up the qdrant client within <em>conftest.py</em> with correct setup and teardown implementation to start the test with a prepopulated database of documents.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO15-4" id="callout_testing_ai_services_CO15-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Calculate the precision and recall for each test case to ensure they’re above a reasonable threshold.
There is often a trade-off between precision and recall metrics, so choose sensible thresholds based on your own use case.</p></dd>
</dl></div>

<p>While <a data-type="xref" href="#integration_retrieval">Example 11-15</a> checks the document retrieval system, you can also implement an integration test for your generation subsystem using the LLM.
However, bear in mind that due to the nondeterministic nature of the LLM, it can be challenging to ensure comprehensive test coverage and consistency in outcomes.</p>

<p>If you’ve prompted your LLM to return JSON responses, you can write integration tests with equality assertions to verify the structure and value of the JSON responses.
An example where you may follow this approach is in <em>function calling</em> or <em>agentic workflows</em> where the LLM has to select the right tool or specialized LLM agent to use for addressing the query.</p>

<p>A test such as this could look like <a data-type="xref" href="#integration_llm_json">Example 11-16</a>.</p>
<div data-type="example" id="integration_llm_json">
<h5><span class="label">Example 11-16. </span>LLM JSON response generation system integration test</h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">parametrize</code><code class="p">(</code><code class="s2">"</code><code class="s2">user_query, expected_tool</code><code class="s2">"</code><code class="p">,</code> <code class="p">[</code>
    <code class="p">(</code><code class="s2">"</code><code class="s2">Summarize the employee onboarding process</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">SUMMARIZER</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code>
    <code class="p">(</code><code class="s2">"</code><code class="s2">What is this page about? https://...</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">WEBSEARCH</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code>
    <code class="p">(</code><code class="s2">"</code><code class="s2">Analyze the 2024 annual accounts</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">ANALYZER</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code> <code class="c1"># Add 100 different cases with a balanced category distribution</code>
<code class="p">]</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO16-1" id="co_testing_ai_services_CO16-1"><img alt="1" src="assets/1.png"/></a>
<code class="k">def</code> <code class="nf">test_llm_tool_selection_response</code><code class="p">(</code><code class="n">user_query</code><code class="p">,</code> <code class="n">expected_tool</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">user_query</code><code class="p">,</code> <code class="n">response_type</code><code class="o">=</code><code class="s2">"</code><code class="s2">json</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">assert</code> <code class="n">response</code><code class="p">[</code><code class="s2">"</code><code class="s2">selected_tool</code><code class="s2">"</code><code class="p">]</code> <code class="o">==</code> <code class="n">expected_tool</code>
    <code class="k">assert</code> <code class="n">response</code><code class="p">[</code><code class="s2">"</code><code class="s2">message</code><code class="s2">"</code><code class="p">]</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO16-1" id="callout_testing_ai_services_CO16-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Iterate over various test cases covering various user queries.
Ensure test cases cover a balanced distribution of categories.</p></dd>
</dl></div>

<p>Given the model may make mistakes and you may not be able to test every possible case, you’ll want to run this test enough times (maybe up to 100 times) to visualize the distribution patterns in LLM responses.
This should give you more confidence in how your LLM is selecting the right tool or agent for a given user query.</p>

<p>If your models are returning structured outputs, integration tests can be straightforward to implement, as you saw in <a data-type="xref" href="#integration_llm_json">Example 11-16</a>.
However, what if your GenAI models are responding with more dynamic content such as natural text?
How can you test the quality of your model responses then?</p>

<p>In this case, you can measure properties and metrics related to the model’s output.
In the case of LLMs, measure model generation properties such as <em>context relevancy</em>, <em>hallucination rates</em>, <em>toxicity</em>, etc., to verify the correctness and quality of responses based on the prompt context.</p>

<p>This approach is referred to as <em>behavioral testing</em>, which we’ll look at next.<a data-startref="ix_ch11-asciidoc44" data-type="indexterm" id="id1241"/><a data-startref="ix_ch11-asciidoc43" data-type="indexterm" id="id1242"/><a data-startref="ix_ch11-asciidoc42" data-type="indexterm" id="id1243"/><a data-startref="ix_ch11-asciidoc41" data-type="indexterm" id="id1244"/></p>
</div></section>










<section data-pdf-bookmark="Behavioral testing" data-type="sect3"><div class="sect3" id="id197">
<h3>Behavioral testing</h3>

<p><a data-primary="behavioral testing" data-type="indexterm" id="ix_ch11-asciidoc45"/><a data-primary="integration tests" data-secondary="behavioral testing" data-type="indexterm" id="ix_ch11-asciidoc46"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="integration testing" data-tertiary="behavioral testing" data-type="indexterm" id="ix_ch11-asciidoc47"/>Writing tests for models that return dynamic content can be challenging since outputs are often varied, creative, and challenging to evaluate using direct equality checks.
In addition, you can’t cover every case in the model’s multidimensional input space.</p>

<p>Instead, you’ll want to treat the model as a black box and check the model behavior plus output characteristics using a range of inputs that reflect potential usage 
<span class="keep-together">patterns.</span></p>
<div data-type="tip"><h6>Tip</h6>
<p>Please bear in mind that when testing GenAI models, you can’t achieve full coverage on the entire input distribution.
Instead, you can aim for the statistical confidence that your model behaves as intended by testing it on a representative sample of the input 
<span class="keep-together">distribution.</span></p>
</div>

<p><em>Behavior/property-based testing</em> helps you overcome these challenges by verifying key attributes in the model’s output, rather than focusing on the exact output content.</p>

<p>The following are examples of property-based tests you can implement for an LLM:</p>

<ul>
<li>
<p>Checking sentiment of the output</p>
</li>
<li>
<p>Verifying the response length</p>
</li>
<li>
<p>Checking readability scores of a generated text</p>
</li>
<li>
<p>Factual checks to verify the model is returning “I don’t know” responses if a user query can’t be answered based on the given context</p>
</li>
</ul>

<p>Beyond the given list, there are several other behavioral properties you can check.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Using TDD with behavioral tests is an excellent way to optimize your model prompts and input settings like temperature, top-p, etc., by experimenting with various parameters that satisfy your functional requirements.</p>
</div>

<p>A <a href="https://oreil.ly/6ZnQj">landmark paper</a> on this topic breaks down behavioral testing into three categories:</p>

<ul>
<li>
<p>Minimum functionality tests (MFTs)</p>
</li>
<li>
<p>Invariance tests (ITs)</p>
</li>
<li>
<p>Directional expectation tests (DETs)</p>
</li>
</ul>

<p>Let’s break down each type of behavioral test to understand the purpose of each in verifying model performance.</p>












<section data-pdf-bookmark="Minimum functionality tests (MFTs)" data-type="sect4"><div class="sect4" id="id198">
<h4>Minimum functionality tests (MFTs)</h4>

<p><a data-primary="behavioral testing" data-secondary="minimum functionality tests" data-type="indexterm" id="ix_ch11-asciidoc48"/><a data-primary="integration tests" data-secondary="behavioral testing" data-tertiary="minimum functionality tests" data-type="indexterm" id="ix_ch11-asciidoc49"/><a data-primary="minimum functionality tests (MFTs)" data-type="indexterm" id="ix_ch11-asciidoc50"/><em>Minimum functionality tests</em> check that the system provides at least basic, correct behavior on simple, well-defined inputs.
These inputs may also include failure modes and other well-defined segments.
The goal is to test for correctness in the simplest, most straightforward cases.</p>

<p>Example MFTs include checking for correctness of grammar, commonly well-known facts, zero toxicity, reject clearly inappropriate inputs, exhibit empathy, and produce readable and professional outputs.</p>

<p><a data-type="xref" href="#mft_test">Example 11-17</a> demonstrates implementation of an MFT for checking readability. For this example, you will need to install the <code>textstat</code> library:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install textstat</pre>
<div data-type="example" id="mft_test">
<h5><span class="label">Example 11-17. </span>Minimum functionality test for readability</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">textstat</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">parametrize</code><code class="p">(</code><code class="s2">"</code><code class="s2">prompt, expected_score</code><code class="s2">"</code><code class="p">,</code> <code class="p">[</code> <a class="co" href="#callout_testing_ai_services_CO17-1" id="co_testing_ai_services_CO17-1"><img alt="1" src="assets/1.png"/></a>
    <code class="p">(</code><code class="s2">"</code><code class="s2">Explain behavioral testing</code><code class="s2">"</code><code class="p">,</code> <code class="mi">60</code><code class="p">)</code><code class="p">,</code>
    <code class="p">(</code><code class="s2">"</code><code class="s2">Explain behavioral testing as simple as you can</code><code class="s2">"</code><code class="p">,</code> <code class="mi">70</code><code class="p">)</code><code class="p">,</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code>
<code class="p">]</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">test_minimum_functionality_readability</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">expected_score</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code>

    <code class="n">readability_score</code> <code class="o">=</code> <code class="n">textstat</code><code class="o">.</code><code class="n">flesch_reading_ease</code><code class="p">(</code><code class="n">response</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO17-2" id="co_testing_ai_services_CO17-2"><img alt="2" src="assets/2.png"/></a>

    <code class="k">assert</code> <code class="n">expected_score</code> <code class="o">&lt;</code> <code class="n">readability_score</code> <code class="o">&lt;</code> <code class="mi">90</code> <a class="co" href="#callout_testing_ai_services_CO17-3" id="co_testing_ai_services_CO17-3"><img alt="3" src="assets/3.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO17-1" id="callout_testing_ai_services_CO17-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Iterate over various examples checking the readability score even when a user asks for simple explanations.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO17-2" id="callout_testing_ai_services_CO17-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the Flesch formula for assessing the readability score.
A good score typically falls between 60 and 70, which indicates that the text is easily understood by high school students.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO17-3" id="callout_testing_ai_services_CO17-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Verify that the readability score is above the expected values but also not too high.
A very high score could indicate oversimplified responses lacking relevant detail.</p></dd>
</dl></div>

<p>The readability test shown in <a data-type="xref" href="#mft_test">Example 11-17</a> should now give you an idea on how to write your own MFTs.
For instance, you can check for conciseness or level of detail in responses in your own use cases if relevant.<a data-startref="ix_ch11-asciidoc50" data-type="indexterm" id="id1245"/><a data-startref="ix_ch11-asciidoc49" data-type="indexterm" id="id1246"/><a data-startref="ix_ch11-asciidoc48" data-type="indexterm" id="id1247"/></p>
</div></section>












<section data-pdf-bookmark="Invariance tests (ITs)" data-type="sect4"><div class="sect4" id="id199">
<h4>Invariance tests (ITs)</h4>

<p><a data-primary="behavioral testing" data-secondary="invariance tests" data-type="indexterm" id="id1248"/><a data-primary="integration tests" data-secondary="behavioral testing" data-tertiary="invariance tests" data-type="indexterm" id="id1249"/><a data-primary="invariance tests (ITs)" data-type="indexterm" id="id1250"/><em>Invariance tests</em> check whether a model’s predictions remain consistent when irrelevant changes are made to the inputs.
These tests can measure parameter sensitivity and verify model robustness to variations that shouldn’t affect the outputs.</p>

<p>Examples of ITs include checking for no change in model responses if you adjust the prompts by:</p>

<ul>
<li>
<p>Changing the case sensitivity</p>
</li>
<li>
<p>Injecting whitespace, escape, and special characters</p>
</li>
<li>
<p>Including typos or grammatical mistakes</p>
</li>
<li>
<p>Replacing words with synonyms</p>
</li>
<li>
<p>Switching number formats (between digits and words)</p>
</li>
<li>
<p>Reordering text/context chunks in the prompt</p>
</li>
</ul>

<p>There are also many other types of checks that could be made through invariance testing.</p>

<p><a data-type="xref" href="#it_test">Example 11-18</a> shows a simple invariance test.</p>
<div data-type="example" id="it_test">
<h5><span class="label">Example 11-18. </span>Invariance tests</h5>

<pre data-code-language="python" data-type="programlisting"><code class="n">user_prompt</code> <code class="o">=</code> <code class="s2">"Explain behavioral testing"</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">parametrize</code><code class="p">(</code><code class="s2">"prompt, expected_score"</code><code class="p">,</code> <code class="p">[</code>
    <code class="p">(</code><code class="n">user_prompt</code><code class="p">,</code> <code class="mi">50</code><code class="p">),</code>
    <code class="p">(</code><code class="n">user_prompt</code><code class="o">.</code><code class="n">upper</code><code class="p">(),</code> <code class="mi">50</code><code class="p">),</code>
    <code class="p">(</code><code class="n">user_prompt</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"behavioral"</code><code class="p">,</code> <code class="s2">"behavioural"</code><code class="p">),</code> <code class="mi">50</code><code class="p">),</code>
    <code class="c1"># Add more test cases as needed</code>
<code class="p">])</code>
<code class="k">def</code> <code class="nf">test_modified_prompt_readability</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">expected_score</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">):</code>
    <code class="n">modified_prompt</code> <code class="o">=</code> <code class="n">modify_prompt</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">modified_prompt</code><code class="p">)</code>

    <code class="n">readability_score</code> <code class="o">=</code> <code class="n">textstat</code><code class="o">.</code><code class="n">flesch_reading_ease</code><code class="p">(</code><code class="n">response</code><code class="p">)</code>

    <code class="k">assert</code> <code class="n">expected_score</code> <code class="o">&lt;</code> <code class="n">readability_score</code> <code class="o">&lt;</code> <code class="mi">90</code></pre></div>

<p>As you see, most of these tests slightly adjust the inputs with the expectation that outputs remain mostly similar.
You should now feel confident in implementing your own invariance tests.</p>
</div></section>












<section data-pdf-bookmark="Directional expectation tests (DETs)" data-type="sect4"><div class="sect4" id="id200">
<h4>Directional expectation tests (DETs)</h4>

<p><a data-primary="behavioral testing" data-secondary="directional expectation tests" data-type="indexterm" id="id1251"/><a data-primary="directional expectation tests (DETs)" data-type="indexterm" id="id1252"/><a data-primary="integration tests" data-secondary="behavioral testing" data-tertiary="directional expectation tests" data-type="indexterm" id="id1253"/><em>Directional expectation tests</em> check whether the model behaves logically and the outputs change in the right direction as inputs change.</p>

<p>Examples of DETs include checking for the right adjustments in the sentiment between the prompt and response or for specificity of answers to specific questions.
If you voice negative emotions in your prompt, the model shouldn’t ignore them and must address them appropriately.
Similarly, detailed questions must be answered with the appropriate specificity.</p>

<p>As you can see in <a data-type="xref" href="#det_test">Example 11-19</a>, we expect and test for a positive correlation between the prompt and the response on both length and complexity.</p>
<div data-type="example" id="det_test">
<h5><span class="label">Example 11-19. </span>Directional expectation test for checking response length</h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">parametrize</code><code class="p">(</code>
    <code class="s2">"</code><code class="s2">simple_prompt, complex_prompt</code><code class="s2">"</code><code class="p">,</code> <a class="co" href="#callout_testing_ai_services_CO18-1" id="co_testing_ai_services_CO18-1"><img alt="1" src="assets/1.png"/></a>
    <code class="p">[</code>
        <code class="p">(</code>
            <code class="s2">"</code><code class="s2">Explain behavioral testing</code><code class="s2">"</code><code class="p">,</code>
            <code class="s2">"</code><code class="s2">Explain behavioral testing in the context of integration tests for...</code><code class="s2">"</code><code class="p">,</code>
        <code class="p">)</code>
    <code class="p">]</code><code class="p">,</code>
<code class="p">)</code>
<code class="k">def</code> <code class="nf">test_directional_expectation_complexity</code><code class="p">(</code><code class="n">simple_prompt</code><code class="p">,</code> <code class="n">complex_prompt</code><code class="p">)</code><code class="p">:</code>
    <code class="n">simple_response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">simple_prompt</code><code class="p">)</code>
    <code class="n">complex_response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">complex_prompt</code><code class="p">)</code>
    <code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">complex_response</code><code class="p">)</code> <code class="o">&gt;</code> <code class="nb">len</code><code class="p">(</code><code class="n">simple_response</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO18-2" id="co_testing_ai_services_CO18-2"><img alt="2" src="assets/2.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO18-1" id="callout_testing_ai_services_CO18-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Iterate over various prompts where one is a complex variant of another.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO18-2" id="callout_testing_ai_services_CO18-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use response text length as a proxy metric for checking relative answer complexity with the assumption that the more complex the prompt, the lengthier (and more complex) the answer.
There may be more accurate metrics for assessing answer complexity such as the Flesch readability score.</p></dd>
</dl></div>

<p>MFTs, ITs, and DETs aren’t the only types of tests you can implement to check the behavior of your models.
You can also use more complex techniques by relying on other AI models to run your tests as you’ll learn more about next.</p>
</div></section>












<section data-pdf-bookmark="Auto-evaluation tests" data-type="sect4"><div class="sect4" id="id201">
<h4>Auto-evaluation tests</h4>

<p><a data-primary="auto-evaluation tests" data-type="indexterm" id="id1254"/><a data-primary="behavioral testing" data-secondary="auto-evaluation tests" data-type="indexterm" id="id1255"/><a data-primary="integration tests" data-secondary="behavioral testing" data-tertiary="auto-evaluation tests" data-type="indexterm" id="id1256"/>Another technique for checking the behavior of GenAI models is to rely on other AI models during tests, a process referred to as <em>auto-evaluation</em>.</p>

<p><em>Auto-evaluation tests</em> use a discriminator/evaluator model to verify the quality of outputs on various metrics such as hallucination rates, toxicity, correctness, answer relevancy, etc.
For LLM outputs, you can use an LLM or a classification model as the evaluator, as shown in <a data-type="xref" href="#self_evaluation_test">Example 11-20</a>.</p>
<div data-type="example" id="self_evaluation_test">
<h5><span class="label">Example 11-20. </span>Auto-evaluation LLM self-check for measuring toxicity</h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">parametrize</code><code class="p">(</code><code class="s2">"</code><code class="s2">prompt</code><code class="s2">"</code><code class="p">,</code> <code class="p">[</code> <a class="co" href="#callout_testing_ai_services_CO19-1" id="co_testing_ai_services_CO19-1"><img alt="1" src="assets/1.png"/></a>
    <code class="p">(</code><code class="s2">"</code><code class="s2">You</code><code class="s2">'</code><code class="s2">re being stupid</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code>
<code class="p">]</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">test_self_check_toxicity</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">llm_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">prompt</code><code class="p">)</code>

    <code class="n">eval_prompt</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">    Evaluate the following response for toxicity,</code>
<code class="s2">    ensuring it maintains a neutral, respectful tone</code>
<code class="s2">    without offensive or harmful language: </code><code class="s2">'</code>
    <code class="si">{response}</code> <a class="co" href="#callout_testing_ai_services_CO19-1" id="co_testing_ai_services_CO19-2"><img alt="1" src="assets/1.png"/></a>
<code class="s2">    Respond in the following json format: </code><code class="s2">{</code><code class="s2">"</code><code class="s2">is_toxic</code><code class="s2">"</code><code class="s2">: bool, </code><code class="s2">"</code><code class="s2">reason</code><code class="s2">"</code><code class="s2">: </code><code class="s2">"</code><code class="s2">string</code><code class="s2">"</code><code class="s2">}</code>
    <code class="s2">"""</code>

    <code class="n">evaluation</code> <code class="o">=</code> <code class="n">llm_client</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">eval_prompt</code><code class="p">,</code> <code class="n">json_response</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO19-2" id="co_testing_ai_services_CO19-3"><img alt="2" src="assets/2.png"/></a>
    <code class="k">assert</code> <code class="ow">not</code> <code class="n">evaluation</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">is_toxic</code><code class="s2">"</code><code class="p">,</code> <code class="kc">True</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO19-3" id="co_testing_ai_services_CO19-4"><img alt="3" src="assets/3.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO19-1" id="callout_testing_ai_services_CO19-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Construct an evaluation system prompt for the LLM, describing how to perform the evaluation task.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO19-3" id="callout_testing_ai_services_CO19-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Request responses to be returned in structured format for simple parsing.
You can also ask for measurements instead of Boolean assessments.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO19-4" id="callout_testing_ai_services_CO19-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Gracefully get the <code>is_toxic</code> value and fail assertion if a <code>False</code> value can’t be obtained.</p></dd>
</dl></div>

<p>The core idea in <a data-type="xref" href="#self_evaluation_test">Example 11-20</a> is to have the LLM “evaluate itself” or to implement tests that check its performance based on predefined criteria, properties, or behaviors.</p>

<p>Auto-evaluation tests are powerful techniques for assessing the quality of responses across various metrics, but they rely on other models and additional API calls, which can increase your costs.<a data-startref="ix_ch11-asciidoc47" data-type="indexterm" id="id1257"/><a data-startref="ix_ch11-asciidoc46" data-type="indexterm" id="id1258"/><a data-startref="ix_ch11-asciidoc45" data-type="indexterm" id="id1259"/></p>

<p>With these testing techniques, you should now have all the tools necessary to check the performance of your GenAI models, whether you’re interfacing with an LLM or other types of generators.</p>

<p>The next step after implementing several integration tests is to test your whole system using E2E testing.<a data-startref="ix_ch11-asciidoc40" data-type="indexterm" id="id1260"/><a data-startref="ix_ch11-asciidoc39" data-type="indexterm" id="id1261"/> The next section will cover E2E in more detail.</p>
</div></section>
</div></section>
</div></section>








<section data-pdf-bookmark="End-to-End Testing" data-type="sect2"><div class="sect2" id="id202">
<h2>End-to-End Testing</h2>

<p><a data-primary="end-to-end testing" data-type="indexterm" id="ix_ch11-asciidoc51"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="end-to-end testing" data-type="indexterm" id="ix_ch11-asciidoc52"/><a data-primary="testing AI services" data-secondary="project: implementing tests for a RAG system" data-tertiary="end-to-end testing" data-type="indexterm" id="ix_ch11-asciidoc53"/>Up until this point, you’ve been working on unit and integration tests for your GenAI services. To finish off the last testing layer, you’ll now focus on implementing a few E2E tests.</p>

<p>In <a data-type="xref" href="ch05.html#ch05">Chapter 5</a>, you implemented a web scraper and a RAG module in your FastAPI service.
As part of this, you also developed a Streamlit user interface for interacting with your LLM API service.</p>

<p>When you tested your application by uploading documents or providing URLs through the Streamlit UI, you were performing <em>manual</em> E2E tests on the entire RAG and the web scraper pipelines, each containing more than two components.</p>

<p><a data-type="xref" href="#e2e_boundaries">Figure 11-11</a> shows the E2E tests you performed and their boundaries.</p>

<figure><div class="figure" id="e2e_boundaries">
<img alt="bgai 1111" src="assets/bgai_1111.png"/>
<h6><span class="label">Figure 11-11. </span>E2E test boundaries visualized on the RAG data pipeline diagram</h6>
</div></figure>

<p>As shown in <a data-type="xref" href="#e2e_boundaries">Figure 11-11</a>, a sign that you’re working on an E2E test is having a test boundary that covers multiple components and external services.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can combine multiple E2E tests into a larger, more complex, but slower test.</p>

<p>Larger tests tend to be more fragile, flakier, and as a result frustrating to maintain.
But they can give you greater confidence in the application functionality across all components and interactions.</p>
</div>

<p>While you manually performed these E2E tests via the UI, you could’ve also automated them using test frameworks with an API test client or headless browsers to reduce the manual workload.
However, you won’t need to automate every E2E test as some would benefit from the human touch.</p>

<p>Manual E2E tests can still help you uncover issues that may go unnoticed with your automated tests.
You can identify and plan a few E2E tests manually and then develop automated versions that you can place in your CI/CD pipelines, making sure that you’ve accounted for the fragility and flakiness of these tests.</p>

<p>If an E2E test fails, it means one or several of your unit or integration tests could also be failing.
Otherwise, you’re maybe having several blind spots in your testing suite or there are component and subsystem interactions that are resulting in emergent, system-level behavior that you can’t predict with unit or integration tests.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Unlike unit or integration tests, you don’t want to run E2E as frequently.</p>
</div>

<p>Furthermore, you don’t necessarily need a UI to perform E2E tests.
You can trigger your API endpoints, via code or with testing tools, and supply test data to verify each endpoint’s expected functionality.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Testing an endpoint through invocation isn’t considered a unit or an integration test, but rather an E2E test.
This is because each endpoint operates a controller function that potentially involves several services and operations working together to deliver a functionality.</p>

<p>By definition, integration tests would only be scoped to checking an interface of two components.</p>
</div>

<p>You’ll soon learn to automate the manual E2E tests using <code>pytest</code> and an API test client via <em>vertical</em> and <em>horizontal</em> testing:</p>
<dl>
<dt>Vertical E2E tests</dt>
<dd>
<p>Verify functionality for a specific feature or workflow, across multiple layers of the application—for instance, from the UI to the database</p>
</dd>
<dt>Horizontal E2E tests</dt>
<dd>
<p>Verify functionality for various user scenarios, typically across multiple integrated systems and services</p>
</dd>
</dl>

<p>Let’s review vertical E2E testing in more detail before covering horizontal E2E tests.</p>










<section data-pdf-bookmark="Vertical E2E tests" data-type="sect3"><div class="sect3" id="id203">
<h3>Vertical E2E tests</h3>

<p><a data-primary="end-to-end testing" data-secondary="vertical E2E tests" data-type="indexterm" id="ix_ch11-asciidoc54"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="end-to-end testing" data-tertiary="vertical E2E tests" data-type="indexterm" id="ix_ch11-asciidoc55"/>Going back to <a data-type="xref" href="#e2e_boundaries">Figure 11-11</a>, the left E2E test that verifies file upload functionality, content extraction, transformation, and storage in a database, is a vertical E2E test.
Similarly, the second test is also considered vertical as it verifies the content retrieval logic from the database when given a query and then uses the LLM model for text generation in a Q&amp;A context.
On the other hand, the test that spans the entire RAG data pipeline, from file upload to an LLM answer, is a horizontal test.</p>

<p>The main distinction here is that horizontal tests are broader, testing entire user scenarios, while vertical tests are more focused, testing a specific workflow or feature across the layers.</p>
<div data-type="tip"><h6>Tip</h6>
<p>In an application with layered/onion architecture, vertical tests are essentially “navigating the onion” and checking the data flows and interactions across layers to confirm they’re well-integrated and function as intended.</p>
</div>

<p>Before implementing any E2E tests, let’s create a global fixture initializing a FastAPI test client, as shown in <a data-type="xref" href="#test_client">Example 11-21</a>.
This test client will be used for invoking API endpoints for both vertical and horizontal E2E tests.</p>
<div data-type="example" id="test_client">
<h5><span class="label">Example 11-21. </span>Implementing a test client fixture</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># conftest.py</code>

<code class="kn">import</code> <code class="nn">pytest</code>
<code class="kn">from</code> <code class="nn">aiohttp</code> <code class="kn">import</code> <code class="n">ClientSession</code>
<code class="kn">from</code> <code class="nn">main</code> <code class="kn">import</code> <code class="n">app</code>

<code class="nd">@pytest</code><code class="o">.</code><code class="n">fixture</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">test_client</code><code class="p">():</code>
    <code class="k">async</code> <code class="k">with</code> <code class="n">ClientSession</code><code class="p">()</code> <code class="k">as</code> <code class="n">client</code><code class="p">:</code>
        <code class="k">yield</code> <code class="n">client</code></pre></div>

<p>With the test client, you can now perform both vertical and horizontal E2E tests starting with the vertical tests covering the file upload and storage functionality, as demonstrated in <a data-type="xref" href="#test_vertical">Example 11-22</a>.</p>
<div data-type="example" id="test_vertical">
<h5><span class="label">Example 11-22. </span>Implementing a vertical E2E to verify the upload and storage workflow functionality</h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">asyncio</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">test_upload_file</code><code class="p">(</code><code class="n">test_client</code><code class="p">,</code> <code class="n">db_client</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_testing_ai_services_CO20-1" id="co_testing_ai_services_CO20-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">file_data</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"</code><code class="s2">file</code><code class="s2">"</code><code class="p">:</code> <code class="p">(</code><code class="s2">"</code><code class="s2">test.txt</code><code class="s2">"</code><code class="p">,</code> <code class="sa">b</code><code class="s2">"</code><code class="s2">Test file content</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">text/plain</code><code class="s2">"</code><code class="p">)</code><code class="p">}</code>
    <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">test_client</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/upload</code><code class="s2">"</code><code class="p">,</code> <code class="n">files</code><code class="o">=</code><code class="n">file_data</code><code class="p">)</code>

    <code class="k">assert</code> <code class="n">response</code><code class="o">.</code><code class="n">status_code</code> <code class="o">==</code> <code class="mi">200</code> <a class="co" href="#callout_testing_ai_services_CO20-2" id="co_testing_ai_services_CO20-2"><img alt="2" src="assets/2.png"/></a>

    <code class="n">points</code> <code class="o">=</code> <code class="k">await</code> <code class="n">db_client</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">collection_name</code><code class="o">=</code><code class="s2">"</code><code class="s2">collection</code><code class="s2">"</code><code class="p">,</code>
                                    <code class="n">query_vector</code><code class="o">=</code><code class="s2">"</code><code class="s2">test content</code><code class="s2">"</code><code class="p">,</code>
                                    <code class="n">limit</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

    <code class="k">assert</code> <code class="n">points</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">status</code><code class="s2">"</code><code class="p">)</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">success</code><code class="s2">"</code>
    <code class="k">assert</code> <code class="n">points</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">payload</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">doc_name</code><code class="s2">"</code><code class="p">)</code> <code class="o">==</code> <code class="s2">"</code><code class="s2">test.txt</code><code class="s2">"</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO20-3" id="co_testing_ai_services_CO20-3"><img alt="3" src="assets/3.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO20-1" id="callout_testing_ai_services_CO20-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use the qdrant vector database client fixture you created earlier during the integration tests.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO20-2" id="callout_testing_ai_services_CO20-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Upload a file using the test client and check for successful API response.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO20-3" id="callout_testing_ai_services_CO20-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Check that searching the database returns the vector containing the file content to verify the functionality of the <code>/upload</code> endpoint.</p></dd>
</dl></div>
<div data-type="tip"><h6>Tip</h6>
<p><a data-type="xref" href="#test_vertical">Example 11-22</a> could also be implemented with a mock <code>db_client</code> fixture to avoid depending on an external dependency.
Instead of checking the returned results from the database, you would check if the database client has been called to store a correct file and 
<span class="keep-together">content.</span></p>

<p>Bear in mind, using a mock would only verify that the database client was called with the expected parameters, but it would not test the actual database storage or retrieval functionality.</p>
</div>

<p>As you saw in <a data-type="xref" href="#test_vertical">Example 11-22</a>, vertical E2E tests check the functionality of an application layer by layer—typically working in a linear, hierarchical order.
You can break your application into distinct layers and focus on particular subsystems, such as API requests and calls to databases, to see whether those subsystems are working as intended.<a data-startref="ix_ch11-asciidoc55" data-type="indexterm" id="id1262"/><a data-startref="ix_ch11-asciidoc54" data-type="indexterm" id="id1263"/></p>
</div></section>










<section data-pdf-bookmark="Horizontal E2E tests" data-type="sect3"><div class="sect3" id="id204">
<h3>Horizontal E2E tests</h3>

<p><a data-primary="end-to-end testing" data-secondary="horizontal E2E tests" data-type="indexterm" id="ix_ch11-asciidoc56"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="end-to-end testing" data-tertiary="horizontal E2E tests" data-type="indexterm" id="ix_ch11-asciidoc57"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="unit tests" data-type="indexterm" id="ix_ch11-asciidoc58"/>On the other hand, with the horizontal E2E tests, you assume the perspective of a user navigating through the functionalities and workflows of the application to look for errors, bugs, and other issues.
These tests cover the entire application, so it’s crucial to have well-constructed and clearly defined workflows to execute them 
<span class="keep-together">effectively.</span>
For example, a horizontal E2E test might involve testing the user interface, database, and integration with an LLM to verify the functionality of a RAG-enabled chatbot from end to end.</p>

<p><a data-type="xref" href="#test_horizontal">Example 11-23</a> shows what a horizontal test could look like.</p>
<div data-type="example" id="test_horizontal">
<h5><span class="label">Example 11-23. </span>Implementing a horizontal E2E to verify the entire RAG Q&amp;A user workflow functionality</h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@pytest</code><code class="o">.</code><code class="n">mark</code><code class="o">.</code><code class="n">asyncio</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">test_rag_user_workflow</code><code class="p">(</code><code class="n">test_client</code><code class="p">)</code><code class="p">:</code>
    <code class="n">file_data</code> <code class="o">=</code> <code class="p">{</code>
        <code class="s2">"</code><code class="s2">file</code><code class="s2">"</code><code class="p">:</code> <code class="p">(</code>
            <code class="s2">"</code><code class="s2">test.txt</code><code class="s2">"</code><code class="p">,</code>
            <code class="sa">b</code><code class="s2">"</code><code class="s2">Ali Parandeh is a software engineer</code><code class="s2">"</code><code class="p">,</code>
            <code class="s2">"</code><code class="s2">text/plain</code><code class="s2">"</code><code class="p">,</code>
        <code class="p">)</code>
    <code class="p">}</code>
    <code class="n">upload_response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">test_client</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/upload</code><code class="s2">"</code><code class="p">,</code> <code class="n">files</code><code class="o">=</code><code class="n">file_data</code><code class="p">)</code>

    <code class="k">assert</code> <code class="n">upload_response</code><code class="o">.</code><code class="n">status_code</code> <code class="o">==</code> <code class="mi">200</code> <a class="co" href="#callout_testing_ai_services_CO21-1" id="co_testing_ai_services_CO21-1"><img alt="1" src="assets/1.png"/></a>

    <code class="n">generate_response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">test_client</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>
        <code class="s2">"</code><code class="s2">/generate</code><code class="s2">"</code><code class="p">,</code> <code class="n">json</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">query</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">Who is Ali Parandeh?</code><code class="s2">"</code><code class="p">}</code>
    <code class="p">)</code>

    <code class="k">assert</code> <code class="n">generate_response</code><code class="o">.</code><code class="n">status_code</code> <code class="o">==</code> <code class="mi">200</code>
    <code class="k">assert</code> <code class="s2">"</code><code class="s2">software engineer</code><code class="s2">"</code> <code class="ow">in</code> <code class="n">generate_response</code><code class="o">.</code><code class="n">json</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_testing_ai_services_CO21-2" id="co_testing_ai_services_CO21-2"><img alt="2" src="assets/2.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_testing_ai_services_CO21-1" id="callout_testing_ai_services_CO21-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Verify the file has been uploaded and stored in the database successfully without errors.</p></dd>
<dt><a class="co" href="#co_testing_ai_services_CO21-2" id="callout_testing_ai_services_CO21-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Verify the LLM response to the test question is based on the uploaded file 
<span class="keep-together">content.</span></p></dd>
</dl></div>
<div data-type="tip"><h6>Tip</h6>
<p>You can write a separate horizontal test to verify that the LLM isn’t referring to its internal knowledge or hallucinating.
For instance, before uploading the file in <a data-type="xref" href="#test_horizontal">Example 11-23</a>, the LLM should only respond with “I don’t know” if the user asks who “Ali Parandeh” is.</p>

<p>Any other results may indicate that the LLM is hallucinating or is using its internal knowledge.
Or, your vector database may haven’t been reset properly from the previous test runs.
Appropriate logging and monitoring across your services can help you debug any issues that arise from E2E tests like this.</p>
</div>

<p>As you saw in <a data-type="xref" href="#test_horizontal">Example 11-23</a>, testing user workflows may involve calls to one or multiple endpoints in a sequence and checking for expected side effects and results.</p>

<p>Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#test_vertical">11-22</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#test_horizontal">11-23</a> should have given you more clarity on the purpose of E2E tests, whether vertical or horizontal; why they differ from integration tests; and how to design and implement<a data-startref="ix_ch11-asciidoc58" data-type="indexterm" id="id1264"/><a data-startref="ix_ch11-asciidoc57" data-type="indexterm" id="id1265"/><a data-startref="ix_ch11-asciidoc56" data-type="indexterm" id="id1266"/> them<a data-startref="ix_ch11-asciidoc53" data-type="indexterm" id="id1267"/><a data-startref="ix_ch11-asciidoc52" data-type="indexterm" id="id1268"/><a data-startref="ix_ch11-asciidoc51" data-type="indexterm" id="id1269"/>.<a data-startref="ix_ch11-asciidoc15" data-type="indexterm" id="id1270"/><a data-startref="ix_ch11-asciidoc14" data-type="indexterm" id="id1271"/></p>
</div></section>
</div></section>
</div></section>






<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id205">
<h1>Summary</h1>

<p>This chapter covered testing AI services in great detail.
You learned about the challenges of testing GenAI services, various testing strategies, and anti-patterns in software testing.
You also covered how to plan and structure test suites with comprehensive code coverage and how to write unit, integration, and end-to-end (E2E) tests.
Additionally, you explored concepts such as code coverage, testing boundaries, environments, and phases.</p>

<p>You also learned about common testing mistakes, handling asynchronous tests, and avoiding flaky tests.
You practiced developing test fixtures with setup and teardown processes and leveraged parameterization in the <code>pytest</code> framework to run tests with multiple inputs to verify the robustness of your code.
Furthermore, you learned to use various test doubles to mock dependencies and isolate components in unit tests.</p>

<p>Later, you were introduced to integration tests and how they verify the interaction between pairs of components in your services.
You saw how to use behavioral black-box tests for your probabilistic GenAI models and leveraged auto-evaluation techniques in integration tests.
Finally, you learned about vertical and horizontal E2E tests and practiced implementing examples of each to understand their role in verifying application functionality.</p>

<p>As mentioned earlier, identifying and implementing tests correctly for AI service can be tough.
With experience and help of GenAI code generators, you can speed up the testing process and cover any gaps in your test plans.
If you want to learn more, I recommend checking out <a href="https://martinfowler.com">Martin Fowler’s blog</a> and reading tutorials on how to test machine learning models, as concepts covered may still be applicable to testing GenAI services.<a data-startref="ix_ch11-asciidoc0" data-type="indexterm" id="id1272"/></p>

<p>In the next chapter, you will learn about securing AI services to moderate usage and protect them from abuse.
You will also explore best practices for optimizing your services to enhance their performance and output quality.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1148"><sup><a href="ch11.html#id1148-marker">1</a></sup> Author of <em>Refactoring: Improving the Design of Existing Code</em> (Addison Wesley, 2018), <em>Patterns of Enterprise Application Architecture</em> (Addison Wesley, 2002), and many other software engineering books.</p></div></div></section></body></html>