- en: Chapter 6\. A Journey into Sound
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章. 声音之旅
- en: One of the most successful applications of deep learning is something that we
    carry around with us every day. Whether it’s Siri or Google Now, the engines that
    power both systems and Amazon’s Alexa are neural networks. In this chapter, we’ll
    take a look at PyTorch’s `torchaudio` library. You’ll learn how to use it to construct
    a pipeline for classifying audio data with a convolutional-based model. After
    that, I’ll suggest a different approach that will allow you to use some of the
    tricks you learned for images and obtain good accuracy on the ESC-50 audio dataset.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习最成功的应用之一是我们每天随身携带的东西。无论是Siri还是Google Now，驱动这两个系统以及亚马逊的Alexa的引擎都是神经网络。在本章中，我们将看一下PyTorch的`torchaudio`库。您将学习如何使用它来构建一个用于分类音频数据的基于卷积的模型的流水线。之后，我将建议一种不同的方法，让您可以使用一些您学到的图像技巧，并在ESC-50音频数据集上获得良好的准确性。
- en: But first, let’s take a look at sound itself. What is it? How is it often represented
    in data form, and does that provide us with any clues as to what type of neural
    net we should use to gain insight from our data?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们看看声音本身。它是什么？它通常以数据形式表示，这是否为我们提供了任何线索，告诉我们应该使用什么类型的神经网络来从数据中获得洞察？
- en: Sound
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声音
- en: Sound is created via the vibration of air. All the sounds we hear are combinations
    of high and low pressure that we often represent in a waveform, like the one in
    [Figure 6-1](#sine-wave). In this image, the wave above the origin is high pressure,
    and the part below is low pressure.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 声音是通过空气的振动产生的。我们听到的所有声音都是高低压力的组合，我们通常用波形来表示，就像[图6-1](#sine-wave)中的那个。在这个图像中，原点上方的波是高压，下方的部分是低压。
- en: '![Sine wave](assets/ppdl_0601.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![正弦波](assets/ppdl_0601.png)'
- en: Figure 6-1\. Sine wave
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1. 正弦波
- en: '[Figure 6-2](#song-waveform) shows a more complex waveform of a complete song.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-2](#song-waveform)显示了一首完整歌曲的更复杂的波形。'
- en: '![Song waveform](assets/ppdl_0602.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![歌曲波形](assets/ppdl_0602.png)'
- en: Figure 6-2\. Song waveform
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2. 歌曲波形
- en: In digital sound, we *sample* this waveform many times a second, traditionally
    44,100 for CD-quality sound, and store the amplitude values of the wave during
    each sample point. At a time *t*, we have a single value stored. This is slightly
    different from an image, which requires two values, *x* and *y*, to store a value
    (for a grayscale image). If we use convolutional filters in our neural network,
    we need a 1D filter rather than the 2D filters we were using for images.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字声音中，我们以每秒多次对这个波形进行*采样*，传统上是44,100次，用于CD音质，然后存储每个采样点的波的振幅值。在时间*t*，我们有一个单一的存储值。这与图像略有不同，图像需要两个值*x*和*y*来存储值（对于灰度图像）。如果我们在神经网络中使用卷积滤波器，我们需要一个1D滤波器，而不是我们用于图像的2D滤波器。
- en: Now that you know a little more about sound, let’s look at the dataset we use
    so you can get a little more familiar with it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您对声音有了一些了解，让我们看看我们使用的数据集，这样您就可以更加熟悉它。
- en: The ESC-50 Dataset
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ESC-50数据集
- en: The *Environmental Sound Classification* (ESC) dataset is a collection of field
    recordings, each of which is 5 seconds long and assigned to one of 50 classes
    (e.g., a dog barking, snoring, a knock on a door). We use this set for the rest
    of the chapter to experiment with two ways of classifying audio, as well as to
    explore using `torchaudio` to simplify loading and manipulating audio.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*环境声音分类*（ESC）数据集是一组现场录音，每个录音长达5秒，并分配给50个类别之一（例如，狗叫、打鼾、敲门声）。我们将在本章的其余部分中使用此集合，以尝试两种分类音频的方法，并探索使用`torchaudio`来简化加载和操作音频。'
- en: Obtaining the Dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据集
- en: 'The [ESC-50 dataset](https://github.com/karoldvl/ESC-50) is a set of WAV files.
    You can download it either by cloning the Git repository:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[ESC-50数据集](https://github.com/karoldvl/ESC-50)是一组WAV文件。您可以通过克隆Git存储库来下载它：'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Or you can download the entire repo just by using curl:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您可以使用curl下载整个存储库：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'All the WAV files are stored in the *audio* directory with filenames like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的WAV文件都存储在*audio*目录中，文件名如下：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We care about the final number in the filename, because that tells us what class
    this sound clip has been assigned to. The other parts of the filename don’t matter
    to us but mostly relate to the larger Freesound dataset from which ESC-50 has
    been drawn (with one exception that I’ll come back to shortly). If you’re interested
    in finding out more, the *README* document in the ESC-50 repo goes into further
    detail.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关心文件名中的最后一个数字，因为这告诉我们这个声音片段被分配到了哪个类别。文件名的其他部分对我们来说并不重要，但大多数与ESC-50所绘制的更大的Freesound数据集相关（有一个例外，我马上会回来解释）。如果您想了解更多信息，ESC-50存储库中的*README*文档会提供更详细的信息。
- en: Now that we’ve downloaded the dataset, let’s look at some of the sounds it contains.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载了数据集，让我们看看它包含的一些声音。
- en: Playing Audio in Jupyter
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Jupyter中播放音频
- en: 'If you want to actually hear a sound from ESC-50, then instead of loading one
    of the files into a standard music player such as iTunes, you can use Jupyter’s
    built-in player for audio, `IPython.display.Audio`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想真正听到ESC-50中的声音，那么您可以使用Jupyter内置的音频播放器`IPython.display.Audio`，而不是将文件加载到标准音乐播放器（如iTunes）中：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The function will read in our WAV files and MP3 files. You can also generate
    tensors, convert them into NumPy arrays, and play those directly. Play some of
    the files in the *ESC-50* directory to get a feel for the sounds available. Once
    you’ve done that, we’ll explore the dataset in depth a little more.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将读取我们的WAV文件和MP3文件。您还可以生成张量，将它们转换为NumPy数组，并直接播放这些数组。播放*ESC-50*目录中的一些文件，以了解可用的声音。完成后，我们将更深入地探索数据集。
- en: Exploring ESC-50
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索ESC-50
- en: When dealing with a new dataset, it’s always a good idea to get a feeling for
    the *shape* of the data before you dive right into building models. In classification
    tasks, for example, you’ll want to know whether your dataset actually contains
    examples from all the possible classes, and ideally that all classes are present
    in equal numbers. Let’s take a look at how ESC-50 breaks down.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 处理新数据集时，最好在构建模型之前先了解数据的*形状*。例如，在分类任务中，你会想知道你的数据集是否实际包含了所有可能的类别的示例，并且最好所有类别的数量是相等的。让我们看看
    ESC-50 是如何分解的。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If your dataset has an *unbalanced* amount of data, a simple solution is to
    randomly duplicate the smaller class examples until you have increased them to
    the number of the other classes. Although this feels like fake accounting, it’s
    surprisingly effective (and cheap!) in practice.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集的数据量是*不平衡*的，一个简单的解决方案是随机复制较小类别的示例，直到你将它们增加到其他类别的数量。虽然这感觉像是虚假的账务，但在实践中它是令人惊讶地有效（而且便宜！）。
- en: 'We know that the final set of digits in each filename describes the class it
    belongs to, so what we need to do is grab a list of the files and count up the
    occurrences of each class:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道每个文件名中最后一组数字描述了它所属的类别，所以我们需要做的是获取文件列表并计算每个类别的出现次数：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: First, we build up a list of our ESC-50 filenames. Because we care about only
    the class number at the end of the filename, we chop off the *.wav* extension
    and split the filename on the `-` separator. We finally take the last element
    in that split string. If you inspect `esc50_list`, you’ll get a bunch of strings
    that range from 0 to 49\. We could write more code that builds a `dict` and counts
    all the occurrences for us, but I’m lazy, so I’m using a Python convenience function,
    `Counter`, that does all that for us.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们建立一个 ESC-50 文件名列表。因为我们只关心文件名末尾的类别编号，我们去掉 *.wav* 扩展名，并在 `-` 分隔符上分割文件名。最后我们取分割字符串中的最后一个元素。如果你检查
    `esc50_list`，你会得到一堆从 0 到 49 的字符串。我们可以编写更多的代码来构建一个 `dict` 并为我们计算所有出现的次数，但我懒，所以我使用了一个
    Python 的便利函数 `Counter`，它可以为我们做所有这些。
- en: Here’s the output!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出！
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We have one of those rare things, a perfectly balanced dataset. Let’s break
    out the champagne and install a few more libraries that we’re going to need shortly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一种罕见的完全平衡的数据集。让我们拿出香槟，安装一些我们很快会需要的库。
- en: SoX and LibROSA
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SoX 和 LibROSA
- en: 'Most of the audio processing that `torchaudio` carries out relies on two other
    pieces of software: *SoX* and *LibROSA*. [*LibROSA*](https://github.com/librosa/librosa)
    is a Python library for audio analysis, including generating mel spectrograms
    (You’ll see what these are a little later in the chapter), detecting beats, and
    even generating music.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchaudio` 进行的大部分音频处理依赖于另外两个软件：*SoX* 和 *LibROSA*。[*LibROSA*](https://github.com/librosa/librosa)
    是一个用于音频分析的 Python 库，包括生成梅尔频谱图（你将在本章稍后看到这些），检测节拍，甚至生成音乐。'
- en: '*SoX*, on the other hand, is a program that you might already be familiar with
    if you’ve been using Linux for years. In fact, *SoX* is so old that it predates
    Linux itself; its first release was in July 1991, compared to the Linux debut
    in September 1991\. I remember using it back in 1997 to convert WAV files into
    MP3s on my first ever Linux box. But it’s still useful!^([1](ch06.html#idm45762360156264))'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*SoX* 是一个你可能已经熟悉的程序，如果你多年来一直在使用 Linux 的话。事实上，*SoX* 是如此古老，以至于它早于 Linux 本身；它的第一个版本是在
    1991 年 7 月发布的，而 Linux 的首次亮相是在 1991 年 9 月。我记得在 1997 年使用它将 WAV 文件转换为 MP3 文件在我的第一台
    Linux 电脑上。但它仍然很有用！
- en: 'If you’re installing `torchaudio` via `conda`, you can skip to the next section.
    If you’re using `pip`, you’ll probably need to install *SoX* itself. For a Red
    Hat-based system, enter the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过 `conda` 安装 `torchaudio`，你可以跳到下一节。如果你使用 `pip`，你可能需要安装 *SoX* 本身。对于基于 Red
    Hat 的系统，输入以下命令：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Or on a Debian-based system, you’ll use this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在基于 Debian 的系统上，你将使用以下命令：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once *SoX* is installed, you can move on to obtaining `torchaudio` itself.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 *SoX* 后，你可以继续获取 `torchaudio` 本身。
- en: torchaudio
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torchaudio
- en: 'Installing `torchaudio` can be performed with either `conda` or `pip`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `torchaudio` 可以通过 `conda` 或 `pip` 进行：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In comparison with `torchvision`, `torchaudio` is similar to `torchtext` in
    that it’s not quite as well loved, maintained, or documented. I’d expect this
    to change in the near future as PyTorch gets more popular and better text and
    audio handling pipelines are created. Still, `torchaudio` is plenty for our needs;
    we just have to write some custom dataloaders (which we didn’t have to do for
    audio or text processing).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `torchvision` 相比，`torchaudio` 类似于 `torchtext`，因为它并不像 `torchvision` 那样受到喜爱、维护或文档化。我预计随着
    PyTorch 变得更受欢迎，更好的文本和音频处理流程将被创建，这种情况将在不久的将来发生改变。不过，`torchaudio` 对我们的需求来说已经足够了；我们只需要编写一些自定义的数据加载器（对于音频或文本处理，我们不需要这样做）。
- en: Anyhow, the core of `torchaudio` is found within `load()` and `save()`. We’re
    concerned only with `load()` in this chapter, but you’ll need to use `save()`
    if you’re generating new audio from your input (e.g., a text-to-speech model).
    `load()` takes a file specified in `filepath` and returns a tensor representation
    of the audio file and the sample rate of that audio file as a separate variable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，`torchaudio` 的核心在于 `load()` 和 `save()`。在本章中，我们只关心 `load()`，但如果你从输入生成新的音频（例如文本到语音模型），你需要使用
    `save()`。`load()` 接受在 `filepath` 中指定的文件，并返回音频文件的张量表示和该音频文件的采样率作为一个单独的变量。
- en: We now have the means for loading one of the WAV files from the ESC-50 dataset
    and turning it into a tensor. Unlike our earlier work with text and images, we
    need to write a bit more code before we can get on with creating and training
    a model. We need to write a custom *dataset*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了从 ESC-50 数据集中加载一个 WAV 文件并将其转换为张量的方法。与我们之前处理文本和图像的工作不同，我们需要写更多的代码才能继续创建和训练模型。我们需要编写一个自定义的
    *dataset*。
- en: Building an ESC-50 Dataset
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 ESC-50 数据集
- en: We’ve talked about datasets in [Chapter 2](ch02.html#image-classification-with-pytorch),
    but `torchvision` and `torchtext` did all the heavy lifting for us, so we didn’t
    have to worry too much about the details. As you may remember, a custom dataset
    has to implement two class methods, `__getitem__` and `__len__`, so that the data
    loader can get a batch of tensors and their labels, as well as a total count of
    tensors in the dataset. We also have an `__init__` method for setting up things
    like file paths that’ll be used over and over again.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](ch02.html#image-classification-with-pytorch)中讨论过数据集，但`torchvision`和`torchtext`为我们做了所有繁重的工作，所以我们不必太担心细节。你可能还记得，自定义数据集必须实现两个类方法，`__getitem__`和`__len__`，以便数据加载器可以获取一批张量及其标签，以及数据集中张量的总数。我们还有一个`__init__`方法用于设置诸如文件路径之类的东西，这些东西将一遍又一遍地使用。
- en: 'Here’s our first pass at the ESC-50 dataset:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们对ESC-50数据集的第一次尝试：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The majority of the work in the class happens when a new instance of it is created.
    The `__init__` method takes the `path` parameter, finds all the WAV files inside
    that path, and then produces tuples of *`(filename, label)`* by using the same
    string split we used earlier in the chapter to get the label of that audio sample.
    When PyTorch requests an item from the dataset, we index into the `items` list,
    use `torchaudio.load` to make `torchaudio` load in the audio file, turn it into
    a tensor, and then return both the tensor and the label.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 类中的大部分工作发生在创建其新实例时。`__init__`方法接受`path`参数，找到该路径内的所有WAV文件，然后通过使用我们在本章早些时候使用的相同字符串拆分来生成*`(filename,
    label)`*元组，以获取该音频样本的标签。当PyTorch从数据集请求项目时，我们索引到`items`列表，使用`torchaudio.load`使`torchaudio`加载音频文件，将其转换为张量，然后返回张量和标签。
- en: 'And that’s enough for us to start with. For a sanity check, let’s create an
    `ESC50` object and extract the first item:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就足够让我们开始了。为了进行健全性检查，让我们创建一个`ESC50`对象并提取第一个项目：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can construct a data loader by using standard PyTorch constructs:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用标准的PyTorch构造来构建数据加载器：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'But before we do that, we have to go back to our data. As you might remember,
    we should always create training, validation, and test sets. At the moment, we
    have just one directory with all the data, which is no good for our purposes.
    A 60/20/20 split of data into training, validation, and test collections should
    suffice. Now, we could do this by taking random samples of our entire dataset
    (taking care to sample without replacement and making sure that our newly constructed
    datasets are still balanced), but again the ESC-50 dataset saves us from having
    to do much work. The compilers of the dataset separated the data into five equal
    balanced *folds*, indicated by the *first* digit in the filename. We’ll have folds
    `1,2,3` be the training set, `4` the validation set, and `5` the test set. But
    feel free to mix it up if you don’t want to be boring and consecutive! Move each
    of the folds to *test*, *train*, and *validation* directories:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这之前，我们必须回到我们的数据。您可能还记得，我们应该始终创建训练、验证和测试集。目前，我们只有一个包含所有数据的目录，这对我们的目的来说不好。将数据按60/20/20的比例分成训练、验证和测试集应该足够了。现在，我们可以通过随机抽取整个数据集的样本来做到这一点（注意要进行无重复抽样，并确保我们新构建的数据集仍然是平衡的），但是ESC-50数据集再次帮助我们省去了很多工作。数据集的编译者将数据分成了五个相等的平衡*folds*，文件名中的*第一个*数字表示。我们将`1,2,3`折作为训练集，`4`折作为验证集，`5`折作为测试集。但如果你不想无聊和连续，可以随意混合！将每个折叠移到*test*、*train*和*validation*目录中：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can create the individual datasets and loaders:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建各个数据集和加载器：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have our data all set up, so we’re all ready to look at a classification
    model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好了我们的数据，所以我们现在准备好查看分类模型了。
- en: A CNN Model for ESC-50
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ESC-50的CNN模型
- en: 'For our first attempt at classifying sounds, we build a model that borrows
    heavily from a paper called “Very Deep Convolutional Networks For Raw Waveforms.”^([2](ch06.html#idm45762359589592))
    You’ll see that it uses a lot of our building blocks from [Chapter 3](ch03.html#convolutional-neural-networks),
    but instead of using 2D layers, we’re using 1D variants, as we have one fewer
    dimension in our audio input:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们第一次尝试分类声音，我们构建了一个模型，它大量借鉴了一篇名为“用于原始波形的非常深度卷积网络”的论文。^([2](ch06.html#idm45762359589592))
    您会发现它使用了我们在[第3章](ch03.html#convolutional-neural-networks)中的许多构建模块，但是我们使用的是1D变体，而不是2D层，因为我们的音频输入少了一个维度：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We also need an optimizer and a loss function. For the optimizer, we use Adam
    as before, but what loss function do you think we should use? (If you answered
    `CrossEntropyLoss`, give yourself a gold star!)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个优化器和一个损失函数。对于优化器，我们像以前一样使用Adam，但你认为我们应该使用什么损失函数？（如果你回答`CrossEntropyLoss`，给自己一个金星！）
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Having created our model, we save our weights and use the `find_lr()` function
    from [Chapter 4](ch04.html#transfer-learning-and-other-tricks):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建完我们的模型后，我们保存我们的权重，并使用[第4章](ch04.html#transfer-learning-and-other-tricks)中的`find_lr()`函数：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'From the plot in [Figure 6-3](#audionet-learning-rate-plot), we determine that
    the appropriate learning rate is around `1e-5` (based on where the descent looks
    steepest). We set that to be our learning rate and reload our model’s initial
    weights:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图6-3](#audionet-learning-rate-plot)中的图表中，我们确定适当的学习率大约是`1e-5`（基于下降最陡的地方）。我们将其设置为我们的学习率，并重新加载我们模型的初始权重：
- en: '![AudioNet learning rate plot](assets/ppdl_0603.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![AudioNet学习率图](assets/ppdl_0603.png)'
- en: Figure 6-3\. AudioNet learning rate plot
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. AudioNet学习率图
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We train the model for 20 epochs:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型进行20个周期的训练：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After training, you should find that the model attains around 13%–17% accuracy
    on our dataset. That’s better than the 2% we could expect if we were just picking
    one of the 50 classes at random. But perhaps we can do better; let’s investigate
    a different way of looking at our audio data that may yield better results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，您应该发现模型在我们的数据集上达到了大约13%至17%的准确率。这比我们随机选择50个类别中的一个时可以期望的2%要好。但也许我们可以做得更好；让我们探讨一种不同的查看音频数据的方式，可能会产生更好的结果。
- en: This Frequency Is My Universe
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这个频率是我的宇宙
- en: If you look back at the GitHub page for ESC-50, you’ll see a leaderboard of
    network architectures and their accuracy scores. You’ll notice that in comparison,
    we’re not doing great. We could extend the model we’ve created to be deeper, and
    that would likely increase our accuracy a little, but for a real increase in performance,
    we need to switch domains. In audio processing, you can work on the pure waveform
    as we’ve been doing; but most of the time, you’ll work in the *frequency domain*.
    This different representation transforms the raw waveform into a view that shows
    all of the frequencies of sound at a given point in time. This is perhaps a more
    information-rich representation to present to a neural network, as it’ll be able
    to work on those frequencies directly, rather than having to work out how to map
    the raw waveform signal into something the model can use.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to generate frequency spectrograms with *LibROSA*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Mel Spectrograms
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Traditionally, getting into the frequency domain requires applying the Fourier
    transform on the audio signal. We’re going to go beyond that a little by generating
    our spectrograms in the mel scale. The *mel scale* defines a scale of pitches
    that are equal in distance from another, where 1000 mels = 1000 Hz. This scale
    is commonly used in audio processing, especially in speech recognition and classification
    applications. Producing a mel spectrogram with *LibROSA* requires two lines of
    code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This results in a NumPy array containing the spectrogram data. If we display
    this spectrogram as shown in [Figure 6-4](#mel-spectrogram), we can see the frequencies
    in our sound:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Mel Spectrogram](assets/ppdl_0604.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Mel spectrogram
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'However, not a lot of information is present in the image. We can do better!
    If we convert the spectrogram to a logarithmic scale, we can see a lot more of
    the audio’s structure, due to the scale being able to represent a wider range
    of values. And this is common enough in audio procressing that *LibROSA* includes
    a method for it:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This computes a scaling factor of `10 * log10(spectrogram / ref)`. `ref` defaults
    to `1.0`, but here we’re passing in `np.max()` so that `spectrogram / ref` will
    fall within the range of `[0,1]`. [Figure 6-5](#log-mel-spectrogram) shows the
    new spectrogram.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![Log Mel Spectrogram](assets/ppdl_0605.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Log mel spectrogram
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We now have a log-scaled mel spectrogram! If you call `log_spectrogram.shape`,
    you’ll see it’s a 2D tensor, which makes sense because we’ve plotted images with
    the tensor. We could create a new neural network architecture and feed this new
    data into it, but I have a diabolical trick up my sleeve. We literally just generated
    images of the spectrogram data. Why don’t we work on those instead?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: This might seem silly at first; after all, we have the underlying spectrogram
    data, and that’s more exact than the image representation (to our eyes, knowing
    that a data point is 58 rather than 60 means *more* to us than a different shade
    of, say, purple). And if we were starting from scratch, that’d definitely be the
    case. But! We have, just lying around the place, already-trained networks such
    as ResNet and Inception that we *know* are amazing at recognizing structure and
    other parts of images. We can construct image representations of our audio and
    use a pretrained network to make big jumps in accuracy with very little training
    by using the super power of transfer learning once again. This could be useful
    with our dataset, as we don’t have a lot of examples (only 2,000!) to train our
    network.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: This trick can be employed across many disparate datasets. If you can find a
    way of cheaply turning your data into an image representation, it’s worth doing
    that and throwing a ResNet network against it to get a baseline of what transfer
    learning can do for you, so you know what you have to beat by using a different
    approach. Armed with this, let’s create a new dataset that will generate these
    images for us on demand.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: A New Dataset
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个新的数据集
- en: 'Now throw away the original `ESC50` dataset class and build a new one, `ESC50Spectrogram`.
    Although this will share some code with the older class, quite a lot more is going
    on in the `__get_item__` method in this version. We generate the spectrogram by
    using *LibROSA*, and then we do some fancy `matplotlib` footwork to get the data
    into a NumPy array. We apply the array to our transformation pipeline (which just
    uses `ToTensor`) and return that and the item’s label. Here’s the code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在丢弃原始的`ESC50`数据集类，构建一个新的`ESC50Spectrogram`。虽然这将与旧类共享一些代码，但在这个版本的`__get_item__`方法中会有更多的操作。我们通过*LibROSA*生成频谱图，然后通过一些复杂的`matplotlib`操作将数据转换为NumPy数组。我们将该数组应用于我们的转换流水线（只使用`ToTensor`），并返回该数组和项目的标签。以下是代码：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We’re not going to spend too much time on this version of the dataset because
    it has a large flaw, which I demonstrate with Python’s `process_time()` method:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会花太多时间在这个数据集的版本上，因为它有一个很大的缺陷，我用Python的`process_time()`方法演示了这一点：
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The new dataset is almost one hundred times slower than our original one that
    just returned the raw audio! That will make training incredibly slow, and may
    even negate any of the benefits we could get from using transfer learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 新数据集的速度几乎比我们原始的只返回原始音频的数据集慢一百倍！这将使训练变得非常缓慢，甚至可能抵消使用迁移学习所能带来的任何好处。
- en: 'We can use a couple of tricks to get around most of our troubles here. The
    first approach would be to add a cache to store the generated spectrogram in memory,
    so we don’t have to regenerate it every time the `__getitem__` method is called.
    Using Python’s `functools` package, we can do this easily:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一些技巧来解决大部分问题。第一种方法是添加一个缓存，将生成的频谱图存储在内存中，这样我们就不必每次调用`__getitem__`方法时都重新生成它。使用Python的`functools`包，我们可以很容易地做到这一点：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Provided you have enough memory to store the entire contents of the dataset
    into RAM, this may be good enough. We’ve set up a *least recently used* (LRU)
    cache that will keep the contents in memory for as long as possible, with indices
    that haven’t been accessed recently being the first for ejection from the cache
    when memory gets tight. However, if you don’t have enough memory to store everything,
    you’ll hit slowdowns on every batch iteration as ejected spectrograms need to
    be regenerated.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 只要您有足够的内存来存储整个数据集的内容到RAM中，这可能就足够了。我们设置了一个*最近最少使用*（LRU）缓存，将尽可能长时间地保留内容在内存中，最近没有被访问的索引在内存紧张时首先被驱逐出缓存。然而，如果您没有足够的内存来存储所有内容，您将在每个批次迭代时遇到减速，因为被驱逐的频谱图需要重新生成。
- en: My preferred approach is to *precompute* all the possible plots and then create
    a new custom dataset class that loads these images from the disk. (You can even
    add the LRU cache annotation as well for further speed-up.)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我的首选方法是*预计算*所有可能的图表，然后创建一个新的自定义数据集类，从磁盘加载这些图像。（您甚至可以添加LRU缓存注释以进一步加快速度。）
- en: 'We don’t need to do anything fancy for precomputing, just a method that saves
    the plots into the same directory it’s traversing:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要为预计算做任何花哨的事情，只需要一个将图表保存到正在遍历的目录中的方法：
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This method is simpler than our previous dataset because we can use `matplotlib`’s
    `savefig` method to save a plot directly to disk rather than having to mess around
    with NumPy. We also provide an additional input parameter, `dpi`, which allows
    us to control the quality of the generated output. Run this on all the `train`,
    `test`, and `valid` paths that we have already set up (it will likely take a couple
    of hours to get through all the images).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法比我们之前的数据集更简单，因为我们可以使用`matplotlib`的`savefig`方法直接将图表保存到磁盘，而不必与NumPy搞在一起。我们还提供了一个额外的输入参数`dpi`，允许我们控制生成输出的质量。在我们已经设置好的所有`train`、`test`和`valid`路径上运行（可能需要几个小时才能处理完所有图像）。
- en: 'All we need now is a new dataset that reads these images. We can’t use the
    standard `ImageDataLoader` from Chapters [2](ch02.html#image-classification-with-pytorch)–[4](ch04.html#transfer-learning-and-other-tricks),
    as the PNG filename scheme doesn’t match the directory structure that it uses.
    But no matter, we can just open an image by using the Python Imaging Library:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需要一个新的数据集来读取这些图像。我们不能使用第[2](ch02.html#image-classification-with-pytorch)章到第[4](ch04.html#transfer-learning-and-other-tricks)章中的标准`ImageDataLoader`，因为PNG文件名方案与其使用的目录结构不匹配。但没关系，我们可以使用Python
    Imaging Library打开一张图片：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This code is much simpler, and hopefully that’s also reflected in the time
    it takes to get an entry from the dataset:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码更简单，希望从数据集中获取一个条目所需的时间也反映在其中：
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Obtaining an element from this dataset takes roughly the same time as in our
    original audio-based one, so we won’t be losing anything by moving to our image-based
    approach, except for the one-time cost of precomputing all the images before creating
    the database. We’ve also supplied a default transform pipeline that turns an image
    into a tensor, but it can be swapped out for a different pipeline during initialization.
    Armed with these optimizations, we can start to apply transfer learning to the
    problem.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个数据集获取一个元素大致需要与我们原始基于音频的数据集相同的时间，所以我们不会因为转向基于图像的方法而失去任何东西，除了预计算所有图像并创建数据库的一次性成本。我们还提供了一个默认的转换流水线，将图像转换为张量，但在初始化期间可以替换为不同的流水线。有了这些优化，我们可以开始将迁移学习应用到这个问题上。
- en: A Wild ResNet Appears
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一只野生的ResNet出现了
- en: 'As you may remember from [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    transfer learning requires that we take a model that has already been trained
    on a particular dataset (in the case of images, likely ImageNet), and then fine-tune
    it on our particular data domain, the ESC-50 dataset that we’re turning into spectrogram
    images. You might be wondering whether a model that is trained on *normal* photographs
    is of any use to us. It turns out that the pretrained models *do* learn a lot
    of structure that can be applied to domains that at first glance might seem wildly
    different. Here’s our code from [Chapter 4](ch04.html#transfer-learning-and-other-tricks)
    that initializes a model:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This initializes us with a pretrained (and frozen) `ResNet50` model and swaps
    out the head of the model for an untrained `Sequential` module that ends with
    a `Linear` with an output of 50, one for each of the classes in the ESC-50 dataset.
    We also need to create a `DataLoader` that takes our precomputed spectrograms.
    When we create our ESC-50 dataset, we’ll also want to normalize the incoming images
    with the standard ImageNet standard deviation and mean, as that’s what the pretrained
    ResNet-50 architecture was trained with. We can do that by passing in a new pipeline:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: With our data loaders set up, we can move on to finding a learning rate and
    get ready to train.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Finding a Learning Rate
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to find a learning rate to use in our model. As in [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    we’ll save the model’s initial parameters and use our `find_lr()` function to
    find a decent learning rate for training. [Figure 6-6](#specresnet-learning-rate-plot)
    shows the plot of the losses against the learning rate.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![SpecResNet learning rate plot](assets/ppdl_0606.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. A SpecResNet learning rate plot
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at the graph of the learning rate plotted against loss, it seems like
    `1e-2` is a good place to start. As our ResNet-50 model is somewhat deeper than
    our previous one, we’re also going to use differential learning rates of `[1e-2,1e-4,1e-8]`,
    with the highest learning rate applied to our classifier (as it requires the most
    training!) and slower rates for the already-trained backbone. Again, we use Adam
    as our optimizer, but feel free to experiment with the others available.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we apply those differential rates, though, we train for a few epochs
    that update only the classifier, as we *froze* the ResNet-50 backbone when we
    created our network:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We now unfreeze the backbone and apply our differential rates:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see, with a validation accuracy of around 80%, we’re already vastly
    outperforming our original `AudioNet` model. The power of transfer learning strikes
    again! Feel free to train for more epochs to see if your accuracy continues to
    improve. If we look at the ESC-50 leaderboard, we’re closing in on human-level
    accuracy. And that’s just with ResNet-50\. You could try with ResNet-101 and perhaps
    an ensemble of different architectures to push the score up even higher.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: And there’s data augmentation to consider. Let’s take a look at a few ways of
    doing that in both domains that we’ve been working in so far.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Audio Data Augmentation
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we were looking at images in [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    we saw that we could improve the accuracy of our classifier by making changes
    to our incoming pictures. By flipping them, cropping them, or applying other transformations,
    we made our neural network work harder in the training phase and obtained a more
    *generalized* model at the end of it, one that was not simply fitting to the data
    presented (the scourge of overfitting, don’t forget). Can we do the same here?
    Yes! In fact, there are two approaches that we can use—one obvious approach that
    works on the original audio waveform, and a perhaps less-obvious idea that arises
    from our decision to use a ResNet-based classifier on images of mel spectrograms.
    Let’s take a look at audio transforms first.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: torchaudio Transforms
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torchaudio转换
- en: 'In a similar manner to `torchvision`, `torchaudio` includes a `transforms`
    module that perform transformations on incoming data. However, the number of transformations
    offered is somewhat sparse, especially compared to the plethora that we get when
    we’re working with images. If you’re interested, have a look at the [documentation](https://oreil.ly/d1kp6)
    for a full list, but the only one we look at here is `torchaudio.transforms.PadTrim`.
    In the ESC-50 dataset, we are fortunate in that every audio clip is the same length.
    That isn’t something that happens in the real world, but our neural networks like
    (and sometimes insist on, depending on how they’re constructed) input data to
    be regular. `PadTrim` will take an incoming audio tensor and either pad it out
    to the required length, or trim it down so it doesn’t exceed that length. If we
    wanted to trim down a clip to a new length, we’d use `PadTrim` like this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与`torchvision`类似，`torchaudio`包括一个`transforms`模块，对传入数据执行转换。然而，提供的转换数量有些稀少，特别是与处理图像时得到的丰富多样相比。如果您感兴趣，请查看[文档](https://oreil.ly/d1kp6)获取完整列表，但我们在这里只看一个`torchaudio.transforms.PadTrim`。在ESC-50数据集中，每个音频剪辑的长度都是相同的。这在现实世界中并不常见，但我们的神经网络喜欢（有时也坚持，取决于它们的构建方式）输入数据是规则的。`PadTrim`将接收到的音频张量填充到所需长度，或者将其修剪到不超过该长度。如果我们想将剪辑修剪到新长度，我们会这样使用`PadTrim`：
- en: '[PRE33]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: However, if you’re looking for augmentation that actually changes how the audio
    sounds (e.g., adding an echo, noise, or changing the tempo of the clip), then
    the `torchaudio.transforms` module is of no use to you. Instead, we need to use
    *SoX*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您正在寻找实际改变音频声音的增强（例如添加回声、噪音或更改剪辑的节奏），那么`torchaudio.transforms`模块对您没有用。相反，我们需要使用*SoX*。
- en: SoX Effect Chains
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SoX效果链
- en: 'Why it’s not part of the `transforms` module, I’m really not sure, but `torchaudio.sox_effects.SoxEffectsChain`
    allows you to create a chain of one or more *SoX* effects and apply those to an
    input file. The interface is a bit fiddly, so let’s see it in action in a new
    version of the dataset that changes the pitch of the audio file:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它不是`transforms`模块的一部分，我真的不确定，但`torchaudio.sox_effects.SoxEffectsChain`允许您创建一个或多个*SoX*效果链，并将这些效果应用于输入文件。界面有点棘手，让我们在一个新版本的数据集中看看它的运行方式，该数据集改变了音频文件的音调：
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In our `__init__` method, we create a new instance variable, `E`, a `SoxEffectsChain`,
    that will contain all the effects that we want to apply to our audio data. We
    then add a new effect by using `append_effect_to_chain`, which takes a string
    indicating the name of the effect, and an array of parameters to send to `sox`.
    You can get a list of available effects by calling `torchaudio.sox_effects.effect_names()`.
    If we were to add another effect, it would take place after the pitch effect we
    have already set up, so if you want to create a list of separate effects and randomly
    apply them, you’ll need to create separate chains for each one.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`__init__`方法中，我们创建一个新的实例变量`E`，一个`SoxEffectsChain`，它将包含我们要应用于音频数据的所有效果。然后，我们通过使用`append_effect_to_chain`添加一个新效果，该方法接受一个指示效果名称的字符串，以及要发送给`sox`的参数数组。您可以通过调用`torchaudio.sox_effects.effect_names()`获取可用效果的列表。如果我们要添加另一个效果，它将在我们已经设置的音调效果之后发生，因此如果您想创建一系列单独的效果并随机应用它们，您需要为每个效果创建单独的链。
- en: When it comes to selecting an item to return to the data loader, things are
    a little different. Instead of using `torchaudio.load()`, we refer to our effects
    chain and point it to the file by using `set_input_file`. But note that this doesn’t
    load the file! Instead, we have to use `sox_build_flow_effects()`, which kicks
    off *SoX* in the background, applies the effects in the chain, and returns the
    tensor and sample rate information we would have otherwise obtained from `load()`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择要返回给数据加载器的项目时，情况有所不同。我们不再使用`torchaudio.load()`，而是引用我们的效果链，并使用`set_input_file`指向文件。但请注意，这并不会加载文件！相反，我们必须使用`sox_build_flow_effects()`，它在后台启动*SoX*，应用链中的效果，并返回我们通常从`load()`中获得的张量和采样率信息。
- en: The number of things that *SoX* can do is pretty staggering, and I won’t go
    into more detail on all the possible effects you could use. I suggest having a
    look at the [*SoX* documentation](https://oreil.ly/uLBTF) in conjunction with
    `list_effects()` to see the possibilities.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*SoX*可以做的事情数量相当惊人，我不会详细介绍您可以使用的所有可能效果。我建议结合`list_effects()`查看[*SoX*文档](https://oreil.ly/uLBTF)以了解可能性。'
- en: These transformations allow us to alter the original audio, but we’ve spent
    quite a bit of this chapter building up a processing pipeline that works on images
    of mel spectrograms. We could do what we did to generate the initial dataset for
    that pipeline, by creating altered audio samples and then creating the spectrograms
    from them, but at that point we’re creating an awful lot of data that we will
    need to mix together at run-time. Thankfully, we can do some transformations on
    the spectrograms themselves.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换允许我们改变原始音频，但在本章的大部分时间里，我们都在构建一个处理管道，用于处理梅尔频谱图像。我们可以像为该管道生成初始数据集所做的那样，创建修改后的音频样本，然后从中创建频谱图像，但在那时，我们将创建大量数据，需要在运行时混合在一起。幸运的是，我们可以对频谱图像本身进行一些转换。
- en: SpecAugment
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SpecAugment
- en: 'Now, you might be thinking at this point: “Wait, these spectrograms are just
    images! We can use any image transform we want on them!” And yes! Gold star for
    you in the back. But we do have to be a little careful; it’s possible, for example,
    that a random crop may cut out enough frequencies that it potentially changes
    the output class. This is much less of an issue in our ESC-50 dataset, but if
    you were doing something like speech recognition, that would definitely be something
    you’d have to consider when applying augmentations. Another intriguing possibility
    is that because we know that all the spectrograms have the same structure (they’re
    always going to be a frequency graph!), we could create image-based transforms
    that work specifically around that structure.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会想到：“等等，这些频谱图只是图片！我们可以对它们使用任何图片变换！” 是的！后面的你获得金星。但是我们必须小心一点；例如，随机裁剪可能会剪掉足够的频率，从而潜在地改变输出类别。在我们的ESC-50数据集中，这不是一个大问题，但如果你在做类似语音识别的事情，那么在应用增强时肯定要考虑这一点。另一个有趣的可能性是，因为我们知道所有的频谱图具有相同的结构（它们总是一个频率图！），我们可以创建基于图像的变换，专门围绕这种结构工作。
- en: 'In 2019, Google released a paper on SpecAugment,^([3](ch06.html#idm45762357061000))
    which reported new state-of-the-art results on many audio datasets. The team obtained
    these results by using three new data augmentation techniques that they applied
    directly to a mel spectrogram: time warping, frequency masking, and time masking.
    We won’t look at time warping because the benefit derived from it is small, but
    we’ll implement custom transforms for masking time and frequency.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，谷歌发布了一篇关于SpecAugment的论文，[3]在许多音频数据集上报告了新的最先进结果。该团队通过使用三种新的数据增强技术直接应用于梅尔频谱图：时间弯曲、频率掩码和时间掩码，从中获得了这些结果。我们不会讨论时间弯曲，因为从中获得的好处很小，但我们将为掩码时间和频率实现自定义变换。
- en: Frequency masking
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 频率掩码
- en: '*Frequency masking* randomly removes a frequency or set of frequencies from
    our audio input. This attempts to make the model work harder; it cannot simply
    *memorize* an input and its class, because the input will have different frequencies
    masked during each batch. The model will instead have to learn other features
    that can determine how to map the input to a class, which hopefully should result
    in a more accurate model.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*频率掩码*会随机地从我们的音频输入中移除一个频率或一组频率。这样做是为了让模型更加努力；它不能简单地*记忆*输入及其类别，因为在每个批次中输入的不同频率会被掩码。模型将不得不学习其他特征，以确定如何将输入映射到一个类别，这希望会导致一个更准确的模型。'
- en: 'In our mel spectrograms, this is shown by making sure that nothing appears
    in the spectrograph for that frequency at any time step. [Figure 6-7](#frequency-mask-applied)
    shows what this looks like: essentially, a blank line drawn across a natural spectrogram.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的梅尔频谱图中，这通过确保在任何时间步长中该频率的频谱图中没有任何内容来显示。[图6-7](#frequency-mask-applied)展示了这是什么样子：基本上是在自然频谱图上画了一条空白线。
- en: 'Here’s the code for a custom `Transform` that implements frequency masking:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个实现频率掩码的自定义`Transform`的代码：
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'When the transform is applied, PyTorch will call the `__call__` method with
    the tensor representation of the image (so we need to place it in a `Compose`
    chain after the image has been converted to a tensor, not before). We’re assuming
    that the tensor will be in *channels × height × width* format, and we want to
    set the height values in a small range, to either zero or the mean of the image
    (because we’re using log mel spectrograms, the mean should be the same as zero,
    but we include both options so you can experiment to see if one works better than
    the other). The range is provided by the `max_width` parameter, and our resulting
    pixel mask will be between 1 and `max_pixels` wide. We also need to pick a random
    starting point for the mask, which is what the `start` variable is for. Finally,
    the complicated part of this transform—we apply our generated mask:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用变换时，PyTorch将使用图像的张量表示调用`__call__`方法（因此我们需要将其放在将图像转换为张量后的`Compose`链中，而不是之前）。我们假设张量将以*通道×高度×宽度*格式呈现，并且我们希望将高度值设置在一个小范围内，要么为零，要么为图像的平均值（因为我们使用对数梅尔频谱图，平均值应该与零相同，但我们包括两种选项，以便您可以尝试看哪种效果更好）。范围由`max_width`参数提供，我们得到的像素掩码将在1到`max_pixels`之间。我们还需要为掩码选择一个随机起点，这就是`start`变量的作用。最后，这个变换的复杂部分——我们应用我们生成的掩码：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This isn’t quite so bad when we break it down. Our tensor has three dimensions,
    but we want to apply this transform across all the red, green, and blue channels,
    so we use the bare `:` to select everything in that dimension. Using `start:end`,
    we select our height range, and then we select everything in the width channel,
    as we want to apply our mask across every time step. And then on the righthand
    side of the expression, we set the value; in this case, `tensor.mean()`. If we
    take a random tensor from the ESC-50 dataset and apply the transform to it, we
    can see in [Figure 6-7](#frequency-mask-applied) that this class is creating the
    required mask.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将其分解时，情况就没有那么糟糕了。我们的张量有三个维度，但我们希望在所有红色、绿色和蓝色通道上应用这个变换，所以我们使用裸的`:`来选择该维度中的所有内容。使用`start:end`，我们选择我们的高度范围，然后我们选择宽度通道中的所有内容，因为我们希望在每个时间步长上应用我们的掩码。然后在表达式的右侧，我们设置值；在这种情况下，是`tensor.mean()`。如果我们从ESC-50数据集中取一个随机张量并将变换应用于它，我们可以在[图6-7](#frequency-mask-applied)中看到这个类别正在创建所需的掩码。
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Frequency mask applied to random ESC-50 sample](assets/ppdl_0607.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![应用于随机ESC-50样本的频率掩码](assets/ppdl_0607.png)'
- en: Figure 6-7\. Frequency mask applied to a random ESC-50 sample
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7。应用于随机ESC-50样本的频率掩码
- en: Next we’ll turn our attention to time masking.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将转向时间掩码。
- en: Time masking
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间掩码
- en: 'With our frequency mask complete, we can turn to the *time mask*, which does
    the same as the frequency mask, but in the time domain. The code here is mostly
    the same:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的频率掩码完成后，我们可以转向*时间掩码*，它与频率掩码相同，但在时间域中。这里的代码大部分是相同的：
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As you can see, this class is similar to the frequency mask. The only difference
    is that our `start` variable now ranges at some point on the height axis, and
    when we’re doing our masking, we do this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这个类与频率掩码类似。唯一的区别是我们的`start`变量现在在高度轴上的某个点范围内，当我们进行掩码处理时，我们这样做：
- en: '[PRE39]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This indicates that we select all the values of the first two dimensions of
    our tensor and the `start:end` range in the last dimension. And again, we can
    apply this to a random tensor from ESC-50 to see that the mask is being applied
    correctly, as shown in [Figure 6-8](#time-mask-applied).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们选择张量的前两个维度的所有值和最后一个维度中的`start:end`范围。再次，我们可以将这应用于来自ESC-50的随机张量，以查看掩码是否被正确应用，如[图6-8](#time-mask-applied)所示。
- en: '[PRE40]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![Time mask applied to random ESC-50 sample](assets/ppdl_0608.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![应用于随机ESC-50样本的时间掩码](assets/ppdl_0608.png)'
- en: Figure 6-8\. Time mask applied to a random ESC-50 sample
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8。应用于随机ESC-50样本的时间掩码
- en: 'To finish our augmentation, we create a new wrapper transformation that ensures
    that one or both of the masks is applied to a spectrogram image:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们的增强，我们创建一个新的包装器转换，确保一个或两个掩码应用于频谱图像：
- en: '[PRE41]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Try rerunning the training loop with this data augmentation and see if you,
    like Google, achieve better accuracy with these masks. But maybe there’s still
    more that we can try with this dataset?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用这种数据增强重新运行训练循环，看看您是否像谷歌一样通过这些掩码获得更好的准确性。但也许我们还可以尝试更多与这个数据集有关的内容？
- en: Further Experiments
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步实验
- en: 'So far, we’ve created two neural networks—one based on the raw audio waveform,
    and the other based on the images of mel spectrograms—to classify sounds from
    the ESC-50 dataset. Although you’ve seen that the ResNet-powered model is more
    accurate using the power of transfer learning, it would be an interesting experiment
    to create a combination of the two networks to see whether that increases or decreases
    the accuracy. A simple way of doing this would be to revisit the ensembling approach
    from [Chapter 4](ch04.html#transfer-learning-and-other-tricks): just combine and
    average the predictions. Also, we skipped over the idea of building a network
    based on the raw data we were getting from the spectrograms. If a model is created
    that works on that data, does it help overall accuracy if it is introduced to
    the ensemble? We can also use other versions of ResNet, or we could create new
    architectures that use different pretrained models such as VGG or Inception as
    a backbone. Explore some of these options and see what happens; in my experiments,
    SpecAugment improves ESC-50 classification accuracy by around 2%.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了两个神经网络——一个基于原始音频波形，另一个基于mel频谱图像——用于对ESC-50数据集中的声音进行分类。尽管您已经看到基于ResNet的模型在使用迁移学习的力量时更准确，但创建这两个网络的组合来查看是否增加或减少准确性将是一个有趣的实验。这样做的一个简单方法是重新审视第4章中的集成方法：只需组合和平均预测。此外，我们跳过了基于我们从频谱图中获取的原始数据构建网络的想法。如果创建了一个适用于该数据的模型，那么如果将其引入集成，是否会提高整体准确性？我们还可以使用其他版本的ResNet，或者我们可以创建使用不同预训练模型（如VGG或Inception）作为骨干的新架构。探索一些这些选项并看看会发生什么；在我的实验中，SpecAugment将ESC-50分类准确性提高了约2%。
- en: Conclusion
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we used two very different strategies for audio classification,
    took a brief tour of PyTorch’s `torchaudio` library, and saw how to precompute
    transformations on datasets when doing transformations on the fly would have a
    severe impact on training time. We discussed two approaches to data augmentation.
    As an unexpected bonus, we again stepped through how to train an image-based model
    by using transfer learning to quickly generate a classifier with decent accuracy
    compared to the others on the ESC-50 leaderboard.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了两种非常不同的音频分类策略，简要介绍了PyTorch的`torchaudio`库，并看到了在数据集上预先计算转换的方法，而在进行实时转换时会严重影响训练时间。我们讨论了两种数据增强方法。作为一个意外的奖励，我们再次通过使用迁移学习来训练基于图像的模型，快速生成一个与ESC-50排行榜上其他模型相比准确度较高的分类器。
- en: This wraps up our tour through images, test, and audio, though we return to
    all three in [Chapter 9](ch09.html#pytorch_in_the_wild) when we look at some applications
    that use PyTorch. Next up, though, we look at how to debug models when they’re
    not training quite right or fast enough.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对图像、测试和音频的导览，尽管我们将在第9章中再次涉及这三个方面，当我们看一些使用PyTorch的应用程序时。不过，接下来，我们将看看在模型训练不够正确或速度不够快时如何调试模型。
- en: Further Reading
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[“Interpreting and Explaining Deep Neural Networks for Classification of Audio
    Signals”](https://arxiv.org/abs/1807.03418) by Sören Becker et al. (2018)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“解释和说明用于音频信号分类的深度神经网络”](https://arxiv.org/abs/1807.03418) 由Sören Becker等人（2018年）撰写'
- en: '[“CNN Architectures for Large-Scale Audio Classification”](https://arxiv.org/abs/1609.09430v2)
    by Shawn Hershey et al. (2016)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“用于大规模音频分类的CNN架构”](https://arxiv.org/abs/1609.09430v2) 由Shawn Hershey等人（2016年）'
- en: ^([1](ch06.html#idm45762360156264-marker)) Understanding [all of what *SoX*
    can do](http://sox.sourceforge.net) is beyond the scope of this book, and won’t
    be necessary for what we’re going to be doing in the rest of this chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 理解*SoX*可以做什么超出了本书的范围，并且对于我们接下来在本章中要做的事情并不是必要的。
- en: ^([2](ch06.html#idm45762359589592-marker)) See [“Very Deep Convolutional Neural
    Networks for Raw Waveforms”](https://arxiv.org/pdf/1610.00087.pdf) by Wei Dai
    et al. (2016).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[“用于原始波形的非常深度卷积神经网络”](https://arxiv.org/pdf/1610.00087.pdf) 由Wei Dai等人（2016年）。
- en: '^([3](ch06.html#idm45762357061000-marker)) See [“SpecAugment: A Simple Data
    Augmentation Method for Automatic Speech Recognition”](https://arxiv.org/abs/1904.08779)
    by Daniel S. Park et al. (2019).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[“SpecAugment：用于自动语音识别的简单数据增强方法”](https://arxiv.org/abs/1904.08779) 由Daniel
    S. Park等人（2019年）。
