- en: Chapter 6\. A Journey into Sound
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most successful applications of deep learning is something that we
    carry around with us every day. Whether it’s Siri or Google Now, the engines that
    power both systems and Amazon’s Alexa are neural networks. In this chapter, we’ll
    take a look at PyTorch’s `torchaudio` library. You’ll learn how to use it to construct
    a pipeline for classifying audio data with a convolutional-based model. After
    that, I’ll suggest a different approach that will allow you to use some of the
    tricks you learned for images and obtain good accuracy on the ESC-50 audio dataset.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s take a look at sound itself. What is it? How is it often represented
    in data form, and does that provide us with any clues as to what type of neural
    net we should use to gain insight from our data?
  prefs: []
  type: TYPE_NORMAL
- en: Sound
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sound is created via the vibration of air. All the sounds we hear are combinations
    of high and low pressure that we often represent in a waveform, like the one in
    [Figure 6-1](#sine-wave). In this image, the wave above the origin is high pressure,
    and the part below is low pressure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sine wave](assets/ppdl_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Sine wave
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-2](#song-waveform) shows a more complex waveform of a complete song.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Song waveform](assets/ppdl_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Song waveform
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In digital sound, we *sample* this waveform many times a second, traditionally
    44,100 for CD-quality sound, and store the amplitude values of the wave during
    each sample point. At a time *t*, we have a single value stored. This is slightly
    different from an image, which requires two values, *x* and *y*, to store a value
    (for a grayscale image). If we use convolutional filters in our neural network,
    we need a 1D filter rather than the 2D filters we were using for images.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know a little more about sound, let’s look at the dataset we use
    so you can get a little more familiar with it.
  prefs: []
  type: TYPE_NORMAL
- en: The ESC-50 Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Environmental Sound Classification* (ESC) dataset is a collection of field
    recordings, each of which is 5 seconds long and assigned to one of 50 classes
    (e.g., a dog barking, snoring, a knock on a door). We use this set for the rest
    of the chapter to experiment with two ways of classifying audio, as well as to
    explore using `torchaudio` to simplify loading and manipulating audio.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The [ESC-50 dataset](https://github.com/karoldvl/ESC-50) is a set of WAV files.
    You can download it either by cloning the Git repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you can download the entire repo just by using curl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'All the WAV files are stored in the *audio* directory with filenames like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We care about the final number in the filename, because that tells us what class
    this sound clip has been assigned to. The other parts of the filename don’t matter
    to us but mostly relate to the larger Freesound dataset from which ESC-50 has
    been drawn (with one exception that I’ll come back to shortly). If you’re interested
    in finding out more, the *README* document in the ESC-50 repo goes into further
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve downloaded the dataset, let’s look at some of the sounds it contains.
  prefs: []
  type: TYPE_NORMAL
- en: Playing Audio in Jupyter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to actually hear a sound from ESC-50, then instead of loading one
    of the files into a standard music player such as iTunes, you can use Jupyter’s
    built-in player for audio, `IPython.display.Audio`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The function will read in our WAV files and MP3 files. You can also generate
    tensors, convert them into NumPy arrays, and play those directly. Play some of
    the files in the *ESC-50* directory to get a feel for the sounds available. Once
    you’ve done that, we’ll explore the dataset in depth a little more.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring ESC-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with a new dataset, it’s always a good idea to get a feeling for
    the *shape* of the data before you dive right into building models. In classification
    tasks, for example, you’ll want to know whether your dataset actually contains
    examples from all the possible classes, and ideally that all classes are present
    in equal numbers. Let’s take a look at how ESC-50 breaks down.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your dataset has an *unbalanced* amount of data, a simple solution is to
    randomly duplicate the smaller class examples until you have increased them to
    the number of the other classes. Although this feels like fake accounting, it’s
    surprisingly effective (and cheap!) in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the final set of digits in each filename describes the class it
    belongs to, so what we need to do is grab a list of the files and count up the
    occurrences of each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: First, we build up a list of our ESC-50 filenames. Because we care about only
    the class number at the end of the filename, we chop off the *.wav* extension
    and split the filename on the `-` separator. We finally take the last element
    in that split string. If you inspect `esc50_list`, you’ll get a bunch of strings
    that range from 0 to 49\. We could write more code that builds a `dict` and counts
    all the occurrences for us, but I’m lazy, so I’m using a Python convenience function,
    `Counter`, that does all that for us.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the output!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We have one of those rare things, a perfectly balanced dataset. Let’s break
    out the champagne and install a few more libraries that we’re going to need shortly.
  prefs: []
  type: TYPE_NORMAL
- en: SoX and LibROSA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the audio processing that `torchaudio` carries out relies on two other
    pieces of software: *SoX* and *LibROSA*. [*LibROSA*](https://github.com/librosa/librosa)
    is a Python library for audio analysis, including generating mel spectrograms
    (You’ll see what these are a little later in the chapter), detecting beats, and
    even generating music.'
  prefs: []
  type: TYPE_NORMAL
- en: '*SoX*, on the other hand, is a program that you might already be familiar with
    if you’ve been using Linux for years. In fact, *SoX* is so old that it predates
    Linux itself; its first release was in July 1991, compared to the Linux debut
    in September 1991\. I remember using it back in 1997 to convert WAV files into
    MP3s on my first ever Linux box. But it’s still useful!^([1](ch06.html#idm45762360156264))'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re installing `torchaudio` via `conda`, you can skip to the next section.
    If you’re using `pip`, you’ll probably need to install *SoX* itself. For a Red
    Hat-based system, enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Or on a Debian-based system, you’ll use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Once *SoX* is installed, you can move on to obtaining `torchaudio` itself.
  prefs: []
  type: TYPE_NORMAL
- en: torchaudio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installing `torchaudio` can be performed with either `conda` or `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In comparison with `torchvision`, `torchaudio` is similar to `torchtext` in
    that it’s not quite as well loved, maintained, or documented. I’d expect this
    to change in the near future as PyTorch gets more popular and better text and
    audio handling pipelines are created. Still, `torchaudio` is plenty for our needs;
    we just have to write some custom dataloaders (which we didn’t have to do for
    audio or text processing).
  prefs: []
  type: TYPE_NORMAL
- en: Anyhow, the core of `torchaudio` is found within `load()` and `save()`. We’re
    concerned only with `load()` in this chapter, but you’ll need to use `save()`
    if you’re generating new audio from your input (e.g., a text-to-speech model).
    `load()` takes a file specified in `filepath` and returns a tensor representation
    of the audio file and the sample rate of that audio file as a separate variable.
  prefs: []
  type: TYPE_NORMAL
- en: We now have the means for loading one of the WAV files from the ESC-50 dataset
    and turning it into a tensor. Unlike our earlier work with text and images, we
    need to write a bit more code before we can get on with creating and training
    a model. We need to write a custom *dataset*.
  prefs: []
  type: TYPE_NORMAL
- en: Building an ESC-50 Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve talked about datasets in [Chapter 2](ch02.html#image-classification-with-pytorch),
    but `torchvision` and `torchtext` did all the heavy lifting for us, so we didn’t
    have to worry too much about the details. As you may remember, a custom dataset
    has to implement two class methods, `__getitem__` and `__len__`, so that the data
    loader can get a batch of tensors and their labels, as well as a total count of
    tensors in the dataset. We also have an `__init__` method for setting up things
    like file paths that’ll be used over and over again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s our first pass at the ESC-50 dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The majority of the work in the class happens when a new instance of it is created.
    The `__init__` method takes the `path` parameter, finds all the WAV files inside
    that path, and then produces tuples of *`(filename, label)`* by using the same
    string split we used earlier in the chapter to get the label of that audio sample.
    When PyTorch requests an item from the dataset, we index into the `items` list,
    use `torchaudio.load` to make `torchaudio` load in the audio file, turn it into
    a tensor, and then return both the tensor and the label.
  prefs: []
  type: TYPE_NORMAL
- en: 'And that’s enough for us to start with. For a sanity check, let’s create an
    `ESC50` object and extract the first item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can construct a data loader by using standard PyTorch constructs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'But before we do that, we have to go back to our data. As you might remember,
    we should always create training, validation, and test sets. At the moment, we
    have just one directory with all the data, which is no good for our purposes.
    A 60/20/20 split of data into training, validation, and test collections should
    suffice. Now, we could do this by taking random samples of our entire dataset
    (taking care to sample without replacement and making sure that our newly constructed
    datasets are still balanced), but again the ESC-50 dataset saves us from having
    to do much work. The compilers of the dataset separated the data into five equal
    balanced *folds*, indicated by the *first* digit in the filename. We’ll have folds
    `1,2,3` be the training set, `4` the validation set, and `5` the test set. But
    feel free to mix it up if you don’t want to be boring and consecutive! Move each
    of the folds to *test*, *train*, and *validation* directories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create the individual datasets and loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We have our data all set up, so we’re all ready to look at a classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN Model for ESC-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For our first attempt at classifying sounds, we build a model that borrows
    heavily from a paper called “Very Deep Convolutional Networks For Raw Waveforms.”^([2](ch06.html#idm45762359589592))
    You’ll see that it uses a lot of our building blocks from [Chapter 3](ch03.html#convolutional-neural-networks),
    but instead of using 2D layers, we’re using 1D variants, as we have one fewer
    dimension in our audio input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We also need an optimizer and a loss function. For the optimizer, we use Adam
    as before, but what loss function do you think we should use? (If you answered
    `CrossEntropyLoss`, give yourself a gold star!)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Having created our model, we save our weights and use the `find_lr()` function
    from [Chapter 4](ch04.html#transfer-learning-and-other-tricks):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From the plot in [Figure 6-3](#audionet-learning-rate-plot), we determine that
    the appropriate learning rate is around `1e-5` (based on where the descent looks
    steepest). We set that to be our learning rate and reload our model’s initial
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![AudioNet learning rate plot](assets/ppdl_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. AudioNet learning rate plot
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We train the model for 20 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: After training, you should find that the model attains around 13%–17% accuracy
    on our dataset. That’s better than the 2% we could expect if we were just picking
    one of the 50 classes at random. But perhaps we can do better; let’s investigate
    a different way of looking at our audio data that may yield better results.
  prefs: []
  type: TYPE_NORMAL
- en: This Frequency Is My Universe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you look back at the GitHub page for ESC-50, you’ll see a leaderboard of
    network architectures and their accuracy scores. You’ll notice that in comparison,
    we’re not doing great. We could extend the model we’ve created to be deeper, and
    that would likely increase our accuracy a little, but for a real increase in performance,
    we need to switch domains. In audio processing, you can work on the pure waveform
    as we’ve been doing; but most of the time, you’ll work in the *frequency domain*.
    This different representation transforms the raw waveform into a view that shows
    all of the frequencies of sound at a given point in time. This is perhaps a more
    information-rich representation to present to a neural network, as it’ll be able
    to work on those frequencies directly, rather than having to work out how to map
    the raw waveform signal into something the model can use.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to generate frequency spectrograms with *LibROSA*.
  prefs: []
  type: TYPE_NORMAL
- en: Mel Spectrograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Traditionally, getting into the frequency domain requires applying the Fourier
    transform on the audio signal. We’re going to go beyond that a little by generating
    our spectrograms in the mel scale. The *mel scale* defines a scale of pitches
    that are equal in distance from another, where 1000 mels = 1000 Hz. This scale
    is commonly used in audio processing, especially in speech recognition and classification
    applications. Producing a mel spectrogram with *LibROSA* requires two lines of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in a NumPy array containing the spectrogram data. If we display
    this spectrogram as shown in [Figure 6-4](#mel-spectrogram), we can see the frequencies
    in our sound:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Mel Spectrogram](assets/ppdl_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Mel spectrogram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'However, not a lot of information is present in the image. We can do better!
    If we convert the spectrogram to a logarithmic scale, we can see a lot more of
    the audio’s structure, due to the scale being able to represent a wider range
    of values. And this is common enough in audio procressing that *LibROSA* includes
    a method for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This computes a scaling factor of `10 * log10(spectrogram / ref)`. `ref` defaults
    to `1.0`, but here we’re passing in `np.max()` so that `spectrogram / ref` will
    fall within the range of `[0,1]`. [Figure 6-5](#log-mel-spectrogram) shows the
    new spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: '![Log Mel Spectrogram](assets/ppdl_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Log mel spectrogram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We now have a log-scaled mel spectrogram! If you call `log_spectrogram.shape`,
    you’ll see it’s a 2D tensor, which makes sense because we’ve plotted images with
    the tensor. We could create a new neural network architecture and feed this new
    data into it, but I have a diabolical trick up my sleeve. We literally just generated
    images of the spectrogram data. Why don’t we work on those instead?
  prefs: []
  type: TYPE_NORMAL
- en: This might seem silly at first; after all, we have the underlying spectrogram
    data, and that’s more exact than the image representation (to our eyes, knowing
    that a data point is 58 rather than 60 means *more* to us than a different shade
    of, say, purple). And if we were starting from scratch, that’d definitely be the
    case. But! We have, just lying around the place, already-trained networks such
    as ResNet and Inception that we *know* are amazing at recognizing structure and
    other parts of images. We can construct image representations of our audio and
    use a pretrained network to make big jumps in accuracy with very little training
    by using the super power of transfer learning once again. This could be useful
    with our dataset, as we don’t have a lot of examples (only 2,000!) to train our
    network.
  prefs: []
  type: TYPE_NORMAL
- en: This trick can be employed across many disparate datasets. If you can find a
    way of cheaply turning your data into an image representation, it’s worth doing
    that and throwing a ResNet network against it to get a baseline of what transfer
    learning can do for you, so you know what you have to beat by using a different
    approach. Armed with this, let’s create a new dataset that will generate these
    images for us on demand.
  prefs: []
  type: TYPE_NORMAL
- en: A New Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now throw away the original `ESC50` dataset class and build a new one, `ESC50Spectrogram`.
    Although this will share some code with the older class, quite a lot more is going
    on in the `__get_item__` method in this version. We generate the spectrogram by
    using *LibROSA*, and then we do some fancy `matplotlib` footwork to get the data
    into a NumPy array. We apply the array to our transformation pipeline (which just
    uses `ToTensor`) and return that and the item’s label. Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re not going to spend too much time on this version of the dataset because
    it has a large flaw, which I demonstrate with Python’s `process_time()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The new dataset is almost one hundred times slower than our original one that
    just returned the raw audio! That will make training incredibly slow, and may
    even negate any of the benefits we could get from using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use a couple of tricks to get around most of our troubles here. The
    first approach would be to add a cache to store the generated spectrogram in memory,
    so we don’t have to regenerate it every time the `__getitem__` method is called.
    Using Python’s `functools` package, we can do this easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Provided you have enough memory to store the entire contents of the dataset
    into RAM, this may be good enough. We’ve set up a *least recently used* (LRU)
    cache that will keep the contents in memory for as long as possible, with indices
    that haven’t been accessed recently being the first for ejection from the cache
    when memory gets tight. However, if you don’t have enough memory to store everything,
    you’ll hit slowdowns on every batch iteration as ejected spectrograms need to
    be regenerated.
  prefs: []
  type: TYPE_NORMAL
- en: My preferred approach is to *precompute* all the possible plots and then create
    a new custom dataset class that loads these images from the disk. (You can even
    add the LRU cache annotation as well for further speed-up.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t need to do anything fancy for precomputing, just a method that saves
    the plots into the same directory it’s traversing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This method is simpler than our previous dataset because we can use `matplotlib`’s
    `savefig` method to save a plot directly to disk rather than having to mess around
    with NumPy. We also provide an additional input parameter, `dpi`, which allows
    us to control the quality of the generated output. Run this on all the `train`,
    `test`, and `valid` paths that we have already set up (it will likely take a couple
    of hours to get through all the images).
  prefs: []
  type: TYPE_NORMAL
- en: 'All we need now is a new dataset that reads these images. We can’t use the
    standard `ImageDataLoader` from Chapters [2](ch02.html#image-classification-with-pytorch)–[4](ch04.html#transfer-learning-and-other-tricks),
    as the PNG filename scheme doesn’t match the directory structure that it uses.
    But no matter, we can just open an image by using the Python Imaging Library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is much simpler, and hopefully that’s also reflected in the time
    it takes to get an entry from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Obtaining an element from this dataset takes roughly the same time as in our
    original audio-based one, so we won’t be losing anything by moving to our image-based
    approach, except for the one-time cost of precomputing all the images before creating
    the database. We’ve also supplied a default transform pipeline that turns an image
    into a tensor, but it can be swapped out for a different pipeline during initialization.
    Armed with these optimizations, we can start to apply transfer learning to the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: A Wild ResNet Appears
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you may remember from [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    transfer learning requires that we take a model that has already been trained
    on a particular dataset (in the case of images, likely ImageNet), and then fine-tune
    it on our particular data domain, the ESC-50 dataset that we’re turning into spectrogram
    images. You might be wondering whether a model that is trained on *normal* photographs
    is of any use to us. It turns out that the pretrained models *do* learn a lot
    of structure that can be applied to domains that at first glance might seem wildly
    different. Here’s our code from [Chapter 4](ch04.html#transfer-learning-and-other-tricks)
    that initializes a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This initializes us with a pretrained (and frozen) `ResNet50` model and swaps
    out the head of the model for an untrained `Sequential` module that ends with
    a `Linear` with an output of 50, one for each of the classes in the ESC-50 dataset.
    We also need to create a `DataLoader` that takes our precomputed spectrograms.
    When we create our ESC-50 dataset, we’ll also want to normalize the incoming images
    with the standard ImageNet standard deviation and mean, as that’s what the pretrained
    ResNet-50 architecture was trained with. We can do that by passing in a new pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: With our data loaders set up, we can move on to finding a learning rate and
    get ready to train.
  prefs: []
  type: TYPE_NORMAL
- en: Finding a Learning Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to find a learning rate to use in our model. As in [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    we’ll save the model’s initial parameters and use our `find_lr()` function to
    find a decent learning rate for training. [Figure 6-6](#specresnet-learning-rate-plot)
    shows the plot of the losses against the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![SpecResNet learning rate plot](assets/ppdl_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. A SpecResNet learning rate plot
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at the graph of the learning rate plotted against loss, it seems like
    `1e-2` is a good place to start. As our ResNet-50 model is somewhat deeper than
    our previous one, we’re also going to use differential learning rates of `[1e-2,1e-4,1e-8]`,
    with the highest learning rate applied to our classifier (as it requires the most
    training!) and slower rates for the already-trained backbone. Again, we use Adam
    as our optimizer, but feel free to experiment with the others available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we apply those differential rates, though, we train for a few epochs
    that update only the classifier, as we *froze* the ResNet-50 backbone when we
    created our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We now unfreeze the backbone and apply our differential rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, with a validation accuracy of around 80%, we’re already vastly
    outperforming our original `AudioNet` model. The power of transfer learning strikes
    again! Feel free to train for more epochs to see if your accuracy continues to
    improve. If we look at the ESC-50 leaderboard, we’re closing in on human-level
    accuracy. And that’s just with ResNet-50\. You could try with ResNet-101 and perhaps
    an ensemble of different architectures to push the score up even higher.
  prefs: []
  type: TYPE_NORMAL
- en: And there’s data augmentation to consider. Let’s take a look at a few ways of
    doing that in both domains that we’ve been working in so far.
  prefs: []
  type: TYPE_NORMAL
- en: Audio Data Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we were looking at images in [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    we saw that we could improve the accuracy of our classifier by making changes
    to our incoming pictures. By flipping them, cropping them, or applying other transformations,
    we made our neural network work harder in the training phase and obtained a more
    *generalized* model at the end of it, one that was not simply fitting to the data
    presented (the scourge of overfitting, don’t forget). Can we do the same here?
    Yes! In fact, there are two approaches that we can use—one obvious approach that
    works on the original audio waveform, and a perhaps less-obvious idea that arises
    from our decision to use a ResNet-based classifier on images of mel spectrograms.
    Let’s take a look at audio transforms first.
  prefs: []
  type: TYPE_NORMAL
- en: torchaudio Transforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a similar manner to `torchvision`, `torchaudio` includes a `transforms`
    module that perform transformations on incoming data. However, the number of transformations
    offered is somewhat sparse, especially compared to the plethora that we get when
    we’re working with images. If you’re interested, have a look at the [documentation](https://oreil.ly/d1kp6)
    for a full list, but the only one we look at here is `torchaudio.transforms.PadTrim`.
    In the ESC-50 dataset, we are fortunate in that every audio clip is the same length.
    That isn’t something that happens in the real world, but our neural networks like
    (and sometimes insist on, depending on how they’re constructed) input data to
    be regular. `PadTrim` will take an incoming audio tensor and either pad it out
    to the required length, or trim it down so it doesn’t exceed that length. If we
    wanted to trim down a clip to a new length, we’d use `PadTrim` like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: However, if you’re looking for augmentation that actually changes how the audio
    sounds (e.g., adding an echo, noise, or changing the tempo of the clip), then
    the `torchaudio.transforms` module is of no use to you. Instead, we need to use
    *SoX*.
  prefs: []
  type: TYPE_NORMAL
- en: SoX Effect Chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Why it’s not part of the `transforms` module, I’m really not sure, but `torchaudio.sox_effects.SoxEffectsChain`
    allows you to create a chain of one or more *SoX* effects and apply those to an
    input file. The interface is a bit fiddly, so let’s see it in action in a new
    version of the dataset that changes the pitch of the audio file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In our `__init__` method, we create a new instance variable, `E`, a `SoxEffectsChain`,
    that will contain all the effects that we want to apply to our audio data. We
    then add a new effect by using `append_effect_to_chain`, which takes a string
    indicating the name of the effect, and an array of parameters to send to `sox`.
    You can get a list of available effects by calling `torchaudio.sox_effects.effect_names()`.
    If we were to add another effect, it would take place after the pitch effect we
    have already set up, so if you want to create a list of separate effects and randomly
    apply them, you’ll need to create separate chains for each one.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to selecting an item to return to the data loader, things are
    a little different. Instead of using `torchaudio.load()`, we refer to our effects
    chain and point it to the file by using `set_input_file`. But note that this doesn’t
    load the file! Instead, we have to use `sox_build_flow_effects()`, which kicks
    off *SoX* in the background, applies the effects in the chain, and returns the
    tensor and sample rate information we would have otherwise obtained from `load()`.
  prefs: []
  type: TYPE_NORMAL
- en: The number of things that *SoX* can do is pretty staggering, and I won’t go
    into more detail on all the possible effects you could use. I suggest having a
    look at the [*SoX* documentation](https://oreil.ly/uLBTF) in conjunction with
    `list_effects()` to see the possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: These transformations allow us to alter the original audio, but we’ve spent
    quite a bit of this chapter building up a processing pipeline that works on images
    of mel spectrograms. We could do what we did to generate the initial dataset for
    that pipeline, by creating altered audio samples and then creating the spectrograms
    from them, but at that point we’re creating an awful lot of data that we will
    need to mix together at run-time. Thankfully, we can do some transformations on
    the spectrograms themselves.
  prefs: []
  type: TYPE_NORMAL
- en: SpecAugment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, you might be thinking at this point: “Wait, these spectrograms are just
    images! We can use any image transform we want on them!” And yes! Gold star for
    you in the back. But we do have to be a little careful; it’s possible, for example,
    that a random crop may cut out enough frequencies that it potentially changes
    the output class. This is much less of an issue in our ESC-50 dataset, but if
    you were doing something like speech recognition, that would definitely be something
    you’d have to consider when applying augmentations. Another intriguing possibility
    is that because we know that all the spectrograms have the same structure (they’re
    always going to be a frequency graph!), we could create image-based transforms
    that work specifically around that structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2019, Google released a paper on SpecAugment,^([3](ch06.html#idm45762357061000))
    which reported new state-of-the-art results on many audio datasets. The team obtained
    these results by using three new data augmentation techniques that they applied
    directly to a mel spectrogram: time warping, frequency masking, and time masking.
    We won’t look at time warping because the benefit derived from it is small, but
    we’ll implement custom transforms for masking time and frequency.'
  prefs: []
  type: TYPE_NORMAL
- en: Frequency masking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Frequency masking* randomly removes a frequency or set of frequencies from
    our audio input. This attempts to make the model work harder; it cannot simply
    *memorize* an input and its class, because the input will have different frequencies
    masked during each batch. The model will instead have to learn other features
    that can determine how to map the input to a class, which hopefully should result
    in a more accurate model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our mel spectrograms, this is shown by making sure that nothing appears
    in the spectrograph for that frequency at any time step. [Figure 6-7](#frequency-mask-applied)
    shows what this looks like: essentially, a blank line drawn across a natural spectrogram.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code for a custom `Transform` that implements frequency masking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'When the transform is applied, PyTorch will call the `__call__` method with
    the tensor representation of the image (so we need to place it in a `Compose`
    chain after the image has been converted to a tensor, not before). We’re assuming
    that the tensor will be in *channels × height × width* format, and we want to
    set the height values in a small range, to either zero or the mean of the image
    (because we’re using log mel spectrograms, the mean should be the same as zero,
    but we include both options so you can experiment to see if one works better than
    the other). The range is provided by the `max_width` parameter, and our resulting
    pixel mask will be between 1 and `max_pixels` wide. We also need to pick a random
    starting point for the mask, which is what the `start` variable is for. Finally,
    the complicated part of this transform—we apply our generated mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This isn’t quite so bad when we break it down. Our tensor has three dimensions,
    but we want to apply this transform across all the red, green, and blue channels,
    so we use the bare `:` to select everything in that dimension. Using `start:end`,
    we select our height range, and then we select everything in the width channel,
    as we want to apply our mask across every time step. And then on the righthand
    side of the expression, we set the value; in this case, `tensor.mean()`. If we
    take a random tensor from the ESC-50 dataset and apply the transform to it, we
    can see in [Figure 6-7](#frequency-mask-applied) that this class is creating the
    required mask.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Frequency mask applied to random ESC-50 sample](assets/ppdl_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Frequency mask applied to a random ESC-50 sample
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next we’ll turn our attention to time masking.
  prefs: []
  type: TYPE_NORMAL
- en: Time masking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With our frequency mask complete, we can turn to the *time mask*, which does
    the same as the frequency mask, but in the time domain. The code here is mostly
    the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this class is similar to the frequency mask. The only difference
    is that our `start` variable now ranges at some point on the height axis, and
    when we’re doing our masking, we do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that we select all the values of the first two dimensions of
    our tensor and the `start:end` range in the last dimension. And again, we can
    apply this to a random tensor from ESC-50 to see that the mask is being applied
    correctly, as shown in [Figure 6-8](#time-mask-applied).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![Time mask applied to random ESC-50 sample](assets/ppdl_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Time mask applied to a random ESC-50 sample
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To finish our augmentation, we create a new wrapper transformation that ensures
    that one or both of the masks is applied to a spectrogram image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Try rerunning the training loop with this data augmentation and see if you,
    like Google, achieve better accuracy with these masks. But maybe there’s still
    more that we can try with this dataset?
  prefs: []
  type: TYPE_NORMAL
- en: Further Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve created two neural networks—one based on the raw audio waveform,
    and the other based on the images of mel spectrograms—to classify sounds from
    the ESC-50 dataset. Although you’ve seen that the ResNet-powered model is more
    accurate using the power of transfer learning, it would be an interesting experiment
    to create a combination of the two networks to see whether that increases or decreases
    the accuracy. A simple way of doing this would be to revisit the ensembling approach
    from [Chapter 4](ch04.html#transfer-learning-and-other-tricks): just combine and
    average the predictions. Also, we skipped over the idea of building a network
    based on the raw data we were getting from the spectrograms. If a model is created
    that works on that data, does it help overall accuracy if it is introduced to
    the ensemble? We can also use other versions of ResNet, or we could create new
    architectures that use different pretrained models such as VGG or Inception as
    a backbone. Explore some of these options and see what happens; in my experiments,
    SpecAugment improves ESC-50 classification accuracy by around 2%.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used two very different strategies for audio classification,
    took a brief tour of PyTorch’s `torchaudio` library, and saw how to precompute
    transformations on datasets when doing transformations on the fly would have a
    severe impact on training time. We discussed two approaches to data augmentation.
    As an unexpected bonus, we again stepped through how to train an image-based model
    by using transfer learning to quickly generate a classifier with decent accuracy
    compared to the others on the ESC-50 leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: This wraps up our tour through images, test, and audio, though we return to
    all three in [Chapter 9](ch09.html#pytorch_in_the_wild) when we look at some applications
    that use PyTorch. Next up, though, we look at how to debug models when they’re
    not training quite right or fast enough.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[“Interpreting and Explaining Deep Neural Networks for Classification of Audio
    Signals”](https://arxiv.org/abs/1807.03418) by Sören Becker et al. (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“CNN Architectures for Large-Scale Audio Classification”](https://arxiv.org/abs/1609.09430v2)
    by Shawn Hershey et al. (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch06.html#idm45762360156264-marker)) Understanding [all of what *SoX*
    can do](http://sox.sourceforge.net) is beyond the scope of this book, and won’t
    be necessary for what we’re going to be doing in the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.html#idm45762359589592-marker)) See [“Very Deep Convolutional Neural
    Networks for Raw Waveforms”](https://arxiv.org/pdf/1610.00087.pdf) by Wei Dai
    et al. (2016).
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch06.html#idm45762357061000-marker)) See [“SpecAugment: A Simple Data
    Augmentation Method for Automatic Speech Recognition”](https://arxiv.org/abs/1904.08779)
    by Daniel S. Park et al. (2019).'
  prefs: []
  type: TYPE_NORMAL
