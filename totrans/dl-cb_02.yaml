- en: Chapter 2\. Getting Unstuck
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。摆脱困境
- en: Deep learning models are often treated as a black box; we pour data in at one
    end and an answer comes out at the other without us having to care much about
    how our network learns. While it is true that deep neural nets can be remarkably
    good at distilling a signal out of complex input data, the flip side of treating
    these networks as black boxes is that it isn’t always clear what to do when things
    get stuck.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型经常被视为黑匣子；我们在一端输入数据，另一端输出答案，而我们不必太关心网络是如何学习的。虽然深度神经网络确实擅长从复杂输入数据中提取信号，但将这些网络视为黑匣子的反面是，当事情陷入困境时并不总是清楚该怎么办。
- en: A common theme among the techniques we discuss here is that we want the network
    to *generalize* rather than to *memorize*. It is worth pondering the question
    of why neural networks generalize at all. Some of the models described in this
    book and used in production contain millions of parameters that would allow the
    network to memorize inputs with very many examples. If everything goes well, though,
    it doesn’t do this, but rather develops generalized rules about its input.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的技术中的一个共同主题是，我们希望网络*泛化*而不是*记忆*。值得思考的问题是为什么神经网络总体化。本书中描述并用于生产的一些模型包含数百万个参数，这些参数允许网络记忆大量示例的输入。然而，如果一切顺利，它不会这样做，而是会发展出关于其输入的泛化规则。
- en: If things don’t go well, you can try the techniques described in this chapter.
    We’ll start out by looking at how we know that we’re stuck. We’ll then look at
    various ways in which we can preprocess our input data to make it easier for the
    network to work with.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果事情不顺利，您可以尝试本章描述的技术。我们将首先看看如何知道我们陷入困境。然后，我们将看看各种方法，我们可以预处理输入数据，使网络更容易处理。
- en: 2.1 Determining That You Are Stuck
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.1 确定您陷入困境
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you know when your network is stuck?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的网络陷入困境时，您如何知道？
- en: Solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Look at various metrics while the network trains.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络训练时查看各种指标。
- en: The most common signs that things are not well with a neural network are that
    the network is not learning anything or that it is learning the wrong thing. When
    we set up the network, we specify the *loss function*. This determines what the
    network is trying to optimize for. During training the loss is continuously printed.
    If this value doesn’t go down after a few iterations, we’re in trouble. The network
    is not learning anything measured by its own notion of progress.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络出现问题的最常见迹象是网络没有学到任何东西，或者学到了错误的东西。当我们设置网络时，我们指定*损失函数*。这决定了网络试图优化的内容。在训练过程中，损失会持续打印。如果这个值在几次迭代后没有下降，我们就有麻烦了。网络没有学到任何东西，根据自己的进展概念来衡量。
- en: A second metric that comes in handy is *accuracy*. This shows the percentage
    of the inputs for which the network is predicting the right answer. As the loss
    goes down, the accuracy should go up. If accuracy does not go up even though the
    loss is decreasing, then our network is learning something, but not the thing
    we were hoping for. Accuracy can take a while, though, to pick up. A complex visual
    network will take a long time before it gets any labels right while still learning,
    so take this into account before giving up prematurely.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方便的指标是*准确率*。这显示网络预测正确答案的输入百分比。随着损失的降低，准确率应该提高。如果准确率没有提高，即使损失在减少，那么我们的网络正在学习某些东西，但不是我们希望的东西。准确率可能需要一段时间才能提高。一个复杂的视觉网络在学习的同时可能需要很长时间才能正确地获取任何标签，因此在过早放弃之前，请考虑这一点。
- en: The third thing to look for, and this is probably the most common way to get
    stuck, is *overfitting*. With overfitting we see our loss decrease and our accuracy
    increase, but the accuracy we see over our testing set doesn’t keep up. Assuming
    we have a testing set and have added this to the metrics to track, we can see
    this each time an epoch finishes. Typically the testing accuracy at first increases
    with the accuracy of the training set, but then a gap appears, and oftentimes
    the testing accuracy even starts to drop while the training accuracy keeps increasing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要寻找的第三个问题，这可能是陷入困境的最常见方式，是*过拟合*。过拟合时，我们看到损失减少，准确率增加，但我们在测试集上看到的准确率并没有跟上。假设我们有一个测试集，并已将其添加到要跟踪的指标中，我们可以在每个时代结束时看到这一点。通常，测试准确率一开始会随着训练集的准确率增加，但随后会出现差距，而且测试准确率甚至有时会开始下降，而训练准确率则继续增加。
- en: What’s happening here is that our network is learning a direct mapping between
    the inputs and the expected outputs, rather than learning generalizations. As
    long as it sees an input it has seen before, everything looks cool. But confronted
    with a sample from the test set, which it hasn’t seen during training, it starts
    to fail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的情况是，我们的网络正在学习输入和预期输出之间的直接映射，而不是学习泛化。只要它看到之前见过的输入，一切都很顺利。但当面对测试集中的样本时，它开始失败。
- en: Discussion
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Paying attention to the metrics that are displayed during training is a good
    way to keep track of the progress of the learning process. The three metrics we
    discussed here are the most important, but frameworks like Keras offer many more
    and the option to build them yourselves.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关注训练过程中显示的指标是跟踪学习过程进展的好方法。我们在这里讨论的三个指标是最重要的，但像Keras这样的框架提供了更多选项，也可以自己构建它们。
- en: 2.2 Solving Runtime Errors
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.2 解决运行时错误
- en: Problem
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: What should you do when your network complains about incompatible shapes?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的网络抱怨不兼容的形状时，您应该怎么办？
- en: Solution
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Look at the network structure and experiment with different numbers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查看网络结构，并尝试不同的数字。
- en: Keras is a great abstraction over hairier frameworks like TensorFlow or Theano,
    but like any abstraction, this comes at a cost. When all is well our clearly defined
    model runs happily on top of TensorFlow or Theano. When it doesn’t, though, we
    get an error from the depths of the underlying framework. These errors are hard
    to make sense of without understanding the intricacies of those frameworks—which
    is what we wanted to avoid in the first place by using Keras.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是对TensorFlow或Theano等复杂框架的很好的抽象，但是像任何抽象一样，这是有代价的。当一切顺利时，我们清晰定义的模型可以在TensorFlow或Theano之上愉快地运行。但是，当出现问题时，我们会从底层框架的深处收到错误。如果不了解这些框架的复杂性，这些错误很难理解，而我们使用Keras的初衷就是要避免这种情况。
- en: 'There are two things that can help and don’t require us to go on a deep dive.
    The first is to print the structure of our network. Let’s say we have a simple
    model that takes in five variables and classifies into eight categories:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有两件事可以帮助我们，而不需要深入研究。第一件事是打印网络的结构。假设我们有一个简单的模型，接受五个变量并分类为八个类别：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can now inspect the model with:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下方式检查模型：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now if we get a runtime error about an incompatible shape, of the feared form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们遇到一个关于不兼容形状的运行时错误，可能不是我们担心的形式：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: we know something internal must be wrong that isn’t easy to track down using
    the stack trace. There are some other things to try, though.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道内部肯定有问题，这不容易通过堆栈跟踪来追踪。不过，还有一些其他尝试的方法。
- en: First, take a look at whether any of the shapes are either `X` or `Y`. If so,
    that’s probably where the problem is. Knowing that is half the work—which of course
    still leaves the other half. The other thing to pay attention to is the names
    of the layers. Often they come back in the error message, sometimes in a mangled
    form. Keras auto-assigns names to anonymous layers, so looking at the summary
    is useful in this respect too. If needed we can assign our own names, like with
    the input layer in the example shown here.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，看看是否有任何形状是`X`或`Y`。如果是这样，那可能就是问题所在。知道这一点已经完成了一半的工作——当然还有另一半。另一件需要注意的事情是层的名称。它们经常出现在错误消息中，有时以扭曲的形式出现。Keras会自动为匿名层分配名称，因此查看摘要在这方面也很有用。如果需要，我们可以为自己分配名称，就像在这里显示的示例中的输入层一样。
- en: 'If we can’t find the shape or the name that the runtime error is mentioning,
    we can try something else before having to dive in (or post on StackOverflow):
    use different numbers.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找不到运行时错误提到的形状或名称，我们可以在深入研究之前尝试其他方法（或在StackOverflow上发布）：使用不同的数字。
- en: Neural networks contain loads of hyperparameters, like the sizes of the various
    layers. These are usually picked because they seem reasonable, given other networks
    that do similar things. But their actual value is somewhat arbitrary. In our example,
    does the hidden layer really need 12 units? Would 11 do a lot worse, and would
    13 lead to overfitting?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络包含大量的超参数，如各个层的大小。这些通常是因为它们看起来合理，与做类似事情的其他网络相似。但是它们的实际值有些是任意的。在我们的示例中，隐藏层是否真的需要12个单元？11个会差很多吗，13会导致过拟合吗？
- en: Probably not. We tend to pick numbers that feel nice, often powers of two. So
    if you are stuck on a runtime error, change these numbers and see what it does
    to the error message. If the error message remains the same, the variable that
    you changed has nothing to do with it. Once it starts changing, though, you know
    you’ve reached something related.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们倾向于选择感觉好的数字，通常是2的幂。因此，如果您遇到运行时错误，请更改这些数字，看看它对错误消息有何影响。如果错误消息仍然相同，则您更改的变量与此无关。但是一旦开始更改，您就知道已经接近相关内容了。
- en: 'This can be subtle. For example, some networks require that all batches have
    the same size. If your data isn’t divisible by the batch size, your last batch
    will be too small and you’ll get an error like:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能有点微妙。例如，有些网络要求所有批次具有相同的大小。如果您的数据不能被批次大小整除，那么最后一个批次将太小，您将收到类似以下错误的错误消息：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here `X` would be the batch size and `Y` the size of your last incomplete batch.
    You might recognize `X` as your batch size, but `Y` is hard to place. But if you
    change the batch size, `Y` also changes, which provides a hint as to where to
    look.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`X`将是批量大小，`Y`将是您最后一个不完整批次的大小。您可能会认出`X`作为批量大小，但是很难确定`Y`的位置。但是，如果更改批量大小，`Y`也会更改，这为您提供了一个查找位置的提示。
- en: Discussion
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Understanding errors reported by the framework that is abstracted away by Keras
    is fundamentally tricky. The abstraction breaks, and we suddenly see the internals
    of the machinery. The techniques from this recipe allow you to postpone looking
    into those details by spotting shapes and names in the errors and, failing that,
    experimenting with numbers and seeing what changes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 理解由Keras抽象的框架报告的错误基本上是棘手的。抽象被打破，我们突然看到了机器的内部。这个配方中的技术允许您通过发现错误中的形状和名称来推迟查看这些细节，如果失败，则尝试使用数字并查看变化。
- en: 2.3 Checking Intermediate Results
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.3 检查中间结果
- en: Problem
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Your network quickly gets to a promising level of accuracy but refuses to go
    beyond that.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您的网络很快达到了一个令人满意的准确度水平，但拒绝超越这一水平。
- en: Solution
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Check whether it hasn’t gotten stuck at an obvious local maximum.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 检查是否没有陷入明显的局部最大值。
- en: One situation in which this can happen is when one label is far more common
    than any others, and your network quickly learns that always predicting this outcome
    gives decent results. It is not hard to verify that this is happening; just feed
    the network a sample of inputs and look at the outputs. If all outputs are the
    same, you are stuck this way.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况可能发生在一个标签比其他任何标签都更常见的情况下，您的网络很快学会了总是预测这种结果会得到不错的结果。验证这一点并不难；只需向网络提供一些输入样本并查看输出。如果所有输出都相同，您就陷入了这种情况。
- en: Some of the following recipes in this chapter offer suggestions for how to fix
    this. Alternatively, you could play with the distribution of the data. If 95%
    of your examples are dogs and only 5% cats, the network might not see enough cats.
    By artificially changing the distribution to, say, 65%/35%, you make it a little
    easier for the network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的一些配方提供了如何解决这个问题的建议。或者，你可以调整数据的分布。如果你的例子中有95%是狗，只有5%是猫，那么网络可能看不到足够的猫。通过人为改变分布，比如说，65%/35%，可以让网络更容易一些。
- en: This is, of course, not without its own risks. The network might now have more
    of a chance to learn about cats, but it will also learn the wrong base distribution,
    or prior. This means that in case of doubt the network will now be more likely
    to pick “cat” as the answer, even though, all things being equal, “dog” is more
    likely.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这也不是没有风险的。网络现在可能有更多机会了解猫，但它也会学习错误的基础分布，或先验。这意味着在疑惑的情况下，网络现在更有可能选择“猫”作为答案，尽管一切相等，“狗”更有可能。
- en: Discussion
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Looking at the distribution of output labels of a network for a small sample
    of inputs is an easy way to get an idea of what is actually being done, yet it
    is often overlooked. Playing with the distribution is a way to try to get the
    network unstuck if it focuses on just the top answer, but you should probably
    consider other techniques too.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 查看网络输出标签的分布对于一小部分输入样本是了解实际操作的简单方法，但往往被忽视。调整分布是尝试使网络摆脱僵局的一种方式，如果网络只关注顶部答案，你可能应该考虑其他技术。
- en: There are other things to look out for in the output when a network isn’t converging
    quickly; the occurrence of NaNs is an indication of exploding gradients, and if
    the outputs of your network seem to be clipped and can’t seem to reach the right
    values, you might have an incorrect activation function on your final layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络不快速收敛时，还有其他要注意的输出情况；NaN的出现表明梯度爆炸，如果网络的输出似乎被截断，无法达到正确的值，那么你可能在最终层上使用了不正确的激活函数。
- en: 2.4 Picking the Right Activation Function (for Your Final Layer)
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.4 选择正确的激活函数（用于最终层）
- en: Problem
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you pick the right activation function for your final layer when things
    are off?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当情况不对时，如何为最终层选择正确的激活函数？
- en: Solution
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Make sure that the activation function corresponds with the intention of the
    network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 确保激活函数与网络的意图相对应。
- en: A good way to get started with deep learning is to find an example online somewhere
    and modify it step by step until it does what you want it to do. However, if the
    intention of the example network is different from what your goal, you might have
    to change the activation function of the final layer. Let’s take a look at some
    common choices.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 开始深度学习的一个好方法是在网上找到一个示例，并逐步修改直到它达到你想要的效果。然而，如果示例网络的意图与你的目标不同，你可能需要更改最终层的激活函数。让我们看一些常见的选择。
- en: The softmax activation function makes sure that the sum of the output vector
    is exactly 1\. This is an appropriate activation function for networks that output
    exactly one label for a given input (for example, an image classifier). The output
    vector will then represent the probability distribution—if the entry for “cat”
    in the output vector is .65, then the network thinks that it sees a cat with 65%
    certainty. Softmax only works when there is one answer. When multiple answers
    are possible, give the sigmoid activation a try.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax激活函数确保输出向量的总和恰好为1。这是一个适合网络的激活函数，它为给定输入输出一个标签（例如，图像分类器）。输出向量将表示概率分布——如果输出向量中“猫”的条目为0.65，那么网络认为它以65%的确定性看到了一只猫。Softmax只在有一个答案时有效。当可能有多个答案时，可以尝试使用sigmoid激活。
- en: A linear activation function is appropriate for regression problems when we
    need to predict a numeric value given an input. An example would be to predict
    a movie rating given a series of movie reviews. The linear activation function
    will take the values of the previous layer and multiply them with a set of weights
    such that it best fits the expected output. Just as it is a good idea to normalize
    the input data into a [–1, 1] range or thereabouts, it often helps to do the same
    for outputs. So, if our movie ratings are between 0 and 5, we’d subtract 2.5 and
    divide by the same when creating the training data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数适用于回归问题，当我们需要根据输入预测数值时。一个例子是根据一系列电影评论预测电影评分。线性激活函数将取前一层的值，并将它们与一组权重相乘，以使其最好地适应预期输出。就像将输入数据归一化为[-1,
    1]范围或附近一样是一个好主意，通常也有助于对输出进行相同的处理。因此，如果我们的电影评分在0到5之间，我们会在创建训练数据时减去2.5并除以相同的值。
- en: If the network outputs an image, make sure that the activation function you
    use is in line with how you normalize the pixels. The standard normalization of
    deducting the mean pixel value and dividing by the standard deviation results
    in values that center around 0, so it won’t work with sigmoid, and since 30% of
    the values will fall outside the range [–1, 1] tanh won’t be a good fit either.
    You can still use these, but you’d have to change the normalization applied to
    your output.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络输出图像，请确保所使用的激活函数与像素的归一化方式一致。标准的减去平均像素值并除以标准差的归一化会导致值围绕0中心，因此它不适用于sigmoid，而且由于30%的值将落在范围[-1,
    1]之外，tanh也不是一个好选择。你仍然可以使用这些，但你需要改变应用于输出的归一化。
- en: Depending on what you know about the output distribution, it might be useful
    to do something even more fancy. Movie ratings, for example, tend to center around
    3.7 or so, so using that as the center could well yield better results. When the
    actual distribution is skewed such that values around the average are much more
    likely than outliers, using a tanh activation function can be appropriate. This
    squashes any value into a [–1, 1] range. By mapping the expected outputs to the
    same range, keeping the expected distribution in mind, we can mimic any shape
    of our output data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您对输出分布的了解，可能会有更高级的方法。例如，电影评分通常集中在3.7左右，因此以此作为中心可能会产生更好的结果。当实际分布偏斜，使得平均值附近的值比异常值更有可能时，使用tanh激活函数可能是合适的。这将任何值压缩到[-1,
    1]范围内。通过将预期输出映射到相同的范围，考虑预期分布，我们可以模拟输出数据的任何形状。
- en: Discussion
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Picking the right output activation function is crucial, but in most cases not
    difficult. If your output represents a probability distribution with one possible
    outcome, softmax is for you; otherwise, you need to experiment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的输出激活函数至关重要，但在大多数情况下并不困难。如果您的输出代表具有一个可能结果的概率分布，那么softmax适合您；否则，您需要进行实验。
- en: You also need to make sure that the loss function works with the activation
    function of the final layer. The loss function steers the training of the network
    by calculating how “wrong” a prediction is, given an expected value. We saw that
    a softmax activation function is the right choice when a network does multilabel
    predictions; in that case you probably want to go with a categorical loss function
    like Keras’s `categorical_crossentropy`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要确保损失函数与最终层的激活函数配合使用。损失函数通过计算给定预期值时预测有多“错误”来引导网络的训练。我们看到当网络进行多标签预测时，softmax激活函数是正确的选择；在这种情况下，您可能希望选择像Keras的`categorical_crossentropy`这样的分类损失函数。
- en: 2.5 Regularization and Dropout
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.5 正则化和Dropout
- en: Problem
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Once you have detected your network is overfitting, what can you do about it?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您发现您的网络过拟合了，您可以采取什么措施？
- en: Solution
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Restrict what the network can do by using regularization and dropout.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用正则化和dropout来限制网络的功能。
- en: A neural network with enough parameters can fit any input/output mapping by
    memorizing. Accuracy seems great while training, but of course the network fails
    to perform very well on data it hasn’t seen before and so does poorly on the test
    data or indeed in production. The network is overfitting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有足够参数的神经网络可以通过记忆来适应任何输入/输出映射。在训练时准确率看起来很好，但当然网络在未见过的数据上表现不佳，因此在测试数据或实际生产中表现不佳。网络过拟合了。
- en: One obvious way to stop the network from overfitting is to reduce the number
    of parameters that we have by decreasing the number of layers or making each layer
    smaller. But this of course also reduces the expressive power of our network.
    Regularization and dropout offer us something in between by restricting the expressive
    power of our network in a way that doesn’t hurt the ability to learn (too much).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 防止网络过拟合的一个明显方法是通过减少参数的数量，可以通过减少层数或使每一层更小来实现。但这当然也会降低网络的表达能力。正则化和dropout在这方面为我们提供了一种折中方案，通过限制网络的表达能力，而不会损害学习的能力（太多）。
- en: With regularization we add penalties to *extreme* values for parameters. The
    intuition here is that in order to fit an arbitrary input/output mapping, a network
    would need arbitrary parameters, while learned parameters tend to be in a normal
    range. So, making it harder to get to those arbitrary parameters should keep the
    network on the path of learning rather than memorizing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正则化，我们对参数的*极端*值添加惩罚。这里的直觉是，为了适应任意的输入/输出映射，网络需要任意的参数，而学习到的参数往往在一个正常范围内。因此，使得难以达到那些任意参数应该保持网络在学习的道路上，而不是记忆。
- en: 'Application in Keras is straightforward:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中的应用很简单：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Regularizers can be applied to the weights of the kernel or the bias of the
    layer, or to the output of the layer. Which one and what penalty to use is mostly
    a matter of trial and error. 0.01 seems like a popular starting value.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化器可以应用于核的权重或层的偏置，也可以应用于层的输出。选择哪种方法和使用什么惩罚主要是一个试错的问题。0.01似乎是一个受欢迎的起始值。
- en: Dropout is a similar technique, but more radical. Rather than keeping the weights
    of neurons in check, we randomly ignore a percentage of all neurons during training.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种类似的技术，但更激进。与保持神经元的权重不同，我们在训练期间随机忽略一定百分比的所有神经元。
- en: Similar to regularization, this makes it harder for a network to memorize input/output
    pairs, since it can’t rely on specific neurons working during training. This nudges
    the network into learning general, robust features rather than one-off, specific
    ones to cover one training instance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与正则化类似，这使得网络更难记住输入/输出对，因为它不能依赖特定的神经元在训练期间工作。这促使网络学习一般的、稳健的特征，而不是一次性的、特定的特征来覆盖一个训练实例。
- en: 'In Keras dropout is applied to a layer using the `Dropout` (pseudo)layer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，通过使用`Dropout`（伪）层将dropout应用于一个层：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This applies a dropout of 30% to the max-pooling layer, ignoring 30% of its
    neurons during training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在最大池化层应用30%的dropout，在训练期间忽略30%的神经元。
- en: When doing inference, dropout is not applied. All things being equal this would
    increase the output of the layer by over 40%, so the framework automatically scales
    these outputs back.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行推断时，不会应用dropout。一切相等的情况下，这将使层的输出增加超过40%，因此框架会自动将这些输出缩小。
- en: Discussion
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: As you make your network more expressive, its tendency to overfit or memorize
    its inputs rather than learn general features will increase. Both regularization
    and dropout can play a role to reduce this effect. Both work by reducing the freedom
    of the network to develop arbitrary features, by punishing extreme values (regularization)
    or by ignoring the contribution of a percentage of the neurons in a layer (dropout).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络变得更具表现力，它倾向于过拟合或记忆输入而不是学习一般特征的倾向会增加。正则化和dropout都可以起到减少这种影响的作用。它们都通过减少网络发展任意特征的自由度来起作用，通过惩罚极端值（正则化）或忽略层中百分比的神经元的贡献（dropout）。
- en: An interesting alternative way to look at how networks with dropout work is
    to consider that if we have *N* neurons and we randomly switch a certain percentage
    of the neurons off, we really have created a generator that can create a very
    large variety of different but related networks. During training these different
    networks all learn the task at hand, but at evaluation time they all run in parallel
    and their average opinion is taken. So even if some of them start overfitting,
    chances are that this is drowned out in the aggregate vote.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 观察具有dropout的网络如何工作的一个有趣的替代方式是考虑，如果我们有*N*个神经元，并随机关闭一定百分比的神经元，我们实际上创建了一个可以创建非常多不同但相关网络的生成器。在训练期间，这些不同的网络都学习手头的任务，但在评估时，它们都并行运行，取其平均意见。因此，即使其中一些开始过拟合，很可能在总体投票中被淹没。
- en: 2.6 Network Structure, Batch Size, and Learning Rate
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2.6 网络结构、批量大小和学习率
- en: Problem
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you find the best network structure, batch size, and learning rate for
    a given problem?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如何找到给定问题的最佳网络结构、批量大小和学习率？
- en: Solution
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Start small and work your way up.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从小处开始，逐步扩大。
- en: Once we’ve identified the sort of network we’ll need to solve a specific problem,
    we still have to make a number of implementation decisions. The more important
    among those are decisions about the network structure, the learning rate, and
    the batch size.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了解决特定问题所需的网络类型，我们仍然必须做出许多实现决策。其中最重要的决策是关于网络结构、学习率和批量大小的决策。
- en: Let’s start with the network structure. How many layers will we have? How big
    will each of those layers be? A decent strategy is to start with the smallest
    sizes that could possibly work. Being all enthusiastic about the “deep” in deep
    learning, there is a certain temptation to start with many layers. But typically
    if a one- or two-layer network doesn’t perform at all, chances are that adding
    more layers isn’t going to really help.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从网络结构开始。我们将有多少层？每一层的大小将是多少？一个不错的策略是从可能有效的最小尺寸开始。对于“深度学习”感到热情，有一种诱惑是从许多层开始。但通常，如果一个一层或两层的网络根本不起作用，增加更多层也不会真正有所帮助。
- en: Continuing with the size of each individual layer, larger layers can learn more,
    but they also take longer and have more space to hide problems. As with the number
    of layers, start small and expand from there. If you suspect that the expressive
    power of a smaller network will be insufficient to make any sense of your data,
    consider simplifying your data; start with a small network that only distinguishes
    between the two most popular labels and then gradually increase the complexity
    of both the data and the network.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 继续考虑每个单独层的大小，较大的层可以学到更多，但也需要更长时间，并且有更多隐藏问题的空间。与层数相同，从小处开始，逐步扩大。如果你怀疑较小网络的表现力不足以理解你的数据，考虑简化你的数据；从一个只区分两个最流行标签的小网络开始，然后逐渐增加数据和网络的复杂性。
- en: The batch size is the number of samples we feed into the network before adjusting
    the weights. The larger the batch size, the longer it takes to finish one, but
    the more accurate the gradient is. In order to get results quickly, it is advisable
    to start with a smallish batch size—32 seems to work well.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小是我们在调整权重之前向网络输入的样本数量。批量大小越大，完成一个批次所需的时间就越长，但梯度更准确。为了快速获得结果，建议从一个较小的批量大小开始——32似乎效果很好。
- en: The learning rate determines how much we’ll change the weights in our network
    in the direction of the derived gradient. The higher the rate, the quicker we
    move through the landscapes. Too big a rate, though, and we risk skipping over
    the good bits and we start thrashing. When we take into account that a smaller
    batch size leads to a less accurate gradient, it stands to reason that we should
    combine a small batch size with a smaller learning rate. So, the suggestion here
    is again to start out small and, when things work, experiment with larger batch
    rates and higher learning rates.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率确定我们将如何根据导数梯度改变网络中的权重。学习率越高，我们在景观中移动得越快。然而，如果学习率太大，我们就有可能跳过好的部分，开始折腾。考虑到较小的批量大小会导致梯度不够准确，我们应该将较小的批量大小与较小的学习率结合起来。因此，建议在开始时再次从小处开始，当事情顺利时，尝试较大的批量率和更高的学习率。
- en: Note
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Training on GPUs impacts this assessment. GPUs efficiently run steps in parallel,
    so there is no real reason to pick a batch size that is so small that it leaves
    part of the GPU idle. What batch size that is depends of course on the network,
    but as long as the time per batch doesn’t increase by much when you increase the
    batch size, you’re still on the right side of things. A second consideration when
    running on GPUs is memory. When a batch no longer fits in the memory of the GPU
    things start to fail and you’ll start to see out of memory messages.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上进行训练会影响这个评估。GPU可以高效地并行运行步骤，因此没有真正的理由选择一个批量大小太小以至于让GPU的一部分空闲。批量大小取决于网络，但只要每个批次的时间不会因增加批量大小而显著增加，你仍然处于正确的一边。在GPU上运行时的第二个考虑因素是内存。当一个批次不再适合GPU内存时，事情开始失败，你会开始看到内存不足的消息。
- en: Discussion
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Network structure, batch size, and learning rate are some of the important hyper
    parameters that impact the performance of networks but have little to do with
    the actual strategy. For all of these a reasonable strategy is to start small
    (but big enough that things still work) and go bigger step by step, observing
    that the network still performs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 网络结构、批量大小和学习率是影响网络性能的重要超参数之一，但与实际策略关系不大。对于所有这些，一个合理的策略是从小开始（但足够大以至于事情仍然有效），逐步扩大，观察网络仍然表现。
- en: As we increase the number of layers and the size of each layer, we’ll start
    to see symptoms of overfitting at some point (training and testing accuracy start
    to diverge, for example). That might be a good time to look at regularization
    and dropout.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 随着层数和每层大小的增加，我们会在某个时候开始看到过拟合的症状（例如，训练和测试准确度开始发散）。这可能是一个好时机来考虑正则化和丢弃。
