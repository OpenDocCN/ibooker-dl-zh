["```py\nAfter a physical altercation with the patrons of a restaurant, Alex was feeling\nextremely pleased with himself. He walked out with a swagger and confidence\nthat betrayed his insecurities. Smiling from ear to ear, he noticed rain drops\ngrazing his face and proceeded to walk toward the hostel.\n```", "```py\nUser: 'I am not feeling well'\n```", "```py\nELIZA: 'Do you believe it is normal to be not feeling well?'\n```", "```py\nGiven equation:\n\n34 + 44 + 3 * 23 / 3 * 2\n\nFirst, perform multiplication and division from left to right:\n\n= 34 + 44 + (3 * 23 / 3 * 2)\n= 34 + 44 + (69 / 3 * 2)\n= 34 + 44 + (23 * 2)\n= 34 + 44 + 46\n\nNow, perform addition:\n\n= 78 + 46\n\nFinally, complete the addition:\n\n= 124\n\nSo, the solution to the equation is 124.\n```", "```py\nimport os\nimport openai\nopenai.api_key = <INSERT YOUR KEY HERE>\n\noutput = openai.ChatCompletion.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are an expert storywriter.\"},\n    {\"role\": \"user\", \"content\": \"Write me a short children's story `about` `a` `dog` `and` `an` `elephant` `stopping`\n    `being` `friends` `with` `each` `other``.``\"}` ```", "```py\n```", "```py ``Roles can be system, user, assistant, or tool.    *   The system role is used to specify an overarching prompt.           *   The user role refers to user inputs.           *   The assistant role refers to the model responses.           *   The tool role is used to interact with external software tools.              We will discuss tools in more detail in [Chapter 10](ch10.html#ch10).    ###### Note    What is the difference between the system and user roles? Which instructions should go into the system prompt and which ones into the user prompt? System prompts are used for dictating the high-level overarching behavior of an LLM, like “You are a financial expert well versed in writing formal reports.” If you are allowing your users to directly interact with the LLM, then the system prompt can be used to provide your own instruction to the LLM along with the user request. My experiments have shown that it doesn’t matter much if you place your instructions in the system prompt versus user prompt. What does matter is the length and number of instructions. LLMs typically can handle only a few instructions at a time. Instructions at the end or the beginning of the prompt are more likely to be adhered to.    Here are some of the parameters made available by OpenAI:    `n`      The number of completions the model has to generate for each input. For example, if we used n = 5 in the given example, it would generate five different children’s stories.      ###### Tip    For tasks with high reliability requirements, I advise generating multiple completions, that is, n > 1 and then using a postprocessing function (which could involve an LLM call) to choose the best one. This is because the LLM samples the generated text from a probability distribution, and in some cases the answer might be wrong/bad just due to an unlucky token sampling. You might have to balance this process against your budget limitations.    `stop` and `max_completion_tokens`      Used to limit the length of the generated output. `stop` allows you to specify end tokens that, if generated, would stop the generation process. An example stop sequence is the newline token. If you ask the model to adhere to a particular output format, like a numbered list of sentences, then to stop generating after a particular number of sentences have been output, you can just provide the final number as a stop parameter.      `presence_penalty` and `frequency_penalty`      Used to limit the repetitiveness of the generated output. By penalizing the probability for tokens that have already appeared in the output, we can ensure that the model isn’t being too repetitive. These parameters can be used while performing more creative tasks.      `logit_bias`      Using `logit_bias`, we can specify the tokens whose generation probability we want to increase or decrease.      `top_p` and `temperature`      Both parameters relate to decoding strategies. LLMs produce a distribution of token probabilities and will sample from this distribution to generate the next token. There are many strategies to choose the next token to generate given the token probability distribution. We will discuss them in detail in [Chapter 5](ch05.html#chapter_utilizing_llms). For now, just remember that a higher temperature setting results in more creative and diverse outputs, and a lower temperature setting results in more predictable outputs. This [cheat sheet](https://oreil.ly/DAa66) provides some recommended values for various use cases.      `logprobs`      Provides the most probable tokens for each output token along with their log probabilities. OpenAI limits this to the top 20 most probable tokens. In later chapters, we will discuss how we can leverage `logprobs` information in various forms.`` ```", "```py`` ```", "```py !pip install openai langchain gradio unstructured  from langchain_community.document_loaders import UnstructuredPDFLoader from langchain_community.embeddings import HuggingFaceEmbeddings from langchain_community.vectorstores import Chroma from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI import gradio as gr ```", "```py loader = UnstructuredPDFLoader(input_file.name) data = loader.load() ```", "```py embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") ```", "```py db = Chroma.from_documents(data, embeddings) ```", "```py query = \"How do I request a refund?\" docs = db.similarity_search(query) print(docs[0].page_content) ```", "```py conversational_chain =  ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0.1),     retriever=pdfsearch.as_retriever(search_kwargs={\"k\": 3})) ```", "```py output = conversational_chain({'question': query, 'chat_history':  conversational_history}) conversational_history += [(query, output['answer'])] ```", "```py with gr.Blocks() as app:       with gr.Row():         chatbot = gr.Chatbot(value=[], elem_id='qa_chatbot').style(height=500)      with gr.Row():         with gr.Column(scale=0.80):             textbox = gr.Textbox(                 placeholder=\"Enter text\"             ).style(container=False)           with gr.Column(scale=0.10):         upload_button = gr.UploadButton(\"Upload a PDF\",           file_types=[\".pdf\"]).style() ```", "```py if __name__ == \"__main__\":     app.launch() ```", "```py` ```"]