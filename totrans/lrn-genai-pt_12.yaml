- en: 10 Training a Transformer to translate English to French
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing English and French phrases to subwords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding word embedding and positional encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a Transformer from scratch to translate English to French
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the trained Transformer to translate an English phrase into French
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the last chapter, we built a Transformer from scratch that can translate
    between any two languages, based on the paper “Attention Is All You Need.”^([1](#footnote-000))
    Specifically, we implemented the self-attention mechanism, using query, key, and
    value vectors to calculate scaled dot product attention (SDPA).
  prefs: []
  type: TYPE_NORMAL
- en: To have a deeper understanding of self-attention and Transformers, we’ll use
    English-to-French translation as our case study in this chapter. By exploring
    the process of training a model for converting English sentences into French,
    you will gain a deep understanding of the Transformer’s architecture and the functioning
    of the attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Picture yourself having amassed a collection of more than 47,000 English-to-French
    translation pairs. Your objective is to train the encoder-decoder Transformer
    from the last chapter using this dataset. This chapter will walk you through all
    phases of the project. You’ll first use subword tokenization to break English
    and French phrases into tokens. You’ll then build your English and French vocabularies,
    which contain all unique tokens in each language. The vocabularies allow you to
    represent English and French phrases as sequences of indexes. After that, you’ll
    use word embedding to transform these indexes (essentially one-hot vectors) into
    compact vector representations. We’ll add positional encodings to the word embeddings
    to form input embeddings. Positional encodings allow the Transformer to know the
    ordering of tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you’ll train the encoder-decoder Transformer from chapter 9 to translate
    English to French by using the collection of English-to-French translations as
    the training dataset. After training, you’ll learn to translate common English
    phrases to French with the trained Transformer. Specifically, you’ll use the encoder
    to capture the meaning of the English phrase. You’ll then use the decoder in the
    trained Transformer to generate the French translation in an autoregressive manner,
    starting with the beginning token `"BOS"`. In each time step, the decoder generates
    the most likely next token based on previously generated tokens and the encoder’s
    output, until the predicted token is `"EOS"`, which signals the end of the sentence.
    The trained model can translate common English phrases accurately as if you were
    using Google Translate for the task.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Subword tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in chapter 8, there are three tokenization methods: character-level
    tokenization, word-level tokenization, and subword tokenization. In this chapter,
    we’ll use subword tokenization, which strikes a balance between the other two
    methods. It keeps frequently used words whole in the vocabulary and splits less
    common or more complex words into subcomponents.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’ll learn to tokenize both English and French phrases into
    subwords. You’ll then create dictionaries to map tokens to indexes. The training
    data are then converted to sequences of indexes and placed in batches for training
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Tokenizing English and French phrases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Go to [https://mng.bz/WVAw](https://mng.bz/WVAw) to download the zip file that
    contains the English-to-French translations I collected from various sources.
    Unzip the file and place en2fr.csv in the folder /files/ on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll load the data and print out an English phrase, along with its French
    translation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Loads the CSV file
  prefs: []
  type: TYPE_NORMAL
- en: ② Counts how many pairs of phrases are in the data
  prefs: []
  type: TYPE_NORMAL
- en: ③ Prints out an example of an English phrase
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints out the corresponding French translation
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code snippet is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are 47,173 pairs of English-to-French translations in the training data.
    We have printed out the English phrase “How are you?” and the corresponding French
    translation “Comment êtes-vous?” as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following line of code in a new cell in this Jupyter Notebook to install
    the `transformers` library on your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll tokenize both the English and the French phrases in the dataset.
    We’ll use the pretrained XLM model from Hugging Face as the tokenizer because
    it excels at handling multiple languages, including English and French phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 A pretrained tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Imports the pretrained tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses the tokenizer to tokenize an English sentence
  prefs: []
  type: TYPE_NORMAL
- en: ③ Tokenizes a French sentence
  prefs: []
  type: TYPE_NORMAL
- en: The output from code listing 10.1 is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we use a pretrained tokenizer from the XLM model
    to divide the English sentence “I don’t speak French.” into a group of tokens.
    In chapter 8, you developed a custom word-level tokenizer. However, this chapter
    introduces the use of a more efficient pretrained subword tokenizer, surpassing
    the word-level tokenizer in effectiveness. The sentence “I don’t speak French.”
    is thus tokenized into `[''i'', ''don'', "''t", ''speak'', ''fr'', ''ench'', ''.'']`.
    Similarly, the French sentence “Je ne parle pas français.” is split into six tokens:
    `[''je'', ''ne'', ''parle'', ''pas'', ''franc'', ''ais'', ''.'']`. We have also
    tokenized the English phrase “How are you?” and its French translation. The results
    are shown in the last two lines of the preceding output.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE You may have noticed that the XLM model uses '`</w>`' as a token separator,
    except in cases where two tokens are part of the same word. Subword tokenization
    typically results in each token being either a complete word or a punctuation
    mark, but there are occasions when a word is divided into syllables. For example,
    the word “French” is divided into “fr” and “ench.” It’s noteworthy that the model
    doesn’t insert `</w>` between “fr” and “ench,” as these syllables jointly constitute
    the word “French.”
  prefs: []
  type: TYPE_NORMAL
- en: Deep-learning models such as Transformers cannot process raw text directly;
    hence we need to convert text into numerical representations before feeding them
    to the models. For that purpose, we create a dictionary to map all English tokens
    to integers.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 Mapping English tokens to indexes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Obtains all English sentences from the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: ② Tokenizes all English sentences
  prefs: []
  type: TYPE_NORMAL
- en: ③ Counts the frequency of tokens
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a dictionary to map tokens to indexes
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a dictionary to map indexes to tokens
  prefs: []
  type: TYPE_NORMAL
- en: We insert the tokens `"BOS"` (beginning of the sentence) and `"EOS"` (end of
    the sentence) at the start and end of each phrase, respectively. The dictionary
    `en_word_dict` assigns each token a unique integer value. Further, the `"PAD"`
    token, used for padding, is allocated the integer 0, while the `"UNK"` token,
    representing unknown tokens, is given the integer 1\. A reverse dictionary, `en_idx_dict`,
    maps integers (indexes) back to their corresponding tokens. This reverse mapping
    is essential for converting a sequence of integers back into a sequence of tokens,
    enabling us to reconstruct the original English phrase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the dictionary `en_word_dict`, we can transform the English sentence
    “I don’t speak French.” into its numerical representation. This process involves
    looking up each token in the dictionary to find its corresponding integer value.
    For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding lines of code produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This means that the English sentence “I don’t speak French.” is now represented
    by a sequence of integers [15, 100, 38, 377, 476, 574, 5].
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also revert the numerical representations into tokens using the dictionary
    `en_idx_dict`. This process involves mapping each integer in the numerical sequence
    back to its corresponding token as defined in the dictionary. Here’s how it is
    done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① Converts indexes to tokens
  prefs: []
  type: TYPE_NORMAL
- en: ② Joins tokens into a string
  prefs: []
  type: TYPE_NORMAL
- en: ③ Replaces the separator with a space
  prefs: []
  type: TYPE_NORMAL
- en: ④ Removes the space before punctuations
  prefs: []
  type: TYPE_NORMAL
- en: The output of the preceding code snippet is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The dictionary `en_idx_dict` is used to translate numbers back into their original
    tokens. Following this, these tokens are transformed into the complete English
    phrase. This is done by first joining the tokens into a single string and then
    substituting the separator `''</w>''` with a space. We also remove the space before
    punctuation marks. Notice that the restored English phrase has all lowercase letters
    because the pretrained tokenizer automatically converts uppercase letters into
    lowercase to reduce the number of unique tokens. As you’ll see in the next chapter,
    some models, such as GPT2 and ChatGPT, don’t do this; hence, they have a larger
    vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.1
  prefs: []
  type: TYPE_NORMAL
- en: In listing 10.1, we have split the sentence “How are you?” into tokens `['how</w>',
    'are</w>', 'you</w>', '?</w>']`. Follow the steps in this subsection to (i) convert
    the tokens into indexes using the dictionary `en_word_dict`; (ii) convert the
    indexes back to tokens using the dictionary `en_idx_dict`; (iii) restore the English
    sentence by joining the tokens into a string, changing the separator `'</w>'`
    to a space, and removing the space before punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: We can apply the same steps to French phrases to map tokens to indexes and vice
    versa.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 Mapping French tokens to indexes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① Tokenizes all French sentences
  prefs: []
  type: TYPE_NORMAL
- en: ② Counts the frequency of French tokens
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a dictionary to map French tokens to indexes
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a dictionary to map indexes to French tokens
  prefs: []
  type: TYPE_NORMAL
- en: 'The dictionary `fr_word_dict` assigns an integer to each French token, while
    `fr_idx_dict` maps these integers back to their corresponding French tokens. Next,
    I’ll demonstrate how to transform the French phrase “Je ne parle pas français.”
    into its numerical representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The output from the preceding code snippet is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The tokens for the French phrase “Je ne parle pas français.” are converted into
    a sequence of integers, as shown.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can transform the numerical representations back into French tokens using
    the dictionary `fr_idx_dict`. This involves translating each number in the sequence
    back to its respective French token in the dictionary. Once the tokens are retrieved,
    they can be joined to reconstruct the original French phrase. Here’s how it’s
    done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output from the preceding code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to recognize that the restored French phrase doesn’t exactly
    match its original form. This discrepancy is due to the tokenization process,
    which transforms all uppercase letters into lowercase and eliminates accent marks
    in French.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.2
  prefs: []
  type: TYPE_NORMAL
- en: In listing 10.1, we have split the sentence “Comment êtes-vous?” into tokens
    `['comment</w>', 'et', 'es-vous</w>', '?</w>']`. Follow the steps in this subsection
    to (i) convert the tokens into indexes using the dictionary `fr_word_dict`; (ii)
    convert the indexes back to tokens using the dictionary `fr_idx_dict`; (iii) restore
    the French phrase by joining the tokens into a string, changing the separator
    `'</w>'` to a space, and removing the space before punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the four dictionaries in the folder /files/ on your computer so that you
    can load them up and start translating later without worrying about first mapping
    tokens to indexes and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The four dictionaries are now saved in a single pickle file `dict.p`. Alternatively,
    you can download the file from the book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Sequence padding and batch creation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll divide the training data into batches during training for computational
    efficiency and accelerated convergence, as we have done in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating batches for other data formats such as images is straightforward:
    simply group a specific number of inputs to form a batch since they all have the
    same size. However, in natural language processing, batching can be more complex
    due to the varying lengths of sentences. To standardize the length within a batch,
    we pad the shorter sequences. This uniformity is crucial since the numerical representations
    fed into the Transformer need to have the same length. For instance, English phrases
    in a batch may vary in length (this can also happen to French phrases in a batch).
    To address this, we append zeros to the end of the numerical representations of
    shorter phrases in a batch, ensuring that all inputs to the Transformer model
    are of equal length.'
  prefs: []
  type: TYPE_NORMAL
- en: Note Incorporating `BOS` and `EOS` tokens at the beginning and end of each sentence,
    as well as padding shorter sequences within a batch, is a distinctive feature
    in machine language translation. This distinction arises from the fact that the
    input consists of entire sentences or phrases. In contrast, as you will see in
    the next two chapters, training a text generation model does not entail these
    processes; the model’s input contains a predetermined number of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by converting all English phrases into their numerical representations
    and then apply the same process to the French phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we put the numerical representations into batches for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have sorted observations in the training dataset by the length
    of the English phrases before placing them into batches. This method ensures that
    the observations within each batch are of a comparable length, consequently decreasing
    the need for padding. As a result, this approach not only reduces the overall
    size of the training data but also accelerates the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To pad sequences in a batch to the same length, we define the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: ① Find out the length of the longest sequence in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: ② If a batch is shorter than the longest sequence, add 0s to the sequence at
    the end.
  prefs: []
  type: TYPE_NORMAL
- en: The function `seq_padding()` first identifies the longest sequence within the
    batch. Then it appends zeros to the end of shorter sequences to ensure that every
    sequence in the batch matches this maximum length.
  prefs: []
  type: TYPE_NORMAL
- en: To conserve space, we have created a `Batch()` class within the local module
    ch09util.py that you downloaded in the last chapter (see figure 10.1).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 Creating a `B`atc`h()` class in the local module
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a source mask to hide padding at the end of the sentence
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates input to the decoder
  prefs: []
  type: TYPE_NORMAL
- en: ③ Shifts the input one token to the right and uses it as output
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a target mask
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH10_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1 What does the `Batch()` class do? The `Batch()` class takes two
    inputs: `src` and `trg`, sequences of indexes for the source language and the
    target language, respectively. It adds several attributes to the training data:
    `src_mask`, the source mask to conceal padding; `modified trg`, the input to the
    decoder; `trg_y`, the output to the decoder; `trg_mask`, the target mask to hide
    padding and future tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Batch()` class processes a batch of English and French phrases, converting
    them into a format suitable for training. To make this explanation more tangible,
    consider the English phrase “How are you?” and its French equivalent “Comment
    êtes-vous?” as our example. The `Batch()` class receives two inputs: `src`, which
    is the sequence of indexes representing the tokens in “How are you?”, and `trg`,
    the sequence of indexes for the tokens in “Comment êtes-vous?”. This class generates
    a tensor, `src_mask`, to conceal the padding at the sentence’s end. For instance,
    the sentence “How are you?” is broken down into six tokens: `[''BOS'', ''how'',
    ''are'', ''you'', ''?'', ''EOS'']`. If this sequence is part of a batch with a
    maximum length of eight tokens, two zeros are added to the end. The `src_mask`
    tensor instructs the model to disregard the final two tokens in such scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Batch()` class additionally prepares the input and output for the Transformer’s
    decoder. Consider the French phrase “Comment êtes-vous?”, which is transformed
    into six tokens: `[''BOS'', ''comment'', ''et'', ''es-vous'', ''?'', ''EOS'']`.
    The indexes of these first five tokens serve as the input to the decoder, named
    `trg`. Next, we shift this input one token to the right to form the decoder’s
    output, `trg_y`. Hence, the input comprises indexes for `[''BOS'', ''comment'',
    ''et'', ''es-vous'', ''?'']`, while the output consists of indexes for `[''comment'',
    ''et'', ''es-vous'', ''?'', ''EOS'']`. This approach mirrors what we discussed
    in chapter 8 and is designed to force the model to predict the next token based
    on the previous ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Batch()` class also generates a mask, `trg_mask`, for the decoder’s input.
    The aim of this mask is to conceal the subsequent tokens in the input, ensuring
    that the model relies solely on previous tokens for making predictions. This mask
    is produced by the `make_std_mask()` function, which is defined within the local
    module ch09util:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `subsequent_mask()` function generates a mask specifically for a sequence,
    instructing the model to focus solely on the actual sequence and disregard the
    padded zeros at the end, which are used only to standardize sequence lengths.
    The `make_std_mask()` function, on the other hand, constructs a standard mask
    for the target sequence. This standard mask has the dual role of concealing both
    the padded zeros and the future tokens in the target sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we import the `Batch()` class from the local module and use it to create
    batches of training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `BatchLoader()` class creates data batches intended for training. Each batch
    in this list contains 128 pairs, where each pair contains numerical representations
    of an English phrase and its corresponding French translation.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Word embedding and positional encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After tokenization in the last section, English and French phrases are represented
    by sequences of indexes. In this section, you’ll use word embedding to transform
    these indexes (essentially one-hot vectors) into compact vector representations.
    Doing so captures the semantic information and interrelationship of tokens in
    a phrase. Word embedding also improves training efficiency: instead of bulky one-hot
    vectors, word embedding uses continuous, lower-dimensional vectors to reduce the
    model’s complexity and dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism processes all tokens in a phrase at the same time instead
    of sequentially. This enhances its efficiency but doesn’t inherently allow it
    to recognize the sequence order of the tokens. Therefore, we’ll add positional
    encodings to the input embeddings by using sine and cosine functions of varying
    frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Word embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The numerical representations of the English and French phrases involve a large
    number of indexes. To determine the exact number of distinct indexes required
    for each language, we can count the number of unique elements in the `en_word_dict`
    and `fr_word_dict` dictionaries. Doing so generates the total number of unique
    tokens in each language’s vocabulary (we’ll use them as inputs to the Transformer
    later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In our dataset, there are 11,055 unique English tokens and 11,239 unique French
    tokens. Utilizing one-hot encoding for these would result in an excessively high
    number of parameters to train. To address this, we will employ word embeddings,
    which compress the numerical representations into continuous vectors, each with
    a length of `d_model = 256`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is achieved through the use of the `Embeddings()` class, which is defined
    in the local module ch09util:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `Embeddings()` class defined previously utilizes PyTorch’s `Embedding()`
    class. It also multiplies the output by the square root of `d_model`, which is
    256\. This multiplication is intended to counterbalance the division by the square
    root of `d_model` that occurs later during the computation of attention scores.
    The `Embeddings()` class decreases the dimensionality of the numerical representations
    of English and French phrases. We discussed in detail how PyTorch’s `Embedding()`
    class works in chapter 8\.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Positional encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To accurately represent the sequence order of elements in both input and output,
    we introduce the `PositionalEncoding()` class in the local module.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 A class to calculate positional encoding
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: ① Initiates the class, allowing a maximum of 5,000 positions
  prefs: []
  type: TYPE_NORMAL
- en: ② Applies sine function to even indexes in the vector
  prefs: []
  type: TYPE_NORMAL
- en: ③ Applies cosine function to odd indexes in the vector
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adds positional encoding to word embedding
  prefs: []
  type: TYPE_NORMAL
- en: The `PositionalEncoding()` class generates vectors for sequence positions using
    sine functions for even indexes and cosine functions for odd indexes. It’s important
    to note that in the `PositionalEncoding()` class, the `requires_grad_(False)`
    argument is included because there is no need to train these values. They remain
    constant across all inputs, and they don’t change during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the indexes for the six tokens `[''BOS'', ''how'', ''are'', ''you'',
    ''?'', ''EOS'']` from the English phrase are first processed through a word embedding
    layer. This step transforms these indexes into a tensor with the dimensions of
    (1, 6, 256): 1 means there is only 1 sequence in the batch; 6 means there are
    6 tokens in the sequence; 256 means each token is represented by a 256-value vector.
    After this word embedding process, the `PositionalEncoding()` class is employed
    to calculate the positional encodings for the indexes corresponding to the tokens
    `[''BOS'', ''how'', ''are'', ''you'', ''?'', ''EOS'']`. This is done to provide
    the model with information about the position of each token in the sequence. Better
    yet, we can tell you the exact values of the positional encodings for the previous
    six tokens by using the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: ① Instantiates the PositionalEncoding() class and set the model dimension to
    256
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a word embedding and fills it with zeros
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates the input embedding by adding positional encoding to the word embedding
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints out the input embedding, which is the same as positional encoding since
    word embedding is set to zero
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create an instance, `pe`, of the `PositionalEncoding()` class by setting
    the model dimension to 256 and the dropout rate to 0.1\. Since the output from
    this class is the sum of word embedding and positional encoding, we create a word
    embedding filled with zeros and feed it to `pe`: this way, the output is the same
    as the positional encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the preceding code block, you’ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The preceding tensor represents the positional encoding for the English phrase
    “How are you?” It’s important to note that this positional encoding also has the
    dimensions of (1, 6, 256), which matches the size of the word embedding for “How
    are you?”. The next step involves combining the word embedding and positional
    encoding into a single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: An essential characteristic of positional encodings is that their values are
    the same no matter what the input sequences are. This means that regardless of
    the specific input sequence, the positional encoding for the first token will
    always be the same 256-value vector, `[0.0000e+00, 1.1111e+00, ..., 1.1111e+00]`,
    as shown in the above output. Similarly, the positional encoding for the second
    token will always be `[9.3497e-01, 6.0034e-01, ..., 1.1111e+00]`, and so on. Their
    values don’t change during the training process either.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Training the Transformer for English-to-French translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our constructed English-to-French translation model can be viewed as a multicategory
    classifier. The core objective is to predict the next token in the French vocabulary
    when translating an English sentence. This is somewhat similar to the image classification
    project we discussed in chapter 2, though this model is significantly more complex.
    This complexity necessitates careful selection of the loss function, optimizer,
    and training loop parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will detail the process of selecting an appropriate loss
    function and optimizer. We will train the Transformer using batches of English-to-French
    translations as our training dataset. After the model is trained, you’ll learn
    how to translate common English phrases into French.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Loss function and the optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we import the `create_model()` function from the local module ch09util.py
    and construct a Transformer so that we can train it to translate English to French:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The paper “Attention Is All You Need” uses various combinations of hyperparameters
    when constructing the model. Here we choose a model dimension of 256 with 8 heads
    because we find this combination does a good job translating English to French
    in our setting. Interested readers could potentially use a validation set to tune
    hyperparameters to select the best model in their own projects.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll follow the original paper “Attention Is All You Need” and use label smoothing
    during training. Label smoothing is commonly used in training deep neural networks
    to improve the generalization of the model. It is used to address overconfidence
    problems (the predicted probability is greater than the true probability) and
    overfitting in classifications. Specifically, it modifies the way the model learns
    by adjusting the target labels, aiming to reduce the model’s confidence in the
    training data, which can lead to better performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a typical classification task, target labels are represented in a one-hot
    encoding format. This representation implies absolute certainty about the correctness
    of the label for each training sample. Training with absolute certainty can lead
    to two main problems. The first is overfitting: the model becomes overly confident
    in its predictions, fitting too closely to the training data, which can harm its
    performance on new, unseen data. The second problem is poor calibration: models
    trained this way often output overconfident probabilities. For instance, they
    might output a probability of 99% for a correct class when, realistically, the
    confidence should be lower.'
  prefs: []
  type: TYPE_NORMAL
- en: Label smoothing adjusts the target labels to be less confident. Instead of having
    a target label of `[1, 0, 0]` for a three-class problem, you might have something
    like `[0.9, 0.05, 0.05]`. This approach encourages the model not to be too confident
    about its predictions by penalizing overconfident outputs. The smoothed labels
    are a mixture of the original label and some distribution over the other labels
    (usually the uniform distribution).
  prefs: []
  type: TYPE_NORMAL
- en: We define the following `LabelSmoothing()` class in the local module ch09util.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 A class to conduct label smoothing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: ① Extracts predictions from the model
  prefs: []
  type: TYPE_NORMAL
- en: ② Extracts actual labels from the training data and adds noise to them
  prefs: []
  type: TYPE_NORMAL
- en: ③ Uses the smoothed labels as targets when calculating loss
  prefs: []
  type: TYPE_NORMAL
- en: The `LabelSmoothing()` class first extracts the predictions from the model.
    It then smoothes the actual labels in the training dataset by adding noise to
    it. The parameter `smoothing` controls how much noise we inject into the actual
    label. The label `[1, 0, 0]` is smoothed to `[0.9, 0.05, 0.05]` if you set `smoothing=0.1`,
    and it is smoothed to `[0.95, 0.025, 0.025]` if you set `smoothing=0.05`, for
    example. The class then calculates the loss by comparing the predictions with
    the smoothed labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in previous chapters, the optimizer we use is the Adam optimizer. However,
    instead of using a constant learning rate throughout training, we define the `NoamOpt()`
    class in the local module to change the learning rate during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines warm-up steps
  prefs: []
  type: TYPE_NORMAL
- en: ② A step() method to apply the optimizer to adjust model parameters
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates the learning rate based on steps
  prefs: []
  type: TYPE_NORMAL
- en: The `NoamOpt()` class, as defined previously, implements a warm-up learning
    rate strategy. First, it increases the learning rate linearly during the initial
    warmup steps of training. Following this warm-up period, the class then decreases
    the learning rate, adjusting it in proportion to the inverse square root of the
    training step number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the optimizer for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: To define the loss function for training, we first create the following `SimpleLossCompute()`
    class in the local module.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 A class to compute loss
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: ① Uses the model to make predictions
  prefs: []
  type: TYPE_NORMAL
- en: ② Compares the predictions with labels to calculate loss, utilizing label smoothing
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates gradients with respect to model parameters
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adjusts model parameters (backpropagate)
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SimpleLossCompute()` class is designed with three key elements: `generator`,
    serving as the prediction model; `criterion`, which is a function to calculate
    loss; and `opt`, the optimizer. This class processes a batch of training data,
    denoted as (x, y), by utilizing the generator for predictions. It subsequently
    evaluates the loss by comparing these predictions with the actual labels y (which
    is handled by the `LabelSmoothing()` class defined earlier; the actual labels
    y will be smoothed in the process). The class computes gradients relative to the
    model parameters and utilizes the optimizer to update these parameters accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to define the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll train the Transformer by using the data we prepared earlier in the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 The training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We could potentially divide the training data into a train set and a validation
    set and train the model until the performance of the model doesn’t improve on
    the validation set, similar to what we have done in chapter 2\. However, to save
    space, we’ll train the model for 100 epochs. We’ll calculate the loss and the
    number of tokens from each batch. After each epoch, we calculate the average loss
    in the epoch as the ratio between the total loss and the total number of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Training a Transformer to translate English to French
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: ① Makes predictions using the Transformer
  prefs: []
  type: TYPE_NORMAL
- en: ② Calculates loss and adjusts model parameters
  prefs: []
  type: TYPE_NORMAL
- en: ③ Counts the number of tokens in the batch
  prefs: []
  type: TYPE_NORMAL
- en: ④ Saves the weights in the trained model after training
  prefs: []
  type: TYPE_NORMAL
- en: This training process takes a couple of hours if you are using a CUDA-enabled
    GPU. It may take a full day if you are using CPU training. Once the training is
    done, the model weights are saved as en2fr.pth on your computer. Alternatively,
    you can download the trained weights from my website ([https://gattonweb.uky.edu/faculty/lium/gai/ch9.zip](https://gattonweb.uky.edu/faculty/lium/gai/ch9.zip)).
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Translating English to French with the trained model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have trained the Transformer, you can use it to translate any English
    sentence to French. We define a function `translate()` as shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 Defining a `translate()` function to translate English to French
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: ① Uses encoder to convert the English phrase to a vector representation
  prefs: []
  type: TYPE_NORMAL
- en: ② Predicts the next token using the decoder
  prefs: []
  type: TYPE_NORMAL
- en: ③ Stops translating when the next token is “EOS”
  prefs: []
  type: TYPE_NORMAL
- en: ④ Joins the predicted tokens to form a French sentence
  prefs: []
  type: TYPE_NORMAL
- en: To translate an English phrase to French, we first use the tokenizer to convert
    the English sentence to tokens. We then add `"BOS"` and `"EOS"` at the beginning
    and the end of the phrase. We use the dictionary `en_word_dict` we created earlier
    in the chapter to convert tokens to indexes. We feed the sequence of indexes to
    the encoder in the trained model. The encoder produces an abstract vector representation
    and passes it on to the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the abstract vector representation of the English sentence produced
    by the encoder, the decoder in the trained model starts translating in an autoregressive
    manner, starting with the beginning token `"BOS"`. In each time step, the decoder
    generates the most likely next token based on previously generated tokens, until
    the predicted token is `"EOS"`, which signals the end of the sentence. Note this
    is slightly different from the text generation approach discussed in chapter 8,
    where the next token is chosen randomly in accordance with its predicted probabilities.
    Here, the method for selecting the next token is deterministic, meaning the token
    with the highest probability is chosen with certainty because we mainly care about
    accuracy. However, you can switch to stochastic prediction as we did in chapter
    8 and use `top-K` sampling and temperature if you want your translation to be
    creative.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we change the token separator to a space and remove the space before
    the punctuation marks. The output is the French translation in a clean format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try the `translate()` function with the English phrase “Today is a beautiful
    day!”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You can verify that the French translation indeed means “Today is a beautiful
    day!” by using, say, Google Translate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try a longer sentence and see if the trained model can successfully translate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: When I translate the preceding output back to English using Google Translate,
    it says, “a little boy in jeans climbs a small tree while another child watches”—not
    exactly the same as the original English sentence, but the meaning is the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll test if the trained model generates the same translation for the
    two English sentences “I don’t speak French.” and “I do not speak French.” First,
    let’s try the sentence “I don’t speak French.”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s try the sentence “I do not speak French.”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The output this time is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The results indicate that French translations of the two sentences are exactly
    the same. This suggests that the encoder component of the Transformer successfully
    grasps the semantic essence of the two phrases. It then represents them as similar
    abstract continuous vector forms, which are subsequently passed on to the decoder.
    The decoder then generates translations based on these vectors and produces identical
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.3
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `translate()` function to translate the following two English sentences
    to French. Compare the results with those from Google Translate and see if they
    are the same: (i) I love skiing in the winter! (ii) How are you?'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you trained an encoder-decoder Transformer to translate English
    to French by using more than 47,000 pairs of English-to-French translations. The
    trained model works well, translating common English phrases correctly!
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, you’ll explore decoder-only Transformers. You’ll
    learn to build them from scratch and use them to generate coherent text, better
    than the text you generated in chapter 8 using long short-term memory.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers process input data such as sentences in parallel, unlike recurrent
    neural networks, which handle data sequentially. This parallelism enhances their
    efficiency but doesn’t inherently allow them to recognize the sequence order of
    the input. To address this, Transformers add positional encodings to the input
    embeddings. These positional encodings are unique vectors assigned to each position
    in the input sequence and align in dimension with the input embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label smoothing is commonly used in training deep neural networks to improve
    the generalization of the model. It is used to address overconfidence problems
    (the predicted probability is greater than the true probability) and overfitting
    in classifications. Specifically, it modifies the way the model learns by adjusting
    the target labels, aiming to reduce the model’s confidence in the training data,
    which can lead to better performance on unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the encoder’s output that captures the meaning of the English phrase,
    the decoder in the trained Transformer starts translating in an autoregressive
    manner, starting with the beginning token `"BOS"`. In each time step, the decoder
    generates the most likely next token based on previously generated tokens, until
    the predicted token is `"EOS"`, which signals the end of the sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-000-backlink))  Vaswani et al, 2017, “Attention Is All You Need.”
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
  prefs: []
  type: TYPE_NORMAL
