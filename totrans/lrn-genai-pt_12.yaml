- en: 10 Training a Transformer to translate English to French
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 训练Transformer以将英语翻译成法语
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Tokenizing English and French phrases to subwords
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将英语和法语短语分词为子词
- en: Understanding word embedding and positional encoding
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解词嵌入和位置编码
- en: Training a Transformer from scratch to translate English to French
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练Transformer以将英语翻译成法语
- en: Using the trained Transformer to translate an English phrase into French
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的Transformer将英语短语翻译成法语
- en: In the last chapter, we built a Transformer from scratch that can translate
    between any two languages, based on the paper “Attention Is All You Need.”^([1](#footnote-000))
    Specifically, we implemented the self-attention mechanism, using query, key, and
    value vectors to calculate scaled dot product attention (SDPA).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们从头开始构建了一个Transformer，可以根据“Attention Is All You Need.”这篇论文在任意两种语言之间进行翻译。具体来说，我们实现了自注意力机制，使用查询、键和值向量来计算缩放点积注意力（SDPA）。
- en: To have a deeper understanding of self-attention and Transformers, we’ll use
    English-to-French translation as our case study in this chapter. By exploring
    the process of training a model for converting English sentences into French,
    you will gain a deep understanding of the Transformer’s architecture and the functioning
    of the attention mechanism.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解自注意力机制和Transformer，我们将在本章中使用英语到法语翻译作为案例研究。通过探索将英语句子转换为法语的模型训练过程，你将深入了解Transformer的架构和注意力机制的运作。
- en: Picture yourself having amassed a collection of more than 47,000 English-to-French
    translation pairs. Your objective is to train the encoder-decoder Transformer
    from the last chapter using this dataset. This chapter will walk you through all
    phases of the project. You’ll first use subword tokenization to break English
    and French phrases into tokens. You’ll then build your English and French vocabularies,
    which contain all unique tokens in each language. The vocabularies allow you to
    represent English and French phrases as sequences of indexes. After that, you’ll
    use word embedding to transform these indexes (essentially one-hot vectors) into
    compact vector representations. We’ll add positional encodings to the word embeddings
    to form input embeddings. Positional encodings allow the Transformer to know the
    ordering of tokens in the sequence.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你已经收集了超过47,000个英语到法语翻译对。你的目标是使用这个数据集训练第9章中的编码器-解码器Transformer。本章将带你了解项目的所有阶段。首先，你将使用子词分词将英语和法语短语分解成标记。然后，你将构建你的英语和法语词汇表，其中包含每种语言中所有唯一的标记。词汇表允许你将英语和法语短语表示为索引序列。之后，你将使用词嵌入将这些索引（本质上是一维向量）转换为紧凑的向量表示。我们将在词嵌入中添加位置编码以形成输入嵌入。位置编码允许Transformer知道序列中标记的顺序。
- en: Finally, you’ll train the encoder-decoder Transformer from chapter 9 to translate
    English to French by using the collection of English-to-French translations as
    the training dataset. After training, you’ll learn to translate common English
    phrases to French with the trained Transformer. Specifically, you’ll use the encoder
    to capture the meaning of the English phrase. You’ll then use the decoder in the
    trained Transformer to generate the French translation in an autoregressive manner,
    starting with the beginning token `"BOS"`. In each time step, the decoder generates
    the most likely next token based on previously generated tokens and the encoder’s
    output, until the predicted token is `"EOS"`, which signals the end of the sentence.
    The trained model can translate common English phrases accurately as if you were
    using Google Translate for the task.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将训练第9章中的编码器-解码器Transformer，使用英语到法语翻译的集合作为训练数据集，将英语翻译成法语。训练完成后，你将学会使用训练好的Transformer将常见的英语短语翻译成法语。具体来说，你将使用编码器来捕捉英语短语的含义。然后，你将使用训练好的Transformer中的解码器以自回归的方式生成法语翻译，从开始标记“BOS”开始。在每一步中，解码器根据之前生成的标记和解码器的输出生成最可能的下一个标记，直到预测的标记是“EOS”，这标志着句子的结束。训练好的模型可以像使用谷歌翻译进行任务一样，准确地将常见的英语短语翻译成法语。
- en: 10.1 Subword tokenization
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 子词分词
- en: 'As we discussed in chapter 8, there are three tokenization methods: character-level
    tokenization, word-level tokenization, and subword tokenization. In this chapter,
    we’ll use subword tokenization, which strikes a balance between the other two
    methods. It keeps frequently used words whole in the vocabulary and splits less
    common or more complex words into subcomponents.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第8章中讨论的，有三种标记化方法：字符级标记化、词级标记化和子词标记化。在本章中，我们将使用子词标记化，它在其他两种方法之间取得平衡。它在词汇表中保留了常用词的完整性，并将不常见或更复杂的词分解成子组件。
- en: In this section, you’ll learn to tokenize both English and French phrases into
    subwords. You’ll then create dictionaries to map tokens to indexes. The training
    data are then converted to sequences of indexes and placed in batches for training
    purposes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何将英文和法语短语标记为子词。然后，你将创建将标记映射到索引的字典。训练数据随后被转换为索引序列，并放入批量中进行训练。
- en: 10.1.1 Tokenizing English and French phrases
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 标记英文和法语短语
- en: Go to [https://mng.bz/WVAw](https://mng.bz/WVAw) to download the zip file that
    contains the English-to-French translations I collected from various sources.
    Unzip the file and place en2fr.csv in the folder /files/ on your computer.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[https://mng.bz/WVAw](https://mng.bz/WVAw)下载包含我从各种来源收集的英文到法语翻译的zip文件。解压文件并将en2fr.csv放在计算机上的/files/文件夹中。
- en: 'We’ll load the data and print out an English phrase, along with its French
    translation, as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载数据并打印出一句英文短语及其法语翻译，如下所示：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Loads the CSV file
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载CSV文件
- en: ② Counts how many pairs of phrases are in the data
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算数据中有多少对短语
- en: ③ Prints out an example of an English phrase
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 打印出一个英文短语的示例
- en: ④ Prints out the corresponding French translation
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印出相应的法语翻译
- en: The output from the preceding code snippet is
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码片段的输出如下
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There are 47,173 pairs of English-to-French translations in the training data.
    We have printed out the English phrase “How are you?” and the corresponding French
    translation “Comment êtes-vous?” as an example.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中有47,173对英文到法语的翻译。我们已打印出英文短语“你好吗？”及其对应的法语翻译“Comment êtes-vous？”作为示例。
- en: 'Run the following line of code in a new cell in this Jupyter Notebook to install
    the `transformers` library on your computer:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Jupyter Notebook的新单元格中运行以下代码行，以在您的计算机上安装`transformers`库：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we’ll tokenize both the English and the French phrases in the dataset.
    We’ll use the pretrained XLM model from Hugging Face as the tokenizer because
    it excels at handling multiple languages, including English and French phrases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对数据集中的英文和法语短语进行标记。我们将使用Hugging Face的预训练XLM模型作为标记器，因为它擅长处理多种语言，包括英文和法语短语。
- en: Listing 10.1 A pretrained tokenizer
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1 预训练标记器
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Imports the pretrained tokenizer
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入预训练标记器
- en: ② Uses the tokenizer to tokenize an English sentence
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用标记器对英文句子进行标记
- en: ③ Tokenizes a French sentence
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 标记一个法语句子
- en: The output from code listing 10.1 is
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1的输出如下
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the preceding code block, we use a pretrained tokenizer from the XLM model
    to divide the English sentence “I don’t speak French.” into a group of tokens.
    In chapter 8, you developed a custom word-level tokenizer. However, this chapter
    introduces the use of a more efficient pretrained subword tokenizer, surpassing
    the word-level tokenizer in effectiveness. The sentence “I don’t speak French.”
    is thus tokenized into `[''i'', ''don'', "''t", ''speak'', ''fr'', ''ench'', ''.'']`.
    Similarly, the French sentence “Je ne parle pas français.” is split into six tokens:
    `[''je'', ''ne'', ''parle'', ''pas'', ''franc'', ''ais'', ''.'']`. We have also
    tokenized the English phrase “How are you?” and its French translation. The results
    are shown in the last two lines of the preceding output.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们使用XLM模型预训练的标记器将英文句子“我不说法语。”分解成一组标记。在第8章中，你开发了一个自定义的词级标记器。然而，本章介绍了使用更高效的预训练子词标记器，其有效性超过了词级标记器。因此，句子“我不说法语。”被标记为`['i',
    'don', "'t", 'speak', 'fr', 'ench', '.']`。同样，法语句子“Je ne parle pas français.”被分割成六个标记：`['je',
    'ne', 'parle', 'pas', 'franc', 'ais', '.']`。我们还将英文短语“你好吗？”及其法语翻译进行了标记。结果显示在上面的输出最后两行。
- en: NOTE You may have noticed that the XLM model uses '`</w>`' as a token separator,
    except in cases where two tokens are part of the same word. Subword tokenization
    typically results in each token being either a complete word or a punctuation
    mark, but there are occasions when a word is divided into syllables. For example,
    the word “French” is divided into “fr” and “ench.” It’s noteworthy that the model
    doesn’t insert `</w>` between “fr” and “ench,” as these syllables jointly constitute
    the word “French.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能已经注意到，XLM模型使用 '`</w>`' 作为标记分隔符，除非两个标记是同一个单词的一部分。子词标记化通常导致每个标记要么是一个完整的单词或标点符号，但有时一个单词会被分成音节。例如，单词“French”被分成“fr”和“ench。”值得注意的是，模型在“fr”和“ench”之间不会插入
    `</w>`，因为这些音节共同构成了单词“French”。
- en: Deep-learning models such as Transformers cannot process raw text directly;
    hence we need to convert text into numerical representations before feeding them
    to the models. For that purpose, we create a dictionary to map all English tokens
    to integers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型如Transformers不能直接处理原始文本；因此，在将文本输入模型之前，我们需要将文本转换为数值表示。为此，我们创建一个字典，将所有英语标记映射到整数。
- en: Listing 10.2 Mapping English tokens to indexes
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.2 将英语标记映射到索引
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Obtains all English sentences from the training dataset
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从训练数据集中获取所有英语句子
- en: ② Tokenizes all English sentences
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ② 对所有英语句子进行标记化
- en: ③ Counts the frequency of tokens
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算标记的频率
- en: ④ Creates a dictionary to map tokens to indexes
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建一个字典将标记映射到索引
- en: ⑤ Creates a dictionary to map indexes to tokens
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个字典将索引映射到标记
- en: We insert the tokens `"BOS"` (beginning of the sentence) and `"EOS"` (end of
    the sentence) at the start and end of each phrase, respectively. The dictionary
    `en_word_dict` assigns each token a unique integer value. Further, the `"PAD"`
    token, used for padding, is allocated the integer 0, while the `"UNK"` token,
    representing unknown tokens, is given the integer 1\. A reverse dictionary, `en_idx_dict`,
    maps integers (indexes) back to their corresponding tokens. This reverse mapping
    is essential for converting a sequence of integers back into a sequence of tokens,
    enabling us to reconstruct the original English phrase.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分别在每句话的开始和结束处插入标记 `"BOS"`（句子开始）和 `"EOS"`（句子结束）。字典 `en_word_dict` 为每个标记分配一个唯一的整数值。此外，用于填充的
    `"PAD"` 标记被分配整数0，而代表未知标记的 `"UNK"` 标记被分配整数1。反向字典 `en_idx_dict` 将整数（索引）映射回相应的标记。这种反向映射对于将整数序列转换回标记序列至关重要，使我们能够重建原始的英语短语。
- en: 'Using the dictionary `en_word_dict`, we can transform the English sentence
    “I don’t speak French.” into its numerical representation. This process involves
    looking up each token in the dictionary to find its corresponding integer value.
    For instance:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字典 `en_word_dict`，我们可以将英语句子“我不说法语。”转换为其数值表示。这个过程涉及在字典中查找每个标记以找到其对应的整数值。例如：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding lines of code produce the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码行产生以下输出：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This means that the English sentence “I don’t speak French.” is now represented
    by a sequence of integers [15, 100, 38, 377, 476, 574, 5].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着英语句子“我不说法语。”现在由一系列整数[15, 100, 38, 377, 476, 574, 5]表示。
- en: 'We can also revert the numerical representations into tokens using the dictionary
    `en_idx_dict`. This process involves mapping each integer in the numerical sequence
    back to its corresponding token as defined in the dictionary. Here’s how it is
    done:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用字典 `en_idx_dict` 将数值表示转换回标记。这个过程涉及将数值序列中的每个整数映射回字典中定义的相应标记。以下是操作方法：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Converts indexes to tokens
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将索引转换为标记
- en: ② Joins tokens into a string
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将标记连接成一个字符串
- en: ③ Replaces the separator with a space
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将分隔符替换为空格
- en: ④ Removes the space before punctuations
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 删除标点符号前的空格
- en: The output of the preceding code snippet is
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出是
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The dictionary `en_idx_dict` is used to translate numbers back into their original
    tokens. Following this, these tokens are transformed into the complete English
    phrase. This is done by first joining the tokens into a single string and then
    substituting the separator `''</w>''` with a space. We also remove the space before
    punctuation marks. Notice that the restored English phrase has all lowercase letters
    because the pretrained tokenizer automatically converts uppercase letters into
    lowercase to reduce the number of unique tokens. As you’ll see in the next chapter,
    some models, such as GPT2 and ChatGPT, don’t do this; hence, they have a larger
    vocabulary.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 字典`en_idx_dict`用于将数字转换回它们原始的标记。在此之后，这些标记被转换成完整的英语短语。这是通过首先将标记连接成一个字符串，然后将分隔符`''</w>''`替换为空格来完成的。我们还移除了标点符号前的空格。请注意，恢复的英语短语全部为小写字母，因为预训练的标记器自动将大写字母转换为小写以减少唯一标记的数量。正如你将在下一章中看到的，一些模型，如GPT2和ChatGPT，并不这样做；因此，它们的词汇量更大。
- en: Exercise 10.1
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.1
- en: In listing 10.1, we have split the sentence “How are you?” into tokens `['how</w>',
    'are</w>', 'you</w>', '?</w>']`. Follow the steps in this subsection to (i) convert
    the tokens into indexes using the dictionary `en_word_dict`; (ii) convert the
    indexes back to tokens using the dictionary `en_idx_dict`; (iii) restore the English
    sentence by joining the tokens into a string, changing the separator `'</w>'`
    to a space, and removing the space before punctuation marks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表10.1中，我们将句子“你好？”分解成了标记`['how</w>', 'are</w>', 'you</w>', '?</w>']`。按照本小节中的步骤进行操作，(i)
    使用字典`en_word_dict`将标记转换为索引；(ii) 使用字典`en_idx_dict`将索引转换回标记；(iii) 通过将标记连接成一个字符串，将分隔符`'</w>'`改为空格，并移除标点符号前的空格来恢复英语句子。
- en: We can apply the same steps to French phrases to map tokens to indexes and vice
    versa.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将相同的步骤应用于法语短语，将标记映射到索引，反之亦然。
- en: Listing 10.3 Mapping French tokens to indexes
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.3 将法语标记映射到索引
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Tokenizes all French sentences
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将所有法语句子进行标记化
- en: ② Counts the frequency of French tokens
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ② 统计法语标记的频率
- en: ③ Creates a dictionary to map French tokens to indexes
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个将法语标记映射到索引的字典
- en: ④ Creates a dictionary to map indexes to French tokens
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建一个将索引映射到法语标记的字典
- en: 'The dictionary `fr_word_dict` assigns an integer to each French token, while
    `fr_idx_dict` maps these integers back to their corresponding French tokens. Next,
    I’ll demonstrate how to transform the French phrase “Je ne parle pas français.”
    into its numerical representation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 字典`fr_word_dict`为每个法语标记分配一个整数，而`fr_idx_dict`将这些整数映射回它们相应的法语标记。接下来，我将演示如何将法语短语“Je
    ne parle pas français.”转换成其数值表示：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output from the preceding code snippet is
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码片段的输出结果是
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The tokens for the French phrase “Je ne parle pas français.” are converted into
    a sequence of integers, as shown.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 法语短语“Je ne parle pas français.”的标记被转换成一系列整数，如下所示。
- en: 'We can transform the numerical representations back into French tokens using
    the dictionary `fr_idx_dict`. This involves translating each number in the sequence
    back to its respective French token in the dictionary. Once the tokens are retrieved,
    they can be joined to reconstruct the original French phrase. Here’s how it’s
    done:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用字典`fr_idx_dict`将数值表示转换回法语标记。这涉及到将序列中的每个数字转换回字典中相应的法语标记。一旦检索到标记，它们就可以被连接起来以重建原始的法语短语。以下是完成方式：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output from the preceding code block is
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块输出的结果是
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It’s important to recognize that the restored French phrase doesn’t exactly
    match its original form. This discrepancy is due to the tokenization process,
    which transforms all uppercase letters into lowercase and eliminates accent marks
    in French.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要认识到恢复的法语短语并不完全匹配其原始形式。这种差异是由于标记化过程，该过程将所有大写字母转换为小写，并消除了法语中的重音符号。
- en: Exercise 10.2
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.2
- en: In listing 10.1, we have split the sentence “Comment êtes-vous?” into tokens
    `['comment</w>', 'et', 'es-vous</w>', '?</w>']`. Follow the steps in this subsection
    to (i) convert the tokens into indexes using the dictionary `fr_word_dict`; (ii)
    convert the indexes back to tokens using the dictionary `fr_idx_dict`; (iii) restore
    the French phrase by joining the tokens into a string, changing the separator
    `'</w>'` to a space, and removing the space before punctuation marks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表10.1中，我们将句子“Comment êtes-vous?”分解为标记`['comment</w>', 'et', 'es-vous</w>',
    '?</w>']`。按照本小节中的步骤进行以下操作：(i) 使用字典`fr_word_dict`将标记转换为索引；(ii) 使用字典`fr_idx_dict`将索引转换回标记；(iii)
    通过将标记连接成一个字符串来恢复法语文句，将分隔符`'</w>'`更改为空格，并删除标点符号前的空格。
- en: 'Save the four dictionaries in the folder /files/ on your computer so that you
    can load them up and start translating later without worrying about first mapping
    tokens to indexes and vice versa:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将四个字典保存在您的计算机上的文件夹/files/中，以便您可以加载它们并在稍后开始翻译，而无需担心首先将标记映射到索引，反之亦然：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The four dictionaries are now saved in a single pickle file `dict.p`. Alternatively,
    you can download the file from the book’s GitHub repository.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在四个字典已保存为单个pickle文件`dict.p`。或者，您也可以从本书的GitHub仓库下载该文件。
- en: 10.1.2 Sequence padding and batch creation
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 序列填充和批次创建
- en: We’ll divide the training data into batches during training for computational
    efficiency and accelerated convergence, as we have done in previous chapters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在训练期间将训练数据划分为批次以提高计算效率和加速收敛，正如我们在前面的章节中所做的那样。
- en: 'Creating batches for other data formats such as images is straightforward:
    simply group a specific number of inputs to form a batch since they all have the
    same size. However, in natural language processing, batching can be more complex
    due to the varying lengths of sentences. To standardize the length within a batch,
    we pad the shorter sequences. This uniformity is crucial since the numerical representations
    fed into the Transformer need to have the same length. For instance, English phrases
    in a batch may vary in length (this can also happen to French phrases in a batch).
    To address this, we append zeros to the end of the numerical representations of
    shorter phrases in a batch, ensuring that all inputs to the Transformer model
    are of equal length.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为其他数据格式（如图像）创建批次是直接的：只需将特定数量的输入分组形成一个批次，因为它们都具有相同的大小。然而，在自然语言处理中，由于句子长度的不同，批次可能会更复杂。为了在批次内标准化长度，我们填充较短的序列。这种一致性至关重要，因为输入到Transformer中的数值表示需要具有相同的长度。例如，一个批次中的英文短语长度可能不同（这也可能发生在批次中的法语文句）。为了解决这个问题，我们在批次中较短的短语的数值表示的末尾添加零，确保所有输入到Transformer模型中的长度都相等。
- en: Note Incorporating `BOS` and `EOS` tokens at the beginning and end of each sentence,
    as well as padding shorter sequences within a batch, is a distinctive feature
    in machine language translation. This distinction arises from the fact that the
    input consists of entire sentences or phrases. In contrast, as you will see in
    the next two chapters, training a text generation model does not entail these
    processes; the model’s input contains a predetermined number of tokens.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在机器翻译中，在每句话的开始和结尾加入`BOS`和`EOS`标记，以及在一个批次中对较短的序列进行填充，这是一个显著的特征。这种区别源于输入由整个句子或短语组成。相比之下，正如你将在下一章中看到的，训练一个文本生成模型不需要这些过程；模型的输入包含一个预定的标记数量。
- en: 'We start by converting all English phrases into their numerical representations
    and then apply the same process to the French phrases:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将所有英文短语转换为它们的数值表示，然后对法语文句应用相同的过程：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we put the numerical representations into batches for training:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数值表示放入批次进行训练：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we have sorted observations in the training dataset by the length
    of the English phrases before placing them into batches. This method ensures that
    the observations within each batch are of a comparable length, consequently decreasing
    the need for padding. As a result, this approach not only reduces the overall
    size of the training data but also accelerates the training process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在将观察结果放入批次之前，已经根据英文短语的长度对训练数据集中的观察结果进行了排序。这种方法确保了每个批次中的观察结果具有可比的长度，从而减少了填充的需要。因此，这种方法不仅减少了训练数据的总体大小，还加速了训练过程。
- en: 'To pad sequences in a batch to the same length, we define the following function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将批次中的序列填充到相同的长度，我们定义了以下函数：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Find out the length of the longest sequence in the batch.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ① 找出批次中最长序列的长度。
- en: ② If a batch is shorter than the longest sequence, add 0s to the sequence at
    the end.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ② 如果批次比最长的序列短，则在序列末尾添加 0。
- en: The function `seq_padding()` first identifies the longest sequence within the
    batch. Then it appends zeros to the end of shorter sequences to ensure that every
    sequence in the batch matches this maximum length.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `seq_padding()` 首先在批次中识别最长的序列。然后，它将零添加到较短的序列的末尾，以确保批次中的每个序列都与这个最大长度匹配。
- en: To conserve space, we have created a `Batch()` class within the local module
    ch09util.py that you downloaded in the last chapter (see figure 10.1).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，我们在本地模块 ch09util.py 中创建了一个 `Batch()` 类，您在上一个章节中已下载（见图 10.1）。
- en: Listing 10.4 Creating a `B`atc`h()` class in the local module
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.4 在本地模块中创建一个 `B`atc`h()` 类
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Creates a source mask to hide padding at the end of the sentence
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个源掩码以隐藏句子末尾的填充
- en: ② Creates input to the decoder
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为解码器创建输入
- en: ③ Shifts the input one token to the right and uses it as output
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入向右移动一个标记并将其用作输出
- en: ④ Creates a target mask
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建一个目标掩码
- en: '![](../../OEBPS/Images/CH10_F01_Liu.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH10_F01_Liu.png)'
- en: 'Figure 10.1 What does the `Batch()` class do? The `Batch()` class takes two
    inputs: `src` and `trg`, sequences of indexes for the source language and the
    target language, respectively. It adds several attributes to the training data:
    `src_mask`, the source mask to conceal padding; `modified trg`, the input to the
    decoder; `trg_y`, the output to the decoder; `trg_mask`, the target mask to hide
    padding and future tokens.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 `Batch()` 类的作用是什么？`Batch()` 类接受两个输入：`src` 和 `trg`，分别代表源语言和目标语言的索引序列。它向训练数据添加了几个属性：`src_mask`，用于隐藏填充的源掩码；`modified
    trg`，解码器的输入；`trg_y`，解码器的输出；`trg_mask`，用于隐藏填充和未来标记的目标掩码。
- en: 'The `Batch()` class processes a batch of English and French phrases, converting
    them into a format suitable for training. To make this explanation more tangible,
    consider the English phrase “How are you?” and its French equivalent “Comment
    êtes-vous?” as our example. The `Batch()` class receives two inputs: `src`, which
    is the sequence of indexes representing the tokens in “How are you?”, and `trg`,
    the sequence of indexes for the tokens in “Comment êtes-vous?”. This class generates
    a tensor, `src_mask`, to conceal the padding at the sentence’s end. For instance,
    the sentence “How are you?” is broken down into six tokens: `[''BOS'', ''how'',
    ''are'', ''you'', ''?'', ''EOS'']`. If this sequence is part of a batch with a
    maximum length of eight tokens, two zeros are added to the end. The `src_mask`
    tensor instructs the model to disregard the final two tokens in such scenarios.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`Batch()` 类处理一批英语和法语短语，将它们转换为适合训练的格式。为了使这个解释更具体，以英语短语“How are you?”及其法语对应短语“Comment
    êtes-vous?”为例。`Batch()` 类接收两个输入：`src`，代表“How are you?”中标记的索引序列，以及 `trg`，代表“Comment
    êtes-vous?”中标记的索引序列。这个类生成一个张量 `src_mask`，用于隐藏句子末尾的填充。例如，句子“How are you?”被分解成六个标记：`[''BOS'',
    ''how'', ''are'', ''you'', ''?'', ''EOS'']`。如果这个序列是长度为八个标记的批次的一部分，则在末尾添加两个零。`src_mask`
    张量指示模型在这种情况下忽略最后的两个标记。'
- en: 'The `Batch()` class additionally prepares the input and output for the Transformer’s
    decoder. Consider the French phrase “Comment êtes-vous?”, which is transformed
    into six tokens: `[''BOS'', ''comment'', ''et'', ''es-vous'', ''?'', ''EOS'']`.
    The indexes of these first five tokens serve as the input to the decoder, named
    `trg`. Next, we shift this input one token to the right to form the decoder’s
    output, `trg_y`. Hence, the input comprises indexes for `[''BOS'', ''comment'',
    ''et'', ''es-vous'', ''?'']`, while the output consists of indexes for `[''comment'',
    ''et'', ''es-vous'', ''?'', ''EOS'']`. This approach mirrors what we discussed
    in chapter 8 and is designed to force the model to predict the next token based
    on the previous ones.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`Batch()` 类还准备了 Transformer 解码器的输入和输出。以法语短语“Comment êtes-vous?”为例，它被转换成六个标记：`[''BOS'',
    ''comment'', ''et'', ''es-vous'', ''?'', ''EOS'']`。这些前五个标记的索引作为解码器的输入，命名为 `trg`。接下来，我们将这个输入向右移动一个标记以形成解码器的输出，`trg_y`。因此，输入包含
    `[''BOS'', ''comment'', ''et'', ''es-vous'', ''?'']` 的索引，而输出则包含 `[''comment'',
    ''et'', ''es-vous'', ''?'', ''EOS'']` 的索引。这种方法与我们第 8 章讨论的内容相似，旨在迫使模型根据前面的标记预测下一个标记。'
- en: 'The `Batch()` class also generates a mask, `trg_mask`, for the decoder’s input.
    The aim of this mask is to conceal the subsequent tokens in the input, ensuring
    that the model relies solely on previous tokens for making predictions. This mask
    is produced by the `make_std_mask()` function, which is defined within the local
    module ch09util:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`Batch()` 类还生成了一个用于解码器输入的掩码，`trg_mask`。这个掩码的目的是隐藏输入中的后续标记，确保模型仅依赖于先前标记进行预测。这个掩码是由
    `make_std_mask()` 函数生成的，该函数定义在本地模块 ch09util 中：'
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `subsequent_mask()` function generates a mask specifically for a sequence,
    instructing the model to focus solely on the actual sequence and disregard the
    padded zeros at the end, which are used only to standardize sequence lengths.
    The `make_std_mask()` function, on the other hand, constructs a standard mask
    for the target sequence. This standard mask has the dual role of concealing both
    the padded zeros and the future tokens in the target sequence.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`subsequent_mask()` 函数为序列生成一个特定的掩码，指示模型仅关注实际序列，并忽略末尾的填充零，这些填充零仅用于标准化序列长度。另一方面，`make_std_mask()`
    函数构建了一个针对目标序列的标准掩码。这个标准掩码具有双重作用，即隐藏填充零和目标序列中的后续标记。'
- en: 'Next, we import the `Batch()` class from the local module and use it to create
    batches of training data:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们导入 `Batch()` 类从本地模块，并使用它来创建训练数据批次：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `BatchLoader()` class creates data batches intended for training. Each batch
    in this list contains 128 pairs, where each pair contains numerical representations
    of an English phrase and its corresponding French translation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`BatchLoader()` 类创建用于训练的数据批次。列表中的每个批次包含 128 对，其中每对包含一个英语短语及其对应的法语翻译的数值表示。'
- en: 10.2 Word embedding and positional encoding
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 词嵌入和位置编码
- en: 'After tokenization in the last section, English and French phrases are represented
    by sequences of indexes. In this section, you’ll use word embedding to transform
    these indexes (essentially one-hot vectors) into compact vector representations.
    Doing so captures the semantic information and interrelationship of tokens in
    a phrase. Word embedding also improves training efficiency: instead of bulky one-hot
    vectors, word embedding uses continuous, lower-dimensional vectors to reduce the
    model’s complexity and dimensionality.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节进行分词之后，英语和法语短语被表示为一系列的索引。在本节中，您将使用词嵌入将这些索引（本质上是一维热向量）转换为紧凑的向量表示。这样做可以捕捉短语中标记的语义信息和相互关系。词嵌入还可以提高训练效率：与庞大的热向量相比，词嵌入使用连续的、低维向量来减少模型的复杂性和维度。
- en: The attention mechanism processes all tokens in a phrase at the same time instead
    of sequentially. This enhances its efficiency but doesn’t inherently allow it
    to recognize the sequence order of the tokens. Therefore, we’ll add positional
    encodings to the input embeddings by using sine and cosine functions of varying
    frequencies.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制同时处理短语中的所有标记，而不是按顺序处理。这提高了其效率，但本身并不允许它识别标记的序列顺序。因此，我们将通过使用不同频率的正弦和余弦函数，将位置编码添加到输入嵌入中。
- en: 10.2.1 Word embedding
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 词嵌入
- en: 'The numerical representations of the English and French phrases involve a large
    number of indexes. To determine the exact number of distinct indexes required
    for each language, we can count the number of unique elements in the `en_word_dict`
    and `fr_word_dict` dictionaries. Doing so generates the total number of unique
    tokens in each language’s vocabulary (we’ll use them as inputs to the Transformer
    later):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 英语和法语短语的数值表示涉及大量的索引。为了确定每种语言所需的唯一索引的确切数量，我们可以计算 `en_word_dict` 和 `fr_word_dict`
    字典中唯一元素的数量。这样做会生成每种语言词汇表中唯一标记的总数（我们将在后面将它们作为 Transformer 的输入使用）：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output is
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In our dataset, there are 11,055 unique English tokens and 11,239 unique French
    tokens. Utilizing one-hot encoding for these would result in an excessively high
    number of parameters to train. To address this, we will employ word embeddings,
    which compress the numerical representations into continuous vectors, each with
    a length of `d_model = 256`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，有 11,055 个唯一的英语标记和 11,239 个唯一的法语标记。对这些使用一维热编码会导致训练时参数数量过高。为了解决这个问题，我们将采用词嵌入，它将数值表示压缩成连续的向量，每个向量的长度为
    `d_model = 256`。
- en: 'This is achieved through the use of the `Embeddings()` class, which is defined
    in the local module ch09util:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过使用定义在本地模块 ch09util 中的 `Embeddings()` 类来实现的：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `Embeddings()` class defined previously utilizes PyTorch’s `Embedding()`
    class. It also multiplies the output by the square root of `d_model`, which is
    256\. This multiplication is intended to counterbalance the division by the square
    root of `d_model` that occurs later during the computation of attention scores.
    The `Embeddings()` class decreases the dimensionality of the numerical representations
    of English and French phrases. We discussed in detail how PyTorch’s `Embedding()`
    class works in chapter 8\.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 之前定义的 `Embeddings()` 类使用了 PyTorch 的 `Embedding()` 类。它还将输出乘以 `d_model` 的平方根，即
    256。这种乘法是为了抵消在计算注意力分数过程中发生的除以 `d_model` 的平方根。`Embeddings()` 类降低了英语和法语短语的数值表示的维度。我们在第
    8 章详细讨论了 PyTorch 的 `Embedding()` 类是如何工作的。
- en: 10.2.2 Positional encoding
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 位置编码
- en: To accurately represent the sequence order of elements in both input and output,
    we introduce the `PositionalEncoding()` class in the local module.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确表示输入和输出中元素序列的顺序，我们在本地模块中引入了 `PositionalEncoding()` 类。
- en: Listing 10.5 A class to calculate positional encoding
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.5 计算位置编码的类
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① Initiates the class, allowing a maximum of 5,000 positions
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化类，允许最大 5,000 个位置
- en: ② Applies sine function to even indexes in the vector
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将正弦函数应用于向量的偶数索引
- en: ③ Applies cosine function to odd indexes in the vector
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将余弦函数应用于向量的奇数索引
- en: ④ Adds positional encoding to word embedding
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将位置编码添加到词嵌入中
- en: The `PositionalEncoding()` class generates vectors for sequence positions using
    sine functions for even indexes and cosine functions for odd indexes. It’s important
    to note that in the `PositionalEncoding()` class, the `requires_grad_(False)`
    argument is included because there is no need to train these values. They remain
    constant across all inputs, and they don’t change during the training process.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`PositionalEncoding()` 类使用正弦函数对偶数索引进行编码，使用余弦函数对奇数索引进行编码来生成序列位置的向量。需要注意的是，在
    `PositionalEncoding()` 类中，包含了 `requires_grad_(False)` 参数，因为这些值不需要进行训练。它们在所有输入中保持不变，并且在训练过程中不会改变。'
- en: 'For example, the indexes for the six tokens `[''BOS'', ''how'', ''are'', ''you'',
    ''?'', ''EOS'']` from the English phrase are first processed through a word embedding
    layer. This step transforms these indexes into a tensor with the dimensions of
    (1, 6, 256): 1 means there is only 1 sequence in the batch; 6 means there are
    6 tokens in the sequence; 256 means each token is represented by a 256-value vector.
    After this word embedding process, the `PositionalEncoding()` class is employed
    to calculate the positional encodings for the indexes corresponding to the tokens
    `[''BOS'', ''how'', ''are'', ''you'', ''?'', ''EOS'']`. This is done to provide
    the model with information about the position of each token in the sequence. Better
    yet, we can tell you the exact values of the positional encodings for the previous
    six tokens by using the following code block:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，来自英语短语 `['BOS', 'how', 'are', 'you', '?', 'EOS']` 的六个标记的索引首先通过一个词嵌入层进行处理。这一步将这些索引转换为一个维度为
    (1, 6, 256) 的张量：1 表示批处理中只有一个序列；6 表示序列中有 6 个标记；256 表示每个标记由一个 256 价值的向量表示。在完成词嵌入过程后，`PositionalEncoding()`
    类被用来计算对应于标记 `['BOS', 'how', 'are', 'you', '?', 'EOS']` 的索引的位置编码。这样做是为了向模型提供有关每个标记在序列中位置的信息。更好的是，我们可以通过以下代码块告诉你前六个标记的位置编码的确切值：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ① Instantiates the PositionalEncoding() class and set the model dimension to
    256
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ① 实例化 PositionalEncoding() 类并将模型维度设置为 256
- en: ② Creates a word embedding and fills it with zeros
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个词嵌入并将其填充为零
- en: ③ Calculates the input embedding by adding positional encoding to the word embedding
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 通过向词嵌入添加位置编码来计算输入嵌入
- en: ④ Prints out the input embedding, which is the same as positional encoding since
    word embedding is set to zero
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印出输入嵌入，由于词嵌入被设置为零，因此与位置编码相同
- en: 'We first create an instance, `pe`, of the `PositionalEncoding()` class by setting
    the model dimension to 256 and the dropout rate to 0.1\. Since the output from
    this class is the sum of word embedding and positional encoding, we create a word
    embedding filled with zeros and feed it to `pe`: this way, the output is the same
    as the positional encoding.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个 `PositionalEncoding()` 类的实例 `pe`，将模型维度设置为 256，并将dropout率设置为 0.1。由于这个类的输出是词嵌入和位置编码的和，我们创建一个填充为零的词嵌入并将其输入到
    `pe` 中：这样输出就是位置编码。
- en: 'After running the preceding code block, you’ll see the following output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码块后，你会看到以下输出：
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The preceding tensor represents the positional encoding for the English phrase
    “How are you?” It’s important to note that this positional encoding also has the
    dimensions of (1, 6, 256), which matches the size of the word embedding for “How
    are you?”. The next step involves combining the word embedding and positional
    encoding into a single tensor.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的张量表示了英语短语“你好吗？”的位置编码。重要的是要注意，这个位置编码也有(1, 6, 256)的维度，这与“你好吗？”的词嵌入大小相匹配。下一步是将词嵌入和位置编码组合成一个单独的张量。
- en: An essential characteristic of positional encodings is that their values are
    the same no matter what the input sequences are. This means that regardless of
    the specific input sequence, the positional encoding for the first token will
    always be the same 256-value vector, `[0.0000e+00, 1.1111e+00, ..., 1.1111e+00]`,
    as shown in the above output. Similarly, the positional encoding for the second
    token will always be `[9.3497e-01, 6.0034e-01, ..., 1.1111e+00]`, and so on. Their
    values don’t change during the training process either.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码的一个基本特征是，无论输入序列是什么，它们的值都是相同的。这意味着无论具体的输入序列是什么，第一个标记的位置编码始终是相同的256值向量，如上输出所示，即`[0.0000e+00,
    1.1111e+00, ..., 1.1111e+00]`。同样，第二个标记的位置编码始终是`[9.3497e-01, 6.0034e-01, ..., 1.1111e+00]`，依此类推。它们的值在训练过程中也不会改变。
- en: 10.3 Training the Transformer for English-to-French translation
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 训练英语到法语翻译的Transformer
- en: Our constructed English-to-French translation model can be viewed as a multicategory
    classifier. The core objective is to predict the next token in the French vocabulary
    when translating an English sentence. This is somewhat similar to the image classification
    project we discussed in chapter 2, though this model is significantly more complex.
    This complexity necessitates careful selection of the loss function, optimizer,
    and training loop parameters.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的英语到法语翻译模型可以被视为一个多类别分类器。核心目标是预测在翻译英语句子时法语词汇中的下一个标记。这与我们在第2章讨论的图像分类项目有些相似，尽管这个模型要复杂得多。这种复杂性需要仔细选择损失函数、优化器和训练循环参数。
- en: In this section, we will detail the process of selecting an appropriate loss
    function and optimizer. We will train the Transformer using batches of English-to-French
    translations as our training dataset. After the model is trained, you’ll learn
    how to translate common English phrases into French.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细说明选择合适的损失函数和优化器的过程。我们将使用英语到法语翻译的批次作为我们的训练数据集来训练Transformer。在模型训练完成后，你将学习如何将常见的英语短语翻译成法语。
- en: 10.3.1 Loss function and the optimizer
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 损失函数和优化器
- en: 'First, we import the `create_model()` function from the local module ch09util.py
    and construct a Transformer so that we can train it to translate English to French:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从本地模块ch09util.py中导入`create_model()`函数，构建一个Transformer，以便我们可以训练它将英语翻译成法语：
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The paper “Attention Is All You Need” uses various combinations of hyperparameters
    when constructing the model. Here we choose a model dimension of 256 with 8 heads
    because we find this combination does a good job translating English to French
    in our setting. Interested readers could potentially use a validation set to tune
    hyperparameters to select the best model in their own projects.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 论文“Attention Is All You Need”在构建模型时使用了各种超参数的组合。在这里，我们选择了一个维度为256，8个头的模型，因为我们发现这个组合在我们的设置中在将英语翻译成法语方面做得很好。感兴趣的读者可以使用验证集来调整超参数，以选择他们自己项目中的最佳模型。
- en: We’ll follow the original paper “Attention Is All You Need” and use label smoothing
    during training. Label smoothing is commonly used in training deep neural networks
    to improve the generalization of the model. It is used to address overconfidence
    problems (the predicted probability is greater than the true probability) and
    overfitting in classifications. Specifically, it modifies the way the model learns
    by adjusting the target labels, aiming to reduce the model’s confidence in the
    training data, which can lead to better performance on unseen data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循原始论文“Attention Is All You Need”并在训练过程中使用标签平滑。标签平滑通常在训练深度神经网络时使用，以提高模型的泛化能力。它用于解决过自信问题（预测概率大于真实概率）和分类中的过拟合问题。具体来说，它通过调整目标标签来修改模型的学习方式，旨在降低模型对训练数据的信心，这可能导致在未见数据上的更好性能。
- en: 'In a typical classification task, target labels are represented in a one-hot
    encoding format. This representation implies absolute certainty about the correctness
    of the label for each training sample. Training with absolute certainty can lead
    to two main problems. The first is overfitting: the model becomes overly confident
    in its predictions, fitting too closely to the training data, which can harm its
    performance on new, unseen data. The second problem is poor calibration: models
    trained this way often output overconfident probabilities. For instance, they
    might output a probability of 99% for a correct class when, realistically, the
    confidence should be lower.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Label smoothing adjusts the target labels to be less confident. Instead of having
    a target label of `[1, 0, 0]` for a three-class problem, you might have something
    like `[0.9, 0.05, 0.05]`. This approach encourages the model not to be too confident
    about its predictions by penalizing overconfident outputs. The smoothed labels
    are a mixture of the original label and some distribution over the other labels
    (usually the uniform distribution).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: We define the following `LabelSmoothing()` class in the local module ch09util.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 A class to conduct label smoothing
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ① Extracts predictions from the model
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: ② Extracts actual labels from the training data and adds noise to them
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: ③ Uses the smoothed labels as targets when calculating loss
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The `LabelSmoothing()` class first extracts the predictions from the model.
    It then smoothes the actual labels in the training dataset by adding noise to
    it. The parameter `smoothing` controls how much noise we inject into the actual
    label. The label `[1, 0, 0]` is smoothed to `[0.9, 0.05, 0.05]` if you set `smoothing=0.1`,
    and it is smoothed to `[0.95, 0.025, 0.025]` if you set `smoothing=0.05`, for
    example. The class then calculates the loss by comparing the predictions with
    the smoothed labels.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'As in previous chapters, the optimizer we use is the Adam optimizer. However,
    instead of using a constant learning rate throughout training, we define the `NoamOpt()`
    class in the local module to change the learning rate during the training process:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ① Defines warm-up steps
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: ② A step() method to apply the optimizer to adjust model parameters
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates the learning rate based on steps
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The `NoamOpt()` class, as defined previously, implements a warm-up learning
    rate strategy. First, it increases the learning rate linearly during the initial
    warmup steps of training. Following this warm-up period, the class then decreases
    the learning rate, adjusting it in proportion to the inverse square root of the
    training step number.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the optimizer for training:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: To define the loss function for training, we first create the following `SimpleLossCompute()`
    class in the local module.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 A class to compute loss
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① Uses the model to make predictions
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: ② Compares the predictions with labels to calculate loss, utilizing label smoothing
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates gradients with respect to model parameters
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算相对于模型参数的梯度
- en: ④ Adjusts model parameters (backpropagate)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 调整模型参数（反向传播）
- en: 'The `SimpleLossCompute()` class is designed with three key elements: `generator`,
    serving as the prediction model; `criterion`, which is a function to calculate
    loss; and `opt`, the optimizer. This class processes a batch of training data,
    denoted as (x, y), by utilizing the generator for predictions. It subsequently
    evaluates the loss by comparing these predictions with the actual labels y (which
    is handled by the `LabelSmoothing()` class defined earlier; the actual labels
    y will be smoothed in the process). The class computes gradients relative to the
    model parameters and utilizes the optimizer to update these parameters accordingly.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleLossCompute()` 类设计有三个关键元素：`generator` 作为预测模型；`criterion`，这是一个计算损失的功能；以及
    `opt`，优化器。这个类通过利用生成器进行预测来处理一个批次的训练数据，表示为 (x, y)。随后，它通过比较这些预测与实际标签 y（由之前定义的 `LabelSmoothing()`
    类处理；实际标签 y 将在过程中进行平滑）来评估损失。该类计算相对于模型参数的梯度，并利用优化器相应地更新这些参数。'
- en: 'We are now ready to define the loss function:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义损失函数：
- en: '[PRE33]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Next, we’ll train the Transformer by using the data we prepared earlier in the
    chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用本章前面准备的数据来训练 Transformer。
- en: 10.3.2 The training loop
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 训练循环
- en: We could potentially divide the training data into a train set and a validation
    set and train the model until the performance of the model doesn’t improve on
    the validation set, similar to what we have done in chapter 2\. However, to save
    space, we’ll train the model for 100 epochs. We’ll calculate the loss and the
    number of tokens from each batch. After each epoch, we calculate the average loss
    in the epoch as the ratio between the total loss and the total number of tokens.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将训练数据分成训练集和验证集，并训练模型，直到模型在验证集上的性能不再提高，这与我们在第 2 章中所做的一样。然而，为了节省空间，我们将训练模型
    100 个周期。我们将计算每个批次的损失和标记数。在每个周期之后，我们计算该周期的平均损失，作为总损失与总标记数的比率。
- en: Listing 10.8 Training a Transformer to translate English to French
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.8 训练一个 Transformer 将英语翻译成法语
- en: '[PRE34]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Makes predictions using the Transformer
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用 Transformer 进行预测
- en: ② Calculates loss and adjusts model parameters
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算损失并调整模型参数
- en: ③ Counts the number of tokens in the batch
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算批次的标记数
- en: ④ Saves the weights in the trained model after training
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 训练后保存训练模型的权重
- en: This training process takes a couple of hours if you are using a CUDA-enabled
    GPU. It may take a full day if you are using CPU training. Once the training is
    done, the model weights are saved as en2fr.pth on your computer. Alternatively,
    you can download the trained weights from my website ([https://gattonweb.uky.edu/faculty/lium/gai/ch9.zip](https://gattonweb.uky.edu/faculty/lium/gai/ch9.zip)).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是支持 CUDA 的 GPU，这个过程可能需要几个小时。如果你使用 CPU 训练，可能需要整整一天。一旦训练完成，模型权重将保存在你的电脑上作为
    en2fr.pth。或者，你也可以从我的网站上下载训练好的权重（[https://gattonweb.uky.edu/faculty/lium/gai/ch9.zip](https://gattonweb.uky.edu/faculty/lium/gai/ch9.zip)）。
- en: 10.4 Translating English to French with the trained model
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 使用训练好的模型将英语翻译成法语
- en: Now that you have trained the Transformer, you can use it to translate any English
    sentence to French. We define a function `translate()` as shown in the following
    listing.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经训练了 Transformer，你可以用它将任何英文句子翻译成法语。我们定义了一个名为 `translate()` 的函数，如下所示。
- en: Listing 10.9 Defining a `translate()` function to translate English to French
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.9 定义一个 `translate()` 函数以将英语翻译成法语
- en: '[PRE35]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ① Uses encoder to convert the English phrase to a vector representation
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用编码器将英文短语转换为向量表示
- en: ② Predicts the next token using the decoder
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用解码器预测下一个标记
- en: ③ Stops translating when the next token is “EOS”
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 当下一个标记是“EOS”时停止翻译
- en: ④ Joins the predicted tokens to form a French sentence
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将预测的标记连接起来形成一个法语句子
- en: To translate an English phrase to French, we first use the tokenizer to convert
    the English sentence to tokens. We then add `"BOS"` and `"EOS"` at the beginning
    and the end of the phrase. We use the dictionary `en_word_dict` we created earlier
    in the chapter to convert tokens to indexes. We feed the sequence of indexes to
    the encoder in the trained model. The encoder produces an abstract vector representation
    and passes it on to the decoder.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要将英语短语翻译成法语，我们首先使用分词器将英语句子转换为标记。然后，我们在短语的开始和结束处添加`"BOS"`和`"EOS"`。我们使用本章前面创建的`en_word_dict`字典将标记转换为索引。我们将索引序列输入到训练好的模型的编码器中。编码器产生一个抽象向量表示，并将其传递给解码器。
- en: Based on the abstract vector representation of the English sentence produced
    by the encoder, the decoder in the trained model starts translating in an autoregressive
    manner, starting with the beginning token `"BOS"`. In each time step, the decoder
    generates the most likely next token based on previously generated tokens, until
    the predicted token is `"EOS"`, which signals the end of the sentence. Note this
    is slightly different from the text generation approach discussed in chapter 8,
    where the next token is chosen randomly in accordance with its predicted probabilities.
    Here, the method for selecting the next token is deterministic, meaning the token
    with the highest probability is chosen with certainty because we mainly care about
    accuracy. However, you can switch to stochastic prediction as we did in chapter
    8 and use `top-K` sampling and temperature if you want your translation to be
    creative.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 根据编码器产生的英语句子的抽象向量表示，训练好的模型中的解码器以自回归的方式开始翻译，从开始标记`"BOS"`开始。在每个时间步，解码器根据先前生成的标记生成最可能的下一个标记，直到预测的标记是`"EOS"`，这标志着句子的结束。注意，这与第8章中讨论的文本生成方法略有不同，在那里下一个标记是随机选择的，根据其预测概率。在这里，选择下一个标记的方法是确定性的，这意味着我们主要关注准确性，因此我们选择概率最高的标记。然而，如果您希望翻译具有创造性，您可以像第8章中那样切换到随机预测，并使用`top-K`采样和温度。
- en: Finally, we change the token separator to a space and remove the space before
    the punctuation marks. The output is the French translation in a clean format.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将标记分隔符更改为空格，并移除标点符号前的空格。输出结果是格式整洁的法语翻译。
- en: 'Let’s try the `translate()` function with the English phrase “Today is a beautiful
    day!”:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用`translate()`函数翻译英语短语“今天是个美好的一天！”：
- en: '[PRE36]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The output is
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是
- en: '[PRE37]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: You can verify that the French translation indeed means “Today is a beautiful
    day!” by using, say, Google Translate.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用，比如说，谷歌翻译来验证法语翻译确实意味着“今天是个美好的一天！”
- en: 'Let’s try a longer sentence and see if the trained model can successfully translate:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个更长的句子，看看训练好的模型是否能够成功翻译：
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The output is
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是
- en: '[PRE39]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: When I translate the preceding output back to English using Google Translate,
    it says, “a little boy in jeans climbs a small tree while another child watches”—not
    exactly the same as the original English sentence, but the meaning is the same.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当我用谷歌翻译将前面的输出翻译回英语时，它说，“一个穿牛仔裤的小男孩爬上一棵小树，另一个孩子在一旁观看”——并不完全与原始的英语句子相同，但意思是一样的。
- en: 'Next, we’ll test if the trained model generates the same translation for the
    two English sentences “I don’t speak French.” and “I do not speak French.” First,
    let’s try the sentence “I don’t speak French.”:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将测试训练好的模型是否为两个英语句子“我不会说法语。”和“I do not speak French.”生成相同的翻译。首先，让我们尝试句子“I
    don’t speak French.”：
- en: '[PRE40]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The output is
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是
- en: '[PRE41]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now let’s try the sentence “I do not speak French.”:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试句子“I do not speak French.”：
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The output this time is
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这次输出结果是
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The results indicate that French translations of the two sentences are exactly
    the same. This suggests that the encoder component of the Transformer successfully
    grasps the semantic essence of the two phrases. It then represents them as similar
    abstract continuous vector forms, which are subsequently passed on to the decoder.
    The decoder then generates translations based on these vectors and produces identical
    results.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这两个句子的法语翻译完全相同。这表明Transformer的编码器组件成功地把握了这两个短语的语义本质。然后，它将它们表示为相似的抽象连续向量形式，随后传递给解码器。解码器随后根据这些向量生成翻译，并产生相同的结果。
- en: Exercise 10.3
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.3
- en: 'Use the `translate()` function to translate the following two English sentences
    to French. Compare the results with those from Google Translate and see if they
    are the same: (i) I love skiing in the winter! (ii) How are you?'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `translate()` 函数将以下两个英文句子翻译成法语。将结果与谷歌翻译的结果进行比较，看看它们是否相同：(i) 我喜欢冬天滑雪！(ii)
    你好吗？
- en: In this chapter, you trained an encoder-decoder Transformer to translate English
    to French by using more than 47,000 pairs of English-to-French translations. The
    trained model works well, translating common English phrases correctly!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您训练了一个编码器-解码器 Transformer，通过使用超过 47,000 对英法翻译来将英语翻译成法语。训练的模型表现良好，能够正确翻译常见的英语短语！
- en: In the following chapters, you’ll explore decoder-only Transformers. You’ll
    learn to build them from scratch and use them to generate coherent text, better
    than the text you generated in chapter 8 using long short-term memory.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将探索仅解码器 Transformer。您将学习从头开始构建它们，并使用它们生成比第 8 章中使用长短期记忆生成的文本更连贯的文本。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Transformers process input data such as sentences in parallel, unlike recurrent
    neural networks, which handle data sequentially. This parallelism enhances their
    efficiency but doesn’t inherently allow them to recognize the sequence order of
    the input. To address this, Transformers add positional encodings to the input
    embeddings. These positional encodings are unique vectors assigned to each position
    in the input sequence and align in dimension with the input embeddings.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与处理数据序列的循环神经网络不同，Transformers 并行处理输入数据，例如句子。这种并行性提高了它们的效率，但并不固有地允许它们识别输入的序列顺序。为了解决这个问题，Transformers
    将位置编码添加到输入嵌入中。这些位置编码是分配给输入序列中每个位置的独特向量，并在维度上与输入嵌入对齐。
- en: Label smoothing is commonly used in training deep neural networks to improve
    the generalization of the model. It is used to address overconfidence problems
    (the predicted probability is greater than the true probability) and overfitting
    in classifications. Specifically, it modifies the way the model learns by adjusting
    the target labels, aiming to reduce the model’s confidence in the training data,
    which can lead to better performance on unseen data.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签平滑在训练深度神经网络时常用，以提高模型的泛化能力。它用于解决过度自信问题（预测概率大于真实概率）和分类中的过拟合问题。具体来说，它通过调整目标标签来修改模型的学习方式，旨在降低模型对训练数据的信心，这可能导致在未见过的数据上表现更好。
- en: Based on the encoder’s output that captures the meaning of the English phrase,
    the decoder in the trained Transformer starts translating in an autoregressive
    manner, starting with the beginning token `"BOS"`. In each time step, the decoder
    generates the most likely next token based on previously generated tokens, until
    the predicted token is `"EOS"`, which signals the end of the sentence.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于编码器输出的捕获英语短语意义的输出，训练的 Transformer 中的解码器以自回归的方式开始翻译，从开始标记 `"BOS"` 开始。在每个时间步，解码器根据先前生成的标记生成最可能的下一个标记，直到预测标记是
    `"EOS"`，这表示句子的结束。
- en: '* * *'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([1](#footnote-000-backlink))  Vaswani et al, 2017, “Attention Is All You Need.”
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-000-backlink)) Vaswani 等人，2017，“Attention Is All You Need.”
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
