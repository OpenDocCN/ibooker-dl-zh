- en: 2 Working with natural language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hidden structures in unstructured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A search-centric philosophy of language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring distributional semantics and vector-based embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling domain-specific knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges with natural language and querys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying natural language understanding techniques to both content and signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first chapter, we provided a high-level overview of what it means to
    build an AI-powered search system. Throughout the rest of the book, we’ll explore
    and demonstrate the numerous ways your search application can continuously learn
    from your content and your users’ behavioral signals to better understand your
    content, your users, and your domain, and to ultimately deliver users the answers
    they need. We will get much more hands-on in chapter 3, firing up a search server
    of your choice and a data processing layer (Apache Spark) and starting with the
    first of our Jupyter notebooks, which we’ll use throughout the book to walk through
    many step-by-step examples.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into those hands-on examples and specific implementations, however,
    it is important in this chapter that we first establish a shared mental model
    for the higher-level problems we’re trying to solve. Specifically, when it comes
    to intelligent search, we have to deal with many complexities and nuances in natural
    language—both in the content we’re searching and in our users’ search queries.
    We have to deal with keywords, entities, concepts, misspellings, synonyms, acronyms,
    initialisms, ambiguous terms, explicit and implied relationships between concepts,
    hierarchical relationships usually found in taxonomies, higher-level relationships
    usually found in ontologies, and specific instances of entity relationships usually
    found in comprehensive knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: While it might be tempting to dive immediately into some specific problems,
    like how to automatically learn misspellings from content or how to discover synonyms
    from mining user search sessions, it will be more prudent to first establish a
    conceptual foundation that explains what *kinds* of problems we have to deal with
    in search and natural language understanding (NLU). Establishing that philosophical
    foundation will enable us to build better end-to-end solutions in our AI-powered
    search system, where all the parts work together in a cohesive and integrated
    way. This chapter will thus provide the philosophical underpinnings for how we
    tackle the problems of natural language understanding throughout this book and
    how we apply those solutions to make our search applications more intelligent.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin by discussing some common misconceptions about the nature of free
    text and other unstructured data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 The myth of unstructured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The term “unstructured data” has been used for years to describe textual data,
    because it does not appear to be formatted in a way that can be readily interpreted
    and queried. The widely held idea that text, or any other data that doesn’t fit
    a predefined schema (“structure”), is actually “unstructured”, however, is a myth
    that we’ll spend time reconsidering throughout this section.
  prefs: []
  type: TYPE_NORMAL
- en: If you look up *unstructured data* in Wikipedia, it is defined as “information
    that either does not have a pre-defined data model or is not organized in a pre-defined
    manner”. The entry goes on to say that “unstructured information is typically
    text-heavy, but may contain data such as dates, numbers, and facts as well”.
  prefs: []
  type: TYPE_NORMAL
- en: The phrase “unstructured data” is a poor term to describe textual content, however.
    In reality, the terms and phrases present in text encode an enormous amount of
    meaning, and the linguistic rules applied to the text to give it meaning serve
    as their own structure. Calling text unstructured is a bit like calling a song
    playing on the radio “arbitrary audio waves”. Even though every song has unique
    characteristics, most exhibit common attributes (tempo, melodies, harmonies, lyrics,
    and so on). Though these attributes may differ or be absent from song to song,
    they nevertheless fit common expectations that enable meaning to be conveyed by
    and extracted from each song. Textual information typically follows similar rules—sentence
    structure, grammar, punctuation, interaction between parts of speech, and so on.
    Figure 2.1 shows an example of text we’ll explore a bit more in the upcoming sections
    as we investigate this structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F01_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 Unstructured data. This text represents typical unstructured data
    you may find in a search engine.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While text is the most commonly recognized type of unstructured data, there
    are several other kinds of unstructured data with similar characteristics, as
    we’ll see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Types of unstructured data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Free text content is considered the primary type of unstructured data, but search
    engines also commonly index many other kinds of data that similarly don’t fit
    neatly into a structured database. Common examples include images, audio, video,
    and event logs. Figure 2.2 expands on our text example from figure 2.1 and includes
    several other types of unstructured data, such as audio, images, and video.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F02_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 Multiple types of unstructured data. In addition to the text from
    the last figure, we now see images, audio, and video, which are other forms of
    unstructured data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Audio is the most similar to text content, since it is often just another way
    to encode words and sentences. Of course, audio can include much more than just
    spoken words—it can include music and non-language sounds, and it can more effectively
    encode nuances such as emotion, tone of voice, and simultaneously overlapping
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: Images are another kind of unstructured data. Just as words form sentences and
    paragraphs to express ideas, images form grids of colors that, taken together,
    form pictures.
  prefs: []
  type: TYPE_NORMAL
- en: Video, then, serves as yet another kind of unstructured data, as it is a combination
    of multiple images over time, as well as optional audio that coincides with the
    progression of images.
  prefs: []
  type: TYPE_NORMAL
- en: When unstructured data is found mixed with structured data, we typically refer
    to this as *semi-structured* data. Log data is a great example of such semi-structured
    data. Often, log messages contain a structured event date, structured event types
    (such as warning versus error or search versus click), and then some kind of unstructured
    message or description in free text.
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, virtually any kind of file could be considered unstructured
    data, but we’ll primarily deal with the aforementioned types. Search engines are
    often tasked with handling each of these kinds of unstructured data, so we’ll
    discuss strategies for handling them throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Data types in traditional structured databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better deal with our unstructured data, it may be useful to first contrast
    it with structured data in a SQL database. This will allow us to later draw parallels
    between how we can query unstructured data representations versus structured ones.
  prefs: []
  type: TYPE_NORMAL
- en: A record (row) in a SQL database is segmented into columns, which can each be
    of a particular data type. Some of these data types represent discrete values—values
    that come from an enumerated list, such as IDs, names, or textual attributes.
    Other columns may hold continuous values, such as date/time ranges, numbers, and
    other column types that represent ranges without a finite number of possible values.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, when one wants to relate different rows together or to relate
    them to rows in other database tables, “joins” will be performed on the discrete
    values. *Joins* use a shared value (often an ID field) to link two or more records
    together to form a composite record.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if someone had two tables of data, one representing employees and
    another representing companies, then there would likely be an `id` column on the
    `companies` table, and a corresponding `company_id` column on the `employees`
    table. The `company_id` field on the employees table is known as a *foreign key*,
    which is a value in one table that refers to an entity in another table, linking
    the records together based upon a shared identifier. Figure 2.3 demonstrates this,
    showing examples of discrete values, continuous values, and a join across tables
    using a foreign key.
  prefs: []
  type: TYPE_NORMAL
- en: This notion of joining different records together based upon known relationships
    (keys and foreign keys) is a powerful way to work with relational data across
    explicitly
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F03_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 Structured data in a typical database. Discrete values represent
    identifiers and enumerated values, continuous values represent data that falls
    within ranges, and foreign keys exist when the same value exists across two tables
    and can thus be used as a shared attribute that creates a relationship between
    corresponding rows from each table.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: modeled tables, but as we’ll see in the next section, very similar techniques
    can also be applied to free-form unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Joins, fuzzy joins, and entity resolution in unstructured data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whereas structured data in a database is already in an easily queryable form,
    the reality is that unstructured data suffers less from a lack of structure, and
    more from having a large amount of information packed into a very flexible structure.
    In this section, we’ll walk through a concrete example that uncovers this hidden
    structure in unstructured data and demonstrates the ways it can similarly be used
    to find and join relationships between documents.
  prefs: []
  type: TYPE_NORMAL
- en: Foreign keys in unstructured data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve discussed how foreign keys can be used to join two rows together in a
    database, based on a shared identifier between the two records. In this section,
    we’ll show how the same objective can be achieved with text data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can easily map the idea of foreign keys used in a SQL table
    to the unstructured information we explored in figure 2.2\. Notice in figure 2.4
    that two different sections of text both contain the word “Haystack”, which refers
    to a technology conference focused on search relevance.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F04_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 Foreign keys in unstructured data. In this example, the same term
    is being used to join across two related text documents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first instance indicates a conference being spoken at, while the second
    block of text contains general information about the event. For the purposes of
    our example, let’s assume that every piece of information (block of text, image,
    video, and audio clip) is represented as a separate document in our search engine.
    There is functionally very little difference between having two rows in a database
    table that each contain a column with the value “Haystack”, and having separate
    documents in our search engine that each contain the value “Haystack”. In both
    cases, we can think of these documents as related by a foreign key.
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzy foreign keys in unstructured data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With unstructured data, however, we have much more power than with traditional
    structured data modeling. In figure 2.5, for example, notice that now two documents
    are linked and that they both refer to the lead author of this book—one using
    my full name of “Trey Grainger”, and one simply using my first name of “Trey”.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F05_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 Fuzzy foreign keys. In this example, the same entity is being referenced
    using different terms, and a join is occurring based upon multiple phrases resolving
    to the same entity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is an example of *entity resolution*, where there are two different representations
    of the entity, but they can still be resolved to the same meaning, and therefore
    can still be used to join information between two documents. You can think of
    this as a “fuzzy foreign key”, since it’s still a foreign key, but not in a strict
    token-matching sense, as it requires additional natural language processing and
    entity resolution techniques to resolve.
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve opened this door to advanced text processing for entity resolution,
    we can learn even more from our unstructured information. For example, not only
    do the names “Trey” and “Trey Grainger” in these documents refer to the same entity,
    but so do the words “he” and “his”.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll also notice that both an image of me (in the bottom-left corner, in case
    you have no idea what I look like) and a video containing a reference to my name
    are identified as related and joined back to the textual references. We’re relying
    on the hidden structure present in all of this unstructured data to understand
    the meaning, relate the documents together, and learn even more about each of
    the referenced entities in those documents.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with ambiguous terms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far, so good, but in real-world content it’s not always appropriate to assume
    that the same term in multiple places carries the same meaning, or even that our
    entity resolution always resolves entities correctly. This problem of the same
    spelling of words and phrases having multiple potential meanings is called *polysemy*,
    and dealing with these ambiguous terms can be a huge problem in search applications.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed an odd image in the upper-right corner of the previous
    figures that seemed a bit out of place. This image is of a fairly terrifying man
    holding a machete. Apparently, if you go to Google and search for `Trey` `Grainger`,
    this image comes back. If you dig in further, you’ll see in figure 2.6 that there’s
    an x.com (formerly Twitter) user also named “Trey Grainger”, and this image is
    his profile picture.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F06_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 Polysemy. This image shows a Google search for `Trey` `Grainger`.
    Pictures of multiple different people are returned because those people’s names
    share the same spelling, making the phrase “Trey Grainger” ambiguous.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The picture is apparently of Robert Shaw (who plays Quint in the 1975 movie
    *Jaws*), but it’s definitely not the kind of thing you want people to first come
    across when they search for you online!
  prefs: []
  type: TYPE_NORMAL
- en: There are two key lessons to take away here. First, perhaps never Google yourself
    (you might be terrified at what you find!). Second, and on a more serious note,
    polysemy is a major problem in search and natural language understanding. It’s
    not safe to assume a term has a single meaning or even a consistent meaning across
    different contexts, so our AI-powered search engine needs to use context to differentiate
    these various meanings.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured data as a giant graph of relationships
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous sections, we saw that unstructured data not only contains rich
    information (entities and their relationships) but also that it’s possible to
    relate different documents by joining them on shared entities, similar to how
    foreign keys work in traditional databases. Typical unstructured data contains
    so many of these relationships, however, that instead of thinking in terms of
    rows and columns, it may be more useful to think of the collection of data as
    a giant graph of relationships, as we’ll explore in this section.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it should be clear that there is much more structure hidden in
    unstructured data than most people appreciate. Unstructured information is really
    more like “hyper-structured” information—it is a graph that contains much more
    structure than typical “structured data”.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 demonstrates this giant graph of relationships that is present in
    even the small handful of documents in our example. You can see names, dates,
    events, locations, people, companies, and other entities, and you can infer relationships
    between them, using joins between the entities across documents. You’ll also notice
    that the images have been correctly disambiguated so that the machete guy is now
    disconnected from the graph. If all of this can be learned from just a few documents,
    imagine what can be learned from the thousands, millions, or billions of documents
    you have within your search engine.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F07_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 Giant graph of relationships. A rich graph of relationships emerges
    from even a small collection of related documents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Part of the value of an AI-powered search platform is being able to learn insights
    like this from your data. The question is, how do you use this enormous graph
    of semantic knowledge to drive this intelligence?
  prefs: []
  type: TYPE_NORMAL
- en: One of the most powerful ways to use a graph from text data is through a large
    language model (LLM), such as a Transformer model, which was introduced in section
    1.3.4\. These models use deep learning to learn billions of parameters across
    massive datasets, such as crawls of most of the internet, to build a detailed
    understanding of language. This understanding includes both the meanings of words
    in different contexts and the linguistic and conceptual connections between words.
    These models internally represent the giant graph of relationships found in all
    the data they are trained on, which is usually more general than your dataset,
    so the models must be fine-tuned to learn any domain-specific relationships from
    your data. This need for fine-tuning can create some challenges due to LLMs being
    somewhat of a black box, as they otherwise don’t optimally represent your dataset,
    and the information they return could be erroneous.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the inherent structure of the inverted index in your search engine
    makes it very easy to traverse the large graph of relationships in your data without
    any additional explicit data modeling required. An *inverted index* is the primary
    data structure used for lexical search, mapping each keyword or term in the fields
    of your documents to lists (called *postings lists*) of all the documents containing
    those keywords. The inverted index enables very fast lookups of the set of documents
    containing any given term (or term sequences, when considering positional matching
    and Boolean logic implemented through set operations). With those lookups, it
    is possible to traverse between different term sequences using their shared documents
    to calculate a weighted edge in a graph. We will dive deep into how to harness
    this semantic knowledge graph hidden in your data in chapter 5\.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 The structure of natural language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last section, we discussed how text and unstructured data typically contain
    a giant graph of relationships, which can be derived by looking at shared terms
    between different records. If you’ve been building search engines for a while,
    you’re used to thinking about content in terms of “documents”, “fields”, and “terms”
    within those fields. When interpreting the semantic meaning of your content, however,
    there are a few more levels to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 walks through these additional levels of semantic meaning. At the
    most basic level, you have *characters*, which are single letters, numbers, or
    symbols, such as the letter “e” in the figure. One or more characters are then
    combined to form *character sequences* such as “e”, “en”, “eng”, … “engineer”,
    and “engineers”. Some character sequences form terms, which are completed words
    or tokens that carry an identifiable meaning, such as “engineer”, “engineers”,
    “engineering”, or “software”. One or more terms can then be combined into *term
    sequences*—usually called *phrases* when the terms are all sequential. These include
    things like “software engineer”, “software engineers”, and “senior software engineer”.
    For simplicity in this book, we also consider single terms to be “term sequences”,
    so any time we refer to “phrases”, this includes single terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F08_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 Semantic data encoded into free text content. Characters form character
    sequences, which form terms, which form term sequences, which form fields, which
    form documents, which form a corpus.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Term sequences vs. phrases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You may be wondering what the difference is between a “term sequence” and a
    “phrase”. Quite simply, a phrase is a term sequence where all of the terms appear
    sequentially. For example, `"chief executive officer"` is both a phrase and a
    term sequence, whereas `"chief officer"~2` (meaning “officer” within two positions,
    or edit distances, of “chief”) is only a term sequence, since it specifies a sequence
    of terms that is not necessarily sequential. In the vast majority of cases, you
    will only be dealing with sequential term sequences, so we’ll mostly use the word
    “phrase” for simplicity throughout the book when referring inclusively to both
    single terms and multi-term sequential sequences. To avoid confusion, note that
    the word “term” is separately used to refer to “a unique value in a field in the
    search engine”. As such, we will sometimes also refer to unsplit strings with
    multiple words in them in the search engine as “terms”, even though linguistically
    they are considered “phrases” or “term sequences”.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we know that a number of term sequences together can form sentences,
    multiple sentences can form paragraphs, and paragraphs can then be rolled up into
    even larger groups of text. For a search engine, though, the next higher-level
    level of grouping we’ll typically focus on after term sequences is a field. A
    *field* in a search engine is a partitioned and labeled section of a document,
    usually for purposes of searching on or returning as an independent portion of
    the document. Fields containing text can be analyzed in any number of ways using
    a text analyzer, which typically includes techniques like splitting on white space
    and punctuation, lowercasing all terms so they are case-insensitive, stripping
    out noise (stop words and certain characters), stemming or lemmatization to reduce
    terms to a base form, and removing accents. If the text analysis process is unfamiliar
    to you, or you would like a refresher, we recommend checking out chapter 6 of
    *Solr in Action* by Trey Grainger and Timothy Potter (Manning, 2014).
  prefs: []
  type: TYPE_NORMAL
- en: One or more fields are then composed together into a *document*, and multiple
    documents form a *corpus* or collection of data. Whenever a query is executed
    against the search index, it filters the corpus into a *document set*, which is
    a subset of the corpus that specifically relates to the query in question.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these linguistic levels—character sequences, terms, term sequences,
    fields, documents, document sets, and the corpus—provides unique insights into
    understanding your content and its unique meaning within your specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Distributional semantics and embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributional semantics is a research area within the field of natural language
    processing that focuses on the semantic relationships between terms and phrases
    based on the distributional hypothesis. The distributional hypothesis is that
    words that occur in similar contexts tend to share similar meanings. It is summarized
    well by the popular quote, “You shall know a word by the company it keeps.”[¹](#footnote-84)
  prefs: []
  type: TYPE_NORMAL
- en: 'When applying machine learning approaches to your text, these distributional
    semantics become increasingly important, and the search engine makes it incredibly
    easy to derive the context for most linguistic representations present in your
    corpus. For example, if someone wants to find all documents about C-level executives,
    they could issue a query like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This query would match “CEO”, “CMO”, “CFO”, or any other CXO-style title, as
    it is asking for any character sequence starting with “c” and ending with “o”
    with a single character in between.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same kind of freedom exists to query for arbitrarily complex term sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This query would match “VP Engineering”, “VP of Engineering”, “Engineering VP”,
    or even “VP of Software Engineering”, as it is asking to find “VP” and “Engineering”
    within two positions (edit distances) of each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the nature of a search engine’s inverted index also makes it trivial
    to support arbitrary Boolean queries. For example, if someone searches for the
    term “Word”, but we want to ensure any matched documents also contain either the
    term “Microsoft” or “MS” somewhere in the document, we could issue the following
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Search engines support arbitrarily complex combinations of queries for character
    sequences, terms, and term sequences throughout your corpus, returning document
    sets that serve as a unique context of content matching that query. For example,
    if you run a query for `pizza`, the documents returned are more likely going to
    be restaurants than car rental companies, and if you run a query for `machine`
    `learning`, you’re more likely to see jobs for data scientists or software engineers
    than for accountants, food service workers, or pharmacists. This means that you
    can infer a strong relationship between “machine learning” and “software engineering”,
    and a weak relationship between “machine learning” and “food service worker”.
    If you dig deeper, you’ll also be able to see what other terms and phrases most
    commonly co-occur within the machine learning document set relative to the rest
    of your corpus, and thereby better understand the meaning and usage of the phrase
    “machine learning”. We’ll dive into hands-on examples of using these relationships
    in chapter 5.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A basic understanding of vector operations will be important as you progress
    through this book. A *vector* is a list of values describing some attributes of
    an item. For example, if your items are houses, you may have a list of attributes
    like `price`, `size`, and `number of bedrooms`. If you have a home costing $100,000
    with 1,000 square feet and 2 bedrooms, this could be represented as the vector
    `[100000,` `1000,` `2]`.
  prefs: []
  type: TYPE_NORMAL
- en: These attributes (price, size, and number of bedrooms in this example) are called
    *dimensions* or *features*, and a specific collection of dimensions is called
    a *vector space*. You can represent any other items (like other homes, apartments,
    or dwellings) within the same vector space if you can assign them values within
    the dimensions of the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: If we consider other vectors within the same vector space (such as a house `[1000000,`
    `9850,` `12]` and another house `[120000,` `1400,` `3]`), we can perform mathematical
    operations on the vectors to learn trends and compare vectors. For example, you
    may intuitively look at these three example vectors and determine that “home prices
    tend to increase as the number of rooms increases” or that “the number of rooms
    tends to increase as home size increases”. We can also perform similarity calculations
    on vectors to determine that the $120,000 home with 1,400 square feet and 3 bedrooms
    is more similar to the $100,000 home with 1,000 square feet and 2 bedrooms than
    to the $1,000,000 home with 9,850 square feet and 12 bedrooms.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, the distributional hypothesis has been applied to create semantic
    understandings of terms and term sequences through what are known as embeddings.
    An *embedding* is a set of coordinates in a vector space into which we map (or
    “embed”) a concept. More concretely, that set of coordinates is a numerical vector
    (a list of numbers) that is intended to represent the semantic meaning of your
    data (text, image, audio, behavior, or other data modalities). Text-based embeddings
    can represent term sequences of any length, but when representing individual words
    or phrases, we call those embeddings *word embeddings*.
  prefs: []
  type: TYPE_NORMAL
- en: The term sequence is often encoded into a reduced-dimension embedding that can
    be compared with the vectors for all of the other embeddings within the corpus
    to find the most semantically related documents.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this process, it may be useful to think of how a search engine
    works out of the box. Let’s imagine a vector exists for each term that contains
    a value (dimension) for every word in your corpus. It might look something like
    figure 2.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F09_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 Vectors with one dimension per term from the inverted index. Every
    query on the left maps to a vector on the right, with a value of `1` for any term
    from the index that is also in the query, and a `0` for any term from the index
    that is not in the query.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 2.9 demonstrates conceptually how document matching works in lexical
    search engines by default. A *lexical search* is a search where documents are
    matched and ranked based on the degree to which they contain the actual keywords
    or other attributes specified in the query. For every keyword query, a vector
    exists that contains a dimension for every term in the inverted index. If that
    term exists in the query, the value in the vector is `1` for that dimension, and
    if that value does not exist in the query, then the value is `0` for that dimension.
    A similar vector exists for every document in the inverted index, with a `1` value
    for any term from the index that appears in the document, and a `0` for all other
    terms.
  prefs: []
  type: TYPE_NORMAL
- en: When a query is executed, a lookup occurs in the index for any matched terms,
    and then a similarity score is calculated based on a comparison of the vector
    for the query and the vector for the document that is being scored relative to
    the query. We’ll walk through the specific scoring calculation in chapter 3, but
    this high-level understanding is sufficient for now.
  prefs: []
  type: TYPE_NORMAL
- en: There are obvious downsides to this approach. While it’s great for finding documents
    with exact keyword matches, what happens when you want to find “related” things
    instead? For example, you’ll notice that the term “soda” appears in a query, but
    never in the index. Even though there are other kinds of drinks (“apple juice”,
    “water”, “cappuccino”, and “latte”), the search engine will always return zero
    results because it doesn’t understand that the user is searching for a drink.
    Similarly, you’ll notice that even though the term “caffeine” exists in the index,
    queries for `latte`, `cappuccino`, and `green tea` will never match the term “caffeine”,
    even though they are related.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, it is now common practice to use reduced-dimension dense
    embeddings to model a semantic meaning for term sequences in your index and queries.
    A *dense embedding* (also known as a *dense vector embedding*) is a vector of
    more abstract features that encodes an input’s conceptual meaning in a semantic
    space. Figure 2.10 demonstrates the terms now mapped to a dimensionally reduced
    vector that can serve as a dense embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F10_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 Dense embeddings with reduced dimensions. In this case, instead
    of one dimension per term (exists or missing), now higher-level dimensions exist
    that score shared attributes across items such as “healthy”, contains “caffeine”
    or “bread” or “dairy”, or whether the item is “food” or a “drink”.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: With a new embedding vector now available for each term sequence in the leftmost
    column of figure 2.10, we can now score the relationship between each pair of
    term sequences, using the similarity between their vectors. In linear algebra,
    we use a cosine similarity function (or another similarity measure) to score the
    relationship between two vectors. Cosine similarity is computed by performing
    a dot product between the two vectors and scaling it by the magnitudes (lengths)
    of each of the vectors. We’ll visit the math in more detail in the next chapter,
    but for now, figure 2.11 shows the results of scoring the similarity between several
    of these vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F11_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 Similarity between embeddings. The cosine between vectors shows
    the items list sorted by similarity with “green tea”, with “cheese pizza”, and
    with “donut”.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As you can see in figure 2.11, since each term sequence is encoded into a vector
    that represents its meaning in terms of higher-level features, this embedding
    can now be used to score the similarity of that term sequence with any other similar
    vector. You’ll see three vector similarity lists at the bottom of the figure:
    one for “green tea”, one for “cheese pizza”, and one for “donut”.'
  prefs: []
  type: TYPE_NORMAL
- en: By comparing the vector similarity of “green tea” with all the other term sequences,
    we find that the top related items are “water”, “cappuccino”, “latte”, “apple
    juice”, and “soda”, with the least related being “donut”. This makes intuitive
    sense, as “green tea” shares more attributes with the items higher in the list.
    For the “cheese pizza” vector, we see that the most similar other embeddings are
    for “cheese bread sticks”, “cinnamon bread sticks”, and “donut”, with “water”
    being at the bottom of the list. Finally, for the term “donut”, we find the top
    items to be “cinnamon bread sticks”, “cheese bread sticks”, and “cheese pizza”,
    with “water” once again being at the bottom of the list. These results do a great
    job of finding the most similar items to our original query.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that this vector scoring is only used in the calculation of
    similarity between items. In your search engine, there’s usually a two-phase process
    whereby you first filter to a set of documents (the *matching* phase) and then
    score those resulting documents (the *ranking* phase). Unless you’re going to
    skip the first step and score all of your documents relative to your query vectors
    (which can be time- and processing-intensive), you’ll still need some form of
    initial matching prior to the ranking phase to filter the query to a reasonable
    number of documents to score. We’ll dive more into the mechanics for successfully
    implementing embeddings and vector search in chapters 3, 9, 13, 14, and 15\.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings might represent queries, portions of documents, or even entire documents.
    It is commonplace to encode terms and term sequences into word embeddings, but
    *sentence embeddings* (encoding a vector with the meaning of a sentence), *paragraph
    embeddings* (encoding a vector with the meaning of a paragraph), and *document
    embeddings* (encoding a vector with the meaning of an entire document) are also
    common techniques.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also very common for dimensions to be more abstract than our examples here.
    For example, deep learning models like LLMs may detect seemingly unintelligible
    features from character sequences and the way that documents cluster together
    within the corpus. We wouldn’t be able to easily apply a human-readable label
    to these dimensions in the embedding vector, but as long as it improves the predictive
    power of the model and increases relevance, this is usually not a concern for
    most search teams. In fact, since vectors encode “meaning” through different abstract
    numeric features, it’s also possible to create and search on vectors representing
    different types (or *modalities*) of data, such as images, audio, video, or even
    signals and activity patterns. We’ll cover *multimodal search* (searches on different
    data modalities) in section 15.3\.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, combining multiple models for harnessing the power of distributional
    semantics and embeddings tends to create the best outcomes, and we’ll dive further
    into numerous graph and vector-based approaches to using these techniques throughout
    the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Modeling domain-specific knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 1, we discussed the search intelligence progression (refer to figure
    1.8), whereby organizations start with basic keyword search and progress through
    several additional stages of improvement before they ultimately achieve a full
    self-learning system. The second stage in that search intelligence progression
    was building taxonomies and ontologies, and the third stage (“query intent”) included
    the building and use of knowledge graphs. Unfortunately, there can sometimes be
    significant confusion among practitioners in the industry on proper definitions
    and key terminology, like “ontology”, “taxonomy”, “synonym lists”, “knowledge
    graphs”, “alternative labels”, and so on. It will benefit us to provide some definitions
    for use in this book to prevent any ambiguity. Specifically, we’ll lay out definitions
    for the key terms of “knowledge graph”, “ontology”, “taxonomy”, “synonyms”, and
    “alternative labels”. Figure 2.12 shows a high-level diagram of how they relate.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F12_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 Levels of domain-specific knowledge modeling. Knowledge graphs extend
    ontologies, which extend taxonomies. Synonyms extend alternative labels and map
    to entries in taxonomies.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We define each of these knowledge modeling techniques as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Alternative labels (or alt. labels)*—Replacement term sequences with identical
    meanings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Synonyms*—Replacement term sequences that can be used to represent the same
    or very similar things.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Taxonomy*—A classification of things into categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Ontology*—A mapping of relationships between types of things.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Knowledge graph*—An instantiation of an ontology that also contains the things
    that are related.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Creating alternative labels is the most straightforward of these techniques
    to understand. Initialisms (such as “RN” => “Registered Nurse”) and acronyms virtually
    always serve as alternative labels, as do misspellings and alternative spellings.
    Sometimes it is useful to store these mappings in separate lists, particularly
    if you’re using algorithms to determine them and you expect to allow for human
    modification of them or if you plan to rerun the algorithms later.
  prefs: []
  type: TYPE_NORMAL
- en: Synonyms are the next most common of the techniques, as virtually every search
    engine will have some implementation of a synonym list. Alternative labels are
    a subset of a synonym list and are the most obvious kind of synonym. Most people
    consider “highly related” term sequences to be synonyms, as well. For example,
    “software engineer” and “software developer” are often considered synonyms since
    they are usually used interchangeably, even though there are some nuances in meaning
    between the two. Sometimes, you’ll even see translations of words between languages
    showing up in synonyms for bilingual search use cases.
  prefs: []
  type: TYPE_NORMAL
- en: One key difference between alternative labels and more general synonyms is that
    alternative labels can be seen as *replacement terms* for the original, whereas
    synonyms are more often used as *expansion terms* to add alongside the original.
    Implementations can vary widely, but this ultimately boils down to whether you
    are confident that two term sequences carry exactly the same meaning (and you
    want to normalize them), or whether you’re just trying to include additional related
    term sequences so you don’t miss other relevant results.
  prefs: []
  type: TYPE_NORMAL
- en: Taxonomies are the next step up from synonyms. Taxonomies focus less on substitute
    or expansion words and instead focus on categorizing your content into a hierarchy.
    Taxonomical information will often be used to drive website navigation, to change
    behavior for a subset of search results (for example, to show different faceting
    or filtering options based upon a parent product category), or to apply dynamic
    filtering based upon a category to which a query maps. For example, if someone
    searches for `range` on a home improvement website, the site might automatically
    filter down to “appliances” to remove the noise of other products that contain
    phrases like “fall within the range” in their product description. Synonyms then
    map into a taxonomy, pointing to particular items within the taxonomy.
  prefs: []
  type: TYPE_NORMAL
- en: Whereas taxonomies tend to specify parent-child relationships between categories
    and then map things into those categories, ontologies provide the ability to define
    much richer relationships between things (term sequences, entities) within a domain.
    Ontologies typically define more abstract relationships, attempting to model the
    relationships between kinds of things in a domain, such as “employee reports to
    boss,” “CMO’s boss is CEO,” “CMO is employee”. This makes ontologies really useful
    for deriving new information from known facts by mapping the facts into the ontology
    and then drawing logical conclusions based on relationships in the ontology that
    can be applied to those facts.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs are the relative newcomer to the knowledge management space.
    Whereas ontologies define high-level relationships that apply to types of things,
    knowledge graphs tend to be full instantiations of ontologies that also include
    each of the specific entities that fall within those types. Using our previous
    ontology example, a knowledge graph would additionally have “Michael is CMO,”
    “Michael reports to Marcia,” and “Marcia is CEO” as relationships in the graph.
    Before knowledge graphs came to the forefront, it was common for these more detailed
    relationships to be modeled into ontologies, and many people still do this today.
    As a result, you’ll often see the terms “knowledge graph” and “ontology” used
    interchangeably, though this is becoming less common over time.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will mostly focus our discussions on alternative labels,
    synonyms, and knowledge graphs, since taxonomies and ontologies are mostly subsumed
    into knowledge graphs. We’ll explore knowledge graphs more thoroughly in chapter
    5\.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Challenges in natural language understanding for search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last few sections, we’ve discussed the rich graph of meaning embedded
    within unstructured data, like text, as well as how distributional semantics and
    embeddings can be used to derive and score semantic relationships between term
    sequences in queries and documents. We also introduced key techniques for knowledge
    modeling and defined related terminology we’ll use throughout this book. In this
    section, we’ll discuss a few key challenges associated with natural language understanding
    that we’ll seek to overcome in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 The challenge of ambiguity (polysemy)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In section 2.1.3, we introduced the idea of polysemy, or ambiguous terms. In
    that section, we were dealing with an image tagged with the name “Trey Grainger”
    but that referred to a different person than the author of this book. In textual
    data, however, we have the same problem, and it can get very messy.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a word like “driver”. It can refer broadly to a “vehicle driver”, a
    kind of golf club for hitting the ball off a tee, software that enables a hardware
    device to work, a kind of tool (screwdriver), or the impetus for pushing something
    forward (“a key driver of success”). There are many potential meanings for this
    word, and you could explore even more granular meanings. For example, within the
    “vehicle driver” category, it could mean a taxi driver, Uber driver, Lyft driver,
    professional trucker like a CDL driver (someone with a commercial drivers license),
    or even a bus driver. Within the subset of bus drivers, it could mean a school
    bus driver, a driver of a public city bus, a driver of a tour bus, and so on.
    We could continue breaking down this list into dozens of additional categories
    at a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: When building search applications, engineers will often naively create static
    synonym lists and assume terms have a singular meaning that can be applied universally.
    The reality, however, is that every term (word or phrase) takes on a unique meaning
    that is based on the specific context in which it is being used.
  prefs: []
  type: TYPE_NORMAL
- en: TIP  Every term takes on a unique meaning that is based on the specific context
    in which it is being used.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not often practical to support an infinite number of potential meanings,
    though we discuss techniques to approximate this with a semantic knowledge graph
    in chapter 5\. Nevertheless, whether you support many meanings per phrase or just
    a few, it’s important to recognize the clear need to be able to generate an accurate
    (and often nuanced) interpretation for any given phrase your users may encounter.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2 The challenge of understanding context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I like to say that every term (word or phrase) you ever encounter is a “context-dependent
    cluster of meaning with an ambiguous label”. That is to say, there is a label
    (the textual representation of the term) that is being applied to some concept
    (a cluster of meaning) that is dependent upon the context in which it is found.
    By this definition, it’s impossible to ever precisely interpret a term without
    understanding its context. As a result, creating fixed synonym lists that aren’t
    able to take context into account is likely to create suboptimal search experiences
    for your users.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models largely operate on this premise by using input prompts as
    the context in which to interpret each word part (or token) in the prompt. Attention
    is paid to every token, based on the surrounding tokens and how they relate to
    the learned representation in the model, which is also contextual. We’ll dive
    into the nuances of how Transformers work in chapter 13, and we’ll fine-tune a
    Transformer for a question-answering task in chapter 14.
  prefs: []
  type: TYPE_NORMAL
- en: Just because context is important doesn’t mean it’s always easy to apply correctly.
    It’s often necessary to perform basic keyword search as a fallback when your search
    engine doesn’t understand a query, and it’s almost always useful to have prebuilt
    domain understanding that can similarly be relied upon to help interpret queries.
    This prebuilt domain understanding then ends up overriding some of the default
    keyword-based matching behavior (such as joining individual keywords into phrases,
    injecting synonyms, and correcting misspellings).
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in chapter 1, the context for a query includes more than just
    the search keywords and the content within your documents. It also includes an
    understanding of your domain, as well as an understanding of your user. Queries
    can take on entirely different meanings based on what you know about your user
    and any domain-specific understanding you may have. This context is necessary
    both to detect and resolve the kinds of ambiguity we discussed in the last section,
    as well as to ensure your users are receiving the most intelligent search experience
    possible. Throughout this book, our focus will be on techniques to automatically
    learn contextual interpretations of each query, based on the unique context in
    which it’s being used.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.3 The challenge of personalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When considering user-specific context as a tool for enhancing query understanding,
    it’s not always obvious how to apply user-specific personalization on top of the
    preexisting content and domain-specific scoring. For example, say you learn that
    a particular user really likes Apple as a brand because they keep searching for
    iPhones. Does this mean that Apple should also be boosted when they are searching
    for watches, computers, keyboards, headphones, and music players? It could be
    that the user only likes Apple-branded phones and that by boosting the brand in
    other categories you may frustrate the user. For example, even if the user did
    search for an iPhone previously, how do you know they weren’t just trying to compare
    the iPhone with other phones they were considering?
  prefs: []
  type: TYPE_NORMAL
- en: Out of all of the dimensions of user intent (figure 1.5), personalization is
    the easiest one to trip up on, and, as a result, it is the one that is least commonly
    seen in modern AI-powered search applications (outside of recommendation engines,
    of course). We’ll work through these problems in detail in chapter 9 to highlight
    how we can strike the right balance when rolling out a personalized search experience.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.4 Challenges interpreting queries vs. documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One common problem we see when engineers and data scientists first get started
    with search is a propensity to apply standard natural language processing techniques,
    like language detection, part-of-speech detection, phrase detection, and sentiment
    analysis to queries. Usually, those techniques were trained to operate on longer
    blocks of text—often at the document, paragraph, or at least sentence level.
  prefs: []
  type: TYPE_NORMAL
- en: Documents tend to be longer and to supply significantly more context to the
    surrounding text, whereas queries tend to be short (a few keywords only) in most
    use cases. Even when they are longer, queries tend to combine multiple ideas as
    opposed to supplying more linguistic context. As a result, when trying to interpret
    queries, you need to use external context as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a natural language processing library that relies on sentence
    structure to interpret the query, for example, you can try looking up the phrases
    from your query in your corpus of documents to find their most common domain-specific
    interpretations. Likewise, you can use the co-occurrence of terms within your
    query across previous user search sessions by mining user behavioral signals.
    This enables you to learn real intent from similar users, which would be very
    challenging to reliably derive from a standard natural language processing library.
  prefs: []
  type: TYPE_NORMAL
- en: In short, queries need special handling and interpretation due to their tendency
    to be short and to often imply more than they state explicitly, so fully using
    search-centric data science approaches to queries is going to generate much better
    results than traditional natural language processing approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.5 Challenges interpreting query intent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While the process of parsing a query to understand the terms and phrases it
    contains is important, there is often a higher-level intent behind the query—a
    query intent, if you will. For example, let’s consider the inherent differences
    between the following queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The intent of the first query for `who` `is` `the` `CEO?` is clearly to find
    a factual answer and not a list of documents. The second query for `support` is
    trying to navigate to the support section of a website, or to otherwise contact
    the support team. The third query for `iphone` `screen` `blacked` `out` is also
    looking for support, but it is for a specific problem, and the person likely wants
    to find troubleshooting pages that may help with that specific problem before
    reaching out to the actual support team.
  prefs: []
  type: TYPE_NORMAL
- en: The next two queries for `iphone` and for `verizon` `silver` `iphone` `8` `plus`
    `64GB` are quite interesting. While they are both for iPhones, the first search
    is a general search, likely indicating a browsing or product research intent,
    whereas the second query is a much more specific variant of the first search,
    indicating the user knows exactly what they are looking for and may be closer
    to making a purchasing decision. The general query for `iphone` may be better
    to return a landing page providing an overview of iphones and the available options,
    whereas the more specific query may be better for going straight to a product
    page with a purchase button. As a general rule of thumb, the more general a query,
    the more likely the user is just browsing. More specific queries—especially when
    they refer to specific items by name—often indicate a purchase intent or desire
    to find a particular known item.
  prefs: []
  type: TYPE_NORMAL
- en: The query for `sale` indicates that the user is looking for items that are available
    for purchase at a discounted rate, which will invoke some specially implemented
    filter or redirect to a particular landing page for an ongoing sale event. The
    query for `refrigerators` indicates that the user wants to browse a particular
    category of product documents. Finally, the query for `pay` `my` `bill` indicates
    that the user wants to take an action—the best response to this query isn’t a
    set of search results or even an answer, but instead a redirect to a bill review
    and payment section of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these queries contains an intent beyond just a set of keywords to be
    matched. Whether the intent is to redirect to a particular page, apply particular
    filters, browse or purchase items, or even take domain-specific actions, the point
    is that there is domain-specific nuance to how users may express their goals to
    your search engine. Oftentimes, it can be difficult to derive these domain-specific
    user intents automatically. It is fairly common for businesses to implement specific
    business rules to handle these as one-off requests. Query intent classifiers can
    certainly be built to handle subsets of this problem, but successfully interpreting
    every possible query intent still remains challenging when building out natural
    language query interpretation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '2.6 Content + signals: The fuel powering AI-powered search'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first chapter, we introduced the idea of reflected intelligence—using
    feedback loops to continually learn from both content and user interactions. This
    chapter has focused entirely on understanding the meaning and intelligence embedded
    within your content, but it’s important to recognize that many of the techniques
    we’ll apply to the “unstructured data” in your documents can also be just as readily
    applied to your user behavioral signals. For example, we discussed in section
    2.3 how the meanings of phrases can be derived from finding the other phrases
    they most commonly appear with across your corpus. We noted that “machine learning”
    appears more commonly with “data scientist” and “software engineer” than it does
    with “accountants,” “food service workers,” or “pharmacists”.
  prefs: []
  type: TYPE_NORMAL
- en: If you abstract the distributional hypothesis beyond documents and also apply
    it to user behavior, you might expect that similar users querying your search
    engine are likely to exhibit similar query behavior. Specifically, people who
    are data scientists or who are searching for data scientists are far more likely
    to also search for or interact with documents about machine learning, and the
    likelihood of a food service worker or accountant searching for machine learning
    content is much lower than the likelihood of a software engineer doing so. We
    can thus apply these same techniques to learn related terms and term sequences
    from query logs, where instead of thinking of terms and term sequences mapping
    to fields in documents, we think of terms in queries and clicks on search results
    mapping to user sessions, which then map to users. We’ll follow this approach
    in chapter 6 to learn related terms, synonyms, and misspellings from user query
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: Some search applications are content-rich but have very few user signals. Other
    search applications have an enormous number of signals but either very little
    content or content that poses challenges from an automated learning perspective.
    In an ideal scenario, you’ll have great content and an enormous quantity of user
    signals to learn from, which allows you to combine the best of both worlds into
    an even smarter AI-powered search application. Regardless of which scenario you’re
    in, keep in mind that your content and your user signals can both serve as fuel
    to power learning algorithms, and you should do your best to maximize the collection
    and quality of each.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final note on natural language understanding: with the rise of LLMs, which
    are deep neural networks trained on a large percentage of human knowledge (much
    of the internet, plus selected sources), we now have the ability to interpret
    the meaning and intent of general-knowledge questions at an unprecedented level
    of quality. LLMs do not handle domain-specific understanding very well out of
    the box, at least for information that is not part of their training sets, but
    with the ability to fine-tune LLMs on domain-specific data, these models can often
    be quickly adapted to more closed-domain data. LLMs represent a large leap forward
    in our ability to learn the nuances of natural language, interpret arbitrary documents
    and queries based on those nuances, and drive more relevant search.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, while generally the most impressive technique for wide-scale natural language
    understanding, are far from the only powerful tools in our AI-powered search toolbox.
    We’ll dive into using LLMs for search in chapters 9, 13, 14, and 15\. In the meantime,
    we have many other critical algorithms and techniques to explore for natural language
    and domain understanding, interpreting user behavior, and learning optimal relevance
    ranking models.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered all the background needed to begin extracting meaning
    from your natural language content, it’s time to roll up our sleeves and get hands-on.
    In the next chapter, we’ll dive into lots of examples as we begin to explore content-based
    relevance in an AI-powered search application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unstructured data is a misnomer—it is really more like hyper-structured data,
    as it represents a giant graph of domain-specific knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search engines can use distributional semantics—interpreting the semantic relationships
    between terms and phrases based upon the distributional hypothesis—to harness
    rich semantic meaning at the level of character sequences, terms, term sequences
    (typically phrases), fields, documents, document sets, and an entire corpus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributional semantics approaches enable us to learn the nuanced meaning of
    our queries and content from their larger surrounding context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings are a powerful technique for search results ranking based on the
    semantic meaning of text (and other data modalities) instead of just the existence
    and occurrence counts of specific keywords.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-specific knowledge is commonly modeled through a combination of alternative
    labels, synonym lists, taxonomies, ontologies, and knowledge graphs. Knowledge
    graphs typically model the output from each of the other approaches into a unified
    knowledge representation of a particular domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polysemy (ambiguous terms), context, personalization, and query-specific natural
    language processing approaches represent some of the more interesting challenges
    in natural language search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content and user signals are both important fuel for our AI-powered search applications
    to use when solving natural language challenges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#ftnote-84) John Rupert Firth, “A synopsis of linguistic theory, 1930–1955,”
    in J.R. Firth et al., *Studies in Linguistic Analysis*, Special Volume of the
    Philological Society (Oxford University Press, 1957).'
  prefs: []
  type: TYPE_NORMAL
