- en: 2 Working with natural language
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 与自然语言一起工作
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The hidden structures in unstructured data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非结构化数据中的隐藏结构
- en: A search-centric philosophy of language
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以搜索为中心的语言哲学
- en: Exploring distributional semantics and vector-based embeddings
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索分布语义和基于向量的嵌入
- en: Modeling domain-specific knowledge
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模特定领域的知识
- en: Challenges with natural language and querys
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言和查询的挑战
- en: Applying natural language understanding techniques to both content and signals
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将自然语言理解技术应用于内容和信号
- en: In the first chapter, we provided a high-level overview of what it means to
    build an AI-powered search system. Throughout the rest of the book, we’ll explore
    and demonstrate the numerous ways your search application can continuously learn
    from your content and your users’ behavioral signals to better understand your
    content, your users, and your domain, and to ultimately deliver users the answers
    they need. We will get much more hands-on in chapter 3, firing up a search server
    of your choice and a data processing layer (Apache Spark) and starting with the
    first of our Jupyter notebooks, which we’ll use throughout the book to walk through
    many step-by-step examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们提供了一个关于构建人工智能搜索系统的高层次概述。在本书的其余部分，我们将探索并展示您的搜索应用如何从您的内容和用户的行为信号中持续学习，以更好地理解您的内容、用户和领域，并最终为用户提供他们需要的答案。在第3章中，我们将更加深入地实践，启动您选择的搜索服务器和数据处理层（Apache
    Spark），并从我们的第一个Jupyter笔记本开始，我们将使用它来逐步演示许多示例。
- en: Before we dive into those hands-on examples and specific implementations, however,
    it is important in this chapter that we first establish a shared mental model
    for the higher-level problems we’re trying to solve. Specifically, when it comes
    to intelligent search, we have to deal with many complexities and nuances in natural
    language—both in the content we’re searching and in our users’ search queries.
    We have to deal with keywords, entities, concepts, misspellings, synonyms, acronyms,
    initialisms, ambiguous terms, explicit and implied relationships between concepts,
    hierarchical relationships usually found in taxonomies, higher-level relationships
    usually found in ontologies, and specific instances of entity relationships usually
    found in comprehensive knowledge graphs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们深入探讨那些实际案例和具体实现之前，在本章中我们首先建立一个共享的思维方式来解决问题是非常重要的。具体来说，当涉及到智能搜索时，我们必须处理自然语言中的许多复杂性和细微差别——无论是我们在搜索的内容，还是用户的搜索查询。我们必须处理关键词、实体、概念、拼写错误、同义词、首字母缩略词、缩写词、模糊术语、概念之间的显性和隐含关系、通常在分类学中找到的层次关系、通常在本体中找到的高级关系，以及通常在综合知识图中找到的实体关系的具体实例。
- en: While it might be tempting to dive immediately into some specific problems,
    like how to automatically learn misspellings from content or how to discover synonyms
    from mining user search sessions, it will be more prudent to first establish a
    conceptual foundation that explains what *kinds* of problems we have to deal with
    in search and natural language understanding (NLU). Establishing that philosophical
    foundation will enable us to build better end-to-end solutions in our AI-powered
    search system, where all the parts work together in a cohesive and integrated
    way. This chapter will thus provide the philosophical underpinnings for how we
    tackle the problems of natural language understanding throughout this book and
    how we apply those solutions to make our search applications more intelligent.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能很诱人立即深入研究一些具体问题，比如如何从内容中自动学习拼写错误，或者如何从用户搜索会话中挖掘同义词，但首先建立一个概念基础来解释我们在搜索和自然语言理解（NLU）中必须处理的问题类型将更为谨慎。建立这个哲学基础将使我们能够在我们的AI搜索系统中构建更好的端到端解决方案，其中所有部分以协调和整合的方式协同工作。因此，本章将为我们在本书中处理自然语言理解问题提供哲学基础，以及我们如何将这些解决方案应用到使我们的搜索应用更加智能。
- en: We’ll begin by discussing some common misconceptions about the nature of free
    text and other unstructured data sources.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论一些关于自由文本和其他非结构化数据源性质的常见误解。
- en: 2.1 The myth of unstructured data
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 非结构化数据的神话
- en: The term “unstructured data” has been used for years to describe textual data,
    because it does not appear to be formatted in a way that can be readily interpreted
    and queried. The widely held idea that text, or any other data that doesn’t fit
    a predefined schema (“structure”), is actually “unstructured”, however, is a myth
    that we’ll spend time reconsidering throughout this section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “非结构化数据”这个术语多年来一直被用来描述文本数据，因为它看起来没有以可以轻松解释和查询的方式格式化。然而，广泛持有的观点认为文本，或任何不符合预定义模式（“结构”）的其他数据实际上是“非结构化的”，但这是一种在本节中我们将花时间重新考虑的神话。
- en: If you look up *unstructured data* in Wikipedia, it is defined as “information
    that either does not have a pre-defined data model or is not organized in a pre-defined
    manner”. The entry goes on to say that “unstructured information is typically
    text-heavy, but may contain data such as dates, numbers, and facts as well”.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查阅维基百科中的“非结构化数据”，它被定义为“既没有预定义的数据模型，也没有以预定义方式组织的信息”。条目接着说，“非结构化信息通常是文本密集型的，但可能包含日期、数字和事实等数据”。
- en: The phrase “unstructured data” is a poor term to describe textual content, however.
    In reality, the terms and phrases present in text encode an enormous amount of
    meaning, and the linguistic rules applied to the text to give it meaning serve
    as their own structure. Calling text unstructured is a bit like calling a song
    playing on the radio “arbitrary audio waves”. Even though every song has unique
    characteristics, most exhibit common attributes (tempo, melodies, harmonies, lyrics,
    and so on). Though these attributes may differ or be absent from song to song,
    they nevertheless fit common expectations that enable meaning to be conveyed by
    and extracted from each song. Textual information typically follows similar rules—sentence
    structure, grammar, punctuation, interaction between parts of speech, and so on.
    Figure 2.1 shows an example of text we’ll explore a bit more in the upcoming sections
    as we investigate this structure.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，“非结构化数据”这个短语并不是描述文本内容的良好术语。实际上，文本中出现的术语和短语编码了大量的意义，应用于文本以赋予其意义的语言规则本身也构成了其结构。称文本为非结构化有点像称广播电台播放的歌曲为“任意的音频波”。尽管每首歌曲都有独特的特征，但大多数都表现出共同的属性（节奏、旋律、和声、歌词等）。尽管这些属性可能不同或在某些歌曲中缺失，但它们仍然符合共同的期望，使得意义可以通过和从每首歌曲中传达和提取。文本信息通常遵循类似的规则——句子结构、语法、标点符号、词性之间的相互作用等。图2.1展示了我们将要更深入探讨的文本示例，作为我们研究这一结构的下一节内容。
- en: '![figure](../Images/CH02_F01_Grainger.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F01_Grainger.png)'
- en: Figure 2.1 Unstructured data. This text represents typical unstructured data
    you may find in a search engine.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 非结构化数据。这段文本代表了在搜索引擎中可能找到的典型非结构化数据。
- en: While text is the most commonly recognized type of unstructured data, there
    are several other kinds of unstructured data with similar characteristics, as
    we’ll see in the next section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文本是最常见的非结构化数据类型，但还有几种其他类型的非结构化数据具有相似的特征，我们将在下一节中看到。
- en: 2.1.1 Types of unstructured data
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 非结构化数据的类型
- en: Free text content is considered the primary type of unstructured data, but search
    engines also commonly index many other kinds of data that similarly don’t fit
    neatly into a structured database. Common examples include images, audio, video,
    and event logs. Figure 2.2 expands on our text example from figure 2.1 and includes
    several other types of unstructured data, such as audio, images, and video.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自由文本内容被认为是非结构化数据的主要类型，但搜索引擎也通常索引许多其他类似地不适合整齐地放入结构化数据库的数据。常见的例子包括图像、音频、视频和事件日志。图2.2在图2.1的文本示例基础上进行了扩展，并包括了几种其他类型的非结构化数据，如音频、图像和视频。
- en: '![figure](../Images/CH02_F02_Grainger.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F02_Grainger.png)'
- en: Figure 2.2 Multiple types of unstructured data. In addition to the text from
    the last figure, we now see images, audio, and video, which are other forms of
    unstructured data.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 多种类型的非结构化数据。除了上一图中的文本外，我们现在还看到了图像、音频和视频，这些都是非结构化数据的其他形式。
- en: Audio is the most similar to text content, since it is often just another way
    to encode words and sentences. Of course, audio can include much more than just
    spoken words—it can include music and non-language sounds, and it can more effectively
    encode nuances such as emotion, tone of voice, and simultaneously overlapping
    communication.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 音频与文本内容最为相似，因为它通常只是编码单词和句子的另一种方式。当然，音频可以包含比仅说话的词语更多——它可以包含音乐和非语言声音，并且它可以更有效地编码细微差别，如情感、语调和同时重叠的交流。
- en: Images are another kind of unstructured data. Just as words form sentences and
    paragraphs to express ideas, images form grids of colors that, taken together,
    form pictures.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是另一种非结构化数据。正如文字形成句子和段落来表达思想一样，图像形成由颜色组成的网格，这些颜色组合在一起形成图片。
- en: Video, then, serves as yet another kind of unstructured data, as it is a combination
    of multiple images over time, as well as optional audio that coincides with the
    progression of images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，视频作为另一种非结构化数据，它是由多个图像随时间组合而成的，以及与图像进展同步的可选音频。
- en: When unstructured data is found mixed with structured data, we typically refer
    to this as *semi-structured* data. Log data is a great example of such semi-structured
    data. Often, log messages contain a structured event date, structured event types
    (such as warning versus error or search versus click), and then some kind of unstructured
    message or description in free text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当非结构化数据与结构化数据混合在一起时，我们通常将其称为*半结构化*数据。日志数据是这种半结构化数据的一个很好的例子。通常，日志消息包含结构化的事件日期、结构化的事件类型（如警告与错误或搜索与点击），然后是一些非结构化的消息或描述文本。
- en: Technically speaking, virtually any kind of file could be considered unstructured
    data, but we’ll primarily deal with the aforementioned types. Search engines are
    often tasked with handling each of these kinds of unstructured data, so we’ll
    discuss strategies for handling them throughout the book.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，几乎任何类型的文件都可以被认为是非结构化数据，但我们将主要处理上述类型。搜索引擎通常负责处理每种类型的非结构化数据，因此我们将在整本书中讨论处理它们的策略。
- en: 2.1.2 Data types in traditional structured databases
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 传统结构化数据库中的数据类型
- en: To better deal with our unstructured data, it may be useful to first contrast
    it with structured data in a SQL database. This will allow us to later draw parallels
    between how we can query unstructured data representations versus structured ones.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地处理我们的非结构化数据，首先将其与SQL数据库中的结构化数据进行对比可能是有用的。这将使我们能够后来在查询非结构化数据表示与结构化数据表示之间建立平行关系。
- en: A record (row) in a SQL database is segmented into columns, which can each be
    of a particular data type. Some of these data types represent discrete values—values
    that come from an enumerated list, such as IDs, names, or textual attributes.
    Other columns may hold continuous values, such as date/time ranges, numbers, and
    other column types that represent ranges without a finite number of possible values.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SQL数据库中的一个记录（行）被分割成列，每一列都可以是特定数据类型。其中一些数据类型表示离散值——来自枚举列表的值，如ID、名称或文本属性。其他列可能持有连续值，如日期/时间范围、数字以及其他表示没有有限可能值的范围的列类型。
- en: Generally speaking, when one wants to relate different rows together or to relate
    them to rows in other database tables, “joins” will be performed on the discrete
    values. *Joins* use a shared value (often an ID field) to link two or more records
    together to form a composite record.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，当一个人想要将不同的行关联起来，或者将它们与其他数据库表中的行关联起来时，会在离散值上执行“连接”操作。*连接*使用共享值（通常是ID字段）将两个或多个记录链接在一起，形成一个组合记录。
- en: For example, if someone had two tables of data, one representing employees and
    another representing companies, then there would likely be an `id` column on the
    `companies` table, and a corresponding `company_id` column on the `employees`
    table. The `company_id` field on the employees table is known as a *foreign key*,
    which is a value in one table that refers to an entity in another table, linking
    the records together based upon a shared identifier. Figure 2.3 demonstrates this,
    showing examples of discrete values, continuous values, and a join across tables
    using a foreign key.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果有人有两个数据表，一个代表员工，另一个代表公司，那么在“公司”表中可能有一个`id`列，在“员工”表中有一个相应的`company_id`列。员工表中的`company_id`字段被称为*外键*，它是一个表中的值，它引用另一个表中的实体，基于共享标识符将记录链接在一起。图2.3展示了这一点，展示了离散值、连续值以及使用外键在表之间进行连接的示例。
- en: This notion of joining different records together based upon known relationships
    (keys and foreign keys) is a powerful way to work with relational data across
    explicitly
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基于已知关系（键和外键）将不同记录组合在一起的概念，是处理关系数据的一种强大方式，这种处理方式是明确跨视频数据进行的。
- en: '![figure](../Images/CH02_F03_Grainger.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F03_Grainger.png)'
- en: Figure 2.3 Structured data in a typical database. Discrete values represent
    identifiers and enumerated values, continuous values represent data that falls
    within ranges, and foreign keys exist when the same value exists across two tables
    and can thus be used as a shared attribute that creates a relationship between
    corresponding rows from each table.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 典型数据库中的结构化数据。离散值代表标识符和枚举值，连续值代表落在范围内的数据，当相同值存在于两个表之间时，外键存在，因此可以用作创建每个表对应行之间关系的共享属性。
- en: modeled tables, but as we’ll see in the next section, very similar techniques
    can also be applied to free-form unstructured data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 模型表，但正如我们将在下一节中看到的，非常类似的技术也可以应用于自由形式的非结构化数据。
- en: 2.1.3 Joins, fuzzy joins, and entity resolution in unstructured data
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 非结构化数据中的连接、模糊连接和实体解析
- en: Whereas structured data in a database is already in an easily queryable form,
    the reality is that unstructured data suffers less from a lack of structure, and
    more from having a large amount of information packed into a very flexible structure.
    In this section, we’ll walk through a concrete example that uncovers this hidden
    structure in unstructured data and demonstrates the ways it can similarly be used
    to find and join relationships between documents.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 而在数据库中的结构化数据已经是易于查询的形式，现实是未结构化数据受结构缺乏的影响较小，而更多地是由于大量信息被包装在一个非常灵活的结构中。在本节中，我们将通过一个具体的例子来揭示未结构化数据中的这种隐藏结构，并展示它如何以类似的方式用于在文档之间找到和连接关系。
- en: Foreign keys in unstructured data
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非结构化数据中的外键
- en: We’ve discussed how foreign keys can be used to join two rows together in a
    database, based on a shared identifier between the two records. In this section,
    we’ll show how the same objective can be achieved with text data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何使用外键根据两个记录之间的共享标识符在数据库中将两行连接起来。在本节中，我们将展示如何使用文本数据实现相同的目标。
- en: For example, we can easily map the idea of foreign keys used in a SQL table
    to the unstructured information we explored in figure 2.2\. Notice in figure 2.4
    that two different sections of text both contain the word “Haystack”, which refers
    to a technology conference focused on search relevance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以轻松地将SQL表中使用的外键的概念映射到我们在图2.2中探索的非结构化信息。注意图2.4中，两个不同的文本部分都包含单词“Haystack”，它指的是一个专注于搜索相关性的技术会议。
- en: '![figure](../Images/CH02_F04_Grainger.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH02_F04_Grainger.png)'
- en: Figure 2.4 Foreign keys in unstructured data. In this example, the same term
    is being used to join across two related text documents.
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 非结构化数据中的外键。在这个例子中，相同的术语被用来连接两个相关的文本文档。
- en: The first instance indicates a conference being spoken at, while the second
    block of text contains general information about the event. For the purposes of
    our example, let’s assume that every piece of information (block of text, image,
    video, and audio clip) is represented as a separate document in our search engine.
    There is functionally very little difference between having two rows in a database
    table that each contain a column with the value “Haystack”, and having separate
    documents in our search engine that each contain the value “Haystack”. In both
    cases, we can think of these documents as related by a foreign key.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实例表示一个正在进行的会议，而第二个文本块包含关于该事件的一般信息。为了我们的示例，让我们假设每个信息块（文本块、图像、视频和音频剪辑）在我们的搜索引擎中都被表示为一个单独的文档。在数据库表中，有两个行各包含一个值为“Haystack”的列与我们的搜索引擎中有两个包含值为“Haystack”的单独文档之间，在功能上几乎没有区别。在两种情况下，我们可以将这些文档视为通过外键相关联。
- en: Fuzzy foreign keys in unstructured data
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非结构化数据中的模糊外键
- en: With unstructured data, however, we have much more power than with traditional
    structured data modeling. In figure 2.5, for example, notice that now two documents
    are linked and that they both refer to the lead author of this book—one using
    my full name of “Trey Grainger”, and one simply using my first name of “Trey”.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与传统的结构化数据建模相比，我们在非结构化数据方面拥有更多的能力。例如，在图2.5中，注意现在有两个文档被链接，并且它们都提到了本书的首席作者——一个使用我的全名“Trey
    Grainger”，另一个简单地使用我的名字“Trey”。
- en: '![figure](../Images/CH02_F05_Grainger.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH02_F05_Grainger.png)'
- en: Figure 2.5 Fuzzy foreign keys. In this example, the same entity is being referenced
    using different terms, and a join is occurring based upon multiple phrases resolving
    to the same entity.
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 模糊外键。在这个例子中，使用不同的术语引用了相同的实体，并且基于多个短语解析为同一实体而进行连接。
- en: This is an example of *entity resolution*, where there are two different representations
    of the entity, but they can still be resolved to the same meaning, and therefore
    can still be used to join information between two documents. You can think of
    this as a “fuzzy foreign key”, since it’s still a foreign key, but not in a strict
    token-matching sense, as it requires additional natural language processing and
    entity resolution techniques to resolve.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个实体识别的例子，其中实体有两种不同的表示形式，但它们仍然可以被解析为相同的意义，因此仍然可以用来在两个文档之间连接信息。你可以将其视为“模糊的外键”，因为它仍然是一个外键，但它不是在严格的标记匹配意义上，因为它需要额外的自然语言处理和实体识别技术来解析。
- en: Once we’ve opened this door to advanced text processing for entity resolution,
    we can learn even more from our unstructured information. For example, not only
    do the names “Trey” and “Trey Grainger” in these documents refer to the same entity,
    but so do the words “he” and “his”.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们打开了这扇门，进入了高级文本处理以进行实体识别，我们就能从我们的非结构化信息中学到更多。例如，在这些文档中，“Trey”和“Trey Grainger”这两个名字不仅指代同一个实体，"he"和"his"这两个词也是如此。
- en: You’ll also notice that both an image of me (in the bottom-left corner, in case
    you have no idea what I look like) and a video containing a reference to my name
    are identified as related and joined back to the textual references. We’re relying
    on the hidden structure present in all of this unstructured data to understand
    the meaning, relate the documents together, and learn even more about each of
    the referenced entities in those documents.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到，一张我（位于左下角，以防你不知道我长什么样）的照片以及包含我名字的视频都被识别为相关内容，并重新与文本引用连接起来。我们正依赖于所有这些非结构化数据中存在的隐藏结构来理解意义，将文档联系起来，并更多地了解这些文档中引用的每个实体。
- en: Dealing with ambiguous terms
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理模糊术语
- en: So far, so good, but in real-world content it’s not always appropriate to assume
    that the same term in multiple places carries the same meaning, or even that our
    entity resolution always resolves entities correctly. This problem of the same
    spelling of words and phrases having multiple potential meanings is called *polysemy*,
    and dealing with these ambiguous terms can be a huge problem in search applications.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利，但在现实世界的内容中，并不总是合适的假设多个地方出现的相同术语具有相同的意义，或者我们的实体识别总是正确地解析实体。这种相同拼写但具有多种潜在意义的问题被称为多义性，在搜索应用中处理这些模糊术语可能是一个大问题。
- en: You may have noticed an odd image in the upper-right corner of the previous
    figures that seemed a bit out of place. This image is of a fairly terrifying man
    holding a machete. Apparently, if you go to Google and search for `Trey` `Grainger`,
    this image comes back. If you dig in further, you’ll see in figure 2.6 that there’s
    an x.com (formerly Twitter) user also named “Trey Grainger”, and this image is
    his profile picture.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到在前面图例右上角有一个奇怪的照片，看起来有点格格不入。这张照片是一个相当可怕的人拿着大刀。显然，如果你去Google搜索“Trey” “Grainger”，这张照片就会返回。如果你进一步挖掘，你会在图2.6中看到有一个名为“Trey
    Grainger”的x.com（前身为Twitter）用户，这张照片是他的个人头像。
- en: '![figure](../Images/CH02_F06_Grainger.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F06_Grainger.png)'
- en: Figure 2.6 Polysemy. This image shows a Google search for `Trey` `Grainger`.
    Pictures of multiple different people are returned because those people’s names
    share the same spelling, making the phrase “Trey Grainger” ambiguous.
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 多义性。这张图片显示了搜索“Trey” “Grainger”的Google搜索结果。返回了多张不同人的照片，因为这些人的名字拼写相同，使得短语“Trey
    Grainger”变得模糊。
- en: The picture is apparently of Robert Shaw (who plays Quint in the 1975 movie
    *Jaws*), but it’s definitely not the kind of thing you want people to first come
    across when they search for you online!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这张照片显然是罗伯特·肖（在1975年的电影《大白鲨》中扮演昆特）的，但绝对不是你希望人们在搜索你时首先看到的那种类型！
- en: There are two key lessons to take away here. First, perhaps never Google yourself
    (you might be terrified at what you find!). Second, and on a more serious note,
    polysemy is a major problem in search and natural language understanding. It’s
    not safe to assume a term has a single meaning or even a consistent meaning across
    different contexts, so our AI-powered search engine needs to use context to differentiate
    these various meanings.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个关键教训可以吸取。首先，也许永远不要搜索自己的名字（你可能会对找到的东西感到震惊！）。其次，更严肃地说，多义性是搜索和自然语言理解中的一个主要问题。我们不能假设一个术语在不同的上下文中只有一个意义，甚至是一个一致的意义，因此我们的基于人工智能的搜索引擎需要使用上下文来区分这些不同的意义。
- en: Unstructured data as a giant graph of relationships
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非结构化数据作为一个关系的大图
- en: In the previous sections, we saw that unstructured data not only contains rich
    information (entities and their relationships) but also that it’s possible to
    relate different documents by joining them on shared entities, similar to how
    foreign keys work in traditional databases. Typical unstructured data contains
    so many of these relationships, however, that instead of thinking in terms of
    rows and columns, it may be more useful to think of the collection of data as
    a giant graph of relationships, as we’ll explore in this section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们了解到非结构化数据不仅包含丰富的信息（实体及其关系），而且通过在共享实体上连接不同文档，可以关联不同的文档，这与传统数据库中外键的工作方式类似。然而，典型的非结构化数据包含如此多的这些关系，因此，与其从行和列的角度思考，不如将数据集合视为一个巨大的关系图，正如我们将在本节中探讨的那样。
- en: At this point, it should be clear that there is much more structure hidden in
    unstructured data than most people appreciate. Unstructured information is really
    more like “hyper-structured” information—it is a graph that contains much more
    structure than typical “structured data”.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，应该已经很清楚，非结构化数据中隐藏的结构比大多数人所欣赏的要更多。非结构化信息实际上更像“超结构化”信息——它是一个包含比典型“结构化数据”更多结构的图。
- en: Figure 2.7 demonstrates this giant graph of relationships that is present in
    even the small handful of documents in our example. You can see names, dates,
    events, locations, people, companies, and other entities, and you can infer relationships
    between them, using joins between the entities across documents. You’ll also notice
    that the images have been correctly disambiguated so that the machete guy is now
    disconnected from the graph. If all of this can be learned from just a few documents,
    imagine what can be learned from the thousands, millions, or billions of documents
    you have within your search engine.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7展示了即使在我们的例子中只有少量文档的情况下也存在的关系巨大图。你可以看到名称、日期、事件、地点、人物、公司和其他实体，你可以通过在文档间实体之间的连接来推断它们之间的关系。你还会注意到，图像已经被正确地消歧，因此砍刀男子现在与图无关。如果所有这些都可以从仅仅几份文档中学习到，那么想象一下从你搜索引擎中的成千上万、数百万或数十亿份文档中可以学习到什么。
- en: '![figure](../Images/CH02_F07_Grainger.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F07_Grainger.png)'
- en: Figure 2.7 Giant graph of relationships. A rich graph of relationships emerges
    from even a small collection of related documents.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 关系巨大图。即使是从一小部分相关文档中也能出现丰富的关系图。
- en: Part of the value of an AI-powered search platform is being able to learn insights
    like this from your data. The question is, how do you use this enormous graph
    of semantic knowledge to drive this intelligence?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能搜索平台的一部分价值在于能够从你的数据中学习这样的见解。问题是，你如何使用这个巨大的语义知识图来驱动这种智能？
- en: One of the most powerful ways to use a graph from text data is through a large
    language model (LLM), such as a Transformer model, which was introduced in section
    1.3.4\. These models use deep learning to learn billions of parameters across
    massive datasets, such as crawls of most of the internet, to build a detailed
    understanding of language. This understanding includes both the meanings of words
    in different contexts and the linguistic and conceptual connections between words.
    These models internally represent the giant graph of relationships found in all
    the data they are trained on, which is usually more general than your dataset,
    so the models must be fine-tuned to learn any domain-specific relationships from
    your data. This need for fine-tuning can create some challenges due to LLMs being
    somewhat of a black box, as they otherwise don’t optimally represent your dataset,
    and the information they return could be erroneous.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本数据中的图的最强大方式之一是通过大型语言模型（LLM），例如在1.3.4节中介绍的Transformer模型。这些模型通过深度学习在庞大的数据集上学习数十亿个参数，例如对大部分互联网的爬取，以构建对语言的详细理解。这种理解包括不同语境中单词的含义以及单词之间的语言和概念联系。这些模型内部表示所有训练数据中找到的巨大关系图，这通常比你的数据集更通用，因此模型必须经过微调来从你的数据中学习任何特定领域的关联。这种微调的需求可能会由于LLM某种程度上是一个黑盒而带来一些挑战，因为它们否则不会最优地表示你的数据集，它们返回的信息可能是错误的。
- en: Fortunately, the inherent structure of the inverted index in your search engine
    makes it very easy to traverse the large graph of relationships in your data without
    any additional explicit data modeling required. An *inverted index* is the primary
    data structure used for lexical search, mapping each keyword or term in the fields
    of your documents to lists (called *postings lists*) of all the documents containing
    those keywords. The inverted index enables very fast lookups of the set of documents
    containing any given term (or term sequences, when considering positional matching
    and Boolean logic implemented through set operations). With those lookups, it
    is possible to traverse between different term sequences using their shared documents
    to calculate a weighted edge in a graph. We will dive deep into how to harness
    this semantic knowledge graph hidden in your data in chapter 5\.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，你的搜索引擎中倒排索引的固有结构使得在没有额外显式数据建模要求的情况下，轻松遍历数据中的大型关系图。*倒排索引*是用于词汇搜索的主要数据结构，将你的文档字段中的每个关键词或术语映射到包含这些关键词的所有文档的列表（称为*倒排列表*）。倒排索引使得对包含任何给定术语（或术语序列，当考虑位置匹配和通过集合操作实现的布尔逻辑时）的文档集的查找非常快速。通过这些查找，可以遍历不同的术语序列，使用它们的共享文档来计算图中的一个加权边。我们将在第5章深入探讨如何利用隐藏在数据中的这个语义知识图。
- en: 2.2 The structure of natural language
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 自然语言的结构
- en: In the last section, we discussed how text and unstructured data typically contain
    a giant graph of relationships, which can be derived by looking at shared terms
    between different records. If you’ve been building search engines for a while,
    you’re used to thinking about content in terms of “documents”, “fields”, and “terms”
    within those fields. When interpreting the semantic meaning of your content, however,
    there are a few more levels to consider.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了文本和非结构化数据通常包含一个巨大的关系图，这些关系可以通过查看不同记录之间的共享术语来推导。如果你已经构建搜索引擎一段时间了，你习惯于从“文档”、“字段”和字段内的“术语”的角度来考虑内容。然而，在解释你内容的意义时，还有更多层次需要考虑。
- en: Figure 2.8 walks through these additional levels of semantic meaning. At the
    most basic level, you have *characters*, which are single letters, numbers, or
    symbols, such as the letter “e” in the figure. One or more characters are then
    combined to form *character sequences* such as “e”, “en”, “eng”, … “engineer”,
    and “engineers”. Some character sequences form terms, which are completed words
    or tokens that carry an identifiable meaning, such as “engineer”, “engineers”,
    “engineering”, or “software”. One or more terms can then be combined into *term
    sequences*—usually called *phrases* when the terms are all sequential. These include
    things like “software engineer”, “software engineers”, and “senior software engineer”.
    For simplicity in this book, we also consider single terms to be “term sequences”,
    so any time we refer to “phrases”, this includes single terms.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8解释了这些额外的语义层次。在最基本层面上，你有*字符*，它们是单个字母、数字或符号，如图中的字母“e”。然后一个或多个字符组合成*字符序列*，如“e”、“en”、“eng”……“engineer”和“engineers”。一些字符序列形成术语，这些术语是完整的单词或标记，带有可识别的意义，如“engineer”、“engineers”、“engineering”或“software”。然后一个或多个术语可以组合成*术语序列*——当术语都是连续的时，通常称为*短语*。这包括像“software
    engineer”、“software engineers”和“senior software engineer”这样的东西。为了本书的简洁性，我们也认为单个术语是“术语序列”，所以当我们提到“短语”时，这包括单个术语。
- en: '![figure](../Images/CH02_F08_Grainger.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F08_Grainger.png)'
- en: Figure 2.8 Semantic data encoded into free text content. Characters form character
    sequences, which form terms, which form term sequences, which form fields, which
    form documents, which form a corpus.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8展示了语义数据编码到自由文本内容中。字符形成字符序列，这些序列形成术语，这些术语形成术语序列，这些序列形成字段，这些字段形成文档，这些文档形成一个语料库。
- en: Term sequences vs. phrases
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 术语序列与短语
- en: You may be wondering what the difference is between a “term sequence” and a
    “phrase”. Quite simply, a phrase is a term sequence where all of the terms appear
    sequentially. For example, `"chief executive officer"` is both a phrase and a
    term sequence, whereas `"chief officer"~2` (meaning “officer” within two positions,
    or edit distances, of “chief”) is only a term sequence, since it specifies a sequence
    of terms that is not necessarily sequential. In the vast majority of cases, you
    will only be dealing with sequential term sequences, so we’ll mostly use the word
    “phrase” for simplicity throughout the book when referring inclusively to both
    single terms and multi-term sequential sequences. To avoid confusion, note that
    the word “term” is separately used to refer to “a unique value in a field in the
    search engine”. As such, we will sometimes also refer to unsplit strings with
    multiple words in them in the search engine as “terms”, even though linguistically
    they are considered “phrases” or “term sequences”.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道“词序列”和“短语”之间的区别是什么。简单来说，短语是一个词序列，其中所有词都按顺序出现。例如，“首席执行官”既是一个短语也是一个词序列，而“首席官员~2”（意味着在“首席”两个位置或编辑距离内的“官员”）只是一个词序列，因为它指定了一个不一定按顺序的词序列。在绝大多数情况下，你只会处理按顺序的词序列，所以为了简便起见，本书在指代单词和多词序列序列时，我们将主要使用“短语”这个词。为了避免混淆，请注意，“术语”这个词单独用来指代“搜索引擎字段中的一个唯一值”。因此，我们有时也会将搜索引擎中包含多个单词的未分割字符串称为“术语”，尽管从语言学角度来看，它们被认为是“短语”或“词序列”。
- en: Of course, we know that a number of term sequences together can form sentences,
    multiple sentences can form paragraphs, and paragraphs can then be rolled up into
    even larger groups of text. For a search engine, though, the next higher-level
    level of grouping we’ll typically focus on after term sequences is a field. A
    *field* in a search engine is a partitioned and labeled section of a document,
    usually for purposes of searching on or returning as an independent portion of
    the document. Fields containing text can be analyzed in any number of ways using
    a text analyzer, which typically includes techniques like splitting on white space
    and punctuation, lowercasing all terms so they are case-insensitive, stripping
    out noise (stop words and certain characters), stemming or lemmatization to reduce
    terms to a base form, and removing accents. If the text analysis process is unfamiliar
    to you, or you would like a refresher, we recommend checking out chapter 6 of
    *Solr in Action* by Trey Grainger and Timothy Potter (Manning, 2014).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们知道许多词序列可以组成句子，多个句子可以组成段落，而段落又可以进一步组合成更大的文本组。对于搜索引擎来说，然而，在词序列之后，我们通常关注的下一个更高层次的分组是字段。在搜索引擎中，“字段”是文档的一个分区和标记的部分，通常用于搜索或作为文档的独立部分返回。包含文本的字段可以使用文本分析器以任何数量的方式进行分析，这通常包括在空白和标点符号上分割、将所有术语转换为小写以便它们不区分大小写、去除噪声（停用词和某些字符）、词干提取或词形还原以将术语还原到基本形式，以及去除重音。如果你对文本分析过程不熟悉，或者你想复习一下，我们建议查看Trey
    Grainger和Timothy Potter所著的《Solr in Action》第六章（Manning, 2014）。
- en: One or more fields are then composed together into a *document*, and multiple
    documents form a *corpus* or collection of data. Whenever a query is executed
    against the search index, it filters the corpus into a *document set*, which is
    a subset of the corpus that specifically relates to the query in question.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一个或多个字段组合成一个“文档”，多个文档形成一个“语料库”或数据集。每当对搜索索引执行查询时，它将语料库过滤成一个“文档集”，这是一个与特定查询特别相关的语料库子集。
- en: Each of these linguistic levels—character sequences, terms, term sequences,
    fields, documents, document sets, and the corpus—provides unique insights into
    understanding your content and its unique meaning within your specific domain.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语言层次中的每一个——字符序列、术语、词序列、字段、文档、文档集和语料库——都能为你理解你的内容及其在你特定领域中的独特含义提供独特的见解。
- en: 2.3 Distributional semantics and embeddings
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 分布式语义和嵌入
- en: Distributional semantics is a research area within the field of natural language
    processing that focuses on the semantic relationships between terms and phrases
    based on the distributional hypothesis. The distributional hypothesis is that
    words that occur in similar contexts tend to share similar meanings. It is summarized
    well by the popular quote, “You shall know a word by the company it keeps.”[¹](#footnote-84)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式语义是自然语言处理领域中的一个研究领域，它侧重于基于分布假设的术语和短语之间的语义关系。分布假设是，在相似上下文中出现的词往往具有相似的意义。这被一句流行的引语很好地概括了：“你可以通过它所伴随的词语来了解一个词。”[¹](#footnote-84)
- en: 'When applying machine learning approaches to your text, these distributional
    semantics become increasingly important, and the search engine makes it incredibly
    easy to derive the context for most linguistic representations present in your
    corpus. For example, if someone wants to find all documents about C-level executives,
    they could issue a query like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当将机器学习方法应用于您的文本时，这些分布语义变得越来越重要，搜索引擎使得从您的语料库中的大多数语言表示中推导出上下文变得极其容易。例如，如果有人想找到所有关于C级高管的文章，他们可以发出如下查询：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This query would match “CEO”, “CMO”, “CFO”, or any other CXO-style title, as
    it is asking for any character sequence starting with “c” and ending with “o”
    with a single character in between.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询将匹配“CEO”、“CMO”、“CFO”或任何其他CXO风格的头衔，因为它要求找到以“c”开头并以“o”结尾的单个字符之间的任何字符序列。
- en: 'The same kind of freedom exists to query for arbitrarily complex term sequences:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 查询任意复杂的术语序列也存在同样的自由度：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This query would match “VP Engineering”, “VP of Engineering”, “Engineering VP”,
    or even “VP of Software Engineering”, as it is asking to find “VP” and “Engineering”
    within two positions (edit distances) of each other.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询将匹配“VP Engineering”、“VP of Engineering”、“Engineering VP”或甚至是“VP of Software
    Engineering”，因为它要求在两个位置（编辑距离）内找到“VP”和“Engineering”。
- en: 'Of course, the nature of a search engine’s inverted index also makes it trivial
    to support arbitrary Boolean queries. For example, if someone searches for the
    term “Word”, but we want to ensure any matched documents also contain either the
    term “Microsoft” or “MS” somewhere in the document, we could issue the following
    query:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，搜索引擎倒排索引的性质也使得支持任意布尔查询变得非常简单。例如，如果有人搜索“Word”这个词，但我们想确保匹配的任何文档也包含文档中的“Microsoft”或“MS”这两个词中的任何一个，我们可以发出以下查询：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Search engines support arbitrarily complex combinations of queries for character
    sequences, terms, and term sequences throughout your corpus, returning document
    sets that serve as a unique context of content matching that query. For example,
    if you run a query for `pizza`, the documents returned are more likely going to
    be restaurants than car rental companies, and if you run a query for `machine`
    `learning`, you’re more likely to see jobs for data scientists or software engineers
    than for accountants, food service workers, or pharmacists. This means that you
    can infer a strong relationship between “machine learning” and “software engineering”,
    and a weak relationship between “machine learning” and “food service worker”.
    If you dig deeper, you’ll also be able to see what other terms and phrases most
    commonly co-occur within the machine learning document set relative to the rest
    of your corpus, and thereby better understand the meaning and usage of the phrase
    “machine learning”. We’ll dive into hands-on examples of using these relationships
    in chapter 5.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎支持在整个语料库中对字符序列、术语和术语序列进行任意复杂的查询组合，返回作为查询内容匹配唯一上下文的文档集。例如，如果您运行一个针对`pizza`的查询，返回的文档更有可能是餐厅而不是汽车租赁公司，如果您运行一个针对`machine`
    `learning`的查询，您更有可能看到数据科学家或软件工程师的工作，而不是会计师、餐饮工人或药剂师。这意味着您可以从“machine learning”和“software
    engineering”之间推断出强关系，以及从“machine learning”和“food service worker”之间推断出弱关系。如果您进一步挖掘，您还将能够看到与您的语料库中的其他部分相比，在机器学习文档集中最常共同出现的其他术语和短语，从而更好地理解“machine
    learning”这个短语的含义和用法。我们将在第5章中深入探讨使用这些关系进行实际操作的例子。
- en: Introducing vectors
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 引入向量
- en: A basic understanding of vector operations will be important as you progress
    through this book. A *vector* is a list of values describing some attributes of
    an item. For example, if your items are houses, you may have a list of attributes
    like `price`, `size`, and `number of bedrooms`. If you have a home costing $100,000
    with 1,000 square feet and 2 bedrooms, this could be represented as the vector
    `[100000,` `1000,` `2]`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在你阅读这本书的过程中，对向量操作的基本理解将非常重要。*向量*是一系列描述某个项目属性的值。例如，如果你的项目是房子，你可能有一系列属性，如`价格`、`大小`和`卧室数量`。如果你有一套价值10万美元、1000平方英尺和2个卧室的房子，这可以表示为向量
    `[100000, 1000, 2]`。
- en: These attributes (price, size, and number of bedrooms in this example) are called
    *dimensions* or *features*, and a specific collection of dimensions is called
    a *vector space*. You can represent any other items (like other homes, apartments,
    or dwellings) within the same vector space if you can assign them values within
    the dimensions of the vector space.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性（在这个例子中是价格、大小和卧室数量）被称为*维度*或*特征*，而特定维度的集合被称为*向量空间*。如果你可以在向量空间的维度内为其他项目（如其他房子、公寓或住宅）分配值，你就可以在同一个向量空间中表示这些项目。
- en: If we consider other vectors within the same vector space (such as a house `[1000000,`
    `9850,` `12]` and another house `[120000,` `1400,` `3]`), we can perform mathematical
    operations on the vectors to learn trends and compare vectors. For example, you
    may intuitively look at these three example vectors and determine that “home prices
    tend to increase as the number of rooms increases” or that “the number of rooms
    tends to increase as home size increases”. We can also perform similarity calculations
    on vectors to determine that the $120,000 home with 1,400 square feet and 3 bedrooms
    is more similar to the $100,000 home with 1,000 square feet and 2 bedrooms than
    to the $1,000,000 home with 9,850 square feet and 12 bedrooms.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑同一向量空间内的其他向量（例如，一套价值100万美元、9850平方英尺和12个卧室的房子 `[1000000, 9850, 12]` 和另一套价值12万美元、1400平方英尺和3个卧室的房子
    `[120000, 1400, 3]`），我们可以在向量上执行数学运算来学习趋势并比较向量。例如，你可能直观地查看这三个示例向量并确定“房价随着房间数量的增加而增加”或“房间数量随着房子大小的增加而增加”。我们还可以在向量上执行相似度计算，以确定价值12万美元、1400平方英尺和3个卧室的房子比价值100万美元、1000平方英尺和2个卧室的房子更相似，而不是与价值100万美元、9850平方英尺和12个卧室的房子相似。
- en: In recent years, the distributional hypothesis has been applied to create semantic
    understandings of terms and term sequences through what are known as embeddings.
    An *embedding* is a set of coordinates in a vector space into which we map (or
    “embed”) a concept. More concretely, that set of coordinates is a numerical vector
    (a list of numbers) that is intended to represent the semantic meaning of your
    data (text, image, audio, behavior, or other data modalities). Text-based embeddings
    can represent term sequences of any length, but when representing individual words
    or phrases, we call those embeddings *word embeddings*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，分布假设已被应用于通过所谓的嵌入来创建术语和术语序列的语义理解。*嵌入*是一组坐标，我们在其中映射（或“嵌入”）一个概念。更具体地说，这组坐标是一个数值向量（一系列数字），旨在表示你数据的语义意义（文本、图像、音频、行为或其他数据模式）。基于文本的嵌入可以表示任何长度的术语序列，但当表示单个单词或短语时，我们称这些嵌入为*词嵌入*。
- en: The term sequence is often encoded into a reduced-dimension embedding that can
    be compared with the vectors for all of the other embeddings within the corpus
    to find the most semantically related documents.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 术语序列通常被编码成一个低维嵌入，可以与语料库中所有其他嵌入的向量进行比较，以找到语义上最相关的文档。
- en: To understand this process, it may be useful to think of how a search engine
    works out of the box. Let’s imagine a vector exists for each term that contains
    a value (dimension) for every word in your corpus. It might look something like
    figure 2.9.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个过程，可能有用的是思考一下搜索引擎是如何从零开始工作的。让我们想象每个术语都有一个向量存在，它包含了你语料库中每个单词的值（维度）。它可能看起来像图2.9所示。
- en: '![figure](../Images/CH02_F09_Grainger.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F09_Grainger.png)'
- en: Figure 2.9 Vectors with one dimension per term from the inverted index. Every
    query on the left maps to a vector on the right, with a value of `1` for any term
    from the index that is also in the query, and a `0` for any term from the index
    that is not in the query.
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 从倒排索引中每个术语一个维度的向量。左侧的每个查询映射到右侧的向量，对于索引中也在查询中的任何术语，其值为`1`，而对于索引中不在查询中的任何术语，其值为`0`。
- en: Figure 2.9 demonstrates conceptually how document matching works in lexical
    search engines by default. A *lexical search* is a search where documents are
    matched and ranked based on the degree to which they contain the actual keywords
    or other attributes specified in the query. For every keyword query, a vector
    exists that contains a dimension for every term in the inverted index. If that
    term exists in the query, the value in the vector is `1` for that dimension, and
    if that value does not exist in the query, then the value is `0` for that dimension.
    A similar vector exists for every document in the inverted index, with a `1` value
    for any term from the index that appears in the document, and a `0` for all other
    terms.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9展示了默认情况下词汇搜索引擎中文档匹配的概念工作方式。一个 *词汇搜索* 是一种搜索，其中文档根据它们包含查询中指定的实际关键词或其他属性的程度的匹配和排名。对于每个关键词查询，存在一个包含倒排索引中每个术语维度的向量。如果该术语存在于查询中，则该维度的值是
    `1`，如果该值不存在于查询中，则该维度的值是 `0`。倒排索引中的每个文档都有一个类似的向量，对于索引中出现在文档中的任何术语，其值为 `1`，而对于所有其他术语，其值为
    `0`。
- en: When a query is executed, a lookup occurs in the index for any matched terms,
    and then a similarity score is calculated based on a comparison of the vector
    for the query and the vector for the document that is being scored relative to
    the query. We’ll walk through the specific scoring calculation in chapter 3, but
    this high-level understanding is sufficient for now.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行查询时，会在索引中查找任何匹配的术语，然后根据查询向量与被评分文档的向量之间的比较来计算相似度得分。我们将在第3章中详细介绍具体的评分计算，但现在的这个高级理解已经足够了。
- en: There are obvious downsides to this approach. While it’s great for finding documents
    with exact keyword matches, what happens when you want to find “related” things
    instead? For example, you’ll notice that the term “soda” appears in a query, but
    never in the index. Even though there are other kinds of drinks (“apple juice”,
    “water”, “cappuccino”, and “latte”), the search engine will always return zero
    results because it doesn’t understand that the user is searching for a drink.
    Similarly, you’ll notice that even though the term “caffeine” exists in the index,
    queries for `latte`, `cappuccino`, and `green tea` will never match the term “caffeine”,
    even though they are related.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有明显的缺点。虽然它非常适合查找具有精确关键词匹配的文档，但当你想查找“相关”的东西时会发生什么？例如，你会注意到术语“soda”出现在查询中，但从未出现在索引中。即使有其他类型的饮料（“苹果汁”、“水”、“卡布奇诺”和“拿铁”），搜索引擎总是会返回零结果，因为它不理解用户正在搜索饮料。同样，你会注意到尽管术语“caffeine”存在于索引中，但查询
    `latte`、`cappuccino` 和 `green tea` 永远不会匹配术语“caffeine”，即使它们是相关的。
- en: For these reasons, it is now common practice to use reduced-dimension dense
    embeddings to model a semantic meaning for term sequences in your index and queries.
    A *dense embedding* (also known as a *dense vector embedding*) is a vector of
    more abstract features that encodes an input’s conceptual meaning in a semantic
    space. Figure 2.10 demonstrates the terms now mapped to a dimensionally reduced
    vector that can serve as a dense embedding.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，现在普遍的做法是使用降维密集嵌入来为索引和查询中的术语序列建模语义意义。一个 *密集嵌入*（也称为 *密集向量嵌入*）是一个包含更抽象特征的向量，它在语义空间中编码输入的概念意义。图2.10展示了现在映射到降维向量中的术语，这些向量可以作为密集嵌入使用。
- en: '![figure](../Images/CH02_F10_Grainger.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F10_Grainger.png)'
- en: Figure 2.10 Dense embeddings with reduced dimensions. In this case, instead
    of one dimension per term (exists or missing), now higher-level dimensions exist
    that score shared attributes across items such as “healthy”, contains “caffeine”
    or “bread” or “dairy”, or whether the item is “food” or a “drink”.
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10 展示了降维密集嵌入。在这种情况下，不再是每个术语一个维度（存在或缺失），而是现在存在更高层次的维度，这些维度评估跨项目共享的属性，例如“健康”，包含“咖啡因”或“面包”或“乳制品”，或者项目是“食物”还是“饮料”。
- en: With a new embedding vector now available for each term sequence in the leftmost
    column of figure 2.10, we can now score the relationship between each pair of
    term sequences, using the similarity between their vectors. In linear algebra,
    we use a cosine similarity function (or another similarity measure) to score the
    relationship between two vectors. Cosine similarity is computed by performing
    a dot product between the two vectors and scaling it by the magnitudes (lengths)
    of each of the vectors. We’ll visit the math in more detail in the next chapter,
    but for now, figure 2.11 shows the results of scoring the similarity between several
    of these vectors.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于图2.10最左侧列中的每个词序列都可用一个新的嵌入向量，我们可以现在使用它们向量之间的相似度来评估每一对词序列之间的关系。在线性代数中，我们使用余弦相似度函数（或另一个相似度度量）来评估两个向量之间的关系。余弦相似度是通过执行两个向量的点积并按每个向量的模（长度）进行缩放来计算的。我们将在下一章更详细地介绍数学，但现在，图2.11显示了评估这些向量之间相似度的结果。
- en: '![figure](../Images/CH02_F11_Grainger.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F11_Grainger.png)'
- en: Figure 2.11 Similarity between embeddings. The cosine between vectors shows
    the items list sorted by similarity with “green tea”, with “cheese pizza”, and
    with “donut”.
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11 嵌入之间的相似性。向量之间的余弦值显示了按与“绿茶”、“芝士披萨”和“甜甜圈”的相似度排序的项目列表。
- en: 'As you can see in figure 2.11, since each term sequence is encoded into a vector
    that represents its meaning in terms of higher-level features, this embedding
    can now be used to score the similarity of that term sequence with any other similar
    vector. You’ll see three vector similarity lists at the bottom of the figure:
    one for “green tea”, one for “cheese pizza”, and one for “donut”.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在图2.11中看到的，由于每个词序列都被编码成一个向量，该向量代表其在高级特征方面的意义，现在这个嵌入可以用来评估该词序列与任何其他相似向量的相似度。你会在图的下部看到三个向量相似度列表：一个用于“绿茶”，一个用于“芝士披萨”，一个用于“甜甜圈”。
- en: By comparing the vector similarity of “green tea” with all the other term sequences,
    we find that the top related items are “water”, “cappuccino”, “latte”, “apple
    juice”, and “soda”, with the least related being “donut”. This makes intuitive
    sense, as “green tea” shares more attributes with the items higher in the list.
    For the “cheese pizza” vector, we see that the most similar other embeddings are
    for “cheese bread sticks”, “cinnamon bread sticks”, and “donut”, with “water”
    being at the bottom of the list. Finally, for the term “donut”, we find the top
    items to be “cinnamon bread sticks”, “cheese bread sticks”, and “cheese pizza”,
    with “water” once again being at the bottom of the list. These results do a great
    job of finding the most similar items to our original query.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较“绿茶”与其他所有词序列的向量相似度，我们发现最相关的项目是“水”、“卡布奇诺”、“拿铁”、“苹果汁”和“苏打水”，而最不相关的是“甜甜圈”。这从直观上是有道理的，因为“绿茶”与列表中更高的项目共享更多的属性。对于“芝士披萨”向量，我们看到最相似的其它嵌入是“芝士面包棒”、“肉桂面包棒”和“甜甜圈”，而“水”位于列表底部。最后，对于“甜甜圈”这个术语，我们发现最相关的项目是“肉桂面包棒”、“芝士面包棒”和“芝士披萨”，而“水”再次位于列表底部。这些结果很好地找到了与我们原始查询最相似的项目。
- en: It’s worth noting that this vector scoring is only used in the calculation of
    similarity between items. In your search engine, there’s usually a two-phase process
    whereby you first filter to a set of documents (the *matching* phase) and then
    score those resulting documents (the *ranking* phase). Unless you’re going to
    skip the first step and score all of your documents relative to your query vectors
    (which can be time- and processing-intensive), you’ll still need some form of
    initial matching prior to the ranking phase to filter the query to a reasonable
    number of documents to score. We’ll dive more into the mechanics for successfully
    implementing embeddings and vector search in chapters 3, 9, 13, 14, and 15\.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这种向量评分仅在计算项目之间的相似度时使用。在你的搜索引擎中，通常有一个两阶段过程，你首先过滤到一组文档（匹配阶段），然后评分这些结果文档（排名阶段）。除非你打算跳过第一步并相对于你的查询向量评分所有文档（这可能需要时间和处理资源），否则你仍然需要在排名阶段之前使用某种形式的初始匹配来过滤查询到一个合理的文档数量进行评分。我们将在第3章、第9章、第13章、第14章和第15章中更深入地探讨成功实现嵌入和向量搜索的机制。
- en: Embeddings might represent queries, portions of documents, or even entire documents.
    It is commonplace to encode terms and term sequences into word embeddings, but
    *sentence embeddings* (encoding a vector with the meaning of a sentence), *paragraph
    embeddings* (encoding a vector with the meaning of a paragraph), and *document
    embeddings* (encoding a vector with the meaning of an entire document) are also
    common techniques.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入可以表示查询、文档的部分，甚至整个文档。将术语和术语序列编码到词嵌入中是很常见的，但*句子嵌入*（编码一个表示句子意义的向量）、*段落嵌入*（编码一个表示段落意义的向量）和*文档嵌入*（编码一个表示整个文档意义的向量）也是常见的技巧。
- en: It’s also very common for dimensions to be more abstract than our examples here.
    For example, deep learning models like LLMs may detect seemingly unintelligible
    features from character sequences and the way that documents cluster together
    within the corpus. We wouldn’t be able to easily apply a human-readable label
    to these dimensions in the embedding vector, but as long as it improves the predictive
    power of the model and increases relevance, this is usually not a concern for
    most search teams. In fact, since vectors encode “meaning” through different abstract
    numeric features, it’s also possible to create and search on vectors representing
    different types (or *modalities*) of data, such as images, audio, video, or even
    signals and activity patterns. We’ll cover *multimodal search* (searches on different
    data modalities) in section 15.3\.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 维度比我们这里给出的例子更抽象也是很常见的。例如，像LLMs这样的深度学习模型可能会从字符序列以及文档在语料库中的聚类方式中检测出看似难以理解的特征。我们可能无法轻易将这些维度在嵌入向量中应用一个可读的标签，但只要它能提高模型的预测能力并增加相关性，这通常不会成为大多数搜索团队的担忧。事实上，由于向量通过不同的抽象数值特征来编码“意义”，因此也有可能创建和搜索代表不同类型（或*模态*）数据的向量，例如图像、音频、视频，甚至是信号和活动模式。我们将在第15.3节中介绍*多模态搜索*（在不同数据模态上的搜索）。
- en: Ultimately, combining multiple models for harnessing the power of distributional
    semantics and embeddings tends to create the best outcomes, and we’ll dive further
    into numerous graph and vector-based approaches to using these techniques throughout
    the rest of this book.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，结合多个模型以利用分布语义和嵌入的力量往往能创造出最佳结果，我们将在本书的其余部分深入探讨众多基于图和向量的方法来使用这些技术。
- en: 2.4 Modeling domain-specific knowledge
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 域特定知识建模
- en: In chapter 1, we discussed the search intelligence progression (refer to figure
    1.8), whereby organizations start with basic keyword search and progress through
    several additional stages of improvement before they ultimately achieve a full
    self-learning system. The second stage in that search intelligence progression
    was building taxonomies and ontologies, and the third stage (“query intent”) included
    the building and use of knowledge graphs. Unfortunately, there can sometimes be
    significant confusion among practitioners in the industry on proper definitions
    and key terminology, like “ontology”, “taxonomy”, “synonym lists”, “knowledge
    graphs”, “alternative labels”, and so on. It will benefit us to provide some definitions
    for use in this book to prevent any ambiguity. Specifically, we’ll lay out definitions
    for the key terms of “knowledge graph”, “ontology”, “taxonomy”, “synonyms”, and
    “alternative labels”. Figure 2.12 shows a high-level diagram of how they relate.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们讨论了搜索智能的进步（参见图1.8），组织从基本的关键词搜索开始，经过几个额外的改进阶段，最终实现一个完全的自学习系统。搜索智能进步的第二阶段是构建分类法和本体，第三阶段（“查询意图”）包括构建和使用知识图谱。不幸的是，有时行业内从业者对于正确的定义和关键术语，如“本体”、“分类法”、“同义词列表”、“知识图谱”、“替代标签”等，可能会有很大的混淆。为此，我们将在本书中提供一些定义以避免任何歧义。具体来说，我们将为“知识图谱”、“本体”、“分类法”、“同义词”和“替代标签”等关键术语列出定义。图2.12显示了它们之间的高层次关系。
- en: '![figure](../Images/CH02_F12_Grainger.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F12_Grainger.png)'
- en: Figure 2.12 Levels of domain-specific knowledge modeling. Knowledge graphs extend
    ontologies, which extend taxonomies. Synonyms extend alternative labels and map
    to entries in taxonomies.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12 域特定知识建模的层次。知识图谱扩展了本体，本体扩展了分类法。同义词扩展了替代标签并映射到分类法中的条目。
- en: 'We define each of these knowledge modeling techniques as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如下定义这些知识建模技术：
- en: '*Alternative labels (or alt. labels)*—Replacement term sequences with identical
    meanings.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*替代标签（或alt. labels）*—用具有相同意义的替换术语序列。'
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Synonyms*—Replacement term sequences that can be used to represent the same
    or very similar things.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同义词*——可以用来表示相同或非常相似事物的替换词序列。'
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Taxonomy*—A classification of things into categories.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类法*——将事物分类到类别中的方法。'
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Ontology*—A mapping of relationships between types of things.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本体*——事物类型之间关系的映射。'
- en: '[PRE6]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Knowledge graph*—An instantiation of an ontology that also contains the things
    that are related.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*知识图谱*——本体的实例化，其中还包含相关的事物。'
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Creating alternative labels is the most straightforward of these techniques
    to understand. Initialisms (such as “RN” => “Registered Nurse”) and acronyms virtually
    always serve as alternative labels, as do misspellings and alternative spellings.
    Sometimes it is useful to store these mappings in separate lists, particularly
    if you’re using algorithms to determine them and you expect to allow for human
    modification of them or if you plan to rerun the algorithms later.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 创建替代标签是这些技术中最容易理解的。缩写（如“RN”=>“注册护士”）和首字母缩略词几乎总是作为替代标签，同样，拼写错误和替代拼写也是如此。有时，将这些映射存储在单独的列表中是有用的，尤其是如果你正在使用算法来确定它们，并且你预计将允许人类修改它们，或者如果你计划稍后重新运行算法。
- en: Synonyms are the next most common of the techniques, as virtually every search
    engine will have some implementation of a synonym list. Alternative labels are
    a subset of a synonym list and are the most obvious kind of synonym. Most people
    consider “highly related” term sequences to be synonyms, as well. For example,
    “software engineer” and “software developer” are often considered synonyms since
    they are usually used interchangeably, even though there are some nuances in meaning
    between the two. Sometimes, you’ll even see translations of words between languages
    showing up in synonyms for bilingual search use cases.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词是这些技术中最常见的，因为几乎每个搜索引擎都会有一些同义词列表的实现。替代标签是同义词列表的一个子集，并且是最明显的同义词类型。大多数人认为“高度相关”的词序列也是同义词。例如，“软件工程师”和“软件开发者”通常被认为是同义词，因为它们通常可以互换使用，尽管这两个词之间有一些细微的含义差异。有时，你甚至会在双语搜索用例的同义词中看到不同语言之间单词的翻译。
- en: One key difference between alternative labels and more general synonyms is that
    alternative labels can be seen as *replacement terms* for the original, whereas
    synonyms are more often used as *expansion terms* to add alongside the original.
    Implementations can vary widely, but this ultimately boils down to whether you
    are confident that two term sequences carry exactly the same meaning (and you
    want to normalize them), or whether you’re just trying to include additional related
    term sequences so you don’t miss other relevant results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 替代标签和更一般的同义词之间的一个关键区别在于，替代标签可以被视为原始标签的*替换词*，而同义词则更常被用作*扩展词*，与原始词一起使用。实现方式可能大相径庭，但这最终归结于你是否确信两个词序列具有完全相同的含义（并且你希望将它们标准化），或者你只是试图包含额外的相关词序列，以免错过其他相关结果。
- en: Taxonomies are the next step up from synonyms. Taxonomies focus less on substitute
    or expansion words and instead focus on categorizing your content into a hierarchy.
    Taxonomical information will often be used to drive website navigation, to change
    behavior for a subset of search results (for example, to show different faceting
    or filtering options based upon a parent product category), or to apply dynamic
    filtering based upon a category to which a query maps. For example, if someone
    searches for `range` on a home improvement website, the site might automatically
    filter down to “appliances” to remove the noise of other products that contain
    phrases like “fall within the range” in their product description. Synonyms then
    map into a taxonomy, pointing to particular items within the taxonomy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 分类法是同义词之上的下一步。分类法更少关注替代或扩展词，而是专注于将你的内容分类到层次结构中。分类信息通常用于驱动网站导航，改变搜索结果子集的行为（例如，根据父产品类别显示不同的细分或过滤选项），或根据查询映射的类别应用动态过滤。例如，如果有人在家居装修网站上搜索“范围”，网站可能会自动过滤到“电器”，以去除其他产品描述中包含“在范围内”等短语的其他产品的噪音。同义词随后映射到分类法中，指向分类法中的特定项目。
- en: Whereas taxonomies tend to specify parent-child relationships between categories
    and then map things into those categories, ontologies provide the ability to define
    much richer relationships between things (term sequences, entities) within a domain.
    Ontologies typically define more abstract relationships, attempting to model the
    relationships between kinds of things in a domain, such as “employee reports to
    boss,” “CMO’s boss is CEO,” “CMO is employee”. This makes ontologies really useful
    for deriving new information from known facts by mapping the facts into the ontology
    and then drawing logical conclusions based on relationships in the ontology that
    can be applied to those facts.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类法倾向于指定类别之间的父子关系并将事物映射到这些类别不同，本体提供了在领域内定义事物（术语序列、实体）之间更丰富关系的能力。本体通常定义更抽象的关系，试图模拟领域内事物种类之间的关系，例如“员工向老板汇报”、“CMO的老板是CEO”、“CMO是员工”。这使得本体在通过将事实映射到本体并基于本体中的关系进行逻辑推理以应用于这些事实来推导新信息时非常有用。
- en: Knowledge graphs are the relative newcomer to the knowledge management space.
    Whereas ontologies define high-level relationships that apply to types of things,
    knowledge graphs tend to be full instantiations of ontologies that also include
    each of the specific entities that fall within those types. Using our previous
    ontology example, a knowledge graph would additionally have “Michael is CMO,”
    “Michael reports to Marcia,” and “Marcia is CEO” as relationships in the graph.
    Before knowledge graphs came to the forefront, it was common for these more detailed
    relationships to be modeled into ontologies, and many people still do this today.
    As a result, you’ll often see the terms “knowledge graph” and “ontology” used
    interchangeably, though this is becoming less common over time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱是知识管理领域中的相对新成员。虽然本体定义了适用于事物类型的高级关系，但知识图谱往往是对本体的完整实现，包括那些属于这些类型的每个特定实体。以我们之前提到的本体为例，知识图谱还会包含“Michael是CMO”、“Michael向Marcia汇报”和“Marcia是CEO”等作为图中的关系。在知识图谱成为主流之前，这些更详细的关系通常会被建模到本体中，而且许多人至今仍在这样做。因此，你经常会看到“知识图谱”和“本体”这两个术语被交替使用，尽管随着时间的推移，这种情况变得越来越少见。
- en: Throughout this book, we will mostly focus our discussions on alternative labels,
    synonyms, and knowledge graphs, since taxonomies and ontologies are mostly subsumed
    into knowledge graphs. We’ll explore knowledge graphs more thoroughly in chapter
    5\.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将主要讨论替代标签、同义词和知识图谱，因为分类法和本体主要被包含在知识图谱中。我们将在第5章更深入地探讨知识图谱。
- en: 2.5 Challenges in natural language understanding for search
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 搜索中自然语言理解面临的挑战
- en: In the last few sections, we’ve discussed the rich graph of meaning embedded
    within unstructured data, like text, as well as how distributional semantics and
    embeddings can be used to derive and score semantic relationships between term
    sequences in queries and documents. We also introduced key techniques for knowledge
    modeling and defined related terminology we’ll use throughout this book. In this
    section, we’ll discuss a few key challenges associated with natural language understanding
    that we’ll seek to overcome in the coming chapters.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后几节中，我们讨论了嵌入在非结构化数据（如文本）中的丰富语义图，以及如何使用分布语义和嵌入来推导和评分查询和文档中术语序列之间的语义关系。我们还介绍了知识建模的关键技术和本书中使用的相关术语。在本节中，我们将讨论与自然语言理解相关的一些关键挑战，我们将在接下来的章节中努力克服这些挑战。
- en: 2.5.1 The challenge of ambiguity (polysemy)
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 不确定性的挑战（多义性）
- en: In section 2.1.3, we introduced the idea of polysemy, or ambiguous terms. In
    that section, we were dealing with an image tagged with the name “Trey Grainger”
    but that referred to a different person than the author of this book. In textual
    data, however, we have the same problem, and it can get very messy.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.1.3节中，我们介绍了多义性或模糊术语的概念。在那个章节中，我们处理的是一个被标记为“Trey Grainger”名称但指代不同于本书作者的图像。然而，在文本数据中，我们面临相同的问题，而且可能会变得非常混乱。
- en: Consider a word like “driver”. It can refer broadly to a “vehicle driver”, a
    kind of golf club for hitting the ball off a tee, software that enables a hardware
    device to work, a kind of tool (screwdriver), or the impetus for pushing something
    forward (“a key driver of success”). There are many potential meanings for this
    word, and you could explore even more granular meanings. For example, within the
    “vehicle driver” category, it could mean a taxi driver, Uber driver, Lyft driver,
    professional trucker like a CDL driver (someone with a commercial drivers license),
    or even a bus driver. Within the subset of bus drivers, it could mean a school
    bus driver, a driver of a public city bus, a driver of a tour bus, and so on.
    We could continue breaking down this list into dozens of additional categories
    at a minimum.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以“driver”这个词为例。它可以广泛地指代“车辆驾驶员”，一种用于击打球从tee上起飞的高尔夫球杆，使硬件设备能够工作的软件，一种工具（螺丝刀），或者推动某物前进的动因（“成功的关键推动者”）。这个词有许多潜在的含义，你可以探索更细粒度的含义。例如，在“车辆驾驶员”类别中，它可能指出租车驾驶员、Uber驾驶员、Lyft驾驶员、CDL驾驶员（持有商业驾驶执照的专业卡车司机），甚至公交车驾驶员。在公交车驾驶员的子集中，它可能指校车驾驶员、公共城市公交车驾驶员、旅游巴士驾驶员等等。我们至少可以将这个列表进一步细分为几十个额外的类别。
- en: When building search applications, engineers will often naively create static
    synonym lists and assume terms have a singular meaning that can be applied universally.
    The reality, however, is that every term (word or phrase) takes on a unique meaning
    that is based on the specific context in which it is being used.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建搜索应用时，工程师们通常会天真地创建静态的同义词列表，并假设术语具有单一的含义，可以普遍应用。然而，现实情况是，每个术语（单词或短语）都基于其被使用的特定上下文获得独特的含义。
- en: TIP  Every term takes on a unique meaning that is based on the specific context
    in which it is being used.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: TIP  每个术语都基于其被使用的特定上下文获得独特的含义。
- en: It’s not often practical to support an infinite number of potential meanings,
    though we discuss techniques to approximate this with a semantic knowledge graph
    in chapter 5\. Nevertheless, whether you support many meanings per phrase or just
    a few, it’s important to recognize the clear need to be able to generate an accurate
    (and often nuanced) interpretation for any given phrase your users may encounter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在第5章讨论了使用语义知识图来近似无限多种潜在含义的技术，但通常并不实用。不过，无论你支持每个短语多种含义还是只有几种，认识到能够为用户可能遇到的任何短语生成准确（并且往往是细微的）解释的明确需求是很重要的。
- en: 2.5.2 The challenge of understanding context
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 理解上下文的挑战
- en: I like to say that every term (word or phrase) you ever encounter is a “context-dependent
    cluster of meaning with an ambiguous label”. That is to say, there is a label
    (the textual representation of the term) that is being applied to some concept
    (a cluster of meaning) that is dependent upon the context in which it is found.
    By this definition, it’s impossible to ever precisely interpret a term without
    understanding its context. As a result, creating fixed synonym lists that aren’t
    able to take context into account is likely to create suboptimal search experiences
    for your users.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢说，你遇到的每一个术语（单词或短语）都是一个“具有模糊标签的、依赖于上下文的含义集群”。也就是说，有一个标签（术语的文本表示）被应用于某个概念（含义集群），这个概念依赖于它所处的上下文。根据这个定义，如果不理解其上下文，就永远无法精确地解释一个术语。因此，创建无法考虑上下文的固定同义词列表可能会为你的用户提供次优的搜索体验。
- en: Transformer models largely operate on this premise by using input prompts as
    the context in which to interpret each word part (or token) in the prompt. Attention
    is paid to every token, based on the surrounding tokens and how they relate to
    the learned representation in the model, which is also contextual. We’ll dive
    into the nuances of how Transformers work in chapter 13, and we’ll fine-tune a
    Transformer for a question-answering task in chapter 14.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型在很大程度上基于这个前提，通过使用输入提示作为上下文来解释提示中的每个词部分（或标记）。根据周围的标记以及它们与模型中学习到的表示的关系，对每个标记给予关注，这种表示也是上下文相关的。我们将在第13章深入探讨Transformer的工作原理，并在第14章中微调一个Transformer以用于问答任务。
- en: Just because context is important doesn’t mean it’s always easy to apply correctly.
    It’s often necessary to perform basic keyword search as a fallback when your search
    engine doesn’t understand a query, and it’s almost always useful to have prebuilt
    domain understanding that can similarly be relied upon to help interpret queries.
    This prebuilt domain understanding then ends up overriding some of the default
    keyword-based matching behavior (such as joining individual keywords into phrases,
    injecting synonyms, and correcting misspellings).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 仅因为上下文很重要并不意味着总是容易正确应用。当你的搜索引擎不理解查询时，通常有必要执行基本的关键词搜索作为备选方案，而且几乎总是有用预先构建的领域理解来帮助解释查询。这种预先构建的领域理解最终会覆盖一些默认的关键词匹配行为（例如将单个关键词组合成短语、注入同义词和纠正拼写错误）。
- en: As we discussed in chapter 1, the context for a query includes more than just
    the search keywords and the content within your documents. It also includes an
    understanding of your domain, as well as an understanding of your user. Queries
    can take on entirely different meanings based on what you know about your user
    and any domain-specific understanding you may have. This context is necessary
    both to detect and resolve the kinds of ambiguity we discussed in the last section,
    as well as to ensure your users are receiving the most intelligent search experience
    possible. Throughout this book, our focus will be on techniques to automatically
    learn contextual interpretations of each query, based on the unique context in
    which it’s being used.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第一章中讨论的，查询的上下文不仅包括搜索关键词和你的文档中的内容。它还包括对你所在领域的理解，以及对你用户的理解。根据你对用户和任何特定领域理解的了解，查询可以完全不同的含义。这种上下文对于检测和解决我们在上一节中讨论的模糊性是必要的，同时也确保你的用户能够获得尽可能智能的搜索体验。在这本书的整个过程中，我们将关注基于查询使用的独特上下文自动学习每个查询的上下文解释的技术。
- en: 2.5.3 The challenge of personalization
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 个性化挑战
- en: When considering user-specific context as a tool for enhancing query understanding,
    it’s not always obvious how to apply user-specific personalization on top of the
    preexisting content and domain-specific scoring. For example, say you learn that
    a particular user really likes Apple as a brand because they keep searching for
    iPhones. Does this mean that Apple should also be boosted when they are searching
    for watches, computers, keyboards, headphones, and music players? It could be
    that the user only likes Apple-branded phones and that by boosting the brand in
    other categories you may frustrate the user. For example, even if the user did
    search for an iPhone previously, how do you know they weren’t just trying to compare
    the iPhone with other phones they were considering?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑用户特定上下文作为增强查询理解的一种工具时，并不总是明显如何将用户特定个性化应用于现有内容和领域特定评分之上。例如，假设你了解到某个用户非常喜欢苹果这个品牌，因为他们一直在搜索iPhone。这意味着当他们在搜索手表、电脑、键盘、耳机和音乐播放器时，苹果也应该被提升吗？可能这个用户只喜欢苹果品牌的手机，通过在其他类别中提升品牌，你可能会让用户感到沮丧。例如，即使用户之前搜索过iPhone，你怎么知道他们不是只是想比较他们正在考虑的其他手机呢？
- en: Out of all of the dimensions of user intent (figure 1.5), personalization is
    the easiest one to trip up on, and, as a result, it is the one that is least commonly
    seen in modern AI-powered search applications (outside of recommendation engines,
    of course). We’ll work through these problems in detail in chapter 9 to highlight
    how we can strike the right balance when rolling out a personalized search experience.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有用户意图维度（图1.5）中，个性化是最容易出错的一个，因此，它也是现代基于AI的搜索应用（当然不包括推荐引擎）中最少见的一个。我们将在第9章中详细讨论这些问题，以突出我们如何在推出个性化搜索体验时找到正确的平衡点。
- en: 2.5.4 Challenges interpreting queries vs. documents
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 解释查询与文档的挑战
- en: One common problem we see when engineers and data scientists first get started
    with search is a propensity to apply standard natural language processing techniques,
    like language detection, part-of-speech detection, phrase detection, and sentiment
    analysis to queries. Usually, those techniques were trained to operate on longer
    blocks of text—often at the document, paragraph, or at least sentence level.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当工程师和数据科学家刚开始使用搜索时，我们常见的一个问题是倾向于将标准自然语言处理技术，如语言检测、词性检测、短语检测和情感分析应用于查询。通常，这些技术是在较长的文本块上训练的——通常是文档、段落或至少句子级别。
- en: Documents tend to be longer and to supply significantly more context to the
    surrounding text, whereas queries tend to be short (a few keywords only) in most
    use cases. Even when they are longer, queries tend to combine multiple ideas as
    opposed to supplying more linguistic context. As a result, when trying to interpret
    queries, you need to use external context as much as possible.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a natural language processing library that relies on sentence
    structure to interpret the query, for example, you can try looking up the phrases
    from your query in your corpus of documents to find their most common domain-specific
    interpretations. Likewise, you can use the co-occurrence of terms within your
    query across previous user search sessions by mining user behavioral signals.
    This enables you to learn real intent from similar users, which would be very
    challenging to reliably derive from a standard natural language processing library.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: In short, queries need special handling and interpretation due to their tendency
    to be short and to often imply more than they state explicitly, so fully using
    search-centric data science approaches to queries is going to generate much better
    results than traditional natural language processing approaches.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.5 Challenges interpreting query intent
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While the process of parsing a query to understand the terms and phrases it
    contains is important, there is often a higher-level intent behind the query—a
    query intent, if you will. For example, let’s consider the inherent differences
    between the following queries:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The intent of the first query for `who` `is` `the` `CEO?` is clearly to find
    a factual answer and not a list of documents. The second query for `support` is
    trying to navigate to the support section of a website, or to otherwise contact
    the support team. The third query for `iphone` `screen` `blacked` `out` is also
    looking for support, but it is for a specific problem, and the person likely wants
    to find troubleshooting pages that may help with that specific problem before
    reaching out to the actual support team.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The next two queries for `iphone` and for `verizon` `silver` `iphone` `8` `plus`
    `64GB` are quite interesting. While they are both for iPhones, the first search
    is a general search, likely indicating a browsing or product research intent,
    whereas the second query is a much more specific variant of the first search,
    indicating the user knows exactly what they are looking for and may be closer
    to making a purchasing decision. The general query for `iphone` may be better
    to return a landing page providing an overview of iphones and the available options,
    whereas the more specific query may be better for going straight to a product
    page with a purchase button. As a general rule of thumb, the more general a query,
    the more likely the user is just browsing. More specific queries—especially when
    they refer to specific items by name—often indicate a purchase intent or desire
    to find a particular known item.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The query for `sale` indicates that the user is looking for items that are available
    for purchase at a discounted rate, which will invoke some specially implemented
    filter or redirect to a particular landing page for an ongoing sale event. The
    query for `refrigerators` indicates that the user wants to browse a particular
    category of product documents. Finally, the query for `pay` `my` `bill` indicates
    that the user wants to take an action—the best response to this query isn’t a
    set of search results or even an answer, but instead a redirect to a bill review
    and payment section of the application.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Each of these queries contains an intent beyond just a set of keywords to be
    matched. Whether the intent is to redirect to a particular page, apply particular
    filters, browse or purchase items, or even take domain-specific actions, the point
    is that there is domain-specific nuance to how users may express their goals to
    your search engine. Oftentimes, it can be difficult to derive these domain-specific
    user intents automatically. It is fairly common for businesses to implement specific
    business rules to handle these as one-off requests. Query intent classifiers can
    certainly be built to handle subsets of this problem, but successfully interpreting
    every possible query intent still remains challenging when building out natural
    language query interpretation capabilities.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '2.6 Content + signals: The fuel powering AI-powered search'
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first chapter, we introduced the idea of reflected intelligence—using
    feedback loops to continually learn from both content and user interactions. This
    chapter has focused entirely on understanding the meaning and intelligence embedded
    within your content, but it’s important to recognize that many of the techniques
    we’ll apply to the “unstructured data” in your documents can also be just as readily
    applied to your user behavioral signals. For example, we discussed in section
    2.3 how the meanings of phrases can be derived from finding the other phrases
    they most commonly appear with across your corpus. We noted that “machine learning”
    appears more commonly with “data scientist” and “software engineer” than it does
    with “accountants,” “food service workers,” or “pharmacists”.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: If you abstract the distributional hypothesis beyond documents and also apply
    it to user behavior, you might expect that similar users querying your search
    engine are likely to exhibit similar query behavior. Specifically, people who
    are data scientists or who are searching for data scientists are far more likely
    to also search for or interact with documents about machine learning, and the
    likelihood of a food service worker or accountant searching for machine learning
    content is much lower than the likelihood of a software engineer doing so. We
    can thus apply these same techniques to learn related terms and term sequences
    from query logs, where instead of thinking of terms and term sequences mapping
    to fields in documents, we think of terms in queries and clicks on search results
    mapping to user sessions, which then map to users. We’ll follow this approach
    in chapter 6 to learn related terms, synonyms, and misspellings from user query
    logs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Some search applications are content-rich but have very few user signals. Other
    search applications have an enormous number of signals but either very little
    content or content that poses challenges from an automated learning perspective.
    In an ideal scenario, you’ll have great content and an enormous quantity of user
    signals to learn from, which allows you to combine the best of both worlds into
    an even smarter AI-powered search application. Regardless of which scenario you’re
    in, keep in mind that your content and your user signals can both serve as fuel
    to power learning algorithms, and you should do your best to maximize the collection
    and quality of each.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final note on natural language understanding: with the rise of LLMs, which
    are deep neural networks trained on a large percentage of human knowledge (much
    of the internet, plus selected sources), we now have the ability to interpret
    the meaning and intent of general-knowledge questions at an unprecedented level
    of quality. LLMs do not handle domain-specific understanding very well out of
    the box, at least for information that is not part of their training sets, but
    with the ability to fine-tune LLMs on domain-specific data, these models can often
    be quickly adapted to more closed-domain data. LLMs represent a large leap forward
    in our ability to learn the nuances of natural language, interpret arbitrary documents
    and queries based on those nuances, and drive more relevant search.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, while generally the most impressive technique for wide-scale natural language
    understanding, are far from the only powerful tools in our AI-powered search toolbox.
    We’ll dive into using LLMs for search in chapters 9, 13, 14, and 15\. In the meantime,
    we have many other critical algorithms and techniques to explore for natural language
    and domain understanding, interpreting user behavior, and learning optimal relevance
    ranking models.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered all the background needed to begin extracting meaning
    from your natural language content, it’s time to roll up our sleeves and get hands-on.
    In the next chapter, we’ll dive into lots of examples as we begin to explore content-based
    relevance in an AI-powered search application.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unstructured data is a misnomer—it is really more like hyper-structured data,
    as it represents a giant graph of domain-specific knowledge.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search engines can use distributional semantics—interpreting the semantic relationships
    between terms and phrases based upon the distributional hypothesis—to harness
    rich semantic meaning at the level of character sequences, terms, term sequences
    (typically phrases), fields, documents, document sets, and an entire corpus.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributional semantics approaches enable us to learn the nuanced meaning of
    our queries and content from their larger surrounding context.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings are a powerful technique for search results ranking based on the
    semantic meaning of text (and other data modalities) instead of just the existence
    and occurrence counts of specific keywords.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-specific knowledge is commonly modeled through a combination of alternative
    labels, synonym lists, taxonomies, ontologies, and knowledge graphs. Knowledge
    graphs typically model the output from each of the other approaches into a unified
    knowledge representation of a particular domain.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polysemy (ambiguous terms), context, personalization, and query-specific natural
    language processing approaches represent some of the more interesting challenges
    in natural language search.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content and user signals are both important fuel for our AI-powered search applications
    to use when solving natural language challenges.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#ftnote-84) John Rupert Firth, “A synopsis of linguistic theory, 1930–1955,”
    in J.R. Firth et al., *Studies in Linguistic Analysis*, Special Volume of the
    Philological Society (Oxford University Press, 1957).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
