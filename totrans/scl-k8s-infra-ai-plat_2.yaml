- en: Chapter 3\. Making Training Repeatable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#ch02_model_development_on_kubernetes_1738498450538975),
    you learned about techniques for customizing a model, including fine-tuning, a
    special case of model training. Once you’ve fine-tuned or trained your model for
    the first time, you might be tempted to think that you are done with model development
    and all you have left is to evaluate and deploy your model.
  prefs: []
  type: TYPE_NORMAL
- en: You’d be half right. But because the data that informs the model will likely
    change over time, the model must be regularly retrained throughout its lifetime
    to ensure that it can continue to deliver value. In this chapter, you will dive
    into the AI model lifecycle, learning how to track model versions, automate model
    training, and implement GitOps for model training pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining and the Model Development Lifecycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world changes. The data that we use to describe the real world, then, must
    change too. Consequently, if the data changes, then any models that attempt to
    model a problem in the real world must also change.
  prefs: []
  type: TYPE_NORMAL
- en: Since models in production are static, over time the input data that a given
    model will process in production for inference requests will differ from the data
    that the model was trained on. This variability can come from any number of sources,
    such as changing user behavior over time, seasonality effects in the data, changes
    to the input data format, etc. The phenomenon is called *data drift*.
  prefs: []
  type: TYPE_NORMAL
- en: Data drift isn’t the only reason to retrain a model, however. Retraining is
    a good option when a metric that is monitored in production (such as accuracy,
    model responsiveness, compute resources, etc.) falls outside its optimal range.
  prefs: []
  type: TYPE_NORMAL
- en: 'When and how often a model should be retrained are two key considerations,
    and they depend heavily on your use case and training data. There are two main
    choices here: either regularly retrain the model on some fixed cadence, or retrain
    the model on demand.'
  prefs: []
  type: TYPE_NORMAL
- en: Retraining a model at a fixed cadence can be costly if the cadence is rapid
    or if the retraining doesn’t actually show any improvement. This method assumes
    that the data changes according to some predictable pattern that can be detected
    at a chosen retraining cadence. If retraining doesn’t show any improvement, it’s
    possible that the data isn’t changing in such a way to be captured by the regular
    cadence.
  prefs: []
  type: TYPE_NORMAL
- en: The other option is to retrain your model on demand. This ensures that models
    are not retrained unnecessarily, but it requires reliable monitoring of the performance
    of the current version of the model (discussed in [Chapter 4](ch04.html#ch04_model_deployment_and_monitoring_1738498450837987))
    and well-defined thresholds for when the performance has sufficiently degraded.
  prefs: []
  type: TYPE_NORMAL
- en: The full lifecycle of a model, then, looks something like [Figure 3-1](#ch03_figure_1_1738498450651715).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/skia_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. The full lifecycle of an AI model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practice, this is a never-ending cycle, which repeats most often from the
    evaluation and monitoring stages, where unacceptable performance is typically
    discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Where [Figure 1-1](ch01.html#ch01_figure_1_1738498450402392) was focused on
    the model development cycle (stage 2), [Figure 3-1](#ch03_figure_1_1738498450651715)
    zooms out to view the entire lifecycle of a model. The lifecycle starts with gathering
    data, continues through developing training code, executing the training job,
    evaluating the trained model, promoting the model to production, and monitoring
    the served model.
  prefs: []
  type: TYPE_NORMAL
- en: Crucially, this lifecycle can repeat at any stage (with the exception of training
    code development, which typically remains static), most commonly after the evaluation
    and monitoring stages (4 and 6). From those, it’s common to discover that either
    the training job needs to be run again, or that more or higher quality data needs
    to be collected. If more data is to be collected, typically the existing training
    code can be used as-is. In this case, stage 3 would follow stage 1.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking Model Versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While they iterate on developing a model, data scientists will run many experiments
    by varying the dataset they use to create the model, the model’s architecture,
    the hyperparameters used for training the model, and more. Even after the initial
    handoff of the model to production, future retraining cycles of the model will
    yield new variants.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises need robust solutions for tracking all of these versions of a given
    model. While version control systems have been table stakes for traditional software
    projects for decades now, model version control systems are still in their infancy
    despite the heavy reliance on models by enterprises of all sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Model version tracking benefits the whole enterprise, from unlocking the ability
    of data scientists to recreate previous experiments, share their experiments with
    colleagues, and revert to a model from a previous experiment, to enabling model
    auditing and ensuring responsible AI use. For example, in order to explain a given
    model’s results and how it was created, it is necessary to know which version
    of a model was served in production at what time and how that model was created,
    including the data sources that went into the model.
  prefs: []
  type: TYPE_NORMAL
- en: More and more often, enterprises use a centralized model registry throughout
    the model’s lifecycle, and this is rapidly becoming a recognized best practice.
    During model training and development, model training code should integrate with
    the model registry to register each subsequent version of the model as a distinct
    model artifact. Alongside the model artifact, model training code should register
    metadata about the training data and code that was used to create the model. The
    model registry should also be integrated at model deployment and monitoring stages,
    which will be discussed in [Chapter 4](ch04.html#ch04_model_deployment_and_monitoring_1738498450837987).
  prefs: []
  type: TYPE_NORMAL
- en: Many training platforms that are available today offer model registries as part
    of their product lineup. It is important to choose a platform with a strong model
    registry that is well integrated into the platform’s distributed training engine.
  prefs: []
  type: TYPE_NORMAL
- en: The [Kubeflow project](https://kubeflow.org) offers a powerful integrated solution,
    where the Kubeflow Training Operator, Kubeflow Pipelines, [Katib](https://oreil.ly/NgK4E),
    and the Kubeflow Model Registry can be used together to track training results
    in a central database. Similarly, the [MLflow project](https://mlflow.org) offers
    a robust model registry solution that can be combined with MLflow Runs and Experiments
    to streamline tracking of model versions.
  prefs: []
  type: TYPE_NORMAL
- en: Today, proper model versioning tends to require behavior change by data scientists
    to deliberately integrate model version tracking into their training code and
    workflows. As projects like MLflow and Kubeflow evolve and become better integrated
    with frameworks like PyTorch, you can expect model version tracking capabilities
    to integrate seamlessly out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Automating Model Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training a model is just like any other regularly repeated computing activity:
    it must be automated. Failure to do so leads to several negative outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: Wasted human resources by having to manually rerun training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unpredictability in model retraining cadence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inconsistency in the model’s performance through discrepancies (deliberate or
    accidental) in the model training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initial model development, then, should not be considered complete until the
    training process is automated end to end. A fully featured automated training
    process should include at the very least:'
  prefs: []
  type: TYPE_NORMAL
- en: Input parameters that specify any variables that typically need to be tweaked
    (e.g., a version identifier for the training code or training data, or *hyperparameters*—parameters
    that define how training is done—for the training job)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any necessary data preparation or preprocessing to collect data from any storage
    locations and prepare it for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetching and executing the training job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing the model and related artifacts in a chosen storage endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registering the model and related artifacts in the model registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance of the trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another feature that is nice to have but by no means essential is an automated
    process that promotes a trained model to production or any other post-training
    steps if the model meets some minimum performance thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For many teams, promoting a model to production without any human oversight
    may not be appropriate. When evaluating a tool that has this feature, be sure
    to weigh the trade-offs of not having human review of a trained model’s metrics
    against keeping a human in the loop.
  prefs: []
  type: TYPE_NORMAL
- en: This process (and the software that implements it) is often referred to as a
    *pipeline* or *workflow*. These pipelines are usually authored by data scientists,
    data engineers, machine learning engineers, and/or MLOps teams, with Python being
    the prevailing language of choice.
  prefs: []
  type: TYPE_NORMAL
- en: For enterprises that have adopted Kubernetes as their model development and
    serving platform, we strongly recommend adopting a pipeline engine that is Kubernetes
    native and thus is able to leverage the existing Kubernetes infrastructure, integrating
    seamlessly with the overall MLOps infrastructure in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three major open source pipeline engines that have strong community
    adoption and should be considered for your training infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: Airflow
  prefs: []
  type: TYPE_NORMAL
- en: '[Airflow](https://airflow.apache.org) is typically preferred by data scientists
    and tends to present the cleanest experience for authoring pipelines—or directed
    acyclic graphs (DAGs) in Airflow’s parlance)—but Airflow is not strictly Kubernetes
    native, which presents challenges when operationalizing it at scale on Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow Pipelines
  prefs: []
  type: TYPE_NORMAL
- en: A part of the broader Kubeflow project, [Kubeflow Pipelines](https://oreil.ly/0bP2X)
    is Kubernetes native at its core, making it more customizable, scalable, and well
    suited to enterprises with large or multiple data science teams wishing to share
    Kubernetes infrastructure, or with central IT/MLOps teams managing consistent
    infrastructure across teams.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow
  prefs: []
  type: TYPE_NORMAL
- en: '[MLflow](https://mlflow.org) excels at model version tracking, making it easy
    for data scientists to adopt and track multiple versions of their models over
    time. However, MLflow requires more effort by operations teams to deploy and scale
    on Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Do you want to see what a continuous model training pipeline looks like in action?
    Red Hat offers an [MLOps lab exercise](https://oreil.ly/l04gD) that helps you
    build one yourself.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps for Model Training Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model training pipelines are like code and should be treated accordingly. Pipeline
    definitions should be stored in source control and versioned, just like traditional
    application code. And like traditional application code, authors of pipeline definitions
    should follow a robust peer review process when making changes to the definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Production training runs, then, should run from clean versions of these pipelines,
    pulled from a well-defined version (such as a branch or tag) of the pipeline in
    version control. This is [GitOps](https://oreil.ly/u9oe-) in a nutshell. GitOps
    is a recent development in traditional software operations whereby applications
    are cleanly deployed from version control and continuously reconciled to ensure
    that the deployed application matches the desired state in version control. When
    teams wish to change the state of the application in production, they do so by
    changing the application’s definition in version control.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes’ declarative approach to deploying applications, along with the reconciliation
    loop that keeps Kubernetes applications in their desired state, make Kubernetes
    an ideal platform for managing pipelines with GitOps.
  prefs: []
  type: TYPE_NORMAL
- en: For managing applications on Kubernetes, one of the most popular projects is
    [Argo CD](https://oreil.ly/2yB2U). Argo CD is a continuous delivery tool for Kubernetes
    that allows users to implement GitOps principles into their workflow. While deploying
    pipeline definitions with Argo CD typically requires the development of custom
    code to convert pipeline definitions from version control into runnable training
    jobs, this is an area of rapid innovation, especially from the Kubeflow project,
    to allow for simpler pipeline definition management.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you’ve learned how to reliably retrain a model, keep track of its
    versions (and that of the data and training pipelines), and build a more robust
    open source MLOps infrastructure, you are prepared to move on to the next step
    of the AI model lifecycle: deployment and monitoring.'
  prefs: []
  type: TYPE_NORMAL
