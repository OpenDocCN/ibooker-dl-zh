- en: 5 Simulated annealing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Introducing trajectory-based optimization algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the simulated annealing algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving function optimization as an example of continuous optimization problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving puzzle game problems like Sudoku as an example of constraint-satisfaction
    problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving permutation problems like TSP as an example of discrete problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving a real-world delivery semi-truck routing problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at simulated annealing as a trajectory-based metaheuristic
    optimization technique. We’ll discuss different elements of this algorithm and
    its adaptation aspects. A number of case studies will be presented to show the
    ability of this metaheuristic algorithm to solve continuous and discrete optimization
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Introducing trajectory-based optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine yourself on a hiking trip looking for the lowest valley in a rugged
    landscape that has many valleys and hills. You don’t have access to global information
    or a map that shows the location of the lowest valley. You start your hiking journey
    by randomly choosing a direction. You keep moving step by step until you get stuck
    in a local valley surrounded by hills. You are not highly satisfied with the location,
    as you believe that there is a lower valley in the area that may be behind the
    hills. Your curiosity drives you to move up one of the hills to find the lowest
    valley.
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly what simulated annealing (SA) does. The basic idea of the SA
    algorithm is to use a stochastic search that follows a trial-and-error approach,
    accepting changes that improve the objective function and also keeping some changes
    that are not ideal. In a minimization problem, for example, any better moves or
    changes that decrease the value of the objective function will be accepted. However,
    some moves that increase the objective function will also be accepted with a certain
    probability. SA is a trajectory-based metaheuristic algorithm that can be used
    to find the global optimum solution for complex optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, *metaheuristic algorithms* can be classified into *trajectory-based*
    and *population-based* algorithms, as shown in figure 5.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F01_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 Exploration and exploitation of optimization algorithms
  prefs: []
  type: TYPE_NORMAL
- en: '*Trajectory-based metaheuristic algorithms* or *S-metaheuristics*, such as
    SA or tabu search, use a single search agent that moves through the search space
    in a piecewise style. A better move or solution is always accepted, while a not-so-good
    move can be accepted with a certain probability. The steps, or moves, trace a
    trajectory in the search space, with a nonzero probability that this trajectory
    can reach the global optimum.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, *population-based algorithms* or *P-metaheuristics*, such as genetic
    algorithms, particle swarm optimization, and ant colony optimization, use multiple
    agents to search for an optimal or near-optimal global solution.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the large diversity of initial populations, population-based algorithms
    are naturally more exploration-based, whereas single or trajectory-based algorithms
    are more exploitation-based. The following section explains the SA algorithm in
    more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 The simulated annealing algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether you need to solve a complex nonlinear nondifferential function optimization
    problem, a puzzle game like Sudoku, an academic course scheduling problem, a travelling
    salesman problem (TSP), a network design problem, a task allocation problem, a
    circuit partitioning and placement problem, a production planning and scheduling
    problem, or even a tennis tournament planning problem, SA can be used as a generic
    solver for these different continuous and discrete optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first look at the details of this solver before we use it to solve different
    problems. We’ll start by shedding some light on the physical annealing process,
    which was the inspiration for SA.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Physical annealing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Annealing, as a heat treatment process, has been used for centuries across various
    industries, including metallurgy, glassmaking, and ceramics. For example, in the
    context of making glass bottles, annealing removes the stresses and strains in
    the glass resulting from shaping. This is an important step, and if it’s not done,
    the glass may shatter due to the buildup of tension caused by uneven cooling.
    After the bottles have cooled to room temperature, they are inspected and finally
    packaged.
  prefs: []
  type: TYPE_NORMAL
- en: Annealing alters a material, causing changes in its properties, such as strength
    and hardness. This process heats the material to above the recrystallization temperature,
    maintains a suitable temperature, and then cools the material. As the temperature
    reduces, the mobility of molecules reduces, with the tendency that molecules will
    align themselves in a crystalline structure (figure 5.2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F02_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 The effect of temperature on the mobility of the molecules
  prefs: []
  type: TYPE_NORMAL
- en: The aligned structure is the minimum energy state for the system. To ensure
    that this alignment is obtained, cooling must occur at a sufficiently slow rate.
    If the substance is cooled too rapidly, a noncrystalline state with irregular
    three-dimensional patterns may be reached, as illustrated in figure 5.3\. Quartz,
    sodium chloride, diamond, and sugar are examples of crystalline solids that have
    a regular order for the arrangement of constituent particles, atoms, ions, or
    molecules. Glass, rubber, pitch, and many plastics are examples of noncrystalline
    amorphous solids. As you may know, quartz crystals are harder than glass thanks
    to their symmetrical molecular structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F03_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3 Physical annealing. Left: a metal with a crystalline structure.
    Right: an amorphous metal with a disordered atomic-scale structure.'
  prefs: []
  type: TYPE_NORMAL
- en: Note The annealing process involves the careful control of the temperature and
    cooling rate, often called the annealing or cooling schedule. The annealing time
    should be long enough for the material to undergo the required transformation.
    If the difference in the temperature rate of change between the outside and inside
    of a material is too big, this may cause defects and cracks.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the aligned structure represents the minimum energy state for
    the system inspired scientists to think about mimicking this process to solve
    optimization problems. *Simulated* annealing is a computational model that mimics
    the physical annealing process. In the context of mathematical optimization, the
    minimum of an objective function represents the minimum energy of the system.
    SA is an algorithmic implementation of the cooling process, used to find the optimum
    of an objective function. Table 5.1 outlines the analogy between SA and the physical
    annealing process.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 The physical annealing and simulated annealing analogy
  prefs: []
  type: TYPE_NORMAL
- en: '| Physical annealing | Simulated annealing |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| State of a material | Solution of an optimization problem |'
  prefs: []
  type: TYPE_TB
- en: '| The energy of a state | The cost of a solution |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature | Control parameter (temperature) |'
  prefs: []
  type: TYPE_TB
- en: '| High temperature makes molecules move freely | High temperature favors search
    space exploration |'
  prefs: []
  type: TYPE_TB
- en: '| Low temperature restricts molecules’ motion | Low temperature leads to exploiting
    the search space |'
  prefs: []
  type: TYPE_TB
- en: '| Gradual cooling helps to reduce stress and increase homogeneity and structural
    stability. | Gradual cooling helps to avoid getting stuck in suboptimal local
    minima and to find the globally optimal or near-optimal solution. |'
  prefs: []
  type: TYPE_TB
- en: In 1953, the first computational model that replicated the physical process
    of annealing was introduced. This model was presented as a universal method for
    computing the properties of substances that can be considered collections of individual
    molecules interacting with each other. S. Kirkpatrick et al. were trailblazers
    in utilizing SA for optimization, as described in their paper "Optimization by
    Simulated Annealing" [1]. The following subsection explains the steps involved
    in the SA algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 SA pseudocode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SA employs a Markov chain-based random search approach, which not only accepts
    new solutions that decrease the objective function (assuming a minimization problem)
    but can also accept probabilistic solutions that increase objective function values.
  prefs: []
  type: TYPE_NORMAL
- en: Markov chain
  prefs: []
  type: TYPE_NORMAL
- en: The Markov property, named after Russian mathematician Andrey Markov (1856–1922),
    is a memoryless random process. This means that the next state depends only on
    the current state and not on the sequence of events that preceded it. A Markov
    chain (MC) is a stochastic or probabilistic model that describes a sequence of
    possible moves in which the probability of each move depends only on the state
    attained in the previous move. This means that the transition from one state to
    another depends only on the current fully observable state and a transition probability.
  prefs: []
  type: TYPE_NORMAL
- en: Following this memoryless random process, the transition between the current
    known state A, for example, to a next neighboring state B is governed by a transition
    probability as illustrated in the following figure. Markov chains are used in
    different domains such as stochastic optimization, economy, speech recognition,
    weather prediction, and control systems. It's also worth mentioning that Google’s
    PageRank algorithm uses a Markov chain to model the behavior of users navigating
    the web. SymPy provides a Python implementation for a finite discrete time-homogeneous
    Markov chain through the class `sympy.stats.DiscreteMarkovChain`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F03_UN01_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Markov chain—*p[AB]*, *p[BA]*, *p[BC]*, and *p[CB]* are transition probabilities
    between the states A, B, and C.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in figure 5.4, a new neighboring solution or state *x[k]* is
    always accepted if it’s an improving solution (i.e., *f*(*x[k]*) < *f*(*x[i]*)).
    An improving solution is a solution that gives a lower value for the objective
    function if we’re dealing with a minimization problem or gives a higher value
    in the case of a maximization problem. In the case of non-improving solutions,
    such as *x[j]*, the solution can still be probabilistically accepted as a way
    to avoid the risk of getting trapped in a local minimum. This contrasts with a
    greedy algorithm’s tendency to accept only improving solutions, making greedy
    algorithms more susceptible to getting stuck in local minima.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F04_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 Transition probability, assuming a minimization problem. As solution
    *x[k]* is an improving move, it is always accepted, and as solution *x[j]* is
    non-improving, it may be probabilistically accepted based on the transition probability.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature *T* appears in the transition probability and controls the exploration
    and exploitation in the search space. At high temperatures, non-improving moves
    will have a good chance of being accepted, but as the temperature decreases, the
    probability of accepting worse moves decreases. We’ll discuss this in more detail
    in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: The steps in SA can be summarized in the following pseudocode.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 5.1 The SA algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: SA has the advantages of ease of use and the ability to provide optimal or near-optimal
    solutions for a wide range of continuous and discrete problems. The main drawbacks
    of this algorithm are the need to tune many parameters and the occasional slow
    convergence of the algorithm to the optimal or near-optimal solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from this original SA algorithm—classical simulated annealing (CSA)—various
    variants have been proposed to improve the algorithm's performance. For example,
    fast simulated annealing (FSA) is a semi-local search and consists of occasional
    long jumps. Dual annealing is a stochastic global optimization algorithm that
    is useful for dealing with complex nonlinear optimization problems. It is based
    on the combined classical simulated annealing and fast simulated annealing algorithms.
    The generalized simulated annealing (GSA) algorithm uses a distorted Cauchy-Lorentz
    visiting distribution [2].
  prefs: []
  type: TYPE_NORMAL
- en: Quantum annealing (QA)
  prefs: []
  type: TYPE_NORMAL
- en: In quantum mechanics, a quantum particle is treated as an electromagnetic wave
    that can penetrate with a certain probability through a potential barrier. Due
    to the wave nature of matter on the quantum level, there is indeed some probability
    that a quantum particle can traverse such a barrier if the barrier is thin enough.
    This phenomenon is known as quantum tunneling. The quantum tunneling effect is
    a phenomenon whereby wave functions or particles can penetrate through a supposedly
    impassable barrier even if the total energy of the particle is less than the barrier
    height.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in the following figure, SA uses a thermal jump to push the search
    particle out of the local valley to avoid getting trapped in local minima. QA,
    on the other hand, searches an energy landscape to find an optimal or near-optimal
    solution by applying quantum effects. Instead of just walking through the landscape
    of the function, quantum annealing can tunnel through. This allows the algorithm
    to escape from local minima using quantum tunneling (tunnel effect) instead of
    the thermal jumps used in SA.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F04_UN02_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Simulated annealing versus quantum annealing
  prefs: []
  type: TYPE_NORMAL
- en: 'In QA, a number of candidate states are initialized with equal weights. Quantum-mechanical
    probability is used to change adiabatically and gradually the amplitudes of all
    states in parallel. For more information and an example of quantum annealers,
    see the D-Wave implementation: [https://docs.dwavesys.com/docs/latest/c_gs_2.html](https://docs.dwavesys.com/docs/latest/c_gs_2.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections explain the different components of the SA algorithm,
    starting with the transition probability that allows SA to accept or reject non-improving
    moves.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Acceptance probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike hill climbing (see section 4.3.1), SA probabilistically allows downward
    steps, controlled by the current temperature and how bad the move is. In SA, better
    moves are always accepted. As shown in figure 5.4, non-improving moves can be
    probabilistically accepted based on the Boltzmann-Gibbs distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In thermodynamics, a state at a temperature *t* has a probability of an increase
    in the energy magnitude Δ*E* given by the Boltzmann-Gibbs distribution as in equation
    5.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F04_UN02_Khamis-EQ01.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.1 |'
  prefs: []
  type: TYPE_TB
- en: where *k* is the Boltzmann constant, which is the proportionality factor that
    relates the average relative kinetic energy of particles in a gas with the thermodynamic
    temperature of the gas, and which has the value of 1.380,649 × 10^(-23) m² kg
    s^(-2) K^(-1). However, there’s no need to use this constant in a computational
    model that mimics the physical annealing process, so it’s replaced by 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the change in the energy can be replaced by the change in the objective
    function as a way to quantify the search progress toward the optimal or near-optimal
    state. So Δ*E* can be linked with the change of the objective function using equation
    5.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F04_UN02_Khamis-EQ02.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.2 |'
  prefs: []
  type: TYPE_TB
- en: where *γ* is a real constant. For simplicity, and without altering the core
    meaning, we can use *k* = 1 and *γ* = 1. Thus, the transition probability *p*
    simply becomes
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F04_UN02_Khamis-EQ03.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.3 |'
  prefs: []
  type: TYPE_TB
- en: where *T* is the temperature of the system. To determine whether or not we accept
    a change, we usually use a random number *r* in the interval [0,1] as a threshold.
    Thus, if *p* > *r*, or *p* = *e*^((–Δ)*^f* ^/*^T*^) > *r*, the move is accepted.
    Otherwise, the move is rejected.
  prefs: []
  type: TYPE_NORMAL
- en: If *P[ij]* is the probability of moving from point *x[i]* to *x[j]*, then *P[ij]*
    is calculated using
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F04_UN02_Khamis-EQ04a.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.4 |'
  prefs: []
  type: TYPE_TB
- en: The probability *P[ij]* is called transition or acceptance probability. Accepting
    non-improving moves probabilistically makes the algorithm able to avoid getting
    trapped in some local minima. If the acceptance probability is set to 0, SA behaves
    similarly to hill climbing, as it will only accept solutions that are better than
    the current one. Conversely, if the acceptance probability is set to 1, SA becomes
    more exploratory, as it will always accept worse solutions, making it more akin
    to a random search.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of accepting a worse state is a function of both the temperature
    of the system and the change in the cost function. As the temperature decreases,
    the probability of accepting worse moves decreases. Temperature can be seen as
    a parameter to balance the exploration and the exploitation in the search space.
    At high temperatures, the acceptance probability is high, which means that the
    algorithm accepts most of the moves to explore the parameter space. On the other
    hand, when the temperature is low, the acceptance probability is low, meaning
    that the algorithm restricts exploration. As shown in figure 5.5, if *T* = 0,
    no non-improving moves are accepted. In this case, SA is converted into hill climbing.
    As can be seen, the cooling process has an important effect on the search progress.
    The next section will present the different components of the cooling schedules
    used in SA.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F05_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 Change of acceptance probability with the temperature and the change
    in the objective function. The objective function change is the difference in
    the objective function’s value between the current solution and a candidate solution.
    In minimization problems, a positive objective function change indicates that
    the candidate solution is worse than the current solution. The acceptance probability
    gets lower as the objective function change increases. At high temperatures, SA
    tends to explore more by accepting non-improving moves. As the temperature gets
    lower, the algorithm restricts exploration, favoring exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: Given that the Boltzmann-based acceptance probability takes significant computational
    time (~1/3 of the SA computations), lookup tables or non-exponential probability
    formulas can be used instead. A lookup table can be generated by performing the
    exponential calculations offline only once for a range of values for changes in
    *f* and *T*. Other non-exponential probability formulas, such as *p*(Δ*f*) = 1
    – Δ*f*/*T*, can be used as an acceptance probability. This formula should be normalized
    to make sure that the maximum value is 1 and the minimum value is 0\.
  prefs: []
  type: TYPE_NORMAL
- en: In a computational model like SA, there is no need to strictly mimic the thermodynamic
    models that govern the physical annealing process. Figure 5.6 shows the difference
    between exponential and non-exponential acceptance probability functions. The
    code is available in the book’s GitHub repo. The difference between exponential
    and non-exponential acceptance probability functions is small for small changes
    in the objective function—you can experiment with this using the provided code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F06_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Exponential versus non-exponential acceptance probability
  prefs: []
  type: TYPE_NORMAL
- en: As temperature is part of the acceptance probability, it plays an important
    role in controlling the behavior of SA. The following subsection looks at how
    we can control the temperature to achieve a trade-off between exploration and
    exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 The annealing process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The annealing process in SA involves the careful control of temperature and
    the cooling rate, often called the *annealing schedule*. This process involves
    defining the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature decrement following a cooling schedule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of iterations at each temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is shown in figure 5.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 Annealing process parameters
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections provide in-depth information about each of these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Initial temperature
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the right initial temperature is crucially important. As shown
    in equation 5.4, for a given change Δ*f*
  prefs: []
  type: TYPE_NORMAL
- en: If *T* is too high (*T* → ∞), then *p* → 1, which means almost all the changes
    will be accepted and the algorithm will behave like a random search algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *T* is too low (*T* → 0), then any Δ*f* > 0 (worse solution assuming a minimization
    problem) will rarely be accepted as *p* → 0, and thus the diversity of the solution
    is limited, but any improvement (i.e., any Δ*f* < 0 in the case of a minimization
    problem) will almost always be accepted. In this case, SA behaves like a local
    search and may easily become trapped in local minima.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To find a suitable starting temperature, we can use any available information
    about the objective function. If we know the maximum change max(Δ*f*) of the objective
    function, we can use this to estimate an initial temperature *T[o]* for a given
    acceptance probability *p[o]* using equation 5.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Khamis-EQ05.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.5 |'
  prefs: []
  type: TYPE_TB
- en: 'If the potential maximum alteration of the objective function is unknown, we
    can use the following heuristic approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Initiate evaluations at a very high temperature to allow for nearly all changes
    to be accepted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the temperature quickly until roughly 50% to 60% of the inferior moves
    are accepted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use this temperature as the new initial temperature *T[o]* for proper and relatively
    slow cooling processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Temperature decrement
  prefs: []
  type: TYPE_NORMAL
- en: 'The cooling schedule is the rate at which the temperature is systematically
    decreased as the algorithm proceeds. This schedule is among the tunable parameters
    of SA. The following cooling schedules are commonly used:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Linear cooling schedule*—The temperature is decremented linearly using equation
    5.6:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Khamis-EQ06.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.6 |'
  prefs: []
  type: TYPE_TB
- en: where *T[o]* is the initial temperature, *i* is the pseudo time for iterations,
    and *β* is the cooling rate, which should be chosen in such a way that *T* → 0
    when *i* → *i[f]* (or the maximum number *N* of iterations). This usually gives
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Khamis-EQ07.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.7 |'
  prefs: []
  type: TYPE_TB
- en: This cooling schedule is simple and easy to implement, but may not be the best
    choice for all types of problems. Moreover, it requires prior knowledge or assumptions
    about the maximum number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Linear-inverse cooling schedule*—In linear-inverse cooling, the temperature
    decreases quickly at high temperatures and more gradually at low temperatures,
    as per equation 5.8\. In this equation, *α* is the cooling factor and should be
    between 0 and 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Khamis-EQ08.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.8 |'
  prefs: []
  type: TYPE_TB
- en: '*Geometric cooling schedule*—A geometric cooling schedule essentially decreases
    the temperature by a cooling factor 0 < *α* < 1 following equation 5.9:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Khamis-EQ09.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.9 |'
  prefs: []
  type: TYPE_TB
- en: The cooling process should be slow enough to allow the system to stabilize easily.
    In practice, *α* = 0.7 ~ 0.95 is commonly used. The higher the value of *α*, the
    longer it will take to reach the final (low) temperature. The main advantage of
    the geometric method is that *T* → 0 when *i* → ∞, and thus there is no need to
    specify the maximum number of iterations. Moreover, the geometric annealing schedule
    provides more gradual cooling, as shown in figure 5.8.
  prefs: []
  type: TYPE_NORMAL
- en: '*Logarithmic cooling schedule*—In this cooling schedule, the temperature is
    decreased logarithmically according to equation 5.10:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Khamis-EQ10.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.10 |'
  prefs: []
  type: TYPE_TB
- en: where *α* > 1. Theoretically, this cooling process asymptotically converges
    toward the global minimum. However, it requires prohibitive computing time.
  prefs: []
  type: TYPE_NORMAL
- en: '*Exponential cooling schedule*—In this cooling schedule, the temperature is
    decreased exponentially according to equation 5.11:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F07_Khamis-EQ11.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.11 |'
  prefs: []
  type: TYPE_TB
- en: where *α* is the cooling factor and *n* is the dimensionality of the model space.
    In this cooling process, the temperature is decreased very quickly during the
    first iterations, but the speed of the exponential decay is slowed down later
    and can be controlled using the cooling factor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F08_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 Different SA cooling schedules
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, these cooling schedules are all monotonically decreasing functions
    that don’t explicitly take into consideration how the search is progressing. In
    section 5.2.5, we’ll look at a nonmonotonic adaptive cooling schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Iterations at each temperature
  prefs: []
  type: TYPE_NORMAL
- en: Before applying the cooling schedule (i.e., decreasing the temperature), it
    is important to allow a sufficient number of iterations at each temperature level
    to stabilize the system at that temperature. Typically, this is achieved by using
    a constant value. For example, the number of iterations at each temperature might
    be exponential to the problem size (e.g., the number of cities in TSP as a discrete
    problem or the dimensionality of a mathematical function in the case of continuous
    problems). However, this value can be altered dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: One way to accomplish this is by limiting the number of iterations during the
    exploration phase of the search at the beginning, when the temperature is high.
    For example, when the temperature is high, we could perform a small number of
    iterations at each temperature and then implement the cooling process. As the
    search continues and the temperature decreases, we can shift toward exploitation
    by conducting a larger number of iterations at lower temperatures.
  prefs: []
  type: TYPE_NORMAL
- en: Final temperature
  prefs: []
  type: TYPE_NORMAL
- en: 'It is usual to let the temperature decrease until it reaches zero. However,
    this can make the algorithm run a lot longer, especially when certain cooling
    schedules, such as geometric cooling, are used. In reality, it is not necessary
    to let the temperature reach zero if the chances of accepting a non-improving
    move at the current temperature are almost the same as if the temperature were
    zero. Therefore, the stopping criteria can be either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A suitably low temperature (*T[f]* = 10^(–10) ~ 10^(–5))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the system reaches a “frozen” or minimum energy state (assuming a minimization
    problem), where neither better nor worse moves are accepted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.2.5 Adaptation in SA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several parameters in SA can be used to make the algorithm more adaptive to
    the search’s progress. The initial temperature, the cooling schedule, and the
    number of iterations per temperature are the most critical of these parameters.
    Other components include the cost function, the method of generating neighborhood
    solutions, and the acceptance probability.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in figure 5.9, the initial temperature can be used to control
    the exploration and exploitation behavior of SA. A high temperature leads to a
    high level of exploration, and a low temperature results in exploitative behavior
    (i.e., restricting the search around neighbors).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F09_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 Effect of temperature in SA. High temperatures result in more exploration,
    whereas a low temperature restricts the exploration and leads to more exploitation
    in the search space.
  prefs: []
  type: TYPE_NORMAL
- en: You can think about it in terms of the movement of molecules. Assume that the
    molecule is the search agent. At high temperatures, the molecule moves freely
    in the search space, exploring different solutions. At low temperatures, the movement
    of the molecule becomes limited, so the exploration is restricted, and the search
    agent focuses on a specific part of the search space. With high temperatures at
    the beginning of the search, SA oscillates due to the exploration behavior that
    makes the algorithm accept non-improving moves with high probability. As the search
    progresses and the temperature gets lower, the algorithm starts to stabilize due
    to the exploitation behavior that makes the algorithm accept fewer non-improving
    moves and instead focus on the elite improving solutions.
  prefs: []
  type: TYPE_NORMAL
- en: It is always recommended that you start with a high temperature and gradually
    decrease it as the search progresses. However, the right initial temperature is
    problem-dependent. You can try different values and see which leads to better
    solutions. Some researchers suggest doing this adaptively, using other search
    methods or metaheuristics, such as a genetic algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cooling schedules can also be used to make the algorithm more adaptive. Different
    cooling schedules can be used in different phases of the search, taking into consideration
    that most useful work is done in the middle of the schedule. Reheating can also
    be tried if no progress is observed. Cooling may take place every time a move
    (or a specific number of moves) is accepted. A nonmonotonic adaptive cooling schedule
    can be tried, where an adaptive factor is used, based on the difference between
    the current solution objective and the best objective achieved by the algorithm
    up to that moment, according to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F09_Khamis-EQ12.png)'
  prefs: []
  type: TYPE_IMG
- en: '| 5.12 |'
  prefs: []
  type: TYPE_TB
- en: where *T* is the system temperature at each state transition, *T*(*i*) is the
    current temperature, *f[i]* is the value of the objective function at iteration
    *i*, and *f*^* is the best value of the objective function obtained so far.
  prefs: []
  type: TYPE_NORMAL
- en: Another adaptation parameter is the number of iterations per temperature. This
    number can be adaptively changed by allowing a small number of iterations at high
    temperatures and allowing a large number of iterations at low temperatures to
    fully explore the local optimum.
  prefs: []
  type: TYPE_NORMAL
- en: The adaptation ability of the SA algorithm can be also influenced by the objective
    function and the representation of problem constraints utilized. As a general
    rule, it is advisable to steer clear of cost functions that yield the same result
    for multiple states (e.g., the number of edges incorporated in a TSP route). This
    type of function does not guide the search because it may not change in the objective
    function from one state to another.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine two feasible routes with the same number of edges (i.e.,
    Δ*f* = 0). Counting on the number of edges as a cost function wouldn’t be a good
    idea. However, many problems have constraints that can be represented in the cost
    function using reward or penalty terms. One way to make the algorithm more adaptive
    is to dynamically change the weighting of the reward and penalty terms. In the
    initial phase, the constraints can be relaxed more than in the advanced phases
    of the search.
  prefs: []
  type: TYPE_NORMAL
- en: 'There have been numerous efforts to make the selection and control of SA parameters
    totally adaptive. One example of such an effort was proposed by Ingber in “Adaptive
    simulated annealing (ASA): Lessons learned” [3]. ASA automatically adjusts the
    algorithm parameters that control the temperature schedule, requiring the user
    to only specify the cooling rate. The method uses a linear random combination
    of previously accepted steps and parameters to estimate new steps and parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: An ASA algorithm with greedy search (ASA-GS) is proposed by Geng et al. to solve
    the TSP [4]. ASA-GS is based on the classical SA algorithm and utilizes a greedy
    search technique to speed up the convergence rate. ASA utilizes dynamic adjustments
    in parameters like the temperature cooling coefficient, greedy search iterations,
    compulsive accept instances, and probability of accepting a new solution. These
    adaptive parameter controls aim to enhance the trade-off between quality and time
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: SA finds extensive application across various domains. Its utility extends to
    solving diverse optimization problems encompassing nonlinear function optimization,
    TSP, academic course scheduling, network design, task allocation, circuit partitioning
    and placement, robot motion planning, and vehicle routing, as well as resource
    allocation and scheduling. The following sections show how you can use SA to solve
    continuous and discrete optimization problems in different domains.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Function optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As an example of continuous optimization problems, let’s consider the following
    simple function optimization problem: find *x* which minimizes *f*(*x*) = (*x*
    – 6)², subject to the constraint 0 ≤ *x* ≤ 31.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start with an initial random solution from the range of *x*. Different
    neighboring solutions can be generated by adding a random floating value chosen
    from a Gaussian or normal distribution with a given mean and standard deviation.
    The `random.gauss()` function in Python can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Assume that the initial temperature *T*[0] = 5, the number of iterations per
    temperature is 2, and the geometric cooling factor *α* = 0.85. Let’s carry out
    a few hand iterations to show how SA can solve this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Initialization*—An initial solution is randomly generated, and its cost is
    evaluated as follows: *x* = 2 and *f*(2) = 16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Iteration 1*—A new solution *x* = 2.25 is generated by adding a random value
    from a Gaussian distribution using the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*f*(2.25) = 14.06 is an improving solution and so is accepted.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Iteration 2*—A new solution is generated by adding a random value to the last
    accepted solution from the previous iteration. The new solution *x* =2.25 – 1.07
    = 1.18, *f*(1.18) = 23.23 is a non-improving solution, so the acceptance probability
    must be calculated: *p* = *e*^(–Δf /T) = *e*^(–(23.23 – 14.06)/5) = 0.1597. We
    generate a random number *r* between (0,1), and let’s say it was *r* = 0.37. As
    *p* ≯ *r*, we reject this solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Iteration 3*—We update the temperature because we have used the initial temperature
    *T[0]* = 5 for two iterations so far. Following geometric cooling, the new temperature
    is *T[1]* = *T[o] α^i* = 5*0.85¹ = 4.25. We’ll use this value for two iterations
    starting with this iteration. We’ll now generate a new solution based on the last
    accepted solution by adding a random value from a Gaussian distribution. The new
    solution is *x* = 2.25 + 1.57 = 3.82 with *f*(3.82) = 4.75. This is an improving
    solution, so it’s accepted, and the search continues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SciPy provides Python implementations for SA algorithms and other algorithms
    to handle mathematical optimization problems. `scipy.optimize.anneal` is deprecated
    in SciPy, and a `dual_annealing()` function is available instead. The following
    listing shows the Bohachevsky function (which has the formula *f*(*x[1]*,*x[2]*)
    = *x[1]*² + 2*x[2]*² – 0.3cos(3*πx[1]*) – 0.4cos(3*πx[2]*) + 0.7) solution using
    the SciPy dual annealing algorithm. The complete listing is available in the book’s
    GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Function optimization using `scipy.optimize.dual_annealing`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Define the objective function or functions (e.g., Bohachevsky function).
  prefs: []
  type: TYPE_NORMAL
- en: ② Define the boundary constraints of the decision variables.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Perform the dual annealing search.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Print the dual annealing solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'MEALPY is another Python library that provides implementations of different
    nature-inspired metaheuristic algorithms (see appendix A for more details). As
    a continuation, the following code shows the Bohachevsky function solution using
    MEALPY SA (the complete version of listing 5.1 is available in the book’s GitHub
    repo):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Define the problem
  prefs: []
  type: TYPE_NORMAL
- en: ② Define the MEALPY algorithm parameters to perform an SA search using MEALPY.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Define a MEALPY SA solver.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Solve the problem using a defined solver.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Print the MEALPY SA solution.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 shows the Bohachevsky function’s solution. The performance of the
    algorithm depends mainly on its parameter tuning and stopping criteria. MEALPY
    runs a parallel version of SA and exposes many parameters to be tuned.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F10_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 Solution of a continuous function optimization problem using SA.
    The cross in the center is the optimal solution. The triangle is the solution
    obtained by MEALPY SA. The dot is the SciPy dual annealing solution.
  prefs: []
  type: TYPE_NORMAL
- en: Note Appendix A shows how to use SA in other Python packages to solve mathematical
    optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement an SA algorithm from scratch so we can gain more control and
    better handle different types of continuous and discrete optimization problems.
    In our implementation in the optalgotools package, we decouple the problem definition
    from the solver so we can use the solvers to handle different problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply our implementation to find the global minimum of the aforementioned
    simple function optimization problem and of more complex function optimization
    problems as well. There are several complex mathematical functions in multidimensional
    space, such as Rosenbrock’s function, the Ackley function, the Rastrigin function,
    the Schaffer function, the Schwefel function, the Langermann function, the Levy
    function, the Bukin function, the Eggholder function, the cross-in-tray function,
    the drop wave function, and the Griewank function. Examples of function optimization
    test problems and datasets can be found in appendix B.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 5.2 shows how SA can be used to solve the following mathematical functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A simple quadratic equation*—This is what we used in the hand iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Bohachevsky function (global minimum 0)*—This is a 2D unimodal function
    with a bowl shape. This function is known to be continuous, convex, separable,
    differentiable, nonmultimodal, nonrandom, and nonparametric, so derivate-based
    solvers can efficiently handle it. Note that a function whose variables can be
    separated is known as a *separable function*. Nonrandom functions contain no random
    variables. Nonparametric functions assume that the data distribution cannot be
    defined in terms of a finite set of parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Bukin function*—This function has many local minima, all of which lie
    on a ridge, and one global minimum *f*(*x[0]*) = 0 at *x[0]* = *f*(−10,1). This
    function is continuous, convex, nonseparable, nondifferentiable, multimodal, nonrandom,
    and nonparametric. This requires a derivative-free solver (also known as a black-box
    solver) such as SA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Gramacy & Lee function*—This is a 1D function with multiple local minima
    and local and global trends. This function is continuous, nonconvex, separable,
    differentiable, nonmultimodal, nonrandom, and nonparametric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Griewank 1D, 2D, and 3D functions*—These have many widespread local minima.
    These functions are continuous, nonconvex, separable, differentiable, multimodal,
    nonrandom, and nonparametric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our implementation, these are the SA parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A maximum number of iterations: `max_iter=1000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maximum iterations per temperature: `max_iter_per_temp=100`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An initial temperature: `initial_temp=1000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A final temperature: `final_temp=0.0001`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A cooling schedule: `cooling_schedule=''geometric''` (available options: `''linear''`,
    `''geometric''`, `''logarithmic''`, `''exponential''`, `''linear_inverse''`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A cooling factor: `cooling_alpha=0.9`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A debug option: `debug=1` (`debug=1` prints the initial and final solution;
    `debug=2` provides hand-iteration tracing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feel free to change these settings and observe their effect on the performance
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 Continuous function optimization using SA
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Quadratic function SA-based solution
  prefs: []
  type: TYPE_NORMAL
- en: ② Bohachevsky SA-based solution
  prefs: []
  type: TYPE_NORMAL
- en: ③ Bukin SA-based solution
  prefs: []
  type: TYPE_NORMAL
- en: ④ Gramacy & Lee SA-based solution
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Griewank 1D SA-based solution
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Griewank 2D SA-based solution
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Griewank 3D SA-based solution
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of the output for the Bukin function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen, SA is able to handle different multidimensional, nonlinear function
    optimization problems. This stochastic global optimization algorithm is able to
    adapt to the landscape of the objective function and avoid getting trapped in
    local minima. However, in the case of multidimensional functions such as Griewank
    2D and 3D, SA takes time to converge. The following sections show how SA can handle
    discrete problems such as Sudoku and TSP.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Solving Sudoku
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sudoku, also known as Su Doku, is one of the most popular number puzzles of
    all time. This game is adapted from a mathematical concept called a *Latin square*.
    The first version of the Sudoku puzzle was created by a retired architect named
    Howard Garns and appeared in the late 1970s as a puzzle in *Dell Pencil Puzzles
    and Word Games*. The game subsequently showed up in Japan in 1984 under the name
    “Sudoku,” which is abbreviated from the Japanese “Sūji wa dokushin ni kagiru,”
    which means the numbers (or digits) must remain single. Nowadays, Sudoku games
    are popular around the globe and are published in game websites, puzzle booklets,
    and newspapers.
  prefs: []
  type: TYPE_NORMAL
- en: The Sudoku game can be seen as a constraint-satisfaction problem (CSP) that
    is solved by correctly filling a 9 × 9 grid with digits so that each column, each
    row, and each of the nine 3 × 3 subgrids (aka “boxes,” “blocks,” or “regions”)
    contain all of the digits from 1 to 9\. Any row or column or 3 × 3 subgrid shouldn't
    contain more than one of the same number from 1 to 9\.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from entertainment, Sudoku is used in real-life applications such as developmental
    psychology and steganography. For example, several studies have showed that solving
    Sudoku or crosswords or other brain games may help in keeping your brain 10 years
    younger and can slow down the progression of conditions such as Alzheimer’s. Sudoku
    can also be used as a tool to improve problem-solving skills, critical thinking,
    and attention. Finally, steganography is the technique of hiding images, messages,
    files, or other secret data within something that isn’t a secret. In secret data
    delivery applications, digital images can be used to conceal secret data. The
    Sudoku puzzle is then used to modify selected pixel pairs in the cover image,
    based on a specially designed reference matrix, to insert secret digits.
  prefs: []
  type: TYPE_NORMAL
- en: Latin square
  prefs: []
  type: TYPE_NORMAL
- en: Latin squares were devised in the 10th century by Arabic numerologists who dealt
    with the mystical power of numbers. Islamic amulets, known as wafq majazi, from
    the 13th century have been found, and they were sketched in the margins of a 16th
    century Arabic medical text. The name “Latin” was inspired by the famous Swiss
    mathematician Leonhard Euler (1707–1783) who used Latin letters as symbols in
    the squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *Latin square* is an *n* × *n* array filled with *n* different numbers, symbols,
    or colors arranged in such a way that no orthogonal (row or column) contains the
    same number, symbol, or color twice. An example of a 4 × 4 Latin square is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F10_UN03_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Latin squares are different from *magic squares*. A magic square is a square
    array of positive integers 1, 2, ..., *n*² arranged such that the sum of the *n*
    numbers in any horizontal, vertical, or main diagonal line is always the same
    number. Sudoku is based on Latin squares. In fact, any solution to a Sudoku puzzle
    is a Latin square. KenKen and KenDoku are other number puzzles based on an enhanced
    version of Latin squares and require some degree of arithmetic skills.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, the search space of Sudoku is huge. There are 6.671 × 10^(21)
    possible solvable Sudoku grids that yield a unique result [5]. According to Encyclopedia
    Britannica, if each human on earth solves one Sudoku puzzle every second, they
    wouldn’t get through all of them until about the year 30,992\. However, taking
    out symmetries, such as rotations, reflections, permuting columns and rows, and
    swapping digits, the number of essentially different Sudoku grids is reduced to
    5,472,730,538 ≈5.473 × 10⁹ [6]. The generalized *n* × *n* Sudoku problem is an
    NP-complete problem. However, some instances, such as standard 9 × 9 Sudoku, are
    not NP-complete. Constant-time algorithms exist to solve some instances of 9 ×
    9 Sudoku, in *O*(1) time, as each and every 9 × 9 Sudoku can be listed, enumerated,
    and indexed in a finite dictionary or a lookup table used to find a solution.
    However, these algorithms cannot handle arbitrary generalized *n* × *n* Sudoku
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Backtracking, dancing links, and Crook’s pencil-and-paper are common algorithms
    for solving Sudoku, especially if the size of the problem is small. *Backtracking*
    is mainly a classical depth-first search that tests a whole branch until that
    branch violates the rules or returns a solution.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dancing links* (DLX), invented by Donald Knuth in 2000, uses algorithm X to
    solve Sudoku puzzles, handled as exact cover problems. In the exact cover problem,
    given a binary matrix (i.e., a matrix composed only of 0 and 1), it is necessary
    to find a set of rows containing exactly one 1 in each column. Algorithm X, a
    recursive search algorithm, is applied to solve the exact cover problem using
    the backtracking method.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Crook’s pencil-and-paper algorithm*, all possible numbers in each cell are
    listed. This list of numbers is called marking-up of the cell. We then try to
    find out if there is a row, column, or block with only one possible value throughout
    the row, column, or block. Once found, we fill in this cell with this number and
    update the markups in any affected row, column, or box. The next step is to find
    preemptive sets. As described in Crook’s paper, a preemptive set is composed of
    numbers from the set [1,2,…,9] and is a set of size *m*, 2 ≤ *m* ≤ 9, whose numbers
    are potential occupants of *m* cells exclusively, where exclusively means that
    no other numbers in the set [1,2,…,9], other than the members of the preemptive
    set, are potential occupants of those *m* cells. The last step is to eliminate
    possible numbers outside preemptive sets.
  prefs: []
  type: TYPE_NORMAL
- en: Backtracking
  prefs: []
  type: TYPE_NORMAL
- en: Backtracking algorithms are commonly used to solve search and optimization problems
    by recursion. The backtracking algorithm builds a feasible solution or a set of
    feasible solutions incrementally. Given a 9 × 9 Sudoku board, the algorithm visits
    all the empty cells following depth-first traversal order, filling the digits
    incrementally, and it backtracks when a number is not found to be valid. The following
    figure illustrates the backtracking algorithm steps for a 9 × 9 Sudoku puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F10_UN04_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Backtracking steps for a 9 × 9 Sudoku puzzle
  prefs: []
  type: TYPE_NORMAL
- en: The next listing shows how a 9 × 9 Sudoku is solved using the SA algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 Solving Sudoku using SA
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① Import the SA solver.
  prefs: []
  type: TYPE_NORMAL
- en: ② Import a Sudoku problem.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Create a SA solver with the selected parameters.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Create a hard 9 × 9 Sudoku (available variants include trivial, easy, medium,
    hard, and evil).
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Solve the Sudoku using the backtracking algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Solve the Sudoku using SA.
  prefs: []
  type: TYPE_NORMAL
- en: You can try different variants of Sudoku by changing the puzzle configuration.
    In easy Sudoku problems, cells contain more prefilled numbers than medium or hard
    ones. Evil Sudoku is the highest level of puzzle difficulty. Table 5.2 compares
    the time it takes for SA, backtracking, and the Python Linear Programming (PuLP)
    library to solve different instances of 9 × 9 Sudoku puzzles. PuLP provides linear
    and mixed programming solvers. The default solver used in PuLP is Cbc (COIN-OR
    branch and cut), which is an open source solver for mixed integer linear programming
    problems. More information about PuLP is available in appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.2 SA versus backtracking versus PuLP in solving a 9 × 9 Sudoku puzzle
  prefs: []
  type: TYPE_NORMAL
- en: '| Time to find the solution(s) | Trivial | Easy | Medium | Hard | Evil |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Backtracking | 0.01 | 0.01 | 0.11 | 0.69 | 1.58 |'
  prefs: []
  type: TYPE_TB
- en: '| PuLP | 0.69 | 0.12 | 0.11 | 0.13 | 0.12 |'
  prefs: []
  type: TYPE_TB
- en: '| Classical SA | 0.10 | 0.07 | 0.01 | 3:17 | Suboptimal in 3:16 |'
  prefs: []
  type: TYPE_TB
- en: As you can see, classical SA does not outperform the backtracking approach and
    is much slower in the case of hard and evil instances of Sudoku problems. PuLP
    is efficiently able to handle different variants of Sudoku in a consistent time,
    compared to backtracking and SA.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of evil Sudoku, SA converges to a suboptimal solution despite trying
    different parameter settings. Given that this is a constraint-satisfaction problem,
    the notion of suboptimality is not valid, as a suboptimal solution is an invalid
    solution. This means that SA doesn’t manage to solve the evil instance of Sudoku.
    Being a well-structured problem, 9 × 9 Sudoku with different levels of difficulty
    can be easily solved using a backtracking algorithm. Generally speaking, if the
    problem is well-structured with a well-known algorithm solution, metaheuristics
    approaches don’t usually outperform these classical and more deterministic approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Solving TSP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As described in section 2.1.1, the traveling salesman problem (TSP) is used
    as a platform for the study of general methods that can be applied to a wide range
    of discrete optimization problems. Consider solving the instance of TSP shown
    in figure 5.11 using SA. In this TSP, a traveling salesman must visit five cities
    and return home, making a loop (a round trip).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F11_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 TSP for five cities—there are 5!/2 = 60 possible tours, assuming
    symmetric TSP. The weights on the edges of the graph represent the travel distances
    between the cities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume the following values: initial temperature = 500, final temperature =
    50, a linear decrement rate of 50, and one iteration at each temperature. A TSP
    solution takes the form of a permutation as follows: Solution = [ 1, 3, 4, 2,
    5]. The objective function is the total distance of the route. Swapping is a suitable
    operator that can be used to generate neighboring solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Iteration 0*—The initial solution is Solution = [ 1, 3, 4, 2, 5], cost = 2
    + 4 + 5 + 5 + 12 = 28, as shown in figure 5.12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F12_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 SA iteration 0 of a 5-city TSP
  prefs: []
  type: TYPE_NORMAL
- en: '*Iteration 1*—To generate a candidate solution, select two random cities (e.g.,
    2 and 3), and swap them. This results in a new solution [ 1, 2, 4, 3, 5] with
    a cost of 35 (figure 5.13).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F13_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 SA iteration 1 of a 5-city TSP
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the new solution has a longer tour length, it will be conditionally accepted,
    according to a probability of *p* = *e*^–^Δ*^f* ^/*^T* = *e*^(–(35–28) /)*^T*
    = *e*^(–7 /)*^T* (at higher temperatures, there’s a higher probability of acceptance).
    We pick a random value *r* within 0 and 1\. If *P* > *r*, we accept this solution.
    Otherwise, we reject this solution. Assuming the new solution was not accepted,
    we generate a different one, starting from the initial solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Iteration 2*—A solution is generated by swapping cities 2 and 5 in the initial
    solution. The tour length of the candidate solution is 18 (figure 5.14).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F14_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 SA iteration 2 of a 5-city TSP
  prefs: []
  type: TYPE_NORMAL
- en: Since this solution has a shorter tour length, it will be accepted, and the
    search continues until the termination criteria are met. The following listing
    shows an SA solution for this simple TSP problem.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 Solving TSP using SA
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① Create an instance of a TSP problem.
  prefs: []
  type: TYPE_NORMAL
- en: ② Create an instance of the SA solver.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Run the SA solver, and show the results in each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now consider some benchmark instances of TSP, such as Berlin52 from TSPLIB
    ([http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/](http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/)).
    This dataset contains 52 locations in the city of Berlin. The shortest route obtained
    for the Berlin52 dataset is 7,542\. The next listing shows how we can solve this
    TSP instance using our SA implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 Solving the Berlin52 TSP using SA
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Permanent URL for the Berlin52 dataset
  prefs: []
  type: TYPE_NORMAL
- en: ② Create a TSP object for Berlin52.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Create an SA model.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Run SA, and evaluate the best solution distance.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Plot the route.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Figure 5.15 shows the route generated by SA for Belin52 TSP.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F15_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 Solution of Belin52 using SA
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the near-optimal solution found by SA is 8,106.88\. This value
    is a bit higher than the best-known solution for the Berlin52 TSP, which is 7,542\.
    Parameter tuning and algorithm adaptation can help improve the results. For example,
    Geng et al.’s “Solving the traveling salesman problem based on an adaptive simulated
    annealing algorithm with greedy search” paper discusses using three kinds of mutations
    (vertex insert mutation, block insert mutation, and block reverse mutation) with
    different probabilities during the search to improve the accuracy of SA in solving
    TSP problems. Moreover, parameters such as the cooling coefficient of the temperature,
    the times of greedy search used to speed up the convergence rate, the times of
    compulsive accept, and the probability of accepting a new solution, can be adapted
    according to the size of the TSP instances. An implementation of this adaptive
    algorithm is available from this GitHub repo: [https://github.com/ildoonet/simulated-annealing-for-tsp](https://github.com/ildoonet/simulated-annealing-for-tsp).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The effect of the algorithm’s parameters can be studied, such as the initial
    temperature, the cooling schedule, the number of iterations per temperature, and
    the final temperature. For example, SA was applied for the Berlin52 TSP instance
    with the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum number of iterations = 1200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of iterations per each temperature T = 500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T*[initial]* = 150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T*[final]* = 0.01
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear cooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our implementation supports the following methods for mutating a new solution
    from an old one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`random_swap`—Swap two cities in the path. This can be done multiple times
    for the same solution by using `num_swaps`. Also, the swap can be done in a smaller
    window of the whole path using `swap_wind = [1 - n]`. For example, suppose the
    route is [A, B, C, D, F]. Swapping two random cities, like B and F, will result
    in a new route [A, F, C, D, B].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reverse`—Reverse the order of a subset of the cities with either a random
    length, using `rand_len`, or using `rev_len`, which has a default of 2\. For example,
    starting with the solution [A, B, C, D, F], if we apply `reverse` with length
    3, we can get a new solution [A, D, C, B, F].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`insert`—Pick a random city, remove it from the path, and reinsert it before
    a different random city. For example, starting from the solution [A, B, C, D,
    F], we could pick city B and insert it before city F so we get a new solution
    [A, C, D, B, F].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mutate`—Randomly pick a number of consecutive cities from the current solution,
    and shuffle them. For example, starting from the solution [A, B, C, D, F], we
    may pick C, D, F and shuffle them so we get a new solution [A, B, F, C, D].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The implementation also supports two methods of initializing the path:'
  prefs: []
  type: TYPE_NORMAL
- en: '`random`—This means the path is generated completely randomly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`greedy`—This tries to select a possibly suboptimal initial path by selecting
    the pairwise shortest distances between cities. This will not lead to the shortest
    path, but it may be better than the random initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is worth noting that the results of the SA algorithm may not be exactly
    repeatable. Due to the randomness included in the algorithm, each time you run
    the algorithm, you may get slightly different results. To avoid this, the `run`
    function included in the `SimulatedAnnealing` class contains a `repetition` argument
    that allows you to report the best solution generated out of multiple runs, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can set repetition to be 10, so the algorithm reports the best solution
    generated out of 10 runs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Solving a delivery semi-truck routing problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider a more real-life example of TSP. Assume that Walmart Supercenters
    are points of interest (POIs) to be visited by a delivery semi-truck. The vehicle
    will start from Walmart Supercenter number 3001, located at 270 Kingston Rd. E
    in Ajax, Ontario. It is required to find the shortest possible route the truck
    can follow to visit each POI only once and get back to the home location. There
    are 18 Walmart Supercenters in the selected part of the Greater Toronto Area (GTA),
    as shown in figure 5.16\. This results in 18! possible routes to visit these stores
    located in Durham Region, York Region, and Toronto, Ontario.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F16_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 Selected Walmart Supercenters in the Greater Toronto Area (GTA)
  prefs: []
  type: TYPE_NORMAL
- en: The GPS coordinates (longitude and latitude) of each POI and the addresses are
    available on the POI Factory website ([www.poi-factory.com/node/25560](http://www.poi-factory.com/node/25560))
    and are included in the Walmart_United States&Canada.csv file that can be downloaded
    for free (for noncommercial use) after registration. Google Places API, Here Places
    API, and SafeGraph on ArcGIS Marketplace can also be used to get data about points
    of interest such as hospitals, restaurants, retail stores, and grocery stores.
    Appendix B provides more details about open data sources.
  prefs: []
  type: TYPE_NORMAL
- en: The OSMnx library is used in this example to create a NetworkX graph that represents
    the supercenter locations. Pyrosm can also be used instead of OSMnx. The shortest
    distances between these locations are computed using the NetworkX built-in function
    `shortest_path`, which uses Dijkstra’s algorithm as a default method (see section
    3.4.1). Supercenter locations are rendered on OpenStreetMap based on their GPS
    coordinates using the folium library. Appendix A provides more details about these
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In our implementation, the problem solver is decoupled from the problem object.
    We start by creating a TSP object for this discrete problem. We then create an
    SA object to solve the TSP problem. An initial solution is generated using the
    `mutate` method. As shown in figure 5.17, this initial solution is far from optimal.
    The total length of the initial route is 593.88 km, and this route is not convenient
    or easy to follow in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F17_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 Initial solution for the Walmart delivery semi-truck route with
    a total distance of 593.88 km
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run SA with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum number of iterations = 10000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum interaction per temperature = 100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial temperature = 85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final temperature = 0.0001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear cooling schedule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other cooling schedules could also be used. For example, geometric cooling can
    generate consistent, superior-quality, and timely solutions compared to other
    schemes. However, this is one of the algorithm parameters that can be tuned, as
    it sometimes depends on the nature of the problem. Figure 5.18 shows the shortest
    route with a total distance of 227.17 km.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F18_Khamis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 SA solution for the Walmart delivery semi-truck route with a total
    distance of 227.17 km
  prefs: []
  type: TYPE_NORMAL
- en: The next listing is a snippet of the code used to generate the shortest route
    for the delivery semi-truck using SA. The complete code is available in the book’s
    GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Generating a Walmart delivery semi-truck route using SA
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Load a list of all Walmart locations in Ontario.
  prefs: []
  type: TYPE_NORMAL
- en: ② Select cities that are in Durham Region, York Region, or Toronto.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Select Walmart stores that are in the preceding list and are Supercenters.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Get the lat. and long. locations of the preceding set of Walmarts, and create
    a graph of roads that connects them and that is within 42 km.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Calculate the distances between the Walmart locations using the graph.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Create a TSP object for the problem.
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Create an SA object to help solve the TSP problem.
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Get an initial random solution, and check its length
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Run SA, and evaluate the best solution distance.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we separated the solver class from the problem object in optalgotools.
    The solver is imported from `algorithms`, and the problem is an instance of the
    TSP problem in the `problems` class. This implementation allows you to change
    the problem instances and tune the parameters of the algorithm to reach an optimal
    or near-optimal solution. You may consider trying the adaptation aspects of SA
    explained in section 5.2.5 to figure out their effect on the algorithm’s performance
    in terms of the length of the obtained route and the run time (CPU time and wall-clock
    time).
  prefs: []
  type: TYPE_NORMAL
- en: A metaheuristic algorithm like SA seeks optimal or near-optimal solutions at
    a reasonable computational cost, but it cannot guarantee either their feasibility
    or degree of optimality. With proper parameter tuning, the algorithm can provide
    acceptable solutions without further postprocessing. In the next chapter, we will
    discuss tabu search as another trajectory-based optimization algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metaheuristic algorithms can be broadly classified into trajectory-based algorithms
    and population-based algorithms. A trajectory-based metaheuristic algorithm, or
    S-metaheuristic, uses a single search agent that moves through the design or search
    space in a piecewise style. Population-based algorithms, or P-metaheuristics,
    use multiple agents to search for an optimal or near-optimal global solution.
    Simulated annealing is a trajectory-based metaheuristic algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulated annealing mimics the annealing process in material processing, where
    a metal cools and freezes into a crystalline state with the minimum energy and
    larger crystal size so as to reduce the defects in metallic structures. The annealing
    process involves the careful control of temperature and cooling rate, often called
    the annealing schedule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulated annealing runs a series of moves under different thermodynamic conditions
    and always accepts improving moves and can probabilistically accept non-improving
    moves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The acceptance probability is proportional to the temperature. A high temperature
    increases the chance of accepting non-improving moves to favor exploration of
    the search space at the beginning of the search. As the search progresses, the
    temperature is decremented to restrict exploration and favor exploitation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the temperature goes to zero, SA acts greedily like hill climbing, and as
    the temperature goes to infinity, SA behaves like a random walk. The temperature
    should decrease gradually to achieve the best trade-off between exploration and
    exploitation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulated annealing is a stochastic search algorithm and a derivative-free solver
    that can be used when derivative information is unavailable, unreliable, or prohibitively
    expensive. SA seeks optimal or near-optimal solutions at a reasonable computational
    cost but it cannot guarantee either the feasibility or degree of optimality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive simulated annealing can dynamically change its parameters with the
    search progress to control the exploration and exploitation behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulated annealing is an easy-to-implement probabilistic approximation algorithm
    that can be used to solve continuous and discrete problems in different domains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
