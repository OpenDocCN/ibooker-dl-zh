- en: Chapter 2\. Introduction to Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this chapter is to introduce the basic principles of deep learning.
    If you already have lots of experience with deep learning, you should feel free
    to skim this chapter and then go on to the next. If you have less experience,
    you should study this chapter carefully as the material it covers will be essential
    to understanding the rest of the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In most of the problems we will discuss, our task will be to create a mathematical
    function:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi
    mathvariant="bold">x</mi> <mo>)</mo></mrow></math>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that <math><mi mathvariant="bold">x</mi></math> and <math><mi mathvariant="bold">y</mi></math>
    are written in bold. This indicates they are vectors. The function might take
    many numbers as input, perhaps thousands or even millions, and it might produce
    many numbers as outputs. Here are some examples of functions you might want to
    create:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: <math><mi mathvariant="bold">x</mi></math> contains the colors of all the pixels
    in an image. <math><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>
    should equal 1 if the image contains a cat and 0 if it does not.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same as above, except <math><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>
    should be a vector of numbers. The first element indicates whether the image contains
    a cat, the second whether it contains a dog, the third whether it contains an
    airplane, and so on for thousands of types of objects.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math><mi mathvariant="bold">x</mi></math> contains the DNA sequence for a chromosome.
    <math><mi mathvariant="bold">y</mi></math> should be a vector whose length equals
    the number of bases in the chromosome. Each element should equal 1 if that base
    is part of a region that codes for a protein, or 0 if not.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '<math><mi mathvariant="bold">x</mi></math> describes the structure of a molecule.
    (We will discuss various ways of representing molecules in later chapters.) <math><mi
    mathvariant="bold">y</mi></math> should be a vector where each element describes
    some physical property of the molecule: how easily it dissolves in water, how
    strongly it binds to some other molecule, and so on.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, <math><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>
    could be a very, very complicated function! It usually takes a long vector as
    input and tries to extract information from it that is not at all obvious just
    from looking at the input numbers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The traditional approach to solving this problem is to design a function by
    hand. You would start by analyzing the problem. What patterns of pixels tend to
    indicate the presence of a cat? What patterns of DNA tend to distinguish coding
    regions from noncoding ones? You would write computer code to recognize particular
    types of features, then try to identify combinations of features that reliably
    produce the result you want. This process is slow and labor-intensive, and depends
    heavily on the expertise of the person carrying it out.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning takes a totally different approach. Instead of designing a
    function by hand, you allow the computer to learn its own function based on data.
    You collect thousands or millions of images, each labeled to indicate whether
    it includes a cat. You present all of this training data to the computer, and
    let it search for a function that is consistently close to 1 for the images with
    cats and close to 0 for the ones without.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: What does it mean to ‚Äúlet the computer search for a function‚Äù? Generally speaking,
    you create a *model* that defines some large class of functions. The model includes
    *parameters*, variables that can take on any value. By choosing the values of
    the parameters, you select a particular function out of all the many functions
    in the class defined by the model. The computer‚Äôs job is to select values for
    the parameters. It tries to find values such that, when your training data is
    used as input, the output is as close as possible to the corresponding targets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Linear Models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the simplest models you might consider trying is a linear model:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <mi mathvariant="bold">Mx</mi>
    <mo>+</mo> <mi mathvariant="bold">b</mi></mrow></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, <math><mi mathvariant="bold">M</mi></math> is a matrix (sometimes
    referred to as the ‚Äúweights‚Äù) and <math><mi mathvariant="bold">b</mi></math> is
    a vector (referred to as the ‚Äúbiases‚Äù). Their sizes are determined by the numbers
    of input and output values. If <math><mi mathvariant="bold">x</mi></math> has
    length T and you want <math><mi mathvariant="bold">y</mi></math> to have length
    S, then <math><mi mathvariant="bold">M</mi></math> will be an S √ó T matrix and
    <math><mi mathvariant="bold">b</mi></math> will be a vector of length S. Together,
    they make up the parameters of the model. This equation simply says that each
    output component is a linear combination of the input components. By setting the
    parameters (<math><mi mathvariant="bold">M</mi></math> and <math><mi mathvariant="bold">b</mi></math>),
    you can choose any linear combination you want for each component.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'This was one of the very earliest machine learning models. It was introduced
    back in 1957 and was called a *perceptron*. The name is an amazing piece of marketing:
    it has a science fiction sound to it and seems to promise wonderful things, when
    in fact it is nothing more than a linear transform. In any case, the name has
    managed to stick for more than half a century.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear model is very easy to formulate in a completely generic way. It
    has exactly the same form no matter what problem you apply it to. The only differences
    between linear models are the lengths of the input and output vectors. From there,
    it is just a matter of choosing the parameter values, which can be done in a straightforward
    way with generic algorithms. That is exactly what we want for machine learning:
    a model and algorithms that are independent of what problem you are trying to
    solve. Just provide the training data, and parameters are automatically determined
    that transform the generic model into a function that solves your problem.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, linear models are also very limited. As demonstrated in [Figure¬†2-1](#a_linear_model_cannot_fit_data_points_that_follow),
    a linear model (in one dimension, that means a straight line) simply cannot fit
    most real datasets. The problem becomes even worse when you move to very high-dimensional
    data. No linear combination of pixel values in an image will reliably identify
    whether the image contains a cat. The task requires a much more complicated nonlinear
    model. In fact, any model that solves that problem will necessarily be *very*
    complicated and *very* nonlinear. But how can we formulate it in a generic way?
    The space of all possible nonlinear functions is infinitely complex. How can we
    define a model such that, just by choosing values of parameters, we can create
    almost any nonlinear function we are ever likely to want?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/dlls_0201.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. A linear model cannot fit data points that follow a curve. This
    requires a nonlinear model.
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multilayer Perceptrons
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A simple approach is to stack multiple linear transforms, one after another.
    For example, we could write:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>2</mn></msub> <mi>ùúô</mi> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>1</mn></msub> <mi mathvariant="bold">x</mi> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>2</mn></msub></mrow></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Look carefully at what we have done here. We start with an ordinary linear transform,
    <math><mrow><msub><mi mathvariant="bold">M</mi> <mn>1</mn></msub> <mi mathvariant="bold">x</mi><mo>+</mo><msub><mi
    mathvariant="bold">b</mi> <mn>1</mn></msub></mrow></math> . We then pass the result
    through a nonlinear function <math><mrow><mi>ùúô</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>,
    and then apply a second linear transform to the result. The function <math><mrow><mi>ùúô</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>,
    which is known as the *activation function*, is an essential part of what makes
    this work. Without it, the model would still be linear, and no more powerful than
    the previous one. A linear combination of linear combinations is itself nothing
    more than a linear combination of the original inputs! By inserting a nonlinearity,
    we enable the model to learn a much wider range of functions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'We don‚Äôt need to stop at two linear transforms. We can stack as many as we
    want on top of each other:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><msub><mi mathvariant="bold">h</mi> <mn>1</mn></msub> <mo>=</mo>
    <msub><mi>ùúô</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>1</mn></msub> <mi mathvariant="bold">x</mi> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math><math><mrow><msub><mi mathvariant="bold">h</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>ùúô</mi> <mn>2</mn></msub> <mrow><mo>(</mo>
    <msub><mi mathvariant="bold">M</mi> <mn>2</mn></msub> <msub><mi mathvariant="bold">h</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <math><mrow><msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <msub><mi>ùúô</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math><math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo>
    <msub><mi>ùúô</mi> <mi>n</mi></msub> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mi>n</mi></msub> <msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: This model is called a *multilayer perceptron*, or MLP for short. The middle
    steps <math><msub><mi>h</mi> <mi>i</mi></msub></math> are called *hidden layers*.
    The name refers to the fact that they are neither inputs nor outputs, just intermediate
    values used in the process of calculating the result. Also notice that we have
    added a subscript to each <math><mrow><mi>ùúô</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>.
    This indicates that different layers might use different nonlinearities.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: You can visualize this calculation as a stack of layers, as shown in [Figure¬†2-2](#a_multilayer_perceptron_viewed_as_a_stack).
    Each layer corresponds to a linear transformation followed by a nonlinearity.
    Information flows from one layer to another, the output of one layer becoming
    the input to the next. Each layer has its own set of parameters that determine
    how its output is calculated from its input.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/dlls_0202.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. A multilayer perceptron, viewed as a stack of layers with information
    flowing from one layer to the next.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multilayer perceptrons and their variants are also sometimes called *neural
    networks*. The name reflects the parallels between machine learning and neurobiology.
    A biological neuron connects to many other neurons. It receives signals from them,
    adds the signals together, and then sends out its own signals based on the result.
    As a very rough approximation, you can think of MLPs as working the same way as
    the neurons in your brain!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: What should the activation function <math><mrow><mi>ùúô</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>
    be? The surprising answer is that it mostly doesn‚Äôt matter. Of course, that is
    not entirely true. It obviously does matter, but not as much as you might expect.
    Nearly any reasonable function (monotonic, reasonably smooth) can work. Lots of
    different functions have been tried over the years, and although some work better
    than others, nearly all of them can produce decent results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The most popular activation function today is probably the *rectified linear
    unit* (ReLU), <math><mrow><mi>ùúô</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo> <mi>max</mi>
    <mo>(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo>)</mo></mrow></math> . If you aren‚Äôt
    sure what function to use, this is probably a good default. Other common choices
    include the *hyperbolic tangent*, <math><mrow><mi>tanh</mi> <mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>,
    and the *logistic sigmoid*, <math><mrow><mi>ùúô</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mn>1</mn><mo>/</mo><mrow><mo>(</mo><mn>1</mn>
    <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup> <mo>)</mo></mrow></mrow></math>.
    All of these functions are shown in [Figure¬†2-3](#three_common_activation_functions).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/dlls_0203.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-3\. Three common activation functions: the rectified linear unit,
    hyperbolic tangent, and logistic sigmoid.'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We also must choose two other properties for an MLP: its *width* and its *depth*.
    With the simple linear model, we had no choices to make. Given the lengths of
    <math><mi mathvariant="bold">x</mi></math> and <math><mi mathvariant="bold">y</mi></math>,
    the sizes of <math><mi mathvariant="bold">M</mi></math> and <math><mi mathvariant="bold">b</mi></math>
    were completely determined. Not so with hidden layers. Width refers to the size
    of the hidden layers. We can choose each <math><msub><mi mathvariant="bold">h</mi><mi>i</mi></msub></math>
    to have any length we want. Depending on the problem, you might want them to be
    much larger or much smaller than the input and output vectors.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Depth refers to the number of layers in the model. A model with only one hidden
    layer is described as *shallow*. A model with many hidden layers is described
    as *deep*. This is, in fact, the origin of the term ‚Äúdeep learning‚Äù; it simply
    means ‚Äúmachine learning using models with lots of layers.‚Äù
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the number and widths of layers in your model involves as much art
    as science. Or, to put it more formally, ‚ÄúThis is still an active field of research.‚Äù
    Often it just comes down to trying lots of combinations and seeing what works.
    There are a few principles that may provide guidance, however, or at least help
    you understand your results in hindsight:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: An MLP with one hidden layer is a *universal approximator*.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This means it can approximate any function at all (within certain fairly reasonable
    limits). In a sense, you never need more than one hidden layer. That is already
    enough to reproduce any function you are ever likely to want. Unfortunately, this
    result comes with a major caveat: the accuracy of the approximation depends on
    the width of the hidden layer, and you may need a very wide layer to get sufficient
    accuracy for a given problem. This brings us to the second principle.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deep models tend to require fewer parameters than shallow ones.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This statement is intentionally somewhat vague. More rigorous statements can
    be proven for particular special cases, but it does still apply as a general guideline.
    Here is perhaps a better way of stating it: every problem requires a model with
    a certain depth to efficiently achieve acceptable accuracy. At shallower depths,
    the required widths of the layers (and hence the total number of parameters) increase
    rapidly. This makes it sound like you should always prefer deep models over shallow
    ones. Unfortunately, it is partly contradicted by the third principle.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deep models tend to be harder to train than shallow ones.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Until about 2007, most machine learning models were shallow. The theoretical
    advantages of deep models were known, but researchers were usually unsuccessful
    at training them. Since then, a series of advances has gradually improved the
    usefulness of deep models. These include better training algorithms, new types
    of models that are easier to train, and of course faster computers combined with
    larger datasets on which to train the models. These advances gave rise to ‚Äúdeep
    learning‚Äù as a field. Yet despite the improvements, the general principle remains
    true: deeper models tend to be harder to train than shallower ones.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Training Models
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This brings us to the next subject: just how do we train a model anyway? MLPs
    provide us with a (mostly) generic model that can be used for any problem. (We
    will discuss other, more specialized types of models a little later.) Now we want
    a similarly generic algorithm to find the optimal values of the model‚Äôs parameters
    for a given problem. How do we do that?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The first thing you need, of course, is a collection of data to train it on.
    This dataset is known as the *training set*. It should consist of a large number
    of (**x**,**y**) pairs, also known as *samples*. Each sample specifies an input
    to the model, and what you want the model‚Äôs output to be when given that input.
    For example, the training set could be a collection of images, along with labels
    indicating whether or not each image contains a cat.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Next you need to define a loss function <math><mrow><mi>L</mi><mo>(</mo><mi
    mathvariant="bold">y</mi><mo>,</mo><mover accent="true"><mi mathvariant="bold">y</mi>
    <mo>^</mo></mover><mo>)</mo></mrow></math>, where <math><mi mathvariant="bold">y</mi></math>
    is the actual output from the model and <math><mover accent="true"><mi mathvariant="bold">y</mi>
    <mo>^</mo></mover></math> is the target value specified in the training set. This
    is how you measure whether the model is doing a good job of reproducing the training
    data. It is then averaged over every sample in the training set:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: <math><mi>average loss</mi> <mo>=</mo> <mrow><mfrac><mn>1</mn> <mi>N</mi></mfrac>
    <munderover><mo>Œ£</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mi>L</mi> <mrow><mo>(</mo> <msub><mi mathvariant="bold">y</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mover accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mi>L</mi><mo>(</mo><mi mathvariant="bold">y</mi><mo>,</mo><mover
    accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover><mo>)</mo></mrow></math>
    should be small when its arguments are close together and large when they are
    far apart. In other words, we take every sample in the training set, try using
    each one as an input to the model, and see how close the output is to the target
    value. Then we average this over the whole training set.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: An appropriate loss function needs to be chosen for each problem.¬† A common
    choice is the Euclidean distance (also known as the <math><msub><mi>L</mi><mi>2</mi></msub></math>
    distance), <math><mrow><mi>L</mi> <mrow><mo>(</mo> <mi mathvariant="bold">y</mi>
    <mo>,</mo> <mover accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><msub><mo>Œ£</mo> <mi>i</mi></msub> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
    . (In this expression, <math><msub><mi>y</mi><mi>i</mi></msub></math> means the
    *i*‚Äòth component of the vector <math><mi mathvariant="bold">y</mi></math>.) When
    <math><mi mathvariant="bold">y</mi></math> represents a probability distribution,
    a popular choice is the cross entropy, <math><mrow><mi>L</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">y</mi> <mo>,</mo> <mover accent="true"><mi mathvariant="bold">y</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo><msub><mo>Œ£</mo> <mi>i</mi></msub>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo form="prefix">log</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>i</mi></msub></mrow></math> . Other choices are also possible,
    and there is no universal ‚Äúbest‚Äù choice. It depends on the details of your problem.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a way to measure how well the model works, we need a way to
    improve it. We want to search for the parameter values that minimize the average
    loss over the training set. There are many ways to do this, but most work in deep
    learning uses some variant of the *gradient descent* algorithm. Let <math><mi>Œ∏</mi></math>
    represent the set of all parameters in the model. Gradient descent involves taking
    a series of small steps:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mi>Œ∏</mi> <mo>‚Üê</mo> <mi>Œ∏</mi> <mo>-</mo> <mi>œµ</mi> <mfrac><mi>‚àÇ</mi>
    <mrow><mi>‚àÇ</mi><mi>Œ∏</mi></mrow></mfrac> <mrow><mo>‚å©</mo> <mi>L</mi> <mo>‚å™</mo></mrow></mrow></math>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math><mrow><mo>‚å©</mo> <mi>L</mi> <mo>‚å™</mo></mrow></math> is the average
    loss over the training set. Each step moves a tiny distance in the ‚Äúdownhill‚Äù
    direction. It changes each of the model‚Äôs parameters by a little bit, with the
    goal of causing the average loss to decrease. If all the stars align and the phase
    of the moon is just right, this will eventually produce parameters that do a good
    job of solving your problem. <math><mi>œµ</mi></math> is called the *learning rate*,
    and it determines how much the parameters change on each step. It needs to be
    chosen very carefully: too small a value will cause learning to be very slow,
    while too large a value will prevent the algorithm from learning at all.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm really does work, but it has a serious problem. For every step
    of gradient descent, we need to loop over every sample in the training set. That
    means the time required to train the model is proportional to the size of the
    training set! Suppose that you have one million samples in the training set, that
    computing the gradient of the loss for one sample requires one million operations,
    and that it takes one million steps to find a good model. (All of these numbers
    are fairly typical of real deep learning applications.) Training will then require
    *one quintillion* operations. That takes quite a long time, even on a fast computer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there is a better solution: estimate <math><mrow><mo>‚å©</mo> <mi>L</mi>
    <mo>‚å™</mo></mrow></math> by averaging over a much smaller number of samples. This
    is the basis of the *stochastic gradient descent* (SGD) algorithm. For every step,
    we take a small set of samples (known as a *batch*) from the training set and
    compute the gradient of the loss function, averaged over only the samples in the
    batch. We can view this as an estimate of what we would have gotten if we had
    averaged over the entire training set, although it may be a very noisy estimate.
    We perform a single step of gradient descent, then select a new batch of samples
    for the next step.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm tends to be much faster. The time required for each step depends
    only on the size of each batch, which can be quite small (often on the order of
    100 samples) and is independent of the size of the training set. The disadvantage
    is that each step does a less good job of reducing the loss, because it is based
    on a noisy estimate of the gradient rather than the true gradient. Still, it leads
    to a much shorter training time overall.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Most optimization algorithms used in deep learning are based on SGD, but there
    are many variations that improve on it in different ways. Fortunately, you can
    usually treat these algorithms as black boxes and trust them to do the right thing
    without understanding all the details of how they work. Two of the most popular
    algorithms used today are called Adam and RMSProp. If you are in doubt about what
    algorithm to use, either one of those will probably be a reasonable choice.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Validation
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you have done everything described so far. You collected a large set
    of training data. You selected a model, then ran a training algorithm until the
    loss became very small. Congratulations, you now have a function that solves your
    problem!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Right?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Sorry, it‚Äôs not that simple! All you really know for sure is that the function
    works well *on the training data*. You might hope it will also work well on other
    data, but you certainly can‚Äôt count on it. Now you need to validate the model
    to see whether it works on data that it hasn‚Äôt been specifically trained on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this you need a second dataset, called the *test set*. It has exactly
    the same form as the training set, a collection of <math><mrow><mo>(</mo> <mi
    mathvariant="bold">x</mi> <mo>,</mo> <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></math>
    pairs, but the two should have no samples in common. You train the model on the
    training set, then test it on the test set. This brings us to one of the most
    important principles in machine learning:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: You must not use the test set in any way while designing or training the model.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, it is best if you never even look at the data in the test set. Test
    set data is only for testing the fully trained model to find out how well it works.
    If you allow the test set to influence the model in any way, you risk getting
    a model that works better on the test set than on other data that was not involved
    in creating the model. It ceases to be a true test set, and becomes just another
    type of training set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: This is connected to the mathematical concept of *overfitting*. The training
    data is supposed to be representative of a much larger data distribution, the
    set of all inputs you might ever want to use the model on. But you can‚Äôt train
    it on all possible inputs. You can only create a finite set of training samples,
    train the model on those, and hope it learns general strategies that work equally
    well on other samples. Overfitting is what happens when the training picks up
    on specific features of the training samples, such that the model works better
    on them than it does on other samples.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overfitting is a major problem for anyone who uses machine learning. Given that,
    you won‚Äôt be surprised to learn that lots of techniques have been developed for
    avoiding it. These techniques are collectively known as *regularization*. The
    goal of any regularization technique is to avoid overfitting and produce a trained
    model that works well on any input, not just the particular inputs that were used
    for training.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ËøáÊãüÂêàÊòØ‰ªª‰Ωï‰ΩøÁî®Êú∫Âô®Â≠¶‰π†ÁöÑ‰∫∫ÈÉΩ‰ºöÈÅáÂà∞ÁöÑ‰∏ªË¶ÅÈóÆÈ¢ò„ÄÇÈâ¥‰∫éÊ≠§ÔºåÊÇ®‰∏ç‰ºöÊÑüÂà∞ÊÉäËÆ∂Âú∞‰∫ÜËß£Âà∞Â∑≤ÁªèÂºÄÂèë‰∫ÜËÆ∏Â§öÊäÄÊúØÊù•ÈÅøÂÖçËøáÊãüÂêà„ÄÇËøô‰∫õÊäÄÊúØÁªüÁß∞‰∏∫*Ê≠£ÂàôÂåñ*„ÄÇ‰ªª‰ΩïÊ≠£ÂàôÂåñÊäÄÊúØÁöÑÁõÆÊ†áÈÉΩÊòØÈÅøÂÖçËøáÊãüÂêàÔºåÂπ∂‰∫ßÁîü‰∏Ä‰∏™Âú®‰ªª‰ΩïËæìÂÖ•‰∏äÈÉΩË°®Áé∞ËâØÂ•ΩÁöÑËÆ≠ÁªÉÊ®°ÂûãÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÁî®‰∫éËÆ≠ÁªÉÁöÑÁâπÂÆöËæìÂÖ•„ÄÇ
- en: Before we discuss particular regularization techniques, there are two very important
    points to understand about it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®ËÆ®ËÆ∫ÁâπÂÆöÁöÑÊ≠£ÂàôÂåñÊäÄÊúØ‰πãÂâçÔºåÊúâ‰∏§‰∏™ÈùûÂ∏∏ÈáçË¶ÅÁöÑË¶ÅÁÇπÈúÄË¶Å‰∫ÜËß£„ÄÇ
- en: 'First, the best way to avoid overfitting is *almost always* to get more training
    data. The bigger your training set, the better it represents the ‚Äútrue‚Äù data distribution,
    and the less likely the learning algorithm is to overfit. Of course, that is sometimes
    impossible: maybe you simply have no way to get more data, or the data may be
    very expensive to collect. In that case, you just have to do the best you can
    with the data you have, and if overfitting is a problem, you will have to use
    regularization to avoid it. But more data will probably lead to a better result
    than regularization.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: È¶ñÂÖàÔºåÈÅøÂÖçËøáÊãüÂêàÁöÑÊúÄ‰Ω≥ÊñπÊ≥ï *Âá†‰πéÊÄªÊòØ* Ëé∑ÂæóÊõ¥Â§öÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÊÇ®ÁöÑËÆ≠ÁªÉÈõÜË∂äÂ§ßÔºåÂÆÉË∂äËÉΩÊõ¥Â•ΩÂú∞‰ª£Ë°®‚ÄúÁúüÂÆû‚ÄùÊï∞ÊçÆÂàÜÂ∏ÉÔºåÂ≠¶‰π†ÁÆóÊ≥ïËøáÊãüÂêàÁöÑÂèØËÉΩÊÄßÂ∞±Ë∂äÂ∞è„ÄÇÂΩìÁÑ∂ÔºåÊúâÊó∂ËøôÊòØ‰∏çÂèØËÉΩÁöÑÔºö‰πüËÆ∏ÊÇ®Ê†πÊú¨Êó†Ê≥ïËé∑ÂæóÊõ¥Â§öÊï∞ÊçÆÔºåÊàñËÄÖÊî∂ÈõÜÊï∞ÊçÆÂèØËÉΩÈùûÂ∏∏ÊòÇË¥µ„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÊÇ®Âè™ËÉΩÂ∞ΩÂäõÂà©Áî®ÊâãÂ§¥ÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊûúËøáÊãüÂêàÊòØ‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊÇ®Â∞Ü‰∏çÂæó‰∏ç‰ΩøÁî®Ê≠£ÂàôÂåñÊù•ÈÅøÂÖçÂÆÉ„ÄÇ‰ΩÜÊõ¥Â§öÁöÑÊï∞ÊçÆÂèØËÉΩ‰ºöÊØîÊ≠£ÂàôÂåñ‰∫ßÁîüÊõ¥Â•ΩÁöÑÁªìÊûú„ÄÇ
- en: Second, there is no universally ‚Äúbest‚Äù way to do regularization. It all depends
    on the problem. After all, the training algorithm doesn‚Äôt know that it‚Äôs overfitting.
    All it knows about is the training data. It doesn‚Äôt know how the true data distribution
    differs from the training data, so the best it can do is produce a model that
    works well on the training set. If that isn‚Äôt what you want, it‚Äôs up to you to
    tell it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ÂÖ∂Ê¨°ÔºåÊ≤°ÊúâÊôÆÈÅç‚ÄúÊúÄ‰Ω≥‚ÄùÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ï„ÄÇËøô‰∏ÄÂàáÈÉΩÂèñÂÜ≥‰∫éÈóÆÈ¢ò„ÄÇÊØïÁ´üÔºåËÆ≠ÁªÉÁÆóÊ≥ïÂπ∂‰∏çÁü•ÈÅìËá™Â∑±Ê≠£Âú®ËøáÊãüÂêà„ÄÇÂÆÉÂè™Áü•ÈÅìËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÂÆÉ‰∏çÁü•ÈÅìÁúüÂÆûÊï∞ÊçÆÂàÜÂ∏É‰∏éËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ∑ÆÂºÇÔºåÂõ†Ê≠§ÂÆÉËÉΩÂÅöÁöÑÊúÄÂ•ΩÂ∞±ÊòØÁîüÊàê‰∏Ä‰∏™Âú®ËÆ≠ÁªÉÈõÜ‰∏äË°®Áé∞ËâØÂ•ΩÁöÑÊ®°Âûã„ÄÇÂ¶ÇÊûúËøô‰∏çÊòØÊÇ®ÊÉ≥Ë¶ÅÁöÑÔºåÈÇ£Â∞±Áî±ÊÇ®ÂëäËØâÂÆÉ„ÄÇ
- en: 'That is the essence of any regularization method: biasing the training process
    to prefer certain types of models over others. You make assumptions about what
    properties a ‚Äúgood‚Äù model should have, and how it differs from an overfit one,
    and then you tell the training algorithm to prefer models with those properties.
    Of course, those assumptions are often implicit rather than explicit. It may not
    be obvious what assumptions you are making by choosing a particular regularization
    method. But they are always there.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ËøôÂ∞±ÊòØ‰ªª‰ΩïÊ≠£ÂàôÂåñÊñπÊ≥ïÁöÑÊú¨Ë¥®ÔºöÂÅèÂêëËÆ≠ÁªÉËøáÁ®ãÔºåÊõ¥ÂñúÊ¨¢Êüê‰∫õÁ±ªÂûãÁöÑÊ®°ÂûãËÄå‰∏çÊòØÂÖ∂‰ªñÁ±ªÂûã„ÄÇÊÇ®ÂØπ‚ÄúÂ•Ω‚ÄùÊ®°ÂûãÂ∫îËØ•ÂÖ∑ÊúâÂì™‰∫õÁâπÊÄß‰ª•ÂèäÂÆÉ‰∏éËøáÊãüÂêàÊ®°ÂûãÊúâ‰Ωï‰∏çÂêåÂÅöÂá∫ÂÅáËÆæÔºåÁÑ∂ÂêéÂëäËØâËÆ≠ÁªÉÁÆóÊ≥ïÊõ¥ÂñúÊ¨¢ÂÖ∑ÊúâËøô‰∫õÁâπÊÄßÁöÑÊ®°Âûã„ÄÇÂΩìÁÑ∂ÔºåËøô‰∫õÂÅáËÆæÈÄöÂ∏∏ÊòØÈöêÂê´ÁöÑËÄå‰∏çÊòØÊòéÁ°ÆÁöÑ„ÄÇÈÄöËøáÈÄâÊã©ÁâπÂÆöÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ïÔºåÊÇ®ÂèØËÉΩ‰∏çÊ∏ÖÊ•öËá™Â∑±ÂÅöÂá∫‰∫Ü‰ªÄ‰πàÂÅáËÆæ„ÄÇ‰ΩÜÂÆÉ‰ª¨ÊÄªÊòØÂ≠òÂú®ÁöÑ„ÄÇ
- en: One of the simplest regularization methods is just to train the model for fewer
    steps. Early in training, it tends to pick up on coarse properties of the training
    data that likely apply to the true distribution. The longer it runs, the more
    likely it is to start picking up on fine details of particular training samples.
    By limiting the number of training steps, you give it less opportunity to overfit.
    More formally, you are really assuming that ‚Äúgood‚Äù parameter values should not
    be too different from whatever values you start training from.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ÊúÄÁÆÄÂçïÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ï‰πã‰∏ÄÂ∞±ÊòØÂáèÂ∞ëÊ®°ÂûãÁöÑËÆ≠ÁªÉÊ≠•È™§„ÄÇÂú®ËÆ≠ÁªÉÂàùÊúüÔºåÊ®°ÂûãÂÄæÂêë‰∫éÊçïÊçâÂà∞ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÁ≤óÁï•ÁâπÊÄßÔºåËøô‰∫õÁâπÊÄßÂèØËÉΩÈÄÇÁî®‰∫éÁúüÂÆûÂàÜÂ∏É„ÄÇÂÆÉËøêË°åÂæóË∂ä‰πÖÔºåÂ∞±Ë∂äÊúâÂèØËÉΩÂºÄÂßãÊçïÊçâÁâπÂÆöËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÁªÜËäÇ„ÄÇÈÄöËøáÈôêÂà∂ËÆ≠ÁªÉÊ≠•È™§ÁöÑÊï∞ÈáèÔºåÊÇ®ÂáèÂ∞ë‰∫ÜËøáÊãüÂêàÁöÑÊú∫‰ºö„ÄÇÊõ¥Ê≠£ÂºèÂú∞ËØ¥ÔºåÊÇ®ÂÆûÈôÖ‰∏äÊòØÂÅáËÆæ‚ÄúÂ•Ω‚ÄùÁöÑÂèÇÊï∞ÂÄº‰∏çÂ∫îËØ•‰∏éÊÇ®ÂºÄÂßãËÆ≠ÁªÉÊó∂ÁöÑÂÄºÊúâÂ§™Â§ßÁöÑ‰∏çÂêå„ÄÇ
- en: Another method is to restrict the magnitude of the parameters in the model.
    For example, you might add a term to the loss function that is proportional to
    <math><msup><mrow><mo>|</mo><mi>Œ∏</mi><mo>|</mo></mrow> <mn>2</mn></msup></math>
    , where <math><mi>Œ∏</mi></math> is a vector containing all of the model‚Äôs parameters.
    By doing this, you are assuming that ‚Äúgood‚Äù parameter values should not be any
    larger than necessary. It reflects the fact that overfitting often (though not
    always) involves some parameters becoming very large.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Âè¶‰∏ÄÁßçÊñπÊ≥ïÊòØÈôêÂà∂Ê®°Âûã‰∏≠ÂèÇÊï∞ÁöÑÂ§ßÂ∞è„ÄÇ‰æãÂ¶ÇÔºåÊÇ®ÂèØ‰ª•ÂêëÊçüÂ§±ÂáΩÊï∞Ê∑ªÂä†‰∏Ä‰∏™‰∏é <math><msup><mrow><mo>|</mo><mi>Œ∏</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math> ÊàêÊØî‰æãÁöÑÈ°πÔºåÂÖ∂‰∏≠ <math><mi>Œ∏</mi></math> ÊòØÂåÖÂê´Ê®°ÂûãÊâÄÊúâÂèÇÊï∞ÁöÑÂêëÈáè„ÄÇÈÄöËøáËøôÊ†∑ÂÅöÔºåÊÇ®ÂÅáËÆæ‚ÄúÂ•Ω‚ÄùÁöÑÂèÇÊï∞ÂÄº‰∏çÂ∫îËØ•ÊØîÂøÖË¶ÅÁöÑÂ§ß„ÄÇËøôÂèçÊò†‰∫ÜËøáÊãüÂêàÈÄöÂ∏∏ÔºàËôΩÁÑ∂‰∏çÊÄªÊòØÔºâÊ∂âÂèä‰∏Ä‰∫õÂèÇÊï∞ÂèòÂæóÈùûÂ∏∏Â§ßÁöÑ‰∫ãÂÆû„ÄÇ
- en: 'A very popular method of regularization is called *dropout*. It involves doing
    something that at first seems ridiculous, but actually works surprisingly well.
    For each hidden layer in the model, you randomly select a subset of elements in
    the output vector <math><msub><mi>h</mi><mi>i</mi></msub></math> and set them
    to 0\. On every step of gradient descent, you pick a different random subset of
    elements. This might seem like it would just break the model: how can you expect
    it to work when internal calculations keep randomly getting set to 0? The mathematical
    theory for why dropout works is a bit complicated. Very roughly speaking, by using
    dropout you are assuming that no individual calculation within the model should
    be too important. You should be able to randomly remove any individual calculation,
    and the rest of the model should continue to work without it. This forces it to
    learn redundant, highly distributed representations of data that make overfitting
    unlikely. If you are unsure of what regularization method to use, dropout is a
    good first thing to try.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Optimization
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now you have probaly noticed that there are a lot of choices to make, even
    when using a supposedly generic model with a ‚Äúgeneric‚Äù learning algorithm. Examples
    include:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers in the model
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The width of each layer
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of training steps to perform
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate to use during training
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fraction of elements to set to 0 when using dropout
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These options are called *hyperparameters*. A hyperparameter is any aspect of
    the model or training algorithm that must be set in advance rather than being
    learned by the training algorithm. But how are you supposed to choose them‚Äîand
    isn‚Äôt the whole point of machine learning to select settings automatically based
    on data?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the subject of *hyperparameter optimization*. The simplest
    way of doing it is just to try lots of values for each hyperparameter and see
    what works best. This becomes very expensive when you want to try lots of values
    for lots of hyperparameters, so there are more sophisticated approaches, but the
    basic idea remains the same: try different combinations and see what works best.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: But how can you tell what works best? The simplest answer would be to just see
    what produces the lowest value of the loss function (or some other measure of
    accuracy) on the training set. But remember, that isn‚Äôt what we really care about.
    We want to minimize error on the test set, not the training set. This is especially
    important for hyperparameters that affect regularization, such as the dropout
    rate. A low training set error might just mean the model is overfitting, optimizing
    for the precise details of the training data. So instead we want to try lots of
    hyperparameter values, then use the ones that minimize the loss on the test set.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'But we mustn‚Äôt do that! Remember: you must not use the test set in any way
    while designing or training the model. Its job is to tell you how well the model
    is likely to work on new data it has never seen before. Just because a particular
    set of hyperparameters happens to work best on the test set doesn‚Äôt guarantee
    those values will always work best. We must not allow the test set to influence
    the model, or it is no longer an unbiased test set.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is to create yet another dataset, which is called the *validation
    set*. It must not share any samples with either the training set or the test set.
    The full procedure now works as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: For each set of hyperparameter values, train the model on the training set,
    then compute the loss on the validation set.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whichever set of hyperparameters give the lowest loss on the validation set,
    accept them as your final model.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate that final model on the test set to get an unbiased measure of how
    well it works.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Other Types of Models
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This still leaves one more decision you need to make, and it is a huge subject
    in itself: what kind of model to use. Earlier in this chapter we introduced multilayer
    perceptrons. They have the advantage of being a generic class of models that can
    be applied to many different problems. Unfortunately, they also have serious disadvantages.
    They require a huge number of parameters, which makes them very susceptible to
    overfitting. They become difficult to train when they have more than one or two
    hidden layers. In many cases, you can get a better result by using a less generic
    model that takes advantage of specific features of your problem.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Much of the content of this book consists of discussing particular types of
    models that are especially useful in the life sciences. Those can wait until later
    chapters. But for the purposes of this introduction, there are two very important
    classes of models we should discuss that are widely used in many different fields.
    They are called convolutional neural networks and recurrent neural networks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Convolutional neural networks* (CNNs for short) were one of the very first
    classes of deep models to be widely used. They were developed for use in image
    processing and computer vision. They remain an excellent choice for many kinds
    of problems that involve continuous data sampled on a rectangular grid: audio
    signals (1D), images (2D), volumetric MRI data (3D), and so on.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: They are also a class of models that truly justify the term ‚Äúneural network.‚Äù
    The design of CNNs was originally inspired by the workings of the feline visual
    cortex. (Cats have played a central role in deep learning from the dawn of the
    field.) Research performed from the 1950s to the 1980s revealed that vision is
    processed through a series of layers. Each neuron in the first layer takes input
    from a small region of the visual field (its *receptive field*). Different neurons
    are specialized to detect particular local patterns or features, such as vertical
    or horizontal lines. Cells in the second layer take input from local clusters
    of cells in the first layer, combining their signals to detect more complicated
    patterns over a larger receptive field. Each layer can be viewed as a new representation
    of the original image, described in terms of larger and more abstract patterns
    than the ones in the previous layer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: CNNs mirror this design, sending an input image through a series of layers.
    In that sense, they are just like MLPs, but the structure of each layer is very
    different. MLPs use *fully connected layers*. Every element of the output vector
    depends on every element of the input vector. CNNs use *convolutional layers*
    that take advantage of spatial locality. Each output element corresponds to a
    small region of the image, and only depends on the input values in that region.
    This enormously reduces the number of parameters defining each layer. In effect,
    it assumes that most elements of the weight matrix <math><msub><mi mathvariant="bold">M</mi><mi>i</mi></msub></math>
    are 0, since each output element only depends on a small number of input elements.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional layers take this a step further: they assume the parameters are
    the same for *every local region of the image*. If a layer uses one set of parameters
    to detect horizontal lines at one location in the image, it also uses exactly
    the same parameters to detect horizontal lines everywhere else in the image. This
    makes the number of parameters for the layer independent of the size of the image.
    All it has to learn is a single *convolutional kernel* that defines how output
    features are computed from any local region of the image. That local region is
    often very small, perhaps 5 by 5 pixels. In that case, the number of parameters
    to learn is only 25 times the number of output features for each region. This
    is tiny compared to the number in a fully connected layer, making CNNs much easier
    to train and much less susceptible to overfitting than MLPs.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Recurrent neural networks* (RNNs for short) are a bit different. They are
    normally used to process data that takes the form of a sequence of elements: words
    in a text document, bases in a DNA molecule, etc. The elements in the sequence
    are fed into the network‚Äôs input one at a time. But then the network does something
    very different: the output from each layer is fed back into its own input on the
    next step! This allows RNNs to have a sort of memory. ¬†When an element (word,
    DNA base, etc.) from the sequence is fed into the network, the input to each layer
    depends on that element, but also on all of the previous elements ([Figure¬†2-4](#recurrent-neural-network)).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/dlls_0204.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. A recurrent neural network. As each element ( <math><msub><mi>x</mi>
    <mn>1</mn></msub></math> , <math><msub><mi>x</mi> <mn>2</mn></msub></math> , ...)
    of the sequence is fed into the input, the output ( <math><msub><mi>y</mi> <mn>1</mn></msub></math>
    , <math><msub><mi>y</mi> <mn>2</mn></msub></math> , ...) depends both on the input
    element and on the RNN‚Äôs own output during the previous step.
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, the input to a recurrent layer has two parts: the regular input (that is,
    the output from the previous layer in the network) and the recurrent input (which
    equals its own output from the previous step). It then needs to calculate a new
    output based on those inputs. In principle you could use a fully connected layer,
    but in practice that usually doesn‚Äôt work very well. Researchers have developed
    other types of layers that work much better in RNNs. The two most popular ones
    are called the *gated recurrent unit* (GRU) and the *long short-term memory* (LSTM).
    Don‚Äôt worry about the details for now; just remember that if you are creating
    an RNN, you should usually build it out of one of those types of layers.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Having memory makes RNNs fundamentally different from the other models we have
    discussed. With a CNN or MLP, you simply feed a value into the network‚Äôs input
    and get a different value out. The output is entirely determined by the input.
    Not so with an RNN. The model has its own internal state, composed of the outputs
    of all its layers from the most recent step. Each time you feed a new value into
    the model, the output depends not just on the input value but also on the internal
    state. Likewise, the internal state is altered by each new input value. This makes
    RNNs very powerful, and allows them to be used for lots of different applications.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning is a huge subject, and this chapter has only given the briefest
    introduction to it. It should be enough to help you read and understand the rest
    of this book, but if you plan to do serious work in the field, you will want to
    acquire a much more thorough background. Fortunately, there are many excellent
    deep learning resources available online. Here are some suggestions for material
    you might consult:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '*[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)*
    by Michael Nielsen (Determination Press) covers roughly the same material as this
    chapter, but goes into far more detail on every subject. If you want a solid working
    knowledge of the fundamentals of deep learning, sufficient to make use of it in
    your own work, this is an excellent place to start.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Deep Learning*](http://www.deeplearningbook.org) by Ian Goodfellow, Yoshua
    Bengio, and Aaron Courville (MIT Press) is a more advanced introduction written
    by some of the top researchers in the field. It expects the reader to have a background
    similar to that of a graduate student in computer science and goes into far more
    detail on the mathematical theory behind the subject. You can easily use deep
    models without understanding all of the theory, but if you want to do original
    research in deep learning (rather than just using deep models as a tool to solve
    problems in other fields), this book is a fantastic resource.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*TensorFlow for Deep Learning*](http://shop.oreilly.com/product/0636920065869.do)
    by Bharath Ramsundar and Reza Zadeh (O‚ÄôReilly) provides a practitioner‚Äôs introduction
    to deep learning that seeks to build intuition about the core concepts without
    delving too deeply into the mathematical underpinnings of such models. It might
    be a useful reference for those who are interested in the practical aspects of
    deep learning.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
