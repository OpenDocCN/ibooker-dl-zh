- en: Chapter 2\. Introduction to Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。深度学习简介
- en: The goal of this chapter is to introduce the basic principles of deep learning.
    If you already have lots of experience with deep learning, you should feel free
    to skim this chapter and then go on to the next. If you have less experience,
    you should study this chapter carefully as the material it covers will be essential
    to understanding the rest of the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍深度学习的基本原理。如果您已经有很多深度学习经验，可以随意略读本章，然后继续下一章。如果您经验较少，应该仔细学习本章，因为它涵盖的内容对于理解本书的其余部分至关重要。
- en: 'In most of the problems we will discuss, our task will be to create a mathematical
    function:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论的大多数问题中，我们的任务是创建一个数学函数：
- en: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi
    mathvariant="bold">x</mi> <mo>)</mo></mrow></math>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi
    mathvariant="bold">x</mi> <mo>)</mo></mrow></math>
- en: 'Notice that <math><mi mathvariant="bold">x</mi></math> and <math><mi mathvariant="bold">y</mi></math>
    are written in bold. This indicates they are vectors. The function might take
    many numbers as input, perhaps thousands or even millions, and it might produce
    many numbers as outputs. Here are some examples of functions you might want to
    create:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意<math><mi mathvariant="bold">x</mi></math>和<math><mi mathvariant="bold">y</mi></math>是用粗体书写的。这表示它们是向量。该函数可能接受许多数字作为输入，也许是成千上万，它可能产生许多数字作为输出。以下是您可能想要创建的一些函数示例：
- en: <math><mi mathvariant="bold">x</mi></math> contains the colors of all the pixels
    in an image. <math><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>
    should equal 1 if the image contains a cat and 0 if it does not.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math><mi mathvariant="bold">x</mi></math>包含图像中所有像素的颜色。如果图像中包含猫，则<math><mrow><mi>f</mi><mo>(</mo><mi
    mathvariant="bold">x</mi><mo>)</mo></mrow></math>应该等于1，如果不包含则等于0。
- en: The same as above, except <math><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>
    should be a vector of numbers. The first element indicates whether the image contains
    a cat, the second whether it contains a dog, the third whether it contains an
    airplane, and so on for thousands of types of objects.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与上述相同，只是<math><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>应该是一个数字向量。第一个元素表示图像是否包含猫，第二个元素表示是否包含狗，第三个元素表示是否包含飞机，以此类推，针对成千上万种对象。
- en: <math><mi mathvariant="bold">x</mi></math> contains the DNA sequence for a chromosome.
    <math><mi mathvariant="bold">y</mi></math> should be a vector whose length equals
    the number of bases in the chromosome. Each element should equal 1 if that base
    is part of a region that codes for a protein, or 0 if not.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math><mi mathvariant="bold">x</mi></math>包含染色体的DNA序列。<math><mi mathvariant="bold">y</mi></math>应该是一个向量，其长度等于染色体中碱基的数量。如果该碱基是编码蛋白质的区域的一部分，则每个元素应该等于1，否则为0。
- en: '<math><mi mathvariant="bold">x</mi></math> describes the structure of a molecule.
    (We will discuss various ways of representing molecules in later chapters.) <math><mi
    mathvariant="bold">y</mi></math> should be a vector where each element describes
    some physical property of the molecule: how easily it dissolves in water, how
    strongly it binds to some other molecule, and so on.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math><mi mathvariant="bold">x</mi></math>描述了分子的结构。（我们将在后面的章节中讨论表示分子的各种方法。）<math><mi
    mathvariant="bold">y</mi></math>应该是一个向量，其中每个元素描述分子的某些物理性质：它在水中溶解的容易程度，它与其他分子结合的强度等等。
- en: As you can see, <math><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>
    could be a very, very complicated function! It usually takes a long vector as
    input and tries to extract information from it that is not at all obvious just
    from looking at the input numbers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，<math><mrow><mi>f</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>可能是一个非常复杂的函数！它通常接受一个长向量作为输入，并尝试从中提取信息，这些信息仅仅从查看输入数字是不明显的。
- en: The traditional approach to solving this problem is to design a function by
    hand. You would start by analyzing the problem. What patterns of pixels tend to
    indicate the presence of a cat? What patterns of DNA tend to distinguish coding
    regions from noncoding ones? You would write computer code to recognize particular
    types of features, then try to identify combinations of features that reliably
    produce the result you want. This process is slow and labor-intensive, and depends
    heavily on the expertise of the person carrying it out.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的传统方法是手动设计一个函数。您将从分析问题开始。哪些像素模式倾向于表明猫的存在？哪些DNA模式倾向于区分编码区域和非编码区域？您将编写计算机代码来识别特定类型的特征，然后尝试识别可靠产生所需结果的特征组合。这个过程缓慢而费力，且严重依赖于执行者的专业知识。
- en: Machine learning takes a totally different approach. Instead of designing a
    function by hand, you allow the computer to learn its own function based on data.
    You collect thousands or millions of images, each labeled to indicate whether
    it includes a cat. You present all of this training data to the computer, and
    let it search for a function that is consistently close to 1 for the images with
    cats and close to 0 for the ones without.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习采用完全不同的方法。与手动设计函数不同，您允许计算机根据数据学习自己的函数。您收集成千上万甚至数百万张图像，每张都标记了是否包含猫。您将所有这些训练数据呈现给计算机，并让它搜索一个函数，对于包含猫的图像始终接近1，对于没有猫的图像接近0。
- en: What does it mean to “let the computer search for a function”? Generally speaking,
    you create a *model* that defines some large class of functions. The model includes
    *parameters*, variables that can take on any value. By choosing the values of
    the parameters, you select a particular function out of all the many functions
    in the class defined by the model. The computer’s job is to select values for
    the parameters. It tries to find values such that, when your training data is
    used as input, the output is as close as possible to the corresponding targets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “让计算机搜索函数”是什么意思？一般来说，你创建一个*模型*，定义了一大类函数。模型包括*参数*，可以取任何值的变量。通过选择参数的值，你从模型定义的所有函数中选择一个特定的函数。计算机的任务是选择参数的值。它试图找到这样的值，使得当你的训练数据作为输入时，输出尽可能接近相应的目标。
- en: Linear Models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型
- en: 'One of the simplest models you might consider trying is a linear model:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能考虑尝试的最简单的模型之一是线性模型：
- en: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <mi mathvariant="bold">Mx</mi>
    <mo>+</mo> <mi mathvariant="bold">b</mi></mrow></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <mi mathvariant="bold">Mx</mi>
    <mo>+</mo> <mi mathvariant="bold">b</mi></mrow></math>
- en: In this equation, <math><mi mathvariant="bold">M</mi></math> is a matrix (sometimes
    referred to as the “weights”) and <math><mi mathvariant="bold">b</mi></math> is
    a vector (referred to as the “biases”). Their sizes are determined by the numbers
    of input and output values. If <math><mi mathvariant="bold">x</mi></math> has
    length T and you want <math><mi mathvariant="bold">y</mi></math> to have length
    S, then <math><mi mathvariant="bold">M</mi></math> will be an S × T matrix and
    <math><mi mathvariant="bold">b</mi></math> will be a vector of length S. Together,
    they make up the parameters of the model. This equation simply says that each
    output component is a linear combination of the input components. By setting the
    parameters (<math><mi mathvariant="bold">M</mi></math> and <math><mi mathvariant="bold">b</mi></math>),
    you can choose any linear combination you want for each component.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，<math><mi mathvariant="bold">M</mi></math>是一个矩阵（有时被称为“权重”），<math><mi
    mathvariant="bold">b</mi></math>是一个向量（称为“偏差”）。它们的大小由输入和输出值的数量确定。如果<math><mi mathvariant="bold">x</mi></math>的长度为T，你希望<math><mi
    mathvariant="bold">y</mi></math>的长度为S，那么<math><mi mathvariant="bold">M</mi></math>将是一个S×T矩阵，<math><mi
    mathvariant="bold">b</mi></math>将是长度为S的向量。它们一起构成模型的参数。这个方程简单地表示每个输出组件是输入组件的线性组合。通过设置参数（<math><mi
    mathvariant="bold">M</mi></math>和<math><mi mathvariant="bold">b</mi></math>），你可以选择任何你想要的每个组件的线性组合。
- en: 'This was one of the very earliest machine learning models. It was introduced
    back in 1957 and was called a *perceptron*. The name is an amazing piece of marketing:
    it has a science fiction sound to it and seems to promise wonderful things, when
    in fact it is nothing more than a linear transform. In any case, the name has
    managed to stick for more than half a century.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最早的机器学习模型之一。它是在1957年引入的，被称为*感知器*。这个名字是一个了不起的营销手段：听起来像是科幻，似乎承诺了美好的事物，但实际上它只是一个线性变换。无论如何，这个名字已经坚持了半个多世纪。
- en: 'The linear model is very easy to formulate in a completely generic way. It
    has exactly the same form no matter what problem you apply it to. The only differences
    between linear models are the lengths of the input and output vectors. From there,
    it is just a matter of choosing the parameter values, which can be done in a straightforward
    way with generic algorithms. That is exactly what we want for machine learning:
    a model and algorithms that are independent of what problem you are trying to
    solve. Just provide the training data, and parameters are automatically determined
    that transform the generic model into a function that solves your problem.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型非常容易以完全通用的方式来构建。无论你应用它到什么问题，它的形式都完全相同。线性模型之间唯一的区别是输入和输出向量的长度。从那里开始，只需要选择参数值，可以通过通用算法简单地完成。这正是我们在机器学习中想要的：一个与你要解决的问题无关的模型和算法。只需提供训练数据，参数会自动确定，将通用模型转换为解决你的问题的函数。
- en: Unfortunately, linear models are also very limited. As demonstrated in [Figure 2-1](#a_linear_model_cannot_fit_data_points_that_follow),
    a linear model (in one dimension, that means a straight line) simply cannot fit
    most real datasets. The problem becomes even worse when you move to very high-dimensional
    data. No linear combination of pixel values in an image will reliably identify
    whether the image contains a cat. The task requires a much more complicated nonlinear
    model. In fact, any model that solves that problem will necessarily be *very*
    complicated and *very* nonlinear. But how can we formulate it in a generic way?
    The space of all possible nonlinear functions is infinitely complex. How can we
    define a model such that, just by choosing values of parameters, we can create
    almost any nonlinear function we are ever likely to want?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，线性模型也非常有限。正如在[图2-1](#a_linear_model_cannot_fit_data_points_that_follow)中所示，线性模型（在一维中，即一条直线）简单地无法拟合大多数真实数据集。当你转向非常高维的数据时，问题变得更糟。在图像中像素值的线性组合将无法可靠地识别图像中是否包含猫。这个任务需要一个更加复杂的非线性模型。事实上，任何解决这个问题的模型都必然会非常复杂和非常非线性。但是我们如何以通用的方式来构建它呢？所有可能的非线性函数空间都是无限复杂的。我们如何定义一个模型，以便通过选择参数值，我们几乎可以创建任何我们想要的非线性函数？
- en: '![](Images/dlls_0201.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlls_0201.png)'
- en: Figure 2-1\. A linear model cannot fit data points that follow a curve. This
    requires a nonlinear model.
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1. 线性模型无法拟合遵循曲线的数据点。这需要一个非线性模型。
- en: Multilayer Perceptrons
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'A simple approach is to stack multiple linear transforms, one after another.
    For example, we could write:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法是堆叠多个线性变换，一个接一个。例如，我们可以写成：
- en: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>2</mn></msub> <mi>𝜙</mi> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>1</mn></msub> <mi mathvariant="bold">x</mi> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>2</mn></msub></mrow></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>2</mn></msub> <mi>𝜙</mi> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>1</mn></msub> <mi mathvariant="bold">x</mi> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>2</mn></msub></mrow></math>
- en: Look carefully at what we have done here. We start with an ordinary linear transform,
    <math><mrow><msub><mi mathvariant="bold">M</mi> <mn>1</mn></msub> <mi mathvariant="bold">x</mi><mo>+</mo><msub><mi
    mathvariant="bold">b</mi> <mn>1</mn></msub></mrow></math> . We then pass the result
    through a nonlinear function <math><mrow><mi>𝜙</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>,
    and then apply a second linear transform to the result. The function <math><mrow><mi>𝜙</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>,
    which is known as the *activation function*, is an essential part of what makes
    this work. Without it, the model would still be linear, and no more powerful than
    the previous one. A linear combination of linear combinations is itself nothing
    more than a linear combination of the original inputs! By inserting a nonlinearity,
    we enable the model to learn a much wider range of functions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看看我们在这里做了什么。我们从一个普通的线性变换开始，<math><mrow><msub><mi mathvariant="bold">M</mi>
    <mn>1</mn></msub> <mi mathvariant="bold">x</mi><mo>+</mo><msub><mi mathvariant="bold">b</mi>
    <mn>1</mn></msub></mrow></math>。然后我们通过一个非线性函数<math><mrow><mi>𝜙</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>将结果传递，然后对结果应用第二个线性变换。函数<math><mrow><mi>𝜙</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>，也就是*激活函数*，是使这个工作正常运行的重要部分。没有它，模型仍然是线性的，比之前的模型没有更强大。线性组合的线性组合本身只不过是原始输入的线性组合！通过插入一个非线性，我们使模型能够学习更广泛的函数。
- en: 'We don’t need to stop at two linear transforms. We can stack as many as we
    want on top of each other:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要停在两个线性变换上。我们可以堆叠任意多个线性变换在一起：
- en: <math><mrow><msub><mi mathvariant="bold">h</mi> <mn>1</mn></msub> <mo>=</mo>
    <msub><mi>𝜙</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>1</mn></msub> <mi mathvariant="bold">x</mi> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math><math><mrow><msub><mi mathvariant="bold">h</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>𝜙</mi> <mn>2</mn></msub> <mrow><mo>(</mo>
    <msub><mi mathvariant="bold">M</mi> <mn>2</mn></msub> <msub><mi mathvariant="bold">h</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi mathvariant="bold">h</mi> <mn>1</mn></msub> <mo>=</mo>
    <msub><mi>𝜙</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mn>1</mn></msub> <mi mathvariant="bold">x</mi> <mo>+</mo> <msub><mi mathvariant="bold">b</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math><math><mrow><msub><mi mathvariant="bold">h</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>𝜙</mi> <mn>2</mn></msub> <mrow><mo>(</mo>
    <msub><mi mathvariant="bold">M</mi> <mn>2</mn></msub> <msub><mi mathvariant="bold">h</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mrow></math>
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <math><mrow><msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <msub><mi>𝜙</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math><math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo>
    <msub><mi>𝜙</mi> <mi>n</mi></msub> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mi>n</mi></msub> <msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <msub><mi>𝜙</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mrow><mi>n</mo>-<mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math><math><mrow><mi mathvariant="bold">y</mi> <mo>=</mo>
    <msub><mi>𝜙</mi> <mi>n</mi></msub> <mrow><mo>(</mo> <msub><mi mathvariant="bold">M</mi>
    <mi>n</mi></msub> <msub><mi mathvariant="bold">h</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: This model is called a *multilayer perceptron*, or MLP for short. The middle
    steps <math><msub><mi>h</mi> <mi>i</mi></msub></math> are called *hidden layers*.
    The name refers to the fact that they are neither inputs nor outputs, just intermediate
    values used in the process of calculating the result. Also notice that we have
    added a subscript to each <math><mrow><mi>𝜙</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>.
    This indicates that different layers might use different nonlinearities.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型被称为*多层感知器*，简称MLP。中间步骤<math><msub><mi>h</mi> <mi>i</mi></msub></math>被称为*隐藏层*。这个名字指的是它们既不是输入也不是输出，只是在计算结果的过程中使用的中间值。还要注意，我们为每个<math><mrow><mi>𝜙</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>添加了一个下标。这表示不同的层可能使用不同的非线性。
- en: You can visualize this calculation as a stack of layers, as shown in [Figure 2-2](#a_multilayer_perceptron_viewed_as_a_stack).
    Each layer corresponds to a linear transformation followed by a nonlinearity.
    Information flows from one layer to another, the output of one layer becoming
    the input to the next. Each layer has its own set of parameters that determine
    how its output is calculated from its input.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将这个计算视为一堆层，如[图2-2](#a_multilayer_perceptron_viewed_as_a_stack)所示。每一层对应于一个线性变换，后面跟着一个非线性。信息从一层流向另一层，一层的输出成为下一层的输入。每一层都有自己的一组参数，这些参数确定了如何从输入计算输出。
- en: '![](Images/dlls_0202.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlls_0202.png)'
- en: Figure 2-2\. A multilayer perceptron, viewed as a stack of layers with information
    flowing from one layer to the next.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。一个多层感知器，被视为一堆层，信息从一层流向下一层。
- en: Multilayer perceptrons and their variants are also sometimes called *neural
    networks*. The name reflects the parallels between machine learning and neurobiology.
    A biological neuron connects to many other neurons. It receives signals from them,
    adds the signals together, and then sends out its own signals based on the result.
    As a very rough approximation, you can think of MLPs as working the same way as
    the neurons in your brain!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器及其变体有时也被称为*神经网络*。这个名称反映了机器学习和神经生物学之间的相似之处。生物神经元连接到许多其他神经元。它从它们那里接收信号，将信号相加，然后根据结果发送自己的信号。作为一个非常粗略的近似，你可以将MLP想象成与你的大脑中的神经元工作方式相同！
- en: What should the activation function <math><mrow><mi>𝜙</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>
    be? The surprising answer is that it mostly doesn’t matter. Of course, that is
    not entirely true. It obviously does matter, but not as much as you might expect.
    Nearly any reasonable function (monotonic, reasonably smooth) can work. Lots of
    different functions have been tried over the years, and although some work better
    than others, nearly all of them can produce decent results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数<math><mrow><mi>𝜙</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow></math>应该是什么？令人惊讶的答案是，这大部分并不重要。当然，这并不完全正确。显然它很重要，但并不像你可能期望的那样重要。几乎任何合理的函数（单调，相当平滑）都可以工作。多年来已经尝试了许多不同的函数，尽管有些比其他的效果更好，但几乎所有的函数都可以产生不错的结果。
- en: The most popular activation function today is probably the *rectified linear
    unit* (ReLU), <math><mrow><mi>𝜙</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo> <mi>max</mi>
    <mo>(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo>)</mo></mrow></math> . If you aren’t
    sure what function to use, this is probably a good default. Other common choices
    include the *hyperbolic tangent*, <math><mrow><mi>tanh</mi> <mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>,
    and the *logistic sigmoid*, <math><mrow><mi>𝜙</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mn>1</mn><mo>/</mo><mrow><mo>(</mo><mn>1</mn>
    <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup> <mo>)</mo></mrow></mrow></math>.
    All of these functions are shown in [Figure 2-3](#three_common_activation_functions).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 今天最流行的激活函数可能是*修正线性单元*（ReLU），<math><mrow><mi>𝜙</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo>
    <mi>max</mi> <mo>(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo>)</mo></mrow></math>。如果你不确定使用什么函数，这可能是一个很好的默认选择。其他常见选择包括*双曲正切*，<math><mrow><mi>tanh</mi>
    <mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>，和*逻辑Sigmoid*，<math><mrow><mi>𝜙</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mn>1</mn><mo>/</mo><mrow><mo>(</mo><mn>1</mn>
    <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup> <mo>)</mo></mrow></mrow></math>。所有这些函数都显示在[图2-3](#three_common_activation_functions)中。
- en: '![](Images/dlls_0203.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlls_0203.png)'
- en: 'Figure 2-3\. Three common activation functions: the rectified linear unit,
    hyperbolic tangent, and logistic sigmoid.'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3。三种常见的激活函数：修正线性单元，双曲正切和逻辑Sigmoid。
- en: 'We also must choose two other properties for an MLP: its *width* and its *depth*.
    With the simple linear model, we had no choices to make. Given the lengths of
    <math><mi mathvariant="bold">x</mi></math> and <math><mi mathvariant="bold">y</mi></math>,
    the sizes of <math><mi mathvariant="bold">M</mi></math> and <math><mi mathvariant="bold">b</mi></math>
    were completely determined. Not so with hidden layers. Width refers to the size
    of the hidden layers. We can choose each <math><msub><mi mathvariant="bold">h</mi><mi>i</mi></msub></math>
    to have any length we want. Depending on the problem, you might want them to be
    much larger or much smaller than the input and output vectors.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须为MLP选择另外两个属性：它的*宽度*和*深度*。对于简单的线性模型，我们没有选择。鉴于<math><mi mathvariant="bold">x</mi></math>和<math><mi
    mathvariant="bold">y</mi></math>的长度，<math><mi mathvariant="bold">M</mi></math>和<math><mi
    mathvariant="bold">b</mi></math>的大小完全确定。但隐藏层不是这样。宽度指的是隐藏层的大小。我们可以选择每个<math><msub><mi
    mathvariant="bold">h</mi><mi>i</mi></msub></math>的长度。根据问题的不同，你可能希望它们比输入和输出向量大得多或小得多。
- en: Depth refers to the number of layers in the model. A model with only one hidden
    layer is described as *shallow*. A model with many hidden layers is described
    as *deep*. This is, in fact, the origin of the term “deep learning”; it simply
    means “machine learning using models with lots of layers.”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 深度指的是模型中的层数。只有一个隐藏层的模型被描述为*浅层*。有许多隐藏层的模型被描述为*深层*。事实上，这就是“深度学习”这个术语的起源；它只是意味着“使用具有许多层的模型的机器学习”。
- en: 'Choosing the number and widths of layers in your model involves as much art
    as science. Or, to put it more formally, “This is still an active field of research.”
    Often it just comes down to trying lots of combinations and seeing what works.
    There are a few principles that may provide guidance, however, or at least help
    you understand your results in hindsight:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中选择层数和宽度涉及到与科学一样多的艺术。或者，更正式地说，“这仍然是一个活跃的研究领域。”通常只是尝试很多组合，看看哪种有效。然而，有一些原则可能提供指导，或者至少帮助你事后理解你的结果：
- en: An MLP with one hidden layer is a *universal approximator*.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有一个隐藏层的MLP是一个*通用逼近器*。
- en: 'This means it can approximate any function at all (within certain fairly reasonable
    limits). In a sense, you never need more than one hidden layer. That is already
    enough to reproduce any function you are ever likely to want. Unfortunately, this
    result comes with a major caveat: the accuracy of the approximation depends on
    the width of the hidden layer, and you may need a very wide layer to get sufficient
    accuracy for a given problem. This brings us to the second principle.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这意味着它可以近似任何函数（在某些相当合理的限制范围内）。在某种意义上，你永远不需要超过一个隐藏层。这已经足够复制你可能想要的任何函数。不幸的是，这个结果带来了一个重要的警告：近似的准确性取决于隐藏层的宽度，你可能需要一个非常宽的层来获得对于特定问题的足够准确性。这将引出我们到第二个原则。
- en: Deep models tend to require fewer parameters than shallow ones.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度模型通常需要比浅层模型更少的参数。
- en: 'This statement is intentionally somewhat vague. More rigorous statements can
    be proven for particular special cases, but it does still apply as a general guideline.
    Here is perhaps a better way of stating it: every problem requires a model with
    a certain depth to efficiently achieve acceptable accuracy. At shallower depths,
    the required widths of the layers (and hence the total number of parameters) increase
    rapidly. This makes it sound like you should always prefer deep models over shallow
    ones. Unfortunately, it is partly contradicted by the third principle.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个陈述故意有些模糊。对于特定特殊情况可以证明更严格的陈述，但它仍然适用作为一个一般指导原则。也许以下是更好的陈述方式：每个问题都需要一个具有一定深度的模型才能有效地实现可接受的准确性。在较浅的深度下，层的宽度（因此参数的总数）会迅速增加。这使得听起来你应该总是更喜欢深度模型而不是浅层模型。不幸的是，这在一定程度上与第三个原则相矛盾。
- en: Deep models tend to be harder to train than shallow ones.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度模型往往比浅层模型更难训练。
- en: 'Until about 2007, most machine learning models were shallow. The theoretical
    advantages of deep models were known, but researchers were usually unsuccessful
    at training them. Since then, a series of advances has gradually improved the
    usefulness of deep models. These include better training algorithms, new types
    of models that are easier to train, and of course faster computers combined with
    larger datasets on which to train the models. These advances gave rise to “deep
    learning” as a field. Yet despite the improvements, the general principle remains
    true: deeper models tend to be harder to train than shallower ones.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直到2007年左右，大多数机器学习模型都是浅层的。深度模型的理论优势是已知的，但研究人员通常无法成功地训练它们。从那时起，一系列进步逐渐提高了深度模型的实用性。这些进步包括更好的训练算法，更容易训练的新型模型，当然还有更快的计算机以及更大的数据集，用于训练模型。这些进步催生了“深度学习”作为一个领域。然而，尽管有所改进，总体原则仍然成立：深度模型往往比浅层模型更难训练。
- en: Training Models
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'This brings us to the next subject: just how do we train a model anyway? MLPs
    provide us with a (mostly) generic model that can be used for any problem. (We
    will discuss other, more specialized types of models a little later.) Now we want
    a similarly generic algorithm to find the optimal values of the model’s parameters
    for a given problem. How do we do that?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到下一个主题：我们到底如何训练一个模型？MLPs为我们提供了一个（大多数）通用的模型，可以用于任何问题。（我们稍后将讨论其他更专门的模型类型。）现在我们需要一个类似的通用算法来找到给定问题的模型参数的最佳值。我们该怎么做？
- en: The first thing you need, of course, is a collection of data to train it on.
    This dataset is known as the *training set*. It should consist of a large number
    of (**x**,**y**) pairs, also known as *samples*. Each sample specifies an input
    to the model, and what you want the model’s output to be when given that input.
    For example, the training set could be a collection of images, along with labels
    indicating whether or not each image contains a cat.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要的当然是一组数据来进行训练。这个数据集被称为*训练集*。它应该包含大量的(**x**,**y**)对，也被称为*样本*。每个样本指定了模型的输入，以及当给定该输入时希望模型的输出是什么。例如，训练集可以是一组图像，以及标签指示每个图像是否包含猫。
- en: 'Next you need to define a loss function <math><mrow><mi>L</mi><mo>(</mo><mi
    mathvariant="bold">y</mi><mo>,</mo><mover accent="true"><mi mathvariant="bold">y</mi>
    <mo>^</mo></mover><mo>)</mo></mrow></math>, where <math><mi mathvariant="bold">y</mi></math>
    is the actual output from the model and <math><mover accent="true"><mi mathvariant="bold">y</mi>
    <mo>^</mo></mover></math> is the target value specified in the training set. This
    is how you measure whether the model is doing a good job of reproducing the training
    data. It is then averaged over every sample in the training set:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要定义一个损失函数 <math><mrow><mi>L</mi><mo>(</mo><mi mathvariant="bold">y</mi><mo>,</mo><mover
    accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover><mo>)</mo></mrow></math>，其中
    <math><mi mathvariant="bold">y</mi></math> 是模型的实际输出，<math><mover accent="true"><mi
    mathvariant="bold">y</mi> <mo>^</mo></mover></math> 是训练集中指定的目标值。这是你衡量模型是否很好地复制训练数据的方式。然后对训练集中的每个样本进行平均：
- en: <math><mi>average loss</mi> <mo>=</mo> <mrow><mfrac><mn>1</mn> <mi>N</mi></mfrac>
    <munderover><mo>Σ</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mi>L</mi> <mrow><mo>(</mo> <msub><mi mathvariant="bold">y</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mover accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mi>平均损失</mi> <mo>=</mo> <mrow><mfrac><mn>1</mn> <mi>N</mi></mfrac> <munderover><mo>Σ</mo>
    <rrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover> <mi>L</mi>
    <mrow><mo>(</mo> <msub><mi mathvariant="bold">y</mi> <mi>i</mi></msub> <mo>,</mo>
    <msub><mover accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math>
- en: <math><mrow><mi>L</mi><mo>(</mo><mi mathvariant="bold">y</mi><mo>,</mo><mover
    accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover><mo>)</mo></mrow></math>
    should be small when its arguments are close together and large when they are
    far apart. In other words, we take every sample in the training set, try using
    each one as an input to the model, and see how close the output is to the target
    value. Then we average this over the whole training set.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>L</mi><mo>(</mo><mi mathvariant="bold">y</mi><mo>,</mo><mover
    accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover><mo>)</mo></mrow></math>
    当其参数接近时应该很小，而当它们相距较远时应该很大。换句话说，我们拿训练集中的每个样本，尝试将每个样本作为模型的输入，并查看输出与目标值的接近程度。然后我们对整个训练集进行平均。
- en: An appropriate loss function needs to be chosen for each problem.  A common
    choice is the Euclidean distance (also known as the <math><msub><mi>L</mi><mi>2</mi></msub></math>
    distance), <math><mrow><mi>L</mi> <mrow><mo>(</mo> <mi mathvariant="bold">y</mi>
    <mo>,</mo> <mover accent="true"><mi mathvariant="bold">y</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><msub><mo>Σ</mo> <mi>i</mi></msub> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
    . (In this expression, <math><msub><mi>y</mi><mi>i</mi></msub></math> means the
    *i*‘th component of the vector <math><mi mathvariant="bold">y</mi></math>.) When
    <math><mi mathvariant="bold">y</mi></math> represents a probability distribution,
    a popular choice is the cross entropy, <math><mrow><mi>L</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">y</mi> <mo>,</mo> <mover accent="true"><mi mathvariant="bold">y</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo><msub><mo>Σ</mo> <mi>i</mi></msub>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo form="prefix">log</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>i</mi></msub></mrow></math> . Other choices are also possible,
    and there is no universal “best” choice. It depends on the details of your problem.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a way to measure how well the model works, we need a way to
    improve it. We want to search for the parameter values that minimize the average
    loss over the training set. There are many ways to do this, but most work in deep
    learning uses some variant of the *gradient descent* algorithm. Let <math><mi>θ</mi></math>
    represent the set of all parameters in the model. Gradient descent involves taking
    a series of small steps:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mi>θ</mi> <mo>←</mo> <mi>θ</mi> <mo>-</mo> <mi>ϵ</mi> <mfrac><mi>∂</mi>
    <mrow><mi>∂</mi><mi>θ</mi></mrow></mfrac> <mrow><mo>〈</mo> <mi>L</mi> <mo>〉</mo></mrow></mrow></math>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math><mrow><mo>〈</mo> <mi>L</mi> <mo>〉</mo></mrow></math> is the average
    loss over the training set. Each step moves a tiny distance in the “downhill”
    direction. It changes each of the model’s parameters by a little bit, with the
    goal of causing the average loss to decrease. If all the stars align and the phase
    of the moon is just right, this will eventually produce parameters that do a good
    job of solving your problem. <math><mi>ϵ</mi></math> is called the *learning rate*,
    and it determines how much the parameters change on each step. It needs to be
    chosen very carefully: too small a value will cause learning to be very slow,
    while too large a value will prevent the algorithm from learning at all.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm really does work, but it has a serious problem. For every step
    of gradient descent, we need to loop over every sample in the training set. That
    means the time required to train the model is proportional to the size of the
    training set! Suppose that you have one million samples in the training set, that
    computing the gradient of the loss for one sample requires one million operations,
    and that it takes one million steps to find a good model. (All of these numbers
    are fairly typical of real deep learning applications.) Training will then require
    *one quintillion* operations. That takes quite a long time, even on a fast computer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there is a better solution: estimate <math><mrow><mo>〈</mo> <mi>L</mi>
    <mo>〉</mo></mrow></math> by averaging over a much smaller number of samples. This
    is the basis of the *stochastic gradient descent* (SGD) algorithm. For every step,
    we take a small set of samples (known as a *batch*) from the training set and
    compute the gradient of the loss function, averaged over only the samples in the
    batch. We can view this as an estimate of what we would have gotten if we had
    averaged over the entire training set, although it may be a very noisy estimate.
    We perform a single step of gradient descent, then select a new batch of samples
    for the next step.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个更好的解决方案：通过对少量样本进行平均来估计<math><mrow><mo>〈</mo> <mi>L</mi> <mo>〉</mo></mrow></math>。这是*随机梯度下降*（SGD）算法的基础。对于每一步，我们从训练集中取一小组样本（称为*批次*），计算损失函数的梯度，仅在批次中的样本上进行平均。我们可以将其视为对整个训练集进行平均的估计，尽管这可能是一个非常嘈杂的估计。我们执行一步梯度下降，然后为下一步选择一个新的样本批次。
- en: This algorithm tends to be much faster. The time required for each step depends
    only on the size of each batch, which can be quite small (often on the order of
    100 samples) and is independent of the size of the training set. The disadvantage
    is that each step does a less good job of reducing the loss, because it is based
    on a noisy estimate of the gradient rather than the true gradient. Still, it leads
    to a much shorter training time overall.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种算法往往更快。每个步骤所需的时间仅取决于每个批次的大小，这可以相当小（通常在100个样本左右的数量级），并且与训练集的大小无关。缺点是每个步骤在减少损失方面的效果较差，因为它是基于梯度的嘈杂估计而不是真实梯度。尽管如此，它确实会导致整体训练时间大大缩短。
- en: Most optimization algorithms used in deep learning are based on SGD, but there
    are many variations that improve on it in different ways. Fortunately, you can
    usually treat these algorithms as black boxes and trust them to do the right thing
    without understanding all the details of how they work. Two of the most popular
    algorithms used today are called Adam and RMSProp. If you are in doubt about what
    algorithm to use, either one of those will probably be a reasonable choice.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中使用的大多数优化算法都基于SGD，但有许多改进它的变体。幸运的是，您通常可以将这些算法视为黑匣子，并相信它们会在不了解其工作细节的情况下做正确的事情。今天使用最广泛的两种算法称为Adam和RMSProp。如果您对要使用的算法有疑问，那么这两种算法中的任何一种可能都是一个合理的选择。
- en: Validation
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证
- en: Suppose you have done everything described so far. You collected a large set
    of training data. You selected a model, then ran a training algorithm until the
    loss became very small. Congratulations, you now have a function that solves your
    problem!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经按照迄今所述的一切做了。您收集了大量的训练数据。您选择了一个模型，然后运行了一个训练算法，直到损失变得非常小。恭喜，您现在有一个解决问题的函数！
- en: Right?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对吗？
- en: Sorry, it’s not that simple! All you really know for sure is that the function
    works well *on the training data*. You might hope it will also work well on other
    data, but you certainly can’t count on it. Now you need to validate the model
    to see whether it works on data that it hasn’t been specifically trained on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，事情并不那么简单！您唯一确切知道的是该函数在*训练数据*上表现良好。您可能希望它在其他数据上也表现良好，但肯定不能指望。现在您需要验证模型，看看它是否适用于未经专门训练的数据。
- en: 'To do this you need a second dataset, called the *test set*. It has exactly
    the same form as the training set, a collection of <math><mrow><mo>(</mo> <mi
    mathvariant="bold">x</mi> <mo>,</mo> <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></math>
    pairs, but the two should have no samples in common. You train the model on the
    training set, then test it on the test set. This brings us to one of the most
    important principles in machine learning:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，您需要第二个数据集，称为*测试集*。它与训练集具有完全相同的形式，即一组<math><mrow><mo>(</mo> <mi mathvariant="bold">x</mi>
    <mo>,</mo> <mi mathvariant="bold">y</mi> <mo>)</mo></mrow></math>对，但两者不应有共同的样本。您在训练集上训练模型，然后在测试集上测试它。这将引出机器学习中最重要的原则之一：
- en: You must not use the test set in any way while designing or training the model.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设计或训练模型时，绝对不能使用测试集。
- en: In fact, it is best if you never even look at the data in the test set. Test
    set data is only for testing the fully trained model to find out how well it works.
    If you allow the test set to influence the model in any way, you risk getting
    a model that works better on the test set than on other data that was not involved
    in creating the model. It ceases to be a true test set, and becomes just another
    type of training set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，最好永远不要查看测试集中的数据。测试集数据仅用于测试完全训练的模型，以了解其工作效果如何。如果允许测试集以任何方式影响模型，您就有可能得到一个在测试集上比在未参与创建模型的其他数据上表现更好的模型。它不再是一个真正的测试集，而只是另一种类型的训练集。
- en: This is connected to the mathematical concept of *overfitting*. The training
    data is supposed to be representative of a much larger data distribution, the
    set of all inputs you might ever want to use the model on. But you can’t train
    it on all possible inputs. You can only create a finite set of training samples,
    train the model on those, and hope it learns general strategies that work equally
    well on other samples. Overfitting is what happens when the training picks up
    on specific features of the training samples, such that the model works better
    on them than it does on other samples.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这与数学概念*过拟合*有关。训练数据应该代表一个更大的数据分布，即您可能希望在模型上使用的所有输入集。但您无法对所有可能的输入进行训练。您只能创建一个有限的训练样本集，对其进行训练，并希望它学习到适用于其他样本的通用策略。过拟合是指当训练捕捉到训练样本的特定特征时，模型在这些样本上的表现比其他样本更好。
- en: Regularization
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: Overfitting is a major problem for anyone who uses machine learning. Given that,
    you won’t be surprised to learn that lots of techniques have been developed for
    avoiding it. These techniques are collectively known as *regularization*. The
    goal of any regularization technique is to avoid overfitting and produce a trained
    model that works well on any input, not just the particular inputs that were used
    for training.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是任何使用机器学习的人都会遇到的主要问题。鉴于此，您不会感到惊讶地了解到已经开发了许多技术来避免过拟合。这些技术统称为*正则化*。任何正则化技术的目标都是避免过拟合，并产生一个在任何输入上都表现良好的训练模型，而不仅仅是用于训练的特定输入。
- en: Before we discuss particular regularization techniques, there are two very important
    points to understand about it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论特定的正则化技术之前，有两个非常重要的要点需要了解。
- en: 'First, the best way to avoid overfitting is *almost always* to get more training
    data. The bigger your training set, the better it represents the “true” data distribution,
    and the less likely the learning algorithm is to overfit. Of course, that is sometimes
    impossible: maybe you simply have no way to get more data, or the data may be
    very expensive to collect. In that case, you just have to do the best you can
    with the data you have, and if overfitting is a problem, you will have to use
    regularization to avoid it. But more data will probably lead to a better result
    than regularization.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，避免过拟合的最佳方法 *几乎总是* 获得更多的训练数据。您的训练集越大，它越能更好地代表“真实”数据分布，学习算法过拟合的可能性就越小。当然，有时这是不可能的：也许您根本无法获得更多数据，或者收集数据可能非常昂贵。在这种情况下，您只能尽力利用手头的数据，如果过拟合是一个问题，您将不得不使用正则化来避免它。但更多的数据可能会比正则化产生更好的结果。
- en: Second, there is no universally “best” way to do regularization. It all depends
    on the problem. After all, the training algorithm doesn’t know that it’s overfitting.
    All it knows about is the training data. It doesn’t know how the true data distribution
    differs from the training data, so the best it can do is produce a model that
    works well on the training set. If that isn’t what you want, it’s up to you to
    tell it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，没有普遍“最佳”的正则化方法。这一切都取决于问题。毕竟，训练算法并不知道自己正在过拟合。它只知道训练数据。它不知道真实数据分布与训练数据的差异，因此它能做的最好就是生成一个在训练集上表现良好的模型。如果这不是您想要的，那就由您告诉它。
- en: 'That is the essence of any regularization method: biasing the training process
    to prefer certain types of models over others. You make assumptions about what
    properties a “good” model should have, and how it differs from an overfit one,
    and then you tell the training algorithm to prefer models with those properties.
    Of course, those assumptions are often implicit rather than explicit. It may not
    be obvious what assumptions you are making by choosing a particular regularization
    method. But they are always there.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是任何正则化方法的本质：偏向训练过程，更喜欢某些类型的模型而不是其他类型。您对“好”模型应该具有哪些特性以及它与过拟合模型有何不同做出假设，然后告诉训练算法更喜欢具有这些特性的模型。当然，这些假设通常是隐含的而不是明确的。通过选择特定的正则化方法，您可能不清楚自己做出了什么假设。但它们总是存在的。
- en: One of the simplest regularization methods is just to train the model for fewer
    steps. Early in training, it tends to pick up on coarse properties of the training
    data that likely apply to the true distribution. The longer it runs, the more
    likely it is to start picking up on fine details of particular training samples.
    By limiting the number of training steps, you give it less opportunity to overfit.
    More formally, you are really assuming that “good” parameter values should not
    be too different from whatever values you start training from.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的正则化方法之一就是减少模型的训练步骤。在训练初期，模型倾向于捕捉到训练数据的粗略特性，这些特性可能适用于真实分布。它运行得越久，就越有可能开始捕捉特定训练样本的细节。通过限制训练步骤的数量，您减少了过拟合的机会。更正式地说，您实际上是假设“好”的参数值不应该与您开始训练时的值有太大的不同。
- en: Another method is to restrict the magnitude of the parameters in the model.
    For example, you might add a term to the loss function that is proportional to
    <math><msup><mrow><mo>|</mo><mi>θ</mi><mo>|</mo></mrow> <mn>2</mn></msup></math>
    , where <math><mi>θ</mi></math> is a vector containing all of the model’s parameters.
    By doing this, you are assuming that “good” parameter values should not be any
    larger than necessary. It reflects the fact that overfitting often (though not
    always) involves some parameters becoming very large.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是限制模型中参数的大小。例如，您可以向损失函数添加一个与 <math><msup><mrow><mo>|</mo><mi>θ</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math> 成比例的项，其中 <math><mi>θ</mi></math> 是包含模型所有参数的向量。通过这样做，您假设“好”的参数值不应该比必要的大。这反映了过拟合通常（虽然不总是）涉及一些参数变得非常大的事实。
- en: 'A very popular method of regularization is called *dropout*. It involves doing
    something that at first seems ridiculous, but actually works surprisingly well.
    For each hidden layer in the model, you randomly select a subset of elements in
    the output vector <math><msub><mi>h</mi><mi>i</mi></msub></math> and set them
    to 0\. On every step of gradient descent, you pick a different random subset of
    elements. This might seem like it would just break the model: how can you expect
    it to work when internal calculations keep randomly getting set to 0? The mathematical
    theory for why dropout works is a bit complicated. Very roughly speaking, by using
    dropout you are assuming that no individual calculation within the model should
    be too important. You should be able to randomly remove any individual calculation,
    and the rest of the model should continue to work without it. This forces it to
    learn redundant, highly distributed representations of data that make overfitting
    unlikely. If you are unsure of what regularization method to use, dropout is a
    good first thing to try.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常流行的正则化方法称为*辍学*。它涉及做一些乍看起来荒谬的事情，但实际上效果出奇的好。对于模型中的每个隐藏层，您随机选择输出向量<math><msub><mi>h</mi><mi>i</mi></msub></math>中的一部分元素，并将它们设置为0。在梯度下降的每一步中，您选择不同的随机元素子集。这可能看起来会破坏模型：当内部计算不断随机设置为0时，您如何指望它能够工作？辍学为什么有效的数学理论有点复杂。非常粗略地说，通过使用辍学，您假设模型中没有任何个别计算应该太重要。您应该能够随机删除任何个别计算，而模型的其余部分应该继续在没有它的情况下工作。这迫使它学习冗余的、高度分布的数据表示，使过度拟合不太可能发生。如果您不确定要使用哪种正则化方法，辍学是一个不错的尝试。
- en: Hyperparameter Optimization
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化
- en: 'By now you have probaly noticed that there are a lot of choices to make, even
    when using a supposedly generic model with a “generic” learning algorithm. Examples
    include:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您可能已经注意到，即使使用一个被认为是通用的模型和“通用”学习算法，也有很多选择要做。例如：
- en: The number of layers in the model
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型中的层数
- en: The width of each layer
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层的宽度
- en: The number of training steps to perform
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要执行的训练步数
- en: The learning rate to use during training
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程中使用的学习率
- en: The fraction of elements to set to 0 when using dropout
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用辍学时要设置为0的元素的比例
- en: These options are called *hyperparameters*. A hyperparameter is any aspect of
    the model or training algorithm that must be set in advance rather than being
    learned by the training algorithm. But how are you supposed to choose them—and
    isn’t the whole point of machine learning to select settings automatically based
    on data?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项称为*超参数*。超参数是模型或训练算法的任何方面，必须事先设置而不是由训练算法学习。但是您应该如何选择它们呢？难道机器学习的整个目的不是根据数据自动选择设置吗？
- en: 'This brings us to the subject of *hyperparameter optimization*. The simplest
    way of doing it is just to try lots of values for each hyperparameter and see
    what works best. This becomes very expensive when you want to try lots of values
    for lots of hyperparameters, so there are more sophisticated approaches, but the
    basic idea remains the same: try different combinations and see what works best.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到*超参数优化*的主题。最简单的方法就是尝试每个超参数的许多值，看看哪个效果最好。当您想尝试许多超参数的许多值时，这变得非常昂贵，因此有更复杂的方法，但基本思想仍然相同：尝试不同的组合，看看哪个效果最好。
- en: But how can you tell what works best? The simplest answer would be to just see
    what produces the lowest value of the loss function (or some other measure of
    accuracy) on the training set. But remember, that isn’t what we really care about.
    We want to minimize error on the test set, not the training set. This is especially
    important for hyperparameters that affect regularization, such as the dropout
    rate. A low training set error might just mean the model is overfitting, optimizing
    for the precise details of the training data. So instead we want to try lots of
    hyperparameter values, then use the ones that minimize the loss on the test set.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如何确定哪个效果最好呢？最简单的答案可能是看看哪个产生了训练集上损失函数（或其他准确度度量）的最低值。但请记住，这不是我们真正关心的。我们想要最小化测试集上的错误，而不是训练集。这对于影响正则化的超参数尤为重要，例如辍学率。低训练集错误可能只意味着模型过度拟合，优化于训练数据的精确细节。因此，我们希望尝试许多超参数值，然后使用在测试集上最小化损失的那些值。
- en: 'But we mustn’t do that! Remember: you must not use the test set in any way
    while designing or training the model. Its job is to tell you how well the model
    is likely to work on new data it has never seen before. Just because a particular
    set of hyperparameters happens to work best on the test set doesn’t guarantee
    those values will always work best. We must not allow the test set to influence
    the model, or it is no longer an unbiased test set.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不能这样做！记住：在设计或训练模型时，绝不能以任何方式使用测试集。它的作用是告诉您模型在从未见过的新数据上可能的工作效果。仅因为某个特定的超参数集恰好在测试集上效果最好，并不意味着这些值将始终效果最好。我们不能让测试集影响模型，否则它就不再是一个无偏的测试集。
- en: 'The solution is to create yet another dataset, which is called the *validation
    set*. It must not share any samples with either the training set or the test set.
    The full procedure now works as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是创建另一个数据集，称为*验证集*。它不能与训练集或测试集共享任何样本。完整的流程如下：
- en: For each set of hyperparameter values, train the model on the training set,
    then compute the loss on the validation set.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每组超参数值，在训练集上训练模型，然后计算验证集上的损失。
- en: Whichever set of hyperparameters give the lowest loss on the validation set,
    accept them as your final model.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受在验证集上给出最低损失的超参数集作为最终模型。
- en: Evaluate that final model on the test set to get an unbiased measure of how
    well it works.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终模型，以获得其工作效果的无偏度量。
- en: Other Types of Models
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他类型的模型
- en: 'This still leaves one more decision you need to make, and it is a huge subject
    in itself: what kind of model to use. Earlier in this chapter we introduced multilayer
    perceptrons. They have the advantage of being a generic class of models that can
    be applied to many different problems. Unfortunately, they also have serious disadvantages.
    They require a huge number of parameters, which makes them very susceptible to
    overfitting. They become difficult to train when they have more than one or two
    hidden layers. In many cases, you can get a better result by using a less generic
    model that takes advantage of specific features of your problem.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这还留下了一个你需要做出的决定，这本身就是一个庞大的主题：使用什么样的模型。在本章的前面，我们介绍了多层感知器。它们的优点是作为一类通用模型，可以应用于许多不同的问题。不幸的是，它们也有严重的缺点。它们需要大量的参数，这使它们非常容易过拟合。当它们有超过一两个隐藏层时，训练变得困难。在许多情况下，通过使用一个更少通用的模型，利用问题的特定特征，你可以获得更好的结果。
- en: Much of the content of this book consists of discussing particular types of
    models that are especially useful in the life sciences. Those can wait until later
    chapters. But for the purposes of this introduction, there are two very important
    classes of models we should discuss that are widely used in many different fields.
    They are called convolutional neural networks and recurrent neural networks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的大部分内容都是讨论在生命科学中特别有用的特定类型的模型。这些可以等到后面的章节再讨论。但是在这个介绍中，有两类非常重要的模型我们应该讨论，它们在许多不同领域中被广泛使用。它们被称为卷积神经网络和循环神经网络。
- en: Convolutional Neural Networks
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '*Convolutional neural networks* (CNNs for short) were one of the very first
    classes of deep models to be widely used. They were developed for use in image
    processing and computer vision. They remain an excellent choice for many kinds
    of problems that involve continuous data sampled on a rectangular grid: audio
    signals (1D), images (2D), volumetric MRI data (3D), and so on.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积神经网络*（简称CNN）是最早被广泛使用的深度模型之一。它们被开发用于图像处理和计算机视觉。它们仍然是许多涉及在矩形网格上采样的连续数据的问题的绝佳选择：音频信号（1D）、图像（2D）、体积MRI数据（3D）等等。'
- en: They are also a class of models that truly justify the term “neural network.”
    The design of CNNs was originally inspired by the workings of the feline visual
    cortex. (Cats have played a central role in deep learning from the dawn of the
    field.) Research performed from the 1950s to the 1980s revealed that vision is
    processed through a series of layers. Each neuron in the first layer takes input
    from a small region of the visual field (its *receptive field*). Different neurons
    are specialized to detect particular local patterns or features, such as vertical
    or horizontal lines. Cells in the second layer take input from local clusters
    of cells in the first layer, combining their signals to detect more complicated
    patterns over a larger receptive field. Each layer can be viewed as a new representation
    of the original image, described in terms of larger and more abstract patterns
    than the ones in the previous layer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 它们也是一类真正体现“神经网络”这个术语的模型。CNN的设计最初受到猫的视觉皮层工作的启发。（猫在深度学习领域的黎明时期起到了核心作用。）从20世纪50年代到80年代进行的研究揭示了视觉是通过一系列层次来处理的。第一层中的每个神经元从视野的一个小区域（其*感受野*）接收输入。不同的神经元专门用于检测特定的局部模式或特征，如垂直或水平线。第二层中的细胞从第一层中的局部细胞群接收输入，结合它们的信号来检测更复杂的模式，覆盖更大的感受野。每一层可以被看作是原始图像的一个新表示，用更大更抽象的模式来描述比前一层中的模式。
- en: CNNs mirror this design, sending an input image through a series of layers.
    In that sense, they are just like MLPs, but the structure of each layer is very
    different. MLPs use *fully connected layers*. Every element of the output vector
    depends on every element of the input vector. CNNs use *convolutional layers*
    that take advantage of spatial locality. Each output element corresponds to a
    small region of the image, and only depends on the input values in that region.
    This enormously reduces the number of parameters defining each layer. In effect,
    it assumes that most elements of the weight matrix <math><msub><mi mathvariant="bold">M</mi><mi>i</mi></msub></math>
    are 0, since each output element only depends on a small number of input elements.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: CNN反映了这种设计，将输入图像通过一系列层。在这个意义上，它们就像MLP，但是每一层的结构是非常不同的。MLP使用*全连接层*。输出向量的每个元素都取决于输入向量的每个元素。CNN使用*卷积层*，利用了空间局部性。每个输出元素对应于图像的一个小区域，并且仅取决于该区域内的输入值。这极大地减少了定义每一层的参数数量。实际上，它假设权重矩阵<math><msub><mi
    mathvariant="bold">M</mi><mi>i</mi></msub></math>的大多数元素都是0，因为每个输出元素仅取决于少量的输入元素。
- en: 'Convolutional layers take this a step further: they assume the parameters are
    the same for *every local region of the image*. If a layer uses one set of parameters
    to detect horizontal lines at one location in the image, it also uses exactly
    the same parameters to detect horizontal lines everywhere else in the image. This
    makes the number of parameters for the layer independent of the size of the image.
    All it has to learn is a single *convolutional kernel* that defines how output
    features are computed from any local region of the image. That local region is
    often very small, perhaps 5 by 5 pixels. In that case, the number of parameters
    to learn is only 25 times the number of output features for each region. This
    is tiny compared to the number in a fully connected layer, making CNNs much easier
    to train and much less susceptible to overfitting than MLPs.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层进一步假设参数对于*图像的每个局部区域都是相同的*。如果一个层使用一组参数在图像的一个位置检测水平线，那么它也会使用完全相同的参数在图像的其他任何地方检测水平线。这使得该层的参数数量与图像的大小无关。它只需要学习一个单一的*卷积核*，定义了如何从图像的任何局部区域计算输出特征。这个局部区域通常非常小，可能是5乘5像素。在这种情况下，要学习的参数数量仅为每个区域的输出特征数量的25倍。与全连接层中的参数数量相比，这是微不足道的，使得CNN比MLP更容易训练，更不容易过拟合。
- en: Recurrent Neural Networks
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Recurrent neural networks* (RNNs for short) are a bit different. They are
    normally used to process data that takes the form of a sequence of elements: words
    in a text document, bases in a DNA molecule, etc. The elements in the sequence
    are fed into the network’s input one at a time. But then the network does something
    very different: the output from each layer is fed back into its own input on the
    next step! This allows RNNs to have a sort of memory.  When an element (word,
    DNA base, etc.) from the sequence is fed into the network, the input to each layer
    depends on that element, but also on all of the previous elements ([Figure 2-4](#recurrent-neural-network)).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/dlls_0204.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. A recurrent neural network. As each element ( <math><msub><mi>x</mi>
    <mn>1</mn></msub></math> , <math><msub><mi>x</mi> <mn>2</mn></msub></math> , ...)
    of the sequence is fed into the input, the output ( <math><msub><mi>y</mi> <mn>1</mn></msub></math>
    , <math><msub><mi>y</mi> <mn>2</mn></msub></math> , ...) depends both on the input
    element and on the RNN’s own output during the previous step.
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, the input to a recurrent layer has two parts: the regular input (that is,
    the output from the previous layer in the network) and the recurrent input (which
    equals its own output from the previous step). It then needs to calculate a new
    output based on those inputs. In principle you could use a fully connected layer,
    but in practice that usually doesn’t work very well. Researchers have developed
    other types of layers that work much better in RNNs. The two most popular ones
    are called the *gated recurrent unit* (GRU) and the *long short-term memory* (LSTM).
    Don’t worry about the details for now; just remember that if you are creating
    an RNN, you should usually build it out of one of those types of layers.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Having memory makes RNNs fundamentally different from the other models we have
    discussed. With a CNN or MLP, you simply feed a value into the network’s input
    and get a different value out. The output is entirely determined by the input.
    Not so with an RNN. The model has its own internal state, composed of the outputs
    of all its layers from the most recent step. Each time you feed a new value into
    the model, the output depends not just on the input value but also on the internal
    state. Likewise, the internal state is altered by each new input value. This makes
    RNNs very powerful, and allows them to be used for lots of different applications.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning is a huge subject, and this chapter has only given the briefest
    introduction to it. It should be enough to help you read and understand the rest
    of this book, but if you plan to do serious work in the field, you will want to
    acquire a much more thorough background. Fortunately, there are many excellent
    deep learning resources available online. Here are some suggestions for material
    you might consult:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '*[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)*
    by Michael Nielsen (Determination Press) covers roughly the same material as this
    chapter, but goes into far more detail on every subject. If you want a solid working
    knowledge of the fundamentals of deep learning, sufficient to make use of it in
    your own work, this is an excellent place to start.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Deep Learning*](http://www.deeplearningbook.org) by Ian Goodfellow, Yoshua
    Bengio, and Aaron Courville (MIT Press) is a more advanced introduction written
    by some of the top researchers in the field. It expects the reader to have a background
    similar to that of a graduate student in computer science and goes into far more
    detail on the mathematical theory behind the subject. You can easily use deep
    models without understanding all of the theory, but if you want to do original
    research in deep learning (rather than just using deep models as a tool to solve
    problems in other fields), this book is a fantastic resource.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*TensorFlow for Deep Learning*](http://shop.oreilly.com/product/0636920065869.do)
    by Bharath Ramsundar and Reza Zadeh (O’Reilly) provides a practitioner’s introduction
    to deep learning that seeks to build intuition about the core concepts without
    delving too deeply into the mathematical underpinnings of such models. It might
    be a useful reference for those who are interested in the practical aspects of
    deep learning.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《深度学习的TensorFlow》（O'Reilly出版）由Bharath Ramsundar和Reza Zadeh撰写，为从业者介绍了深度学习的核心概念，旨在建立直观理解，而不深入探讨这些模型的数学基础。对于那些对深度学习的实际方面感兴趣的人来说，这可能是一个有用的参考资料。
