["```py\nCreate a marketing plan for a new software product.\n```", "```py\n1\\. Launch social media campaign\n2\\. Send out email newsletters\n3\\. Offer limited-time discounts\n4\\. Collaborate with influencers\n5\\. Organize webinars\n```", "```py\nCreate a detailed marketing plan for a new project management software\nproduct targeting small and medium-sized businesses.\n\nThe budget is $20,000, and we want to focus on digital marketing channels.\n\nConsider the success of similar products in the market and provide a\nstep-by-step plan that includes the most effective promotional tactics.\n```", "```py\nMarket Research\na. Analyze competitors and their marketing strategies\nb. Identify the unique selling points of the new project management software\nc. Define the target audience and their pain points\n\nBranding and Messaging\na. Develop a consistent brand identity\nb. Craft compelling messaging that addresses the target audience's pain\npoints\nc. Create marketing assets such as website, blog, and social media content\n\n...\n\nEmail Marketing\na. Develop an email marketing strategy to nurture leads and convert them\ninto customers\nb. Send personalized onboarding emails to new users with helpful tips and\nresources\n```", "```py\nnext_action = agent.get_action(...)\nwhile next_action != AgentFinish:\n    observation = run(next_action)\n    next_action = agent.get_action(..., next_action, observation)\nreturn next_action\n```", "```py\nYou will attempt to solve the problem of finding the answer to a question.\nUse chain-of-thought reasoning to solve through the problem, using the\nfollowing pattern:\n\n1\\. Observe the original question:\noriginal_question: original_problem_text\n2\\. Create an observation with the following pattern:\nobservation: observation_text\n3\\. Create a thought based on the observation with the following pattern:\nthought: thought_text\n4\\. Use tools to act on the thought with the following pattern:\naction: tool_name\naction_input: tool_input\n\nDo not guess or assume the tool results. Instead, provide a structured\noutput that includes the action and action_input.\n\nYou have access to the following tools: {tools}.\n\noriginal_problem: {question}\n\nBased on the provided tool result:\n\nEither provide the next observation, action, action_input, or the final\nanswer if available.\n\nIf you are providing the final answer, you must return the following pattern:\n\"I've found the answer: final_answer\"\n```", "```py\nimport re\n\n# Sample text:\ntext = \"\"\"\nAction: search_on_google\nAction_Input: Tom Hanks's current wife\n\naction: search_on_wikipedia\naction_input: How old is Rita Wilson in 2023\n\naction : search_on_google\naction input: some other query\n\"\"\"\n\n# Compile regex patterns:\naction_pattern = re.compile(r\"(?i)action\\s*:\\s*([^\\n]+)\", re.MULTILINE)\naction_input_pattern = re.compile(r\"(?i)action\\s*_*input\\s*:\\s*([^\\n]+)\",\nre.MULTILINE)\n\n# Find all occurrences of action and action_input:\nactions = action_pattern.findall(text)\naction_inputs = action_input_pattern.findall(text)\n\n# Extract the last occurrence of action and action_input:\nlast_action = actions[-1] if actions else None\nlast_action_input = action_inputs[-1] if action_inputs else None\n\nprint(\"Last Action:\", last_action)\nprint(\"Last Action Input:\", last_action_input)\n# Last Action: search_on_google\n# Last Action Input: some other query\n```", "```py\ndef extract_last_action_and_input(text):\n    # Compile regex patterns\n    action_pattern = re.compile(r\"(?i)action\\s*:\\s*([^\\n]+)\", re.MULTILINE)\n    action_input_pattern = re.compile(\n        r\"(?i)action\\s*_*input\\s*:\\s*([^\\n]+)\", re.MULTILINE\n    )\n\n    # Find all occurrences of action and action_input\n    actions = action_pattern.findall(text)\n    action_inputs = action_input_pattern.findall(text)\n\n    # Extract the last occurrence of action and action_input\n    last_action = actions[-1] if actions else None\n    last_action_input = action_inputs[-1] if action_inputs else None\n\n    return {\"action\": last_action, \"action_input\": last_action_input}\n\nextract_last_action_and_input(text)\n# {'action': 'search_on_google', 'action_input': 'some other query'}\n```", "```py\ndef extract_final_answer(text):\n    final_answer_pattern = re.compile(\n        r\"(?i)I've found the answer:\\s*([^\\n]+)\", re.MULTILINE\n    )\n    final_answers = final_answer_pattern.findall(text)\n    if final_answers:\n        return final_answers[0]\n    else:\n        return None\n\nfinal_answer_text = \"I've found the answer: final_answer\"\nprint(extract_final_answer(final_answer_text))\n# final_answer\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import SystemMessagePromptTemplate\n```", "```py\nchat = ChatOpenAI(model_kwargs={\"stop\": [\"tool_result:\"],})\n```", "```py\ntools = {}\n\ndef search_on_google(query: str):\n    return f\"Jason Derulo doesn't have a wife or partner.\"\n\ntools[\"search_on_google\"] = {\n    \"function\": search_on_google,\n    \"description\": \"Searches on google for a query\",\n}\n```", "```py\nbase_prompt = \"\"\"\nYou will attempt to solve the problem of finding the answer to a question.\nUse chain-of-thought reasoning to solve through the problem, using the\nfollowing pattern:\n\n1\\. Observe the original question:\noriginal_question: original_problem_text\n2\\. Create an observation with the following pattern:\nobservation: observation_text\n3\\. Create a thought based on the observation with the following pattern:\nthought: thought_text\n4\\. Use tools to act on the thought with the following pattern:\naction: tool_name\naction_input: tool_input\n\nDo not guess or assume the tool results. Instead, provide a structured\noutput that includes the action and action_input.\n\nYou have access to the following tools: {tools}.\n\noriginal_problem: {question}\n\"\"\"\n```", "```py\noutput = chat.invoke(SystemMessagePromptTemplate \\\n.from_template(template=base_prompt) \\\n.format_messages(tools=tools, question=\"Is Jason Derulo with a partner?\"))\nprint(output)\n```", "```py\ntool_name = extract_last_action_and_input(output.content)[\"action\"]\ntool_input = extract_last_action_and_input(output.content)[\"action_input\"]\ntool_result = tools[tool_name][\"function\"](tool_input)\n```", "```py\nprint(f\"\"\"The agent has opted to use the following tool:\ntool_name: {tool_name}\ntool_input: {tool_input}\ntool_result: {tool_result}\"\"\"\n)\n```", "```py\ncurrent_prompt = \"\"\"\nYou are answering this query: Is Jason Derulo with a partner?\n\nBased on the provided tool result:\ntool_result: {tool_result}\n\nEither provide the next observation, action, action_input, or the final\nanswer if available. If you are providing the final answer, you must return\nthe following pattern: \"I've found the answer: final_answer\"\n\"\"\"\n```", "```py\noutput = chat.invoke(SystemMessagePromptTemplate. \\\nfrom_template(template=current_prompt) \\\n.format_messages(tool_result=tool_result))\n```", "```py\nprint(\"----------\\n\\nThe model output is:\", output.content)\nfinal_answer = extract_final_answer(output.content)\nif final_answer:\n    print(f\"answer: {final_answer}\")\nelse:\n    print(\"No final answer found.\")\n```", "```py\n'''content='1\\. Observe the original question:\\nIs Jason Derulo with a\npartner?\\n\\n2\\. Create an observation:\\nWe don\\'t have any information\nabout Jason Derulo\\'s relationship status.\\n\\n3\\. Create a thought based\non the observation:\\nWe can search for recent news or interviews to find\nout if Jason Derulo is currently with a partner.\\n\\n4\\. Use the tool to act\non the thought:\\naction: search_on_google\\naction_input: \"Jason Derulo\ncurrent relationship status\"' additional_kwargs={} example=False\n\n----------\nThe agent has opted to use the following tool:\ntool_name: search_on_google\ntool_input: \"Jason Derulo current relationship status\"\ntool_result: Jason Derulo doesn't have a wife or partner.\n----------\n\nThe second prompt shows\nBased on the provided tool result:\ntool_result: {tool_result}\n\nEither provide the next observation, action, action_input, or the final\nanswer if available. If you are providing the final answer, you must\nreturn the following pattern: \"I've found the answer: final_answer\"\n----------\n\nThe model output is: I've found the answer: Jason Derulo doesn't have a\nwife or partner. answer: Jason Derulo doesn't have a wife or partner.'''\n```", "```py\nYou are looking to accomplish: {goal}\nYou have access to the following {tools}\n```", "```py\n# Import necessary classes and functions:\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain import hub\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import Tool\n\n# Defining the LLM to use:\nmodel = ChatOpenAI()\n\n# Function to count the number of characters in a string:\ndef count_characters_in_string(string):\n    return len(string)\n\n# Create a list of tools:\n# Currently, only one tool is defined that counts characters in a text string.\ntools = [\n    Tool.from_function(\n        func=count_characters_in_string,\n        name=\"Count Characters in a text string\",\n        description=\"Count the number of characters in a text string\",\n    )\n]\n\n# Download a react prompt!\nprompt = hub.pull(\"hwchase17/react\")\n\n# Construct the ReAct agent:\nagent = create_react_agent(model, tools, prompt)\n\n# Initialize an agent with the defined tools and\n# Create an agent executor by passing in the agent and tools:\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n# Invoke the agent with a query to count the characters in the given word:\nagent_executor.invoke({\"input\": '''How many characters are in the word\n\"supercalifragilisticexpialidocious\"?'''})\n\n# 'There are 34 characters in the word \"supercalifragilisticexpialidocious\".'\n```", "```py\nEntering new AgentExecutor change...\nI should count the number of characters in the word\n\"supercalifragilisticexpiladocious\".\nAction: Count Characters in a text string\nAction Input: \"supercalifragilisticexpiladocious\"\nObservation: 34\nThought: I now know the final answer\nFinal Answer: There are 34 characters in the word\n\"supercalifragilisticexpiladocious\".\n```", "```py\n# Import necessary modules and functions from the langchain package:\nfrom langchain.chains import (\n    LLMMathChain,\n)\nfrom langchain import hub\nfrom langchain.agents import create_openai_functions_agent, Tool, AgentExecutor\nfrom langchain_openai.chat_models import ChatOpenAI\n\n# Initialize the ChatOpenAI with temperature set to 0:\nmodel = ChatOpenAI(temperature=0)\n\n# Create a LLMMathChain instance using the ChatOpenAI model:\nllm_math_chain = LLMMathChain.from_llm(llm=model, verbose=True)\n\n# Download the prompt from the hub:\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\n\ntools = [\n    Tool(\n        name=\"Calculator\",\n        func=llm_math_chain.run, # run the LLMMathChain\n        description=\"useful for when you need to answer questions about math\",\n        return_direct=True,\n    ),\n]\n\n# Create an agent using the ChatOpenAI model and the tools:\nagent = create_openai_functions_agent(llm=model, tools=tools, prompt=prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nresult = agent_executor.invoke({\"input\": \"What is 5 + 5?\"})\nprint(result)\n# {'input': 'What is 5 + 5?', 'output': 'Answer: 10'}\n```", "```py\ndef google_search(query: str) -> str:\n    return \"James Phoenix is 31 years old.\"\n\n# List of tools that the agent can use:\ntools = [\n    Tool(\n        # The LLMMathChain tool for math calculations.\n        func=llm_math_chain.run,\n        name=\"Calculator\",\n        description=\"useful for when you need to answer questions about math\",\n    ),\n    Tool(\n        # Tool for counting characters in a string.\n        func=google_search,\n        name=\"google_search\",\n        description=\"useful for when you need to find out about someones age.\",\n    ),\n]\n\n# Create an agent using the ChatOpenAI model and the tools:\nagent = create_openai_functions_agent(llm=model, tools=tools, prompt=prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n# Asking the agent to run a task and store its result:\nresult = agent_executor.invoke(\n    {\n        \"input\": \"\"\"Task: Google search for James Phoenix's age.\n Then square it.\"\"\"}\n)\nprint(result)\n# {'input': \"...\", 'output': 'James Phoenix is 31 years old.\n# Squaring his age, we get 961.'}\n```", "```py\n# Importing the relevant packages:\nfrom langchain.agents.agent_types import AgentType\nfrom langchain_experimental.agents.agent_toolkits import create_csv_agent\nfrom langchain_openai.chat_models import ChatOpenAI\n\n# Creating a CSV Agent:\nagent = create_csv_agent(\n    ChatOpenAI(temperature=0),\n    \"data/heart_disease_uci.csv\",\n    verbose=True,\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n)\n\nagent.invoke(\"How many rows of data are in the file?\")\n# '920'\n\nagent.invoke(\"What are the columns within the dataset?\")\n# \"'id', 'age', 'sex', 'dataset', 'cp', 'trestbps', 'chol', 'fbs',\n# 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'\"\n\nagent.invoke(\"Create a correlation matrix for the data and save it to a file.\")\n# \"The correlation matrix has been saved to a file named\n# 'correlation_matrix.csv'.\"\n```", "```py\nfrom langchain.agents import create_sql_agent\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.agents.agent_types import AgentType\nfrom langchain_openai.chat_models import ChatOpenAI\n\ndb = SQLDatabase.from_uri(\"sqlite:///./data/demo.db\")\ntoolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(temperature=0))\n\n# Creating an agent executor:\nagent_executor = create_sql_agent(\n    llm=ChatOpenAI(temperature=0),\n    toolkit=toolkit,\n    verbose=True,\n    agent_type=AgentType.OPENAI_FUNCTIONS,\n)\n\n# Identifying all of the tables:\nagent_executor.invoke(\"Identify all of the tables\")\n# 'The database contains the following tables:\\n1\\. Orders\\n2\\. Products\\n3\\. Users'\n```", "```py\nuser_sql = agent_executor.invoke(\n    '''Add 5 new users to the database. Their names are:\n John, Mary, Peter, Paul, and Jane.'''\n)\n'''Based on the schema of the \"Users\" table, I can see that the relevant\ncolumns for adding new users are \"FirstName\", \"LastName\", \"Email\", and\n\"DateJoined\". I will now run the SQL query to add the new\nusers.\\n\\n```", "```py\\n\\nPlease note that I have added the new users with\nthe specified names and email addresses. The \"DateJoined\" column is set to the\nrespective dates mentioned.'''\n```", "```py\n# This the function signature for demonstration purposes and is not executable.\ndef create_sql_agent(\n    llm: BaseLanguageModel,\n    toolkit: SQLDatabaseToolkit,\n    agent_type: Any | None = None,\n    callback_manager: BaseCallbackManager | None = None,\n    prefix: str = SQL_PREFIX,\n    suffix: str | None = None,\n    format_instructions: str | None = None,\n    input_variables: List[str] | None = None,\n    top_k: int = 10,\n    max_iterations: int | None = 15,\n    max_execution_time: float | None = None,\n    early_stopping_method: str = \"force\",\n    verbose: bool = False,\n    agent_executor_kwargs: Dict[str, Any] | None = None,\n    extra_tools: Sequence[BaseTool] = (),\n    **kwargs: Any\n) -> AgentExecutor\n```", "```py\nSQL_PREFIX = \"\"\"You are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to\nrun, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain\nalways limit your query to at most {top_k} results. You can order the\nresults by a relevant column to return the most interesting examples in\nthe database. Never query for all the columns from a specific table, only\nask for the relevant columns given the question. You have access to tools\nfor interacting with the database. Only use the below tools. Only use the\ninformation returned by the below tools to construct your final answer. You\nMUST double-check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again. If the question does\nnot seem related to the database, just return \"I don't know\" as the answer.\n\"\"\"\n\nagent_executor = create_sql_agent(\n    llm=ChatOpenAI(temperature=0),\n    toolkit=toolkit,\n    verbose=True,\n    agent_type=AgentType.OPENAI_FUNCTIONS,\n    prefix=SQL_PREFIX,\n)\n\nagent_executor.invoke(user_sql)\n# '...sql\\nINSERT INTO Users (FirstName, LastName, Email,\n# DateJoined)\\nVALUES (...)...'\n\n# Testing that Peter was inserted into the database:\nagent_executor.invoke(\"Do we have a Peter in the database?\")\n'''Yes, we have a Peter in the database. Their details are as follows:\\n-\nFirst Name: Peter...'''\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\n# 1\\. Create the model:\nllm = ChatOpenAI(temperature=0)\n\n@tool\ndef get_word_length(word: str) -> int:\n    \"\"\"Returns the length of a word.\"\"\"\n    return len(word)\n\n# 2\\. Create the tools:\ntools = [get_word_length]\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n# 3\\. Create the Prompt:\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"You are very powerful assistant, but don't know current events\n and aren't good at calculating word length.\"\"\",\n        ),\n        (\"user\", \"{input}\"),\n        # This is where the agent will write/read its messages from\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ]\n)\n```", "```py\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain.agents.format_scratchpad.openai_tools import (\n    format_to_openai_tool_messages,\n)\n\n# 4\\. Formats the python function tools into JSON schema and binds\n# them to the model:\nllm_with_tools = llm.bind_tools(tools=[convert_to_openai_tool(t)\nfor t in tools])\n\nfrom langchain.agents.output_parsers.openai_tools \\\nimport OpenAIToolsAgentOutputParser\n\n# 5\\. Setting up the agent chain:\nagent = (\n    {\n        \"input\": lambda x: x[\"input\"],\n        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n            x[\"intermediate_steps\"]\n        ),\n    }\n    | prompt\n    | llm_with_tools\n    | OpenAIToolsAgentOutputParser()\n)\n```", "```py\nfrom langchain.agents import AgentExecutor\n\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\nagent_executor.invoke({\"input\": \"How many letters in the word Software?\"})\n#{'input': 'How many letters in the word Software?',\n# 'output': 'There are 8 letters in the word \"Software\".'}\n```", "```py\n# Provide the connection string to connect to the MongoDB database.\nconnection_string = \"mongodb://mongo_user:password123@mongo:27017\"\n\nchat_message_history = MongoDBChatMessageHistory(\n    session_id=\"test_session\",\n    connection_string=connection_string,\n    database_name=\"my_db\",\n    collection_name=\"chat_histories\",\n)\n\nchat_message_history.add_user_message(\"I love programming!!\")\nchat_message_history.add_ai_message(\"What do you like about it?\")\n\nchat_message_history.messages\n# [HumanMessage(content='I love programming!!',\n# AIMessage(content='What do you like about it?')\n```", "```py\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory()\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.load_memory_variables({})\n# {'history': 'Human: hi\\nAI: whats up'}\n```", "```py\nmemory = ConversationBufferMemory(return_messages=True)\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.load_memory_variables({})\n# {'history': [HumanMessage(content='hi'),\n# AIMessage(content='whats up')]}\n```", "```py\n# Using within a chain:\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\nfrom operator import itemgetter\n\nmemory = ConversationBufferMemory(return_messages=True)\n\nmodel = ChatOpenAI(temperature=0)\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"Act as a chatbot that helps users with their queries.\"),\n        # The history of the conversation\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\nchain = (\n    {\n        \"input\": lambda x: x[\"input\"],\n        \"history\": RunnableLambda(memory.load_memory_variables) | \\\n        itemgetter(\"history\"),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n```", "```py\ninputs = {\"input\": \"Hi my name is James!\"}\nresult = chain.invoke(inputs)\nmemory.save_context(inputs, {\"outputs\": result})\nprint(memory.load_memory_variables({}))\n\n# {'history': [HumanMessage(content='Hi my name is James!'),\n# AIMessage(content='Hello James! How can I assist you today?')]}\n```", "```py\ninputs = {\"input\": \"What is my name?\"}\nsecond_result = chain.invoke(inputs)\nprint(second_result)\n# Your name is James.\n```", "```py\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"You are a very powerful assistant, but don't know current events\nand aren't good at calculating word length.\"\"\",\n        ),\n        # This is where the agent will write/read its messages from\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"user\", \"{input}\"),\n    ]\n)\n\n# ... The rest of the code remains the same as before ...\n\n# Create an agent executor by passing in the agent, tools, and memory:\nmemory = ConversationBufferMemory(return_messages=True)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True,\nmemory=memory)\n```", "```py\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.load_memory_variables({})\n# {'history': 'Human: hi\\nAI: whats up'}\n```", "```py\nfrom langchain.memory import ConversationBufferWindowMemory\n\nmemory = ConversationBufferWindowMemory(k=1)\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n# Returns: {'history': 'Human: not much you\\nAI: not much'}\nmemory.load_memory_variables({})\n```", "```py\nfrom langchain.memory import ConversationSummaryMemory, ChatMessageHistory\nfrom langchain_openai import OpenAI\n\nmemory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.load_memory_variables({})\n# Returns: {'history': '\\nThe human greets the AI, to which the AI responds.'}\n```", "```py\nfrom langchain.memory import ConversationSummaryBufferMemory\nfrom langchain_openai.chat_models import ChatOpenAI\n\nmemory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=10)\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.load_memory_variables({})\n# Returns: {'history': 'System: \\nThe human says \"hi\", and the AI responds with\n# \"whats up\".\\nHuman: not much you\\nAI: not much'}\n```", "```py\nfrom langchain.memory import ConversationTokenBufferMemory\nfrom langchain_openai.chat_models import ChatOpenAI\n\nmemory = ConversationTokenBufferMemory(llm=ChatOpenAI(),\nmax_token_limit=50)\nmemory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\nmemory.load_memory_variables({})\n# Returns: {'history': 'Human: not much you\\nAI: not much'}\n```", "```py\nfrom langchain.tools import StructuredTool\n\ndef save_interview(raw_interview_text: str):\n    \"\"\"Tool to save the interview. You must pass the entire interview and\n conversation in here. The interview will then be saved to a local file.\n Remember to include all of the previous chat messages. Include all of\n the messages with the user and the AI, here is a good response:\n AI: some text\n Human: some text\n ...\n ---\n \"\"\"\n    # Save to local file:\n    with open(\"interview.txt\", \"w\") as f:\n        f.write(raw_interview_text)\n    return f'''Interview saved! Content: {raw_interview_text}. File:\n interview.txt. You must tell the user that the interview is saved.'''\n\nsave_interview = StructuredTool.from_function(save_interview)\n```", "```py\nfrom pydantic.v1 import BaseModel\nfrom typing import Union, Literal, Type\nfrom langchain_core.tools import BaseTool\n\nclass ArgumentType(BaseModel):\n    url: str\n    file_type: Union[Literal[\"pdf\"], Literal[\"txt\"]]\n\nclass SummarizeFileFromURL(BaseTool):\n    name = \"SummarizeFileFromURL\"\n    description = \"Summarize a file from a URL.\"\n    args_schema: Type[ArgumentType] = ArgumentType\n```", "```py\nAgentExecutor(.., verbose=True)\n```", "```py\nclass BaseCallbackHandler:\n    \"\"\"Base callback handler that can be used to handle callbacks from\n langchain.\"\"\"\n\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str],\n        **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when LLM starts running.\"\"\"\n\n    def on_chat_model_start(\n        self, serialized: Dict[str, Any],\n        messages: List[List[BaseMessage]], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when Chat Model starts running.\"\"\"\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n        \"\"\"Run when LLM ends running.\"\"\"\n\n    def on_llm_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when LLM errors.\"\"\"\n\n    def on_chain_start(\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any],\n        **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when chain starts running.\"\"\"\n\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n        \"\"\"Run when chain ends running.\"\"\"\n\n    def on_chain_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when chain errors.\"\"\"\n\n    def on_tool_start(\n        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when tool starts running.\"\"\"\n\n    def on_tool_end(self, output: str, **kwargs: Any) -> Any:\n        \"\"\"Run when tool ends running.\"\"\"\n\n    def on_tool_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when tool errors.\"\"\"\n\n    def on_text(self, text: str, **kwargs: Any) -> Any:\n        \"\"\"Run on arbitrary text.\"\"\"\n\n    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n        \"\"\"Run on agent action.\"\"\"\n\n    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n        \"\"\"Run on agent end.\"\"\"\n```", "```py\nfrom langchain.agents import AgentExecutor\nfrom langchain.callbacks import StdOutCallbackHandler\n\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    callbacks=[StdOutCallbackHandler()],\n    tags=['a-tag'])\n\nagent_executor.invoke({\"input\": \"How many letters in the word Software?\"})\n```", "```py\nfrom langchain.callbacks import StdOutCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain_openai import OpenAI\nfrom langchain_core.prompts import PromptTemplate\n\nhandler = StdOutCallbackHandler()\nllm = OpenAI()\nprompt = PromptTemplate.from_template(\"What is 1 + {number} = \")\nchain = LLMChain(llm=llm, prompt=prompt)\nchain.invoke({\"number\": 2}, {\"callbacks\": [handler]})\n```", "```py\nimport asyncio\nfrom langchain.callbacks import get_openai_callback\nfrom langchain_core.messages import SystemMessage\nfrom langchain_openai.chat_models import ChatOpenAI\nmodel = ChatOpenAI()\n```", "```py\nwith get_openai_callback() as cb:\n    model.invoke([SystemMessage(content=\"My name is James\")])\ntotal_tokens = cb.total_tokens\nprint(total_tokens)\n# 25\nassert total_tokens > 0\n```", "```py\nwith get_openai_callback() as cb:\n    model.invoke([SystemMessage(content=\"My name is James\")])\n    model.invoke([SystemMessage(content=\"My name is James\")])\nassert cb.total_tokens > 0\nprint(cb.total_tokens)\n# 50\n```", "```py\n# Async callbacks:\nwith get_openai_callback() as cb:\n    await asyncio.gather(\n        model.agenerate(\n            [\n                [SystemMessage(content=\"Is the meaning of life 42?\")],\n                [SystemMessage(content=\"Is the meaning of life 42?\")],\n            ],\n        )\n    )\nprint(cb.__dict__)\n# {'successful_requests': 2, 'total_cost': 0.000455,\n# 'total_tokens': 235, 'prompt_tokens': 30,\n# 'completion_tokens': 205}\n```"]