- en: appendix C Exercise solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录C 练习解答
- en: The complete code examples for the exercises’ answers can be found in the supplementary
    GitHub repository at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 练习答案的完整代码示例可以在补充的GitHub仓库[https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)中找到。
- en: Chapter 2
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章
- en: Exercise 2.1
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2.1
- en: 'You can obtain the individual token IDs by prompting the encoder with one string
    at a time:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过每次提示编码器一个字符串来获取单个标记ID：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This prints
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can then use the following code to assemble the original string:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下代码组装原始字符串：
- en: '[PRE2]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This returns
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE3]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Exercise 2.2
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2.2
- en: 'The code for the data loader with `max_length=2` `and` `stride=2`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 代码示例，数据加载器`max_length=2`和`stride=2`：
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It produces batches of the following format:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它产生以下格式的批次：
- en: '[PRE5]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The code of the second data loader with `max_length=8` `and` `stride=2`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数据加载器的代码，其中`max_length=8`和`stride=2`：
- en: '[PRE6]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: An example batch looks like
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例批次看起来像
- en: '[PRE7]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Chapter 3
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章
- en: Exercise 3.1
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习3.1
- en: The correct weight assignment is
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的权重分配是
- en: '[PRE8]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Exercise 3.2
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习3.2
- en: To achieve an output dimension of 2, similar to what we had in single-head attention,
    we need to change the projection dimension `d_out` to 1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到2的输出维度，类似于我们在单头注意力中拥有的，我们需要将投影维度`d_out`更改为1。
- en: '[PRE9]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Exercise 3.3
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习3.3
- en: The initialization for the smallest GPT-2 model is
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最小GPT-2模型的初始化如下：
- en: '[PRE10]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Chapter 4
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章
- en: Exercise 4.1
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习4.1
- en: 'We can calculate the number of parameters in the feed forward and attention
    modules as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下计算前馈和注意力模块中的参数数量：
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As we can see, the feed forward module contains approximately twice as many
    parameters as the attention module:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，前馈模块包含大约是注意力模块两倍多的参数：
- en: '[PRE12]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Exercise 4.2
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习4.2
- en: 'To instantiate the other GPT model sizes, we can modify the configuration dictionary
    as follows (here shown for GPT-2 XL):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要实例化其他GPT模型大小，我们可以修改配置字典，如下所示（此处以GPT-2 XL为例）：
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then, reusing the code from section 4.6 to calculate the number of parameters
    and RAM requirements, we find
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，重新使用第4.6节中的代码来计算参数数量和RAM需求，我们发现
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Exercise 4.3
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习4.3
- en: 'There are three distinct places in chapter 4 where we used dropout layers:
    the embedding layer, shortcut layer, and multi-head attention module. We can control
    the dropout rates for each of the layers by coding them separately in the config
    file and then modifying the code implementation accordingly.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们使用dropout层的三个不同位置：嵌入层、快捷层和多头注意力模块。我们可以通过在配置文件中单独编码每个层的dropout率，然后相应地修改代码实现来控制每个层的dropout率。
- en: 'The modified configuration is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的配置如下：
- en: '[PRE15]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Dropout for multi-head attention'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 多头注意力中的Dropout'
- en: '#2 Dropout for shortcut connections'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 快捷连接中的Dropout'
- en: '#3 Dropout for embedding layer'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 嵌入层中的Dropout'
- en: The modified `TransformerBlock` and `GPTModel` look like
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的`TransformerBlock`和`GPTModel`看起来像
- en: '[PRE16]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Dropout for multi-head attention'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 多头注意力中的Dropout'
- en: '#2 Dropout for shortcut connections'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 快捷连接中的Dropout'
- en: '#3 Dropout for embedding layer'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 嵌入层中的Dropout'
- en: Chapter 5
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章
- en: Exercise 5.1
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习5.1
- en: We can print the number of times the token (or word) “pizza” is sampled using
    the `print_sampled_tokens` function we defined in this section. Let’s start with
    the code we defined in section 5.3.1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在本节中定义的`print_sampled_tokens`函数打印标记（或单词）“pizza”被采样的次数。让我们从第5.3.1节中定义的代码开始。
- en: The “pizza” token is sampled 0x if the temperature is 0 or 0.1, and it is sampled
    32× if the temperature is scaled up to 5\. The estimated probability is 32/1000
    × 100% = 3.2%.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当温度为0或0.1时，“pizza”标记被采样0x，当温度提升到5时，它被采样32×。估计概率为32/1000 × 100% = 3.2%。
- en: The actual probability is 4.3% and is contained in the rescaled softmax probability
    tensor (`scaled_probas[2][6]`).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实际概率为4.3%，包含在重新缩放的softmax概率张量（`scaled_probas[2][6]`）中。
- en: Exercise 5.2
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习5.2
- en: Top-k sampling and temperature scaling are settings that have to be adjusted
    based on the LLM and the desired degree of diversity and randomness in the output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k采样和温度缩放是根据LLM和期望的输出多样性和随机程度进行调整的设置。
- en: When using relatively small top-k values (e.g., smaller than 10) and when the
    temperature is set below 1, the model’s output becomes less random and more deterministic.
    This setting is useful when we need the generated text to be more predictable,
    coherent, and closer to the most likely outcomes based on the training data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用相对较小的 top-k 值（例如，小于 10）并且温度设置在 1 以下时，模型的输出变得更加不随机和确定。这种设置在需要生成的文本更加可预测、连贯，并且基于训练数据更接近最可能的结果时很有用。
- en: Applications for such low k and temperature settings include generating formal
    documents or reports where clarity and accuracy are most important. Other examples
    of applications include technical analysis or code-generation tasks, where precision
    is crucial. Also, question answering and educational content require accurate
    answers where a temperature below 1 is helpful.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种低 k 和温度设置的应用包括生成正式文件或报告，其中清晰度和准确性最为重要。其他应用示例包括技术分析或代码生成任务，其中精度至关重要。此外，问答和教育内容需要准确的答案，其中温度低于
    1 有助于提高准确性。
- en: On the other hand, larger top-k values (e.g., values in the range of 20 to 40)
    and temperature values above 1 are useful when using LLMs for brainstorming or
    generating creative content, such as fiction.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，较大的 top-k 值（例如，20 到 40 范围内的值）以及超过 1 的温度值，在用 LLM 进行头脑风暴或生成创意内容（如小说）时很有用。
- en: Exercise 5.3
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.3
- en: 'There are multiple ways to force deterministic behavior with the `generate`
    function:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以通过 `generate` 函数强制实现确定性行为：
- en: Setting to `top_k=None` and applying no temperature scaling
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置为 `top_k=None` 并不应用温度缩放
- en: Setting `top_k=1`
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `top_k=1`
- en: Exercise 5.4
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.4
- en: 'In essence, we have to load the model and optimizer that we saved in the main
    chapter:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，我们必须加载我们在主要章节中保存的模型和优化器：
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Then, call the `train_simple_function` with `num_epochs=1` to train the model
    for another epoch.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 `num_epochs=1` 调用 `train_simple_function` 以训练模型另一个周期。
- en: Exercise 5.5
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.5
- en: 'We can use the following code to calculate the training and validation set
    losses of the GPT model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来计算 GPT 模型的训练和验证集损失：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The resulting losses for the 124-million parameter are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 1240 万参数的结果损失如下：
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The main observation is that the training and validation set performances are
    in the same ballpark. This can have multiple explanations:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 主要观察结果是训练和验证集的性能在同一水平。这可能有多个解释：
- en: “The Verdict” was not part of the pretraining dataset when OpenAI trained GPT-2\.
    Hence, the model is not explicitly overfitting to the training set and performs
    similarly well on the training and validation set portions of “The Verdict.” (The
    validation set loss is slightly lower than the training set loss, which is unusual
    in deep learning. However, it’s likely due to random noise since the dataset is
    relatively small. In practice, if there is no overfitting, the training and validation
    set performances are expected to be roughly identical).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 OpenAI 训练 GPT-2 时，“The Verdict” 并不是预训练数据集的一部分。因此，模型并没有明确地过度拟合训练集，在 “The Verdict”
    的训练和验证集部分表现相似。（验证集损失略低于训练集损失，这在深度学习中是不常见的。然而，这很可能是由于随机噪声，因为数据集相对较小。在实践中，如果没有过度拟合，训练和验证集的性能预计将大致相同）。
- en: “The Verdict” was part of GPT-2’s training dataset. In this case, we can’t tell
    whether the model is overfitting the training data because the validation set
    would have been used for training as well. To evaluate the degree of overfitting,
    we’d need a new dataset generated after OpenAI finished training GPT-2 to make
    sure that it couldn’t have been part of the pretraining.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “The Verdict” 是 GPT-2 训练数据集的一部分。在这种情况下，我们无法判断模型是否过度拟合了训练数据，因为验证集也已被用于训练。为了评估过度拟合的程度，我们需要一个在
    OpenAI 完成训练 GPT-2 后生成的新的数据集，以确保它不可能成为预训练的一部分。
- en: Exercise 5.6
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.6
- en: 'In the main chapter, we experimented with the smallest GPT-2 model, which has
    only 124-million parameters. The reason was to keep the resource requirements
    as low as possible. However, you can easily experiment with larger models with
    minimal code changes. For example, instead of loading the 1,558 million instead
    of 124 million model weights in chapter 5, the only two lines of code that we
    have to change are the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要章节中，我们尝试了最小的 GPT-2 模型，它只有 1240 万个参数。这样做的原因是为了尽可能降低资源需求。然而，你可以通过最小的代码更改轻松地尝试更大的模型。例如，在第五章中，我们不是加载
    1240 万个参数的模型权重，而是只需要更改以下两行代码：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The updated code is
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的代码如下
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Chapter 6
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 6 章
- en: Exercise 6.1
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6.1
- en: 'We can pad the inputs to the maximum number of tokens the model supports by
    setting the max length to `max_length` `=` `1024` when initializing the datasets:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将最大长度设置为`max_length` `=` `1024`来初始化数据集，从而将输入填充到模型支持的标记数最大值：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: However, the additional padding results in a substantially worse test accuracy
    of 78.33% (vs. the 95.67% in the main chapter).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，额外的填充导致测试准确率显著下降至78.33%（与主章节中的95.67%相比）。
- en: Exercise 6.2
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.2
- en: 'Instead of fine-tuning just the final transformer block, we can fine-tune the
    entire model by removing the following lines from the code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以微调最终的transformer块，还可以通过从代码中删除以下行来微调整个模型：
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This modification results in a 1% improved test accuracy of 96.67% (vs. the
    95.67% in the main chapter).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种修改使测试准确率提高了1%，达到96.67%（与主章节中的95.67%相比）。
- en: Exercise 6.3
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.3
- en: Rather than fine-tuning the last output token, we can fine-tune the first output
    token by changing `model(input_batch)[:,` `-1,` `:]` to `model(input_batch)[:,`
    `0,` `:]` everywhere in the code.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不是微调最后一个输出标记，而是通过将代码中的`model(input_batch)[:, -1, :]`更改为`model(input_batch)[:,
    0, :]`来微调第一个输出标记。
- en: As expected, since the first token contains less information than the last token,
    this change results in a substantially worse test accuracy of 75.00% (vs. the
    95.67% in the main chapter).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，由于第一个标记包含的信息少于最后一个标记，这种变化导致测试准确率显著下降至75.00%（与主章节中的95.67%相比）。
- en: Chapter 7
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第七章
- en: Exercise 7.1
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.1
- en: 'The Phi-3 prompt format, which is shown in figure 7.4, looks like the following
    for a given example input:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-3提示格式，如图7.4所示，对于给定的示例输入如下所示：
- en: '[PRE24]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To use this template, we can modify the `format_input` function as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此模板，我们可以按如下方式修改`format_input`函数：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Lastly, we also have to update the way we extract the generated response when
    we collect the test set responses:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们收集测试集响应时，我们还需要更新提取生成响应的方式：
- en: '[PRE26]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#1 New: Adjust ###Response to <|assistant|>'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 新：调整###Response到<|assistant|>'
- en: Fine-tuning the model with the Phi-3 template is approximately 17% faster since
    it results in shorter model inputs. The score is close to 50, which is in the
    same ballpark as the score we previously achieved with the Alpaca-style prompts.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Phi-3模板微调模型大约快17%，因为它导致模型输入更短。分数接近50，这与我们之前使用Alpaca风格提示获得的分数在同一水平。
- en: Exercise 7.2
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.2
- en: 'To mask out the instructions as shown in figure 7.13, we need to make slight
    modifications to the `InstructionDataset` class and `custom_collate_fn` function.
    We can modify the `InstructionDataset` class to collect the lengths of the instructions,
    which we will use in the collate function to locate the instruction content positions
    in the targets when we code the collate function, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了屏蔽如图7.13所示的指令，我们需要对`InstructionDataset`类和`custom_collate_fn`函数进行轻微修改。我们可以修改`InstructionDataset`类以收集指令长度，我们将在collate函数中使用这些长度来定位目标中的指令内容位置，如下所示：
- en: '[PRE27]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 Separate list for instruction lengths'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 分离的指令长度列表'
- en: '#2 Collects instruction lengths'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 收集指令长度'
- en: '#3 Returns both instruction lengths and texts separately'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 分别返回指令长度和文本'
- en: 'Next, we update the `custom_collate_fn` where each `batch` is now a tuple containing
    `(instruction_length,` `item)` instead of just `item` due to the changes in the
    `InstructionDataset` dataset. In addition, we now mask the corresponding instruction
    tokens in the target ID list:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们更新了`custom_collate_fn`，由于`InstructionDataset`数据集的变化，现在每个`batch`都是一个包含`(instruction_length,
    item)`的元组，而不是仅仅`item`。此外，我们现在在目标ID列表中屏蔽相应的指令标记：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#1 batch is now a tuple.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 现在batch是一个元组。'
- en: '#2 Masks all input and instruction tokens in the targets'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在目标中屏蔽所有输入和指令标记'
- en: When evaluating a model fine-tuned with this instruction masking method, it
    performs slightly worse (approximately 4 points using the Ollama Llama 3 method
    from chapter 7). This is consistent with observations in the “Instruction Tuning
    With Loss Over Instructions” paper ([https://arxiv.org/abs/2405.14394](https://arxiv.org/abs/2405.14394)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这种指令屏蔽方法微调模型进行评估时，其表现略差（使用第7章的Ollama Llama 3方法，大约低4分）。这与“带有指令损失的指令调整”论文中的观察结果一致([https://arxiv.org/abs/2405.14394](https://arxiv.org/abs/2405.14394))。
- en: Exercise 7.3
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.3
- en: To fine-tune the model on the original Stanford Alpaca dataset ([https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)),
    we just have to change the file URL from
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要在原始斯坦福Alpaca数据集([https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca))上微调模型，我们只需更改文件URL：
- en: '[PRE29]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: to
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that the dataset contains 52,000 entries (50x more than in chapter 7),
    and the entries are longer than the ones we worked with in chapter 7.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，数据集包含52,000条条目（比第7章多50倍），条目长度也比我们在第7章中处理的条目长。
- en: Thus, it’s highly recommended that the training be run on a GPU.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，强烈建议在 GPU 上运行训练。
- en: If you encounter out-of-memory errors, consider reducing the batch size from
    8 to 4, 2, or 1\. In addition to lowering the batch size, you may also want to
    consider lowering the `allowed_max_length` from 1024 to 512 or 256.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到内存不足的错误，考虑将批大小从8减少到4、2或1。除了降低批大小外，你还可能希望考虑将 `allowed_max_length` 从1024降低到512或256。
- en: 'Below are a few examples from the Alpaca dataset, including the generated model
    responses:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是来自 Alpaca 数据集的一些示例，包括生成的模型响应：
- en: Exercise 7.4
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 7.4
- en: 'To instruction fine-tune the model using LoRA, use the relevant classes and
    functions from appendix E:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 LoRA 指令微调模型，请使用附录 E 中的相关类和函数：
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, add the following lines of code below the model loading code in section
    7.5:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在7.5节中的模型加载代码下方添加以下几行代码：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that, on an Nvidia L4 GPU, the fine-tuning with LoRA takes 1.30 min to
    run on an L4\. On the same GPU, the original code takes 1.80 minutes to run. So,
    LoRA is approximately 28% faster in this case. The score, evaluated with the Ollama
    Llama 3 method from chapter 7, is around 50, which is in the same ballpark as
    the original model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 Nvidia L4 GPU 上，使用 LoRA 进行微调在 L4 上运行需要1.30分钟。在相同的 GPU 上，原始代码运行需要1.80分钟。因此，在这种情况下，LoRA
    大约快了28%。使用第7章的 Ollama Llama 3 方法评估的分数大约为50，与原始模型相当。
- en: Appendix A
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A
- en: Exercise A.1
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 A.1
- en: The optional Python Setup Tips document ([https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences))
    contains additional recommendations and tips if you need additional help to set
    up your Python environment.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的 Python 设置提示文档([https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences))
    包含了额外的建议和提示，如果你需要额外的帮助来设置你的 Python 环境。
- en: Exercise A.2
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 A.2
- en: The the optional "Installing Libraries Used In This Book" document ([https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries))
    contains utilities to check whether your environment is set up correctly.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的 "安装本书中使用的库" 文档([https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries))
    包含了检查你的环境是否正确设置的实用工具。
- en: Exercise A.3
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 A.3
- en: 'The network has two inputs and two outputs. In addition, there are 2 hidden
    layers with 30 and 20 nodes, respectively. Programmatically, we can calculate
    the number of parameters as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 网络有两个输入和两个输出。此外，还有2个隐藏层，分别有30和20个节点。从编程的角度来看，我们可以这样计算参数数量：
- en: '[PRE33]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This returns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE34]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can also calculate this manually as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以手动计算如下：
- en: '*First hidden layer:* 2 inputs times 30 hidden units plus 30 bias units'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第一隐藏层:* 2个输入乘以30个隐藏单元加上30个偏置单元'
- en: '*Second hidden layer:* 30 incoming units times 20 nodes plus 20 bias units'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第二隐藏层:* 30个输入单元乘以20个节点加上20个偏置单元'
- en: '*Output layer:* 20 incoming nodes times 2 output nodes plus 2 bias units'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出层:* 20个输入节点乘以2个输出节点加上2个偏置单元'
- en: Then, adding all the parameters in each layer results in 2×30+30 + 30×20+20
    + 20×2+2 = 752.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将每层的所有参数相加，结果为 2×30+30 + 30×20+20 + 20×2+2 = 752。
- en: Exercise A.4
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 A.4
- en: 'The exact run-time results will be specific to the hardware used for this experiment.
    In my experiments, I observed significant speed-ups even for small matrix multiplications
    when using a Google Colab instance connected to a V100 GPU:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 确切的运行时间结果将取决于用于此实验的硬件。在我的实验中，即使对于小的矩阵乘法，使用连接到 V100 GPU 的 Google Colab 实例也能观察到显著的加速：
- en: '[PRE35]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: On the CPU this resulted in
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上这导致了
- en: '[PRE36]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: When executed on a GPU
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上执行时
- en: '[PRE37]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The result was
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下
- en: '[PRE38]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this case, on a V100, the computation was approximately four times faster.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，在 V100 上，计算速度大约快了四倍。
