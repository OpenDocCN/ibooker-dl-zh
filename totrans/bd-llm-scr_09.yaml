- en: appendix C Exercise solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The complete code examples for the exercises’ answers can be found in the supplementary
    GitHub repository at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch).
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 2.1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can obtain the individual token IDs by prompting the encoder with one string
    at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then use the following code to assemble the original string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This returns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 2.2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code for the data loader with `max_length=2` `and` `stride=2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces batches of the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The code of the second data loader with `max_length=8` `and` `stride=2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: An example batch looks like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 3.1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The correct weight assignment is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 3.2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To achieve an output dimension of 2, similar to what we had in single-head attention,
    we need to change the projection dimension `d_out` to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 3.3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The initialization for the smallest GPT-2 model is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 4.1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can calculate the number of parameters in the feed forward and attention
    modules as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the feed forward module contains approximately twice as many
    parameters as the attention module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 4.2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To instantiate the other GPT model sizes, we can modify the configuration dictionary
    as follows (here shown for GPT-2 XL):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Then, reusing the code from section 4.6 to calculate the number of parameters
    and RAM requirements, we find
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 4.3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are three distinct places in chapter 4 where we used dropout layers:
    the embedding layer, shortcut layer, and multi-head attention module. We can control
    the dropout rates for each of the layers by coding them separately in the config
    file and then modifying the code implementation accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified configuration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Dropout for multi-head attention'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Dropout for shortcut connections'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Dropout for embedding layer'
  prefs: []
  type: TYPE_NORMAL
- en: The modified `TransformerBlock` and `GPTModel` look like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Dropout for multi-head attention'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Dropout for shortcut connections'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Dropout for embedding layer'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 5.1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can print the number of times the token (or word) “pizza” is sampled using
    the `print_sampled_tokens` function we defined in this section. Let’s start with
    the code we defined in section 5.3.1.
  prefs: []
  type: TYPE_NORMAL
- en: The “pizza” token is sampled 0x if the temperature is 0 or 0.1, and it is sampled
    32× if the temperature is scaled up to 5\. The estimated probability is 32/1000
    × 100% = 3.2%.
  prefs: []
  type: TYPE_NORMAL
- en: The actual probability is 4.3% and is contained in the rescaled softmax probability
    tensor (`scaled_probas[2][6]`).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Top-k sampling and temperature scaling are settings that have to be adjusted
    based on the LLM and the desired degree of diversity and randomness in the output.
  prefs: []
  type: TYPE_NORMAL
- en: When using relatively small top-k values (e.g., smaller than 10) and when the
    temperature is set below 1, the model’s output becomes less random and more deterministic.
    This setting is useful when we need the generated text to be more predictable,
    coherent, and closer to the most likely outcomes based on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Applications for such low k and temperature settings include generating formal
    documents or reports where clarity and accuracy are most important. Other examples
    of applications include technical analysis or code-generation tasks, where precision
    is crucial. Also, question answering and educational content require accurate
    answers where a temperature below 1 is helpful.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, larger top-k values (e.g., values in the range of 20 to 40)
    and temperature values above 1 are useful when using LLMs for brainstorming or
    generating creative content, such as fiction.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are multiple ways to force deterministic behavior with the `generate`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting to `top_k=None` and applying no temperature scaling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting `top_k=1`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise 5.4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In essence, we have to load the model and optimizer that we saved in the main
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Then, call the `train_simple_function` with `num_epochs=1` to train the model
    for another epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the following code to calculate the training and validation set
    losses of the GPT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting losses for the 124-million parameter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The main observation is that the training and validation set performances are
    in the same ballpark. This can have multiple explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: “The Verdict” was not part of the pretraining dataset when OpenAI trained GPT-2\.
    Hence, the model is not explicitly overfitting to the training set and performs
    similarly well on the training and validation set portions of “The Verdict.” (The
    validation set loss is slightly lower than the training set loss, which is unusual
    in deep learning. However, it’s likely due to random noise since the dataset is
    relatively small. In practice, if there is no overfitting, the training and validation
    set performances are expected to be roughly identical).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “The Verdict” was part of GPT-2’s training dataset. In this case, we can’t tell
    whether the model is overfitting the training data because the validation set
    would have been used for training as well. To evaluate the degree of overfitting,
    we’d need a new dataset generated after OpenAI finished training GPT-2 to make
    sure that it couldn’t have been part of the pretraining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise 5.6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the main chapter, we experimented with the smallest GPT-2 model, which has
    only 124-million parameters. The reason was to keep the resource requirements
    as low as possible. However, you can easily experiment with larger models with
    minimal code changes. For example, instead of loading the 1,558 million instead
    of 124 million model weights in chapter 5, the only two lines of code that we
    have to change are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The updated code is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 6.1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can pad the inputs to the maximum number of tokens the model supports by
    setting the max length to `max_length` `=` `1024` when initializing the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: However, the additional padding results in a substantially worse test accuracy
    of 78.33% (vs. the 95.67% in the main chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6.2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of fine-tuning just the final transformer block, we can fine-tune the
    entire model by removing the following lines from the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This modification results in a 1% improved test accuracy of 96.67% (vs. the
    95.67% in the main chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6.3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rather than fine-tuning the last output token, we can fine-tune the first output
    token by changing `model(input_batch)[:,` `-1,` `:]` to `model(input_batch)[:,`
    `0,` `:]` everywhere in the code.
  prefs: []
  type: TYPE_NORMAL
- en: As expected, since the first token contains less information than the last token,
    this change results in a substantially worse test accuracy of 75.00% (vs. the
    95.67% in the main chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 7.1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Phi-3 prompt format, which is shown in figure 7.4, looks like the following
    for a given example input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To use this template, we can modify the `format_input` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we also have to update the way we extract the generated response when
    we collect the test set responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New: Adjust ###Response to <|assistant|>'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning the model with the Phi-3 template is approximately 17% faster since
    it results in shorter model inputs. The score is close to 50, which is in the
    same ballpark as the score we previously achieved with the Alpaca-style prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To mask out the instructions as shown in figure 7.13, we need to make slight
    modifications to the `InstructionDataset` class and `custom_collate_fn` function.
    We can modify the `InstructionDataset` class to collect the lengths of the instructions,
    which we will use in the collate function to locate the instruction content positions
    in the targets when we code the collate function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Separate list for instruction lengths'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Collects instruction lengths'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Returns both instruction lengths and texts separately'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we update the `custom_collate_fn` where each `batch` is now a tuple containing
    `(instruction_length,` `item)` instead of just `item` due to the changes in the
    `InstructionDataset` dataset. In addition, we now mask the corresponding instruction
    tokens in the target ID list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#1 batch is now a tuple.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Masks all input and instruction tokens in the targets'
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating a model fine-tuned with this instruction masking method, it
    performs slightly worse (approximately 4 points using the Ollama Llama 3 method
    from chapter 7). This is consistent with observations in the “Instruction Tuning
    With Loss Over Instructions” paper ([https://arxiv.org/abs/2405.14394](https://arxiv.org/abs/2405.14394)).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To fine-tune the model on the original Stanford Alpaca dataset ([https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)),
    we just have to change the file URL from
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that the dataset contains 52,000 entries (50x more than in chapter 7),
    and the entries are longer than the ones we worked with in chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it’s highly recommended that the training be run on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter out-of-memory errors, consider reducing the batch size from
    8 to 4, 2, or 1\. In addition to lowering the batch size, you may also want to
    consider lowering the `allowed_max_length` from 1024 to 512 or 256.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are a few examples from the Alpaca dataset, including the generated model
    responses:'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To instruction fine-tune the model using LoRA, use the relevant classes and
    functions from appendix E:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add the following lines of code below the model loading code in section
    7.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note that, on an Nvidia L4 GPU, the fine-tuning with LoRA takes 1.30 min to
    run on an L4\. On the same GPU, the original code takes 1.80 minutes to run. So,
    LoRA is approximately 28% faster in this case. The score, evaluated with the Ollama
    Llama 3 method from chapter 7, is around 50, which is in the same ballpark as
    the original model.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise A.1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The optional Python Setup Tips document ([https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences))
    contains additional recommendations and tips if you need additional help to set
    up your Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise A.2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The the optional "Installing Libraries Used In This Book" document ([https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries))
    contains utilities to check whether your environment is set up correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise A.3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The network has two inputs and two outputs. In addition, there are 2 hidden
    layers with 30 and 20 nodes, respectively. Programmatically, we can calculate
    the number of parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This returns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate this manually as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*First hidden layer:* 2 inputs times 30 hidden units plus 30 bias units'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Second hidden layer:* 30 incoming units times 20 nodes plus 20 bias units'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output layer:* 20 incoming nodes times 2 output nodes plus 2 bias units'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, adding all the parameters in each layer results in 2×30+30 + 30×20+20
    + 20×2+2 = 752.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise A.4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The exact run-time results will be specific to the hardware used for this experiment.
    In my experiments, I observed significant speed-ups even for small matrix multiplications
    when using a Google Colab instance connected to a V100 GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: On the CPU this resulted in
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: When executed on a GPU
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The result was
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In this case, on a V100, the computation was approximately four times faster.
  prefs: []
  type: TYPE_NORMAL
