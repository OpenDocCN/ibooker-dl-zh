["```py\nclass Decoder(nn.Module):\n\u00a0\u00a0def __init__(self, n_in, n_hidden_1, n_hidden_2, n_hidden_3, n_out):\n\u00a0\u00a0\u00a0\u00a0super(Decoder, self).__init__()\n\u00a0\u00a0\u00a0\u00a0self.layer1 = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(n_in, n_hidden_1, bias=True),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.BatchNorm1d(n_hidden_1),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Sigmoid())\n\u00a0\u00a0\u00a0\u00a0self.layer2 = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(n_hidden_1, n_hidden_2, bias=True),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.BatchNorm1d(n_hidden_2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Sigmoid())\n\u00a0\u00a0\u00a0\u00a0self.layer3 = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(n_hidden_2, n_hidden_3, bias=True),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.BatchNorm1d(n_hidden_3),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Sigmoid())\n\u00a0\u00a0\u00a0\u00a0n_size = math.floor(math.sqrt(n_out))\n\u00a0\u00a0\u00a0\u00a0self.layer4 = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(n_hidden_3, n_out, bias=True),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.BatchNorm1d(n_out),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Sigmoid(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Unflatten(1, torch.Size([1, n_size,n_size])))\n\n\u00a0\u00a0def forward(self, x):\n\u00a0\u00a0\u00a0\u00a0x = self.layer1(x)\n\u00a0\u00a0\u00a0\u00a0x = self.layer2(x)\n\u00a0\u00a0\u00a0\u00a0x = self.layer3(x)\n\u00a0\u00a0\u00a0\u00a0return self.layer4(x)\n\n```", "```py\ndecoder = Decoder(2,250,500,1000,784)\n\n```", "```py\nloss_fn = nn.MSELoss()\noptimizer = optim.Adam(decoder.parameters(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0lr = 0.001,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0betas=(0.9,0.999),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0eps=1e-08)\n\ntrainset = datasets.MNIST('.',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0train=True,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0transform=transforms.ToTensor(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0download=True)\ntrainloader = DataLoader(trainset,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size=32,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0shuffle=True)\n# Training Loop\nNUM_EPOCHS = 5\nfor epoch in range(NUM_EPOCHS):\n\u00a0\u00a0for input, labels in trainloader:\n\u00a0\u00a0\u00a0\u00a0optimizer.zero_grad()\n\u00a0\u00a0\u00a0\u00a0code = encoder(input)\n\u00a0\u00a0\u00a0\u00a0output = decoder(code)\n\u00a0\u00a0\u00a0\u00a0#print(input.shape, output.shape)\n\u00a0\u00a0\u00a0\u00a0loss = loss_fn(output, input)\n\u00a0\u00a0\u00a0\u00a0optimizer.step()\n\u00a0\u00a0print(f\"Epoch: {epoch} Loss: {loss}\")\n\n```", "```py\ni = 0\nwith torch.no_grad():\n\u00a0\u00a0for images, labels in trainloader:\n\u00a0\u00a0\u00a0\u00a0if i == 3:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break\n\u00a0\u00a0\u00a0\u00a0grid = utils.make_grid(images)\n\u00a0\u00a0\u00a0\u00a0plt.figure()\n\u00a0\u00a0\u00a0\u00a0plt.imshow(grid.permute(1,2,0))\n\n\u00a0\u00a0\u00a0\u00a0code = encoder(images)\n\u00a0\u00a0\u00a0\u00a0output = decoder(code)\n\n\u00a0\u00a0\u00a0\u00a0grid = utils.make_grid(output)\n\u00a0\u00a0\u00a0\u00a0plt.figure()\n\u00a0\u00a0\u00a0\u00a0plt.imshow(grid.permute(1,2,0))\n\u00a0\u00a0\u00a0\u00a0i += 1\n\n```", "```py\n$ tensorboard --logdir ~/path/to/mnist_autoencoder_hidden=2_logs\n\n```", "```py\nfrom sklearn import decomposition\nimport input_data\n\nmnist = input_data.read_data_sets(\"data/\", one_hot=False)\npca = decomposition.PCA(n_components=2)\npca.fit(mnist.train.images)\npca_codes = pca.transform(mnist.test.images)\n\n```", "```py\nfrom matplotlib import pyplot as plt\n\npca_recon = pca.inverse_transform(pca_codes[:1])\nplt.imshow(pca_recon[0].reshape((28,28)), cmap=plt.cm.gray)\nplt.show()\n```", "```py\ndef corrupt_input(x):\n\u00a0\u00a0\u00a0\u00a0corrupting_matrix = 2.0*torch.rand_like(x)\n\n\u00a0\u00a0\u00a0\u00a0return x * corrupting_matrix\n\n# x = mnist data image of shape 28*28=784\nx = torch.rand((28,28))\ncorrupt = 1.0 # set to 1.0 to corrupt input\nc_x = (corrupt_input(x) * corrupt) + (x * (1 - corrupt))\n\n```", "```py\nemb = nn.Embedding(10, 100)\nx = torch.tensor([0])\nout = emb(x)\n```", "```py\nvocab_size = 500\nemb_vector_len = 128\n\nembedding = nn.Embedding(num_embeddings = vocab_size,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0embedding_dim = emb_vector_len)\n\n```", "```py\npip install info-nce-pytorch\n```", "```py\nloss = InfoNCE()\nbatch_size, embedding_size = 32, 128\nquery = embedding(outputs)\npositive_key = embedding(targets)\noutput = loss(query, positive_key)\n\n```", "```py\noptimizer = optim.SGD(embedding.parameters(),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0lr = 0.1)\ndef train(inputs, targets, embedding):\n\u00a0\u00a0optimizer.zero_grad()\n\u00a0\u00a0input_emb = embedding(inputs)\n\u00a0\u00a0target_emb = embedding(targets)\n\u00a0\u00a0loss = loss_fn(input_emb, target_emb)\n\u00a0\u00a0loss.backward()\n\u00a0\u00a0optimizer.step()\n\u00a0 return loss\n\n```", "```py\ncosine_similarity = nn.CosineSimilarity()\n\ndef evaluate(inputs, targets, embedding):\n\u00a0\u00a0with torch.no_grad():\n\u00a0\u00a0\u00a0\u00a0input_emb = embedding(inputs)\n\u00a0\u00a0\u00a0\u00a0target_emb = embedding(targets)\n\u00a0\u00a0\u00a0\u00a0norm = torch.sum(input_emb, dim=1)\n\u00a0\u00a0\u00a0\u00a0normalized = input_emb/norm\n\u00a0\u00a0\u00a0\u00a0score = cosine_similarity(normalized, target_emb)\n\u00a0\u00a0\u00a0\u00a0return normalized, score\n\n```", "```py\nn_epochs=1\nfor epoch in range(n_epochs):\n\u00a0\u00a0# Train\n\u00a0\u00a0running_loss = 0.0\n\u00a0\u00a0for inputs, targets in trainloader:\n\u00a0\u00a0\u00a0\u00a0loss = train(inputs, targets)\n\u00a0\u00a0\u00a0\u00a0running_loss += loss.item()\n\n\u00a0\u00a0writer.add_scalar('Train Loss',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0running_loss/len(trainloader), epoch)\n\u00a0\u00a0#Validate\n\u00a0\u00a0running_score = 0.0\n\u00a0\u00a0for inputs, targets in valloader:\n\u00a0\u00a0\u00a0\u00a0_, score = evaluate(inputs, targets)\n\u00a0\u00a0\u00a0\u00a0running_score += score\n\n\u00a0\u00a0writer.add_scalar('Val Score',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0running_score/len(valloader), epoch)\n\n```", "```py\nancient: egyptian, cultures, mythology, civilization, etruscan, \ngreek, classical, preserved\n\nhowever: but, argued, necessarily, suggest, certainly, nor, \nbelieve, believed\n\ntype: typical, kind, subset, form, combination, single, \ndescription, meant\n\nwhite: yellow, black, red, blue, colors, grey, bright, dark\n\nsystem: operating, systems, unix, component, variant, versions, \nversion, essentially\n\nenergy: kinetic, amount, heat, gravitational, nucleus, \nradiation, particles, transfer\n\nworld: ii, tournament, match, greatest, war, ever, championship, \ncold\n\ny: z, x, n, p, f, variable, mathrm, sum,\n\nline: lines, ball, straight, circle, facing, edge, goal, yards,\n\namong: amongst, prominent, most, while, famous, particularly, \nargue, many\n\nimage: png, jpg, width, images, gallery, aloe, gif, angel\n\nkingdom: states, turkey, britain, nations, islands, namely, \nireland, rest\n\nlong: short, narrow, thousand, just, extended, span, length, \nshorter\n\nthrough: into, passing, behind, capture, across, when, apart, \ngoal\n\ni: you, t, know, really, me, want, myself, we\n\nsource: essential, implementation, important, software, content, \ngenetic, alcohol, application\n\nbecause: thus, while, possibility, consequently, furthermore, \nbut, certainly, moral\n\neight: six, seven, five, nine, one, four, three, b\n\nfrench: spanish, jacques, pierre, dutch, italian, du, english, \nbelgian\n\nwritten: translated, inspired, poetry, alphabet, hebrew, \nletters, words, read\n\n```", "```py\ntsne = TSNE(perplexity=30, n_components=2, init='pca', \n            n_iter=5000)\nplot_embeddings = np.asfarray(final_embeddings[:plot_num,:], \n                              dtype='float')\nlow_dim_embs = tsne.fit_transform(plot_embeddings)\nlabels = [reverse_dictionary[i] for i in xrange(plot_only)]\ndata.plot_with_labels(low_dim_embs, labels)\n\n```"]