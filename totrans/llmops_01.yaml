- en: Chapter 1\. Introduction to Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rise in popularity of large language models (LLMs) is no accident; they’re
    transforming how we interact with technology and pushing the boundaries of what
    machine learning models can do.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'But here’s the catch: while these models are impressive, scaling them up and
    managing them in production is no walk in the park. The leap from a research project
    to a fully fledged, reliable tool is filled with obstacles. We’re talking about
    meeting enormous computational requirements, managing complex data, and ensuring
    that everything runs smoothly and securely whether you are self-hosting or using
    proprietary models.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the nitty-gritty of LLM operations, it’s important to understand
    why and how these models came to be. Knowing their origins and trajectory helps
    us appreciate the challenges we face when predicting their behaviors in production.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of LLMs reflects a series of incremental innovations, each addressing
    specific limitations of previous models. Early models were limited in scope and
    required extensive human input for even basic tasks. With advancements in architecture,
    such as the shift from recurrent neural networks (RNNs) to transformers, and the
    scaling of model sizes, LLMs have become more sophisticated. This evolution has
    brought about new challenges, such as managing massive amounts of data and ensuring
    efficient training processes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s get into it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Some Key Terms
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three terms we should clarify before going any further:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '*Foundation models* are advanced ML architectures that serve as the foundational
    building blocks for creating specialized models. They are pretrained on massive
    datasets, often consisting of text and recently including other data types such
    as code, images, audio, and video to develop general language comprehension and
    pattern recognition capabilities. These models encode statistical relationships
    and linguistic structures from their training data, forming a robust starting
    point for further fine-tuning. This fine-tuning tailors the models to specific
    tasks or applications, such as powering LLMs or other AI-driven solutions.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Large language models
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '*Large language models* are specialized implementations of foundation models
    that have undergone additional training or fine-tuning to excel in specific language-based
    tasks. These models are designed to predict and generate human-like text by analyzing
    and emulating natural language patterns. LLMs are highly versatile, supporting
    several natural language processing (NLP) applications such as text generation,
    sentiment analysis, language translation, question answering, and more. Popular
    use cases include chatbots, content creation, multilingual communication, data
    analysis, code generation, recommendation systems, and virtual assistants. [“Enterprise
    Use Cases for LLMs”](#ch01_enterprise_use_cases_for_llms_1748895465616095) will
    look at these applications in more detail.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI models
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative AI*, or *GenAI*, refers to foundation models that have been trained
    specifically to generate content (images, text, audio, or video) based on the
    patterns and information they have learned. Some of the earliest generative AI
    models were generative adversarial networks (GANs), introduced in 2018; more recently,
    diffusion models, LLMs, and multimodal models like Gemini have become available.
    Given their generative nature, LLMs are considered a subset of generative AI models.
    In the context of LLMs, generative AI can generate text responses, creative stories,
    product descriptions, and more, based on input and learned patterns.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Confusingly, these three terms are frequently used interchangeably and loosely.
    For example, a popular image generation model, DALL-E, is better categorized as
    a generative AI model than as a large language model. Recently, however, the DALL-E
    image generation functionality has been integrated into the ChatGPT chatbot, one
    of the most popular LLM applications. Therefore, a user can ask an LLM like ChatGPT
    to generate images. Over time, the language seems to be evolving toward calling
    all of these *AI models,* for simplicity.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transformer model, introduced by the paper [“Attention Is All You Need,”](https://oreil.ly/J8MBW)
    marked one of the biggest shifts in how we approach sequence-based tasks. Transformers
    have set new standards in how to handle language data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Before transformers, the most popular solution for NLP tasks was *recurrent
    neural networks*. RNNs process data sequentially, one step at a time, which makes
    them suitable for handling time-dependent data such as text. However, this sequential
    processing introduces a significant drawback: RNNs often struggle to retain information
    from earlier steps as they move forward in the sequence, especially over long
    inputs.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'During neural network training, the model processes input data and generates
    predictions. These predictions are compared to the correct answers using a loss
    function, which calculates the error (how far the predictions are from the correct
    answers). An algorithm, such as *backpropagation*, calculates *gradients*: values
    that indicate how the model’s parameters (weights and biases) should be adjusted
    to reduce the error and improve accuracy.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: However, in long sequences like those handled by RNNs, gradients can become
    very small as they are repeatedly multiplied during backpropagation. Over time,
    these small values may shrink so much that computers treat them as zero, effectively
    stopping the model from learning. This issue is known as the *vanishing gradient
    problem*, and it prevents the model from learning long-term dependencies in the
    data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '*Transformers*, on the other hand, overcome this limitation by using self-attention
    and parallel processing, allowing them to handle sequences more efficiently and
    capture long-range dependencies effectively. Instead of processing data one step
    at a time, transformers analyze all input tokens (e.g., words in a sentence) simultaneously.
    *Self-attention* is a mechanism that allows each word or token in a sequence to
    focus on other words in the same sequence, regardless of their position. This
    is achieved by calculating a set of attention weights that measure the relevance
    of each token in the sequence to every other token. For instance, in a sentence,
    self-attention can help a word like *it* to align itself with its correct reference,
    even if that reference is several words away. Thus, self-attention allows the
    model to weigh the importance of each token relative to others in the input, enabling
    it to capture relationships across the entire input sequence efficiently. This
    parallel processing not only speeds up computation but also eliminates the issues
    associated with sequential processing, like the vanishing gradient problem.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to their ability to manage long-range dependencies and handle vast amounts
    of data, transformer-based models excel in various NLP tasks, including translation,
    summarization, and question answering. Their ability to focus on different parts
    of the sequence regardless of their relative distance, along with positional encoding
    to retain sequence order, allows transformers to handle long sequences without
    losing context.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Some people wondered, “Well, since they can be scaled much better now, how about
    we throw more computing power and a lot more data at these models to see what
    happens?” Models like GPT-3, LLaMA, and their successors demonstrated that increasing
    the number of parameters can significantly improve the performance of transformer
    models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have extended their influence beyond NLP into image processing
    with innovations like the *vision transformer* (ViT), which treats image patches
    as sequences and applies transformer models to them. ViT has shown promising results
    in image classification, offering a viable alternative to the previous solution,
    convolutional neural networks (CNNs). Additionally, in recommender systems, transformers’
    ability to model complex patterns and dependencies enhances accuracy and personalization.
    [Table 1-1](#ch01_table_1_1748895465606161) compares the abilities of the neural
    network models we’ve discussed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-1\. The evolution of different neural network models
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '|   | CNNs | RNNs | Transformers |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| Application | Best suited for spatial-based tasks (e.g., images) | Well suited
    for sequence-based tasks (e.g., NLP) | Well suited for capturing all three modalities:
    images, NLP, and speech |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| Computation | Highly parallelizable input processing | Sequential processing
    | Parallel processing of inputs |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| Performance on language-specific tasks | Need large number of stacked convolution
    blocks for handling long-range dependencies | Can handle long-range dependencies
    much better than CNNs but can handle the dependencies well only to a given length
    | Can handle long- to very-long-range dependencies much better than other architectures
    such as RNNs or LSTMs |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| Scalability | Scalable | Limited scalability | Highly scalable |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| Data requirements | Work well even on small datasets | Work well even on
    small datasets | Don’t work well on small datasets |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| Ease of training | Easy to train and tune | Require more tuning than CNNs
    | Difficult to train and tune |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| Interpretability | Easy to debug | Difficult to debug | Difficult to debug
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| Deployment | Easy to deploy | Easy to deploy | Difficult to deploy |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| Small edge devices | Works well on edge devices | Works well on edge devices
    | Limited support for edge devices |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| Explainability | Supports wide variety of explainability | Limited explainability
    | Very limited explainability |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: This trend of throwing more compute and data at transformers is what sparked
    the evolution of LLMs, as well as the shift from an architecture that can do well
    on a single modality to one that generalizes on most modalities. Understanding
    this evolution can help you appreciate the differences in model architectures.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs excel at understanding context and making associations among words, phrases,
    and concepts to provide relevant information based on the input query or prompt.
    While structured knowledge bases rely on human-curated data, LLMs can automatically
    extract knowledge from unstructured text. When trained on diverse textual sources,
    they can process a vast amount of information without explicit human intervention.
    However, this also introduces a challenge, as the model can learn biased or incorrect
    information from the training data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are also designed to understand and generate human-like text and to be
    accessible through natural language queries in conversational, interactive settings.
    This makes them convenient and user-friendly for retrieving information and obtaining
    responses.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: These models are “large” not just because of the amount of data they’re trained
    on but also because of their number of parameters. Think of *parameters* as being
    like “knobs” inside the models that may be adjusted during training to help the
    models learn better. In neural networks, parameters are weights and biases. When
    an input like a prompt is presented to a model, it first transforms the input
    into a numerical representation, and then the numbers are processed through the
    neural network. Each node in the neural network contains a bias, adding or subtracting
    to the input value, and each connection between nodes contains a weight that will
    multiply the value of the input as it passes through nodes. Using more parameters
    greatly extends the capabilities of traditional transformer models, but not without
    massive trade-offs in cost and evaluation complexity.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: There are two basic categories of LLMs, discriminative and generative. *Discriminative
    models*, such as ​BERT (Bidirectional Encoder Representations from Transformers),
    which was introduced in 2018, learn the boundary between classes in a classification
    problem. They’re concerned with the conditional probability *P*(*y*|*x*), which
    is the probability of the output given the input. Discriminative transformer models
    are typically used for tasks like text classification, sentiment analysis, and
    named-entity recognition, where the goal is to predict a label or category given
    some input ​text.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative models*, such as GPT-3 and GPT-4, learn the joint probability distribution
    of the input and the output, or *P*(*x*, *y*), and can generate new data points
    similar to the training data. Generative models are used for tasks like text generation,
    where the goal is to generate new text similar to the text the model was trained
    on. Not all LLMs need to be generative, although most are. Throughout this book,
    when we refer to “LLMs,” we mean generative LLMs.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: LLM Architectures
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two main types of architecture for language models: encoders and
    decoders. Encoders and decoders can also be combined, and there is ongoing research
    on new architectures.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-Only LLMs
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Encoder-only models* are designed​ to process and comprehend input text, transforming
    it into a meaningful representation or embedding. *Embeddings* are numerical representations
    of data, such as words, phrases, or sentences, in a high-dimensional vector space.
    Embeddings capture meaning and context in a way that results in words with similar
    meanings or contexts being placed close together in this vector space. This representation
    captures the essence of the input, making it suitable for tasks where understanding
    the context is needed.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: One of the most notable examples of an encoder-only model is [BERT](https://oreil.ly/f2AL4).
    During its pretraining phase, BERT uses *masked language modeling*, a technique
    where random words in the text are masked and the model learns to predict these
    masked words based on the surrounding context. BERT is also trained using next-sentence
    prediction, where it determines whether one sentence follows another ​logically.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The primary advantage of encoder-only models lies in their syntactic understanding
    of text; i.e., their ability to capture the intricate relationships between words
    and their contexts. These models excel in tasks such as sentiment analysis, named-entity
    recognition, and question answering.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: However, encoder-only models have their limitations. They are not designed for
    generating new text; their focus is solely on understanding and analyzing the
    input. This limitation can be restrictive when using them in applications requiring
    text generation or completion.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-Only LLMs
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Decoder-only models* are good​ at generating coherent and contextually relevant
    text based on an input or prompt. Examples of this architecture are the generative
    pretrained transformer (GPT) series, including GPT-2, GPT-3, and the most recent
    GPT-4.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: These models are pretrained using a *language modeling objective*. With this
    technique, they learn to predict the next word in a sequence given the preceding
    context, allowing them to generate text that flows naturally and maintains coherence
    over longer passages.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of decoder-only models is their ability to generate high-quality
    text. This makes them extremely effective for tasks such as text completion, summarization,
    and creative writing. They also exhibit *emergent properties*, meaning that they
    can perform tasks beyond their initial training objective, such as translation
    and question answering, without additional fine-tuning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: However, their focus on text generation can be a limitation in tasks requiring
    deep understanding of the input text. Decoder-only models generate text based
    on patterns learned during training, which may not always align with the specific
    nuances of the input.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Encoder–Decoder LLMs
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Encoder–decoder models* combine​ the strengths of both encoder and decoder
    architectures, making them suitable for tasks involving complex mappings between
    input and output sequences.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: In this setup, the encoder processes the input text to create an embedding,
    which the decoder then uses to generate the output text. Notable examples include
    Bidirectional and Auto-Regressive Transformer (BART) and Text-To-Text Transfer
    Transformer (T5). BART, introduced in 2019, is trained using *denoising auto-encoding*,
    where parts of the input text are corrupted and the model learns to reconstruct
    the original text.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The encoder–decoder architecture excels at tasks where the input and output
    are different in structure and length, such as machine translation and text summarization.
    However, the complexity of training and the computational resources these models
    require can be a drawback. Their dual architecture means they must effectively
    integrate both components, which can be demanding in terms of both data and processing
    power.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: State Space Architectures
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A new approach tries to solve one of the problems with transformers, which is
    that the self-attention mechanism has *quadratic complexity*. This means that
    the number of computations required for inferencing grows with the square of input
    size, since the relationship between each pair of tokens needs to be modeled.
    Mathematically, it is often represented as *O*(*n*²), where *n* is the number
    of tokens (words or subwords in a sentence). Quadratic complexity is generally
    a hard computational problem, especially when using larger datasets.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The *state space architecture* replaces the transformer approach by incorporating
    *state space representations*, which model the state of the system instead of
    recording it at each step. This compression allows for linear computational complexity,
    improving computational performance and reducing memory requirements, but it increases
    the rate of error.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Researchers are trying to solve the error problem. Recent examples are [Mamba
    and Mamba-2](https://oreil.ly/p3rqX), which create a state representation that
    dynamically attempts to determine the important parts of the prompt by modeling
    importance as a state space parameter. In experimental settings, Mamba performs
    as well as a transformer-based model that has double the number of parameters
    for small and medium prompts but still has not delivered on the promise of low
    error rates for larger prompts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Each LLM architectural design has its own sets of strengths and limitations.
    Encoder-only models like BERT are highly effective for understanding and analyzing
    text but fall short in generating new content. Decoder-only models, exemplified
    by the GPT series, excel in generating coherent and contextually relevant text
    but are nondeterministic, which can be problematic for some applications like
    text classification. Emerging architectures like state space models, which promise
    enhancements in performance and applicability, should be monitored, but they haven’t
    been proven yet.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Small Language Models
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another recent development is *small language models* (SLMs), which are compact,
    efficient language models designed to perform NLP tasks while using fewer computational
    resources than LLMs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Unlike LLMs, which contain billions of parameters and require substantial memory
    and processing power, SLMs are often designed to have millions or even just hundreds
    of thousands of parameters. The trade-off is that they must focus on specific
    tasks or subjects. This makes them lightweight, cost-effective, and deployable
    on a wider range of devices, including mobile phones, IoT edge devices, and in
    environments with limited computational resources. The development of SLMs has
    been driven by the demand for efficient, accessible AI solutions that can operate
    in real time and offline, providing functionality without relying on cloud-based
    infrastructure.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: SLMs do not perform well on tasks that require contextual understanding, extensive
    memory, or reasoning abilities. They are not intended for more general problem-solving
    and need to be fine-tuned on specific datasets to perform well on particular tasks,
    maximizing efficiency while maintaining accuracy within a defined scope. While
    LLMs tend to perform several NLP tasks reasonably well in a large number of domains,
    SLMs need to be specifically trained. For instance, an LLM might be able to perform
    moderately well at summarizing legal documents as well as medical articles, while
    an SLM would excel in one and perform poorly at the other.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an LLM
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the LLM world, it’s easy to get swept up in the excitement of the latest
    breakthroughs and cutting-edge technologies. New models pop up all the time. The
    truth is, selecting the right LLM is more than just a technical decision; it’s
    a strategic choice with far-reaching implications.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Considerations in the Selection of an LLM
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are five reasons why the model you choose can make all the difference:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Alignment with objectives
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Are you looking for a model that excels at generating human-like text? Or do
    you need one that can understand complex queries and provide accurate responses?
    The specific capabilities of different models can vary significantly. Some are
    designed with a focus on conversational abilities, while others are optimized
    for tasks like summarization or translation. Choosing a model that aligns with
    your objectives ensures that you’re investing in a tool that will deliver the
    results you need.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Performance and efficiency
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Not all LLMs are created equal. Larger models might offer impressive performance
    and efficiency, but they often come with high computational costs and slower response
    times. Smaller, more optimized models tend to provide faster results and be more
    cost-effective, but rarely do they match the performance of their larger counterparts.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Training data and bias
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The training data used to develop an LLM shapes its behavior and outputs. Variations
    in the datasets on which models are trained can lead to variations in how they
    handle specific topics or issues. Some models exhibit biases based on their training
    data, which can impact the accuracy and fairness of their responses. Choosing
    a model with a diverse and representative training dataset can help mitigate these
    risks and ensure more reliable and equitable outcomes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Customization and adaptability
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Your needs might not fit neatly into the one-size-fits-all approach of a generic
    LLM. Some models offer greater flexibility and can be fine-tuned or customized
    to better suit your specific requirements. If that’s what you need, choose one
    with strong customization capabilities so that you can mold it to better fit your
    use case.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Integration and support
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The practical aspects of integrating an LLM into your existing systems and workflows
    cannot be overlooked. Some models come with robust support and documentation,
    making integration smoother and less time-consuming. Others require more effort
    to set up and maintain. Considering how well a model integrates with your infrastructure
    and the level of support available can save you time and reduce headaches over
    the long run.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the LLM model you choose is not just a technical decision; it’s a
    strategic one that impacts the effectiveness, efficiency, and overall success
    of your AI initiatives. Remember: the model you choose matters. By carefully evaluating
    your needs and understanding the strengths and limitations of different models,
    you can make an informed choice that aligns with your goals and sets you up for
    success.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The Big Debate: Open Source Versus Proprietary LLMs'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Companies must navigate a complex landscape when choosing among open source,
    closed-source, and open weight LLMs. [Figure 1-1](#ch01_figure_1_1748895465600862)
    shows the choices of a sample of companies today. This section looks at each option’s
    limitations and benefits.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/llmo_0101.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-1\. Enterprise adoption of different proprietary LLMs (source: [Andreessen
    Horowitz](https://oreil.ly/lqXYT))'
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Open source and open weight LLMs
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open source and open weight are two types of publicly accessible LLMs that have
    gained traction in the AI community as of this writing, particularly among those
    looking to customize, deploy, or study advanced AI without relying on proprietary
    solutions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '*Open source* LLMs are models with freely available underlying source code.
    Anyone can inspect, modify, and potentially redistribute the model and its architecture.
    These models typically include details about the architecture, training methods,
    and source code for the framework. Using open source models provides technical
    transparency and adaptability and fosters a community of collaboration. However,
    open source LLMs may or may not come with pretrained weights, the trained parameters
    that make the model functional and useful for specific tasks. These weights are
    the model’s “knowledge” gained from its training on large datasets and are essential
    for the model to perform effectively without retraining from scratch. Companies
    that want to take advantage of such models may need to acquire the training data
    themselves.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: With *open weight* LLMs, the weights are publicly accessible. Having access
    to the weights means that users can directly deploy the model for real-world applications
    like text generation, summarization, and translation or fine-tune it on their
    own data. While many open weight models are also open source, some restrict use
    for commercial applications or require adherence to specific licensing terms,
    as seen with models like Meta’s Llama series.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The distinction between open source and open weight LLMs is crucial in determining
    how accessible and useful a model is “out of the box.” Open source models without
    weights can still allow for architectural experimentation and model-training setups,
    but they lack immediate functionality for practical applications until they are
    trained, and training requires substantial computational resources. In contrast,
    open weight models provide ready-to-use capabilities, making them more accessible
    to developers who do not have the resources for large-scale model training but
    want to fine-tune or deploy a pretrained model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging open source and/or open weight models such as Llama or Mistral,
    companies can deploy models on existing hardware. This can be more cost-effective
    than using cloud-based proprietary solutions, which involve renting hardware.
    Such an approach can be particularly advantageous for startups or small to medium
    enterprises (SMEs) operating under tight budget constraints. For these companies,
    the financial savings can free up resources for other needs, like fine-tuning.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: A company may have requirements besides financial concerns, such as wanting
    to ensure that the training data includes or excludes specific datasets. In these
    cases, an open weight model is not sufficient; the business really needs an open
    source model. For example, a company may want to guarantee that a model has never
    seen a specific data point; an open weight model whose training dataset is not
    shared can’t offer such a guarantee.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Community support is another potential advantage of open source LLMs. The collaborative
    nature of the open source ecosystem means that developers, researchers, and organizations
    continuously contribute to improving these models, and newly fine-tuned models
    are easily available via Hugging Face. Companies benefit not only from this collective
    intelligence but also from access to a wider range of resources, tools, and best
    practices. This community-driven development is dynamic and evolving, and it’s
    often where new developments begin.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: However, the open source/open weight approach is not without its challenges.
    Maintenance and support can be significant hurdles. Data privacy and security
    also emerge as big concerns. Transparency can be a double-edged sword, exposing
    a company to potential risks even as it demands significant effort to safeguard
    sensitive information and comply with data protection regulations. Ensuring that
    these models do not become a vector for security breaches requires meticulous
    attention and proactive measures.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Scalability and performance are additional considerations. Open source LLMs
    aren’t always optimized for large-scale deployments. Companies with substantial
    operational demands might face performance bottlenecks or scalability challenges.
    The customization required to adapt open source models for enterprise-grade applications
    can be resource intensive and require significant engineering efforts.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Open source and open weight language models also introduce security concerns.
    Anyone can immediately use, fine-tune, or modify pretrained open weight models
    and potentially apply them in harmful ways, such as generating misinformation,
    creating realistic fake content, or deploying automated tools for phishing and
    social engineering. Since open weight models’ training data often includes both
    public and proprietary datasets, they can also sometimes unintentionally generate
    or reveal sensitive or biased information embedded in the training data, posing
    privacy risks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, open source models, which include the code and architectural blueprints,
    are vulnerable to manipulation and exploitation. Malicious actors can introduce
    harmful code or adjust models to bypass safety mechanisms, then distribute these
    altered versions under the guise of legitimate software. This can lead to scenarios
    where organizations unknowingly adopt models that include backdoors or biased,
    harmful outputs. The decentralized nature of open source development means that
    code modifications don’t always go through rigorous security checks, leaving room
    for vulnerabilities that could be exploited. Addressing these security challenges
    requires adopting responsible AI practices, including rigorous code reviews, security
    audits, and clear usage policies to mitigate risks while promoting open collaboration.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Carefully review any contractual restrictions on usage before adopting a model.
    You don’t want to build a whole commercial application around an open source LLM
    only to find out that it does not allow commercial usage.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Closed-source LLMs
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the other side of the spectrum are *closed-source*, or *proprietary*, LLMs
    such as those developed by leading tech giants. These models often come with robust
    support and maintenance, including dedicated assistance for troubleshooting and
    optimizing performance. This support infrastructure ensures that any issues encountered
    are addressed promptly, allowing companies to focus on their core activities without
    getting sidetracked by technical difficulties.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Closed-source LLMs are generally optimized for large-scale deployments, making
    sure that they can scale with operational loads effectively, so they often come
    with performance guarantees. Their performance benchmarks often reflect their
    ability to deliver consistent and reliable results—a critical factor for companies
    with high operational demands.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary limitations of the closed-source approach is the high cost.
    Another is the lack of transparency, which means that companies have limited visibility
    into the internal workings of these models. While this concern may seem unusual,
    consider a scenario in which a commercial LLM provider inadvertently consumes
    private data during training. You use this LLM provider for your own application,
    and some of your users realize how to get your application to reveal the private
    data. The people whose data was revealed sue you. We recommend that you fully
    understand what legal protections are in place when using information services
    like third-party LLMs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of these drawbacks, companies are willing to make expensive bets
    right now, hoping for excellent returns in the future from investing in GenAI
    applications.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise Use Cases for LLMs
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are transforming enterprise operations in many industries, from changing
    how we retrieve knowledge to enhancing autonomous agents. They do this through
    a handful of applications, including knowledge retrieval, translation, audio–speech
    synthesis, recommender systems, and autonomous agents.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Retrieval
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: People have long used search engines to discover information, but the limitations
    of these tools’ have become more apparent as data volumes and complexity grow.
    LLMs offer a new paradigm for accessing and using information. Unlike conventional
    systems, which rely heavily on keyword matching and ranking algorithms, LLMs bring
    a conversational, personalized approach to information retrieval.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Users can engage in long conversations with LLMs. Instead of simply receiving
    a list of links or documents, they can set parameters for the tone, intent, and
    structure of the information they need. This capability transforms the search
    experience from a transactional process into a dynamic dialogue. For example,
    an LLM can interpret a request like “Explain this concept as if I were a beginner,”
    and provide a tailored explanation that’s both accessible and relevant.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: On the data retrieval side, LLMs can enhance productivity tools, for example
    through integrations with office software suites like those from Google and Microsoft.
    Imagine querying a spreadsheet with natural language to extract insights or asking
    a document to summarize key points. This simplifies data management and makes
    complex information more accessible. Furthermore, LLMs can integrate with internal
    systems to automate routine tasks and create knowledge graphs, streamlining workflows
    and enhancing organizational efficiency. However, while LLMs improve the accuracy
    and relevance of information retrieval, they also require meticulous handling
    to ensure data privacy and system security.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Translation
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Translation is another domain where LLMs are being used heavily. Traditional
    machine translation systems often struggled with languages for which they had
    limited datasets, as they had to rely on statistical methods. LLMs are changing
    this by offering zero-shot and few-shot translation capabilities. *Zero-shot*
    refers to the model’s ability to translate languages without prior examples, a
    feat that was previously challenging. *Few-shot*, on the other hand, allows LLMs
    to perform well with minimal data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: This is particularly advantageous for translating languages that are underrepresented
    in training datasets. For companies involved in global operations or content creation,
    this is a major selling point. It eases localization of content, such as subtitling
    films or translating marketing materials, without extensive data requirements,
    allowing companies to expand into new markets without investing too many resources
    up front.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: LLMs trained on multilingual datasets can easily adapt to new languages, allowing
    translations across a broader spectrum of languages, including those with sparse
    resources. The applications for this extend to literature, film, and even real-time
    communication, where accurate and contextually appropriate translation can be
    helpful.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Yet, while LLMs offer significant improvements over traditional translation
    methods, maintaining accuracy and handling idioms still remain open challenges.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Speech Synthesis
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ability to generate speech that resonates with human listeners can significantly
    enhance user experience and interaction. *Speech synthesis*, generating audio
    that mimics human speech from text, is another area where LLMs are making remarkable
    progress. Historically, speech synthesis systems have struggled with creating
    natural and engaging audio outputs: the sound generated sounded clearly “robotic.”
    LLMs, however, have the potential to revolutionize this field by generating human-like
    speech with impressive fidelity. With training on text and audio datasets, LLMs
    can understand and replicate the subtleties of human speech, such as intonation,
    rhythm, and stress.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: This is useful for applications like virtual assistants, realistic voice-overs
    for characters in video games, or engaging audio content for educational materials.
    Using LLMs to automate the creation of speech content makes it easy for businesses
    to produce large volumes of content without the time and costs of extensive manual
    recording. However, audio–speech synthesis still has room to improve, especially
    with regard to recognizing accents and other variations in speech.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Recommender Systems
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recommender systems are at the heart of many digital platforms, from ecommerce
    to streaming services. LLMs enhance these systems by incorporating a deeper understanding
    of users’ preferences and contextual factors. Earlier recommender systems relied
    on historical user data and predefined algorithms, which often led to limited
    or repetitive suggestions. LLMs, with their ability to process and interpret diverse
    data sources, offer a more nuanced approach.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: LLM-powered recommender systems can analyze user interactions, preferences,
    and even conversational cues, including audio and video inputs, to deliver personalized
    recommendations in real time. For example, if a user describes a product in natural
    language and provides an image, the LLM can integrate both modalities to offer
    more relevant suggestions, even in response to ambiguous or vague requests.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Despite these advantages, many challenges remain unsolved. For example, maintaining
    user trust requires careful attention to the model’s transparency and reasoning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous AI Agents
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*AI agents* are designed to perform specific tasks autonomously, leveraging
    LLMs to execute complex operations that would otherwise require human intervention.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a customer service environment, traditional automated agent
    systems might follow rigid scripts or rely on basic rule-based logic. LLM-powered
    AI agents, however, can engage in dynamic, context-aware conversations. They understand
    user queries more deeply, interpret intent more accurately, and generate responses
    that are more natural and engaging.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: In project management, LLMs can power intelligent project assistants that manage
    schedules, set reminders, and even draft project reports. These AI agents can
    interact with team members, understand project requirements, and adapt their responses
    to ongoing developments.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Agentic Systems
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Agentic systems* represent a more novel application of LLMs, where AI agents
    not only perform tasks but also make strategic decisions. These systems leverage
    LLMs’ data processing and analysis capabilities to discern patterns and make informed
    decisions in real time. This is particularly helpful in environments where decisions
    need to be based on complex, multifaceted information (as shown by the example
    workflow in [Figure 1-2](#ch01_figure_2_1748895465600898)).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/llmo_0102.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-2\. Agentic AI in the enterprise (source: [Haptik](https://oreil.ly/NapOi))'
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In finance, agentic systems can digest data from financial reports, news articles,
    and market analytics, then use it to analyze market trends, assess risk factors,
    and make investment recommendations that align with investment strategies.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in supply chain management, agentic systems can optimize inventory
    levels, predict demand fluctuations, and coordinate logistics based on data from
    various sources—such as sales forecasts, supply chain disruptions, and production
    schedules.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: However, these systems aren’t always reliable. Integrating them into existing
    workflows requires careful planning. Companies must consider how AI agents and
    agentic systems will interact with human teams, how they will be managed, and
    how their outputs will be monitored. Clear guidelines and oversight mechanisms
    are essential to ensure that these systems complement rather than disrupt existing
    operations. These issues are discussed in [Chapter 8](ch08.html#ch08_governance_monitoring_privacy_and_security_1748896766177413).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Data security and privacy are also big concerns. LLMs handle vast amounts of
    sensitive information, and protecting it from breaches or misuse is key. You need
    to establish strong data governance policies and invest in security measures to
    safeguard against potential risks. These issues, too, are discussed in [Chapter 8](ch08.html#ch08_governance_monitoring_privacy_and_security_1748896766177413).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Ten Challenges of Building with LLMs
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs introduce several new challenges, which can be amplified by the enormous
    scale of LLMs and their numerous applications. Addressing these challenges is
    important for integrating and deploying LLMs in production. Following is a list
    of 10 challenges with pointers to the chapters in this book where they are addressed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Size and Complexity
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs generally have millions or even billions of parameters. This makes training,
    monitoring, and evaluating them extremely complex. Moreover, being generative
    models, they can fail silently, producing hallucinations and inaccurate information.
    Addressing this requires a structured approach that not only includes benchmarks
    commonly used for machine learning but also adds several other techniques; [Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823)
    explores this topic further.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Training Scale and Duration
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training LLMs requires processing large datasets. This is difficult not only
    from the data management perspective but also in terms of the memory and computational
    resources required for training the models. We discuss this in [Chapter 3](ch03.html#ch03_llm_based_applications_1748895493844515).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Training LLMs can take days, weeks, or even months, and managing parallel and
    distributed training across large clusters of GPUs and TPUs requires specialized
    hardware and organizational skills. This means that hardware represents a major
    dependency on external organizations and market availability, one that requires
    careful, systematic planning. We discuss this in [Chapter 9](ch09.html#ch09_scaling_hardware_infrastructure_and_resource_ma_1748896826216961).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Handling large, potentially sensitive training datasets requires careful security
    measures and anonymization, as discussed in [Chapter 2](ch02.html#ch02_introduction_to_llmops_1748895480208948).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Prompt Engineering
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common ways to make an LLM work better for a specific problem
    is prompt engineering, the science and art of crafting the text inputs that are
    sent to the models. Prompt updates can significantly improve or degrade the user
    experience. But prompt engineering is iterative and can be difficult to master
    and document, especially with closed-source LLMs. You’ll find a discussion of
    this in [Chapter 5](ch05.html#ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Updates of proprietary models, like OpenAI’s GPT-4, can result in significant
    *model drift*, where the same inputs suddenly provide a different output due to
    a model update. Model drift requires effort and financial commitment to fix. This
    becomes additionally complex when there are many interdependent prompts connected
    to each other, such as in an *orchestration framework* (i.e., a structured platform
    used to automate, coordinate, and manage complex tasks and services) and there’s
    a change in the underlying model, as the entire complex prompt chain can break
    in unexpected and hard-to-detect ways. If your infrastructure relies heavily on
    prompt-engineering pipelines, monitoring is crucial; [Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823)
    goes into this in more depth.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Inference Latency and Throughput
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Responses provided by LLMs are also called *inferences*. LLMs are often deployed
    in applications that require real-time or near-real-time responses, which means
    that optimizing for speed becomes important. This can be especially complex with
    dynamic models like LLMs. Also, maintaining high throughput without having access
    to model parameters can add complexity for LLMOps teams. Edge devices used in
    IoT applications introduce even more challenges related to limited computational
    resources and varying network conditions. These issues are discussed in [Chapter 9](ch09.html#ch09_scaling_hardware_infrastructure_and_resource_ma_1748896826216961).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Ethical Considerations
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like any other machine learning model, LLMs generate outputs based on the data
    that they have been trained on. LLMs applications are frequently designed to create
    the experience of chatting with a human instead of a machine, making them accessible
    to a much larger user base than specialized machine learning systems and greatly
    increasing the impact of potential biases introduced by the training data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823) discusses
    techniques for monitoring LLM outputs, and the privacy and ethical implications
    of their use are explained in [Chapter 8](ch08.html#ch08_governance_monitoring_privacy_and_security_1748896766177413).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Resource Scaling and Orchestration
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The scale at which LLMs operate often requires load balancing and dynamic resource
    scaling. Different proprietary models can also behave very differently based on
    the use case, and constant scenario modeling is expensive and time intensive.
    [Chapter 5](ch05.html#ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361)
    explores how to manage dependencies across various components in distributed multi-model
    environments, ensuring reliability and scalability.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Integrations and Toolkits
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs require several new integrations and toolkits that are adapted to both
    generative as well as discriminative use cases and involve communicating with
    various APIs. Integrating these LLMs into existing systems requires robust security
    protocols to prevent vulnerabilities and potential misuse. Changes in LLMs and
    version management, discussed in [Chapter 8](ch08.html#ch08_governance_monitoring_privacy_and_security_1748896766177413),
    can also lead to compatibility issues across the stack.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Broad Applicability
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are adaptable and easy to use, which means that they can be applied to
    numerous consumer-facing applications, as we will see in [Chapter 3](ch03.html#ch03_llm_based_applications_1748895493844515).
    This makes them more likely to be exposed to untested scenarios than traditional
    machine learning systems, and thus they require a faster feedback loop to monitor
    and improve their performance. [Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823)
    addresses monitoring techniques.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Privacy and Security
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Collecting real-time information involves handling user data, sometimes including
    personally identifying information (PII). This means that security and privacy
    become the cornerstone of maintaining trust and regulatory compliance. This challenge
    extends well beyond inference monitoring, touching the domain of cybersecurity.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Even companies such as OpenAI have [received reports about database leaks](https://oreil.ly/yqPpG)
    into user accounts that made chat interactions visible to unauthorized users.
    We talk more about privacy and security in [Chapter 8](ch08.html#ch08_governance_monitoring_privacy_and_security_1748896766177413).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Regularly auditing your data management processes, both internally and externally,
    is also vital for enhancing user trust and complying with legal requirements.
    Best practices for data management are discussed in [Chapter 4](ch04.html#ch04_data_engineering_for_llms_1748895507364914).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Costs
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the biggest considerations for LLMs is cost, both immediate and long-term.
    While most transformer models require expensive training, maintaining and scaling
    LLMs incurs the highest costs, especially in the inference stages. You could end
    up paying even for failed requests, so experimenting with model performance can
    become very expensive very quickly for companies building on closed and proprietary
    models.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Even in open source models, excessive fine-tuning can quickly lead to a phenomenon
    called *overfitting*, where the model appears to perform extremely well because
    it learns the training dataset but does not generalize to the unseen data that
    will be presented to it by real users. There are always trade-offs between generalization
    ability and cost; these are explored in [Chapter 5](ch05.html#ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adopting LLMs requires careful consideration and strategic planning to navigate
    these intricate challenges, and organizations require a new discipline and a set
    of new tools to succeed. We call this discipline LLMOps, and we start our journey
    by defining it in the next ​chapter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dao, Tri, and Albert Gu. [“Transformers Are SSMs: Generalized Models and Efficient
    Algorithms Through Structured State Space Duality”](https://oreil.ly/POlHU), arXiv,
    May 31, 2024.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Devlin, Jacob, et al. [“BERT: Pre-Training of Deep Bidirectional Transformers
    for Language Understanding”](https://oreil.ly/84NM2), arXiv, May 24, 2019.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Haptik. n.d. [“A Comprehensive Guide to Agentic AI”](https://oreil.ly/CO7uA),
    Accessed May 21, 2025.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI. [“March 20 ChatGPT Outage: Here’s What Happened”](https://oreil.ly/5kdkr),
    March 24, 2023.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Vaswani, Ashish, et al. “Attention Is All You Need”, In [*NIPS’17: Proceedings
    of the 31st International Conference on Neural Information Processing Systems*](https://oreil.ly/hfTxe),
    edited by Ulrike von Luxburg, Isabelle Guyon, Samy Bengio, Hanna Wallach, and
    Rob Fergus (Curran Associates, 2017).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Wang, Sarah, and Shangda Xu. [“16 Changes to the Way Enterprises Are Building
    and Buying Generative AI”](https://oreil.ly/yRrmR), Andreessen Horowitz, March
    21, 2024.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 王莎拉，徐尚达. [“企业构建和购买生成式AI的16项变革”](https://oreil.ly/yRrmR), 安德森·霍洛维茨，2024年3月21日。
