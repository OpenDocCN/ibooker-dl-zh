["```py\nfrom datetime import datetime\n\ndef timestamp_to_isostring(date: int) -> str:\n    return datetime.fromtimestamp(date).isoformat()\n\nprint(timestamp_to_isostring(1736680773))\n# 2025-01-12T11:19:52.876758\n\nprint(timestamp_to_isostring(\"27 Jan 2025 14:48:00\"))\n# error: Argument 1 to \"timestamp_to_isostring\" has incompatible type \"str\";\n# expected \"int\" [arg-type]\n```", "```py\n$ pip install loguru\n```", "```py\n# utils.py\n\nfrom typing import Literal, TypeAlias\nfrom loguru import logger\nimport tiktoken\n\nSupportedModels: TypeAlias = Literal[\"gpt-3.5\", \"gpt-4\"]\nPriceTable: TypeAlias = dict[SupportedModels, float] ![1](assets/1.png) ![2](assets/2.png)\nprice_table: PriceTable = {\"gpt-3.5\": 0.0030, \"gpt-4\": 0.0200} ![3](assets/3.png)\n\ndef count_tokens(text: str | None) -> int: ![4](assets/4.png)\n    if text is None:\n        logger.warning(\"Response is None. Assuming 0 tokens used\")\n        return 0 ![5](assets/5.png)\n    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n    return len(enc.encode(text)) ![6](assets/6.png)\n\ndef calculate_usage_costs(\n    prompt: str,\n    response: str | None,\n    model: SupportedModels,\n) -> tuple[float, float, float]: ![7](assets/7.png)\n    if model not in price_table:\n        # raise at runtime - in case someone ignores type errors\n        raise ValueError(f\"Cost calculation is not supported for {model} model.\") ![8](assets/8.png)\n    price = price_table[model] ![9](assets/9.png)\n    req_costs = price * count_tokens(prompt) / 1000\n    res_costs = price * count_tokens(response) / 1000 ![10](assets/10.png)\n    total_costs = req_costs + res_costs\n    return req_costs, res_costs, total_costs ![11](assets/11.png)\n```", "```py\nfrom typing import Annotated, Literal\n\nSupportedModels = Annotated[\n    Literal[\"gpt-3.5-turbo\", \"gpt-4o\"], \"Supported text models\"\n]\nPriceTableType = Annotated[\n    dict[SupportedModels, float], \"Supported model pricing table\"\n]\n\nprices: PriceTableType = {\n    \"gpt-4o\": 0.000638,\n    # error: Dict entry 1 has incompatible type \"Literal['gpt4-o']\" [dict-item]\n    \"gpt4-o\": 0.000638,\n    # error: Dict entry 2 has incompatible type \"Literal['gpt-4']\" [dict-item]\n    \"gpt-4\": 0.000638,\n}\n```", "```py\n# utils.py\n\nfrom dataclasses import dataclass\nfrom typing import Literal, TypeAlias\nfrom utils import count_tokens\n\nSupportedModels: TypeAlias = Literal[\"gpt-3.5\", \"gpt-4\"]\nPriceTable: TypeAlias = dict[SupportedModels, float]\nprices: PriceTable = {\"gpt-3.5\": 0.0030, \"gpt-4\": 0.0200}\n\n@dataclass ![1](assets/1.png)\nclass Message:\n    prompt: str\n    response: str | None ![2](assets/2.png)\n    model: SupportedModels\n\n@dataclass\nclass MessageCostReport:\n    req_costs: float\n    res_costs: float\n    total_costs: float\n\n# Define count_tokens function as normal\n...\n\ndef calculate_usage_costs(message: Message) -> MessageCostReport: ![3](assets/3.png)\n    if message.model not in prices :\n        # raise at runtime - in case someone ignores type errors\n        raise ValueError(\n            f\"Cost calculation is not supported for {message.model} model.\"\n        )\n    price = prices[message.model]\n    req_costs = price * count_tokens(message.prompt) / 1000\n    res_costs = price * count_tokens(message.response) / 1000\n    total_costs = req_costs + res_costs\n    return MessageCostReport(\n        req_costs=req_costs, res_costs=res_costs, total_costs=total_costs\n    )\n```", "```py\n$ pip install pydantic\n```", "```py\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass TextModelRequest(BaseModel): ![1](assets/1.png)\n    model: Literal[\"gpt-3.5-turbo\", \"gpt-4o\"]\n    prompt: str\n    temperature: float = 0.0 ![2](assets/2.png)\n```", "```py\n# schemas.py\n\nfrom datetime import datetime\nfrom typing import Annotated, Literal\nfrom pydantic import BaseModel\n\nclass ModelRequest(BaseModel): ![1](assets/1.png)\n    prompt: str\n\nclass ModelResponse(BaseModel): ![2](assets/2.png)\n    request_id: str\n    ip: str | None\n    content: str | None\n    created_at: datetime = datetime.now()\n\nclass TextModelRequest(ModelRequest):\n    model: Literal[\"gpt-3.5-turbo\", \"gpt-4o\"]\n    temperature: float = 0.0\n\nclass TextModelResponse(ModelResponse):\n    tokens: int\n\nImageSize = Annotated[tuple[int, int], \"Width and height of an image in pixels\"]\n\nclass ImageModelRequest(ModelRequest): ![3](assets/3.png)\n    model: Literal[\"tinysd\", \"sd1.5\"]\n    output_size: ImageSize\n    num_inference_steps: int = 200\n\nclass ImageModelResponse(ModelResponse): ![4](assets/4.png)\n    size: ImageSize\n    url: str\n```", "```py\n# schemas.py\n\nfrom datetime import datetime\nfrom typing import Annotated, Literal\nfrom uuid import uuid4\nfrom pydantic import BaseModel, Field, HttpUrl, IPvAnyAddress, PositiveInt\n\nclass ModelRequest(BaseModel):\n    prompt: Annotated[str, Field(min_length=1, max_length=10000)] ![1](assets/1.png)\n\nclass ModelResponse(BaseModel):\n    request_id: Annotated[str, Field(default_factory=lambda: uuid4().hex)] ![2](assets/2.png)\n    # no defaults set for ip field\n    # raise ValidationError if a valid IP address or None is not provided\n    ip: Annotated[str, IPvAnyAddress] | None ![3](assets/3.png)\n    content: Annotated[str | None, Field(min_length=0, max_length=10000)] ![4](assets/4.png)\n    created_at: datetime = datetime.now()\n\nclass TextModelRequest(ModelRequest):\n    model: Literal[\"gpt-3.5-turbo\", \"gpt-4o\"]\n    temperature: Annotated[float, Field(ge=0.0, le=1.0, default=0.0)] ![5](assets/5.png)\n\nclass TextModelResponse(ModelResponse):\n    tokens: Annotated[int, Field(ge=0)]\n\nImageSize = Annotated[ ![6](assets/6.png)\n    tuple[PositiveInt, PositiveInt], \"Width and height of an image in pixels\"\n]\n\nclass ImageModelRequest(ModelRequest):\n    model: Literal[\"tinysd\", \"sd1.5\"]\n    output_size: ImageSize ![6](assets/6.png)\n    num_inference_steps: Annotated[int, Field(ge=0, le=2000)] = 200 ![7](assets/7.png)\n\nclass ImageModelResponse(ModelResponse):\n    size: ImageSize ![6](assets/6.png)\n    url: Annotated[str, HttpUrl] | None = None ![8](assets/8.png)\n```", "```py\n$ curl -X 'POST' \\\n  'http://127.0.0.1:8000/validation/failure' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n \"prompt\": \"string\",\n \"model\": \"gpt-4o\",\n \"temperature\": 0\n}'\n\n{\n  \"detail\": [\n    {\n      \"type\": \"literal_error\",\n      \"loc\": [\n        \"body\",\n        \"model\"\n      ],\n      \"msg\": \"Input should be 'tinyllama' or 'gemma2b'\",\n      \"input\": \"gpt-4o\",\n      \"ctx\": {\n        \"expected\": \"'tinyllama' or 'gemma2b'\"\n      }\n    }\n  ]\n}\n```", "```py\n# schemas.py\n\nfrom typing import Annotated, Literal\nfrom pydantic import (\n    AfterValidator,\n    BaseModel,\n    Field,\n    PositiveInt,\n    validate_call,\n)\n\nImageSize = Annotated[\n    tuple[PositiveInt, PositiveInt], \"Width and height of an image in pixels\"\n]\nSupportedModels = Annotated[\n    Literal[\"tinysd\", \"sd1.5\"], \"Supported Image Generation Models\"\n]\n\n@validate_call ![1](assets/1.png)\ndef is_square_image(value: ImageSize) -> ImageSize: ![2](assets/2.png)\n    if value[0] / value[1] != 1:\n        raise ValueError(\"Only square images are supported\")\n    if value[0] not in [512, 1024]:\n        raise ValueError(f\"Invalid output size: {value} - expected 512 or 1024\")\n    return value\n\n@validate_call ![1](assets/1.png)\ndef is_valid_inference_step(\n    num_inference_steps: int, model: SupportedModels\n) -> int:\n    if model == \"tinysd\" and num_inference_steps > 2000: ![3](assets/3.png)\n        raise ValueError(\n            \"TinySD model cannot have more than 2000 inference steps\"\n        )\n    return num_inference_steps\n\nOutputSize = Annotated[ImageSize, AfterValidator(is_square_image)] ![4](assets/4.png)\nInferenceSteps = Annotated[ ![4](assets/4.png)\n    int,\n    AfterValidator(\n        lambda v, values: is_valid_inference_step(v, values[\"model\"])\n    ),\n]\n\nclass ModelRequest(BaseModel):\n    prompt: Annotated[str, Field(min_length=1, max_length=4000)]\n\nclass ImageModelRequest(ModelRequest):\n    model: SupportedModels\n    output_size: OutputSize ![5](assets/5.png)\n    num_inference_steps: InferenceSteps = 200 ![6](assets/6.png)\n```", "```py\n# schemas.py\n\nfrom typing import Annotated\nfrom pydantic import computed_field, Field\nfrom utils import count_tokens\n\n...\n\nclass TextModelResponse(ModelResponse):\n    model: SupportedModels\n    price: Annotated[float, Field(ge=0, default=0.01)]\n    temperature: Annotated[float, Field(ge=0.0, le=1.0, default=0.0)]\n\n    @property\n    @computed_field\n    def tokens(self) -> int:\n        return count_tokens(self.content)\n\n    @property\n    @computed_field\n    def cost(self) -> float:\n        return self.price * self.tokens\n```", "```py\n>> response = TextModelResponse(content=\"FastAPI Generative AI Service\", ip=None)\n>> response.model_dump(exclude_none=True)\n{'content': 'FastAPI Generative AI Service',\n 'cost': 0.06,\n 'created_at': datetime.datetime(2024, 3, 7, 20, 42, 38, 729410),\n 'price': 0.01,\n 'request_id': 'a3f18d85dcb442baa887a505ae8d2cd7',\n 'tokens': 6}\n\n>> response.model_dump_json(exclude_unset=True)\n'{\"ip\":null,\"content\":\"FastAPI Generative AI Service\",\"tokens\":6,\"cost\":0.06}'\n```", "```py\n$ pip install pydantic-settings\n```", "```py\nAPP_SECRET=asdlkajdlkajdklaslkldjkasldjkasdjaslk\nDATABASE_URL=postgres://sa:password@localhost:5432/cms\nCORS_WHITELIST=[\"https://xyz.azurewebsites.net\",\"http://localhost:3000\"]\n```", "```py\n# settings.py\n\nfrom typing import Annotated\nfrom pydantic import Field, HttpUrl, PostgresDsn\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass AppSettings(BaseSettings): ![1](assets/1.png)\n    model_config = SettingsConfigDict(\n        env_file=\".env\", env_file_encoding=\"utf-8\" ![2](assets/2.png)\n    )\n\n    port: Annotated[int, Field(default=8000)]\n    app_secret: Annotated[str, Field(min_length=32)]\n    pg_dsn: Annotated[\n        PostgresDsn,\n        Field(\n            alias=\"DATABASE_URL\",\n            default=\"postgres://user:pass@localhost:5432/database\",\n        ),\n    ] ![3](assets/3.png)\n    cors_whitelist_domains: Annotated[\n        set[HttpUrl],\n        Field(alias=\"CORS_WHITELIST\", default=[\"http://localhost:3000\"]),\n    ] ![4](assets/4.png)\n\nsettings = AppSettings()\nprint(settings.model_dump()) ![5](assets/5.png)\n\"\"\"\n{'port': 8000\n 'app_secret': 'asdlkajdlkajdklaslkldjkasldjkasdjaslk',\n 'pg_dsn': MultiHostUrl('postgres://sa:password@localhost:5432/cms'),\n 'cors_whitelist_domains': {Url('http://localhost:3000/'),\n                            Url('https://xyz.azurewebsites.net/')},\n}\n\"\"\"\n```", "```py\ntest_settings = AppSettings(_env_file=\"test.env\")\n```", "```py\n# schemas.py\n\nfrom dataclasses import dataclass\nfrom typing import Literal\n\n@dataclass\nclass TextModelRequest: ![1](assets/1.png)\n    model: Literal[\"tinyLlama\", \"gemma2b\"]\n    prompt: str\n    temperature: float\n\n@dataclass\nclass TextModelResponse: ![1](assets/1.png)\n    response: str\n    tokens: int\n\n# main.py\n\nfrom fastapi import Body, FastAPI, HTTPException, status\nfrom models import generate_text, load_text_model\nfrom schemas import TextModelRequest, TextModelResponse\nfrom utils import count_tokens\n\n# load lifespan\n...\n\napp = FastAPI(lifespan=lifespan)\n\n@app.post(\"/generate/text\")\ndef serve_text_to_text_controller(\n    body: TextModelRequest = Body(...),\n) -> TextModelResponse: ![2](assets/2.png) ![4](assets/4.png)\n    if body.model not in [\"tinyLlama\", \"gemma2b\"]: ![3](assets/3.png)\n        raise HTTPException(\n            detail=f\"Model {body.model} is not supported\",\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n    output = generate_text(models[\"text\"], body.prompt, body.temperature)\n    tokens = count_tokens(body.prompt) + count_tokens(output)\n    return TextModelResponse(response=output, tokens=tokens) ![4](assets/4.png)\n```", "```py\n# main.py\n\nfrom fastapi import Body, FastAPI, HTTPException, Request, status\nfrom models import generate_text\nfrom schemas import TextModelRequest, TextModelResponse ![1](assets/1.png)\n\n# load lifespan\n...\n\napp = FastAPI(lifespan=lifespan)\n\n@app.post(\"/generate/text\") ![2](assets/2.png)\ndef serve_text_to_text_controller(\n    request: Request, body: TextModelRequest = Body(...)\n) -> TextModelResponse:\n    if body.model not in [\"tinyLlama\", \"gemma2b\"]: ![3](assets/3.png)\n        raise HTTPException(\n            detail=f\"Model {body.model} is not supported\",\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n    output = generate_text(models[\"text\"], body.prompt, body.temperature)\n    return TextModelResponse(content=output, ip=request.client.host) ![4](assets/4.png)\n```", "```py\nRequest\n\ncurl -X 'POST' \\\n    'http://localhost:8000/generate/text' \\\n    -H 'accept: application/json' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n    \"prompt\": \"What is your name?\",\n    \"model\": \"tinyllama\",\n    \"temperature\": 0.01\n}'\n\nhttp://localhost:8000/generate/text\n\n>> Response body\n{\n    \"request_id\": \"7541204d5c684f429fe43ccf360fÐ—3dc\",\n    \"ip\": \"127.0.0.1\"\n    \"content\": \"I am not a person. However, I can provide you with information\n        about my name. My name is fastapi bot.\",\n    \"created_at\": \"2024-03-07T16:06:57.492039\",\n    \"price\": 0.01,\n    \"tokens\": 25,\n    \"cost\": 0.25\n}\n\n>> Response headers\n\ncontent-length: 259\ncontent-type: application/json\ndate: Thu, 07 Mar 2024 16:07:01 GMT\nserver: uvicorn\nx-response-time: 22.9243\n```"]