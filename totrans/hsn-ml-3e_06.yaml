- en: Chapter 5\. Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *support vector machine* (SVM) is a powerful and versatile machine learning
    model, capable of performing linear or nonlinear classification, regression, and
    even novelty detection. SVMs shine with small to medium-sized nonlinear datasets
    (i.e., hundreds to thousands of instances), especially for classification tasks.
    However, they don’t scale very well to very large datasets, as you will see.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explain the core concepts of SVMs, how to use them, and how
    they work. Let’s jump right in!
  prefs: []
  type: TYPE_NORMAL
- en: Linear SVM Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fundamental idea behind SVMs is best explained with some visuals. [Figure 5-1](#large_margin_classification_plot)
    shows part of the iris dataset that was introduced at the end of [Chapter 4](ch04.html#linear_models_chapter).
    The two classes can clearly be separated easily with a straight line (they are
    *linearly separable*). The left plot shows the decision boundaries of three possible
    linear classifiers. The model whose decision boundary is represented by the dashed
    line is so bad that it does not even separate the classes properly. The other
    two models work perfectly on this training set, but their decision boundaries
    come so close to the instances that these models will probably not perform as
    well on new instances. In contrast, the solid line in the plot on the right represents
    the decision boundary of an SVM classifier; this line not only separates the two
    classes but also stays as far away from the closest training instances as possible.
    You can think of an SVM classifier as fitting the widest possible street (represented
    by the parallel dashed lines) between the classes. This is called *large margin
    classification*.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0501](assets/mls3_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Large margin classification
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice that adding more training instances “off the street” will not affect
    the decision boundary at all: it is fully determined (or “supported”) by the instances
    located on the edge of the street. These instances are called the *support vectors*
    (they are circled in [Figure 5-1](#large_margin_classification_plot)).'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SVMs are sensitive to the feature scales, as you can see in [Figure 5-2](#sensitivity_to_feature_scales_plot).
    In the left plot, the vertical scale is much larger than the horizontal scale,
    so the widest possible street is close to horizontal. After feature scaling (e.g.,
    using Scikit-Learn’s `StandardScaler`), the decision boundary in the right plot
    looks much better.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0502](assets/mls3_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Sensitivity to feature scales
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Soft Margin Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we strictly impose that all instances must be off the street and on the
    correct side, this is called *hard margin classification*. There are two main
    issues with hard margin classification. First, it only works if the data is linearly
    separable. Second, it is sensitive to outliers. [Figure 5-3](#sensitivity_to_outliers_plot)
    shows the iris dataset with just one additional outlier: on the left, it is impossible
    to find a hard margin; on the right, the decision boundary ends up very different
    from the one we saw in [Figure 5-1](#large_margin_classification_plot) without
    the outlier, and the model will probably not generalize as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0503](assets/mls3_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Hard margin sensitivity to outliers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To avoid these issues, we need to use a more flexible model. The objective is
    to find a good balance between keeping the street as large as possible and limiting
    the *margin violations* (i.e., instances that end up in the middle of the street
    or even on the wrong side). This is called *soft margin classification*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters,
    including the regularization hyperparameter `C`. If you set it to a low value,
    then you end up with the model on the left of [Figure 5-4](#regularization_plot).
    With a high value, you get the model on the right. As you can see, reducing `C`
    makes the street larger, but it also leads to more margin violations. In other
    words, reducing `C` results in more instances supporting the street, so there’s
    less risk of overfitting. But if you reduce it too much, then the model ends up
    underfitting, as seems to be the case here: the model with `C=100` looks like
    it will generalize better than the one with `C=1`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0504](assets/mls3_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Large margin (left) versus fewer margin violations (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your SVM model is overfitting, you can try regularizing it by reducing `C`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Scikit-Learn code loads the iris dataset and trains a linear
    SVM classifier to detect *Iris virginica* flowers. The pipeline first scales the
    features, then uses a `LinearSVC` with `C=1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The resulting model is represented on the left in [Figure 5-4](#regularization_plot).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, as usual, you can use the model to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The first plant is classified as an *Iris virginica*, while the second is not.
    Let’s look at the scores that the SVM used to make these predictions. These measure
    the signed distance between each instance and the decision boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Unlike `LogisticRegression`, `LinearSVC` doesn’t have a `predict_proba()` method
    to estimate the class probabilities. That said, if you use the `SVC` class (discussed
    shortly) instead of `LinearSVC`, and if you set its `probability` hyperparameter
    to `True`, then the model will fit an extra model at the end of training to map
    the SVM decision function scores to estimated probabilities. Under the hood, this
    requires using 5-fold cross-validation to generate out-of-sample predictions for
    every instance in the training set, then training a `LogisticRegression` model,
    so it will slow down training considerably. After that, the `predict_proba()`
    and `predict_log_proba()` methods will be available.
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear SVM Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although linear SVM classifiers are efficient and often work surprisingly well,
    many datasets are not even close to being linearly separable. One approach to
    handling nonlinear datasets is to add more features, such as polynomial features
    (as we did in [Chapter 4](ch04.html#linear_models_chapter)); in some cases this
    can result in a linearly separable dataset. Consider the lefthand plot in [Figure 5-5](#higher_dimensions_plot):
    it represents a simple dataset with just one feature, *x*[1]. This dataset is
    not linearly separable, as you can see. But if you add a second feature *x*[2]
    = (*x*[1])², the resulting 2D dataset is perfectly linearly separable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0505](assets/mls3_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Adding features to make a dataset linearly separable
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To implement this idea using Scikit-Learn, you can create a pipeline containing
    a `PolynomialFeatures` transformer (discussed in [“Polynomial Regression”](ch04.html#polynomial_regression)),
    followed by a `StandardScaler` and a `LinearSVC` classifier. Let’s test this on
    the moons dataset, a toy dataset for binary classification in which the data points
    are shaped as two interleaving crescent moons (see [Figure 5-6](#moons_polynomial_svc_plot)).
    You can generate this dataset using the `make_moons()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0506](assets/mls3_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Linear SVM classifier using polynomial features
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Polynomial Kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adding polynomial features is simple to implement and can work great with all
    sorts of machine learning algorithms (not just SVMs). That said, at a low polynomial
    degree this method cannot deal with very complex datasets, and with a high polynomial
    degree it creates a huge number of features, making the model too slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, when using SVMs you can apply an almost miraculous mathematical
    technique called the *kernel trick* (which is explained later in this chapter).
    The kernel trick makes it possible to get the same result as if you had added
    many polynomial features, even with a very high degree, without actually having
    to add them. This means there’s no combinatorial explosion of the number of features.
    This trick is implemented by the `SVC` class. Let’s test it on the moons dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code trains an SVM classifier using a third-degree polynomial kernel, represented
    on the left in [Figure 5-7](#moons_kernelized_polynomial_svc_plot). On the right
    is another SVM classifier using a 10th-degree polynomial kernel. Obviously, if
    your model is overfitting, you might want to reduce the polynomial degree. Conversely,
    if it is underfitting, you can try increasing it. The hyperparameter `coef0` controls
    how much the model is influenced by high-degree terms versus low-degree terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0507](assets/mls3_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. SVM classifiers with a polynomial kernel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Although hyperparameters will generally be tuned automatically (e.g., using
    randomized search), it’s good to have a sense of what each hyperparameter actually
    does and how it may interact with other hyperparameters: this way, you can narrow
    the search to a much smaller space.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarity Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another technique to tackle nonlinear problems is to add features computed using
    a similarity function, which measures how much each instance resembles a particular
    *landmark*, as we did in [Chapter 2](ch02.html#project_chapter) when we added
    the geographic similarity features. For example, let’s take the 1D dataset from
    earlier and add two landmarks to it at *x*[1] = –2 and *x*[1] = 1 (see the left
    plot in [Figure 5-8](#kernel_method_plot)). Next, we’ll define the similarity
    function to be the Gaussian RBF with *γ* = 0.3\. This is a bell-shaped function
    varying from 0 (very far away from the landmark) to 1 (at the landmark).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to compute the new features. For example, let’s look at the
    instance *x*[1] = –1: it is located at a distance of 1 from the first landmark
    and 2 from the second landmark. Therefore, its new features are *x*[2] = exp(–0.3
    × 1²) ≈ 0.74 and *x*[3] = exp(–0.3 × 2²) ≈ 0.30\. The plot on the right in [Figure 5-8](#kernel_method_plot)
    shows the transformed dataset (dropping the original features). As you can see,
    it is now linearly separable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0508](assets/mls3_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Similarity features using the Gaussian RBF
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You may wonder how to select the landmarks. The simplest approach is to create
    a landmark at the location of each and every instance in the dataset. Doing that
    creates many dimensions and thus increases the chances that the transformed training
    set will be linearly separable. The downside is that a training set with *m* instances
    and *n* features gets transformed into a training set with *m* instances and *m*
    features (assuming you drop the original features). If your training set is very
    large, you end up with an equally large number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian RBF Kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like the polynomial features method, the similarity features method can
    be useful with any machine learning algorithm, but it may be computationally expensive
    to compute all the additional features (especially on large training sets). Once
    again the kernel trick does its SVM magic, making it possible to obtain a similar
    result as if you had added many similarity features, but without actually doing
    so. Let’s try the `SVC` class with the Gaussian RBF kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This model is represented at the bottom left in [Figure 5-9](#moons_rbf_svc_plot).
    The other plots show models trained with different values of hyperparameters `gamma`
    (*γ*) and `C`. Increasing `gamma` makes the bell-shaped curve narrower (see the
    lefthand plots in [Figure 5-8](#kernel_method_plot)). As a result, each instance’s
    range of influence is smaller: the decision boundary ends up being more irregular,
    wiggling around individual instances. Conversely, a small `gamma` value makes
    the bell-shaped curve wider: instances have a larger range of influence, and the
    decision boundary ends up smoother. So *γ* acts like a regularization hyperparameter:
    if your model is overfitting, you should reduce *γ*; if it is underfitting, you
    should increase *γ* (similar to the `C` hyperparameter).'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0509](assets/mls3_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. SVM classifiers using an RBF kernel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other kernels exist but are used much more rarely. Some kernels are specialized
    for specific data structures. *String kernels* are sometimes used when classifying
    text documents or DNA sequences (e.g., using the string subsequence kernel or
    kernels based on the Levenshtein distance).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With so many kernels to choose from, how can you decide which one to use? As
    a rule of thumb, you should always try the linear kernel first. The `LinearSVC`
    class is much faster than `SVC(kernel="linear")`, especially if the training set
    is very large. If it is not too large, you should also try kernelized SVMs, starting
    with the Gaussian RBF kernel; it often works really well. Then, if you have spare
    time and computing power, you can experiment with a few other kernels using hyperparameter
    search. If there are kernels specialized for your training set’s data structure,
    make sure to give them a try too.
  prefs: []
  type: TYPE_NORMAL
- en: SVM Classes and Computational Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `LinearSVC` class is based on the `liblinear` library, which implements
    an [optimized algorithm](https://homl.info/13) for linear SVMs.⁠^([1](ch05.html#idm45720213459744))
    It does not support the kernel trick, but it scales almost linearly with the number
    of training instances and the number of features. Its training time complexity
    is roughly *O*(*m* × *n*). The algorithm takes longer if you require very high
    precision. This is controlled by the tolerance hyperparameter *ϵ* (called `tol`
    in Scikit-Learn). In most classification tasks, the default tolerance is fine.
  prefs: []
  type: TYPE_NORMAL
- en: The `SVC` class is based on the `libsvm` library, which implements an [algorithm
    that supports the kernel trick](https://homl.info/14).⁠^([2](ch05.html#idm45720213447328))
    The training time complexity is usually between *O*(*m*² × *n*) and *O*(*m*³ ×
    *n*). Unfortunately, this means that it gets dreadfully slow when the number of
    training instances gets large (e.g., hundreds of thousands of instances), so this
    algorithm is best for small or medium-sized nonlinear training sets. It scales
    well with the number of features, especially with sparse features (i.e., when
    each instance has few nonzero features). In this case, the algorithm scales roughly
    with the average number of nonzero features per instance.
  prefs: []
  type: TYPE_NORMAL
- en: The `SGDClassifier` class also performs large margin classification by default,
    and its hyperparameters–especially the regularization hyperparameters (`alpha`
    and `penalty`) and the `learning_rate`–can be adjusted to produce similar results
    as the linear SVMs. For training it uses stochastic gradient descent (see [Chapter 4](ch04.html#linear_models_chapter)),
    which allows incremental learning and uses little memory, so you can use it to
    train a model on a large dataset that does not fit in RAM (i.e., for out-of-core
    learning). Moreover, it scales very well, as its computational complexity is *O*(*m*
    × *n*). [Table 5-1](#svm_classification_algorithm_comparison) compares Scikit-Learn’s
    SVM classification classes.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. Comparison of Scikit-Learn classes for SVM classification
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Time complexity | Out-of-core support | Scaling required | Kernel
    trick |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `LinearSVC` | *O*(*m* × *n*) | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| `SVC` | *O*(*m*² × *n*) to *O*(*m*³ × *n*) | No | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| `SGDClassifier` | *O*(*m* × *n*) | Yes | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: Now let’s see how the SVM algorithms can also be used for linear and nonlinear
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: SVM Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use SVMs for regression instead of classification, the trick is to tweak
    the objective: instead of trying to fit the largest possible street between two
    classes while limiting margin violations, SVM regression tries to fit as many
    instances as possible *on* the street while limiting margin violations (i.e.,
    instances *off* the street). The width of the street is controlled by a hyperparameter,
    *ϵ*. [Figure 5-10](#svm_regression_plot) shows two linear SVM regression models
    trained on some linear data, one with a small margin (*ϵ* = 0.5) and the other
    with a larger margin (*ϵ* = 1.2).'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0510](assets/mls3_0510.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. SVM regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reducing *ϵ* increases the number of support vectors, which regularizes the
    model. Moreover, if you add more training instances within the margin, it will
    not affect the model’s predictions; thus, the model is said to be *ϵ-insensitive*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use Scikit-Learn’s `LinearSVR` class to perform linear SVM regression.
    The following code produces the model represented on the left in [Figure 5-10](#svm_regression_plot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To tackle nonlinear regression tasks, you can use a kernelized SVM model. [Figure 5-11](#svm_with_polynomial_kernel_plot)
    shows SVM regression on a random quadratic training set, using a second-degree
    polynomial kernel. There is some regularization in the left plot (i.e., a small
    `C` value), and much less in the right plot (i.e., a large `C` value).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0511](assets/mls3_0511.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-11\. SVM regression using a second-degree polynomial kernel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code uses Scikit-Learn’s `SVR` class (which supports the kernel
    trick) to produce the model represented on the left in [Figure 5-11](#svm_with_polynomial_kernel_plot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `SVR` class is the regression equivalent of the `SVC` class, and the `LinearSVR`
    class is the regression equivalent of the `LinearSVC` class. The `LinearSVR` class
    scales linearly with the size of the training set (just like the `LinearSVC` class),
    while the `SVR` class gets much too slow when the training set grows very large
    (just like the `SVC` class).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SVMs can also be used for novelty detection, as you will see in [Chapter 9](ch09.html#unsupervised_learning_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this chapter explains how SVMs make predictions and how their training
    algorithms work, starting with linear SVM classifiers. If you are just getting
    started with machine learning, you can safely skip this and go straight to the
    exercises at the end of this chapter, and come back later when you want to get
    a deeper understanding of SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: Under the Hood of Linear SVM Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A linear SVM classifier predicts the class of a new instance **x** by first
    computing the decision function **θ**^⊺ **x** = *θ*[0] *x*[0] + ⋯ + *θ*[*n*] *x*[*n*],
    where *x*[0] is the bias feature (always equal to 1). If the result is positive,
    then the predicted class *ŷ* is the positive class (1); otherwise it is the negative
    class (0). This is exactly like `LogisticRegression` (discussed in [Chapter 4](ch04.html#linear_models_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Up to now, I have used the convention of putting all the model parameters in
    one vector **θ**, including the bias term **θ**[0] and the input feature weights
    **θ**[1] to **θ**[*n*]. This required adding a bias input *x*[0] = 1 to all instances.
    Another very common convention is to separate the bias term *b* (equal to **θ**[0])
    and the feature weights vector **w** (containing **θ**[1] to **θ**[*n*]). In this
    case, no bias feature needs to be added to the input feature vectors, and the
    linear SVM’s decision function is equal to **w**^⊺ **x** + *b* = *w*[1] *x*[1]
    + ⋯ + *w*[*n*] *x*[*n*] + *b*. I will use this convention throughout the rest
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, making predictions with a linear SVM classifier is quite straightforward.
    How about training? This requires finding the weights vector **w** and the bias
    term *b* that make the street, or margin, as wide as possible while limiting the
    number of margin violations. Let’s start with the width of the street: to make
    it larger, we need to make **w** smaller. This may be easier to visualize in 2D,
    as shown in [Figure 5-12](#small_w_large_margin_plot). Let’s define the borders
    of the street as the points where the decision function is equal to –1 or +1\.
    In the left plot the weight *w[1]* is 1, so the points at which *w*[1] *x*[1]
    = –1 or +1 are *x*[1] = –1 and +1: therefore the margin’s size is 2\. In the right
    plot the weight is 0.5, so the points at which *w*[1] *x*[1] = –1 or +1 are *x*[1]
    = –2 and +2: the margin’s size is 4\. So, we need to keep **w** as small as possible.
    Note that the bias term *b* has no influence on the size of the margin: tweaking
    it just shifts the margin around, without affecting its size.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0512](assets/mls3_0512.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-12\. A smaller weight vector results in a larger margin
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We also want to avoid margin violations, so we need the decision function to
    be greater than 1 for all positive training instances and lower than –1 for negative
    training instances. If we define *t*^((*i*)) = –1 for negative instances (when
    *y*^((*i*)) = 0) and *t*^((*i*)) = 1 for positive instances (when *y*^((*i*))
    = 1), then we can write this constraint as *t*^((*i*))(**w**^⊺ **x**^((*i*)) +
    *b*) ≥ 1 for all instances.
  prefs: []
  type: TYPE_NORMAL
- en: We can therefore express the hard margin linear SVM classifier objective as
    the constrained optimization problem in [Equation 5-1](#hard_margin_objective).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-1\. Hard margin linear SVM classifier objective
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><munder><mo
    form="prefix">minimize</mo> <mrow><mi mathvariant="bold">w</mi><mo>,</mo><mi>b</mi></mrow></munder>
    <mrow><mfrac><mn>1</mn> <mn>2</mn></mfrac> <msup><mi mathvariant="bold">w</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">w</mi></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mtext>subject</mtext> <mtext>to</mtext> <msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">w</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mn>1</mn> <mtext>for</mtext> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>m</mi></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We are minimizing ½ **w**^⊺ **w**, which is equal to ½∥ **w** ∥², rather than
    minimizing ∥ **w** ∥ (the norm of **w**). Indeed, ½∥ **w** ∥² has a nice, simple
    derivative (it is just **w**), while ∥ **w** ∥ is not differentiable at **w**
    = 0\. Optimization algorithms often work much better on differentiable functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the soft margin objective, we need to introduce a *slack variable* *ζ*^((*i*))
    ≥ 0 for each instance:⁠^([3](ch05.html#idm45720213138176)) *ζ*^((*i*)) measures
    how much the *i*^(th) instance is allowed to violate the margin. We now have two
    conflicting objectives: make the slack variables as small as possible to reduce
    the margin violations, and make ½ **w**^⊺ **w** as small as possible to increase
    the margin. This is where the `C` hyperparameter comes in: it allows us to define
    the trade-off between these two objectives. This gives us the constrained optimization
    problem in [Equation 5-2](#soft_margin_objective).'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-2\. Soft margin linear SVM classifier objective
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><munder><mo
    form="prefix">minimize</mo> <mrow><mi mathvariant="bold">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>ζ</mi></mrow></munder>
    <mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mn>2</mn></mfrac></mstyle>
    <msup><mi mathvariant="bold">w</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">w</mi>
    <mo>+</mo> <mi>C</mi> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mtext>subject</mtext> <mtext>to</mtext> <msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">w</mi> <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mn>1</mn> <mo>-</mo> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mtext>and</mtext> <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>≥</mo> <mn>0</mn> <mtext>for</mtext> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>m</mi></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: The hard margin and soft margin problems are both convex quadratic optimization
    problems with linear constraints. Such problems are known as *quadratic programming*
    (QP) problems. Many off-the-shelf solvers are available to solve QP problems by
    using a variety of techniques that are outside the scope of this book.⁠^([4](ch05.html#idm45720213085920))
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a QP solver is one way to train an SVM. Another is to use gradient descent
    to minimize the *hinge loss* or the *squared hinge loss* (see [Figure 5-13](#hinge_plot)).
    Given an instance **x** of the positive class (i.e., with *t* = 1), the loss is
    0 if the output *s* of the decision function (*s* = **w**^⊺ **x** + *b*) is greater
    than or equal to 1\. This happens when the instance is off the street and on the
    positive side. Given an instance of the negative class (i.e., with *t* = –1),
    the loss is 0 if *s* ≤ –1\. This happens when the instance is off the street and
    on the negative side. The further away an instance is from the correct side of
    the margin, the higher the loss: it grows linearly for the hinge loss, and quadratically
    for the squared hinge loss. This makes the squared hinge loss more sensitive to
    outliers. However, if the dataset is clean, it tends to converge faster. By default,
    `LinearSVC` uses the squared hinge loss, while `SGDClassifier` uses the hinge
    loss. Both classes let you choose the loss by setting the `loss` hyperparameter
    to `"hinge"` or `"squared_hinge"`. The `SVC` class’s optimization algorithm finds
    a similar solution as minimizing the hinge loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0513](assets/mls3_0513.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-13\. The hinge loss (left) and the squared hinge loss (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we’ll look at yet another way to train a linear SVM classifier: solving
    the dual problem.'
  prefs: []
  type: TYPE_NORMAL
- en: The Dual Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a constrained optimization problem, known as the *primal problem*, it
    is possible to express a different but closely related problem, called its *dual
    problem*. The solution to the dual problem typically gives a lower bound to the
    solution of the primal problem, but under some conditions it can have the same
    solution as the primal problem. Luckily, the SVM problem happens to meet these
    conditions,⁠^([5](ch05.html#idm45720213055696)) so you can choose to solve the
    primal problem or the dual problem; both will have the same solution. [Equation
    5-3](#svm_dual_form) shows the dual form of the linear SVM objective. If you are
    interested in knowing how to derive the dual problem from the primal problem,
    see the extra material section in [this chapter’s notebook](https://homl.info/colab3).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-3\. Dual form of the linear SVM objective
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><munder><mtext>minimize </mtext><mi mathvariant="bold">α</mi></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>α</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><msup><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>⊺</mo></msup><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>-</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mtext>subject to </mtext><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>≥</mo><mn>0</mn><mtext> for all </mtext><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>m</mi><mtext> and </mtext><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><mn>0</mn></math>
  prefs: []
  type: TYPE_NORMAL
- en: Once you find the vector <math><mover accent="true"><mi mathvariant="bold">α</mi>
    <mo>^</mo></mover></math> that minimizes this equation (using a QP solver), use
    [Equation 5-4](#from_alpha_to_w_and_b) to compute the <math><mover accent="true"><mi
    mathvariant="bold">w</mi> <mo>^</mo></mover></math> and <math><mover accent="true"><mi>b</mi><mo>^</mo></mover></math>
    that minimize the primal problem. In this equation, *n*[*s*] represents the number
    of support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-4\. From the dual solution to the primal solution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msup><mrow><mover
    accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <mrow><msup><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></mrow> <mo>⊺</mo></msup>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfenced></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: The dual problem is faster to solve than the primal one when the number of training
    instances is smaller than the number of features. More importantly, the dual problem
    makes the kernel trick possible, while the primal problem does not. So what is
    this kernel trick, anyway?
  prefs: []
  type: TYPE_NORMAL
- en: Kernelized SVMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you want to apply a second-degree polynomial transformation to a two-dimensional
    training set (such as the moons training set), then train a linear SVM classifier
    on the transformed training set. [Equation 5-5](#example_second_degree_polynomial_mapping)
    shows the second-degree polynomial mapping function *ϕ* that you want to apply.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-5\. Second-degree polynomial mapping
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>ϕ</mi> <mfenced open="(" close=")"><mi mathvariant="bold">x</mi></mfenced>
    <mo>=</mo> <mi>ϕ</mi> <mfenced open="(" close=")"><mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></mfenced>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>x</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt>
    <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub></mrow></mtd></mtr>
    <mtr><mtd><msup><mrow><msub><mi>x</mi> <mn>2</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the transformed vector is 3D instead of 2D. Now let’s look at what
    happens to a couple of 2D vectors, **a** and **b**, if we apply this second-degree
    polynomial mapping and then compute the dot product⁠^([6](ch05.html#idm45720212940064))
    of the transformed vectors (see [Equation 5-6](#kernel_trick_for_second_degree_polynomial_mapping)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-6\. Kernel trick for a second-degree polynomial mapping
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>ϕ</mi>
    <msup><mrow><mo>(</mo><mi mathvariant="bold">a</mi><mo>)</mo></mrow> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>a</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt><msub><mi>a</mi>
    <mn>1</mn></msub> <msub><mi>a</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><msup><mrow><msub><mi>a</mi>
    <mn>2</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr></mtable></mfenced> <mo>⊺</mo></msup>
    <mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>b</mi> <mn>1</mn></msub></mrow>
    <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mrow><msqrt><mn>2</mn></msqrt> <msub><mi>b</mi>
    <mn>1</mn></msub> <msub><mi>b</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><msup><mrow><msub><mi>b</mi>
    <mn>2</mn></msub></mrow> <mn>2</mn></msup></mtd></mtr></mtable></mfenced> <mo>=</mo>
    <msup><mrow><msub><mi>a</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup> <msup><mrow><msub><mi>b</mi>
    <mn>1</mn></msub></mrow> <mn>2</mn></msup> <mo>+</mo> <mn>2</mn> <msub><mi>a</mi>
    <mn>1</mn></msub> <msub><mi>b</mi> <mn>1</mn></msub> <msub><mi>a</mi> <mn>2</mn></msub>
    <msub><mi>b</mi> <mn>2</mn></msub> <mo>+</mo> <msup><mrow><msub><mi>a</mi> <mn>2</mn></msub></mrow>
    <mn>2</mn></msup> <msup><mrow><msub><mi>b</mi> <mn>2</mn></msub></mrow> <mn>2</mn></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mfenced separators="" open="("
    close=")"><msub><mi>a</mi> <mn>1</mn></msub> <msub><mi>b</mi> <mn>1</mn></msub>
    <mo>+</mo><msub><mi>a</mi> <mn>2</mn></msub> <msub><mi>b</mi> <mn>2</mn></msub></mfenced>
    <mn>2</mn></msup> <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><msup><mfenced
    open="(" close=")"><mtable><mtr><mtd><msub><mi>a</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>a</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced> <mo>⊺</mo></msup>
    <mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>b</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>b</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></mfenced>
    <mn>2</mn></msup> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">a</mi>
    <mo>⊺</mo></msup> <mi mathvariant="bold">b</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'How about that? The dot product of the transformed vectors is equal to the
    square of the dot product of the original vectors: *ϕ*(**a**)^⊺ *ϕ*(**b**) = (**a**^⊺
    **b**)².'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the key insight: if you apply the transformation *ϕ* to all training
    instances, then the dual problem (see [Equation 5-3](#svm_dual_form)) will contain
    the dot product *ϕ*(**x**^((*i*)))^⊺ *ϕ*(**x**^((*j*))). But if *ϕ* is the second-degree
    polynomial transformation defined in [Equation 5-5](#example_second_degree_polynomial_mapping),
    then you can replace this dot product of transformed vectors simply by <math><msup><mrow><mo>(</mo><msup><mrow><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow>
    <mo>⊺</mo></msup> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mn>2</mn></msup></math> . So, you don’t need to transform the
    training instances at all; just replace the dot product by its square in [Equation
    5-3](#svm_dual_form). The result will be strictly the same as if you had gone
    through the trouble of transforming the training set and then fitting a linear
    SVM algorithm, but this trick makes the whole process much more computationally
    efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: The function *K*(**a**, **b**) = (**a**^⊺ **b**)² is a second-degree polynomial
    kernel. In machine learning, a *kernel* is a function capable of computing the
    dot product *ϕ*(**a**)^⊺ *ϕ*(**b**), based only on the original vectors **a**
    and **b**, without having to compute (or even to know about) the transformation
    *ϕ*. [Equation 5-7](#common_kernels) lists some of the most commonly used kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-7\. Common kernels
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mtext>Linear:</mtext></mtd>
    <mtd columnalign="left"><mrow><mi>K</mi> <mrow><mo>(</mo> <mi mathvariant="bold">a</mi>
    <mo>,</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi
    mathvariant="bold">a</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">b</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mtext>Polynomial:</mtext></mtd> <mtd columnalign="left"><mrow><mi>K</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><mi>γ</mi><msup><mi
    mathvariant="bold">a</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">b</mi><mo>+</mo><mi>r</mi></mfenced>
    <mi>d</mi></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>Gaussian</mtext>
    <mtext>RBF:</mtext></mrow></mtd> <mtd columnalign="left"><mrow><mi>K</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mrow><mo>-</mo> <mi>γ</mi> <msup><mfenced separators="" open="∥"
    close="∥"><mi mathvariant="bold">a</mi><mo>-</mo><mi mathvariant="bold">b</mi></mfenced>
    <mn>2</mn></msup></mrow></mstyle> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mtext>Sigmoid:</mtext></mtd> <mtd columnalign="left"><mrow><mi>K</mi>
    <mrow><mo>(</mo> <mi mathvariant="bold">a</mi> <mo>,</mo> <mi mathvariant="bold">b</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mo form="prefix">tanh</mo> <mfenced separators=""
    open="(" close=")"><mi>γ</mi> <msup><mi mathvariant="bold">a</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">b</mi> <mo>+</mo> <mi>r</mi></mfenced></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: There is still one loose end we must tie up. [Equation 5-4](#from_alpha_to_w_and_b)
    shows how to go from the dual solution to the primal solution in the case of a
    linear SVM classifier. But if you apply the kernel trick, you end up with equations
    that include *ϕ*(*x*^((*i*))). In fact, <math><mover accent="true"><mi mathvariant="bold">w</mi>
    <mo>^</mo></mover></math> must have the same number of dimensions as *ϕ*(*x*^((*i*))),
    which may be huge or even infinite, so you can’t compute it. But how can you make
    predictions without knowing <math><mover accent="true"><mi mathvariant="bold">w</mi>
    <mo>^</mo></mover></math> ? Well, the good news is that you can plug the formula
    for <math><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></math>
    from [Equation 5-4](#from_alpha_to_w_and_b) into the decision function for a new
    instance **x**^((*n*)), and you get an equation with only dot products between
    input vectors. This makes it possible to use the kernel trick ([Equation 5-8](#making_predictions_with_a_kernelized_svm)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-8\. Making predictions with a kernelized SVM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>h</mi>
    <mrow><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover><mo>,</mo><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></msub> <mfenced separators=""
    open="(" close=")"><mi>ϕ</mi> <mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mfenced></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mfenced separators="" open="(" close=")"><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msup><mrow><mover
    accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mi>ϕ</mi><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mfenced>
    <mo>⊺</mo></msup> <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>+</mo>
    <mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mfenced separators="" open="(" close=")"><mi>ϕ</mi> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <mo>⊺</mo></msup> <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mfenced>
    <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mi>K</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>,</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that since *α*^((*i*)) ≠ 0 only for support vectors, making predictions
    involves computing the dot product of the new input vector **x**^((*n*)) with
    only the support vectors, not all the training instances. Of course, you need
    to use the same trick to compute the bias term <math><mover><mi>b</mi><mo>^</mo></mover></math>
    ([Equation 5-9](#bias_term_using_the_kernel_trick)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5-9\. Using the kernel trick to compute the bias term
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <msup><mrow><mover
    accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></mrow> <mo>⊺</mo></msup>
    <mi>ϕ</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced> <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn>
    <msub><mi>n</mi> <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac
    linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <msup><mrow><mfenced
    separators="" open="(" close=")"><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mi>ϕ</mi><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced></mrow> <mo>⊺</mo></msup> <mi>ϕ</mi> <mrow><mo>(</mo>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mfenced></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <msub><mi>n</mi>
    <mi>s</mi></msub></mfrac></mstyle> <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mfenced separators="" open="(" close=")"><msup><mi>t</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo> <munderover><mo>∑</mo>
    <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <mo>></mo><mn>0</mn></mrow></mstyle></mfrac>
    <mi>m</mi></munderover> <mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <msup><mi>t</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mi>K</mi> <mrow><mo>(</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>,</mo> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></mfenced></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are starting to get a headache, that’s perfectly normal: it’s an unfortunate
    side effect of the kernel trick.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is also possible to implement online kernelized SVMs, capable of incremental
    learning, as described in the papers [“Incremental and Decremental Support Vector
    Machine Learning”](https://homl.info/17)⁠^([7](ch05.html#idm45720212562608)) and
    [“Fast Kernel Classifiers with Online and Active Learning”](https://homl.info/18).⁠^([8](ch05.html#idm45720212560496))
    These kernelized SVMs are implemented in Matlab and C++. But for large-scale nonlinear
    problems, you may want to consider using random forests (see [Chapter 7](ch07.html#ensembles_chapter))
    or neural networks (see [Part II](part02.html#neural_nets_part)).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the fundamental idea behind support vector machines?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a support vector?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it important to scale the inputs when using SVMs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can an SVM classifier output a confidence score when it classifies an instance?
    What about a probability?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you choose between `LinearSVC`, `SVC`, and `SGDClassifier`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit
    the training set. Should you increase or decrease *γ* (`gamma`)? What about `C`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does it mean for a model to be *ϵ-insensitive*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the point of using the kernel trick?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a `LinearSVC` on a linearly separable dataset. Then train an `SVC` and
    a `SGDClassifier` on the same dataset. See if you can get them to produce roughly
    the same model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train an SVM classifier on the wine dataset, which you can load using `sklearn.datasets.load_wine()`.
    This dataset contains the chemical analyses of 178 wine samples produced by 3
    different cultivators: the goal is to train a classification model capable of
    predicting the cultivator based on the wine’s chemical analysis. Since SVM classifiers
    are binary classifiers, you will need to use one-versus-all to classify all three
    classes. What accuracy can you reach?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and fine-tune an SVM regressor on the California housing dataset. You
    can use the original dataset rather than the tweaked version we used in [Chapter 2](ch02.html#project_chapter),
    which you can load using `sklearn.datasets.fetch_california_housing()`. The targets
    represent hundreds of thousands of dollars. Since there are over 20,000 instances,
    SVMs can be slow, so for hyperparameter tuning you should use far fewer instances
    (e.g., 2,000) to test many more hyperparameter combinations. What is your best
    model’s RMSE?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch05.html#idm45720213459744-marker)) Chih-Jen Lin et al., “A Dual Coordinate
    Descent Method for Large-Scale Linear SVM”, *Proceedings of the 25th International
    Conference on Machine Learning* (2008): 408–415.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch05.html#idm45720213447328-marker)) John Platt, “Sequential Minimal
    Optimization: A Fast Algorithm for Training Support Vector Machines” (Microsoft
    Research technical report, April 21, 1998).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.html#idm45720213138176-marker)) Zeta (*ζ*) is the sixth letter of
    the Greek alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.html#idm45720213085920-marker)) To learn more about quadratic programming,
    you can start by reading Stephen Boyd and Lieven Vandenberghe’s book [*Convex
    Optimization*](https://homl.info/15) (Cambridge University Press) or watching
    Richard Brown’s [series of video lectures](https://homl.info/16).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.html#idm45720213055696-marker)) The objective function is convex,
    and the inequality constraints are continuously differentiable and convex functions.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.html#idm45720212940064-marker)) As explained in [Chapter 4](ch04.html#linear_models_chapter),
    the dot product of two vectors **a** and **b** is normally noted **a** · **b**.
    However, in machine learning, vectors are frequently represented as column vectors
    (i.e., single-column matrices), so the dot product is achieved by computing **a**^⊺**b**.
    To remain consistent with the rest of the book, we will use this notation here,
    ignoring the fact that this technically results in a single-cell matrix rather
    than a scalar value.
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch05.html#idm45720212562608-marker)) Gert Cauwenberghs and Tomaso Poggio,
    “Incremental and Decremental Support Vector Machine Learning”, *Proceedings of
    the 13th International Conference on Neural Information Processing Systems* (2000):
    388–394.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch05.html#idm45720212560496-marker)) Antoine Bordes et al., “Fast Kernel
    Classifiers with Online and Active Learning”, *Journal of Machine Learning Research*
    6 (2005): 1579–1619.'
  prefs: []
  type: TYPE_NORMAL
