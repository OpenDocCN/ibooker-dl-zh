["```py\noutput = sum([x*w for x,w in zip(inputs,weights)]) + bias\n```", "```py\ndef relu(x): return x if x >= 0 else 0\n```", "```py\nsum([x*w for x,w in zip(input,weight)])\n```", "```py\ny = x @ w.t() + b\n```", "```py\ny[i,j] = sum([a * b for a,b in zip(x[i,:],w[j,:])]) + b[j]\n```", "```py\nsum([a * b for a,b in zip(m[i,:],n[:,j])])\n```", "```py\nimport torch\nfrom torch import tensor\n```", "```py\ndef matmul(a,b):\n    ar,ac = a.shape # n_rows * n_cols\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n```", "```py\nm1 = torch.randn(5,28*28)\nm2 = torch.randn(784,10)\n```", "```py\n%time t1=matmul(m1, m2)\n```", "```py\nCPU times: user 1.15 s, sys: 4.09 ms, total: 1.15 s\nWall time: 1.15 s\n```", "```py\n%timeit -n 20 t2=m1@m2\n```", "```py\n14 \u00b5s \u00b1 8.95 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 20 loops each)\n```", "```py\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na + b\n```", "```py\ntensor([12., 14.,  3.])\n```", "```py\na < b\n```", "```py\ntensor([False,  True,  True])\n```", "```py\n(a < b).all(), (a==b).all()\n```", "```py\n(tensor(False), tensor(False))\n```", "```py\n(a + b).mean().item()\n```", "```py\n9.666666984558105\n```", "```py\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm*m\n```", "```py\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n```", "```py\nn = tensor([[1., 2, 3], [4,5,6]])\nm*n\n```", "```py\n RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at\n dimension 0\n```", "```py\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum()\n    return c\n```", "```py\n%timeit -n 20 t3 = matmul(m1,m2)\n```", "```py\n1.7 ms \u00b1 88.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 20 loops each)\n```", "```py\na = tensor([10., 6, -4])\na > 0\n```", "```py\ntensor([ True,  True, False])\n```", "```py\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n(m - 5) / 2.73\n```", "```py\ntensor([[-1.4652, -1.0989, -0.7326],\n        [-0.3663,  0.0000,  0.3663],\n        [ 0.7326,  1.0989,  1.4652]])\n```", "```py\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm.shape,c.shape\n```", "```py\n(torch.Size([3, 3]), torch.Size([3]))\n```", "```py\nm + c\n```", "```py\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n```", "```py\nc.expand_as(m)\n```", "```py\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n```", "```py\nt = c.expand_as(m)\nt.storage()\n```", "```py\n 10.0\n 20.0\n 30.0\n[torch.FloatStorage of size 3]\n```", "```py\nt.stride(), t.shape\n```", "```py\n((0, 1), torch.Size([3, 3]))\n```", "```py\nc + m\n```", "```py\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n```", "```py\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n```", "```py\ntensor([[11., 22., 33.],\n        [14., 25., 36.]])\n```", "```py\nc = tensor([10.,20])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n```", "```py\n RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at\n dimension 1\n```", "```py\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nc = c.unsqueeze(1)\nm.shape,c.shape\n```", "```py\n(torch.Size([3, 3]), torch.Size([3, 1]))\n```", "```py\nc+m\n```", "```py\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n```", "```py\nt = c.expand_as(m)\nt.storage()\n```", "```py\n 10.0\n 20.0\n 30.0\n[torch.FloatStorage of size 3]\n```", "```py\nt.stride(), t.shape\n```", "```py\n((1, 0), torch.Size([3, 3]))\n```", "```py\nc = tensor([10.,20,30])\nc.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape\n```", "```py\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n```", "```py\nc.shape, c[None,:].shape,c[:,None].shape\n```", "```py\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n```", "```py\nc[None].shape,c[...,None].shape\n```", "```py\n(torch.Size([1, 3]), torch.Size([3, 1]))\n```", "```py\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)\n    return c\n```", "```py\n%timeit -n 20 t4 = matmul(m1,m2)\n```", "```py\n357 \u00b5s \u00b1 7.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 20 loops each)\n```", "```py\nImage  (3d tensor): 256 x 256 x 3\nScale  (1d tensor):  (1)   (1)  3\nResult (3d tensor): 256 x 256 x 3\n```", "```py\nImage  (3d tensor): 256 x 256 x   3\nScale  (1d tensor):  (1)  256 x 256\nError\n```", "```py\nMatrix (2d tensor):   3 x 3\nVector (1d tensor): (1)   3\nResult (2d tensor):   3 x 3\n```", "```py\nik,kj -> ij\n```", "```py\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n```", "```py\ntorch.einsum('ij->ji', a)\n```", "```py\ntorch.einsum('bi,ij,bj->b', a, b, c)\n```", "```py\ntorch.einsum('bik,bkj->bij', a, b)\n```", "```py\n%timeit -n 20 t5 = matmul(m1,m2)\n```", "```py\n68.7 \u00b5s \u00b1 4.06 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 20 loops each)\n```", "```py\ndef lin(x, w, b): return x @ w + b\n```", "```py\nx = torch.randn(200, 100)\ny = torch.randn(200)\n```", "```py\nw1 = torch.randn(100,50)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1)\nb2 = torch.zeros(1)\n```", "```py\nl1 = lin(x, w1, b1)\nl1.shape\n```", "```py\ntorch.Size([200, 50])\n```", "```py\nl1.mean(), l1.std()\n```", "```py\n(tensor(0.0019), tensor(10.1058))\n```", "```py\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]\n```", "```py\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])\n```", "```py\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.01)\nx[0:5,0:5]\n```", "```py\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n```", "```py\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.1)\nx[0:5,0:5]\n```", "```py\ntensor([[ 0.7554,  0.6167, -0.1757, -1.5662,  0.5644],\n        [-0.1987,  0.6292,  0.3283, -1.1538,  0.5416],\n        [ 0.6106,  0.2556, -0.0618, -0.9463,  0.4445],\n        [ 0.4484,  0.7144,  0.1164, -0.8626,  0.4413],\n        [ 0.3463,  0.5930,  0.3375, -0.9486,  0.5643]])\n```", "```py\nx.std()\n```", "```py\ntensor(0.7042)\n```", "```py\nx = torch.randn(200, 100)\ny = torch.randn(200)\n```", "```py\nfrom math import sqrt\nw1 = torch.randn(100,50) / sqrt(100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) / sqrt(50)\nb2 = torch.zeros(1)\n```", "```py\nl1 = lin(x, w1, b1)\nl1.mean(),l1.std()\n```", "```py\n(tensor(-0.0050), tensor(1.0000))\n```", "```py\ndef relu(x): return x.clamp_min(0.)\n```", "```py\nl2 = relu(l1)\nl2.mean(),l2.std()\n```", "```py\n(tensor(0.3961), tensor(0.5783))\n```", "```py\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1))\nx[0:5,0:5]\n```", "```py\ntensor([[0.0000e+00, 1.9689e-08, 4.2820e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.6701e-08, 4.3501e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.0976e-08, 3.0411e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.8457e-08, 4.9469e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.9949e-08, 4.1643e-08, 0.0000e+00, 0.0000e+00]])\n```", "```py\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\nx[0:5,0:5]\n```", "```py\ntensor([[0.2871, 0.0000, 0.0000, 0.0000, 0.0026],\n        [0.4546, 0.0000, 0.0000, 0.0000, 0.0015],\n        [0.6178, 0.0000, 0.0000, 0.0180, 0.0079],\n        [0.3333, 0.0000, 0.0000, 0.0545, 0.0000],\n        [0.1940, 0.0000, 0.0000, 0.0000, 0.0096]])\n```", "```py\nx = torch.randn(200, 100)\ny = torch.randn(200)\n```", "```py\nw1 = torch.randn(100,50) * sqrt(2 / 100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) * sqrt(2 / 50)\nb2 = torch.zeros(1)\n```", "```py\nl1 = lin(x, w1, b1)\nl2 = relu(l1)\nl2.mean(), l2.std()\n```", "```py\n(tensor(0.5661), tensor(0.8339))\n```", "```py\ndef model(x):\n    l1 = lin(x, w1, b1)\n    l2 = relu(l1)\n    l3 = lin(l2, w2, b2)\n    return l3\n```", "```py\nout = model(x)\nout.shape\n```", "```py\ntorch.Size([200, 1])\n```", "```py\ndef mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\n```", "```py\nloss = mse(out, y)\n```", "```py\nloss = mse(out,y) = mse(lin(l2, w2, b2), y)\n```", "```py\ndef mse_grad(inp, targ):\n    # grad of loss with respect to output of previous layer\n    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]\n```", "```py\ndef relu_grad(inp, out):\n    # grad of relu with respect to input activations\n    inp.g = (inp>0).float() * out.g\n```", "```py\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    w.g = inp.t() @ out.g\n    b.g = out.g.sum(0)\n```", "```py\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = inp @ w1 + b1\n    l2 = relu(l1)\n    out = l2 @ w2 + b2\n    # we don't actually need the loss in backward!\n    loss = mse(out, targ)\n\n    # backward pass:\n    mse_grad(out, targ)\n    lin_grad(l2, out, w2, b2)\n    relu_grad(l1, l2)\n    lin_grad(inp, l1, w1, b1)\n```", "```py\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n\n    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g\n```", "```py\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp@self.w + self.b\n        return self.out\n\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n```", "```py\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp = inp\n        self.targ = targ\n        self.out = (inp.squeeze() - targ).pow(2).mean()\n        return self.out\n\n    def backward(self):\n        x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n        self.inp.g = 2.*x/self.targ.shape[0]\n```", "```py\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n\n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n\n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n```", "```py\nmodel = Model(w1, b1, w2, b2)\n```", "```py\nloss = model(x, y)\n```", "```py\nmodel.backward()\n```", "```py\nclass LayerFunction():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n\n    def forward(self):  raise Exception('not implemented')\n    def bwd(self):      raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n```", "```py\nclass Relu(LayerFunction):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n```", "```py\nclass Lin(LayerFunction):\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def forward(self, inp): return inp@self.w + self.b\n\n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = out.g.sum(0)\n```", "```py\nclass Mse(LayerFunction):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ):\n        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n```", "```py\nfrom torch.autograd import Function\n\nclass MyRelu(Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i.clamp_min(0.)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i, = ctx.saved_tensors\n        return grad_output * (i>0).float()\n```", "```py\nimport torch.nn as nn\n\nclass LinearLayer(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))\n        self.bias = nn.Parameter(torch.zeros(n_out))\n\n    def forward(self, x): return x @ self.weight.t() + self.bias\n```", "```py\nlin = LinearLayer(10,2)\np1,p2 = lin.parameters()\np1.shape,p2.shape\n```", "```py\n(torch.Size([2, 10]), torch.Size([2]))\n```", "```py\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n\n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\n```", "```py\nclass Model(Module):\n    def __init__(self, n_in, nh, n_out):\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n\n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\n```", "```py\n    tensor([1,2]) + tensor([1])\n    ```", "```py\n    tensor([1,2]) + tensor([1,2,3])\n    ```"]