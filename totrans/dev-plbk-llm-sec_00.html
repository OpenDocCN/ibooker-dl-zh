<html><head></head><body><section data-pdf-bookmark="Preface" data-type="preface" epub:type="preface"><div class="preface" id="preface_idFEZUNT">&#13;
      <h1>Preface</h1>&#13;
      <p>Everywhere in the world, we’re riding the large language model (LLM) wave, and it’s exhilarating! When ChatGPT burst onto the scene, it didn’t just walk into the record books; it smashed them, becoming the fastest-adopted application in history. Now, it’s as if every software vendor on the planet is racing to embed generative AI and LLM technologies into their stack, pushing us into uncharted territories. The buzz is real, the hype is justified, and the possibilities seem limitless.</p>&#13;
      <p>But hold on because there’s a twist. As we marvel at these technological wonders, their security scaffolding is, to put it mildly, a work in progress. The hard truth? Many developers are stepping into this new era without a map, largely unaware of the security and safety quicksand beneath the surface. It’s almost routine now: every week, we’re hit with another headline screaming about an LLM hiccup. The fallout from these individual incidents has been moderate so far, but make no mistake—we’re flirting with disaster.</p>&#13;
      <p>The risks aren’t just hypothetical; they’re as real as it gets, and the clock is ticking. Without a deep dive into the murky waters of LLM security risks and how to navigate them, we’re not just risking minor glitches; we’re courting major catastrophes. It’s time for developers to gear up, get informed, and get ahead of the curve. Fast!</p>&#13;
      <section data-pdf-bookmark="Who Should Read This Book" data-type="sect1"><div class="sect1" id="who_should_read_this_book">&#13;
        <h1>Who Should Read This Book</h1>&#13;
        <p>The primary audience for this book is development teams that are building custom applications that embed LLM technologies. Through my recent work in this area, I’ve come to understand that these teams are often large and their members include an incredibly diverse set of backgrounds. These include software developers skilled in “web app” technologies who are taking their first steps with AI. These teams may also consist of AI experts who are bringing their craft out of the back office for the first time and into the limelight, where the security risks are much different. They also include application security pros and data science specialists.</p>&#13;
        <p>Beyond that core audience, I’ve learned that others have found much of this information useful. This includes the extended teams involved in these projects, who want to understand the underpinnings of the technologies to help mitigate the critical risks of adopting these new technologies. These include software development executives, chief information security officers (CISOs), quality engineers, and security operations teams.</p>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Why I Wrote This Book" data-type="sect1"><div class="sect1" id="why_i_wrote_this_book">&#13;
        <h1>Why I Wrote This Book</h1>&#13;
        <p>I’ve always been fascinated by artificial intelligence. As a preteen, I fondly remember writing video games on my Atari 400 home computer. Circa 1980, this little machine had only 8 kilobytes of RAM. But I still managed to cram a complete clone of the Tron Lightcycles game onto that machine, complete with a simple but effective AI to drive one of the cycles when you were playing in single-player mode.</p>&#13;
        <p>In my professional career, I’ve been involved with several AI-related projects. After college, my best friend Tom Santos and I started an AI software company based on a few thousand lines of handcrafted C++ code that solved seemingly intractable problems with genetic algorithms. I’d later help build a large-scale machine learning system at Citrix with my friends Kedar Poduri and Ebenezer Schubert. But when I saw ChatGPT for the first time, I knew everything had changed.</p>&#13;
        <p>When I first encountered LLMs, I worked at a company that built cybersecurity software. My job was helping large companies find and track vulnerabilities in their software. It quickly became apparent that LLMs offered unique and serious security vulnerabilities. Over the next few months, I retooled my career to go after this disruption. I started a popular open source project around LLM security, which you’ll hear more about later. I even switched jobs to join Exabeam, a company that works at the intersection of AI and cybersecurity. When an editor from O’Reilly approached me about writing a book on this topic, I knew I had to jump at the chance.</p>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Navigating This Book" data-type="sect1"><div class="sect1" id="navigating_this_book">&#13;
        <h1>Navigating This Book</h1>&#13;
        <p>This book has 12 chapters that are divided into three logical sections. I’ll sketch out each section and chapter here to give you an idea of the approach and so you’ll know what’s coming as you read.</p>&#13;
        <section data-pdf-bookmark="Section 1: Laying the Foundation (Chapters 1–3)" data-type="sect2"><div class="sect2" id="section_1_laying_the_foundation_chapters_1_3">&#13;
          <h2>Section 1: Laying the Foundation (Chapters 1–3)</h2>&#13;
          <p>The initial chapters of this book establish the groundwork for understanding the security posture of LLM-based applications. They should give you the grounding you can use to confidently unpack the issues facing the development of apps using LLMs: </p>&#13;
          <ul>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch01.html#chatbots_breaking_bad">Chapter 1, “Chatbots Breaking Bad”</a>, walks through a real-world case study whereby amateur hackers destroyed an expensive and promising chatbot project from one of the world’s largest software companies. This will set the stage for your forthcoming battles in this arena. </p>&#13;
            </li>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch02.html#the_owasp_top_10_for_llm_applications">Chapter 2, “The OWASP Top 10 for LLM Applications”</a>, introduces a project I founded in 2023 that aims to identify and address the unique security challenges posed by LLMs. The knowledge gained working on that directly led to my writing this book. </p>&#13;
            </li>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch03.html#architectures_and_trust_boundaries">Chapter 3, “Architectures and Trust Boundaries”</a>, explores the structure of applications using LLMs, emphasizing the importance of controlling the various data flows within the application. </p>&#13;
            </li>&#13;
          </ul>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Section 2: Risks, Vulnerabilities, and Remediations (Chapters 4–9)" data-type="sect2"><div class="sect2" id="section_2_risks_vulnerabilities_and_remediation">&#13;
          <h2>Section 2: Risks, Vulnerabilities, and Remediations (Chapters 4–9)</h2>&#13;
          <p>In these chapters, we’ll break down the significant risk areas you face when developing LLM applications. These risks include issues with flavors familiar to any application security practitioner, such as injection attacks, sensitive information leakage, and software supply chain risk. You’ll also be introduced to classes of vulnerabilities well known to machine learning aficionados but less familiar in web development, such as training data poisoning. </p>&#13;
          <p>Along the way, you’ll also learn about all-new security and safety concerns plaguing these new generative AI systems, such as hallucinations, overreliance, and excessive agency. I’ll walk you through real-world case studies to help you understand the risks and implications and advise you on how to prevent or mitigate these risks on a case-by-case basis: </p>&#13;
          <ul>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch04.html#prompt_injection">Chapter 4, “Prompt Injection”</a>, explores how attackers can manipulate LLMs by crafting specific inputs that cause them to perform unintended actions. </p>&#13;
            </li>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch05.html#can_your_llm_know_too_much">Chapter 5, “Can Your LLM Know Too Much?”</a>, dives into the risks of sensitive information leakage, showcasing how LLMs can inadvertently expose data they’ve been trained on and how to safeguard against this vulnerability.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch06.html#do_language_models_dream_of_electric_sheep">Chapter 6, “Do Language Models Dream of <span class="keep-together">Electric Sheep?</span>”</a>, examines the unique phenomenon of “hallucinations” in LLMs—instances where models generate false or misleading information.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch07.html#trust_no_one">Chapter 7, “Trust No One”</a>, focuses on the principle of zero trust, explaining the importance of not taking any output at face value and ensuring rigorous validation processes are in place to handle LLM outputs.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch08.html#don_t_lose_your_wallet">Chapter 8, “Don’t Lose Your Wallet”</a>, tackles the economic risks of deploying LLM technologies, focusing on denial-of-service (DoS), denial-of-wallet (DoW), and model cloning attacks. These threats exploit similar vulnerabilities to impose financial burdens, disrupt services, or steal intellectual property.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p><a data-type="xref" data-xrefstyle="chap-num-title" href="ch09.html#find_the_weakest_link">Chapter 9, “Find the Weakest Link”</a>, highlights the vulnerabilities within the software supply chain and the critical steps needed to secure it from potential breaches that could compromise the entire application.</p>&#13;
            </li>&#13;
          </ul>&#13;
          <p>By understanding and addressing these risks, developers can better secure their applications against an evolving landscape of threats.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Section 3: Building a Security Process and Preparing for the Future (Chapters 10–12)" data-type="sect2"><div class="sect2" id="section_3_building_a_security_process_and_prepari">&#13;
          <h2>Section 3: Building a Security Process and Preparing for the Future (Chapters 10–12)</h2>&#13;
          <p>The chapters in Section 2 will give you the tools you need to understand and address the various individual threats you’ll see in this space. This last section is about <span class="keep-together">bringing</span> it all together: </p>&#13;
          <ul>&#13;
            <li>&#13;
              <p>In <a data-type="xref" data-xrefstyle="chap-num-title" href="ch10.html#learning_from_future_history">Chapter 10, “Learning from Future History”</a>, I’ll use some famous science fiction anecdotes to illustrate how multiple weaknesses and design issues can stitch together to spell disaster. By explaining these futuristic case studies, I hope to help you prevent a future like this from ever occurring. </p>&#13;
            </li>&#13;
            <li>&#13;
              <p>In <a data-type="xref" data-xrefstyle="chap-num-title" href="ch11.html#trust_the_process">Chapter 11, “Trust the Process”</a>, we’ll get down to the serious business of building LLM-savvy security practices into your software factory—without this, I do not believe you can successfully secure this type of software at scale. </p>&#13;
            </li>&#13;
            <li>&#13;
              <p>Finally, in <a data-type="xref" data-xrefstyle="chap-num-title" href="ch12.html#a_practical_framework_for_responsible_ai_security">Chapter 12, “A Practical Framework for <span class="keep-together">Responsible AI Security</span>”</a>, we’ll examine the trajectory of LLM and AI technologies to see where they’re taking us and the likely implications to security and safety requirements. I’ll also introduce you to the Responsible Artificial Intelligence Software Engineering (RAISE) framework that will give you a simple, checklist-based approach to ensuring you’re putting into practice the most important tools and lessons to keep your software safe and secure.</p>&#13;
            </li>&#13;
          </ul>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Conventions Used in This Book" data-type="sect1"><div class="sect1" id="_conventions_used_in_this_book">&#13;
<h1>Conventions Used in This Book</h1>&#13;
<p>The following typographical conventions are used in this book:</p>&#13;
<dl>&#13;
<dt><em>Italic</em></dt>&#13;
<dd>&#13;
<p>Indicates new terms, URLs, email addresses, filenames, and file extensions.</p>&#13;
</dd>&#13;
<dt><code>Constant width</code></dt>&#13;
<dd>&#13;
<p>Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.</p>&#13;
</dd>&#13;
<dt><strong><code>Constant width bold</code></strong></dt>&#13;
<dd>&#13;
<p>Shows commands or other text that should be typed literally by the user.</p>&#13;
</dd>&#13;
<dt><em><code>Constant width italic</code></em></dt>&#13;
<dd>&#13;
<p>Shows text that should be replaced with user-supplied values or by values determined by context.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>This element signifies a tip or suggestion.</p>&#13;
</div>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>This element signifies a general note.</p>&#13;
</div>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>This element indicates a warning or caution.</p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="O’Reilly Online Learning" data-type="sect1"><div class="sect1" id="_safari_books_online">&#13;
<h1>O’Reilly Online Learning</h1>&#13;
<div class="ormenabled" data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>For more than 40 years, <a class="orm:hideurl" href="https://oreilly.com"><em class="hyperlink">O’Reilly Media</em></a> has provided technology and business training, knowledge, and insight to help companies succeed.</p>&#13;
</div>&#13;
<p>Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit <a class="orm:hideurl" href="https://oreilly.com"><em>https://oreilly.com</em></a>.</p>&#13;
</div></section>&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="How to Contact Us" data-type="sect1"><div class="sect1" id="_how_to_contact_us">&#13;
<h1>How to Contact Us</h1>&#13;
<p>Please address comments and questions concerning this book to the publisher:</p>&#13;
<ul class="simplelist">&#13;
  <li>O’Reilly Media, Inc.</li>&#13;
  <li>1005 Gravenstein Highway North</li>&#13;
  <li>Sebastopol, CA 95472</li>&#13;
  <li>800-889-8969 (in the United States or Canada)</li>&#13;
  <li>707-827-7019 (international or local)</li>&#13;
  <li>707-829-0104 (fax)</li>&#13;
  <li><a class="email" href="mailto:support@oreilly.com"><em>support@oreilly.com</em></a></li>&#13;
  <li><a href="https://www.oreilly.com/about/contact.html"><em>https://www.oreilly.com/about/contact.html</em></a></li>&#13;
</ul>&#13;
<p>We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at <a href="https://oreil.ly/the-developers-playbook"><em class="hyperlink">https://oreil.ly/the-developers-playbook</em></a>.</p>&#13;
<!--Don't forget to update the link above.-->&#13;
&#13;
<p>For news and information about our books and courses, visit <a href="https://oreilly.com"><em class="hyperlink">https://oreilly.com</em></a>.</p>&#13;
<p>Find us on LinkedIn: <a href="https://linkedin.com/company/oreilly-media"><em class="hyperlink">https://linkedin.com/company/oreilly-media</em></a>.</p>&#13;
<p>Watch us on YouTube: <a href="https://youtube.com/oreillymedia"><em class="hyperlink">https://youtube.com/oreillymedia</em></a>.</p>&#13;
</div></section>&#13;
&#13;
      <section data-pdf-bookmark="Acknowledgments" data-type="sect1"><div class="sect1" id="acknowledgments">&#13;
        <h1>Acknowledgments</h1>&#13;
        <p>I’d like to thank all the friends, family, and colleagues who encouraged me or provided feedback on various aspects of the project: Will Chilcutt, Fabrizio Cilli, Ads Dawson, Ron Del Rosario, Sherri Douville, Sandy Dunn, Ken Huang, Gavin <span class="keep-together">Klondike,</span> Marko Lihter, Marten Mickos, Eugene Neelou, Chase Peterson, Karla Roland, Jason Ross, Tom Santos, Robert Simonoff, Yuvraj Singh, Rachit Sood, Seth <span class="keep-together">Summersett,</span> Darcie Tuuri, Ashish Verma, Jeff Williams, Alexa Wilson, Dave Wilson, and Zoe Wilson.</p>&#13;
        <p>I want to thank the team at O’Reilly for supporting and guiding me on this project. I also owe a tremendous debt of gratitude to Nicole Butterfield, who approached me with the idea for this book and guided me through the proposal phase. I also want to express my appreciation for Jeff Bleiel, my editor, whose patience, skills, and expertise significantly impacted the book. Special thanks to our technical reviewers: Pamela Isom, Chenta Lee, Thomas Nield, and Matteo Dora. </p>&#13;
      </div></section>&#13;
    </div></section></body></html>