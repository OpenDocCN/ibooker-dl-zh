- en: Chapter 14\. Building the Purrfect Cat Locator App with TensorFlow Object Detection
    API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章。使用TensorFlow Object Detection API构建完美的猫定位器应用
- en: Bob gets frequent visits from a neighborhood feral cat. Those visits result
    in less than pleasant outcomes. You see, Bob has a nice fairly large garden that
    he tends to with great care. However, this furry little fella visits his garden
    every night and starts chewing on a bunch of plants. Months of hard work gets
    destroyed overnight. Clearly unhappy with this situation, Bob is keen on doing
    something about it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 鲍勃经常受到附近的流浪猫的访问。这些访问导致了不太愉快的结果。你看，鲍勃有一个相当大的花园，他非常用心地照料。然而，这只毛茸茸的小家伙每天晚上都会来到他的花园，并开始咬一堆植物。几个月的辛勤工作在一夜之间被摧毁。显然对这种情况不满意，鲍勃渴望采取一些行动。
- en: 'Channeling his inner Ace Ventura (Pet Detective), he tries to stay awake at
    night to drive the cat away, but clearly, that’s not sustainable in the long term.
    After all, Red Bull has its limits. Reaching a breaking point, he decides to use
    the nuclear option: use AI in conjunction with his sprinkler system to literally
    “turn the hose” on the cat.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 发挥他内心的宠物侦探艾斯·文图拉（Ace Ventura）的精神，他试图在夜间保持清醒，赶走猫，但显然，这在长期内是不可持续的。毕竟，红牛也有其限制。达到一个临界点后，他决定使用核选项：将AI与他的洒水系统结合起来，从而真正“打开水管”对付猫。
- en: In his sprawling garden, he sets up a camera that can track the motion of the
    cat and turn on the nearest set of sprinklers to scare the cat away. He has an
    old phone lying around, and all he needs is a way to determine the cat’s location
    in real time, so he can control accordingly which sprinkler to trigger.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在他广阔的花园里，他设置了一个可以跟踪猫的运动并打开最近的洒水器来吓跑猫的摄像头。他周围有一部旧手机，他所需要的就是一种实时确定猫位置的方法，这样他就可以相应地控制哪个洒水器触发。
- en: Hopefully, after a few unwelcome baths, as demonstrated in [Figure 14-1](part0016.html#building_an_ai_cat_sprinkler_systemcomma),
    the cat would be disincentivized from raiding Bob’s garden.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 希望经过几次不受欢迎的浴，如[图14-1](part0016.html#building_an_ai_cat_sprinkler_systemcomma)所示，猫会不再去鲍勃的花园里捣乱。
- en: '![Building an AI cat sprinkler system, like this Havahart Spray Away Motion
    Detector](../images/00166.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![建立一个AI猫喷水系统，就像这个Havahart Spray Away Motion Detector](../images/00166.jpeg)'
- en: Figure 14-1\. Building an AI cat sprinkler system, like this Havahart Spray
    Away Motion Detector
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1\. 建立一个AI猫喷水系统，就像这个Havahart Spray Away Motion Detector
- en: Tip
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Keen observers might have noticed that we’re attempting to find the position
    of the cat in 2D space, whereas the cat itself exists in 3D space. We can make
    assumptions to simplify the problem of locating the cat in real-world coordinates
    by assuming that the camera will always be in a fixed position. To determine the
    distance of the cat, we can use the size of an average cat to take measurements
    of its apparent size on a picture at regular intervals of distance from the camera.
    For example, an average cat might appear to have a bounding box height of 300
    pixels at a distance of two feet from the camera, whereas it might occupy 250
    pixels when it’s three feet away from the camera lens.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 敏锐的观察者可能已经注意到，我们试图在二维空间中找到猫的位置，而猫本身存在于三维空间中。我们可以做出一些假设来简化在真实世界坐标中定位猫的问题，假设摄像头始终处于固定位置。为了确定猫的距离，我们可以使用一个平均猫的大小来测量它在图片上的表观大小，这些图片是在距离摄像头的固定距离处定期拍摄的。例如，一个平均猫在距离摄像头两英尺处可能看起来有一个300像素的边界框高度，而当它离摄像头镜头三英尺时，它可能占据250像素。
- en: 'In this chapter, we help Bob make his cat detector. (Note to animal-rights
    advocates: no cats were harmed in the making of this chapter.) We answer the following
    questions along the way:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们帮助鲍勃制作他的猫探测器。（动物权益倡导者请注意：在制作本章时没有伤害到任何猫。）我们沿途回答以下问题：
- en: What are the different kinds of computer-vision tasks?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哪些不同类型的计算机视觉任务？
- en: How do I reuse a model pretrained for an existing class of objects?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何重用为现有对象类别预训练的模型？
- en: Can I train an object detector model without writing any code?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能否在不编写任何代码的情况下训练一个物体检测器模型？
- en: I want more granular control for greater accuracy and speed. How do I train
    a custom object detector?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我想要更精细的控制，以获得更高的准确性和速度。如何训练一个自定义物体检测器？
- en: Types of Computer-Vision Tasks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉任务的类型
- en: 'In most of the previous chapters, we’ve essentially looked at one kind of problem:
    object classification. In that, we found out whether an image contained objects
    of a certain class. In Bob’s scenario, in addition to knowing whether the camera
    is seeing a cat, it is necessary to know exactly where the cat is in order to
    trigger the nearest sprinkler. Determining the location of an object within an
    image is a different type of problem: *object detection*. Before we jump into
    object detection, let’s look at the variety of frequent computer-vision tasks
    and the questions they purport to answer.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的大部分章节中，我们基本上看了一种问题：物体分类。在这种情况下，我们找出了图像中是否包含某个特定类别的对象。在鲍勃的情况下，除了知道摄像头是否看到了一只猫之外，还需要确切地知道猫的位置，以触发最近的洒水器。确定图像中对象的位置是一种不同类型的问题：*物体检测*。在我们深入研究物体检测之前，让我们看看频繁的计算机视觉任务的种类以及它们试图回答的问题。
- en: Classification
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: 'We have looked at several examples of classification throughout this book.
    Simply put, the task of classification assigns an image with the class(es) of
    the object(s) present in the image. It answers the question: “Is there an object
    of class X in the image?” Classification is one of the most fundamental computer-vision
    tasks. In an image, there might be objects of more than one class—called multiclass
    classification. The label in this case for one image would be a list of all object
    classes that the image contains.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经看过几个分类的例子。简单来说，分类的任务是将图像分配给图像中存在的对象类别。它回答了这样一个问题：“图像中是否有X类对象？”分类是最基本的计算机视觉任务之一。在一张图像中，可能有多个类别的对象，称为多类别分类。在这种情况下，一张图像的标签将是图像包含的所有对象类别的列表。
- en: Localization
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定位
- en: The downside of classification is that it does not tell us where in the image
    a particular object is, how big or small it is, or how many objects there are.
    It only tells us that an object of a particular class exists somewhere within
    it. *Localization*, otherwise known as *classification with localization*, can
    inform us which classes are in an image and where they are located within the
    image. The keyword here is *classes*, as opposed to individual objects, because
    localization gives us only one bounding box per class. Just like classification,
    it cannot answer the question “How many objects of class X are in this image?”
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的缺点是它不告诉我们图像中特定对象的位置在哪里，它有多大或多小，或者有多少个对象。它只告诉我们特定类别的对象存在于图像中的某个地方。*定位*，又称*带定位的分类*，可以告诉我们图像中有哪些类别以及它们在图像中的位置。这里的关键词是*类别*，而不是个别对象，因为定位每个类别只给出一个边界框。就像分类一样，它无法回答“这个图像中有多少个X类对象？”的问题。
- en: As you will see momentarily, localization will work correctly only when it is
    guaranteed that there is a single instance of each class. If there is more than
    one instance of a class, one bounding box might encompass some or all objects
    of that class (depending upon the probabilities). But just one bounding box per
    class seems rather restrictive, doesn’t it?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您马上会看到的，只有在可以保证每个类别只有一个实例时，定位才能正常工作。如果一个类别有多个实例，一个边界框可能包含该类别的一些或所有对象（取决于概率）。但每个类别只有一个边界框似乎相当受限制，不是吗？
- en: Detection
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测
- en: When we have multiple objects belonging to multiple classes in the same image,
    localization will not suffice. Object detection would give us a bounding rectangle
    for every instance of a class, for all classes. It also informs us as to the class
    of the object within each of the detected bounding boxes. For example, in the
    case of a self-driving car, object detection would return one bounding box per
    car, person, lamp post, and so on that the car sees. Because of this ability,
    we also can use it in applications that need to count. For example, counting the
    number of people in a crowd.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在同一图像中有多个属于多个类别的对象时，定位就不够了。对象检测会为每个类的每个实例提供一个边界矩形，对于所有类别。它还会告诉我们每个检测到的边界框中的对象的类别。例如，在自动驾驶汽车的情况下，对象检测会为汽车看到的每辆车、每个人、每个路灯等返回一个边界框。由于这种能力，我们也可以将其用于需要计数的应用程序。例如，计算人群中的人数。
- en: Unlike localization, detection does not have a restriction on the number of
    instances of a class in an image. This is why it is used in real-world applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与定位不同，检测在图像中某个类的实例数量上没有限制。这就是为什么它在现实世界的应用中被使用的原因。
- en: Warning
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Often people use the term “object localization” when, really, they mean “object
    detection.” It’s important to note the distinction between the two.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常人们在说“对象定位”时实际上是指“对象检测”。重要的是要注意这两者之间的区别。
- en: Segmentation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割
- en: Object segmentation is the task of assigning a class label to individual pixels
    throughout an image. A children’s coloring book where we use crayons to color
    various objects is a real-world analogy. Considering each pixel in the image is
    being assigned a class, it is a highly compute-intensive task. Although object
    localization and detection give bounding boxes, segmentation produces groups of
    pixels—also known as *masks*. Segmentation is obviously much more precise about
    the boundaries of an object compared to detection. For example, a cosmetic app
    that changes a user’s hair color in real-time would use segmentation. Based on
    the types of masks, segmentation comes in different flavors.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对象分割是将类标签分配给整个图像中的每个像素的任务。我们使用蜡笔给各种对象上色的儿童涂色书是一个现实世界的类比。考虑到图像中的每个像素都被分配一个类，这是一个高度计算密集的任务。虽然对象定位和检测提供边界框，分割产生像素组，也被称为*掩码*。与检测相比，分割在对象的边界方面显然更加精确。例如，一个可以实时改变用户头发颜色的化妆应用程序会使用分割。根据掩码的类型，分割有不同的风格。
- en: Semantic segmentation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义分割
- en: Given an image, *semantic segmentation* assigns one mask per class. If there
    are multiple objects of the same class, all of them are assigned to the same mask.
    There is no differentiation between the instances of the same class. Just like
    localization, semantic segmentation cannot count.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一幅图像，*语义分割*为每个类分配一个掩码。如果有多个相同类别的对象，则它们都分配给同一个掩码。同一类别的实例之间没有区分。就像定位一样，语义分割无法计数。
- en: Instance-level segmentation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实例级分割
- en: Given an image, *instance-level segmentation* identifies the area occupied by
    each instance of each class. Different instances of the same class will be segmented
    uniquely.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一幅图像，*实例级分割*识别每个类别每个实例占据的区域。同一类别的不同实例将被唯一分割。
- en: '[Table 14-1](part0016.html#types_of_computer-vision_tasks_illustrat) lists
    the different computer-vision tasks.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[表14-1](part0016.html#types_of_computer-vision_tasks_illustrat)列出了不同的计算机视觉任务。'
- en: Table 14-1\. Types of computer-vision tasks illustrated using image ID [120524](https://oreil.ly/ieJ2d)
    from the MS COCO dataset
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-1。使用图像ID [120524](https://oreil.ly/ieJ2d) 从 MS COCO 数据集说明的计算机视觉任务类型
- en: '| **Task** | **Image** | **In lay terms** | **Outputs** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **任务** | **图像** | **通俗地说** | **输出** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Object classification | ![](../images/00118.jpeg) | Is there a sheep in this
    image? | Class probabilities |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 对象分类 | ![](../images/00118.jpeg) | 这个图像中有一只羊吗？ | 类概率 |'
- en: '| Object localization | ![](../images/00015.jpeg) | Is there “a” sheep in the
    image and where is it? | Bounding box and class probability |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 对象定位 | ![](../images/00015.jpeg) | 图像中是否有“一只”羊，它在哪里？ | 边界框和类概率 |'
- en: '| Objectdetection | ![](../images/00147.jpeg) | What and where are “all” the
    objects in this image? | Bounding boxes, class probabilities, class ID prediction
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 对象检测 | ![](../images/00147.jpeg) | 这个图像中的“所有”对象是什么？ | 边界框，类概率，类ID预测 |'
- en: '| Semantic segmentation | ![](../images/00325.jpeg) | Which pixels in this
    image belong to different classes; e.g., the class “sheep,” “dog,” “person”? |
    One mask per class |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 语义分割 | ![](../images/00325.jpeg) | 这幅图像中哪些像素属于不同的类别；例如，“羊”，“狗”，“人”？ | 每个类别一个掩码
    |'
- en: '| Instance-level segmentation | ![](../images/00064.jpeg) | Which pixels in
    this image belong to each instance of each class; e.g., “sheep,” “dog,” “person”?
    | One mask per instance of each class |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 实例级别分割 | ![](../images/00064.jpeg) | 这幅图像中每个类别的每个实例属于哪些像素；例如，“羊”，“狗”，“人”？
    | 每个类别的每个实例一个掩码 |'
- en: Approaches to Object Detection
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对象检测方法
- en: Depending on the scenario, your needs, and technical know-how, there are a few
    ways to go about getting the object detection functionality in your applications.
    [Table 14-2](part0016.html#different_approaches_to_object_detection) takes a look
    at some approaches.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据情景、需求和技术知识，有几种方法可以实现在应用程序中获取对象检测功能。[表14-2](part0016.html#different_approaches_to_object_detection)介绍了一些方法。
- en: Table 14-2\. Different approaches to object detection and their trade-offs
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-2\. 对象检测的不同方法及其权衡
- en: '|  | **Custom classes** | **Effort** | **Cloud or local** | **Pros and cons**
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | **自定义类别** | **工作量** | **云端或本地** | **优缺点** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Cloud-based object detection APIs | No | <5 minutes | Cloud only | + Thousands
    of classes+ State of the art+ Fast setup+ Scalable API– No customization– Latency
    due to network |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 基于云的对象检测API | 否 | <5分钟 | 仅限云端 | + 成千上万种类别+ 处于技术前沿+ 快速设置+ 可扩展API– 无定制化– 由于网络而产生的延迟
    |'
- en: '| Pretrained model | No | <15 minutes | Local | + ~100 classes+ Choice of model
    to fit speed and accuracy needs+ Can run on the edge– No customization |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 预训练模型 | 否 | <15分钟 | 本地 | + ~100个类别+ 选择适合速度和准确性需求的模型+ 可在边缘运行– 无定制化 |'
- en: '| Cloud-based model training | Yes | <20 minutes | Cloud and local | + GUI-based,
    no coding for training+ Customizability+ Choice between powerful model (for cloud)
    and efficient model (for edge)+ Scalable API– Uses transfer learning, might not
    allow full-model retraining |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 基于云的模型训练 | 是 | <20分钟 | 云端和本地 | + 基于GUI，无需编码进行训练+ 可定制性+ 选择强大模型（用于云端）和高效模型（用于边缘）+
    可扩展API– 使用迁移学习，可能不允许完整模型重新训练 |'
- en: '| Custom training a model | Yes | 4 hours to 2 days | Local | + Highly customizable+
    Largest variety of models available for different speed and accuracy requirements–
    Time consuming and highly involved |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 自定义训练模型 | 是 | 4小时至2天 | 本地 | + 高度可定制+ 提供各种速度和准确性要求的模型– 耗时且复杂 |'
- en: It’s worth noting that the options marked “Cloud” are already engineered for
    scalability. At the same time, the options marked as being available for “Local”
    inferences can also be deployed on the cloud. To achieve high scale, we can host
    them on the cloud similar to how we did so for the classification models in [Chapter 9](part0011.html#AFM63-13fa565533764549a6f0ab7f11eed62b).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，“云”标记的选项已经为可扩展性而设计。同时，标记为“本地”推断的选项也可以部署在云端。为了实现高规模，我们可以将它们部署在云端，类似于我们在[第9章](part0011.html#AFM63-13fa565533764549a6f0ab7f11eed62b)中为分类模型所做的方式。
- en: Invoking Prebuilt Cloud-Based Object Detection APIs
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调用预构建的基于云的对象检测API
- en: As we already saw in [Chapter 8](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b),
    calling cloud-based APIs is relatively straightforward. The process involves setting
    up an account, getting an API key, reading through the documentation, and writing
    a REST API client. To make this process easier, many of these services provide
    Python-based (and other language) SDKs. The main cloud-based object detection
    API providers are Google’s Vision AI ([Figure 14-2](part0016.html#running_a_familiar_photo_on_googleapostr))
    and Microsoft’s Cognitive Services.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第8章](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b)中已经看到的，调用基于云的API相对简单。该过程涉及设置账户、获取API密钥、阅读文档以及编写REST
    API客户端。为了简化这个过程，许多服务提供基于Python（和其他语言）的SDK。主要的基于云的对象检测API提供商是Google的Vision AI（[图14-2](part0016.html#running_a_familiar_photo_on_googleapostr)）和微软的认知服务。
- en: The main advantages of using cloud-based APIs are that they are scalable out
    of the box and they support recognition of thousands of classes of objects. These
    cloud giants built their models using large proprietary datasets that are constantly
    growing, resulting in a very rich taxonomy. Considering that the number of classes
    in the largest public datasets with object detection labels is usually only a
    few hundred, there exists no nonproprietary solution out there that is capable
    of detecting thousands of classes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于云的API的主要优势在于它们具有可扩展性，并支持识别成千上万种对象类别。这些云巨头使用大型专有数据集构建了他们的模型，这些数据集不断增长，导致了非常丰富的分类法。考虑到最大的公共数据集中带有对象检测标签的类别数量通常只有几百个，目前还没有非专有解决方案能够检测成千上万种类别。
- en: The main limitation of cloud-based APIs, of course, is the latency due to network
    requests. It’s not possible to power real-time experiences with this kind of lag.
    In the next section, we look at how to power real-time experiences without any
    data or training.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的API的主要限制当然是由于网络请求而产生的延迟。无法通过这种延迟实现实时体验。在下一节中，我们将看看如何在没有任何数据或训练的情况下实现实时体验。
- en: '![Running a familiar photo on Google’s Vision AI API to obtain object detection
    results](../images/00328.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![在Google的Vision AI API上运行熟悉的照片以获取对象检测结果](../images/00328.jpeg)'
- en: Figure 14-2\. Running a familiar photo on Google’s Vision AI API to obtain object
    detection results
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-2\. 在Google的Vision AI API上运行熟悉的照片以获取对象检测结果
- en: Reusing a Pretrained Model
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重用预训练模型
- en: In this section, we explore how easy it is to get an object detector running
    on a mobile phone with a pretrained model. Hopefully, you are already comfortable
    with running a neural network on mobile from some of the previous chapters. The
    code to run object detection models on iOS and Android is referenced on the book’s
    GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-14*. Changing functionality is simply a matter of replacing the
    existing *.tflite* model file with a new one.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨如何轻松地在预训练模型上在手机上运行目标检测器。希望您已经熟悉了在之前的一些章节中在移动设备上运行神经网络的方法。在书的GitHub网站上引用了在iOS和Android上运行目标检测模型的代码（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-14*。更改功能只是简单地用新的*.tflite*模型文件替换现有的模型文件。
- en: In the previous chapters, we often used MobileNet to do classification tasks.
    Although the MobileNet family, including MobileNetV2 (2018) and MobileNetV3 (2019)
    by itself are classification networks only, they can act as a backbone for object
    detection architectures like SSD MobileNetV2, which we use in this chapter.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们经常使用MobileNet来进行分类任务。尽管MobileNet系列，包括MobileNetV2（2018）和MobileNetV3（2019）本身只是分类网络，但它们可以作为SSD
    MobileNetV2等目标检测架构的骨干，我们在本章中使用。
- en: Obtaining the Model
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取模型
- en: 'The best way to uncover the magic behind the scenes is to take a deeper look
    at a model. We’d need to get our hands on the [TensorFlow Models repository](https://oreil.ly/9CsHM),
    which features models for more than 50 deep learning tasks, including audio classification,
    text summarization, and object detection. This repository contains models as well
    as utility scripts that we use throughout this chapter. Without further ado, let’s
    clone the repository on our machines:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示幕后魔术的最佳方法是深入研究一个模型。我们需要获取我们的手上的[TensorFlow Models仓库](https://oreil.ly/9CsHM)，其中包含50多个深度学习任务的模型，包括音频分类、文本摘要和目标检测。这个仓库包含模型以及我们在本章中使用的实用脚本。话不多说，让我们在我们的机器上克隆这个仓库：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we update the `PYTHONPATH` to make all scripts visible to Python:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们更新`PYTHONPATH`以使所有脚本对Python可见：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we copy over the Protocol Buffer (protobuf) compiler command script from
    our GitHub repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    into our current directory to make the .`proto` files available to Python:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从我们的GitHub仓库（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）复制协议缓冲区（protobuf）编译器命令脚本到我们当前目录，以使.proto文件对Python可用：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we run the *setup.py* script as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们按照以下步骤运行*setup.py*脚本：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now is the time to download our prebuilt model. In our case, we’re using the
    SSD MobileNetV2 model. You can find a large list of models at TensorFlow’s [object
    detection model zoo](https://oreil.ly/FwSlM), featuring models by the datasets
    on which they were trained, speed of inference, and Mean Average Precision (mAP).
    Within that list, download the model titled `ssd_mobilenet_v2_coco`, which, as
    the name suggests, was trained on the MS COCO dataset. Benchmarked at 22 mAP,
    this model runs at 31 ms on an NVIDIA GeForce GTX TITAN X with a 600x600 resolution
    input. After downloading that model, we unzip it into the *models/research/object_detection*
    directory inside the TensorFlow repository. We can inspect the contents of the
    directory as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是下载我们预构建模型的时候了。在我们的情况下，我们使用SSD MobileNetV2模型。您可以在TensorFlow的[目标检测模型动物园](https://oreil.ly/FwSlM)找到一个大型模型列表，其中包含按照它们训练的数据集、推理速度和平均精度（mAP）进行分类的模型。在那个列表中，下载名为`ssd_mobilenet_v2_coco`的模型，正如其名称所示，该模型是在MS
    COCO数据集上训练的。在NVIDIA GeForce GTX TITAN X上以600x600分辨率输入运行时，该模型在22 mAP上运行，耗时31毫秒。下载完该模型后，我们将其解压缩到TensorFlow仓库中的*models/research/object_detection*目录中。我们可以按照以下方式检查目录的内容：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Test Driving Our Model
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试我们的模型
- en: Before we plug our model into a mobile app, it’s a good idea to verify whether
    the model is able to make predictions. The TensorFlow Models repository contains
    a Jupyter Notebook where we can simply plug in a photograph to make predictions
    on it. You can find the notebook at *models/research/object_detection/object_detection_tutorial.ipynb*.
    [Figure 14-3](part0016.html#object_detection_prediction_in_the_ready) shows a
    prediction from that notebook on a familiar photograph.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在将我们的模型插入移动应用程序之前，验证模型是否能够进行预测是一个好主意。TensorFlow Models仓库包含一个Jupyter Notebook，我们可以简单地插入一张照片进行预测。您可以在*models/research/object_detection/object_detection_tutorial.ipynb*找到这个笔记本。[图14-3](part0016.html#object_detection_prediction_in_the_ready)展示了来自该笔记本的一张熟悉照片的预测。
- en: '![Object detection prediction in the ready-to-use Jupyter Notebook from the
    TensorFlow Models repository](../images/00022.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![来自TensorFlow Models仓库的可直接使用的Jupyter Notebook中的目标检测预测](../images/00022.jpeg)'
- en: Figure 14-3\. Object detection prediction in the ready-to-use Jupyter Notebook
    from the TensorFlow Models repository
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-3。来自TensorFlow Models仓库的可直接使用的Jupyter Notebook中的目标检测预测
- en: Deploying to a Device
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署到设备
- en: 'Now that we’ve verified that the model works, it’s time to convert it into
    a mobile-friendly format. For conversion to the TensorFlow Lite format, we use
    our familiar `tflite_convert` tool from [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
    It should be noted that this tool operates on *.pb* files, whereas the TensorFlow
    Object Detection model zoo provides only model checkpoints. So, we first need
    to generate a *.pb* model from the checkpoints and graphs. Let’s do that using
    the handy script that comes with the TensorFlow Models repository. You can find
    the *export_tflite_ssd_graph.py* file in *models/research/object_detection*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经验证了模型的工作原理，是时候将其转换为适合移动设备的格式了。为了将其转换为TensorFlow Lite格式，我们使用了我们熟悉的`tfite_convert`工具，来自[第13章](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b)。值得注意的是，该工具操作*.pb*文件，而TensorFlow目标检测模型动物园仅提供模型检查点。因此，我们首先需要从检查点和图生成*.pb*模型。让我们使用TensorFlow
    Models仓库附带的方便脚本来做到这一点。您可以在*models/research/object_detection*中找到*export_tflite_ssd_graph.py*文件：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If the preceding script execution was successful, we’ll see the following files
    in the *tflite_model* directory:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的脚本执行成功，我们将在*tflite_model*目录中看到以下文件：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We will convert this to the TensorFlow Lite format using the `tflite_convert`
    tool.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`tflite_convert`工具将其转换为TensorFlow Lite格式。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now the only thing that remains is to plug the model into an app. The app that
    we’ll be using is referenced in the book’s GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-14*. We have already looked at how to swap a model in an app
    in Chapters [11](part0013.html#CCNA3-13fa565533764549a6f0ab7f11eed62b), [12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b),
    and [13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b), so we won’t discuss
    that here again. [Figure 14-4](part0016.html#running_a_real-time_object_detector_mode)
    shows the result of the object detector model when plugged into the Android app.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在唯一剩下的事情就是将模型插入应用程序中。我们将使用的应用程序在书的GitHub网站中引用（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）位于*code/chapter-14*。我们已经看过如何在第[11](part0013.html#CCNA3-13fa565533764549a6f0ab7f11eed62b)、[12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b)和[13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b)章中如何在应用程序中更换模型，因此我们不会在这里再次讨论。[图14-4](part0016.html#running_a_real-time_object_detector_mode)显示了将对象检测器模型插入Android应用程序时的结果。
- en: '![Running a real-time object detector model on an Android device](../images/00287.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![在Android设备上运行实时对象检测模型](../images/00287.jpeg)'
- en: Figure 14-4\. Running a real-time object detector model on an Android device
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-4\. 在Android设备上运行实时对象检测模型
- en: The reason why we’re able to see the “Cat” prediction right away is because
    ‘Cat’ is one of the 80 categories already present in the MS COCO dataset that
    was used to train the model. This model might be sufficient for Bob to deploy
    on his devices (keeping in mind that although Bob might not use mobile phones
    to track his garden, the process for deploying a TensorFlow Lite model to edge
    hardware is quite similar). However, Bob would do well for himself to improve
    the precision of his model by fine tuning it on data that has been generated within
    his garden. In the next section, we explore how to use transfer learning to build
    an object detector by using only a web-based tool. If you’ve read [Chapter 8](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b),
    what comes next should feel familiar.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够立即看到“猫”预测的原因是因为“猫”是用于训练模型的MS COCO数据集中已经存在的80个类别之一。这个模型可能足以供Bob部署在他的设备上（请记住，尽管Bob可能不使用手机来跟踪他的花园，但将TensorFlow
    Lite模型部署到边缘硬件的过程是非常相似的）。然而，Bob最好通过在花园内生成的数据对模型进行微调以提高精度。在接下来的部分中，我们将探讨如何使用迁移学习来构建一个只使用基于网络的工具的对象检测器。如果你已经阅读了[第8章](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b)，接下来的内容应该感觉很熟悉。
- en: Building a Custom Detector Without Any Code
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个无需任何代码的自定义检测器
- en: My cat’s breath smells like cat food!
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我的猫的呼吸闻起来像猫粮！
- en: ''
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ralph Wiggum
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 拉尔夫·威格姆
- en: Let’s face it. Your authors could go around and snap a lot of cat pictures within
    their gardens to perform this experiment. It would be tedious, and we might be
    rewarded with only a few extra percentage points of precision and recall. Or we
    could look at a really fun example that also tests the limits of CNNs. We are,
    of course, referring to *The Simpsons*. Given that most models are not trained
    with features from cartoon images, it would be interesting to see how our model
    performs here.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们面对现实吧。你们的作者可以四处走动，在花园里拍摄很多猫的照片来进行这个实验。这将是乏味的，我们可能只会获得几个额外百分点的精度和召回率。或者我们可以看一个真正有趣的例子，也测试CNN的极限。当然，我们指的是*辛普森一家*。鉴于大多数模型没有使用卡通图像的特征进行训练，看看我们的模型在这里的表现将是有趣的。
- en: First, we need to get our hands on a dataset. Thankfully, we have one readily
    available on [Kaggle](https://oreil.ly/-3wvj). Let’s download the dataset and
    use CustomVision.ai (similar to [Chapter 8](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b))
    to train our Simpsons Classifier using the following steps. *Excellent!*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要获取一个数据集。幸运的是，我们在[Kaggle](https://oreil.ly/-3wvj)上有一个可用的数据集。让我们下载数据集并使用CustomVision.ai（类似于[第8章](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b)）来训练我们的辛普森分类器，以下是使用的步骤。*太棒了！*
- en: Note
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Contrary to what some may believe, we, the authors, do not have millions of
    dollars lying around. This means that we cannot afford to buy the rights to *The
    Simpsons* from Fox. Copyright laws, as a consequence, prevent us from publishing
    any images from that show. Instead, in this section, we are using the next best
    thing—public domain images from the United States Congress. In place of the images
    from the cartoon, we use the photographs of congresswomen and congressmen with
    the same first names as the Simpsons family: *Homer* Hoch, *Marge* Roukema, *Bart*
    Gordon, and *Lisa* Blunt Rochester. Keep in mind that this is only for publishing
    purposes; we have still trained on the original dataset from the cartoon.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与一些人可能认为的相反，我们这些作者并没有数百万美元散落在周围。这意味着我们无法负担从福克斯购买*辛普森一家*的版权。版权法律的结果是阻止我们发布该节目的任何图像。因此，在这一部分中，我们使用了下一个最好的选择——来自美国国会的公共领域图像。我们使用国会议员的照片，这些议员与辛普森一家有相同的名字：*霍默*·霍奇、*玛吉*·鲁克玛、*巴特*·戈登和*丽莎*·布伦特·罗切斯特。请记住，这仅用于出版目的；我们仍然在原始卡通数据集上进行了训练。
- en: Go to CustomVision.ai and start a new Object Detection Project ([Figure 14-5](part0016.html#creating_a_new_object_detection_project)).
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往CustomVision.ai并开始一个新的目标检测项目（[图14-5](part0016.html#creating_a_new_object_detection_project)）。
- en: '![Creating a new Object Detection project in CustomVision.ai](../images/00154.jpeg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在CustomVision.ai中创建一个新的目标检测项目](../images/00154.jpeg)'
- en: Figure 14-5\. Creating a new Object Detection project in CustomVision.ai
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-5\. 在CustomVision.ai中创建一个新的目标检测项目
- en: Create tags for Homer, Marge, Bart, and Lisa. Upload 15 images for each character
    and draw a bounding box for each of those images. The dashboard should resemble
    [Figure 14-6](part0016.html#dashboard_with_bounding_box_and_class_na) right about
    now.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为霍默、玛吉、巴特和丽莎创建标签。为每个角色上传15张图像，并为这些图像中的每一个绘制一个边界框。仪表板现在应该类似于[图14-6](part0016.html#dashboard_with_bounding_box_and_class_na)。
- en: '![Dashboard with bounding box and class name](../images/00207.jpeg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![带有边界框和类名的仪表板](../images/00207.jpeg)'
- en: Figure 14-6\. Dashboard with bounding box and class name
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-6.带有边界框和类名的仪表板
- en: Click the Train button and let the training commence. There are sliders to control
    the probability and overlap thresholds on the dashboard. We can play around with
    them and adjust them to something we like and with which get good precision and
    recall.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“训练”按钮，让训练开始。仪表板上有滑块来控制概率和重叠阈值。我们可以尝试调整它们，使其达到我们喜欢的效果，并获得良好的精度和召回率。
- en: 'Click the Quick Test button and use any random image of the Simpsons (not previously
    used for training) to test the performance of the model. The results might not
    be great yet, but that’s okay: we can fix it by training with more data.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“快速测试”按钮，并使用辛普森家族的任意随机图像（之前未用于训练）来测试模型的性能。结果可能还不太好，但没关系：我们可以通过使用更多数据进行训练来解决这个问题。
- en: Let’s experiment with how many images we need to increase precision, recall,
    and mAP. Let’s start by adding five more images for each class and retrain the
    model.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试增加多少图像以提高精度、召回率和mAP。让我们从为每个类别添加五张图像并重新训练模型开始。
- en: Repeat this process until we have acceptable results. [Figure 14-7](part0016.html#measuring_improvement_in_percent_mean_av)
    shows the results from our experiment.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复此过程，直到我们获得可接受的结果。[图14-7](part0016.html#measuring_improvement_in_percent_mean_av)显示了我们实验的结果。
- en: '![Measuring improvement in percent mean average precision with increasing number
    of images per class](../images/00193.jpeg)'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![随着每个类别图像数量的增加，百分比平均精度改善的测量](../images/00193.jpeg)'
- en: Figure 14-7\. Measuring improvement in percent mean average precision with increasing
    number of images per class
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-7.随着每个类别图像数量的增加，百分比平均精度改善的测量
- en: With our final model, we are able to get reasonably good detections for a random
    image off the internet, as can be seen in [Figure 14-8](part0016.html#detected_simpsons_characters_with_the_fi).
    Although not expected, it is surprising that a model pretrained on natural images
    is able to fine tune on cartoons with relatively little data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的最终模型，我们能够对互联网上的随机图像进行相当好的检测，如[图14-8](part0016.html#detected_simpsons_characters_with_the_fi)所示。尽管不是预期的，但令人惊讶的是，一个在自然图像上预训练的模型能够在相对少量数据上对卡通进行微调。
- en: '![Detected Simpsons characters with the final model, represented by US congress
    members of the same first name (see note at the beginning of this section)](../images/00128.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![使用最终模型检测到的辛普森角色，由同名的美国国会议员代表（请参见本节开头的注释）](../images/00128.jpeg)'
- en: Figure 14-8\. Detected Simpsons characters with the final model, represented
    by US congress members of the same first name (see note at the beginning of this
    section)
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-8.使用最终模型检测到的辛普森角色，由同名的美国国会议员代表（请参见本节开头的注释）
- en: As we already know by now, CustomVision.ai allows this model to be exported
    in a variety of formats including Core ML, TensorFlow Lite, ONNX, and more. We
    can simply download into our desired format (*.tflite* in our case) and plug this
    file into the app similar to see how we did with the pretrained model in the previous
    section. With real-time Simpsons detection in our hands, we could show it off
    to our friends and family the next time Principal Skinner talks about steamed
    hams on our TV.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们现在已经知道的那样，CustomVision.ai允许将此模型导出为各种格式，包括Core ML、TensorFlow Lite、ONNX等。我们可以简单地下载到我们想要的格式（在我们的情况下为*.tflite*），并将此文件插入到应用程序中，类似于我们在上一节中使用预训练模型的方式。有了实时的辛普森检测，我们可以在下次斯金纳校长在电视上谈论蒸汽火腿时向朋友和家人展示。
- en: Note
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: CustomVision.ai is not the only platform that allows you to label, train, and
    deploy online without writing any code. Google’s Cloud AutoML (in beta as of October
    2019) and Apple’s Create ML (only for the Apple ecosystem) offer a very similar
    feature set. Additionally, Matroid allows you to build custom object detectors
    from video feeds, which can be extremely useful to train a network without spending
    too much effort on building a dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: CustomVision.ai并不是唯一一个允许您在不编写任何代码的情况下在线标记、训练和部署的平台。Google的Cloud AutoML（截至2019年10月仍处于测试阶段）和Apple的Create
    ML（仅适用于Apple生态系统）提供非常相似的功能集。此外，Matroid允许您从视频源构建自定义对象检测器，这对于训练网络而不需要花费太多精力来构建数据集非常有用。
- en: So far, we have looked at three quick ways to get object detection running with
    a very minimal amount of code. We estimate that about 90% of use cases can be
    handled with one of these options. If you can solve for your scenario using these,
    you can stop reading this chapter and jump straight to the “Case Studies” section.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过三种快速的方法来运行目标检测，只需很少的代码。我们估计大约90%的用例可以通过这些选项之一来处理。如果您可以使用这些解决您的情况，您可以停止阅读本章，直接跳转到“案例研究”部分。
- en: In a very small set of scenarios, we might want further fine-grained control
    on factors such as accuracy, speech, model size, and resource usage. In the upcoming
    sections, we look at the world of object detection in a little more detail, from
    labeling the data all the way to deploying the model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在极少数情况下，我们可能希望对准确性、语音、模型大小和资源使用等因素进行进一步细化控制。在接下来的章节中，我们将更详细地了解目标检测的世界，从标记数据一直到部署模型。
- en: The Evolution of Object Detection
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测的演变
- en: Over the years, as the deep learning revolution happened, there was a renaissance
    not only for classification, but also for other computer-vision tasks, including
    object detection. During this time, several architectures were proposed for object
    detection, often building on top of each other. [Figure 14-9](part0016.html#a_timeline_of_different_object_detection)
    shows a timeline of some of them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，随着深度学习革命的发生，不仅分类，还有其他计算机视觉任务，包括目标检测，都经历了一次复兴。在这段时间里，提出了几种用于目标检测的架构，通常是在其他架构的基础上构建的。[图14-9](part0016.html#a_timeline_of_different_object_detection)展示了其中一些的时间轴。
- en: '![A timeline of different object detection architectures (image source: Recent
    Advances in Deep Learning for Object Detection by Xiongwei Wu et al.)](../images/00059.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![不同目标检测架构的时间轴（图片来源：吴雄伟等人的“深度学习目标检测的最新进展”）](../images/00059.jpeg)'
- en: 'Figure 14-9\. A timeline of different object detection architectures (image
    source: Recent Advances in Deep Learning for Object Detection by Xiongwei Wu et
    al.)'
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-9\. 不同目标检测架构的时间轴（图片来源：吴雄伟等人的“深度学习目标检测的最新进展”）
- en: In object classification, our CNN architecture extracts the features from an
    image and computes the probabilities for a specific number of classes. These CNN
    architectures (ResNet, MobileNet) work as the *backbone* for object detection
    networks. In object detection networks, our end result is bounding boxes (defined
    by the center of rectangle, height, width). Although covering the inner details
    of many of these will probably need significantly more in-depth exploration (which
    you can find on this book’s GitHub website, linked at [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)),
    we can broadly divide them into two categories, as shown in [Table 14-3](part0016.html#categories_of_object_detectors).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标分类中，我们的CNN架构从图像中提取特征并计算特定数量类别的概率。这些CNN架构（ResNet，MobileNet）作为目标检测网络的*骨干*。在目标检测网络中，我们的最终结果是边界框（由矩形中心，高度，宽度定义）。尽管涵盖许多这些内部细节可能需要更深入的探索（您可以在本书的GitHub网站上找到，链接在[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)），但我们可以广泛地将它们分为两类，如[表14-3](part0016.html#categories_of_object_detectors)所示。
- en: Table 14-3\. Categories of object detectors
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-3\. 目标检测器的类别
- en: '|  | **Description** | **Pros and cons** |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | **描述** | **优缺点** |'
- en: '| --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Two-stage detectors** | Generate category-agnostic bounding box candidates
    (regions of interest) using a Region Proposal Network (RPN).Run a CNN on these
    region proposals to assign each a category.Examples: Faster R-CNN, Mask R-CNN.
    | + High Accuracy– Slower |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **两阶段检测器** | 使用区域建议网络（RPN）生成类别不可知的边界框候选区域（感兴趣区域）。在这些区域提议上运行CNN以为每个分配一个类别。示例：Faster
    R-CNN，Mask R-CNN。 | + 高精度- 较慢 |'
- en: '| **One-stage detectors** | Directly make categorical predictions on objects
    on each location of the feature maps; can be trained end-to-end training.Examples:
    YOLO, Single-Shot Detector (SSD). | + Speed, better suited for real-time applications–
    Lower accuracy |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **一阶段检测器** | 直接在特征图的每个位置上对对象进行分类预测；可以进行端到端训练。示例：YOLO，Single-Shot Detector
    (SSD)。 | + 速度快，更适合实时应用- 精度较低 |'
- en: Note
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the world of object detection, Ross Girshik is a name you will encounter
    often while reading papers. Working before the deep learning era as well as during
    it, his work includes Deformable Part Models (DPM), R-CNN, Fast R-CNN, Faster
    R-CNN, You Only Look Once (YOLO), Mask R-CNN, Feature Pyramid Network (FPN), RetinaNet,
    and ResNeXt to name just a few, often breaking his own records year after year
    on public benchmarks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测领域，Ross Girshik是一个你在阅读论文时经常会遇到的名字。他在深度学习时代之前和之后都有作品，包括可变形部件模型（DPM），R-CNN，Fast
    R-CNN，Faster R-CNN，You Only Look Once（YOLO），Mask R-CNN，Feature Pyramid Network（FPN），RetinaNet和ResNeXt等等，经常在公共基准上年复一年地打破自己的记录。
- en: I participated in several first-place entries into the PASCAL VOC object detection
    challenge, and was awarded a “lifetime achievement” prize for my work on deformable
    part models. I think this refers to the lifetime of the PASCAL challenge—and not
    mine!
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我参与了几次PASCAL VOC目标检测挑战的第一名入选，并因我在可变形部件模型上的工作而获得了“终身成就”奖。我认为这指的是PASCAL挑战的终身，而不是我的！
- en: ''
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Ross Girschik
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —Ross Girschik
- en: Performance Considerations
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能考虑
- en: From a practitioner’s point of view, the primary concerns tend to be accuracy
    and speed. These two factors usually have an inverse relationship with each other.
    We want to find the detector that strikes the ideal balance for our scenario.
    [Table 14-4](part0016.html#speed_versus_percent_mean_average_precis) summarizes
    some of those readily available pretrained models on the TensorFlow object detection
    model zoo. The speed reported was on a 600x600 pixel resolution image input, processed
    on an NVIDIA GeForce GTX TITAN X card.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从从业者的角度来看，主要关注点往往是精度和速度。这两个因素通常相互呈反比关系。我们希望找到在我们的场景中达到理想平衡的检测器。[表14-4](part0016.html#speed_versus_percent_mean_average_precis)总结了一些在TensorFlow目标检测模型库中可用的预训练模型。报告的速度是在一个600x600像素分辨率的图像输入上，使用NVIDIA
    GeForce GTX TITAN X卡进行处理的。
- en: Table 14-4\. Speed versus percent mean average precision for some readily available
    architectures on the TensorFlow object detection model zoo website
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-4\. TensorFlow目标检测模型库网站上一些可用架构的速度与平均精度百分比
- en: '|  | **Inference speed (ms)** | **% mAP on MS COCO** |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | **推理速度（毫秒）** | **在MS COCO上的mAP百分比** |'
- en: '| --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **ssd_mobilenet_v1_coco** | 30 | 21 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **ssd_mobilenet_v1_coco** | 30 | 21 |'
- en: '| **ssd_mobilenet_v2_coco** | 31 | 22 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **ssd_mobilenet_v2_coco** | 31 | 22 |'
- en: '| **ssdlite_mobilenet_v2_coco** | 27 | 22 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **ssdlite_mobilenet_v2_coco** | 27 | 22 |'
- en: '| **ssd_resnet_50_fpn_coco** | 76 | 35 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **ssd_resnet_50_fpn_coco** | 76 | 35 |'
- en: '| **faster_rcnn_nas** | 1,833 | 43 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **faster_rcnn_nas** | 1,833 | 43 |'
- en: 'A 2017 study done by Google (see [Figure 14-10](part0016.html#the_effect_of_object_detection_architect))
    revealed the following key findings:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在2017年进行的一项研究（参见[图14-10](part0016.html#the_effect_of_object_detection_architect)）揭示了以下关键发现：
- en: One-stage detectors are faster than two-stage, though less accurate.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一阶段检测器比两阶段检测器更快，但精度较低。
- en: The higher the accuracy of the backbone CNN architecture on classification tasks
    (such as ImageNet), the higher the precision of the object detector.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 骨干CNN架构在分类任务（如ImageNet）上的准确性越高，对象检测器的精度也越高。
- en: Small-sized objects are challenging for object detectors, which give poor performance
    (usually under 10% mAP). The effect is more severe on one-stage detectors.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小尺寸物体对对象检测器具有挑战性，表现较差（通常在10% mAP以下）。这种影响在单阶段检测器上更为严重。
- en: On large-sized objects, most detectors give similar performance.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大尺寸物体上，大多数检测器表现相似。
- en: Higher-resolution images give better results because small objects appear larger
    to the network.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高分辨率的图像会产生更好的结果，因为小物体在网络中看起来更大。
- en: Two-stage object detectors internally generate a large number of proposals to
    be considered as the potential locations of objects. The number of proposals can
    be tuned. Decreasing the number of proposals can lead to speedups with little
    loss in accuracy (the threshold will depend on the use case).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两阶段对象检测器内部生成大量建议，用作对象的潜在位置。建议数量可以进行调整。减少建议数量可以在准确性略微降低的情况下加快速度（阈值将取决于使用情况）。
- en: '![The effect of object detection architecture as well as the backbone architecture
    (feature extractor) on the percent mean average precision and prediction time
    (note that the colors might not be clear in grayscale printing; refer to the book’s
    GitHub for the color image) (image source: Speed/accuracy trade-offs for modern
    convolutional object detectors by Jonathan Huang et al.)](../images/00044.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: 对象检测架构以及骨干架构（特征提取器）对平均精度和预测时间的影响（请注意，在灰度打印中颜色可能不清晰；请参考书籍的GitHub获取彩色图像）（图片来源：Jonathan
    Huang等人的现代卷积对象检测器速度/准确性权衡）
- en: 'Figure 14-10\. The effect of object detection architecture as well as the backbone
    architecture (feature extractor) on the percent mean average precision and prediction
    time (note that the colors might not be clear in grayscale printing; refer to
    the book’s GitHub website [see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)]
    for the color image) (image source: Speed/accuracy trade-offs for modern convolutional
    object detectors by Jonathan Huang et al.)'
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-10。对象检测架构以及骨干架构（特征提取器）对平均精度和预测时间的影响（请注意，在灰度打印中颜色可能不清晰；请参考书籍的GitHub网站[参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)]获取彩色图像）（图片来源：Jonathan
    Huang等人的现代卷积对象检测器速度/准确性权衡）
- en: Key Terms in Object Detection
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对象检测中的关键术语
- en: The terms used heavily in object detection include Intersection over Union,
    Mean Average Precision, Non-Maximum Suppression, and Anchor. Let’s examine what
    each of these means.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测中经常使用的术语包括交并比、平均精度、非极大值抑制和锚点。让我们逐个解释这些术语的含义。
