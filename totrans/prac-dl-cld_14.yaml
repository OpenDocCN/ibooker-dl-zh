- en: Chapter 14\. Building the Purrfect Cat Locator App with TensorFlow Object Detection
    API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bob gets frequent visits from a neighborhood feral cat. Those visits result
    in less than pleasant outcomes. You see, Bob has a nice fairly large garden that
    he tends to with great care. However, this furry little fella visits his garden
    every night and starts chewing on a bunch of plants. Months of hard work gets
    destroyed overnight. Clearly unhappy with this situation, Bob is keen on doing
    something about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Channeling his inner Ace Ventura (Pet Detective), he tries to stay awake at
    night to drive the cat away, but clearly, that’s not sustainable in the long term.
    After all, Red Bull has its limits. Reaching a breaking point, he decides to use
    the nuclear option: use AI in conjunction with his sprinkler system to literally
    “turn the hose” on the cat.'
  prefs: []
  type: TYPE_NORMAL
- en: In his sprawling garden, he sets up a camera that can track the motion of the
    cat and turn on the nearest set of sprinklers to scare the cat away. He has an
    old phone lying around, and all he needs is a way to determine the cat’s location
    in real time, so he can control accordingly which sprinkler to trigger.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, after a few unwelcome baths, as demonstrated in [Figure 14-1](part0016.html#building_an_ai_cat_sprinkler_systemcomma),
    the cat would be disincentivized from raiding Bob’s garden.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an AI cat sprinkler system, like this Havahart Spray Away Motion
    Detector](../images/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-1\. Building an AI cat sprinkler system, like this Havahart Spray
    Away Motion Detector
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keen observers might have noticed that we’re attempting to find the position
    of the cat in 2D space, whereas the cat itself exists in 3D space. We can make
    assumptions to simplify the problem of locating the cat in real-world coordinates
    by assuming that the camera will always be in a fixed position. To determine the
    distance of the cat, we can use the size of an average cat to take measurements
    of its apparent size on a picture at regular intervals of distance from the camera.
    For example, an average cat might appear to have a bounding box height of 300
    pixels at a distance of two feet from the camera, whereas it might occupy 250
    pixels when it’s three feet away from the camera lens.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we help Bob make his cat detector. (Note to animal-rights
    advocates: no cats were harmed in the making of this chapter.) We answer the following
    questions along the way:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the different kinds of computer-vision tasks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I reuse a model pretrained for an existing class of objects?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I train an object detector model without writing any code?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I want more granular control for greater accuracy and speed. How do I train
    a custom object detector?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of Computer-Vision Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most of the previous chapters, we’ve essentially looked at one kind of problem:
    object classification. In that, we found out whether an image contained objects
    of a certain class. In Bob’s scenario, in addition to knowing whether the camera
    is seeing a cat, it is necessary to know exactly where the cat is in order to
    trigger the nearest sprinkler. Determining the location of an object within an
    image is a different type of problem: *object detection*. Before we jump into
    object detection, let’s look at the variety of frequent computer-vision tasks
    and the questions they purport to answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have looked at several examples of classification throughout this book.
    Simply put, the task of classification assigns an image with the class(es) of
    the object(s) present in the image. It answers the question: “Is there an object
    of class X in the image?” Classification is one of the most fundamental computer-vision
    tasks. In an image, there might be objects of more than one class—called multiclass
    classification. The label in this case for one image would be a list of all object
    classes that the image contains.'
  prefs: []
  type: TYPE_NORMAL
- en: Localization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The downside of classification is that it does not tell us where in the image
    a particular object is, how big or small it is, or how many objects there are.
    It only tells us that an object of a particular class exists somewhere within
    it. *Localization*, otherwise known as *classification with localization*, can
    inform us which classes are in an image and where they are located within the
    image. The keyword here is *classes*, as opposed to individual objects, because
    localization gives us only one bounding box per class. Just like classification,
    it cannot answer the question “How many objects of class X are in this image?”
  prefs: []
  type: TYPE_NORMAL
- en: As you will see momentarily, localization will work correctly only when it is
    guaranteed that there is a single instance of each class. If there is more than
    one instance of a class, one bounding box might encompass some or all objects
    of that class (depending upon the probabilities). But just one bounding box per
    class seems rather restrictive, doesn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we have multiple objects belonging to multiple classes in the same image,
    localization will not suffice. Object detection would give us a bounding rectangle
    for every instance of a class, for all classes. It also informs us as to the class
    of the object within each of the detected bounding boxes. For example, in the
    case of a self-driving car, object detection would return one bounding box per
    car, person, lamp post, and so on that the car sees. Because of this ability,
    we also can use it in applications that need to count. For example, counting the
    number of people in a crowd.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike localization, detection does not have a restriction on the number of
    instances of a class in an image. This is why it is used in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Often people use the term “object localization” when, really, they mean “object
    detection.” It’s important to note the distinction between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object segmentation is the task of assigning a class label to individual pixels
    throughout an image. A children’s coloring book where we use crayons to color
    various objects is a real-world analogy. Considering each pixel in the image is
    being assigned a class, it is a highly compute-intensive task. Although object
    localization and detection give bounding boxes, segmentation produces groups of
    pixels—also known as *masks*. Segmentation is obviously much more precise about
    the boundaries of an object compared to detection. For example, a cosmetic app
    that changes a user’s hair color in real-time would use segmentation. Based on
    the types of masks, segmentation comes in different flavors.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given an image, *semantic segmentation* assigns one mask per class. If there
    are multiple objects of the same class, all of them are assigned to the same mask.
    There is no differentiation between the instances of the same class. Just like
    localization, semantic segmentation cannot count.
  prefs: []
  type: TYPE_NORMAL
- en: Instance-level segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given an image, *instance-level segmentation* identifies the area occupied by
    each instance of each class. Different instances of the same class will be segmented
    uniquely.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 14-1](part0016.html#types_of_computer-vision_tasks_illustrat) lists
    the different computer-vision tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-1\. Types of computer-vision tasks illustrated using image ID [120524](https://oreil.ly/ieJ2d)
    from the MS COCO dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Image** | **In lay terms** | **Outputs** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Object classification | ![](../images/00118.jpeg) | Is there a sheep in this
    image? | Class probabilities |'
  prefs: []
  type: TYPE_TB
- en: '| Object localization | ![](../images/00015.jpeg) | Is there “a” sheep in the
    image and where is it? | Bounding box and class probability |'
  prefs: []
  type: TYPE_TB
- en: '| Objectdetection | ![](../images/00147.jpeg) | What and where are “all” the
    objects in this image? | Bounding boxes, class probabilities, class ID prediction
    |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic segmentation | ![](../images/00325.jpeg) | Which pixels in this
    image belong to different classes; e.g., the class “sheep,” “dog,” “person”? |
    One mask per class |'
  prefs: []
  type: TYPE_TB
- en: '| Instance-level segmentation | ![](../images/00064.jpeg) | Which pixels in
    this image belong to each instance of each class; e.g., “sheep,” “dog,” “person”?
    | One mask per instance of each class |'
  prefs: []
  type: TYPE_TB
- en: Approaches to Object Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the scenario, your needs, and technical know-how, there are a few
    ways to go about getting the object detection functionality in your applications.
    [Table 14-2](part0016.html#different_approaches_to_object_detection) takes a look
    at some approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-2\. Different approaches to object detection and their trade-offs
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Custom classes** | **Effort** | **Cloud or local** | **Pros and cons**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Cloud-based object detection APIs | No | <5 minutes | Cloud only | + Thousands
    of classes+ State of the art+ Fast setup+ Scalable API– No customization– Latency
    due to network |'
  prefs: []
  type: TYPE_TB
- en: '| Pretrained model | No | <15 minutes | Local | + ~100 classes+ Choice of model
    to fit speed and accuracy needs+ Can run on the edge– No customization |'
  prefs: []
  type: TYPE_TB
- en: '| Cloud-based model training | Yes | <20 minutes | Cloud and local | + GUI-based,
    no coding for training+ Customizability+ Choice between powerful model (for cloud)
    and efficient model (for edge)+ Scalable API– Uses transfer learning, might not
    allow full-model retraining |'
  prefs: []
  type: TYPE_TB
- en: '| Custom training a model | Yes | 4 hours to 2 days | Local | + Highly customizable+
    Largest variety of models available for different speed and accuracy requirements–
    Time consuming and highly involved |'
  prefs: []
  type: TYPE_TB
- en: It’s worth noting that the options marked “Cloud” are already engineered for
    scalability. At the same time, the options marked as being available for “Local”
    inferences can also be deployed on the cloud. To achieve high scale, we can host
    them on the cloud similar to how we did so for the classification models in [Chapter 9](part0011.html#AFM63-13fa565533764549a6f0ab7f11eed62b).
  prefs: []
  type: TYPE_NORMAL
- en: Invoking Prebuilt Cloud-Based Object Detection APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we already saw in [Chapter 8](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b),
    calling cloud-based APIs is relatively straightforward. The process involves setting
    up an account, getting an API key, reading through the documentation, and writing
    a REST API client. To make this process easier, many of these services provide
    Python-based (and other language) SDKs. The main cloud-based object detection
    API providers are Google’s Vision AI ([Figure 14-2](part0016.html#running_a_familiar_photo_on_googleapostr))
    and Microsoft’s Cognitive Services.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantages of using cloud-based APIs are that they are scalable out
    of the box and they support recognition of thousands of classes of objects. These
    cloud giants built their models using large proprietary datasets that are constantly
    growing, resulting in a very rich taxonomy. Considering that the number of classes
    in the largest public datasets with object detection labels is usually only a
    few hundred, there exists no nonproprietary solution out there that is capable
    of detecting thousands of classes.
  prefs: []
  type: TYPE_NORMAL
- en: The main limitation of cloud-based APIs, of course, is the latency due to network
    requests. It’s not possible to power real-time experiences with this kind of lag.
    In the next section, we look at how to power real-time experiences without any
    data or training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Running a familiar photo on Google’s Vision AI API to obtain object detection
    results](../images/00328.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-2\. Running a familiar photo on Google’s Vision AI API to obtain object
    detection results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reusing a Pretrained Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we explore how easy it is to get an object detector running
    on a mobile phone with a pretrained model. Hopefully, you are already comfortable
    with running a neural network on mobile from some of the previous chapters. The
    code to run object detection models on iOS and Android is referenced on the book’s
    GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-14*. Changing functionality is simply a matter of replacing the
    existing *.tflite* model file with a new one.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we often used MobileNet to do classification tasks.
    Although the MobileNet family, including MobileNetV2 (2018) and MobileNetV3 (2019)
    by itself are classification networks only, they can act as a backbone for object
    detection architectures like SSD MobileNetV2, which we use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The best way to uncover the magic behind the scenes is to take a deeper look
    at a model. We’d need to get our hands on the [TensorFlow Models repository](https://oreil.ly/9CsHM),
    which features models for more than 50 deep learning tasks, including audio classification,
    text summarization, and object detection. This repository contains models as well
    as utility scripts that we use throughout this chapter. Without further ado, let’s
    clone the repository on our machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we update the `PYTHONPATH` to make all scripts visible to Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we copy over the Protocol Buffer (protobuf) compiler command script from
    our GitHub repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    into our current directory to make the .`proto` files available to Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we run the *setup.py* script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now is the time to download our prebuilt model. In our case, we’re using the
    SSD MobileNetV2 model. You can find a large list of models at TensorFlow’s [object
    detection model zoo](https://oreil.ly/FwSlM), featuring models by the datasets
    on which they were trained, speed of inference, and Mean Average Precision (mAP).
    Within that list, download the model titled `ssd_mobilenet_v2_coco`, which, as
    the name suggests, was trained on the MS COCO dataset. Benchmarked at 22 mAP,
    this model runs at 31 ms on an NVIDIA GeForce GTX TITAN X with a 600x600 resolution
    input. After downloading that model, we unzip it into the *models/research/object_detection*
    directory inside the TensorFlow repository. We can inspect the contents of the
    directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Test Driving Our Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we plug our model into a mobile app, it’s a good idea to verify whether
    the model is able to make predictions. The TensorFlow Models repository contains
    a Jupyter Notebook where we can simply plug in a photograph to make predictions
    on it. You can find the notebook at *models/research/object_detection/object_detection_tutorial.ipynb*.
    [Figure 14-3](part0016.html#object_detection_prediction_in_the_ready) shows a
    prediction from that notebook on a familiar photograph.
  prefs: []
  type: TYPE_NORMAL
- en: '![Object detection prediction in the ready-to-use Jupyter Notebook from the
    TensorFlow Models repository](../images/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-3\. Object detection prediction in the ready-to-use Jupyter Notebook
    from the TensorFlow Models repository
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deploying to a Device
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve verified that the model works, it’s time to convert it into
    a mobile-friendly format. For conversion to the TensorFlow Lite format, we use
    our familiar `tflite_convert` tool from [Chapter 13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b).
    It should be noted that this tool operates on *.pb* files, whereas the TensorFlow
    Object Detection model zoo provides only model checkpoints. So, we first need
    to generate a *.pb* model from the checkpoints and graphs. Let’s do that using
    the handy script that comes with the TensorFlow Models repository. You can find
    the *export_tflite_ssd_graph.py* file in *models/research/object_detection*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If the preceding script execution was successful, we’ll see the following files
    in the *tflite_model* directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We will convert this to the TensorFlow Lite format using the `tflite_convert`
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now the only thing that remains is to plug the model into an app. The app that
    we’ll be using is referenced in the book’s GitHub website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-14*. We have already looked at how to swap a model in an app
    in Chapters [11](part0013.html#CCNA3-13fa565533764549a6f0ab7f11eed62b), [12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b),
    and [13](part0015.html#E9OE3-13fa565533764549a6f0ab7f11eed62b), so we won’t discuss
    that here again. [Figure 14-4](part0016.html#running_a_real-time_object_detector_mode)
    shows the result of the object detector model when plugged into the Android app.
  prefs: []
  type: TYPE_NORMAL
- en: '![Running a real-time object detector model on an Android device](../images/00287.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-4\. Running a real-time object detector model on an Android device
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The reason why we’re able to see the “Cat” prediction right away is because
    ‘Cat’ is one of the 80 categories already present in the MS COCO dataset that
    was used to train the model. This model might be sufficient for Bob to deploy
    on his devices (keeping in mind that although Bob might not use mobile phones
    to track his garden, the process for deploying a TensorFlow Lite model to edge
    hardware is quite similar). However, Bob would do well for himself to improve
    the precision of his model by fine tuning it on data that has been generated within
    his garden. In the next section, we explore how to use transfer learning to build
    an object detector by using only a web-based tool. If you’ve read [Chapter 8](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b),
    what comes next should feel familiar.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Custom Detector Without Any Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My cat’s breath smells like cat food!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ralph Wiggum
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s face it. Your authors could go around and snap a lot of cat pictures within
    their gardens to perform this experiment. It would be tedious, and we might be
    rewarded with only a few extra percentage points of precision and recall. Or we
    could look at a really fun example that also tests the limits of CNNs. We are,
    of course, referring to *The Simpsons*. Given that most models are not trained
    with features from cartoon images, it would be interesting to see how our model
    performs here.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to get our hands on a dataset. Thankfully, we have one readily
    available on [Kaggle](https://oreil.ly/-3wvj). Let’s download the dataset and
    use CustomVision.ai (similar to [Chapter 8](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b))
    to train our Simpsons Classifier using the following steps. *Excellent!*
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Contrary to what some may believe, we, the authors, do not have millions of
    dollars lying around. This means that we cannot afford to buy the rights to *The
    Simpsons* from Fox. Copyright laws, as a consequence, prevent us from publishing
    any images from that show. Instead, in this section, we are using the next best
    thing—public domain images from the United States Congress. In place of the images
    from the cartoon, we use the photographs of congresswomen and congressmen with
    the same first names as the Simpsons family: *Homer* Hoch, *Marge* Roukema, *Bart*
    Gordon, and *Lisa* Blunt Rochester. Keep in mind that this is only for publishing
    purposes; we have still trained on the original dataset from the cartoon.'
  prefs: []
  type: TYPE_NORMAL
- en: Go to CustomVision.ai and start a new Object Detection Project ([Figure 14-5](part0016.html#creating_a_new_object_detection_project)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Creating a new Object Detection project in CustomVision.ai](../images/00154.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 14-5\. Creating a new Object Detection project in CustomVision.ai
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Create tags for Homer, Marge, Bart, and Lisa. Upload 15 images for each character
    and draw a bounding box for each of those images. The dashboard should resemble
    [Figure 14-6](part0016.html#dashboard_with_bounding_box_and_class_na) right about
    now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Dashboard with bounding box and class name](../images/00207.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 14-6\. Dashboard with bounding box and class name
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Click the Train button and let the training commence. There are sliders to control
    the probability and overlap thresholds on the dashboard. We can play around with
    them and adjust them to something we like and with which get good precision and
    recall.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the Quick Test button and use any random image of the Simpsons (not previously
    used for training) to test the performance of the model. The results might not
    be great yet, but that’s okay: we can fix it by training with more data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s experiment with how many images we need to increase precision, recall,
    and mAP. Let’s start by adding five more images for each class and retrain the
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this process until we have acceptable results. [Figure 14-7](part0016.html#measuring_improvement_in_percent_mean_av)
    shows the results from our experiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Measuring improvement in percent mean average precision with increasing number
    of images per class](../images/00193.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 14-7\. Measuring improvement in percent mean average precision with increasing
    number of images per class
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: With our final model, we are able to get reasonably good detections for a random
    image off the internet, as can be seen in [Figure 14-8](part0016.html#detected_simpsons_characters_with_the_fi).
    Although not expected, it is surprising that a model pretrained on natural images
    is able to fine tune on cartoons with relatively little data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Detected Simpsons characters with the final model, represented by US congress
    members of the same first name (see note at the beginning of this section)](../images/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-8\. Detected Simpsons characters with the final model, represented
    by US congress members of the same first name (see note at the beginning of this
    section)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we already know by now, CustomVision.ai allows this model to be exported
    in a variety of formats including Core ML, TensorFlow Lite, ONNX, and more. We
    can simply download into our desired format (*.tflite* in our case) and plug this
    file into the app similar to see how we did with the pretrained model in the previous
    section. With real-time Simpsons detection in our hands, we could show it off
    to our friends and family the next time Principal Skinner talks about steamed
    hams on our TV.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: CustomVision.ai is not the only platform that allows you to label, train, and
    deploy online without writing any code. Google’s Cloud AutoML (in beta as of October
    2019) and Apple’s Create ML (only for the Apple ecosystem) offer a very similar
    feature set. Additionally, Matroid allows you to build custom object detectors
    from video feeds, which can be extremely useful to train a network without spending
    too much effort on building a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have looked at three quick ways to get object detection running with
    a very minimal amount of code. We estimate that about 90% of use cases can be
    handled with one of these options. If you can solve for your scenario using these,
    you can stop reading this chapter and jump straight to the “Case Studies” section.
  prefs: []
  type: TYPE_NORMAL
- en: In a very small set of scenarios, we might want further fine-grained control
    on factors such as accuracy, speech, model size, and resource usage. In the upcoming
    sections, we look at the world of object detection in a little more detail, from
    labeling the data all the way to deploying the model.
  prefs: []
  type: TYPE_NORMAL
- en: The Evolution of Object Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the years, as the deep learning revolution happened, there was a renaissance
    not only for classification, but also for other computer-vision tasks, including
    object detection. During this time, several architectures were proposed for object
    detection, often building on top of each other. [Figure 14-9](part0016.html#a_timeline_of_different_object_detection)
    shows a timeline of some of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![A timeline of different object detection architectures (image source: Recent
    Advances in Deep Learning for Object Detection by Xiongwei Wu et al.)](../images/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-9\. A timeline of different object detection architectures (image
    source: Recent Advances in Deep Learning for Object Detection by Xiongwei Wu et
    al.)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In object classification, our CNN architecture extracts the features from an
    image and computes the probabilities for a specific number of classes. These CNN
    architectures (ResNet, MobileNet) work as the *backbone* for object detection
    networks. In object detection networks, our end result is bounding boxes (defined
    by the center of rectangle, height, width). Although covering the inner details
    of many of these will probably need significantly more in-depth exploration (which
    you can find on this book’s GitHub website, linked at [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)),
    we can broadly divide them into two categories, as shown in [Table 14-3](part0016.html#categories_of_object_detectors).
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-3\. Categories of object detectors
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Description** | **Pros and cons** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Two-stage detectors** | Generate category-agnostic bounding box candidates
    (regions of interest) using a Region Proposal Network (RPN).Run a CNN on these
    region proposals to assign each a category.Examples: Faster R-CNN, Mask R-CNN.
    | + High Accuracy– Slower |'
  prefs: []
  type: TYPE_TB
- en: '| **One-stage detectors** | Directly make categorical predictions on objects
    on each location of the feature maps; can be trained end-to-end training.Examples:
    YOLO, Single-Shot Detector (SSD). | + Speed, better suited for real-time applications–
    Lower accuracy |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the world of object detection, Ross Girshik is a name you will encounter
    often while reading papers. Working before the deep learning era as well as during
    it, his work includes Deformable Part Models (DPM), R-CNN, Fast R-CNN, Faster
    R-CNN, You Only Look Once (YOLO), Mask R-CNN, Feature Pyramid Network (FPN), RetinaNet,
    and ResNeXt to name just a few, often breaking his own records year after year
    on public benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: I participated in several first-place entries into the PASCAL VOC object detection
    challenge, and was awarded a “lifetime achievement” prize for my work on deformable
    part models. I think this refers to the lifetime of the PASCAL challenge—and not
    mine!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Ross Girschik
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Performance Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a practitioner’s point of view, the primary concerns tend to be accuracy
    and speed. These two factors usually have an inverse relationship with each other.
    We want to find the detector that strikes the ideal balance for our scenario.
    [Table 14-4](part0016.html#speed_versus_percent_mean_average_precis) summarizes
    some of those readily available pretrained models on the TensorFlow object detection
    model zoo. The speed reported was on a 600x600 pixel resolution image input, processed
    on an NVIDIA GeForce GTX TITAN X card.
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-4\. Speed versus percent mean average precision for some readily available
    architectures on the TensorFlow object detection model zoo website
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Inference speed (ms)** | **% mAP on MS COCO** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **ssd_mobilenet_v1_coco** | 30 | 21 |'
  prefs: []
  type: TYPE_TB
- en: '| **ssd_mobilenet_v2_coco** | 31 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| **ssdlite_mobilenet_v2_coco** | 27 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| **ssd_resnet_50_fpn_coco** | 76 | 35 |'
  prefs: []
  type: TYPE_TB
- en: '| **faster_rcnn_nas** | 1,833 | 43 |'
  prefs: []
  type: TYPE_TB
- en: 'A 2017 study done by Google (see [Figure 14-10](part0016.html#the_effect_of_object_detection_architect))
    revealed the following key findings:'
  prefs: []
  type: TYPE_NORMAL
- en: One-stage detectors are faster than two-stage, though less accurate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The higher the accuracy of the backbone CNN architecture on classification tasks
    (such as ImageNet), the higher the precision of the object detector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small-sized objects are challenging for object detectors, which give poor performance
    (usually under 10% mAP). The effect is more severe on one-stage detectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On large-sized objects, most detectors give similar performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher-resolution images give better results because small objects appear larger
    to the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two-stage object detectors internally generate a large number of proposals to
    be considered as the potential locations of objects. The number of proposals can
    be tuned. Decreasing the number of proposals can lead to speedups with little
    loss in accuracy (the threshold will depend on the use case).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The effect of object detection architecture as well as the backbone architecture
    (feature extractor) on the percent mean average precision and prediction time
    (note that the colors might not be clear in grayscale printing; refer to the book’s
    GitHub for the color image) (image source: Speed/accuracy trade-offs for modern
    convolutional object detectors by Jonathan Huang et al.)](../images/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-10\. The effect of object detection architecture as well as the backbone
    architecture (feature extractor) on the percent mean average precision and prediction
    time (note that the colors might not be clear in grayscale printing; refer to
    the book’s GitHub website [see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)]
    for the color image) (image source: Speed/accuracy trade-offs for modern convolutional
    object detectors by Jonathan Huang et al.)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Key Terms in Object Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The terms used heavily in object detection include Intersection over Union,
    Mean Average Precision, Non-Maximum Suppression, and Anchor. Let’s examine what
    each of these means.
  prefs: []
  type: TYPE_NORMAL
