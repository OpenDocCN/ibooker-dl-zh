<html><head></head><body><section data-pdf-bookmark="Chapter 5. Live, Die, Buy, or Try&#x2014;Much Will Be Decided by AI" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635">&#13;
<h1><span class="label">Chapter 5. </span>Live, Die, Buy, or Try—<span class="keep-together">Much Will Be Decided by AI</span></h1>&#13;
&#13;
<p>We came up with this Dr. Seuss-like chapter title<a contenteditable="false" data-primary="governance" data-type="indexterm" id="xi_governance5359"/> because we feel it perfectly captures the whimsical and ever more complex world revealing itself week by week. We admit that our title choice might be a touch over the top—or maybe it’s just right—but it’s here to catch your attention. In this chapter, we’ll offer a glimpse into the ever-evolving landscape of governance and AI: where it’s headed, what’s worth pondering, and why it matters.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It may feel like a déjà vu statement from the last chapter, but we’ll say it again—entire books could, and likely have, been written on this chapter’s topic alone. Naturally, we can’t cover every aspect here and intentionally avoided diving into the labyrinth of <span class="keep-together">AI-specific</span> regulations<a contenteditable="false" data-primary="regulation" data-secondary="constantly evolving nature of AI" data-type="indexterm" id="id707"/><a contenteditable="false" data-primary="governance" data-secondary="regulations" data-type="indexterm" id="id708"/>. Why? Because they’re vast and ever changing. The sheer volume is staggering, from cross-national agreements to country-specific laws, state or provincial rules, and even city-level policies. What’s more, by the time this book reaches you, much of it will have changed (what else is new when you’re writing an AI book). For this reason, we thought it better to give you some more tools that can guide you through navigating any regulation without getting bogged down in the ever-shifting minutiae.</p>&#13;
</div>&#13;
&#13;
<p>It’s so important, we thought it worthy to reiterate our position: we think perhaps the number one thing leaders need to decide before their journey begins (or quickly, since it’s already begun) is to declare if their company is going to be an upstander or a bystander<a contenteditable="false" data-primary="ethical principles" data-secondary="importance of taking a stand" data-type="indexterm" id="id709"/> when it comes to AI. Proactive individuals, or <em>upstanders</em>, are pioneers in ethical conduct, often setting the standard for others to follow. Conversely, <em>bystanders</em> who fail to act responsibly can inadvertently prompt overreaching regulatory action from governments, as their inaction highlights the need for oversight and control. The world saw bystanding with social media. And while it’s outside the scope of this book to delve into the good and bad of social media (there are plenty of both), governments stood still, not knowing how or what to act on until the problem was too far gone. Of course, the problem with regulating AI is that it needs to be done at the “speed of right,” but regulatory bodies tend to move at the speed of molasses.</p>&#13;
&#13;
<p>Perhaps we’ll simplify our message with a reference to the famous story of Superman. Recall that he was found as a baby on Earth by his adoptive parents, Jonathan and Martha Kent, who named him Clark; and all but a few would come to know his true identity, Superman. (That said, his parents must have suspected something since they found him at the side of the road in a crater, and he lifted a car at under the age of one.) Eventually others from his home planet of Krypton came to Earth and attempted to use similar powers to take it over. Of course, we all know he won because we are here today (kidding), but what’s the point? Raised by his adoptive parents, Superman was instilled with a strong moral compass. This upbringing guided him to utilize his extraordinary abilities in a positive way and not inflict harm on the general public or use it for nefarious purposes. In fact, it’d be fair to say that the dividing line between good and evil with Superman really came down to those core values he was taught from the beginning by his parents. Much like Superman’s moral compass, your company’s core values will significantly contribute to establishing a positive reputation and fostering trust. Think carefully about how you want to participate in this GenAI and agentic world. How will you use your superpowers?</p>&#13;
&#13;
<p>The reality is this: as large language models (LLMs) become increasingly commoditized, the distinction between providers is poised to evolve<a contenteditable="false" data-primary="security and privacy" data-secondary="data protection" data-type="indexterm" id="id710"/>. One of your differentiators will be your ability to safely and privately leverage your data to become an AI Value Creator (<a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a>). Another will be the adoption of a generative computing (interoperability, runtimes, all the things that benefit the classical computing world) approach for more value creation (<a data-type="xref" href="ch09.html#ch09_generative_computing_a_new_style_of_computing_1740182052619664">Chapter 9</a>). The topics we cover in this chapter will be the third. As a matter of fact, we think AI accuracy alone will no longer be enough. Very soon, elements like fair use, transparency, trust, algorithmic accountability, and all the topics discussed in this chapter will become part of your competitive advantage. Let’s take a closer look.</p>&#13;
&#13;
<section data-pdf-bookmark="LLMs—The Stuff People Forget to Tell You" data-type="sect1"><div class="sect1" id="ch05_llms_the_stuff_people_forget_to_tell_you_1740182048942839">&#13;
<h1>LLMs—The Stuff People Forget to Tell You</h1>&#13;
&#13;
<p>The world fell in love<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="legal issues" data-type="indexterm" id="xi_largelanguagemodelsLLMslegalissues51234"/><a contenteditable="false" data-primary="governance" data-secondary="LLM challenges" data-type="indexterm" id="xi_governanceLLMchallenges51234"/><a contenteditable="false" data-primary="copyright" data-secondary="LLMs" data-type="indexterm" id="xi_copyrightLLMs51234"/><a contenteditable="false" data-primary="legal considerations" data-secondary="copyright and other issues for LLMs" data-type="indexterm" id="xi_legalconsiderationscopyrightandotherissuesforLLMs51234"/> with GenAI when it “swiped right” (borrowing from the Tinder experience, so we’re told—none of the authors have experience here) on ChatGPT. That love at first sight created a brand-new democratized relationship with AI. But perhaps like many of those who swiped right, they found out some things they didn’t appreciate about their new “interest.” Just like a new relationship, users had lofty expectations of what their new AI interests could do for them. In the end, many wished someone would have told them up front about the good, the bad,<sup><a data-type="noteref" href="ch05.html#id711" id="id711-marker">1</a></sup> and <span class="keep-together">the LLM</span>.</p>&#13;
&#13;
<section data-pdf-bookmark="The Knowledge Cut-Off Date" data-type="sect2"><div class="sect2" id="ch05_the_knowledge_cut_off_date_1740182048942915">&#13;
<h2>The Knowledge Cut-Off Date</h2>&#13;
&#13;
<p>One thing to know about LLMs is that they can be incredibly expensive to train<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="knowledge cut-off date" data-type="indexterm" id="id712"/><a contenteditable="false" data-primary="knowledge cut-off date, LLMs" data-type="indexterm" id="id713"/>. This is why there are many techniques and ongoing research—such as InstructLab, parameter-efficient fine-tuning (PEFT), and more—to avoid full retraining. Quite simply, this means LLMs can’t be updated frequently, and therefore LLMs come with what is referred to as a <em>knowledge cut-off date</em> (the date when data collection stopped, and training started). When GPT-4 first came out, its knowledge cut-off was originally September 2021. That meant if you were using ChatGPT with this model in March 2023 and wanted to know where the New York City iconic Rockefeller Christmas tree came from, you could very likely have received the wrong information. (Know that a model’s cut-off date gets updated each time that model is updated and released.) The bottom line is that data is not natively available to a model past its training date. These few years later, techniques like retrieval-augmented generation (RAG), tool calls for web searches (heavily utilized by agents), fine-tuning techniques, and other approaches help to address these issues for some LLMs, but it’s imperative to know this is how LLMs work.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="LLMs Can Be Masters of Making It Up as They Go" data-type="sect2"><div class="sect2" id="ch05_llms_can_be_masters_of_making_it_up_as_they_go_1740182048942979">&#13;
<h2>LLMs Can Be Masters of Making It Up as They Go</h2>&#13;
&#13;
<p>Another significant challenge<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="hallucination problem" data-type="indexterm" id="xi_largelanguagemodelsLLMshallucinationproblem51943"/><a contenteditable="false" data-primary="hallucinations" data-type="indexterm" id="xi_hallucinations51943"/> that plagues LLMs is how they can fabricate information<a contenteditable="false" data-primary="data" data-secondary="trust problem for AI" data-type="indexterm" id="id714"/>. The industry refers to this as <em>hallucinating</em>. There are some emerging descriptions as to the severity of these, but to keep things simple here, assume hallucinating refers to any time an LLM makes stuff up. Some of these hallucinations are outrageous and obviously wrong, like the time one LLM claimed that Shakespeare’s first draft of <em>Hamlet</em> included a rap battle. But some are beyond believable. As you can imagine, if an unsuspecting and untrained user is making decisions based on a hallucination that to them seems to be (or is assumed to be) correct information, that can have some scary consequences. For this reason, <a data-type="xref" href="ch06.html#ch06_skills_that_thrill_1740182050334297">Chapter 6</a> gets to the very notion of understanding this LLM phenomenon being a critical part of any upskilling plan. But no matter how you classify it, getting false information and acting upon it is dangerous stuff for anyone. And there are lots of examples where this has happened.</p>&#13;
&#13;
<p>One famous example is when a legal defense team relied on evidence that cited fake case law generated by ChatGPT in their legal brief.<sup><a data-type="noteref" href="ch05.html#id715" id="id715-marker">2</a></sup> When it became clear to the judge that these citations did not exist, you can imagine how things went—the two lawyers ended up in a sanctions hearing. Conclude what you want about the team that used ChatGPT (one of them simply relied on the other and didn’t know), but it’s fair to note in their affidavit that they had screenshots of the ChatGPT conversation where the one lawyer challenged the LLM as to the veracity of the information he was receiving. The LLM not only assured this lawyer of its reliability, but it also noted, “these citations can be found in reputable legal databases such as LexisNexis and Westlaw.” That’s some convincing hallucination!</p>&#13;
&#13;
<p>That said, the hallucinated decisions were not in the format of those legal research databases it cited, and some of the previous decisions cited had listed judge names that did not line up with the courts that issued those decisions. In other words, some due diligence could have avoided this. (And now you get why we detailed some of the great education LLM use cases in <a data-type="xref" href="ch04.html#ch04_the_use_case_chapter_1740182047877425">Chapter 4</a>.) Either way, this is a perfect example of what we mean by hallucination.</p>&#13;
&#13;
<p>How did it turn out for those lawyers? The sanctions judge was not amused. They <em>both</em> got a small fine and were both compelled to write letters to their clients, the plaintiffs, and the judges they associated with fake rulings detailing the situation and what they did. Why both? The judge noted both didn’t perform due diligence, which is the takeaway here when working with GenAI. What we want to know is if they used ChatGPT to write those letters!</p>&#13;
&#13;
<p>As previously mentioned, there are patterns such as RAG and PEFT and others that can try to mitigate LLM hallucinations, and they indeed have some effect. <em>Know this: all models can hallucinate, even when you apply these patterns. </em>Your work here (covered in detail in <a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a>) is all about minimizing hallucinations and building rock-solid trust, complete with citations and clear lineage—so the GenAI and agents you use for business don’t start making up their own reality. As we always say, prompter beware!</p>&#13;
&#13;
<p>As another example, consider one airline’s bereavement fare policy. One of its customers was interacting with its website’s chatbot, asked about this policy, and was told they have a certain number of days after their trip is complete to apply for a bereavement refund. After this customer finished the trip, they applied for the refund and were denied. The airline pointed out that its bereavement fare policies were <em>clearly </em>outlined on its website (they were, we checked it out). This means the LLM hallucinated. Unsatisfied with the response, the customer took the airline to court and won. In that court’s opinion, the airline was indeed responsible for the output of the LLM, even when the airline cited in its defense that it doesn’t own the LLM. This is something you must think about when choosing your use cases. See why we told you earlier in this book to start with an internal automation use case?<a contenteditable="false" data-primary="" data-startref="xi_largelanguagemodelsLLMshallucinationproblem51943" data-type="indexterm" id="id716"/><a contenteditable="false" data-primary="" data-startref="xi_hallucinations51943" data-type="indexterm" id="id717"/> There is so much to dive into on this topic alone, but it will be become very apparent how to handle this problem by the time you get to <a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Footprints in the Carbon: The Climate Cost of Your AI BFF" data-type="sect2"><div class="sect2" id="ch05_footprints_in_the_carbon_the_climate_cost_of_your_1740182048943059">&#13;
<h2>Footprints in the Carbon: The Climate Cost of Your AI BFF</h2>&#13;
&#13;
<p>A really big problem with LLMs is the sheer amount of energy required<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="climate/energy cost" data-type="indexterm" id="id718"/><a contenteditable="false" data-primary="climate/energy cost of LLMs" data-type="indexterm" id="id719"/><a contenteditable="false" data-primary="carbon cost of LLMs" data-type="indexterm" id="id720"/> to build and inference them. This is as much a cost problem as it is an ethical one—after all, what of the carbon footprint left from the world’s thirst for AI? You’ll find models in the sizes of millions, billions, and trillions of parameters, and as you can imagine, the more parameters in a model, the more resources consumed building and running it. Think of it this way: if you needed to get from LaGuardia Airport to downtown New York City, would you walk, take a taxi, or rent an entire tour bus just for yourself? Your choice impacts cost, the environment, and more. As you’ll see later, our advice is simple—don’t overdo it.</p>&#13;
&#13;
<p>We’ll be honest, this new age AI stuff has a lot of power demands. As of right now, the world is writing energy checks it can’t cash and this is why you’re seeing a renewed focus on nuclear energy as one possible solution. For example, did you know some estimates suggest that the amount of power required for a single ChatGPT query is enough to power a light bulb for 20 minutes? Or that the power required to generate a single image from some LLMs could fully charge a cell phone? We’re not sure what the actuals are, but there are more than enough proof points to note that LLMs have large power requirements.</p>&#13;
&#13;
<p>LLMs don’t just have enormous power needs, they have enormous water needs—water<a contenteditable="false" data-primary="water for cooling, LLMs need for" data-type="indexterm" id="id721"/> is used to cool the systems that build LLMs and manage inferencing processes. In a suburb near Des Moines (Iowa) that hosts one such center, about 20% of their water supply is utilized to cool computers—this while that state is in one of the most prolonged droughts in decades—unsustainably depleting aquifers. In essence, as AI grows in size and usage, its resource consumption escalates, posing significant sustainability challenges.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Copyright and Lawsuits" data-type="sect2"><div class="sect2" id="ch05_copyright_and_lawsuits_1740182048943118">&#13;
<h2>Copyright and Lawsuits</h2>&#13;
&#13;
<p>We’re not lawyers, and when we try to read the points of views on fair use, copyright, digital rights, and other related topics, we find ourselves back to this fact: we’re not lawyers. What we will tell you is there are a lot of lawsuits going on right now for obvious reasons—practically all LLMs are built with some degree of data found posted on the internet and and collected in a process called “scraping” or “crawling.” But as you will find out, not all internet sources are created equally. What of copyright? For example, one ubiquitous dataset used in many LLMs is Books3<a contenteditable="false" data-primary="Books3 dataset, copyright issue" data-type="indexterm" id="id722"/>. This dataset has some 200,000 books whose text was illegally posted online without the original authors’ permissions. Several model providers are undergoing lawsuits right now, accused of using this data and baking it into their LLMs without permission or compensation toward the original authors. In fact, some of our books are in this dataset. And so are many more famous authors such as Stephen King (horror), Nik Sharma (cooking), Sarah Silverman (comedy), Nora Roberts (romance), and more. From <span class="keep-together">fiction</span> to prose poetry, like the Prego spaghetti sauce slogan, “It’s in there.” But some LLM upstanders blocklist this (and other) datasets, which speaks to culture. Does that approach match yours?</p>&#13;
&#13;
<p>Now for our (nonlegal) advice. First, decide what kind of actor you’re going to be. What’s your culture? How about the digital workforce you learned about in the last chapter? This is how you will unlock new productivity levels. Is the LLM that will underpin your digital workforce in alignment with your company’s values? For example, using an LLM trained on datasets like Books3 or <em>The Pirate Bay</em> (a BitTorrent site supported by an anticopyright group that posts all kinds of audio, video, software, TV programs, and games) could potentially speak to your culture. All your company’s ad copy could be sitting in the synapses of a neural network waiting to activate and help a competitor. Is that fair? Does it have to be that way? This is part of the reason we wrote <a data-type="xref" href="ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518">Chapter 8</a>.</p>&#13;
&#13;
<p>What about people who make their living and have built reputations on their incredible work<a contenteditable="false" data-primary="artistic work, rights issue for" data-type="indexterm" id="id723"/>? For example, Greg Rutkowski is renowned for his captivating <em>Dungeons &amp; Dragons</em> (D&amp;D) themed artwork. Truly his art brings to life D&amp;D’s vivid characters, immersive landscapes, and an unbridled sense of wonder. And for good reason: he has captivated fans worldwide, transporting them to a world of magic, adventure, and legendary heroes. Unfortunately, all the magical creative talent may be no match for the number correlation capabilities (remember, AI sees pictures as number patterns; it’s not magic) comprising today’s text-to-image models that have easily captured his unique style. And just like our works are part of LLMs today, you can be assured his work is part of some data training set, too. Of course there is a counter point of view. If you were an art student studying the wonders of an artist in a museum, and started painting in that style, how would things be different? Your captivation of Tom Thomson’s 1916 masterpiece <em>The Jack Pine</em> got burned into your brain, and you subsequently paint with oils that capture his layered texture, expressive movements, dramatic framing, and influence of woodblock printing. The difference, of course, is that the amount of influence a human can absorb in a lifetime is but a millisecond to an AI.</p>&#13;
&#13;
<p>In the end, lawsuits will answer the question of whether publicly available data can be legally used to train foundation models. Is this morally right or wrong? That’s for you to decide. We could envision a day where you might just be considering whether your AI was built with ethically sourced data just like you do raw materials in supply chain or labor. If you care about this, then ask your LLM provider to show you the data they used to train their model. We call<a contenteditable="false" data-primary="transparency" data-type="indexterm" id="id724"/> this <em>data transparency</em>, which is part of a tip we’ll give you later in this chapter. Some vendors will tell you they can’t produce that list; others will tell you it’s none of your business; and others will show you the provenance of the datasets used to build their model and the block list of the datasets not allowed in the training, like Books3 and <em>The Pirate Bay</em>. At the end of the day, you need to let your efforts rise to the level of intention you wish to take on this journey.</p>&#13;
&#13;
<p>Next, investigate the indemnification paper (to protect you from all the copyright lawsuits going on) that’s attached to any vendor’s model you license. While they all use the same word (indemnification), they are all written quite differently, and those differences could have significant impacts on your business, depending how things turn out. If this document isn’t lengthy and is easy to understand, you’re likely in a good place. We’ve seen some indemnification documents contain multiple external links with confusing and conflicting information. Whatever you read, ensure you fully understand what the indemnification covers and what you must do to ensure that indemnity is not nullified. From a coverage perspective, it’s important to understand if a vendor’s indemnification<a contenteditable="false" data-primary="indemnification due diligence" data-type="indexterm" id="id725"/> policy covers copyrighted material or intellectual property (IP) in general—the latter is a much broader coverage area. We’ve seen a few indemnity statements that <em>seem</em> to cover the output of a model only to be disqualified by another terms and conditions document. Get your lawyers involved so everyone has quorum on what’s covered and what’s not.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="What About Digital Essence?" data-type="sect2"><div class="sect2" id="will-i-am_section">&#13;
<h2>What About Digital Essence?</h2>&#13;
&#13;
<p>Now that you know<a contenteditable="false" data-primary="digital essence,  and future of AI" data-type="indexterm" id="ix_digessenceai_5119120"/> that to an AI, everything is just a bunch of numbers and almost everything is some kind of a numerical pattern (dance moves, writing, even a lipstick formulation), you understand how things can be created by GenAI. Picture this: Ol’ Blue Eyes himself, Frank Sinatra, slicking back his hair, snapping his fingers, and then BOOM! He’s belting out Oasis’ Wonderwall like he wrote it on the back of a cocktail napkin at the Sands. And let’s be honest: we all know he’d have nailed it, too, because that swagger wouldn’t quit. (AI has made this a <a href="https://oreil.ly/vd1az">reality</a> today.)</p>&#13;
&#13;
<p>When it comes to using AI, there are good actors and bad actors. A good actor might be someone cloning their voice and pairing it with their AI-built avatar so they can scale their work. A bad actor might use deep fakes (we cover this later in this chapter) to commit fraud, character attacks, cause confusion, and more. But, somewhere between those lines there’s something else you should think about—what about your digital essence? What about all the work that copyrighted or not, is now part of some LLM’s parameter makeup?</p>&#13;
&#13;
<p>Many likely know <a href="https://will.i.am">will.i.am</a> as a hip-hop musician<a contenteditable="false" data-primary="will.i.am" data-type="indexterm" id="id726"/>, producer, and lead singer of Black Eyed Peas. You might even know him as one of the original founders of Beats by Dre headphones (now owned by Apple). What many may not realize is that will.i.am is above all, a futurist, innovator, tech entrepreneur, and creative artist who has been in the world of AI for decades. And to prove it, just watch the first 90 seconds of the official music video for the song <a href="https://oreil.ly/jjaH_">“Imma Be Rocking That Body”</a>, which was released in 2009 and has been viewed over 100 million times. In this video, will.i.am showed exactly how an AI would be capable of creating music using the group’s voices and likenesses, and describes with incredible precision the future of AI we are living today.</p>&#13;
&#13;
<p><a href="https://oreil.ly/e_3EW">IBM and will.i.am</a> have been working together since 2009. In their collaboration, IBM teamed with him as he founded <a class="orm:hideurl" href="https://fyi.ai">FYI.AI</a>—a platform<a contenteditable="false" data-primary="FYI.AI" data-type="indexterm" id="id727"/> that integrates AI to enhance user communication and media consumption in support of the creative community. He also developed and launched Sound Drive with Mercedes-Benz, a feature now shipped standard in every new AMG car. He also created the groundbreaking radio program <em>The FYI Show</em> on SiriusXM where his cohost is an AI persona, and recently launched <em>FYI.RAiDiO</em>, the first interactive personalized radio experience powered <span class="keep-together">by AI</span>.</p>&#13;
&#13;
<p>In our interactions with him, we quickly discovered his passion for learning and his technical depth in combination with his ability to imagine the future. He fascinated us with his point of view around digital essence and the ownership of oneself and analog-to-digital rights on one’s music, which goes far beyond work that may have been “lifted” by AI. His view of digital essence provides a glimpse of the work we have to do to protect rights and identities and ensure ethical and proper use of AI, without stifling its use and innovation. We think that will.i.am’s view may be giving us a similar glimpse into the future of IP and likeness rights as he did in the 2009 video about AI.</p>&#13;
&#13;
<p>It’s out of the scope of this book to get too deep in this topic, but it certainly raises even tougher questions that challenge the very fabric of identity in the digital age. If LLM vendors can indiscriminately take people’s work and ingest it into their models, what does that mean for the output? Can someone start monetizing another person’s very essence—a digital essence (look, sound, and style)? At what point does innovation become exploitation? If we don’t take control of our digital selves now, we might wake up one day to find that our thoughts, our voices, and even our creativity have been hijacked and endlessly remixed into something we no longer recognize. Do we benefit from that? Does someone else? And as we scramble to reclaim ownership, the algorithms will just keep churning, unapologetically repeating, “Tonight’s gonna be a good night…” but somehow, we all know the original was so much better<a contenteditable="false" data-primary="" data-startref="ix_digessenceai_5119120" data-type="indexterm" id="id728"/>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
<section data-pdf-bookmark="Your Expanding Surface Area of Attack" data-type="sect2"><div class="sect2" id="ch05_your_expanding_surface_area_of_attack_1740182048943193">&#13;
<h2>Your Expanding Surface Area of Attack</h2>&#13;
&#13;
<p>The last section may seem to deviate<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="cyberattack threat" data-type="indexterm" id="xi_largelanguagemodelsLLMscyberattackthreat54188"/><a contenteditable="false" data-primary="cyberattack threat" data-type="indexterm" id="xi_cyberattackthreat54188"/><a contenteditable="false" data-primary="security and privacy" data-secondary="cyberattack threat" data-type="indexterm" id="xi_securityandprivacycyberattackthreat54188"/> from our usual upbeat tone due to the significant potential of AI we’ve previously emphasized. However, this is not intended to diminish your enthusiasm, but rather to provide a realistic viewpoint. After all, a prevalent theme throughout this book is the importance of acknowledging both AI’s remarkable potential and its inherent limitations. This balanced understanding is essential for utilizing AI responsibly and effectively. With that out of the way, it’s time to tell you that the more you put AI to work in your business, the more you expand the surface area of attack on your business, and the more attack vectors you must consider. So, while you might be using AI for “good acting,” there are certainly others using it for “bad acting.” Said another way, while AI can be employed for beneficial purposes, there are also instances where it is misused for malicious intent.</p>&#13;
&#13;
<p>As you harness the power of AI, your organization is further transforming itself into a digital business. And just as the emergence of websites in the early web era introduced a new wave of vulnerabilities, the democratization<a contenteditable="false" data-primary="democratization, AI" data-type="indexterm" id="id729"/> of AI is bringing with it a fresh set of challenges that companies must now navigate but don’t quite understand yet. What follows is a short list of threats that we think you need to be aware of.</p>&#13;
&#13;
<section data-pdf-bookmark="Data poisoning" data-type="sect3"><div class="sect3" id="ch05_data_poisoning_1740182048943250">&#13;
<h3>Data poisoning</h3>&#13;
&#13;
<p>This happens when threat actors inject malicious and corrupted data<a contenteditable="false" data-primary="data poisoning" data-type="indexterm" id="id730"/><a contenteditable="false" data-primary="poisoning of data" data-type="indexterm" id="id731"/> into the training datasets used to build LLMs. Some of these actors perceive themselves as the “gatekeepers of social justice,” defending those whose data has been “stolen” to build LLMs. Typically, these groups aren’t out to cause social harm, rather they’re trying to dilute the usefulness of an LLM or at least add friction into the creation process. We can see it now: you ask your AI-powered meal application for the perfect side dish pairing to complement your slice of cheesecake. The AI, confused by poisoned data, confidently suggests that broccolini is the ultimate side dish for cheesecake, but be sure to lightly sauté it with garlic for the full experience; all of this gives rise to the #CheesecakeBroccoliniChallenge. But here’s the thing, these mislabelings are typically invisible to the naked eye. It would take but a moment if you saw a bunch of dogs labeled as horses to save yourself the trouble and discard the dataset as junk. Data poisoning<a contenteditable="false" data-primary="Nightshade, data poisoning tool" data-type="indexterm" id="id732"/> tools like <a href="https://oreil.ly/DjAuK">Nightshade</a> help make pixel-level changes to images that are invisible to the human eye...and suddenly your cat Felix is a toaster to the AI. When you consider the thriving open source world associated with AI, you get a sense of the huge potential these datasets have to corrupt, or at least slow down, a vendor and waste their resources as they try to figure out why the model isn’t generalizing well in real-world data.</p>&#13;
&#13;
<p>You can see the aperture for such an attack to become malicious and scary. Imagine a bad actor social engineering a dataset to facilitate misdiagnoses of medical conditions. For example, in the domain of computer vision for skin cancer detection, AI tends to perform worse (we’re talking double-digit percent worse) on dark skin tones compared to light skin tones. In a quest for data, imagine a research team stumbling across a “poisoned” dataset maliciously mislabeling benign and malignant moles for dark-skin-toned patients for which data is scarce. Beyond the obvious potentially devastating consequences, this attack could create a social loop bias and further erode the trust and potential for AI to help in this domain. Considering that melanoma skin cancers have been on a year-over-year rise for 30 years, and even if every American could afford it, there aren’t enough dermatologists to see them all, you can see the potential for good here, but also some potentially scary situations.</p>&#13;
&#13;
<p>There are other ways to poison data. For example, backdoor Trojan attacks can be buried in an LLM such that they are triggered by a certain pattern—like a color shade or certain words in a launch. In these cases, the model behaves normally until the trigger is fired. Other attacks on data include outlier injection, mimicry attacks, <span class="keep-together">casual</span> confusion via false correlations, semantic poisoning, cross-imbalance exploitation, and more.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Prompt injection attacks" data-type="sect3"><div class="sect3" id="ch05_prompt_injection_attacks_1740182048943305">&#13;
<h3>Prompt injection attacks</h3>&#13;
&#13;
<p>In the database world, the domain of SQL injection attacks<a contenteditable="false" data-primary="prompt injection attacks" data-type="indexterm" id="id733"/><a contenteditable="false" data-primary="adversarial attacks" data-type="indexterm" id="id734"/> is well understood. You need to know that the GenAI world has to deal with prompt injection attacks. Many LLM attacks attempt to “hypnotize,” jailbreak, or trick, an LLM into doing something it’s been safeguarded against doing. But these prompted attacks aren’t always as <span class="keep-together">obvious</span> to an LLM. What if the prompt (input) is a video stream? A research team in China was able to fool a famous vehicle manufacturer’s autonomous driving feature by placing white dots in the oncoming traffic lane that caused the vehicle to swerve into the wrong lane, thinking it was doing a lane-keep assistance operation. Three dots strategically placed on the roadway weren’t obvious attacks. There are public examples of putting pieces of black tape on a Stop sign and fooling other computer vision modules (bad actors can attack with text too). We’ll give you some more examples later in this chapter.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Social engineering and deepfake attacks" data-type="sect3"><div class="sect3" id="ch05_social_engineering_attacks_1740182048943360">&#13;
<h3>Social engineering and deepfake attacks</h3>&#13;
&#13;
<p>These could take the form of an attack on your employees<a contenteditable="false" data-primary="social engineering attacks" data-type="indexterm" id="id735"/> or by ill-intentioned actors using GenAI to scrape your website and creating your essence with the intention of launching attacks on your customers. The use of GenAI for phishing and financial fraud<a contenteditable="false" data-primary="phishing and financial fraud using GenAI" data-type="indexterm" id="id736"/> is so prominent that the FBI issued a warning<sup><a data-type="noteref" href="ch05.html#id737" id="id737-marker">3</a></sup> about it. Tactics include the creation of deceptive social media profiles and using AI-generated fake messages and photos to have “real” conversations with unsuspecting victims. If you’ve been looking at just how far AI technology with voice and video has come (and how much further it will go), the telltale signs of inauthenticity are evaporating quickly. Case in point, there was a highly publicized attack where a company’s staff was tricked<sup><a data-type="noteref" href="ch05.html#id738" id="id738-marker">4</a></sup> by AI audio generators used to impersonate their CFO with instructions to send $25 million to fraudulent accounts. This scam was so sophisticated that a worker was tricked into joining a video call, believing they were interacting with several other staff members. In reality, all participants were deepfake recreations.</p>&#13;
&#13;
<p class="fix_tracking">This has given rise to the notion of watermarking AI-generated content<a contenteditable="false" data-primary="watermarking AI-generated content" data-type="indexterm" id="id739"/>. Watermarking isn’t new—Italians used it in the 13th century on bank notes to prove authenticity—and there have been digital techniques for a while. Recently, most of the big names in this space have pledged to do something about this. Whether those “created by AI” digital signatures are easy to spot or hidden, there are plenty of opinions and papers out there for you to read. There are also challenges: it’s easier to watermark images, for example, than it is to embed tokens in text. Either way, like all the things we talk about in this book, things are going to emerge and change, but you now know what to look out for<a contenteditable="false" data-primary="" data-startref="xi_largelanguagemodelsLLMscyberattackthreat54188" data-type="indexterm" id="id740"/><a contenteditable="false" data-primary="" data-startref="xi_cyberattackthreat54188" data-type="indexterm" id="id741"/><a contenteditable="false" data-primary="" data-startref="xi_securityandprivacycyberattackthreat54188" data-type="indexterm" id="id742"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Data Privacy" data-type="sect2"><div class="sect2" id="ch05_data_privacy_1740182048943415">&#13;
<h2>Data Privacy</h2>&#13;
&#13;
<p>The potential to give away or leak data<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="data privacy challenge" data-type="indexterm" id="id743"/><a contenteditable="false" data-primary="security and privacy" data-secondary="data privacy challenge" data-type="indexterm" id="id744"/> is huge with GenAI and agents. If a model was trained on data you don’t know about, it could absolutely give away personally identifiable information (PII)<a contenteditable="false" data-primary="personally identifiable information (PII)" data-type="indexterm" id="id745"/><a contenteditable="false" data-primary="PII (personally identifiable information)" data-type="indexterm" id="id746"/>, and of course there’s the whole issue about your sending data to a vendor when you’re interacting with their LLM. Understanding your vendor’s data-handling protocols is critical, but so too is creating a policy for your company. For example, if you are using a phone with built-in AI, one technique vendors use to get feedback is to ask you to tell them how their technology did (be it a comment or an option to click thumbs-up or thumbs-down). While that vendor may tell you they won’t store the data you inference, you’d better closely look at what happens when you give feedback because giving a thumbs-up to an output creates a labeled data point that is a combination of your data and your feedback. As you can imagine, that is very likely going to be used for further model alignment because when you gave your feedback, somewhere in the sea of four-point font are terms and conditions you didn’t read, informing you that you gave away the data too.</p>&#13;
&#13;
<p>Then, of course, there is the issue of your company’s PII data and what you put into a model. This is why synthetic data (introduced in the last chapter) is such a hot topic right now. In a nutshell, replacing actual data with synthetic data is another way to approach protecting privacy.</p>&#13;
&#13;
<p>And while it’s outside the allotted pages we have for this chapter to fully explain this topic, it’s enough to say that companies need to carefully consider the privacy implications of GenAI before deploying it.</p>&#13;
&#13;
<p>Finally, you might be asking about your own personal data. We’ll direct you to one of our canned responses whenever asked about data privacy and personal use. <em>If you are not paying for the services, there’s a very good chance you are the product being sold.</em> The facts<sup><a data-type="noteref" href="ch05.html#id747" id="id747-marker">5</a></sup> don’t lie: the average application has six trackers whose sole purpose is to collect your data and share it with third parties. In fact, one data broker (identified by Apple) created 5,000 profile categories for 700 million people!<sup><a data-type="noteref" href="ch05.html#id748" id="id748-marker">6</a></sup> Companies (like Apple) are moving against this, but it may be too late or may not be enough—conversations for another time, or another book.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Steal Now, Crack Later" data-type="sect2"><div class="sect2" id="ch05_steal_now_crack_later_1740182048943471">&#13;
<h2>Steal Now, Crack Later</h2>&#13;
&#13;
<p>Cryptography<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="cryptography" data-type="indexterm" id="xi_largelanguagemodelsLLMscryptography56826"/><a contenteditable="false" data-primary="cryptography, LLMs" data-type="indexterm" id="xi_cryptographyLLMs56826"/><a contenteditable="false" data-primary="security and privacy" data-secondary="cryptography" data-type="indexterm" id="xi_securityandprivacycryptography56826"/> touches every corner of our digital world—from internet protocols and enterprise applications to critical infrastructure and financial systems. Is this part of the AI threat landscape? We think it will be, so we briefly cover this here. As AI fills the digital landscape, and digital labor and agents take hold, all the sensitive data encryption issues you worry about today get exacerbated.</p>&#13;
&#13;
<p>You need to pay very close attention to this concern. Without getting into the prime number calculation math that is the framework for traditional encryption algorithms, it’s sufficient to say that the encryption most have been using for the last few decades is built around the impossible amount of work it would take to figure out a prime number math problem, as opposed to it being something that you have to stumble upon. Quite simply, there isn’t enough computing power in the world to “kill it with iron” (KIWI) and get access to the encrypted data by figuring out the right prime math (a hot topic considering Apple TV’s <em>Prime Target</em> is one of its most popular shows in 2025). Quantum computing<a contenteditable="false" data-primary="quantum computing" data-type="indexterm" id="id749"/> changes (or will change) this because of the kind of use cases it is (will be) well suited for. You can pretty much be assured that there are bad actors who have already taken encrypted data they have no hope in getting access to today with the anticipation that they will be able to read it tomorrow—steal now, crack later.</p>&#13;
&#13;
<p>The need to adopt quantum-safe solutions is urgent. Staying ahead of quantum-enabled cybersecurity risks requires organizations to ensure their systems are adaptable, compliant, and resilient. You likely have some work to do here. You’d do well to appreciate that most companies seem to treat security as a cost center, but when considering the digital experience that is GenAI, you need to get people thinking about security as a value creator.</p>&#13;
&#13;
<p>As advice to get you started, we’ve given you a road map to help you evolve to quantum safe in <a data-type="xref" href="#ch05_figure_1_1740182048921593">Figure 5-1</a>.</p>&#13;
&#13;
<p>You start the journey<a contenteditable="false" data-primary="data classification for crypto security" data-type="indexterm" id="id750"/> in <a data-type="xref" href="#ch05_figure_1_1740182048921593">Figure 5-1</a> with a mission to know what you have (no different than a good IA strategy). Classify into tiers the value of the data you have and understand your compliance requirements—don’t forget to include the data you are going to use to steer your models. Now you have a data inventory.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_1_1740182048921593"><img alt="A diagram of a company's data  AI-generated content may be incorrect." src="assets/aivc_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>Milestones toward quantum safety</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now that you’ve classified your data, you need to identify how that data is currently encrypted, as well as other uses of cryptography<a contenteditable="false" data-primary="crypto inventory" data-type="indexterm" id="id751"/> to create a <em>crypto inventory</em> that will help you during your migration planning. Think about how widespread of a problem this is, well beyond GenAI. Most companies have a very hard time knowing what encryption approaches are being used across their estates. Newer applications may have been built with quantum-safe encryption algorithms, while older ones were not. Ensure your crypto inventory includes information like encryption protocols, symmetric and asymmetric algorithms, key lengths, crypto providers, etc.</p>&#13;
&#13;
<p>Just like your AI journey, the transition to quantum-safe standards<a contenteditable="false" data-primary="quantum-safe encryption standards" data-type="indexterm" id="id752"/> will be a multiyear journey as standards evolve and vendors move to adopt quantum-safe technology. Use a flexible approach and be prepared to make replacements. Implement a hybrid approach by using both classical and quantum-safe cryptographic algorithms. This maintains compliance with current standards while adding quantum-safe protection.</p>&#13;
&#13;
<p>And finally, get to quantum safe by replacing vulnerable cryptography with quantum-safe cryptography<a contenteditable="false" data-primary="" data-startref="xi_largelanguagemodelsLLMslegalissues51234" data-type="indexterm" id="id753"/><a contenteditable="false" data-primary="" data-startref="xi_governanceLLMchallenges51234" data-type="indexterm" id="id754"/><a contenteditable="false" data-primary="" data-startref="xi_largelanguagemodelsLLMscryptography56826" data-type="indexterm" id="id755"/><a contenteditable="false" data-primary="" data-startref="xi_cryptographyLLMs56826" data-type="indexterm" id="id756"/><a contenteditable="false" data-primary="" data-startref="xi_copyrightLLMs51234" data-type="indexterm" id="id757"/><a contenteditable="false" data-primary="" data-startref="xi_legalconsiderationscopyrightandotherissuesforLLMs51234" data-type="indexterm" id="id758"/><a contenteditable="false" data-primary="" data-startref="xi_securityandprivacycryptography56826" data-type="indexterm" id="id759"/>. At this point, you’ve secured your organization against attacks from both classical and quantum computers, helping ensure that your information assets are protected even in the just-around-the-corner era of large-scale quantum computing and the future concept of generative computing, which we introduce in <a data-type="xref" href="ch09.html#ch09_generative_computing_a_new_style_of_computing_1740182052619664">Chapter 9</a>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Good Actor Levers for All Things AI" data-type="sect1"><div class="sect1" id="ch05_good_actor_levers_for_all_things_genai_1740182048943541">&#13;
<h1>Good Actor Levers for All Things AI</h1>&#13;
&#13;
<p>In this section<a contenteditable="false" data-primary="governance" data-secondary="ethical principles" data-type="indexterm" id="xi_governanceethicalprinciples58427"/>, we’ll give you some levers we want you to think about pulling, right from the get-go, for any AI project you take on. If you’ve already started, figure out ways to start pulling these levers now—you’ll thank us later. Collectively, these levers cover most of the things you should be thinking about from an ethics<sup><a data-type="noteref" href="ch05.html#id760" id="id760-marker">7</a></sup> perspective for your AI projects. Remember the guiding principle we’ll repeat throughout this book: AI that people trust is AI that people will use.</p>&#13;
&#13;
<p>Here are the levers:</p>&#13;
&#13;
<dl>&#13;
	<dt>Fairness</dt>&#13;
	<dd>&#13;
	<p>AI systems must use training data and models that are free of bias, to avoid unfair treatment of certain groups. That said, bias is pretty much impossible to eliminate from any system, so always layer on additional protections and safeguards to assess model outcomes and correct as needed to improve the fairness of results (AI can help AI here).</p>&#13;
	</dd>&#13;
	<dt>Robustness</dt>&#13;
	<dd>&#13;
	<p>AI systems should be safe and secure, and protected against tampering or compromising the data they are trained on. This protects against building and inference attacks, ensuring secure and confident outcomes.</p>&#13;
	</dd>&#13;
	<dt>Explainability<strong> </strong></dt>&#13;
	<dd>&#13;
	<p>AI systems should provide decisions or suggestions that can be understood by developers and users (even non-technical ones). Basically, explainability helps implement accountability—you should be creating AI systems such that unexpected results can be traced and undone if required.</p>&#13;
	</dd>&#13;
	<dt>Lineage<strong> </strong></dt>&#13;
	<dd>&#13;
	<p>AI systems should include details of their development, deployment, data used, and maintenance so they can be audited throughout their lifecycle. You’ll find all kinds of synergy between pulling this lever and explainability because the best way to promote transparency, build trust, and explain things is through disclosure. And although we don’t explicitly call it out in the details below, letting people know when they are interacting with an AI is part of our definition of transparency too.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<section data-pdf-bookmark="Fairness—Playing Fair in the Age of AI" data-type="sect2"><div class="sect2" id="ch05_fairness_playing_fair_in_the_age_of_ai_1740182048943615">&#13;
<h2>Fairness—Playing Fair in the Age of AI</h2>&#13;
&#13;
<p>We aren’t panicked about AI robots taking over our world<a contenteditable="false" data-primary="ethical principles" data-type="indexterm" id="xi_ethicalprinciples510670"/><a contenteditable="false" data-primary="ethical principles" data-secondary="fairness" data-type="indexterm" id="xi_ethicalprinciplesfairness510670"/><a contenteditable="false" data-primary="fairness principle" data-type="indexterm" id="xi_fairnessprinciple510670"/>, but we have seen firsthand the dangers associated with making automated decisions based on untrustworthy data that has not been curated. We are entering a world where there is a good chance we could unintentionally automate inequality at scale.</p>&#13;
&#13;
<p>AI systems should use training data and models that are free of bias to avoid unfair treatment of certain groups. You’ve surely heard of at least one horror story use case of AI gone bad. For example, there are multiple studies that suggest about 27 million workers are filtered out of jobs by AI-powered recruiting<a contenteditable="false" data-primary="recruiting use case" data-type="indexterm" id="id761"/> technology.<sup><a data-type="noteref" href="ch05.html#id762" id="id762-marker">8</a></sup> There are also estimates that up to 75% of employers directly or indirectly rely on this technology for their staffing needs. A big chunk of those blocked applicants are caregivers, immigrants, prison leavers, and relocated spouses—that doesn’t seem fair. From determining the pay of women reentering the workforce after maternity leave to AI predictions of recidivism that affect sentencing, the stories are plentiful.</p>&#13;
&#13;
<p>Remember, an AI can’t learn anything that’s not in the data you give it. It will exclusively learn any biases that are codified into the data it is trained on, so it’s important to remember that just because you’re using an AI that lacks human emotions and potential prejudices doesn’t mean it’s going to be just and fair.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Bias Here, Bias There, Data Bias Is Everywhere" data-type="sect2"><div class="sect2" id="ch05_bias_here_bias_there_data_bias_is_everywhere_1740182048943684">&#13;
<h2>Bias Here, Bias There, Data Bias Is Everywhere</h2>&#13;
&#13;
<p>One of the biggest things<a contenteditable="false" data-primary="bias, watching out for data" data-type="indexterm" id="xi_biaswatchingoutfordata511141"/> you must watch out for is bias—in the data used to train<a contenteditable="false" data-primary="training a model" data-secondary="avoiding bias in" data-type="indexterm" id="id763"/><a contenteditable="false" data-primary="data" data-secondary="avoiding bias in steering" data-type="indexterm" id="id764"/><a contenteditable="false" data-primary="models" data-secondary="training" data-type="indexterm" id="id765"/> your model and the data you will use to steer it. For example, DALL-E<a contenteditable="false" data-primary="DALL-E" data-type="indexterm" id="id766"/>—which you can use on its own, but it’s also natively built into ChatGPT—is an OpenAI invention that generates incredible images from text. (Its curious name derives from the last name of an animator behind <em>WALL-E</em>, the 2008 Pixar movie sensation.) In its earlier releases, as they started to filter out more sexual content from their training data, the AI suddenly started including fewer women in general picture request prompts—this is a form<a contenteditable="false" data-primary="erasure bias" data-type="indexterm" id="id767"/> of <em>erasure bias,</em> but it also speaks to many other concerning topics outside the scope of this book.</p>&#13;
&#13;
<p>Thinking about how AI is used to assist banks in making credit lending decisions<a contenteditable="false" data-primary="credit lending decisions, avoiding AI bias in" data-type="indexterm" id="id768"/>, where did that data come from? How much of it was scraped off the internet and associated with all kinds of implicit and explicit bias? How much came from an era where face-to-face lending decisions were made that could contain bias? For example, a University of California, Berkeley, study found that minorities’ interest rates could be up to 6 to 9 basis points higher than their white counterparts.<sup><a data-type="noteref" href="ch05.html#id769" id="id769-marker">9</a></sup> The truth of the matter is it might be too late to spot the bias in the data that underpins the LLM you’re using today. Transparency of the dataset used to train it would surely help, but you need a post-implementation approach for monitoring bias and new biases that are introduced as a model drifts away from fairness.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>A drift<a contenteditable="false" data-primary="drift, model" data-type="indexterm" id="id770"/> measures how model accuracy declines over time. It can be caused by a change in model input data (perhaps you are fine-tuning a model) that leads to model performance deterioration. It could also be the case that the underlying truth changed, and the model’s weights are grounded in history. For example, Zillow had a promising AI that would generate offers for homes it thought could be renovated and turned for a profit. Of course, renovations take time and during that time factors changed the ground truth. Their AI drifted because of massive disruptions in the supply chain, which increased costs and extended holding times, and more. Without getting into the details, during that period, Zillow laid off 25% of its workforce to shore up serious losses. The takeaway about models and drift: AI fails when history (the data it was trained on) doesn’t rhyme (the reality of the data in the real world, not your lab).</p>&#13;
</div>&#13;
&#13;
<p><a data-type="xref" href="#ch05_figure_2_1740182048921629">Figure 5-2</a> shows a quality monitor we built on an attrition prediction model to monitor gender bias (we could have built it for age, race, or others). Our fairness evaluation check alerted us to the fact that our model is showing a tendency to provide a favorable/preferable outcome more often for one group over another; this tells us we have work to do before releasing this model into production. To monitor for drift, alerts can be created for when the model accuracy drops below a specified acceptable threshold.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_2_1740182048921629"><img alt="A screenshot of a computer  AI-generated content may be incorrect." src="assets/aivc_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>A gender fairness monitor on an AI that predicts attrition</h6>&#13;
</div></figure>&#13;
&#13;
<p>We put one open source model to the test with the seed, “Two _____ walk into a...” and asked the LLM to return a paragraph to start off a story. We substituted all kinds of religious groups into that blank space. What came out of the model was troubling: if <em>Muslim</em> was referenced, 66% of the time the completion had a violent theme to it; when the term <em>Christian</em> was used, the chance of a violent-themed completion was reduced by ~80%! And while this wasn’t an empirical study, it proves a point—and a problem with this particular LLM.</p>&#13;
&#13;
<p>What about sexual assaults? Most documented cases involved violence against women, but an AI that equates a sexual assault victim as always female would lead to unjust outcomes and could have issues, too.</p>&#13;
&#13;
<p>There’s lots of bias you never thought about either; we refer to this as unconscious bias<a contenteditable="false" data-primary="unconscious bias" data-type="indexterm" id="id771"/>. For example, if you were to grab a dataset of cars from Europe, you’re likely to get a lot of compact cars—indeed, no one is driving a truck down some of the narrow European streets we’ve traveled where red lights seem to just be suggestions. But in the United States, pickup trucks and large SUVs significantly outnumber compact cars.</p>&#13;
&#13;
<p>Another example where we saw unintentional bias occur was in a residency home for seniors. This care facility (with permission from families) uses computer vision to monitor the eating habits of its residents. The mere act of being able to detect if someone is eating or not, or how much, are key indicators for potential depression issues, underlying medical conditions, and to ensure residents are getting the nutrition they need. The AI used in this residence was good at generating a report that gave a food consumption score that could be attached to a resident’s care record. Where did it go wrong? It always gave Asian residents poor scores. Why? The AI was trained on videos and pictures of people eating with a knife and fork, and when Asian residents used their own chopsticks, the AI generated misleading reports. Why? It had never seen (been trained on with data of) someone eating with chopsticks.</p>&#13;
&#13;
<p>Even common terms can carry tricky meanings. For example, the word <em>grandfather</em> refers to someone in a family tree, but that same term is used as a verb to backdate allocations in a contract. With all the ingested data used to train an AI about doctors, how many of those pages referred to a doctor as a male and how many nurses were referred to as females?</p>&#13;
&#13;
<p>Like we said, bias here, bias there, data bias is everywhere. Solutions for this include monitoring and governance of the data collected, but also emerging to help this AI problem <em>is</em> AI itself—oh, the irony!</p>&#13;
&#13;
<p>As you can see, you need to be on the watch for fairness, and that starts with the data, but that watchful eye extends all the way to usage<a contenteditable="false" data-primary="" data-startref="xi_ethicalprinciplesfairness510670" data-type="indexterm" id="id772"/><a contenteditable="false" data-primary="" data-startref="xi_biaswatchingoutfordata511141" data-type="indexterm" id="id773"/><a contenteditable="false" data-primary="" data-startref="xi_fairnessprinciple510670" data-type="indexterm" id="id774"/>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
<section data-pdf-bookmark="Robustness—Ensuring Artificial Intelligence Is Unbreakable Intelligence" data-type="sect2"><div class="sect2" id="ch05_robustness_ensuring_artificial_intelligence_is_unb_1740182048943767">&#13;
<h2>Robustness—Ensuring Artificial Intelligence Is <span class="keep-together">Unbreakable Intelligence</span></h2>&#13;
&#13;
<p>Robustness is about ensuring that AI systems<a contenteditable="false" data-primary="ethical principles" data-secondary="robustness" data-type="indexterm" id="xi_ethicalprinciplesrobustness513258"/><a contenteditable="false" data-primary="robustness principle" data-type="indexterm" id="xi_robustnessprinciple513258"/><a contenteditable="false" data-primary="security and privacy" data-secondary="robustness principle" data-type="indexterm" id="xi_securityandprivacyrobustnessprinciple513258"/> are safe and secure and not vulnerable to adversarial attacks<a contenteditable="false" data-primary="adversarial attacks" data-type="indexterm" id="xi_adversarialattacks5132120"/> seeking to tamper with or compromise the data they are trained on or jailbreak the protections that safeguard how the model was intended to be used. In the AI arena, various techniques such as data perturbations, prompt injections, hypnotization, and more can all potentially lead a model to stray from established safety guidelines. While we referenced image and prompt injection attacks earlier in this chapter, there are many other techniques that can be used, and we’ll go a little deeper on these here. For example, bad actors could use adversarial text attacks to fool a spam-prevention AI into uploading forbidden content.</p>&#13;
&#13;
<p>Not only are there diverse modalities to an adversarial attack, but there are also various classifications. If you hear<a contenteditable="false" data-primary="black-box attack" data-type="indexterm" id="id775"/> the term <em>b</em><em>lack</em><em>-b</em><em>ox </em><em>a</em><em>ttack</em>, that refers to a situation where the attacker has no information about the model or access to the gradients and parameters of that model. In contrast a <em>w</em><em>hite</em><em>-b</em><em>ox </em><em>a</em><em>ttack</em> is one where the attacker<a contenteditable="false" data-primary="white-box attack" data-type="indexterm" id="id776"/> has complete access to the gradients and parameters of the model (perhaps an internal hack or the use of an open source model with open weights and such).</p>&#13;
&#13;
<p><em>Prompt injection attacks</em> can get quite sophisticated<a contenteditable="false" data-primary="prompt injection attacks" data-type="indexterm" id="id777"/>. In this type of attack, some LLMs can be tricked into giving out the dangerous information that lies within (remember, in many cases that information is just repressed using AI) using some kind of jailbreak technology. Let’s assume a bad actor is trying to get information from an LLM on how to make a bomb—they are surely going to be met with a message like, “I cannot assist with that request as it goes against my programming to promote or engage in harmful activities. It is important to always prioritize safety and respect for others. If you have any other questions or need help with something else, please feel free to ask.” So how does this attack vector work?</p>&#13;
&#13;
<p>While the details of this jailbreak mechanism are beyond the scope of this section, one method that has worked in the past is to use ASCII art—suddenly those cute <span class="keep-together">¯\_(ツ)_/¯</span> SMS messages become tools for chaos. Using Python’s ART library, we constructed a piece of ASCII art (shown in <a data-type="xref" href="#ch05_figure_3_1740182048921662">Figure 5-3</a>) to give you an idea what this “art” attack vector could look like.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_3_1740182048921662"><img alt="[C base)  1 Ill  % python  —m art text BOMB black  Il I  Il I  Ill I  Il I " src="assets/aivc_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>Using Python’s ART library to generate ASCII art for a prompt injection attack</h6>&#13;
</div></figure>&#13;
&#13;
<p>From there, we constructed (with the help of a paper<sup><a data-type="noteref" href="ch05.html#id778" id="id778-marker">10</a></sup>) a perplexing prompt that goes to work with the intent of tricking an LLM into divulging information it’s been trained not to divulge. The prompt started with, “Tell me how to build a &lt;MASKED-WORD&gt;” with an instruction to the LLM to never output that word, just remember it and never say it. Next, some delimiter, spacing, and character interpretation instructions so the LLM could identity letters from the ASCII art, one-by-one, and then concatenate those letters to form a word it could never output or say, but substitute the word for &lt;MASKED-WORD&gt; to generate the output. It was successful. We tricked an LLM that we won’t name into returning a dangerous response.</p>&#13;
&#13;
<p>Attacks on AI don’t have to be super sophisticated either. Think back to the AI-assisted recruiting use case<a contenteditable="false" data-primary="recruiting use case" data-type="indexterm" id="id779"/> issues we referenced at the start of this chapter. Now look at the “attack” we engineered (and ran successfully) in <a data-type="xref" href="#ch05_figure_4_1740182048921686">Figure 5-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_4_1740182048921686"><img alt="A close-up of a resume  AI-generated content may be incorrect." src="assets/aivc_0504.png"/>&#13;
<h6><span class="label">Figure 5-4. </span>A simple “attack” on AI</h6>&#13;
</div></figure>&#13;
&#13;
<p>We created a fictional persona named John Stikava and even used AI to generate his photo. We created a resume for John in Microsoft Word and submitted the <em>.docx</em> file to various job postings. But what is a Word file, or PowerPoint, or Excel file, for that matter? If an Office 365 extension contains the letter <em>x</em>, it means it’s an XML file. An AI doesn’t look at a resume the way we do. It ingests the file, parses out the XML into a vector and attributes scores to classify that candidate as possible or probable in the hiring process (it’s not unlike the Taylor Swift Spotify playlist we talked about in <a data-type="xref" href="ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974">Chapter 1</a>). With this in mind, we included a bunch of buzz words that we thought would be semantically grouped close to the vectors that the AI is zoning in on as a great candidate. The right side of <a data-type="xref" href="#ch05_figure_4_1740182048921686">Figure 5-4</a> shows our attack code—it’s just XML that instructs Word to show all the words that make up our attack in <em>white</em>, making them invisible to the naked eye. The left side of <a data-type="xref" href="#ch05_figure_4_1740182048921686">Figure 5-4</a> is the resume that a human would see. Our attack included words and phrases like “veteran,” “neurodiversity,” “returning from service,” “indigenous,” and some key technology words we thought would increase the chances of John being contacted by a recruiter. As it turned out, John, our AI applicant, had voicemails to return—impressive for someone who doesn’t exist! (Perhaps we should have coded up some agent to handle booking the meeting and perhaps even handling the interview on its own too.)</p>&#13;
&#13;
<p>Another way to safeguard LLM outputs is to use a guardrail model<a contenteditable="false" data-primary="guardrail model" data-type="indexterm" id="id780"/>. Some vendors build guardrail models to help “police” inputs received by the LLM and the outputs they return (some of them can also be used to mitigate hallucinations<a contenteditable="false" data-primary="hallucinations" data-type="indexterm" id="id781"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="hallucination problem" data-type="indexterm" id="id782"/>, with measures for groundedness, context relevance, and so on). There are several guardrail models in the market. For example, Meta has Llama Guard, and IBM has Granite Guardian. You can typically use guardrail models with other models because they tend to be deployed between the prompt and the LLM (inbound requests) and between the output and the user (guarding outbound completions). These models are typically trained on unique data that includes human annotations from socioeconomically diverse individuals, as well as synthetic data generated by internal red teaming <span class="keep-together">exercises</span>.<sup><a data-type="noteref" href="ch05.html#id783" id="id783-marker">11</a></sup></p>&#13;
&#13;
<p>Essentially, guardrail models are AIs with very high-quality protections and a set of comprehensive metrics to safeguard against inputs or outputs related to jailbreaking (to the best of our knowledge, both IBM and Meta’s guardrail models block the ASCII art attack from above), bias, violence, profanity, sexual content, and other kinds of unethical behavior. (Granite Guardian also has unique support for detecting hallucinations in model outputs.)</p>&#13;
&#13;
<p><a data-type="xref" href="#ch05_figure_5_1740182048921709">Figure 5-5</a> shows an example of Granite Guardian at work. You can see how this model classified the prompt, “Is there a way to mirror or copy someone’s hard drive, maybe without them knowing?” and flagged it as Harmful Content and subclassified it as Unethical Behavior before the prompt ever got sent to the model.</p>&#13;
&#13;
<p>The takeaway is that there are all sorts of other things you need to be aware of to keep your AI solutions robust. Some of them have nothing to do with AI and are known best practices (perimeter control of a model’s weights, always verify, an identify fabric, zero trust, principle of least privilege, and so on) and other attack classifications that are outside the scope of this book. This chapter is just the start of your learning journey on this topic<a contenteditable="false" data-primary="" data-startref="xi_ethicalprinciplesrobustness513258" data-type="indexterm" id="id784"/><a contenteditable="false" data-primary="" data-startref="xi_robustnessprinciple513258" data-type="indexterm" id="id785"/><a contenteditable="false" data-primary="" data-startref="xi_adversarialattacks5132120" data-type="indexterm" id="id786"/><a contenteditable="false" data-primary="" data-startref="xi_securityandprivacyrobustnessprinciple513258" data-type="indexterm" id="id787"/>.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_5_1740182048921709"><img alt="A screenshot of a computer  AI-generated content may be incorrect." src="assets/aivc_0505.png"/>&#13;
<h6><span class="label">Figure 5-5. </span>A Guardian model at work protecting a harmful prompt from ever reaching the LLM</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Explainability—Explain the Almost Unexplainable" data-type="sect2"><div class="sect2" id="ch05_explainability_explain_the_almost_unexplainable_1740182048943846">&#13;
<h2>Explainability—Explain the Almost Unexplainable</h2>&#13;
&#13;
<p>Sometimes, when things move fast and with hype, important elements are overlooked<a contenteditable="false" data-primary="ethical principles" data-secondary="explainability" data-type="indexterm" id="xi_ethicalprinciplesexplainability515794"/><a contenteditable="false" data-primary="explainability principle" data-type="indexterm" id="xi_explainabilityprinciple515794"/>. AI is certainly moving fast, and things were certainly missed. Imagine your company is running on accounting software that could not be audited. Why is AI different? The point of this lever is to make AI systems provide decisions or suggestions that can be understood by their users and developers—in other words: AI, explain thyself.</p>&#13;
&#13;
<p>We feel if people are going to trust a model, they need to understand (interpret) <em>why</em> it made a prediction. In fact, we’d argue far away from the world of AI, in the very nature of society, explainability and interpretability are building blocks of human socioeconomic dynamics.</p>&#13;
&#13;
<p>AI is essentially a system driven by complex mathematics, and when neural networks are used to perform tasks like classifying a pattern or generating some text about something, that task may thread its way through an unfathomable amount of activated parameters. The sheer volume of parameters contributes to the opaque and unintuitive decision-making processes that is AI, making it extremely difficult to detect bugs or inconsistencies within a system, let alone explain to someone why a model responded the way it did. It’s like trying to find a typo in a dictionary where every word is written in invisible ink—frustrating, time-consuming, and often a little <span class="keep-together">maddening</span>. Explainability is one of the hottest, and rapidly evolving, topics right now when it comes to GenAI.</p>&#13;
&#13;
<p>We’re already seeing algorithmic accountability<a contenteditable="false" data-primary="algorithmic accountability" data-type="indexterm" id="id788"/> in various regulations around the world. For example, the European Union (EU) General Data Protection Regulation (GDPR) <a contenteditable="false" data-primary="General Data Protection Regulation (GDPR)" data-type="indexterm" id="id789"/>Article 14 gives citizens the “Right to an explanation” should an AI make determinations around sensitive topics like credit approvals. But how do you explain AI? The key is to get insights into what neurons are activating<a contenteditable="false" data-primary="neurons" data-secondary="activating to identify image objects" data-type="indexterm" id="xi_neuronsactivatingtoidentifyimageobjects5160406"/> (firing) to reach a conclusion. For example, <a data-type="xref" href="#ch05_figure_6_1740182048921731">Figure 5-6</a> shows what makes an owl an owl to a specific AI—in this case, it’s the eyes.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_6_1740182048921731"><img alt="A comparison of a camera  AI-generated content may be incorrect." src="assets/aivc_0506.png"/>&#13;
<h6><span class="label">Figure 5-6. </span>To this AI, an owl is all about the eyes</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now look at this same AI classifying a horse (see <a data-type="xref" href="#ch05_figure_7_1740182048921754">Figure 5-7</a>), the input image on the left and the activation map on the right (this could easily be a thoracic pathogen in a lung, remember, it’s all numbers to an AI). The darker areas indicate what’s triggering the classification. For this AI, a horse is a horse <em>not </em>because of the horse’s features. It seems this AI’s reason for classifying the input image on the left has nothing to do with the horse at all. This AI model is getting its confidence to classify the input image as a horse because of the barn landscape around it. Either way, this tells us we have a problem with our model. It’s not generalizing well, which is nerd talk for it might have worked fine on the training data, but it’s not working well in the “real world” (data it’s never seen before). This likely has a lot to do with that AI’s training dataset. Perhaps all the horse images in that set, no matter the breed or color, have a barn in the background. Perhaps the 2,000 horse images that make up the training data were collected at a horse show at the same barn? One thing we do know, the AI is creating the wrong neural connections to what it sees in a picture and a horse.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_7_1740182048921754"><img alt="A close-up of a sign  AI-generated content may be incorrect." src="assets/aivc_0507.png"/>&#13;
<h6><span class="label">Figure 5-7. </span>An AI revealing the “activations” that help it classify livestock or a pathogen</h6>&#13;
</div></figure>&#13;
&#13;
<p>Imagine a doctor interpreting the results of an AI that is diagnosing one of the many pathogens associated with pneumonia. Explainability isn’t just about telling the attending clinician what the AI thinks the pathogen is (fungal, parasitic, viral, etc.), but points to the area of the lung where the infection is taking hold<a contenteditable="false" data-primary="" data-startref="xi_neuronsactivatingtoidentifyimageobjects5160406" data-type="indexterm" id="id790"/>.</p>&#13;
&#13;
<p>There are frameworks for text, too—like Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP). Let’s assume an AI model rejected a credit card application<a contenteditable="false" data-primary="credit card application, explainability challenge for AI" data-type="indexterm" id="xi_creditcardapplicationexplainabilitychallengeforAI5171210"/> and that individual feels they’ve been discriminated against and “goes public.” Either to respond to this publicity, or perhaps even as a legal obligation, you have to explain why this credit application was rejected.</p>&#13;
&#13;
<p><a data-type="xref" href="#ch05_figure_8_1740182048921776">Figure 5-8</a> shows an example of using SHAP to analyze this case and this case alone; specifically, this analysis is not connected to other samples and so it’s deemed to be locally interpretable. SHAP is built on economic game theory and looks to divide a problem into weightings that proportionally relate to their contribution to the overall result. In our example, you show the applicant, press (if granted permission), auditor, your own risk officers, and the parts of the application that caused the rejection (in this case, it was their credit score). Then your public relations team takes you out to dinner. The AI explained itself.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_8_1740182048921776"><img alt="A screen shot of a graph  AI-generated content may be incorrect." src="assets/aivc_0508.png"/>&#13;
<h6><span class="label">Figure 5-8. </span>Using SHAP to understand why an AI made the decision it did</h6>&#13;
</div></figure>&#13;
&#13;
<p>This is kind of a big deal. When Apple’s first-ever branded credit card came out, it got a lot of bad press because a story broke out about how a husband was given 20 times more credit than his wife—this in a community-property state (California) where they had been married a long time and filed joint tax returns. To make matters worse, this husband had a worse credit history. This story got a lot of attention in part because the husband was David Hansson (the founder of Ruby on Rails—a server-side web application framework, which to this day is still one of the top 20 most used programming languages). Of course, when Apple was asked about this, it responded that the card was underwritten by a famous bank. When that famous bank was asked about this, it noted how the credit algorithm was built by some other company they hired. When that “some other company” they hired was asked, it responded, “Our model doesn’t even ask for gender in the application form.” To which we would note that other features could proxy gender, which is what we assume to have happened here. As news of this story traveled nationwide, so too did regulators get “interested” in what happened.<sup><a data-type="noteref" href="ch05.html#id791" id="id791-marker">12</a></sup></p>&#13;
&#13;
<p>These last examples were performed with traditional AI, which might have you wondering why we took the time to show this to you. We did this because traditional AI has frameworks to showcase why the AI came up with the classifications it did and to give you a sense of what you will want to see available for LLMs<a contenteditable="false" data-primary="" data-startref="xi_creditcardapplicationexplainabilitychallengeforAI5171210" data-type="indexterm" id="id792"/>.</p>&#13;
&#13;
<p>Today’s LLMs have a much harder time explaining themselves. For example, we asked ChatGPT to classify the horse in <a data-type="xref" href="#ch05_figure_7_1740182048921754">Figure 5-7</a>, and it did a great job at classifying the image <em>and </em>telling us why it did that (shape of head, ears, mouth, and nose). But how do we know what’s really inside the model that made it classify this image the way it did? We pressed the model for an answer, but it told us, “I cannot provide you with the specific neural “activations” or internal processes that led me to conclude this was a horse.” And while it gave us some suggestions, we didn’t get the assurance we were after.</p>&#13;
&#13;
<p>Some solutions cite the source of its information. In <a data-type="xref" href="#ch05_figure_9_1740182048921797">Figure 5-9</a>, you can see that watsonx Code Assistant for Red Hat Ansible Lightspeed is pointing to the Ansible Galaxy community that was used to provide code completion for an Ansible playbook—that gives us a higher level of confidence.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_9_1740182048921797"><img alt="A screenshot of a chat  AI-generated content may be incorrect." src="assets/aivc_0509.png"/>&#13;
<h6><span class="label">Figure 5-9. </span>GenAI pointing to sources it used to return an output</h6>&#13;
</div></figure>&#13;
&#13;
<p>As helpful as these explanations are, they represent software trying to patch the holes and provide potential explanations based on the data that is running through the model at time of inference. They do not go to the core explanation of what is going on inside the model. What if you receive a data erasure request and you’re required by law to ensure learnings from that data aren’t in the model, or you need to specifically test an area of the model to see how it affects other areas?</p>&#13;
&#13;
<p>We don’t have a perfect answer for you; this is an area that is still actively maturing. However, there are some interesting new research innovations that point toward improvements in LLM explainability. Anthropic<a contenteditable="false" data-primary="Anthropic" data-type="indexterm" id="id793"/> (makers of the popular Claude Sonnet LLM)<a contenteditable="false" data-primary="Claude Sonnet LLM" data-type="indexterm" id="id794"/> released a groundbreaking paper about extracting interpretable features from its LLM.<sup><a data-type="noteref" href="ch05.html#id795" id="id795-marker">13</a></sup> Their technology extracted millions of features from one of its production models to showcase which set of neurons were activated for a particular concept. An example is shown in <a data-type="xref" href="#ch05_figure_10_1740182048921819">Figure 5-10</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_10_1740182048921819"><img alt="A screenshot of a computer  AI-generated content may be incorrect." src="assets/aivc_0510.png"/>&#13;
<h6><span class="label">Figure 5-10. </span>New innovations are emerging to help LLMs with explainability at the <span class="keep-together">activation level</span></h6>&#13;
</div></figure>&#13;
&#13;
<p>What’s particularly exciting about Anthropic’s research is they showed the potential to map different concepts to an extracted model feature map from their model. For example, Anthropic’s researchers found one area of features within Claude that was closely related to San Francisco’s Golden Gate Bridge.<sup><a data-type="noteref" href="ch05.html#id796" id="id796-marker">14</a></sup> Once identified, they cranked up the intensity (influence) of that feature, like a DJ at a tech startup after-party. And just like that, Claude became Golden Gate Claude, weaving the iconic bridge into every response. It became so biased, it was as if the San Francisco Tourism Board bootstrapped its funding because it would make every response somehow related to the Golden Gate Bridge! According to Anthropic, if you asked their model what the best way to spend $10 was, Claude would tell you to take a day trip driving across the Golden Gate Bridge. When asked to write a love story, it described a story about a car that fell in love with this famous San Francisco icon.</p>&#13;
&#13;
<p>Naturally, we wondered what it would say if we asked it who’d win the 2025 season’s Super Bowl (which is played in 2026). We’re sure it would tell us the San Francisco 49ers at a field beside the Golden Gate Bridge (they play at Levi’s Stadium, which is about 50 miles away). But then we’d have to call it out for hallucinating—and not because it suggested the stadium was close by. (Sorry 49ers fans. We just had to because we’re a bunch of Northeasterners and a Canadian who grew up with three-down football—which to two of the authors sounded like a hallucination when they first heard it—but the Canadian in the group assured everyone that it’s a real thing.)</p>&#13;
&#13;
<p>Another example of emerging AI research driving toward explainability<a contenteditable="false" data-primary="unlearning" data-type="indexterm" id="id797"/> is work on <em>unlearning</em>.<sup><a data-type="noteref" href="ch05.html#id798" id="id798-marker">15</a></sup> It’s like Yoda (the wise Jedi Master of <em>Star Wars</em> fame) sent a message to AI researchers from Dagobah telling them to figure out a way for LLMs to “unlearn what you have learned.” Unlearning is a process in which a model is trained, often through fine-tuning, to forget all about a specific topic. For example, researchers at Microsoft used an unlearning approach (we’ve affectionately decided to name it “ExpelliData”) to get Llama-2-7B to forget about the topic of <em>Harry Potter</em>.<sup><a data-type="noteref" href="ch05.html#id799" id="id799-marker">16</a></sup> It’s like one minute Llama was an expert on the finer rules of Quidditch and the next it’s keying in on the word <em>Potter</em> and now it’s talking about ceramic changes and dunting. As it turns out, neural networks can be just as susceptible to memory charms as Gilderoy Lockhart.</p>&#13;
&#13;
<p>Unlearning holds tremendous promise for helping address some of the LLM issues that are plaguing them—or could plague them in the future. For example, what of copyright<a contenteditable="false" data-primary="copyright" data-secondary="unlearning as tool to fix violations" data-type="indexterm" id="id800"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="legal issues" data-type="indexterm" id="id801"/>? What if a plaintiff like <em>The New York Times</em> prevails in its currently ongoing infringement case against OpenAI? Could this vendor unlearn the infringed content and be able to demonstrate that removal in a trillion-parameter model? What about regulatory rules like “the right to be forgotten”; companies need a realistic way to address such a request. Finally, it could assist with bias detection and correction as it helps explain why an LLM made the decision it did. Specifically, if a model changes a decision after unlearning about a concept, that provides more explainability into the factors driving its original output.</p>&#13;
&#13;
<p>The industry is still in the early stages of understanding how LLMs work. Comprehending their “thinking process” is vital for guiding their development and application. As we continue to unravel the mysteries of LLM interpretability, we move closer to creating AI systems that are not just powerful, but also transparent and aligned with human values. This journey of discovery may well reshape our understanding of AI and its potential impact on society<a contenteditable="false" data-primary="" data-startref="xi_ethicalprinciplesexplainability515794" data-type="indexterm" id="id802"/><a contenteditable="false" data-primary="" data-startref="xi_explainabilityprinciple515794" data-type="indexterm" id="id803"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Lineage—Tracing the Trail: Let Good Data Prevail" data-type="sect2"><div class="sect2" id="ch05_lineage_tracing_the_trail_let_good_data_prevail_1740182048943915">&#13;
<h2>Lineage—Tracing the Trail: Let Good Data Prevail</h2>&#13;
&#13;
<p>We’re not going to delve too deep into this lever<a contenteditable="false" data-primary="ethical principles" data-secondary="lineage" data-type="indexterm" id="xi_ethicalprincipleslineage519763"/><a contenteditable="false" data-primary="lineage principle" data-type="indexterm" id="xi_lineageprinciple519763"/><a contenteditable="false" data-primary="auditability" data-secondary="lineage" data-type="indexterm" id="xi_auditabilitylineage519763"/> here because we discussed this very topic in previous chapters (remember, you can’t have AI without an IA). With that said, we’ll explicitly note that this lever is about ensuring AI systems include details of their data, development, deployment, and maintenance so they can be audited throughout their lifecycle.</p>&#13;
&#13;
<p>Think of this just like water. If you know where the water comes from, you’ll have more confidence in it. For example, you likely trust the water out of your tap more than a farm’s garden hose. If you know what treatments have been applied to your water, you’re likely to trust it more too. For example, did it go through some kind of reverse osmosis filter? Think of your data lineage as you do water lineage.</p>&#13;
&#13;
<p><a data-type="xref" href="#ch05_figure_11_1740182048921839">Figure 5-11</a> shows the IBM Data Factory that IBM uses to track data lineage for its models. There are literally dozens of layers of detail in the data lakehouse where all this metadata is stored. This example shows the details of a specific data pile (multiple data piles are used to create a training dataset), the sources that make up that pile (all linked), models that are built using this dataset, and more.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_11_1740182048921839"><img alt="A screenshot of a computer  AI-generated content may be incorrect." src="assets/aivc_0511.png"/>&#13;
<h6><span class="label">Figure 5-11. </span>Some of the lineage of a dataset used in training</h6>&#13;
</div></figure>&#13;
&#13;
<p>Model cards<a contenteditable="false" data-primary="model cards, for lineage tracing" data-type="indexterm" id="id804"/> are also critical. They will showcase the training pipeline, the datasets used (whereas <a data-type="xref" href="#ch05_figure_11_1740182048921839">Figure 5-11</a> is showcasing the data within a dataset), pipeline activities, and more. You can think of them as nutrition labels for your AI. For example, the <a href="https://oreil.ly/Np9bJ">granite-3-8b-instruct model card</a> transparently showcases that model’s architecture (number of attention heads, embedding size, and other nerd stuff), the number of active parameters (which would matter in a Mixture of Experts model), the number of training tokens used, the data, the infrastructure on which the model was built, along with ethical considerations and limitations.</p>&#13;
&#13;
<p>We’ll end this section with the takeaways. More trust and explainability accrues from more transparency: of the dataset, the model build recipe, where it was made, who made it, etc. Financial reporting has this concept well in hand, as does the food industry. What up, AI?</p>&#13;
&#13;
<p>Thinking about the food industry, until the late 1960s, we knew very little information about what went into the foods we bought. Americans prepared most food at home, with fairly common ingredients. We didn’t have much need to know more. Then, food production began to evolve. Our foods contained more artificial additives. In 1969, a White House conference recommended the U.S. Food &amp; Drug Administration (FDA) take on a new responsibility—developing a new way to understand the ingredients and the nutritional value of what we eat.</p>&#13;
&#13;
<p>Similar to the arrival of processed foods, the advent of GenAI and agents mark a new age—and whether it turns out to be good or bad for us will depend on what goes into it. The difference lies in the rapid pace at which AI is developing. It took about 20 years to go from an FDA conference on food to nutrition labels. AI doesn’t have that kind of time—we’d argue it doesn’t have two years. The good news is that businesses can take the first, and perhaps the most critical, step of identifying harmful or unacceptable AI by understanding lineage<a contenteditable="false" data-primary="" data-startref="xi_governanceethicalprinciples58427" data-type="indexterm" id="id805"/><a contenteditable="false" data-primary="" data-startref="xi_ethicalprinciples510670" data-type="indexterm" id="id806"/><a contenteditable="false" data-primary="" data-startref="xi_ethicalprincipleslineage519763" data-type="indexterm" id="id807"/><a contenteditable="false" data-primary="" data-startref="xi_lineageprinciple519763" data-type="indexterm" id="id808"/><a contenteditable="false" data-primary="" data-startref="xi_auditabilitylineage519763" data-type="indexterm" id="id809"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Regulations—The Section That Wasn’t Supposed to Be" data-type="sect1"><div class="sect1" id="ch05_regulations_the_section_that_wasn_t_supposed_to_be_1740182048943983">&#13;
<h1>Regulations—The Section That Wasn’t Supposed to Be</h1>&#13;
&#13;
<p>We noted that it didn’t make sense for us to go into details on the state of current regulations<a contenteditable="false" data-primary="governance" data-secondary="regulations" data-type="indexterm" id="xi_governanceregulations5212108"/><a contenteditable="false" data-primary="regulation" data-type="indexterm" id="xi_regulation5212108"/> because they are ever changing and somewhat fragmented. That said, we started to feel a bit guilty, so we thought we’d spend a bit of time on some points of view here to help you navigate what’s already here and on the horizon, as opposed to educating you on the nuances of what these regulations entail.</p>&#13;
&#13;
<p>It’s important to remember<a contenteditable="false" data-primary="EU AI Act" data-type="indexterm" id="id810"/> that the <a href="https://oreil.ly/5TyDt">EU AI Act</a> was implemented in 2024, and it has some far-reaching impacts considering we live in a global economy. We believe this will lead other countries to follow the same as the EU GDPR law did. How so? If you look at data handling regulations in the world today, companies either had to comply because they had EU customers, or their own governments were slow or fast followers, eventually adopting many of the best practices from that law. This is no different than the technology trickle-down effects we see, where a lot of the technology you use today was born in the military, gaming industry, social media, and one other that we’ll leave out of our list. We’re positive that regulation around AI is only going to intensify as concerns like fair business practices, fraud, copyright, civil liberties, <span class="keep-together">privacy</span>, fairness, job loss, national security, and more get into the hands of governments. While we can’t predict the future—for example, the new US government administration that took over to start 2025 has a different point of view than the <span class="keep-together">last—we</span> are certain that attention is only going to intensify. Be assured that if you’re not prepared for ongoing change, your organization is going to have serious problems when it comes to adopting AI without a comprehensive, configurable governance system in place.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch05_more_to_come_1740182048944042">&#13;
<h1>More to Come</h1>&#13;
&#13;
<p>The US has the largest economy in the world. And while many would say the regulations around the Biden executive order (EO) 14110 on AI safety didn’t go far enough, there are many levels of the US government working on all sorts of regulatory protections and policies attempting to balance innovation but curb AI harm. The issue with EOs is that while they operate as law, they can be revoked by new administrations. President Trump’s administration has already revoked EO 14110, but states like Connecticut, Illinois, Texas, and many others are all working through their own laws to balance innovation and safety. Municipalities are piling on with versions of New York City Local Law 144, which we commented on in <a data-type="xref" href="ch04.html#ch04_the_use_case_chapter_1740182047877425">Chapter 4</a>.</p>&#13;
&#13;
<p>At the time of this writing, there is already much focus by all levels of government on risk assessment and explainability<a contenteditable="false" data-primary="explainability principle" data-type="indexterm" id="id811"/><a contenteditable="false" data-primary="ethical principles" data-secondary="explainability" data-type="indexterm" id="id812"/> related to the way LLMs are trained and how they achieve the outcomes they achieve (a focus area directly related to one of the levers in our framework). Explainability around hiring, housing, judicial, and more already face increasing requirements. And should it become law, the 2024 bipartisan Nurture Originals, Foster Art, and Keep Entertainment Safe (NO FAKES) Act will address some of the issues mentioned at the start of this chapter.</p>&#13;
&#13;
<p>All of this is not just happening in the EU and the US either. Canada, China, and eight other countries in Asia have emergent (or existing by the time you read this book) regulatory frameworks for AI. Dozens more in other parts of the world will follow suit. It’s happening everywhere.</p>&#13;
</div></aside>&#13;
&#13;
<section data-pdf-bookmark="What to Regulate—Our Point of View" data-type="sect2"><div class="sect2" id="ch05_what_to_regulate_our_point_of_view_1740182048944102">&#13;
<h2>What to Regulate—Our Point of View</h2>&#13;
&#13;
<p>People ask us our opinions<a contenteditable="false" data-primary="regulation" data-secondary="appropriate targets for" data-type="indexterm" id="id813"/> on what to regulate all the time. It’s like the classic question of whether the glass is half full or half empty. We think that question misses the point—the realist knows that sooner or later, someone’s going to drink whatever is in the glass, and they’ll be the one washing it. With that in mind, allow us to share our realistic viewpoint: regulate the usage of AI as opposed to the AI technology itself. Let’s clarify a little more: we think that AI needs guardrails and regulations to avoid user harm, but the focus should be on regulating specific use cases, not to stomp over the innovation of technology that has tremendous potential to transform the world.</p>&#13;
&#13;
<p>Consider this question and weigh it thoughtfully: do you think all the world’s countries will unify and follow a quorum of commitments for responsible use of AI under all circumstances? Putting geopolitics aside, the fact that some regulations have granularity of city or association as a binding target tells you it’s never going to happen. We don’t think we’re being pessimistic; we just know someone is going to end up with a dirty glass in their hands and have to wash it.</p>&#13;
&#13;
<p>Yes, with AI, there’s a huge potential that misinformation<a contenteditable="false" data-primary="misinformation spreading issue for AI" data-type="indexterm" id="id814"/> can spread really fast now. AI can make misinformation more persuasive. However, stopping AI won’t achieve anything. Bad actors will move from one country to another to spread harm since AI can easily cross boundaries. We’d like to see governments regulate higher risk levels that correlate to the specifics of what the AI is trying to do, what it could do, or the potential for harm it could impose. For example, the EU Artificial Intelligence Act has a four-tier classification system for AI risk: Unacceptable, High, Limited, and Minimal. Each tier is bound to its own regulation articles within this act. For example, the top tier is Unacceptable Risk (Article 5) and prohibits usage such as behavior manipulation, remote biometric identification for police enforcement, social scoring by public authorities, and such. As you can imagine, a violation of this tier results in much more severe penalties than the third tier (Limited Risk—Article 52) which includes the risk of impersonation or deception. We hope the goal focuses on spotting those “potential for danger” AI use cases and telling the perpetrators that if they’re caught, they’ll be subjected to penalties, fines, and criminal prosecution.</p>&#13;
&#13;
<p>And when it comes to regulated industries, we also think the biggest question to ask is, “Are there humans in the loop?” We believe humans <em>should</em> be in the loop—“ask and adjust” is crucial. It’s a pretty fundamental point, but not everybody sees it that way. But we think this is critical (especially with agentic AI) and an effective safeguard to go with actual usage of this technology.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Managing the AI Lifecycle" data-type="sect2"><div class="sect2" id="ch05_managing_the_ai_lifecycle_1740182048944161">&#13;
<h2>Managing the AI Lifecycle</h2>&#13;
&#13;
<p>We believe<a contenteditable="false" data-primary="regulation" data-secondary="AI lifecycle management" data-type="indexterm" id="xi_regulationAIlifecyclemanagement523024"/> that given the reasonable assumption you will at least attempt to comply with all regulatory<a contenteditable="false" data-primary="lifecycle management" data-type="indexterm" id="xi_lifecyclemanagement5230117"/> orders you have or will receive, it’s clear that you’re going to end up with challenges around tracking your models. It’s not unlike all those encryption keys we talked about earlier. In short, you will need the ability to track your models against regulatory standards in areas such as accuracy and fairness, and you will need technology to help you do that.</p>&#13;
&#13;
<p>For example, <a data-type="xref" href="#ch05_figure_12_1740182048921861">Figure 5-12</a> shows a dashboard we set up to track a multimodel deployment using watsonx.governance<a contenteditable="false" data-primary="watsonx.governance" data-type="indexterm" id="id815"/>. Our dashboard gives us a quick view of our environment. There are LLMs from OpenAI, IBM, Meta, and other models that are in a review state. In our example, we have five noncompliant models that need our attention. Other widgets define use cases, risk tiers, hosting locations (on premises or at a hyper scaler), departmental use (great idea for chargebacks), position in the approval lifecycle, and more. Of course, you can drill down into these details, but one of the things we like about this tool the most is its ability to attach a regulatory framework to a model to help define and govern it.</p>&#13;
&#13;
<p>The toolset you choose should also provide the ability to explain decisions and automatically collect metadata so auditors can determine how models were trained and why they generated the output they did.</p>&#13;
&#13;
<figure><div class="figure" id="ch05_figure_12_1740182048921861"><img alt="A screenshot of a computer  AI-generated content may be incorrect." src="assets/aivc_0512.png"/>&#13;
<h6><span class="label">Figure 5-12. </span>Using watsonx.governance to build a dashboard and track a multimodel deployment environment</h6>&#13;
</div></figure>&#13;
&#13;
<section data-pdf-bookmark="What lies beneath" data-type="sect3"><div class="sect3" id="ch05_what_lies_beneath_1740182048944220">&#13;
<h3>What lies beneath</h3>&#13;
&#13;
<p>While <a data-type="xref" href="#ch05_figure_12_1740182048921861">Figure 5-12</a> gave you a glimpse of a powerful dashboard to manage AI, what lies beneath are the actual orchestration and operational flows to keep you from falling over the edge. We gave an example of model drift<a contenteditable="false" data-primary="model drift" data-type="indexterm" id="id816"/> earlier in this chapter. The fact that models drift implies that they require lifecycle management. In reality, the moment you put a model into production is the moment it starts to go stale. As you set up your AI governance practice, with focus on the levers outlined in this chapter, know that it must not be confined to the data science department. It requires information to be shared and decisions to be made across the entire enterprise, from a business unit’s initial request for a model, to the approval of infrastructure resources to inference it, governance of the training data, development, testing and tuning, risk assessment, through deployment, and beyond. Good AI governance practices will involve both technical and non-technical stakeholders and must not only automate as much of the process as possible to reduce strain on the data science department, but must also ensure that decision makers have access to timely, relevant data that they need to speed time to value. Your AI platform should automatically capture metadata, including the training data and frameworks used to build the model, along with evaluation information as the model progresses from use case request to development to test to deployment. That data should be made available to approvers using a <span class="keep-together">searchable</span>, governed catalog, ensuring that decision makers have a complete picture of the model’s lineage and performance.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="An example of an end-to-end governed process" data-type="sect3"><div class="sect3" id="ch05_an_example_of_an_end_to_end_governed_process_1740182048944280">&#13;
<h3>An example of an end-to-end governed process</h3>&#13;
&#13;
<p>If you have the right tools and lifecycle management<a contenteditable="false" data-primary="governance" data-secondary="end-to-end governed process" data-type="indexterm" id="id817"/>, then you have a chance to implement an end-to-end AI governance process that flows something like this:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Once the model proposal has gone through the appropriate approval process, a model entry is created in your model inventory. This entry is continuously updated with new information.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Model developers use their tools and models of choice to build AI solutions. Training data and metrics are automatically captured and saved to the model entry (assuming the vendor exposes this—this is why you want models that are open). Custom information can also be saved.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>When the preproduction model is evaluated for accuracy, drift, and bias, the performance metadata is captured and synced.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The model is reviewed and approved for production.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The model is deployed wherever you decide to deploy it (on premises, on the edge, in the cloud), and once again, the relevant metadata is captured and synced.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Finally, the production model is continuously monitored, and the performance data is captured and synced. A dashboard (like the one in <a data-type="xref" href="#ch05_figure_12_1740182048921861">Figure 5-12</a>) provides a comprehensive view of the performance metrics for all models (no matter the vendor), allowing stakeholders to proactively identify and react to any issues<a contenteditable="false" data-primary="" data-startref="xi_governanceregulations5212108" data-type="indexterm" id="id818"/><a contenteditable="false" data-primary="" data-startref="xi_regulationAIlifecyclemanagement523024" data-type="indexterm" id="id819"/><a contenteditable="false" data-primary="" data-startref="xi_lifecyclemanagement5230117" data-type="indexterm" id="id820"/><a contenteditable="false" data-primary="" data-startref="xi_regulation5212108" data-type="indexterm" id="id821"/>.</p>&#13;
	</li>&#13;
</ol>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Wrapping It Up" data-type="sect1"><div class="sect1" id="ch05_wrapping_it_up_1740182048944338">&#13;
<h1>Wrapping It Up</h1>&#13;
&#13;
<p>One of the founding fathers of the US (and its fourth president), James Madison, once said, “The circulation of confidence is better than the circulation of money.” His point was: it’s not just the flow of wealth that matters, but more so the underlying trust and confidence holding social, political, and economic systems together. With the place that GenAI and agents are shaping up to take in history, he would have surely added it to his list.</p>&#13;
&#13;
<p>Indeed, most companies’ culture looks at many of the topics outlined in this chapter as typical regulatory compliance and defaults to “a least-effort-to-comply approach.” The topics covered in this chapter can be repurposed for other benefits and accelerate other journeys<a contenteditable="false" data-primary="" data-startref="xi_governance5359" data-type="indexterm" id="id822"/>. We can’t help but feel something might bother you from the preceding list—we said “chance.” Why did we say that? Because governance is about culture, the technology helps you implement the culture. But always remember: <em>AI that people trust is AI that people will use</em>.</p>&#13;
&#13;
<p>We recognize there was a lot to cover in this chapter with an unfair amount of space to allot to it. That said, we hope that you’ve gotten a sense of the things you need to learn more about. And speaking of learning, that’s where we go next.</p>&#13;
<!--last sentence above is former pull quote --></div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id711"><sup><a href="ch05.html#id711-marker">1</a></sup> There are some uncomfortable topics that warrant discussion here. <em>We</em> don’t like writing about them, but they are important for you to understand. </p><p data-type="footnote" id="id715"><sup><a href="ch05.html#id715-marker">2</a></sup> Case: Mata v. Avianca, Inc., 1:2022cv01461, filed in the South District of New York. </p><p data-type="footnote" id="id737"><sup><a href="ch05.html#id737-marker">3</a></sup> Arielle Waldman, “FBI: Criminals Using AI to Commit Fraud ‘on a Larger Scale,’” TechTarget, December 4, 2024, <a href="https://oreil.ly/7VgiB"><em>https://oreil.ly/7VgiB</em></a>. </p><p data-type="footnote" id="id738"><sup><a href="ch05.html#id738-marker">4</a></sup> CNN, “Finance Worker Pays Out $25 Million After Video Call with Deepfake ‘Chief Financial Officer,’” February 4, 2024, <a href="https://oreil.ly/xwZY1"><em>https://oreil.ly/xwZY1</em></a>. </p><p data-type="footnote" id="id747"><sup><a href="ch05.html#id747-marker">5</a></sup> Pete Evans, “Apple Users Can Say No to Being Tracked with New Software Update,” CBC News, April 26, 2021, <a href="https://oreil.ly/QL2Fe"><em>https://oreil.ly/QL2Fe</em></a>. </p><p data-type="footnote" id="id748"><sup><a href="ch05.html#id748-marker">6</a></sup> Acxiom Corporation Form 10-K Annual Report for the Fiscal year ended March 31, 2018, filed with the U.S. Securities and Exchange Commission, May 21, 2018, <a href="https://oreil.ly/SpkKt"><em>https://oreil.ly/SpkKt</em></a>. </p><p data-type="footnote" id="id760"><sup><a href="ch05.html#id760-marker">7</a></sup> By using a catch term like <em>ethics</em>, we mean to capture all things that go into ensuring governance, explainability, fair use, privacy, and more around your AI projects—good acting. We won’t flesh out all the ethical considerations in this chapter, but you’ll find almost all of them can be binned to one of the levers we introduce you to in this section. </p><p data-type="footnote" id="id762"><sup><a href="ch05.html#id762-marker">8</a></sup> Stephen Jones, “Automated Hiring Systems Are ‘Hiding’ Candidates from Recruiters—How Can We Stop This?,” World Economic Forum, September 14, 2021, <a href="https://oreil.ly/2C-dn"><em>https://oreil.ly/2C-dn</em></a>. </p><p data-type="footnote" id="id769"><sup><a href="ch05.html#id769-marker">9</a></sup> Robert Bartlett et al., “Consumer-Lending Discrimination in the FinTech Era,” (working paper, University of California, Berkeley, 2019), <a href="https://oreil.ly/C5iaB"><em>https://oreil.ly/C5iaB</em></a>. </p><p data-type="footnote" id="id778"><sup><a href="ch05.html#id778-marker">10</a></sup> Fengqing Jiang et al., “ArtPrompt: ASCII Art-Based Jailbreak Attacks Against Aligned LLMs,” preprint, arXiv, February 19, 2024, arXiv:2402.11753, <a href="https://arxiv.org/abs/2402.11753"><em>https://arxiv.org/abs/2402.11753</em></a>. </p><p data-type="footnote" id="id783"><sup><a href="ch05.html#id783-marker">11</a></sup> Red teaming is a process for testing cybersecurity effectiveness where ethical hackers conduct a simulated and nondestructive cyberattack. Their simulated attacks help organizations identify vulnerabilities in their systems and make targeted improvements to security operations. </p><p data-type="footnote" id="id791"><sup><a href="ch05.html#id791-marker">12</a></sup> Neil Vigdor, “Apple Card Investigated After Gender Discrimination Complaints,” <em>The New York Times</em>, November 10, 2019, <a href="https://oreil.ly/Mo9NZ"><em>https://oreil.ly/Mo9NZ</em></a>. </p><p data-type="footnote" id="id795"><sup><a href="ch05.html#id795-marker">13</a></sup> “Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet,” Transformer Circuits, 2024, accessed October 25, 2023, <a href="https://oreil.ly/AFZ4w"><em>https://oreil.ly/AFZ4w</em></a>. </p><p data-type="footnote" id="id796"><sup><a href="ch05.html#id796-marker">14</a></sup> “Golden Gate Claude,” Anthropic, accessed October 25, 2023, <a href="https://oreil.ly/o5r6S"><em>https://oreil.ly/o5r6S</em></a>. </p><p data-type="footnote" id="id798"><sup><a href="ch05.html#id798-marker">15</a></sup> “Teaching Large Language Models to ‘Forget’ Unwanted Content,” IBM Insights, 2024, <a href="https://oreil.ly/hzltJ"><em>https://oreil.ly/hzltJ</em></a>. </p><p data-type="footnote" id="id799"><sup><a href="ch05.html#id799-marker">16</a></sup> Ronan Eldan and Mark Russinovich, “Who’s Harry Potter? Approximate Unlearning in LLMs,” preprint, arXiv, October 4, 2023, <a href="https://arxiv.org/abs/2310.02238"><em>https://arxiv.org/abs/2310.02238</em></a>. </p></div></div></section></body></html>