["```py\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport numpy as np\nimport matplotlib.pylab as plt\nimport os\nfrom datetime import datetime\n```", "```py\n(train_images, train_labels), (test_images, test_labels) = \ndatasets.cifar10.load_data()\n\ntrain_images, test_images = train_images / 255.0, \n test_images / 255.0\n```", "```py\nCLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat',\n               'deer','dog', 'frog', 'horse', 'ship', 'truck']\n```", "```py\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[:500], test_labels[:500]))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[500:], test_labels[500:]))\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n(train_images, train_labels))\n```", "```py\ntrain_dataset_size = len(list(train_dataset.as_numpy_iterator()))\nprint('Training data sample size: ', train_dataset_size)\n\nvalidation_dataset_size = len(list(validation_dataset.\nas_numpy_iterator()))\nprint('Validation data sample size: ', validation_dataset_size)\n\ntest_dataset_size = len(list(test_dataset.as_numpy_iterator()))\nprint('Test data sample size: ', test_dataset_size)\n```", "```py\nTraining data sample size:  50000\nValidation data sample size:  500\nTest data sample size:  9500\n```", "```py\nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices: {}'.format(\nstrategy.num_replicas_in_sync))\n```", "```py\nNumber of devices: 1\n```", "```py\nBUFFER_SIZE = 10000\nBATCH_SIZE_PER_REPLICA = 64\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n```", "```py\ntrain_dataset = train_dataset.repeat().shuffle(BUFFER_SIZE).batch(\n                BATCH_SIZE)\n\nvalidation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(\n                validation_dataset_size)\n\ntest_dataset = test_dataset.batch(test_dataset_size)\n\nSTEPS_PER_EPOCH = train_dataset_size // BATCH_SIZE_PER_REPLICA\nVALIDATION_STEPS = 1\n```", "```py\ndef build_model():\n  with strategy.scope():\n    model = tf.keras.Sequential([\n      tf.keras.layers.Conv2D(32, kernel_size=(3, 3), \n        activation='relu', \n        name = 'conv_1',\n        kernel_initializer='glorot_uniform', \n        padding='same', \n        input_shape = (32,32,3)),\n      tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n      tf.keras.layers.Conv2D(64, kernel_size=(3, 3), \n        activation='relu', \n        name = 'conv_2',\n        kernel_initializer='glorot_uniform', \n        padding='same'),\n      tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n      tf.keras.layers.Flatten(name = 'flat_1'),\n      tf.keras.layers.Dense(256, activation='relu', \nkernel_initializer='glorot_uniform', name = 'dense_64'),\n      tf.keras.layers.Dense(10, activation='softmax', \n        name = 'custom_class')\n    ])\n    model.build([None, 32, 32, 3])\n\n    model.compile(\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(\n           from_logits=True),\n      optimizer=tf.keras.optimizers.Adam(),\n      metrics=['accuracy'])\n    return model\n```", "```py\nmodel = build_model()\n```", "```py\nMODEL_NAME = 'myCIFAR10-{}'.format(datetime.now().strftime(\n\"%Y%m%d-%H%M%S\"))\n```", "```py\nprint(MODEL_NAME)\nmyCIFAR10-20210319-214456\n```", "```py\ncheckpoint_dir = './' + MODEL_NAME\ncheckpoint_prefix = os.path.join(checkpoint_dir, \n    \"ckpt-{epoch}\")\nprint(checkpoint_prefix)\n```", "```py\n./myCIFAR10-20210319-214456/ckpt-{epoch}\n```", "```py\nmyCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    monitor='val_accuracy',\n    mode='max',\n    save_weights_only = True,\n    save_best_only = True\n    )\n```", "```py\nmyCallbacks = [\nmyCheckPoint\n]\n```", "```py\nmodel.fit(\n    train_dataset,\n    epochs=30,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=validation_dataset,\n    validation_steps=VALIDATION_STEPS,\n    callbacks=myCallbacks)\n```", "```py\nEpoch 7/30\n781/781 [==============================] - 4s 6ms/step \nloss: 0.0202 - accuracy: 0.9972 - val_loss: 10.5573 \nval_accuracy: 0.6900\nEpoch 8/30\n781/781 [==============================] - 4s 6ms/step \nloss: 0.0217 - accuracy: 0.9967 - val_loss: 10.4517 \nval_accuracy: 0.7000\nEpoch 9/30\n781/781 [==============================] - 5s 6ms/step \nloss: 0.0203 - accuracy: 0.9971 - val_loss: 10.4553 \nval_accuracy: 0.7080\nEpoch 10/30\n781/781 [==============================] - 5s 6ms/step \nloss: 0.0232 - accuracy: 0.9966 - val_loss: 11.3774 \nval_accuracy: 0.6600\n\u2026\nEpoch 30/30\n781/781 [==============================] - 5s 6ms/step\nloss: 0.0221 - accuracy: 0.9971 - val_loss: 11.9106 \nval_accuracy: 0.6680\n<tensorflow.python.keras.callbacks.History at 0x7fd447ce9a50>\n```", "```py\n!ls -lrt {checkpoint_dir}\n```", "```py\ntotal 87896\n-rw-r--r-- 1 root root 12852661 Mar 19 21:45 \nckpt-1.data-00000-of-00001\n-rw-r--r-- 1 root root     2086 Mar 19 21:45 ckpt-1.index\n-rw-r--r-- 1 root root 12852661 Mar 19 21:45 \nckpt-2.data-00000-of-00001\n-rw-r--r-- 1 root root     2086 Mar 19 21:45 ckpt-2.index\n-rw-r--r-- 1 root root 12852661 Mar 19 21:45 \nckpt-3.data-00000-of-00001\n-rw-r--r-- 1 root root     2086 Mar 19 21:45 ckpt-3.index\n-rw-r--r-- 1 root root 12852661 Mar 19 21:45 \nckpt-4.data-00000-of-00001\n-rw-r--r-- 1 root root     2086 Mar 19 21:45 ckpt-4.index\n-rw-r--r-- 1 root root 12852661 Mar 19 21:46 \nckpt-7.data-00000-of-00001\n-rw-r--r-- 1 root root     2086 Mar 19 21:46 ckpt-7.index\n-rw-r--r-- 1 root root 12852661 Mar 19 21:46 \nckpt-8.data-00000-of-00001\n-rw-r--r-- 1 root root     2086 Mar 19 21:46 ckpt-8.index\n-rw-r--r-- 1 root root 12852661 Mar 19 21:46 \nckpt-9.data-00000-of-00001\n-rw-r--r-- 1 root root     2086 Mar 19 21:46 ckpt-9.index\n-rw-r--r-- 1 root root       69 Mar 19 21:46 checkpoint\n```", "```py\ntf.train.latest_checkpoint(checkpoint_dir)\n```", "```py\n./myCIFAR10-20210319-214456/ckpt-9\n```", "```py\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n```", "```py\nKERAS_MODEL_PATH = \"/models/HDF5/tfkeras_cifar10.h5\"\nmodel.save(KERAS_MODEL_PATH)\n```", "```py\n!ls -lrt {KERAS_MODEL_PATH}\n```", "```py\n-rw-r--r-- 1 root root 12891752 Mar 20 21:19 \n/models/HDF5/tfkeras_cifar10.h5\n```", "```py\nnew_h5_model = tf.keras.models.load_model(\n'/models/HDF5/tfkeras_cifar10.h5')\n```", "```py\nnew_h5_model.predict(test_dataset)\n```", "```py\narray([[1.77631108e-07, 8.12380506e-07, 9.94834751e-02, ...,\n        4.93609463e-04, 1.97697682e-05, 2.55090754e-06],\n\n       [6.76187535e-12, 6.38716233e-11, 1.67756411e-07, ...,\n        9.99815047e-01, 1.25759464e-14, 1.24499985e-11]], \n        dtype=float32)\n```", "```py\nSAVED_MODEL_PATH = \"/models/pb/tfsaved_cifar10\"\ntf.saved_model.save(model, SAVED_MODEL_PATH)\n```", "```py\n!ls -lrt {SAVED_MODEL_PATH}\n```", "```py\ndrwxr-xr-x 2 root root   4096 Mar 20 21:50 variables\ndrwxr-xr-x 2 root root   4096 Mar 20 21:50 assets\n-rw-r--r-- 1 root root 138184 Mar 20 21:50 saved_model.pb\n```", "```py\n!ls -lrt {SAVED_MODEL_PATH}/variables\n```", "```py\n-rw-r--r-- 1 root root 12856259 Mar 20 21:50 \nvariables.data-00000-of-00001\n-rw-r--r-- 1 root root     2303 Mar 20 21:50 variables.index\n```", "```py\nload_strategy = tf.distribute.MirroredStrategy()\nwith load_strategy.scope():\n  load_options = tf.saved_model.LoadOptions(\n              experimental_io_device='/job:localhost')\n  loaded_pb = tf.keras.models.load_model(\n              SAVED_MODEL_PATH, \n              options=load_options)\n```", "```py\nloaded_pb.predict(test_dataset)\n```", "```py\narray([[1.77631108e-07, 8.12380506e-07, 9.94834751e-02, ...,\n        4.93609463e-04, 1.97697682e-05, 2.55090754e-06],\n\n       [6.76187535e-12, 6.38716233e-11, 1.67756411e-07, ...,\n        9.99815047e-01, 1.25759464e-14, 1.24499985e-11]], \ndtype=float32)\n```", "```py\nloaded_pb.predict(test_images[500:])\n```", "```py\narray([[1.77631108e-07, 8.12380506e-07, 9.94834751e-02, ...,\n        4.93609463e-04, 1.97697682e-05, 2.55090754e-06],\n\n       [6.76187535e-12, 6.38716233e-11, 1.67756411e-07, ...,\n        9.99815047e-01, 1.25759464e-14, 1.24499985e-11]], \ndtype=float32)\n```", "```py\nSAVED_MODEL_PATH = \"./models/CIFAR10/001\"\ntf.saved_model.save(model, SAVED_MODEL_PATH)\n```", "```py\nmodels\n    CIFAR10\n        001\n            assets\n            saved_model.pb\n            variables\n```", "```py\ndocker pull tensorflow/serving\n```", "```py\ndocker run -d --name serv_base_img tensorflow/serving\n```", "```py\ndocker cp $PWD/CIFAR10 serv_base_img:/models/cifar10\n```", "```py\ndocker commit --change \"ENV MODEL_NAME cifar10model\" \nserv_base_img cifar10model\\\n```", "```py\ndocker kill serv_base_img\n```", "```py\ndocker run -p 8501:8501 \\\n  --mount type=bind,\\\nsource=$PWD/CIFAR10,\\\ntarget=/models/cifar10 \\\n  -e MODEL_NAME=cifar10 -t tensorflow/serving &\n```", "```py\n    import tensorflow as tf\n    from tensorflow.keras import datasets\n    import requests\n    import json\n    import numpy as np\n    ```", "```py\n    (train_images, train_labels), (test_images, test_labels) = \n    datasets.cifar10.load_data()\n\n    # Normalize pixel values to be between 0 and 1\n    train_images, test_images = train_images / 255.0, \n     test_images / 255.0\n    test_images = test_images[500:510]\n    ```", "```py\n    DATA = json.dumps({\n        \"instances\": test_images.tolist()\n    })\n    ```", "```py\n    HEADERS = {\"content-type\": \"application/json\"}:\n    ```", "```py\n    response = requests.post(\n    'http://localhost:8501/v1/models/cifar10:predict', \n    data=DATA, headers=HEADERS)\n    ```", "```py\n    response.json()\n    ```", "```py\n    {'predictions': [[9.83229938e-07,\n       1.24386987e-10,\n       0.0419323482,\n       0.00232415553,\n       0.91928196,\n       3.26286099e-05,\n       0.0276549552,\n       0.00877290778,\n       8.02750222e-08,\n       5.4040652e-09],\n    \u2026..\n    [2.60355654e-10,\n       5.17050935e-09,\n       0.000181202529,\n       1.92517109e-06,\n       0.999798834,\n       1.04122219e-05,\n       3.32912987e-06,\n       4.38272036e-06,\n       4.2479078e-09,\n       9.54967494e-11]]}\n    ```", "```py\n    predictions_prob_list = response.json().get('predictions')\n    ```", "```py\n    CLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat',\n                   'deer','dog', 'frog', 'horse', 'ship', 'truck']\n    ```", "```py\n    predictions_array = np.asarray(predictions_prob_list)\n    predictions_idx = np.argmax(predictions_array, axis = 0)\n    ```", "```py\n    for i in predictions_idx:\n        print(CLASS_NAMES[i])\n    ```", "```py\n    ship\n    ship\n    airplane\n    bird\n    truck\n    ship\n    automobile\n    frog\n    ship\n    horse\n    ```"]