<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span> </span> <span class="chapter-title-text">Working with natural language</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">The hidden structures in unstructured data</li>
<li class="readable-text" id="p3">A search-centric philosophy of language </li>
<li class="readable-text" id="p4">Exploring distributional semantics and vector-based embeddings</li>
<li class="readable-text" id="p5">Modeling domain-specific knowledge</li>
<li class="readable-text" id="p6">Challenges with natural language and querys </li>
<li class="readable-text" id="p7">Applying natural language understanding techniques to both content and signals</li>
</ul>
</div>
<div class="readable-text" id="p8">
<p>In the first chapter, we provided a high-level overview of what it means to build an AI-powered search system. Throughout the rest of the book, we’ll explore and demonstrate the numerous ways your search application can continuously learn from your content and your users’ behavioral signals to better understand your content, your users, and your domain, and to ultimately deliver users the answers they need. We will get much more hands-on in chapter 3, firing up a search server of your choice and a data processing layer (Apache Spark) and starting with the first of our Jupyter notebooks, which we’ll use throughout the book to walk through many step-by-step examples.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>Before we dive into those hands-on examples and specific implementations, however, it is important in this chapter that we first establish a shared mental model for the higher-level problems we’re trying to solve. Specifically, when it comes to intelligent search, we have to deal with many complexities and nuances in natural language—both in the content we’re searching and in our users’ search queries. We have to deal with keywords, entities, concepts, misspellings, synonyms, acronyms, initialisms, ambiguous terms, explicit and implied relationships between concepts, hierarchical relationships usually found in taxonomies, higher-level relationships usually found in ontologies, and specific instances of entity relationships usually found in comprehensive knowledge graphs.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>While it might be tempting to dive immediately into some specific problems, like how to automatically learn misspellings from content or how to discover synonyms from mining user search sessions, it will be more prudent to first establish a conceptual foundation that explains what <em>kinds</em> of problems we have to deal with in search and natural language understanding (NLU). Establishing that philosophical foundation will enable us to build better end-to-end solutions in our AI-powered search system, where all the parts work together in a cohesive and integrated way. This chapter will thus provide the philosophical underpinnings for how we tackle the problems of natural language understanding throughout this book and how we apply those solutions to make our search applications more intelligent. </p>
</div>
<div class="readable-text intended-text" id="p11">
<p>We’ll begin by discussing some common misconceptions about the nature of free text and other unstructured data sources.</p>
</div>
<div class="readable-text" id="p12">
<h2 class="readable-text-h2" id="sigil_toc_id_22"><span class="num-string">2.1</span> The myth of unstructured data</h2>
</div>
<div class="readable-text" id="p13">
<p>The term “unstructured data” has been used for years to describe textual data, because it does not appear to be formatted in a way that can be readily interpreted and queried. The widely held idea that text, or any other data that doesn’t fit a predefined schema (“structure”), is actually “unstructured”, however, is a myth that we’ll spend time reconsidering throughout this section. </p>
</div>
<div class="readable-text intended-text" id="p14">
<p>If you look up <em>unstructured data</em> in Wikipedia, it is defined as “information that either does not have a pre-defined data model or is not organized in a pre-defined manner”. The entry goes on to say that “unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as well”.</p>
</div>
<div class="readable-text intended-text" id="p15">
<p>The phrase “unstructured data” is a poor term to describe textual content, however. In reality, the terms and phrases present in text encode an enormous amount of meaning, and the linguistic rules applied to the text to give it meaning serve as their own structure. Calling text unstructured is a bit like calling a song playing on the radio “arbitrary audio waves”. Even though every song has unique characteristics, most exhibit common attributes (tempo, melodies, harmonies, lyrics, and so on). Though these attributes may differ or be absent from song to song, they nevertheless fit common expectations that enable meaning to be conveyed by and extracted from each song. Textual information typically follows similar rules—sentence structure, grammar, punctuation, interaction between parts of speech, and so on. Figure 2.1 shows an example of text we’ll explore a bit more in the upcoming sections as we investigate this structure.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p16">
<img alt="figure" height="134" src="../Images/CH02_F01_Grainger.png" width="461"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.1</span> Unstructured data. This text represents typical unstructured data you may find in a search engine.</h5>
</div>
<div class="readable-text" id="p17">
<p>While text is the most commonly recognized type of unstructured data, there are several other kinds of unstructured data with similar characteristics, as we’ll see in the next section.</p>
</div>
<div class="readable-text" id="p18">
<h3 class="readable-text-h3" id="sigil_toc_id_23"><span class="num-string">2.1.1</span> Types of unstructured data</h3>
</div>
<div class="readable-text" id="p19">
<p>Free text content is considered the primary type of unstructured data, but search engines also commonly index many other kinds of data that similarly don’t fit neatly into a structured database. Common examples include images, audio, video, and event logs. Figure 2.2 expands on our text example from figure 2.1 and includes several other types of unstructured data, such as audio, images, and video. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p20">
<img alt="figure" height="420" src="../Images/CH02_F02_Grainger.png" width="682"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.2</span> Multiple types of unstructured data. In addition to the text from the last figure, we now see images, audio, and video, which are other forms of unstructured data.</h5>
</div>
<div class="readable-text" id="p21">
<p>Audio is the most similar to text content, since it is often just another way to encode words and sentences. Of course, audio can include much more than just spoken words—it can include music and non-language sounds, and it can more effectively encode nuances such as emotion, tone of voice, and simultaneously overlapping communication.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>Images are another kind of unstructured data. Just as words form sentences and paragraphs to express ideas, images form grids of colors that, taken together, form pictures.</p>
</div>
<div class="readable-text intended-text" id="p23">
<p>Video, then, serves as yet another kind of unstructured data, as it is a combination of multiple images over time, as well as optional audio that coincides with the progression of images.</p>
</div>
<div class="readable-text intended-text" id="p24">
<p>When unstructured data is found mixed with structured data, we typically refer to this as <em>semi-structured</em> data. Log data is a great example of such semi-structured data. Often, log messages contain a structured event date, structured event types (such as warning versus error or search versus click), and then some kind of unstructured message or description in free text.</p>
</div>
<div class="readable-text intended-text" id="p25">
<p>Technically speaking, virtually any kind of file could be considered unstructured data, but we’ll primarily deal with the aforementioned types. Search engines are often tasked with handling each of these kinds of unstructured data, so we’ll discuss strategies for handling them throughout the book. </p>
</div>
<div class="readable-text" id="p26">
<h3 class="readable-text-h3" id="sigil_toc_id_24"><span class="num-string">2.1.2</span> Data types in traditional structured databases</h3>
</div>
<div class="readable-text" id="p27">
<p>To better deal with our unstructured data, it may be useful to first contrast it with structured data in a SQL database. This will allow us to later draw parallels between how we can query unstructured data representations versus structured ones. </p>
</div>
<div class="readable-text intended-text" id="p28">
<p>A record (row) in a SQL database is segmented into columns, which can each be of a particular data type. Some of these data types represent discrete values—values that come from an enumerated list, such as IDs, names, or textual attributes. Other columns may hold continuous values, such as date/time ranges, numbers, and other column types that represent ranges without a finite number of possible values.</p>
</div>
<div class="readable-text intended-text" id="p29">
<p>Generally speaking, when one wants to relate different rows together or to relate them to rows in other database tables, “joins” will be performed on the discrete values. <em>Joins</em> use a shared value (often an ID field) to link two or more records together to form a composite record. </p>
</div>
<div class="readable-text intended-text" id="p30">
<p>For example, if someone had two tables of data, one representing employees and another representing companies, then there would likely be an <code>id</code> column on the <code>companies</code> table, and a corresponding <code>company_id</code> column on the <code>employees</code> table. The <code>company_id</code> field on the employees table is known as a <em>foreign key</em>, which is a value in one table that refers to an entity in another table, linking the records together based upon a shared identifier. Figure 2.3 demonstrates this, showing examples of discrete values, continuous values, and a join across tables using a foreign key. </p>
</div>
<div class="readable-text intended-text" id="p31">
<p>This notion of joining different records together based upon known relationships (keys and foreign keys) is a powerful way to work with relational data across explicitly</p>
</div>
<div class="browsable-container figure-container" id="p32">
<img alt="figure" height="400" src="../Images/CH02_F03_Grainger.png" width="886"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.3</span> Structured data in a typical database. Discrete values represent identifiers and enumerated values, continuous values represent data that falls within ranges, and foreign keys exist when the same value exists across two tables and can thus be used as a shared attribute that creates a relationship between corresponding rows from each table.</h5>
</div>
<div class="readable-text" id="p33">
<p>modeled tables, but as we’ll see in the next section, very similar techniques can also be applied to free-form unstructured data. </p>
</div>
<div class="readable-text" id="p34">
<h3 class="readable-text-h3" id="sigil_toc_id_25"><span class="num-string">2.1.3</span> Joins, fuzzy joins, and entity resolution in unstructured data</h3>
</div>
<div class="readable-text" id="p35">
<p>Whereas structured data in a database is already in an easily queryable form, the reality is that unstructured data suffers less from a lack of structure, and more from having a large amount of information packed into a very flexible structure. In this section, we’ll walk through a concrete example that uncovers this hidden structure in unstructured data and demonstrates the ways it can similarly be used to find and join relationships between documents. </p>
</div>
<div class="readable-text" id="p36">
<h4 class="readable-text-h4 sigil_not_in_toc">Foreign keys in unstructured data</h4>
</div>
<div class="readable-text" id="p37">
<p>We’ve discussed how foreign keys can be used to join two rows together in a database, based on a shared identifier between the two records. In this section, we’ll show how the same objective can be achieved with text data. </p>
</div>
<div class="readable-text intended-text" id="p38">
<p>For example, we can easily map the idea of foreign keys used in a SQL table to the unstructured information we explored in figure 2.2. Notice in figure 2.4 that two different sections of text both contain the word “Haystack”, which refers to a technology conference focused on search relevance.</p>
</div>
<div class="browsable-container figure-container" id="p39">
<img alt="figure" height="420" src="../Images/CH02_F04_Grainger.png" width="682"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.4</span> Foreign keys in unstructured data. In this example, the same term is being used to join across two related text documents.</h5>
</div>
<div class="readable-text intended-text" id="p40">
<p>The first instance indicates a conference being spoken at, while the second block of text contains general information about the event. For the purposes of our example, let’s assume that every piece of information (block of text, image, video, and audio clip) is represented as a separate document in our search engine. There is functionally very little difference between having two rows in a database table that each contain a column with the value “Haystack”, and having separate documents in our search engine that each contain the value “Haystack”. In both cases, we can think of these documents as related by a foreign key. <span class="aframe-location"/></p>
</div>
<div class="readable-text" id="p41">
<h4 class="readable-text-h4 sigil_not_in_toc">Fuzzy foreign keys in unstructured data</h4>
</div>
<div class="readable-text" id="p42">
<p>With unstructured data, however, we have much more power than with traditional structured data modeling. In figure 2.5, for example, notice that now two documents are linked and that they both refer to the lead author of this book—one using my full name of “Trey Grainger”, and one simply using my first name of “Trey”. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p43">
<img alt="figure" height="457" src="../Images/CH02_F05_Grainger.png" width="700"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.5</span> Fuzzy foreign keys. In this example, the same entity is being referenced using different terms, and a join is occurring based upon multiple phrases resolving to the same entity.</h5>
</div>
<div class="readable-text" id="p44">
<p>This is an example of <em>entity resolution</em>, where there are two different representations of the entity, but they can still be resolved to the same meaning, and therefore can still be used to join information between two documents. You can think of this as a “fuzzy foreign key”, since it’s still a foreign key, but not in a strict token-matching sense, as it requires additional natural language processing and entity resolution techniques to resolve. </p>
</div>
<div class="readable-text intended-text" id="p45">
<p>Once we’ve opened this door to advanced text processing for entity resolution, we can learn even more from our unstructured information. For example, not only do the names “Trey” and “Trey Grainger” in these documents refer to the same entity, but so do the words “he” and “his”.</p>
</div>
<div class="readable-text intended-text" id="p46">
<p>You’ll also notice that both an image of me (in the bottom-left corner, in case you have no idea what I look like) and a video containing a reference to my name are identified as related and joined back to the textual references. We’re relying on the hidden structure present in all of this unstructured data to understand the meaning, relate the documents together, and learn even more about each of the referenced entities in those documents. </p>
</div>
<div class="readable-text" id="p47">
<h4 class="readable-text-h4 sigil_not_in_toc">Dealing with ambiguous terms</h4>
</div>
<div class="readable-text" id="p48">
<p>So far, so good, but in real-world content it’s not always appropriate to assume that the same term in multiple places carries the same meaning, or even that our entity resolution always resolves entities correctly. This problem of the same spelling of words and phrases having multiple potential meanings is called <em>polysemy</em>, and dealing with these ambiguous terms can be a huge problem in search applications. </p>
</div>
<div class="readable-text intended-text" id="p49">
<p>You may have noticed an odd image in the upper-right corner of the previous figures that seemed a bit out of place. This image is of a fairly terrifying man holding a machete. Apparently, if you go to Google and search for <code>Trey</code> <code>Grainger</code>, this image comes back. If you dig in further, you’ll see in figure 2.6 that there’s an x.com (formerly Twitter) user also named “Trey Grainger”, and this image is his profile picture.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p50">
<img alt="figure" height="518" src="../Images/CH02_F06_Grainger.png" width="924"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.6</span> Polysemy. This image shows a Google search for <code>Trey</code> <code>Grainger</code>. Pictures of multiple different people are returned because those people’s names share the same spelling, making the phrase “Trey Grainger” ambiguous.</h5>
</div>
<div class="readable-text" id="p51">
<p>The picture is apparently of Robert Shaw (who plays Quint in the 1975 movie <em>Jaws</em>), but it’s definitely not the kind of thing you want people to first come across when they search for you online!</p>
</div>
<div class="readable-text intended-text" id="p52">
<p>There are two key lessons to take away here. First, perhaps never Google yourself (you might be terrified at what you find!). Second, and on a more serious note, polysemy is a major problem in search and natural language understanding. It’s not safe to assume a term has a single meaning or even a consistent meaning across different contexts, so our AI-powered search engine needs to use context to differentiate these various meanings. </p>
</div>
<div class="readable-text" id="p53">
<h4 class="readable-text-h4 sigil_not_in_toc">Unstructured data as a giant graph of relationships</h4>
</div>
<div class="readable-text" id="p54">
<p>In the previous sections, we saw that unstructured data not only contains rich information (entities and their relationships) but also that it’s possible to relate different documents by joining them on shared entities, similar to how foreign keys work in traditional databases. Typical unstructured data contains so many of these relationships, however, that instead of thinking in terms of rows and columns, it may be more useful to think of the collection of data as a giant graph of relationships, as we’ll explore in this section. </p>
</div>
<div class="readable-text intended-text" id="p55">
<p>At this point, it should be clear that there is much more structure hidden in unstructured data than most people appreciate. Unstructured information is really more like “hyper-structured” information—it is a graph that contains much more structure than typical “structured data”.</p>
</div>
<div class="readable-text intended-text" id="p56">
<p>Figure 2.7 demonstrates this giant graph of relationships that is present in even the small handful of documents in our example. You can see names, dates, events, locations, people, companies, and other entities, and you can infer relationships between<span class="aframe-location"/> them, using joins between the entities across documents. You’ll also notice that the images have been correctly disambiguated so that the machete guy is now disconnected from the graph. If all of this can be learned from just a few documents, imagine what can be learned from the thousands, millions, or billions of documents you have within your search engine.</p>
</div>
<div class="browsable-container figure-container" id="p57">
<img alt="figure" height="462" src="../Images/CH02_F07_Grainger.png" width="700"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.7</span> Giant graph of relationships. A rich graph of relationships emerges from even a small collection of related documents.</h5>
</div>
<div class="readable-text intended-text" id="p58">
<p>Part of the value of an AI-powered search platform is being able to learn insights like this from your data. The question is, how do you use this enormous graph of semantic knowledge to drive this intelligence?</p>
</div>
<div class="readable-text intended-text" id="p59">
<p>One of the most powerful ways to use a graph from text data is through a large language model (LLM), such as a Transformer model, which was introduced in section 1.3.4. These models use deep learning to learn billions of parameters across massive datasets, such as crawls of most of the internet, to build a detailed understanding of language. This understanding includes both the meanings of words in different contexts and the linguistic and conceptual connections between words. These models internally represent the giant graph of relationships found in all the data they are trained on, which is usually more general than your dataset, so the models must be fine-tuned to learn any domain-specific relationships from your data. This need for fine-tuning can create some challenges due to LLMs being somewhat of a black box, as they otherwise don’t optimally represent your dataset, and the information they return could be erroneous. </p>
</div>
<div class="readable-text intended-text" id="p60">
<p>Fortunately, the inherent structure of the inverted index in your search engine makes it very easy to traverse the large graph of relationships in your data without any additional explicit data modeling required. An <em>inverted index</em> is the primary data structure used for lexical search, mapping each keyword or term in the fields of your documents to lists (called <em>postings lists</em>) of all the documents containing those keywords. The inverted index enables very fast lookups of the set of documents containing any given term (or term sequences, when considering positional matching and Boolean logic implemented through set operations). With those lookups, it is possible to traverse between different term sequences using their shared documents to calculate a weighted edge in a graph. We will dive deep into how to harness this semantic knowledge graph hidden in your data in chapter 5. </p>
</div>
<div class="readable-text" id="p61">
<h2 class="readable-text-h2" id="sigil_toc_id_26"><span class="num-string">2.2</span> The structure of natural language</h2>
</div>
<div class="readable-text" id="p62">
<p>In the last section, we discussed how text and unstructured data typically contain a giant graph of relationships, which can be derived by looking at shared terms between different records. If you’ve been building search engines for a while, you’re used to thinking about content in terms of “documents”, “fields”, and “terms” within those fields. When interpreting the semantic meaning of your content, however, there are a few more levels to consider. </p>
</div>
<div class="readable-text intended-text" id="p63">
<p>Figure 2.8 walks through these additional levels of semantic meaning. At the most basic level, you have <em>characters</em>, which are single letters, numbers, or symbols, such as the letter “e” in the figure. One or more characters are then combined to form <em>character sequences</em> such as “e”, “en”, “eng”, … “engineer”, and “engineers”. Some character sequences form terms, which are completed words or tokens that carry an identifiable meaning, such as “engineer”, “engineers”, “engineering”, or “software”. One or more terms can then be combined into <em>term sequences</em>—usually called <em>phrases</em> when the terms are all sequential. These include things like “software engineer”, “software engineers”, and “senior software engineer”. For simplicity in this book, we also consider single terms to be “term sequences”, so any time we refer to “phrases”, this includes single terms. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p64">
<img alt="figure" height="675" src="../Images/CH02_F08_Grainger.png" width="858"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.8</span> Semantic data encoded into free text content. Characters form character sequences, which form terms, which form term sequences, which form fields, which form documents, which form a corpus.</h5>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p65">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Term sequences vs. phrases</h5>
</div>
<div class="readable-text" id="p66">
<p>You may be wondering what the difference is between a “term sequence” and a “phrase”. Quite simply, a phrase is a term sequence where all of the terms appear sequentially. For example, <code>"chief executive officer"</code> is both a phrase and a term sequence, whereas <code>"chief officer"~2</code> (meaning “officer” within two positions, or edit distances, of “chief”) is only a term sequence, since it specifies a sequence of terms that is not necessarily sequential. In the vast majority of cases, you will only be dealing with sequential term sequences, so we’ll mostly use the word “phrase” for simplicity throughout the book when referring inclusively to both single terms and multi-term sequential sequences. To avoid confusion, note that the word “term” is separately used to refer to “a unique value in a field in the search engine”. As such, we will sometimes also refer to unsplit strings with multiple words in them in the search engine as “terms”, even though linguistically they are considered “phrases” or “term sequences”.</p>
</div>
</div>
<div class="readable-text" id="p68">
<p>Of course, we know that a number of term sequences together can form sentences, multiple sentences can form paragraphs, and paragraphs can then be rolled up into even larger groups of text. For a search engine, though, the next higher-level level of grouping we’ll typically focus on after term sequences is a field. A <em>field</em> in a search engine is a partitioned and labeled section of a document, usually for purposes of searching on or returning as an independent portion of the document. Fields containing text can be analyzed in any number of ways using a text analyzer, which typically includes techniques like splitting on white space and punctuation, lowercasing all terms so they are case-insensitive, stripping out noise (stop words and certain characters), stemming or lemmatization to reduce terms to a base form, and removing accents. If the text analysis process is unfamiliar to you, or you would like a refresher, we recommend checking out chapter 6 of <em>Solr in Action</em> by Trey Grainger and Timothy Potter (Manning, 2014). </p>
</div>
<div class="readable-text intended-text" id="p69">
<p>One or more fields are then composed together into a <em>document</em>, and multiple documents form a <em>corpus</em> or collection of data. Whenever a query is executed against the search index, it filters the corpus into a <em>document set</em>, which is a subset of the corpus that specifically relates to the query in question. </p>
</div>
<div class="readable-text intended-text" id="p70">
<p>Each of these linguistic levels—character sequences, terms, term sequences, fields, documents, document sets, and the corpus—provides unique insights into understanding your content and its unique meaning within your specific domain. </p>
</div>
<div class="readable-text" id="p71">
<h2 class="readable-text-h2" id="sigil_toc_id_27"><span class="num-string">2.3</span> Distributional semantics and embeddings</h2>
</div>
<div class="readable-text" id="p72">
<p>Distributional semantics is a research area within the field of natural language processing that focuses on the semantic relationships between terms and phrases based on the distributional hypothesis. The distributional hypothesis is that words that occur in similar contexts tend to share similar meanings. It is summarized well by the popular quote, “You shall know a word by the company it keeps.”<a href="#footnote-84" id="ftnote-84"><sup class="footnote-reference">1</sup></a></p>
</div>
<div class="readable-text intended-text" id="p73">
<p>When applying machine learning approaches to your text, these distributional semantics become increasingly important, and the search engine makes it incredibly easy to derive the context for most linguistic representations present in your corpus. For example, if someone wants to find all documents about C-level executives, they could issue a query like this:</p>
</div>
<div class="browsable-container listing-container" id="p74">
<div class="code-area-container">
<pre class="code-area">c?o</pre>
</div>
</div>
<div class="readable-text" id="p75">
<p>This query would match “CEO”, “CMO”, “CFO”, or any other CXO-style title, as it is asking for any character sequence starting with “c” and ending with “o” with a single character in between.</p>
</div>
<div class="readable-text intended-text" id="p76">
<p>The same kind of freedom exists to query for arbitrarily complex term sequences:</p>
</div>
<div class="browsable-container listing-container" id="p77">
<div class="code-area-container">
<pre class="code-area">"VP Engineering"~2</pre>
</div>
</div>
<div class="readable-text" id="p78">
<p>This query would match “VP Engineering”, “VP of Engineering”, “Engineering VP”, or even “VP of Software Engineering”, as it is asking to find “VP” and “Engineering” within two positions (edit distances) of each other.</p>
</div>
<div class="readable-text intended-text" id="p79">
<p>Of course, the nature of a search engine’s inverted index also makes it trivial to support arbitrary Boolean queries. For example, if someone searches for the term “Word”, but we want to ensure any matched documents also contain either the term “Microsoft” or “MS” somewhere in the document, we could issue the following query:</p>
</div>
<div class="browsable-container listing-container" id="p80">
<div class="code-area-container">
<pre class="code-area">(Microsoft OR MS) AND Word</pre>
</div>
</div>
<div class="readable-text" id="p81">
<p>Search engines support arbitrarily complex combinations of queries for character sequences, terms, and term sequences throughout your corpus, returning document sets that serve as a unique context of content matching that query. For example, if you run a query for <code>pizza</code>, the documents returned are more likely going to be restaurants than car rental companies, and if you run a query for <code>machine</code> <code>learning</code>, you’re more likely to see jobs for data scientists or software engineers than for accountants, food service workers, or pharmacists. This means that you can infer a strong relationship between “machine learning” and “software engineering”, and a weak relationship between “machine learning” and “food service worker”. If you dig deeper, you’ll also be able to see what other terms and phrases most commonly co-occur within the machine learning document set relative to the rest of your corpus, and thereby better understand the meaning and usage of the phrase “machine learning”. We’ll dive into hands-on examples of using these relationships in chapter 5.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p82">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Introducing vectors</h5>
</div>
<div class="readable-text" id="p83">
<p>A basic understanding of vector operations will be important as you progress through this book. A <em>vector</em> is a list of values describing some attributes of an item. For example, if your items are houses, you may have a list of attributes like <code>price</code>, <code>size</code>, and <code>number of bedrooms</code>. If you have a home costing $100,000 with 1,000 square feet and 2 bedrooms, this could be represented as the vector <code>[100000,</code> <code>1000,</code> <code>2]</code>. </p>
</div>
<div class="readable-text" id="p84">
<p>These attributes (price, size, and number of bedrooms in this example) are called <em>dimensions</em> or <em>features</em>, and a specific collection of dimensions is called a <em>vector space</em>. You can represent any other items (like other homes, apartments, or dwellings) within the same vector space if you can assign them values within the dimensions of the vector space. </p>
</div>
<div class="readable-text" id="p85">
<p>If we consider other vectors within the same vector space (such as a house <code>[1000000,</code> <code>9850,</code> <code>12]</code> and another house <code>[120000,</code> <code>1400,</code> <code>3]</code>), we can perform mathematical operations on the vectors to learn trends and compare vectors. For example, you may intuitively look at these three example vectors and determine that “home prices tend to increase as the number of rooms increases” or that “the number of rooms tends to increase as home size increases”. We can also perform similarity calculations on vectors to determine that the $120,000 home with 1,400 square feet and 3 bedrooms is more similar to the $100,000 home with 1,000 square feet and 2 bedrooms than to the $1,000,000 home with 9,850 square feet and 12 bedrooms.</p>
</div>
</div>
<div class="readable-text" id="p87">
<p>In recent years, the distributional hypothesis has been applied to create semantic understandings of terms and term sequences through what are known as embeddings. An <em>embedding</em> is a set of coordinates in a vector space into which we map (or “embed”) a concept. More concretely, that set of coordinates is a numerical vector (a list of numbers) that is intended to represent the semantic meaning of your data (text, image, audio, behavior, or other data modalities). Text-based embeddings can represent term sequences of any length, but when representing individual words or phrases, we call those embeddings <em>word embeddings</em>. </p>
</div>
<div class="readable-text intended-text" id="p88">
<p>The term sequence is often encoded into a reduced-dimension embedding that can be compared with the vectors for all of the other embeddings within the corpus to find the most semantically related documents.</p>
</div>
<div class="readable-text intended-text" id="p89">
<p>To understand this process, it may be useful to think of how a search engine works out of the box. Let’s imagine a vector exists for each term that contains a value (dimension) for every word in your corpus. It might look something like figure 2.9.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p90">
<img alt="figure" height="391" src="../Images/CH02_F09_Grainger.png" width="872"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.9</span> Vectors with one dimension per term from the inverted index. Every query on the left maps to a vector on the right, with a value of <code>1</code> for any term from the index that is also in the query, and a <code>0</code> for any term from the index that is not in the query.</h5>
</div>
<div class="readable-text" id="p91">
<p>Figure 2.9 demonstrates conceptually how document matching works in lexical search engines by default. A <em>lexical search</em> is a search where documents are matched and ranked based on the degree to which they contain the actual keywords or other attributes specified in the query. For every keyword query, a vector exists that contains a dimension for every term in the inverted index. If that term exists in the query, the value in the vector is <code>1</code> for that dimension, and if that value does not exist in the query, then the value is <code>0</code> for that dimension. A similar vector exists for every document in the inverted index, with a <code>1</code> value for any term from the index that appears in the document, and a <code>0</code> for all other terms. </p>
</div>
<div class="readable-text intended-text" id="p92">
<p>When a query is executed, a lookup occurs in the index for any matched terms, and then a similarity score is calculated based on a comparison of the vector for the query and the vector for the document that is being scored relative to the query. We’ll walk through the specific scoring calculation in chapter 3, but this high-level understanding is sufficient for now.</p>
</div>
<div class="readable-text intended-text" id="p93">
<p>There are obvious downsides to this approach. While it’s great for finding documents with exact keyword matches, what happens when you want to find “related” things instead? For example, you’ll notice that the term “soda” appears in a query, but never in the index. Even though there are other kinds of drinks (“apple juice”, “water”, “cappuccino”, and “latte”), the search engine will always return zero results because it doesn’t understand that the user is searching for a drink. Similarly, you’ll notice that even though the term “caffeine” exists in the index, queries for <code>latte</code>, <code>cappuccino</code>, and <code>green tea</code> will never match the term “caffeine”, even though they are related.</p>
</div>
<div class="readable-text intended-text" id="p94">
<p>For these reasons, it is now common practice to use reduced-dimension dense embeddings to model a semantic meaning for term sequences in your index and queries. A <em>dense embedding</em> (also known as a <em>dense vector embedding</em>) is a vector of more abstract features that encodes an input’s conceptual meaning in a semantic space. Figure 2.10 demonstrates the terms now mapped to a dimensionally reduced vector that can serve as a dense embedding. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p95">
<img alt="figure" height="359" src="../Images/CH02_F10_Grainger.png" width="710"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.10</span> Dense embeddings with reduced dimensions. In this case, instead of one dimension per term (exists or missing), now higher-level dimensions exist that score shared attributes across items such as “healthy”, contains “caffeine” or “bread” or “dairy”, or whether the item is “food” or a “drink”.</h5>
</div>
<div class="readable-text" id="p96">
<p>With a new embedding vector now available for each term sequence in the leftmost column of figure 2.10, we can now score the relationship between each pair of term sequences, using the similarity between their vectors. In linear algebra, we use a cosine similarity function (or another similarity measure) to score the relationship between two vectors. Cosine similarity is computed by performing a dot product between the two vectors and scaling it by the magnitudes (lengths) of each of the vectors. We’ll visit the math in more detail in the next chapter, but for now, figure 2.11 shows the results of scoring the similarity between several of these vectors.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p97">
<img alt="figure" height="503" src="../Images/CH02_F11_Grainger.png" width="860"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.11</span> Similarity between embeddings. The cosine between vectors shows the items list sorted by similarity with “green tea”, with “cheese pizza”, and with “donut”.</h5>
</div>
<div class="readable-text" id="p98">
<p>As you can see in figure 2.11, since each term sequence is encoded into a vector that represents its meaning in terms of higher-level features, this embedding can now be used to score the similarity of that term sequence with any other similar vector. You’ll see three vector similarity lists at the bottom of the figure: one for “green tea”, one for “cheese pizza”, and one for “donut”.</p>
</div>
<div class="readable-text intended-text" id="p99">
<p>By comparing the vector similarity of “green tea” with all the other term sequences, we find that the top related items are “water”, “cappuccino”, “latte”, “apple juice”, and “soda”, with the least related being “donut”. This makes intuitive sense, as “green tea” shares more attributes with the items higher in the list. For the “cheese pizza” vector, we see that the most similar other embeddings are for “cheese bread sticks”, “cinnamon bread sticks”, and “donut”, with “water” being at the bottom of the list. Finally, for the term “donut”, we find the top items to be “cinnamon bread sticks”, “cheese bread sticks”, and “cheese pizza”, with “water” once again being at the bottom of the list. These results do a great job of finding the most similar items to our original query.</p>
</div>
<div class="readable-text intended-text" id="p100">
<p>It’s worth noting that this vector scoring is only used in the calculation of similarity between items. In your search engine, there’s usually a two-phase process whereby you first filter to a set of documents (the <em>matching</em> phase) and then score those resulting documents (the <em>ranking</em> phase). Unless you’re going to skip the first step and score all of your documents relative to your query vectors (which can be time- and processing-intensive), you’ll still need some form of initial matching prior to the ranking phase to filter the query to a reasonable number of documents to score. We’ll dive more into the mechanics for successfully implementing embeddings and vector search in chapters 3, 9, 13, 14, and 15. </p>
</div>
<div class="readable-text intended-text" id="p101">
<p>Embeddings might represent queries, portions of documents, or even entire documents. It is commonplace to encode terms and term sequences into word embeddings, but <em>sentence embeddings</em> (encoding a vector with the meaning of a sentence), <em>paragraph embeddings</em> (encoding a vector with the meaning of a paragraph), and <em>document embeddings</em> (encoding a vector with the meaning of an entire document) are also common techniques. </p>
</div>
<div class="readable-text intended-text" id="p102">
<p>It’s also very common for dimensions to be more abstract than our examples here. For example, deep learning models like LLMs may detect seemingly unintelligible features from character sequences and the way that documents cluster together within the corpus. We wouldn’t be able to easily apply a human-readable label to these dimensions in the embedding vector, but as long as it improves the predictive power of the model and increases relevance, this is usually not a concern for most search teams. In fact, since vectors encode “meaning” through different abstract numeric features, it’s also possible to create and search on vectors representing different types (or <em>modalities</em>) of data, such as images, audio, video, or even signals and activity patterns. We’ll cover <em>multimodal search</em> (searches on different data modalities) in section 15.3. </p>
</div>
<div class="readable-text intended-text" id="p103">
<p>Ultimately, combining multiple models for harnessing the power of distributional semantics and embeddings tends to create the best outcomes, and we’ll dive further into numerous graph and vector-based approaches to using these techniques throughout the rest of this book. </p>
</div>
<div class="readable-text" id="p104">
<h2 class="readable-text-h2" id="sigil_toc_id_28"><span class="num-string">2.4</span> Modeling domain-specific knowledge</h2>
</div>
<div class="readable-text" id="p105">
<p>In chapter 1, we discussed the search intelligence progression (refer to figure 1.8), whereby organizations start with basic keyword search and progress through several additional stages of improvement before they ultimately achieve a full self-learning system. The second stage in that search intelligence progression was building taxonomies and ontologies, and the third stage (“query intent”) included the building and use of knowledge graphs. Unfortunately, there can sometimes be significant confusion among practitioners in the industry on proper definitions and key terminology, like “ontology”, “taxonomy”, “synonym lists”, “knowledge graphs”, “alternative labels”, and so on. It will benefit us to provide some definitions for use in this book to prevent any ambiguity. Specifically, we’ll lay out definitions for the key terms of “knowledge graph”, “ontology”, “taxonomy”, “synonyms”, and “alternative labels”. Figure 2.12 shows a high-level diagram of how they relate. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p106">
<img alt="figure" height="371" src="../Images/CH02_F12_Grainger.png" width="403"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.12</span> Levels of domain-specific knowledge modeling. Knowledge graphs extend ontologies, which extend taxonomies. Synonyms extend alternative labels and map to entries in taxonomies.</h5>
</div>
<div class="readable-text" id="p107">
<p>We define each of these knowledge modeling techniques as follows:</p>
</div>
<ul>
<li class="readable-text" id="p108"> <em>Alternative labels (or alt. labels)</em>—Replacement term sequences with identical meanings. </li>
</ul>
<div class="browsable-container listing-container" id="p109">
<div class="code-area-container">
<pre class="code-area">Examples:
   CTO =&gt; Chief Technology Officer
   specialise =&gt; specialize</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p110"> <em>Synonyms</em>—Replacement term sequences that can be used to represent the same or very similar things.  </li>
</ul>
<div class="browsable-container listing-container" id="p111">
<div class="code-area-container">
<pre class="code-area">Examples:
   human =&gt; homo sapiens, mankind
   food =&gt; sustenance, meal</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p112"> <em>Taxonomy</em>—A classification of things into categories.  </li>
</ul>
<div class="browsable-container listing-container" id="p113">
<div class="code-area-container">
<pre class="code-area">Examples:
   human is mammal
   mammal is animal</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p114"> <em>Ontology</em>—A mapping of relationships between types of things.  </li>
</ul>
<div class="browsable-container listing-container" id="p115">
<div class="code-area-container">
<pre class="code-area">Examples:
   animal eats food
   food contains ingredients</pre>
</div>
</div>
<ul>
<li class="readable-text" id="p116"> <em>Knowledge graph</em>—An instantiation of an ontology that also contains the things that are related.  </li>
</ul>
<div class="browsable-container listing-container" id="p117">
<div class="code-area-container">
<pre class="code-area">Examples:
   John is human
   John eats food</pre>
</div>
</div>
<div class="readable-text" id="p118">
<p>Creating alternative labels is the most straightforward of these techniques to understand. Initialisms (such as “RN” =&gt; “Registered Nurse”) and acronyms virtually always serve as alternative labels, as do misspellings and alternative spellings. Sometimes it is useful to store these mappings in separate lists, particularly if you’re using algorithms to determine them and you expect to allow for human modification of them or if you plan to rerun the algorithms later. </p>
</div>
<div class="readable-text intended-text" id="p119">
<p>Synonyms are the next most common of the techniques, as virtually every search engine will have some implementation of a synonym list. Alternative labels are a subset of a synonym list and are the most obvious kind of synonym. Most people consider “highly related” term sequences to be synonyms, as well. For example, “software engineer” and “software developer” are often considered synonyms since they are usually used interchangeably, even though there are some nuances in meaning between the two. Sometimes, you’ll even see translations of words between languages showing up in synonyms for bilingual search use cases. </p>
</div>
<div class="readable-text intended-text" id="p120">
<p>One key difference between alternative labels and more general synonyms is that alternative labels can be seen as <em>replacement terms</em> for the original, whereas synonyms are more often used as <em>expansion terms</em> to add alongside the original. Implementations can vary widely, but this ultimately boils down to whether you are confident that two term sequences carry exactly the same meaning (and you want to normalize them), or whether you’re just trying to include additional related term sequences so you don’t miss other relevant results.</p>
</div>
<div class="readable-text intended-text" id="p121">
<p>Taxonomies are the next step up from synonyms. Taxonomies focus less on substitute or expansion words and instead focus on categorizing your content into a hierarchy. Taxonomical information will often be used to drive website navigation, to change behavior for a subset of search results (for example, to show different faceting or filtering options based upon a parent product category), or to apply dynamic filtering based upon a category to which a query maps. For example, if someone searches for <code>range</code> on a home improvement website, the site might automatically filter down to “appliances” to remove the noise of other products that contain phrases like “fall within the range” in their product description. Synonyms then map into a taxonomy, pointing to particular items within the taxonomy. </p>
</div>
<div class="readable-text intended-text" id="p122">
<p>Whereas taxonomies tend to specify parent-child relationships between categories and then map things into those categories, ontologies provide the ability to define much richer relationships between things (term sequences, entities) within a domain. Ontologies typically define more abstract relationships, attempting to model the relationships between kinds of things in a domain, such as “employee reports to boss,” “CMO’s boss is CEO,” “CMO is employee”. This makes ontologies really useful for deriving new information from known facts by mapping the facts into the ontology and then drawing logical conclusions based on relationships in the ontology that can be applied to those facts. </p>
</div>
<div class="readable-text intended-text" id="p123">
<p>Knowledge graphs are the relative newcomer to the knowledge management space. Whereas ontologies define high-level relationships that apply to types of things, knowledge graphs tend to be full instantiations of ontologies that also include each of the specific entities that fall within those types. Using our previous ontology example, a knowledge graph would additionally have “Michael is CMO,” “Michael reports to Marcia,” and “Marcia is CEO” as relationships in the graph. Before knowledge graphs came to the forefront, it was common for these more detailed relationships to be modeled into ontologies, and many people still do this today. As a result, you’ll often see the terms “knowledge graph” and “ontology” used interchangeably, though this is becoming less common over time. </p>
</div>
<div class="readable-text intended-text" id="p124">
<p>Throughout this book, we will mostly focus our discussions on alternative labels, synonyms, and knowledge graphs, since taxonomies and ontologies are mostly subsumed into knowledge graphs. We’ll explore knowledge graphs more thoroughly in chapter 5. </p>
</div>
<div class="readable-text" id="p125">
<h2 class="readable-text-h2" id="sigil_toc_id_29"><span class="num-string">2.5</span> Challenges in natural language understanding for search</h2>
</div>
<div class="readable-text" id="p126">
<p>In the last few sections, we’ve discussed the rich graph of meaning embedded within unstructured data, like text, as well as how distributional semantics and embeddings can be used to derive and score semantic relationships between term sequences in queries and documents. We also introduced key techniques for knowledge modeling and defined related terminology we’ll use throughout this book. In this section, we’ll discuss a few key challenges associated with natural language understanding that we’ll seek to overcome in the coming chapters. </p>
</div>
<div class="readable-text" id="p127">
<h3 class="readable-text-h3" id="sigil_toc_id_30"><span class="num-string">2.5.1</span> The challenge of ambiguity (polysemy)</h3>
</div>
<div class="readable-text" id="p128">
<p>In section 2.1.3, we introduced the idea of polysemy, or ambiguous terms. In that section, we were dealing with an image tagged with the name “Trey Grainger” but that referred to a different person than the author of this book. In textual data, however, we have the same problem, and it can get very messy. </p>
</div>
<div class="readable-text intended-text" id="p129">
<p>Consider a word like “driver”. It can refer broadly to a “vehicle driver”, a kind of golf club for hitting the ball off a tee, software that enables a hardware device to work, a kind of tool (screwdriver), or the impetus for pushing something forward (“a key driver of success”). There are many potential meanings for this word, and you could explore even more granular meanings. For example, within the “vehicle driver” category, it could mean a taxi driver, Uber driver, Lyft driver, professional trucker like a CDL driver (someone with a commercial drivers license), or even a bus driver. Within the subset of bus drivers, it could mean a school bus driver, a driver of a public city bus, a driver of a tour bus, and so on. We could continue breaking down this list into dozens of additional categories at a minimum.</p>
</div>
<div class="readable-text intended-text" id="p130">
<p>When building search applications, engineers will often naively create static synonym lists and assume terms have a singular meaning that can be applied universally. The reality, however, is that every term (word or phrase) takes on a unique meaning that is based on the specific context in which it is being used.</p>
</div>
<div class="readable-text print-book-callout" id="p131">
<p><span class="print-book-callout-head">TIP</span>  Every term takes on a unique meaning that is based on the specific context in which it is being used.</p>
</div>
<div class="readable-text" id="p132">
<p>It’s not often practical to support an infinite number of potential meanings, though we discuss techniques to approximate this with a semantic knowledge graph in chapter 5. Nevertheless, whether you support many meanings per phrase or just a few, it’s important to recognize the clear need to be able to generate an accurate (and often nuanced) interpretation for any given phrase your users may encounter. </p>
</div>
<div class="readable-text" id="p133">
<h3 class="readable-text-h3" id="sigil_toc_id_31"><span class="num-string">2.5.2</span> The challenge of understanding context</h3>
</div>
<div class="readable-text" id="p134">
<p>I like to say that every term (word or phrase) you ever encounter is a “context-dependent cluster of meaning with an ambiguous label”. That is to say, there is a label (the textual representation of the term) that is being applied to some concept (a cluster of meaning) that is dependent upon the context in which it is found. By this definition, it’s impossible to ever precisely interpret a term without understanding its context. As a result, creating fixed synonym lists that aren’t able to take context into account is likely to create suboptimal search experiences for your users. </p>
</div>
<div class="readable-text intended-text" id="p135">
<p>Transformer models largely operate on this premise by using input prompts as the context in which to interpret each word part (or token) in the prompt. Attention is paid to every token, based on the surrounding tokens and how they relate to the learned representation in the model, which is also contextual. We’ll dive into the nuances of how Transformers work in chapter 13, and we’ll fine-tune a Transformer for a question-answering task in chapter 14.</p>
</div>
<div class="readable-text intended-text" id="p136">
<p>Just because context is important doesn’t mean it’s always easy to apply correctly. It’s often necessary to perform basic keyword search as a fallback when your search engine doesn’t understand a query, and it’s almost always useful to have prebuilt domain understanding that can similarly be relied upon to help interpret queries. This prebuilt domain understanding then ends up overriding some of the default keyword-based matching behavior (such as joining individual keywords into phrases, injecting synonyms, and correcting misspellings).</p>
</div>
<div class="readable-text intended-text" id="p137">
<p>As we discussed in chapter 1, the context for a query includes more than just the search keywords and the content within your documents. It also includes an understanding of your domain, as well as an understanding of your user. Queries can take on entirely different meanings based on what you know about your user and any domain-specific understanding you may have. This context is necessary both to detect and resolve the kinds of ambiguity we discussed in the last section, as well as to ensure your users are receiving the most intelligent search experience possible. Throughout this book, our focus will be on techniques to automatically learn contextual interpretations of each query, based on the unique context in which it’s being used. </p>
</div>
<div class="readable-text" id="p138">
<h3 class="readable-text-h3" id="sigil_toc_id_32"><span class="num-string">2.5.3</span> The challenge of personalization</h3>
</div>
<div class="readable-text" id="p139">
<p>When considering user-specific context as a tool for enhancing query understanding, it’s not always obvious how to apply user-specific personalization on top of the preexisting content and domain-specific scoring. For example, say you learn that a particular user really likes Apple as a brand because they keep searching for iPhones. Does this mean that Apple should also be boosted when they are searching for watches, computers, keyboards, headphones, and music players? It could be that the user only likes Apple-branded phones and that by boosting the brand in other categories you may frustrate the user. For example, even if the user did search for an iPhone previously, how do you know they weren’t just trying to compare the iPhone with other phones they were considering?</p>
</div>
<div class="readable-text intended-text" id="p140">
<p>Out of all of the dimensions of user intent (figure 1.5), personalization is the easiest one to trip up on, and, as a result, it is the one that is least commonly seen in modern AI-powered search applications (outside of recommendation engines, of course). We’ll work through these problems in detail in chapter 9 to highlight how we can strike the right balance when rolling out a personalized search experience. </p>
</div>
<div class="readable-text" id="p141">
<h3 class="readable-text-h3" id="sigil_toc_id_33"><span class="num-string">2.5.4</span> Challenges interpreting queries vs. documents</h3>
</div>
<div class="readable-text" id="p142">
<p>One common problem we see when engineers and data scientists first get started with search is a propensity to apply standard natural language processing techniques, like language detection, part-of-speech detection, phrase detection, and sentiment analysis to queries. Usually, those techniques were trained to operate on longer blocks of text—often at the document, paragraph, or at least sentence level. </p>
</div>
<div class="readable-text intended-text" id="p143">
<p>Documents tend to be longer and to supply significantly more context to the surrounding text, whereas queries tend to be short (a few keywords only) in most use cases. Even when they are longer, queries tend to combine multiple ideas as opposed to supplying more linguistic context. As a result, when trying to interpret queries, you need to use external context as much as possible.</p>
</div>
<div class="readable-text intended-text" id="p144">
<p>Instead of using a natural language processing library that relies on sentence structure to interpret the query, for example, you can try looking up the phrases from your query in your corpus of documents to find their most common domain-specific interpretations. Likewise, you can use the co-occurrence of terms within your query across previous user search sessions by mining user behavioral signals. This enables you to learn real intent from similar users, which would be very challenging to reliably derive from a standard natural language processing library.</p>
</div>
<div class="readable-text intended-text" id="p145">
<p>In short, queries need special handling and interpretation due to their tendency to be short and to often imply more than they state explicitly, so fully using search-centric data science approaches to queries is going to generate much better results than traditional natural language processing approaches. </p>
</div>
<div class="readable-text" id="p146">
<h3 class="readable-text-h3" id="sigil_toc_id_34"><span class="num-string">2.5.5</span> Challenges interpreting query intent</h3>
</div>
<div class="readable-text" id="p147">
<p>While the process of parsing a query to understand the terms and phrases it contains is important, there is often a higher-level intent behind the query—a query intent, if you will. For example, let’s consider the inherent differences between the following queries:</p>
</div>
<div class="browsable-container listing-container" id="p148">
<div class="code-area-container">
<pre class="code-area">who is the CEO?
support
iphone screen blacked out
iphone
verizon silver iphone 8 plus 64GB
sale
refrigerators
pay my bill</pre>
</div>
</div>
<div class="readable-text" id="p149">
<p>The intent of the first query for <code>who</code> <code>is</code> <code>the</code> <code>CEO?</code> is clearly to find a factual answer and not a list of documents. The second query for <code>support</code> is trying to navigate to the support section of a website, or to otherwise contact the support team. The third query for <code>iphone</code> <code>screen</code> <code>blacked</code> <code>out</code> is also looking for support, but it is for a specific problem, and the person likely wants to find troubleshooting pages that may help with that specific problem before reaching out to the actual support team.</p>
</div>
<div class="readable-text intended-text" id="p150">
<p>The next two queries for <code>iphone</code> and for <code>verizon</code> <code>silver</code> <code>iphone</code> <code>8</code> <code>plus</code> <code>64GB</code> are quite interesting. While they are both for iPhones, the first search is a general search, likely indicating a browsing or product research intent, whereas the second query is a much more specific variant of the first search, indicating the user knows exactly what they are looking for and may be closer to making a purchasing decision. The general query for <code>iphone</code> may be better to return a landing page providing an overview of iphones and the available options, whereas the more specific query may be better for going straight to a product page with a purchase button. As a general rule of thumb, the more general a query, the more likely the user is just browsing. More specific queries—especially when they refer to specific items by name—often indicate a purchase intent or desire to find a particular known item.</p>
</div>
<div class="readable-text intended-text" id="p151">
<p>The query for <code>sale</code> indicates that the user is looking for items that are available for purchase at a discounted rate, which will invoke some specially implemented filter or redirect to a particular landing page for an ongoing sale event. The query for <code>refrigerators</code> indicates that the user wants to browse a particular category of product documents. Finally, the query for <code>pay</code> <code>my</code> <code>bill</code> indicates that the user wants to take an action—the best response to this query isn’t a set of search results or even an answer, but instead a redirect to a bill review and payment section of the application.</p>
</div>
<div class="readable-text intended-text" id="p152">
<p>Each of these queries contains an intent beyond just a set of keywords to be matched. Whether the intent is to redirect to a particular page, apply particular filters, browse or purchase items, or even take domain-specific actions, the point is that there is domain-specific nuance to how users may express their goals to your search engine. Oftentimes, it can be difficult to derive these domain-specific user intents automatically. It is fairly common for businesses to implement specific business rules to handle these as one-off requests. Query intent classifiers can certainly be built to handle subsets of this problem, but successfully interpreting every possible query intent still remains challenging when building out natural language query interpretation capabilities. </p>
</div>
<div class="readable-text" id="p153">
<h2 class="readable-text-h2" id="sigil_toc_id_35"><span class="num-string">2.6</span> Content + signals: The fuel powering AI-powered search</h2>
</div>
<div class="readable-text" id="p154">
<p>In the first chapter, we introduced the idea of reflected intelligence—using feedback loops to continually learn from both content and user interactions. This chapter has focused entirely on understanding the meaning and intelligence embedded within your content, but it’s important to recognize that many of the techniques we’ll apply to the “unstructured data” in your documents can also be just as readily applied to your user behavioral signals. For example, we discussed in section 2.3 how the meanings of phrases can be derived from finding the other phrases they most commonly appear with across your corpus. We noted that “machine learning” appears more commonly with “data scientist” and “software engineer” than it does with “accountants,” “food service workers,” or “pharmacists”. </p>
</div>
<div class="readable-text intended-text" id="p155">
<p>If you abstract the distributional hypothesis beyond documents and also apply it to user behavior, you might expect that similar users querying your search engine are likely to exhibit similar query behavior. Specifically, people who are data scientists or who are searching for data scientists are far more likely to also search for or interact with documents about machine learning, and the likelihood of a food service worker or accountant searching for machine learning content is much lower than the likelihood of a software engineer doing so. We can thus apply these same techniques to learn related terms and term sequences from query logs, where instead of thinking of terms and term sequences mapping to fields in documents, we think of terms in queries and clicks on search results mapping to user sessions, which then map to users. We’ll follow this approach in chapter 6 to learn related terms, synonyms, and misspellings from user query logs. </p>
</div>
<div class="readable-text intended-text" id="p156">
<p>Some search applications are content-rich but have very few user signals. Other search applications have an enormous number of signals but either very little content or content that poses challenges from an automated learning perspective. In an ideal scenario, you’ll have great content and an enormous quantity of user signals to learn from, which allows you to combine the best of both worlds into an even smarter AI-powered search application. Regardless of which scenario you’re in, keep in mind that your content and your user signals can both serve as fuel to power learning algorithms, and you should do your best to maximize the collection and quality of each.</p>
</div>
<div class="readable-text intended-text" id="p157">
<p>As a final note on natural language understanding: with the rise of LLMs, which are deep neural networks trained on a large percentage of human knowledge (much of the internet, plus selected sources), we now have the ability to interpret the meaning and intent of general-knowledge questions at an unprecedented level of quality. LLMs do not handle domain-specific understanding very well out of the box, at least for information that is not part of their training sets, but with the ability to fine-tune LLMs on domain-specific data, these models can often be quickly adapted to more closed-domain data. LLMs represent a large leap forward in our ability to learn the nuances of natural language, interpret arbitrary documents and queries based on those nuances, and drive more relevant search. </p>
</div>
<div class="readable-text intended-text" id="p158">
<p>LLMs, while generally the most impressive technique for wide-scale natural language understanding, are far from the only powerful tools in our AI-powered search toolbox. We’ll dive into using LLMs for search in chapters 9, 13, 14, and 15. In the meantime, we have many other critical algorithms and techniques to explore for natural language and domain understanding, interpreting user behavior, and learning optimal relevance ranking models.</p>
</div>
<div class="readable-text intended-text" id="p159">
<p>Now that we’ve covered all the background needed to begin extracting meaning from your natural language content, it’s time to roll up our sleeves and get hands-on. In the next chapter, we’ll dive into lots of examples as we begin to explore content-based relevance in an AI-powered search application.</p>
</div>
<div class="readable-text" id="p160">
<h2 class="readable-text-h2" id="sigil_toc_id_36">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p161"> Unstructured data is a misnomer—it is really more like hyper-structured data, as it represents a giant graph of domain-specific knowledge.  </li>
<li class="readable-text" id="p162"> Search engines can use distributional semantics—interpreting the semantic relationships between terms and phrases based upon the distributional hypothesis—to harness rich semantic meaning at the level of character sequences, terms, term sequences (typically phrases), fields, documents, document sets, and an entire corpus.  </li>
<li class="readable-text" id="p163"> Distributional semantics approaches enable us to learn the nuanced meaning of our queries and content from their larger surrounding context. </li>
<li class="readable-text" id="p164"> Embeddings are a powerful technique for search results ranking based on the semantic meaning of text (and other data modalities) instead of just the existence and occurrence counts of specific keywords.  </li>
<li class="readable-text" id="p165"> Domain-specific knowledge is commonly modeled through a combination of alternative labels, synonym lists, taxonomies, ontologies, and knowledge graphs. Knowledge graphs typically model the output from each of the other approaches into a unified knowledge representation of a particular domain.  </li>
<li class="readable-text" id="p166"> Polysemy (ambiguous terms), context, personalization, and query-specific natural language processing approaches represent some of the more interesting challenges in natural language search.  </li>
<li class="readable-text" id="p167"> Content and user signals are both important fuel for our AI-powered search applications to use when solving natural language challenges.  </li>
</ul>
<div class="readable-text footnote-readable-text" id="p168">
<p><span class="footnote-definition"><a href="#ftnote-84" id="footnote-84">[1]</a></span> John Rupert Firth, “A synopsis of linguistic theory, 1930–1955,” in J.R. Firth et al., <em>Studies in Linguistic Analysis</em>, Special Volume of the Philological Society (Oxford University Press, 1957).</p>
</div>
</div></body></html>