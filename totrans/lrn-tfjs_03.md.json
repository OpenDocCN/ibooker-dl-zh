["```py\n# Import with npm\n$ npm i @tensorflow/tfjs\n\n# Or Yarn\n$ yarn add @tensorflow/tfjs\n```", "```py\nimport * as tf from '@tensorflow/tfjs';\n```", "```py\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.7.0/dist/tf.min.js\">\n</script>\n```", "```py\n# Import with npm\n$ npm i @tensorflow/tfjs-node\n\n# Or Yarn\n$ yarn add @tensorflow/tfjs-node\n```", "```py\nimport * as tf from '@tensorflow/tfjs-node';\n```", "```py\nconst tf = require('@tensorflow/tfjs-node');\n```", "```py\nconsole.log(tf.version.tfjs);\n```", "```py\n$ npm run start\n```", "```py\n$ npm run start\n```", "```py\n[{\n      \"label\":\"identity_attack\",\n      \"results\":[{\n            \"probabilities\":{\n               \"0\":0.9935033917427063,\n               \"1\":0.006496586836874485\n            }, \"match\":false\n         }]\n   },{\n      \"label\":\"insult\",\n      \"results\":[{\n            \"probabilities\":{\n               \"0\":0.5021483898162842,\n               \"1\":0.4978516101837158\n            }, \"match\":false\n         }]\n   },{\n      \"label\":\"obscene\",\n      \"results\":[{\n            \"probabilities\":{\n               \"0\":0.9993441700935364,\n               \"1\":0.0006558519671671093\n            }, \"match\":false\n         }]\n   },{\n      \"label\":\"severe_toxicity\",\n      \"results\":[{\n            \"probabilities\":{\n               \"0\":0.9999980926513672,\n               \"1\":0.0000018614349528434104\n            }, \"match\":false\n         }]\n   },{\n      \"label\":\"sexual_explicit\",\n      \"results\":[{\n            \"probabilities\":{\n               \"0\":0.9997043013572693,\n               \"1\":0.00029564235592260957\n            }, \"match\":false\n         }]\n   },{\n      \"label\":\"threat\",\n      \"results\":[{\n            \"probabilities\":{\n               \"0\":0.9989342093467712,\n               \"1\":0.0010658185929059982\n            }, \"match\":false\n         }]\n   },{\n      \"label\":\"toxicity\",\n      \"results\":[{\n            \"probabilities\":{\n               \"0\":0.4567308723926544,\n               \"1\":0.543269157409668\n            }, \"match\":true\n         }]\n}]\n```", "```py\n$ npm install @tensorflow-models/toxicity\n```", "```py\nimport * as toxicity from \"@tensorflow-models/toxicity\";\n```", "```py\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity@1.2.2\">\n</script>\n```", "```py\n// minimum positive prediction confidence // If this isn't passed, the default is 0.85 constthreshold=0.5;// Load the model ![1](assets/1.png)toxicity.load(threshold).then((model)=>{constsentences=[\"You are a poopy head!\",\"I like turtles\",\"Shut up!\"];// Ask the model to classify inputs ![2](assets/2.png)model.classify(sentences).then((predictions)=>{// semi-pretty-print results\nconsole.log(JSON.stringify(predictions,null,2));![3](assets/3.png)});});\n```", "```py\n  ...\n  {\n    \"label\": \"insult\",\n    \"results\": [\n      {\n        \"probabilities\": {\n          \"0\": 0.05905626341700554,\n          \"1\": 0.9409437775611877\n        },\n        \"match\": true\n      },\n      {\n        \"probabilities\": {\n          \"0\": 0.9987999200820923,\n          \"1\": 0.0012000907445326447\n        },\n        \"match\": false\n      },\n      {\n        \"probabilities\": {\n          \"0\": 0.029087694361805916,\n          \"1\": 0.9709123373031616\n        },\n        \"match\": true\n      }\n    ]\n  },\n  ...\n```"]