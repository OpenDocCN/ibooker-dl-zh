["```py\nimport numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n  squared_errors = (y_true - y_pred) ** 2\n  return np.mean(squared_errors)\n\n```", "```py\ny_true = np.array([1.1, 0.1, 1.0])\ny_pred = np.array([0.9, 0.2, 1.2])\nmean_squared_error(y_true, y_pred)\n\n```", "```py\nnp.float64(0.030000000000000002)\n\n```", "```py\ndef mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n  \"\"\"\n Calculate the Mean Squared Error (MSE) between two NumPy arrays.\n\n Args:\n y_true (np.ndarray): Ground-truth values.\n y_pred (np.ndarray): Predicted values.\n \"\"\"\n  squared_errors = (y_true - y_pred) ** 2\n  return np.mean(squared_errors)\n\n```", "```py\nimport jax\nimport jax.numpy as jnp\n\ndef compute_ten_power_sum(arr: jax.Array) -> float:\n  \"\"\"Raise values to the power of 10 and then sum.\"\"\"\n  return jnp.sum(arr**10)\n\narr = jnp.array([1, 2, 3, 4, 5])\ncompute_ten_power_sum(arr)\n\n```", "```py\nArray(10874275, dtype=int32)\n\n```", "```py\njitted_compute_ten_power_sum = jax.jit(compute_ten_power_sum)\njitted_compute_ten_power_sum(arr)\n\n```", "```py\nArray(10874275, dtype=int32)\n\n```", "```py\n@jax.jit\ndef compute_ten_power_sum(arr: jax.Array) -> float:\n  \"\"\"Raise values to the power of 10 and then sum.\"\"\"\n  return jnp.sum(arr**10)\n\ncompute_ten_power_sum(arr)\n\n```", "```py\nArray(10874275, dtype=int32)\n\n```", "```py\nfrom functools import partial\n\ndef scale(x, scaling_factor):\n  return x * scaling_factor\n\n# Create a new function 'scale_by_10' where 'scaling_factor' is fixed to 10.\nscale_by_10 = partial(scale, scaling_factor=10)\nscale_by_10(3)\n\n```", "```py\n30\n```", "```py\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.jit, static_argnums=(0,))\ndef summarize(average_method: str, x: jax.Array) -> float:\n  if average_method == \"mean\":\n    return jnp.mean(x)\n  elif average_method == \"median\":\n    return jnp.median(x)\n  else:\n    raise ValueError(f\"Unsupported average type: {average_method}\")\n\ndata_array = jnp.array([1.0, 2.0, 100.0])\n\n# JAX compiles one version of 'summarize' for average_method=\"mean\".\nprint(f\"Mean: {summarize('mean', data_array)}\")\n\n# JAX compiles another version for average_method=\"median\".\nprint(f\"Median: {summarize('median', data_array)}\")\n\n# Calling with \"mean\" again uses the cached compiled version.\nprint(f\"Mean again: {summarize('mean', data_array)}\")\n\n```", "```py\nMean: 34.333335876464844\nMedian: 2.0\nMean again: 34.333335876464844\n\n```", "```py\ndef outer_function(x):\n  def inner_function(y):\n    return x + y  # inner_function \"closes over\" x.\n\n  return inner_function\n\nadd_five = outer_function(5)  # x is 5.\nresult = add_five(10)  # y is 10.\nprint(f\"Closure result: {result}\")\n\n```", "```py\nClosure result: 15\n```", "```py\nfrom typing import Iterator\n\ndef data_generator() -> Iterator[dict]:\n  \"\"\"Yield data samples with features and labels.\"\"\"\n  for i in range(5):\n    yield {\"feature\": i, \"label\": i % 2}\n\n# Example usage.\ngenerator = data_generator()\nnext(generator)\n\n```", "```py\n{'feature': 0, 'label': 0}\n\n```", "```py\nimport tensorflow as tf\n\nfeatures = np.array([1, 2, 3, 4, 5])\nlabels = np.array([0, 1, 0, 1, 0])\n\n# Create a TensorFlow dataset from the NumPy arrays.\ndataset = tf.data.Dataset.from_tensor_slices((features, labels))\n\n# Batch dataset with batch size of 2 and drop the final batch if incomplete.\nbatched_dataset = dataset.batch(2, drop_remainder=True)\n\n# Create a dataset (ds) iterator and retrieve the first batch using next().\nds = iter(batched_dataset)\nnext(ds)\n\n```", "```py\n(<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>,\n <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n\n```", "```py\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom flax import linen as nn\n\n# In JAX, randomness is handled explicitly by passing a random key. \n# We create a key here to seed the random number generator.\nrng = jax.random.PRNGKey(42)\n\n# Generate toy data: x values uniformly sampled between 0 and 1.\nrng, rng_data, rng_noise = jax.random.split(rng, 3)\nx_data = jax.random.uniform(rng_data, shape=(100, 1))\n\n# Add Gaussian noise.\nnoise = 0.1 * jax.random.normal(rng_noise, shape=(100, 1))\n\n# Define target: y = 2x + 1 + noise.\ny_data = 2 * x_data + 1 + noise\n\n# Visualize the noisy linear relationship.\nplt.scatter(x_data, y_data)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Toy Dataset: y = 2x + 1 + noise\")\nplt.show()\n\n```", "```py\nclass LinearModel(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    # Applies a single dense (fully connected) layer with 1 output neuron.\n    # That is, it computes y = xW + b, where the output has dimension 1.\n    return nn.Dense(features=1)(x)\n\n```", "```py\nmodel = LinearModel()\n\n```", "```py\nrng = jax.random.PRNGKey(42)\nvariables = model.init(rng, jnp.ones([1, 1]))\n\n```", "```py\nprint_short_dict(variables)\n\n```", "```py\n{'params': {'Dense_0': {'kernel': Array([[-0.5220277]], dtype=float32), 'bias':\nArray([0.], dtype=float32)}}}\n\n```", "```py\nimport optax\nfrom flax.training import train_state\n\n# Define an optimizer â€” here we use Adam with a learning rate of 1.0.\n# (Note: in most real settings you'd use a smaller learning rate like 1e-3).\ntx = optax.adam(1.0)\n\n# Create the training state.\nstate = train_state.TrainState.create(\n  apply_fn=model.apply,  # The model's forward pass function.\n  params=variables[\"params\"],  # The initialized model parameters.\n  tx=tx,  # The optimizer.\n)\n\n```", "```py\ndef calculate_loss(params, x, y):\n  # Run a forward pass of the model to get predictions.\n  predictions = model.apply({\"params\": params}, x)\n\n  # Compute MSE loss.\n  return jnp.mean((predictions - y) ** 2)\n\n```", "```py\nloss = calculate_loss(variables[\"params\"], x_data, y_data)\nprint(f\"Loss: {loss:.4f}\")\n\n```", "```py\nLoss: 5.2768\n\n```", "```py\n@jax.jit\ndef train_step(state, x, y):\n  # Compute the loss and its gradients with respect to the parameters.\n  loss, grads = jax.value_and_grad(compute_loss)(state.params, x, y)\n\n  # Apply gradient updates.\n  new_state = state.apply_gradients(grads=grads)\n\n  return new_state, loss\n\n```", "```py\n@jax.jit\ndef train_step(state, x, y):\n  def calculate_loss(params):\n    # state, x and y are not part of the function signature but are accessed.\n    predictions = state.apply_fn({\"params\": params}, x)\n    return jnp.mean((predictions - y) ** 2)\n\n  loss, grads = jax.value_and_grad(calculate_loss)(state.params)\n  state = state.apply_gradients(grads=grads)\n  return state, loss\n\n```", "```py\n@jax.jit\ndef train_step(state, x, y):\n  def calculate_loss(params):\n    predictions = state.apply_fn({\"params\": params}, x)\n    loss = jnp.mean((predictions - y) ** 2)\n    return loss, predictions  # Return both loss and preds (aux info).\n\n  (loss, predictions), grads = jax.value_and_grad(calculate_loss, has_aux=True)(\n    state.params\n  )\n  state = state.apply_gradients(grads=grads)\n  return state, (loss, predictions)\n\n```", "```py\nnum_epochs = 150  # Number of full passes through the training data.\n\nfor epoch in range(num_epochs):\n  state, (loss, _) = train_step(state, x_data, y_data)\n  if epoch % 10 == 0:\n    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n```", "```py\nEpoch 0, Loss: 5.2768\nEpoch 10, Loss: 0.9498\nEpoch 20, Loss: 0.1091\nEpoch 30, Loss: 0.0845\nEpoch 40, Loss: 0.0283\nEpoch 50, Loss: 0.0258\nEpoch 60, Loss: 0.0106\nEpoch 70, Loss: 0.0105\nEpoch 80, Loss: 0.0106\nEpoch 90, Loss: 0.0102\nEpoch 100, Loss: 0.0101\nEpoch 110, Loss: 0.0100\nEpoch 120, Loss: 0.0100\nEpoch 130, Loss: 0.0100\nEpoch 140, Loss: 0.0100\n\n```", "```py\n# Generate test data (x values between 0 and 1).\nx_test = jnp.linspace(0, 1, 10).reshape(-1, 1)\ny_test = 2 * x_test + 1  # Ground truth: linear function without noise.\n\n# Get model predictions.\ny_pred = state.apply_fn({\"params\": state.params}, x_test)\n\nplt.scatter(x_test, y_test, label=\"True values\")\nplt.plot(x_test, y_pred, color=\"red\", label=\"Model predictions\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.title(\"Linear Model Predictions vs. True Relationship\")\nplt.show()\n\n```", "```py\nfrom flax.training.early_stopping import EarlyStopping\n\n```"]