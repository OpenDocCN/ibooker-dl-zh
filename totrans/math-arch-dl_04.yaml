- en: 5 Probability distributions in machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 机器学习中的概率分布
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The role of probability distributions in machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率分布 在机器学习中的作用
- en: Working with binomial, multinomial, categorical, Bernoulli, beta, and Dirichlet
    distributions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用二项式、多项式、分类、伯努利、贝塔和狄利克雷分布
- en: The significance of entropy and cross-entropy in machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵和交叉熵在机器学习中的重要性
- en: Life often requires us to estimate the chances of an event occurring or make
    a decision in the face of uncertainty. Probability and statistics form the common
    toolbox to use in such circumstances. In machine learning, we take large feature
    vectors as inputs. As stated earlier, we can view these feature vectors as points
    in a high-dimensional space. For instance, gray-level images of size 224 × 224
    can be viewed as points in a 50, 176-dimensional space, with each pixel corresponding
    to a specific dimension. Inputs with common characteristics, such as images of
    animals, will correspond to a cluster of points in that space. Probability distributions
    provide an effective tool for analyzing such loosely structured point distributions
    in arbitrarily high-dimensional spaces. Instead of simply developing a machine
    that emits a class given an input, we can fit a probability distribution to the
    clusters of input points (or a transformed version of them) satisfying some property
    of interest. This often lends more insight into the problem we are trying to solve.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 生活中常常需要我们估计事件发生的概率或在不确定的情况下做出决策。概率和统计学是这种情况下常用的工具箱。在机器学习中，我们将大特征向量作为输入。如前所述，我们可以将这些特征向量视为高维空间中的点。例如，尺寸为
    224 × 224 的灰度图像可以视为 50,176 维空间中的点，每个像素对应一个特定的维度。具有共同特征（如动物图像）的输入将对应于该空间中点的一个簇。概率分布为分析这种在任意高维空间中松散结构化的点分布提供了有效工具。我们不仅可以简单地开发一个给定输入输出类别的机器，还可以将概率分布拟合到满足某些感兴趣特性的输入点簇（或它们的变换版本）。这通常能为我们试图解决的问题提供更多见解。
- en: For instance, suppose we are trying to design a recommendation system. We could
    design one or more classifiers that emit yes/no decisions about whether to recommend
    product X to person Y. On the other hand, we can fit probability distributions
    to specific groups of people. Doing so can lead to the discovery of significant
    overlap between the point clusters representing various groups—for instance, people
    who drink black coffee and start-up founders. We may not know the explanation
    or even the direction in which causality (if any) flows in the relationship. But
    we see the correlation and may design a better recommendation system using it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在尝试设计一个推荐系统。我们可以设计一个或多个分类器，它们会发出是否向人 Y 推荐产品 X 的是/否决策。另一方面，我们可以将概率分布拟合到特定的人群。这样做可能会导致代表不同群体的点簇之间出现显著的重叠——例如，喝黑咖啡的人和初创公司创始人。我们可能不知道解释，甚至不知道因果关系（如果有的话）的方向。但我们看到了相关性，并可能利用它设计一个更好的推荐系统。
- en: 'Another situation in which probabilistic models are used in machine learning
    is when the problem involves a very large number of (perhaps infinitely many)
    classes. For instance, suppose we are creating a machine that not only recognizes
    cats in an image but also labels each pixel as belonging or not belonging to a
    cat. Effectively, the machine segments the image pixels into foreground versus
    background. This is called *semantic segmentation*. It is hard to cast this problem
    as a classification problem: we typically design a system that emits a probability
    of being foreground for each pixel.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，概率模型被用于处理涉及大量（可能无限多）类别的问题时。例如，假设我们正在创建一个机器，它不仅能在图像中识别猫，还能为每个像素标记其属于或不属于猫。实际上，机器将图像像素分割为前景与背景。这被称为*语义分割*。将这个问题视为一个分类问题是很困难的：我们通常设计一个系统，为每个像素输出一个前景的概率。
- en: 'Probabilistic models are also used in unsupervised and minimally supervised
    learning: for instance, in *variational autoencoders* VAEs), which we discuss
    in chapter [14](../Text/14.xhtml#ch-ae-vae).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 概率模型也用于无监督学习和最小监督学习：例如，在*变分自编码器*（VAEs）中，我们将在第 [14](../Text/14.xhtml#ch-ae-vae)
    章节中讨论。
- en: This chapter introduces the fundamental notion of probability and discusses
    probability distributions (including multivariates), with specific examples, in
    a machine learning-centric way. As usual, we emphasize the geometrical view of
    multivariate statistics. An equally important goal of this chapter is to familiarize
    you with PyTorch `distributions`, the PyTorch statistical package, which we use
    throughout the book. All the distributions discussed are accompanied by code snippets
    from PyTorch `distributions`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了概率的基本概念，并从机器学习的角度讨论了概率分布（包括多元分布），并提供了具体示例。像往常一样，我们强调多元统计的几何视图。本章的另一个同样重要的目标是使您熟悉PyTorch
    `distributions`，这是我们在整本书中使用的PyTorch统计包。所有讨论的分布都附有PyTorch `distributions`的代码片段。
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng](http://mng.bz/8NVg)
    [.bz/8NVg](http://mng.bz/8NVg) in the form of fully functional and executable
    Jupyter notebooks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章的完整PyTorch代码以完全功能性和可执行的Jupyter笔记本形式，可在[http://mng](http://mng.bz/8NVg)
    [.bz/8NVg](http://mng.bz/8NVg)找到。
- en: '5.1 Probability: The classical frequentist view'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 概率：经典频率主义观点
- en: Consider a mythical city called Statsville. Suppose we choose a random adult
    inhabitant of Statsville. What are the chances of this person’s height being greater
    than 6 ft? Less than 3 ft? Between 5 ft 5 in. and 6 ft? What are the chances of
    this person’s weight being between 50 and 70 kg (physicists would rather use the
    term *mass* here, but we have chosen to stick to the more common word *weight*)?
    Greater than 100 kg? What is the probability of the person’s home being exactly
    6 km from the city center? What are the chances of the person’s weight being in
    the 50–70 kg range *and* their height being in the 5 ft 5 in. to 6 ft range? What
    are the chances of the person’s weight being greater than 90 kg *and* their home
    being within 5 km of the city center?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个虚构的城市Statsville。假设我们随机选择Statsville的一个成年居民。这个人的身高超过6英尺的概率是多少？小于3英尺的概率是多少？在5英尺5英寸到6英尺之间吗？这个人的体重在50到70公斤之间的概率是多少？（物理学家可能会在这里使用*质量*这个词，但我们选择坚持使用更常见的*体重*这个词）？大于100公斤的概率是多少？这个人的家距离市中心正好6公里的概率是多少？这个人的体重在50到70公斤范围内*并且*他们的身高在5英尺5英寸到6英尺范围内的概率是多少？这个人的体重大于90公斤*并且*他们的家在市中心5公里范围内的概率是多少？
- en: 'All these questions can be answered in the frequentist paradigm by adopting
    the following approach:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题都可以通过采用以下方法在频率主义范式中得到解答：
- en: 'Count the size of the population belonging to the desired event satisfying
    the criterion or criteria of interest): for instance, the number of Statsville
    adults taller than 6 ft. Divide that by the total size of the population (here,
    the number of adults in Statsville). This is the probability (chance) of that
    criterion/criteria being satisfied.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算满足感兴趣事件标准或标准的人口规模：例如，Statsville成年人中身高超过6英尺的人数。将这个数字除以总人口规模（在这里，Statsville的成年人数量）。这就是满足该标准/标准的概率（机会）。
- en: Formally,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，
- en: '![](../../OEBPS/Images/eq_05-01-2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![公式图](../../OEBPS/Images/eq_05-01-2.png)'
- en: Equation 5.1
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 公式5.1
- en: For instance, suppose there are 100,000 adults in the city. Of them, 25,000
    are 6 ft or taller. Then the size of the population satisfying the event of interest
    (aka the number of favorable outcomes) is 25,000\. The total population size (aka
    the number of possible outcomes) is 100,000\. So,
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设这个城市有10万名成年人。其中，有25,000人身高6英尺或更高。那么满足感兴趣事件（即有利结果的数量）的人口规模是25,000。总人口规模（即可能结果的数量）是100,000。因此，
- en: '![](../../OEBPS/Images/eq_05-01-a-2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![公式图](../../OEBPS/Images/eq_05-01-a-2.png)'
- en: Since the total population is always a superset of the population belonging
    to any event, the denominator is always greater than or equal to the numerator.
    Consequently, *probabilities are always lesser than or equal to 1*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于总人口总是任何事件所属人群的超集，所以分母总是大于或等于分子。因此，*概率总是小于或等于1*。
- en: 5.1.1 Random variables
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 随机变量
- en: 'When we talk about probability, a relevant question is, “The probability of
    what?” The simplest answer is, “The probability of the occurrence of an event.”
    For example, in the previous subsection, we discussed the probability of the event
    that the height of an adult Statsville resident is less than 6 ft. A little thought
    reveals that an event always corresponds to a numerical entity of interest taking
    a particular value or lying in a particular range of values. This entity is called
    a *random variable*. For instance, the height of adult Statsville residents can
    be a random variable, and we can talk about the probability of it being less than
    6 ft, or the weight of adult Statsville residents can be a random variable, and
    we can talk about the probability of it being less than 60 kg. When predicting
    the performance of stock markets, the Dow Jones index maybe a random variable:
    we can talk about the probability of this random variable crossing 19,000\. And
    when discussing the spread of a virus, the total number of infected people may
    be a random variable, and we can talk about the probability of it being less than
    2,000, and so on.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论概率时，一个相关的问题是，“概率是什么的概率？”最简单的答案是，“事件发生的概率。”例如，在前一小节中，我们讨论了成年人Statsville居民身高小于6英尺的事件的概率。稍加思考就可以发现，事件总是对应于一个感兴趣的数值实体取特定值或位于特定值范围内的数值。这个实体被称为*随机变量*。例如，Statsville成年人的身高可以是一个随机变量，我们可以谈论它小于6英尺的概率；Statsville成年人的体重也可以是一个随机变量，我们可以谈论它小于60公斤的概率。在预测股市表现时，道琼斯指数可能是一个随机变量：我们可以谈论这个随机变量超过19,000点的概率。在讨论病毒的传播时，感染人数的总量可能是一个随机变量，我们可以谈论它小于2,000人的概率，等等。
- en: 'The defining characteristic of a random variable is that every allowed value
    (or range of values) is associated with a probability (of the random variable
    taking that value or value range). For instance, we may allow a set of only three
    weight ranges for adults of Statsville: *S*[1], less than 60 kg; *S*[2], between
    60 and 90 kg; and *S*[3], greater than 90 kg. Then we can have a corresponding
    random variable *X* representing the quantized weight. It can take one of only
    three values: *X* = 1 (corresponding to the weight in *S*[1]), *X* = 2 (corresponding
    to the weight in *S*[2]), or *X* = 3 (corresponding to the weight in *S*[3]).
    Each value comes with a fixed probability: for example, *p*(*X* = 1) = 0.25, *p*(*X*
    = 2) = = 0.5, and *p*(*X* = 3) = 0.25, respectively, in the example from section
    [5.1](#sec-prob_frequentist). Such random variables that take values from a countable
    set are known as *discrete* random variables.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量的定义特征是每个允许的值（或值的范围）都与一个概率相关联（即随机变量取该值或值范围的概率）。例如，对于Statsville的成年人，我们可以允许只有三个体重范围：*S*[1]，小于60公斤；*S*[2]，介于60至90公斤之间；和*S*[3]，大于90公斤。然后我们可以有一个相应的随机变量
    *X* 来表示量化体重。它可以取三个值之一：*X* = 1（对应于 *S*[1] 中的体重），*X* = 2（对应于 *S*[2] 中的体重），或 *X*
    = 3（对应于 *S*[3] 中的体重）。每个值都伴随着一个固定的概率：例如，在5.1节[5.1](#sec-prob_frequentist)的例子中，*p*(*X*
    = 1) = 0.25，*p*(*X* = 2) = 0.5，*p*(*X* = 3) = 0.25，分别。这样的随机变量，其值来自可数集合，被称为*离散*随机变量。
- en: Random variables can also be *continuous*. For a continuous random variable
    *X*, we associate a probability with its value being in an infinitesimally small
    range, *p*(*x* ≤ *X* < *x* + *δx*), with *δx* → 0. This is called *probability
    density* and is explained in more detail in section [5.6](#sec-cont-rv).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量也可以是*连续的*。对于连续随机变量 *X*，我们将其值在一个无限小的范围内与一个概率相关联，*p*(*x* ≤ *X* < *x* + *δx*)，其中
    *δx* → 0。这被称为*概率密度*，在5.6节[5.6](#sec-cont-rv)中有更详细的解释。
- en: 'NOTE In this book, we always use uppercase letters to denote random variables.
    Usually, the same letter in lowercase refers to a specific value of the random
    variable: for example, *p*(*X* = *x*) denotes the probability of random variable
    *X* taking the value *x* and *p*(*X*∈{*x*, *x* + *δx*}) denotes the probability
    of random variable *X* taking a value between *x* and *x* + *δx*. Also note that
    sometimes we use the letter *X* to denote a data set. This popular but confusing
    convention is rampant in the literature—generally, the usage is obvious from the
    context.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在这本书中，我们总是使用大写字母来表示随机变量。通常，相同的小写字母表示随机变量的一个特定值：例如，*p*(*X* = *x*)表示随机变量 *X*
    取值 *x* 的概率，而 *p*(*X*∈{*x*, *x* + *δx*}) 表示随机变量 *X* 取值在 *x* 和 *x* + *δx* 之间的概率。此外，请注意，有时我们使用字母
    *X* 来表示一个数据集。这种流行但容易混淆的惯例在文献中很常见——通常，从上下文中可以明显看出其用法。
- en: 5.1.2 Population histograms
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 总体直方图
- en: 'Histograms help us to visualize discrete random variables. Let’s continue with
    our Statsville example. We are only interested in three weight ranges for Statsville
    adults: *S*[1]: less than 60 kg; *S*[2]: between 60 and 90 kg; and *S*[3]: greater
    than 90 kg. Suppose the counts of Statsville adults in these weight ranges are
    as shown in table 5.1.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '直方图帮助我们可视化离散随机变量。让我们继续以Statsville为例。我们只对Statsville成年人的三个体重范围感兴趣：*S*[1]: 低于60公斤；*S*[2]:
    介于60至90公斤之间；以及*S*[3]: 超过90公斤。假设Statsville成年人这些体重范围内的数量如表5.1所示。'
- en: Table 5.1 Frequency table for the weights of adults in the city of Statsville
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 Statsville城市成年人重量的频率表
- en: '| *S*[1]: Less than 60 kg | *S*[2]: Between 60 and 90 kg | *S*[3]: More than
    90 kg |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| *S*[1]: 低于60公斤 | *S*[2]: 介于60至90公斤之间 | *S*[3]: 超过90公斤 |'
- en: '| --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 25,000 | 50,000 | 25,000 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 25,000 | 50,000 | 25,000 |'
- en: 'The same information can be visualized by the histogram shown in figure [5.1](#fig-statsville-weight-histogram).
    The *X*-axis of the histogram corresponds to possible values of the discrete random
    variable from section [5.1.1](#sec-RVs). The *Y*-axis shows the size of the population
    in the corresponding weight range. There are 25,000 people in range *S*[1], 50,000
    people in range *S*[2], and 25,000 people in range *S*[3]. Together, these categories
    account for the entire adult population of Statsville—every adult belongs to one
    category or another. This can be verified by adding the *Y*-axis values for all
    the categories: 25, 000 + 50, 000 + 25, 000 = 100, 000, the adult population of
    Statsville.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的信息可以通过图[5.1](#fig-statsville-weight-histogram)所示的直方图来可视化。直方图的X轴对应于第[5.1.1](#sec-RVs)节中离散随机变量的可能值。Y轴显示了相应体重范围内的总体规模。有25,000人在S[1]范围内，50,000人在S[2]范围内，25,000人在S[3]范围内。这些类别共同构成了Statsville的整个成年人口——每个成年人属于一个或另一个类别。这可以通过将所有类别的Y轴值相加来验证：25,000
    + 50,000 + 25,000 = 100,000，这是Statsville的成年人口。
- en: 5.2 Probability distributions
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 概率分布
- en: 'Figure [5.1](#fig-statsville-weight-histogram) and its equivalent, table [5.1](#tab-hist-wt),
    can easily be converted to probabilities, as shown in table [5.2](#tab-hist-prob).
    The table shows the probabilities corresponding to allowed values of the discrete
    random variable *X* representing the quantized weight of a randomly chosen adult
    resident of Statsville. Table [5.2](#tab-hist-prob) represents what is formally
    known as a *probability distribution*: a mathematical function that takes a random
    variable as input and outputs the probability of it taking any allowed value.
    It must be defined over all possible values of the random variable.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5.1](#fig-statsville-weight-histogram)及其等价表格[5.1](#tab-hist-wt)可以很容易地转换为概率，如表[5.2](#tab-hist-prob)所示。该表显示了代表随机选择的Statsville成年居民量化体重的离散随机变量X的允许值的概率。表[5.2](#tab-hist-prob)代表形式上所知的*概率分布*：一个数学函数，它接受一个随机变量作为输入，并输出它取任何允许值的概率。它必须在随机变量的所有可能值上定义。
- en: Note that the set of ranges {*S*[1], *S*[2], *S*[3]} is exhaustive in the sense
    that all possible values of *X* belong to one range or other—we cannot have a
    weight that does not belong to any of them. In set-theoretical terms, the union
    of these ranges, *S*[1] ∪ *S*[2] ∪ *S*[3], covers a space that contains the entire
    population (all possible values of *X*).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，集合{S[1]，S[2]，S[3]}是穷尽的，因为在某种意义上，所有可能的X值都属于其中一个范围——我们不可能有一个不属于这些范围的权重。在集合论术语中，这些范围的并集，S[1]
    ∪ S[2] ∪ S[3]，覆盖了一个包含整个总体（所有可能的X值）的空间。
- en: '![](../../OEBPS/Images/CH05_F01_Chaudhury.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F01_Chaudhury.png)'
- en: Figure 5.1 Histogram depicting the weights of adults in Statsville, corresponding
    to table [5.1](#tab-hist-wt)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 描述Statsville成年人重量的直方图，对应表[5.1](#tab-hist-wt)
- en: Table 5.2 Probability distribution for quantized weights of Statsville adults
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 Statsville成年人量化体重的概率分布
- en: '| *S*[1]: Less than 60 kg | *S*[2]: Between 60 and 90 kg | *S*[3]: More than
    90 kg |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| *S*[1]: 低于60公斤 | *S*[2]: 介于60至90公斤之间 | *S*[3]: 超过90公斤 |'
- en: '| --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *p*(*X* = 1) = 25,000/100,000 = 0.25 | *p*(*X* = 2) = 50,000/100,000 = 0.5
    | *p*(*X* = 3) = 25,000/100,000 = 0.25 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| *p*(*X* = 1) = 25,000/100,000 = 0.25 | *p*(*X* = 2) = 50,000/100,000 = 0.5
    | *p*(*X* = 3) = 25,000/100,000 = 0.25 |'
- en: NOTE The set-theoretic operator ∪ denotes set union.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：集合论运算符∪表示集合的并。
- en: 'The ranges are also mutually exclusive in that any given observation of *X*
    can belong to only a single range, never more. In set-theoretic terms, the intersection
    of any pair of ranges is null: *S*[1] ∩ *S*[2] = *S*[1] ∩ *S*[3] = *S*[2] ∩ *S*[3]
    = *ϕ*.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 范围也是互斥的，因为任何给定的 *X* 观测只能属于一个范围，永远不可能更多。在集合论术语中，任何一对范围的交集为零：*S*[1] ∩ *S*[2] =
    *S*[1] ∩ *S*[3] = *S*[2] ∩ *S*[3] = *ϕ*。
- en: NOTE The set-theoretic operator ∩ denotes set intersection.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：集合论运算符∩表示集合交集。
- en: 'For a set of exhaustive and mutually exclusive events, the function ielding
    the probabilities of these events is a probability distribution. For instance,
    the probability distribution in our tiny example comprises three probabilities:
    *P*(*X* = 1) = 0.25, *P*(*X* = 2) = 0.5, and *P*(*X* = 3) = 0.25. This is shown
    in figure [5.2](#fig-statsville-weight-probability-distribution), which is a three-point
    graph.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组穷尽且互斥的事件，给出这些事件概率的函数称为概率分布。例如，我们这个微小例子中的概率分布包括三个概率：*P*(*X* = 1) = 0.25，*P*(*X*
    = 2) = 0.5，和 *P*(*X* = 3) = 0.25。这如图[5.2](#fig-statsville-weight-probability-distribution)所示，这是一个三点图。
- en: '![](../../OEBPS/Images/CH05_F02_Chaudhury.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH05_F02_Chaudhury.png)'
- en: Figure 5.2 Probability distribution graph for the weights of adults in Statsville,
    corresponding to table [5.2](#tab-hist-prob). Event *E*[1] ≡ *X* = 1 ⟹ a weight
    in the range *S*[1], Event *E*[2] ≡ *X* = 2 ⟹ a weight in the range *S*[2], and
    Event *E*[3] ≡ *x* = 3 ⟹ a weight in the range *S*[3].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 Statsville成年人重量的概率分布图，对应于表[5.2](#tab-hist-prob)。事件 *E*[1] ≡ *X* = 1 ⟹ 重量在
    *S*[1] 范围内，事件 *E*[2] ≡ *X* = 2 ⟹ 重量在 *S*[2] 范围内，事件 *E*[3] ≡ *x* = 3 ⟹ 重量在 *S*[3]
    范围内。
- en: 5.3 Basic concepts of probability theory
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 概率论的基本概念
- en: In this section, we briefly touch on impossible and certain events; the sum
    of probabilities of exhaustive, mutually exclusive events; and independent events.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要地涉及了不可能事件和必然事件；穷尽且互斥事件的概率之和；以及独立事件。
- en: 5.3.1 Probabilities of impossible and certain events
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 不可能事件和必然事件的概率
- en: The probability of an impossible event is zero (for example, the probability
    that the sun will rise in the west). The probability of an event that occurs with
    certitude is 1 the probability that the sun will rise in the east). Improbable
    events such as this author beating Roger Federer in competitive tennis) have low
    but nonzero probabilities, like 0.001. Highly probable events (such as Roger Federer
    beating this author in competitive tennis) have probabilities close to but not
    exactly equal to 1, like 0.999.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能事件的概率为零（例如，太阳从西边升起的概率）。必然发生的事件的概率为1（例如，太阳从东边升起的概率）。像作者在网球比赛中击败罗杰·费德勒这样的不可能事件具有低但非零的概率，例如0.001。高度可能的事件（例如罗杰·费德勒在网球比赛中击败作者）的概率接近但并不完全等于1，例如0.999。
- en: 5.3.2 Exhaustive and mutually exclusive events
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 穷尽且互斥事件
- en: 'Consider the events *E*[1], *E*[2], *E*[3] corresponding to the quantized weight
    of a Statsville adults from section [5.2](#sec-prob-distr) belonging to the range
    *S*[1], *S*[2], or *S*[3], respectively equivalently, *E*[1] is the event corresponding
    to *X* = 1, *E*[2] is the event corresponding to *X* = 2, and *E*[3] is the event
    corresponding to *X* = 3). The events are exhaustive: their union covers the entire
    population space. This means all quantized weights of Statsville adults belong
    to one of the ranges *S*[1], *S*[2], *S*[3]. The events are also mutually exclusive:
    their mutual intersections are null, meaning no member of the population can belong
    to more than one range. For example, if a weight belongs to *S*[1], it cannot
    belong to *S*[2] or *S*[3]. For such events, the following holds true:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑事件 *E*[1]、*E*[2]、*E*[3]，它们对应于Statsville成年人从[5.2](#sec-prob-distr)节中量化的体重，分别属于
    *S*[1]、*S*[2]、或 *S*[3] 范围，或者说，*E*[1] 是对应于 *X* = 1 的事件，*E*[2] 是对应于 *X* = 2 的事件，*E*[3]
    是对应于 *X* = 3 的事件。这些事件是穷尽的：它们的并集覆盖了整个总体空间。这意味着Statsville成年人的所有量化体重都属于 *S*[1]、*S*[2]、或
    *S*[3] 中的一个范围。这些事件也是互斥的：它们的交集为零，意味着总体中的任何成员不能属于多个范围。例如，如果一个重量属于 *S*[1]，它就不能属于
    *S*[2] 或 *S*[3]。对于这类事件，以下成立：
- en: The sum of the probabilities of mutually exclusive events yields the probability
    of one or the other of them occurring.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥事件的概率之和给出了它们中任何一个发生的概率。
- en: For instance, for events *E*[1], *E*[2], *E*[3],
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于事件 *E*[1]、*E*[2]、*E*[3]，
- en: '*p*(*E*[1] or *E*[2]) = *p*(*E*[1]) + *p*(*E*[2])'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*E*[1] 或 *E*[2]) = *p*(*E*[1]) + *p*(*E*[2])'
- en: Equation 5.2
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式5.2
- en: the *sum rule* says that
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**求和法则**指出，'
- en: The sum of the probabilities of an exhaustive, mutually exclusive set of events
    is always 1.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 穷尽且互斥事件概率之和总是 1。
- en: For example,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，
- en: '*p*(*E*[1]) + *p*(*E*[2]) + *p*(*E*[3]) = *p*(*E*[1] or *E*[2] or *E*[3]) =
    1'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*E*[1]) + *p*(*E*[2]) + *p*(*E*[3]) = *p*(*E*[1] or *E*[2] or *E*[3]) =
    1'
- en: This is intuitively obvious. We are merely saying that *we can say with certainty
    that either *E*[1] or *E*[2] or *E*[3] will occur*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这在直观上是显而易见的。我们只是在说，*我们可以肯定地说，*E*[1] 或 *E*[2] 或 *E*[3] 中至少有一个会发生*。
- en: In general, given a set of exhaustive, mutually exclusive events *E*[1], *E*[2],
    ⋯, *E[n]*,
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，给定一组穷尽且互斥的事件 *E*[1], *E*[2], ⋯, *E[n]*，
- en: '![](../../OEBPS/Images/eq_05-03.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-03.png)'
- en: Equation 5.3
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.3
- en: 5.3.3 Independent events
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 独立事件
- en: Consider the two events *E*[1] ≡ “weight of an adult inhabitant of Statsville
    is less than 60 kg” and *G*[1] ≡ “home of an adult inhabitant of Statsville is
    within 5 km of the city center”. These events do not influence each other at all.
    The knowledge that a member of the population weighs less than 60 kg does not
    reveal anything about the distance of their home from the city center and vice
    versa. We say *E*[1] and *G*[1] are *independent events*. Formally,
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个事件 *E*[1] ≡ “Statsville 成年居民的体重小于 60 公斤” 和 *G*[1] ≡ “Statsville 成年居民的住宅距离市中心
    5 公里以内”。这些事件之间没有任何影响。知道人口中某个成员体重小于 60 公斤并不能揭示他们住宅距离市中心的距离，反之亦然。我们说 *E*[1] 和 *G*[1]
    是 *独立事件*。正式地，
- en: A set of events are independent if the occurrence of one does not affect the
    probability of the occurrence of another.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个事件的发生不影响另一个事件发生的概率，那么这组事件是独立的。
- en: 5.4 Joint probabilities and their distributions
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 联合概率及其分布
- en: Given an adult Statsville resident, let *E*[1] be, as before, the event that
    their weight is less than 60 kg. The corresponding probability is *p*(*E*[1]).
    Also, let *G*[1] be the event that the distance of their home from the city center
    is less than 5 km. The corresponding probability is *p*(*G*[1]). Now consider
    the probability that a resident weights less than 60 kg *and* their home is less
    than 5 km from the city center. This probability, denoted *p*(*E*[1], *G*[1]),
    is called a *joint probability*. Formally,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个 Statsville 的成年居民，让 *E*[1] 仍然是他们体重小于 60 公斤的事件。相应概率是 *p*(*E*[1])。同样，让 *G*[1]
    是他们住宅距离市中心小于 5 公里的事件。相应概率是 *p*(*G*[1])。现在考虑一个居民体重小于 60 公斤且住宅距离市中心小于 5 公里的概率。这个概率，表示为
    *p*(*E*[1], *G*[1])，被称为联合概率。正式地，
- en: The joint probability of a set of events is the probability of all those events
    occurring together.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一组事件的联合概率是所有这些事件同时发生的概率。
- en: The *product rule* says that the joint probability of independent events can
    be obtained by multiplying their individual probabilities. Thus, for the current
    example, *p*(*E*[1], *G*[1]) = *p*(*E*[1])*p*(*G*[1]).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法法则指出，独立事件的联合概率可以通过乘以它们的单个概率来获得。因此，对于当前示例，*p*(*E*[1], *G*[1]) = *p*(*E*[1])*p*(*G*[1])。
- en: 'Let’s continue our discussion of joint probabilities with a slightly more elaborate
    example. We have consolidated the weight categories, corresponding populations,
    and probability distributions in table [5.3](#tab-hist-prob-wt) for quick reference.
    Similarly, we quantize the distance of residents’ homes from the city center into
    three ranges: *D*[1] ≡ less than 5 km, *D*[2] ≡ between 5 and 15 km, and *D*[3]
    ≡ greater than 15 km. Table [5.4](#tab-hist-prob-dist) shows the corresponding
    population and probability distributions. The joint probability distribution of
    the events {*E*[1], *E*[2], *E*[3]} and {*G*[1], *G*[2], *G*[3]} is shown in table
    [5.5](#tab-joint-prob-distr).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个稍微更复杂的例子继续我们的联合概率讨论。为了方便查阅，我们将权重类别、对应人口和概率分布汇总在表 [5.3](#tab-hist-prob-wt)
    中。同样，我们将居民住宅距离市中心距离量化为三个范围：*D*[1] ≡ 小于 5 公里，*D*[2] ≡ 5 到 15 公里之间，*D*[3] ≡ 大于 15
    公里。表 [5.4](#tab-hist-prob-dist) 显示了相应的人口和概率分布。事件 {*E*[1], *E*[2], *E*[3]} 和 {*G*[1],
    *G*[2], *G*[3]} 的联合概率分布显示在表 [5.5](#tab-joint-prob-distr) 中。
- en: Table 5.3 Population probability distribution table for the weights of adult
    residents of Statsville. *E*[1], *E*[2], *E*[3] are exhaustive, mutually exclusive
    events, *p*(*E*[1]) + *p*(*E*[2]) + *p*(*E*[3]) = 1.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.3 Statsville 成年居民体重的人口概率分布表。*E*[1]，*E*[2]，*E*[3] 是穷尽且互斥的事件，*p*(*E*[1]) +
    *p*(*E*[2]) + *p*(*E*[3]) = 1。
- en: '| Less than 60 kg (range *S*[1]) | Between 60 and 90 kg (range *S*[2]) | More
    than 90 kg (range *S*[3]) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 低于 60 公斤（范围 *S*[1]） | 60 到 90 公斤（范围 *S*[2]） | 高于 90 公斤（范围 *S*[3]） |'
- en: '| --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Event *E*[1] ≡ *weight* ∈ *S*[1] | Event *E*[2] ≡ *weight* ∈ *S*[2] | Event
    *E*[3] ≡ *weight* ∈ *S*[3] |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 事件 *E*[1] ≡ *重量* ∈ *S*[1] | 事件 *E*[2] ≡ *重量* ∈ *S*[2] | 事件 *E*[3] ≡ *重量*
    ∈ *S*[3] |'
- en: '| Population size = 25,000 | Population size = 50,000 | Population size = 25,000
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 人口规模 = 25,000 | 人口规模 = 50,000 | 人口规模 = 25,000 |'
- en: '| *p*(*S*[1]) = 25,000/100,000 = 0.25 | *p*(*S*[2]) = 50,000/100,000 = 0.5
    | *p*(*S*[3]) = 25,000/100,000 = 0.25 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| *p*(*S*[1]) = 25,000/100,000 = 0.25 | *p*(*S*[2]) = 50,000/100,000 = 0.5
    | *p*(*S*[3]) = 25,000/100,000 = 0.25 |'
- en: Table 5.4 Population probability distribution table for the distance of adult
    Statsville residents’ homes from the city center. *G*[1], *G*[2], *G*[3] are exhaustive,
    mutually exclusive events, *p*(*G*[1]) + *p*(*G*[2]) + *p*(*G*[3]) = 1.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.4 统计城居民家庭距离市中心距离的人口概率分布表。*G*[1]，*G*[2]，*G*[3] 是穷尽且互斥的事件，*p*(*G*[1]) + *p*(*G*[2])
    + *p*(*G*[3]) = 1。
- en: '| Less than 5 km (range *D*[1]) | Between 5 and 15 km (range *D*[2]) | Greater
    than 15 km (range *D*[3]) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 小于5公里（范围 *D*[1]） | 5至15公里（范围 *D*[2]） | 大于15公里（范围 *D*[3]） |'
- en: '| --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Event *G*[1] ≡ *distance* ∈ *D*[1] | Event *G*[2] ≡ *distance* ∈ *D*[2] |
    Event *G*[3] ≡ *distance* ∈ *D*[3] |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 事件 *G*[1] ≡ *距离* ∈ *D*[1] | 事件 *G*[2] ≡ *距离* ∈ *D*[2] | 事件 *G*[3] ≡ *距离*
    ∈ *D*[3] |'
- en: '| Population size = 20,000 | Population size = 60,000 | Population size = 20,000
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 人口规模 = 20,000 | 人口规模 = 60,000 | 人口规模 = 20,000 |'
- en: '| *p*(*G*[1]) = 20,000/100,000 = 0.20 | *p*(*G*[1]) = 60,000/100,000 = 0.6
    | *p*(*G*[1]) = 20,000/100,000 = 0.20 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| *p*(*G*[1]) = 20,000/100,000 = 0.20 | *p*(*G*[1]) = 60,000/100,000 = 0.6
    | *p*(*G*[1]) = 20,000/100,000 = 0.20 |'
- en: Table 5.5 Joint probability distribution of independent events. The sum of all
    elements in the table is 1.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.5 独立事件的联合概率分布。表中所有元素的总和为1。
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | 小于60公斤 (*E*[1]) | 60至90公斤 (*E*[2]) | 超过90公斤 (*E*[3]) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | '
- en: '| **Less than 5 km** (*G*[1]) | *p*(*E*[1], *G*[1])= 0.25 × 0.2= 0.05 | *p*(*E*[2],
    *G*[1])= 0.5 × 0.2= 0.1 | *p*(*E*[3], *G*[1])= 0.25 × 0.2= 0.05 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| **小于5公里** (*G*[1]) | *p*(*E*[1], *G*[1])= 0.25 × 0.2= 0.05 | *p*(*E*[2],
    *G*[1])= 0.5 × 0.2= 0.1 | *p*(*E*[3], *G*[1])= 0.25 × 0.2= 0.05 |'
- en: '| **Between 5 and 15 km** (*G*[2]) | *p*(*E*[1], *G*[2])= 0.25 × 0.6= 0.15
    | *p*(*E*[2], *G*[2])= 0.5 × 0.6= 0.3 | *p*(*E*[3], *G*[2])= 0.25 × 0.6= 0.15
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **5至15公里之间** (*G*[2]) | *p*(*E*[1], *G*[2])= 0.25 × 0.6= 0.15 | *p*(*E*[2],
    *G*[2])= 0.5 × 0.6= 0.3 | *p*(*E*[3], *G*[2])= 0.25 × 0.6= 0.15 |'
- en: '| **More than 15 km** (*G*[3]) | *p*(*E*[1], *G*[3])= 0.25 × 0.2= 0.05 | *p*(*E*[2],
    *G*[3])= 0.5 × 0.2= 0.1 | *p*(*E*[3], *G*[3])= 0.25 × 0.2= 0.05 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **超过15公里** (*G*[3]) | *p*(*E*[1], *G*[3])= 0.25 × 0.2= 0.05 | *p*(*E*[2],
    *G*[3])= 0.5 × 0.2= 0.1 | *p*(*E*[3], *G*[3])= 0.25 × 0.2= 0.05 |'
- en: 'We can make the following statements about table [5.5](#tab-joint-prob-distr):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 关于表 [5.5](#tab-joint-prob-distr) 我们可以做出以下陈述：
- en: 'The sum total of all elements in table [5.5](#tab-joint-prob-distr) is 1. In
    other words, *p*(*E[i]*, *G[j]*) is a proper probability distribution indicating
    the probabilities of event *E[i]* and event *G[j]* occurring together: here, (*i*,
    *j*) ∈ {1,2,3} × {1,2,3}.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 [5.5](#tab-joint-prob-distr) 中所有元素的总和为1。换句话说，*p*(*E[i]*, *G[j]*) 是一个合适的概率分布，表示事件
    *E[i]* 和事件 *G[j]* 同时发生的概率：在这里，(*i*, *j*) ∈ {1,2,3} × {1,2,3}。
- en: '*p*(*E[i]*, *G[j]*) = *p*(*E[i]*)*p*(*G[j]*) ∀(*i*, *j*) ∈ {1,2,3} × {1,2,3}.
    This is because the events are independent.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*(*E[i]*, *G[j]*) = *p*(*E[i]*)*p*(*G[j]*) ∀(*i*, *j*) ∈ {1,2,3} × {1,2,3}。这是因为事件是独立的。'
- en: 'NOTE The symbol × denotes the *Cartesian product*. The Cartesian product of
    two sets {1,2,3} × {1,2,3} is the set {(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)}.
    And the symbol ∀ indicates “for all.” Read ∀(*i*, *j*) ∈ {1,2,3} × {1,2,3} as
    follows: for all pairs (*i*, *j*) in the Cartesian product, {1,2,3} × {1,2,3}.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：符号 × 表示 *笛卡尔积*。集合 {1,2,3} × {1,2,3} 的笛卡尔积是集合 {(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)}。符号
    ∀ 表示“对于所有”。读作 ∀(*i*, *j*) ∈ {1,2,3} × {1,2,3}，即对于笛卡尔积中的所有对 (*i*, *j*)。
- en: 'In general, given a set of independent events *E*[1], *E*[2], ⋯, *E[n]*, the
    joint probability *p*(*E*[1], *E*[2],⋯, *E[n]*) of all the events occurring together
    is the product of their individual probabilities of occurring:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，给定一组独立事件 *E*[1]，*E*[2]，⋯，*E[n]*，所有事件同时发生的联合概率 *p*(*E*[1], *E*[2],⋯, *E[n]*)
    是它们各自发生概率的乘积：
- en: '![](../../OEBPS/Images/eq_05-04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-04.png)'
- en: Equation 5.4
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 5.4
- en: NOTE The symbol ∏ stands for “product.”
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：符号 ∏ 表示“乘积”。
- en: 5.4.1 Marginal probabilities
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 边缘概率
- en: 'Suppose we do not have the individual probabilities *p*(*E[i]*) and *p*(*G[j]*).
    All we have is the joint probability distribution: that is, table [5.5](#tab-joint-prob-distr).
    Can we find the individual probabilities from them? If so, how?'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们没有个体概率 *p*(*E[i]*) 和 *p*(*G[j]*)。我们拥有的只是联合概率分布：即表 [5.5](#tab-joint-prob-distr)。我们能从这些中找到个体概率吗？如果能，那么如何？
- en: 'To answer this question, consider a particular row or column in table [5.5](#tab-joint-prob-distr)—say,
    the top row. In this row, the *E* values iterate over all possibilities (the entire
    space of *E*s), but *G* is fixed at *G*[1]. If *G*[1] is to occur, there are only
    three possibilities: it occurs with *E*[1], *E*[2], or *E*[3]. The corresponding
    joint probabilities are *p*(*E*[1], *G*[1]), *p*(*E*[2], *G*[1]), and *p*(*E*[3],
    *G*[1]). If we add them, we get the probability of *G*[1] occurring with *E*[1]
    or *E*[2], or *E*[3]: that is, event (*E*[1], *G*[1]) or (*E*[2], *G*[1]) or (*E*[3],
    *G*[1]). Thus we have considered all situations under which *G*[1] can occur.
    The sum represents the probability of event *G*[1] occurring. Thus, *p*(*G*[1])
    can be obtained by adding all the probabilities in the row corresponding to *G*[1]
    and writing it in the margin (this is why it is called the *marginal* probability).
    Similarly, by adding all the probabilities in the middle column, we obtain the
    probability *p*(*E*[2]), and so forth. Table [5.6](../Text/05.xhtml#tab-jmarginal-prob)
    shows table [5.5](#tab-joint-prob-distr) updated with marginal probabilities.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，考虑表 [5.5](#tab-joint-prob-distr) 中的特定行或列——比如说，最上面的行。在这一行中，*E* 的值遍历所有可能性（*E*
    的整个空间），但 *G* 固定在 *G*[1]。如果 *G*[1] 要发生，只有三种可能性：它与 *E*[1]，*E*[2]，或 *E*[3] 同时发生。相应的联合概率是
    *p*(*E*[1], *G*[1])，*p*(*E*[2], *G*[1])，和 *p*(*E*[3], *G*[1])。如果我们把它们加起来，我们得到
    *G*[1] 与 *E*[1] 或 *E*[2] 或 *E*[3] 同时发生的概率：即事件 (*E*[1], *G*[1]) 或 (*E*[2], *G*[1])
    或 (*E*[3], *G*[1])。因此，我们已经考虑了 *G*[1] 可以发生的所有情况。总和代表了事件 *G*[1] 发生的概率。因此，*p*(*G*[1])
    可以通过将对应于 *G*[1] 的行的所有概率相加，并在边缘写下它来获得（这就是为什么它被称为边缘概率）。同样，通过将中间列的所有概率相加，我们获得 *p*(*E*[2])
    的概率，等等。表 [5.6](../Text/05.xhtml#tab-jmarginal-prob) 显示了带有边缘概率的更新后的表 [5.5](#tab-joint-prob-distr)。
- en: Table 5.6 Joint probability distribution with marginal probabilities shown
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.6 显示边缘概率的联合概率分布
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) | Marginals for G’s |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 小于60公斤 (*E*[1]) | 60至90公斤之间 (*E*[2]) | 超过90公斤 (*E*[3]) | G的边缘概率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Less than 5 km** (*G*[1]) | *p*(*E*[1], *G*[1])= 0.25 × 0.2= 0.05 | *p*(*E*[2],
    *G*[1])= 0.5 × 0.2= 0.1 | *p*(*E*[3], *G*[1])= 0.25 × 0.2= 0.05 | *p*(*G*[1])=
    0.05 + 0.1 + 0.05= 0.2 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **小于5公里** (*G*[1]) | *p*(*E*[1], *G*[1])= 0.25 × 0.2= 0.05 | *p*(*E*[2],
    *G*[1])= 0.5 × 0.2= 0.1 | *p*(*E*[3], *G*[1])= 0.25 × 0.2= 0.05 | *p*(*G*[1])=
    0.05 + 0.1 + 0.05= 0.2 |'
- en: '| **Between 5 and 15 km** (*G*[2]) | *p*(*E*[1], *G*[2])= 0.25 × 0.6= 0.15
    | *p*(*E*[2], *G*[2])= 0.5 × 0.6= 0.3 | *p*(*E*[3], *G*[2])= 0.25 × 0.6= 0.15
    | *p*(*G*[2])0.15 + 0.3 + 0.15= 0.6 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **5至15公里之间** (*G*[2]) | *p*(*E*[1], *G*[2])= 0.25 × 0.6= 0.15 | *p*(*E*[2],
    *G*[2])= 0.5 × 0.6= 0.3 | *p*(*E*[3], *G*[2])= 0.25 × 0.6= 0.15 | *p*(*G*[2])=
    0.15 + 0.3 + 0.15= 0.6 |'
- en: '| **More than 15 km** (*G*[3]) | *p*(*E*[1], *G*[3])= 0.25 × 0.2= 0.05 | *p*(*E*[2],
    *G*[3])= 0.5 × 0.2= 0.1 | *p*(*E*[3], *G*[3])= 0.25 × 0.2= 0.05 | *p*(*G*[3])=
    0.05 + 0.1 + 0.05= 0.2 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **超过15公里** (*G*[3]) | *p*(*E*[1], *G*[3])= 0.25 × 0.2= 0.05 | *p*(*E*[2],
    *G*[3])= 0.5 × 0.2= 0.1 | *p*(*E*[3], *G*[3])= 0.25 × 0.2= 0.05 | *p*(*G*[3])=
    0.05 + 0.1 + 0.05= 0.2 |'
- en: '| **Marginals for** *E***s** | *p*(*E*[1])= 0.05 + 0.15 + 0.050.05 = 0.25 |
    *p*(*E*[2])= 0.1 + 0.3 + 0.1= 0.5 | *p*(*E*[3])= 0.05 + 0.15 += 0.25 |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **E的边缘概率** | *p*(*E*[1])= 0.05 + 0.15 + 0.05= 0.25 | *p*(*E*[2])= 0.1 + 0.3
    + 0.1= 0.5 | *p*(*E*[3])= 0.05 + 0.15 += 0.25 |  |'
- en: In general, given a set of exhaustive, mutually exclusive events *E*[1], *E*[2],
    ⋯, *E[n]*, another event *G*, and joint probabilities *p*(*E*[1], *G*), *p*(*E*[2],
    *G*), ⋯, *p*(*E[n]*, *G*),
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，给定一组穷尽且互斥的事件 *E*[1]，*E*[2]，⋯，*E[n]*，另一个事件 *G*，以及联合概率 *p*(*E*[1], *G*), *p*(*E*[2],
    *G*), ⋯, *p*(*E[n]*, *G*),
- en: '![](../../OEBPS/Images/eq_05-05.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-05.png)'
- en: Equation 5.5
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 5.5
- en: By summing over all possible values of *E[i]*s, we factor out the *E*s. This
    is because the *E*s are mutually exclusive and exhaustive; summing over them results
    in a certain event that is factored out (remember, the probability of a certain
    event is 1).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对所有可能的 *E[i]* 值求和，我们消除了 *E*。这是因为 *E* 是互斥且穷尽的；对它们的求和会导致一个被消去的特定事件（记住，特定事件的概率是
    1）。
- en: 5.4.2 Dependent events and their joint probability distribution
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 依赖事件及其联合概率分布
- en: 'So far, the events we have considered jointly are “weights” and “distance of
    a resident’s home from the city center.” These are independent of each other—their
    joint is the product of their individual probabilities. Now, let’s discuss a different
    situation where the variables are connected and knowing the value of one does
    help us predict the other. For instance, the weights and heights of adult residents
    of Statsville are not independent: typically, taller people weigh more, and vice
    versa.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们共同考虑的事件是“体重”和“居民家离市中心距离”。它们是相互独立的——它们的联合是各自概率的乘积。现在，让我们讨论一个不同的情况，其中变量是相互关联的，知道一个变量的值确实有助于我们预测另一个变量。例如，Statsville
    成年居民的体重和身高不是独立的：通常，身高较高的人体重较重，反之亦然。
- en: 'As usual, we use a toy example to understand the idea. We quantize heights
    into three ranges, *H*[1] ≡ less than 5 ft 5 in., *H*[2] ≡ between 5 ft 5 in.
    and 6 ft, and *H*[3] ≡ greater than 6 ft. Let *z* be the random variable corresponding
    to height. We have three possible events with respect to height: *F*[1] ≡ *z*
    ∈ *H*[1], *F*[2] ≡ *z* ∈ *H*[2], and *F*[3] ≡ *z* ∈ *H*[3]. The joint probability
    distribution of height and weight is shown in table [5.7](#tab-joint-prob-distr-dep).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们用一个玩具例子来理解这个概念。我们将身高量化为三个范围，*H*[1] ≡ 低于 5 英尺 5 英寸，*H*[2] ≡ 介于 5 英尺 5
    英寸和 6 英尺之间，以及 *H*[3] ≡ 超过 6 英尺。令 *z* 为对应身高的随机变量。关于身高，我们有三种可能的事件：*F*[1] ≡ *z* ∈
    *H*[1]，*F*[2] ≡ *z* ∈ *H*[2]，和 *F*[3] ≡ *z* ∈ *H*[3]。身高和体重的联合概率分布显示在表 [5.7](#tab-joint-prob-distr-dep)
    中。
- en: Table 5.7 Joint probability distribution of dependent events
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.7 相关事件的联合概率分布
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | 低于 60 公斤 (*E*[1]) | 60 至 90 公斤 (*E*[2]) | 超过 90 公斤 (*E*[3]) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Less than** 5 ft 5 **in. (*F*[1])** | *p*(*E*[1], *F*[1]) = 0.25 | *p*(*E*[2],
    *F*[1]) = 0 | *p*(*E*[3], *F*[1]) = 0 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **低于 5 英尺 5 英寸** (*F*[1])** | *p*(*E*[1], *F*[1]) = 0.25 | *p*(*E*[2], *F*[1]) =
    0 | *p*(*E*[3], *F*[1]) = 0 |'
- en: '| **Between 5 ft 5 in. and 6 ft** (*F*[2]) | *p*(*E*[1], *F*[2]) = 0. | *p*(*E*[2],
    *F*[2]) = 0.5 | *p*(*E*[3], *F*[2]) = 0 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **介于 5 英尺 5 英寸和 6 英尺之间** (*F*[2]) | *p*(*E*[1], *F*[2]) = 0. | *p*(*E*[2],
    *F*[2]) = 0.5 | *p*(*E*[3], *F*[2]) = 0 |'
- en: '| **More than** **6 ft (*F*[3])**. | *p*(*E*[1], *F*[3]) = 0 | *p*(*E*[2],
    *F*[3]) = 0 | *p*(*E*[3], *F*[3]) = 0.25 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **超过 6 英尺** (*F*[3]). | *p*(*E*[1], *F*[3]) = 0 | *p*(*E*[2], *F*[3]) = 0
    | *p*(*E*[3], *F*[3]) = 0.25 |'
- en: 'Note the following about table [5.7](#tab-joint-prob-distr-dep):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下关于表 [5.7](#tab-joint-prob-distr-dep) 的内容：
- en: The sum total of all elements in table [5.7](#tab-joint-prob-distr-dep) is 1.
    In other words, *p*(*E[i]*, *F[j]*) is a proper probability distribution indicating
    the probabilities of event *E[i]* and event *F[j]* occurring together. Here (*i*,
    *j*) ∈ {1,2,3} × {1,2,3}.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表 [5.7](#tab-joint-prob-distr-dep) 中所有元素的总和为 1。换句话说，*p*(*E[i]*, *F[j]*) 是一个合适的概率分布，表示事件
    *E[i]* 和事件 *F[j]* 同时发生的概率。这里 (*i*, *j*) ∈ {1,2,3} × {1,2,3}。
- en: '*p*(*E[i]*, *F[j]*) = 0 *if* *i* ≠ *j* ∀(*i*, *j*) ∈ {1,2,3} × {1,2,3}. This
    essentially means the events are perfectly correlated: the occurrence of *E*[1]
    implies the occurrence of *F*[1] and vice versa, the occurrence of *E*[2] implies
    the occurrence of *F*[2] and vice versa, and the occurrence of *E*[3] implies
    the occurrence of *F*[3] and vice versa. In other words, every adult resident
    of Statsville who weighs less than 60 kg is also shorter than 5 ft 5 in., and
    so on. (In life, such perfect correlations rarely exist; but Statsville is a mythical
    town.)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*(*E[i]*, *F[j]*) = 0 *if* *i* ≠ *j* ∀(*i*, *j*) ∈ {1,2,3} × {1,2,3}。这实际上意味着事件是完全相关的：*E*[1]
    的发生意味着 *F*[1] 的发生，反之亦然，*E*[2] 的发生意味着 *F*[2] 的发生，反之亦然，*E*[3] 的发生意味着 *F*[3] 的发生，反之亦然。换句话说，Statsville
    每个体重低于 60 公斤的成年居民身高也低于 5 英尺 5 英寸，等等。（在现实生活中，这种完美的相关性很少存在；但 Statsville 是一个神话般的小镇。）'
- en: '5.5 Geometrical view: Sample point distributions for dependent and independent
    variables'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 几何视图：相关变量和独立变量的样本点分布
- en: Let’s look at a graphical view of the point distributions corresponding to tables
    [5.5](#tab-joint-prob-distr) and [5.7](#tab-joint-prob-distr-dep). There is a
    fundamental difference in how the point distributions look for independent and
    dependent variables; it is connected to principal component analysis (PCA) and
    dimensionality reduction, which we discussed in section [4.4](../Text/04.xhtml#sec-pca).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看与表 [5.5](#tab-joint-prob-distr) 和 [5.7](#tab-joint-prob-distr-dep) 对应的点分布的图形视图。独立变量和相关变量的点分布看起来有根本性的不同；这与主成分分析
    (PCA) 和降维有关，我们已在第 [4.4](../Text/04.xhtml#sec-pca) 节中讨论过。
- en: We use a rectangular bucket-based technique to visualize joint 2D discrete events.
    For instance, we have three weight-related events, *E*[1], *E*[2], *E*[3], and
    three distance-related events, *G*[1], *G*[2], *G*[3]. Hence the joint distribution
    has 3 × 3 = 9 possible events (*E[i]*, *G[j]*), ∀(*i*, *j*) ∈ {1,2,3} × {1,2,3},
    as shown in table [5.5](#tab-joint-prob-distr). Each of these nine events is represented
    by a small rectangle (bucket for the joint event); altogether, we have a 3 × 3
    grid of rectangular buckets. To visualize the sample point distribution, we have
    drawn 1,000 samples from the joint distribution. A joint event sample is placed
    at a random location within its bucket (that is, all points within the bucket
    have an equal probability of being selected). Notice that the concentration of
    points is greater inside high-probability buckets and vice versa.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用基于矩形桶的技术来可视化二维离散事件的联合。例如，我们有三个与权重相关的事件，*E*[1]、*E*[2]、*E*[3]，以及三个与距离相关的事件，*G*[1]、*G*[2]、*G*[3]。因此，联合分布有3
    × 3 = 9种可能的事件（*E[i]*，*G[j]*），∀(*i*, *j*) ∈ {1,2,3} × {1,2,3}，如表[5.5](#tab-joint-prob-distr)所示。这九个事件中的每一个都由一个小矩形（联合事件的桶）表示；总共，我们有一个3
    × 3的矩形桶网格。为了可视化样本点分布，我们从联合分布中抽取了1,000个样本。联合事件样本放置在其桶内的随机位置（即，桶内的所有点被选中的概率相等）。请注意，点在概率高的桶内集中，反之亦然。
- en: '![](../../OEBPS/Images/CH05_F03a_Chaudhury.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 联合概率分布的图形可视化](../../OEBPS/Images/CH05_F03a_Chaudhury.png)'
- en: (a)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![](../../OEBPS/Images/CH05_F03b_Chaudhury.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 联合概率分布的图形可视化](../../OEBPS/Images/CH05_F03b_Chaudhury.png)'
- en: (b)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: Figure 5.3 Graphical visualization of joint probability distributions. Rectangles
    represent buckets of different discrete events. (a) From table [5.5](#tab-joint-prob-distr)
    independent events). The probabilities of all nine events are non-negligible,
    and all nine rectangles have a relatively high concentration of sample points.
    Not suitable for PCA. (b) From table [5.7](#tab-joint-prob-distr-dep) (non-independent
    events). Events (*E*[1], *F*[1]), (*E*[2], *F*[2]), and (*E*[3], *F*[3]) have
    very high probabilities, and other events have negligible probabilities. Sample
    points are concentrated along the rectangles on the diagonal. Suitable for PCA.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 联合概率分布的图形可视化。矩形代表不同离散事件的桶。（a）来自表[5.5](#tab-joint-prob-distr)的独立事件）。所有九个事件的概率都不是微不足道的，所有九个矩形都有相对较高的样本点集中度。不适合PCA。（b）来自表[5.7](#tab-joint-prob-distr-dep)（非独立事件）。事件(*E*[1]，*F*[1])、(*E*[2]，*F*[2])和(*E*[3]，*F*[3])的概率非常高，而其他事件的概率可以忽略不计。样本点集中在对角线上的矩形上。适合PCA。
- en: Graphical views of the point distribution for the independent table [5.5](#tab-joint-prob-distr))
    and non-independent table [5.7](#tab-joint-prob-distr-dep)) joint variable pairs
    are shown in figures [5.3a](#fig5_3) and [5.3b](#fig5_3), respectively. We see
    that *the sample point distribution for the independent events is spread somewhat
    symmetrically over the domain*, while *that for the dependent events is spread
    narrowly around a particular line* (in this case, the diagonal). This holds true
    in general and for higher dimensions, too. You should have this mental picture
    with respect to independent versus non-independent point distributions. If we
    sample independent events (uncorrelated), all possible combinations of events
    {*E*[1], *G*[1]}, {*E*[1], *G*[2]}, {*E*[1], *G*[3]}, ⋯, {*E*[3], *G*[3]} have
    a non-negligible probability of occurrence (see table [5.5](#tab-joint-prob-distr)),
    which is equivalent to saying that none of the events have a very high probability
    of occurring remember that probabilities sum to 1, so if some events have very
    low probabilities [close to zero], other events must have high probabilities [near
    one] to compensate). This precludes the concentration of points in a small region
    of the space. All buckets will have many points. In other words, the joint probability
    samples of independent events are diffused throughout the population space (see
    figure [5.3a](#fig5_3), for instance).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 独立表[5.5](#tab-joint-prob-distr)和非独立表[5.7](#tab-joint-prob-distr-dep)的联合变量对的点分布图分别显示在图[5.3a](#fig5_3)和[5.3b](#fig5_3)中。我们看到，独立事件的样本点分布在整个域上分布得相对对称，而相关事件的样本点分布则围绕特定的线条（在这种情况下，对角线）较窄。这在一般情况下以及更高维的情况下都成立。你应该对独立与非独立点分布有这种心理图景。如果我们采样独立事件（不相关），所有可能的事件组合（如{*E*[1],
    *G*[1]}，{*E*[1], *G*[2]}，{*E*[1], *G*[3]}，⋯，{*E*[3], *G*[3]}）都有非微不足道的发生概率（见表[5.5](#tab-joint-prob-distr)），这相当于说，没有任何事件有非常高的发生概率记住，概率总和为1，所以如果某些事件有非常低的概率[接近零]，其他事件必须有高概率[接近一]来补偿）。这阻止了点在空间的小区域内集中。所有桶都将有许多点。换句话说，独立事件的联合概率样本在整个种群空间中扩散（例如，见图[5.3a](#fig5_3)）。
- en: On the other hand, if the events are correlated, the joint probability samples
    are concentrated in certain high-probability regions of the joint space. For instance,
    in table [5.7](#tab-joint-prob-distr-dep), events (*E*[1], *F*[1]), (*E*[2], *F*[2]),
    (*E*[3], *F*[3]) are far more likely than the other combinations. Hence, the sample
    points are concentrated along the corresponding diagonal (see figure [5.3b](#fig5_3)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果事件是相关的，联合概率样本将集中在联合空间中某些高概率区域。例如，在表[5.7](#tab-joint-prob-distr-dep)中，事件(*E*[1],
    *F*[1])，(*E*[2], *F*[2])，(*E*[3], *F*[3])比其他组合更有可能。因此，样本点集中在相应的对角线上（见图[5.3b](#fig5_3)）。
- en: 'If this does not remind you of PCA (section [4.4](../Text/04.xhtml#sec-pca)),
    you should re-read that section. Dependent events such as that shown in figure
    [5.3a](#fig5_3) are good candidates for dimensionality reduction: the two dimensions
    essentially carry the same information, and if we know one, we can derive the
    other. We can drop one of the highly correlated dimensions without losing significant
    information.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这没有让你想起PCA（第[4.4](../Text/04.xhtml#sec-pca)节），你应该重新阅读那一节。如图[5.3a](#fig5_3)所示的相关事件是降维的良好候选：两个维度本质上携带相同的信息，如果我们知道其中一个，我们就可以推导出另一个。我们可以去掉一个高度相关的维度，而不会丢失显著的信息。
- en: 5.6 Continuous random variables and probability density
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 连续随机变量和概率密度
- en: So far, we have quantized our random variables and made them discrete. For instance,
    weight has been quantized into three buckets—less than 60 kg, between 60 and 90
    kg, and greater than 90 kg—and probabilities have been assigned to each bucket.
    What if we want to know probabilities at a more granular level like 0 to 10 kg,
    10 to 20 kg, 20 to 30 kg, and so on? Well, we have to create more buckets. Each
    bucket covers a narrower range of values (a smaller portion of the population
    space), but there are more of them. In all cases, following the frequentist approach,
    we count the number of adult Statsvilleans in each bucket, divide that by the
    total population size, and call that the probability of belonging to that bucket.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将随机变量量化并使其离散化。例如，体重已经被量化为三个区间——小于60公斤、60至90公斤之间，以及大于90公斤——并且每个区间都分配了概率。如果我们想了解更细粒度的概率，比如0至10公斤、10至20公斤、20至30公斤等等，怎么办？嗯，我们必须创建更多的区间。每个区间覆盖的值范围更窄（人口空间中更小的一部分），但数量更多。在所有情况下，遵循频率主义方法，我们计算每个区间中成年Statsville人的数量，将其除以总人口规模，并将这个值称为属于该区间的概率。
- en: What if we want even further granularity? We create even more buckets, each
    covering an even smaller portion of the population space. In the limit, we have
    an infinite number of buckets, each covering an infinitesimally small portion
    of the population space. Together they still cover the population space—a very
    large number of very small pieces can cover arbitrary regions. At this limit,
    the probability distribution function is called a *probability density function*.
    Formally,
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要更细的粒度呢？我们创建更多的区间，每个区间覆盖的人口空间部分更小。在极限情况下，我们有一个无限多的区间，每个区间覆盖的人口空间部分无限小。它们共同覆盖了人口空间——大量非常小的部分可以覆盖任意区域。在这个极限情况下，概率分布函数被称为*概率密度函数*。正式地，
- en: The probability density function *p*(*x*) for a continuous random variable *X*
    is defined as the probability that *X* lies between *x* and *x* + *δx* with *δx*
    → 0
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续随机变量 *X* 的概率密度函数 *p*(*x*) 定义为 *X* 位于 *x* 和 *x* + *δx* 之间的概率，其中 *δx* → 0
- en: '*p*(*x*) = lim[*δx*→0] *probability*(*x* ≤ *X* < *x* + *δx*)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*x*) = lim[*δx*→0] *概率*(*x* ≤ *X* < *x* + *δx*)'
- en: NOTE It is slightly unfortunate that the typical symbol for a random variable,
    *X*, collides with that for a dataset (collection of data vectors), also *X*.
    But the context is usually enough to tell them apart.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：有一点遗憾的是，随机变量 *X* 的典型符号与数据集（数据向量的集合）的符号也相同，即 *X*。但通常情况下，上下文足以区分它们。
- en: There is a bit of theoretical nuance here. We are saying that *p*(*x*) is the
    probability of the random variable *X* lying between *x* and *x* + *δx*. This
    is not exactly the same as saying that *p*(*x*) is the probability that *X* is
    *equal* to *x*. But because *δx* is infinitesimally small, they amount to the
    same thing.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一点理论上的细微差别。我们说的是 *p*(*x*) 是随机变量 *X* 位于 *x* 和 *x* + *δx* 之间的概率。这并不完全等同于说 *p*(*x*)
    是 *X* 等于 *x* 的概率。但由于 *δx* 无穷小，它们实际上是相同的东西。
- en: 'Consider the set of events *E* = lim[*δx*→0] {*x* ≤ *X* < *x* + *δx*} for all
    possible values of *x*. All possible values of *x* range from negative infinity
    to infinity: *x* ∈ [−∞,∞]. There are infinite such events, each of which is infinitesimally
    narrow, but together they cover the entire domain *x* ∈ [−∞,∞]. In other words,
    they are exhaustive. They are also mutually exclusive because *x* cannot belong
    to more than one of them at the same time. They are continuous counterparts of
    the discrete events *E*[1], *E*[2], *E*[3] that we have seen before.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑事件集 *E* = lim[*δx*→0] {*x* ≤ *X* < *x* + *δx*} 对所有可能的 *x* 值。所有可能的 *x* 值范围从负无穷大到无穷大：*x*
    ∈ [−∞,∞]。存在无限多个这样的事件，每个事件都非常窄，但共同覆盖了整个域 *x* ∈ [−∞,∞]。换句话说，它们是穷尽的。它们也是互斥的，因为 *x*
    不能同时属于它们中的多个。它们是之前看到的离散事件 *E*[1]、*E*[2]、*E*[3] 的连续对应物。
- en: The fact that the set of events *E* = lim[*δx* → 0]{*x* ≤ *X* < *x* + *δx*}
    in continuous space is exhaustive and mutually exclusive means we can apply equation
    [5.3](../Text/05.xhtml#eq-discrete-prob-sum) but the sum will be replaced by an
    integral as the variable is continuous.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续空间中，事件集 *E* = lim[*δx* → 0]{*x* ≤ *X* < *x* + *δx*} 的穷尽性和互斥性意味着我们可以应用方程式
    [5.3](../Text/05.xhtml#eq-discrete-prob-sum)，但求和将被积分所取代，因为变量是连续的。
- en: The sum rule in a continuous domain is expressed as
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 连续域中的求和规则表示为
- en: '![](../../OEBPS/Images/eq_05-06.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-06.png)'
- en: Equation 5.6
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 5.6
- en: Equation [5.6](#eq-continuous-prob-sum) is the continuous analog of equation
    [5.3](../Text/05.xhtml#eq-discrete-prob-sum). It physically means we can say with
    certainty that *x* lies somewhere in the interval −∞ to ∞.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [5.6](#eq-continuous-prob-sum) 是方程 [5.3](../Text/05.xhtml#eq-discrete-prob-sum)
    的连续对应物。在物理上意味着我们可以肯定地说 *x* 位于区间 −∞ 到 ∞ 的某个地方。
- en: The random variable can also be multidimensional (that is, a vector). Then the
    probability density function is denoted as *p*(![](../../OEBPS/Images/AR_x.png)).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量也可以是多维的（即，一个向量）。那么概率密度函数表示为 *p*(![](../../OEBPS/Images/AR_x.png))。
- en: The sum rule for a continuous multidimensional probability density function
    is
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 连续多维概率密度函数的求和法则为
- en: '![](../../OEBPS/Images/eq_05-07.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-07.png)'
- en: Equation 5.7
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.7
- en: where *D* is the domain of ![](../../OEBPS/Images/AR_x.png)—that is, the space
    containing all possible values of the vector ![](../../OEBPS/Images/AR_x.png).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *D* 是 ![](../../OEBPS/Images/AR_x.png) 的定义域——即包含向量 ![](../../OEBPS/Images/AR_x.png)
    所有可能值的空间。
- en: For instance, the 2D vector ![](../../OEBPS/Images/eq_05-07-a.png) has the *XY*
    plane as its domain. Note that the integral in equation [5.7](#eq-continuous-vec-prob-sum)
    is a *multidimensional* integral (for example, for 2D ![](../../OEBPS/Images/AR_x.png),
    it is ∬[![](../../OEBPS/Images/AR_x.png)∈*D*] *p*(![](../../OEBPS/Images/AR_x.png))
    *d*![](../../OEBPS/Images/AR_x.png) = 1).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，二维向量 ![](../../OEBPS/Images/eq_05-07-a.png) 的定义域是 *XY* 平面。请注意，方程 [5.7](#eq-continuous-vec-prob-sum)
    中的积分是一个 *多维* 积分（例如，对于二维 ![](../../OEBPS/Images/AR_x.png)，它是 ∬[![](../../OEBPS/Images/AR_x.png)∈*D*]
    *p*(![](../../OEBPS/Images/AR_x.png)) *d*![](../../OEBPS/Images/AR_x.png) = 1）。
- en: NOTE For simplicity of notation, we usually use a single integral sign to denote
    multidimensional integrals. The vector sign in the domain (for example, ![](../../OEBPS/Images/AR_x.png)
    ∈ *D*), as well the vector sign in *d*![](../../OEBPS/Images/AR_x.png), indicates
    multiple dimensions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了符号的简洁性，我们通常用一个单独的积分符号来表示多维积分。定义域中的向量符号（例如，![](../../OEBPS/Images/AR_x.png)
    ∈ *D*），以及 *d*![](../../OEBPS/Images/AR_x.png) 中的向量符号，都表示多个维度。
- en: You may remember from elementary integral calculus that equation [5.6](#eq-continuous-prob-sum)
    corresponds to the area under the curve for *p*(*x*) (or *p*(![](../../OEBPS/Images/AR_x.png))).
    In higher dimensions, equation [5.7](#eq-continuous-vec-prob-sum) corresponds
    to the volume under the hypersurface for *p*(![](../../OEBPS/Images/AR_x.png)).
    Thus, *the total area under a univariate probability density curve is always 1*.
    And in higher dimensions, *the volume under the hypersurface for a multivariate
    probability density function is always 1*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得从初等积分微积分中，方程 [5.6](#eq-continuous-prob-sum) 对应于 *p*(*x*)（或 *p*(![](../../OEBPS/Images/AR_x.png)))
    曲线下的面积。在更高维的情况下，方程 [5.7](#eq-continuous-vec-prob-sum) 对应于 *p*(![](../../OEBPS/Images/AR_x.png))
    超曲面下的体积。因此，*单变量概率密度曲线下的总面积总是 1*。在更高维的情况下，*多变量概率密度函数的超曲面下的体积总是 1*。
- en: '5.7 Properties of distributions: Expected value, variance, and covariance'
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 分布的性质：期望值、方差和协方差
- en: Toward the beginning of this chapter, we stated that generative machine learning
    models are often developed by fitting a distribution from a known family to the
    available training data. Thus, we postulate a parameterized distribution from
    a known family and estimate the exact parameters that best fit the training data.
    Most distribution families are parameterized in terms of intuitive properties
    like the mean, variance, and so on. Understanding these concepts and their geometric
    significance is essential for understanding the models based on them.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们提到生成式机器学习模型通常是通过将一个已知家族的分布拟合到可用训练数据来开发的。因此，我们从已知家族中假设一个参数化分布，并估计最适合训练数据的精确参数。大多数分布家族都是用诸如均值、方差等直观属性来参数化的。理解这些概念及其几何意义对于理解基于它们的模型至关重要。
- en: In this section, we explain a few properties/parameters common to all distributions.
    Later, when we discuss individual distributions, we connect them to the parameters
    of those distributions. We also show how to programmatically obtain the values
    of these for each individual distribution via the PyTorch `distributions` package.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们解释了所有分布都通用的几个属性/参数。稍后，当我们讨论单个分布时，我们将它们与这些分布的参数联系起来。我们还展示了如何通过 PyTorch
    `distributions` 包编程地获取每个单个分布的这些值。
- en: 5.7.1 Expected value (aka mean)
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7.1 期望值（又称均值）
- en: 'If we sample a random variable with a given distribution many times and take
    the average of the sampled values, what value do we expect to end up with? The
    average will be closer to the values with higher probabilities (as these appear
    more often during sampling). If we sample enough times, for a given probability
    distribution, this average always settles down to a fixed value for that distribution:
    the *expected value* of the distribution.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们多次从给定的分布中抽取随机变量并取抽样值的平均值，我们期望得到什么值？平均值将更接近概率较高的值（因为这些在抽样中出现得更频繁）。如果我们抽样足够多次，对于给定的概率分布，这个平均值总是稳定到一个固定值，即该分布的*期望值*。
- en: Formally,
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，
- en: given a discrete distribution *D* where a discrete random variable *X* can take
    any value from the sets {*x*[1], *x*[2],⋯, *x[n]*} with respective probabilities
    {*p*(*x*[1]), *p*(*x*[2])⋯, *p*(*x[n]*)}, the expected value is given by the formula
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个离散分布 *D*，其中离散随机变量 *X* 可以从集合 {*x*[1], *x*[2],⋯, *x[n]*} 中取任何值，相应的概率为 {*p*(*x*[1]),
    *p*(*x*[2])⋯, *p*(*x[n]*)}，期望值由以下公式给出
- en: '![](../../OEBPS/Images/eq_05-08.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![质量中心或形心的概念](../../OEBPS/Images/eq_05-08.png)'
- en: Equation 5.8
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 5.8
- en: where *x[k]* → *D* denotes the *k*th sample drawn from the distribution *D*.
    Overall, equation [5.8](#eq-discrete-univar-expected-val) says that *the average
    or expected value of a very large number of samples drawn from the distribution
    approaches the probability-weighted sum of all possible sample values*. When we
    sample, the higher-probability values appear more frequently than the lower-probability
    values, so the average over a large number of samples is pulled closer to the
    higher-probability values.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x[k]* → *D* 表示从分布 *D* 中抽取的第 *k* 个样本。总体而言，方程 [5.8](#eq-discrete-univar-expected-val)
    表示“从分布中抽取的大量样本的平均值或期望值趋近于所有可能样本值的概率加权和”。当我们抽样时，高概率值比低概率值出现得更频繁，所以大量样本的平均值被拉向高概率值。
- en: 'For multivariate random variables:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多维随机变量：
- en: Given a discrete distribution where a discrete multidimensional random variable
    *X* can take any value from the sets {![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],⋯,
    ![](../../OEBPS/Images/AR_x.png)*[n]*} with respective probabilities {*p*(![](../../OEBPS/Images/AR_x.png)[1]),
    *p*(![](../../OEBPS/Images/AR_x.png)[2]),⋯, *p*(![](../../OEBPS/Images/AR_x.png)*[n]*)},
    the expected value is given by the formula
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个离散分布，其中离散的多维随机变量 *X* 可以从集合 {![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],⋯,
    ![](../../OEBPS/Images/AR_x.png)*[n]*} 中取任何值，相应的概率为 {*p*(![](../../OEBPS/Images/AR_x.png)[1]),
    *p*(![](../../OEBPS/Images/AR_x.png)[2]),⋯, *p*(![](../../OEBPS/Images/AR_x.png)*[n]*)}，期望值由以下公式给出
- en: '![](../../OEBPS/Images/eq_05-09.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![公式 5.9](../../OEBPS/Images/eq_05-09.png)'
- en: Equation 5.9
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 5.9
- en: 'For continuous random variables (note how the sum is replaced by an integral):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续随机变量（注意求和被替换为积分）：
- en: The expected value of a continuous random variable *X* that takes values from
    −∞ to ∞ (that is, *x* ∈ { − ∞, ∞}) is
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于取值从 −∞ 到 ∞ 的连续随机变量 *X* 的期望值是
- en: '![](../../OEBPS/Images/eq_05-10.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![随机变量函数的期望值](../../OEBPS/Images/eq_05-10.png)'
- en: Equation 5.10
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 5.10
- en: Expected value and center of mass in physics
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 物理学中的期望值和质心
- en: In physics, we have the concept of the center of mass or centroid. If we have
    a set of points, each with a mass, the entire point set can be replaced by a single
    point. This point is called the *centroid*. The position of the centroid is the
    weighted average of the positions of the individual points, weighted by their
    individual masses. If we mentally think of the probabilities of individual points
    as masses, the notion of expected value in statistics corresponds to the notion
    of centroid in physics.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理学中，我们有质量中心或形心的概念。如果我们有一组具有质量的点，整个点集可以被一个单独的点所代替。这个点被称为*形心*。形心的位置是各个点位置的加权平均值，权重为它们的各自质量。如果我们把单个点的概率视为质量，那么统计学中的期望值概念对应于物理学中的形心概念。
- en: Expected value of an arbitrary function of a random variable
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量任意函数的期望值
- en: So far, we have seen the expected value of the random variable itself. The notion
    can be extended to functions of the random variable.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了随机变量本身的期望值。这个概念可以扩展到随机变量的函数。
- en: The expected value of a function of a random variable is the probability-weighted
    sum of the values of that function at all possible values of the random variable.
    Formally,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量函数的期望值是该函数在随机变量所有可能值处的概率加权和。形式上，
- en: '![](../../OEBPS/Images/eq_05-11.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![连续随机变量期望值](../../OEBPS/Images/eq_05-11.png)'
- en: Equation 5.11
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.11
- en: Expected value and dot product
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值和点积
- en: In equation [2.6](02.xhtml#eq-dot-product), we looked at the dot product between
    two vectors. Further, in section [2.5.6.2](02.xhtml#subsubsec-dotproduct_as_agreement),
    we saw that the dot product between two vectors measures the agreement between
    the two vectors. If both point in the same direction, the dot product is larger.
    In this section, we show that the expected value of a function of a random variable
    can be viewed as a dot product between a vector representing the probability and
    another vector representing the function itself.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 [2.6](02.xhtml#eq-dot-product) 中，我们研究了两个向量之间的点积。进一步地，在第 [2.5.6.2](02.xhtml#subsubsec-dotproduct_as_agreement)
    节中，我们看到两个向量之间的点积衡量了这两个向量之间的协议。如果它们指向同一方向，点积就更大。在本节中，我们展示了随机变量的函数期望值可以看作是表示概率的向量和表示函数本身的向量的点积。
- en: First let’s consider the discrete case. Our random variable can take values
    *x[i]*, *i* ∈ {1, *n*}. Now, imagine a vector ![](../../OEBPS/Images/eq_05-11-a.png)
    and a vector ![](../../OEBPS/Images/eq_05-11-b.png). From equation [5.11](#eq-func-rv-expected-val),
    we see that the expected value of the function 𝔼(*f*(*X*)) of random variable
    *X* is the same as *![](../../OEBPS/Images/AR_f.png)^T![](../../OEBPS/Images/AR_p.png)*
    = *![](../../OEBPS/Images/AR_f.png)* ⋅ ![](../../OEBPS/Images/AR_p.png). This
    is high when ![](../../OEBPS/Images/AR_f.png) and ![](../../OEBPS/Images/AR_p.png)
    are aligned; thus, the expected value of the function of the random variable is
    high when the high function values coincide with high probabilities of the random
    variable and vice versa. In the continuous case, these vectors have an infinite
    number of components and the summation is replaced by an integral, but the idea
    stays the same.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑离散情况。我们的随机变量可以取值 *x[i]*，*i* ∈ {1, *n*}。现在，想象一个向量 ![](../../OEBPS/Images/eq_05-11-a.png)
    和一个向量 ![](../../OEBPS/Images/eq_05-11-b.png)。从方程 [5.11](#eq-func-rv-expected-val)
    中，我们看到随机变量 *X* 的函数期望值 𝔼(*f*(*X*)) 与 *![](../../OEBPS/Images/AR_f.png)^T![](../../OEBPS/Images/AR_p.png)*
    = *![](../../OEBPS/Images/AR_f.png)* ⋅ ![](../../OEBPS/Images/AR_p.png) 相同。当 ![](../../OEBPS/Images/AR_f.png)
    和 ![](../../OEBPS/Images/AR_p.png) 对齐时，这个值会很高；因此，当随机变量的高函数值与高概率相一致时，随机变量的函数期望值就很高，反之亦然。在连续情况下，这些向量有无限多个分量，求和被积分所取代，但基本思想保持不变。
- en: Expected value of linear combinations of random variables
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量线性组合的期望值
- en: The expected value is a linear operator. This means the expected value of a
    linear combination of random variables is a linear combination (with the same
    weights) of the expected values of the random variables. Formally,
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值是一个线性算子。这意味着随机变量线性组合的期望值是随机变量期望值的线性组合（具有相同的权重）。形式上，
- en: 𝔼(*α*[1]*X*[1] + *α*[2]*X*[2] ⋯ *α[n]X[n]*) = *α*[1]𝔼(*X*[1]) + *α*[2]𝔼(*X*[2])
    + ⋯ *α[n]*𝔼(*X[n]*)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 𝔼(*α*[1]*X*[1] + *α*[2]*X*[2] ⋯ *α[n]X[n]*) = *α*[1]𝔼(*X*[1]) + *α*[2]𝔼(*X*[2])
    + ⋯ *α[n]*𝔼(*X[n]*)
- en: Equation 5.12
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.12
- en: 5.7.2 Variance, covariance, and standard deviation
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7.2 方差、协方差和标准差
- en: When we draw a very large number of samples from a given point distribution,
    we often like to know the spread of the point set. The spread is not merely a
    matter of measuring the largest distance between two points in the distribution.
    Rather, we want to know how densely packed the points are. If most of the points
    fit within a very small ball, then even if one or two points are far from the
    ball, we call that a *small spread* or *high packing density*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从给定的点分布中抽取大量样本时，我们通常想知道点集的分布范围。分布范围不仅仅是测量分布中两点之间最大距离的问题。相反，我们想知道点是如何紧密排列的。如果大多数点都适合在一个非常小的球内，那么即使一个或两个点离球很远，我们也称之为
    *小分布* 或 *高包装密度*。
- en: 'Why is this important in machine learning? Let’s start with a few informal
    examples. If we discover that the points are tightly packed in a small region
    around a single point, we may want to replace the entire distribution with that
    point without causing much error. Or if the points are packed tightly around a
    single straight line, we can replace the entire distribution with that line. Doing
    so gives us a simpler lower-dimensional) representation and often leads to a view
    of the data that is more amenable to understanding the big picture. This is because
    small variations about a particular point or direction are usually caused by noise,
    while large variations are caused by meaningful things. By eliminating small variations
    and focusing on the large ones, we capture the main information content. (This
    could be why older people tend to be better at forming big-picture views: perhaps
    there are too few neurons in their heads to retain the huge amount of memory data
    they have accumulated over the years. Their brain performs dimensionality reduction.)
    This is the basic idea behind PCA and dimensionality reduction, which we saw in
    section [4.4](../Text/04.xhtml#sec-pca).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这在机器学习中为什么很重要？让我们从几个非正式的例子开始。如果我们发现点在单个点周围的一个小区域内紧密排列，我们可能想要用那个点替换整个分布，而不会造成很大的误差。或者如果点围绕一条直线紧密排列，我们可以用那条直线替换整个分布。这样做给我们提供了一个更简单的低维表示，并且通常会导致对数据的理解更加容易把握整体。这是因为围绕特定点或方向的微小变化通常是由噪声引起的，而大的变化是由有意义的事物引起的。通过消除小的变化并关注大的变化，我们捕捉到了主要的信息内容。（这可能是为什么老年人倾向于更好地形成整体观点的原因：也许他们头部的神经元太少，无法保留他们多年来积累的大量记忆数据。他们的大脑执行降维。）这是主成分分析（PCA）和降维的基本思想，我们在第[4.4](../Text/04.xhtml#sec-pca)节中看到了这一点。
- en: 'Variance—or its square root, standard deviation—measures how densely packed
    around the expected value the points in the distribution are: that is, the spread
    of the point distribution. Formally, the variance of a probability distribution
    is defined as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 方差——或者其平方根，标准差——衡量分布中的点围绕期望值的密集程度：即点分布的分散程度。形式上，概率分布的方差定义为如下：
- en: '![](../../OEBPS/Images/eq_05-13.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../../OEBPS/Images/eq_05-13.png)'
- en: Equation 5.13
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 方程5.13
- en: By comparing equation [5.13](../Text/05.xhtml#eq-variance) to equations [5.10](#eq-continuous-expected-val)
    and [5.11](#eq-func-rv-expected-val), we see that the variance is the expected
    value of the distance (*x* − *μ*)² of sample points *x* from the mean *μ*. So
    if the more probable (more frequently occurring) sample points lie within a short
    distance of the mean, the variance is small, and vice versa. That is to say, the
    variance measures how tightly packed the points are around the mean.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较方程[5.13](../Text/05.xhtml#eq-variance)与方程[5.10](#eq-continuous-expected-val)和[5.11](#eq-func-rv-expected-val)，我们看到方差是样本点*x*与均值*μ*的距离(*x*
    − *μ*)²的期望值。所以如果更可能的（更频繁出现的）样本点位于均值附近的一个短距离内，方差就小，反之亦然。也就是说，方差衡量了点围绕均值的密集程度。
- en: 'Covariance: Variance in higher dimensions'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差：高维度的方差
- en: Extending the notion of the expected value from the univariate case to the multivariate
    case was straightforward. In the univariate case, we take a probability-weighted
    average of a scalar quantity, *x*. The resulting expected value is a scalar, *μ*
    = ∫[*x* = −∞]^∞ *x* *p*(*x*)*dx*. In the multivariate case, we take the probability-weighted
    average of a vector quantity, ![](../../OEBPS/Images/AR_x.png). The resulting
    expected value is a vector, ![](../../OEBPS/Images/AR_micro.png) = ∫[![](../../OEBPS/Images/AR_x.png)∈*D*]
    ![](../../OEBPS/Images/AR_x.png) *p*(![](../../OEBPS/Images/AR_x.png))*d*![](../../OEBPS/Images/AR_x.png).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 将期望值的观念从单变量情况扩展到多变量情况是直接的。在单变量情况下，我们取一个标量量*ξ*的概率加权平均值。得到的期望值是一个标量，*μ* = ∫[*ξ*
    = −∞]^∞ *ξ* *p*(*ξ*)*d*ξ。在多变量情况下，我们取一个向量量![](../../OEBPS/Images/AR_x.png)的概率加权平均值。得到的期望值是一个向量，![](../../OEBPS/Images/AR_micro.png)
    = ∫[![](../../OEBPS/Images/AR_x.png)∈*D*] ![](../../OEBPS/Images/AR_x.png) *p*(![](../../OEBPS/Images/AR_x.png))*d*![](../../OEBPS/Images/AR_x.png).
- en: Extending the notion of variance to the multivariate case is not as straightforward.
    This is because we can traverse the multidimensional random vector’s domain (the
    space over which the vector is defined) in an infinite number of possible directions—think
    how many possible directions we can walk on a 2D plane—and the spread or packing
    density can be different for each direction. For instance, in figure [5.3b](#fig5_3),
    the spread along the main diagonal is much larger than the spread in a perpendicular
    direction.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将方差的观念扩展到多元情况并不简单。这是因为我们可以以无限多种可能的方向遍历多维随机向量的域（向量定义的空间）——想想在二维平面上我们可以走多少种可能的方向——并且每个方向上的分布或包装密度可能不同。例如，在图[5.3b](#fig5_3)中，沿主对角线的分布范围远大于垂直方向的分布范围。
- en: The covariance of a multidimensional point distribution is a matrix that allows
    us to easily measure the spread or packing density in any desired direction. It
    also allows us to easily figure out the direction in which the maximum spread
    occurs and what that spread is.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 多维点分布的协方差是一个矩阵，它使我们能够轻松地测量任何所需方向上的分布或包装密度。它还使我们能够轻松地找出最大分布发生方向及其分布情况。
- en: 'Consider a multivariate random variable *X* that takes vector values ![](../../OEBPS/Images/AR_x.png).
    Let *l̂* be an arbitrary direction (as always, we use overhead hats to denote
    unit-length vectors signifying directions) in which we want to measure the packing
    density of *X*. We discussed in sections [2.5.2](02.xhtml#subsec-dotprod-ml) and
    [2.5.6](02.xhtml#subsection-dot_product) that the dot product of ![](../../OEBPS/Images/AR_x.png)
    in the direction *l̂* (that is, ![](../../OEBPS/Images/AR_x.png)*^Tl̂*) is the
    projection or component (effective value) of *x* along *l̂*. Thus the spread or
    packing density of the random vector ![](../../OEBPS/Images/AR_x.png) in direction
    *l̂* is the same as the spread of the dot product (aka component or projection)
    *l̂^T*![](../../OEBPS/Images/AR_x.png). This projection *l̂^T*![](../../OEBPS/Images/AR_x.png)
    is a scalar quantity: we can use the univariate formula to measure its variance.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个取向量值的多元随机变量 *X*，其值为 ![](../../OEBPS/Images/AR_x.png)。设 *l̂* 为一个任意方向（如往常一样，我们使用顶部的帽子表示表示方向的单位长度向量）来测量
    *X* 的包装密度。我们在第[2.5.2](02.xhtml#subsec-dotprod-ml)节和[2.5.6](02.xhtml#subsection-dot_product)节中讨论了在方向
    *l̂* 上 ![](../../OEBPS/Images/AR_x.png) 的点积（即 ![](../../OEBPS/Images/AR_x.png)*^Tl̂*)
    是 *x* 沿 *l̂* 的投影或分量（有效值）。因此，随机向量 ![](../../OEBPS/Images/AR_x.png) 在方向 *l̂* 上的分布或包装密度与点积（也称为分量或投影）*l̂^T*![](../../OEBPS/Images/AR_x.png)
    的分布相同。这个投影 *l̂^T*![](../../OEBPS/Images/AR_x.png) 是一个标量量：我们可以使用单变量公式来测量其方差。
- en: NOTE In this context, we can use ![](../../OEBPS/Images/AR_x.png)*^Tl̂* and
    *l̂^T*![](../../OEBPS/Images/AR_x.png) interchangeably. The dot product is symmetric.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在此上下文中，我们可以将 ![](../../OEBPS/Images/AR_x.png)*^Tl̂* 和 *l̂^T*![](../../OEBPS/Images/AR_x.png)
    互换使用。点积是对称的。
- en: The expected value of the projection is
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 投影的期望值是
- en: '![](../../OEBPS/Images/eq_05-13-a.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-13-a.png)'
- en: The variance is given by
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 方差由
- en: '![](../../OEBPS/Images/eq_05-13-b.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-13-b.png)'
- en: 'Now, since the transpose of a scalar is the same scalar, we can write the square
    term within the integral as the product of the scalar *l̂^T*(![](../../OEBPS/Images/AR_x.png)
    - ![](../../OEBPS/Images/AR_micro.png)) and its transpose:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于标量的转置与标量本身相同，我们可以将积分内的平方项写成标量 *l̂^T*(![](../../OEBPS/Images/AR_x.png) -
    ![](../../OEBPS/Images/AR_micro.png)) 和其转置的乘积：
- en: '![](../../OEBPS/Images/eq_05-13-c.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-13-c.png)'
- en: Using equation [2.10](02.xhtml#eq-mat-prod-transpose),
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方程[2.10](02.xhtml#eq-mat-prod-transpose)，
- en: '![](../../OEBPS/Images/eq_05-13-d.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-13-d.png)'
- en: Since
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于
- en: '![](../../OEBPS/Images/eq_05-13-e.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-13-e.png)'
- en: where
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../../OEBPS/Images/eq_05-14.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-14.png)'
- en: Equation 5.14
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 方程5.14
- en: 'For simplicity, we drop the *X* in parentheses and simply write ℂ(*X*) as ℂ.
    An equivalent way of looking at the covariance matrix of a *d*-dimensional random
    variable *X* taking vector values ![](../../OEBPS/Images/AR_x.png) is as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们省略括号中的 *X*，并简单地写成 ℂ(*X*) 为 ℂ。将一个取向量值的 *d*-维随机变量 *X* 的协方差矩阵视为如下是等效的：
- en: '![](../../OEBPS/Images/eq_05-15.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-15.png)'
- en: Equation 5.15
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 方程5.15
- en: where
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../../OEBPS/Images/eq_05-15-a.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-15-a.png)'
- en: is the co-variance of the *i*th and *j*th dimensions of the random vector ![](../../OEBPS/Images/AR_x.png).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 是随机向量 ![](../../OEBPS/Images/AR_x.png) 的第 *i* 个和第 *j* 个维度的协方差。
- en: ℂ(*X*) or ℂ is the *covariance matrix* of the random variable *X*. A little
    thought reveals that equations [5.14](#eq-covariance) and [5.15](#eq-covar-eltwise)
    are equivalent.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ℂ(*X*) 或 ℂ 是随机变量 *X* 的 *协方差矩阵*。稍加思考就可以发现方程 [5.14](#eq-covariance) 和 [5.15](#eq-covar-eltwise)
    是等价的。
- en: 'The following things are noteworthy:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下事项值得关注：
- en: From equation [5.14](#eq-covariance), ℂ is the sum of the products of *d* ×
    1 vectors (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_micro.png))
    and their transpose (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_micro.png))*^T*,
    1 × *d* vectors. Hence, ℂ is a *d* × *d* matrix.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从方程 [5.14](#eq-covariance) 可以看出，ℂ 是由 *d* × 1 向量 (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_micro.png))
    和它们的转置 (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_micro.png))*^T*，1
    × *d* 向量的乘积之和。因此，ℂ 是一个 *d* × *d* 矩阵。
- en: This matrix is independent of the direction, *l̂*, in which we are measuring
    the variance or spread. We can precompute ℂ; then, when we need to measure the
    variance in any direction *l̂*, we can evaluate the quadratic form *l̂^T* ℂ*l̂*
    to obtain the variance in that direction. Thus ℂ is a generic property of the
    distribution, much like ![](../../OEBPS/Images/AR_micro.png). ℂ is called the
    *covariance* of the distribution.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个矩阵与我们在测量方差或扩散方向 *l̂* 无关。我们可以预先计算 ℂ；当我们需要测量任何方向 *l̂* 的方差时，我们可以评估二次型 *l̂^T*
    ℂ*l̂* 来获得该方向的方差。因此，ℂ 是分布的通用属性，就像 ![](../../OEBPS/Images/AR_micro.png)。ℂ 被称为分布的
    *协方差*。
- en: Covariance is the multivariate peer of the univariate entity variance.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协方差是多变量方差的单变量对应物。
- en: That covariance is the multivariate analog of variance is evident by comparing
    the expressions in equations [5.13](../Text/05.xhtml#eq-variance) and [5.14](#eq-covariance).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较方程 [5.13](../Text/05.xhtml#eq-variance) 和 [5.14](#eq-covariance) 的表达式，可以明显看出协方差是多变量方差的类比。
- en: Variance and expected value
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 方差和期望值
- en: 'As outlined previously, the variance is the expected value of the distance
    (*x* − *μ*)² of sample points *x* from the mean *μ*. This can be easily seen by
    comparing equations [5.13](../Text/05.xhtml#eq-variance), [5.10](#eq-continuous-expected-val),
    and [5.11](#eq-func-rv-expected-val) and leads to the following formula (where
    we use the principle of the expected value of linear combinations):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，方差是样本点 *x* 与均值 *μ* 的距离 (*x* − *μ*)² 的期望值。通过比较方程 [5.13](../Text/05.xhtml#eq-variance)，[5.10](#eq-continuous-expected-val)
    和 [5.11](#eq-func-rv-expected-val)，可以很容易地看出这一点，并导致以下公式（我们使用线性组合期望值原理）：
- en: '*var*(*X*) = 𝔼((*X* − *μ*)²) = 𝔼(*X*²) − 𝔼(2*μX*) + 𝔼(*μ*²)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*var*(*X*) = 𝔼((*X* − *μ*)²) = 𝔼(*X*²) − 𝔼(2*μX*) + 𝔼(*μ*²)'
- en: Since *μ* is a constant, we can take it out of the expected value (a special
    case of the principal of the expected value of linear combinations). Thus we get
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *μ* 是一个常数，我们可以将其从期望值中提取出来（这是线性组合期望值原理的一个特例）。因此，我们得到
- en: '*var*(*X*) = 𝔼(*X*²) − 2*μ*𝔼(*X*) + *μ*²𝔼(1)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*var*(*X*) = 𝔼(*X*²) − 2*μ*𝔼(*X*) + *μ*²𝔼(1)'
- en: But *μ* = 𝔼(*X*). Also, the expected value of a constant is that constant. So,
    𝔼(1) = 1.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 但 *μ* = 𝔼(*X*). 此外，常数的期望值就是该常数。因此，𝔼(1) = 1。
- en: Hence,
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '*var*(*X*) = 𝔼(*X*²) − 2*μ*² + *μ*²𝔼(1) = 𝔼(*X*²) − *μ*²'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*var*(*X*) = 𝔼(*X*²) − 2*μ*² + *μ*²𝔼(1) = 𝔼(*X*²) − *μ*²'
- en: or
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '*var*(*X*) = 𝔼(*X*²) − 𝔼(*X*)²'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '*var*(*X*) = 𝔼(*X*²) − 𝔼(*X*)²'
- en: Equation 5.16
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.16
- en: 5.8 Sampling from a distribution
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 从分布中进行抽样
- en: Drawing a sample from the probability distribution of a random variable yields
    an arbitrary value from the set of possible values. If we draw many samples, the
    higher-probability values show up more often than lower-probability values. The
    sampled points form a cloud in the domain of possible values, and the region where
    the probabilities are higher is more densely populated than lower-probability
    regions. In other words, in a sample point cloud, higher-probability values are
    overrepresented. Thus a collection of sample points is often referred to as a
    *sample point cloud*. The hope, of course, is that the sample point cloud is a
    good representation of the entire population so that analyzing the points in the
    cloud will yield insights about the entire population. In univariate cases, the
    sample value is a scalar and represented by a point on the number line. In multivariate
    cases, the sample value is a vector and represented as a point in a higher-dimensional
    space.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从随机变量的概率分布中抽取样本得到的是可能值集合中的一个任意值。如果我们抽取许多样本，高概率值出现的次数会比低概率值多。抽取的点在可能值的域中形成一个云，概率较高的区域比低概率区域更密集。换句话说，在样本点云中，高概率值被过度表示。因此，一组样本点通常被称为*样本点云*。当然，我们希望样本点云是整个群体的良好表示，这样分析云中的点就能对整个群体有洞察。在单变量情况下，样本值是一个标量，在数轴上表示为一个点。在多变量情况下，样本值是一个向量，在更高维度的空间中表示为一个点。
- en: It is often useful to compute aggregate statistics (such as the mean and variance)
    to describe the population. If we know a distribution, we can use closed-form
    expressions to obtain these properties. Many standard distributions and closed-form
    equations for obtaining their means and variance are discussed in section [5.9](#sec-famous-distr).
    But often, we don’t know the underlying distribution. Under those circumstances,
    the sample mean and sample variance can be used. Given a set of *n* samples *X*
    = ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2]⋯![](../../OEBPS/Images/AR_x.png)*[n]*
    from any distribution, the sample mean and variance are computed as
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 计算汇总统计量（如均值和方差）通常很有用，以描述群体。如果我们知道一个分布，我们可以使用闭式表达式来获得这些属性。许多标准分布及其均值和方差的闭式方程在[5.9](#sec-famous-distr)节中讨论。但通常，我们不知道潜在的分布。在这种情况下，可以使用样本均值和样本方差。给定一组*n*个样本*X*
    = ![样本X](../../OEBPS/Images/AR_x.png)[1]，![样本X](../../OEBPS/Images/AR_x.png)[2]⋯![样本X](../../OEBPS/Images/AR_x.png)*[n]*来自任何分布，样本均值和方差的计算如下
- en: '![](../../OEBPS/Images/eq_05-16-a.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../../OEBPS/Images/eq_05-16-a.png)'
- en: In some situations, like Gaussian distributions (which we discuss shortly),
    it can be theoretically proved that the sample mean and variance are optimal (the
    best possible guesses of the true mean and variance, given the sampled data).
    Also, the sample mean approaches the true mean as the number of samples increases,
    and with enough samples, we get a pretty good approximation of the true mean.
    In the next subsection, we learn more about how much is “enough.”
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，例如高斯分布（我们将在稍后讨论），可以从理论上证明样本均值和方差是最佳的（给定样本数据对真实均值和方差的最好猜测）。此外，随着样本数量的增加，样本均值接近真实均值，并且随着样本数量的增加，我们得到对真实均值的良好近似。在下一小节中，我们将了解更多关于“足够”是多少的信息。
- en: 'Law of large numbers: How many samples are enough?'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 大数定律：需要多少样本才算足够？
- en: Informally speaking, the law of large numbers says that if we draw a large number
    of sample values from a probability distribution, their average should be close
    to the expected value of the distribution. In the limit, the average over an infinite
    number of samples will match the mean.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式地说，大数定律表明，如果我们从一个概率分布中抽取大量样本值，它们的平均值应该接近分布的期望值。在极限情况下，无限多个样本的平均值将匹配均值。
- en: In practice, we cannot draw an infinite number of samples, so there is no guarantee
    that the sample mean will coincide with the expected value (true mean) in real-life
    sampling. But if the number of samples is large, they will not be too different.
    This is not a matter of mere theory. Casinos design games where the probability
    of the house winning a bet against the guest is slightly higher than the probability
    of the guest winning. The expected value of the outcome is that the casino wins
    rather than the guest. Over the very large number of bets placed in a casino,
    this is exactly what happens—and that is why casinos make money on the whole,
    even though they may lose individual bets.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们无法抽取无限数量的样本，因此不能保证样本均值会与期望值（真实均值）在现实生活中的抽样中一致。但如果样本数量很大，它们不会相差太远。这不仅仅是一个理论问题。赌场设计游戏，其中赌场赢得赌注的概率略高于客人赢得赌注的概率。结果的期望值是赌场赢而不是客人。在赌场进行的非常大量的赌注中，这正是发生的事情——这也是为什么赌场总体上赚钱，尽管他们可能会输掉个别赌注。
- en: 'How many samples is “a large number of samples?” Well, it is not defined precisely.
    But one thing is known: if the variance is larger, more samples need to be drawn
    to make the law of large numbers apply.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: “大量样本”是多少个样本？这并没有一个精确的定义。但有一点是明确的：如果方差较大，就需要抽取更多的样本来使大数定律适用。
- en: 'Let’s illustrate this with an example. Consider a betting game. Suppose that
    the famous soccer club FC Barcelona, for unknown reasons, has agreed to play a
    very large number of matches against the Machine Learning Experts’ Soccer Club
    of Silicon Valley. We can place a bet of $100 on a team. If that team wins, we
    get back $200: that is, we make $100\. If that team loses, we lose the bet: that
    is, we make –$100\. The betting game is happening in a country where nobody knows
    anything about the reputations of these clubs. A bettor bets on FC Barcelona in
    the first game and wins $100\. Based on this one observation, can the bettor say
    that by betting on Barcelona, they expect to win $100 every time? Obviously not.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明这一点。考虑一个赌注游戏。假设著名的足球俱乐部巴塞罗那，出于未知的原因，同意与硅谷的机器学习专家足球俱乐部进行大量比赛。我们可以对一支球队下注100美元。如果该队获胜，我们将收回200美元：也就是说，我们赚了100美元。如果该队输掉比赛，我们将输掉赌注：也就是说，我们亏了100美元。这个赌注游戏发生在一个没有人了解这些俱乐部声誉的国家。一个赌徒在第一场比赛中下注巴塞罗那并赢得了100美元。基于这一观察，赌徒能否说通过下注巴塞罗那，他们每次都期望赢得100美元？显然不能。
- en: But suppose the bettor places 100 bets and wins $100 99 times and loses $100
    once. Now the bettor can expect with some confidence that they will win $100 (or
    close to it) by betting on Barcelona. Based on these observations, the sample
    mean winnings from a bet on FC Barcelona are 0.99 × (100) + 0.01 × (−100) = 98.
    The sample standard deviation is √(.99 × (98 - 100)² + 0.01 × (98 - (-100))²)
    = 19.8997. Relative to the sample mean, the sample standard deviation is 19.8997/98
    = 0.203.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设赌徒下了100次赌注，赢了99次100美元，输了1次100美元。现在赌徒可以有些信心地预期，通过下注巴塞罗那，他们将赢得100美元（或接近这个数字）。基于这些观察，对巴塞罗那投注的样本平均收益为0.99
    × (100) + 0.01 × (−100) = 98。样本标准差为√(.99 × (98 - 100)² + 0.01 × (98 - (-100))²)
    = 19.8997。相对于样本均值，样本标准差为19.8997/98 = 0.203。
- en: Next, consider the same game, except now FC Barcelona is playing the Real Madrid
    football club. Since the two teams are evenly matched (the theoretical win probability
    of Barcelona is 0.5), the results are no longer one-sided. Suppose that after
    100 games, FC Barcelona has won 60 times and Real Madrid has won 40 times. The
    sample mean winnings on a Barcelona bet are 0.6 × (100) + 0.4 × (−100) = 20. The
    sample standard deviation is √(.6 × (20 - 100)² + 0.4 × (20 - (-100))²) = 97.9795.
    Relative to the sample mean, the sample standard deviation is 97.9795/20 = 4.89897.
    This is a much larger number than the previous 0.203. In this case, even after
    100 trials, a bettor cannot be very confident in predicting that the expected
    win is the sample mean, $20.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑同样的比赛，但现在巴塞罗那足球俱乐部正在对阵皇家马德里足球俱乐部。由于两队实力相当（巴塞罗那的理论胜率为0.5），结果不再是单方面的。假设经过100场比赛，巴塞罗那赢得了60场，皇家马德里赢得了40场。对巴塞罗那投注的样本平均收益为0.6
    × (100) + 0.4 × (−100) = 20。样本标准差为√(.6 × (20 - 100)² + 0.4 × (20 - (-100))²) =
    97.9795。相对于样本均值，样本标准差为97.9795/20 = 4.89897。这个数字比之前的0.203大得多。在这种情况下，即使经过100次试验，赌徒也不能非常自信地预测预期的胜利是样本均值，即20。
- en: 'The overall intuition is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 整体直觉如下：
- en: If we take a sufficiently large number of samples, their average is close to
    the expected value. The exact definition of what constitutes a “sufficiently large”
    number of samples is not known. However, the larger the variance (relative to
    the mean), the more samples are needed.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取足够大的样本数量，它们的平均值将接近期望值。构成“足够大的”样本数量的确切定义尚不清楚。然而，方差（相对于均值）越大，所需的样本数量就越多。
- en: 5.9 Some famous probability distributions
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.9 一些著名的概率分布
- en: 'In this section, we introduce some probability distributions and density functions
    often used in deep learning. We will use PyTorch code snippets to demonstrate
    how to set up, sample, and compute properties like expected values, variance/covariance,
    and so on for each distribution. Note the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一些在深度学习中经常使用的概率分布和密度函数。我们将使用 PyTorch 代码片段来演示如何为每个分布设置、采样以及计算期望值、方差/协方差等属性。请注意以下内容：
- en: In the code snippets, for every distribution, we evaluate the probability using
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码片段中，对于每个分布，我们使用
- en: A PyTorch `distributions` function call
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch `distributions` 函数调用
- en: A raw evaluation from the formula (to understand the math)
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从公式中直接评估（为了理解数学）
- en: Both should yield the same result. In practice, you should use the PyTorch `distributions`
    function call instead of the raw formula.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这两者应该得到相同的结果。在实际应用中，你应该使用 PyTorch 的 `distributions` 函数调用而不是原始公式。
- en: In the code snippets, for every distribution,
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码片段中，对于每个分布，
- en: We evaluate the theoretical mean and variance using a PyTorch `distributions`
    function call.
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 PyTorch 的 `distributions` 函数调用评估理论均值和方差。
- en: We evaluate the sample mean and variance.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们评估样本均值和方差。
- en: When the sample set is large enough, the sample mean and theoretical mean should
    be close. Ditto for variance.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当样本集足够大时，样本均值和理论均值应该很接近。方差也是如此。
- en: NOTE Fully functional code for these distributions, executable via Jupyter Notebook,
    can be found at [http://mng.bz/8NVg](http://mng.bz/8NVg).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些分布的完整功能代码，可通过 Jupyter Notebook 执行，可以在 [http://mng.bz/8NVg](http://mng.bz/8NVg)
    找到。
- en: 'Another point to remember: In machine learning, we often work with the logarithm
    of the probability. Since the popular distributions are exponential, this leads
    to simpler computations. With that, let’s dive into the probability distributions.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要记住的是：在机器学习中，我们经常处理概率的对数。由于流行的分布是指数分布，这导致计算更加简单。有了这个，让我们深入了解概率分布。
- en: 5.9.1 Uniform random distributions
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.9.1 均匀随机分布
- en: Consider a continuous random variable *x* that can take any value from a fixed
    compact range, say [*a*, *b*], *with equal probability, while the probability
    of x taking a value outside the range is zero*. The corresponding *p*(*x*) is
    a uniform probability distribution. Formally stated,
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个连续随机变量 *x*，它可以取一个固定紧凑范围内的任何值，例如 [*a*，*b*]，*以相等的概率，而 x 取值在范围之外的几率是零*。相应的
    *p*(*x*) 是一个均匀概率分布。形式上表述为，
- en: '![](../../OEBPS/Images/eq_05-17.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-17.png)'
- en: Equation 5.17
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 5.17
- en: Equation [5.17](#eq-uniform-random-univar) means *p*(*x*) is constant, 1/*b-a*,
    for *x* between *a* and *b* and zero for other values of *x*. Note how the value
    of the constant is cleverly chosen to make the total area under the curve 1. This
    equation is depicted graphically in figure [5.4](#fig-univar-uniform-distr), and
    listing 5.1 shows the PyTorch code for the log probability of a univariate uniform
    random distribution.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 [5.17](#eq-uniform-random-univar) 表示 *p*(*x*) 是常数，为 1/(b-a)，当 *x* 在 *a*
    和 *b* 之间时，对于其他 *x* 的值则为零。注意常数的选择是如何巧妙地使得曲线下的总面积为 1。此方程在图 [5.4](#fig-univar-uniform-distr)
    中以图形方式表示，列表 5.1 展示了 PyTorch 代码用于计算单变量均匀随机分布的对数概率。
- en: '![](../../OEBPS/Images/CH05_F04_Chaudhury.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F04_Chaudhury.png)'
- en: Figure 5.4 Univariate (single-variable) uniform random probability density function.
    Probability *p*(*x*) is constant, 0.05, in the interval [−10,10] and zero everywhere
    outside the interval. Thus it depicts equation [5.17](#eq-uniform-random-univar)
    with *b* = 10, *a* = −10. The area under the curve is the area of the shaded rectangle
    of width 20 and height 0.05, 20 × 0.05 = 1. The thin rectangle depicts an infinitesimally
    small interval corresponding to event *E* = {*x* ≤ *X* < *x* + *δx*}. If we draw
    a random sample *x* from this distribution, the probability that the value of
    the sample is between, say, 4 and 4 + *δx*, with *δx* → 0, is *p*(4) = 0.05. The
    probability that the value of the sample is between, say, 15 and 15 + *δx*, with
    *δx* → 0, is *p*(15) = 0.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 单变量（单变量）均匀随机概率密度函数。概率 *p*(*x*) 在区间 [−10,10] 内是常数，0.05，在其他所有地方都是零。因此，它描绘了方程
    [5.17](#eq-uniform-random-univar) 中 *b* = 10，*a* = −10 的情况。曲线下的面积是宽度为 20、高度为 0.05
    的阴影矩形的面积，20 × 0.05 = 1。细长的矩形表示对应于事件 *E* = {*x* ≤ *X* < *x* + *δx*} 的无穷小区间。如果我们从这个分布中抽取一个随机样本
    *x*，那么样本值在 4 和 4 + *δx* 之间的概率，当 *δx* → 0 时，是 *p*(4) = 0.05。样本值在 15 和 15 + *δx*
    之间的概率，当 *δx* → 0 时，是 *p*(15) = 0。
- en: Listing 5.1 Log probability of a univariate uniform random distribution
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 单变量均匀随机分布的对数概率
- en: '[PRE0]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Imports a PyTorch uniform distribution
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 PyTorch 均匀分布
- en: ② Sets the distribution parameters
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置分布参数
- en: ③ Instantiates a uniform distribution object
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化一个均匀分布对象
- en: ④ Instantiates a single-point test dataset
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化一个单点测试数据集
- en: ⑤ Evaluates the probability using PyTorch
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用 PyTorch 评估概率
- en: ⑥ Evaluates the probability using the formula
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用公式评估概率
- en: ⑦ Asserts that the probabilities match
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 断言概率匹配
- en: NOTE Fully functional code for the uniform distribution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/E2Jr](http://mng.bz/E2Jr).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：完全功能的均匀分布代码，可通过 Jupyter Notebook 执行，可在[http://mng.bz/E2Jr](http://mng.bz/E2Jr)找到。
- en: Expected value of a uniform distribution
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀分布的期望值
- en: We do this for the univariate case, although the computations can be easily
    extended to the multivariate case. Substituting the probability density function
    from equation [5.17](#eq-uniform-random-univar) into the expression for the expected
    value for a continuous variable, equation [5.10](#eq-continuous-expected-val),
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是为了单变量情况，尽管计算可以很容易地扩展到多变量情况。将方程 [5.17](#eq-uniform-random-univar) 中的概率密度函数代入连续变量的期望值表达式，方程
    [5.10](#eq-continuous-expected-val)，
- en: '![](../../OEBPS/Images/eq_05-18.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-18.png)'
- en: Equation 5.18
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.18
- en: NOTE The limits of integration changed because *p*(*x*) is zero outside the
    interval [*a*, *b*].
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：积分的极限改变了，因为 *p*(*x*) 在区间 [*a*, *b*] 外部为零。
- en: Overall, equation [5.18](#eq-continuous-uniform-expected-val) agrees with our
    intuition. The expected value is right in the middle of the uniform interval,
    as shown in figure [5.5](#fig-univar-uniform-distr-with-expectedvalue).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，方程 [5.18](#eq-continuous-uniform-expected-val) 与我们的直觉相符。期望值正好位于均匀区间的中间，如图
    [5.5](#fig-univar-uniform-distr-with-expectedvalue) 所示。
- en: Variance of a uniform distribution
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀分布的方差
- en: 'If we look at figure [5.5](#fig-univar-uniform-distr-with-expectedvalue), it
    is intuitively obvious that the packing density of the samples is related to the
    width of the rectangle. The smaller the width, the tighter the packing and the
    smaller the variance, and vice versa. Let’s see if the math supports that intuition:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看图 [5.5](#fig-univar-uniform-distr-with-expectedvalue)，直观上很明显，样本的填充密度与矩形的宽度有关。宽度越小，填充越紧密，方差越小，反之亦然。让我们看看数学是否支持这种直觉：
- en: '![](../../OEBPS/Images/eq_05-19.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-19.png)'
- en: Equation 5.19
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.19
- en: '![](../../OEBPS/Images/CH05_F05_Chaudhury.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F05_Chaudhury.png)'
- en: Figure 5.5 Univariate (single-variable) uniform random probability density function.
    The solid line in the middle indicates the expected value. Interactive visualizations
    (where ou can change the parameters and observe how the graph changes as a result)
    can be found at [http://mng.bz/E2Jr](http://mng.bz/E2Jr).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 单变量（单变量）均匀随机概率密度函数。中间的实线表示期望值。交互式可视化（您可以更改参数并观察图形如何随之变化）可在[http://mng.bz/E2Jr](http://mng.bz/E2Jr)找到。
- en: 'Figure [5.5](#fig-univar-uniform-distr-with-expectedvalue) shows that the variance
    in equation [5.19](#eq-uniform-var) is proportional to the square of the width
    of the rectangle: that is, (*b* − *a*)².'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5.5](#fig-univar-uniform-distr-with-expectedvalue) 显示，方程 [5.19](#eq-uniform-var)
    中的方差与矩形的宽度平方成正比：即，(*b* − *a*)²。
- en: Here is the PyTorch code for the mean and variance of a uniform random distribution.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 PyTorch 代码，用于计算均匀随机分布的均值和方差。
- en: Listing 5.2 Mean and variance of a uniform random distribution
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 均匀随机分布的均值和方差
- en: '[PRE1]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Number of sample points
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ① 样本点数量
- en: ② 100000 × 1
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ② 100000 × 1
- en: ③ Obtains samples from ufm_dist instantiated in listing [5.1](#lst-ufm-dist-logprob)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从在列表 [5.1](#lst-ufm-dist-logprob) 中实例化的 ufm_dist 获取样本
- en: ④ Sample mean
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 样本均值
- en: ⑤ Mean via PyTorch function
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 通过 PyTorch 函数计算均值
- en: ⑥ Sample variance
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 样本方差
- en: ⑦ Variance via PyTorch function
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 通过 PyTorch 函数计算方差
- en: Multivariate uniform distribution
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 多元均匀分布
- en: 'Uniform distributions also can be multivariate. In that case, the random variable
    is a vector, ![](../../OEBPS/Images/AR_x.png) not a single value, but a sequence
    of values). Its domain is a multidimensional volume instead of the *X*-axis, and
    the graph has more than two dimensions. For example, this is a two-variable uniform
    random distribution:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀分布也可以是多元的。在这种情况下，随机变量是一个向量，![](../../OEBPS/Images/AR_x.png) 而不是一个单一值，而是一系列值）。其域是一个多维体积，而不是
    *X* 轴，并且图形具有超过两个维度。例如，这是一个双变量均匀随机分布：
- en: '![](../../OEBPS/Images/eq_05-20.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-20.png)'
- en: Equation 5.20
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.20
- en: 'Here, (*x*, ![](../../OEBPS/Images/AR_y.png)) ∈ [*a*[1], *b*[1]] × [*a*[2],
    *b*[2]] indicates a rectangular domain on the two-dimensional *XY* plane where
    *x* lies between *a*[1] and *b*[1] and ![](../../OEBPS/Images/AR_y.png) lies between
    *a*[2] and *b*[2]. Equation [5.20](#eq-uniform-random-bivar) is shown graphically
    in figure [5.6](#fig-bivar-uniform-distr). In the general multidimensional case,:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，(*x*, ![](../../OEBPS/Images/AR_y.png)) ∈ [*a*[1], *b*[1]] × [*a*[2], *b*[2]]
    表示在二维 *XY* 平面上的一个矩形区域，其中 *x* 在 *a*[1] 和 *b*[1] 之间，而 ![](../../OEBPS/Images/AR_y.png)
    在 *a*[2] 和 *b*[2] 之间。方程 [5.20](#eq-uniform-random-bivar) 在图 [5.6](#fig-bivar-uniform-distr)
    中以图形方式展示。在一般的多维情况下，：
- en: '![](../../OEBPS/Images/eq_05-21.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-21.png)'
- en: Equation 5.21
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5.21
- en: '![](../../OEBPS/Images/CH05_F06_Chaudhury.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F06_Chaudhury.png)'
- en: Figure 5.6 Bivariate uniform random probability density The probability *p*(*x*,
    *y*) is constant, 0.0025, in the domain (*x*, *y*) ∈ [−10,10] × [−10,10] and zero
    everywhere outside the interval. The volume of the box of width 20 × 20 and height
    0.0025, 20 * 20 * 0.0025 = 1.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 双变量均匀随机概率密度 在域 (*x*, *y*) ∈ [−10,10] × [−10,10] 内，概率 *p*(*x*, *y*) 是常数，0.0025，而在区间之外的所有地方都是零。宽度为
    20 × 20、高度为 0.0025 的盒子的体积为 20 * 20 * 0.0025 = 1。
- en: 'Here, *V* is the volume of the hyperdimensional box with base *D*. Equation
    [5.21](#eq-uniform-random-multivar) means *p*(![](../../OEBPS/Images/AR_x.png))
    is constant for ![](../../OEBPS/Images/AR_x.png) in the domain *D* and zero for
    other values of *x*. When nonzero, it has a constant value, the inverse of the
    volume *V*: this makes the total volume under the density function 1.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*V* 是以 *D* 为底的超维盒子的体积。方程 [5.21](#eq-uniform-random-multivar) 表示 *p*(![](../../OEBPS/Images/AR_x.png))
    在域 *D* 内对 ![](../../OEBPS/Images/AR_x.png) 是常数，而对于其他 *x* 的值则是零。当不为零时，它有一个常数值，即体积
    *V* 的倒数：这使得密度函数下的总体积为 1。
- en: 5.9.2 Gaussian (normal) distribution
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.9.2 高斯（正态）分布
- en: 'This is probably the most famous distribution in the world. Let’s consider,
    one more time, the weights of adult residents of Statsville. If Statsville is
    anything like a real city, the likeliest weight is around 75 kg: the largest percentage
    of the population will weigh this much. Weights near this value (say 70 or 80
    kg) will also be quite likely, although slightly less likely than 75 kg. Weights
    further away from 75 kg are still less likely, and so on. The further we go from
    75 kg, the lower the percentage of the population with that weight. *Outlier*
    values like 40 and 110 kg are unlikely. Informally speaking, a Gaussian probability
    density function looks like a *bell-shaped curve*. The central value has the highest
    probability. The probability falls gradually as we move away from the center.
    In theory, however, it never disappears completely (the function *p*(*x*) never
    becomes equal to 0), although it becomes almost zero for all practical purposes.
    This behavior is described in mathematics as *asymptotically approaching zero*.
    Figure [5.7](#fig-univar-gaussian-distr) shows a Gaussian probability density
    function. Formally,'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是世界上最著名的分布。让我们再次考虑Statsville成年居民的体重。如果Statsville像真正的城市一样，最可能的体重是大约75公斤：最大比例的人口将会有这个体重。接近这个值（比如70或80公斤）的体重也很可能，尽管比75公斤的可能性略低。远离75公斤的体重仍然不太可能，以此类推。我们离75公斤越远，具有该体重的百分比人口就越低。像40和110公斤这样的*异常值*不太可能。非正式地说，高斯概率密度函数看起来像一条*钟形曲线*。中心值具有最高的概率。当我们远离中心时，概率逐渐下降。然而，在理论上，它永远不会完全消失（函数*p*(*x*)永远不会等于0），尽管对于所有实际目的来说，它几乎为零。这种行为在数学上被描述为*渐近趋于零*。图[5.7](#fig-univar-gaussian-distr)显示了高斯概率密度函数。正式地，
- en: '![](../../OEBPS/Images/eq_05-22.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-22.png)'
- en: Equation 5.22
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式5.22
- en: '![](../../OEBPS/Images/CH05_F07_Chaudhury.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH05_F07_Chaudhury.png)'
- en: 'Figure 5.7 Univariate Gaussian random probability density function, *μ* = 0
    and *σ* = 4. The bell-shaped curve is highest at the center and decreases more
    and more as we move away from the center, approaching zero asymptotically. The
    value *x* = 0 has the highest probability, corresponding to the center of the
    probability density function. Note that the curve is symmetric. Thus, for instance,
    the probability of a random sample being in the vicinity of −5 is the same as
    that of 5 (0.04): that is, *p*(−5) = *p*(5) = 0.04. An interactive visualization
    (where you can change the parameters and observe how the graph changes as a result)
    can be found at [http://mng.bz/NYJX](http://mng.bz/NYJX).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 一元高斯随机概率密度函数，*μ* = 0 和 *σ* = 4。钟形曲线在中心最高，当我们远离中心时，曲线越来越低，渐近接近零。*x* = 0的值具有最高的概率，对应于概率密度函数的中心。请注意，曲线是对称的。因此，例如，随机样本在-5附近的概率与5的概率相同（0.04）：即*p*（-5）=
    *p*（5）= 0.04。一个交互式可视化（您可以更改参数并观察图形如何因此变化）可以在[http://mng.bz/NYJX](http://mng.bz/NYJX)找到。
- en: Here, *μ* and *σ* are parameters; *μ* corresponds to the center (for example,
    in figure [5.7](#fig-univar-gaussian-distr), *μ* = 0). The parameter *σ* controls
    the width of the bell. A larger *σ* implies that *p*(*x*) falls more slowly as
    we move away from the center.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*μ*和*σ*是参数；*μ*对应于中心（例如，在图[5.7](#fig-univar-gaussian-distr)中，*μ* = 0）。参数*σ*控制钟形的宽度。较大的*σ*意味着当我们远离中心时，*p*(*x*)下降得更慢。
- en: 'The Gaussian (normal) probability density function is so popular that we have
    a special symbol for it: 𝒩(*x*, *μ*, *σ*²). It can be proved (but doing so is
    exceedingly tedious, so we will skip the proof here) that'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯（正态）概率密度函数非常流行，以至于我们有一个特殊的符号表示它：𝒩(*x*, *μ*, *σ*²)。可以证明（但这非常繁琐，所以我们在这里省略证明）：
- en: '![](../../OEBPS/Images/eq_05-22-a.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_05-22-a.png)'
- en: This establishes that 𝒩(*x*;*μ*, *σ*²) is a true probability (satisfying the
    sum rule in equation [5.7](#eq-continuous-vec-prob-sum)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这确立了𝒩(*x*;*μ*, *σ*²)是一个真正的概率（满足方程[5.7](#eq-continuous-vec-prob-sum)中的求和规则）。
- en: Listing 5.3 Log probability of a univariate normal distribution
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3 一元正态分布的对数概率
- en: '[PRE2]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Imports a PyTorch univariate normal distribution
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入PyTorch一元正态分布
- en: ② Sets the distribution params
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置分布参数
- en: ③ Instantiates a univariate normal distribution object
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化一个一元正态分布对象
- en: ④ Instantiates a single-point test dataset
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化一个单点测试数据集
- en: ⑤ Evaluates the probability using PyTorch
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用PyTorch评估概率
- en: ⑥ Evaluates the probability using the formula
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用公式评估概率
- en: ⑦ Asserts that the probabilities match
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 断言概率匹配
- en: NOTE Fully functional code for this normal distribution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/NYJX](http://mng.bz/NYJX).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此正态分布的完整功能代码，可通过 Jupyter Notebook 执行，可在 [http://mng.bz/NYJX](http://mng.bz/NYJX)
    找到。
- en: Multivariate Gaussian
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量高斯
- en: A Gaussian distribution can also be multivariate. Then the random variable *x*
    is a vector ![](../../OEBPS/Images/AR_x.png), as usual. The parameter *μ* also
    becomes a vector ![](../../OEBPS/Images/AR_micro.png), and the parameter *σ* becomes
    a matrix Σ. As in the univariate case, these parameters are related to the expected
    value and variance. The Gaussian multivariate probability distribution function
    is
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布也可以是多变量的。然后随机变量 *x* 是一个向量 ![](../../OEBPS/Images/AR_x.png)，如通常情况。参数 *μ*
    也变成一个向量 ![](../../OEBPS/Images/AR_micro.png)，参数 *σ* 变成矩阵 Σ。与单变量情况一样，这些参数与期望值和方差相关。高斯多变量概率分布函数是
- en: '![](../../OEBPS/Images/eq_05-23.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-23.png)'
- en: Equation 5.23
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 5.23
- en: Equation [5.23](../Text/05.xhtml#eq-multivar-normal) describes the probability
    density function for the random vector ![](../../OEBPS/Images/AR_x.png) to lie
    within the infinitesimally small volume with dimensions *δ*![](../../OEBPS/Images/AR_x.png)
    around the point ![](../../OEBPS/Images/AR_x.png). (Imagine a tiny box (cuboid)
    whose sides are successive elements of *δ*![](../../OEBPS/Images/AR_x.png), with
    the top-left corner of the box at ![](../../OEBPS/Images/AR_x.png).) The vector
    ![](../../OEBPS/Images/AR_micro.png) and the matrix Σ are parameters. As in the
    univariate case, ![](../../OEBPS/Images/AR_micro.png) corresponds to the most
    likely value of the random vector. Figure [5.8](#fig-multivar-gaussian-distr)
    shows the Gaussian normal) distribution with two variables in three dimensions.
    The shape of the base of the bell is controlled by the parameter Σ.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [5.23](../Text/05.xhtml#eq-multivar-normal) 描述了随机向量 ![](../../OEBPS/Images/AR_x.png)
    落在以点 ![](../../OEBPS/Images/AR_x.png) 为中心、尺寸为 *δ*![](../../OEBPS/Images/AR_x.png)
    的无穷小体积内的概率密度函数。（想象一个边长为 *δ*![](../../OEBPS/Images/AR_x.png) 的微小长方体，其顶点在 ![](../../OEBPS/Images/AR_x.png)。）向量
    ![](../../OEBPS/Images/AR_micro.png) 和矩阵 Σ 是参数。与单变量情况一样，![](../../OEBPS/Images/AR_micro.png)
    对应于随机向量的最可能值。图 [5.8](#fig-multivar-gaussian-distr) 展示了三维空间中两个变量的高斯正态分布）的形状由参数
    Σ 控制。
- en: Listing 5.4 Log probability of a multivariate normal distribution
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 多变量正态分布的对数概率
- en: '[PRE3]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Imports a PyTorch multivariate normal distribution
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 PyTorch 多变量正态分布
- en: ② Sets the distribution params
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置分布参数
- en: ③ Instantiates a multivariate normal distribution object
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 实例化一个多变量正态分布对象
- en: ④ Instantiates a single point test dataset
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 实例化一个单点测试数据集
- en: ⑤ Evaluates the probability using PyTorch
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用 PyTorch 评估概率
- en: ⑥ Evaluates the probability using the formula
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用公式评估概率
- en: ⑦ Asserts that the probabilities match
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 断言概率匹配
- en: '![](../../OEBPS/Images/CH05_F08_Chaudhury.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH05_F08_Chaudhury.jpg)'
- en: 'Figure 5.8 Bivariate Gaussian random probability density function. It is a
    bell-shaped surface: highest at the center and decreasing as we move away from
    the center, approaching zero asymptotically. *x* = 0, ![](../../OEBPS/Images/AR_y.png)
    = 0 has the highest probability, corresponding to the center of the probability
    density function. The bell has a circular base, and the Σ matrix is a scalar multiple
    of the identity matrix 𝕀. An interactive visualization (where you can change the
    parameters and observe how the graph changes as a result) can be found at [http://mng.bz/NYJX](http://mng.bz/NYJX).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 双变量高斯随机概率密度函数。它是一个钟形表面：在中心最高，随着我们远离中心而降低，趋于零。*x* = 0，![](../../OEBPS/Images/AR_y.png)
    = 0 的概率最高，对应于概率密度函数的中心。钟形有一个圆形底部，Σ 矩阵是单位矩阵 𝕀 的标量倍。一个交互式可视化（你可以更改参数并观察图形如何随之变化）可在
    [http://mng.bz/NYJX](http://mng.bz/NYJX) 找到。
- en: Expected value of a Gaussian distribution
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布的期望值
- en: Substituting the probability density function from equation [5.22](../Text/05.xhtml#eq-univar-normal)
    into the expression for the expected value of a continuous variable, equation
    [5.10](#eq-continuous-expected-val), we get
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 [5.22](../Text/05.xhtml#eq-univar-normal) 中的概率密度函数代入连续变量的期望值表达式，即方程 [5.10](#eq-continuous-expected-val)，我们得到
- en: '![](../../OEBPS/Images/eq_05-23-a.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-23-a.png)'
- en: Substituting ![](../../OEBPS/Images/eq_05-23-b.png)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 代入 ![](../../OEBPS/Images/eq_05-23-b.png)
- en: '![](../../OEBPS/Images/eq_05-23-c.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_05-23-c.png)'
- en: Substituting *u* = *y*² and using equation [5.6](#eq-continuous-prob-sum)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-23-d.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
- en: Note that the limits of the integral in the first term are identical. This is
    because *u* = *y*² →∞ whether *y* → ∞ or *y* → −∞. But an integral with the same
    lower and upper limits is zero. Thus the first term is zero. Hence,
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 𝔼*[gaussian]*(*X*) = *μ*
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.24
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, this makes perfect sense. The probability density ![](../../OEBPS/Images/eq_05-24-a2.png)
    peaks (maximizes) at *x* = *μ*. At this *x*, the exponent becomes zero, which
    makes the term ![](../../OEBPS/Images/eq_05-24-b2.png) attain its maximum possible
    value of 1. This is right in the middle of the bell, as shown in figure [5.10](#fig-univar-normal-distr-with-expectedvalue).
    And, of course, the expected value coincides with the middle value if the density
    is symmetric and peaks in the middle. Analogously, in the multivariate case, the
    Gaussian multidimensional random variable *X* that takes vector values ![](../../OEBPS/Images/AR_x.png)
    in the *d*-dimensional domain ℝ*^d* (that is, ![](../../OEBPS/Images/AR_x.png)
    ∈ ℝ*^d*) has an expected value
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 𝔼*[gaussian]*(*X*) = ![](../../OEBPS/Images/AR_micro.png)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.25
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F09_Chaudhury.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 Univariate (single-variable) normal (Gaussian) random probability
    density function, *μ* = 0 and *σ* = 4. The solid line in the middle indicates
    the expected value.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a Gaussian distribution
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The variance of the Gaussian distribution is obtained by substituting equation
    [5.22](../Text/05.xhtml#eq-univar-normal) in the integral form of equation [5.13](../Text/05.xhtml#eq-variance).
    The mathematical derivation is shown in the book’s appendix; here we only state
    the result.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance of a Gaussian distribution with probability density function ![](../../OEBPS/Images/eq_05-24-c.png)
    is *σ*², and the standard deviation is the square root of that (*σ*). This makes
    intuitive sense. *σ* appears in the denominator of a negative exponent in the
    expression for the probability density function ![](../../OEBPS/Images/eq_05-24-d.png).
    As such, *p*(*x*) is an increasing function of *σ*: that is, for a given *x* and
    *μ*, a larger *σ* implies a larger *p*(*x*). In other words, a larger *σ* implies
    that the probability decays more slowly as we move away from the center: a fatter
    bell curve, a bigger spread, and hence a larger variance. Figure [5.10](../Text/05.xhtml#fig-multi-univar-gauss)
    depicts this.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F10a_Chaudhury.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: (a) Different *μ*s but the same *σ*s.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F10b_Chaudhury.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
- en: (b) The same *μ*s but different *σ*s.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 Gaussian densities with varying *μ*s and *σ*s. Changing *μ* shifts
    the center of the curve. A larger *σ* (variance) implies a fatter bell ⇒ more
    spread. Note that fatter curves are smaller in height as the total area under
    the curve must be 1.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 Mean and variance of a univariate Gaussian
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Number of sample points
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: ② 100000 × 1
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: ③ Obtains samples from uvn_dist
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: instantiated in listing [5.3](#lst-univar-normal-dist-logprob)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sample mean
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Mean via PyTorch function
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Sample variance
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Variance via PyTorch function
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Covariance of a multivariate Gaussian distribution and geometry of the bell
    surface
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Comparing equation [5.22](../Text/05.xhtml#eq-univar-normal) for a univariate
    Gaussian probability density with equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    for a multivariate Gaussian probability density, we intuitively feel that the
    matrix Σ is the multivariate peer of the univariate variance *σ*². Indeed it is.
    Formally, for a multivariate Gaussian random variable with a probability distribution
    given in equation [5.23](../Text/05.xhtml#eq-multivar-normal), the covariance
    matrix is given by the equation
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: ℂ*[gaussian]*(*X*) = **Σ**
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.26
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: As shown in table [5.11](#fig-gaussian-multivar-pointclouds), Σ regulates the
    shape of the base of the bell-shaped probability density function.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to see that the exponent in equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    is a quadratic form (introduced in section [4.2](../Text/04.xhtml#sec-quadratic-form)).
    As such, it defines a hyper-ellipse, as shown in figure [5.11](#fig-gaussian-multivar-pointclouds)
    and section [2.17](02.xhtml#sec-hyper-ellipse). All the properties of quadratic
    forms and hyper-ellipses apply here.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Mean and variance of a multivariate normal distribution
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Number of sample points
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: ② 100000 × 1
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: ③ Obtains samples from uvn_dist
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: instantiated in listing [5.4](#lst-multivar-normal-dist-logprob)
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sample mean
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Mean via PyTorch function
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Sample variance
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Variance via PyTorch function
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the geometric properties of the Gaussian covariance matrix Σ.
    Consider a 2D version of equation [5.23](../Text/05.xhtml#eq-multivar-normal).
    We rewrite ![](../../OEBPS/Images/eq_05-26-a2.png) and ![](../../OEBPS/Images/eq_05-26-b2.png)—2D
    vectors both. Also ![](../../OEBPS/Images/eq_05-26-c2.png)—a 2 × 2 matrix. The
    probability density function from equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    becomes
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-27.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
- en: Equation 5.27
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: (Use what you learned in chapter [3](../Text/03.xhtml#chapter-intro-vec-mat)
    to satisfy yourself that equation [5.27](#eq-bivar-normal) is a 2D analog of equation
    [5.23](../Text/05.xhtml#eq-multivar-normal).)
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: If we plot the surface *p*(*x*, *y*) against (*x*, *y*), it looks like a bell
    in 3D space. The shape of the bell’s base, on the (*x*, *y*) plane, is governed
    by the 2 × 2 matrix Σ. In particular,
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: If Σ is a diagonal matrix with equal diagonal elements, the bell is symmetric
    in all directions, and its base is circular.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If Σ is a diagonal matrix with unequal diagonal elements, the base of the bell
    is elliptical. The axes of the ellipse are aligned with the coordinate axes.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a general Σ matrix, the base of the bell is elliptical. The axes of the
    ellipse are not necessarily aligned with the coordinate axes.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenvectors of Σ yield the axes of the elliptical base of the bell surface.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, if we sample the distribution from equation [5.27](#eq-bivar-normal), we
    get a set of points (*x*, *y*) on the base plane of the surface shown in figure
    [5.8](#fig-multivar-gaussian-distr). The taller the *z* coordinate (depicting
    *p*(*x*, *y*)) of the surface at a point (*x*, *y*), the greater its probability
    of being selected in the sampling. If we draw a large number of samples, the corresponding
    point cloud will look more or less like the base of the bell surface.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Figure [5.11](#fig-gaussian-multivar-pointclouds) shows various point clouds
    formed by sampling Gaussian distributions with different covariance matrices Σ.
    Compare it to figure [5.10](../Text/05.xhtml#fig-multi-univar-gauss).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometry of sampled point clouds: Covariance and direction of maximum or minimum
    spread'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that if a multivariate distribution has a covariance matrix ℂ,
    its variance (spread) in any specific direction *l̂* is *l̂^T* ℂ*l̂*. What is
    the direction of maximum spread?
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Asking this is the same as asking “What direction *l̂* maximizes the quadratic
    form *l̂^T* ℂ*l̂*?” In section [4.2](../Text/04.xhtml#sec-quadratic-form), we
    saw that a quadratic form like this is maximized or minimized when the direction
    *l̂* is aligned with the eigenvector corresponding to the maximum or minimum eigenvalue
    of the matrix ℂ. Thus, *the maximum spread of a distribution occurs along the
    eigenvector of the covariance matrix corresponding to its maximum eigenvalue*.
    This led to the PCA technique in section [4.4](../Text/04.xhtml#sec-pca).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discuss the covariance of the Gaussian distribution and geometry of
    the point cloud formed by sampling a multivariate Gaussian a large number of times.
    You may want to take a look at figure [5.11](#fig-gaussian-multivar-pointclouds),
    which shows various point clouds formed by sampling Gaussian distributions with
    different covariance matrices Σ.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '![(a)](../../OEBPS/Images/CH05_F11a_Chaudhury.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
- en: (a) ![](../../OEBPS/Images/fig_05-11_a.png)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '![(b)](../../OEBPS/Images/CH05_F11b_Chaudhury.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
- en: (b) ![](../../OEBPS/Images/fig_05-11_b.png)
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '![(c)](../../OEBPS/Images/CH05_F11c_Chaudhury.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: (c) ![](../../OEBPS/Images/fig_05-11_c.png)
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '![(d)](../../OEBPS/Images/CH05_F11d_Chaudhury.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
- en: (d) ![](../../OEBPS/Images/fig_05-11_d.png)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 Point clouds formed by sampling multivariate Gaussians with the
    same ![](../../OEBPS/Images/AR_micro.png) = [0,0]*^T* but different Σs. These
    point clouds correspond to the bases of the bell curves for multivariate Gaussian
    probability densities. All the point clouds except (a) may be replaced by a univariate
    Gaussian after rotation to align the coordinate axes with the eigenvectors of
    Σ (dimensionality reduction). See sections [4.4](../Text/04.xhtml#sec-pca), [4.5](../Text/04.xhtml#sec-svd),
    and [4.6](../Text/04.xhtml#sec-lsa) for details. Interactive contour plots for
    the base of the bell curve can be found at [http://mng.bz/NYJX](http://mng.bz/NYJX).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Gaussian point clouds and hyper-ellipses
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: The numerator of the exponential term in equation [5.23](../Text/05.xhtml#eq-multivar-normal),
    (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_micro.png))*^T***Σ**^(−1)(![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_micro.png)),
    is a quadratic form as we discussed in section [4.2](../Text/04.xhtml#sec-quadratic-form).
    It should also remind you of the hyper-ellipse we looked at in section [2.17](02.xhtml#sec-hyper-ellipse),
    equation [2.33](02.xhtml#eq-hyper-ellipse), and equation [4.1](../Text/04.xhtml#eq-hyper-ellipse-again).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Now consider the plot of *p*(![](../../OEBPS/Images/AR_x.png)) against ![](../../OEBPS/Images/AR_x.png).
    This is a hypersurface in *n* + 1-dimensional space, where the random variable
    ![](../../OEBPS/Images/AR_x.png) is *n*-dimensional. For instance, if the random
    Gaussian variable ![](../../OEBPS/Images/AR_x.png) is 2D, the (![](../../OEBPS/Images/AR_x.png),
    *p*(![](../../OEBPS/Images/AR_x.png))) plot in 3D is as shown in figure [5.8](#fig-multivar-gaussian-distr).
    It is a bell-shaped surface. The hyper-ellipse corresponding to the quadratic
    form in the numerator of the probability density function in equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    governs the shape and size of the base of this bell.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: If the matrix Σ is diagonal (with equal diagonal elements), the base is *circular*—this
    is the special case shown in figure [5.8](#fig-multivar-gaussian-distr). Otherwise,
    the base of the bell is elliptic. The eigenvectors of the covariance matrix Σ
    correspond to the directions of the axes of the elliptical base. The eigenvalues
    correspond to the lengths of the axes.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.3 Binomial distribution
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we have a database containing photos of people. Also, suppose we know
    that 20% of the photos contain a celebrity and the remaining 80% do not. If we
    randomly select three photos from this database, what is the probability that
    two of them contain a celebrity? This is the kind of problem the binomial distribution
    deals with.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: In a computer vision-centric machine learning setting, we would probably inspect
    the selected photos and try to predict whether they contained a celebrity. But
    for now, let’s restrict ourselves to the simpler task of blindly predicting the
    chances from aggregate statistics.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: If we select a single photo, the probability of it containing a celebrity is
    *π* = 0.2.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: NOTE This has nothing to do with the natural number *π* denoting the ratio of
    the circumference to the diameter of a circle. We are just reusing the symbol
    *π* following popular convention.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability of the photo not containing a celebrity is 1 − *π* = 0.8. From
    that, we can compute the probability of, say, the first two sampled photos containing
    a celebrity but the last one containing a non-celebrity: that is, the event {*S*,
    *S*, *F*} (where S denotes success in finding a celebrity and F denotes failure
    in finding a celebrity). Using equation [5.4](../Text/05.xhtml#eq-joint-prob-indep),
    the probability of the event {*S*, *S*, *F*} is *π* × *π* × (1−*π*) = 0.2 × 0.2
    × 0.8. However, many other combinations are also possible.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'All the possible combinations that can occur in three trials are shown in table
    [5.8](#tab-binomial-distr). In the table, event ids 3, 5, and 6 correspond to
    two successes and one failure. They occur with probabilities 0.8 × 0.2 × 0.2,
    0.2 × 0.8 × 0.2, and 0.2 × 0.2 × 0.8, respectively. If any one of them occurs,
    we have two celebrity photos in three trials. Thus, using equation [5.3](../Text/05.xhtml#eq-discrete-prob-sum),
    the overall probability of selecting two celebrity photos in three trials is the
    sum of these event probabilities: 0.8 × 0.2 × 0.2 + 0.2 × 0.8 × 0.2 + 0.2 × 0.2
    × 0.8 = 0.096.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.8 All possible combinations of three trials
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '| Event Id | Event | Probability |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| 0 | {*F*, *F*, *F*} | (1−*π*) × (1−*π*) × (1−*π*) = 0.8 × 0.8 × 0.8 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| 1 | {*F*, *F*, *S*} | (1−*π*) × (1−*π*) × *π* = 0.8 × 0.8 × 0.2 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| 2 | {*F*, *S*, *F*} | (1−*π*) × *π* × (1−*π*) = 0.8 × 0.2 × 0.8 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| 3 | {*F*, *S*, *S*} | (1−*π*) × *π* × *π* = 0.8 × 0.2 × 0.2 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| 4 | {*S*, *F*, *F*} | *π* × (1−*π*) × (1−*π*) = 0.2 × 0.8 × 0.8 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| 5 | {*S*, *F*, *S*} | *π* × (1−*π*) × *π* = 0.2 × 0.8 × 0.2 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '| 6 | {*S*, *S*, *F*} | *π* × *π* × (1−*π*) = 0.2 × 0.2 × 0.8 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: '| 7 | {*S*, *S*, *S*} | *π* × *π* × *π* = 0.2 × 0.2 × 0.2 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: 'In the general case, with more than three trials, it would be impossibly tedious
    to enumerate all the possible combinations of *success* and *failure* that can
    occur in a set of *n* trials. Fortunately, we can derive a formula. But before
    doing that, let’s state the task of a binomial distribution in more general terms:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Given a process that has a binary outcome (success or failure) in any given
    trial, and given that the probability of success in a trial is a known constant
    (say, *π*), a binomial distribution deals with the probability of observing *k*
    successes in *n* trials of the process.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Imagine events with *n* successive items, where each individual item can be
    either *S* or *F*. Table [5.8](#tab-binomial-distr) shows such events with *n*
    = 3. Each item has two possible values (*S* or *F*), and there are *n* items.
    Hence, altogether there can be 2 × 2 × ⋯2 = 2*^n* possible events.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: We are only interested in events with *k* occurrences of *S* (and therefore
    (*n* − *k*) occurrences of *F*). How many of the *n* events are like that? Well,
    asking this is the same as asking how many ways we can choose *k* slots from a
    total of *n* possible slots. Another way to pose the same question is, “How many
    different orderings of *n* items exist, where each item is either *S* or *F* and
    the total count of *S* is *k*?” The answer, from combination theory, is
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-27-a.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
- en: Each of these events has a probability of *π^k* × (1−*π*)^(*n* − *k*). Hence,
    the overall probability of *k* successes in *n* trials is ![](../../OEBPS/Images/eq_05-27-b-2.png).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Formally, if *X* is a random variable denoting the number of successes in *n*
    trials, with the probability of success in any single trial being some constant
    value *π*,
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-28.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
- en: Equation 5.28
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'What values can *k* take? Of course, we cannot have more than *n* successes
    in *n* trials; therefore, the maximum possible value of *k* is *n*. All integer
    values between 0 and *n* are possible:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-28-a.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
- en: The right-hand side is an expression for the generic term in the famous binomial
    expansion of (*a* + *b*)*^n* with *a* = *π* and *b* = 1 − *π*. Hence, we get
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-29.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
- en: Equation 5.29
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: This agrees with intuition, since given *n*, *k* can only take values 0, 1,
    ⋯, *n*; the sum of the probabilities on the left-hand side of equation [5.29](#eq-eq-binom-sum)
    corresponds to a certain event with probability 1.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, plugging *n* = 3, *k* = 2, and *π* = 0.2 into equation [5.28](#eq-binom-distr)
    yields 3!/2!1! (0.2)², (0.8)^(3-2) = 0.096: exactly what we get from explicit
    enumeration.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 Log probability of a binomial distribution
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Imports a PyTorch binomial distribution
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets the distribution params
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: ③ Instantiates a binomial distribution object
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: ④ Instantiates a single point test dataset
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Evaluates the probability using PyTorch
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Evaluates the probability using formula
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Asserts that the probabilities match
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the binomial distribution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/DRJ0](http://mng.bz/DRJ0).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a binomial distribution
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that the binomial distribution deals with a random variable *X*
    that depicts the number of successes in *n* trials, where the probability of success
    in a given trial is a constant *π* (again, this has nothing to do with the *π*
    denoting the ratio of the circumference to the diameter of a circle). This *X*
    can take any integer value 0 to *n*. Hence,
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-29-a.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
- en: We can drop the first term, which has the multiplier *k* = 0. Thus we get
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-29-b.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
- en: We can factor *n*! = *n*(*n* − 1)! and *π^k* = *π* *π*^(*k* − 1). Also, *n*
    − *k* = (*n* − 1) − (*k* − 1). This gives us
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-29-c.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
- en: Substituting *j* for *k* − 1 and *m* for *n* − 1, we get
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-30.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
- en: Equation 5.30
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: The quantity within the summation is similar to that in equation [5.29](#eq-eq-binom-sum)
    (should sum to 1). This leaves us with
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: 𝔼*[binomial]* (*X*) = *nπ*
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.31
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: Equation [5.31](#eq-binomial-expected-value) says that if *π* is the probability
    of success in a single trial, then the expected number of successes in *n* trials
    is *n**π*. For instance, if the probability of success in a single trial is 0.2,
    then the expected number of successes in 100 trials is 20—which is almost intuitively
    obvious.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a binomial distribution
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: The variance of a binomial random variable depicting the number of successes
    in *n* trials where the probability of success in a given trial is a constant
    *π* is
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '*var[binomial]* = *nπ* (1 − *π*)'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.32
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: The proof follows the same lines as that of the expected value.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 Mean and variance of a binomial distribution
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Number of sample points
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: ② 100000 × 1
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: ③ Obtains samples from ufm_dist instantiated in listing [5.7](#lst-binom-dist-logprob)
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sample mean
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Mean via PyTorch function
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Sample variance
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Variance via PyTorch function
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.4 Multinomial distribution
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider again the example problem we discussed in section [5.9.3](#sec-binomial-distr).
    We have a database of photos of people. But instead of two classes, celebrity
    and non-celebrity, we have four classes:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: 'Photos of Albert Einstein (class 1): 10% of the photos'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photos of Marie Curie (class 2): 42% of the photos'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photos of Carl Friedrich Gauss (class 3): 4% of the photos'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other photos (class 4): 44% of the photos'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we randomly select a photo from the database (that is, perform a random trial),
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: The probability of selecting class 1 (picking an Einstein photo) is *π*[1] =
    0.1.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 2 (picking a Marie Curie photo) is *π*[2]
    = 0.42.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 3 (picking a Gauss photo) is *π*[3] = 0.04.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 4 (picking a photo of none of the above)
    is *π*[4] = 0.44.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that *π*[1] + *π*[2] + *π*[3] + *π*[4] = 1. This is because the classes
    are mutually exclusive and exhaustive, so exactly one of these classes must occur
    in every trial.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 'Given all this, let’s ask the question: “What is the probability that in a
    set of 10 random trials, class 1 occurs 1 time, class 2 occurs 2 times, class
    3 occurs 1 time, and class 4 occurs the remaining 6 times?” This is the kind of
    problem multinomial distributions deal with.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Formally,
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: Let *C*[1], *C*[2], ⋯, *C[m]* be a set of *m* classes such that in any random
    trial, exactly one of these classes will be selected with the respective probabilities
    *π*[1], *π*[2], ⋯, *π[m]*.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let *X*[1], *X*[2], ⋯, *X[m]* be a set of random variables. *X[i]* corresponds
    to the number of occurrences of class *C[i]* in a set of *n* trials.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the multinomial probability function depicting the probability that class
    *C*[1] is selected *k*[1] times, class *C*[2] is selected *k*[2] times, and class
    *C*[3] is selected *k[m]* times is
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-33.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
- en: Equation 5.33
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-33-a.png)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
- en: We can verify that for *m* = 2, this becomes the binomial distribution (equation
    [5.28](#eq-binom-distr)). A noteworthy point is that if we look at any one of
    the *m* variables *X*[1], *X*[2], ⋯, *X[m]* individually, its distribution is
    binomial.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s work out the final probability for the example we started with: the probability
    that in a set of 10 random trials, class 1 occurs 1 time, class 2 occurs 2 times,
    class 3 occurs 1 time, and class 4 occurs the remaining 6 times. This is'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-33-b.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
- en: Listing 5.9 Log probability of a multinomial distribution
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Imports a PyTorch multinomial distribution
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets the distribution params
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: ③ Instantiates a multinomial dist object
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: ④ Instantiates a single-point test dataset
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Evaluates the probability using PyTorch
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Evaluates the probability using formula
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Asserts that the probabilities match
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the multinomial distribution, executable via
    Jupyter Notebook, can be found at [http://mng.bz/l1gz](http://mng.bz/l1gz).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a multinomial distribution
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: Each of the random variables *X*[1], *X*[2], ⋯, *X[m]* individually subscribes
    to a binomial distribution. Accordingly, following the binomial distribution expected
    value formula from equation [5.31](#eq-binomial-expected-value),
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 𝔼*[multinomial]*(*X[i]*) = *nπ[i]*
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.34
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a multinomial distribution
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: The variation of the random variables *X*[1], *X*[2], ⋯, *X[m]*, following the
    binomial distribution variance formula from equation [5.32](#eq-binomial-var),
    is
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '*var[multinomial]*(*X[i]*) = *nπ[i]*(1−*π[i]*)'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.35
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: If each of the *X*[1], *X*[2], ⋯, *X[m]* is a scalar, then we can think of a
    random vector ![](../../OEBPS/Images/eq_05-35-a.png). The expected value of such
    a random variable is
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-35-b.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
- en: and the covariance is
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-36.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
- en: Equation 5.36
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: where the diagonal terms are like the binomial variance *σ[ii]* = *nπ[i]*(1−*π[i]*)
    ∀*i* ∈ [1, *m*] and the off-diagonal terms are *σ[ij]* = −*nπ[i]π[j]* ∀(*i*, *j*)
    ∈ [1, *m*] × [1, *m*]. The cross-covariance terms in the diagonal are negative
    because an increase in one element implies a decrease in the others.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 Mean and variance of a multinomial distribution
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Number of sample points
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: ② 100000 × 1
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: ③ Obtains samples from ufm_dist instantiated in listing [5.9](#lst-multinom-dist-logprob)
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sample mean
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Mean via PyTorch function
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Sample variance
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Variance via PyTorch function
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.5 Bernoulli distribution
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Bernoulli distribution is a special case of a binomial distribution where
    *n* = 1: that is, a single success-or-failure trial is performed. The probability
    of success is *π*, and the probability of failure is 1 − *π*.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, let *X* be a discrete random variable that takes the value
    1 (success) with probability *π* and the value 0 (failure) with probability 1
    − *π*. The distribution of *X* is the Bernoulli distribution:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*X* = 1) = *π*'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*X* = 0) = 1 - *π*'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 Log probability of a Bernoulli distribution
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Imports a PyTorch Bernoulli distribution
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets the distribution params
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: ③ Instantiates a Bernoulli distribution object
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: ④ Instantiates a single-point test dataset
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Evaluates the probability using PyTorch
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Evaluates the probability using the formula
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Asserts that the probabilities match
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the Bernoulli distribution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/BRwq](http://mng.bz/BRwq).
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a Bernoulli distribution
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: If there are only two classes, *success* and *failure*, we cannot speak directly
    of an expected value. If we run, say, 100 trials and get 30 *successes* and 70
    *failures*, the average is 0.3 *success*, which is not a valid outcome. We cannot
    have fractional *success* or *failure* in this binary system.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: 'We can, however, talk about the expected value of a Bernoulli distribution
    if we introduce an artificial construct. We assign numerical values to these binary
    entities: *success* = 1 and *failure* = 0. Then the expected value of *X* is'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-37.png)'
  id: totrans-575
  prefs: []
  type: TYPE_IMG
- en: Equation 5.37
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a Bernoulli distribution
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if we assign numerical values to these binary entities—*success*
    = 1 and *failure* = 0—the variance of the Bernoulli distribution is
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-38.png)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
- en: Equation 5.38
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 Mean and variance of a Bernoulli distribution
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Number of sample points
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: ② 100000 × 1
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: ③ Obtains samples from ufm_dist instantiated in listing [5.11](#lst-bern-dist-logprob)
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sample mean
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Mean via PyTorch function
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Sample variance
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Variance via PyTorch function
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.6 Categorical distribution and one-hot vectors
  id: totrans-590
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider again the example problem introduced in section [5.9.4](#sec-multinomial-distr).
    We have a database with four classes of photos:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: 'Photos of Albert Einstein (class 1): 10%'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photos of Marie Curie (class 2): 42%'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photos of Carl Friedrich Gauss (class 3): 4%'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other photos (class 4): 44%'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we randomly select a photo from the database,
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: The probability of selecting class 1 is *π*[1] = 0.1.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 2 is *π*[2] = 0.42.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 3 is *π*[3] = 0.04.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 4 is *π*[4] = 0.44.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As before, *π*[1] + *π*[2] + *π*[3] + *π*[4] = 1 because the classes are mutually
    exclusive and exhaustive so exactly one class must occur in each trial.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: In multinomial distribution, we performed *n* trials and asked how many times
    each specific class would occur. What if we perform only one trial? Then we get
    categorical distribution.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: Categorical distribution is a special case of multinomial distribution (with
    the number of trials *n* = 1). It is also an extension of the Bernoulli distribution
    where instead of just two classes, *success* and *failure*, we can have an arbitrary
    number of classes.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: Formally,
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: Let *C*[1], *C*[2], ⋯, *C[m]* be a set of *m* classes such that in any random
    trial, exactly one of these classes will be selected, with the respective probabilities
    *π*[1], *π*[2], ⋯, *π[m]*. We sometimes refer to the probabilities of all the
    classes together as a vector ![](../../OEBPS/Images/eq_05-38-a2.png)
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let *X*[1], *X*[2], ⋯, *X[m]* be a set of random variables. *X[i]* corresponds
    to the number of occurrences of class *C[i]* in a set of *n* trials.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the categorical probability function depicts the probability of each of
    the classes *C*[1], *C*[2], and so on, in a single trial.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot vector
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: We can use a one-hot vector to compactly express the outcome of a single trial
    of categorical distribution. This is a vector with *m* elements. Exactly a single
    element is 1; all other elements are 0. The 1 indicates which of the *m* possible
    classes occurred in that specific trial. For instance, in the example with the
    database of photos, if a Marie Curie photo comes up in a given trial, the corresponding
    one-hot vector is ![](../../OEBPS/Images/eq_05-38-b2.png).
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: Probability of a categorical distribution
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: We can think of a one-hot vector *X* as a random variable with a categorical
    distribution. Note that each individual class follows a Bernoulli distribution.
    The probability of class *C[i]* occurring in any given trial is
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*C[i]*) = *π[i]*'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: We can express the probability distribution of all the classes together compactly
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-39.png)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
- en: Equation 5.39
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: where ![](../../OEBPS/Images/AR_x.png) is a one-hot vector. Note that all but
    one of the powers in equation [5.39](#eq-prob-categorical-distr) is 0; hence the
    corresponding factor evaluates to 1\. The remaining power is 1. Hence the overall
    probability always evaluates to *π[i]*, where *i* is the index of the class that
    occurred in the trial.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a categorical distribution
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are talking about classes, expected value and variance do not make
    sense in this context. We encountered a similar situation with the Bernoulli distribution.
    We assigned numerical values to each class and somewhat artificially defined the
    expected value and variance. A similar idea can also be applied here: we can talk
    about the expected value and variance of the one-hot vector (which consists of
    numerical values 0 and 1). But it remains an artificial construct.'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: Given a random variable *X* whose instances are one-hot vectors ![](../../OEBPS/Images/AR_x.png)
    following a categorical distribution with *m* classes with respective probabilities
    *π*[1], *π*[2], ⋯, *π[m]*,
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-40.png)'
  id: totrans-620
  prefs: []
  type: TYPE_IMG
- en: Equation 5.40
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: We skip the variance of a categorical distribution.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-623
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we first looked at probability and statistics from a machine
    learning point of view. We also introduced the PyTorch `distributions` package
    and illustrated each concept with PyTorch `distributions` code samples immediately
    following the math.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: The probability of a specific event type is defined as the fraction of the total
    population of all possible events occupied by events of that specific type.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A random variable is a variable that can assume any value from a predefined
    range of possible values. Random variables can be discrete or continuous. A probability
    is associated with a discrete random variable taking a specific value. A probability
    is also associated with a continuous random variable taking a value in an infinitesimally
    small range around a specific value, called its probability density at that value.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum rule of probabilities states that the sum of the probabilities of a
    set of mutually exclusive events is the probability of one or another of them
    occurring. If the set of events is exhaustive that is, among them, they cover
    the entire space of possible events), then their sum is 1 because one or another
    of them must occur. For continuous random variables, integrating the probability
    density function over the domain of possible values yields 1.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The joint probability of a set of events is the probability of all those events
    occurring together. If the events are independent, the joint probability is the
    product of their individual probabilities.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing a sample from the probability distribution of a random variable returns
    an arbitrary value from the set of possible values. If we draw many samples, the
    higher-probability values show up more often than the lower-probability values.
    The sampled points occupy a region (called the sample point cloud) in the domain
    of possible values. In a sample point cloud, the region where the probabilities
    are higher is more densely populated than lower-probability regions.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expected value of a random variable is the average of the values of points
    in a very large (approaching infinity) sample cloud. It is equal to the weighted
    sum of all possible values of the random variable, where the weight for each value
    is its probability of occurrence. For continuous random variables, this boils
    down to integration—over the domain of possible values—of the product of the random
    variable’s value and the probability density. The physical significance of the
    expected value is that it is a single-point representation of the entire distribution.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance of a random variable is the square root of the average squared
    distances of the sample point values from the mean in a very large (approaching
    infinity) sample cloud. It is equal to the weighted sum of the squared distances
    of all possible values of the random variable from the mean. The weight for each
    value is its probability of occurrence. For continuous random variables, this
    boils down to integration—over the domain of possible values—of the product of
    the squared distance of the random variable’s value from the mean and the probability
    density. Physically, the variance is a measure of the spread of the points in
    the distribution around its mean. In the multivariate case, this spread depends
    on the direction. Since there are infinite possible directions in a space with
    two or more dimensions, we cannot speak of a single variance value. Instead, we
    compute a covariance matrix with which to compute the spread along any specified
    direction. The eigenvector corresponding to the largest eigenvalue of this covariance
    matrix yields the direction of maximum spread. That eigenvalue yields the maximum
    spread. The eigenvector corresponding to the next-largest eigenvalue yields the
    orthogonal direction with the next-highest spread, and so forth.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis (PCA) is a technique in multivariate statistics
    to identify the directions of the maximum spread of data. It uses the eigenvectors
    and eigenvalues of the covariance matrix.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gaussian distribution is the most important probability distribution. The
    Gaussian random variable has one value with the highest probability of occurrence.
    The probability decreases smoothly with increasing distance from that highest
    probability value. The probability density function is continuous and looks like
    a bell-shaped surface. The center of the bell is the highest probability value,
    which also happens to be the expected value of the Gaussian random variable. The
    covariance matrix determines the shape of the base of the bell surface. It is
    circular when the covariance matrix is diagonal, with equal values on the diagonal;
    it is elliptical in general, with the axes of the ellipse along the eigenvectors
    of the covariance matrix.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample point cloud of a Gaussian distribution is elliptical. It corresponds
    to the base of the bell-shaped probability density function. The longest spread
    corresponds to the ellipse’s major axis, which corresponds to the eigenvector
    corresponding to the largest eigenvalue of the covariance matrix. In the GitHub
    repository, we have provided an interactive visualizer for observing the shapes
    of Gaussian distributions in one and two dimensions as you change the parameter
    values. Take a look at the interactive visualization section at [http://mng.bz/NYJX](http://mng.bz/NYJX).
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
