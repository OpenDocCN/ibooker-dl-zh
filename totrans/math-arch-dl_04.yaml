- en: 5 Probability distributions in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: The role of probability distributions in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with binomial, multinomial, categorical, Bernoulli, beta, and Dirichlet
    distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The significance of entropy and cross-entropy in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Life often requires us to estimate the chances of an event occurring or make
    a decision in the face of uncertainty. Probability and statistics form the common
    toolbox to use in such circumstances. In machine learning, we take large feature
    vectors as inputs. As stated earlier, we can view these feature vectors as points
    in a high-dimensional space. For instance, gray-level images of size 224 √ó 224
    can be viewed as points in a 50, 176-dimensional space, with each pixel corresponding
    to a specific dimension. Inputs with common characteristics, such as images of
    animals, will correspond to a cluster of points in that space. Probability distributions
    provide an effective tool for analyzing such loosely structured point distributions
    in arbitrarily high-dimensional spaces. Instead of simply developing a machine
    that emits a class given an input, we can fit a probability distribution to the
    clusters of input points (or a transformed version of them) satisfying some property
    of interest. This often lends more insight into the problem we are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, suppose we are trying to design a recommendation system. We could
    design one or more classifiers that emit yes/no decisions about whether to recommend
    product X to person Y. On the other hand, we can fit probability distributions
    to specific groups of people. Doing so can lead to the discovery of significant
    overlap between the point clusters representing various groups‚Äîfor instance, people
    who drink black coffee and start-up founders. We may not know the explanation
    or even the direction in which causality (if any) flows in the relationship. But
    we see the correlation and may design a better recommendation system using it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another situation in which probabilistic models are used in machine learning
    is when the problem involves a very large number of (perhaps infinitely many)
    classes. For instance, suppose we are creating a machine that not only recognizes
    cats in an image but also labels each pixel as belonging or not belonging to a
    cat. Effectively, the machine segments the image pixels into foreground versus
    background. This is called *semantic segmentation*. It is hard to cast this problem
    as a classification problem: we typically design a system that emits a probability
    of being foreground for each pixel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Probabilistic models are also used in unsupervised and minimally supervised
    learning: for instance, in *variational autoencoders* VAEs), which we discuss
    in chapter [14](../Text/14.xhtml#ch-ae-vae).'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces the fundamental notion of probability and discusses
    probability distributions (including multivariates), with specific examples, in
    a machine learning-centric way. As usual, we emphasize the geometrical view of
    multivariate statistics. An equally important goal of this chapter is to familiarize
    you with PyTorch `distributions`, the PyTorch statistical package, which we use
    throughout the book. All the distributions discussed are accompanied by code snippets
    from PyTorch `distributions`.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng](http://mng.bz/8NVg)
    [.bz/8NVg](http://mng.bz/8NVg) in the form of fully functional and executable
    Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: '5.1 Probability: The classical frequentist view'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a mythical city called Statsville. Suppose we choose a random adult
    inhabitant of Statsville. What are the chances of this person‚Äôs height being greater
    than 6 ft? Less than 3 ft? Between 5 ft 5 in. and 6 ft? What are the chances of
    this person‚Äôs weight being between 50 and 70 kg (physicists would rather use the
    term *mass* here, but we have chosen to stick to the more common word *weight*)?
    Greater than 100 kg? What is the probability of the person‚Äôs home being exactly
    6 km from the city center? What are the chances of the person‚Äôs weight being in
    the 50‚Äì70 kg range *and* their height being in the 5 ft 5 in. to 6 ft range? What
    are the chances of the person‚Äôs weight being greater than 90 kg *and* their home
    being within 5 km of the city center?
  prefs: []
  type: TYPE_NORMAL
- en: 'All these questions can be answered in the frequentist paradigm by adopting
    the following approach:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Count the size of the population belonging to the desired event satisfying
    the criterion or criteria of interest): for instance, the number of Statsville
    adults taller than 6 ft. Divide that by the total size of the population (here,
    the number of adults in Statsville). This is the probability (chance) of that
    criterion/criteria being satisfied.'
  prefs: []
  type: TYPE_NORMAL
- en: Formally,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-01-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.1
  prefs: []
  type: TYPE_NORMAL
- en: For instance, suppose there are 100,000 adults in the city. Of them, 25,000
    are 6 ft or taller. Then the size of the population satisfying the event of interest
    (aka the number of favorable outcomes) is 25,000\. The total population size (aka
    the number of possible outcomes) is 100,000\. So,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-01-a-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the total population is always a superset of the population belonging
    to any event, the denominator is always greater than or equal to the numerator.
    Consequently, *probabilities are always lesser than or equal to 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we talk about probability, a relevant question is, ‚ÄúThe probability of
    what?‚Äù The simplest answer is, ‚ÄúThe probability of the occurrence of an event.‚Äù
    For example, in the previous subsection, we discussed the probability of the event
    that the height of an adult Statsville resident is less than 6 ft. A little thought
    reveals that an event always corresponds to a numerical entity of interest taking
    a particular value or lying in a particular range of values. This entity is called
    a *random variable*. For instance, the height of adult Statsville residents can
    be a random variable, and we can talk about the probability of it being less than
    6 ft, or the weight of adult Statsville residents can be a random variable, and
    we can talk about the probability of it being less than 60 kg. When predicting
    the performance of stock markets, the Dow Jones index maybe a random variable:
    we can talk about the probability of this random variable crossing 19,000\. And
    when discussing the spread of a virus, the total number of infected people may
    be a random variable, and we can talk about the probability of it being less than
    2,000, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The defining characteristic of a random variable is that every allowed value
    (or range of values) is associated with a probability (of the random variable
    taking that value or value range). For instance, we may allow a set of only three
    weight ranges for adults of Statsville: *S*[1], less than 60 kg; *S*[2], between
    60 and 90 kg; and *S*[3], greater than 90 kg. Then we can have a corresponding
    random variable *X* representing the quantized weight. It can take one of only
    three values: *X* = 1 (corresponding to the weight in *S*[1]), *X* = 2 (corresponding
    to the weight in *S*[2]), or *X* = 3 (corresponding to the weight in *S*[3]).
    Each value comes with a fixed probability: for example, *p*(*X* = 1) = 0.25, *p*(*X*
    = 2) = = 0.5, and *p*(*X* = 3) = 0.25, respectively, in the example from section
    [5.1](#sec-prob_frequentist). Such random variables that take values from a countable
    set are known as *discrete* random variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Random variables can also be *continuous*. For a continuous random variable
    *X*, we associate a probability with its value being in an infinitesimally small
    range, *p*(*x* ‚â§ *X* < *x* + *Œ¥x*), with *Œ¥x* ‚Üí 0. This is called *probability
    density* and is explained in more detail in section [5.6](#sec-cont-rv).
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE In this book, we always use uppercase letters to denote random variables.
    Usually, the same letter in lowercase refers to a specific value of the random
    variable: for example, *p*(*X* = *x*) denotes the probability of random variable
    *X* taking the value *x* and *p*(*X*‚àà{*x*, *x* + *Œ¥x*}) denotes the probability
    of random variable *X* taking a value between *x* and *x* + *Œ¥x*. Also note that
    sometimes we use the letter *X* to denote a data set. This popular but confusing
    convention is rampant in the literature‚Äîgenerally, the usage is obvious from the
    context.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Population histograms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Histograms help us to visualize discrete random variables. Let‚Äôs continue with
    our Statsville example. We are only interested in three weight ranges for Statsville
    adults: *S*[1]: less than 60 kg; *S*[2]: between 60 and 90 kg; and *S*[3]: greater
    than 90 kg. Suppose the counts of Statsville adults in these weight ranges are
    as shown in table 5.1.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 Frequency table for the weights of adults in the city of Statsville
  prefs: []
  type: TYPE_NORMAL
- en: '| *S*[1]: Less than 60 kg | *S*[2]: Between 60 and 90 kg | *S*[3]: More than
    90 kg |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 25,000 | 50,000 | 25,000 |'
  prefs: []
  type: TYPE_TB
- en: 'The same information can be visualized by the histogram shown in figure [5.1](#fig-statsville-weight-histogram).
    The *X*-axis of the histogram corresponds to possible values of the discrete random
    variable from section [5.1.1](#sec-RVs). The *Y*-axis shows the size of the population
    in the corresponding weight range. There are 25,000 people in range *S*[1], 50,000
    people in range *S*[2], and 25,000 people in range *S*[3]. Together, these categories
    account for the entire adult population of Statsville‚Äîevery adult belongs to one
    category or another. This can be verified by adding the *Y*-axis values for all
    the categories: 25, 000 + 50, 000 + 25, 000 = 100, 000, the adult population of
    Statsville.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Probability distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure [5.1](#fig-statsville-weight-histogram) and its equivalent, table [5.1](#tab-hist-wt),
    can easily be converted to probabilities, as shown in table [5.2](#tab-hist-prob).
    The table shows the probabilities corresponding to allowed values of the discrete
    random variable *X* representing the quantized weight of a randomly chosen adult
    resident of Statsville. Table [5.2](#tab-hist-prob) represents what is formally
    known as a *probability distribution*: a mathematical function that takes a random
    variable as input and outputs the probability of it taking any allowed value.
    It must be defined over all possible values of the random variable.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the set of ranges {*S*[1], *S*[2], *S*[3]} is exhaustive in the sense
    that all possible values of *X* belong to one range or other‚Äîwe cannot have a
    weight that does not belong to any of them. In set-theoretical terms, the union
    of these ranges, *S*[1] ‚à™ *S*[2] ‚à™ *S*[3], covers a space that contains the entire
    population (all possible values of *X*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F01_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 Histogram depicting the weights of adults in Statsville, corresponding
    to table [5.1](#tab-hist-wt)
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.2 Probability distribution for quantized weights of Statsville adults
  prefs: []
  type: TYPE_NORMAL
- en: '| *S*[1]: Less than 60 kg | *S*[2]: Between 60 and 90 kg | *S*[3]: More than
    90 kg |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *p*(*X* = 1) = 25,000/100,000 = 0.25 | *p*(*X* = 2) = 50,000/100,000 = 0.5
    | *p*(*X* = 3) = 25,000/100,000 = 0.25 |'
  prefs: []
  type: TYPE_TB
- en: NOTE The set-theoretic operator ‚à™ denotes set union.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ranges are also mutually exclusive in that any given observation of *X*
    can belong to only a single range, never more. In set-theoretic terms, the intersection
    of any pair of ranges is null: *S*[1] ‚à© *S*[2] = *S*[1] ‚à© *S*[3] = *S*[2] ‚à© *S*[3]
    = *œï*.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The set-theoretic operator ‚à© denotes set intersection.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a set of exhaustive and mutually exclusive events, the function ielding
    the probabilities of these events is a probability distribution. For instance,
    the probability distribution in our tiny example comprises three probabilities:
    *P*(*X* = 1) = 0.25, *P*(*X* = 2) = 0.5, and *P*(*X* = 3) = 0.25. This is shown
    in figure [5.2](#fig-statsville-weight-probability-distribution), which is a three-point
    graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F02_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 Probability distribution graph for the weights of adults in Statsville,
    corresponding to table [5.2](#tab-hist-prob). Event *E*[1] ‚â° *X* = 1 ‚üπ a weight
    in the range *S*[1], Event *E*[2] ‚â° *X* = 2 ‚üπ a weight in the range *S*[2], and
    Event *E*[3] ‚â° *x* = 3 ‚üπ a weight in the range *S*[3].
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Basic concepts of probability theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we briefly touch on impossible and certain events; the sum
    of probabilities of exhaustive, mutually exclusive events; and independent events.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Probabilities of impossible and certain events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The probability of an impossible event is zero (for example, the probability
    that the sun will rise in the west). The probability of an event that occurs with
    certitude is 1 the probability that the sun will rise in the east). Improbable
    events such as this author beating Roger Federer in competitive tennis) have low
    but nonzero probabilities, like 0.001. Highly probable events (such as Roger Federer
    beating this author in competitive tennis) have probabilities close to but not
    exactly equal to 1, like 0.999.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Exhaustive and mutually exclusive events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider the events *E*[1], *E*[2], *E*[3] corresponding to the quantized weight
    of a Statsville adults from section [5.2](#sec-prob-distr) belonging to the range
    *S*[1], *S*[2], or *S*[3], respectively equivalently, *E*[1] is the event corresponding
    to *X* = 1, *E*[2] is the event corresponding to *X* = 2, and *E*[3] is the event
    corresponding to *X* = 3). The events are exhaustive: their union covers the entire
    population space. This means all quantized weights of Statsville adults belong
    to one of the ranges *S*[1], *S*[2], *S*[3]. The events are also mutually exclusive:
    their mutual intersections are null, meaning no member of the population can belong
    to more than one range. For example, if a weight belongs to *S*[1], it cannot
    belong to *S*[2] or *S*[3]. For such events, the following holds true:'
  prefs: []
  type: TYPE_NORMAL
- en: The sum of the probabilities of mutually exclusive events yields the probability
    of one or the other of them occurring.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, for events *E*[1], *E*[2], *E*[3],
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*E*[1] or *E*[2]) = *p*(*E*[1]) + *p*(*E*[2])'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.2
  prefs: []
  type: TYPE_NORMAL
- en: the *sum rule* says that
  prefs: []
  type: TYPE_NORMAL
- en: The sum of the probabilities of an exhaustive, mutually exclusive set of events
    is always 1.
  prefs: []
  type: TYPE_NORMAL
- en: For example,
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*E*[1]) + *p*(*E*[2]) + *p*(*E*[3]) = *p*(*E*[1] or *E*[2] or *E*[3]) =
    1'
  prefs: []
  type: TYPE_NORMAL
- en: This is intuitively obvious. We are merely saying that *we can say with certainty
    that either *E*[1] or *E*[2] or *E*[3] will occur*.
  prefs: []
  type: TYPE_NORMAL
- en: In general, given a set of exhaustive, mutually exclusive events *E*[1], *E*[2],
    ‚ãØ, *E[n]*,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.3
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Independent events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the two events *E*[1] ‚â° ‚Äúweight of an adult inhabitant of Statsville
    is less than 60 kg‚Äù and *G*[1] ‚â° ‚Äúhome of an adult inhabitant of Statsville is
    within 5 km of the city center‚Äù. These events do not influence each other at all.
    The knowledge that a member of the population weighs less than 60 kg does not
    reveal anything about the distance of their home from the city center and vice
    versa. We say *E*[1] and *G*[1] are *independent events*. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: A set of events are independent if the occurrence of one does not affect the
    probability of the occurrence of another.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Joint probabilities and their distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given an adult Statsville resident, let *E*[1] be, as before, the event that
    their weight is less than 60 kg. The corresponding probability is *p*(*E*[1]).
    Also, let *G*[1] be the event that the distance of their home from the city center
    is less than 5 km. The corresponding probability is *p*(*G*[1]). Now consider
    the probability that a resident weights less than 60 kg *and* their home is less
    than 5 km from the city center. This probability, denoted *p*(*E*[1], *G*[1]),
    is called a *joint probability*. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: The joint probability of a set of events is the probability of all those events
    occurring together.
  prefs: []
  type: TYPE_NORMAL
- en: The *product rule* says that the joint probability of independent events can
    be obtained by multiplying their individual probabilities. Thus, for the current
    example, *p*(*E*[1], *G*[1]) = *p*(*E*[1])*p*(*G*[1]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs continue our discussion of joint probabilities with a slightly more elaborate
    example. We have consolidated the weight categories, corresponding populations,
    and probability distributions in table [5.3](#tab-hist-prob-wt) for quick reference.
    Similarly, we quantize the distance of residents‚Äô homes from the city center into
    three ranges: *D*[1] ‚â° less than 5 km, *D*[2] ‚â° between 5 and 15 km, and *D*[3]
    ‚â° greater than 15 km. Table [5.4](#tab-hist-prob-dist) shows the corresponding
    population and probability distributions. The joint probability distribution of
    the events {*E*[1], *E*[2], *E*[3]} and {*G*[1], *G*[2], *G*[3]} is shown in table
    [5.5](#tab-joint-prob-distr).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.3 Population probability distribution table for the weights of adult
    residents of Statsville. *E*[1], *E*[2], *E*[3] are exhaustive, mutually exclusive
    events, *p*(*E*[1]) + *p*(*E*[2]) + *p*(*E*[3]) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: '| Less than 60 kg (range *S*[1]) | Between 60 and 90 kg (range *S*[2]) | More
    than 90 kg (range *S*[3]) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Event *E*[1] ‚â° *weight* ‚àà *S*[1] | Event *E*[2] ‚â° *weight* ‚àà *S*[2] | Event
    *E*[3] ‚â° *weight* ‚àà *S*[3] |'
  prefs: []
  type: TYPE_TB
- en: '| Population size = 25,000 | Population size = 50,000 | Population size = 25,000
    |'
  prefs: []
  type: TYPE_TB
- en: '| *p*(*S*[1]) = 25,000/100,000 = 0.25 | *p*(*S*[2]) = 50,000/100,000 = 0.5
    | *p*(*S*[3]) = 25,000/100,000 = 0.25 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.4 Population probability distribution table for the distance of adult
    Statsville residents‚Äô homes from the city center. *G*[1], *G*[2], *G*[3] are exhaustive,
    mutually exclusive events, *p*(*G*[1]) + *p*(*G*[2]) + *p*(*G*[3]) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: '| Less than 5 km (range *D*[1]) | Between 5 and 15 km (range *D*[2]) | Greater
    than 15 km (range *D*[3]) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Event *G*[1] ‚â° *distance* ‚àà *D*[1] | Event *G*[2] ‚â° *distance* ‚àà *D*[2] |
    Event *G*[3] ‚â° *distance* ‚àà *D*[3] |'
  prefs: []
  type: TYPE_TB
- en: '| Population size = 20,000 | Population size = 60,000 | Population size = 20,000
    |'
  prefs: []
  type: TYPE_TB
- en: '| *p*(*G*[1]) = 20,000/100,000 = 0.20 | *p*(*G*[1]) = 60,000/100,000 = 0.6
    | *p*(*G*[1]) = 20,000/100,000 = 0.20 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.5 Joint probability distribution of independent events. The sum of all
    elements in the table is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Less than 5 km** (*G*[1]) | *p*(*E*[1], *G*[1])= 0.25 √ó 0.2= 0.05 | *p*(*E*[2],
    *G*[1])= 0.5 √ó 0.2= 0.1 | *p*(*E*[3], *G*[1])= 0.25 √ó 0.2= 0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| **Between 5 and 15 km** (*G*[2]) | *p*(*E*[1], *G*[2])= 0.25 √ó 0.6= 0.15
    | *p*(*E*[2], *G*[2])= 0.5 √ó 0.6= 0.3 | *p*(*E*[3], *G*[2])= 0.25 √ó 0.6= 0.15
    |'
  prefs: []
  type: TYPE_TB
- en: '| **More than 15 km** (*G*[3]) | *p*(*E*[1], *G*[3])= 0.25 √ó 0.2= 0.05 | *p*(*E*[2],
    *G*[3])= 0.5 √ó 0.2= 0.1 | *p*(*E*[3], *G*[3])= 0.25 √ó 0.2= 0.05 |'
  prefs: []
  type: TYPE_TB
- en: 'We can make the following statements about table [5.5](#tab-joint-prob-distr):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sum total of all elements in table [5.5](#tab-joint-prob-distr) is 1. In
    other words, *p*(*E[i]*, *G[j]*) is a proper probability distribution indicating
    the probabilities of event *E[i]* and event *G[j]* occurring together: here, (*i*,
    *j*) ‚àà {1,2,3} √ó {1,2,3}.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*(*E[i]*, *G[j]*) = *p*(*E[i]*)*p*(*G[j]*) ‚àÄ(*i*, *j*) ‚àà {1,2,3} √ó {1,2,3}.
    This is because the events are independent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NOTE The symbol √ó denotes the *Cartesian product*. The Cartesian product of
    two sets {1,2,3} √ó {1,2,3} is the set {(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)}.
    And the symbol ‚àÄ indicates ‚Äúfor all.‚Äù Read ‚àÄ(*i*, *j*) ‚àà {1,2,3} √ó {1,2,3} as
    follows: for all pairs (*i*, *j*) in the Cartesian product, {1,2,3} √ó {1,2,3}.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, given a set of independent events *E*[1], *E*[2], ‚ãØ, *E[n]*, the
    joint probability *p*(*E*[1], *E*[2],‚ãØ, *E[n]*) of all the events occurring together
    is the product of their individual probabilities of occurring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.4
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The symbol ‚àè stands for ‚Äúproduct.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Marginal probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we do not have the individual probabilities *p*(*E[i]*) and *p*(*G[j]*).
    All we have is the joint probability distribution: that is, table [5.5](#tab-joint-prob-distr).
    Can we find the individual probabilities from them? If so, how?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this question, consider a particular row or column in table [5.5](#tab-joint-prob-distr)‚Äîsay,
    the top row. In this row, the *E* values iterate over all possibilities (the entire
    space of *E*s), but *G* is fixed at *G*[1]. If *G*[1] is to occur, there are only
    three possibilities: it occurs with *E*[1], *E*[2], or *E*[3]. The corresponding
    joint probabilities are *p*(*E*[1], *G*[1]), *p*(*E*[2], *G*[1]), and *p*(*E*[3],
    *G*[1]). If we add them, we get the probability of *G*[1] occurring with *E*[1]
    or *E*[2], or *E*[3]: that is, event (*E*[1], *G*[1]) or (*E*[2], *G*[1]) or (*E*[3],
    *G*[1]). Thus we have considered all situations under which *G*[1] can occur.
    The sum represents the probability of event *G*[1] occurring. Thus, *p*(*G*[1])
    can be obtained by adding all the probabilities in the row corresponding to *G*[1]
    and writing it in the margin (this is why it is called the *marginal* probability).
    Similarly, by adding all the probabilities in the middle column, we obtain the
    probability *p*(*E*[2]), and so forth. Table [5.6](../Text/05.xhtml#tab-jmarginal-prob)
    shows table [5.5](#tab-joint-prob-distr) updated with marginal probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.6 Joint probability distribution with marginal probabilities shown
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) | Marginals for G‚Äôs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Less than 5 km** (*G*[1]) | *p*(*E*[1], *G*[1])= 0.25 √ó 0.2= 0.05 | *p*(*E*[2],
    *G*[1])= 0.5 √ó 0.2= 0.1 | *p*(*E*[3], *G*[1])= 0.25 √ó 0.2= 0.05 | *p*(*G*[1])=
    0.05 + 0.1 + 0.05= 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Between 5 and 15 km** (*G*[2]) | *p*(*E*[1], *G*[2])= 0.25 √ó 0.6= 0.15
    | *p*(*E*[2], *G*[2])= 0.5 √ó 0.6= 0.3 | *p*(*E*[3], *G*[2])= 0.25 √ó 0.6= 0.15
    | *p*(*G*[2])0.15 + 0.3 + 0.15= 0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| **More than 15 km** (*G*[3]) | *p*(*E*[1], *G*[3])= 0.25 √ó 0.2= 0.05 | *p*(*E*[2],
    *G*[3])= 0.5 √ó 0.2= 0.1 | *p*(*E*[3], *G*[3])= 0.25 √ó 0.2= 0.05 | *p*(*G*[3])=
    0.05 + 0.1 + 0.05= 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Marginals for** *E***s** | *p*(*E*[1])= 0.05 + 0.15 + 0.050.05 = 0.25 |
    *p*(*E*[2])= 0.1 + 0.3 + 0.1= 0.5 | *p*(*E*[3])= 0.05 + 0.15 += 0.25 |  |'
  prefs: []
  type: TYPE_TB
- en: In general, given a set of exhaustive, mutually exclusive events *E*[1], *E*[2],
    ‚ãØ, *E[n]*, another event *G*, and joint probabilities *p*(*E*[1], *G*), *p*(*E*[2],
    *G*), ‚ãØ, *p*(*E[n]*, *G*),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.5
  prefs: []
  type: TYPE_NORMAL
- en: By summing over all possible values of *E[i]*s, we factor out the *E*s. This
    is because the *E*s are mutually exclusive and exhaustive; summing over them results
    in a certain event that is factored out (remember, the probability of a certain
    event is 1).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Dependent events and their joint probability distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, the events we have considered jointly are ‚Äúweights‚Äù and ‚Äúdistance of
    a resident‚Äôs home from the city center.‚Äù These are independent of each other‚Äîtheir
    joint is the product of their individual probabilities. Now, let‚Äôs discuss a different
    situation where the variables are connected and knowing the value of one does
    help us predict the other. For instance, the weights and heights of adult residents
    of Statsville are not independent: typically, taller people weigh more, and vice
    versa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we use a toy example to understand the idea. We quantize heights
    into three ranges, *H*[1] ‚â° less than 5 ft 5 in., *H*[2] ‚â° between 5 ft 5 in.
    and 6 ft, and *H*[3] ‚â° greater than 6 ft. Let *z* be the random variable corresponding
    to height. We have three possible events with respect to height: *F*[1] ‚â° *z*
    ‚àà *H*[1], *F*[2] ‚â° *z* ‚àà *H*[2], and *F*[3] ‚â° *z* ‚àà *H*[3]. The joint probability
    distribution of height and weight is shown in table [5.7](#tab-joint-prob-distr-dep).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.7 Joint probability distribution of dependent events
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Less than** 5 ft 5 **in. (*F*[1])** | *p*(*E*[1], *F*[1])¬†= 0.25 | *p*(*E*[2],
    *F*[1])¬†= 0 | *p*(*E*[3], *F*[1])¬†= 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Between 5 ft 5 in. and 6 ft** (*F*[2]) | *p*(*E*[1], *F*[2])¬†= 0. | *p*(*E*[2],
    *F*[2])¬†= 0.5 | *p*(*E*[3], *F*[2])¬†= 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **More than** **6 ft (*F*[3])**. | *p*(*E*[1], *F*[3])¬†= 0 | *p*(*E*[2],
    *F*[3])¬†= 0 | *p*(*E*[3], *F*[3])¬†= 0.25 |'
  prefs: []
  type: TYPE_TB
- en: 'Note the following about table [5.7](#tab-joint-prob-distr-dep):'
  prefs: []
  type: TYPE_NORMAL
- en: The sum total of all elements in table [5.7](#tab-joint-prob-distr-dep) is 1.
    In other words, *p*(*E[i]*, *F[j]*) is a proper probability distribution indicating
    the probabilities of event *E[i]* and event *F[j]* occurring together. Here (*i*,
    *j*) ‚àà {1,2,3} √ó {1,2,3}.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*(*E[i]*, *F[j]*) = 0 *if* *i* ‚â† *j* ‚àÄ(*i*, *j*) ‚àà {1,2,3} √ó {1,2,3}. This
    essentially means the events are perfectly correlated: the occurrence of *E*[1]
    implies the occurrence of *F*[1] and vice versa, the occurrence of *E*[2] implies
    the occurrence of *F*[2] and vice versa, and the occurrence of *E*[3] implies
    the occurrence of *F*[3] and vice versa. In other words, every adult resident
    of Statsville who weighs less than 60 kg is also shorter than 5 ft 5 in., and
    so on. (In life, such perfect correlations rarely exist; but Statsville is a mythical
    town.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '5.5 Geometrical view: Sample point distributions for dependent and independent
    variables'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs look at a graphical view of the point distributions corresponding to tables
    [5.5](#tab-joint-prob-distr) and [5.7](#tab-joint-prob-distr-dep). There is a
    fundamental difference in how the point distributions look for independent and
    dependent variables; it is connected to principal component analysis (PCA) and
    dimensionality reduction, which we discussed in section [4.4](../Text/04.xhtml#sec-pca).
  prefs: []
  type: TYPE_NORMAL
- en: We use a rectangular bucket-based technique to visualize joint 2D discrete events.
    For instance, we have three weight-related events, *E*[1], *E*[2], *E*[3], and
    three distance-related events, *G*[1], *G*[2], *G*[3]. Hence the joint distribution
    has 3 √ó 3 = 9 possible events (*E[i]*, *G[j]*), ‚àÄ(*i*, *j*) ‚àà {1,2,3} √ó {1,2,3},
    as shown in table [5.5](#tab-joint-prob-distr). Each of these nine events is represented
    by a small rectangle (bucket for the joint event); altogether, we have a 3 √ó 3
    grid of rectangular buckets. To visualize the sample point distribution, we have
    drawn 1,000 samples from the joint distribution. A joint event sample is placed
    at a random location within its bucket (that is, all points within the bucket
    have an equal probability of being selected). Notice that the concentration of
    points is greater inside high-probability buckets and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F03a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F03b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 Graphical visualization of joint probability distributions. Rectangles
    represent buckets of different discrete events. (a) From table [5.5](#tab-joint-prob-distr)
    independent events). The probabilities of all nine events are non-negligible,
    and all nine rectangles have a relatively high concentration of sample points.
    Not suitable for PCA. (b) From table [5.7](#tab-joint-prob-distr-dep) (non-independent
    events). Events (*E*[1], *F*[1]), (*E*[2], *F*[2]), and (*E*[3], *F*[3]) have
    very high probabilities, and other events have negligible probabilities. Sample
    points are concentrated along the rectangles on the diagonal. Suitable for PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Graphical views of the point distribution for the independent table [5.5](#tab-joint-prob-distr))
    and non-independent table [5.7](#tab-joint-prob-distr-dep)) joint variable pairs
    are shown in figures [5.3a](#fig5_3) and [5.3b](#fig5_3), respectively. We see
    that *the sample point distribution for the independent events is spread somewhat
    symmetrically over the domain*, while *that for the dependent events is spread
    narrowly around a particular line* (in this case, the diagonal). This holds true
    in general and for higher dimensions, too. You should have this mental picture
    with respect to independent versus non-independent point distributions. If we
    sample independent events (uncorrelated), all possible combinations of events
    {*E*[1], *G*[1]}, {*E*[1], *G*[2]}, {*E*[1], *G*[3]}, ‚ãØ, {*E*[3], *G*[3]} have
    a non-negligible probability of occurrence (see table [5.5](#tab-joint-prob-distr)),
    which is equivalent to saying that none of the events have a very high probability
    of occurring remember that probabilities sum to 1, so if some events have very
    low probabilities [close to zero], other events must have high probabilities [near
    one] to compensate). This precludes the concentration of points in a small region
    of the space. All buckets will have many points. In other words, the joint probability
    samples of independent events are diffused throughout the population space (see
    figure [5.3a](#fig5_3), for instance).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the events are correlated, the joint probability samples
    are concentrated in certain high-probability regions of the joint space. For instance,
    in table [5.7](#tab-joint-prob-distr-dep), events (*E*[1], *F*[1]), (*E*[2], *F*[2]),
    (*E*[3], *F*[3]) are far more likely than the other combinations. Hence, the sample
    points are concentrated along the corresponding diagonal (see figure [5.3b](#fig5_3)).
  prefs: []
  type: TYPE_NORMAL
- en: 'If this does not remind you of PCA (section [4.4](../Text/04.xhtml#sec-pca)),
    you should re-read that section. Dependent events such as that shown in figure
    [5.3a](#fig5_3) are good candidates for dimensionality reduction: the two dimensions
    essentially carry the same information, and if we know one, we can derive the
    other. We can drop one of the highly correlated dimensions without losing significant
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Continuous random variables and probability density
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have quantized our random variables and made them discrete. For instance,
    weight has been quantized into three buckets‚Äîless than 60 kg, between 60 and 90
    kg, and greater than 90 kg‚Äîand probabilities have been assigned to each bucket.
    What if we want to know probabilities at a more granular level like 0 to 10 kg,
    10 to 20 kg, 20 to 30 kg, and so on? Well, we have to create more buckets. Each
    bucket covers a narrower range of values (a smaller portion of the population
    space), but there are more of them. In all cases, following the frequentist approach,
    we count the number of adult Statsvilleans in each bucket, divide that by the
    total population size, and call that the probability of belonging to that bucket.
  prefs: []
  type: TYPE_NORMAL
- en: What if we want even further granularity? We create even more buckets, each
    covering an even smaller portion of the population space. In the limit, we have
    an infinite number of buckets, each covering an infinitesimally small portion
    of the population space. Together they still cover the population space‚Äîa very
    large number of very small pieces can cover arbitrary regions. At this limit,
    the probability distribution function is called a *probability density function*.
    Formally,
  prefs: []
  type: TYPE_NORMAL
- en: The probability density function *p*(*x*) for a continuous random variable *X*
    is defined as the probability that *X* lies between *x* and *x* + *Œ¥x* with *Œ¥x*
    ‚Üí 0
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*x*) = lim[*Œ¥x*‚Üí0] *probability*(*x* ‚â§ *X* < *x* + *Œ¥x*)'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE It is slightly unfortunate that the typical symbol for a random variable,
    *X*, collides with that for a dataset (collection of data vectors), also *X*.
    But the context is usually enough to tell them apart.
  prefs: []
  type: TYPE_NORMAL
- en: There is a bit of theoretical nuance here. We are saying that *p*(*x*) is the
    probability of the random variable *X* lying between *x* and *x* + *Œ¥x*. This
    is not exactly the same as saying that *p*(*x*) is the probability that *X* is
    *equal* to *x*. But because *Œ¥x* is infinitesimally small, they amount to the
    same thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the set of events *E* = lim[*Œ¥x*‚Üí0] {*x* ‚â§ *X* < *x* + *Œ¥x*} for all
    possible values of *x*. All possible values of *x* range from negative infinity
    to infinity: *x* ‚àà [‚àí‚àû,‚àû]. There are infinite such events, each of which is infinitesimally
    narrow, but together they cover the entire domain *x* ‚àà [‚àí‚àû,‚àû]. In other words,
    they are exhaustive. They are also mutually exclusive because *x* cannot belong
    to more than one of them at the same time. They are continuous counterparts of
    the discrete events *E*[1], *E*[2], *E*[3] that we have seen before.'
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the set of events *E* = lim[*Œ¥x* ‚Üí 0]{*x* ‚â§ *X* < *x* + *Œ¥x*}
    in continuous space is exhaustive and mutually exclusive means we can apply equation
    [5.3](../Text/05.xhtml#eq-discrete-prob-sum) but the sum will be replaced by an
    integral as the variable is continuous.
  prefs: []
  type: TYPE_NORMAL
- en: The sum rule in a continuous domain is expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.6
  prefs: []
  type: TYPE_NORMAL
- en: Equation [5.6](#eq-continuous-prob-sum) is the continuous analog of equation
    [5.3](../Text/05.xhtml#eq-discrete-prob-sum). It physically means we can say with
    certainty that *x* lies somewhere in the interval ‚àí‚àû to ‚àû.
  prefs: []
  type: TYPE_NORMAL
- en: The random variable can also be multidimensional (that is, a vector). Then the
    probability density function is denoted as *p*(![](../../OEBPS/Images/AR_x.png)).
  prefs: []
  type: TYPE_NORMAL
- en: The sum rule for a continuous multidimensional probability density function
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.7
  prefs: []
  type: TYPE_NORMAL
- en: where *D* is the domain of ![](../../OEBPS/Images/AR_x.png)‚Äîthat is, the space
    containing all possible values of the vector ![](../../OEBPS/Images/AR_x.png).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the 2D vector ![](../../OEBPS/Images/eq_05-07-a.png) has the *XY*
    plane as its domain. Note that the integral in equation [5.7](#eq-continuous-vec-prob-sum)
    is a *multidimensional* integral (for example, for 2D ![](../../OEBPS/Images/AR_x.png),
    it is ‚à¨[![](../../OEBPS/Images/AR_x.png)‚àà*D*] *p*(![](../../OEBPS/Images/AR_x.png))
    *d*![](../../OEBPS/Images/AR_x.png) = 1).
  prefs: []
  type: TYPE_NORMAL
- en: NOTE For simplicity of notation, we usually use a single integral sign to denote
    multidimensional integrals. The vector sign in the domain (for example, ![](../../OEBPS/Images/AR_x.png)
    ‚àà *D*), as well the vector sign in *d*![](../../OEBPS/Images/AR_x.png), indicates
    multiple dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: You may remember from elementary integral calculus that equation [5.6](#eq-continuous-prob-sum)
    corresponds to the area under the curve for *p*(*x*) (or *p*(![](../../OEBPS/Images/AR_x.png))).
    In higher dimensions, equation [5.7](#eq-continuous-vec-prob-sum) corresponds
    to the volume under the hypersurface for *p*(![](../../OEBPS/Images/AR_x.png)).
    Thus, *the total area under a univariate probability density curve is always 1*.
    And in higher dimensions, *the volume under the hypersurface for a multivariate
    probability density function is always 1*.
  prefs: []
  type: TYPE_NORMAL
- en: '5.7 Properties of distributions: Expected value, variance, and covariance'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Toward the beginning of this chapter, we stated that generative machine learning
    models are often developed by fitting a distribution from a known family to the
    available training data. Thus, we postulate a parameterized distribution from
    a known family and estimate the exact parameters that best fit the training data.
    Most distribution families are parameterized in terms of intuitive properties
    like the mean, variance, and so on. Understanding these concepts and their geometric
    significance is essential for understanding the models based on them.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explain a few properties/parameters common to all distributions.
    Later, when we discuss individual distributions, we connect them to the parameters
    of those distributions. We also show how to programmatically obtain the values
    of these for each individual distribution via the PyTorch `distributions` package.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.1 Expected value (aka mean)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we sample a random variable with a given distribution many times and take
    the average of the sampled values, what value do we expect to end up with? The
    average will be closer to the values with higher probabilities (as these appear
    more often during sampling). If we sample enough times, for a given probability
    distribution, this average always settles down to a fixed value for that distribution:
    the *expected value* of the distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Formally,
  prefs: []
  type: TYPE_NORMAL
- en: given a discrete distribution *D* where a discrete random variable *X* can take
    any value from the sets {*x*[1], *x*[2],‚ãØ, *x[n]*} with respective probabilities
    {*p*(*x*[1]), *p*(*x*[2])‚ãØ, *p*(*x[n]*)}, the expected value is given by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-08.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.8
  prefs: []
  type: TYPE_NORMAL
- en: where *x[k]* ‚Üí *D* denotes the *k*th sample drawn from the distribution *D*.
    Overall, equation [5.8](#eq-discrete-univar-expected-val) says that *the average
    or expected value of a very large number of samples drawn from the distribution
    approaches the probability-weighted sum of all possible sample values*. When we
    sample, the higher-probability values appear more frequently than the lower-probability
    values, so the average over a large number of samples is pulled closer to the
    higher-probability values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For multivariate random variables:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a discrete distribution where a discrete multidimensional random variable
    *X* can take any value from the sets {![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],‚ãØ,
    ![](../../OEBPS/Images/AR_x.png)*[n]*} with respective probabilities {*p*(![](../../OEBPS/Images/AR_x.png)[1]),
    *p*(![](../../OEBPS/Images/AR_x.png)[2]),‚ãØ, *p*(![](../../OEBPS/Images/AR_x.png)*[n]*)},
    the expected value is given by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-09.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.9
  prefs: []
  type: TYPE_NORMAL
- en: 'For continuous random variables (note how the sum is replaced by an integral):'
  prefs: []
  type: TYPE_NORMAL
- en: The expected value of a continuous random variable *X* that takes values from
    ‚àí‚àû to ‚àû (that is, *x* ‚àà { ‚àí ‚àû, ‚àû}) is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.10
  prefs: []
  type: TYPE_NORMAL
- en: Expected value and center of mass in physics
  prefs: []
  type: TYPE_NORMAL
- en: In physics, we have the concept of the center of mass or centroid. If we have
    a set of points, each with a mass, the entire point set can be replaced by a single
    point. This point is called the *centroid*. The position of the centroid is the
    weighted average of the positions of the individual points, weighted by their
    individual masses. If we mentally think of the probabilities of individual points
    as masses, the notion of expected value in statistics corresponds to the notion
    of centroid in physics.
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of an arbitrary function of a random variable
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen the expected value of the random variable itself. The notion
    can be extended to functions of the random variable.
  prefs: []
  type: TYPE_NORMAL
- en: The expected value of a function of a random variable is the probability-weighted
    sum of the values of that function at all possible values of the random variable.
    Formally,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.11
  prefs: []
  type: TYPE_NORMAL
- en: Expected value and dot product
  prefs: []
  type: TYPE_NORMAL
- en: In equation [2.6](02.xhtml#eq-dot-product), we looked at the dot product between
    two vectors. Further, in section [2.5.6.2](02.xhtml#subsubsec-dotproduct_as_agreement),
    we saw that the dot product between two vectors measures the agreement between
    the two vectors. If both point in the same direction, the dot product is larger.
    In this section, we show that the expected value of a function of a random variable
    can be viewed as a dot product between a vector representing the probability and
    another vector representing the function itself.
  prefs: []
  type: TYPE_NORMAL
- en: First let‚Äôs consider the discrete case. Our random variable can take values
    *x[i]*, *i* ‚àà {1, *n*}. Now, imagine a vector ![](../../OEBPS/Images/eq_05-11-a.png)
    and a vector ![](../../OEBPS/Images/eq_05-11-b.png). From equation [5.11](#eq-func-rv-expected-val),
    we see that the expected value of the function ùîº(*f*(*X*)) of random variable
    *X* is the same as *![](../../OEBPS/Images/AR_f.png)^T![](../../OEBPS/Images/AR_p.png)*
    = *![](../../OEBPS/Images/AR_f.png)* ‚ãÖ ![](../../OEBPS/Images/AR_p.png). This
    is high when ![](../../OEBPS/Images/AR_f.png) and ![](../../OEBPS/Images/AR_p.png)
    are aligned; thus, the expected value of the function of the random variable is
    high when the high function values coincide with high probabilities of the random
    variable and vice versa. In the continuous case, these vectors have an infinite
    number of components and the summation is replaced by an integral, but the idea
    stays the same.
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of linear combinations of random variables
  prefs: []
  type: TYPE_NORMAL
- en: The expected value is a linear operator. This means the expected value of a
    linear combination of random variables is a linear combination (with the same
    weights) of the expected values of the random variables. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: ùîº(*Œ±*[1]*X*[1] + *Œ±*[2]*X*[2] ‚ãØ *Œ±[n]X[n]*) = *Œ±*[1]ùîº(*X*[1]) + *Œ±*[2]ùîº(*X*[2])
    + ‚ãØ *Œ±[n]*ùîº(*X[n]*)
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.12
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.2 Variance, covariance, and standard deviation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we draw a very large number of samples from a given point distribution,
    we often like to know the spread of the point set. The spread is not merely a
    matter of measuring the largest distance between two points in the distribution.
    Rather, we want to know how densely packed the points are. If most of the points
    fit within a very small ball, then even if one or two points are far from the
    ball, we call that a *small spread* or *high packing density*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why is this important in machine learning? Let‚Äôs start with a few informal
    examples. If we discover that the points are tightly packed in a small region
    around a single point, we may want to replace the entire distribution with that
    point without causing much error. Or if the points are packed tightly around a
    single straight line, we can replace the entire distribution with that line. Doing
    so gives us a simpler lower-dimensional) representation and often leads to a view
    of the data that is more amenable to understanding the big picture. This is because
    small variations about a particular point or direction are usually caused by noise,
    while large variations are caused by meaningful things. By eliminating small variations
    and focusing on the large ones, we capture the main information content. (This
    could be why older people tend to be better at forming big-picture views: perhaps
    there are too few neurons in their heads to retain the huge amount of memory data
    they have accumulated over the years. Their brain performs dimensionality reduction.)
    This is the basic idea behind PCA and dimensionality reduction, which we saw in
    section [4.4](../Text/04.xhtml#sec-pca).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variance‚Äîor its square root, standard deviation‚Äîmeasures how densely packed
    around the expected value the points in the distribution are: that is, the spread
    of the point distribution. Formally, the variance of a probability distribution
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.13
  prefs: []
  type: TYPE_NORMAL
- en: By comparing equation [5.13](../Text/05.xhtml#eq-variance) to equations [5.10](#eq-continuous-expected-val)
    and [5.11](#eq-func-rv-expected-val), we see that the variance is the expected
    value of the distance (*x* ‚àí *Œº*)¬≤ of sample points *x* from the mean *Œº*. So
    if the more probable (more frequently occurring) sample points lie within a short
    distance of the mean, the variance is small, and vice versa. That is to say, the
    variance measures how tightly packed the points are around the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Covariance: Variance in higher dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: Extending the notion of the expected value from the univariate case to the multivariate
    case was straightforward. In the univariate case, we take a probability-weighted
    average of a scalar quantity, *x*. The resulting expected value is a scalar, *Œº*
    = ‚à´[*x* = ‚àí‚àû]^‚àû *x* *p*(*x*)*dx*. In the multivariate case, we take the probability-weighted
    average of a vector quantity, ![](../../OEBPS/Images/AR_x.png). The resulting
    expected value is a vector, ![](../../OEBPS/Images/AR_micro.png) = ‚à´[![](../../OEBPS/Images/AR_x.png)‚àà*D*]
    ![](../../OEBPS/Images/AR_x.png) *p*(![](../../OEBPS/Images/AR_x.png))*d*![](../../OEBPS/Images/AR_x.png).
  prefs: []
  type: TYPE_NORMAL
- en: Extending the notion of variance to the multivariate case is not as straightforward.
    This is because we can traverse the multidimensional random vector‚Äôs domain (the
    space over which the vector is defined) in an infinite number of possible directions‚Äîthink
    how many possible directions we can walk on a 2D plane‚Äîand the spread or packing
    density can be different for each direction. For instance, in figure [5.3b](#fig5_3),
    the spread along the main diagonal is much larger than the spread in a perpendicular
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: The covariance of a multidimensional point distribution is a matrix that allows
    us to easily measure the spread or packing density in any desired direction. It
    also allows us to easily figure out the direction in which the maximum spread
    occurs and what that spread is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a multivariate random variable *X* that takes vector values ![](../../OEBPS/Images/AR_x.png).
    Let *lÃÇ* be an arbitrary direction (as always, we use overhead hats to denote
    unit-length vectors signifying directions) in which we want to measure the packing
    density of *X*. We discussed in sections [2.5.2](02.xhtml#subsec-dotprod-ml) and
    [2.5.6](02.xhtml#subsection-dot_product) that the dot product of ![](../../OEBPS/Images/AR_x.png)
    in the direction *lÃÇ* (that is, ![](../../OEBPS/Images/AR_x.png)*^TlÃÇ*) is the
    projection or component (effective value) of *x* along *lÃÇ*. Thus the spread or
    packing density of the random vector ![](../../OEBPS/Images/AR_x.png) in direction
    *lÃÇ* is the same as the spread of the dot product (aka component or projection)
    *lÃÇ^T*![](../../OEBPS/Images/AR_x.png). This projection *lÃÇ^T*![](../../OEBPS/Images/AR_x.png)
    is a scalar quantity: we can use the univariate formula to measure its variance.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE In this context, we can use ![](../../OEBPS/Images/AR_x.png)*^TlÃÇ* and
    *lÃÇ^T*![](../../OEBPS/Images/AR_x.png) interchangeably. The dot product is symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: The expected value of the projection is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-13-a.png)'
  prefs: []
  type: TYPE_IMG
- en: The variance is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-13-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, since the transpose of a scalar is the same scalar, we can write the square
    term within the integral as the product of the scalar *lÃÇ^T*(![](../../OEBPS/Images/AR_x.png)
    - ![](../../OEBPS/Images/AR_micro.png)) and its transpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-13-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Using equation [2.10](02.xhtml#eq-mat-prod-transpose),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-13-d.png)'
  prefs: []
  type: TYPE_IMG
- en: Since
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-13-e.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.14
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we drop the *X* in parentheses and simply write ‚ÑÇ(*X*) as ‚ÑÇ.
    An equivalent way of looking at the covariance matrix of a *d*-dimensional random
    variable *X* taking vector values ![](../../OEBPS/Images/AR_x.png) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.15
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-15-a.png)'
  prefs: []
  type: TYPE_IMG
- en: is the co-variance of the *i*th and *j*th dimensions of the random vector ![](../../OEBPS/Images/AR_x.png).
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÑÇ(*X*) or ‚ÑÇ is the *covariance matrix* of the random variable *X*. A little
    thought reveals that equations [5.14](#eq-covariance) and [5.15](#eq-covar-eltwise)
    are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following things are noteworthy:'
  prefs: []
  type: TYPE_NORMAL
- en: From equation [5.14](#eq-covariance), ‚ÑÇ is the sum of the products of *d* √ó
    1 vectors (![](../../OEBPS/Images/AR_x.png)‚àí![](../../OEBPS/Images/AR_micro.png))
    and their transpose (![](../../OEBPS/Images/AR_x.png)‚àí![](../../OEBPS/Images/AR_micro.png))*^T*,
    1 √ó *d* vectors. Hence, ‚ÑÇ is a *d* √ó *d* matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This matrix is independent of the direction, *lÃÇ*, in which we are measuring
    the variance or spread. We can precompute ‚ÑÇ; then, when we need to measure the
    variance in any direction *lÃÇ*, we can evaluate the quadratic form *lÃÇ^T* ‚ÑÇ*lÃÇ*
    to obtain the variance in that direction. Thus ‚ÑÇ is a generic property of the
    distribution, much like ![](../../OEBPS/Images/AR_micro.png). ‚ÑÇ is called the
    *covariance* of the distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Covariance is the multivariate peer of the univariate entity variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That covariance is the multivariate analog of variance is evident by comparing
    the expressions in equations [5.13](../Text/05.xhtml#eq-variance) and [5.14](#eq-covariance).
  prefs: []
  type: TYPE_NORMAL
- en: Variance and expected value
  prefs: []
  type: TYPE_NORMAL
- en: 'As outlined previously, the variance is the expected value of the distance
    (*x* ‚àí *Œº*)¬≤ of sample points *x* from the mean *Œº*. This can be easily seen by
    comparing equations [5.13](../Text/05.xhtml#eq-variance), [5.10](#eq-continuous-expected-val),
    and [5.11](#eq-func-rv-expected-val) and leads to the following formula (where
    we use the principle of the expected value of linear combinations):'
  prefs: []
  type: TYPE_NORMAL
- en: '*var*(*X*) = ùîº((*X* ‚àí *Œº*)¬≤) = ùîº(*X*¬≤) ‚àí ùîº(2*ŒºX*) + ùîº(*Œº*¬≤)'
  prefs: []
  type: TYPE_NORMAL
- en: Since *Œº* is a constant, we can take it out of the expected value (a special
    case of the principal of the expected value of linear combinations). Thus we get
  prefs: []
  type: TYPE_NORMAL
- en: '*var*(*X*) = ùîº(*X*¬≤) ‚àí 2*Œº*ùîº(*X*) + *Œº*¬≤ùîº(1)'
  prefs: []
  type: TYPE_NORMAL
- en: But *Œº* = ùîº(*X*). Also, the expected value of a constant is that constant. So,
    ùîº(1) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: '*var*(*X*) = ùîº(*X*¬≤) ‚àí 2*Œº*¬≤ + *Œº*¬≤ùîº(1) = ùîº(*X*¬≤) ‚àí *Œº*¬≤'
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '*var*(*X*) = ùîº(*X*¬≤) ‚àí ùîº(*X*)¬≤'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.16
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 Sampling from a distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Drawing a sample from the probability distribution of a random variable yields
    an arbitrary value from the set of possible values. If we draw many samples, the
    higher-probability values show up more often than lower-probability values. The
    sampled points form a cloud in the domain of possible values, and the region where
    the probabilities are higher is more densely populated than lower-probability
    regions. In other words, in a sample point cloud, higher-probability values are
    overrepresented. Thus a collection of sample points is often referred to as a
    *sample point cloud*. The hope, of course, is that the sample point cloud is a
    good representation of the entire population so that analyzing the points in the
    cloud will yield insights about the entire population. In univariate cases, the
    sample value is a scalar and represented by a point on the number line. In multivariate
    cases, the sample value is a vector and represented as a point in a higher-dimensional
    space.
  prefs: []
  type: TYPE_NORMAL
- en: It is often useful to compute aggregate statistics (such as the mean and variance)
    to describe the population. If we know a distribution, we can use closed-form
    expressions to obtain these properties. Many standard distributions and closed-form
    equations for obtaining their means and variance are discussed in section [5.9](#sec-famous-distr).
    But often, we don‚Äôt know the underlying distribution. Under those circumstances,
    the sample mean and sample variance can be used. Given a set of *n* samples *X*
    = ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2]‚ãØ![](../../OEBPS/Images/AR_x.png)*[n]*
    from any distribution, the sample mean and variance are computed as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-16-a.png)'
  prefs: []
  type: TYPE_IMG
- en: In some situations, like Gaussian distributions (which we discuss shortly),
    it can be theoretically proved that the sample mean and variance are optimal (the
    best possible guesses of the true mean and variance, given the sampled data).
    Also, the sample mean approaches the true mean as the number of samples increases,
    and with enough samples, we get a pretty good approximation of the true mean.
    In the next subsection, we learn more about how much is ‚Äúenough.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: 'Law of large numbers: How many samples are enough?'
  prefs: []
  type: TYPE_NORMAL
- en: Informally speaking, the law of large numbers says that if we draw a large number
    of sample values from a probability distribution, their average should be close
    to the expected value of the distribution. In the limit, the average over an infinite
    number of samples will match the mean.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we cannot draw an infinite number of samples, so there is no guarantee
    that the sample mean will coincide with the expected value (true mean) in real-life
    sampling. But if the number of samples is large, they will not be too different.
    This is not a matter of mere theory. Casinos design games where the probability
    of the house winning a bet against the guest is slightly higher than the probability
    of the guest winning. The expected value of the outcome is that the casino wins
    rather than the guest. Over the very large number of bets placed in a casino,
    this is exactly what happens‚Äîand that is why casinos make money on the whole,
    even though they may lose individual bets.
  prefs: []
  type: TYPE_NORMAL
- en: 'How many samples is ‚Äúa large number of samples?‚Äù Well, it is not defined precisely.
    But one thing is known: if the variance is larger, more samples need to be drawn
    to make the law of large numbers apply.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs illustrate this with an example. Consider a betting game. Suppose that
    the famous soccer club FC Barcelona, for unknown reasons, has agreed to play a
    very large number of matches against the Machine Learning Experts‚Äô Soccer Club
    of Silicon Valley. We can place a bet of $100 on a team. If that team wins, we
    get back $200: that is, we make $100\. If that team loses, we lose the bet: that
    is, we make ‚Äì$100\. The betting game is happening in a country where nobody knows
    anything about the reputations of these clubs. A bettor bets on FC Barcelona in
    the first game and wins $100\. Based on this one observation, can the bettor say
    that by betting on Barcelona, they expect to win $100 every time? Obviously not.'
  prefs: []
  type: TYPE_NORMAL
- en: But suppose the bettor places 100 bets and wins $100 99 times and loses $100
    once. Now the bettor can expect with some confidence that they will win $100 (or
    close to it) by betting on Barcelona. Based on these observations, the sample
    mean winnings from a bet on FC Barcelona are 0.99 √ó (100) + 0.01 √ó (‚àí100) = 98.
    The sample standard deviation is ‚àö(.99 √ó (98 - 100)¬≤ + 0.01 √ó (98 - (-100))¬≤)
    = 19.8997. Relative to the sample mean, the sample standard deviation is 19.8997/98
    = 0.203.
  prefs: []
  type: TYPE_NORMAL
- en: Next, consider the same game, except now FC Barcelona is playing the Real Madrid
    football club. Since the two teams are evenly matched (the theoretical win probability
    of Barcelona is 0.5), the results are no longer one-sided. Suppose that after
    100 games, FC Barcelona has won 60 times and Real Madrid has won 40 times. The
    sample mean winnings on a Barcelona bet are 0.6 √ó (100) + 0.4 √ó (‚àí100) = 20. The
    sample standard deviation is ‚àö(.6 √ó (20 - 100)¬≤ + 0.4 √ó (20 - (-100))¬≤) = 97.9795.
    Relative to the sample mean, the sample standard deviation is 97.9795/20 = 4.89897.
    This is a much larger number than the previous 0.203. In this case, even after
    100 trials, a bettor cannot be very confident in predicting that the expected
    win is the sample mean, $20.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall intuition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If we take a sufficiently large number of samples, their average is close to
    the expected value. The exact definition of what constitutes a ‚Äúsufficiently large‚Äù
    number of samples is not known. However, the larger the variance (relative to
    the mean), the more samples are needed.
  prefs: []
  type: TYPE_NORMAL
- en: 5.9 Some famous probability distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce some probability distributions and density functions
    often used in deep learning. We will use PyTorch code snippets to demonstrate
    how to set up, sample, and compute properties like expected values, variance/covariance,
    and so on for each distribution. Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In the code snippets, for every distribution, we evaluate the probability using
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PyTorch `distributions` function call
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A raw evaluation from the formula (to understand the math)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both should yield the same result. In practice, you should use the PyTorch `distributions`
    function call instead of the raw formula.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the code snippets, for every distribution,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluate the theoretical mean and variance using a PyTorch `distributions`
    function call.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluate the sample mean and variance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When the sample set is large enough, the sample mean and theoretical mean should
    be close. Ditto for variance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NOTE Fully functional code for these distributions, executable via Jupyter Notebook,
    can be found at [http://mng.bz/8NVg](http://mng.bz/8NVg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another point to remember: In machine learning, we often work with the logarithm
    of the probability. Since the popular distributions are exponential, this leads
    to simpler computations. With that, let‚Äôs dive into the probability distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.1 Uniform random distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider a continuous random variable *x* that can take any value from a fixed
    compact range, say [*a*, *b*], *with equal probability, while the probability
    of x taking a value outside the range is zero*. The corresponding *p*(*x*) is
    a uniform probability distribution. Formally stated,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.17
  prefs: []
  type: TYPE_NORMAL
- en: Equation [5.17](#eq-uniform-random-univar) means *p*(*x*) is constant, 1/*b-a*,
    for *x* between *a* and *b* and zero for other values of *x*. Note how the value
    of the constant is cleverly chosen to make the total area under the curve 1. This
    equation is depicted graphically in figure [5.4](#fig-univar-uniform-distr), and
    listing 5.1 shows the PyTorch code for the log probability of a univariate uniform
    random distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F04_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 Univariate (single-variable) uniform random probability density function.
    Probability *p*(*x*) is constant, 0.05, in the interval [‚àí10,10] and zero everywhere
    outside the interval. Thus it depicts equation [5.17](#eq-uniform-random-univar)
    with *b* = 10, *a* = ‚àí10. The area under the curve is the area of the shaded rectangle
    of width 20 and height 0.05, 20 √ó 0.05 = 1. The thin rectangle depicts an infinitesimally
    small interval corresponding to event *E* = {*x* ‚â§ *X* < *x* + *Œ¥x*}. If we draw
    a random sample *x* from this distribution, the probability that the value of
    the sample is between, say, 4 and 4 + *Œ¥x*, with *Œ¥x* ‚Üí 0, is *p*(4) = 0.05. The
    probability that the value of the sample is between, say, 15 and 15 + *Œ¥x*, with
    *Œ¥x* ‚Üí 0, is *p*(15) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Log probability of a univariate uniform random distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Imports a PyTorch uniform distribution
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Sets the distribution parameters
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Instantiates a uniform distribution object
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Instantiates a single-point test dataset
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Evaluates the probability using PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Evaluates the probability using the formula
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Asserts that the probabilities match
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the uniform distribution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/E2Jr](http://mng.bz/E2Jr).
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a uniform distribution
  prefs: []
  type: TYPE_NORMAL
- en: We do this for the univariate case, although the computations can be easily
    extended to the multivariate case. Substituting the probability density function
    from equation [5.17](#eq-uniform-random-univar) into the expression for the expected
    value for a continuous variable, equation [5.10](#eq-continuous-expected-val),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.18
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The limits of integration changed because *p*(*x*) is zero outside the
    interval [*a*, *b*].
  prefs: []
  type: TYPE_NORMAL
- en: Overall, equation [5.18](#eq-continuous-uniform-expected-val) agrees with our
    intuition. The expected value is right in the middle of the uniform interval,
    as shown in figure [5.5](#fig-univar-uniform-distr-with-expectedvalue).
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a uniform distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at figure [5.5](#fig-univar-uniform-distr-with-expectedvalue), it
    is intuitively obvious that the packing density of the samples is related to the
    width of the rectangle. The smaller the width, the tighter the packing and the
    smaller the variance, and vice versa. Let‚Äôs see if the math supports that intuition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-19.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F05_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 Univariate (single-variable) uniform random probability density function.
    The solid line in the middle indicates the expected value. Interactive visualizations
    (where ou can change the parameters and observe how the graph changes as a result)
    can be found at [http://mng.bz/E2Jr](http://mng.bz/E2Jr).
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5.5](#fig-univar-uniform-distr-with-expectedvalue) shows that the variance
    in equation [5.19](#eq-uniform-var) is proportional to the square of the width
    of the rectangle: that is, (*b* ‚àí *a*)¬≤.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is the PyTorch code for the mean and variance of a uniform random distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 Mean and variance of a uniform random distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Number of sample points
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° 100000 √ó 1
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Obtains samples from ufm_dist instantiated in listing [5.1](#lst-ufm-dist-logprob)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Sample mean
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Mean via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Sample variance
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Variance via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate uniform distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Uniform distributions also can be multivariate. In that case, the random variable
    is a vector, ![](../../OEBPS/Images/AR_x.png) not a single value, but a sequence
    of values). Its domain is a multidimensional volume instead of the *X*-axis, and
    the graph has more than two dimensions. For example, this is a two-variable uniform
    random distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-20.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.20
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, (*x*, ![](../../OEBPS/Images/AR_y.png)) ‚àà [*a*[1], *b*[1]] √ó [*a*[2],
    *b*[2]] indicates a rectangular domain on the two-dimensional *XY* plane where
    *x* lies between *a*[1] and *b*[1] and ![](../../OEBPS/Images/AR_y.png) lies between
    *a*[2] and *b*[2]. Equation [5.20](#eq-uniform-random-bivar) is shown graphically
    in figure [5.6](#fig-bivar-uniform-distr). In the general multidimensional case,:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-21.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F06_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Bivariate uniform random probability density The probability *p*(*x*,
    *y*) is constant, 0.0025, in the domain (*x*, *y*) ‚àà [‚àí10,10] √ó [‚àí10,10] and zero
    everywhere outside the interval. The volume of the box of width 20 √ó 20 and height
    0.0025, 20 * 20 * 0.0025 = 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *V* is the volume of the hyperdimensional box with base *D*. Equation
    [5.21](#eq-uniform-random-multivar) means *p*(![](../../OEBPS/Images/AR_x.png))
    is constant for ![](../../OEBPS/Images/AR_x.png) in the domain *D* and zero for
    other values of *x*. When nonzero, it has a constant value, the inverse of the
    volume *V*: this makes the total volume under the density function 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.2 Gaussian (normal) distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is probably the most famous distribution in the world. Let‚Äôs consider,
    one more time, the weights of adult residents of Statsville. If Statsville is
    anything like a real city, the likeliest weight is around 75 kg: the largest percentage
    of the population will weigh this much. Weights near this value (say 70 or 80
    kg) will also be quite likely, although slightly less likely than 75 kg. Weights
    further away from 75 kg are still less likely, and so on. The further we go from
    75 kg, the lower the percentage of the population with that weight. *Outlier*
    values like 40 and 110 kg are unlikely. Informally speaking, a Gaussian probability
    density function looks like a *bell-shaped curve*. The central value has the highest
    probability. The probability falls gradually as we move away from the center.
    In theory, however, it never disappears completely (the function *p*(*x*) never
    becomes equal to 0), although it becomes almost zero for all practical purposes.
    This behavior is described in mathematics as *asymptotically approaching zero*.
    Figure [5.7](#fig-univar-gaussian-distr) shows a Gaussian probability density
    function. Formally,'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-22.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F07_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7 Univariate Gaussian random probability density function, *Œº* = 0
    and *œÉ* = 4. The bell-shaped curve is highest at the center and decreases more
    and more as we move away from the center, approaching zero asymptotically. The
    value *x* = 0 has the highest probability, corresponding to the center of the
    probability density function. Note that the curve is symmetric. Thus, for instance,
    the probability of a random sample being in the vicinity of ‚àí5 is the same as
    that of 5 (0.04): that is, *p*(‚àí5) = *p*(5) = 0.04. An interactive visualization
    (where you can change the parameters and observe how the graph changes as a result)
    can be found at [http://mng.bz/NYJX](http://mng.bz/NYJX).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *Œº* and *œÉ* are parameters; *Œº* corresponds to the center (for example,
    in figure [5.7](#fig-univar-gaussian-distr), *Œº* = 0). The parameter *œÉ* controls
    the width of the bell. A larger *œÉ* implies that *p*(*x*) falls more slowly as
    we move away from the center.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gaussian (normal) probability density function is so popular that we have
    a special symbol for it: ùí©(*x*, *Œº*, *œÉ*¬≤). It can be proved (but doing so is
    exceedingly tedious, so we will skip the proof here) that'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-22-a.png)'
  prefs: []
  type: TYPE_IMG
- en: This establishes that ùí©(*x*;*Œº*, *œÉ*¬≤) is a true probability (satisfying the
    sum rule in equation [5.7](#eq-continuous-vec-prob-sum)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 Log probability of a univariate normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Imports a PyTorch univariate normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Sets the distribution params
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Instantiates a univariate normal distribution object
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Instantiates a single-point test dataset
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Evaluates the probability using PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Evaluates the probability using the formula
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Asserts that the probabilities match
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this normal distribution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/NYJX](http://mng.bz/NYJX).
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Gaussian
  prefs: []
  type: TYPE_NORMAL
- en: A Gaussian distribution can also be multivariate. Then the random variable *x*
    is a vector ![](../../OEBPS/Images/AR_x.png), as usual. The parameter *Œº* also
    becomes a vector ![](../../OEBPS/Images/AR_micro.png), and the parameter *œÉ* becomes
    a matrix Œ£. As in the univariate case, these parameters are related to the expected
    value and variance. The Gaussian multivariate probability distribution function
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-23.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.23
  prefs: []
  type: TYPE_NORMAL
- en: Equation [5.23](../Text/05.xhtml#eq-multivar-normal) describes the probability
    density function for the random vector ![](../../OEBPS/Images/AR_x.png) to lie
    within the infinitesimally small volume with dimensions *Œ¥*![](../../OEBPS/Images/AR_x.png)
    around the point ![](../../OEBPS/Images/AR_x.png). (Imagine a tiny box (cuboid)
    whose sides are successive elements of *Œ¥*![](../../OEBPS/Images/AR_x.png), with
    the top-left corner of the box at ![](../../OEBPS/Images/AR_x.png).) The vector
    ![](../../OEBPS/Images/AR_micro.png) and the matrix Œ£ are parameters. As in the
    univariate case, ![](../../OEBPS/Images/AR_micro.png) corresponds to the most
    likely value of the random vector. Figure [5.8](#fig-multivar-gaussian-distr)
    shows the Gaussian normal) distribution with two variables in three dimensions.
    The shape of the base of the bell is controlled by the parameter Œ£.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 Log probability of a multivariate normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Imports a PyTorch multivariate normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Sets the distribution params
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Instantiates a multivariate normal distribution object
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Instantiates a single point test dataset
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Evaluates the probability using PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Evaluates the probability using the formula
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Asserts that the probabilities match
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F08_Chaudhury.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8 Bivariate Gaussian random probability density function. It is a
    bell-shaped surface: highest at the center and decreasing as we move away from
    the center, approaching zero asymptotically. *x* = 0, ![](../../OEBPS/Images/AR_y.png)
    = 0 has the highest probability, corresponding to the center of the probability
    density function. The bell has a circular base, and the Œ£ matrix is a scalar multiple
    of the identity matrix ùïÄ. An interactive visualization (where you can change the
    parameters and observe how the graph changes as a result) can be found at [http://mng.bz/NYJX](http://mng.bz/NYJX).'
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a Gaussian distribution
  prefs: []
  type: TYPE_NORMAL
- en: Substituting the probability density function from equation [5.22](../Text/05.xhtml#eq-univar-normal)
    into the expression for the expected value of a continuous variable, equation
    [5.10](#eq-continuous-expected-val), we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-23-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting ![](../../OEBPS/Images/eq_05-23-b.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-23-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting *u* = *y*¬≤ and using equation [5.6](#eq-continuous-prob-sum)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-23-d.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the limits of the integral in the first term are identical. This is
    because *u* = *y*¬≤ ‚Üí‚àû whether *y* ‚Üí ‚àû or *y* ‚Üí ‚àí‚àû. But an integral with the same
    lower and upper limits is zero. Thus the first term is zero. Hence,
  prefs: []
  type: TYPE_NORMAL
- en: ùîº*[gaussian]*(*X*) = *Œº*
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.24
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, this makes perfect sense. The probability density ![](../../OEBPS/Images/eq_05-24-a2.png)
    peaks (maximizes) at *x* = *Œº*. At this *x*, the exponent becomes zero, which
    makes the term ![](../../OEBPS/Images/eq_05-24-b2.png) attain its maximum possible
    value of 1. This is right in the middle of the bell, as shown in figure [5.10](#fig-univar-normal-distr-with-expectedvalue).
    And, of course, the expected value coincides with the middle value if the density
    is symmetric and peaks in the middle. Analogously, in the multivariate case, the
    Gaussian multidimensional random variable *X* that takes vector values ![](../../OEBPS/Images/AR_x.png)
    in the *d*-dimensional domain ‚Ñù*^d* (that is, ![](../../OEBPS/Images/AR_x.png)
    ‚àà ‚Ñù*^d*) has an expected value
  prefs: []
  type: TYPE_NORMAL
- en: ùîº*[gaussian]*(*X*) = ![](../../OEBPS/Images/AR_micro.png)
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F09_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 Univariate (single-variable) normal (Gaussian) random probability
    density function, *Œº* = 0 and *œÉ* = 4. The solid line in the middle indicates
    the expected value.
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a Gaussian distribution
  prefs: []
  type: TYPE_NORMAL
- en: The variance of the Gaussian distribution is obtained by substituting equation
    [5.22](../Text/05.xhtml#eq-univar-normal) in the integral form of equation [5.13](../Text/05.xhtml#eq-variance).
    The mathematical derivation is shown in the book‚Äôs appendix; here we only state
    the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance of a Gaussian distribution with probability density function ![](../../OEBPS/Images/eq_05-24-c.png)
    is *œÉ*¬≤, and the standard deviation is the square root of that (*œÉ*). This makes
    intuitive sense. *œÉ* appears in the denominator of a negative exponent in the
    expression for the probability density function ![](../../OEBPS/Images/eq_05-24-d.png).
    As such, *p*(*x*) is an increasing function of *œÉ*: that is, for a given *x* and
    *Œº*, a larger *œÉ* implies a larger *p*(*x*). In other words, a larger *œÉ* implies
    that the probability decays more slowly as we move away from the center: a fatter
    bell curve, a bigger spread, and hence a larger variance. Figure [5.10](../Text/05.xhtml#fig-multi-univar-gauss)
    depicts this.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F10a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Different *Œº*s but the same *œÉ*s.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F10b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The same *Œº*s but different *œÉ*s.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 Gaussian densities with varying *Œº*s and *œÉ*s. Changing *Œº* shifts
    the center of the curve. A larger *œÉ* (variance) implies a fatter bell ‚áí more
    spread. Note that fatter curves are smaller in height as the total area under
    the curve must be 1.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 Mean and variance of a univariate Gaussian
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Number of sample points
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° 100000 √ó 1
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Obtains samples from uvn_dist
  prefs: []
  type: TYPE_NORMAL
- en: instantiated in listing [5.3](#lst-univar-normal-dist-logprob)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Sample mean
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Mean via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Sample variance
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Variance via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: Covariance of a multivariate Gaussian distribution and geometry of the bell
    surface
  prefs: []
  type: TYPE_NORMAL
- en: Comparing equation [5.22](../Text/05.xhtml#eq-univar-normal) for a univariate
    Gaussian probability density with equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    for a multivariate Gaussian probability density, we intuitively feel that the
    matrix Œ£ is the multivariate peer of the univariate variance *œÉ*¬≤. Indeed it is.
    Formally, for a multivariate Gaussian random variable with a probability distribution
    given in equation [5.23](../Text/05.xhtml#eq-multivar-normal), the covariance
    matrix is given by the equation
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÑÇ*[gaussian]*(*X*) = **Œ£**
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.26
  prefs: []
  type: TYPE_NORMAL
- en: As shown in table [5.11](#fig-gaussian-multivar-pointclouds), Œ£ regulates the
    shape of the base of the bell-shaped probability density function.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to see that the exponent in equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    is a quadratic form (introduced in section [4.2](../Text/04.xhtml#sec-quadratic-form)).
    As such, it defines a hyper-ellipse, as shown in figure [5.11](#fig-gaussian-multivar-pointclouds)
    and section [2.17](02.xhtml#sec-hyper-ellipse). All the properties of quadratic
    forms and hyper-ellipses apply here.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Mean and variance of a multivariate normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Number of sample points
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° 100000 √ó 1
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Obtains samples from uvn_dist
  prefs: []
  type: TYPE_NORMAL
- en: instantiated in listing [5.4](#lst-multivar-normal-dist-logprob)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Sample mean
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Mean via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Sample variance
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Variance via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs look at the geometric properties of the Gaussian covariance matrix Œ£.
    Consider a 2D version of equation [5.23](../Text/05.xhtml#eq-multivar-normal).
    We rewrite ![](../../OEBPS/Images/eq_05-26-a2.png) and ![](../../OEBPS/Images/eq_05-26-b2.png)‚Äî2D
    vectors both. Also ![](../../OEBPS/Images/eq_05-26-c2.png)‚Äîa 2 √ó 2 matrix. The
    probability density function from equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-27.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.27
  prefs: []
  type: TYPE_NORMAL
- en: (Use what you learned in chapter [3](../Text/03.xhtml#chapter-intro-vec-mat)
    to satisfy yourself that equation [5.27](#eq-bivar-normal) is a 2D analog of equation
    [5.23](../Text/05.xhtml#eq-multivar-normal).)
  prefs: []
  type: TYPE_NORMAL
- en: If we plot the surface *p*(*x*, *y*) against (*x*, *y*), it looks like a bell
    in 3D space. The shape of the bell‚Äôs base, on the (*x*, *y*) plane, is governed
    by the 2 √ó 2 matrix Œ£. In particular,
  prefs: []
  type: TYPE_NORMAL
- en: If Œ£ is a diagonal matrix with equal diagonal elements, the bell is symmetric
    in all directions, and its base is circular.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If Œ£ is a diagonal matrix with unequal diagonal elements, the base of the bell
    is elliptical. The axes of the ellipse are aligned with the coordinate axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a general Œ£ matrix, the base of the bell is elliptical. The axes of the
    ellipse are not necessarily aligned with the coordinate axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenvectors of Œ£ yield the axes of the elliptical base of the bell surface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, if we sample the distribution from equation [5.27](#eq-bivar-normal), we
    get a set of points (*x*, *y*) on the base plane of the surface shown in figure
    [5.8](#fig-multivar-gaussian-distr). The taller the *z* coordinate (depicting
    *p*(*x*, *y*)) of the surface at a point (*x*, *y*), the greater its probability
    of being selected in the sampling. If we draw a large number of samples, the corresponding
    point cloud will look more or less like the base of the bell surface.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [5.11](#fig-gaussian-multivar-pointclouds) shows various point clouds
    formed by sampling Gaussian distributions with different covariance matrices Œ£.
    Compare it to figure [5.10](../Text/05.xhtml#fig-multi-univar-gauss).
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometry of sampled point clouds: Covariance and direction of maximum or minimum
    spread'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that if a multivariate distribution has a covariance matrix ‚ÑÇ,
    its variance (spread) in any specific direction *lÃÇ* is *lÃÇ^T* ‚ÑÇ*lÃÇ*. What is
    the direction of maximum spread?
  prefs: []
  type: TYPE_NORMAL
- en: Asking this is the same as asking ‚ÄúWhat direction *lÃÇ* maximizes the quadratic
    form *lÃÇ^T* ‚ÑÇ*lÃÇ*?‚Äù In section [4.2](../Text/04.xhtml#sec-quadratic-form), we
    saw that a quadratic form like this is maximized or minimized when the direction
    *lÃÇ* is aligned with the eigenvector corresponding to the maximum or minimum eigenvalue
    of the matrix ‚ÑÇ. Thus, *the maximum spread of a distribution occurs along the
    eigenvector of the covariance matrix corresponding to its maximum eigenvalue*.
    This led to the PCA technique in section [4.4](../Text/04.xhtml#sec-pca).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discuss the covariance of the Gaussian distribution and geometry of
    the point cloud formed by sampling a multivariate Gaussian a large number of times.
    You may want to take a look at figure [5.11](#fig-gaussian-multivar-pointclouds),
    which shows various point clouds formed by sampling Gaussian distributions with
    different covariance matrices Œ£.
  prefs: []
  type: TYPE_NORMAL
- en: '![(a)](../../OEBPS/Images/CH05_F11a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) ![](../../OEBPS/Images/fig_05-11_a.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![(b)](../../OEBPS/Images/CH05_F11b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) ![](../../OEBPS/Images/fig_05-11_b.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![(c)](../../OEBPS/Images/CH05_F11c_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) ![](../../OEBPS/Images/fig_05-11_c.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![(d)](../../OEBPS/Images/CH05_F11d_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) ![](../../OEBPS/Images/fig_05-11_d.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 Point clouds formed by sampling multivariate Gaussians with the
    same ![](../../OEBPS/Images/AR_micro.png) = [0,0]*^T* but different Œ£s. These
    point clouds correspond to the bases of the bell curves for multivariate Gaussian
    probability densities. All the point clouds except (a) may be replaced by a univariate
    Gaussian after rotation to align the coordinate axes with the eigenvectors of
    Œ£ (dimensionality reduction). See sections [4.4](../Text/04.xhtml#sec-pca), [4.5](../Text/04.xhtml#sec-svd),
    and [4.6](../Text/04.xhtml#sec-lsa) for details. Interactive contour plots for
    the base of the bell curve can be found at [http://mng.bz/NYJX](http://mng.bz/NYJX).
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Gaussian point clouds and hyper-ellipses
  prefs: []
  type: TYPE_NORMAL
- en: The numerator of the exponential term in equation [5.23](../Text/05.xhtml#eq-multivar-normal),
    (![](../../OEBPS/Images/AR_x.png)‚àí![](../../OEBPS/Images/AR_micro.png))*^T***Œ£**^(‚àí1)(![](../../OEBPS/Images/AR_x.png)‚àí![](../../OEBPS/Images/AR_micro.png)),
    is a quadratic form as we discussed in section [4.2](../Text/04.xhtml#sec-quadratic-form).
    It should also remind you of the hyper-ellipse we looked at in section [2.17](02.xhtml#sec-hyper-ellipse),
    equation [2.33](02.xhtml#eq-hyper-ellipse), and equation [4.1](../Text/04.xhtml#eq-hyper-ellipse-again).
  prefs: []
  type: TYPE_NORMAL
- en: Now consider the plot of *p*(![](../../OEBPS/Images/AR_x.png)) against ![](../../OEBPS/Images/AR_x.png).
    This is a hypersurface in *n* + 1-dimensional space, where the random variable
    ![](../../OEBPS/Images/AR_x.png) is *n*-dimensional. For instance, if the random
    Gaussian variable ![](../../OEBPS/Images/AR_x.png) is 2D, the (![](../../OEBPS/Images/AR_x.png),
    *p*(![](../../OEBPS/Images/AR_x.png))) plot in 3D is as shown in figure [5.8](#fig-multivar-gaussian-distr).
    It is a bell-shaped surface. The hyper-ellipse corresponding to the quadratic
    form in the numerator of the probability density function in equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    governs the shape and size of the base of this bell.
  prefs: []
  type: TYPE_NORMAL
- en: If the matrix Œ£ is diagonal (with equal diagonal elements), the base is *circular*‚Äîthis
    is the special case shown in figure [5.8](#fig-multivar-gaussian-distr). Otherwise,
    the base of the bell is elliptic. The eigenvectors of the covariance matrix Œ£
    correspond to the directions of the axes of the elliptical base. The eigenvalues
    correspond to the lengths of the axes.
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.3 Binomial distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we have a database containing photos of people. Also, suppose we know
    that 20% of the photos contain a celebrity and the remaining 80% do not. If we
    randomly select three photos from this database, what is the probability that
    two of them contain a celebrity? This is the kind of problem the binomial distribution
    deals with.
  prefs: []
  type: TYPE_NORMAL
- en: In a computer vision-centric machine learning setting, we would probably inspect
    the selected photos and try to predict whether they contained a celebrity. But
    for now, let‚Äôs restrict ourselves to the simpler task of blindly predicting the
    chances from aggregate statistics.
  prefs: []
  type: TYPE_NORMAL
- en: If we select a single photo, the probability of it containing a celebrity is
    *œÄ* = 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE This has nothing to do with the natural number *œÄ* denoting the ratio of
    the circumference to the diameter of a circle. We are just reusing the symbol
    *œÄ* following popular convention.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability of the photo not containing a celebrity is 1 ‚àí *œÄ* = 0.8. From
    that, we can compute the probability of, say, the first two sampled photos containing
    a celebrity but the last one containing a non-celebrity: that is, the event {*S*,
    *S*, *F*} (where S denotes success in finding a celebrity and F denotes failure
    in finding a celebrity). Using equation [5.4](../Text/05.xhtml#eq-joint-prob-indep),
    the probability of the event {*S*, *S*, *F*} is *œÄ* √ó *œÄ* √ó (1‚àí*œÄ*) = 0.2 √ó 0.2
    √ó 0.8. However, many other combinations are also possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the possible combinations that can occur in three trials are shown in table
    [5.8](#tab-binomial-distr). In the table, event ids 3, 5, and 6 correspond to
    two successes and one failure. They occur with probabilities 0.8 √ó 0.2 √ó 0.2,
    0.2 √ó 0.8 √ó 0.2, and 0.2 √ó 0.2 √ó 0.8, respectively. If any one of them occurs,
    we have two celebrity photos in three trials. Thus, using equation [5.3](../Text/05.xhtml#eq-discrete-prob-sum),
    the overall probability of selecting two celebrity photos in three trials is the
    sum of these event probabilities: 0.8 √ó 0.2 √ó 0.2 + 0.2 √ó 0.8 √ó 0.2 + 0.2 √ó 0.2
    √ó 0.8 = 0.096.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.8 All possible combinations of three trials
  prefs: []
  type: TYPE_NORMAL
- en: '| Event Id | Event | Probability |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | {*F*, *F*, *F*} | (1‚àí*œÄ*) √ó (1‚àí*œÄ*) √ó (1‚àí*œÄ*) = 0.8 √ó 0.8 √ó 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | {*F*, *F*, *S*} | (1‚àí*œÄ*) √ó (1‚àí*œÄ*) √ó *œÄ* = 0.8 √ó 0.8 √ó 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | {*F*, *S*, *F*} | (1‚àí*œÄ*) √ó *œÄ* √ó (1‚àí*œÄ*) = 0.8 √ó 0.2 √ó 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | {*F*, *S*, *S*} | (1‚àí*œÄ*) √ó *œÄ* √ó *œÄ* = 0.8 √ó 0.2 √ó 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | {*S*, *F*, *F*} | *œÄ* √ó (1‚àí*œÄ*) √ó (1‚àí*œÄ*) = 0.2 √ó 0.8 √ó 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | {*S*, *F*, *S*} | *œÄ* √ó (1‚àí*œÄ*) √ó *œÄ* = 0.2 √ó 0.8 √ó 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | {*S*, *S*, *F*} | *œÄ* √ó *œÄ* √ó (1‚àí*œÄ*) = 0.2 √ó 0.2 √ó 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | {*S*, *S*, *S*} | *œÄ* √ó *œÄ* √ó *œÄ* = 0.2 √ó 0.2 √ó 0.2 |'
  prefs: []
  type: TYPE_TB
- en: 'In the general case, with more than three trials, it would be impossibly tedious
    to enumerate all the possible combinations of *success* and *failure* that can
    occur in a set of *n* trials. Fortunately, we can derive a formula. But before
    doing that, let‚Äôs state the task of a binomial distribution in more general terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a process that has a binary outcome (success or failure) in any given
    trial, and given that the probability of success in a trial is a known constant
    (say, *œÄ*), a binomial distribution deals with the probability of observing *k*
    successes in *n* trials of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine events with *n* successive items, where each individual item can be
    either *S* or *F*. Table [5.8](#tab-binomial-distr) shows such events with *n*
    = 3. Each item has two possible values (*S* or *F*), and there are *n* items.
    Hence, altogether there can be 2 √ó 2 √ó ‚ãØ2 = 2*^n* possible events.
  prefs: []
  type: TYPE_NORMAL
- en: We are only interested in events with *k* occurrences of *S* (and therefore
    (*n* ‚àí *k*) occurrences of *F*). How many of the *n* events are like that? Well,
    asking this is the same as asking how many ways we can choose *k* slots from a
    total of *n* possible slots. Another way to pose the same question is, ‚ÄúHow many
    different orderings of *n* items exist, where each item is either *S* or *F* and
    the total count of *S* is *k*?‚Äù The answer, from combination theory, is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-27-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Each of these events has a probability of *œÄ^k* √ó (1‚àí*œÄ*)^(*n* ‚àí *k*). Hence,
    the overall probability of *k* successes in *n* trials is ![](../../OEBPS/Images/eq_05-27-b-2.png).
  prefs: []
  type: TYPE_NORMAL
- en: Formally, if *X* is a random variable denoting the number of successes in *n*
    trials, with the probability of success in any single trial being some constant
    value *œÄ*,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-28.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.28
  prefs: []
  type: TYPE_NORMAL
- en: 'What values can *k* take? Of course, we cannot have more than *n* successes
    in *n* trials; therefore, the maximum possible value of *k* is *n*. All integer
    values between 0 and *n* are possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-28-a.png)'
  prefs: []
  type: TYPE_IMG
- en: The right-hand side is an expression for the generic term in the famous binomial
    expansion of (*a* + *b*)*^n* with *a* = *œÄ* and *b* = 1 ‚àí *œÄ*. Hence, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-29.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.29
  prefs: []
  type: TYPE_NORMAL
- en: This agrees with intuition, since given *n*, *k* can only take values 0, 1,
    ‚ãØ, *n*; the sum of the probabilities on the left-hand side of equation [5.29](#eq-eq-binom-sum)
    corresponds to a certain event with probability 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, plugging *n* = 3, *k* = 2, and *œÄ* = 0.2 into equation [5.28](#eq-binom-distr)
    yields 3!/2!1! (0.2)¬≤, (0.8)^(3-2) = 0.096: exactly what we get from explicit
    enumeration.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 Log probability of a binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Imports a PyTorch binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Sets the distribution params
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Instantiates a binomial distribution object
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Instantiates a single point test dataset
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Evaluates the probability using PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Evaluates the probability using formula
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Asserts that the probabilities match
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the binomial distribution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/DRJ0](http://mng.bz/DRJ0).
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that the binomial distribution deals with a random variable *X*
    that depicts the number of successes in *n* trials, where the probability of success
    in a given trial is a constant *œÄ* (again, this has nothing to do with the *œÄ*
    denoting the ratio of the circumference to the diameter of a circle). This *X*
    can take any integer value 0 to *n*. Hence,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-29-a.png)'
  prefs: []
  type: TYPE_IMG
- en: We can drop the first term, which has the multiplier *k* = 0. Thus we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-29-b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can factor *n*! = *n*(*n* ‚àí 1)! and *œÄ^k* = *œÄ* *œÄ*^(*k* ‚àí 1). Also, *n*
    ‚àí *k* = (*n* ‚àí 1) ‚àí (*k* ‚àí 1). This gives us
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-29-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting *j* for *k* ‚àí 1 and *m* for *n* ‚àí 1, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-30.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.30
  prefs: []
  type: TYPE_NORMAL
- en: The quantity within the summation is similar to that in equation [5.29](#eq-eq-binom-sum)
    (should sum to 1). This leaves us with
  prefs: []
  type: TYPE_NORMAL
- en: ùîº*[binomial]* (*X*) = *nœÄ*
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.31
  prefs: []
  type: TYPE_NORMAL
- en: Equation [5.31](#eq-binomial-expected-value) says that if *œÄ* is the probability
    of success in a single trial, then the expected number of successes in *n* trials
    is *n**œÄ*. For instance, if the probability of success in a single trial is 0.2,
    then the expected number of successes in 100 trials is 20‚Äîwhich is almost intuitively
    obvious.
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: The variance of a binomial random variable depicting the number of successes
    in *n* trials where the probability of success in a given trial is a constant
    *œÄ* is
  prefs: []
  type: TYPE_NORMAL
- en: '*var[binomial]* = *nœÄ* (1 ‚àí *œÄ*)'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.32
  prefs: []
  type: TYPE_NORMAL
- en: The proof follows the same lines as that of the expected value.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 Mean and variance of a binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Number of sample points
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° 100000 √ó 1
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Obtains samples from ufm_dist instantiated in listing [5.7](#lst-binom-dist-logprob)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Sample mean
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Mean via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Sample variance
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Variance via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.4 Multinomial distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider again the example problem we discussed in section [5.9.3](#sec-binomial-distr).
    We have a database of photos of people. But instead of two classes, celebrity
    and non-celebrity, we have four classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Photos of Albert Einstein (class 1): 10% of the photos'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photos of Marie Curie (class 2): 42% of the photos'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photos of Carl Friedrich Gauss (class 3): 4% of the photos'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other photos (class 4): 44% of the photos'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we randomly select a photo from the database (that is, perform a random trial),
  prefs: []
  type: TYPE_NORMAL
- en: The probability of selecting class 1 (picking an Einstein photo) is *œÄ*[1] =
    0.1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 2 (picking a Marie Curie photo) is *œÄ*[2]
    = 0.42.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 3 (picking a Gauss photo) is *œÄ*[3] = 0.04.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 4 (picking a photo of none of the above)
    is *œÄ*[4] = 0.44.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that *œÄ*[1] + *œÄ*[2] + *œÄ*[3] + *œÄ*[4] = 1. This is because the classes
    are mutually exclusive and exhaustive, so exactly one of these classes must occur
    in every trial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given all this, let‚Äôs ask the question: ‚ÄúWhat is the probability that in a
    set of 10 random trials, class 1 occurs 1 time, class 2 occurs 2 times, class
    3 occurs 1 time, and class 4 occurs the remaining 6 times?‚Äù This is the kind of
    problem multinomial distributions deal with.'
  prefs: []
  type: TYPE_NORMAL
- en: Formally,
  prefs: []
  type: TYPE_NORMAL
- en: Let *C*[1], *C*[2], ‚ãØ, *C[m]* be a set of *m* classes such that in any random
    trial, exactly one of these classes will be selected with the respective probabilities
    *œÄ*[1], *œÄ*[2], ‚ãØ, *œÄ[m]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let *X*[1], *X*[2], ‚ãØ, *X[m]* be a set of random variables. *X[i]* corresponds
    to the number of occurrences of class *C[i]* in a set of *n* trials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the multinomial probability function depicting the probability that class
    *C*[1] is selected *k*[1] times, class *C*[2] is selected *k*[2] times, and class
    *C*[3] is selected *k[m]* times is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-33.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.33
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-33-a.png)'
  prefs: []
  type: TYPE_IMG
- en: We can verify that for *m* = 2, this becomes the binomial distribution (equation
    [5.28](#eq-binom-distr)). A noteworthy point is that if we look at any one of
    the *m* variables *X*[1], *X*[2], ‚ãØ, *X[m]* individually, its distribution is
    binomial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs work out the final probability for the example we started with: the probability
    that in a set of 10 random trials, class 1 occurs 1 time, class 2 occurs 2 times,
    class 3 occurs 1 time, and class 4 occurs the remaining 6 times. This is'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-33-b.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 5.9 Log probability of a multinomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Imports a PyTorch multinomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Sets the distribution params
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Instantiates a multinomial dist object
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Instantiates a single-point test dataset
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Evaluates the probability using PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Evaluates the probability using formula
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Asserts that the probabilities match
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the multinomial distribution, executable via
    Jupyter Notebook, can be found at [http://mng.bz/l1gz](http://mng.bz/l1gz).
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a multinomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: Each of the random variables *X*[1], *X*[2], ‚ãØ, *X[m]* individually subscribes
    to a binomial distribution. Accordingly, following the binomial distribution expected
    value formula from equation [5.31](#eq-binomial-expected-value),
  prefs: []
  type: TYPE_NORMAL
- en: ùîº*[multinomial]*(*X[i]*) = *nœÄ[i]*
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.34
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a multinomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: The variation of the random variables *X*[1], *X*[2], ‚ãØ, *X[m]*, following the
    binomial distribution variance formula from equation [5.32](#eq-binomial-var),
    is
  prefs: []
  type: TYPE_NORMAL
- en: '*var[multinomial]*(*X[i]*) = *nœÄ[i]*(1‚àí*œÄ[i]*)'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 5.35
  prefs: []
  type: TYPE_NORMAL
- en: If each of the *X*[1], *X*[2], ‚ãØ, *X[m]* is a scalar, then we can think of a
    random vector ![](../../OEBPS/Images/eq_05-35-a.png). The expected value of such
    a random variable is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-35-b.png)'
  prefs: []
  type: TYPE_IMG
- en: and the covariance is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-36.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.36
  prefs: []
  type: TYPE_NORMAL
- en: where the diagonal terms are like the binomial variance *œÉ[ii]* = *nœÄ[i]*(1‚àí*œÄ[i]*)
    ‚àÄ*i* ‚àà [1, *m*] and the off-diagonal terms are *œÉ[ij]* = ‚àí*nœÄ[i]œÄ[j]* ‚àÄ(*i*, *j*)
    ‚àà [1, *m*] √ó [1, *m*]. The cross-covariance terms in the diagonal are negative
    because an increase in one element implies a decrease in the others.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 Mean and variance of a multinomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Number of sample points
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° 100000 √ó 1
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Obtains samples from ufm_dist instantiated in listing [5.9](#lst-multinom-dist-logprob)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Sample mean
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Mean via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Sample variance
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Variance via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.5 Bernoulli distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Bernoulli distribution is a special case of a binomial distribution where
    *n* = 1: that is, a single success-or-failure trial is performed. The probability
    of success is *œÄ*, and the probability of failure is 1 ‚àí *œÄ*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, let *X* be a discrete random variable that takes the value
    1 (success) with probability *œÄ* and the value 0 (failure) with probability 1
    ‚àí *œÄ*. The distribution of *X* is the Bernoulli distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*X* = 1) = *œÄ*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*X* = 0) = 1 - *œÄ*'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 Log probability of a Bernoulli distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Imports a PyTorch Bernoulli distribution
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Sets the distribution params
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Instantiates a Bernoulli distribution object
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Instantiates a single-point test dataset
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Evaluates the probability using PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Evaluates the probability using the formula
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Asserts that the probabilities match
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the Bernoulli distribution, executable via Jupyter
    Notebook, can be found at [http://mng.bz/BRwq](http://mng.bz/BRwq).
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a Bernoulli distribution
  prefs: []
  type: TYPE_NORMAL
- en: If there are only two classes, *success* and *failure*, we cannot speak directly
    of an expected value. If we run, say, 100 trials and get 30 *successes* and 70
    *failures*, the average is 0.3 *success*, which is not a valid outcome. We cannot
    have fractional *success* or *failure* in this binary system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can, however, talk about the expected value of a Bernoulli distribution
    if we introduce an artificial construct. We assign numerical values to these binary
    entities: *success* = 1 and *failure* = 0. Then the expected value of *X* is'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-37.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.37
  prefs: []
  type: TYPE_NORMAL
- en: Variance of a Bernoulli distribution
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if we assign numerical values to these binary entities‚Äî*success*
    = 1 and *failure* = 0‚Äîthe variance of the Bernoulli distribution is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-38.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.38
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 Mean and variance of a Bernoulli distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Number of sample points
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° 100000 √ó 1
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Obtains samples from ufm_dist instantiated in listing [5.11](#lst-bern-dist-logprob)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Sample mean
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ Mean via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë• Sample variance
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¶ Variance via PyTorch function
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.6 Categorical distribution and one-hot vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider again the example problem introduced in section [5.9.4](#sec-multinomial-distr).
    We have a database with four classes of photos:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Photos of Albert Einstein (class 1): 10%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photos of Marie Curie (class 2): 42%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photos of Carl Friedrich Gauss (class 3): 4%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other photos (class 4): 44%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we randomly select a photo from the database,
  prefs: []
  type: TYPE_NORMAL
- en: The probability of selecting class 1 is *œÄ*[1] = 0.1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 2 is *œÄ*[2] = 0.42.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 3 is *œÄ*[3] = 0.04.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of selecting class 4 is *œÄ*[4] = 0.44.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As before, *œÄ*[1] + *œÄ*[2] + *œÄ*[3] + *œÄ*[4] = 1 because the classes are mutually
    exclusive and exhaustive so exactly one class must occur in each trial.
  prefs: []
  type: TYPE_NORMAL
- en: In multinomial distribution, we performed *n* trials and asked how many times
    each specific class would occur. What if we perform only one trial? Then we get
    categorical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical distribution is a special case of multinomial distribution (with
    the number of trials *n* = 1). It is also an extension of the Bernoulli distribution
    where instead of just two classes, *success* and *failure*, we can have an arbitrary
    number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: Formally,
  prefs: []
  type: TYPE_NORMAL
- en: Let *C*[1], *C*[2], ‚ãØ, *C[m]* be a set of *m* classes such that in any random
    trial, exactly one of these classes will be selected, with the respective probabilities
    *œÄ*[1], *œÄ*[2], ‚ãØ, *œÄ[m]*. We sometimes refer to the probabilities of all the
    classes together as a vector ![](../../OEBPS/Images/eq_05-38-a2.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let *X*[1], *X*[2], ‚ãØ, *X[m]* be a set of random variables. *X[i]* corresponds
    to the number of occurrences of class *C[i]* in a set of *n* trials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the categorical probability function depicts the probability of each of
    the classes *C*[1], *C*[2], and so on, in a single trial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot vector
  prefs: []
  type: TYPE_NORMAL
- en: We can use a one-hot vector to compactly express the outcome of a single trial
    of categorical distribution. This is a vector with *m* elements. Exactly a single
    element is 1; all other elements are 0. The 1 indicates which of the *m* possible
    classes occurred in that specific trial. For instance, in the example with the
    database of photos, if a Marie Curie photo comes up in a given trial, the corresponding
    one-hot vector is ![](../../OEBPS/Images/eq_05-38-b2.png).
  prefs: []
  type: TYPE_NORMAL
- en: Probability of a categorical distribution
  prefs: []
  type: TYPE_NORMAL
- en: We can think of a one-hot vector *X* as a random variable with a categorical
    distribution. Note that each individual class follows a Bernoulli distribution.
    The probability of class *C[i]* occurring in any given trial is
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*C[i]*) = *œÄ[i]*'
  prefs: []
  type: TYPE_NORMAL
- en: We can express the probability distribution of all the classes together compactly
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-39.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.39
  prefs: []
  type: TYPE_NORMAL
- en: where ![](../../OEBPS/Images/AR_x.png) is a one-hot vector. Note that all but
    one of the powers in equation [5.39](#eq-prob-categorical-distr) is 0; hence the
    corresponding factor evaluates to 1\. The remaining power is 1. Hence the overall
    probability always evaluates to *œÄ[i]*, where *i* is the index of the class that
    occurred in the trial.
  prefs: []
  type: TYPE_NORMAL
- en: Expected value of a categorical distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are talking about classes, expected value and variance do not make
    sense in this context. We encountered a similar situation with the Bernoulli distribution.
    We assigned numerical values to each class and somewhat artificially defined the
    expected value and variance. A similar idea can also be applied here: we can talk
    about the expected value and variance of the one-hot vector (which consists of
    numerical values 0 and 1). But it remains an artificial construct.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a random variable *X* whose instances are one-hot vectors ![](../../OEBPS/Images/AR_x.png)
    following a categorical distribution with *m* classes with respective probabilities
    *œÄ*[1], *œÄ*[2], ‚ãØ, *œÄ[m]*,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_05-40.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5.40
  prefs: []
  type: TYPE_NORMAL
- en: We skip the variance of a categorical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we first looked at probability and statistics from a machine
    learning point of view. We also introduced the PyTorch `distributions` package
    and illustrated each concept with PyTorch `distributions` code samples immediately
    following the math.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of a specific event type is defined as the fraction of the total
    population of all possible events occupied by events of that specific type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A random variable is a variable that can assume any value from a predefined
    range of possible values. Random variables can be discrete or continuous. A probability
    is associated with a discrete random variable taking a specific value. A probability
    is also associated with a continuous random variable taking a value in an infinitesimally
    small range around a specific value, called its probability density at that value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum rule of probabilities states that the sum of the probabilities of a
    set of mutually exclusive events is the probability of one or another of them
    occurring. If the set of events is exhaustive that is, among them, they cover
    the entire space of possible events), then their sum is 1 because one or another
    of them must occur. For continuous random variables, integrating the probability
    density function over the domain of possible values yields 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The joint probability of a set of events is the probability of all those events
    occurring together. If the events are independent, the joint probability is the
    product of their individual probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing a sample from the probability distribution of a random variable returns
    an arbitrary value from the set of possible values. If we draw many samples, the
    higher-probability values show up more often than the lower-probability values.
    The sampled points occupy a region (called the sample point cloud) in the domain
    of possible values. In a sample point cloud, the region where the probabilities
    are higher is more densely populated than lower-probability regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expected value of a random variable is the average of the values of points
    in a very large (approaching infinity) sample cloud. It is equal to the weighted
    sum of all possible values of the random variable, where the weight for each value
    is its probability of occurrence. For continuous random variables, this boils
    down to integration‚Äîover the domain of possible values‚Äîof the product of the random
    variable‚Äôs value and the probability density. The physical significance of the
    expected value is that it is a single-point representation of the entire distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance of a random variable is the square root of the average squared
    distances of the sample point values from the mean in a very large (approaching
    infinity) sample cloud. It is equal to the weighted sum of the squared distances
    of all possible values of the random variable from the mean. The weight for each
    value is its probability of occurrence. For continuous random variables, this
    boils down to integration‚Äîover the domain of possible values‚Äîof the product of
    the squared distance of the random variable‚Äôs value from the mean and the probability
    density. Physically, the variance is a measure of the spread of the points in
    the distribution around its mean. In the multivariate case, this spread depends
    on the direction. Since there are infinite possible directions in a space with
    two or more dimensions, we cannot speak of a single variance value. Instead, we
    compute a covariance matrix with which to compute the spread along any specified
    direction. The eigenvector corresponding to the largest eigenvalue of this covariance
    matrix yields the direction of maximum spread. That eigenvalue yields the maximum
    spread. The eigenvector corresponding to the next-largest eigenvalue yields the
    orthogonal direction with the next-highest spread, and so forth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis (PCA) is a technique in multivariate statistics
    to identify the directions of the maximum spread of data. It uses the eigenvectors
    and eigenvalues of the covariance matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gaussian distribution is the most important probability distribution. The
    Gaussian random variable has one value with the highest probability of occurrence.
    The probability decreases smoothly with increasing distance from that highest
    probability value. The probability density function is continuous and looks like
    a bell-shaped surface. The center of the bell is the highest probability value,
    which also happens to be the expected value of the Gaussian random variable. The
    covariance matrix determines the shape of the base of the bell surface. It is
    circular when the covariance matrix is diagonal, with equal values on the diagonal;
    it is elliptical in general, with the axes of the ellipse along the eigenvectors
    of the covariance matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample point cloud of a Gaussian distribution is elliptical. It corresponds
    to the base of the bell-shaped probability density function. The longest spread
    corresponds to the ellipse‚Äôs major axis, which corresponds to the eigenvector
    corresponding to the largest eigenvalue of the covariance matrix. In the GitHub
    repository, we have provided an interactive visualizer for observing the shapes
    of Gaussian distributions in one and two dimensions as you change the parameter
    values. Take a look at the interactive visualization section at [http://mng.bz/NYJX](http://mng.bz/NYJX).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
