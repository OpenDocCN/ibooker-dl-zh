- en: 3 Artificial intelligence, automation, and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: The value of good testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How tools aid in testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to identify when AI tools could be of use in testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we delve deeper into the use of large language models (LLMs) in testing,
    let’s ask ourselves the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the purpose and value of testing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can tooling help us?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When is it appropriate to use AI tools?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may seem that asking these fundamental questions is unnecessary. But if you
    are someone who sees testing merely as a confirmatory exercise, as in executing
    test cases to confirm a requirement is correct, then your mileage from the subsequent
    chapters will be limited. Understanding the value and performance of testing is
    critical for determining how tools can be used effectively. Thus, in this chapter,
    we’re going to explore why a deeper understanding of testing can help us utilize
    tools. That said, if you are someone who already has that deep understanding,
    feel free to skim through this chapter and move on. For the rest of us, let’s
    go back to square one by asking why we test.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The value of testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To help us appreciate having a clear understanding of why we need testing in
    software development, let’s return to the common perspective that testing is a
    confirmatory exercise. By this, we mean that testing is viewed as something done
    to confirm the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: Written requirements have been met.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All critical paths have been covered in a system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system works as expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teams that hold this perspective tend to overly rely on the use of test cases/scripts
    that contain explicit instructions for a human, or machine, to follow and confirm
    whether an expected outcome has been met. The problem with this mindset and approach
    is less that it’s utilizing test scripts and more that it only uses test scripts
    and nothing else, which results in edge cases being missed, more complex bugs
    or behavior left untested, and generally in a limited understanding of how our
    products behave. Many biases come from an overreliance on test scripts, but if
    we bring it back to the use of LLMs in testing, then it limits our appreciation
    of how these tools can help us. When tools such as ChatGPT grew in popularity,
    a large majority of demonstrations and debates around the use of LLMs in testing
    focused on one thing: test scripts. People would demonstrate how LLMs could generate
    test scripts to be executed manually by either a human or a test automation tool.'
  prefs: []
  type: TYPE_NORMAL
- en: Although initially these might have had some use, the options for what else
    could be done with LLMs to help testing began to dry out. On the surface, this
    appears to be a limitation of the tooling in question, but instead, the real problem
    is the limited idea of what testing is and how it can help. So, if we are to expand
    our use of LLMs in testing, we have to first expand our understanding of what
    testing is and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 A different way of thinking about testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To help us establish a deeper understanding, let’s explore a model of testing
    that I use to define what I believe testing is for and how it helps, which is
    shown in figure 3.1.
  prefs: []
  type: TYPE_NORMAL
- en: The model, based on one created by James Lyndsay in his paper “Exploration and
    Strategy” ([https://mng.bz/mRdn](https://mng.bz/mRdn)), consists of two circles.
    The left circle represents imagination, or what we *want* in a product, and the
    right circle represents implementation, or what we *have* in a product. The purpose
    of testing is to learn as much as possible about what’s going on in each of these
    circles by carrying out testing activities. The more we test in these two circles,
    the more we learn. We can then
  prefs: []
  type: TYPE_NORMAL
- en: Discover potential problems that might affect the quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlap these two circles of information, ensuring we trust we are building
    the intended product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH03_F01_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 A model that helps describe the value and purpose of testing
  prefs: []
  type: TYPE_NORMAL
- en: To help describe this principle further, let’s look at an example in which a
    team is delivering a hypothetical search feature that we want to ensure is delivered
    to a high degree of quality.
  prefs: []
  type: TYPE_NORMAL
- en: Imagination
  prefs: []
  type: TYPE_NORMAL
- en: 'The Imagination circle represents what it is that we want from our product,
    and that includes expectations that are both explicit and implicit. So in this
    circle, our testing is focused on learning as much as possible about those explicit
    and implicit expectations. By doing so, we not only learn what has been explicitly
    stated in writing or verbally shared, but also dig down into the details and remove
    ambiguity over terms and ideas. Let’s say a representative of the business or
    a user, such as a product owner, has shared this requirement with their team:
    “Search results are to be ordered by relevance.”'
  prefs: []
  type: TYPE_NORMAL
- en: The explicit information shared here tells us that the product owner wants search
    results ordered by relevance. However, a lot of implied information can be uncovered
    by testing the ideas and concepts behind what is being asked. This might come
    in the form of a series of questions, such as
  prefs: []
  type: TYPE_NORMAL
- en: What is meant by relevant results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who benefits from the results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What information is shared?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we order results by relevancy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data should we use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By asking these questions, we gain a broader perspective of what is wanted,
    remove any misunderstandings and assumptions in our team’s thinking, and identify
    potential risks that could affect those expectations. If we know more about what
    we are being asked to build, then we’re more likely to build the right thing the
    first time.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs: []
  type: TYPE_NORMAL
- en: By testing the imagination, we get a stronger sense of what we are being asked
    to build. But, just because we might know what to build doesn’t mean we end up
    with a product that matches our expectations. This is why we also test the implementation
    to learn
  prefs: []
  type: TYPE_NORMAL
- en: Whether the product matches our expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the product might not match our expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both goals are equally important. We want to ensure that we have built the right
    thing, but there will always be side effects, such as unintended behavior, vulnerabilities,
    missed expectations, and downright weirdness that might appear in our products.
    With our search results example, we could not only test whether the feature delivers
    results in the relevant order but also ask
  prefs: []
  type: TYPE_NORMAL
- en: What if I use different search terms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if the relevant results don’t match the behavior of other search tools?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if part of the service is down when I search?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if I request results 1000 times in less than 5 seconds?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if there are no results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By exploring beyond our expectations, we become more aware of what is going
    on in our product—warts and all. This ensures we don’t end up making incorrect
    assumptions about how our product behaves and releasing a poor-quality product.
    It also means that if we find unexpected behavior, we have the choice to attempt
    to remove or readjust our expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 A holistic approach to testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The described model of testing the imagination and implementation demonstrates
    that the testing goes beyond a simple confirmation of expectations and lays out
    a more holistic approach to testing. By implementing different activities that
    focus on the imagination and implementation spaces, through the testing we execute,
    we learn more about what we want to build and what we have built. The more we
    learn in these two areas, the more they align with one another. And the more they
    align, the more accurate our perception of quality becomes.
  prefs: []
  type: TYPE_NORMAL
- en: A team that is well-informed about their work has a better understanding of
    their product’s quality. We are then also better equipped to decide what steps
    to take to improve quality. This enables us to focus our attention on specific
    risks, make changes in our product to align with users’ expectations or determine
    what problems we want to invest time in to fix and which to leave alone. This
    is the value of good testing—to help teams get into a position where they can
    make these informed decisions and feel confident in the steps they are taking
    to develop a high-quality product.
  prefs: []
  type: TYPE_NORMAL
- en: To help us better appreciate this model, let’s consider a sample context in
    which testing is required. For our example, we are responsible for the delivery
    of a fast-food ordering system. Users log on, find the restaurant they want to
    order from, place their order (which is sent to the restaurant), and then track
    the delivery of their order from within the product. A product like this one would
    need to be highly available, easy to use, and secure. So, to deliver a high-quality
    product, we might need to utilize different testing activities to focus on different
    types of risks, as shown in figure 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH03_F02_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 The imagination/implementation model with sample activities shown
  prefs: []
  type: TYPE_NORMAL
- en: In the model, we can see a range of various activities that are placed in specific
    areas because they are focused on specific types of risks. For example, on the
    imagination side, we might be concerned with risks that affect the usability of
    a site. Therefore, we employ testing activities that focus on user experience
    testing and collaborative design. On the implementation side, we want to ensure
    the product is stable and minimize risks that might affect the product availability
    and application. Thus, we employ activities such as exploratory and performance
    testing. Finally, notice how in the area that overlaps, we have added test scripts.
    These are useful because they are informed by our explicit expectations (imagination)
    to mitigate risks around unexpected changes appearing in the product (implementation)
    as it grows and morphs over time, demonstrating that test scripting and automated
    checks are of use but are only one part of the holistic whole.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these activities has different implementations, different challenges,
    and different ways in which tooling can help. But we wouldn’t be able to easily
    identify these uses if we didn’t understand that testing is an exercise in information
    gathering and knowledge sharing. With this model in place, an appreciation of
    the many different risks our work faces, and the testing activities that help
    mitigate them, we can begin to drill deeper into how tooling plays a part in testing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 How tools help with testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ll likely hear a tester say (or you may even have said it yourself) that
    there is never enough time to test everything. It will likely be repeated in this
    book a few times. Teams are always limited by time, budgets, meetings, staffing,
    and other factors, so to implement and execute effective, modern testing, we must
    rely on tools to help us. Tools are essential to testing, but they bring us to
    our next misconception around testing—that a tool, or machine, can test the way
    a human can.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Automation bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To appreciate how machines and humans differ regarding testing, let’s consider
    an example where both are used to test a website feature. The feature is a full-width
    banner for an e-commerce website with an image and some text to highlight the
    deal of the day. Initially, we test it manually, or in a human-led way, and observe
    the feature is working properly—the image is shown and all the text associated
    with it is correct. Then we decide to use tools to automate testing. We create
    code that will open the browser and assert that element A, which is where the
    deal of the day is loaded, exists. We run the automated test, and it passes. And
    then, one day, after another successful release in which all our automated tests
    pass, an end-user raises a bug and informs us they can’t see the deal of the day.
    All they see is an empty white box at the top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'What happened? During the process of creating the automated test, we have transferred
    our knowledge, which is built on implicit information based on mental heuristics
    and oracles, and made it explicit. We have stripped a complex understanding of
    a feature down to a single instruction: element A should exist on a web page.
    So, when the latest release of our product went out with a faulty function to
    retrieve the deal of the day, or the CSS was incorrect or broken, the automated
    test still passed because element A still exists. However, it takes a human a
    matter of seconds to see something is wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: The moral of this story isn’t that tools are bad or unnecessary but that they
    are often misused or misinterpreted. This behavior is a type of *automation bias*
    that creeps into our perspective on the value of tooling, where we ascribe more
    value to the output of a tool than what it is relaying to us. That is, when we
    designed our automated test to look for element A, we assumed that all we as humans
    were doing was looking for element A. But we were considering many other factors,
    even if we weren’t doing it consciously.
  prefs: []
  type: TYPE_NORMAL
- en: If we fall prey to automation bias, we run the risk of selecting and implementing
    tools that we believe can reveal and report information in the same way humans
    can, when in fact they don’t, leading to misguided overconfidence in the products
    we’re delivering, or generating a level of workload to have tools emulate human
    behavior that is unsustainable for a modern project. Tools cannot replace testing
    activities, and subscribing to that notion will ultimately lead to problems with
    quality and an increase in risks to a project. So instead, we have to shift our
    thinking more toward how tools can support our testing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Being selective with tooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Success with tools comes from situations in which some thought has been put
    into the problem we want to solve and what tools could potentially help. To better
    understand this, let’s return to our deal-of-the-day feature and look closer at
    what a human is doing when they test a feature like this.
  prefs: []
  type: TYPE_NORMAL
- en: First, we consider different ways of testing the feature. We use our current
    understanding of said feature to formulate test ideas and select what we want
    to test first. Next, we need to set up our test. This might include setting up
    an environment or creating/updating the necessary test data. (We might need to
    create a deal of the day to observe as well as test users to administer and view
    the deal.) With everything set up, we then need to execute our test, loading the
    browser or perhaps multiple browsers to verify that the deal renders correctly.
    Then once we’ve observed the results, we take notes or report our findings to
    our team, all of which would update our understanding of the feature, ready for
    us to start the process again. This flow can be summarized as shown in figure
    3.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH03_F03_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 A visualization of the process of testing the deal-of-the-day feature
  prefs: []
  type: TYPE_NORMAL
- en: This cycle may be something that happens rapidly—for example, in an exploratory
    testing session. Or it may take place in a longer form, such as performance testing,
    in which each step has many details to consider. Regardless of the type of activity,
    to carry out the loop successfully, we need tools to complete the process. We
    would likely need to use tools such as database clients, test data managers, or
    infrastructure tools to set up state management. We would use note-taking tools,
    screenshot applications, and project management tools to report what has been
    learned. Figure 3.4 summarizes these tools used in testing by updating our initial
    visualization.
  prefs: []
  type: TYPE_NORMAL
- en: The visualization demonstrates how modern testing utilizes a range of tools
    for various tasks that occur rather than attempting to wrap the whole testing
    performance into the use of one tool. This is because there are many different
    activities at play when testing is carried out. A tool’s ability to observe patterns,
    changes, and problems will always be limited compared to a human’s observation
    skills. So instead, we get value from tools that do one job well, as opposed to
    a tool that does many things poorly.
  prefs: []
  type: TYPE_NORMAL
- en: What is so interesting about this kind of thinking is that when we take the
    time to consider it, it seems obvious to us. We all use tools to help us with
    distinct tasks that make up a larger activity. However, most of us do it without
    deliberate thought. Although we know that the use of tools in specific tasks is
    sensible, we need to develop the skills to start choosing and using tools intentionally.
    This means familiarizing ourselves with tools and being more in tune with what
    we’re doing daily in our testing so that we can pick the right tool or, in the
    case of LLMs, the right prompt or enhancement. This is what the rest of this book
    will be focusing on, providing us with the skills needed to know when LLMs can
    help with specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH03_F04_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 A visualization of the testing process, this time with the addition
    of tools
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Knowing when to use LLMs in testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now understand how effective tool use in testing is about looking to enhance
    tasks within testing rather than replicating broad testing activities. But where
    do LLMs fit into this? Given their broad applications, let’s take a look at some
    of the common ways in which LLMs are currently being used and see how each can
    be used to support testing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Generative capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the vast field of artificial intelligence, LLMs fall under the banner of
    generative AI; that is, LLMs have been built with the purpose of generating content
    (which differs from other AI that may be used to predict or classify). For example,
    this means that if an LLM was asked to generate test cases for us, it would do
    so. However, just because it can generate material like test cases doesn’t mean
    that those test cases are of value to us. We must remind ourselves that how an
    LLM generates outputs is based on a complex model of probabilities. This means
    its generative abilities differ from how we as humans might generate ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, in chapter 1 we explored this concept when sending a prompt
    to an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Create tests for a file upload feature |'
  prefs: []
  type: TYPE_TB
- en: 'which might result in a response like this one (shortened for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  Test for File Upload Functionality:   •   Test Case 1.1: Upload a
    Valid File   −   Input: Choose a file with a valid format (e.g., .jpg, .pdf).  
    −   Expected Outcome: The file is successfully uploaded, and a confirmation message
    is displayed.   •   Test Case 1.2: Attempt to Upload an Invalid File   −   Input:
    Choose a file with an invalid format (e.g., .exe, .xhtml).   −   Expected Outcome:
    An error message is displayed, indicating that the file format is not supported.  
    2.  Test for File Size Limit:   •   Test Case 2.1: Upload a File Below the Size
    Limit   −   Input: Choose a file smaller than the maximum allowed size.   −  
    Expected Outcome: The file is successfully uploaded, and a confirmation message
    is displayed.   •   Test Case 2.2: Attempt to Upload a File Exceeding the Size
    Limit   −   Input: Choose a file larger than the maximum allowed size.   −   Expected
    Outcome: An error message is displayed, indicating that the file exceeds the maximum
    size limit. |'
  prefs: []
  type: TYPE_TB
- en: How an LLM has generated these test cases is impressive, but they lack context
    and specificity for the feature or product we are testing. So instead, when working
    with LLMs, we need to make sure that how we prompt or extend them provides them
    with enough context to generate valuable outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The consequence of ensuring that we provide sufficient context is that it’s
    easier to use LLMs to generate outputs for very specific and targeted tasks. The
    alternative means we would have to provide a massive amount of input data that
    would result in a prompt that is expensive to build and maintain. For example,
    imagine the amount of context you would have to put into an LLM to obtain a test
    strategy relevant to our working context.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we can get more value from LLMs if we focus on using them to help with
    tasks such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Test data generation*—When given explicit rules around data sets, LLMs can
    be used to generate rapid sets of data for use in a range of testing activities,
    from exploratory to performance testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Suggestions for risks and test ideas*—We should always avoid letting the output
    of an LLM be the sole arbiter of what to test. We can use them to suggest test
    ideas and risks that can be used as jumping-off points for new ideas or factored
    into our existing work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Code snippets*—Similar to the earlier test case example, we gain little value
    from LLMs if we ask them to generate complete automated tests or frameworks. However,
    using them to generate smaller parts of automation or scripts used to support
    testing activities such as exploratory testing can be advantageous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3.2 Transformation capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another benefit that LLMs offer is the ability to transform natural language
    from one structure to another. A common example of LLM transformation is language
    translation. Suppose that we sent something like this to an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Convert the following text delimited by three hashes into French:###Hello,
    my name is Mark### |'
  prefs: []
  type: TYPE_TB
- en: Then it will return a response such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Bonjour, je m’appelle Mark |'
  prefs: []
  type: TYPE_TB
- en: 'This is a useful way to illustrate how LLMs transform data, but we shouldn’t
    be restricted to spoken languages only. LLMs are capable of transforming all types
    of data from one abstraction to another. Here are some examples that can help
    with testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Transforming test data*—Using LLMs to rapidly transform data from one structure
    to another can help speed up testing. For example, we might ask an LLM to convert
    plain-text test data into SQL statements or to convert SQL statements into helper
    functions that are called in test automation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Converting code*—LLMs can convert functions, classes, and other data into
    new iterations of code. What makes this valuable is that LLMs can transform code
    into different languages but still keep the logic and flow of the original code
    in the newly translated output (although we should always test it to be sure).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Summarizing notes*—Although the conversion of data isn’t as direct as, say,
    converting a code snippet from one language to another, we can use LLMs to transform
    and summarize at the same time. Also, we can use LLMs to take raw testing notes
    from testing activities such as exploratory or design testing sessions and have
    them converted into summary notes to be shared with others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3.3 Enhancing capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we can use LLMs to enhance and expand existing material. This usually
    means providing an LLM with a snippet of data and prompting the LLM to expand
    on it. This has some overlap with the generative capabilities because we’re asking
    LLMs to generate a certain degree of new output, but in this situation, we’re
    providing a lot more upfront context and instructing it to focus on existing material,
    as opposed to prompting an LLM to generate something completely new. This means
    we can use this ability to help us with testing tasks such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Reviewing code*—Not all who conduct testing are confident coders, and even
    those who are comfortable with reading code can struggle at times to make sense
    of the code required to analyze or test. LLMs can enhance our understanding by
    taking code snippets and providing a natural language breakdown of how said code
    works, which can help with risk analysis, test design, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Descriptive code*—Similar to reviewing code, we can use LLMs to help improve
    the descriptiveness of code—for example, rapidly creating code comments that can
    be easily created and maintained. This can be especially useful for automated
    testing, where communicating what our automation code is doing is important for
    maintenance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Expanding analysis*—We can also use LLMs to expand our analysis activities,
    such as risk analysis and design testing (where we ask questions about requirements
    before the feature is built). By providing it with our current analysis data,
    we can ask LLMs to review and expand on it, suggesting new ideas that we can either
    incorporate into our analysis or ignore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3.4 LLMs in use in testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To put these different abilities of an LLM into context, let’s return to our
    visualization of testing with the support of tools (figure 3.5).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH03_F05_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 A visualization of the testing process, this time with the addition
    of LLMs
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see how LLMs can be inserted into distinct and specific tasks within
    the wider testing lifecycle. It brings us back to our area of effect model that
    we covered in chapter 1\. Instead of attempting to use LLMs to replicate the full
    gamut of testing activities that exist within a lifecycle, we prioritize the best
    of our abilities as humans and the value we bring to testing. Then, we choose
    to add LLMs in select areas to expand our work so that we can move faster, learn
    more, and help ensure our teams are better informed so that they can build higher-quality
    products.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we have a limited understanding of what testing is, then the use of tools
    will also be limited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing is not a confirmatory exercise but rather a collection of different
    activities that help those seeking to learn about the imagination and implementation
    of a product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The imagination implies our understanding of what we want to build.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation implies our understanding of what we have built.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As our understanding of both aspects increases, we bring them into alignment,
    which helps us deliver a higher-quality product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We run many different types of testing activities to focus on different types
    of risks and how they affect both imagination and implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools are an essential component of successful testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be used to generate, transform, and enhance outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs should be used with smaller, specific tasks to generate outputs that provide
    value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can generate content that can help with specific tasks or create suggested
    content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can transform data to help convert raw data into useful formats or summarize
    it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can also enhance existing material, adding new suggestions or expanding
    details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can insert LLMs into many distinct and specific testing tasks, which reflects
    the area of effect model we learned about in chapter 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
