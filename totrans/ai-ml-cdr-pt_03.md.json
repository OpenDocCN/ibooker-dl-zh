["```py\nself.linear_relu_stack = nn.Sequential(\n    nn.Linear(28*28, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10),\n    nn.LogSoftmax(dim=1)\n)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Load the dataset\ntransform = transforms.Compose([transforms.ToTensor()])\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                             download=True, transform=transform)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                             download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, \n                          shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, \n                          shuffle=False)\n\n# Define the model\nclass FashionMNISTModel(nn.Module):\n    def __init__(self):\n        super(FashionMNISTModel, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.LogSoftmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = FashionMNISTModel()\n\n# Define the loss function and optimizer\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  \n                    `[{``current``:``>``5``d``}``/``{``size``:``>``5``d``}]``\")` ```", "```py\n```", "```py`` ```", "```py datasets.FashionMNIST ```", "```py train_dataset = datasets.FashionMNIST(root='./data', train=True,                               download=True, transform=transform) test_dataset = datasets.FashionMNIST(root='./data', train=False,                               download=True, transform=transform) ```", "```py transform = transforms.Compose([transforms.ToTensor()]) ```", "```py # Define the model class FashionMNISTModel(nn.Module):     def __init__(self):         super(FashionMNISTModel, self).__init__()         self.flatten = nn.Flatten()         self.linear_relu_stack = nn.Sequential(             nn.Linear(28*28, 128),             nn.ReLU(),             nn.Linear(128, 10),             nn.LogSoftmax(dim=1)         )       def forward(self, x):         x = self.flatten(x)         logits = self.linear_relu_stack(x)         return logits   model = FashionMNISTModel() ```", "```py # Define the loss function and optimizer loss_function = nn.NLLLoss() optimizer = optim.Adam(model.parameters()) ```", "```py # Train the model def train(dataloader, model, loss_fn, optimizer):     size = len(dataloader.dataset)     model.train()     for batch, (X, y) in enumerate(dataloader):         # Compute prediction and loss         pred = model(X)         loss = loss_fn(pred, y)           # Backpropagation         optimizer.zero_grad()         loss.backward()         optimizer.step()           if batch % 100 == 0:             loss, current = loss.item(), batch * len(X)             print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\") ```", "```py     for batch, (X, y) in enumerate(dataloader): ```", "```py # Training process epochs = 5 for t in range(epochs):     print(f\"Epoch {t+1}\\n-------------------------------\")     train(train_loader, model, loss_function, optimizer) print(\"Done!\") ```", "```py` ```", "```py```", "````py ````", "``` # Training the Neural Network    Once you’ve executed the code, you’ll see the network train epoch by epoch. Then, after running the training, you’ll see something at the end that looks like this:    ```", "```py    You can see here that over time, the loss has gone down. For example, in my case, the loss value at the end of the first epoch was .345, and by the end of the fifth epoch, it was .205\\. This data shows us that the network is learning.    But how can we tell how *accurately* it’s learning? Note that loss and accuracy, while related, don’t have a direct linear relationship—for example, we can’t say that if loss is 20%, then accuracy is 80%. So, we need to go a little deeper.    Recall that when we were getting the data, we got *two* datasets: one for training and one for testing. Here’s a great place where we can write code to pass the test data through our network and evaluate how accurate the network is at predicting answers. We already know the correct answers, so we could do inference on all 10,000 test records, get the answers that the model predicts, and then check them against the ground truth for accuracy.    Here’s the code:    ```", "```py `# Evaluate the model` `test``(``test_loader``,` `model``)` ```", "```py   ```", "```py` ```", "```py There are a few things to note in this code. First is the `model.eval()` line, which indicates that we are switching the model from training mode to inference mode. Similarly, `torch.no_grad()`will turn off gradient calculation in PyTorch to speed up inference. We’re no longer *training* the model, so we don’t need to do all the loss function backpropagation and optimization. We can just turn that off.    Then, as it does during training, the network just goes through every item in the data loader, gets the prediction for that item, and checks its correctness with this line:    ```", "```py    That’s a bit of a mouthful, so let’s break it down.    First, the `pred` value will give us the prediction from the network. The network outputs 10 values, each of which includes the probability of the class it represents being the correct one. Calling `argmax` on this will give us which one had the biggest value (i.e., the one with the probability closest to 1). The *y* value is the correct answer. For example, if we get a prediction, the neuron with the highest value is the sixth one, and *y* = 6, so we know we have a correct answer. Also, because we’re dealing in batches, we want to count each time `pred.argmax(1) == y` for this batch, hence, the `sum()`.    Therefore, our accuracy value will be the sum of correct items divided by the total number of items. So, when you run this code after training the model, you should see output like this:    ```", "```py    Remarkably, after running the neural network for only five epochs, we can see that it is 86.9% accurate on data it hadn’t previously seen!    At this point, you may be thinking that it’s really nice to see the accuracy of the model on the test set, but you may also be asking why we’ve only reported loss on the training—why not also report accuracy there? It seems silly to finish training the model by only looking at minimizing loss and *then* to figure out the accuracy. And you’d be right!    Fortunately, updating the model training code to *also* report on accuracy is pretty easy to do. Here’s a function called `get_accuracy()` that you can use during training:    ```", "```py    Then, in your training loop, you can simply call this function after the loss function call like this:    ```", "```py    And when you’re reporting on the output of the training, you can use the accuracy metric like this:    ```", "```py   ```", "```py`Running this will give you output a bit like this:    ```", "```py    Now, you’re probably wondering why the accuracy for the test data (86.9%) is *lower* than the accuracy for the training data (88.59%). This is very common, and when you think about it, it makes sense: the neural network only really knows how to match the inputs it has been trained on with the outputs for those values. Our hope is that given enough data, the network will be able to generalize from the examples it has seen and thus “learn” what a shoe or a dress looks like. But there will always be examples of items that it hasn’t seen that are also different enough from what it has seen to confuse it.    For example, if you grew up only ever seeing sneakers, then that’s what a shoe looks like to you. So, when you first see a high-heeled shoe, you might be a little confused. From your experience, it’s probably a shoe, but you don’t know for sure. That’s exactly what a neural network “thinks” when it “sees” inputs that are different enough from what it’s been trained on.```", "```py`` ```", "```py ```", "```py`  ```", "```py`` ```", "```py` ```", "```py # Exploring the Model Output    Now that we’ve trained the model and gotten a good gauge of its accuracy by using the test set, let’s explore it a little. Here’s a function we can use to predict a single image:    ```", "```py    Let’s start with this code, which should look familiar to you now that you’ve seen the previous accuracy calculation code:    ```", "```py    Here, we get the `image`, send it to the `model`, get back a `prediction`, and `print` it out. Then, we get the `argmax` of that to show the label. Here’s an example output of the `prediction`:    ```", "```py    These numbers may seem vague, but ultimately, our goal is simply to look for the biggest one! The `Softmax` function gets the `log()` of the value, where `log(1)` is zero and the log of any value less than one is a negative value. As you look through the list, you’ll notice that the value closest to 0 (–0.3285) is the very last one. This indicates that the function believes the class for this image should be class number 9\\. (There are 10 classes in Fashion MNIST, which are numbered 0 through 9.)    Fashion MNIST’s class number 9 is “Ankle Boot,” so I’ve also included the code to render the image in [Figure 2-6](#ch02_figure_6_1748548889066448).    Also, as we can see, this is an example of where the model got the prediction right. The ground truth was that it’s label 9, and the prediction was for number 9\\. Drawing the image so that we mere humans can compare the two also gives us an ankle boot!  ![](assets/aiml_0206.png)  ###### Figure 2-6\\. Exploring the output of the predictive model    Now, try a few different values for yourself and see if you can find anywhere the model gets it wrong.    # Overfitting    In the last example, we trained for only five epochs. That is, we went through the entire training loop of having the neurons randomly initialized and checked against their labels, then that performance was measured by the loss function and updated by the optimizer five times. And the results we got were pretty good: 88.59% accuracy on the training set and 86.5% on the test set. So what happens if we train for longer?    Next, try updating it to train for 50 epochs instead of 5\\. In my case, I got the following accuracy figures on the training set:    ```", "```py    This is particularly exciting because we’re doing much better: we’re getting 96.15% accuracy!    However, for the test set, accuracy reached 89.2%:    ```", "```py    So, we got a big improvement over the training set and a smaller one over the test set. This might suggest that training our network for much longer would lead to much better results—but that’s not always the case. The network is doing much better with the training data, but the model is not necessarily a better model. In fact, the divergence in the accuracy numbers shows that the model might have become overspecialized to the training data, in a process that’s often called *overfitting*. As you build more neural networks, this problem is something to watch out for—and as you go through this book, you’ll learn a number of techniques to avoid it!    # Early Stopping    In each of the cases so far, we’ve hardcoded the number of epochs we’re training for. While that works, we might want to train until we reach the desired accuracy instead of constantly trying different numbers of epochs and training and retraining until we get to our desired value. So, for example, if we want to train until the model is at 95% accuracy on the training set, and if we want to do it without knowing in advance how many epochs it will take. . .how can we do it?    Given that we’ve updated our code to check the accuracy as the model trained and to print it out, now, all we have to do is check that accuracy and end the training if it’s above a certain amount—such as 95% (or 0.95 when normalized). For example, we can do this:    ```", "```py `# Early stopping condition` `if` `avg_accuracy` `>=` `95``:`     `print``(``\"Reached 95``% a``ccuracy, stopping training.\"``)`     `return` `True`  `# Stop training` ```", "```py   ```", "```py`Note that if we use this code inside the `if batch % 100 == 0` block, we can break the training loop before all batches in a particular epoch have been processed. It’s better to do this check at the end of the epoch, so we need to be sure to place the `if avg_accuracy >= 95` in the right place!    Now, when we’re training, at the end of every epoch, the average accuracy for the epoch will be calculated—and if it hits 95%, the training will stop. Previously, I had trained the model for 50 epochs to get 96.15% accuracy, but with this early stopping, where I’ve defined 95% as “good enough,” you can see that the model stopped training after only 37 epochs. Interestingly, accuracy was 94.99% for a couple of epochs before that, so I might have been able to stop even earlier!    This process of *early stopping* is very powerful in helping you save time as you evaluate different model architectures for solving specific problems. It helps you train your model until it’s “good enough,” instead of having a fixed training loop. For example, the process can look like this:    ```", "```py    This process can save you a lot of time you would otherwise spend manually checking on the network to see if it’s learning appropriately.```", "```py``  `` `# Summary    In [Chapter 1](ch01.html#ch01_introduction_to_pytorch_1748548870019566), you learned about how ML is based on fitting features to labels through sophisticated pattern matching with a neural network. In this chapter, you took that to the next level by going beyond a single neuron and learning how to create your first (very basic) computer vision neural network. The network was somewhat limited because of the data: all the images were 28 × 28 grayscale, with the item of clothing centered in the frame. This is a good start, but it’s a very controlled scenario.    To do better at vision, you may need the computer to learn features of an image instead of learning merely the raw pixels. You can do that with a process called *convolutions*, and in the next chapter, you’ll learn how to define convolutional neural networks to understand the contents of images.` `` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py````"]