["```py\nself.linear_relu_stack = nn.Sequential(\n    nn.Linear(28*28, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10),\n    nn.LogSoftmax(dim=1)\n)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Load the dataset\ntransform = transforms.Compose([transforms.ToTensor()])\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                             download=True, transform=transform)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                             download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, \n                          shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, \n                          shuffle=False)\n\n# Define the model\nclass FashionMNISTModel(nn.Module):\n    def __init__(self):\n        super(FashionMNISTModel, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.LogSoftmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = FashionMNISTModel()\n\n# Define the loss function and optimizer\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  \n                    [{current:>5d}/{size:>5d}]\")\n\n# Training process\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_loader, model, loss_function, optimizer)\nprint(\"Done!\")\n```", "```py\ndatasets.FashionMNIST\n```", "```py\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                             download=True, transform=transform)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                             download=True, transform=transform)\n```", "```py\ntransform = transforms.Compose([transforms.ToTensor()])\n```", "```py\n# Define the model\nclass FashionMNISTModel(nn.Module):\n    def __init__(self):\n        super(FashionMNISTModel, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.LogSoftmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = FashionMNISTModel()\n```", "```py\n# Define the loss function and optimizer\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())\n```", "```py\n# Train the model\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n```", "```py\n    for batch, (X, y) in enumerate(dataloader):\n```", "```py\n# Training process\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_loader, model, loss_function, optimizer)\nprint(\"Done!\")\n```", "```py\nEpoch 5\n-------------------------------\nloss: 0.429329  [    0/60000]\nloss: 0.348756  [ 6400/60000]\nloss: 0.237481  [12800/60000]\nloss: 0.336960  [19200/60000]\nloss: 0.435592  [25600/60000]\nloss: 0.272769  [32000/60000]\nloss: 0.362881  [38400/60000]\nloss: 0.202799  [44800/60000]\nloss: 0.354268  [51200/60000]\nloss: 0.205381  [57600/60000]\nDone!\n```", "```py\n# Function to test the model\ndef test(dataloader, model):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()  # Set the model to evaluation mode\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_function(pred, y).item()\n            correct += (pred.argmax(1) == \n                        y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, \n            Avg loss: {test_loss:>8f} \\n\")\n\n# Evaluate the model\ntest(test_loader, model)\n```", "```py\ncorrect += (pred.argmax(1) ==  y).type(torch.float).sum().item()\n```", "```py\nTest Error:\n Accuracy: 86.9%, Avg loss: 0.366243\n```", "```py\n# Function to calculate accuracy\ndef get_accuracy(pred, labels):\n    _, predictions = torch.max(pred, 1)\n    correct = (predictions == labels).float().sum()\n    accuracy = correct / labels.shape[0]\n    return accuracy\n```", "```py\nfor batch, (X, y) in enumerate(dataloader):\n    # Compute prediction and loss\n    pred = model(X)\n    loss = loss_fn(pred, y)\n    accuracy = get_accuracy(pred, y)\n\n    # Backpropagation\n```", "```py\nif batch % 100 == 0:\n    current = batch * len(X)\n    avg_loss = total_loss / (batch + 1)\n    avg_accuracy = total_accuracy / (batch + 1) * 100\n    print(f\"Batch {batch}, Loss: {avg_loss:>7f}, \n            Accuracy: {avg_accuracy:>0.2f}% \n                      [{current:>5d}/{size:>5d}]\")\n```", "```py\nEpoch 5\n-------------------------------\nBatch 0, Loss: 0.177518, Accuracy: 95.31% [    0/60000]\nBatch 100, Loss: 0.304973, Accuracy: 88.89% [ 6400/60000]\nBatch 200, Loss: 0.311628, Accuracy: 88.51% [12800/60000]\nBatch 300, Loss: 0.307373, Accuracy: 88.63% [19200/60000]\nBatch 400, Loss: 0.309722, Accuracy: 88.67% [25600/60000]\nBatch 500, Loss: 0.310240, Accuracy: 88.60% [32000/60000]\nBatch 600, Loss: 0.306988, Accuracy: 88.70% [38400/60000]\nBatch 700, Loss: 0.308556, Accuracy: 88.64% [44800/60000]\nBatch 800, Loss: 0.309518, Accuracy: 88.67% [51200/60000]\nBatch 900, Loss: 0.311487, Accuracy: 88.59% [57600/60000]\nDone!\n```", "```py\n import matplotlib.pyplot as plt\n\ndef predict_single_image(image, label, model):\n    # Set the model to evaluation mode\n    model.eval()\n\n# Unsqueeze image as the model expects a batch dimension\n    image = image.unsqueeze(0)\n\n    with torch.no_grad():\n        prediction = model(image)\n        print(prediction)\n        predicted_label = prediction.argmax(1).item()\n\n    # Display the image and predictions\n    plt.imshow(image.squeeze(), cmap='gray')\n    plt.title(f'Predicted: {predicted_label}, Actual: {label}')\n    plt.show()\n\n    return predicted_label\n\n# Choose an image from the test set\nimage, label = test_dataset[0]  # Change index to test different images\n\n# Predict the class for the chosen image\npredicted_label = predict_single_image(image, label, model)\nprint(f\"The model predicted {predicted_label}, and the actual label is {label}.\")\n```", "```py\n    with torch.no_grad():\n        prediction = model(image)\n        print(prediction)\n        predicted_label = prediction.argmax(1).item()\n```", "```py\ntensor([[–12.4290, –16.0639, –14.3148, –16.2861, –13.1672,  –4.5377, –13.6284, \n         –1.3124,  –8.9946,  –0.3285]])\n```", "```py\nEpoch 50\n-------------------------------\nBatch 0, Loss: 0.077159, Accuracy: 96.88% [    0/60000]\nBatch 100, Loss: 0.094825, Accuracy: 96.57% [ 6400/60000]\nBatch 200, Loss: 0.093598, Accuracy: 96.67% [12800/60000]\nBatch 300, Loss: 0.095906, Accuracy: 96.54% [19200/60000]\nBatch 400, Loss: 0.096683, Accuracy: 96.48% [25600/60000]\nBatch 500, Loss: 0.101872, Accuracy: 96.31% [32000/60000]\nBatch 600, Loss: 0.103130, Accuracy: 96.22% [38400/60000]\nBatch 700, Loss: 0.103901, Accuracy: 96.17% [44800/60000]\nBatch 800, Loss: 0.104216, Accuracy: 96.15% [51200/60000]\nBatch 900, Loss: 0.104010, Accuracy: 96.15% [57600/60000]\nDone!\n```", "```py\nTest Error:\n Accuracy: 89.2%, Avg loss: 0.433885\n```", "```py\nif batch % 100 == 0:\n    current = batch * len(X)\n    avg_loss = total_loss / (batch + 1)\n    avg_accuracy = total_accuracy / (batch + 1) * 100\n    print(f\"Batch {batch}, Loss: {avg_loss:>7f}, \n           Accuracy: {avg_accuracy:>0.2f}% [{current:>5d}/{size:>5d}]\")\n\n# Early stopping condition\nif avg_accuracy >= 95:\n    print(\"Reached 95% accuracy, stopping training.\")\n    return True  # Stop training\n```", "```py\nEpoch 36\n-------------------------------\nBatch 0, Loss: 0.098307, Accuracy: 96.88% [    0/60000]\nBatch 100, Loss: 0.119195, Accuracy: 95.45% [ 6400/60000]\nBatch 200, Loss: 0.127049, Accuracy: 95.20% [12800/60000]\nBatch 300, Loss: 0.126001, Accuracy: 95.34% [19200/60000]\nBatch 400, Loss: 0.127823, Accuracy: 95.25% [25600/60000]\nBatch 500, Loss: 0.131262, Accuracy: 95.11% [32000/60000]\nBatch 600, Loss: 0.135573, Accuracy: 94.95% [38400/60000]\nBatch 700, Loss: 0.135920, Accuracy: 94.95% [44800/60000]\nBatch 800, Loss: 0.135125, Accuracy: 94.99% [51200/60000]\nBatch 900, Loss: 0.134854, Accuracy: 94.99% [57600/60000]\nEpoch 37\n-------------------------------\nBatch 0, Loss: 0.104421, Accuracy: 96.88% [    0/60000]\nBatch 100, Loss: 0.122693, Accuracy: 95.34% [ 6400/60000]\nBatch 200, Loss: 0.124787, Accuracy: 95.26% [12800/60000]\nBatch 300, Loss: 0.127841, Accuracy: 95.16% [19200/60000]\nBatch 400, Loss: 0.130558, Accuracy: 95.05% [25600/60000]\nBatch 500, Loss: 0.131684, Accuracy: 95.00% [32000/60000]\nBatch 600, Loss: 0.132620, Accuracy: 94.95% [38400/60000]\nBatch 700, Loss: 0.132498, Accuracy: 95.01% [44800/60000]\nBatch 800, Loss: 0.132462, Accuracy: 95.05% [51200/60000]\nBatch 900, Loss: 0.133915, Accuracy: 95.03% [57600/60000]\nReached 95% accuracy, stopping training.\n```"]