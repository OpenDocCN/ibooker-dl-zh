- en: Chapter 2\. Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 基础知识
- en: 'In [Chapter 1](ch01.html#foundations), I described the major conceptual building
    block for understanding deep learning: nested, continuous, differentiable functions.
    I showed how to represent these functions as computational graphs, with each node
    in a graph representing a single, simple function. In particular, I demonstrated
    that such a representation showed easily how to calculate the derivative of the
    output of the nested function with respect to its input: we simply take the derivatives
    of all the constituent functions, evaluate these derivatives at the input that
    these functions received, and then multiply all of the results together; this
    will result in a correct derivative for the nested function because of the chain
    rule. I illustrated that this does in fact work with some simple examples, with
    functions that took NumPy’s `ndarray`s as inputs and produced `ndarray`s as outputs.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](ch01.html#foundations)中，我描述了理解深度学习的主要概念构建块：嵌套、连续、可微函数。我展示了如何将这些函数表示为计算图，图中的每个节点代表一个简单的函数。特别是，我演示了这种表示如何轻松地计算嵌套函数的输出相对于其输入的导数：我们只需对所有组成函数取导数，将这些导数在这些函数接收到的输入处进行评估，然后将所有结果相乘；这将导致嵌套函数的正确导数，因为链式法则。我用一些简单的例子说明了这实际上是有效的，这些函数以NumPy的`ndarray`作为输入，并产生`ndarray`作为输出。
- en: 'I showed that this method of computing derivatives works even when the function
    takes in multiple `ndarray`s as inputs and combines them via a *matrix multiplication*
    operation, which, unlike the other operations we saw, changes the shape of its
    inputs. Specifically, if one input to this operation—call the input *X*—is a B
    × N `ndarray`, and another input to this operation, *W*, is an N × M `ndarray`,
    then its output *P* is a B × M `ndarray`. While it isn’t clear what the derivative
    of such an operation would be, I showed that when a matrix multiplication *ν*(*X,
    W*) is included as a “constituent operation” in a nested function, we can still
    use a simple expression *in place of* its derivative to compute the derivatives
    of its inputs: specifically, the role of <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow>
    <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
    can be filled by *X*^(*T*), and the role of <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow>
    <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math>
    can be played by *W*^(*T*).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示了即使在函数接受多个`ndarray`作为输入并通过*矩阵乘法*操作将它们组合在一起时，计算导数的方法仍然有效，这与我们看到的其他操作不同，矩阵乘法操作会改变其输入的形状。具体来说，如果这个操作的一个输入——称为输入*X*——是一个B
    × N的`ndarray`，另一个输入到这个操作的*W*是一个N × M的`ndarray`，那么它的输出*P*是一个B × M的`ndarray`。虽然这种操作的导数不太清楚，但我展示了当矩阵乘法*ν*(*X,
    W*)被包含为嵌套函数中的一个“组成操作”时，我们仍然可以使用一个简单的表达式*代替*它的导数来计算其输入的导数：具体来说，<math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow>
    <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>的作用可以由*X*^(*T*)来填充，<math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow>
    <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math>的作用可以由*W*^(*T*)来扮演。
- en: 'In this chapter, we’ll start translating these concepts into real-world applications,
    Specifically, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始将这些概念转化为现实世界的应用，具体来说，我们将：
- en: Express linear regression in terms of these building blocks
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用这些基本组件来表达线性回归
- en: Show that the reasoning around derivatives that we did in [Chapter 1](ch01.html#foundations)
    allows us to train this linear regression model
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示我们在[第一章](ch01.html#foundations)中所做的关于导数的推理使我们能够训练这个线性回归模型
- en: Extend this model (still using our building blocks) to a one-layer neural network
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个模型（仍然使用我们的基本组件）扩展到一个单层神经网络
- en: Then, in [Chapter 3](ch03.html#deep_learning_from_scratch), it will be straightforward
    to use these same building blocks to build deep learning models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在[第三章](ch03.html#deep_learning_from_scratch)中，使用这些相同的基本组件构建深度学习模型将变得简单。
- en: Before we dive into all this, though, let’s give an overview of *supervised
    learning*, the subset of machine learning that we’ll focus on as we see how to
    use neural networks to solve problems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究所有这些之前，让我们先概述一下*监督学习*，这是我们将专注于的机器学习的子集，我们将看到如何使用神经网络来解决问题。
- en: Supervised Learning Overview
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习概述
- en: At a high level, machine learning can be described as building algorithms that
    can uncover or “learn” *relationships* in data; supervised learning can be described
    as the subset of machine learning dedicated to finding relationships *between
    characteristics of the data that have already been measured*.^([1](ch02.html#idm45732627218840))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，机器学习可以被描述为构建能够揭示或“学习”数据中的*关系*的算法；监督学习可以被描述为机器学习的子集，专注于找到已经被测量的数据特征之间的关系。^([1](ch02.html#idm45732627218840))
- en: 'In this chapter, we’ll deal with a typical supervised learning problem that
    you might encounter in the real world: finding the relationship between characteristics
    of a house and the value of the house. Clearly, there is some relationship between
    characteristics such as the number of rooms, the square footage, or the proximity
    to schools and how desirable a house is to live in or own. At a high level, the
    aim of supervised learning is to uncover these relationships, given that we’ve
    *already measured* these characteristics.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理一个在现实世界中可能遇到的典型监督学习问题：找到房屋特征与房屋价值之间的关系。显然，诸如房间数量、平方英尺、或者与学校的距离等特征与一所房屋的居住或拥有价值之间存在某种关系。在高层次上，监督学习的目的是揭示这些关系，鉴于我们已经*测量了*这些特征。
- en: By “measure,” I mean that each characteristic has been defined precisely and
    represented as a number. Many characteristics of a house, such as the number of
    bedrooms, the square footage, and so on, naturally lend themselves to being represented
    as numbers, but if we had other, different kinds of information, such as natural
    language descriptions of the house’s neighborhood from TripAdvisor, this part
    of the problem would be much less straightforward, and doing the translation of
    this less-structured data into numbers in a reasonable way could make or break
    our ability to uncover relationships. In addition, for any concept that is ambiguously
    defined, such as the value of a house, we simply have to pick a single number
    to describe it; here, an obvious choice is to use the price of the house.^([2](ch02.html#idm45732627215256))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve translated our “characteristics” into numbers, we have to decide
    what structure to use to represent these numbers. One that is nearly universal
    across machine learning and turns out to make computations easy is to represent
    each set of numbers for a single observation—for example, a single house—as a
    *row* of data, and then stack these rows on top of each other to form “batches”
    of data that will get fed into our models as two-dimensional `ndarray`s. Our models
    will then return predictions as output `ndarray`s with each prediction in a row,
    similarly stacked on top of each other, with one prediction for each observation
    in the batch.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for some definitions: we say that the length of each row in this `ndarray`
    is the number of *features* of our data. In general, a single characteristic can
    map to many features, a classic example being a characteristic that describes
    our data as belonging to one of several *categories*, such as being a red brick
    house, a tan brick house, or a slate house;^([3](ch02.html#idm45732627208776))
    in this specific case we might describe this single characteristic with three
    features. The process of mapping what we informally think of as characteristics
    of our observations into features is called *feature engineering*. I won’t spend
    much time discussing this process in this book; indeed, in this chapter we’ll
    deal with a problem in which we have 13 characteristics of each observation, and
    we simply represent each characteristic with a single numeric feature.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: I said that the goal of supervised learning is ultimately to uncover relationships
    between characteristics of data. In practice, we do this by choosing one characteristic
    that we want to predict from the others; we call this characteristic our *target*.
    The choice of which characteristic to use as the target is completely arbitrary
    and depends on the problem you are trying to solve. For example, if your goal
    is just to *describe* the relationship between the prices of houses and the number
    of rooms they have, you could do this by training a model with the prices of houses
    as the target and the number of rooms as a feature, or vice versa; either way,
    the resulting model will indeed contain a description of the relationship between
    these two characteristics, allowing you to say, for example, a higher number of
    rooms in a house is associated with higher prices. On the other hand, if your
    goal is to *predict* the prices of houses *for which no price information is available*,
    you have to choose the price as your target, so that you can ultimately feed the
    other information into your model once it is trained.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-1](#fig_02-01) shows this hierarchy of descriptions of supervised
    learning, from the highest-level description of finding relationships in data,
    to the lowest level of quantifying those relationships by training models to uncover
    numerical representations between the features and the target.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised Learning overview](assets/dlfs_0201.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Supervised learning overview
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As mentioned, we’ll spend almost all our time on the level highlighted at the
    bottom of [Figure 2-1](#fig_02-01); nevertheless, in many problems, getting the
    parts at the top correct—collecting the right data, defining the problem you are
    trying to solve, and doing feature engineering—is much harder than the actual
    modeling. Still, since this book is focused on modeling—specifically, on understanding
    how deep learning models work—let’s return to that subject.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning Models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we know at a high level what supervised learning models are trying to do—and
    as I alluded to earlier in the chapter, such models are just nested, mathematical
    functions. We spent the last chapter seeing how to represent such functions in
    terms of diagrams, math, and code, so now I can state the goal of supervised learning
    more precisely in terms of both math and code (I’ll show plenty of diagrams later):
    the goal is to *find* (a mathematical function) / (a function that takes an `ndarray`
    as input and produces an `ndarray` as output) that can (map characteristics of
    observations to the target) / (given an input `ndarray` containing the features
    we created, produce an output `ndarray` whose values are “close to” the `ndarray`
    containing the target).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, our data will be represented in a matrix *X* with *n* rows, each
    of which represents an observation with *k* features, all of which are numbers.
    Each row observation will be a vector, as in <math><mrow><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub></mtd>
    <mtd><msub><mi>x</mi> <mrow><mi>i</mi><mn>2</mn></mrow></msub></mtd> <mtd><msub><mi>x</mi>
    <mrow><mi>i</mi><mn>3</mn></mrow></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
    , and these observations will be stacked on top of one another to form a batch.
    For example, a batch of size 3 would look like:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>13</mn></msub></mtd>
    <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>13</mn></msub></mtd>
    <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'For each batch of observations, we will have a corresponding batch of *targets*,
    each element of which is the target number for the corresponding observation.
    We can represent these in a one-dimensional vector:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>y</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>y</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced></math>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>y</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>y</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced></math>
- en: In terms of these arrays, our goal with supervised learning will be to use the
    tools I described in the last chapter to build a function that can take as input
    batches of observations with the structure of *X*[*batch*] and produce vectors
    of values *p*[i]—which we’ll interpret as “predictions”—that (for data in our
    particular dataset *X*, at least) are “close to the target values” *y*[i] for
    some reasonable measure of closeness.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we are ready to make all of this concrete and start building our first
    model for a real-world dataset. We’ll start with a straightforward model—*linear
    regression*—and show how to express it in terms of the building blocks from the
    prior chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression is often shown as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mo>×</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>β</mi> <mi>n</mi></msub>
    <mo>×</mo> <msub><mi>x</mi> <mi>k</mi></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mo>×</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>β</mi> <mi>n</mi></msub>
    <mo>×</mo> <msub><mi>x</mi> <mi>k</mi></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
- en: This representation describes mathematically our belief that the numeric value
    of each target is a linear combination of the *k* features of *X*, plus the *β*[0]
    term to adjust the “baseline” value of the prediction (specifically, the prediction
    that will be made when the value of all of the features is 0).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, doesn’t give us much insight into how we would code this up
    so that we could “train” such a model. To do that, we have to translate this model
    into the language of the functions we saw in [Chapter 1](ch01.html#foundations);
    the best place to start is with a diagram.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Regression: A Diagram'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can we represent linear regression as a computational graph? We *could*
    break it down all the way to the individual elements, with each *x*[i] being multiplied
    by another element *w*[i] and then the results being added together, as in [Figure 2-2](#fig_02-02).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression full](assets/dlfs_0202.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. The operations of a linear regression shown at the level of individual
    multiplications and additions
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But again, as we saw in [Chapter 1](ch01.html#foundations), if we can represent
    these operations as just a matrix multiplication, we’ll be able to write the function
    more concisely while still being able to correctly calculate the derivative of
    the output with respect to the input, which will allow us to train the model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we do this? First, let’s handle the simpler scenario in which we don’t
    have an intercept term (*β*[0] shown previously). Note that we can represent the
    output of a linear regression model as the *dot product* of each observation vector
    <math><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>1</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mi>k</mi></msub></mtd></mtr></mtable></mfenced></mrow></math>
    with another vector of parameters that we’ll call *W*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mi>k</mi></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mi>k</mi></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Our prediction would then simply be:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>×</mo> <mi>W</mi> <mo>=</mo> <msub><mi>w</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>+</mo>
    <msub><mi>w</mi> <mn>2</mn></msub> <mo>×</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mn>2</mn></mrow></msub>
    <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>w</mi> <mi>k</mi></msub> <mo>×</mo>
    <msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>×</mo> <mi>W</mi> <mo>=</mo> <msub><mi>w</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>+</mo>
    <msub><mi>w</mi> <mn>2</mn></msub> <mo>×</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mn>2</mn></mrow></msub>
    <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>w</mi> <mi>k</mi></msub> <mo>×</mo>
    <msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></math>
- en: 'So, we can represent “generating the predictions” for a linear regression using
    a single operation: the dot product.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, when we want to make predictions using linear regression with
    a batch of observations, we can use another, single operation: the matrix multiplication.
    If we have a batch of size 3, for example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>13</mn></msub></mtd>
    <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>13</mn></msub></mtd>
    <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'then performing the *matrix multiplication* of this batch *X*[*batch*] with
    *W* gives a vector of predictions for the batch, as desired:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>×</mo> <mi>W</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>13</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced>
    <mo>×</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi>
    <mn>3</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msub><mi>w</mi>
    <mi>k</mi></msub></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><mrow><msub><mi>x</mi> <mn>11</mn></msub> <mo>×</mo>
    <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>12</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>13</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd>
    <mtd><mrow><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi>
    <mn>21</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>22</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>23</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>2</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi>
    <mn>31</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>32</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>33</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>×</mo> <mi>W</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>13</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced>
    <mo>×</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi>
    <mn>3</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msub><mi>w</mi>
    <mi>k</mi></msub></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><mrow><msub><mi>x</mi> <mn>11</mn></msub> <mo>×</mo>
    <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>12</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>13</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd>
    <mtd><mrow><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi>
    <mn>21</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>22</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>23</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>2</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi>
    <mn>31</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>32</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>33</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: So generating predictions for a batch of observations in a linear regression
    can be done with a matrix multiplication. Next, I’ll show how to use this fact,
    along with the reasoning about derivatives from the prior chapter, to train this
    model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: “Training” this model
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What does it mean to “train” a model? At a high level, models^([4](ch02.html#idm45732626899528))
    take in data, combine them with *parameters* in some way, and produce predictions.
    For example, the linear regression model shown earlier takes in data *X* and parameters
    *W* and produces the predictions *p*[*batch*] using a matrix multiplication:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'To train our model, however, we need another crucial piece of information:
    whether or not these predictions are good. To learn this, we bring in the vector
    of *targets* *y*[*batch*] associated with the batch of observations *X*[*batch*]
    fed into the function, and we compute a *single number* that is a function of
    *y*[*batch*] and *p*[*batch*] and that represents the model’s “penalty” for making
    the predictions that it did. A reasonable choice is *mean squared error*, which
    is simply the average squared value that our model’s predictions “missed” by:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>M</mi> <mi>S</mi> <mi>E</mi> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>,</mo> <msub><mi>y</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>M</mi> <mi>S</mi> <mi>E</mi> <mrow><mo>(</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>,</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><msub><mi>y</mi> <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi>
    <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>p</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo><msup><mrow><mo>(</mo><msub><mi>y</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>p</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo><msup><mrow><mo>(</mo><msub><mi>y</mi> <mn>3</mn></msub> <mo>-</mo><msub><mi>p</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mn>3</mn></mfrac></mrow></math>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>M</mi> <mi>S</mi> <mi>E</mi> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>,</mo> <msub><mi>y</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>M</mi> <mi>S</mi> <mi>E</mi> <mrow><mo>(</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>,</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><msub><mi>y</mi> <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi>
    <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>p</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo><msup><mrow><mo>(</mo><msub><mi>y</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>p</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo><msup><mrow><mo>(</mo><msub><mi>y</mi> <mn>3</mn></msub> <mo>-</mo><msub><mi>p</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mn>3</mn></mfrac></mrow></math>
- en: 'Getting to this number, which we can call *L*, is key: once we have it, we
    can use all the techniques we saw in [Chapter 1](ch01.html#foundations) to compute
    the *gradient* of this number with respect to each element of *W*. Then *we can
    use these derivatives to update each element of W in the direction that would
    cause L to decrease*. Repeating this procedure many times, we hope, will “train”
    our model; in this chapter, we’ll see that this can indeed work in practice. To
    see clearly how to compute these gradients, we’ll complete the process of representing
    linear regression as a computational graph.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Regression: A More Helpful Diagram (and the Math)'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 2-3](#fig_02-03) shows how to represent linear regression in terms
    of the diagrams from the last chapter.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression simple](assets/dlfs_0203.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. The linear regression equations expressed as a computational graph—the
    dark blue letters are the data inputs to the function, and the light blue W denotes
    the weights
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, to reinforce that we’re still representing a nested mathematical function
    with this diagram, we could represent the loss value *L* that we ultimately compute
    as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>L</mi> <mo>=</mo> <mi>Λ</mi> <mo>(</mo> <mo>(</mo>
    <mi>ν</mi> <mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo> <mo>,</mo> <mi>Y</mi>
    <mo>)</mo></mrow></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>L</mi> <mo>=</mo> <mi>Λ</mi> <mo>(</mo> <mo>(</mo>
    <mi>ν</mi> <mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo> <mo>,</mo> <mi>Y</mi>
    <mo>)</mo></mrow></math>
- en: Adding in the Intercept
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Representing models as diagrams shows us conceptually how we can add an intercept
    to the model. We simply add an extra step at the end that involves adding a “bias,”
    as shown in [Figure 2-4](#fig_02-04).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](assets/dlfs_0204.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. The computational graph of linear regression, with the addition
    of a bias term at the end
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here, though, we should reason mathematically about what is going on before
    moving on to the code; with the bias added, each element of our model’s prediction
    *p*[*i*] will be the dot product described earlier with the quantity *b* added
    to it:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo>_</mo><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>_</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi></mrow></msub>
    <mo>=</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mtext>dot</mtext> <mi>W</mi> <mo>+</mo>
    <mi>b</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><msub><mi>x</mi>
    <mn>11</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>12</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>13</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub> <mo>+</mo> <mi>b</mi></mrow></mtd></mtr>
    <mtr><mtd><mrow><msub><mi>x</mi> <mn>21</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>22</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>23</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub> <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub>
    <mo>+</mo> <mi>b</mi></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi> <mn>31</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>32</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>33</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd>
    <mtd><mrow><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mi>k</mi></msub> <mo>+</mo> <mi>b</mi></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo>_</mo><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>_</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi></mrow></msub>
    <mo>=</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mtext>dot</mtext> <mi>W</mi> <mo>+</mo>
    <mi>b</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><msub><mi>x</mi>
    <mn>11</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>12</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>13</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub> <mo>+</mo> <mi>b</mi></mrow></mtd></mtr>
    <mtr><mtd><mrow><msub><mi>x</mi> <mn>21</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>22</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>23</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub> <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub>
    <mo>+</mo> <mi>b</mi></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi> <mn>31</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>32</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>33</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd>
    <mtd><mrow><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mi>k</mi></msub> <mo>+</mo> <mi>b</mi></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: Note that because the intercept in linear regression should be just a single
    number rather than being different for each observation, the *same number* should
    get added to each observation of the input to the bias operation that is passed
    in; we’ll discuss what this means for computing the derivatives in a later section
    of this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Regression: The Code'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll now tie things together and code up the function that makes predictions
    and computes losses given batches of observations *X*[*batch*] and their corresponding
    targets *y*[*batch*]. Recall that computing derivatives for nested functions using
    the chain rule involves two sets of steps: first, we perform a “forward pass,”
    passing the input successively forward through a series of operations and saving
    the quantities computed as we go; then we use those quantities to compute the
    appropriate derivatives during the backward pass.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code does this, saving the quantities computed on the forward
    pass in a dictionary; furthermore, to differentiate between the quantities computed
    on the forward pass and the parameters themselves (which we’ll also need for the
    backward pass), our function will expect to receive a dictionary containing the
    parameters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we have all the pieces in place to start “training” this model. Next, we’ll
    cover exactly what this means and how we’ll do it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now going to use all the tools we learned in the last chapter to compute
    <math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>w</mi>
    <mi>i</mi></msub></mrow></mfrac></math> for every *w*[*i*] in *W*, as well as
    <math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac></math>
    . How? Well, since the “forward pass” of this function was passing the input through
    a series of nested functions, the backward pass will simply involve computing
    the partial derivatives of each function, evaluating those derivatives at the
    functions’ inputs, and multiplying them together—and even though a matrix multiplication
    is involved, we’ll be able to handle this using the reasoning we covered in the
    last chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the Gradients: A Diagram'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conceptually, we want something like what is depicted in [Figure 2-5](#fig_02-05).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression full](assets/dlfs_0205.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. The backward pass through the linear regression computational graph
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We simply step backward, computing the derivative of each constituent function
    and evaluating those derivatives at the inputs that those functions received on
    the forward pass, and then multiplying these derivatives together at the end.
    This is straightforward enough, so let’s get into the details.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the Gradients: The Math (and Some Code)'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From [Figure 2-5](#fig_02-05), we can see that the derivative product that
    we ultimately want to compute is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>×</mo>
    <mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>×</mo>
    <mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>×</mo>
    <mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>×</mo>
    <mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
- en: There are three components here; let’s compute each of them in turn.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'First up: <math><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow></mrow></math>
    . Since <math><mrow><mi>Λ</mi> <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mi>Y</mi><mo>-</mo><mi>P</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> for each element in *Y* and *P*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mn>1</mn> <mo>×</mo> <mrow><mo>(</mo> <mn>2</mn> <mo>×</mo> <mrow><mo>(</mo>
    <mi>Y</mi> <mo>-</mo> <mi>P</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mn>1</mn> <mo>×</mo> <mrow><mo>(</mo> <mn>2</mn> <mo>×</mo> <mrow><mo>(</mo>
    <mi>Y</mi> <mo>-</mo> <mi>P</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: 'We’re jumping ahead of ourselves a bit, but note that coding this up would
    simply be:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有点超前了，但请注意编写这个代码只是简单的：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we have an expression involving matrices: <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow>
    <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow></mrow></math> . But since *α* is just addition, the
    same logic that we reasoned through with numbers in the prior chapter applies
    here: increasing any element of *N* by one unit will increase <math><mrow><mi>P</mi>
    <mo>=</mo> <mi>α</mi> <mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo>
    <mi>N</mi> <mo>+</mo> <mi>B</mi></mrow></math> by one unit. Thus, <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow>
    <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow></mrow></math> is just a matrix of +1+s, of the same
    shape as *N*.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有涉及矩阵的表达式：<math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow></mrow></math>
    。但由于 *α* 只是加法，我们在前一章节中对数字推理的逻辑同样适用于这里：将 *N* 的任何元素增加一个单位将使 <math><mrow><mi>P</mi>
    <mo>=</mo> <mi>α</mi> <mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo>
    <mi>N</mi> <mo>+</mo> <mi>B</mi></mrow></math> 增加一个单位。因此，<math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow>
    <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow></mrow></math> 只是一个由 +1 组成的矩阵，形状与 *N* 相同。
- en: 'Coding *this* expression, therefore, would simply be:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，编码*这个*表达式只是：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we have <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
    . As we discussed in detail in the last chapter, when computing derivatives of
    nested functions where one of the constituent functions is a matrix multiplication,
    we can act *as if*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
    。正如我们在上一章节中详细讨论的，当计算嵌套函数的导数时，其中一个组成函数是矩阵乘法时，我们可以*假设*：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mi>X</mi> <mi>T</mi></msup></mrow></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mi>X</mi> <mi>T</mi></msup></mrow></math>
- en: 'which in code is simply:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这只是简单的：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We’ll do the same for the intercept term; since we are just adding it, the
    partial derivative of the intercept term with respect to the output is simply
    1:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对截距项执行相同的操作；因为我们只是将其添加，截距项对输出的偏导数就是 1：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The last step is to simply multiply these together, making sure we use the correct
    order for the matrix multiplications involving `dNdW` and `dNdX` based on what
    we reasoned through at the end of the last chapter.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步就是简单地将它们相乘在一起，确保我们根据我们在上一章节末尾推理出的正确顺序进行涉及 `dNdW` 和 `dNdX` 的矩阵乘法。
- en: 'Calculating the Gradients: The (Full) Code'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算梯度：（完整）代码
- en: 'Recall that our goal is to take everything computed on or inputed into the
    forward pass—which, from the diagram in [Figure 2-5](#fig_02-05), will include
    *X*, *W*, *N*, *B*, *P*, and *y*—and compute <math><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow>
    <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac></math> and <math><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow>
    <mrow><mi>∂</mi><mi>B</mi></mrow></mfrac></math> . The following code does that,
    receiving *W* and *B* as inputs in a dictionary called `weights` and the rest
    of the quantities in a dictionary called `forward_info`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住我们的目标是取出在前向传播中计算的或输入的所有内容——从 [Figure 2-5](#fig_02-05) 中的图表中，这将包括 *X*、*W*、*N*、*B*、*P*
    和 *y*——并计算 <math><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac></math>
    和 <math><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>B</mi></mrow></mfrac></math>
    。以下代码实现了这一点，接收 *W* 和 *B* 作为名为 `weights` 的字典中的输入，其余的量作为名为 `forward_info` 的字典中的输入：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, we simply compute the derivatives with respect to each operation
    and successively multiply them together, taking care that we do the matrix multiplication
    in the right order.^([5](ch02.html#idm45732626271592)) As we’ll see shortly, this
    actually works—and after the intuition we built up around the chain rule in the
    last chapter, this shouldn’t be too surprising.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们只需计算每个操作的导数，然后逐步将它们相乘在一起，确保我们按正确顺序进行矩阵乘法。正如我们很快将看到的，这实际上是有效的——在我们在上一章节围绕链式法则建立的直觉之后，这应该不会太令人惊讶。
- en: Note
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'An implementation detail about those loss gradients: we’re storing them as
    a dictionary, with the names of the weights as keys and the amounts that increasing
    the weights affect the losses as values. The `weights` dictionary is structured
    the same way. Therefore, we’ll iterate through the weights in our model in the
    following way:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些损失梯度的实现细节：我们将它们存储为一个字典，其中权重的名称作为键，增加权重影响损失的数量作为值。`weights` 字典的结构也是一样的。因此，我们将按照以下方式迭代我们模型中的权重：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There is nothing special about storing them in this way; if we stored them differently,
    we would simply iterate through them and refer to them differently.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式存储它们并没有什么特别之处；如果我们以不同的方式存储它们，我们只需迭代它们并以不同的方式引用它们。
- en: Using These Gradients to Train the Model
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用这些梯度来训练模型。
- en: 'Now we’ll simply run the following procedure over and over again:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需一遍又一遍地运行以下过程：
- en: Select a batch of data.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一批数据。
- en: Run the forward pass of the model.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行模型的前向传播。
- en: Run the backward pass of the model using the info computed on the forward pass.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在前向传播中计算的信息运行模型的反向传播。
- en: Use the gradients computed on the backward pass to update the weights.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在反向传播中计算的梯度来更新权重。
- en: 'The [Jupyter Notebook](https://oreil.ly/2TDV5q9) for this chapter of the book
    includes a `train` function that codes this up. It isn’t too interesting; it simply
    implements the preceding steps and adds a few sensible things such as shuffling
    the data to ensure that it is fed through in a random order. The key lines, which
    get repeated inside of a `for` loop, are these:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的这一章节的 [Jupyter Notebook](https://oreil.ly/2TDV5q9) 包含一个名为 `train` 的函数，用于编写这个。这并不太有趣；它只是实现了前面的步骤，并添加了一些明智的事情，比如对数据进行洗牌以确保以随机顺序传递。关键的代码行在一个
    `for` 循环内重复，如下所示：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we run the `train` function for a certain number of *epochs*, or cycles
    through the entire training dataset, as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们运行`train`函数一定数量的*周期*，或者遍历整个训练数据集，如下所示：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `train` function returns `train_info`, a `Tuple`, one element of which is
    the parameters or *weights* that represent what the model has learned.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`函数返回`train_info`，一个`Tuple`，其中一个元素是代表模型学习内容的参数或*权重*。'
- en: Note
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The terms “parameters” and “weights” are used interchangeably throughout deep
    learning, so we will use them interchangeably in this book.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: “参数”和“权重”这两个术语在深度学习中通常是可以互换使用的，因此在本书中我们将它们互换使用。
- en: 'Assessing Our Model: Training Set Versus Testing Set'
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们的模型：训练集与测试集
- en: To understand whether our model uncovered relationships in our data, we have
    to introduce some terms and ways of thinking from statistics. We think of any
    dataset received as being a *sample* from a *population*. Our goal is always to
    find a model that uncovers relationships in the population, despite us seeing
    only a sample.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们的模型是否揭示了数据中的关系，我们必须从统计学中引入一些术语和思考方式。我们认为收到的任何数据集都是从一个*总体*中抽取的*样本*。我们的目标始终是找到一个能够揭示总体关系的模型，尽管我们只看到了一个样本。
- en: There is always a danger that we build a model that picks up relationships that
    exist in the sample but not in the population. For example, it might be the case
    in our sample that yellow slate houses with three bathrooms are relatively inexpensive,
    and a complicated neural network model we build could pick up on this relationship
    even though it may not exist in the population. This is a problem known as *overfitting*.
    How can we detect whether a model structure we use is likely to have this problem?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立的模型可能会捕捉到样本中存在但在总体中不存在的关系。例如，在我们的样本中，黄色板岩房屋带有三个浴室可能相对便宜，我们构建的复杂神经网络模型可能会捕捉到这种关系，尽管在总体中可能不存在。这是一个被称为*过拟合*的问题。我们如何检测我们使用的模型结构是否可能存在这个问题？
- en: The solution is to split our sample into a *training set* and a *testing set*.
    We use the training data to train the model (that is, to iteratively update the
    weights), and then we evaluate the model on the testing set to estimate its performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是将我们的样本分成一个*训练集*和一个*测试集*。我们使用训练数据来训练模型（即迭代更新权重），然后我们在测试集上评估模型以估计其性能。
- en: The full logic here is that if our model was able to successfully pick up on
    relationships that generalize from the *training set* to *the rest of the sample*
    (our whole dataset), then it is likely that the same “model structure” will generalize
    from our *sample*—which, again, is our entire dataset—to the *population*, which
    is what we want.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的逻辑是，如果我们的模型能够成功地发现从*训练集*到*样本其余部分*（我们的整个数据集）泛化的关系，那么同样的“模型结构”很可能会从我们的*样本*——再次强调，是我们的整个数据集——泛化到*总体*，这正是我们想要的。
- en: 'Assessing Our Model: The Code'
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们的模型：代码
- en: 'With that understanding, let’s evaluate our model on the testing set. First,
    we’ll write a function to generate predictions by truncating the `forward_pass`
    function we saw previously:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个理解，让我们在测试集上评估我们的模型。首先，我们将编写一个函数，通过截断我们之前看到的`forward_pass`函数来生成预测：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we simply use the weights returned earlier from the `train` function and
    write:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们只需使用`train`函数之前返回的权重，并写：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How good are these predictions? Keep in mind that at this point we haven’t validated
    our seemingly strange approach of defining models as a series of operations, and
    training them by iteratively adjusting the parameters involved using the partial
    derivatives of the loss calculated with respect to the parameters using the chain
    rule; thus, we should be pleased if this approach works at all.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测有多好？请记住，目前我们还没有验证我们看似奇怪的定义模型的方法，即将模型定义为一系列操作，并通过使用损失的偏导数来调整涉及的参数，使用链式法则迭代地训练模型；因此，如果这种方法有效，我们应该感到高兴。
- en: The first thing we can do to see whether our model worked is to make a plot
    with the model’s predictions on the x-axis and the actual values on the y-axis.
    If every point fell exactly on the 45-degree line, the model would be perfect.
    [Figure 2-6](#fig_02-06) shows a plot of our model’s predicted and actual values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的第一件事是查看我们的模型是否有效，即制作一个图，其中模型的预测值在x轴上，实际值在y轴上。如果每个点都恰好落在45度线上，那么模型就是完美的。[图2-6](#fig_02-06)显示了我们模型的预测值和实际值的图。
- en: '![Custom prediction versus actual](assets/dlfs_0206.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![自定义预测与实际](assets/dlfs_0206.png)'
- en: Figure 2-6\. Predicted versus actual values for our custom linear regression
    model
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6。我们自定义线性回归模型的预测与实际值
- en: 'Our plot looks pretty good, but let’s quantify how good the model is. There
    are a couple of common ways to do that:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图看起来很不错，但让我们量化一下模型的好坏。有几种常见的方法可以做到这一点：
- en: 'Calculate the mean distance, in absolute value, between our model’s predictions
    and the actual values, a metric called *mean absolute error*:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算我们模型预测值与实际值之间的平均距离，即*平均绝对误差*：
- en: '[PRE11]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Calculate the mean squared distance between our model’s predictions and the
    actual values, a metric known as *root mean squared error*:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算我们模型预测值与实际值之间的平均平方距离，这个指标称为*均方根误差*：
- en: '[PRE12]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The values for this particular model are:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定模型的值为：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Root mean squared error is a particularly common metric since it is on the same
    scale as the target. If we divide this number by the mean value of the target,
    we can get a measure of how far off a prediction is, on average, from its actual
    value. Since the mean value of `y_test` is `22.0776`, we see that this model’s
    predictions of house prices are off by 5.0508 / 22.0776 ≅ 22.9% on average.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根误差是一个特别常见的指标，因为它与目标在同一尺度上。如果我们将这个数字除以目标的平均值，我们可以得到一个预测值与实际值之间平均偏差的度量。由于`y_test`的平均值为`22.0776`，我们看到这个模型对房价的预测平均偏差为5.0508
    / 22.0776 ≅ 22.9%。
- en: So are these numbers any good? In the [Jupyter Notebook](https://oreil.ly/2TDV5q9)
    containing the code for this chapter, I show that performing a linear regression
    on this dataset using the most popular Python library for machine learning, Sci-Kit
    Learn, results in a mean absolute error and root mean squared error of `3.5666`
    and `5.0482`, respectively, which are virtually identical to what we calculated
    in our “first-principles-based” linear regression previously. This should give
    you confidence that the approach we’ve been taking so far in this book is in fact
    a valid approach for reasoning about and training models! Both later in this chapter,
    and in the next chapter we’ll extend this approach to neural networks and deep
    learning models.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字好吗？在包含本章代码的[Jupyter Notebook](https://oreil.ly/2TDV5q9)中，我展示了使用最流行的Python机器学习库Sci-Kit
    Learn对这个数据集进行线性回归的结果，平均绝对误差和均方根误差分别为`3.5666`和`5.0482`，与我们之前计算的“基于第一原理”的线性回归几乎相同。这应该让你相信，我们在本书中迄今为止采取的方法实际上是一种用于推理和训练模型的有效方法！在本章后面以及下一章中，我们将把这种方法扩展到神经网络和深度学习模型。
- en: Analyzing the Most Important Feature
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析最重要的特征
- en: 'Before beginning modeling, we scaled each feature of our data to have mean
    0 and standard deviation 1; this has computational advantages that we’ll discuss
    in more detail in [Chapter 4](ch04.html#extensions). A benefit of doing this that
    is specific to linear regression is that we can interpret the absolute values
    of the coefficients as corresponding to the importance of the different features
    to the model; a larger coefficient means that the feature is more important. Here
    are the coefficients:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始建模之前，我们将数据的每个特征缩放为均值为0，标准差为1；这具有计算优势，我们将在[第4章](ch04.html#extensions)中更详细地讨论。这样做的好处是，对于线性回归来说，我们可以解释系数的绝对值与模型中不同特征的重要性相对应；较大的系数意味着该特征更重要。以下是系数：
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The fact that the last coefficient is largest means that the last feature in
    the dataset is the most important one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个系数最大的事实意味着数据集中的最后一个特征是最重要的。
- en: In [Figure 2-7](#fig_02-07), we plot this feature against our target.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-7](#fig_02-07)中，我们将这个特征与我们的目标绘制在一起。
- en: '![Custom prediction versus actual](assets/dlfs_0207.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![自定义预测与实际](assets/dlfs_0207.png)'
- en: Figure 2-7\. Most important feature versus target in custom linear regression
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7。自定义线性回归中最重要的特征与目标
- en: 'We see that this feature is indeed strongly correlated with the target: as
    this feature increases, the value of the target decreases, and vice versa. However,
    this relationship is *not* linear. The expected amount that the target changes
    as the feature changes from –2 to –1 is *not* the same amount that it changes
    as the feature changes from 1 to 2\. We’ll come back to this later.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这个特征与目标确实强相关：随着这个特征的增加，目标值减少，反之亦然。然而，这种关系*不*是线性的。当特征从-2变为-1时，目标变化的预期量*不*等于特征从1变为2时的变化量。我们稍后会回到这个问题。
- en: 'In [Figure 2-8](#fig_02-08), we overlay onto this plot the relationship between
    this feature and the *model predictions*. We’ll generate this by feeding the following
    data through our trained model:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-8](#fig_02-08)中，我们将这个特征与*模型预测*之间的关系叠加到这个图中。我们将通过将以下数据馈送到我们训练过的模型中来生成这个图：
- en: The values of all features set equal to their mean
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有特征的值设置为它们的均值
- en: The values of the most important feature linearly interpolated over 40 steps
    from –1.5 to 3.5, which is roughly the range of this scaled feature in our data
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要特征的值在-1.5到3.5之间线性插值，这大致是我们数据中这个缩放特征的范围
- en: '![Custom prediction versus actual](assets/dlfs_0208.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![自定义预测与实际](assets/dlfs_0208.png)'
- en: Figure 2-8\. Most important feature versus target and predictions in custom
    linear regression
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8。自定义线性回归中最重要的特征与目标和预测
- en: 'This figure shows (literally) a limitation of linear regression: despite the
    fact that there is a visually clear and “model-able” *non*linear relationship
    between this feature and the target, our model is only able to “learn” a linear
    relationship because of its intrinsic structure.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图（字面上）展示了线性回归的一个局限性：尽管这个特征与目标之间存在一个视觉上明显且“可建模”的*非*线性关系，但由于其固有结构，我们的模型只能“学习”线性关系。
- en: To have our model learn a more complex, nonlinear relationship between our features
    and our target, we’re going to have to build a more complicated model than linear
    regression. But how? The answer will lead us, in a principles-based way, to building
    a neural network.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的模型学习特征和目标之间更复杂、非线性的关系，我们将不得不构建一个比线性回归更复杂的模型。但是如何做呢？答案将以基于原则的方式引导我们构建一个神经网络。
- en: Neural Networks from Scratch
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始的神经网络
- en: We’ve just seen how to build and train a linear regression model from first
    principles. How can we extend this chain of reasoning to design a more complex
    model that can learn nonlinear relationships? The central idea is that we’ll first
    do *many* linear regressions, then feed the results through a nonlinear function,
    and finally do one last linear regression that ultimately makes the predictions.
    As it will turn out, we can reason through how to compute the gradients for this
    more complicated model in the same way we did for the linear regression model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到如何从第一原理构建和训练线性回归模型。我们如何将这种推理链扩展到设计一个可以学习非线性关系的更复杂模型？中心思想是，我们首先进行*许多*线性回归，然后将结果馈送到一个非线性函数中，最后进行最后一次线性回归，最终进行预测。事实证明，我们可以通过与线性回归模型相同的方式推理出如何计算这个更复杂模型的梯度。
- en: 'Step 1: A Bunch of Linear Regressions'
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：一堆线性回归
- en: 'What does it mean to do “a bunch of linear regressions”? Well, doing one linear
    regression involved doing a matrix multiplication with a set of parameters: if
    our data *X* had dimensions `[batch_size, num_features]`, then we multiplied it
    by a weight matrix *W* with dimensions `[num_features, 1]` to get an output of
    dimension `[batch_size, 1]`; this output is, for each observation in the batch,
    simply a *weighted sum* of the original features. To do multiple linear regressions,
    we’ll simply multiply our input by a weight matrix with dimensions `[num_features,
    num_outputs]`, resulting in an output of dimensions `[batch_size, num_outputs]`;
    now, *for each observation*, we have `num_outputs` different weighted sums of
    the original features.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: What are these weighted sums? We should think of each of them as a “learned
    feature”—a combination of the original features that, once the network is trained,
    will represent its attempt to learn combinations of features that help it accurately
    predict house prices. How many learned features should we create? Let’s create
    13 of them, since we created 13 original features.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: A Nonlinear Function'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we’ll feed each of these weighted sums through a *non*linear function;
    the first function we’ll try is the `sigmoid` function that was mentioned in [Chapter 1](ch01.html#foundations).
    As a refresher, [Figure 2-9](#fig_02-09) plots the `sigmoid` function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Sigmoid](assets/dlfs_0209.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Sigmoid function plotted from x = –5 to x = 5
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why is using this nonlinear function a good idea? Why not the `square` function
    *f*(*x*) = *x*², for example? There are a couple of reasons. First, we want the
    function we use here to be *monotonic* so that it “preserves” information about
    the numbers that were fed in. Let’s say that, given the date that was fed in,
    two of our linear regressions produced values of –3 and 3, respectively. Feeding
    these through the `square` function would then produce a value of 9 for each,
    so that any function that receives these numbers as inputs after they were fed
    through the `square` function would “lose” the information that one of them was
    originally –3 and the other was 3.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The second reason, of course, is that the function is nonlinear; this nonlinearity
    will enable our neural network to model the inherently nonlinear relationship
    between the features and the target.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `sigmoid` function has the nice property that its derivative can
    be expressed in terms of the function itself:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>σ</mi></mrow> <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>σ</mi></mrow> <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: We’ll make use of this shortly when we use the `sigmoid` function in the backward
    pass of our neural network.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Another Linear Regression'
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we’ll take the resulting 13 elements—each of which is a combination
    of the original features, fed through the `sigmoid` function so that they all
    have values between 0 and 1—and feed them into a regular linear regression, using
    them the same way we used our original features previously.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we’ll try training the *entire* resulting function in the same way we
    trained the standard linear regression earlier in this chapter: we’ll feed data
    through the model, use the chain rule to figure out how much increasing the weights
    would increase (or decrease) the loss, and then update the weights in the direction
    that decreases the loss at each iteration. Over time (we hope) we’ll end up with
    a more accurate model than before, one that has “learned” the inherent nonlinearity
    of the relationship between our features and our target.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: It might be tough to wrap your mind around what’s going on based on this description,
    so let’s look at an illustration.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Diagrams
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 2-10](#fig_02-10) is a diagram of what our more complicated model now
    looks like.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural network forward pass](assets/dlfs_0210.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. Steps 1–3 translated into a computational graph of the kind we
    saw in [Chapter 1](ch01.html#foundations)
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You’ll see that we start with matrix multiplication and matrix addition, as
    before. Now let’s formalize some terminology that was mentioned previously: when
    we apply these operations in the course of a nested function, we’ll call the first
    matrix that we use to transform the input features the *weight* matrix, and we’ll
    call the second matrix, the one that is added to each resulting set of features,
    the *bias*. That’s why we’ll denote these as *W*[1] and *B*[1].'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: After applying these operations, we’ll feed the results through a sigmoid function
    and then repeat the process again with *another* set of weights and biases—now
    called *W*[2] and *B*[2]—to get our final prediction, *P*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Another diagram?
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Does representing things in terms of these individual steps give you intuition
    for what is going on? This question gets at a key theme of this book: to fully
    understand neural networks, we have to see multiple representations, each one
    of which highlights a different aspect of how neural networks work. The representation
    in [Figure 2-10](#fig_02-10) doesn’t give much intuition about the “structure”
    of the network, but it does indicate clearly how to train such a model: on the
    backward pass, we’ll compute the partial derivative of each constituent function,
    evaluated at the input to that function, and then calculate the gradients of the
    loss with respect to each of the weights by simply multiplying all of these derivatives
    together—just as we saw in the simple chain rule examples from [Chapter 1](ch01.html#foundations).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, there is another, more standard way to represent a neural network
    like this: we could represent each of our original features as circles. Since
    we have 13 features, we need 13 circles. Then we need 13 more circles to represent
    the 13 outputs of the “linear regression-sigmoid” operation we’re doing. In addition,
    each of these circles is a function of all 13 of our original features, so we’ll
    need lines connecting all of the first set of 13 circles to all of the second
    set.^([6](ch02.html#idm45732625500232))'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Finally, all of these 13 outputs are used to make a single final prediction,
    so we’ll draw one more circle to represent the final prediction and 13 lines showing
    that these “intermediate outputs” are “connected” to this final prediction.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-11](#fig_02-11) shows the final diagram.^([7](ch02.html#idm45732625497016))'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural network representation 2](assets/dlfs_0211.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. A more common (but in many ways less helpful) visual representation
    of a neural network
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you’ve read anything about neural networks before, you may have seen them
    represented like the diagram in [Figure 2-11](#fig_02-11): as circles with lines
    connecting them. While this representation does have some advantages—it lets you
    see “at a glance” what kind of neural network this is, how many layers it has,
    and so on—it doesn’t give any indication of the actual calculations involved,
    or of how such a network might be trained. Therefore, while this diagram is extremely
    important for you to see because you’ll see it in other places, it’s included
    here primarily so you can see the *connection* between it and the primary way
    we are representing neural networks: as boxes with lines connecting them, where
    each box represents a function that defines both what should happen on the forward
    pass for the model to make predictions and what should happen on the backward
    pass for the model to learn. We’ll see in the next chapter how to translate even
    more directly between these diagrams and code by coding each function as a Python
    class inheriting from a base `Operation` class—and speaking of code, let’s cover
    that next.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Code
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In coding this up, we follow the same function structure as in the simpler
    linear regression function from earlier in the chapter—taking in `weights` as
    a dictionary and returning both the loss value and the `forward_info` dictionary,
    while replacing the internals with the operations specified in [Figure 2-10](#fig_02-10):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Even though we’re now dealing with a more complicated diagram, we’re still just
    going step by step through each operation, doing the appropriate computation,
    and saving the results in `forward_info` as we go.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Networks: The Backward Pass'
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The backward pass works the same way as in the simpler linear regression model
    from earlier in the chapter, just with more steps.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Diagram
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The steps, as a reminder, are:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Compute the derivative of each operation and evaluate it at its input.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply the results together.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we’ll see yet again, this will work because of the chain rule. [Figure 2-12](#fig_02-12)
    shows all the partial derivatives we have to compute.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural network regression backward](assets/dlfs_0212.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. The partial derivatives associated with each operation in the
    neural network that will be multiplied together on the backward pass
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conceptually, we want to compute all these partial derivatives, tracing backward
    through our function, and then multiply them together to get the gradients of
    the loss with respect to each of the weights, just as we did for the linear regression
    model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Math (and code)
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 2-1](#table-2-1) lists these partial derivatives and the lines in the
    code that correspond to each one.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Derivative table for neural network
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '| Derivative | Code |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></mrow></math>
    | `dLdP = -(forward_info[*y*] - forward_info[*P*])` |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>M</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `np.ones_like(forward_info[*M2*])` |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>B</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `np.ones_like(weights[*B2*])` |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>W</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>O</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dM2dW2 = np.transpose(forward_info[*O1*], (1, 0))` |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>O</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>O</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dM2dO1 = np.transpose(weights[*W2*], (1, 0))` |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>σ</mi></mrow> <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac>
    <mrow><mo>(</mo> <msub><mi>N</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dO1dN1 = sigmoid(forward_info[*N1*] × (1 - sigmoid(forward_info[*N1*])` |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>M</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dN1dM1 = np.ones_like(forward_info[*M1*])` |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>B</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dN1dB1 = np.ones_like(weights[*B1*])` |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>W</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <msub><mi>W</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math> | `dM1dW1 = np.transpose(forward_info[*X*],
    (1, 0))` |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: Note
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The expressions we compute for the gradient of the loss with respect to the
    bias terms, `dLdB1` and `dLdB2`, will have to be summed along the rows to account
    for the fact that the same bias element is added to each row in the batch of data
    passed through. See [“Gradient of the Loss with Respect to the Bias Terms”](app01.html#gradient-loss-bias-terms)
    for details.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: The overall loss gradient
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can see the full `loss_gradients` function in the [Jupyter Notebook](https://oreil.ly/2TDV5q9)
    for this chapter on the book’s GitHub page. This function computes each of the
    partial derivatives in [Table 2-1](#table-2-1) and multiplies them together to
    get the gradients of the loss with respect to each of the `ndarray`s containing
    the weights:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '`dLdW2`'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dLdB2`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dLdW1`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dLdB1`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only caveat is that we sum the expressions we compute for `dLdB1` and `dLdB2`
    along `axis = 0`, as described in [“Gradient of the Loss with Respect to the Bias
    Terms”](app01.html#gradient-loss-bias-terms).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: We’ve finally built our first neural network from scratch! Let’s see if it is
    in fact any better than our linear regression model.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Training and Assessing Our First Neural Network
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just as the forward and backward passes worked the same for our neural network
    as for the linear regression model from earlier in the chapter, so too are training
    and evaluation the same: for each iteration of data, we pass the input forward
    through the function on the forward pass, compute the gradients of loss with respect
    to the weights on the backward pass, and then use these gradients to update the
    weights. In fact, we can use the following identical code inside the training
    loop:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The difference simply lies in the internals of the `forward_loss` and `loss_gradients`
    functions, and in the `weights` dictionary, which now has four keys (`W1`, `B1`,
    `W2`, and `B2`) instead of two. Indeed, this is a major takeaway from this book:
    even for very complex architectures, the mathematical principles and high-level
    training procedures are the same as for simple models.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'We also get predictions from this model in the same way:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The difference again is simply in the internals of the `predict` function:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Using these predictions, we can calculate the mean absolute error and root
    mean squared error on the validation set, as before:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Both values are significantly lower than the prior model! Looking at the plot
    of predictions versus actuals in [Figure 2-13](#fig_02-13) shows similar improvements.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![dPdB one element](assets/dlfs_0213.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: Figure 2-13\. Predicted value versus target, in neural network regression
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visually, the points look closer to the 45-degree line than in [Figure 2-6](#fig_02-06).
    I encourage you to step through the [Jupyter Notebook](https://oreil.ly/2TDV5q9)
    on the book’s GitHub page and run the code yourself!
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Two Reasons Why This Is Happening
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why does this model appear to be performing better than the model before? Recall
    that there was a *nonlinear* relationship between the most important feature of
    our earlier model and our target; nevertheless, our model was constrained to learn
    only *linear* relationships between individual features and our target. I claim
    that, by adding a nonlinear function into the mix, we have allowed our model to
    learn the proper, nonlinear relationship between our features and our target.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Let’s visualize this. [Figure 2-14](#fig_02-14) shows the same plot we showed
    in the linear regression section, plotting the normalized values of the most important
    feature from our model along with both the values of the target and the *predictions*
    that would result from feeding the mean values of the other features while varying
    the values of the most important feature from –3.5 to 1.5, as before.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![dPdB one element](assets/dlfs_0214.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 2-14\. Most important feature versus target and predictions, neural network
    regression
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that the relationship shown (a) is now nonlinear and (b) more closely
    matches the relationship between this feature and the target (represented by the
    points), as desired. So adding the nonlinear function to our model allowed it
    to learn, via iteratively updating the weights using the training, the nonlinear
    relationship that existed between the inputs and the outputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s the first reason why our neural network performed better than a straightforward
    linear regression. The second reason is that our neural network can learn relationships
    between *combinations* of our original features and our target, as opposed to
    just individual features. This is because the neural network uses a matrix multiplication
    to create 13 “learned features,” each of which is a combination of all the original
    features, and then essentially applies another linear regression on top of these
    learned features. For example, doing some exploratory analysis that is shared
    on the book’s website, we can see that the most important combinations of the
    13 original features that the model has learned are:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mo>-</mo> <mn>4.44</mn> <mo>×</mo> <msub><mi>feature</mi>
    <mn>6</mn></msub> <mo>-</mo> <mn>2.77</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>1</mn></msub>
    <mo>-</mo> <mn>2.07</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>7</mn></msub> <mo>+</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mo>-</mo> <mn>4.44</mn> <mo>×</mo> <msub><mi>feature</mi>
    <mn>6</mn></msub> <mo>-</mo> <mn>2.77</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>1</mn></msub>
    <mo>-</mo> <mn>2.07</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>7</mn></msub> <mo>+</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>
- en: 'and:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mn>4.43</mn> <mo>×</mo> <msub><mi>feature</mi>
    <mn>2</mn></msub> <mo>-</mo> <mn>3.39</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>4</mn></msub>
    <mo>-</mo> <mn>2.39</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>1</mn></msub> <mo>+</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mn>4.43</mn> <mo>×</mo> <msub><mi>feature</mi>
    <mn>2</mn></msub> <mo>-</mo> <mn>3.39</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>4</mn></msub>
    <mo>-</mo> <mn>2.39</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>1</mn></msub> <mo>+</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>
- en: These will then be included, along with 11 other learned features, in a linear
    regression in the last two layers of the neural network.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: These two things—learning *nonlinear* relationships between individual features
    and our target, and learning relationships between *combinations* of features
    and our target—are what allow neural networks to often work better than straightforward
    regressions on real-world problems.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use the building blocks and mental models
    from [Chapter 1](ch01.html#foundations) to understand, build, and train two standard
    machine learning models to solve real problems. I started by showing how to represent
    a simple machine learning model from classical statistics—linear regression—using
    a computational graph. This representation allowed us to compute the gradients
    of the loss from this model with respect to the model’s parameters and thus train
    the model by continually feeding in data from the training set and updating the
    model’s parameters in the direction that would decrease the loss.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we saw a limitation of this model: it can only learn *linear* relationships
    between the features and target; this motivated us to try building a model that
    could learn *nonlinear* relationships between the features and target, which led
    us to build our first neural network. You learned how neural networks work by
    building one from scratch, and you also learned how to train them using the same
    high-level procedure we used to train our linear regression models. You then saw
    empirically that the neural network performed better than the simple linear regression
    model and learned two key reasons why: the neural network was able to learn *nonlinear*
    relationships between the features and the target and also to learn relationships
    between *combinations* of features and the target.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, there’s a reason we ended this chapter still covering a relatively
    simple model: defining neural networks in this way is an extremely manual process.
    Defining the forward pass involved 6 individually coded operations, and the backward
    pass involved 17\. However, discerning readers will have noticed that there is
    a lot of repetition in these steps, and by properly defining abstractions, we
    can move from defining models in terms of individual operations (as in this chapter)
    to defining models in terms of these abstractions. This will allow us to build
    more complex models, including deep learning models, while deepening our understanding
    of how these models work. That is what we’ll begin to do in the next chapter.
    Onward!'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#idm45732627218840-marker)) The other kind of machine learning,
    *un*supervised learning, can be thought of as finding relationships between things
    you have measured and things that have not been measured yet.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch02.html#idm45732627215256-marker)) Though in a real-world problem,
    even how to choose a price isn’t obvious: would it be the price the house last
    sold for? What about a house that hasn’t been on the market for a long time? In
    this book, we’ll focus on examples in which the numeric representation of the
    data is obvious or has been decided for you, but in many real-world problems,
    getting this right is critical.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#idm45732627208776-marker)) Most of you probably know that these
    are called “categorical” features.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.html#idm45732626899528-marker)) At least the ones we’ll see in this
    book.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch02.html#idm45732626271592-marker)) In addition, we have to sum `dLdB`
    along axis 0; we explain this step in more detail later in this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch02.html#idm45732625500232-marker)) This highlights an interesting idea:
    we *could* have outputs that are connected to only *some* of our original features;
    this is in fact what convolutional neural networks do.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch02.html#idm45732625497016-marker)) Well, not quite: we haven’t drawn
    *all* of the 169 lines we would need to show all the connections between the first
    two “layers” of features, but we have drawn enough of them so that you get the
    idea.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
