- en: Chapter 2\. Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 基础知识
- en: 'In [Chapter 1](ch01.html#foundations), I described the major conceptual building
    block for understanding deep learning: nested, continuous, differentiable functions.
    I showed how to represent these functions as computational graphs, with each node
    in a graph representing a single, simple function. In particular, I demonstrated
    that such a representation showed easily how to calculate the derivative of the
    output of the nested function with respect to its input: we simply take the derivatives
    of all the constituent functions, evaluate these derivatives at the input that
    these functions received, and then multiply all of the results together; this
    will result in a correct derivative for the nested function because of the chain
    rule. I illustrated that this does in fact work with some simple examples, with
    functions that took NumPy’s `ndarray`s as inputs and produced `ndarray`s as outputs.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](ch01.html#foundations)中，我描述了理解深度学习的主要概念构建块：嵌套、连续、可微函数。我展示了如何将这些函数表示为计算图，图中的每个节点代表一个简单的函数。特别是，我演示了这种表示如何轻松地计算嵌套函数的输出相对于其输入的导数：我们只需对所有组成函数取导数，将这些导数在这些函数接收到的输入处进行评估，然后将所有结果相乘；这将导致嵌套函数的正确导数，因为链式法则。我用一些简单的例子说明了这实际上是有效的，这些函数以NumPy的`ndarray`作为输入，并产生`ndarray`作为输出。
- en: 'I showed that this method of computing derivatives works even when the function
    takes in multiple `ndarray`s as inputs and combines them via a *matrix multiplication*
    operation, which, unlike the other operations we saw, changes the shape of its
    inputs. Specifically, if one input to this operation—call the input *X*—is a B
    × N `ndarray`, and another input to this operation, *W*, is an N × M `ndarray`,
    then its output *P* is a B × M `ndarray`. While it isn’t clear what the derivative
    of such an operation would be, I showed that when a matrix multiplication *ν*(*X,
    W*) is included as a “constituent operation” in a nested function, we can still
    use a simple expression *in place of* its derivative to compute the derivatives
    of its inputs: specifically, the role of <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow>
    <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
    can be filled by *X*^(*T*), and the role of <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow>
    <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math>
    can be played by *W*^(*T*).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示了即使在函数接受多个`ndarray`作为输入并通过*矩阵乘法*操作将它们组合在一起时，计算导数的方法仍然有效，这与我们看到的其他操作不同，矩阵乘法操作会改变其输入的形状。具体来说，如果这个操作的一个输入——称为输入*X*——是一个B
    × N的`ndarray`，另一个输入到这个操作的*W*是一个N × M的`ndarray`，那么它的输出*P*是一个B × M的`ndarray`。虽然这种操作的导数不太清楚，但我展示了当矩阵乘法*ν*(*X,
    W*)被包含为嵌套函数中的一个“组成操作”时，我们仍然可以使用一个简单的表达式*代替*它的导数来计算其输入的导数：具体来说，<math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow>
    <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>的作用可以由*X*^(*T*)来填充，<math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow>
    <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math>的作用可以由*W*^(*T*)来扮演。
- en: 'In this chapter, we’ll start translating these concepts into real-world applications,
    Specifically, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始将这些概念转化为现实世界的应用，具体来说，我们将：
- en: Express linear regression in terms of these building blocks
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用这些基本组件来表达线性回归
- en: Show that the reasoning around derivatives that we did in [Chapter 1](ch01.html#foundations)
    allows us to train this linear regression model
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示我们在[第一章](ch01.html#foundations)中所做的关于导数的推理使我们能够训练这个线性回归模型
- en: Extend this model (still using our building blocks) to a one-layer neural network
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个模型（仍然使用我们的基本组件）扩展到一个单层神经网络
- en: Then, in [Chapter 3](ch03.html#deep_learning_from_scratch), it will be straightforward
    to use these same building blocks to build deep learning models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在[第三章](ch03.html#deep_learning_from_scratch)中，使用这些相同的基本组件构建深度学习模型将变得简单。
- en: Before we dive into all this, though, let’s give an overview of *supervised
    learning*, the subset of machine learning that we’ll focus on as we see how to
    use neural networks to solve problems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究所有这些之前，让我们先概述一下*监督学习*，这是我们将专注于的机器学习的子集，我们将看到如何使用神经网络来解决问题。
- en: Supervised Learning Overview
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习概述
- en: At a high level, machine learning can be described as building algorithms that
    can uncover or “learn” *relationships* in data; supervised learning can be described
    as the subset of machine learning dedicated to finding relationships *between
    characteristics of the data that have already been measured*.^([1](ch02.html#idm45732627218840))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，机器学习可以被描述为构建能够揭示或“学习”数据中的*关系*的算法；监督学习可以被描述为机器学习的子集，专注于找到已经被测量的数据特征之间的关系。^([1](ch02.html#idm45732627218840))
- en: 'In this chapter, we’ll deal with a typical supervised learning problem that
    you might encounter in the real world: finding the relationship between characteristics
    of a house and the value of the house. Clearly, there is some relationship between
    characteristics such as the number of rooms, the square footage, or the proximity
    to schools and how desirable a house is to live in or own. At a high level, the
    aim of supervised learning is to uncover these relationships, given that we’ve
    *already measured* these characteristics.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理一个在现实世界中可能遇到的典型监督学习问题：找到房屋特征与房屋价值之间的关系。显然，诸如房间数量、平方英尺、或者与学校的距离等特征与一所房屋的居住或拥有价值之间存在某种关系。在高层次上，监督学习的目的是揭示这些关系，鉴于我们已经*测量了*这些特征。
- en: By “measure,” I mean that each characteristic has been defined precisely and
    represented as a number. Many characteristics of a house, such as the number of
    bedrooms, the square footage, and so on, naturally lend themselves to being represented
    as numbers, but if we had other, different kinds of information, such as natural
    language descriptions of the house’s neighborhood from TripAdvisor, this part
    of the problem would be much less straightforward, and doing the translation of
    this less-structured data into numbers in a reasonable way could make or break
    our ability to uncover relationships. In addition, for any concept that is ambiguously
    defined, such as the value of a house, we simply have to pick a single number
    to describe it; here, an obvious choice is to use the price of the house.^([2](ch02.html#idm45732627215256))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓“度量”，我指的是每个特征都已经被精确定义并表示为一个数字。房屋的许多特征，比如卧室数量、平方英尺等，自然适合被表示为数字，但如果我们有其他不同类型的信息，比如来自TripAdvisor的房屋社区的自然语言描述，这部分问题将会变得不那么直接，将这种不太结构化的数据合理地转换为数字可能会影响我们揭示关系的能力。此外，对于任何模糊定义的概念，比如房屋的价值，我们只需选择一个单一的数字来描述它；在这里，一个明显的选择是使用房屋的价格。
- en: Once we’ve translated our “characteristics” into numbers, we have to decide
    what structure to use to represent these numbers. One that is nearly universal
    across machine learning and turns out to make computations easy is to represent
    each set of numbers for a single observation—for example, a single house—as a
    *row* of data, and then stack these rows on top of each other to form “batches”
    of data that will get fed into our models as two-dimensional `ndarray`s. Our models
    will then return predictions as output `ndarray`s with each prediction in a row,
    similarly stacked on top of each other, with one prediction for each observation
    in the batch.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将我们的“特征”转换为数字，我们必须决定使用什么结构来表示这些数字。在机器学习中几乎是普遍的一种结构，也很容易进行计算，即将单个观察值的每组数字表示为数据的*行*，然后将这些行堆叠在一起形成数据的“批次”，这些数据将作为二维`ndarray`输入到我们的模型中。我们的模型将返回预测作为输出`ndarray`，每个预测在一行中，类似地堆叠在一起，每个批次中的每个观察值都有一个预测。
- en: 'Now for some definitions: we say that the length of each row in this `ndarray`
    is the number of *features* of our data. In general, a single characteristic can
    map to many features, a classic example being a characteristic that describes
    our data as belonging to one of several *categories*, such as being a red brick
    house, a tan brick house, or a slate house;^([3](ch02.html#idm45732627208776))
    in this specific case we might describe this single characteristic with three
    features. The process of mapping what we informally think of as characteristics
    of our observations into features is called *feature engineering*. I won’t spend
    much time discussing this process in this book; indeed, in this chapter we’ll
    deal with a problem in which we have 13 characteristics of each observation, and
    we simply represent each characteristic with a single numeric feature.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看一些定义：我们说这个`ndarray`中每行的长度是我们数据的*特征*数量。一般来说，单个特征可以映射到多个特征，一个经典的例子是描述我们的数据属于几个*类别*之一的特征，比如红砖房屋、黄褐砖房屋或板岩房屋；在这种特定情况下，我们可能用三个特征描述这个单个特征。将我们非正式认为的观察特征映射为特征的过程称为*特征工程*。我不会在本书中花太多时间讨论这个过程；事实上，在本章中，我们将处理一个每个观察值有13个特征的问题，我们只是用一个单一的数值特征表示每个特征。
- en: I said that the goal of supervised learning is ultimately to uncover relationships
    between characteristics of data. In practice, we do this by choosing one characteristic
    that we want to predict from the others; we call this characteristic our *target*.
    The choice of which characteristic to use as the target is completely arbitrary
    and depends on the problem you are trying to solve. For example, if your goal
    is just to *describe* the relationship between the prices of houses and the number
    of rooms they have, you could do this by training a model with the prices of houses
    as the target and the number of rooms as a feature, or vice versa; either way,
    the resulting model will indeed contain a description of the relationship between
    these two characteristics, allowing you to say, for example, a higher number of
    rooms in a house is associated with higher prices. On the other hand, if your
    goal is to *predict* the prices of houses *for which no price information is available*,
    you have to choose the price as your target, so that you can ultimately feed the
    other information into your model once it is trained.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我说过，监督学习的目标最终是揭示数据特征之间的关系。在实践中，我们通过选择一个我们想要从其他特征中预测的特征来实现这一点；我们称这个特征为我们的*目标*。选择哪个特征作为目标是完全任意的，取决于您要解决的问题。例如，如果您的目标只是*描述*房屋价格和房间数量之间的关系，您可以通过训练一个模型，将房屋价格作为目标，房间数量作为特征，或者反之；无论哪种方式，最终的模型都将包含这两个特征之间关系的描述，使您能够说，例如，房屋中房间数量较多与价格较高相关。另一方面，如果您的目标是*预测*房屋价格*没有价格信息可用*，您必须选择价格作为目标，这样您最终可以在模型训练后将其他信息输入模型中。
- en: '[Figure 2-1](#fig_02-01) shows this hierarchy of descriptions of supervised
    learning, from the highest-level description of finding relationships in data,
    to the lowest level of quantifying those relationships by training models to uncover
    numerical representations between the features and the target.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-1](#fig_02-01)展示了监督学习的描述层次结构，从在数据中找到关系的最高级描述，到通过训练模型揭示特征和目标之间的数值表示的最低级别。'
- en: '![Supervised Learning overview](assets/dlfs_0201.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习概述](assets/dlfs_0201.png)'
- en: Figure 2-1\. Supervised learning overview
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 监督学习概述
- en: As mentioned, we’ll spend almost all our time on the level highlighted at the
    bottom of [Figure 2-1](#fig_02-01); nevertheless, in many problems, getting the
    parts at the top correct—collecting the right data, defining the problem you are
    trying to solve, and doing feature engineering—is much harder than the actual
    modeling. Still, since this book is focused on modeling—specifically, on understanding
    how deep learning models work—let’s return to that subject.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如提到的，我们将几乎所有的时间花在[图2-1](#fig_02-01)底部突出显示的层次上；然而，在许多问题中，获得顶部部分的正确性——收集正确的数据，定义您要解决的问题，并进行特征工程——比实际建模要困难得多。然而，由于本书侧重于建模——具体来说，是理解深度学习模型的工作原理——让我们回到这个主题。
- en: Supervised Learning Models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习模型
- en: 'Now we know at a high level what supervised learning models are trying to do—and
    as I alluded to earlier in the chapter, such models are just nested, mathematical
    functions. We spent the last chapter seeing how to represent such functions in
    terms of diagrams, math, and code, so now I can state the goal of supervised learning
    more precisely in terms of both math and code (I’ll show plenty of diagrams later):
    the goal is to *find* (a mathematical function) / (a function that takes an `ndarray`
    as input and produces an `ndarray` as output) that can (map characteristics of
    observations to the target) / (given an input `ndarray` containing the features
    we created, produce an output `ndarray` whose values are “close to” the `ndarray`
    containing the target).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在高层次上知道监督学习模型试图做什么了——正如我在本章前面所暗示的，这些模型只是嵌套的数学函数。我们在上一章中看到了如何用图表、数学和代码表示这样的函数，所以现在我可以更准确地用数学和代码来陈述监督学习的目标（稍后我会展示很多图表）：目标是*找到*（一个数学函数）/（一个以`ndarray`为输入并产生`ndarray`为输出的函数），它可以（将观察特征映射到目标）/（给定一个包含我们创建的特征的输入`ndarray`，产生一个输出`ndarray`，其值“接近”包含目标的`ndarray`）。
- en: 'Specifically, our data will be represented in a matrix *X* with *n* rows, each
    of which represents an observation with *k* features, all of which are numbers.
    Each row observation will be a vector, as in <math><mrow><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub></mtd>
    <mtd><msub><mi>x</mi> <mrow><mi>i</mi><mn>2</mn></mrow></msub></mtd> <mtd><msub><mi>x</mi>
    <mrow><mi>i</mi><mn>3</mn></mrow></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
    , and these observations will be stacked on top of one another to form a batch.
    For example, a batch of size 3 would look like:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们的数据将用矩阵*X*表示，其中有*n*行，每一行代表一个具有*k*个特征的观察，所有这些特征都是数字。每行观察将是一个向量，如<math><mrow><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi>
    <mrow><mi>i</mi><mn>1</mn></mrow></msub></mtd> <mtd><msub><mi>x</mi> <mrow><mi>i</mi><mn>2</mn></mrow></msub></mtd>
    <mtd><msub><mi>x</mi> <mrow><mi>i</mi><mn>3</mn></mrow></msub></mtd> <mtd><mo>...</mo></td>
    <mtd><msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>，这些观察将堆叠在一起形成一个批次。例如，大小为3的批次将如下所示：
- en: <math display="block"><mrow><msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>13</mn></msub></mtd>
    <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>13</mn></msub></mtd>
    <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'For each batch of observations, we will have a corresponding batch of *targets*,
    each element of which is the target number for the corresponding observation.
    We can represent these in a one-dimensional vector:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个观察批次，我们将有一个相应的*目标*批次，其中每个元素是相应观察的目标数值。我们可以用一维向量表示这些：
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>y</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>y</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced></math>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>y</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>y</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced></math>
- en: In terms of these arrays, our goal with supervised learning will be to use the
    tools I described in the last chapter to build a function that can take as input
    batches of observations with the structure of *X*[*batch*] and produce vectors
    of values *p*[i]—which we’ll interpret as “predictions”—that (for data in our
    particular dataset *X*, at least) are “close to the target values” *y*[i] for
    some reasonable measure of closeness.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些数组方面，我们在监督学习中的目标是使用我在上一章中描述的工具来构建一个函数，该函数可以接受具有*X*[*batch*]结构的观察批次作为输入，并产生值向量*p*[i]——我们将其解释为“预测”——这些预测（至少对于我们特定数据集*X*中的数据）与某种合理的接近度量的目标值*y*[i]“接近”。
- en: Finally, we are ready to make all of this concrete and start building our first
    model for a real-world dataset. We’ll start with a straightforward model—*linear
    regression*—and show how to express it in terms of the building blocks from the
    prior chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备具体化所有这些，并开始为真实数据集构建我们的第一个模型。我们将从一个简单的模型——*线性回归*开始，并展示如何用前一章的基本组件来表达它。
- en: Linear Regression
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'Linear regression is often shown as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归通常显示为：
- en: <math display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mo>×</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>β</mi> <mi>n</mi></msub>
    <mo>×</mo> <msub><mi>x</mi> <mi>k</mi></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mo>×</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>β</mi> <mi>n</mi></msub>
    <mo>×</mo> <msub><mi>x</mi> <mi>k</mi></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
- en: This representation describes mathematically our belief that the numeric value
    of each target is a linear combination of the *k* features of *X*, plus the *β*[0]
    term to adjust the “baseline” value of the prediction (specifically, the prediction
    that will be made when the value of all of the features is 0).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示在数学上描述了我们的信念，即每个目标的数值是*X*的*k*个特征的线性组合，再加上*β*[0]项来调整预测的“基准”值（具体来说，当所有特征的值为0时将进行的预测）。
- en: This, of course, doesn’t give us much insight into how we would code this up
    so that we could “train” such a model. To do that, we have to translate this model
    into the language of the functions we saw in [Chapter 1](ch01.html#foundations);
    the best place to start is with a diagram.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并没有让我们深入了解如何编写代码以便“训练”这样一个模型。为了做到这一点，我们必须将这个模型转化为我们在[第1章](ch01.html#foundations)中看到的函数语言；最好的起点是一个图表。
- en: 'Linear Regression: A Diagram'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归：一个图表
- en: How can we represent linear regression as a computational graph? We *could*
    break it down all the way to the individual elements, with each *x*[i] being multiplied
    by another element *w*[i] and then the results being added together, as in [Figure 2-2](#fig_02-02).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将线性回归表示为计算图？我们*可以*将其分解到最细的元素，每个*x*[i]都乘以另一个元素*w*[i]，然后将结果相加，如[图2-2](#fig_02-02)所示。
- en: '![Linear regression full](assets/dlfs_0202.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归全图](assets/dlfs_0202.png)'
- en: Figure 2-2\. The operations of a linear regression shown at the level of individual
    multiplications and additions
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。线性回归的操作显示在个别乘法和加法的水平上
- en: But again, as we saw in [Chapter 1](ch01.html#foundations), if we can represent
    these operations as just a matrix multiplication, we’ll be able to write the function
    more concisely while still being able to correctly calculate the derivative of
    the output with respect to the input, which will allow us to train the model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，正如我们在[第1章](ch01.html#foundations)中看到的，如果我们可以将这些操作表示为仅仅是矩阵乘法，我们将能够更简洁地编写函数，同时仍能正确计算输出相对于输入的导数，这将使我们能够训练模型。
- en: 'How can we do this? First, let’s handle the simpler scenario in which we don’t
    have an intercept term (*β*[0] shown previously). Note that we can represent the
    output of a linear regression model as the *dot product* of each observation vector
    <math><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>1</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mi>k</mi></msub></mtd></mtr></mtable></mfenced></mrow></math>
    with another vector of parameters that we’ll call *W*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何做到这一点？首先，让我们处理一个更简单的情况，即我们没有截距项（*β*[0]之前显示）。请注意，我们可以将线性回归模型的输出表示为每个观察向量的*点积*<math><mrow><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>1</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mi>k</mi></msub></mtd></mtr></mtable></mfenced></mrow></math>与我们将称之为*W*的另一个参数向量进行点积：
- en: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mi>k</mi></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>W</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mi>k</mi></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Our prediction would then simply be:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测将简单地是：
- en: <math display="block"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>×</mo> <mi>W</mi> <mo>=</mo> <msub><mi>w</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>+</mo>
    <msub><mi>w</mi> <mn>2</mn></msub> <mo>×</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mn>2</mn></mrow></msub>
    <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>w</mi> <mi>k</mi></msub> <mo>×</mo>
    <msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>×</mo> <mi>W</mi> <mo>=</mo> <msub><mi>w</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>+</mo>
    <msub><mi>w</mi> <mn>2</mn></msub> <mo>×</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mn>2</mn></mrow></msub>
    <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>w</mi> <mi>k</mi></msub> <mo>×</mo>
    <msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></math>
- en: 'So, we can represent “generating the predictions” for a linear regression using
    a single operation: the dot product.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以用一个操作来表示线性回归的“生成预测”：点积。
- en: 'Furthermore, when we want to make predictions using linear regression with
    a batch of observations, we can use another, single operation: the matrix multiplication.
    If we have a batch of size 3, for example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当我们想要使用一批次观察进行线性回归预测时，我们可以使用另一个单一操作：矩阵乘法。例如，如果我们有一个大小为3的批次：
- en: <math display="block"><mrow><msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>13</mn></msub></mtd>
    <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>13</mn></msub></mtd>
    <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'then performing the *matrix multiplication* of this batch *X*[*batch*] with
    *W* gives a vector of predictions for the batch, as desired:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后执行这批次*X*[*batch*]与*W*的*矩阵乘法*，得到一批次的预测向量，如所需：
- en: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>×</mo> <mi>W</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>13</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced>
    <mo>×</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi>
    <mn>3</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msub><mi>w</mi>
    <mi>k</mi></msub></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><mrow><msub><mi>x</mi> <mn>11</mn></msub> <mo>×</mo>
    <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>12</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>13</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd>
    <mtd><mrow><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi>
    <mn>21</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>22</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>23</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>2</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi>
    <mn>31</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>32</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>33</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <msub><mi>X</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>×</mo> <mi>W</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>x</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>13</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>23</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>31</mn></msub></mtd> <mtd><msub><mi>x</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>x</mi>
    <mn>33</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub></mtd></mtr></mtable></mfenced>
    <mo>×</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi>
    <mn>3</mn></msub></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msub><mi>w</mi>
    <mi>k</mi></msub></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><mrow><msub><mi>x</mi> <mn>11</mn></msub> <mo>×</mo>
    <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>12</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>13</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd>
    <mtd><mrow><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi>
    <mn>21</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>22</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>23</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>2</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi>
    <mn>31</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>32</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>33</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: So generating predictions for a batch of observations in a linear regression
    can be done with a matrix multiplication. Next, I’ll show how to use this fact,
    along with the reasoning about derivatives from the prior chapter, to train this
    model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，用矩阵乘法生成一批次观察的线性回归预测是可以的。接下来，我将展示如何利用这一事实，以及从前一章推导出的关于导数的推理，来训练这个模型。
- en: “Training” this model
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: “训练”这个模型
- en: 'What does it mean to “train” a model? At a high level, models^([4](ch02.html#idm45732626899528))
    take in data, combine them with *parameters* in some way, and produce predictions.
    For example, the linear regression model shown earlier takes in data *X* and parameters
    *W* and produces the predictions *p*[*batch*] using a matrix multiplication:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: “训练”一个模型是什么意思？在高层次上，模型^([4](ch02.html#idm45732626899528))接收数据，以某种方式与*参数*结合，并产生预测。例如，之前显示的线性回归模型接收数据*X*和参数*W*，并使用矩阵乘法产生预测*p*[*batch*]：
- en: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'To train our model, however, we need another crucial piece of information:
    whether or not these predictions are good. To learn this, we bring in the vector
    of *targets* *y*[*batch*] associated with the batch of observations *X*[*batch*]
    fed into the function, and we compute a *single number* that is a function of
    *y*[*batch*] and *p*[*batch*] and that represents the model’s “penalty” for making
    the predictions that it did. A reasonable choice is *mean squared error*, which
    is simply the average squared value that our model’s predictions “missed” by:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要训练我们的模型，我们需要另一个关键信息：这些预测是否准确。为了了解这一点，我们引入与输入到函数中的一批次观察*X*[*batch*]相关联的*目标*向量*y*[*batch*]，并计算一个关于*y*[*batch*]和*p*[*batch*]的函数的*单个数字*，表示模型对所做预测的“惩罚”。一个合理的选择是*均方误差*，简单地是我们模型的预测“偏离”的平均平方值：
- en: <math display="block"><mrow><mi>M</mi> <mi>S</mi> <mi>E</mi> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>,</mo> <msub><mi>y</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>M</mi> <mi>S</mi> <mi>E</mi> <mrow><mo>(</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>,</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><msub><mi>y</mi> <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi>
    <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>p</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo><msup><mrow><mo>(</mo><msub><mi>y</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>p</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo><msup><mrow><mo>(</mo><msub><mi>y</mi> <mn>3</mn></msub> <mo>-</mo><msub><mi>p</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mn>3</mn></mfrac></mrow></math>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>M</mi> <mi>S</mi> <mi>E</mi> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>,</mo> <msub><mi>y</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>M</mi> <mi>S</mi> <mi>E</mi> <mrow><mo>(</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>,</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><msub><mi>y</mi> <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi>
    <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>p</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo><msup><mrow><mo>(</mo><msub><mi>y</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>p</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo><msup><mrow><mo>(</mo><msub><mi>y</mi> <mn>3</mn></msub> <mo>-</mo><msub><mi>p</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mn>3</mn></mfrac></mrow></math>
- en: 'Getting to this number, which we can call *L*, is key: once we have it, we
    can use all the techniques we saw in [Chapter 1](ch01.html#foundations) to compute
    the *gradient* of this number with respect to each element of *W*. Then *we can
    use these derivatives to update each element of W in the direction that would
    cause L to decrease*. Repeating this procedure many times, we hope, will “train”
    our model; in this chapter, we’ll see that this can indeed work in practice. To
    see clearly how to compute these gradients, we’ll complete the process of representing
    linear regression as a computational graph.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 得到这个我们可以称之为*L*的数字是关键的：一旦我们有了它，我们可以使用我们在[第1章](ch01.html#foundations)中看到的所有技术来计算*L*对*W*的每个元素的*梯度*。然后*我们可以使用这些导数来更新W的每个元素，使L减少*。重复这个过程多次，我们希望能够“训练”我们的模型；在本章中，我们将看到这在实践中确实可以起作用。为了清楚地看到如何计算这些梯度，我们将完成将线性回归表示为计算图的过程。
- en: 'Linear Regression: A More Helpful Diagram (and the Math)'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归：更有帮助的图表（和数学）
- en: '[Figure 2-3](#fig_02-03) shows how to represent linear regression in terms
    of the diagrams from the last chapter.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-3](#fig_02-03)展示了如何用上一章的图表来表示线性回归。'
- en: '![Linear regression simple](assets/dlfs_0203.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归简单](assets/dlfs_0203.png)'
- en: Figure 2-3\. The linear regression equations expressed as a computational graph—the
    dark blue letters are the data inputs to the function, and the light blue W denotes
    the weights
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3。线性回归方程表达为计算图—深蓝色字母是函数的数据输入，浅蓝色W表示权重
- en: 'Finally, to reinforce that we’re still representing a nested mathematical function
    with this diagram, we could represent the loss value *L* that we ultimately compute
    as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了强调我们仍然用这个图表示一个嵌套的数学函数，我们可以表示最终计算的损失值*L*为：
- en: <math display="block"><mrow><mi>L</mi> <mo>=</mo> <mi>Λ</mi> <mo>(</mo> <mo>(</mo>
    <mi>ν</mi> <mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo> <mo>,</mo> <mi>Y</mi>
    <mo>)</mo></mrow></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>L</mi> <mo>=</mo> <mi>Λ</mi> <mo>(</mo> <mo>(</mo>
    <mi>ν</mi> <mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo> <mo>,</mo> <mi>Y</mi>
    <mo>)</mo></mrow></math>
- en: Adding in the Intercept
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入截距
- en: Representing models as diagrams shows us conceptually how we can add an intercept
    to the model. We simply add an extra step at the end that involves adding a “bias,”
    as shown in [Figure 2-4](#fig_02-04).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型表示为图表在概念上向我们展示了如何向模型添加截距。我们只需在最后添加一个额外步骤，涉及添加一个“偏差”，如[图2-4](#fig_02-04)所示。
- en: '![Linear regression](assets/dlfs_0204.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](assets/dlfs_0204.png)'
- en: Figure 2-4\. The computational graph of linear regression, with the addition
    of a bias term at the end
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4。线性回归的计算图，最后添加了一个偏置项
- en: 'Here, though, we should reason mathematically about what is going on before
    moving on to the code; with the bias added, each element of our model’s prediction
    *p*[*i*] will be the dot product described earlier with the quantity *b* added
    to it:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在继续编码之前，我们应该对正在发生的事情进行数学推理；添加了偏差后，我们模型预测*p*[*i*]的每个元素将是之前描述的点积，加上数量*b*：
- en: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo>_</mo><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>_</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi></mrow></msub>
    <mo>=</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mtext>dot</mtext> <mi>W</mi> <mo>+</mo>
    <mi>b</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><msub><mi>x</mi>
    <mn>11</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>12</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>13</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub> <mo>+</mo> <mi>b</mi></mrow></mtd></mtr>
    <mtr><mtd><mrow><msub><mi>x</mi> <mn>21</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>22</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>23</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub> <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub>
    <mo>+</mo> <mi>b</mi></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi> <mn>31</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>32</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>33</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd>
    <mtd><mrow><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mi>k</mi></msub> <mo>+</mo> <mi>b</mi></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>p</mi> <mrow><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo>_</mo><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>_</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi></mrow></msub>
    <mo>=</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mtext>dot</mtext> <mi>W</mi> <mo>+</mo>
    <mi>b</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><msub><mi>x</mi>
    <mn>11</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>12</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi>
    <mn>13</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi> <mrow><mn>1</mn><mi>k</mi></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub> <mo>+</mo> <mi>b</mi></mrow></mtd></mtr>
    <mtr><mtd><mrow><msub><mi>x</mi> <mn>21</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>22</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>23</mn></msub> <mo>×</mo> <msub><mi>w</mi>
    <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd> <mtd><mrow><msub><mi>x</mi>
    <mrow><mn>2</mn><mi>k</mi></mrow></msub> <mo>×</mo> <msub><mi>w</mi> <mi>k</mi></msub>
    <mo>+</mo> <mi>b</mi></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>x</mi> <mn>31</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>32</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>33</mn></msub>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <mo>...</mo> <mo>+</mo></mrow></mtd>
    <mtd><mrow><msub><mi>x</mi> <mrow><mn>3</mn><mi>k</mi></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mi>k</mi></msub> <mo>+</mo> <mi>b</mi></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>p</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: Note that because the intercept in linear regression should be just a single
    number rather than being different for each observation, the *same number* should
    get added to each observation of the input to the bias operation that is passed
    in; we’ll discuss what this means for computing the derivatives in a later section
    of this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于线性回归中的截距应该只是一个单独的数字，而不是对每个观察值都不同，应该将*相同的数字*添加到传递给偏置操作的每个输入的每个观察值中；我们将在本章的后面部分讨论这对于计算导数意味着什么。
- en: 'Linear Regression: The Code'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归：代码
- en: 'We’ll now tie things together and code up the function that makes predictions
    and computes losses given batches of observations *X*[*batch*] and their corresponding
    targets *y*[*batch*]. Recall that computing derivatives for nested functions using
    the chain rule involves two sets of steps: first, we perform a “forward pass,”
    passing the input successively forward through a series of operations and saving
    the quantities computed as we go; then we use those quantities to compute the
    appropriate derivatives during the backward pass.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将把这些东西联系起来，并编写一个函数，该函数根据观察批次*X*[*batch*]及其相应目标*y*[*batch*]进行预测并计算损失。请记住，使用链式法则计算嵌套函数的导数涉及两组步骤：首先，我们执行“前向传递”，将输入依次通过一系列操作向前传递，并在进行操作时保存计算的量；然后我们使用这些量在反向传递期间计算适当的导数。
- en: 'The following code does this, saving the quantities computed on the forward
    pass in a dictionary; furthermore, to differentiate between the quantities computed
    on the forward pass and the parameters themselves (which we’ll also need for the
    backward pass), our function will expect to receive a dictionary containing the
    parameters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行此操作，将在字典中保存在前向传递中计算的量；此外，为了区分在前向传递中计算的量和参数本身（我们也需要用于反向传递），我们的函数将期望接收一个包含参数的字典：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we have all the pieces in place to start “training” this model. Next, we’ll
    cover exactly what this means and how we’ll do it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开始“训练”这个模型了。接下来，我们将详细介绍这意味着什么以及我们将如何做到这一点。
- en: Training the Model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: We are now going to use all the tools we learned in the last chapter to compute
    <math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>w</mi>
    <mi>i</mi></msub></mrow></mfrac></math> for every *w*[*i*] in *W*, as well as
    <math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac></math>
    . How? Well, since the “forward pass” of this function was passing the input through
    a series of nested functions, the backward pass will simply involve computing
    the partial derivatives of each function, evaluating those derivatives at the
    functions’ inputs, and multiplying them together—and even though a matrix multiplication
    is involved, we’ll be able to handle this using the reasoning we covered in the
    last chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用上一章学到的所有工具来计算每个*W*中的*w*[*i*]的<math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>w</mi> <mi>i</mi></msub></mrow></mfrac></math>，以及<math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac></math>。如何做到？嗯，由于这个函数的“前向传递”是通过一系列嵌套函数传递输入，因此反向传递将简单涉及计算每个函数的偏导数，在函数的输入处评估这些导数，然后将它们相乘在一起——尽管涉及矩阵乘法，但我们将能够使用上一章中涵盖的推理来处理这个问题。
- en: 'Calculating the Gradients: A Diagram'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算梯度：图表
- en: Conceptually, we want something like what is depicted in [Figure 2-5](#fig_02-05).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们希望得到类似于[图2-5](#fig_02-05)中所示的内容。
- en: '![Linear regression full](assets/dlfs_0205.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![完整的线性回归](assets/dlfs_0205.png)'
- en: Figure 2-5\. The backward pass through the linear regression computational graph
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5。通过线性回归计算图的反向传递
- en: We simply step backward, computing the derivative of each constituent function
    and evaluating those derivatives at the inputs that those functions received on
    the forward pass, and then multiplying these derivatives together at the end.
    This is straightforward enough, so let’s get into the details.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地向后移动，计算每个组成函数的导数，并在前向传递时评估这些函数接收到的输入的导数，然后在最后将这些导数相乘在一起。这是足够简单的，所以让我们深入了解细节。
- en: 'Calculating the Gradients: The Math (and Some Code)'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算梯度：数学（和一些代码）
- en: 'From [Figure 2-5](#fig_02-05), we can see that the derivative product that
    we ultimately want to compute is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图2-5](#fig_02-05)中，我们可以看到我们最终想要计算的导数乘积是：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>×</mo>
    <mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>×</mo>
    <mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>×</mo>
    <mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>×</mo>
    <mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
- en: There are three components here; let’s compute each of them in turn.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有三个组件；让我们依次计算每个组件。
- en: 'First up: <math><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow></mrow></math>
    . Since <math><mrow><mi>Λ</mi> <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mi>Y</mi><mo>-</mo><mi>P</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> for each element in *Y* and *P*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先：<math><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow></mrow></math>。由于对于*Y*和*P*中的每个元素，<math><mrow><mi>Λ</mi>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mrow><mo>(</mo><mi>Y</mi><mo>-</mo><mi>P</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mn>1</mn> <mo>×</mo> <mrow><mo>(</mo> <mn>2</mn> <mo>×</mo> <mrow><mo>(</mo>
    <mi>Y</mi> <mo>-</mo> <mi>P</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mn>1</mn> <mo>×</mo> <mrow><mo>(</mo> <mn>2</mn> <mo>×</mo> <mrow><mo>(</mo>
    <mi>Y</mi> <mo>-</mo> <mi>P</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: 'We’re jumping ahead of ourselves a bit, but note that coding this up would
    simply be:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有点超前了，但请注意编写这个代码只是简单的：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we have an expression involving matrices: <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow>
    <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow></mrow></math> . But since *α* is just addition, the
    same logic that we reasoned through with numbers in the prior chapter applies
    here: increasing any element of *N* by one unit will increase <math><mrow><mi>P</mi>
    <mo>=</mo> <mi>α</mi> <mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo>
    <mi>N</mi> <mo>+</mo> <mi>B</mi></mrow></math> by one unit. Thus, <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow>
    <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow></mrow></math> is just a matrix of +1+s, of the same
    shape as *N*.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有涉及矩阵的表达式：<math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow></mrow></math>
    。但由于 *α* 只是加法，我们在前一章节中对数字推理的逻辑同样适用于这里：将 *N* 的任何元素增加一个单位将使 <math><mrow><mi>P</mi>
    <mo>=</mo> <mi>α</mi> <mo>(</mo> <mi>N</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo>
    <mi>N</mi> <mo>+</mo> <mi>B</mi></mrow></math> 增加一个单位。因此，<math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow>
    <mrow><mi>∂</mi><mi>N</mi></mrow></mfrac> <mrow><mo>(</mo> <mi>N</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow></mrow></math> 只是一个由 +1 组成的矩阵，形状与 *N* 相同。
- en: 'Coding *this* expression, therefore, would simply be:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，编码*这个*表达式只是：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we have <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
    . As we discussed in detail in the last chapter, when computing derivatives of
    nested functions where one of the constituent functions is a matrix multiplication,
    we can act *as if*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow></mrow></math>
    。正如我们在上一章节中详细讨论的，当计算嵌套函数的导数时，其中一个组成函数是矩阵乘法时，我们可以*假设*：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mi>X</mi> <mi>T</mi></msup></mrow></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>W</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mi>X</mi> <mi>T</mi></msup></mrow></math>
- en: 'which in code is simply:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这只是简单的：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We’ll do the same for the intercept term; since we are just adding it, the
    partial derivative of the intercept term with respect to the output is simply
    1:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对截距项执行相同的操作；因为我们只是将其添加，截距项对输出的偏导数就是 1：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The last step is to simply multiply these together, making sure we use the correct
    order for the matrix multiplications involving `dNdW` and `dNdX` based on what
    we reasoned through at the end of the last chapter.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步就是简单地将它们相乘在一起，确保我们根据我们在上一章节末尾推理出的正确顺序进行涉及 `dNdW` 和 `dNdX` 的矩阵乘法。
- en: 'Calculating the Gradients: The (Full) Code'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算梯度：（完整）代码
- en: 'Recall that our goal is to take everything computed on or inputed into the
    forward pass—which, from the diagram in [Figure 2-5](#fig_02-05), will include
    *X*, *W*, *N*, *B*, *P*, and *y*—and compute <math><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow>
    <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac></math> and <math><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow>
    <mrow><mi>∂</mi><mi>B</mi></mrow></mfrac></math> . The following code does that,
    receiving *W* and *B* as inputs in a dictionary called `weights` and the rest
    of the quantities in a dictionary called `forward_info`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住我们的目标是取出在前向传播中计算的或输入的所有内容——从 [Figure 2-5](#fig_02-05) 中的图表中，这将包括 *X*、*W*、*N*、*B*、*P*
    和 *y*——并计算 <math><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>W</mi></mrow></mfrac></math>
    和 <math><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>B</mi></mrow></mfrac></math>
    。以下代码实现了这一点，接收 *W* 和 *B* 作为名为 `weights` 的字典中的输入，其余的量作为名为 `forward_info` 的字典中的输入：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, we simply compute the derivatives with respect to each operation
    and successively multiply them together, taking care that we do the matrix multiplication
    in the right order.^([5](ch02.html#idm45732626271592)) As we’ll see shortly, this
    actually works—and after the intuition we built up around the chain rule in the
    last chapter, this shouldn’t be too surprising.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们只需计算每个操作的导数，然后逐步将它们相乘在一起，确保我们按正确顺序进行矩阵乘法。正如我们很快将看到的，这实际上是有效的——在我们在上一章节围绕链式法则建立的直觉之后，这应该不会太令人惊讶。
- en: Note
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'An implementation detail about those loss gradients: we’re storing them as
    a dictionary, with the names of the weights as keys and the amounts that increasing
    the weights affect the losses as values. The `weights` dictionary is structured
    the same way. Therefore, we’ll iterate through the weights in our model in the
    following way:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些损失梯度的实现细节：我们将它们存储为一个字典，其中权重的名称作为键，增加权重影响损失的数量作为值。`weights` 字典的结构也是一样的。因此，我们将按照以下方式迭代我们模型中的权重：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There is nothing special about storing them in this way; if we stored them differently,
    we would simply iterate through them and refer to them differently.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式存储它们并没有什么特别之处；如果我们以不同的方式存储它们，我们只需迭代它们并以不同的方式引用它们。
- en: Using These Gradients to Train the Model
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用这些梯度来训练模型。
- en: 'Now we’ll simply run the following procedure over and over again:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需一遍又一遍地运行以下过程：
- en: Select a batch of data.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一批数据。
- en: Run the forward pass of the model.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行模型的前向传播。
- en: Run the backward pass of the model using the info computed on the forward pass.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在前向传播中计算的信息运行模型的反向传播。
- en: Use the gradients computed on the backward pass to update the weights.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在反向传播中计算的梯度来更新权重。
- en: 'The [Jupyter Notebook](https://oreil.ly/2TDV5q9) for this chapter of the book
    includes a `train` function that codes this up. It isn’t too interesting; it simply
    implements the preceding steps and adds a few sensible things such as shuffling
    the data to ensure that it is fed through in a random order. The key lines, which
    get repeated inside of a `for` loop, are these:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的这一章节的 [Jupyter Notebook](https://oreil.ly/2TDV5q9) 包含一个名为 `train` 的函数，用于编写这个。这并不太有趣；它只是实现了前面的步骤，并添加了一些明智的事情，比如对数据进行洗牌以确保以随机顺序传递。关键的代码行在一个
    `for` 循环内重复，如下所示：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we run the `train` function for a certain number of *epochs*, or cycles
    through the entire training dataset, as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们运行`train`函数一定数量的*周期*，或者遍历整个训练数据集，如下所示：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `train` function returns `train_info`, a `Tuple`, one element of which is
    the parameters or *weights* that represent what the model has learned.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`函数返回`train_info`，一个`Tuple`，其中一个元素是代表模型学习内容的参数或*权重*。'
- en: Note
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The terms “parameters” and “weights” are used interchangeably throughout deep
    learning, so we will use them interchangeably in this book.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: “参数”和“权重”这两个术语在深度学习中通常是可以互换使用的，因此在本书中我们将它们互换使用。
- en: 'Assessing Our Model: Training Set Versus Testing Set'
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们的模型：训练集与测试集
- en: To understand whether our model uncovered relationships in our data, we have
    to introduce some terms and ways of thinking from statistics. We think of any
    dataset received as being a *sample* from a *population*. Our goal is always to
    find a model that uncovers relationships in the population, despite us seeing
    only a sample.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们的模型是否揭示了数据中的关系，我们必须从统计学中引入一些术语和思考方式。我们认为收到的任何数据集都是从一个*总体*中抽取的*样本*。我们的目标始终是找到一个能够揭示总体关系的模型，尽管我们只看到了一个样本。
- en: There is always a danger that we build a model that picks up relationships that
    exist in the sample but not in the population. For example, it might be the case
    in our sample that yellow slate houses with three bathrooms are relatively inexpensive,
    and a complicated neural network model we build could pick up on this relationship
    even though it may not exist in the population. This is a problem known as *overfitting*.
    How can we detect whether a model structure we use is likely to have this problem?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立的模型可能会捕捉到样本中存在但在总体中不存在的关系。例如，在我们的样本中，黄色板岩房屋带有三个浴室可能相对便宜，我们构建的复杂神经网络模型可能会捕捉到这种关系，尽管在总体中可能不存在。这是一个被称为*过拟合*的问题。我们如何检测我们使用的模型结构是否可能存在这个问题？
- en: The solution is to split our sample into a *training set* and a *testing set*.
    We use the training data to train the model (that is, to iteratively update the
    weights), and then we evaluate the model on the testing set to estimate its performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是将我们的样本分成一个*训练集*和一个*测试集*。我们使用训练数据来训练模型（即迭代更新权重），然后我们在测试集上评估模型以估计其性能。
- en: The full logic here is that if our model was able to successfully pick up on
    relationships that generalize from the *training set* to *the rest of the sample*
    (our whole dataset), then it is likely that the same “model structure” will generalize
    from our *sample*—which, again, is our entire dataset—to the *population*, which
    is what we want.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的逻辑是，如果我们的模型能够成功地发现从*训练集*到*样本其余部分*（我们的整个数据集）泛化的关系，那么同样的“模型结构”很可能会从我们的*样本*——再次强调，是我们的整个数据集——泛化到*总体*，这正是我们想要的。
- en: 'Assessing Our Model: The Code'
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们的模型：代码
- en: 'With that understanding, let’s evaluate our model on the testing set. First,
    we’ll write a function to generate predictions by truncating the `forward_pass`
    function we saw previously:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个理解，让我们在测试集上评估我们的模型。首先，我们将编写一个函数，通过截断我们之前看到的`forward_pass`函数来生成预测：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we simply use the weights returned earlier from the `train` function and
    write:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们只需使用`train`函数之前返回的权重，并写：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How good are these predictions? Keep in mind that at this point we haven’t validated
    our seemingly strange approach of defining models as a series of operations, and
    training them by iteratively adjusting the parameters involved using the partial
    derivatives of the loss calculated with respect to the parameters using the chain
    rule; thus, we should be pleased if this approach works at all.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测有多好？请记住，目前我们还没有验证我们看似奇怪的定义模型的方法，即将模型定义为一系列操作，并通过使用损失的偏导数来调整涉及的参数，使用链式法则迭代地训练模型；因此，如果这种方法有效，我们应该感到高兴。
- en: The first thing we can do to see whether our model worked is to make a plot
    with the model’s predictions on the x-axis and the actual values on the y-axis.
    If every point fell exactly on the 45-degree line, the model would be perfect.
    [Figure 2-6](#fig_02-06) shows a plot of our model’s predicted and actual values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的第一件事是查看我们的模型是否有效，即制作一个图，其中模型的预测值在x轴上，实际值在y轴上。如果每个点都恰好落在45度线上，那么模型就是完美的。[图2-6](#fig_02-06)显示了我们模型的预测值和实际值的图。
- en: '![Custom prediction versus actual](assets/dlfs_0206.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![自定义预测与实际](assets/dlfs_0206.png)'
- en: Figure 2-6\. Predicted versus actual values for our custom linear regression
    model
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6。我们自定义线性回归模型的预测与实际值
- en: 'Our plot looks pretty good, but let’s quantify how good the model is. There
    are a couple of common ways to do that:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图看起来很不错，但让我们量化一下模型的好坏。有几种常见的方法可以做到这一点：
- en: 'Calculate the mean distance, in absolute value, between our model’s predictions
    and the actual values, a metric called *mean absolute error*:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算我们模型预测值与实际值之间的平均距离，即*平均绝对误差*：
- en: '[PRE11]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Calculate the mean squared distance between our model’s predictions and the
    actual values, a metric known as *root mean squared error*:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算我们模型预测值与实际值之间的平均平方距离，这个指标称为*均方根误差*：
- en: '[PRE12]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The values for this particular model are:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定模型的值为：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Root mean squared error is a particularly common metric since it is on the same
    scale as the target. If we divide this number by the mean value of the target,
    we can get a measure of how far off a prediction is, on average, from its actual
    value. Since the mean value of `y_test` is `22.0776`, we see that this model’s
    predictions of house prices are off by 5.0508 / 22.0776 ≅ 22.9% on average.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根误差是一个特别常见的指标，因为它与目标在同一尺度上。如果我们将这个数字除以目标的平均值，我们可以得到一个预测值与实际值之间平均偏差的度量。由于`y_test`的平均值为`22.0776`，我们看到这个模型对房价的预测平均偏差为5.0508
    / 22.0776 ≅ 22.9%。
- en: So are these numbers any good? In the [Jupyter Notebook](https://oreil.ly/2TDV5q9)
    containing the code for this chapter, I show that performing a linear regression
    on this dataset using the most popular Python library for machine learning, Sci-Kit
    Learn, results in a mean absolute error and root mean squared error of `3.5666`
    and `5.0482`, respectively, which are virtually identical to what we calculated
    in our “first-principles-based” linear regression previously. This should give
    you confidence that the approach we’ve been taking so far in this book is in fact
    a valid approach for reasoning about and training models! Both later in this chapter,
    and in the next chapter we’ll extend this approach to neural networks and deep
    learning models.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字好吗？在包含本章代码的[Jupyter Notebook](https://oreil.ly/2TDV5q9)中，我展示了使用最流行的Python机器学习库Sci-Kit
    Learn对这个数据集进行线性回归的结果，平均绝对误差和均方根误差分别为`3.5666`和`5.0482`，与我们之前计算的“基于第一原理”的线性回归几乎相同。这应该让你相信，我们在本书中迄今为止采取的方法实际上是一种用于推理和训练模型的有效方法！在本章后面以及下一章中，我们将把这种方法扩展到神经网络和深度学习模型。
- en: Analyzing the Most Important Feature
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析最重要的特征
- en: 'Before beginning modeling, we scaled each feature of our data to have mean
    0 and standard deviation 1; this has computational advantages that we’ll discuss
    in more detail in [Chapter 4](ch04.html#extensions). A benefit of doing this that
    is specific to linear regression is that we can interpret the absolute values
    of the coefficients as corresponding to the importance of the different features
    to the model; a larger coefficient means that the feature is more important. Here
    are the coefficients:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始建模之前，我们将数据的每个特征缩放为均值为0，标准差为1；这具有计算优势，我们将在[第4章](ch04.html#extensions)中更详细地讨论。这样做的好处是，对于线性回归来说，我们可以解释系数的绝对值与模型中不同特征的重要性相对应；较大的系数意味着该特征更重要。以下是系数：
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The fact that the last coefficient is largest means that the last feature in
    the dataset is the most important one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个系数最大的事实意味着数据集中的最后一个特征是最重要的。
- en: In [Figure 2-7](#fig_02-07), we plot this feature against our target.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-7](#fig_02-07)中，我们将这个特征与我们的目标绘制在一起。
- en: '![Custom prediction versus actual](assets/dlfs_0207.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![自定义预测与实际](assets/dlfs_0207.png)'
- en: Figure 2-7\. Most important feature versus target in custom linear regression
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7。自定义线性回归中最重要的特征与目标
- en: 'We see that this feature is indeed strongly correlated with the target: as
    this feature increases, the value of the target decreases, and vice versa. However,
    this relationship is *not* linear. The expected amount that the target changes
    as the feature changes from –2 to –1 is *not* the same amount that it changes
    as the feature changes from 1 to 2\. We’ll come back to this later.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这个特征与目标确实强相关：随着这个特征的增加，目标值减少，反之亦然。然而，这种关系*不*是线性的。当特征从-2变为-1时，目标变化的预期量*不*等于特征从1变为2时的变化量。我们稍后会回到这个问题。
- en: 'In [Figure 2-8](#fig_02-08), we overlay onto this plot the relationship between
    this feature and the *model predictions*. We’ll generate this by feeding the following
    data through our trained model:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-8](#fig_02-08)中，我们将这个特征与*模型预测*之间的关系叠加到这个图中。我们将通过将以下数据馈送到我们训练过的模型中来生成这个图：
- en: The values of all features set equal to their mean
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有特征的值设置为它们的均值
- en: The values of the most important feature linearly interpolated over 40 steps
    from –1.5 to 3.5, which is roughly the range of this scaled feature in our data
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要特征的值在-1.5到3.5之间线性插值，这大致是我们数据中这个缩放特征的范围
- en: '![Custom prediction versus actual](assets/dlfs_0208.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![自定义预测与实际](assets/dlfs_0208.png)'
- en: Figure 2-8\. Most important feature versus target and predictions in custom
    linear regression
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8。自定义线性回归中最重要的特征与目标和预测
- en: 'This figure shows (literally) a limitation of linear regression: despite the
    fact that there is a visually clear and “model-able” *non*linear relationship
    between this feature and the target, our model is only able to “learn” a linear
    relationship because of its intrinsic structure.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图（字面上）展示了线性回归的一个局限性：尽管这个特征与目标之间存在一个视觉上明显且“可建模”的*非*线性关系，但由于其固有结构，我们的模型只能“学习”线性关系。
- en: To have our model learn a more complex, nonlinear relationship between our features
    and our target, we’re going to have to build a more complicated model than linear
    regression. But how? The answer will lead us, in a principles-based way, to building
    a neural network.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的模型学习特征和目标之间更复杂、非线性的关系，我们将不得不构建一个比线性回归更复杂的模型。但是如何做呢？答案将以基于原则的方式引导我们构建一个神经网络。
- en: Neural Networks from Scratch
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始的神经网络
- en: We’ve just seen how to build and train a linear regression model from first
    principles. How can we extend this chain of reasoning to design a more complex
    model that can learn nonlinear relationships? The central idea is that we’ll first
    do *many* linear regressions, then feed the results through a nonlinear function,
    and finally do one last linear regression that ultimately makes the predictions.
    As it will turn out, we can reason through how to compute the gradients for this
    more complicated model in the same way we did for the linear regression model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到如何从第一原理构建和训练线性回归模型。我们如何将这种推理链扩展到设计一个可以学习非线性关系的更复杂模型？中心思想是，我们首先进行*许多*线性回归，然后将结果馈送到一个非线性函数中，最后进行最后一次线性回归，最终进行预测。事实证明，我们可以通过与线性回归模型相同的方式推理出如何计算这个更复杂模型的梯度。
- en: 'Step 1: A Bunch of Linear Regressions'
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：一堆线性回归
- en: 'What does it mean to do “a bunch of linear regressions”? Well, doing one linear
    regression involved doing a matrix multiplication with a set of parameters: if
    our data *X* had dimensions `[batch_size, num_features]`, then we multiplied it
    by a weight matrix *W* with dimensions `[num_features, 1]` to get an output of
    dimension `[batch_size, 1]`; this output is, for each observation in the batch,
    simply a *weighted sum* of the original features. To do multiple linear regressions,
    we’ll simply multiply our input by a weight matrix with dimensions `[num_features,
    num_outputs]`, resulting in an output of dimensions `[batch_size, num_outputs]`;
    now, *for each observation*, we have `num_outputs` different weighted sums of
    the original features.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 做“一堆线性回归”是什么意思？做一个线性回归涉及使用一组参数进行矩阵乘法：如果我们的数据*X*的维度是`[batch_size, num_features]`，那么我们将它乘以一个维度为`[num_features,
    1]`的权重矩阵*W*，得到一个维度为`[batch_size, 1]`的输出；对于批次中的每个观察值，这个输出只是原始特征的一个*加权和*。要做多个线性回归，我们只需将我们的输入乘以一个维度为`[num_features,
    num_outputs]`的权重矩阵，得到一个维度为`[batch_size, num_outputs]`的输出；现在，*对于每个观察值*，我们有`num_outputs`个不同的原始特征的加权和。
- en: What are these weighted sums? We should think of each of them as a “learned
    feature”—a combination of the original features that, once the network is trained,
    will represent its attempt to learn combinations of features that help it accurately
    predict house prices. How many learned features should we create? Let’s create
    13 of them, since we created 13 original features.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些加权和是什么？我们应该将它们中的每一个看作是一个“学习到的特征”——原始特征的组合，一旦网络训练完成，将代表其尝试学习的特征组合，以帮助准确预测房价。我们应该创建多少个学习到的特征？让我们创建13个，因为我们创建了13个原始特征。
- en: 'Step 2: A Nonlinear Function'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：一个非线性函数
- en: Next, we’ll feed each of these weighted sums through a *non*linear function;
    the first function we’ll try is the `sigmoid` function that was mentioned in [Chapter 1](ch01.html#foundations).
    As a refresher, [Figure 2-9](#fig_02-09) plots the `sigmoid` function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过一个非线性函数来处理这些加权和；我们将尝试的第一个函数是在第1章中提到的`sigmoid`函数。作为提醒，[图2-9](#fig_02-09)展示了`sigmoid`函数。
- en: '![Sigmoid](assets/dlfs_0209.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![Sigmoid](assets/dlfs_0209.png)'
- en: Figure 2-9\. Sigmoid function plotted from x = –5 to x = 5
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9。从x = -5到x = 5绘制的Sigmoid函数
- en: Why is using this nonlinear function a good idea? Why not the `square` function
    *f*(*x*) = *x*², for example? There are a couple of reasons. First, we want the
    function we use here to be *monotonic* so that it “preserves” information about
    the numbers that were fed in. Let’s say that, given the date that was fed in,
    two of our linear regressions produced values of –3 and 3, respectively. Feeding
    these through the `square` function would then produce a value of 9 for each,
    so that any function that receives these numbers as inputs after they were fed
    through the `square` function would “lose” the information that one of them was
    originally –3 and the other was 3.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用这个非线性函数是个好主意？为什么不使用`square`函数*f*(*x*) = *x*²，例如？有几个原因。首先，我们希望在这里使用的函数是*单调*的，以便“保留”输入的数字的信息。假设，给定输入的日期，我们的两个线性回归分别产生值-3和3。然后通过`square`函数传递这些值将为每个产生一个值9，因此任何接收这些数字作为输入的函数在它们通过`square`函数传递后将“丢失”一个原始为-3，另一个为3的信息。
- en: The second reason, of course, is that the function is nonlinear; this nonlinearity
    will enable our neural network to model the inherently nonlinear relationship
    between the features and the target.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，第二个原因是这个函数是非线性的；这种非线性将使我们的神经网络能够建模特征和目标之间固有的非线性关系。
- en: 'Finally, the `sigmoid` function has the nice property that its derivative can
    be expressed in terms of the function itself:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`sigmoid`函数有一个很好的性质，即它的导数可以用函数本身来表示：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>σ</mi></mrow> <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>σ</mi></mrow> <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: We’ll make use of this shortly when we use the `sigmoid` function in the backward
    pass of our neural network.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快在神经网络的反向传播中使用`sigmoid`函数时使用它。
- en: 'Step 3: Another Linear Regression'
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：另一个线性回归
- en: Finally, we’ll take the resulting 13 elements—each of which is a combination
    of the original features, fed through the `sigmoid` function so that they all
    have values between 0 and 1—and feed them into a regular linear regression, using
    them the same way we used our original features previously.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将得到的13个元素——每个元素都是原始特征的组合，通过`sigmoid`函数传递，使它们的值都在0到1之间——并将它们输入到一个常规线性回归中，使用它们的方式与我们之前使用原始特征的方式相同。
- en: 'Then, we’ll try training the *entire* resulting function in the same way we
    trained the standard linear regression earlier in this chapter: we’ll feed data
    through the model, use the chain rule to figure out how much increasing the weights
    would increase (or decrease) the loss, and then update the weights in the direction
    that decreases the loss at each iteration. Over time (we hope) we’ll end up with
    a more accurate model than before, one that has “learned” the inherent nonlinearity
    of the relationship between our features and our target.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将尝试训练*整个*得到的函数，方式与本章前面训练标准线性回归的方式相同：我们将数据通过模型，使用链式法则来计算增加权重会增加（或减少）损失多少，然后在每次迭代中更新权重，以减少损失。随着时间的推移（我们希望），我们将得到比以前更准确的模型，一个已经“学会”了特征和目标之间固有非线性关系的模型。
- en: It might be tough to wrap your mind around what’s going on based on this description,
    so let’s look at an illustration.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个描述，可能很难理解正在发生的事情，所以让我们看一个插图。
- en: Diagrams
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图表
- en: '[Figure 2-10](#fig_02-10) is a diagram of what our more complicated model now
    looks like.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-10](#fig_02-10)是我们更复杂模型的图表。'
- en: '![Neural network forward pass](assets/dlfs_0210.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络前向传播](assets/dlfs_0210.png)'
- en: Figure 2-10\. Steps 1–3 translated into a computational graph of the kind we
    saw in [Chapter 1](ch01.html#foundations)
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-10。将步骤1-3翻译成我们在第1章中看到的计算图的一种类型
- en: 'You’ll see that we start with matrix multiplication and matrix addition, as
    before. Now let’s formalize some terminology that was mentioned previously: when
    we apply these operations in the course of a nested function, we’ll call the first
    matrix that we use to transform the input features the *weight* matrix, and we’ll
    call the second matrix, the one that is added to each resulting set of features,
    the *bias*. That’s why we’ll denote these as *W*[1] and *B*[1].'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到我们从矩阵乘法和矩阵加法开始，就像以前一样。现在让我们正式定义一些之前提到的术语：当我们在嵌套函数中应用这些操作时，我们将称第一个矩阵用于转换输入特征的矩阵为*权重*矩阵，我们将称第二个矩阵，即添加到每个结果特征集的矩阵为*偏置*。这就是为什么我们将它们表示为*W*[1]和*B*[1]。
- en: After applying these operations, we’ll feed the results through a sigmoid function
    and then repeat the process again with *another* set of weights and biases—now
    called *W*[2] and *B*[2]—to get our final prediction, *P*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些操作后，我们将通过一个sigmoid函数将结果传递，并再次用*另一组*权重和偏置（现在称为*W*[2]和*B*[2]）重复这个过程，以获得我们的最终预测*P*。
- en: Another diagram?
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另一个图表？
- en: 'Does representing things in terms of these individual steps give you intuition
    for what is going on? This question gets at a key theme of this book: to fully
    understand neural networks, we have to see multiple representations, each one
    of which highlights a different aspect of how neural networks work. The representation
    in [Figure 2-10](#fig_02-10) doesn’t give much intuition about the “structure”
    of the network, but it does indicate clearly how to train such a model: on the
    backward pass, we’ll compute the partial derivative of each constituent function,
    evaluated at the input to that function, and then calculate the gradients of the
    loss with respect to each of the weights by simply multiplying all of these derivatives
    together—just as we saw in the simple chain rule examples from [Chapter 1](ch01.html#foundations).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 用这些单独的步骤来表示事物是否让你对正在发生的事情有直观的理解？这个问题涉及到本书的一个关键主题：要完全理解神经网络，我们必须看到多种表示，每一种都突出神经网络工作的不同方面。[图2-10](#fig_02-10)中的表示并没有给出关于网络“结构”的直觉，但它清楚地指示了如何训练这样一个模型：在反向传播过程中，我们将计算每个组成函数的偏导数，在该函数的输入处评估，然后通过简单地将所有这些导数相乘来计算损失相对于每个权重的梯度——就像我们在[第1章](ch01.html#foundations)中看到的简单链式法则示例中一样。
- en: 'Nevertheless, there is another, more standard way to represent a neural network
    like this: we could represent each of our original features as circles. Since
    we have 13 features, we need 13 circles. Then we need 13 more circles to represent
    the 13 outputs of the “linear regression-sigmoid” operation we’re doing. In addition,
    each of these circles is a function of all 13 of our original features, so we’ll
    need lines connecting all of the first set of 13 circles to all of the second
    set.^([6](ch02.html#idm45732625500232))'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有另一种更标准的表示神经网络的方式：我们可以将我们原始特征中的每一个表示为圆圈。由于我们有13个特征，我们需要13个圆圈。然后我们需要13个圆圈来表示我们正在进行的“线性回归-
    Sigmoid”操作的13个输出。此外，每个这些圆圈都是我们原始13个特征的函数，所以我们需要将第一组13个圆圈中的所有圆圈连接到第二组中的所有圆圈。^([6](ch02.html#idm45732625500232))
- en: Finally, all of these 13 outputs are used to make a single final prediction,
    so we’ll draw one more circle to represent the final prediction and 13 lines showing
    that these “intermediate outputs” are “connected” to this final prediction.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有这些13个输出都被用来做一个最终的预测，所以我们会再画一个圆圈来代表最终的预测，以及13条线显示这些“中间输出”与最终预测的“连接”。
- en: '[Figure 2-11](#fig_02-11) shows the final diagram.^([7](ch02.html#idm45732625497016))'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-11](#fig_02-11)显示了最终的图表。^([7](ch02.html#idm45732625497016))'
- en: '![Neural network representation 2](assets/dlfs_0211.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络表示2](assets/dlfs_0211.png)'
- en: Figure 2-11\. A more common (but in many ways less helpful) visual representation
    of a neural network
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11\. 神经网络的更常见（但在许多方面不太有用）的视觉表示
- en: 'If you’ve read anything about neural networks before, you may have seen them
    represented like the diagram in [Figure 2-11](#fig_02-11): as circles with lines
    connecting them. While this representation does have some advantages—it lets you
    see “at a glance” what kind of neural network this is, how many layers it has,
    and so on—it doesn’t give any indication of the actual calculations involved,
    or of how such a network might be trained. Therefore, while this diagram is extremely
    important for you to see because you’ll see it in other places, it’s included
    here primarily so you can see the *connection* between it and the primary way
    we are representing neural networks: as boxes with lines connecting them, where
    each box represents a function that defines both what should happen on the forward
    pass for the model to make predictions and what should happen on the backward
    pass for the model to learn. We’ll see in the next chapter how to translate even
    more directly between these diagrams and code by coding each function as a Python
    class inheriting from a base `Operation` class—and speaking of code, let’s cover
    that next.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前读过关于神经网络的任何东西，你可能已经看到它们被表示为[图2-11](#fig_02-11)中的图表：作为连接它们的线的圆圈。虽然这种表示方法确实有一些优点——它让你一眼就能看到这是什么样的神经网络，有多少层等等——但它并没有给出实际计算的任何指示，或者这样一个网络可能如何训练。因此，虽然这个图表对你来说非常重要，因为你会在其他地方看到它，但它主要包含在这里，让你看到它与我们主要表示神经网络的方式之间的*连接*：作为连接它们的线的方框，其中每个方框代表一个函数，定义了模型在前向传递中应该发生什么以进行预测，以及模型在反向传递中应该学习什么。我们将在下一章中看到如何通过将每个函数编码为继承自基础`Operation`类的Python类来更直接地在这些图表和代码之间进行转换——说到代码，让我们接着讨论。
- en: Code
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'In coding this up, we follow the same function structure as in the simpler
    linear regression function from earlier in the chapter—taking in `weights` as
    a dictionary and returning both the loss value and the `forward_info` dictionary,
    while replacing the internals with the operations specified in [Figure 2-10](#fig_02-10):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码时，我们遵循与本章早期更简单的线性回归函数相同的函数结构——将 `weights` 作为字典传入，并返回损失值和 `forward_info` 字典，同时用
    [图2-10](#fig_02-10) 中指定的操作替换内部操作：
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Even though we’re now dealing with a more complicated diagram, we’re still just
    going step by step through each operation, doing the appropriate computation,
    and saving the results in `forward_info` as we go.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们现在处理的是一个更复杂的图表，我们仍然只是一步一步地通过每个操作，进行适当的计算，并在进行时将结果保存在 `forward_info` 中。
- en: 'Neural Networks: The Backward Pass'
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络：向后传递
- en: The backward pass works the same way as in the simpler linear regression model
    from earlier in the chapter, just with more steps.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 向后传递的工作方式与本章早期更简单的线性回归模型相同，只是步骤更多。
- en: Diagram
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图表
- en: 'The steps, as a reminder, are:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，这些步骤是：
- en: Compute the derivative of each operation and evaluate it at its input.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个操作的导数并在其输入处评估。
- en: Multiply the results together.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果相乘。
- en: As we’ll see yet again, this will work because of the chain rule. [Figure 2-12](#fig_02-12)
    shows all the partial derivatives we have to compute.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将再次看到的那样，这将因为链式法则而起作用。[图2-12](#fig_02-12) 展示了我们需要计算的所有偏导数。
- en: '![Neural network regression backward](assets/dlfs_0212.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络回归向后传递](assets/dlfs_0212.png)'
- en: Figure 2-12\. The partial derivatives associated with each operation in the
    neural network that will be multiplied together on the backward pass
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-12\. 与神经网络中的每个操作相关的偏导数将在向后传递中相乘
- en: Conceptually, we want to compute all these partial derivatives, tracing backward
    through our function, and then multiply them together to get the gradients of
    the loss with respect to each of the weights, just as we did for the linear regression
    model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们希望计算所有这些偏导数，通过我们的函数向后追踪，然后将它们相乘以获得损失相对于每个权重的梯度，就像我们为线性回归模型所做的那样。
- en: Math (and code)
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数学（和代码）
- en: '[Table 2-1](#table-2-1) lists these partial derivatives and the lines in the
    code that correspond to each one.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-1](#table-2-1) 列出了这些偏导数以及与每个偏导数对应的代码行。'
- en: Table 2-1\. Derivative table for neural network
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. 神经网络的导数表
- en: '| Derivative | Code |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 导数 | 代码 |'
- en: '| --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mrow></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></mrow></math>
    | `dLdP = -(forward_info[*y*] - forward_info[*P*])` |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>Λ</mi></mrow> <mrow><mi>∂</mi><mi>P</mi></mfrac>
    <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></mrow></math>
    | `dLdP = -(forward_info[*y*] - forward_info[*P*])` |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>M</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `np.ones_like(forward_info[*M2*])` |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>M</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `np.ones_like(forward_info[*M2*])` |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>B</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `np.ones_like(weights[*B2*])` |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>B</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `np.ones_like(weights[*B2*])` |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>W</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>O</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dM2dW2 = np.transpose(forward_info[*O1*], (1, 0))` |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>W</mi>
    <mn>2</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>O</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dM2dW2 = np.transpose(forward_info[*O1*], (1, 0))` |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>O</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>O</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dM2dO1 = np.transpose(weights[*W2*], (1, 0))` |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>O</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>O</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dM2dO1 = np.transpose(weights[*W2*], (1, 0))` |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>σ</mi></mrow> <mrow><mi>∂</mi><mi>u</mi></mrow></mfrac>
    <mrow><mo>(</mo> <msub><mi>N</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dO1dN1 = sigmoid(forward_info[*N1*] × (1 - sigmoid(forward_info[*N1*])` |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>σ</mi></mrow> <mrow><mi>∂</mi><mi>u</mrow></mfrac>
    <mrow><mo>(</mo> <msub><mi>N</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dO1dN1 = sigmoid(forward_info[*N1*] × (1 - sigmoid(forward_info[*N1*])` |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>M</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dN1dM1 = np.ones_like(forward_info[*M1*])` |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>M</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dN1dM1 = np.ones_like(forward_info[*M1*])` |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>B</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dN1dB1 = np.ones_like(weights[*B1*])` |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>α</mi></mrow> <mrow><mi>∂</mi><msub><mi>B</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>M</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    | `dN1dB1 = np.ones_like(weights[*B1*])` |'
- en: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>W</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <msub><mi>W</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math> | `dM1dW1 = np.transpose(forward_info[*X*],
    (1, 0))` |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| <math><mrow><mfrac><mrow><mi>∂</mi><mi>ν</mi></mrow> <mrow><mi>∂</mi><msub><mi>W</mi>
    <mn>1</mn></msub></mrow></mfrac> <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <msub><mi>W</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math> | `dM1dW1 = np.transpose(forward_info[*X*],
    (1, 0))` |'
- en: Note
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The expressions we compute for the gradient of the loss with respect to the
    bias terms, `dLdB1` and `dLdB2`, will have to be summed along the rows to account
    for the fact that the same bias element is added to each row in the batch of data
    passed through. See [“Gradient of the Loss with Respect to the Bias Terms”](app01.html#gradient-loss-bias-terms)
    for details.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算相对于偏差项的损失梯度`dLdB1`和`dLdB2`的表达式时，必须沿行相加，以考虑通过的数据批次中每行添加相同偏差元素的事实。有关详细信息，请参阅[“相对于偏差项的损失梯度”](app01.html#gradient-loss-bias-terms)。
- en: The overall loss gradient
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体损失梯度
- en: 'You can see the full `loss_gradients` function in the [Jupyter Notebook](https://oreil.ly/2TDV5q9)
    for this chapter on the book’s GitHub page. This function computes each of the
    partial derivatives in [Table 2-1](#table-2-1) and multiplies them together to
    get the gradients of the loss with respect to each of the `ndarray`s containing
    the weights:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章的书的GitHub页面上查看完整的`loss_gradients`函数。此函数计算[表2-1](#table-2-1)中的每个偏导数，并将它们相乘以获得相对于包含权重的`ndarray`中的每个权重的损失梯度：
- en: '`dLdW2`'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dLdW2`'
- en: '`dLdB2`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dLdB2`'
- en: '`dLdW1`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dLdW1`'
- en: '`dLdB1`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dLdB1`'
- en: The only caveat is that we sum the expressions we compute for `dLdB1` and `dLdB2`
    along `axis = 0`, as described in [“Gradient of the Loss with Respect to the Bias
    Terms”](app01.html#gradient-loss-bias-terms).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的注意事项是，我们将计算`dLdB1`和`dLdB2`的表达式沿`axis = 0`相加，如[“相对于偏差项的损失梯度”](app01.html#gradient-loss-bias-terms)中所述。
- en: We’ve finally built our first neural network from scratch! Let’s see if it is
    in fact any better than our linear regression model.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于从头开始构建了我们的第一个神经网络！让我们看看它是否比我们的线性回归模型更好。
- en: Training and Assessing Our First Neural Network
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和评估我们的第一个神经网络
- en: 'Just as the forward and backward passes worked the same for our neural network
    as for the linear regression model from earlier in the chapter, so too are training
    and evaluation the same: for each iteration of data, we pass the input forward
    through the function on the forward pass, compute the gradients of loss with respect
    to the weights on the backward pass, and then use these gradients to update the
    weights. In fact, we can use the following identical code inside the training
    loop:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前向和后向传递对我们的神经网络与本章早期的线性回归模型一样有效，训练和评估也是相同的：对于每次数据迭代，我们通过前向传递函数将输入传递，通过后向传递计算损失相对于权重的梯度，然后使用这些梯度来更新权重。实际上，我们可以在训练循环内使用以下相同的代码：
- en: '[PRE17]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The difference simply lies in the internals of the `forward_loss` and `loss_gradients`
    functions, and in the `weights` dictionary, which now has four keys (`W1`, `B1`,
    `W2`, and `B2`) instead of two. Indeed, this is a major takeaway from this book:
    even for very complex architectures, the mathematical principles and high-level
    training procedures are the same as for simple models.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 区别仅仅在于`forward_loss`和`loss_gradients`函数的内部，以及`weights`字典中，现在有四个键（`W1`，`B1`，`W2`和`B2`），而不是两个。事实上，这是本书的一个重要观点：即使对于非常复杂的架构，数学原理和高级训练程序与简单模型相同。
- en: 'We also get predictions from this model in the same way:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以以相同的方式从该模型中获得预测：
- en: '[PRE18]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The difference again is simply in the internals of the `predict` function:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 区别再次仅仅在于`predict`函数的内部：
- en: '[PRE19]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Using these predictions, we can calculate the mean absolute error and root
    mean squared error on the validation set, as before:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些预测，我们可以在验证集上计算平均绝对误差和均方根误差，就像以前一样：
- en: '[PRE20]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Both values are significantly lower than the prior model! Looking at the plot
    of predictions versus actuals in [Figure 2-13](#fig_02-13) shows similar improvements.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个值都明显低于之前的模型！查看[图2-13](#fig_02-13)中预测与实际值的图表显示了类似的改进。
- en: '![dPdB one element](assets/dlfs_0213.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![dPdB一个元素](assets/dlfs_0213.png)'
- en: Figure 2-13\. Predicted value versus target, in neural network regression
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-13。神经网络回归中的预测值与目标值
- en: Visually, the points look closer to the 45-degree line than in [Figure 2-6](#fig_02-06).
    I encourage you to step through the [Jupyter Notebook](https://oreil.ly/2TDV5q9)
    on the book’s GitHub page and run the code yourself!
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，这些点比[图2-6](#fig_02-06)中更接近45度线。我鼓励您逐步查看本书的GitHub页面上的[Jupyter Notebook](https://oreil.ly/2TDV5q9)并自行运行代码！
- en: Two Reasons Why This Is Happening
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么会发生这种情况的两个原因
- en: Why does this model appear to be performing better than the model before? Recall
    that there was a *nonlinear* relationship between the most important feature of
    our earlier model and our target; nevertheless, our model was constrained to learn
    only *linear* relationships between individual features and our target. I claim
    that, by adding a nonlinear function into the mix, we have allowed our model to
    learn the proper, nonlinear relationship between our features and our target.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个模型看起来比之前的模型表现更好？回想一下，我们早期模型的最重要特征与目标之间存在*非线性*关系；尽管如此，我们的模型被限制为仅学习个体特征与目标之间的*线性*关系。我声称，通过将非线性函数加入到混合中，我们使我们的模型能够学习特征和目标之间的正确非线性关系。
- en: Let’s visualize this. [Figure 2-14](#fig_02-14) shows the same plot we showed
    in the linear regression section, plotting the normalized values of the most important
    feature from our model along with both the values of the target and the *predictions*
    that would result from feeding the mean values of the other features while varying
    the values of the most important feature from –3.5 to 1.5, as before.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来可视化一下。[图2-14](#fig_02-14)展示了我们在线性回归部分展示过的相同图，绘制了模型中最重要特征的归一化数值以及目标值和在变化最重要特征值的同时通过输入其他特征的均值得到的*预测*值，最重要特征的值从-3.5变化到1.5，与之前一样。
- en: '![dPdB one element](assets/dlfs_0214.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![dPdB one element](assets/dlfs_0214.png)'
- en: Figure 2-14\. Most important feature versus target and predictions, neural network
    regression
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-14。最重要特征与目标值和预测值，神经网络回归
- en: We can see that the relationship shown (a) is now nonlinear and (b) more closely
    matches the relationship between this feature and the target (represented by the
    points), as desired. So adding the nonlinear function to our model allowed it
    to learn, via iteratively updating the weights using the training, the nonlinear
    relationship that existed between the inputs and the outputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到所示的关系（a）现在是非线性的，（b）更接近于这个特征和目标之间的关系（由点表示），这是期望的。因此，通过向我们的模型添加非线性函数，使其能够通过迭代更新权重来学习存在于输入和输出之间的非线性关系。
- en: 'That’s the first reason why our neural network performed better than a straightforward
    linear regression. The second reason is that our neural network can learn relationships
    between *combinations* of our original features and our target, as opposed to
    just individual features. This is because the neural network uses a matrix multiplication
    to create 13 “learned features,” each of which is a combination of all the original
    features, and then essentially applies another linear regression on top of these
    learned features. For example, doing some exploratory analysis that is shared
    on the book’s website, we can see that the most important combinations of the
    13 original features that the model has learned are:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们的神经网络表现比直接线性回归好的第一个原因。第二个原因是，我们的神经网络可以学习原始特征和目标之间的*组合*关系，而不仅仅是单个特征。这是因为神经网络使用矩阵乘法创建了13个“学习到的特征”，每个特征都是所有原始特征的组合，然后在这些学习到的特征之上实质上应用另一个线性回归。例如，通过在书籍网站上分享的一些探索性分析，我们可以看到模型学习到的13个原始特征的最重要组合是：
- en: <math display="block"><mrow><mo>-</mo> <mn>4.44</mn> <mo>×</mo> <msub><mi>feature</mi>
    <mn>6</mn></msub> <mo>-</mo> <mn>2.77</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>1</mn></msub>
    <mo>-</mo> <mn>2.07</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>7</mn></msub> <mo>+</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mo>-</mo> <mn>4.44</mn> <mo>×</mo> <msub><mi>feature</mi>
    <mn>6</mn></msub> <mo>-</mo> <mn>2.77</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>1</mn></msub>
    <mo>-</mo> <mn>2.07</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>7</mn></msub> <mo>+</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>
- en: 'and:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以及：
- en: <math display="block"><mrow><mn>4.43</mn> <mo>×</mo> <msub><mi>feature</mi>
    <mn>2</mn></msub> <mo>-</mo> <mn>3.39</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>4</mn></msub>
    <mo>-</mo> <mn>2.39</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>1</mn></msub> <mo>+</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mn>4.43</mn> <mo>×</mo> <msub><mi>feature</mi>
    <mn>2</mn></msub> <mo>-</mo> <mn>3.39</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>4</mn></msub>
    <mo>-</mo> <mn>2.39</mn> <mo>×</mo> <msub><mi>feature</mi> <mn>1</mn></msub> <mo>+</mo>
    <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>
- en: These will then be included, along with 11 other learned features, in a linear
    regression in the last two layers of the neural network.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将与其他11个学习到的特征一起包含在神经网络的最后两层的线性回归中。
- en: These two things—learning *nonlinear* relationships between individual features
    and our target, and learning relationships between *combinations* of features
    and our target—are what allow neural networks to often work better than straightforward
    regressions on real-world problems.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个因素——学习个体特征和目标之间的*非线性*关系，以及学习特征和目标之间的*组合*关系——是神经网络通常比实际问题上的直接回归表现更好的原因。
- en: Conclusion
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you learned how to use the building blocks and mental models
    from [Chapter 1](ch01.html#foundations) to understand, build, and train two standard
    machine learning models to solve real problems. I started by showing how to represent
    a simple machine learning model from classical statistics—linear regression—using
    a computational graph. This representation allowed us to compute the gradients
    of the loss from this model with respect to the model’s parameters and thus train
    the model by continually feeding in data from the training set and updating the
    model’s parameters in the direction that would decrease the loss.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何使用[第1章](ch01.html#foundations)中的基本构建块和心智模型来理解、构建和训练两个标准的机器学习模型来解决实际问题。我首先展示了如何使用计算图表示经典统计学中的简单机器学习模型——线性回归。这种表示允许我们计算出该模型的损失相对于模型参数的梯度，从而通过不断地从训练集中输入数据并更新模型参数来训练模型，使损失减少。
- en: 'Then we saw a limitation of this model: it can only learn *linear* relationships
    between the features and target; this motivated us to try building a model that
    could learn *nonlinear* relationships between the features and target, which led
    us to build our first neural network. You learned how neural networks work by
    building one from scratch, and you also learned how to train them using the same
    high-level procedure we used to train our linear regression models. You then saw
    empirically that the neural network performed better than the simple linear regression
    model and learned two key reasons why: the neural network was able to learn *nonlinear*
    relationships between the features and the target and also to learn relationships
    between *combinations* of features and the target.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到了这个模型的一个限制：它只能学习特征和目标之间的*线性*关系；这促使我们尝试构建一个能够学习特征和目标之间的*非线性*关系的模型，这导致我们构建了我们的第一个神经网络。您通过从头开始构建一个神经网络来学习神经网络的工作原理，并学习如何使用与我们训练线性回归模型相同的高级过程来训练它们。然后您从经验上看到神经网络的表现比简单线性回归模型更好，并学到了两个关键原因：神经网络能够学习特征和目标之间的*非线性*关系，还能够学习特征和目标之间的*组合*关系。
- en: 'Of course, there’s a reason we ended this chapter still covering a relatively
    simple model: defining neural networks in this way is an extremely manual process.
    Defining the forward pass involved 6 individually coded operations, and the backward
    pass involved 17\. However, discerning readers will have noticed that there is
    a lot of repetition in these steps, and by properly defining abstractions, we
    can move from defining models in terms of individual operations (as in this chapter)
    to defining models in terms of these abstractions. This will allow us to build
    more complex models, including deep learning models, while deepening our understanding
    of how these models work. That is what we’ll begin to do in the next chapter.
    Onward!'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们之所以在本章结束时仍然涵盖一个相对简单的模型，是有原因的：以这种方式定义神经网络是一个非常手动的过程。定义前向传播涉及6个单独编码的操作，而反向传播涉及17个。然而，敏锐的读者会注意到这些步骤中存在很多重复，并通过适当定义抽象，我们可以从以个别操作定义模型（如本章中）转变为以这些抽象定义模型。这将使我们能够构建更复杂的模型，包括深度学习模型，同时加深我们对这些模型如何工作的理解。这就是我们将在下一章开始做的事情。继续前进！
- en: ^([1](ch02.html#idm45732627218840-marker)) The other kind of machine learning,
    *un*supervised learning, can be thought of as finding relationships between things
    you have measured and things that have not been measured yet.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种机器学习，*无*监督学习，可以被认为是在你已经测量过的事物和尚未被测量的事物之间找到关系。
- en: '^([2](ch02.html#idm45732627215256-marker)) Though in a real-world problem,
    even how to choose a price isn’t obvious: would it be the price the house last
    sold for? What about a house that hasn’t been on the market for a long time? In
    this book, we’ll focus on examples in which the numeric representation of the
    data is obvious or has been decided for you, but in many real-world problems,
    getting this right is critical.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在现实世界的问题中，甚至如何选择价格都不明显：是房子上次卖出的价格吗？那些长时间没有上市的房子呢？在这本书中，我们将专注于数据的数值表示是明显的或已经为您决定的示例，但在许多现实世界的问题中，正确处理这一点至关重要。
- en: ^([3](ch02.html#idm45732627208776-marker)) Most of you probably know that these
    are called “categorical” features.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你们大多数人可能知道这些被称为“分类”特征。
- en: ^([4](ch02.html#idm45732626899528-marker)) At least the ones we’ll see in this
    book.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 至少是我们在这本书中看到的那些。
- en: ^([5](ch02.html#idm45732626271592-marker)) In addition, we have to sum `dLdB`
    along axis 0; we explain this step in more detail later in this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们必须沿着轴0对`dLdB`进行求和；我们将在本章后面更详细地解释这一步骤。
- en: '^([6](ch02.html#idm45732625500232-marker)) This highlights an interesting idea:
    we *could* have outputs that are connected to only *some* of our original features;
    this is in fact what convolutional neural networks do.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这突出了一个有趣的想法：我们*可以*有输出只连接到我们原始特征的*一部分*；这实际上就是卷积神经网络所做的。
- en: '^([7](ch02.html#idm45732625497016-marker)) Well, not quite: we haven’t drawn
    *all* of the 169 lines we would need to show all the connections between the first
    two “layers” of features, but we have drawn enough of them so that you get the
    idea.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，不完全是：我们没有画出我们需要展示所有连接的169条线，以显示第一层和第二层“特征”之间的所有连接，但我们画出了足够多的线，以便您了解。
