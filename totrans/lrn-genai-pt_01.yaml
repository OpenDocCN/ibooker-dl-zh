- en: 1 What is generative AI and why PyTorch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI vs. nongenerative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why PyTorch is ideal for deep learning and generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of Generative Adversarial Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefits of the attention mechanism and Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of creating generative AI models from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI has significantly affected the global landscape, capturing widespread
    attention and becoming a focal point since the advent of ChatGPT in November 2022\.
    This technological advancement has revolutionized numerous aspects of everyday
    life, ushering in a new era in technology and inspiring a host of startups to
    explore the extensive possibilities offered by various generative models.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the advancements made by Midjourney, a pioneering company, which now
    creates high-resolution, realistic images from brief text inputs. Similarly, Freshworks,
    a software company, has accelerated application development dramatically, reducing
    the time required from an average of 10 weeks to mere days, a feat achieved through
    the capabilities of ChatGPT (see the *Forbes* article “10 Amazing Real-World Examples
    of How Companies Are Using ChatGPT in 2023,” by Bernard Barr, 2023, [https://mng.bz/Bgx0](https://mng.bz/Bgx0)).
    To add a case in point, elements of this very introduction have been enhanced
    by generative AI, demonstrating its ability to refine content to be more engaging.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE What better way to explain generative AI than letting generative AI do
    itself? I asked ChatGPT to rewrite an early draft of this introduction in a “more
    engaging manner” before finalizing it.
  prefs: []
  type: TYPE_NORMAL
- en: The repercussions of this technological advancement extend far beyond these
    examples. Industries are experiencing significant disruption due to the advanced
    capabilities of generative AI. This technology now produces essays comparable
    to those written by humans, composes music reminiscent of classical compositions,
    and rapidly generates complex legal documents, tasks that typically require considerable
    human effort and time. Following the release of ChatGPT, CheggMate, an educational
    platform, witnessed a significant decrease in its stock value. Furthermore, the
    Writers Guild of America, during a recent strike, reached a consensus to put guardrails
    around AI’s encroachment on scriptwriting and editing (see the *WIRED* article
    “Hollywood Writers Reached an AI Deal That Will Rewrite History,” by Will Bedingfield,
    2023, [https://mng.bz/1ajj](https://mng.bz/1ajj)).
  prefs: []
  type: TYPE_NORMAL
- en: NOTE CheggMate charges college students to have their questions answered by
    human specialists. Many of these jobs can now be done by ChatGPT or similar tools
    at a fraction of the costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This raises several questions: What is generative AI, and how does it differ
    from other AI technologies? Why is it causing such widespread disruption across
    various sectors? What is the underlying mechanism of generative AI, and why is
    it important to understand?'
  prefs: []
  type: TYPE_NORMAL
- en: 'This book offers an in-depth exploration of generative AI, a groundbreaking
    technology reshaping numerous industries through its efficient and rapid content
    creation capabilities. Specifically, you’ll learn to use state-of-the-art generative
    models to create various forms of content: shapes, numbers, images, text, and
    audio. Further, instead of treating these models as black boxes, you’ll learn
    to create them from scratch so that you have a deep understanding of the inner
    workings of generative AI. In the words of physicist Richard Feynman, “What I
    cannot create, I do not understand.”'
  prefs: []
  type: TYPE_NORMAL
- en: All these models are based on deep neural networks, and you’ll use Python and
    PyTorch to build, train, and use these models. We chose Python for its user-friendly
    syntax, cross-platform compatibility, and wide community support. We also chose
    PyTorch over other frameworks like TensorFlow for its ease of use and adaptability
    to various model architectures. Python is widely regarded as the primary tool
    for machine learning (ML), and PyTorch has become increasingly popular in the
    field of AI. Therefore, using Python and PyTorch allows you to follow the new
    developments in generative AI. Because PyTorch allows for graphics processing
    unit (GPU) training acceleration, you’ll train these models in a matter of minutes
    or hours and witness generative AI in action!
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Introducing generative AI and PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section explains what generative AI is and how it’s different from its
    nongenerative counterparts: discriminative models. Generative AI is a category
    of technologies with the remarkable capacity to produce diverse forms of new content,
    including text, images, audio, video, source code, and intricate patterns. Generative
    AI crafts entirely new worlds of novel and innovative content; ChatGPT is a notable
    example. In contrast, discriminative modeling predominantly concerns itself with
    the task of recognizing and categorizing pre-existing content.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.1 What is generative AI?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative AI is a type of artificial intelligence that creates new content,
    such as text, images, or music, by learning patterns from existing data. It differs
    from discriminative models, which specialize in discerning disparities among distinct
    data instances and learning the boundary between classes. Figure 1.1 illustrates
    the difference between these two modeling methods. For instance, when confronted
    with an array of images featuring dogs and cats, a discriminative model determines
    whether each image portrays a dog or a cat by capturing a few key features that
    distinguish one from the other (e.g., cats have small noses and pointy ears).
    As the top half of the figure shows, a discriminative model takes data as inputs
    and produces probabilities of different labels, which we denote by Prob(dog) and
    Prob(cat). We can then label the inputs based on the highest predicted probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 A comparison of generative models versus discriminative models. A
    discriminative model (top half of the figure) takes data as inputs and produces
    probabilities of different labels, which we denote by Prob(dog) and Prob(cat).
    In contrast, a generative model (bottom half) acquires an in-depth understanding
    of the defining characteristics of these images to synthesize new images representing
    dogs and cats.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, generative models exhibit a unique ability to generate novel instances
    of data. In the context of our dog and cat example, a generative model acquires
    an in-depth understanding of the defining characteristics of these images to synthesize
    new images representing dogs and cats. As the bottom half of figure 1.1 shows,
    a generative model takes task descriptions (such as varying values in a latent
    space that result in different characteristics in the generated image, which we
    will discuss in detail in chapters 4 to 6) as inputs and produces entirely new
    images of dogs and cats.
  prefs: []
  type: TYPE_NORMAL
- en: From a statistical perspective, when presented with data examples with features
    X, which describe the input and various corresponding labels Y, discriminative
    models undertake the responsibility of predicting conditional probabilities, specifically
    the probability prob(Y|X). Conversely, generative models attempt to learn the
    joint probability distribution of the input features X and the target variable
    Y, denoted as prob (X, Y). Armed with this knowledge, they sample from the distribution
    to conjure fresh instances of X.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of generative models depending on the specific forms
    of content you want to create. In this book, we focus primarily on two prominent
    technologies: Generative Adversarial Networks (GANs) and Transformers (although
    we’ll also cover variational autoencoders and diffusion models). The word “adversarial”
    in GANs refers to the fact that the two neural networks compete against each other
    in a zero-sum game framework: the generative network tries to create data instances
    indistinguishable from real samples, while the discriminative network tries to
    identify the generated samples from real ones. The competition between the two
    networks leads to the improvement of both, eventually enabling the generator to
    create highly realistic data. Transformers are deep neural networks that can efficiently
    solve sequence-to-sequence prediction tasks, and we’ll explain them in more detail
    later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: GANs, celebrated for their ease of implementation and versatility, empower individuals
    with even rudimentary knowledge of deep learning to construct their generative
    models from the ground up. These versatile models can give rise to a plethora
    of creations, from geometric shapes and intricate patterns, as exemplified in
    chapter 3 of this book, to high-quality color images like human faces, which you’ll
    learn to generate in chapter 4\. Furthermore, GANs exhibit the ability to transform
    image content, seamlessly morphing a human face image with blond hair into one
    with black hair, a phenomenon discussed in chapter 6\. Notably, they extend their
    creative prowess to the field of music generation, producing realistic-sounding
    musical compositions, as demonstrated in chapter 13.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to shape, number, or image generation, the art of text generation
    poses formidable challenges, chiefly due to the sequential nature of textual information,
    where the order and arrangement of individual characters and words hold significant
    meaning. To confront this complexity, we turn to Transformers, deep neural networks
    designed to proficiently address sequence-to-sequence prediction tasks. Unlike
    their predecessors, such as recurrent neural networks (RNNs) or convolutional
    neural networks (CNNs), Transformers excel in capturing intricate, long-range
    dependencies inherent in both input and output sequences. Notably, their capacity
    for parallel training (a distributed training method in which a model is trained
    on multiple devices simultaneously) has substantially reduced training times,
    making it possible for us to train Transformers on vast amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: The revolutionary architecture of Transformers underpins the emergence of large
    language models (LLMs; deep neural networks with a massive number of parameters
    and trained on large datasets), including ChatGPT, BERT, DALL-E, and T5\. This
    transformative architecture serves as the bedrock of the recent surge in AI advancement,
    ushered in by the introduction of ChatGPT and other generative pretrained Transformer
    (GPT) models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the subsequent sections, we dive into the comprehensive inner workings of
    these two pioneering technologies: their underlying mechanisms and the myriad
    possibilities they unlock.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.2 The Python programming language
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I assume you have a working knowledge of Python. To follow the content in the
    book, you need to know the Python basics such as functions, classes, lists, dictionaries,
    and so on. If not, there are plenty of free resources online to get you started.
    Follow the instructions in appendix A to install Python. After that, create a
    virtual environment for this book and install Jupyter Notebook as the computing
    environment for projects in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Python has established itself as the leading programming language globally since
    the latter part of 2018, as documented by *The Economist* (see the article “Python
    Is Becoming the World’s Most Popular Coding Language” by the Data Team at *The
    Economist,* 2018, [https://mng.bz/2gj0](https://mng.bz/2gj0)). Python is not only
    free for everyone to use but also allows other users to create and tweak libraries.
    Python has a massive community-driven ecosystem, so you can easily find resources
    and assistance from fellow Python enthusiasts. Plus, Python programmers love to
    share their code, so instead of reinventing the wheel, you can import premade
    libraries and share your own with the Python community.
  prefs: []
  type: TYPE_NORMAL
- en: No matter if you’re on Windows, Mac, or Linux, Python’s got you covered. It’s
    a cross-platform language, although the process of installing software and libraries
    might vary a bit depending on your operating system—but don’t worry; I’ll show
    you how to do it in appendix A. Once everything’s set up, Python code behaves
    the same across different systems.
  prefs: []
  type: TYPE_NORMAL
- en: Python is an expressive language that’s suitable for general application development.
    Its syntax is easy to grasp, making it straightforward for AI enthusiasts to understand
    and work with. If you run into any problems with the Python libraries mentioned
    in this book, you can search Python forums or visit sites like Stack Overflow
    ([https://stackoverflow.com/questions/tagged/python](https://stackoverflow.com/questions/tagged/python))
    for answers. And if all else fails, don’t hesitate to reach out to me for assistance.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, Python offers a large collection of libraries that make creating generative
    models easy (relative to other languages such as C++ or R). In this journey, we’ll
    exclusively use PyTorch as our AI framework, and I’ll explain why we pick it over
    competitors like TensorFlow shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.3 Using PyTorch as our AI framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have settled on using Python as the programming language for this
    book, we’ll choose a suitable AI framework for generative modeling. The two most
    popular AI frameworks in Python are PyTorch and TensorFlow. In this book, we use
    PyTorch over TensorFlow for its ease of use, and I strongly encourage you to do
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is an open-source ML library developed by Meta’s AI Research lab. Built
    on the Python programming language and the Torch library, PyTorch aims to offer
    a flexible and intuitive platform for creating and training deep learning models.
    Torch, the predecessor of PyTorch, was an ML library for building deep neural
    networks in C with a Lua wrapper, but its development was discontinued. PyTorch
    was designed to meet the needs of researchers and developers by providing a more
    user-friendly and adaptable framework for deep learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: A computational graph is a fundamental concept in deep learning that plays a
    crucial role in the efficient computation of complex mathematical operations,
    especially those involving multidimensional arrays or tensors. A computational
    graph is a directed graph where the nodes represent mathematical operations, and
    the edges represent data that flow between these operations. One of the key uses
    of computational graphs is the calculation of partial derivatives when implementing
    backpropagation and gradient descent algorithms. The graph structure allows for
    the efficient calculation of gradients required to update the model parameters
    during training. PyTorch creates and modifies the graph on the fly, which is called
    a dynamic computational graph. This makes it more adaptable to varying model architectures
    and simplifies debugging. Further, just like TensorFlow, PyTorch provides accelerated
    computation through GPU training, which can significantly reduce training time
    compared to central processing unit (CPU) training.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s design aligns well with the Python programming language. Its syntax
    is concise and easy to understand, making it accessible to both newcomers and
    experienced developers. Researchers and developers alike appreciate PyTorch for
    its flexibility. It empowers them to experiment with novel ideas quickly, thanks
    to its dynamic computational graph and simple interface. This flexibility is crucial
    in the rapidly evolving fields of generative AI. PyTorch also has a rapidly growing
    community that actively contributes to its development. This results in an extensive
    ecosystem of libraries, tools, and resources for developers.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch excels in transfer learning, a technique where pretrained models designed
    for a general task are fine-tuned for specific tasks. Researchers and practitioners
    can easily utilize pretrained models, saving time and computational resources.
    This feature is especially important in the age of pretrained LLMs and allows
    us to adopt LLMs for downstream tasks such as classification, text summarization,
    and text generation.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is compatible with other Python libraries, such as NumPy and Matplotlib.
    This interoperability allows data scientists and engineers to seamlessly integrate
    PyTorch into their existing workflows, enhancing productivity. PyTorch is also
    known for its commitment to community-driven development. It evolves rapidly,
    with regular updates and enhancements based on real-world usage and user feedback,
    ensuring that it remains at the cutting edge of AI research and development.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A provides detailed instructions on how to install PyTorch on your
    computer. Follow the instructions to install PyTorch in the virtual environment
    for this book. In case you don’t have a Compute Unified Device Architecture (CUDA)-enabled
    GPU installed on your computer, all programs in this book are compatible with
    CPU training as well. Better yet, I’ll provide the trained models on the book’s
    GitHub repository [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)
    so you can see the trained models in action (in case the trained model is too
    large, I’ll provide them on my personal website [https://gattonweb.uky.edu/faculty/lium/](https://gattonweb.uky.edu/faculty/lium/)).
    In chapter 2, you’ll dive deep into PyTorch. You’ll first learn the data structure
    in PyTorch, Tensor, which holds numbers and matrices and provides functions to
    conduct operations. You’ll then learn to perform an end-to-end deep learning project
    using PyTorch. Specifically, you’ll create a neural network in PyTorch and use
    clothing item images and the corresponding labels to train the network. Once done,
    you use the trained model to classify clothing items into 10 different label types.
    The project will get you ready to use PyTorch to build and train various generative
    models in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section first provides a high-level overview of how GANs work. We then
    use the generation of anime face images as an example to show you the inner workings
    of GANs. Finally, we’ll discuss the practical uses of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 A high-level overview of GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GANs represent a category of generative models initially proposed by Ian Goodfellow
    and his collaborators in 2014 (“Generative Adversarial Nets,” [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)).
    GANs have become extremely popular in recent years because they are easy to build
    and train, and they can generate a wide variety of content. As you’ll see from
    the illustrating example in the next subsection, GANs employ a dual-network architecture
    comprising a generative model tasked with capturing the underlying data distribution
    to generate content and a discriminative model that serves to estimate the likelihood
    that a given sample originates from the authentic training dataset (considered
    as “real”) rather than being a product of the generative model (considered as
    “fake”). The primary objective of the model is to produce new data instances that
    closely resemble those in the training dataset. The nature of the data generated
    by GANs is contingent upon the composition of the training dataset. For example,
    if the training data consists of grayscale images of clothing items, the synthesized
    images will closely resemble such clothing items. Conversely, if the training
    dataset comprises color images of human faces, the generated images will also
    resemble human faces.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at figure 1.2—the architecture of our GAN and its components. To
    train the model, both real samples from the training dataset (as shown at the
    top of figure 1.2) and fake samples created by the generator (left) are presented
    to the discriminator (middle). The principal aim of the generator is to create
    data instances that are virtually indistinguishable from the examples found within
    the training dataset. Conversely, the discriminator strives to distinguish fake
    samples generated by the generator from real samples. These two networks engage
    in a continual competitive process similar to a cat-and-mouse game, trying to
    outperform each other iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 GANs architecture and its components. GANs employ a dual-network
    architecture comprising a generative model (left) tasked with capturing the underlying
    data distribution and a discriminative model (center) that serves to estimate
    the likelihood that a given sample originates from the authentic training dataset
    (considered as “real”) rather than being a product of the generative model (considered
    as “fake”).
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process of the GAN model involves multiple iterations. In each
    iteration, the generator takes some form of task description (step 1) and uses
    it to create fake images (step 2). The fake images, along with real images from
    the training set, are presented to the discriminator (step 3). The discriminator
    tries to classify each sample as either real or fake. It then compares the classification
    with the actual labels, the ground truth (step 4). Both the discriminator and
    the generator receive feedback (step 5) from the classification and improve their
    capabilities: while the discriminator adapts its ability to identify fake samples,
    the generator learns to enhance its capacity to generate convincing samples to
    fool the discriminator. As training advances, an equilibrium is reached when neither
    network can further improve. At this point, the generator becomes capable of producing
    data instances that are practically indistinguishable from real samples.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand exactly how GANs work, let’s look at an illustrating example.
  prefs: []
  type: TYPE_NORMAL
- en: '1.2.2 An illustrating example: Generating anime faces'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Picture this: you’re a passionate anime enthusiast, and you’re on a thrilling
    quest to create your very own anime faces using a powerful tool known as a deep
    convolutional GAN (or DCGAN for short; don’t worry, we’ll dive deeper into this
    in chapter 4).'
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the top middle of figure 1.2, you’ll spot a picture that reads
    “Real Image.” We’ll use 63,632 colorful images of anime faces as our training
    dataset. And if you flip to figure 1.3, you’ll see 32 examples from our training
    set. These special images play a crucial role as they form half of the inputs
    to our discriminator network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 Examples from the anime faces training dataset
  prefs: []
  type: TYPE_NORMAL
- en: The left of figure 1.2 is the generator network. To generate different images
    every time, the generator takes as input a vector Z from the latent space. We
    could think of this vector as a “task description.” During training, we draw different
    Z vectors from the latent space, so the network generates different images every
    time. These fake images are the other half of the inputs to the discriminator
    network.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE By altering the values in the vector Z, we generate different outputs.
    In chapter 5, you’ll learn how to select the vector Z to generate images with
    certain characteristics (e.g., male or female features).
  prefs: []
  type: TYPE_NORMAL
- en: 'But here’s the twist: before we teach our two networks the art of creation
    and detection, the images produced by the generator are, well, gibberish! They
    look nothing like the realistic anime faces you see in figure 1.3\. In fact, they
    resemble nothing more than static on a TV screen (you’ll witness this firsthand
    in chapter 4).'
  prefs: []
  type: TYPE_NORMAL
- en: We train the model for multiple iterations. In each iteration, we present a
    group of images created by the generator, along with a group of anime face images
    from our training set to the discriminator. We ask the discriminator to predict
    whether each image is created by the generator (fake) or from the training set
    (real).
  prefs: []
  type: TYPE_NORMAL
- en: 'You may wonder: How do the discriminator and the generator learn during each
    iteration of training? Once the predictions are made, the discriminator doesn’t
    just sit back; it learns from its prediction blunders for each image. With this
    newfound knowledge, it fine-tunes its parameters, shaping itself to make better
    predictions in the next round. The generator isn’t idle either. It takes notes
    from its image generation process and the discriminator’s prediction outcomes.
    With that knowledge in hand, it adjusts its own network parameters, striving to
    create increasingly lifelike images in the next iteration. The goal? To reduce
    the odds of the discriminator sniffing out its fakes.'
  prefs: []
  type: TYPE_NORMAL
- en: As we journey through these iterations, a remarkable transformation takes place.
    The generator network evolves, producing anime faces that grow more and more realistic,
    akin to those in our training collection. Meanwhile, the discriminator network
    hones its skills, becoming a seasoned detective when it comes to spotting fakes.
    It’s a captivating dance between creation and detection.
  prefs: []
  type: TYPE_NORMAL
- en: Gradually, a magical moment arrives. An equilibrium, or perfect balance, is
    achieved. The images created by the generator become so astonishingly real that
    they are indistinguishable from the genuine anime faces in our training archives.
    At this point, the discriminator is so confused that it assigns a 50% chance of
    authenticity to every image, whether it’s from our training set or was crafted
    by the generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, behold some examples of the artwork of the generator, as shown in
    figure 1.4: they do look indistinguishable from those in our training set.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 Generated anime face images by the trained generator in DCGAN
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 Why should you care about GANs?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GANs are easy to implement and versatile: you’ll learn to generate geometric
    shapes, intricate patterns, high-resolution images, and realistic-sounding music
    in this book alone.'
  prefs: []
  type: TYPE_NORMAL
- en: The practical use of GANs doesn’t stop at generating realistic data. GANs can
    also translate attributes in one image domain to another. As you’ll see in chapter
    6, you can train a CycleGAN (a type of generative model in the GAN family) to
    convert blond hair to black hair in human face images. The same trained model
    can also convert black hair to blond hair. Figure 1.5 shows four rows of images.
    The first row is the original images with blond hair. The trained CycleGAN converts
    them to images with black hair (second row). The last two rows are the original
    images with black hair and the converted image with blond hair, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Changing hair color with CycleGAN. If we feed images with blond hair
    (first row) to a trained CycleGAN model, the model converts blond hair to black
    hair in these images (second row). The same trained model can also convert black
    hair (third row) to blond hair (bottom row).
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about all the amazing skills you’ll pick up from training GANs—they’re
    not just cool; they’re super practical too! Let’s say you run an online clothing
    store with a “Make to Order” strategy (which allows users to customize their purchases
    before manufacturing). Your website showcases tons of unique designs for customers
    to pick from, but here’s the catch: you only make the clothes once someone places
    an order. Creating high-quality images of these clothes can be quite expensive
    since you have to produce the items and then photograph them.'
  prefs: []
  type: TYPE_NORMAL
- en: GANs to the rescue! You don’t need a massive collection of manufactured clothing
    items and their images; instead, you can use something like CycleGAN to transform
    features from one set of images into another, creating a whole new array of styles.
    This is just one nifty way to use GANs. The possibilities are endless because
    these models are super versatile and can handle all sorts of data—making them
    a game-changer for practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transformers are deep neural networks that excel at sequence-to-sequence prediction
    problems, such as taking an input sentence and predicting the most likely next
    words. This section introduces you to the key innovation in Transformers: the
    self-attention mechanism. We’ll then discuss the Transformer architecture and
    different types of Transformers. Finally, we’ll discuss some recent developments
    in Transformers, such as multimodal models (Transformers whose inputs include
    not only text but also other data types such as audio and images) and pretrained
    LLMs (models trained on large textual data that can perform various downstream
    tasks).'
  prefs: []
  type: TYPE_NORMAL
- en: Before the Transformer architecture was invented in 2017 by a group of Google
    researchers (Vaswani et al., “Attention Is All You Need,” [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    natural language processing (NLP) and other sequence-to-sequence prediction tasks
    were primarily handled by RNNs. However, RNNs struggle with retaining information
    about earlier elements in a sequence, which hampers their ability to capture long-term
    dependencies. Even advanced RNN variants like long short-term memory (LSTM) networks,
    which can handle longer-range dependencies, fall short when it comes to extremely
    long-range dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, RNNs (including LSTMs) process inputs sequentially, which
    means these models process one element at a time, in sequence, instead of looking
    at the entire sequence simultaneously. The fact that RNNs conduct computation
    along the symbol positions of the input and output sequences prevents parallel
    training, which makes training slow. This, in turn, makes it impossible to train
    the models on huge datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The key innovation of Transformers is the self-attention mechanism, which excels
    at capturing long-term dependencies in a sequence. Further, since the inputs are
    not handled sequentially in the model, Transformers can be trained in parallel,
    which greatly reduces the training time. More importantly, parallel training makes
    it possible to train Transformers on large amounts of data, which makes LLMs intelligent
    and knowledgeable (based on their ability to process and generate human-like text,
    understand context, and perform a variety of language tasks). This has led to
    the rise of LLMs such as ChatGPT and the recent AI boom.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 The attention mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attention mechanism assigns weights on how an element is related to all
    elements in a sequence (including the element itself). The higher the weight,
    the more closely the two elements are related. These weights are learned from
    large sets of training data in the training process. Therefore, a trained LLM
    such as ChatGPT can figure out the relationship between any two words in a sentence,
    hence making sense of the human language.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may wonder: How does the attention mechanism assign scores to elements
    in a sequence to capture the long-term dependencies? The attention weights are
    calculated by first passing the inputs through three neural network layers to
    obtain query Q, key K, and value V (which we’ll explain in detail in chapter 9).
    The method of using query, key, and value to calculate attention comes from retrieval
    systems. For example, you may go to a public library to search for a book. You
    can type in, say, “machine learning in finance” in the library’s search engine.
    In this case, the query Q is “machine learning in finance.” The keys K are the
    book titles, book descriptions, and so on. The library’s retrieval system will
    recommend a list of books (values V) based on the similarities between the query
    and the keys. Naturally, books with the phrases “machine learning” or “finance”
    or both in titles or descriptions come up on top while books with neither phrase
    in the title or description will show up at the bottom of the list because these
    books will be assigned a low matching score.'
  prefs: []
  type: TYPE_NORMAL
- en: In chapters 9 and 10, you’ll learn the details of the attention mechanism—better
    yet, you’ll implement the attention mechanism from scratch to build and train
    a Transformer to successfully translate English to French.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 The Transformer architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers were first proposed when designing models for machine language
    translation (e.g., English to German or English to French). Figure 1.6 is a diagram
    of the Transformer architecture. The left side is the encoder, and the right side
    is the decoder. In chapters 9 and 10, you’ll learn to construct a Transformer
    from scratch to train the model to translate English to French, and we’ll explain
    figure 1.6 in greater detail then.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder in the Transformer “learns” the meaning of the input sequence (e.g.,
    the English phrase “How are you?”) and converts it into vectors that represent
    this meaning before passing the vectors to the decoder. The decoder constructs
    the output (e.g., the French translation of an English phrase) by predicting one
    word at a time, based on previous words in the sequence and the output from the
    encoder. The trained model can translate common English phrases into French.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of Transformers: encoder-only Transformers, decoder-only
    Transformers, and encoder-decoder Transformers. An encoder-only Transformer has
    no decoder and is capable of converting a sequence into an abstract representation
    for various downstream tasks such as sentiment analysis, named entity recognition,
    and text generation. For example, BERT is an encoder-only Transformer. A decoder-only
    Transformer has only a decoder but no encoder, and it’s well suited for text generation,
    language modeling, and creative writing. GPT-2 (the predecessor of ChatGPT) and
    ChatGPT are both decoder-only Transformers. In chapter 11, you’ll learn to create
    GPT-2 from scratch and then extract the trained model weights from Hugging Face
    (an AI community that hosts and collaborates on ML models, datasets, and applications).
    You’ll load the weights to your GPT-2 model and start generating coherent text.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 The Transformer architecture. The encoder in the Transformer (left
    side of the diagram) learns the meaning of the input sequence (e.g., the English
    phrase “How are you?”) and converts it into an abstract representation that captures
    its meaning before passing it to the decoder (right side of the diagram). The
    decoder constructs the output (e.g., the French translation of the English phrase)
    by predicting one word at a time, based on previous words in the sequence and
    the abstract representation from the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder Transformers are needed for complicated tasks such as multimodal
    models that can handle text-to-image generation or speech recognition. Encoder-decoder
    Transformers combine the strengths of both encoders and decoders. Encoders are
    efficient in processing and understanding input data, while decoders excel in
    generating output. This combination allows the model to effectively understand
    complex inputs (like text or speech) and generate intricate outputs (like images
    or transcribed text).
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3 Multimodal Transformers and pretrained LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recent developments in generative AI give rise to various multimodal models:
    Transformers that can use not only text but also other data types, such as audio
    and images, as inputs. Text-to-image Transformers are one such example. DALL-E
    2, Imagen, and Stable Diffusion are all text-to-image models, and they have garnered
    much media attention due to their ability to generate high-resolution images from
    textual prompts. Text-to-image Transformers incorporate the principles of diffusion
    models, which involve a series of transformations to gradually increase the complexity
    of data. Therefore, we first need to understand diffusion models before we discuss
    text-to-image Transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you want to generate high-resolution flower images by using a diffusion-based
    model. You’ll first obtain a training set of high-quality flower images. You then
    ask the model to gradually add noise to the flower images (the so-called diffusion
    process) until they become completely random noise. You then train the model to
    progressively remove noise from these noisy images to generate new data samples.
    The diffusion process is illustrated in figure 1.7\. The left column contains
    four original flower images. As we move to the right, some noise is added to the
    images in each step, until at the right column, the four images are pure random
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F07_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 The diffusion model adds more and more noise to the images and learns
    to reconstruct them. The left column contains four original flower images. As
    we move to the right, some noise is added to the images in each time step, until
    at the right column, the four images are pure random noise. We then use these
    images to train a diffusion-based model to progressively remove noise from noisy
    images to generate new data samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be wondering: How are text-to-image Transformers related to diffusion
    models? Text-to-image Transformers take a text prompt as input and generate images
    that correspond to that textual description. The text prompt serves as a form
    of conditioning, and the model uses a series of neural network layers to transform
    that textual description into an image. Like diffusion models, text-to-image Transformers
    use a hierarchical architecture with multiple layers, each progressively adding
    more detail to the generated image. The core concept of iteratively refining the
    output is similar in both diffusion models and text-to-image Transformers, as
    we’ll explain in chapter 15.'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models have now become more popular due to their ability to provide
    stable training and generate high-quality images, and they have outperformed other
    generative models such as GANs and variational autoencoders. In chapter 15, you’ll
    first learn to train a simple diffusion model using the Oxford Flower dataset.
    You’ll also learn the basic idea behind multimodal Transformers and write a Python
    program to ask OpenAI’s DALL-E 2 to generate images through a text prompt. For
    example, when I entered “an astronaut in a space suit riding a unicorn” as the
    prompt, DALL-E 2 generated the image shown in figure 1.8\.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F08_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 Image generated by DALL-E 2 with text prompt “an astronaut in a space
    suit riding a unicorn”
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 16, you’ll learn how to access pretrained LLMs such as ChatGPT, GPT4,
    and DALL-E 2\. These models are trained on large textual data and have learned
    general knowledge from the data. Hence, they can perform various downstream tasks
    such as text generation, sentiment analysis, question answering, and named entity
    recognition. Since pretrained LLMs were trained on information a few months ago,
    they cannot provide information on events and developments in the last one or
    two months, let alone real-time information such as weather conditions, flight
    status, or stock prices. We’ll use LangChain (a Python library designed for building
    applications with LLMs, providing tools for prompt management, LLM chaining, and
    output parsing) to chain together LLMs with the Wolfram Alpha and Wikipedia APIs
    to create a know-it-all personal assistant.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Why build generative models from scratch?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of this book is to show you how to build and train all generative
    models from scratch. This way, you’ll have a thorough understanding of the inner
    workings of these models and can make better use of them. Creating something from
    scratch is the best way to understand it. You’ll accomplish this goal for GANs:
    all models, including DCGAN and CycleGAN, are built from the ground up and trained
    using well-curated data in the public domain.'
  prefs: []
  type: TYPE_NORMAL
- en: For Transformers, you’ll build and train all models from scratch except for
    LLMs. This exception is due to the vast amount of data and the supercomputing
    facilities needed to train certain LLMs. However, you’ll make serious progress
    in this direction. Specifically, you’ll implement in chapters 9 and 10 the original
    groundbreaking 2017 paper “Attention Is All You Need” line by line with English-to-French
    translation as an example (the same Transformer can be trained on other datasets
    such as Chinese to English or English to German translations). You’ll also build
    a small-size decoder-only Transformer and train it using several of Ernest Hemingway’s
    novels, including *The Old Man and the Sea*. The trained model can generate text
    in Hemingway style. ChatGPT and GPT-4 are too large and complicated to build and
    train from scratch for our purposes, but you’ll peek into their predecessor, GPT-2,
    and learn to build it from scratch. You’ll also extract the trained weights from
    Hugging Face and load them up to the GPT-2 model you built and start to generate
    realistic text that can pass as human-written.
  prefs: []
  type: TYPE_NORMAL
- en: In this sense, the book is taking a more fundamental approach than most books.
    Instead of treating generative AI models as a black box, readers have a chance
    to look under the hood and examine in detail the inner workings of these models.
    The goal is for you to have a deeper understanding of generative models. This,
    in turn, can potentially help you build better and more responsible generative
    AI for the following reasons.
  prefs: []
  type: TYPE_NORMAL
- en: First, having a deep understanding of the architecture of generative models
    helps readers make better practical uses of these models. For example, in chapter
    5, you’ll learn how to select characteristics in generated images such as male
    or female features and with or without eyeglasses. By building a conditional GAN
    from the ground up, you understand that certain features of the generated images
    are determined by the random noise vector, Z, in the latent space. Therefore,
    you can choose different values of Z as inputs to the trained model to generate
    the desired characteristics (such as male or female features). This type of attribute
    selection is hard to do without understanding the design of the model.
  prefs: []
  type: TYPE_NORMAL
- en: For Transformers, knowing the architecture (and what encoders and decoders do)
    gives you the ability to create and train Transformers to generate the types of
    content you are interested in (say, Jane Austin–style novels or Mozart-style music).
    This understanding also helps you with pretrained LLMs. For example, while it
    is hard to train GPT-2 from scratch with its 1.5 billion parameters, you can add
    an additional layer to the model and fine-tune it for other downstream tasks such
    as text classification, sentiment analysis, and question-answering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, a deep understanding of generative AI helps readers have an unbiased
    assessment of the dangers of AI. While the extraordinary powers of generative
    AI have benefitted us in our daily lives and work, it also has the potential to
    create great harm. Elon Musk went so far as saying that “there’s some chance that
    it goes wrong and destroys humanity” (see the article by Julia Mueller in *The
    Hill*, 2023, “Musk: There’s a Chance AI ‘Goes Wrong and Destroys Humanity,’” [https://mng.bz/Aaxz](https://mng.bz/Aaxz)).
    More and more people in academics and in the tech industry are worried about the
    dangers posed by AI in general and generative AI in particular. Generative AI,
    especially LLMs, can lead to unintended consequences, as many pioneers in the
    tech profession have warned (see, e.g., Stuart Russell, 2023, “How to Stop Runaway
    AI,” [https://mng.bz/ZVzP](https://mng.bz/ZVzP)). It’s not a coincidence that
    merely five months after the release of ChatGPT, many tech industry experts and
    entrepreneurs, including Steve Wozniak, Tristan Harris, Yoshua Bengio, and Sam
    Altman, signed an open letter calling for a pause in training any AI system that’s
    more powerful than GPT-4 for at least six months (see the article by Connie Loizos
    in *TechCrunch*, “1,100+ Notable Signatories Just Signed an Open Letter Asking
    ‘All AI Labs to Immediately Pause for at Least 6 Months,’” [https://mng.bz/RNEK](https://mng.bz/RNEK)).
    A thorough understanding of the architecture of generative models helps us provide
    a deep and unbiased evaluation of the benefits and potential dangers of AI.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI is a type of technology with the capacity to produce diverse forms
    of new content, including texts, images, code, music, audio, and video.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminative models specialize in assigning labels while generative models
    generate new instances of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch, with its dynamic computational graphs and the ability for GPU training,
    is well suited for deep learning and generative modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GANs are a type of generative modeling method consisting of two neural networks:
    a generator and a discriminator. The goal of the generator is to create realistic
    data samples to maximize the chance that the discriminator thinks they are real.
    The goal of the discriminator is to correctly identify fake samples from real
    ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers are deep neural networks that use the attention mechanism to identify
    long-term dependencies among elements in a sequence. The original Transformer
    has an encoder and a decoder. When it’s used for English-to-French translation,
    for example, the encoder converts the English sentence into an abstract representation
    before passing it to the decoder. The decoder generates the French translation
    one word at a time, based on the encoder’s output and the previously generated
    words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
