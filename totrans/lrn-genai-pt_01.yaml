- en: 1 What is generative AI and why PyTorch?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 什么是生成式AI，为什么是PyTorch？
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Generative AI vs. nongenerative AI
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式AI与非生成式AI的比较
- en: Why PyTorch is ideal for deep learning and generative AI
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么PyTorch是深度学习和生成式AI的理想选择
- en: The concept of Generative Adversarial Networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络的概念
- en: The benefits of the attention mechanism and Transformers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制和Transformers的好处
- en: Advantages of creating generative AI models from scratch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头创建生成式AI模型的优势
- en: Generative AI has significantly affected the global landscape, capturing widespread
    attention and becoming a focal point since the advent of ChatGPT in November 2022\.
    This technological advancement has revolutionized numerous aspects of everyday
    life, ushering in a new era in technology and inspiring a host of startups to
    explore the extensive possibilities offered by various generative models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI自2022年11月ChatGPT问世以来，对全球格局产生了重大影响，引起了广泛关注，并成为焦点。这项技术进步彻底改变了日常生活的许多方面，迎来了技术新时代，并激发了许多初创公司探索各种生成模型提供的广泛可能性。
- en: Consider the advancements made by Midjourney, a pioneering company, which now
    creates high-resolution, realistic images from brief text inputs. Similarly, Freshworks,
    a software company, has accelerated application development dramatically, reducing
    the time required from an average of 10 weeks to mere days, a feat achieved through
    the capabilities of ChatGPT (see the *Forbes* article “10 Amazing Real-World Examples
    of How Companies Are Using ChatGPT in 2023,” by Bernard Barr, 2023, [https://mng.bz/Bgx0](https://mng.bz/Bgx0)).
    To add a case in point, elements of this very introduction have been enhanced
    by generative AI, demonstrating its ability to refine content to be more engaging.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下Midjourney这家先驱公司取得的进步，它现在可以从简短的文字输入中创建高分辨率、逼真的图像。同样，软件公司Freshworks显著加速了应用程序的开发，将平均所需时间从10周缩短到仅仅几天，这是通过ChatGPT的能力实现的（参见*Forbes*文章“2023年公司如何使用ChatGPT的10个惊人的真实世界例子”，作者Bernard
    Barr，2023年，[https://mng.bz/Bgx0](https://mng.bz/Bgx0)）。为了举例说明，这篇引言的一些内容已经通过生成式AI得到了增强，展示了其精炼内容以使其更具吸引力的能力。
- en: NOTE What better way to explain generative AI than letting generative AI do
    itself? I asked ChatGPT to rewrite an early draft of this introduction in a “more
    engaging manner” before finalizing it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项：有什么比让生成式AI自己解释生成式AI更好的方法呢？在最终确定之前，我让ChatGPT重新撰写了这篇引言的早期草稿，以“更具吸引力”的方式呈现。
- en: The repercussions of this technological advancement extend far beyond these
    examples. Industries are experiencing significant disruption due to the advanced
    capabilities of generative AI. This technology now produces essays comparable
    to those written by humans, composes music reminiscent of classical compositions,
    and rapidly generates complex legal documents, tasks that typically require considerable
    human effort and time. Following the release of ChatGPT, CheggMate, an educational
    platform, witnessed a significant decrease in its stock value. Furthermore, the
    Writers Guild of America, during a recent strike, reached a consensus to put guardrails
    around AI’s encroachment on scriptwriting and editing (see the *WIRED* article
    “Hollywood Writers Reached an AI Deal That Will Rewrite History,” by Will Bedingfield,
    2023, [https://mng.bz/1ajj](https://mng.bz/1ajj)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术进步的影响远远超出了这些例子。由于生成式AI的先进能力，各行业正经历着重大的颠覆。这项技术现在可以创作出与人类写作相当的文章，创作出令人联想到古典作品的音乐，并快速生成复杂的法律文件，这些任务通常需要大量的人类努力和时间。ChatGPT发布后，教育平台CheggMate的股价出现了显著下降。此外，美国编剧工会最近的一次罢工中，达成了一致意见，要为AI在剧本写作和编辑方面的侵犯设定界限（参见*Wired*文章“好莱坞编剧达成AI协议，将改写历史”，作者Will
    Bedingfield，2023年，[https://mng.bz/1ajj](https://mng.bz/1ajj)）。
- en: NOTE CheggMate charges college students to have their questions answered by
    human specialists. Many of these jobs can now be done by ChatGPT or similar tools
    at a fraction of the costs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项：CheggMate向大学生收费，让他们的问题由人类专家回答。现在，许多这些工作可以通过ChatGPT或类似工具以极低成本完成。
- en: 'This raises several questions: What is generative AI, and how does it differ
    from other AI technologies? Why is it causing such widespread disruption across
    various sectors? What is the underlying mechanism of generative AI, and why is
    it important to understand?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一系列问题：什么是生成式AI，它与其他AI技术有何不同？为什么它在各个行业引起了如此广泛的颠覆？生成式AI的潜在机制是什么，为什么了解它很重要？
- en: 'This book offers an in-depth exploration of generative AI, a groundbreaking
    technology reshaping numerous industries through its efficient and rapid content
    creation capabilities. Specifically, you’ll learn to use state-of-the-art generative
    models to create various forms of content: shapes, numbers, images, text, and
    audio. Further, instead of treating these models as black boxes, you’ll learn
    to create them from scratch so that you have a deep understanding of the inner
    workings of generative AI. In the words of physicist Richard Feynman, “What I
    cannot create, I do not understand.”'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书深入探讨了生成式人工智能，这是一种通过从现有数据中学习模式来创建新内容（如文本、图像或音乐）的人工智能类型。它与专注于区分不同数据实例之间的差异并学习类别之间边界的判别模型不同。图1.1展示了这两种建模方法之间的差异。例如，当面对包含狗和猫的图像数组时，判别模型通过捕捉区分两者的几个关键特征（例如，猫有小的鼻子和尖耳朵）来确定每张图像描绘的是狗还是猫。如图表的上半部分所示，判别模型将数据作为输入，并产生不同标签的概率，我们用Prob(狗)和Prob(猫)表示。然后我们可以根据最高的预测概率对输入进行标记。
- en: All these models are based on deep neural networks, and you’ll use Python and
    PyTorch to build, train, and use these models. We chose Python for its user-friendly
    syntax, cross-platform compatibility, and wide community support. We also chose
    PyTorch over other frameworks like TensorFlow for its ease of use and adaptability
    to various model architectures. Python is widely regarded as the primary tool
    for machine learning (ML), and PyTorch has become increasingly popular in the
    field of AI. Therefore, using Python and PyTorch allows you to follow the new
    developments in generative AI. Because PyTorch allows for graphics processing
    unit (GPU) training acceleration, you’ll train these models in a matter of minutes
    or hours and witness generative AI in action!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模型都是基于深度神经网络，你将使用Python和PyTorch来构建、训练和使用这些模型。我们选择Python是因为其用户友好的语法、跨平台兼容性和广泛的社区支持。我们还选择了PyTorch而不是TensorFlow等其他框架，因为它易于使用并且能够适应各种模型架构。Python被广泛认为是机器学习（ML）的主要工具，PyTorch在人工智能领域也越来越受欢迎。因此，使用Python和PyTorch可以使你跟上生成式人工智能的新发展。因为PyTorch允许使用图形处理单元（GPU）进行训练加速，所以你可以在几分钟或几小时内训练这些模型，并见证生成式人工智能的实际应用！
- en: 1.1 Introducing generative AI and PyTorch
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 介绍生成式人工智能和PyTorch
- en: 'This section explains what generative AI is and how it’s different from its
    nongenerative counterparts: discriminative models. Generative AI is a category
    of technologies with the remarkable capacity to produce diverse forms of new content,
    including text, images, audio, video, source code, and intricate patterns. Generative
    AI crafts entirely new worlds of novel and innovative content; ChatGPT is a notable
    example. In contrast, discriminative modeling predominantly concerns itself with
    the task of recognizing and categorizing pre-existing content.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了什么是生成式人工智能以及它与非生成式人工智能（如判别模型）的不同之处。生成式人工智能是一类具有非凡能力，能够产生各种形式的新内容的技术，包括文本、图像、音频、视频、源代码和复杂模式。生成式人工智能能够创造全新的内容世界；ChatGPT是一个显著的例子。相比之下，判别建模主要关注的是识别和分类现有内容的工作。
- en: 1.1.1 What is generative AI?
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 什么是生成式人工智能？
- en: Generative AI is a type of artificial intelligence that creates new content,
    such as text, images, or music, by learning patterns from existing data. It differs
    from discriminative models, which specialize in discerning disparities among distinct
    data instances and learning the boundary between classes. Figure 1.1 illustrates
    the difference between these two modeling methods. For instance, when confronted
    with an array of images featuring dogs and cats, a discriminative model determines
    whether each image portrays a dog or a cat by capturing a few key features that
    distinguish one from the other (e.g., cats have small noses and pointy ears).
    As the top half of the figure shows, a discriminative model takes data as inputs
    and produces probabilities of different labels, which we denote by Prob(dog) and
    Prob(cat). We can then label the inputs based on the highest predicted probabilities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能是一种通过从现有数据中学习模式来创建新内容（如文本、图像或音乐）的人工智能类型。它与专注于区分不同数据实例之间的差异并学习类别之间边界的判别模型不同。图1.1展示了这两种建模方法之间的差异。例如，当面对包含狗和猫的图像数组时，判别模型通过捕捉区分两者的几个关键特征（例如，猫有小的鼻子和尖耳朵）来确定每张图像描绘的是狗还是猫。如图表的上半部分所示，判别模型将数据作为输入，并产生不同标签的概率，我们用Prob(狗)和Prob(猫)表示。然后我们可以根据最高的预测概率对输入进行标记。
- en: '![](../../OEBPS/Images/CH01_F01_Liu.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F01_Liu.png)'
- en: Figure 1.1 A comparison of generative models versus discriminative models. A
    discriminative model (top half of the figure) takes data as inputs and produces
    probabilities of different labels, which we denote by Prob(dog) and Prob(cat).
    In contrast, a generative model (bottom half) acquires an in-depth understanding
    of the defining characteristics of these images to synthesize new images representing
    dogs and cats.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 生成模型与判别模型的比较。判别模型（图的上半部分）将数据作为输入，并产生不同标签的概率，我们用Prob(dog)和Prob(cat)表示。相反，生成模型（图的下半部分）通过深入了解这些图像的显著特征来合成代表狗和猫的新图像。
- en: In contrast, generative models exhibit a unique ability to generate novel instances
    of data. In the context of our dog and cat example, a generative model acquires
    an in-depth understanding of the defining characteristics of these images to synthesize
    new images representing dogs and cats. As the bottom half of figure 1.1 shows,
    a generative model takes task descriptions (such as varying values in a latent
    space that result in different characteristics in the generated image, which we
    will discuss in detail in chapters 4 to 6) as inputs and produces entirely new
    images of dogs and cats.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，生成模型展现出生成数据新实例的独特能力。在我们的狗和猫示例中，生成模型通过深入了解这些图像的显著特征来合成代表狗和猫的新图像。如图1.1的下半部分所示，生成模型将任务描述（例如，在潜在空间中变化的值导致生成的图像具有不同的特征，我们将在第4章到第6章中详细讨论）作为输入，并产生狗和猫的全新图像。
- en: From a statistical perspective, when presented with data examples with features
    X, which describe the input and various corresponding labels Y, discriminative
    models undertake the responsibility of predicting conditional probabilities, specifically
    the probability prob(Y|X). Conversely, generative models attempt to learn the
    joint probability distribution of the input features X and the target variable
    Y, denoted as prob (X, Y). Armed with this knowledge, they sample from the distribution
    to conjure fresh instances of X.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学的角度来看，当面对具有特征X的数据示例，这些特征描述了输入和相应的各种标签Y时，判别模型承担预测条件概率的责任，具体来说是预测Y|X的概率。相反，生成模型试图学习输入特征X和目标变量Y的联合概率分布，表示为prob(X,
    Y)。凭借这种知识，它们从分布中采样，以产生X的新实例。
- en: 'There are different types of generative models depending on the specific forms
    of content you want to create. In this book, we focus primarily on two prominent
    technologies: Generative Adversarial Networks (GANs) and Transformers (although
    we’ll also cover variational autoencoders and diffusion models). The word “adversarial”
    in GANs refers to the fact that the two neural networks compete against each other
    in a zero-sum game framework: the generative network tries to create data instances
    indistinguishable from real samples, while the discriminative network tries to
    identify the generated samples from real ones. The competition between the two
    networks leads to the improvement of both, eventually enabling the generator to
    create highly realistic data. Transformers are deep neural networks that can efficiently
    solve sequence-to-sequence prediction tasks, and we’ll explain them in more detail
    later in this chapter.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你想要创建的具体内容形式，存在不同类型的生成模型。在这本书中，我们主要关注两种突出的技术：生成对抗网络（GANs）和转换器（虽然我们也会涵盖变分自编码器和扩散模型）。在GANs中的“对抗”一词指的是两个神经网络在零和博弈框架中相互竞争的事实：生成网络试图创建与真实样本不可区分的数据实例，而判别网络则试图从真实样本中识别生成的样本。两个网络之间的竞争导致两者都得到改进，最终使生成器能够创建高度逼真的数据。转换器是能够高效解决序列到序列预测任务的深度神经网络，我们将在本章后面更详细地解释它们。
- en: GANs, celebrated for their ease of implementation and versatility, empower individuals
    with even rudimentary knowledge of deep learning to construct their generative
    models from the ground up. These versatile models can give rise to a plethora
    of creations, from geometric shapes and intricate patterns, as exemplified in
    chapter 3 of this book, to high-quality color images like human faces, which you’ll
    learn to generate in chapter 4\. Furthermore, GANs exhibit the ability to transform
    image content, seamlessly morphing a human face image with blond hair into one
    with black hair, a phenomenon discussed in chapter 6\. Notably, they extend their
    creative prowess to the field of music generation, producing realistic-sounding
    musical compositions, as demonstrated in chapter 13.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GANs因其易于实现和多功能性而备受赞誉，使连对深度学习只有初步了解的人也能从头开始构建他们的生成模型。这些多才多艺的模型可以产生各种各样的创作，从本书第3章中展示的几何形状和复杂图案，到第4章中将学习生成的高质量彩色图像，如人类面孔。此外，GANs还表现出转换图像内容的能力，无缝地将金发人类面孔图像转变为黑发人类面孔图像，这在第6章中进行了讨论。值得注意的是，它们将它们的创造力扩展到音乐生成领域，产生听起来逼真的音乐作品，如第13章中所示。
- en: In contrast to shape, number, or image generation, the art of text generation
    poses formidable challenges, chiefly due to the sequential nature of textual information,
    where the order and arrangement of individual characters and words hold significant
    meaning. To confront this complexity, we turn to Transformers, deep neural networks
    designed to proficiently address sequence-to-sequence prediction tasks. Unlike
    their predecessors, such as recurrent neural networks (RNNs) or convolutional
    neural networks (CNNs), Transformers excel in capturing intricate, long-range
    dependencies inherent in both input and output sequences. Notably, their capacity
    for parallel training (a distributed training method in which a model is trained
    on multiple devices simultaneously) has substantially reduced training times,
    making it possible for us to train Transformers on vast amounts of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与形状、数量或图像生成相比，文本生成的艺术面临巨大的挑战，这主要归因于文本信息的序列性质，其中单个字符和单词的顺序和排列具有重大意义。为了应对这种复杂性，我们转向Transformer，这是一种专为高效处理序列到序列预测任务而设计的深度神经网络。与它们的
    predecessors，如循环神经网络（RNN）或卷积神经网络（CNN）不同，Transformer在捕捉输入和输出序列中固有的复杂、长距离依赖关系方面表现出色。值得注意的是，它们并行训练的能力（一种在多个设备上同时训练模型的多设备训练方法）大大减少了训练时间，使我们能够在大量数据上训练Transformer。
- en: The revolutionary architecture of Transformers underpins the emergence of large
    language models (LLMs; deep neural networks with a massive number of parameters
    and trained on large datasets), including ChatGPT, BERT, DALL-E, and T5\. This
    transformative architecture serves as the bedrock of the recent surge in AI advancement,
    ushered in by the introduction of ChatGPT and other generative pretrained Transformer
    (GPT) models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的革命性架构是大型语言模型（LLMs；具有大量参数并在大型数据集上训练的深度神经网络）的出现的基础，包括ChatGPT、BERT、DALL-E和T5。这种变革性的架构是近年来AI进步激增的基石，ChatGPT和其他生成预训练Transformer（GPT）模型的引入开启了这一进步。
- en: 'In the subsequent sections, we dive into the comprehensive inner workings of
    these two pioneering technologies: their underlying mechanisms and the myriad
    possibilities they unlock.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入了解这两种开创性技术的全面内部工作原理：它们的底层机制和它们解锁的众多可能性。
- en: 1.1.2 The Python programming language
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 Python编程语言
- en: I assume you have a working knowledge of Python. To follow the content in the
    book, you need to know the Python basics such as functions, classes, lists, dictionaries,
    and so on. If not, there are plenty of free resources online to get you started.
    Follow the instructions in appendix A to install Python. After that, create a
    virtual environment for this book and install Jupyter Notebook as the computing
    environment for projects in this book.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您已经具备Python的基本知识。为了跟随书中的内容，您需要了解Python的基础知识，例如函数、类、列表、字典等。如果您还没有，网上有大量的免费资源可以帮助您入门。按照附录A中的说明安装Python。之后，为本书创建一个虚拟环境，并安装Jupyter
    Notebook作为本书项目中计算环境。
- en: Python has established itself as the leading programming language globally since
    the latter part of 2018, as documented by *The Economist* (see the article “Python
    Is Becoming the World’s Most Popular Coding Language” by the Data Team at *The
    Economist,* 2018, [https://mng.bz/2gj0](https://mng.bz/2gj0)). Python is not only
    free for everyone to use but also allows other users to create and tweak libraries.
    Python has a massive community-driven ecosystem, so you can easily find resources
    and assistance from fellow Python enthusiasts. Plus, Python programmers love to
    share their code, so instead of reinventing the wheel, you can import premade
    libraries and share your own with the Python community.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自2018年后期以来，Python已经成为全球领先的编程语言，正如《经济学人》杂志所记录的（参见《经济学人》数据团队撰写的文章“Python Is Becoming
    the World’s Most Popular Coding Language”，2018年，[https://mng.bz/2gj0](https://mng.bz/2gj0)）。Python不仅对每个人都是免费的，还允许其他用户创建和调整库。Python有一个庞大的社区驱动的生态系统，因此你可以轻松找到来自其他Python爱好者的资源和帮助。此外，Python程序员喜欢分享他们的代码，所以你不必重新发明轮子，你可以导入现成的库，并与Python社区分享你的代码。
- en: No matter if you’re on Windows, Mac, or Linux, Python’s got you covered. It’s
    a cross-platform language, although the process of installing software and libraries
    might vary a bit depending on your operating system—but don’t worry; I’ll show
    you how to do it in appendix A. Once everything’s set up, Python code behaves
    the same across different systems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是在Windows、Mac还是Linux上，Python都能满足你的需求。它是一种跨平台语言，尽管安装软件和库的过程可能因操作系统而异——但不用担心；我会在附录A中向你展示如何操作。一旦一切准备就绪，Python代码在不同系统上表现一致。
- en: Python is an expressive language that’s suitable for general application development.
    Its syntax is easy to grasp, making it straightforward for AI enthusiasts to understand
    and work with. If you run into any problems with the Python libraries mentioned
    in this book, you can search Python forums or visit sites like Stack Overflow
    ([https://stackoverflow.com/questions/tagged/python](https://stackoverflow.com/questions/tagged/python))
    for answers. And if all else fails, don’t hesitate to reach out to me for assistance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Python是一种适合通用应用开发的表达性语言。它的语法易于理解，使得AI爱好者能够轻松理解和操作。如果你在这本书中提到的Python库遇到任何问题，你可以在Python论坛上搜索或访问像Stack
    Overflow([https://stackoverflow.com/questions/tagged/python](https://stackoverflow.com/questions/tagged/python))这样的网站寻找答案。如果所有其他方法都失败了，请不要犹豫，向我寻求帮助。
- en: Lastly, Python offers a large collection of libraries that make creating generative
    models easy (relative to other languages such as C++ or R). In this journey, we’ll
    exclusively use PyTorch as our AI framework, and I’ll explain why we pick it over
    competitors like TensorFlow shortly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Python提供了一大批库，使得创建生成模型变得容易（相对于C++或R等其他语言）。在这个旅程中，我们将独家使用PyTorch作为我们的AI框架，我将在稍后解释为什么我们选择它而不是像TensorFlow这样的竞争对手。
- en: 1.1.3 Using PyTorch as our AI framework
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 使用PyTorch作为我们的AI框架
- en: Now that we have settled on using Python as the programming language for this
    book, we’ll choose a suitable AI framework for generative modeling. The two most
    popular AI frameworks in Python are PyTorch and TensorFlow. In this book, we use
    PyTorch over TensorFlow for its ease of use, and I strongly encourage you to do
    the same.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经决定将Python作为本书的编程语言，我们将选择一个适合生成建模的AI框架。在Python中，最受欢迎的两个AI框架是PyTorch和TensorFlow。在这本书中，我们选择PyTorch而不是TensorFlow，因为它易于使用，并且我强烈建议你也这样做。
- en: PyTorch is an open-source ML library developed by Meta’s AI Research lab. Built
    on the Python programming language and the Torch library, PyTorch aims to offer
    a flexible and intuitive platform for creating and training deep learning models.
    Torch, the predecessor of PyTorch, was an ML library for building deep neural
    networks in C with a Lua wrapper, but its development was discontinued. PyTorch
    was designed to meet the needs of researchers and developers by providing a more
    user-friendly and adaptable framework for deep learning projects.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是由Meta的AI研究实验室开发的开源ML库。它建立在Python编程语言和Torch库的基础上，旨在提供一个灵活且直观的平台，用于创建和训练深度学习模型。Torch是PyTorch的前身，是一个用C语言构建深度神经网络并带有Lua包装器的ML库，但它的开发已经停止。PyTorch旨在通过提供一个更用户友好和适应性强的框架来满足研究人员和开发者的需求。
- en: A computational graph is a fundamental concept in deep learning that plays a
    crucial role in the efficient computation of complex mathematical operations,
    especially those involving multidimensional arrays or tensors. A computational
    graph is a directed graph where the nodes represent mathematical operations, and
    the edges represent data that flow between these operations. One of the key uses
    of computational graphs is the calculation of partial derivatives when implementing
    backpropagation and gradient descent algorithms. The graph structure allows for
    the efficient calculation of gradients required to update the model parameters
    during training. PyTorch creates and modifies the graph on the fly, which is called
    a dynamic computational graph. This makes it more adaptable to varying model architectures
    and simplifies debugging. Further, just like TensorFlow, PyTorch provides accelerated
    computation through GPU training, which can significantly reduce training time
    compared to central processing unit (CPU) training.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图是深度学习中的一个基本概念，在高效计算复杂数学运算中扮演着至关重要的角色，尤其是涉及多维数组或张量的运算。计算图是一种有向图，其中的节点代表数学运算，而边则代表在这些运算之间流动的数据。计算图的关键用途之一是在实现反向传播和梯度下降算法时计算偏导数。图结构允许高效地计算在训练过程中更新模型参数所需的梯度。PyTorch能够动态创建和修改图，这被称为动态计算图。这使得它能够更好地适应不断变化的结构模型，并简化了调试过程。此外，就像TensorFlow一样，PyTorch通过GPU训练提供加速计算，与中央处理器（CPU）训练相比，可以显著减少训练时间。
- en: PyTorch’s design aligns well with the Python programming language. Its syntax
    is concise and easy to understand, making it accessible to both newcomers and
    experienced developers. Researchers and developers alike appreciate PyTorch for
    its flexibility. It empowers them to experiment with novel ideas quickly, thanks
    to its dynamic computational graph and simple interface. This flexibility is crucial
    in the rapidly evolving fields of generative AI. PyTorch also has a rapidly growing
    community that actively contributes to its development. This results in an extensive
    ecosystem of libraries, tools, and resources for developers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的设计与Python编程语言相得益彰。其语法简洁易懂，使得新手和经验丰富的开发者都能轻松上手。研究人员和开发者都欣赏PyTorch的灵活性。它使他们能够快速实验新想法，这得益于其动态计算图和简单的接口。这种灵活性在快速发展的生成人工智能领域至关重要。PyTorch还拥有一个快速发展的社区，积极为其发展做出贡献。这导致了一个庞大的生态系统，包括库、工具和资源，为开发者提供支持。
- en: PyTorch excels in transfer learning, a technique where pretrained models designed
    for a general task are fine-tuned for specific tasks. Researchers and practitioners
    can easily utilize pretrained models, saving time and computational resources.
    This feature is especially important in the age of pretrained LLMs and allows
    us to adopt LLMs for downstream tasks such as classification, text summarization,
    and text generation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在迁移学习方面表现出色，这是一种将针对通用任务设计的预训练模型微调以适应特定任务的技术。研究人员和实践者可以轻松利用预训练模型，节省时间和计算资源。这一特性在预训练大型语言模型（LLMs）的时代尤为重要，它使我们能够将LLMs应用于下游任务，如分类、文本摘要和文本生成。
- en: PyTorch is compatible with other Python libraries, such as NumPy and Matplotlib.
    This interoperability allows data scientists and engineers to seamlessly integrate
    PyTorch into their existing workflows, enhancing productivity. PyTorch is also
    known for its commitment to community-driven development. It evolves rapidly,
    with regular updates and enhancements based on real-world usage and user feedback,
    ensuring that it remains at the cutting edge of AI research and development.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch与其它Python库兼容，如NumPy和Matplotlib。这种互操作性允许数据科学家和工程师无缝地将PyTorch集成到现有的工作流程中，提高生产力。PyTorch还以其对社区驱动开发的承诺而闻名。它发展迅速，定期根据实际使用和用户反馈进行更新和改进，确保其始终处于人工智能研究和开发的前沿。
- en: Appendix A provides detailed instructions on how to install PyTorch on your
    computer. Follow the instructions to install PyTorch in the virtual environment
    for this book. In case you don’t have a Compute Unified Device Architecture (CUDA)-enabled
    GPU installed on your computer, all programs in this book are compatible with
    CPU training as well. Better yet, I’ll provide the trained models on the book’s
    GitHub repository [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)
    so you can see the trained models in action (in case the trained model is too
    large, I’ll provide them on my personal website [https://gattonweb.uky.edu/faculty/lium/](https://gattonweb.uky.edu/faculty/lium/)).
    In chapter 2, you’ll dive deep into PyTorch. You’ll first learn the data structure
    in PyTorch, Tensor, which holds numbers and matrices and provides functions to
    conduct operations. You’ll then learn to perform an end-to-end deep learning project
    using PyTorch. Specifically, you’ll create a neural network in PyTorch and use
    clothing item images and the corresponding labels to train the network. Once done,
    you use the trained model to classify clothing items into 10 different label types.
    The project will get you ready to use PyTorch to build and train various generative
    models in later chapters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 附录A提供了如何在您的计算机上安装PyTorch的详细说明。按照说明在本书的虚拟环境中安装PyTorch。如果您计算机上没有安装支持Compute Unified
    Device Architecture (CUDA)的GPU，本书中的所有程序也兼容CPU训练。更好的是，我将在本书的GitHub仓库[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)上提供训练好的模型，以便您可以看到训练好的模型在实际中的应用（如果训练模型太大，我将在我的个人网站上提供[https://gattonweb.uky.edu/faculty/lium/](https://gattonweb.uky.edu/faculty/lium/))）。在第二章中，您将深入探索PyTorch。您首先将学习PyTorch中的数据结构，Tensor，它包含数字和矩阵，并提供执行操作的功能。然后，您将学习如何使用PyTorch执行端到端的深度学习项目。具体来说，您将在PyTorch中创建一个神经网络，并使用服装物品图像及其相应的标签来训练网络。完成之后，您将使用训练好的模型将服装物品分类到10种不同的标签类型中。这个项目将使您为在后续章节中使用PyTorch构建和训练各种生成模型做好准备。
- en: 1.2 GANs
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 GANs
- en: This section first provides a high-level overview of how GANs work. We then
    use the generation of anime face images as an example to show you the inner workings
    of GANs. Finally, we’ll discuss the practical uses of GANs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先概述GANs的工作原理。然后，我们以生成动漫人脸图像为例，向您展示GANs的内部工作原理。最后，我们将讨论GANs的实际应用。
- en: 1.2.1 A high-level overview of GANs
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 GANs的高级概述
- en: GANs represent a category of generative models initially proposed by Ian Goodfellow
    and his collaborators in 2014 (“Generative Adversarial Nets,” [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)).
    GANs have become extremely popular in recent years because they are easy to build
    and train, and they can generate a wide variety of content. As you’ll see from
    the illustrating example in the next subsection, GANs employ a dual-network architecture
    comprising a generative model tasked with capturing the underlying data distribution
    to generate content and a discriminative model that serves to estimate the likelihood
    that a given sample originates from the authentic training dataset (considered
    as “real”) rather than being a product of the generative model (considered as
    “fake”). The primary objective of the model is to produce new data instances that
    closely resemble those in the training dataset. The nature of the data generated
    by GANs is contingent upon the composition of the training dataset. For example,
    if the training data consists of grayscale images of clothing items, the synthesized
    images will closely resemble such clothing items. Conversely, if the training
    dataset comprises color images of human faces, the generated images will also
    resemble human faces.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GANs代表一类生成模型，最初由Ian Goodfellow及其合作者于2014年提出（“Generative Adversarial Nets,” [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)）。近年来，GANs因其易于构建和训练，并且能够生成各种内容而变得极为流行。您将从下一小节中的说明示例中看到，GANs采用一个双网络架构，包括一个生成模型，其任务是捕捉潜在的数据分布以生成内容，以及一个判别模型，其作用是估计给定样本是否来自真实的训练数据集（被认为是“真实”）而不是生成模型的产物（被认为是“虚假”）。模型的主要目标是产生与训练数据集中的数据实例非常相似的新数据实例。GANs生成数据的性质取决于训练数据集的组成。例如，如果训练数据由服装物品的灰度图像组成，合成的图像将非常类似于这样的服装物品。相反，如果训练数据集包含人类面部彩色图像，生成的图像也将类似于人类面部。
- en: Take a look at figure 1.2—the architecture of our GAN and its components. To
    train the model, both real samples from the training dataset (as shown at the
    top of figure 1.2) and fake samples created by the generator (left) are presented
    to the discriminator (middle). The principal aim of the generator is to create
    data instances that are virtually indistinguishable from the examples found within
    the training dataset. Conversely, the discriminator strives to distinguish fake
    samples generated by the generator from real samples. These two networks engage
    in a continual competitive process similar to a cat-and-mouse game, trying to
    outperform each other iteratively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下图1.2——我们GAN的架构及其组件。为了训练模型，我们将真实样本（如图1.2顶部所示）和生成器创建的虚假样本（左侧）都展示给判别器（中间）。生成器的主要目的是创建与训练数据集中找到的示例几乎无法区分的数据实例。相反，判别器努力区分生成器生成的虚假样本和真实样本。这两个网络进行着一种持续的竞争过程，类似于猫捉老鼠的游戏，试图迭代地超越对方。
- en: '![](../../OEBPS/Images/CH01_F02_Liu.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F02_Liu.png)'
- en: Figure 1.2 GANs architecture and its components. GANs employ a dual-network
    architecture comprising a generative model (left) tasked with capturing the underlying
    data distribution and a discriminative model (center) that serves to estimate
    the likelihood that a given sample originates from the authentic training dataset
    (considered as “real”) rather than being a product of the generative model (considered
    as “fake”).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 GAN架构及其组件。GAN采用双网络架构，包括一个生成模型（左侧），其任务是捕捉潜在的数据分布，以及一个判别模型（中间），其目的是估计一个给定的样本是来自真实的训练数据集（被认为是“真实”）而不是生成模型的产物（被认为是“虚假”）的可能性。
- en: 'The training process of the GAN model involves multiple iterations. In each
    iteration, the generator takes some form of task description (step 1) and uses
    it to create fake images (step 2). The fake images, along with real images from
    the training set, are presented to the discriminator (step 3). The discriminator
    tries to classify each sample as either real or fake. It then compares the classification
    with the actual labels, the ground truth (step 4). Both the discriminator and
    the generator receive feedback (step 5) from the classification and improve their
    capabilities: while the discriminator adapts its ability to identify fake samples,
    the generator learns to enhance its capacity to generate convincing samples to
    fool the discriminator. As training advances, an equilibrium is reached when neither
    network can further improve. At this point, the generator becomes capable of producing
    data instances that are practically indistinguishable from real samples.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GAN模型的训练过程涉及多个迭代。在每个迭代中，生成器接受某种形式的任务描述（步骤1）并使用它来创建虚假图像（步骤2）。虚假图像以及来自训练集的真实图像被展示给判别器（步骤3）。判别器试图将每个样本分类为真实或虚假。然后，它将分类与实际标签、真实情况（步骤4）进行比较。判别器和生成器都从分类中接收反馈（步骤5），并提高它们的能力：判别器适应其识别虚假样本的能力，而生成器学习提高其生成能够欺骗判别器的令人信服样本的能力。随着训练的进行，当两个网络都无法进一步改进时，达到平衡。此时，生成器能够产生与真实样本几乎无法区分的数据实例。
- en: To understand exactly how GANs work, let’s look at an illustrating example.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确切了解GAN是如何工作的，让我们来看一个说明性例子。
- en: '1.2.2 An illustrating example: Generating anime faces'
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 一个说明性例子：生成动漫面孔
- en: 'Picture this: you’re a passionate anime enthusiast, and you’re on a thrilling
    quest to create your very own anime faces using a powerful tool known as a deep
    convolutional GAN (or DCGAN for short; don’t worry, we’ll dive deeper into this
    in chapter 4).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：你是一个热衷的动漫爱好者，你正在使用一种称为深度卷积GAN（简称DCGAN）的强大工具进行一次激动人心的探索，以创建你自己的动漫面孔。别担心，我们将在第4章中深入探讨这一点。
- en: If you look at the top middle of figure 1.2, you’ll spot a picture that reads
    “Real Image.” We’ll use 63,632 colorful images of anime faces as our training
    dataset. And if you flip to figure 1.3, you’ll see 32 examples from our training
    set. These special images play a crucial role as they form half of the inputs
    to our discriminator network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察图1.2的顶部中间部分，你会看到一个写着“实像”的图片。我们将使用63,632张动漫面孔的彩色图片作为我们的训练数据集。如果你翻到图1.3，你会看到我们训练集中的32个示例。这些特殊图片在形成我们判别网络一半输入方面起着至关重要的作用。
- en: '![](../../OEBPS/Images/CH01_F03_Liu.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F03_Liu.png)'
- en: Figure 1.3 Examples from the anime faces training dataset
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 动漫面孔训练数据集示例
- en: The left of figure 1.2 is the generator network. To generate different images
    every time, the generator takes as input a vector Z from the latent space. We
    could think of this vector as a “task description.” During training, we draw different
    Z vectors from the latent space, so the network generates different images every
    time. These fake images are the other half of the inputs to the discriminator
    network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: NOTE By altering the values in the vector Z, we generate different outputs.
    In chapter 5, you’ll learn how to select the vector Z to generate images with
    certain characteristics (e.g., male or female features).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'But here’s the twist: before we teach our two networks the art of creation
    and detection, the images produced by the generator are, well, gibberish! They
    look nothing like the realistic anime faces you see in figure 1.3\. In fact, they
    resemble nothing more than static on a TV screen (you’ll witness this firsthand
    in chapter 4).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: We train the model for multiple iterations. In each iteration, we present a
    group of images created by the generator, along with a group of anime face images
    from our training set to the discriminator. We ask the discriminator to predict
    whether each image is created by the generator (fake) or from the training set
    (real).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'You may wonder: How do the discriminator and the generator learn during each
    iteration of training? Once the predictions are made, the discriminator doesn’t
    just sit back; it learns from its prediction blunders for each image. With this
    newfound knowledge, it fine-tunes its parameters, shaping itself to make better
    predictions in the next round. The generator isn’t idle either. It takes notes
    from its image generation process and the discriminator’s prediction outcomes.
    With that knowledge in hand, it adjusts its own network parameters, striving to
    create increasingly lifelike images in the next iteration. The goal? To reduce
    the odds of the discriminator sniffing out its fakes.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: As we journey through these iterations, a remarkable transformation takes place.
    The generator network evolves, producing anime faces that grow more and more realistic,
    akin to those in our training collection. Meanwhile, the discriminator network
    hones its skills, becoming a seasoned detective when it comes to spotting fakes.
    It’s a captivating dance between creation and detection.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Gradually, a magical moment arrives. An equilibrium, or perfect balance, is
    achieved. The images created by the generator become so astonishingly real that
    they are indistinguishable from the genuine anime faces in our training archives.
    At this point, the discriminator is so confused that it assigns a 50% chance of
    authenticity to every image, whether it’s from our training set or was crafted
    by the generator.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, behold some examples of the artwork of the generator, as shown in
    figure 1.4: they do look indistinguishable from those in our training set.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F04_Liu.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 Generated anime face images by the trained generator in DCGAN
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 Why should you care about GANs?
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GANs are easy to implement and versatile: you’ll learn to generate geometric
    shapes, intricate patterns, high-resolution images, and realistic-sounding music
    in this book alone.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The practical use of GANs doesn’t stop at generating realistic data. GANs can
    also translate attributes in one image domain to another. As you’ll see in chapter
    6, you can train a CycleGAN (a type of generative model in the GAN family) to
    convert blond hair to black hair in human face images. The same trained model
    can also convert black hair to blond hair. Figure 1.5 shows four rows of images.
    The first row is the original images with blond hair. The trained CycleGAN converts
    them to images with black hair (second row). The last two rows are the original
    images with black hair and the converted image with blond hair, respectively.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F05_Liu.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Changing hair color with CycleGAN. If we feed images with blond hair
    (first row) to a trained CycleGAN model, the model converts blond hair to black
    hair in these images (second row). The same trained model can also convert black
    hair (third row) to blond hair (bottom row).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about all the amazing skills you’ll pick up from training GANs—they’re
    not just cool; they’re super practical too! Let’s say you run an online clothing
    store with a “Make to Order” strategy (which allows users to customize their purchases
    before manufacturing). Your website showcases tons of unique designs for customers
    to pick from, but here’s the catch: you only make the clothes once someone places
    an order. Creating high-quality images of these clothes can be quite expensive
    since you have to produce the items and then photograph them.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: GANs to the rescue! You don’t need a massive collection of manufactured clothing
    items and their images; instead, you can use something like CycleGAN to transform
    features from one set of images into another, creating a whole new array of styles.
    This is just one nifty way to use GANs. The possibilities are endless because
    these models are super versatile and can handle all sorts of data—making them
    a game-changer for practical applications.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Transformers
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transformers are deep neural networks that excel at sequence-to-sequence prediction
    problems, such as taking an input sentence and predicting the most likely next
    words. This section introduces you to the key innovation in Transformers: the
    self-attention mechanism. We’ll then discuss the Transformer architecture and
    different types of Transformers. Finally, we’ll discuss some recent developments
    in Transformers, such as multimodal models (Transformers whose inputs include
    not only text but also other data types such as audio and images) and pretrained
    LLMs (models trained on large textual data that can perform various downstream
    tasks).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Before the Transformer architecture was invented in 2017 by a group of Google
    researchers (Vaswani et al., “Attention Is All You Need,” [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    natural language processing (NLP) and other sequence-to-sequence prediction tasks
    were primarily handled by RNNs. However, RNNs struggle with retaining information
    about earlier elements in a sequence, which hampers their ability to capture long-term
    dependencies. Even advanced RNN variants like long short-term memory (LSTM) networks,
    which can handle longer-range dependencies, fall short when it comes to extremely
    long-range dependencies.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, RNNs (including LSTMs) process inputs sequentially, which
    means these models process one element at a time, in sequence, instead of looking
    at the entire sequence simultaneously. The fact that RNNs conduct computation
    along the symbol positions of the input and output sequences prevents parallel
    training, which makes training slow. This, in turn, makes it impossible to train
    the models on huge datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The key innovation of Transformers is the self-attention mechanism, which excels
    at capturing long-term dependencies in a sequence. Further, since the inputs are
    not handled sequentially in the model, Transformers can be trained in parallel,
    which greatly reduces the training time. More importantly, parallel training makes
    it possible to train Transformers on large amounts of data, which makes LLMs intelligent
    and knowledgeable (based on their ability to process and generate human-like text,
    understand context, and perform a variety of language tasks). This has led to
    the rise of LLMs such as ChatGPT and the recent AI boom.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 The attention mechanism
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attention mechanism assigns weights on how an element is related to all
    elements in a sequence (including the element itself). The higher the weight,
    the more closely the two elements are related. These weights are learned from
    large sets of training data in the training process. Therefore, a trained LLM
    such as ChatGPT can figure out the relationship between any two words in a sentence,
    hence making sense of the human language.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'You may wonder: How does the attention mechanism assign scores to elements
    in a sequence to capture the long-term dependencies? The attention weights are
    calculated by first passing the inputs through three neural network layers to
    obtain query Q, key K, and value V (which we’ll explain in detail in chapter 9).
    The method of using query, key, and value to calculate attention comes from retrieval
    systems. For example, you may go to a public library to search for a book. You
    can type in, say, “machine learning in finance” in the library’s search engine.
    In this case, the query Q is “machine learning in finance.” The keys K are the
    book titles, book descriptions, and so on. The library’s retrieval system will
    recommend a list of books (values V) based on the similarities between the query
    and the keys. Naturally, books with the phrases “machine learning” or “finance”
    or both in titles or descriptions come up on top while books with neither phrase
    in the title or description will show up at the bottom of the list because these
    books will be assigned a low matching score.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In chapters 9 and 10, you’ll learn the details of the attention mechanism—better
    yet, you’ll implement the attention mechanism from scratch to build and train
    a Transformer to successfully translate English to French.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 The Transformer architecture
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers were first proposed when designing models for machine language
    translation (e.g., English to German or English to French). Figure 1.6 is a diagram
    of the Transformer architecture. The left side is the encoder, and the right side
    is the decoder. In chapters 9 and 10, you’ll learn to construct a Transformer
    from scratch to train the model to translate English to French, and we’ll explain
    figure 1.6 in greater detail then.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The encoder in the Transformer “learns” the meaning of the input sequence (e.g.,
    the English phrase “How are you?”) and converts it into vectors that represent
    this meaning before passing the vectors to the decoder. The decoder constructs
    the output (e.g., the French translation of an English phrase) by predicting one
    word at a time, based on previous words in the sequence and the output from the
    encoder. The trained model can translate common English phrases into French.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of Transformers: encoder-only Transformers, decoder-only
    Transformers, and encoder-decoder Transformers. An encoder-only Transformer has
    no decoder and is capable of converting a sequence into an abstract representation
    for various downstream tasks such as sentiment analysis, named entity recognition,
    and text generation. For example, BERT is an encoder-only Transformer. A decoder-only
    Transformer has only a decoder but no encoder, and it’s well suited for text generation,
    language modeling, and creative writing. GPT-2 (the predecessor of ChatGPT) and
    ChatGPT are both decoder-only Transformers. In chapter 11, you’ll learn to create
    GPT-2 from scratch and then extract the trained model weights from Hugging Face
    (an AI community that hosts and collaborates on ML models, datasets, and applications).
    You’ll load the weights to your GPT-2 model and start generating coherent text.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F06_Liu.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 The Transformer architecture. The encoder in the Transformer (left
    side of the diagram) learns the meaning of the input sequence (e.g., the English
    phrase “How are you?”) and converts it into an abstract representation that captures
    its meaning before passing it to the decoder (right side of the diagram). The
    decoder constructs the output (e.g., the French translation of the English phrase)
    by predicting one word at a time, based on previous words in the sequence and
    the abstract representation from the encoder.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder Transformers are needed for complicated tasks such as multimodal
    models that can handle text-to-image generation or speech recognition. Encoder-decoder
    Transformers combine the strengths of both encoders and decoders. Encoders are
    efficient in processing and understanding input data, while decoders excel in
    generating output. This combination allows the model to effectively understand
    complex inputs (like text or speech) and generate intricate outputs (like images
    or transcribed text).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3 Multimodal Transformers and pretrained LLMs
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recent developments in generative AI give rise to various multimodal models:
    Transformers that can use not only text but also other data types, such as audio
    and images, as inputs. Text-to-image Transformers are one such example. DALL-E
    2, Imagen, and Stable Diffusion are all text-to-image models, and they have garnered
    much media attention due to their ability to generate high-resolution images from
    textual prompts. Text-to-image Transformers incorporate the principles of diffusion
    models, which involve a series of transformations to gradually increase the complexity
    of data. Therefore, we first need to understand diffusion models before we discuss
    text-to-image Transformers.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you want to generate high-resolution flower images by using a diffusion-based
    model. You’ll first obtain a training set of high-quality flower images. You then
    ask the model to gradually add noise to the flower images (the so-called diffusion
    process) until they become completely random noise. You then train the model to
    progressively remove noise from these noisy images to generate new data samples.
    The diffusion process is illustrated in figure 1.7\. The left column contains
    four original flower images. As we move to the right, some noise is added to the
    images in each step, until at the right column, the four images are pure random
    noise.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F07_Liu.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 The diffusion model adds more and more noise to the images and learns
    to reconstruct them. The left column contains four original flower images. As
    we move to the right, some noise is added to the images in each time step, until
    at the right column, the four images are pure random noise. We then use these
    images to train a diffusion-based model to progressively remove noise from noisy
    images to generate new data samples.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be wondering: How are text-to-image Transformers related to diffusion
    models? Text-to-image Transformers take a text prompt as input and generate images
    that correspond to that textual description. The text prompt serves as a form
    of conditioning, and the model uses a series of neural network layers to transform
    that textual description into an image. Like diffusion models, text-to-image Transformers
    use a hierarchical architecture with multiple layers, each progressively adding
    more detail to the generated image. The core concept of iteratively refining the
    output is similar in both diffusion models and text-to-image Transformers, as
    we’ll explain in chapter 15.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models have now become more popular due to their ability to provide
    stable training and generate high-quality images, and they have outperformed other
    generative models such as GANs and variational autoencoders. In chapter 15, you’ll
    first learn to train a simple diffusion model using the Oxford Flower dataset.
    You’ll also learn the basic idea behind multimodal Transformers and write a Python
    program to ask OpenAI’s DALL-E 2 to generate images through a text prompt. For
    example, when I entered “an astronaut in a space suit riding a unicorn” as the
    prompt, DALL-E 2 generated the image shown in figure 1.8\.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH01_F08_Liu.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 Image generated by DALL-E 2 with text prompt “an astronaut in a space
    suit riding a unicorn”
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 16, you’ll learn how to access pretrained LLMs such as ChatGPT, GPT4,
    and DALL-E 2\. These models are trained on large textual data and have learned
    general knowledge from the data. Hence, they can perform various downstream tasks
    such as text generation, sentiment analysis, question answering, and named entity
    recognition. Since pretrained LLMs were trained on information a few months ago,
    they cannot provide information on events and developments in the last one or
    two months, let alone real-time information such as weather conditions, flight
    status, or stock prices. We’ll use LangChain (a Python library designed for building
    applications with LLMs, providing tools for prompt management, LLM chaining, and
    output parsing) to chain together LLMs with the Wolfram Alpha and Wikipedia APIs
    to create a know-it-all personal assistant.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Why build generative models from scratch?
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of this book is to show you how to build and train all generative
    models from scratch. This way, you’ll have a thorough understanding of the inner
    workings of these models and can make better use of them. Creating something from
    scratch is the best way to understand it. You’ll accomplish this goal for GANs:
    all models, including DCGAN and CycleGAN, are built from the ground up and trained
    using well-curated data in the public domain.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: For Transformers, you’ll build and train all models from scratch except for
    LLMs. This exception is due to the vast amount of data and the supercomputing
    facilities needed to train certain LLMs. However, you’ll make serious progress
    in this direction. Specifically, you’ll implement in chapters 9 and 10 the original
    groundbreaking 2017 paper “Attention Is All You Need” line by line with English-to-French
    translation as an example (the same Transformer can be trained on other datasets
    such as Chinese to English or English to German translations). You’ll also build
    a small-size decoder-only Transformer and train it using several of Ernest Hemingway’s
    novels, including *The Old Man and the Sea*. The trained model can generate text
    in Hemingway style. ChatGPT and GPT-4 are too large and complicated to build and
    train from scratch for our purposes, but you’ll peek into their predecessor, GPT-2,
    and learn to build it from scratch. You’ll also extract the trained weights from
    Hugging Face and load them up to the GPT-2 model you built and start to generate
    realistic text that can pass as human-written.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: In this sense, the book is taking a more fundamental approach than most books.
    Instead of treating generative AI models as a black box, readers have a chance
    to look under the hood and examine in detail the inner workings of these models.
    The goal is for you to have a deeper understanding of generative models. This,
    in turn, can potentially help you build better and more responsible generative
    AI for the following reasons.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: First, having a deep understanding of the architecture of generative models
    helps readers make better practical uses of these models. For example, in chapter
    5, you’ll learn how to select characteristics in generated images such as male
    or female features and with or without eyeglasses. By building a conditional GAN
    from the ground up, you understand that certain features of the generated images
    are determined by the random noise vector, Z, in the latent space. Therefore,
    you can choose different values of Z as inputs to the trained model to generate
    the desired characteristics (such as male or female features). This type of attribute
    selection is hard to do without understanding the design of the model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: For Transformers, knowing the architecture (and what encoders and decoders do)
    gives you the ability to create and train Transformers to generate the types of
    content you are interested in (say, Jane Austin–style novels or Mozart-style music).
    This understanding also helps you with pretrained LLMs. For example, while it
    is hard to train GPT-2 from scratch with its 1.5 billion parameters, you can add
    an additional layer to the model and fine-tune it for other downstream tasks such
    as text classification, sentiment analysis, and question-answering.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, a deep understanding of generative AI helps readers have an unbiased
    assessment of the dangers of AI. While the extraordinary powers of generative
    AI have benefitted us in our daily lives and work, it also has the potential to
    create great harm. Elon Musk went so far as saying that “there’s some chance that
    it goes wrong and destroys humanity” (see the article by Julia Mueller in *The
    Hill*, 2023, “Musk: There’s a Chance AI ‘Goes Wrong and Destroys Humanity,’” [https://mng.bz/Aaxz](https://mng.bz/Aaxz)).
    More and more people in academics and in the tech industry are worried about the
    dangers posed by AI in general and generative AI in particular. Generative AI,
    especially LLMs, can lead to unintended consequences, as many pioneers in the
    tech profession have warned (see, e.g., Stuart Russell, 2023, “How to Stop Runaway
    AI,” [https://mng.bz/ZVzP](https://mng.bz/ZVzP)). It’s not a coincidence that
    merely five months after the release of ChatGPT, many tech industry experts and
    entrepreneurs, including Steve Wozniak, Tristan Harris, Yoshua Bengio, and Sam
    Altman, signed an open letter calling for a pause in training any AI system that’s
    more powerful than GPT-4 for at least six months (see the article by Connie Loizos
    in *TechCrunch*, “1,100+ Notable Signatories Just Signed an Open Letter Asking
    ‘All AI Labs to Immediately Pause for at Least 6 Months,’” [https://mng.bz/RNEK](https://mng.bz/RNEK)).
    A thorough understanding of the architecture of generative models helps us provide
    a deep and unbiased evaluation of the benefits and potential dangers of AI.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI is a type of technology with the capacity to produce diverse forms
    of new content, including texts, images, code, music, audio, and video.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminative models specialize in assigning labels while generative models
    generate new instances of data.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch, with its dynamic computational graphs and the ability for GPU training,
    is well suited for deep learning and generative modeling.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GANs are a type of generative modeling method consisting of two neural networks:
    a generator and a discriminator. The goal of the generator is to create realistic
    data samples to maximize the chance that the discriminator thinks they are real.
    The goal of the discriminator is to correctly identify fake samples from real
    ones.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers are deep neural networks that use the attention mechanism to identify
    long-term dependencies among elements in a sequence. The original Transformer
    has an encoder and a decoder. When it’s used for English-to-French translation,
    for example, the encoder converts the English sentence into an abstract representation
    before passing it to the decoder. The decoder generates the French translation
    one word at a time, based on the encoder’s output and the previously generated
    words.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
