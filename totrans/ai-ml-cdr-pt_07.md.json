["```py\nchristian bale given neutered male statuette named oscar\n```", "```py\n{ \"christian\" : 1, \"bale\" : 1, \"given\" : 1, \"neutered\": 1, \"male\" : 1, \n  \"statuette\": 1, \"named\" : 1, \"oscar\": 1}\n```", "```py\ngareth bale scores wonder goal against germany\n```", "```py\n{ \"christian\" : 1, \"bale\" : 0, \"given\" : 1, \"neutered\": 1, \"male\" : 1,\n  \"statuette\": 1, \"named\" : 1, \"oscar\": 1, \"gareth\" : –1, \"scores\": –1,\n  \"wonder\" : –1, \"goal\" : –1, \"against\" : –1, \"germany\" : –1}\n```", "```py\nneutered male named against germany, wins statuette!\n```", "```py\ntraining_size = 28000\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]\n```", "```py\nnn.Embedding(vocab_size, embedding_dim)\n```", "```py\nself.embedding = nn.Embedding(vocab_size, embedding_dim)\nself.global_pool = nn.AdaptiveAvgPool1d(1)\nself.fc1 = nn.Linear(embedding_dim, 24)\nself.fc2 = nn.Linear(24, 1)\nself.relu = nn.ReLU()\nself.sigmoid = nn.Sigmoid()\n```", "```py\n==========================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================\nTextClassificationModel                  [32, 1]                   --\n├─Embedding: 1-1                         [32, 100, 100]            2,429,200\n├─AdaptiveAvgPool1d: 1-2                 [32, 100, 1]              --\n├─Linear: 1-3                            [32, 24]                  2,424\n├─ReLU: 1-4                              [32, 24]                  --\n├─Linear: 1-5                            [32, 1]                   25\n├─Sigmoid: 1-6                           [32, 1]                   --\n==========================================================================\nTotal params: 2,431,649\nTrainable params: 2,431,649\nNon-trainable params: 0\nTotal mult-adds (M): 77.81\n==========================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.57\nParams size (MB): 9.73\nEstimated Total Size (MB): 12.32\n==========================================================================\n```", "```py\noptimizer = optim.Adam(model.parameters(), lr=0.001, \n                       betas=(0.9, 0.999), amsgrad=False)\n```", "```py\noptimizer = optim.Adam(model.parameters(), `lr``=``0.0001`, \n                       betas=(0.9, 0.999), amsgrad=False)\n```", "```py\ndef word_frequency(sentences, word_dict):\n    frequency = {word: 0 for word in word_dict}\n\n    for sentence in sentences:\n        words = sentence.lower().split()\n        for word in words:\n            if word in frequency:\n                frequency[word] += 1\n\n    return frequency\n```", "```py\nword_freq = word_frequency(training_sentences, word_index)\nprint(word_freq)\n```", "```py\n{'new': 1318, 'trump': 1117, 'man': 1075, 'not': 634, 'just': 501, \n 'will': 484, 'one': 469, 'year': 440, …\n```", "```py\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nnewlist = (OrderedDict(sorted(word_freq.items(), key=lambda t: t[1], \n                       reverse=True)))\n\nxs=[]\nys=[]\ncurr_x = 1\nfor item in newlist:\n  xs.append(curr_x)\n  curr_x=curr_x+1\n  ys.append(newlist[item])\n\nprint(ys)\nplt.plot(xs,ys)\n\n```", "```py\nplt.plot(xs,ys)\nplt.axis([300,10000,0,100])\nplt.show()\n```", "```py\ndef build_vocab(sentences, max_vocab_size=10000):\n    counter = Counter()\n    for text in sentences:\n        counter.update(tokenize(text))\n\n# Take only the top max_vocab_size-1 most frequent words \n# (leave room for special tokens)\n    most_common = counter.most_common(max_vocab_size – 2)  \n    # -2 for <pad> and <unk>\n\n    # Create vocabulary with indices starting from 2\n    vocab = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}\n    vocab['<pad>'] = 0  # Add padding token\n    vocab['<unk>'] = 1  # Add unknown token\n    return vocab\n```", "```py\nvocab_size = 2000\nword_index = build_vocab(training_sentences, max_vocab_size=vocab_size)\n```", "```py\n\n==========================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================\nTextClassificationModel                  [32, 1]                   --\n├─Embedding: 1-1                         [32, 100, 100]            200,100\n├─AdaptiveAvgPool1d: 1-2                 [32, 100, 1]              --\n├─Linear: 1-3                            [32, 24]                  2,424\n├─ReLU: 1-4                              [32, 24]                  --\n├─Linear: 1-5                            [32, 1]                   25\n├─Sigmoid: 1-6                           [32, 1]                   --\n==========================================================================\nTotal params: 202,549\nTrainable params: 202,549\nNon-trainable params: 0\nTotal mult-adds (M): 6.48\n==========================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.57\nParams size (MB): 0.81\nEstimated Total Size (MB): 3.40\n==========================================================================\n\n```", "```py\nclass TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim=24):\n        super(TextClassificationModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n```", "```py\nclass TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim=8, \n                       `dropout_rate``=``0.25``)``:`\n        super(TextClassificationModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n        self.dropout = nn.Dropout(p=dropout_rate)  # Add dropout layer\n        self.fc2 = nn.Linear(hidden_dim, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.transpose(1, 2)  # Change for pooling layer\n        x = self.global_pool(x).squeeze(2)\n        `x` `=` `self``.``dropout`(self.relu(self.fc1(x)))\n        x = self.sigmoid(self.fc2(x))\n        return x\n\n```", "```py\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), \n                       amsgrad=False, `weight_decay``=``0.01`)\n```", "```py\n# Different weight decay for different layers\noptimizer = torch.optim.Adam([\n# L2 reg on fc1\n        {'params': model.fc1.parameters(), 'weight_decay': 0.01},      \n    # No L2 reg on other layers\n{'params': [p for name, p in model.named_parameters() \n            if 'fc1' not in name]}  \n], lr=0.0001)\n```", "```py\nxs=[]\nys=[]\ncurrent_item=1\nfor item in sentences:\n  xs.append(current_item)\n  current_item=current_item+1\n  ys.append(len(item))\nnewys = sorted(ys)\n\nimport matplotlib.pyplot as plt\nplt.plot(xs,newys)\nplt.show()\n```", "```py\ntest_sentences = [\n             \"granny starting to fear spiders in the garden might be real\", \n             \"game of thrones season finale showing this sunday night\", \n             \"PyTorch book will be a best seller\"]\n```", "```py\nprint(texts_to_sequences(test_sentences, word_index))\n\n```", "```py\n[\n[1, 803, 753, 1, 1, 312, 97], \n[123, 1183, 160, 1, 1, 1543, 152], \n[1, 235, 7, 47, 1]\n]\n```", "```py\npadded = pad_sequences(sequences, max_len)\n```", "```py\n[1, 803, 753, 1, 1, 312, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n 0, 0, 0, 0, 0, 0, 0]\n```", "```py\n# Convert to tensor\ninput_ids = torch.tensor(padded, dtype=torch.long).to(device)\n```", "```py\n# Get predictions\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(input_ids)\n```", "```py\ntensor([[0.5516],\n        [0.0765],\n        [0.0987]], device='cuda:0')\n```", "```py\nprobabilities = outputs.squeeze().cpu().numpy()\npredictions = (probabilities >= threshold).astype(int)\n```", "```py\nText: granny starting to fear spiders in the garden might be real\nProbability: 0.5516\nClassification: `Sarcastic`\nConfidence: 0.5516\n--------------------------------------------------------------------------\n\nText: game of thrones season finale showing this sunday night\nProbability: 0.0765\nClassification: Not Sarcastic\nConfidence: 0.9235\n--------------------------------------------------------------------------\n\nText: PyTorch book will be a best seller\nProbability: 0.0987\nClassification: `Not` `Sarcastic`\nConfidence: 0.9013\n--------------------------------------------------------------------------\n```", "```py\nreverse_word_index = dict([(value, key)\nfor (key, value) in word_index.items()])\n\n```", "```py\nembedding_weights = model.embedding.weight.data.cpu().numpy()\nprint(embedding_weights.shape)\n\n```", "```py\nprint(reverse_word_index[2])\nprint(embedding_weights[2])\n\n```", "```py\nnew\n[–0.27116913 –1.3026129   1.6390767   0.4922502  –0.6025921   1.4584142\n  0.05054485]\n\n```", "```py\nimport io\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\nfor word_num in range(1, vocab_size):\n  word = reverse_word_index[word_num]\n  embeddings = embedding_weights[word_num]\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()\n\n```", "```py\ntry:\n  from google.colab import files\nexcept ImportError:\n  pass\nelse:\n  files.download('vecs.tsv')\n  files.download('meta.tsv')\n\n```", "```py\n# Initialize embedding layer\nself.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n# Load pretrained embeddings if provided\nif pretrained_embeddings is not None:\n    self.embedding.weight.data.copy_(pretrained_embeddings)\n    if freeze_embeddings:\n        self.embedding.weight.requires_grad = False\n```"]