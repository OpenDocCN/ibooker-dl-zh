["```py\n# Install or Upgrade Scikit-learn\n%pip install –-upgrade scikit-learn\n\n# Import TFIDFRetriever class from retrievers library\nfrom langchain_community.retrievers import TFIDFRetriever\n\n# Create an instance of the TFIDFRetriever with texts\nretriever = TFIDFRetriever.from_texts(\n[\"Australia won the Cricket World Cup 2023\",\n \"India and Australia played in the finals\",\n \"Australia won the sixth time having last won in 2015\"]\n)\n\n# Use the retriever using the invoke method\nresult=retriever.invoke(\"won\")\n\n# Print the results\nprint(result)\n```", "```py\n# Install or Upgrade rank_bm25\n%pip install –-upgrade rank_bm25\n\n# Import BM25Retriever class from retrievers library\nfrom langchain_community.retrievers import BM25Retriever\n\n# Create an instance of the TFIDFRetriever with texts\nretriever = BM25Retriever.from_texts(\n[\"Australia won the Cricket World Cup 2023\",\n \"India and Australia played in the finals\",\n \"Australia won the sixth time having last won in 2015\"]\n)\n\n# Use the retriever using the invoke method\nresult=retriever.invoke(\"Who won the 2023 Cricket World Cup?\")\n\n# Print the results\nprint(result)\n```", "```py\n# Install the langchain openai library\n%pip install langchain-openai==0.3.7\n# Import FAISS class from vectorstore library\nfrom langchain_community.vectorstores import FAISS\n\n# Import OpenAIEmbeddings from the library\nfrom langchain_openai import OpenAIEmbeddings\n\n# Set the OPENAI_API_KEY as the environment variable\nimport os\nos.environ[\"OPENAI_API_KEY\"] = <YOUR_API_KEY>\n\n# Instantiate the embeddings object\nembeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Load the database stored in the local directory\nvector_store=FAISS.load_local(\nfolder_path=\"../../Assets/Data\", \nindex_name=\"CWC_index\",\nembeddings=embeddings, \nallow_dangerous_deserialization=True\n)\n\n# Original Question\nquery = \"Who won the 2023 Cricket World Cup?\"\n\n# Ranking the chunks in descending order of similarity\nretrieved_docs = vector_store.similarity_search(query, k=2)\n```", "```py\n# Import FAISS class from vectorstore library\nfrom langchain_community.vectorstores import FAISS\n\n# Import OpenAIEmbeddings from the library\nfrom langchain_openai import OpenAIEmbeddings\n\n# Set the OPENAI_API_KEY as the environment variable\nimport os\nos.environ[\"OPENAI_API_KEY\"] = <YOUR_API_KEY>\n\n# Instantiate the embeddings object\nembeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Load the database stored in the local directory\nvector_store=FAISS.load_local(\nfolder_path=\"../../Assets/Data\", \nindex_name=\"CWC_index\",\nembeddings=embeddings, \nallow_dangerous_deserialization=True\n)\n\n# Original Question\nquery = \"Who won the 2023 Cricket World Cup?\"\n\n# Ranking the chunks in descending order of similarity\nretrieved_docs = vector_store.similarity_search(query, k=2)\n\n# Selecting the first chunk as the retrieved information\nretrieved_context= retrieved_docs[0].page_content\n\n# Creating the prompt\naugmented_prompt=f\"\"\"\n\nGiven the context below, answer the question.\n\nQuestion: {query} \n\nContext : {retrieved_context}\n\nRemember to answer only based on the context provided and not from any other source. \n\nIf the question cannot be answered based on the provided context, say I don't know.\n\n\"\"\"\n```", "```py\n# Import FAISS class from vectorstore library\nfrom langchain_community.vectorstores import FAISS\n\n# Import OpenAIEmbeddings from the library\nfrom langchain_openai import OpenAIEmbeddings\n\n# Set the OPENAI_API_KEY as the environment variable\nimport os\nos.environ[\"OPENAI_API_KEY\"] = <YOUR_API_KEY>\n\n# Instantiate the embeddings object\nembeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Load the database stored in the local directory\nvector_store=FAISS.load_local(\n    folder_path=\"../../Assets/Data\", \n    index_name=\"CWC_index\",\n    embeddings=embeddings, \n    allow_dangerous_deserialization=True\n    )\n\n# Original Question\nquery = \"Who won the 2023 Cricket World Cup?\"\n\n# Ranking the chunks in descending order of similarity\nretrieved_docs = vector_store.similarity_search(query, k=2)\n\n# Selecting the first chunk as the retrieved information\nretrieved_context= retrieved_docs[0].page_content\n\n# Creating the prompt\naugmented_prompt=f\"\"\"\n\nGiven the context below, answer the question.\n\nQuestion: {query} \n\nContext : {retrieved_context}\n\nRemember to answer only based on the context provided and not from any other source. \n\nIf the question cannot be answered based on the provided context, say I don't know.\n\n\"\"\"\n# Importing the OpenAI library from langchain\nfrom langchain_openai import ChatOpenAI\n\n# Instantiate the OpenAI LLM\nllm = ChatOpenAI(\n            model=\"gpt-4o-mini\",\n            temperature=0,\n            max_tokens=None,\n            timeout=None,\n            max_retries=2\n)\n# Make the API call passing the augmented prompt to the LLM\nresponse = llm.invoke (\n     [(\"human\",augmented_prompt)]\n    )\n\n# Extract the answer from the response object\nanswer=response.content\n\nprint(answer)\n```"]