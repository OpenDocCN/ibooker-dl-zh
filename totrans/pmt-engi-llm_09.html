<html><head></head><body><div id="book-content" class="calibre2"><div id="sbo-rt-content" class="calibre3"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Taming the Model" class="calibre6"><div class="preface" id="ch07_taming_the_model_1728407187651669">
<h1 class="calibre5"><span class="firstname">Chapter 7. </span>Taming the Model</h1>

<p class="subtitle">In the previous chapter, you managed to distill all your context into a single, coherent prompt. Now, it’s time for the LLM to do its thing and for you to make sure that it all goes smoothly.</p>

<p class="subtitle">In this chapter, we’re going to start by talking about completion formats and making sure your completions stop when they’re supposed to, as well as how to interpret them using so-called <em class="hyperlink">logprob tricks</em>.</p>

<p class="subtitle">Then, we’re going to take a step back so you can ask yourself which model you’re going to choose to invoke: a professional commercial service, an open source alternative, or even your own bespoke fine-tuned model. Time to get into it.</p>

<section data-type="sect1" data-pdf-bookmark="Anatomy of the Ideal Completion" class="calibre6"><div class="preface" id="ch07_anatomy_of_the_ideal_completion_1728407187651908">
<h1 class="calibre5">Anatomy of the Ideal Completion</h1>

<p class="subtitle">In<a contenteditable="false" data-primary="completions" data-secondary="anatomy of ideal completions" data-type="indexterm" id="Cideal07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> this section, we’ll examine how completions appear, whether they’re classic completions or chat responses. More importantly, we’ll discuss how you want them to look to ensure clear and effective solutions, all while avoiding issues like unnecessary delays or confusing details. As we did in <a data-type="xref" href="ch06.html#ch06a_assembling_the_prompt_1728442733857948" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 6</a> with prompts, we’ll break down the components of an LLM completion and go through them one by one (see <a data-type="xref" href="#ch07_figure_1_1728407187627920" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-1</a>).</p>

<figure class="calibre22"><div id="ch07_figure_1_1728407187627920" class="figure"><img src="assets/pefl_0701.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 7-1. </span>An LLM completion</h6>
</div></figure>

<section data-type="sect2" data-pdf-bookmark="The Preamble" class="calibre6"><div class="preface" id="ch07_the_preamble_1728407187651988">
<h2 class="calibre19">The Preamble</h2>

<p class="subtitle">In the context of completions, the<a contenteditable="false" data-primary="preamble (completion element)" data-type="indexterm" id="preamble07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">preamble</em> is the initial part of the generated text that sets the stage for the main content. Sometimes, this is helpful, and sometimes, it leads to completions that start with uninteresting or useless detail before they produce a solution to the problem you posed. This is often annoying, and it’s costly too: generating tokens<a contenteditable="false" data-primary="latency" data-secondary="acceptable" data-type="indexterm" id="id805" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> costs time (latency) and compute (resources and money). So, producing text that you’re not going to use is wasteful, but sometimes, it’s desirable. We know, it’s confusing, but stay with us here.</p>

<p class="subtitle">Whether it really <em class="hyperlink">is</em> wasteful or whether you can avoid it depends on the exact type of preamble. There are three different types of preambles, and we’ll explore what each of them:</p>

<dl class="stafflist">
	<dt class="calibre13">Structural boilerplate</dt>
	<dd class="calibre14">
	<p class="subtitle">This<a contenteditable="false" data-primary="structural boilerplate preambles" data-type="indexterm" id="id806" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> is the text between the end of a prompt and the start of a completion. When using a completion model, you might be able to eliminate this preamble type, but it’s more efficient to include deterministic boilerplate in the prompt rather than the completion, thus ensuring that the model adheres to the desired format and making the process faster and cheaper. Structural boilerplate makes for a good transition from the prompt to the completion.</p>
	</dd>
	<dt class="calibre13">Reasoning</dt>
	<dd class="calibre14">
	<p class="subtitle">Toward<a contenteditable="false" data-primary="reasoning preambles" data-type="indexterm" id="id807" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the end of 2023, ChatGPT started mirroring a slightly interpreted version of questions to clarify understanding and highlight potential misunderstandings. This approach helps the model make better inferences by focusing on key aspects of the prompt and ensures more accurate responses. Additionally, chain-of-thought prompting, as discussed in <a data-type="xref" href="ch04.html#ch04_designing_llm_applications_1728407230643376" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 4</a>, helps the model break down problems into manageable pieces, with the detailed process often being part of the preamble rather than the main answer. If you’re doing<a contenteditable="false" data-primary="chain-of-thought prompting" data-secondary="length of preambles for" data-type="indexterm" id="id808" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> chain-of-thought prompting, having a long preamble is a virtue, not a vice, even if it’s significantly longer than the actual answer (see the example in <a data-type="xref" href="#ch07_figure_2_1728407187627952" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-2</a>). Also note that in the figure, the <a href="https://oreil.ly/b6T45" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">answer arrived at after a long preamble</a> is correct, while the <a href="https://oreil.ly/X60zf" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">answer arrived at after a short preamble</a> is not. Many of the advanced prompting techniques discussed in <a data-type="xref" href="ch08.html#ch08_01_conversational_agency_1728429579285372" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 8</a> will center on making good use of reasoning preambles as well.</p>

	<figure class="calibre22"><div id="ch07_figure_2_1728407187627952" class="figure"><img src="assets/pefl_0702.png" class="calibre23"/>
	<h6 class="calibre24"><span class="firstname">Figure 7-2. </span>Encouraging long preambles to get a correct answer</h6>
	</div></figure>
	</dd>
	<dt class="calibre13">Fluff</dt>
	<dd class="calibre14">
	<p class="subtitle">RLHF-trained models<a contenteditable="false" data-primary="fluff preambles" data-type="indexterm" id="id809" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> often produce verbose and polite responses, which can be problematic for programmatic use where succinct outputs are needed. While models with RLHF are prone to including unnecessary fluff, even those without it can occasionally produce it. To manage this, you can use techniques like providing instructions with few-shot examples or reformatting prompts to separate the main answer from additional comments. This can be expensive, though. For structured documents, models generally maintain the format, but for free-form contexts, asking for the main answer first followed by any extra information helps in parsing and reducing the impact of fluff.</p>

	<p class="subtitle">Which portions of fluff to reserve depends on what kind of fluff the model you chose tends to supply for the kind of questions your application asks. Typical candidates are comments, disclaimers, background, and explanation (see <a data-type="xref" href="#ch07_figure_4_1728407187627986" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-4</a>). Note that the point of this figure is not to demonstrate a correct answer, but to demonstrate the format. Also note that while this trick is good at banishing most fluff behind the main answer, it will not always get rid of a short introduction before the first numbered list item (see <a data-type="xref" href="#ch07_figure_3_1728407187627971" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-3</a>).</p>
	<figure class="calibre22"><div id="ch07_figure_3_1728407187627971" class="figure"><img src="assets/pefl_0703.png" class="calibre23"/>
	<h6 class="calibre24"><span class="firstname">Figure 7-3. </span><a href="https://oreil.ly/WjlZg" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">A fluff preamble that ChatGPT included against explicit instructions</a> in its second answer</h6>
	</div></figure>
	</dd>
	<dd class="calibre14">

	<figure class="calibre22"><div id="ch07_figure_4_1728407187627986" class="figure"><img src="assets/pefl_0704.png" class="calibre23"/>
	<h6 class="calibre24"><span class="firstname">Figure 7-4. </span><a href="https://oreil.ly/K1l98" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Banishing all of ChatGPT’s fluff into a subsequent point</a>, so it can be parsed out easily<a contenteditable="false" data-primary="" data-startref="preamble07" data-type="indexterm" id="id810" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></h6>
	</div></figure>
	</dd>
</dl>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Recognizable Start and End" class="calibre6"><div class="preface" id="ch07_recognizable_start_and_end_1728407187652054">
<h2 class="calibre19">Recognizable Start and End</h2>

<p class="subtitle">If<a contenteditable="false" data-primary="recognizable start and end (completion element)" data-type="indexterm" id="id811" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="beginning and end (preamble elements)" data-type="indexterm" id="id812" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="start and end (preamble elements)" data-type="indexterm" id="id813" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> you want to pick out your main answer from the LLM’s response, you must be able to recognize the beginning and the end. Many document structures make this relatively easy (see <a data-type="xref" href="#ch07_table_1_1728407187636012" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 7-1</a>).</p>

<table id="ch07_table_1_1728407187636012" class="calibre27">
	<caption class="calibre28"><span class="firstname">Table 7-1. </span>Recognizable start and end examples and whether the test for the recognizable end can be written as a test for the presence of a substring</caption>
	<thead class="calibre29">
		<tr class="calibre30">
			<th class="calibre31">Document structure</th>
			<th class="calibre31">Start</th>
			<th class="calibre31">End</th>
			<th class="calibre31">Test for end is test for substring</th>
		</tr>
	</thead>
	<tbody class="calibre32">
		<tr class="calibre30">
			<td class="calibre33">A<a contenteditable="false" data-primary="Markdown" data-secondary="start and end structure" data-type="indexterm" id="id814" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Markdown document</td>
			<td class="calibre33">The expected section header</td>
			<td class="calibre33">Any other section header</td>
			<td class="calibre33">Yes</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">A<a contenteditable="false" data-primary="YAML format" data-type="indexterm" id="id815" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> YAML document</td>
			<td class="calibre33">The expected keyword after a newline</td>
			<td class="calibre33">A line with lower indentation</td>
			<td class="calibre33">No</td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33">A<a contenteditable="false" data-primary="JSON" data-secondary="recognizable start and end" data-type="indexterm" id="id816" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> JSON document</td>
			<td class="calibre33">The expected keyword in quotation marks, then a colon and a quotation mark</td>
			<td class="calibre33">Any unescaped quotation mark</td>
			<td class="calibre33">No</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">A<a contenteditable="false" data-primary="triple backticks (```)" data-type="indexterm" id="id817" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="backticks (```)" data-type="indexterm" id="id818" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="``` (backticks)" data-type="indexterm" id="id819" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> triple-ticked (```) code listing</td>
			<td class="calibre33"><code class="calibre48">```[language]\n</code></td>
			<td class="calibre33"><code class="calibre48">\n```\n</code></td>
			<td class="calibre33">Yes</td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33">The first item of a numbered list (see comments about fluff)</td>
			<td class="calibre33"><code class="calibre47">1.</code></td>
			<td class="calibre33"><code class="calibre47">2.</code></td>
			<td class="calibre33">Yes</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">A function/class in source code (a bracketed language like Java)</td>
			<td class="calibre33"><code class="calibre48">{</code></td>
			<td class="calibre33">The matching closing bracket</td>
			<td class="calibre33">No</td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33">A function/class in source code (an indent language like Python)</td>
			<td class="calibre33">The expected function/class header</td>
			<td class="calibre33">A lower indentation level (except for the occasional terrible string literal)</td>
			<td class="calibre33">No</td>
		</tr>
	</tbody>
</table>

<p class="subtitle">As <a data-type="xref" href="#ch07_table_1_1728407187636012" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 7-1</a> shows, figuring out the start and end of a section can either be straightforward or a bit tricky. With a well-crafted prompt, you can sometimes improve on the recognition methods shown in the table. For example, in a YAML document, if you know what the next keyword will be, you can look for a lower indentation level followed by that keyword, rather than just any lower indentation. This means you can determine the end by checking for specific substrings, as described in the fourth column of <a data-type="xref" href="#ch07_table_1_1728407187636012" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 7-1</a>. Next, we talk about identifying the end of the main answer.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Postscript" class="calibre6"><div class="preface" id="ch07_postscript_1728407187652114">
<h2 class="calibre19">Postscript</h2>

<p class="subtitle">The<a contenteditable="false" data-primary="postscript (completion element)" data-type="indexterm" id="id820" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> reason why your start should be recognizable is clear: it helps you when parsing the answer to filter out the irrelevant introduction. As with the end, you want to be able to filter out the fluffy postscript that’s not relevant to your question.</p>

<p class="subtitle">But there’s a second, at least equally important consideration that applies here. You want to be able to control the length of the LLM’s answer. Every generated token costs you time and compute, making your application slower and more expensive. So ideally, you want to stop generating tokens whenever you hit your recognizable end. If you’re self-hosting an OS model, you have complete freedom to do that whenever you want. But it’s much more common to call an existing model as a service. Here are the two main ways to do it:</p>

<dl class="stafflist">
	<dt class="calibre13">Stop sequences</dt>
	<dd class="calibre14">
	<p class="subtitle">Many<a contenteditable="false" data-primary="stop sequences" data-type="indexterm" id="id821" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> models, in particular those following the OpenAI API, allow you to provide a <em class="hyperlink">stop argument </em>that’s a list of sequences you know mark the end of the relevant solution. When it reaches one of those stop sequences, the model generation will stop (on the server side, if it’s on a server) and end its answer. You won’t incur any further cost in waiting time, compute, or money.</p>
	</dd>
	<dt class="calibre13">Streaming</dt>
	<dd class="calibre14">
	<p class="subtitle">Several models allow a<a contenteditable="false" data-primary="streaming modes" data-type="indexterm" id="id822" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">streaming mode</em> where either individual tokens or small batches of tokens are sent one at a time, instead of waiting until the model generation is complete. Models following the OpenAI API activate streaming by setting the “stream” parameter to “true.” Recognizing an end while streaming means you don’t have to wait for the generation of additional, uninteresting tokens. If you cancel the generation (and the model supports that), you can even save yourself some compute and money—but not as much as you’d have saved with stop sequences, because network communication delays mean your cancellation signal won’t get through immediately.</p>
	</dd>
</dl>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">Very often, stop sequences will begin with a newline character. For example, in<a contenteditable="false" data-primary="Markdown" data-secondary="new section marker (\n#)" data-type="indexterm" id="id823" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="new section marker (\n#)" data-type="indexterm" id="id824" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> markdown documents, <code class="calibre15">\n#</code> is a typical stop sequence. If you don’t include the newline, then you may erroneously stop on a comment in code or the beginning of a phone number.</p>
</div>

<p class="subtitle">Typically, more models admit stop sequences than allow streaming and cancellation, and stop sequences are a tiny bit more effective. But since stop sequences are limited to a list of specific strings, sometimes canceling streams is the only viable option.</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">If certain sequences occasionally signal the end of the completion, you can enhance your “streaming and cancellation” method by adding them as stop sequences. For example, when generating a<a contenteditable="false" data-primary="\nclass" data-type="indexterm" id="id825" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="\ndef" data-type="indexterm" id="id826" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="\nif" data-type="indexterm" id="id827" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Python class, <code class="calibre15">\nclass</code>, <code class="calibre15">\ndef</code>, and <code class="calibre15">\nif</code> are such sequences. They are not the only way the code can continue after the class, but they are some of the most common ways. You might think that <code class="calibre15">\ndef</code> is incorrect because the class you’re generating will have several methods defined that start with <code class="calibre15">def</code>, but notice that they will be indented and will actually start with<a contenteditable="false" data-primary="\n\tdef" data-type="indexterm" id="id828" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <code class="calibre15">\n\tdef</code>. Therefore, they will not cause the model to halt generation.<a contenteditable="false" data-primary="" data-startref="Cideal07" data-type="indexterm" id="id829" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Beyond the Text: Logprobs" class="calibre6"><div class="preface" id="ch07_beyond_the_text_logprobs_1728407187652313">
<h1 class="calibre5">Beyond the Text: Logprobs</h1>

<p class="subtitle">Throughout<a contenteditable="false" data-primary="completions" data-secondary="enhancing with logprobs" data-type="indexterm" id="Clog07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> this book, we’ve been presenting LLMs as “text in” (prompt) then “text out” (completion). But it’s worth being aware of a couple of tricks that break that paradigm by analyzing not only the text output but the numerical values that describe what the model thinks about the text.</p>

<p class="subtitle">In <a data-type="xref" href="ch02.html#ch02_understanding_llms_1728407258904677" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 2</a>, we discussed how the LLM calculates not just individual tokens but the entire probability distribution for the next token based on previous input. These probabilities are returned as<a contenteditable="false" data-primary="logprobs (logarithm of the probabilities)" data-secondary="definition of term" data-type="indexterm" id="id830" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">logprobs</em> (the logarithm of the probabilities). A logprob is negative; the more negative its value, the less probable the token is considered by the model. A logprob of 0 means the model is certain about the token. To<a contenteditable="false" data-primary="logprobs (logarithm of the probabilities)" data-secondary="converting to standard probability" data-type="indexterm" id="id831" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> convert a logprob to a standard probability, you use the<a contenteditable="false" data-primary="exp function" data-type="indexterm" id="id832" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <code class="calibre15">exp</code> function. For instance, if the logprobs for “Yes” and “No” are ‒0.405 and ‒1.099, respectively, then the model is about 66% sure it will be “Yes” and 33% sure it will be “No.”</p>

<p class="subtitle">For<a contenteditable="false" data-primary="logprobs (logarithm of the probabilities)" data-secondary="retrieving" data-type="indexterm" id="id833" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> models using the OpenAI API, you can request that those logprobs be returned to you as shown in <a data-type="xref" href="ch02.html#ch02_figure_12_1728407258873476" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 2-12</a>. What you get are the calculated probabilities, not just for the tokens that the model ends up choosing, but also for the ones it considered and decided not to use. Since the model calculates these probabilities anyway, retrieving them doesn’t require any additional computing effort.</p>

<div data-type="warning" epub:type="warning" class="calibre16"><h6 class="calibre18">Warning</h6>
<p class="subtitle">Some commercial models disable the part of the API where you get the logprobs, mostly out of fear of being reverse-engineered if they share too much about their internals. If you want to use any of the tricks in this section, consider this in your LLM choice.</p>
</div>

<p class="subtitle">You can do many cool things with logprobs. Let’s talk about how to use them to evaluate answer quality, get the model to estimate certainties, and find critical locations in a (provided or generated) text.</p>

<section data-type="sect2" data-pdf-bookmark="How Good Is the Completion?" class="calibre6"><div class="preface" id="ch07_how_good_is_the_completion_1728407187652377">
<h2 class="calibre19">How Good Is the Completion?</h2>

<p class="subtitle">Albert’s neighbor<a contenteditable="false" data-primary="completions" data-secondary="determining quality of" data-type="indexterm" id="id834" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="logprobs (logarithm of the probabilities)" data-secondary="determining completion quality with" data-type="indexterm" id="id835" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="quality" data-secondary="of completions" data-secondary-sortas="completions" data-type="indexterm" id="id836" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="application design" data-secondary="evaluating completion quality" data-type="indexterm" id="id837" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="evaluation" data-secondary="of completions" data-secondary-sortas="completions" data-type="indexterm" id="id838" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> happens to be an astrophysicist, and when Albert asked her how many minutes light needs to travel from the sun to Mars, roughly; she straightaway replied, “13,” with absolute confidence. Albert then asked his 10-year-old daughter the same question. She looked surprised and then hesitantly guessed, “Maybe 30?” One of these answers is much more reliable than the other, and anyone present can tell which one is more reliable from the respondent’s facial expressions and tone of voice. Well, logprobs are like the model’s tone of voice, and you can use them to see how confident it is in its answer—and that’s a strong indicator of answer quality.</p>

<p class="subtitle">Logprobs indicate a model’s<a contenteditable="false" data-primary="confidence" data-type="indexterm" id="id839" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> confidence in each token choice (refer to <a data-type="xref" href="#ch07_figure_5_1728407187628014" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-5</a>). Summing logprobs across a text shows overall confidence in that text as the “correct” response, considering how it might start with the prompt in training data and conclude with the completion. However, the accuracy of this measure can decrease with longer texts due to the many ways in which the same idea can be expressed, like using “for example” or “for instance,” which can halve the probability without reflecting a decrease in quality.</p>

<p class="subtitle">To assess quality, it’s beneficial to average the logprobs. The simple average—adding all logprobs and dividing by the number of tokens—is effective, especially if experimenting isn’t feasible due to constraints like data scarcity or limited time. For a more nuanced approach, Albert, during GitHub Copilot’s development, found that averaging the probabilities (rather than the logprobs) of early tokens in the completion is predictive of overall quality. (This is calculated as <code class="calibre15">(exp(logprob_1) + … + exp(logprob_n)) / n</code>.)</p>

<p class="subtitle">This average provides a numerical quality indicator, and while it falls short of being an absolute measure of quality, in practical applications, you can explore logprob-based cutoffs for features within your application as follows:</p>

<ol class="stafflist">
	<li class="calibre25">
	<p class="calibre26">Only allow your application to show corrections if it is confident.</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">Include warnings when the model struggles more than usual.</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">Incorporate more context or retry when the model struggles.</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">Switch to a more intelligent (and expensive) LLM for better results.</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">Only interrupt the user with assistance if the certainty that it is necessary is high. Remember <a href="https://oreil.ly/csVva" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Clippy</a>? Don’t be like Clippy.</p>
	</li>
</ol>

<p class="subtitle">For greater quality at a higher compute cost, you can also consider setting<a contenteditable="false" data-primary="temperature parameter" data-type="indexterm" id="id840" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> a higher temperature, generating multiple completions, and choosing the best one based on their logprobs.</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">Many LLM APIs have a parameter<a contenteditable="false" data-primary="n parameter" data-type="indexterm" id="id841" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> called n, which controls the number of completions that are generated from the same prompt in parallel. If n is larger than 1, then the temperature should be larger than 0 or all completions will be the same. A rough (and completely unscientific) rule of thumb we like to use is temperature = sqrt(n) / 10.</p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="LLMs for Classification" class="calibre6"><div class="preface" id="ch07_llms_for_classification_1728407187652495">
<h2 class="calibre19">LLMs for Classification</h2>

<p class="subtitle">The<a contenteditable="false" data-primary="logprobs (logarithm of the probabilities)" data-secondary="classification and" data-type="indexterm" id="logprob07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="classification" data-type="indexterm" id="classif07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> concepts of classification and logprobs are intertwined in the context of LLMs, as logprobs provide critical insights into the model’s decision-making processes, confidence, and reliability. Let’s take a look at classification now.</p>

<p class="subtitle"><em class="hyperlink">Classification</em> is a basic machine learning<a contenteditable="false" data-primary="machine learning (ML)" data-type="indexterm" id="id842" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> task in which you determine which category a specific case belongs to from a set of predefined options. For instance, you might classify an online review as positive, negative, or neutral, or you could predict whether a product is best suited for the American, European, or Asian market. In simpler terms, you could be deciding if the answer to a question is yes or no. The key aspect is that, much as in a detective novel with a limited number of suspects, there are a fixed number of possible categories, and your goal is to identify the correct one and determine your confidence level in that choice.</p>

<p class="subtitle">This is pretty much the opposite of how LLMs were built to work: LLMs lean toward long, creative generation instead of fixed, boxed-in classification. But LLMs are pre-trained generalists, and in domains where the classification task relies on public knowledge and common sense, they have a good chance to excel with little to no extra training data. The prompt engineer has to set up the prompt in a way that the model chooses exactly one of the alternatives. But there are some subtleties, which we’ll talk about now.</p>

<p class="subtitle">At the basic level, you use your LLM just by asking it questions. If you want to find out whether a sentence is positive, negative, or neutral, you might present the sentence to the model and add the question, “Does that sound positive, negative or neutral to you?” Then, you might check the answer for which of these three alternatives occurs in it. Of course, you want to avoid waffling answers that include several alternatives, like “more positive than neutral.” A more refined question might be “Does that sound positive, negative, or neutral to you? Please answer in the format: 1. [negative | positive | neutral], 2. [explanation].” The “1.” in this example is what we called a<a contenteditable="false" data-primary="start and end (preamble elements)" data-type="indexterm" id="id843" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="beginning and end (preamble elements)" data-type="indexterm" id="id844" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="recognizable start and end (completion element)" data-type="indexterm" id="id845" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">recognizable start</em>, and you can expect the answer directly after it.</p>

<p class="subtitle">In this situation, it’s a good idea to ensure that after the first recognizable token, you can immediately tell which option the model chooses. Here’s why: in <a data-type="xref" href="#ch07_figure_5_1728407187628014" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-5</a>, the model has three options: North America, Northeast Asia, and Europe. Two of these, North America and Northeast Asia, both start with the token <em class="hyperlink">North</em>. When the model predicts the next token, the two answers that start with <em class="hyperlink">North</em> combine their chances, since the model predicts only <em class="hyperlink">North</em> at first. If the model is uncertain, it’s more likely to choose <em class="hyperlink">North</em> because both options share it. The actual decision between the two will come afterward. To avoid this, you need to make sure each option starts with a unique token.</p>

<figure class="calibre22"><div id="ch07_figure_5_1728407187628014" class="figure"><img src="assets/pefl_0705.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 7-5. </span>The model’s calculated total probability for Europe is highest (44% versus 55% × 76% = 42% for Northeast Asia), but the suggestion will be Northeast Asia</h6>
</div></figure>

<p class="subtitle">Note that in the figure, because the first decision is between North and Europe, the probabilities for Northeast Asia and North America are added together, leading the model to put out a suggestion it actually considers suboptimal. These are actual probabilities from OpenAI’s gpt-3.5-turbo-instruct.</p>

<p class="subtitle">You can let the model make all kinds of decisions through classification, but in many situations, its prediction will be badly calibrated compared to what you want. For example, let’s say you’re writing an app that helps grumpy users by blocking some of the emails they write if the LLM deems them not sufficiently friendly and asks them to rewrite them. You can pretty easily ask the model, “Is this a professionally written email? Please use the format 1. Yes / No. 2. Explanation.” But even if the model is good at recognizing whether one email is more professionally written than another, the threshold between what you do and don’t consider to be professional is likely not the same as the model’s. To match the model’s threshold more closely, you’ll have to calibrate, and that’s where the logprobs finally come into play.</p>

<p class="subtitle"><em class="hyperlink">Calibration</em> means<a contenteditable="false" data-primary="calibration" data-type="indexterm" id="id846" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> adjusting the certainty of a classification to better match the “true” certainty. A priori, the certainty of the prediction is the logprob, and whatever token has the highest logprob is what the model will produce (at temperature 0). But if, for example, you find that the model lets through too few emails, you’ll wish that the model would only output No if it’s super certain. So maybe it should only choose No if the logprob for No is at least 0.3 higher than the one for Yes.</p>

<p class="subtitle">Generally, to calibrate the LLM’s decision process, you shift the logprobs by a constant (where each a<sub class="calibre57">tok</sub> corresponds to one of the tokens in question). For example, you can make the email classification less strict by adding a constant like a<sub class="calibre57">yes</sub> = 0.3 to the logprob of “Yes” before comparing it with the logprob for <em class="hyperlink">No</em>. You can find these constants either by experimentation or by some classical machine learning: taking ground truth data and minimizing the<a contenteditable="false" data-primary="cross entropy loss" data-type="indexterm" id="id847" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a href="https://oreil.ly/WTiBc" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">cross entropy loss</a> like you do in<a contenteditable="false" data-primary="logistic regression" data-type="indexterm" id="id848" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a href="https://oreil.ly/aR3Wn" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"> logistic regression</a>.</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">If you found constants a<sub class="calibre58">tok</sub> that you like, you don’t actually have to mess with the logprobs anymore—many model providers offer in their API the possibility of a<a contenteditable="false" data-primary="biases" data-secondary="logit bias" data-type="indexterm" id="id849" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="logit bias" data-type="indexterm" id="id850" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">logit bias</em>, where you send the a<sub class="calibre58">tok</sub> to the model and they will be applied for you.<a contenteditable="false" data-primary="" data-startref="logprob07" data-type="indexterm" id="id851" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="classif07" data-type="indexterm" id="id852" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Critical Points in the Prompt" class="calibre6"><div class="preface" id="ch07_critical_points_in_the_prompt_1728407187652632">
<h2 class="calibre19">Critical Points in the Prompt</h2>

<p class="subtitle">Another application<a contenteditable="false" data-primary="logprobs (logarithm of the probabilities)" data-secondary="investigating prompts with" data-type="indexterm" id="id853" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="prompt engineering" data-secondary="investigating prompts with logprobs" data-type="indexterm" id="id854" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> of logprobs isn’t to get the certainty in the complution, but to understand the surprising parts of the prompt. Setting the<a contenteditable="false" data-primary="echo parameter" data-type="indexterm" id="id855" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> parameter “echo” to true tells many APIs to return not only the logprobs for the completion, but also for the prompt. You can run this to better understand the text you send to the model, even if you don’t request a single completion token.</p>

<p class="subtitle">For example, in the paragraph you just read, did you notice a<a contenteditable="false" data-primary="errors" data-secondary="typographical errors" data-type="indexterm" id="id856" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="typographical errors" data-type="indexterm" id="id857" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> typo? So did the model. As shown in <a data-type="xref" href="#ch07_figure_6_1728407187628032" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-6</a>, when you’re displaying the logprobs, that typo stands out like a sore thumb with a logprob of below―13, where instead of the “completion” token, the model got only the “compl” token (followed by “ution”). This way, you can use logprobs to detect not only typos but also otherwise surprising parts of the text. More generally, you can use logprobs to detect passages in the text with<a contenteditable="false" data-primary="content" data-secondary="focusing app on higher density" data-type="indexterm" id="id858" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> higher information density, with the idea of focusing your app’s attention on certain locations or alternatively guiding the user’s attention.</p>

<figure class="calibre22"><div id="ch07_figure_6_1728407187628032" class="figure"><img src="assets/pefl_0706.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 7-6. </span>Logprobs of two versions of a paragraph of text, shown interleaved</h6>
</div></figure>

<p class="subtitle">As you can see in <a data-type="xref" href="#ch07_figure_6_1728407187628032" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-6</a>, negative single-digit logprobs are somewhat common, while negative double-digit logprobs are usually the model picking up some weirdness. However, there’s no clearly delineated threshold, and even heuristics vary from model to model and genre of text to genre of text. In fact, they vary <em class="hyperlink">within</em> a single text: in the beginning, the logprobs are usually lower (i.e., further below 0) than toward the end. That’s because much about the topic and style of the text becomes clear to the model only as it’s reading it.</p>

<div data-type="warning" epub:type="warning" class="calibre16"><h6 class="calibre18">Warning</h6>
<p class="subtitle">When writing<a contenteditable="false" data-primary="tests" data-secondary="unit tests" data-type="indexterm" id="id859" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="unit tests" data-type="indexterm" id="id860" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> unit tests for any parts of your application that deal with logprobs, remember that due to<a contenteditable="false" data-primary="floating-point inaccuracies" data-type="indexterm" id="id861" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> floating-point inaccuracies, logprobs are not deterministic. Depending on the model deployment, they may vary by as much as ± 1, so write your tests to be robust against such variation or mock out the model entirely.<a contenteditable="false" data-primary="" data-startref="Clog07" data-type="indexterm" id="id862" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Choosing the Model" class="calibre6"><div class="preface" id="ch07_choosing_the_model_1728407187652699">
<h1 class="calibre5">Choosing the Model</h1>

<p class="subtitle">In<a contenteditable="false" data-primary="completions" data-secondary="model selection" data-type="indexterm" id="Cselect07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> this chapter thus far, we’ve focused on the model itself, but we’ve danced around an important question: which model should you use? LLM choice is going to be critical to the success of any AI software development project, and yet, there are many alternatives, with new ones popping up every week. In a landscape that’s changing this quickly, recommendations for particular models are going to become stale pretty quickly, so we’ll instead focus on the underlying principles that should guide your choice.</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">Whatever model<a contenteditable="false" data-primary="model selection" data-secondary="allowing for flexibility" data-type="indexterm" id="id863" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> you end up choosing, don’t bake your choice into your code too firmly. You may want to revise, evaluate, and refine your choice. Libraries like<a contenteditable="false" data-primary="LiteLLM" data-type="indexterm" id="id864" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a href="https://litellm.ai" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">LiteLLM</a> may be useful here for providing a unified API to many different models.</p>
</div>

<p class="subtitle">The<a contenteditable="false" data-primary="model selection" data-secondary="guiding principles for" data-type="indexterm" id="id865" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> model you <em class="hyperlink">need</em> depends on what you <em class="hyperlink">want.</em> There’s no single quality that reigns supreme, but here’s a list of of considerations (in order of importance) for most scenarios:</p>

<dl class="stafflist">
	<dt class="calibre13">Intelligence</dt>
	<dd class="calibre14">
	<p class="subtitle">How<a contenteditable="false" data-primary="intelligence" data-secondary="model selection and" data-type="indexterm" id="id866" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> close is the model’s answer to that of an intelligent human expert with strong subject matter expertise? This is especially important for apps that ask the model complicated questions that require complex reasoning or very accurate answers.</p>
	</dd>
	<dt class="calibre13">Speed</dt>
	<dd class="calibre14">
	<p class="subtitle">How<a contenteditable="false" data-primary="speed" data-type="indexterm" id="id867" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> long do you have to wait for your answer? This is especially important for apps that interact very directly with their users (see <a data-type="xref" href="ch05.html#ch05_table_2_1728435524657385" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 5-2</a> about the different levels of urgency users may feel depending on the kind of application).</p>
	</dd>
	<dt class="calibre13">Cost</dt>
	<dd class="calibre14">
	<p class="subtitle">How<a contenteditable="false" data-primary="cost-effective models" data-type="indexterm" id="id868" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> much do you pay for running inference, either directly to the model provider or in costs for GPUs? This is especially important for apps that make very frequent requests to the model.</p>
	</dd>
	<dt class="calibre13">Ease of use</dt>
	<dd class="calibre14">
	<p class="subtitle">How<a contenteditable="false" data-primary="ease of use" data-type="indexterm" id="id869" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> much of the work regarding arranging GPUs, deploying the model, restarting crashed instances, routing, caching, etc., is conveniently done for you?</p>
	</dd>
	<dt class="calibre13">Functionality</dt>
	<dd class="calibre14">
	<p class="subtitle">Does<a contenteditable="false" data-primary="functionality" data-type="indexterm" id="id870" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the model have the capabilities for instruct, chat, and tool use? Does it surface logprobs? Can it process images as well as language?</p>
	</dd>
	<dt class="calibre13">Special requirements</dt>
	<dd class="calibre14">
	<p class="subtitle">These<a contenteditable="false" data-primary="special requirements" data-type="indexterm" id="id871" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> are a lot like dietary requirements; for some people, they are nonnegotiable, but for others, they are completely unimportant. Some app developers might prefer models to be noncommercial, open source, trained on specific data, and regularly updated (or not). They might want to ensure data residency in a particular country, or they may avoid logging off premises. These preferences can quickly narrow down the available options (see <a data-type="xref" href="#ch07_figure_7_1728407187628052" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-7</a>).</p>
	</dd>
</dl>

<figure class="calibre22"><div id="ch07_figure_7_1728407187628052" class="figure"><img src="assets/pefl_0707.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 7-7. </span>The knobs and dials you use to decide what kind of model you’ll have</h6>
</div></figure>

<p class="subtitle">As <a data-type="xref" href="#ch07_figure_7_1728407187628052" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-7</a> illustrates, as you<a contenteditable="false" data-primary="model selection" data-secondary="balancing requirements" data-type="indexterm" id="id872" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="requirements, balancing" data-type="indexterm" id="id873" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> tighten one requirement, you often constrain the type of model available. Here are some examples:</p>

<ul class="stafflist">
	<li class="calibre9">
	<p class="calibre26">If you know that your app will make a high volume of relatively simple requests to the model and that you’ll therefore need it to be cheap but not smart, a small model is likely to be appropriate.</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">If your app is a solo project that you want to knock out quickly, and if it only makes about one request per day, then you should feel encouraged to splurge on a premium-tier model because cost may only be a factor at scale.</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">If you make a ton of very difficult requests and need your model to be super cheap while being super smart…tough luck, because these two qualities are at opposite ends of the spectrum.</p>
	</li>
</ul>

<p class="subtitle">When<a contenteditable="false" data-primary="model selection" data-secondary="choosing a provider" data-type="indexterm" id="id874" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="providers, choosing" data-type="indexterm" id="id875" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> you’re deciding on a model, the first step is usually picking a provider. You’ll likely base this decision on your requirements, desired features, and whether you want a scrappy or premium solution. Most providers offer a range of models, and you’ll narrow them down based on specific capabilities and needs, and then you’ll choose the model size.</p>

<p class="subtitle">At one point, OpenAI, known for its highly advanced models and full-service platform, was the dominant choice. However, over the course of 2024, the playing field has become more even. Here are some other<a contenteditable="false" data-primary="LLM-as-a-service companies" data-type="indexterm" id="id876" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> choices to consider:</p>

<dl class="stafflist">
	<dt class="calibre13">Anthropic</dt>
	<dd class="calibre14">
	<p class="subtitle">Emphasizes human alignment<a contenteditable="false" data-primary="Anthropic" data-secondary="choosing a provider" data-type="indexterm" id="id877" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> and AI safety. Its Claude 3.5 Sonnet model recently (in 2024) jumped to the top of several LLM benchmarks (see <a href="https://oreil.ly/pWZkS" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Claude 3.5 Sonnet’s website</a>).</p>
	</dd>
	<dt class="calibre13">Mistral</dt>
	<dd class="calibre14">
	<p class="subtitle">Specializes<a contenteditable="false" data-primary="Mistral" data-type="indexterm" id="id878" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> in highly efficient, open-weight models; ideal for applications that need very specialized configurations.</p>
	</dd>
	<dt class="calibre13">Cohere</dt>
	<dd class="calibre14">
	<p class="subtitle">Popular<a contenteditable="false" data-primary="Cohere" data-type="indexterm" id="id879" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> for high-performance RAG applications.</p>
	</dd>
	<dt class="calibre13">Google</dt>
	<dd class="calibre14">
	<p class="subtitle">Strong<a contenteditable="false" data-primary="Google" data-type="indexterm" id="id880" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> integration with Google’s ecosystem, cutting-edge research, and large-scale infrastructure.</p>
	</dd>
	<dt class="calibre13">Meta</dt>
	<dd class="calibre14">
	<p class="subtitle">Large, highly capable open-access models.</p>
	</dd>
</dl>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">There are many model comparison sites around to serve as a starting point in your exploration of what to start prototyping with or which alternatives to evaluate in more detail. We quite like<a contenteditable="false" data-primary="Artificial Analysis website" data-type="indexterm" id="id881" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a href="https://artificialanalysis.ai" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">the Artificial Analysis website</a>.</p>
</div>

<p class="subtitle">But<a contenteditable="false" data-primary="open-source models" data-type="indexterm" id="id882" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> if having the premium tier isn’t a requirement for you, then you don’t have to rely on an LLM-as-a-service company at all. Several LLMs like<a contenteditable="false" data-primary="LLaMA" data-type="indexterm" id="id883" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> LLaMA and Mistral are open source and typically trained by academic groups or open source―friendly companies. Hosting these models requires significant effort, though platforms like Hugging Face<a contenteditable="false" data-primary="Hugging Face" data-type="indexterm" id="id884" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> aim to ease the process, whether you use your own servers or their Azure partnership. We recommend this route only if your app is large enough to justify the infrastructure investment and if your model needs to steer you away from full-service solutions. If you’re using agile methods, you can also prototype using the easily accessible OpenAI APIs with the intention of moving to a different platform when going public.</p>

<p class="subtitle">After you’ve found a provider, you’ll probably have to choose among several different models the provider offers, and apart from some consideration of capabilities, this mainly means choosing the<a contenteditable="false" data-primary="model selection" data-secondary="model size" data-type="indexterm" id="id885" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> model size. Whether or not<a contenteditable="false" data-primary="latency" data-secondary="model selection and" data-type="indexterm" id="id886" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> latency matters, completion quality versus cost is always a hard trade-off. Typically, you’ll want the smallest model that can reliably deliver on your task.</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">Feel free to<a contenteditable="false" data-primary="prototyping" data-type="indexterm" id="id887" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> prototype with slightly larger models than you think you can afford. As new flagship models get released, the older ones tend to become<a contenteditable="false" data-primary="cost-effective models" data-type="indexterm" id="id888" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> cheaper over time, so by the time your public beta comes around, there will be better models in scope than there were during prototyping. You’ll be glad if your prompt engineering and postprocessing is already optimized for the better models you now can afford.</p>
</div>

<p class="subtitle">You<a contenteditable="false" data-primary="model selection" data-secondary="fine-tuning models" data-type="indexterm" id="MSfine07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="fine-tuning" data-type="indexterm" id="finetune07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="training" data-type="indexterm" id="train07" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> probably won’t ever want to build and train your own model from scratch, but you may want to take an existing model and <em class="hyperlink">make</em> <em class="hyperlink">it your own</em> by training it specifically on the task that your application will use it for. This process is called <em class="hyperlink">fine-tuning</em>. While this topic moves beyond the scope of this book, we do want to familiarize you enough with the basic concepts so you’ll be able to judge whether it’s a promising idea in your case and whether to invest more time in it.</p>

<p class="subtitle">When an LLM is first trained, it effectively reads through lots of documents and learns how to mimic them. In fine-tuning, you present the model with new documents and train it to mimic those documents. This will often decrease the model’s ability to produce generic documents, but it can dramatically improve the model’s ability to produce the types of documents you anticipate seeing in your work.</p>

<p class="subtitle">To fine-tune a model, you’ll need a set of training documents that show successful interactions. These should have factually correct answers, use only the background information you want the model to learn, and adhere to the expected format. How can you gather these examples? You can create some yourself, hire contractors, or even synthesize them. If your app has users, you might collect examples based on success indicators like accepted suggestions or user likes. If your app automates a task previously done by humans, you could use their interactions as examples. Whether you can gather these examples is key in deciding if fine-tuning is worth it (see <a data-type="xref" href="#ch07_figure_8_1728407187628068" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 7-8</a>).</p>

<p class="subtitle">Some fine-tuning frameworks allow you to only train your model, surgically, on the portion of the document that addresses the problem, rather than, for example, on the portion of the document where a user specifies the problem. Only focusing on these critical parts of the document is called<a contenteditable="false" data-primary="loss masking" data-type="indexterm" id="id889" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">loss masking</em>, and it is useful because probably, you’re not interested in whether the model can produce the part of the document that is the prompt.</p>

<figure class="calibre22"><div id="ch07_figure_8_1728407187628068" class="figure"><img src="assets/pefl_0708.png" class="calibre23"/>
	<h6 class="calibre24"><span class="firstname">Figure 7-8. </span>Should you fine-tune?</h6>
	</div></figure>
	
<p class="subtitle">Depending on how many training documents you come up with, you’ll have options for different kinds of fine-tuning. We’ll discuss the main options here, and we’ve also summarized them in <a data-type="xref" href="#ch07_table_2_1728407187636061" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 7-2</a>.</p>

<p class="subtitle"><em class="hyperlink">Full fine-tuning</em>, or<a contenteditable="false" data-primary="full fine-tuning" data-type="indexterm" id="id890" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="continued pre-training" data-type="indexterm" id="id891" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">continued pre-training</em>, is simply the continuation of the training process with different documents. That means that every one of the model’s billions of parameters is adjusted, and it takes time, computational power, and many, many examples to adjust the parameters in the right way. Like all neural training, this isn’t like explaining a concept to a human and expecting them to learn by understanding. It’s more like a riverbed forming: you pour thousands upon thousands of training documents over the model, and very slowly, a groove is carved out. The advantage is, that new groove can be anything. The original model is the starting point, but you can teach it completely new facts and new domains.</p>

<p class="subtitle">Low-rank adaptation (LoRA) is<a contenteditable="false" data-primary="low-rank adaptation (LoRA)" data-type="indexterm" id="id892" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> a parameter-efficient fine-tuning technique designed to make model training more efficient. The key idea is that when you don’t need the model to learn something entirely new, you don’t have to adjust all of its parameters. Instead, LoRA focuses on a few key parameter matrices in the LLM and trains a<a contenteditable="false" data-primary="diffs" data-type="indexterm" id="id893" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> “<em class="hyperlink">diff</em>” to those matrices, which for each original matrix is a difference matrix that is added to the original, but one that has fewer degrees of freedom (hence, it’s of <em class="hyperlink">low rank</em>).</p>

<p class="subtitle">This approach has practical benefits—since the diffs are small, they can easily be shared between virtual machines, and one deployment can handle multiple diffs, allowing you to use the same machine for different models. More importantly, LoRA fine-tuning is relatively fast, typically taking hours or a few days, making it a compute-efficient option.</p>

<p class="subtitle">But there are also drawbacks: depending on the LoRA dimension (a number measuring the degrees of freedom to the diff you train), the model is limited in how much it can learn. In general, a good intuition is that LoRA doesn’t really teach a model new tricks. Rather, LoRA teaches the model which of the tricks that it’s already capable of performing it should expect to use, and in which way. In particular, this includes things like what to pay attention to in the prompt, how to interpret it, and what is expected from the model in the completion. Format and style are easily learnable with LoRA. Another thing that LoRA is great at doing is giving the model a general feel for the prior distributions it should assume for your domain.</p>

<p class="subtitle">Let’s explain the last point through an example. Say your application helps people select travel destinations and all your customers are based in Europe. Being Europeans, their preferred suggestions will have a very different distribution than if they were based in the United States. Napa Valley is far away, and Monaco is just around the corner. Fine-tuning can teach the model that, but to be fair, so could you: you could just add to the prompt that the customer is European, so the model should choose destinations based on that. But what about factors you don’t know explicitly? Maybe most users of your app are students, and they’re looking for budget destinations. Your app telemetry might show that suggesting Monaco usually gets a thumbs-down but that every time you suggest Prague, the user buys a ticket. If you have such data to feed it, LoRA fine-tuning excels at conditioning the model to such a distribution shift, whether it depends on an aspect you’re aware of (preference for budget destinations) or not.</p>

<p class="subtitle">With either continued pre-training or LoRA fine-tuning, you can normally get rid of <em class="hyperlink">all</em> your<a contenteditable="false" data-primary="static content" data-secondary="removing" data-type="indexterm" id="id894" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="context" data-secondary="removing static prompt context" data-type="indexterm" id="id895" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="prompt engineering" data-secondary="removing static prompt context" data-type="indexterm" id="id896" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> static prompt context, the general explanations, and instructions—the model will just bake them into its parameters. You also don’t need few-shot prompting anymore: all lessons from those few-shots should already be absorbed into the LoRA model, and more effectively than when presented in the prompt. In this sense, fine-tuning is a continuation of prompt engineering by other means.</p>

<p class="subtitle">A technique called<a contenteditable="false" data-primary="prompt engineering" data-secondary="soft prompting" data-type="indexterm" id="id897" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="soft prompting" data-type="indexterm" id="id898" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">soft prompting</em> continues further. Thinking back to <a data-type="xref" href="ch02.html#ch02_understanding_llms_1728407258904677" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 2</a>, consider what happens to the model as it processes the tokens in a prompt. Effectively, the prompt creates a “state of mind” in the model that conditions what tokens it will predict next. So, you can spend a lot of time crafting the words to elicit the right state of mind…or you can simply give a few dozen examples of desired outputs to the model and use<a contenteditable="false" data-primary="machine learning (ML)" data-type="indexterm" id="id899" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> machine learning to find a model state that makes the model most likely produce them. Soft prompting is a cool idea, but you’ll need to check whether your model framework gives you this opportunity—many don’t.</p>

<table id="ch07_table_2_1728407187636061" class="calibre27">
	<caption class="calibre28"><span class="firstname">Table 7-2. </span>Different types of fine-tuning</caption>
	<thead class="calibre29">
		<tr class="calibre30">
			<th class="calibre31"> </th>
			<th class="calibre31">The model typically learns…</th>
			<th class="calibre31">It makes the most sense if your training documents number in the…</th>
			<th class="calibre31">Fine-tuning often takes…</th>
		</tr>
	</thead>
	<tbody class="calibre32">
		<tr class="calibre30">
			<td class="calibre33"><strong class="calibre51">Full fine-tuning or continued pre-training</strong></td>
			<td class="calibre33">New things about a potentially whole new domain.</td>
			<td class="calibre33">Tens of thousands.</td>
			<td class="calibre33">Weeks or months.</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33"><strong class="calibre51">Parameter efficient fine-tuning (e.g., LoRA)</strong></td>
			<td class="calibre33">Prior expectations within an existing domain, interpreting information in a certain way, and obeying a fixed format.</td>
			<td class="calibre33">Hundreds or thousands.</td>
			<td class="calibre33">Days.</td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33"><strong class="calibre51">Soft prompting</strong></td>
			<td class="calibre33">Whatever information is contained in <em class="hyperlink">that</em> prompt.</td>
			<td class="calibre33">Hundreds.</td>
			<td class="calibre33">Hours.</td>
		</tr>
	</tbody>
</table>

<p class="subtitle">No matter which fine-tuning paradigm you go for, there will be one crucial impact: the<a contenteditable="false" data-primary="Red Riding Hood principle" data-type="indexterm" id="id900" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="Little Red Riding Hood principle " data-type="indexterm" id="id901" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Little Red Riding Hood principle will work differently for fine-tuned models. There are now two kinds of documents, two kinds of paths Little Red could follow: the old path of the model’s original training and the new path you have fine-tuned for. The old path might be slightly overgrown, but it’s still visible, and you need to beware: if the prompt looks like it might follow that path, so will the model in the completion—in effect, the model will simply <a href="https://arxiv.org/abs/2309.10105" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">forget its fine-tuning</a>. So, the modified Little Red Riding Hood principle says these things:</p>

<ol class="stafflist">
	<li class="calibre25">
	<p class="calibre26">Try to make your prompt look like the beginning of one of the documents you fine-tuned for.</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">Be very sure not to have it look like one of the original documents instead.<a contenteditable="false" data-primary="" data-startref="Cselect07" data-type="indexterm" id="id902" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="MSfine07" data-type="indexterm" id="id903" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="finetune07" data-type="indexterm" id="id904" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="train07" data-type="indexterm" id="id905" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
	</li>
</ol>
</div></section>

<section class="calibre6" data-type="sect1" data-pdf-bookmark="Conclusion"><div class="preface" id="ch07_conclusion_1728407187652928">
<h1 class="calibre5">Conclusion</h1>

<p class="subtitle">Taming the model is hard: it sometimes feels like LLMs have a mind of their own, and they don’t want to follow the path you laid out for them. But you now have a good understanding of how to lead them along the path that you want—do this by clearly defining the completion you want them to provide and then using the tricks you’ve learned to guide them toward a completion with the expected format, style, and content.</p>

<p class="subtitle">Most of the time, the text of the completion is the focal point of your work. But in this chapter, you also learned about logprobs and how to use them to glean more information from LLM completions. And if the model still won’t do as you say, you have the knowledge at your disposal to decide upon a different model or to even train your model yourself, if that’s the right path for you.</p>

<p class="subtitle">This chapter concludes what we consider the core prompt-engineering techniques. With a solid understanding of how LLMs work and how to make them work for you, you can now call yourself a proper prompt engineer! But what kind of prompt engineer would be satisfied with merely learning the basics? In the next chapters, we’ll discuss advanced techniques that use LLM—mere document completion models—as the central components of flexible agents and powerful workflow execution systems.</p>
</div></section>
</div></section></div></div></body></html>