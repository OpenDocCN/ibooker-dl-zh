- en: '8 Deep learning: The foundational concepts'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 深度学习：基础概念
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Core building blocks of deep learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的基本构建块
- en: Supervised and unsupervised learning approaches
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督和无监督学习方法
- en: Convolutional and recurrent neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络和循环神经网络
- en: The Boltzmann learning rule and deep belief networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍尔兹曼学习规则和深度信念网络
- en: Python coding with TensorFlow and Keras
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow和Keras进行Python编码
- en: Overview of deep learning libraries
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习库概述
- en: The art of simplicity is a puzzle of complexity.—Douglas Horton
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 简单的艺术是复杂性的谜题。——道格拉斯·霍顿
- en: Welcome to the third part of the book. So far, you have covered a lot of concepts
    and case studies and Python code. From this chapter onward, the level of complexity
    will be even higher.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到本书的第三部分。到目前为止，你已经涵盖了大量的概念、案例研究和Python代码。从本章开始，复杂度将会更高。
- en: In the first two parts of the book, we covered various unsupervised learning
    algorithms like clustering, dimensionality reduction, etc. We discussed both simpler
    and advanced algorithms. We also covered working on text data in the second part
    of the book. Starting from this third part of the book, we will start our journey
    on deep learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前两部分中，我们介绍了各种无监督学习算法，如聚类、降维等。我们讨论了简单和高级算法，并在本书的第二部分中介绍了在文本数据上的工作。从本书的第三部分开始，我们将开始深度学习的旅程。
- en: Deep learning and neural networks have changed the world and the business domains.
    You have probably heard about deep learning and neural networks. Their implementations
    and sophistication result in better cancer detection, autonomous driving cars,
    improved disaster management systems, better pollution control systems, reduced
    fraud in transactions, and so on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和神经网络已经改变了世界和商业领域。你可能已经听说过深度学习和神经网络。它们的实现和复杂性导致了更好的癌症检测、自动驾驶汽车、改进的灾害管理系统、更好的污染控制系统、减少交易欺诈等。
- en: In the third part of the book, we will explore unsupervised learning using deep
    learning. We will study what deep learning is and the basics of neural networks,
    as well as the layers in a neural network, activation functions, the process of
    deep learning, and various libraries. Then we will move to autoencoders and generative
    adversarial networks (GANs) and generative AI (GenAI). The topics are indeed complex
    and sometimes quite mathematically heavy. We will use different kinds of datasets
    for working on the problems, but primarily the datasets will be unstructured in
    nature. As always, Python will be used to generate the solutions. We also share
    a lot of external resources to complement the concepts. Please note that these
    are advanced topics, and a lot of research is still ongoing for these topics.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第三部分，我们将探讨使用深度学习进行无监督学习。我们将研究深度学习是什么，神经网络的基础，以及神经网络中的层、激活函数、深度学习的过程和各个库。然后我们将转向自编码器和生成对抗网络（GANs）以及生成人工智能（GenAI）。这些主题确实复杂，有时数学性相当强。我们将使用不同种类的数据集来解决问题，但主要数据集将是非结构化的。像往常一样，Python将用于生成解决方案。我们还分享了许多外部资源来补充这些概念。请注意，这些是高级主题，对这些主题的研究仍在进行中。
- en: We have divided the third part of the book into four chapters. This chapter
    covers the foundational concepts of deep learning and neural networks. The next
    two chapters focus on autoencoders, GAN and GenAI. The final chapter of the book
    talks about the deployment of these models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本书的第三部分分为四章。本章涵盖了深度学习和神经网络的基础概念。接下来的两章专注于自编码器、GAN和GenAI。本书的最后一章讨论了这些模型的部署。
- en: In this chapter, we discuss the concepts of neural networks and deep learning.
    We discuss what a neural network is, its activation functions, different optimization
    functions, the neural network training process, etc. The concepts covered in this
    chapter form the base of neural networks and deep learning and subsequent learning
    in the next two chapters. Hence, it is vital that you are clear about these concepts.
    The best external resources to learn these concepts in more detail are given at
    the end of the chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了神经网络和深度学习的概念。我们讨论了神经网络是什么，它的激活函数，不同的优化函数，神经网络训练过程等。本章涵盖的概念构成了神经网络和深度学习以及下一章后续学习的基础。因此，了解这些概念至关重要。在章节末尾提供了最佳外部资源，以更详细地学习这些概念。
- en: Welcome to the eighth chapter, and all the very best!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第八章，祝您一切顺利！
- en: 8.1 Technical toolkit
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 技术工具包
- en: 'We will continue to use the same version of Python and Jupyter Notebook as
    we have used so far. The codes and datasets used in this chapter have been checked
    in at the same GitHub location. You will need to install a couple of Python libraries
    in this chapter: `tensorflow` and `keras`.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用迄今为止所使用的相同版本的Python和Jupyter Notebook。本章中使用的代码和数据集已在相同的GitHub位置进行检查。你将需要在本章中安装几个Python库：`tensorflow`和`keras`。
- en: '8.1.1 Deep learning: What is it? What does it do?'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 深度学习：它是什么？它能做什么？
- en: Deep learning has gathered a lot of momentum in the past few years. Neural networks
    are pushing the boundaries of machine learning solutions. Deep learning is machine
    learning only. Deep learning is based on neural networks. It utilizes a similar
    concept—that is, using historical data and understanding the attributes and the
    intelligence gathered to find patterns or predict the future, albeit deep learning
    is more complex than the algorithms we have covered so far.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，深度学习积累了大量的动力。神经网络正在推动机器学习解决方案的边界。深度学习仅仅是机器学习。深度学习基于神经网络。它利用了类似的概念——即使用历史数据，理解收集到的属性和智能，以寻找模式或预测未来，尽管深度学习比我们迄今为止所覆盖的算法更复杂。
- en: Recall chapter 1, where we covered the concepts of structured and unstructured
    datasets. Unstructured datasets include text, images, audio, video, etc. Figure
    8.1 describes the major sources of text, images, audio, and video datasets.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第一章，我们介绍了结构化和非结构化数据集的概念。非结构化数据集包括文本、图像、音频、视频等。图8.1描述了文本、图像、音频和视频数据集的主要来源。
- en: '![figure](../Images/CH08_F01_Verdhan.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F01_Verdhan.png)'
- en: Figure 8.1 Unstructured datasets like text, audio, images, and video can be
    analyzed using deep learning. There are multiple sources of such datasets.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1 使用深度学习可以分析非结构化数据集，如文本、音频、图像和视频。此类数据集有多个来源。
- en: 'While deep learning can be implemented for structured datasets too, it is mostly
    working wonders on unstructured datasets. One of the prime reasons is that the
    classical machine learning algorithms are sometimes not that effective on unstructured
    datasets like that of images, text, audio, and video. A few of the path-breaking
    solutions delivered by deep learning across various domains are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习也可以用于结构化数据集，但它主要在非结构化数据集上产生了奇迹。其中一个主要原因是，经典机器学习算法有时对图像、文本、音频和视频等非结构化数据集的效果并不那么有效。深度学习在各个领域提供的突破性解决方案中的一些如下：
- en: '*The medical field and pharmaceuticals*—Deep learning sees application in areas
    such as the identification of bones and joint problems or in determining if there
    are any clots in arteries or veins. In the pharmaceutical field, it expedites
    clinical trials and helps to reach the target drug faster.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*医疗领域和制药业*—深度学习在识别骨骼和关节问题或确定动脉或静脉中是否有血栓等领域的应用。在制药领域，它加速了临床试验，并有助于更快地达到目标药物。'
- en: '*The banking and financial sector*—Deep learning-based algorithms are used
    to detect potential fraud in transactions. Using image recognition-based algorithms,
    we can also distinguish fake signatures on checks.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*银行和金融行业*—基于深度学习的算法用于检测交易中的潜在欺诈。通过基于图像识别的算法，我们还可以区分支票上的伪造签名。'
- en: '*The automobile sector*—You have probably heard about autonomous driving (aka
    self-driving) cars. Using deep learning, the algorithms can detect traffic signals,
    pedestrians, other vehicles on the road, their respective distances, and so on.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*汽车行业*—你可能听说过自动驾驶（也称为自动驾驶）汽车。通过深度学习，算法可以检测交通信号、行人、道路上的其他车辆，以及它们相应的距离等。'
- en: '*Retail*—In the retail sector, using deep learning-based algorithms, humans
    can improve customer targeting and develop advanced and customized marketing tactics.
    The recommended models to provide next-best products to the customers have been
    improved using deep learning. We can get better returns on investments and improve
    cross-sell and upsell strategies.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*零售*—在零售行业，通过使用基于深度学习的算法，人类可以提高客户定位并开发高级和定制化的营销策略。通过深度学习改进的推荐模型可以为客户提供更好的产品。我们可以获得更好的投资回报并改善交叉销售和升级销售策略。'
- en: In addition, automatic speech recognition is possible with deep learning. Using
    sophisticated neural networks, humans can create speech recognition algorithms.
    These solutions are being used across Siri, Alexa, Translator, Baidu, etc.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习还使得自动语音识别成为可能。通过复杂的神经网络，人类可以创建语音识别算法。这些解决方案正在被Siri、Alexa、翻译器、百度等广泛应用。
- en: 'Image recognition is also advancing. Neural networks are improving image recognition
    techniques. This can be done using convolutional neural networks, which are improving
    computer vision. Use cases include the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图像识别也在进步。神经网络正在改进图像识别技术。这可以通过卷积神经网络来实现，它们正在改善计算机视觉。用例包括以下内容：
- en: Deep learning is quite effective for differentiation between cancerous cells
    and benign cells. Differentiation can be achieved by using the images of cancerous
    cells and benign cells.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习在区分癌细胞和良性细胞方面非常有效。可以通过使用癌细胞和良性细胞的图像来实现区分。
- en: An automated number plate reading system has been developed using neural networks.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经开发了一个使用神经网络的自动车牌识别系统。
- en: Object detection methods and monitor sensing and tracking systems can be developed
    using deep learning.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用深度学习开发目标检测方法和监控传感与跟踪系统。
- en: In disaster management systems, deep learning can detect the presence of humans
    in affected areas. Just imagine how, during rescue operations, human lives can
    be saved using better detection.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在灾害管理系统，深度学习可以检测受影响区域的人类存在。想象一下，在救援行动中，如何通过更好的检测来拯救人类生命。
- en: GenAI is changing the world rapidly. Use cases include automating content creation,
    such as writing articles, essays, and social media posts and generating images
    and videos. It improves customer service and customer experience by providing
    chatbots that provide instant, personalized responses to the queries of the customers.
    It can be implemented in any industry. In data-heavy industries, it creates ripples
    by summarizing complex and long documents and generating insights from dashboards
    and reports. These reports can be Power BI/Tableau dashboards, PowerPoints, or
    pdf files, for example. It has also helped software developers in code generation
    and debugging and has improved software development efficiency. The use cases
    are many, ranging from retail; telecommunications; healthcare; R&D; banking, finance,
    and insurance, etc., in improving sales, reducing costs, saving time, and improving
    accuracy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通用人工智能（GenAI）正在迅速改变世界。用例包括自动化内容创作，如撰写文章、论文和社交媒体帖子，以及生成图像和视频。它通过提供即时、个性化的聊天机器人来响应客户的查询，从而改善客户服务和客户体验。它可以应用于任何行业。在数据密集型行业，它通过总结复杂和冗长的文档，并从仪表板和报告中生成见解，产生涟漪效应。例如，这些报告可以是Power
    BI/Tableau仪表板、PowerPoint或pdf文件。它还帮助软件开发者在代码生成和调试中，并提高了软件开发效率。用例很多，从零售；电信；医疗保健；研发；银行、金融和保险等，在提高销售额、降低成本、节省时间和提高准确性方面。
- en: The use cases listed are certainly not exhaustive. Using deep learning, we can
    improve natural language processing solutions used to measure customers’ sentiments,
    language translation, text classification, named-entity recognition, etc. Across
    use cases in bioinformatics, the military, mobile advertising, technology, the
    supply chain, and so on, deep learning is paving the path for the future.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列出的用例当然不是详尽的。使用深度学习，我们可以改进用于衡量客户情绪、语言翻译、文本分类、命名实体识别等自然语言处理解决方案。在生物信息学、军事、移动广告、技术、供应链等领域，深度学习正在为未来铺平道路。
- en: 8.2 Building blocks of a neural network
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 神经网络的基本构建块
- en: Artificial neural networks (ANNs) are said to be inspired by the way the human
    brain works. The human brain is the best machine we currently have access to.
    When we see a picture or a face or hear a tune, we associate a label or a name
    with it. That allows us to train our brain and senses to recognize a picture or
    a face or a tune when we see/hear it again. ANNs learn to perform similar tasks
    by learning or getting trained.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）据说受到人类大脑工作方式的启发。人类大脑是我们目前能接触到的最佳机器。当我们看到一张图片或一张面孔或听到一段曲调时，我们会将其与一个标签或名称关联起来。这使我们能够训练我们的大脑和感官，以便在再次看到/听到时识别图片或面孔或曲调。ANNs通过学习或接受训练来学习执行类似任务。
- en: Exercise 8.1
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 8.1
- en: 'Answer these questions to check your understanding:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: What is the meaning of deep learning?
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度学习的意义是什么？
- en: Neural networks cannot be used for unsupervised learning. True or False?
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络不能用于无监督学习。对还是错？
- en: Explore more use cases for deep learning in nonconventional business domains.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索在非常规商业领域深度学习的更多用例。
- en: 8.2.1 Neural networks for solutions
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 用于解决方案的神经网络
- en: 'In deep learning, too, the concepts of supervised and unsupervised learning
    are applicable. We cover both types of training of the network: supervised and
    unsupervised. This will give you a complete picture. At the same time, to fully
    appreciate unsupervised deep learning, you should be clear on the supervised deep
    learning process.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，监督学习和无监督学习的概念同样适用。我们将涵盖网络的两种训练类型：监督和无监督。这将为您提供一个完整的图景。同时，为了充分理解无监督深度学习，您应该对监督深度学习过程有清晰的认识。
- en: 'Let’s understand the deep learning process by using an example. Consider this:
    we wish to create a solution that can identify faces—a solution that can distinguish
    faces and identify the person by allocating a name to the face. For training the
    model, we will use a dataset that will have images of people’s faces and corresponding
    names. The ANN will start with no prior understanding of the image’s dataset or
    the attributes. During the process of training, it will learn the attributes and
    the identification characteristics from the training data. These learned attributes
    are then used to distinguish between faces. At this moment, we are only covering
    the process at a high level; we will cover this process in much more detail in
    subsequent sections. Figure 8.2 shows a representation of a neural network.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解深度学习过程。考虑这种情况：我们希望创建一个可以识别面部——一个可以通过为面部分配名字来区分面部并识别人的解决方案。为了训练模型，我们将使用一个包含人脸图像和相应名字的数据集。人工神经网络（ANN）将从对图像数据集或属性的无先验理解开始。在训练过程中，它将从训练数据中学习属性和识别特征。这些学到的属性随后被用来区分面部。在此阶段，我们只覆盖了这一过程的高级概述；我们将在后续章节中更详细地介绍这一过程。图8.2展示了神经网络的表示。
- en: '![figure](../Images/CH08_F02_Verdhan.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F02_Verdhan.png)'
- en: Figure 8.2 A typical neural network with neurons and various layers
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2 一个典型的神经网络，包含神经元和各个层次
- en: 'The process in a neural network is quite complex. We will first cover all the
    building blocks of a neural network, like neurons, activation functions, weights,
    bias terms, etc., and then move on to the process followed in a neural network.
    Let’s start with the protagonist: a neuron.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的过程相当复杂。我们首先将涵盖神经网络的所有构建块，如神经元、激活函数、权重、偏置项等，然后继续讨论神经网络中的过程。让我们从主角开始：一个神经元。
- en: 8.2.2 Artificial neurons and perceptrons
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 人工神经元和感知器
- en: The human brain contains billions of neurons. The neurons are interconnected
    cells in our brains. These neurons receive signals, process them, and generate
    results. Artificial neurons are based on biological neurons only and can be considered
    simplified computational models of biological neurons.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑包含数十亿个神经元。这些神经元是我们大脑中相互连接的细胞。它们接收信号，处理它们，并生成结果。人工神经元仅基于生物神经元，可以被认为是生物神经元的简化计算模型。
- en: In 1943, researchers Warren McCullock and Walter Pitts proposed the concept
    of a simplified brain cell called the McCullock-Pitts neuron. It can be thought
    of as a simple logic gate with binary outputs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在1943年，研究人员沃伦·麦克洛克和沃尔特·皮茨提出了一个简化脑细胞的理念，称为麦克洛克-皮茨神经元。它可以被认为是一个具有二进制输出的简单逻辑门。
- en: The working methodology for artificial neurons is similar to that of biological
    neurons, albeit artificial neurons are far simpler than biological neurons. A
    perceptron is a mathematical model of a biological neuron. In the actual biological
    neurons, dendrites receive electrical signals from the axons of other neurons.
    In a perceptron, these electrical signals are represented as numerical values.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元的工作方法与生物神经元相似，尽管人工神经元远比生物神经元简单。感知器是生物神经元的数学模型。在实际的生物神经元中，树突从其他神经元的轴突接收电信号。在感知器中，这些电信号被表示为数值。
- en: The artificial neuron receives inputs from the previous neurons or can receive
    the input data. It then processes that input information and shares an output.
    The input can be the raw data or processed information from a preceding neuron.
    The neuron then combines the input with its own internal state, weighs them separately,
    and passes the output received through a nonlinear function to generate output.
    These nonlinear functions are also called activation functions (we will cover
    them later). You can think of an activation function as a mathematical function.
    A neuron can be represented as shown in figure 8.3.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元从前面的神经元接收输入或可以接收输入数据。然后它处理这些输入信息并共享输出。输入可以是原始数据或来自前一个神经元的处理信息。然后神经元将其输入与其自身的内部状态相结合，分别加权，并通过非线性函数传递接收到的输出以生成输出。这些非线性函数也称为激活函数（我们将在后面介绍）。你可以将激活函数视为一个数学函数。一个神经元可以表示如图
    8.3 所示。
- en: '![figure](../Images/CH08_F03_Verdhan.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F03_Verdhan.png)'
- en: Figure 8.3 A neuron gets the inputs, processes them using mathematical functions,
    and then generates the output.
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.3 一个神经元接收输入，使用数学函数处理它们，然后生成输出。
- en: In simpler terms, a neuron can be termed as a mathematical function that computes
    the weighted average of its input datasets; then this sum is passed through activation
    functions. The output of the neuron can then be the input to the next neuron,
    which will again process the input received. Let’s go a bit deeper.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，一个神经元可以称为一个计算其输入数据集加权平均的数学函数；然后这个总和通过激活函数。神经元的输出然后可以成为下一个神经元的输入，该神经元将再次处理接收到的输入。让我们进一步探讨。
- en: In a perceptron, each input value is multiplied by a factor called the *weight*.
    Biological neurons fire once the total strength of the input signals exceeds a
    certain threshold. A similar format is followed in a perceptron. In a perceptron,
    a weighted sum of the inputs is calculated to get the total strength of the input
    data, and then an activation function is applied to each of the outputs. Each
    output can then be fed to the next layer of perceptron.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知器中，每个输入值都乘以一个称为 *权重* 的因子。生物神经元在输入信号的总强度超过一定阈值时才会触发。感知器遵循类似的格式。在感知器中，计算输入的加权总和以获得输入数据的总强度，然后对每个输出应用激活函数。然后每个输出可以馈送到下一个感知器层。
- en: 'Let’s assume that there are two input values, *a* and *b*, for a perceptron
    *X*, which for the sake of simplicity has only one output. Let the respective
    weights for *a* and *b* be *P* and *Q*. So the weighted sum can be calculated
    as *P* * *X* + *Q* * *b*. The perceptron will fire or will have a nonzero output
    only if the weighted sum exceeds a certain threshold. Let’s call the threshold
    *C*. So, we can say the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个感知器 *X* 的两个输入值，*a* 和 *b*，为了简化，它只有一个输出。设 *a* 和 *b* 的相应权重为 *P* 和 *Q*。因此，加权总和可以计算为
    *P* * *X* + *Q* * *b*。只有当加权总和超过某个特定阈值时，感知器才会触发或产生非零输出。我们可以称这个阈值为 *C*。因此，我们可以这样说：
- en: The output of *X* will be 0 if *P* * *X* + *Q* * *y* <= *C*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *P* * *X* + *Q* * *y* <= *C* 时，*X* 的输出将为 0。
- en: The output of *X* will be 1 if *P* * *S* + *Q* * *y* > *C*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *P* * *S* + *Q* * *y* > *C*，则 *X* 的输出将为 1。
- en: 'If we generalize this understanding, we can represent it as follows. Representing
    a perceptron as a function maps input *x* as the function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这种理解进行推广，我们可以将其表示如下。将感知器表示为一个函数，它将输入 *x* 映射为以下函数：
- en: '![figure](../Images/verdhan-ch8-eqs-0x.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-0x.png)'
- en: where *x* is the vector of input values, *w* represents the vector of weights,
    and *b* is the bias term. We explain the bias and the weight terms next.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 是输入值的向量，*w* 代表权重的向量，*b* 是偏置项。我们将在下面解释偏置和权重项。
- en: 'Recall the linear equation: *y* = *mx* + *c* where *m* is the slope of the
    straight line and *c* is the constant term. Both bias and weight can be defined
    using the same linear equation.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下线性方程：*y* = *mx* + *c*，其中 *m* 是直线的斜率，*c* 是常数项。偏置和权重都可以使用相同的线性方程来定义。
- en: The role of weight is similar to the slope of the line in a linear equation.
    It defines the change in the value of *f*(*x*) by a unit change in the value of
    *x*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的作用类似于线性方程中直线的斜率。它定义了 *f*(*x*) 的值随 *x* 值的单位变化而变化。
- en: The role of the bias is similar to the role of a constant in a linear function.
    In case there is no bias, the input to the activation function is *x* multiplied
    by the weight.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置的作用类似于线性函数中常数的作用。如果没有偏置，激活函数的输入是 *x* 乘以权重。
- en: NOTE  Weights and bias terms are the parameters that get trained in a network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：权重和偏差项是网络中需要训练的参数。
- en: The output of the function will depend on the activation function used. We will
    cover various types of activation functions in the next section after we have
    covered different layers in a network.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输出将取决于所使用的激活函数。在介绍完网络中的不同层之后，我们将在下一节中介绍各种类型的激活函数。
- en: 8.2.3 Different layers in a network
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 网络中的不同层
- en: A simple and effective way of organizing neurons is the following. Rather than
    allowing arbitrary neurons connected with arbitrary others, neurons are organized
    in layers. A neuron in a layer has all its inputs coming only from the previous
    layer and all its output going only to the next. There are no other connections,
    for example, between neurons of the same layer or between neurons in neurons belonging
    to distant layers (with a small exception for specialized cases, which is beyond
    the scope of this book).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 组织神经元的简单有效方法是以下所述。而不是允许任意神经元与任意其他神经元连接，神经元被组织成层。一个层的神经元所有输入都只来自前一层，所有输出都只流向下一层。没有其他连接，例如，同一层的神经元之间或属于不同层的神经元之间的连接（对于特殊情况有小的例外，这超出了本书的范围）。
- en: We know that information flows through a neural network. That information is
    processed and passed on from one layer to another layer in a network. There are
    three layers in a neural network, as shown in figure 8.4\.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道信息通过神经网络流动。这些信息在网络中从一层传递到另一层进行处理。神经网络中有三层，如图 8.4 所示。
- en: '![figure](../Images/CH08_F04_Verdhan.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F04_Verdhan.png)'
- en: Figure 8.4 A typical neural network with neurons and input, hidden, and output
    layers
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4 一个典型的神经网络，包含神经元和输入、隐藏和输出层
- en: 'The neural network shown in figure 8.4 has three input units and two hidden
    layers with four neurons each and one final output layer:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 中所示的神经网络有三个输入单元，两个包含四个神经元的隐藏层和一个最终的输出层：
- en: '*Input layer*—As the name signifies, this receives the input data and shares
    it with the hidden layers.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入层*—正如其名所示，它接收输入数据并将其与隐藏层共享。'
- en: '*Hidden layer*—This is the heart and soul of the network. The number of hidden
    layers depends on the problem at hand; the number of layers can range from a few
    to hundreds. All the processing, feature extraction, and learning of the attributes
    is done in these layers. In the hidden layers, all the input raw data is broken
    into attributes and features. This learning is useful for decision-making at a
    later stage.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐藏层*—这是网络的核心和灵魂。隐藏层的数量取决于具体问题；层数可以从几个到几百个不等。所有的处理、特征提取和属性学习都在这些层中完成。在隐藏层中，所有输入的原始数据都被分解为属性和特征。这种学习对后续的决策阶段是有用的。'
- en: '*Output layer*—This is the decision layer and final piece in a network. It
    accepts the outputs from the preceding hidden layers and then makes a prediction.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出层*—这是网络中的决策层和最后一部分。它接受前面隐藏层的输出，然后做出预测。'
- en: For example, the input training data may have raw images or processed images.
    These images will be fed to the input layer. The data then travels to the hidden
    layers where all the calculations are done. These calculations are done by neurons
    in each layer. The output is the task that needs to be accomplished—for example,
    identification of an object or classification of an image, etc.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，输入训练数据可能包含原始图像或处理后的图像。这些图像将被馈送到输入层。然后数据传递到隐藏层，所有计算都在这里完成。这些计算是由每一层的神经元完成的。输出是需要完成的任务——例如，识别一个对象或对图像进行分类等。
- en: The ANN consists of various connections. Each of the connections aims to receive
    the input and provide the output to the next neuron. This output to the next neuron
    will serve as an input to it. Also, as discussed earlier, each connection is assigned
    a weight, which is representative of its respective importance. It is important
    to note that a neuron can have multiple input and output connections, which means
    it can receive inputs and deliver multiple outputs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ANN 由各种连接组成。每个连接的目的是接收输入并为下一个神经元提供输出。这个输出将作为下一个神经元的输入。此外，如前所述，每个连接都被分配了一个权重，这代表了它各自的重要性。需要注意的是，一个神经元可以有多个输入和输出连接，这意味着它可以接收输入并产生多个输出。
- en: Exercise 8.2
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 8.2
- en: 'Answer these questions to check your understanding:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题以检查你的理解：
- en: The input data is fed to the hidden layers in a neural network. True or False?
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据被馈送到神经网络的隐藏层。对或错？
- en: A bias term is similar to the slope of a linear equation. True or False?
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏置项类似于线性方程的斜率。对或错？
- en: Find and explore the deepest neural network ever trained.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 寻找并探索训练过的最深的神经网络。
- en: So what is the role of a layer? A layer receives inputs, processes them, and
    passes the output to the next layer. Technically, it is imperative that the transformation
    implemented by a layer is parameterized by its weights, which are also referred
    to as parameters of a layer. In simple terms, to ensure a neural network is “trained”
    to a specific task, something must be changed in the network. It turns out that
    changing the architecture of the network (i.e., how neurons are connected) has
    only a small effect. On the other hand, as we will see later in this chapter,
    changing the weights is the key to the “learning” process.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，层的作用是什么？层接收输入，处理它们，并将输出传递到下一层。技术上讲，层的实现必须由其权重参数化，这些权重也被称为层的参数。简单来说，为了确保神经网络“训练”到特定任务，网络中必须有所改变。结果证明，改变网络的架构（即神经元如何连接）只有微小的影响。另一方面，正如我们将在本章后面看到的那样，改变权重是“学习”过程的关键。
- en: We now move to the very important topic of activation functions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向非常重要的激活函数主题。
- en: 8.2.4 Activation functions
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.4 激活函数
- en: We have already mentioned activation functions. The primary role of an activation
    function is to decide whether a neuron/perceptron should fire or not. These functions
    play a central role in the training of the network at a later stage. They are
    sometimes referred to as *transfer functions*. It is also important to know why
    we need nonlinear activation functions. If we use only linear activation functions,
    the output will also be linear. At the same time, the derivative of a linear function
    will be constant. Hence, there will not be much learning possible. Thus, we prefer
    to have nonlinear activation functions. We study the most common activation functions
    next.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了激活函数。激活函数的主要作用是决定神经元/感知器是否应该激活。这些函数在网络训练的后期阶段扮演着核心角色。它们有时也被称为 *传递函数*。了解为什么我们需要非线性激活函数也很重要。如果我们只使用线性激活函数，输出也将是线性的。同时，线性函数的导数将是常数。因此，将不会有太多的学习。因此，我们更喜欢非线性激活函数。我们接下来研究最常见的激活函数。
- en: Sigmoid function
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: A sigmoid is a bounded monotonic mathematical function. It always increases
    its output value when the input values increase. Its output value is always between
    –1 and 1.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 是一个有界的单调数学函数。当输入值增加时，它总是增加其输出值。其输出值总是在 -1 和 1 之间。
- en: A sigmoid is a differentiable function with an S-shaped curve, and its first
    derivative function is bell-shaped. It has a nonnegative derivative function and
    is defined for all real input values. The sigmoid function is used if the output
    value of a neuron is between 0 and 1\.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数是一个可微分的 S 形曲线函数，其第一导数函数呈钟形。它具有非负导数函数，并且对所有实数输入值都有定义。当神经元的输出值在 0 到
    1 之间时，会使用 Sigmoid 函数。
- en: 'Mathematically, a sigmoid function can be represented by equation 8.1:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，Sigmoid 函数可以用方程 8.1 表示：
- en: '![figure](../Images/verdhan-ch8-eqs-1x.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-1x.png)'
- en: (8.1)
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (8.1)
- en: Figure 8.5 shows a graph of a sigmoid function. The sigmoid function finds its
    applications in complex learning systems. It is usually used for binary classification
    and in the final output layer of the network.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 展示了 Sigmoid 函数的图形。Sigmoid 函数在复杂学习系统中找到其应用。它通常用于二元分类和网络的最終输出层。
- en: '![figure](../Images/CH08_F05_Verdhan.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F05_Verdhan.png)'
- en: Figure 8.5 A sigmoid function. Note the shape of the function and the min/max
    values.
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.5 Sigmoid 函数。注意函数的形状和最小/最大值。
- en: TANH function
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TANH 函数
- en: In mathematics, the tangent hyperbolic (TANH) function is a differentiable hyperbolic
    function. It is a smooth function, and its input values are in the range of –1
    to +1.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，双曲正切（TANH）函数是一个可微分的双曲函数。它是一个平滑函数，其输入值在 -1 到 +1 的范围内。
- en: 'A TANH function is written as equation 8.2:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TANH 函数可以表示为方程 8.2：
- en: '![figure](../Images/verdhan-ch8-eqs-2x.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-2x.png)'
- en: (8.2)
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (8.2)
- en: A graphical representation of TANH is shown in figure 8.6\. It is a scaled version
    of the sigmoid function, and hence a TANH function can be derived from a sigmoid
    function and vice versa.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TANH 的图形表示如图 8.6 所示。它是 Sigmoid 函数的缩放版本，因此可以从 Sigmoid 函数导出 TANH 函数，反之亦然。
- en: '![figure](../Images/CH08_F06_Verdhan.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F06_Verdhan.png)'
- en: Figure 8.6 A TAHN function, which is a scaled version of a sigmoid function
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6 TANH函数，它是sigmoid函数的缩放版本
- en: A TANH function is generally used in the hidden layers. It makes the mean closer
    to zero, which makes the training easier for the next layer in the network. This
    is also referred to as centering the data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: TANH函数通常用于隐藏层。它使均值更接近零，这使得网络中下一层的训练更容易。这也被称为数据居中。
- en: Rectified linear unit
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩形线性单元
- en: A rectified linear unit (ReLU) is an activation function that defines the positives
    of an argument. Equation 8.3 shows the ReLU function. Note that the value is 0
    even for the negative values, and from 0 the value starts to incline.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 矩形线性单元（ReLU）是一个定义参数正值的激活函数。方程8.3展示了ReLU函数。请注意，即使是负值，其值也为0，并且从0开始值开始上升。
- en: (8.3)
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (8.3)
- en: '*F*(*x*) = max (0, *x*)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*F*(*x*) = max (0, *x*)'
- en: It will give the output as *x* if positive, else 0\.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入为正，则输出为*x*，否则为0。
- en: The ReLU is a simple function and hence less expensive to compute and much faster.
    It is unbounded and not centered at zero. It can be differentiated at all places
    except zero. Since the ReLU function is less complex, it is computationally less
    expensive and, hence, is widely used in the hidden layers to train the networks
    faster. Figure 8.7 is a graphical representation of a ReLU function.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU是一个简单的函数，因此计算成本较低，速度更快。它是无界的，且不在零点居中。除了零点外，它可以在所有地方进行微分。由于ReLU函数较为简单，计算成本较低，因此广泛用于隐藏层以加速网络训练。图8.7是ReLU函数的图形表示。
- en: '![figure](../Images/CH08_F07_Verdhan.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F07_Verdhan.png)'
- en: Figure 8.7 A ReLU function. It is one of the favored activation functions in
    the hidden layers of a neural network. A ReLU is simple to use and less expensive
    to train.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7 ReLU函数。它是神经网络隐藏层中首选的激活函数之一。ReLU简单易用，训练成本低。
- en: Softmax function
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Softmax函数
- en: The softmax function is used in the final layer of the neural network to generate
    the output from the network. It is an activation function that is useful for multiclass
    classification problems and forces the neural network to output the sum of 1\.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax函数用于神经网络的最外层，以生成网络的输出。它是一个对多类分类问题有用的激活函数，并迫使神经网络输出总和为1。
- en: As an example, say the distinct classes for an image are cars, bikes, or trucks.
    The softmax function will generate three probabilities for each category. The
    category that has received the highest probability will be the predicted category.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设图像的不同类别为汽车、自行车或卡车。Softmax函数将为每个类别生成三个概率。具有最高概率的类别将被预测为类别。
- en: There are other activation functions too, like ELU, PeLU, etc., which are beyond
    the scope of this book. We provide a summary of various activation functions at
    the end of this chapter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他激活函数，如ELU、PeLU等，这些超出了本书的范围。我们在本章末尾提供了各种激活函数的总结。
- en: We next cover hyperparameters, which are the control levers we have while the
    network is trained.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍超参数，这些是我们网络训练过程中的控制杠杆。
- en: 8.2.5 Hyperparameters
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.5 超参数
- en: During training a network, the algorithm is constantly learning the attributes
    of the raw input data. At the same time, the network cannot learn everything itself;
    there are a few parameters for which initial settings must be provided. These
    are the variables that determine the structure of the neural network and the respective
    variables that are useful to train the network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络的过程中，算法不断学习原始输入数据的属性。同时，网络不能自己学习所有内容；对于一些参数，必须提供初始设置。这些是确定神经网络结构和有助于训练网络的相应变量。
- en: A few examples of hyperparameters are the number of hidden layers in a network,
    the number of neurons in each layer, the activation functions used in layers,
    weight initialization, etc. We have to pick the best values of the hyperparameters.
    To do so, we select some reasonable values for the hyperparameters, train the
    network, measure the performance of the network, tweak the hyperparameters and
    retrain the network, reevaluate and retweak, and so on.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数的一些例子包括网络中的隐藏层数量、每层的神经元数量、层中使用的激活函数、权重初始化等。我们必须选择超参数的最佳值。为此，我们为超参数选择一些合理的值，训练网络，测量网络性能，调整超参数并重新训练网络，重新评估并再次调整，如此循环。
- en: NOTE  Hyperparameters are controlled by us, as we input hyperparameters to improve
    the performance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：超参数由我们控制，因为我们输入超参数以提高性能。
- en: 'We now move to the next important component in a neural network: optimization
    functions.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向神经网络中下一个重要组件：优化函数。
- en: 8.2.6 Optimization functions
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.6 优化函数
- en: In deep learning, optimizers play a critical role. They minimize the loss function
    by adjusting the model parameters, which are weights and biases. The optimizers
    facilitate faster convergence and improve the overall performance of the network.
    Some of the most commonly used optimization functions are discussed next.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，优化器起着关键作用。它们通过调整模型参数（权重和偏差）来最小化损失函数，从而加速收敛并提高网络的总体性能。以下将讨论一些最常用的优化函数。
- en: Batch gradient descent, stochastic gradient descent, and mini-batch stochastic
    gradient descent
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量梯度下降、随机梯度下降和迷你批随机梯度下降
- en: In any prediction-based solution, we want to predict as best as we can; or,
    in other words, we want to reduce the error as much as possible. Error is the
    difference between the actual values and the predicted values. The purpose of
    a machine learning solution is to find the optimum value for our functions. We
    want to decrease the error or maximize the accuracy. Gradient descent can help
    to achieve this purpose.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何基于预测的解决方案中，我们希望尽可能准确地预测；换句话说，我们希望尽可能减少误差。误差是实际值与预测值之间的差异。机器学习解决方案的目的是找到我们函数的最优值。我们希望减少误差或最大化准确性。梯度下降可以帮助实现这一目的。
- en: The batch gradient descent technique is an optimization technique used to find
    the global minima of a function. We proceed in the direction of the steepest descent
    iteratively, which is defined by the negative of the gradient.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降技术是一种用于寻找函数全局最小值的优化技术。我们迭代地朝着最陡下降方向前进，这由梯度的负值定义。
- en: But batch gradient descent can be slow to run on very large datasets or datasets
    with a very high number of dimensions. This is due to the fact that one iteration
    of the gradient descent algorithm predicts for every instance in the training
    dataset. Hence, it is obvious that it will take a lot of time if we have thousands
    of records. For such a situation, we have stochastic gradient descent (SGD).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，批量梯度下降在处理非常大的数据集或具有非常高维度的数据集时可能运行缓慢。这是因为梯度下降算法的一次迭代会预测训练数据集中的每个实例。因此，如果我们有数千条记录，很明显这将花费很多时间。对于这种情况，我们有随机梯度下降（SGD）。
- en: In SGD, rather than at the end of the batch of the data, the coefficients are
    updated for each training instance, and hence it takes less time.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在SGD中，而不是在数据批次的末尾，系数会为每个训练实例更新，因此它花费的时间更少。
- en: Figure 8.8 shows the way gradient descent works. Notice how we can progress
    downward toward the global minimum.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8展示了梯度下降的工作方式。注意我们如何可以向下朝着全局最小值前进。
- en: '![figure](../Images/CH08_F08_Verdhan.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F08_Verdhan.png)'
- en: Figure 8.8 The concept of gradient descent. It is the mechanism to minimize
    the loss function.
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8 梯度下降的概念。它是最小化损失函数的机制。
- en: Mini-batch gradient descent batches gradient descent and SGD by using small
    subsets of data. They are called mini-batches. In this fashion, it can balance
    both speed and accuracy. At the same time, it adds a hyperparameter, and we have
    to carefully tune the batch size. Generally, it is kept in the power of 2 (32,
    64, 128, 256, etc.).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 迷你批梯度下降通过使用数据的小子集将批量梯度下降和SGD结合起来。它们被称为迷你批。以这种方式，它可以平衡速度和准确性。同时，它增加了一个超参数，我们必须仔细调整批大小。通常，它保持在2的幂次方（32、64、128、256等）。
- en: Adaptive optimization algorithms
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自适应优化算法
- en: 'Researchers have observed that there is a need for optimization algorithms
    for more complex tasks like image, text, video, or audio analysis. Hence, adaptive
    optimization solutions like momentum, Nesterov accelerated gradient (NAG), Adagrad,
    etc., have been developed. We provide a brief summary of these solutions:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员观察到，对于图像、文本、视频或音频分析等更复杂的任务，需要优化算法。因此，开发了如动量、Nesterov加速梯度（NAG）、Adagrad等自适应优化解决方案。我们简要总结这些解决方案：
- en: '*Momentum*—This optimizer adds a fraction of the previous gradient to the current
    gradient. The idea is to give more weight to the most recent update as compared
    to the previous updates. It accelerates the convergence and achieves better accuracy'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动量*—这个优化器将前一次梯度的部分加到当前梯度上。这种想法是相对于前一次更新，给予最近更新更多的权重。它加速了收敛并实现了更好的准确性'
- en: '![figure](../Images/verdhan-ch8-eqs-4x.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-4x.png)'
- en: and hence the weights are updated by *θ* = *θ* – *V*(*t*).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，权重通过*θ* = *θ* – *V*(*t*)更新。
- en: Generally, the value of the momentum term (*γ*) is set to 0.9\. With momentum,
    the convergence is faster, but at the same time, we must compute one more variable
    for each update.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，动量项（*γ*）的值设置为0.9。有了动量，收敛会更快，但与此同时，我们必须为每个更新计算一个额外的变量。
- en: '*NAG*—This is an improvement over momentum. In momentum, if the value becomes
    too large, the optimizer might miss the local minima. Hence, NAG was developed.
    It is a look-ahead method wherein the weights are modified to determine the future
    location.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NAG*——这是对动量的改进。在动量中，如果值变得太大，优化器可能会错过局部最小值。因此，开发了NAG。这是一种前瞻性方法，其中权重被修改以确定未来的位置。'
- en: Next, we discuss the most widely used optimization algorithms in the industry.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论工业界最广泛使用的优化算法。
- en: Learning and learning rate
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习与学习率
- en: 'For a network, we take various steps to improve the performance of the solution:
    learning rate is one of them. The learning rate will define the size of the corrective
    steps that a model takes to reduce the errors. Learning rate defines the amount
    by which we should adjust the values of weights of the network with respect to
    the loss gradients (more on this process later). If we have a higher learning
    rate, the accuracy will be lower. If we have a very low learning rate, the training
    time will increase.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个网络，我们采取各种步骤来提高解决方案的性能：学习率是其中之一。学习率将定义模型为减少错误所采取的纠正步骤的大小。学习率定义了我们应该调整网络权重的值相对于损失梯度的量（关于这个过程后面会详细介绍）。如果我们有一个较高的学习率，准确性会较低。如果我们有一个非常低的学习率，训练时间会增加。
- en: Exercise 8.3
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习8.3
- en: 'Answer these questions to check your understanding:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: Compare and contrast the sigmoid and TANH functions.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较和对比sigmoid和TANH函数。
- en: ReLU is generally used in the output layer of the network. True or False?
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ReLU通常用于网络的输出层。对还是错？
- en: Gradient descent is an optimization technique. True or False?
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度下降是一种优化技术。对还是错？
- en: We have examined the main concepts of deep learning. Now let us study how a
    neural network works. You will learn how the various layers interact with each
    other and how information is passed from one layer to another.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了深度学习的主要概念。现在让我们研究神经网络是如何工作的。你将了解各个层如何相互作用以及信息是如何从一个层传递到另一个层的。
- en: 8.3 How does deep learning work in a supervised manner?
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 深度学习在监督模式下是如何工作的？
- en: We have covered the major components of a neural network. It is the time for
    all the pieces to come together and orchestrate the entire learning process. The
    training of a neural network is quite a complex process and can be examined in
    a step-by-step fashion.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了神经网络的主要组成部分。现在是时候让所有部件汇集在一起，协调整个学习过程了。神经网络的训练是一个相当复杂的过程，可以逐步检查。
- en: You might be wondering what is meant by “learning” of a neural network. Learning
    is a process to find the best and most optimized values for weights and bias for
    all the layers of the network so that we can achieve the best accuracy. As deep
    neural networks can have practically infinite possibilities for weights and bias
    terms, we have to find the optimum value for all the parameters. This seems like
    a herculean task considering that changing one value affects the other values,
    and indeed, it is a process where the various parameters of the networks are changing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道“神经网络的学习”是什么意思。学习是一个寻找网络所有层中权重和偏置的最佳和最优化值的过程，以便我们可以达到最佳的准确性。由于深度神经网络在权重和偏置项上几乎有无限的可能性，我们必须找到所有参数的最优值。考虑到改变一个值会影响其他值，这似乎是一项艰巨的任务，而实际上，它是一个网络的各种参数不断变化的过程。
- en: Recall in the first chapter we covered the basics of supervised learning. We
    will refresh that understanding here. The reason is to ensure that you are fully
    able to appreciate the process of training the neural network.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在第一章我们介绍了监督学习的基础。在这里我们将刷新这一理解。原因是确保你能够完全欣赏神经网络训练的过程。
- en: 8.3.1 Supervised learning algorithms
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 监督学习算法
- en: Supervised learning algorithms have a “guidance” or “supervision” to direct
    toward the business goal of making predictions for the future. Formally put, supervised
    models are statistical models that use both the input data and the desired output
    to predict the future. The output is the value we wish to predict and is referred
    to as the *target variable,* and the data used to make that prediction is called
    the *training data*. The target variable is sometimes referred to as the *label*.
    The various attributes or variables present in the data are called *independent
    variables*. Each of the historical data points or *training examples* contain
    these independent variables and corresponding target variables. Supervised learning
    algorithms make a prediction for the unseen future data. The accuracy of the solution
    depends on the training done and patterns learned from the labeled historical
    data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法有一个“指导”或“监督”来指导向业务目标——对未来进行预测。正式地说，监督模型是使用输入数据和期望的输出来预测未来的统计模型。输出是我们希望预测的值，被称为*目标变量*，用于进行预测的数据被称为*训练数据*。目标变量有时也被称为*标签*。数据中存在的各种属性或变量被称为*自变量*。每个历史数据点或*训练示例*都包含这些自变量和相应的目标变量。监督学习算法对未见的未来数据进行预测。解决方案的准确性取决于训练和从标记的历史数据中学到的模式。
- en: NOTE  Most deep learning solutions are based on supervised learning. Unsupervised
    deep learning is rapidly gaining traction, however, as unlabeled datasets are
    far more abundant than labeled ones.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：大多数深度学习解决方案都是基于监督学习的。然而，无监督深度学习正在迅速获得关注，因为未标记的数据集比标记的数据集要多得多。
- en: Supervised learning problems are used in demand prediction, credit card fraud
    detection, customer churn prediction, premium estimation, etc. They are heavily
    used across retail, telecom, banking and finance, aviation, insurance, and other
    fields.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题用于需求预测、信用卡欺诈检测、客户流失预测、保费估算等。它们在零售、电信、银行和金融、航空、保险和其他领域被广泛使用。
- en: 'We have now refreshed the concepts of supervised learning. We now move on to
    the first step in the training of the neural network: feed-forward propagation.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经刷新了监督学习的概念。我们现在继续到神经网络训练的第一步：前向传播。
- en: '8.3.2 Step 1: Feed-forward propagation'
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 步骤1：前向传播
- en: Let us start the process that occurs in a neural network (see figure 8.9). This
    is the basic skeleton of a network we have created to explain the process. Let’s
    say we have some input data points and the input data layer, which will consume
    the input data. The information flows from the input layer to the data transformation
    layers (hidden layers). In the hidden layers, the data is processed using the
    activation functions and based on the weights and bias terms. Then a prediction
    is made on the dataset. This is called *feed-forward propagation,* as during this
    process, the input variables are calculated in a sequence from the input layer
    to the output layer.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始一个神经网络中发生的过程（见图8.9）。这是我们创建的基本网络骨架，用于解释这个过程。假设我们有一些输入数据点和输入数据层，它将消耗输入数据。信息从输入层流向数据转换层（隐藏层）。在隐藏层中，数据通过激活函数进行处理，并基于权重和偏置项。然后对数据集进行预测。这被称为*前向传播*，因为在整个过程中，输入变量是按照从输入层到输出层的顺序计算的。
- en: '![figure](../Images/CH08_F09_Verdhan.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F09_Verdhan.png)'
- en: Figure 8.9 The basic skeleton of a neural network training process. We have
    the input layers and data transformation layers.
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9 神经网络训练过程的基本骨架。我们有输入层和数据转换层。
- en: For example, say we wish to create a solution that can identify the faces of
    people. In this case, we will have the training data, which is different images
    of people’s faces from various angles, and a target variable, which is the name
    of the person.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们希望创建一个可以识别人脸的解决方案。在这种情况下，我们将有训练数据，即从不同角度拍摄的人脸的不同图像，以及一个目标变量，即人的名字。
- en: This training dataset can be fed to the algorithm. The algorithm will then understand
    the attributes of various faces or, in other words, *learn* the various attributes.
    Based on the training done, the algorithm can then make a prediction on the faces.
    The prediction will be a probability score if the face belongs to Mr. X. If the
    probability is high enough, we can safely say that the face belongs to Mr. X.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练数据集可以输入到算法中。然后，算法将理解各种面部特征，换句话说，就是*学习*这些特征。基于所做的训练，算法可以对面部进行预测。如果预测是Mr.
    X的面部，预测将是一个概率分数。如果概率足够高，我们可以安全地说这个面部属于Mr. X。
- en: '8.3.3 Step 2: Adding the loss function'
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 步骤2：添加损失函数
- en: The output is generated in step 1\. Now we have to gauge the accuracy of this
    network. We want our network to have the best possible accuracy in identifying
    the faces. Using the prediction made by the algorithm, we will control and improve
    the accuracy of the network.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出在第一步生成。现在我们必须评估这个网络的准确性。我们希望我们的网络在识别面部时具有尽可能高的准确性。使用算法做出的预测，我们将控制和提高网络的准确性。
- en: Accuracy measurement in the network can be achieved by the loss function, also
    called the *objective function*. The loss function compares the actual values
    and the predicted values. The loss function computes the difference score and
    hence is able to measure how well the network has done and what the error rates
    are. Let’s update the diagram we created in step 1 by adding a loss function and
    corresponding loss score, used to measure the accuracy of the network, as shown
    in figure 8.10.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的准确性测量可以通过损失函数实现，也称为*目标函数*。损失函数比较实际值和预测值。损失函数计算差异分数，因此能够衡量网络做得有多好以及误差率是多少。让我们通过添加一个损失函数和相应的损失分数来更新我们在第一步创建的图表，如图8.10所示。
- en: '![figure](../Images/CH08_F10_Verdhan.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH08_F10_Verdhan.png)'
- en: Figure 8.10 A loss function has been added to measure the accuracy.
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.10 已添加损失函数来衡量准确性。
- en: '8.3.4 Step 3: Calculating the error'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 步骤3：计算误差
- en: We generated the predictions in step 1 of the network. In step 2, we compared
    the output with the actual values to get the error in prediction. The objective
    of our solution is to minimize this error, which is the same as maximizing the
    accuracy.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在网络的第一步生成了预测。在第二步，我们将输出与实际值进行比较，以获得预测误差。我们解决方案的目标是使这个误差最小化，这等同于最大化准确性。
- en: To constantly lower the error, the loss score (Predictions – Actual) is then
    used as feedback to adjust the value of the weights. This task is done by the
    backpropagation algorithm.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不断降低误差，损失分数（预测值 - 实际值）随后被用作反馈来调整权重值。这个任务由反向传播算法完成。
- en: 8.4 Backpropagation
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 反向传播
- en: In step 3 of the last section, we said we use an optimizer to constantly update
    the weights to reduce the error. While the learning rate defines the size of the
    corrective steps to reduce the error, backpropagation is used to adjust the connection
    weights. These weights are updated backward based on the error. Following this,
    the errors are recalculated, the gradient descent is calculated, and the respective
    weights are adjusted. Hence, backpropagation is sometimes called the central algorithm
    in deep learning.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节的第三步中，我们提到我们使用优化器不断更新权重以减少误差。虽然学习率定义了减少误差的校正步骤的大小，但反向传播用于调整连接权重。这些权重根据误差反向更新。随后，重新计算误差，计算梯度下降，并相应地调整权重。因此，反向传播有时被称为深度学习中的核心算法。
- en: Backpropagation was originally suggested in the 1970s. Then, in 1986, David
    Rumelhartm, Geoffrey Hinton, and Ronald Williams’s paper received a lot of appreciation.
    Nowadays, backpropagation is the backbone of deep learning solutions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播最初在20世纪70年代被提出。然后，在1986年，David Rumelhart、Geoffrey Hinton和Ronald Williams的论文受到了很多赞誉。如今，反向传播是深度学习解决方案的骨干。
- en: Figure 8.11 shows the process for backpropagation, where the information flows
    from the output layer back to the hidden layers. Note that the flow of information
    is backward as compared to forward propagation, where the information flows from
    left to right.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11显示了反向传播的过程，其中信息从输出层流向隐藏层。请注意，与正向传播相比，信息流是反向的，在正向传播中，信息是从左到右流动的。
- en: '![figure](../Images/CH08_F11_Verdhan.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH08_F11_Verdhan.png)'
- en: 'Figure 8.11 Backpropagation as a process: the information flows from the final
    layers to the initial layers'
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.11 反向传播作为过程：信息从最终层流向初始层
- en: First, we describe the process at a very high level. Remember that in step 1,
    at the start of the training process, some random values were assigned to the
    weights. Using these random values, an initial output is generated. Since this
    is the first attempt, the output received can be quite different from the real
    values and the loss score is accordingly very high. But this is going to improve.
    While training the neural network, the weights (and biases) are adjusted a little
    in the correct direction, and subsequently, the loss score decreases. We iterate
    this training loop many times, and it results in the optimum weight values that
    minimize the loss function.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们以非常高的层次描述这个过程。记住，在步骤 1 中，在训练过程的开始，一些随机值被分配给了权重。使用这些随机值，生成了一个初始输出。由于这是第一次尝试，接收到的输出可能与真实值有很大不同，损失分数因此非常高。但这种情况将会改善。在训练神经网络时，权重（和偏差）会朝着正确的方向进行微调，随后损失分数会降低。我们重复这个训练循环多次，最终得到最小化损失函数的最优权重值。
- en: NOTE  Backpropagation allows us to iteratively reduce the error during the network
    training process.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：反向传播使我们能够在网络训练过程中迭代地减少误差。
- en: The following section is mathematically heavy. If you are not keen to understand
    the mathematics behind the process, you can skip it.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的部分数学内容较多。如果你不热衷于理解这个过程背后的数学，你可以跳过这部分。
- en: 8.4.1 The mathematics behind backpropagation
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 反向传播背后的数学
- en: When we train a neural network, we calculate a loss function. The loss function
    tells us how different the predictions from the actual values are. Backpropagation
    calculates the gradient of the loss function with respect to each of the weights.
    With this information, each weight can be updated individually over iterations,
    which reduces the loss gradually.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练一个神经网络时，我们计算一个损失函数。损失函数告诉我们预测值与实际值之间的差异。反向传播计算损失函数相对于每个权重的梯度。有了这些信息，每个权重都可以在迭代中单独更新，这会逐渐减少损失。
- en: In backpropagation, the gradient is calculated backward—that is, from the last
    layer of the network through the hidden layers to the very first layer. The gradients
    of all the layers are combined using the calculus chain rule to get the gradient
    of any particular layer.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，梯度是向后计算的——也就是说，从网络的最后一层通过隐藏层到最第一层。所有层的梯度通过微积分链式法则组合起来，以得到任何特定层的梯度。
- en: 'We go into more details of the process next. First, let’s denote a few mathematical
    symbols:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来更详细地介绍这个过程。首先，让我们定义一些数学符号：
- en: '*h*^(^(*i*)^)—output of the hidden layer *i*'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h*^(^(*i*)^)——隐藏层 *i* 的输出'
- en: '*g*^(^(*i*)^)—activation function of hidden layer *i*'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*g*^(^(*i*)^)——隐藏层 *i* 的激活函数'
- en: '*w*^(^(*i*)^)—hidden weights matrix in the layer *i*'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*^(^(*i*)^)——层 *i* 中的隐藏权重矩阵'
- en: '*b*^(^(*i*)^)—bias in layer *i*'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*^(^(*i*)^)——层 *i* 中的偏差'
- en: '*x*—input vector'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*——输入向量'
- en: '*N*—total number of layers in the network'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*——网络中的总层数'
- en: '*W*^(^(*i*)^)[*jk*]—weight of the network from node *j* in layer (*i*–1) to
    node *k* in layer *i*'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*^(^(*i*)^)[*jk*]——网络从层 (*i*–1) 中的节点 *j* 到层 *i* 中的节点 *k* 的权重'
- en: '*δ**A*/*δ**B*—partial derivative of *A* with respect to *B*'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*δ**A*/*δ**B*——*A* 对 *B* 的偏导数'
- en: During the training of the network, the input *x* is fed to the network, and
    it passes through the layers to generate an output *y*̂. The expected output is
    *y*. Hence, the cost function or the loss function to compare *y* and *y*̂ is
    *C*(*y*, *y*̂). Also, the output for any hidden layer of the network can be represented
    as equation 8.4
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络的训练过程中，输入 *x* 被送入网络，并经过层生成输出 *y*̂。期望的输出是 *y*。因此，比较 *y* 和 *y*̂ 的损失函数或损失函数是
    *C*(*y*, *y*̂)。此外，网络的任何隐藏层的输出也可以用方程 8.4 表示
- en: '![figure](../Images/verdhan-ch8-eqs-6x.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-6x.png)'
- en: (8.4)
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (8.4)
- en: where *i* (index) can be any layer in the network.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *i*（索引）可以是网络中的任何一层。
- en: The final layer’s output is
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层的输出是
- en: (8.5)
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (8.5)
- en: '*y*(*x*) = *W*^(^(*N*)^)^(*T*) *h*^(^(*N*)^–^(1)) + *b*^(^(*N*)^)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*(*x*) = *W*^(^(*N*)^)^(*T*) *h*^(^(*N*)^–^(1)) + *b*^(^(*N*)^)'
- en: 'During the training of the network, we adjust the network’s weights so that
    *C* is reduced. Hence, we calculate the derivative of *C* with respect to every
    weight in the network. The following is the derivative of *C* with respect to
    every weight in the network:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络的训练过程中，我们调整网络的权重，以减少 *C*。因此，我们计算 *C* 对网络中每个权重的导数。以下是 *C* 对网络中每个权重的导数：
- en: '![figure](../Images/verdhan-ch8-eqs-8x.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-8x.png)'
- en: Now we know that a neural network has many layers. The backpropagation algorithm
    starts at calculating the derivatives at the last layer of the network, which
    is the N^(th) layer. Then these derivatives are fed backward. So the derivatives
    at the *N*^(th) layers will be fed to the (*N* – 1) layer of the network and so
    on.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道神经网络有很多层。反向传播算法从计算网络最后一层的导数开始，即第 N 层。然后这些导数被反向传递。因此，第 N 层的导数将传递到网络的第 (*N*
    – 1) 层，依此类推。
- en: 'Each component of the derivatives of *C* is calculated individually using the
    calculus chain rule. As per the chain rule, for a function *c* depending on *b*,
    where *b* depends on *a*, the derivative of *c* with respect to *a* can be written
    as equation 8.6:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*C* 的导数的每个分量都是单独使用微积分链式法则计算的。根据链式法则，对于依赖于 *b* 的函数 *c*，其中 *b* 依赖于 *a*，*c* 对
    *a* 的导数可以写成方程 8.6：'
- en: '![figure](../Images/verdhan-ch8-eqs-9x.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-9x.png)'
- en: (8.6)
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (8.6)
- en: Hence, in backpropagation the derivatives of the layer *N* are used in the layer
    (*N* – 1) so that they are saved and again used in the (*N* – 2) layer. We start
    with the last layer of the network, through all the layers to the first layer,
    and each time, we use the derivatives of the last calculations made to get the
    derivatives of the current layers. Hence, backpropagation turns out to be extremely
    efficient compared to a normal approach where we would have calculated each weight
    in the network individually.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在反向传播中，层 *N* 的导数用于层 (*N* – 1)，以便它们被保存并再次用于 (*N* – 2) 层。我们从网络的最后一层开始，通过所有层到第一层，每次我们使用最后计算的导数来得到当前层的导数。因此，反向传播与我们会单独计算网络中每个权重的常规方法相比，效率极高。
- en: Once we have calculated the gradients, we update all the weights in the network.
    The objective is to minimize the cost function. We have already studied methods
    like gradient descent in the last section. We now continue to the next step in
    the neural network training process.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了梯度，我们就更新网络中的所有权重。目标是使成本函数最小化。我们已经在上一节研究了像梯度下降这样的方法。我们现在继续到神经网络训练过程的下一步。
- en: '8.4.2 Step 4: Optimization'
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 步骤 4：优化
- en: Backpropagation allows us to optimize our network and achieve the best accuracy
    (see figure 8.12). Notice the optimizer, which provides regular and continuous
    feedback to reach the best solution.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播使我们能够优化我们的网络并达到最佳准确度（见图 8.12）。注意优化器，它提供常规和连续的反馈以找到最佳解决方案。
- en: '![figure](../Images/CH08_F12_Verdhan.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F12_Verdhan.png)'
- en: Figure 8.12 Optimization is the process to minimize the loss function.
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.12 优化是使损失函数最小化的过程。
- en: Once we have achieved the best values of the weights and biases for our network,
    we say that our network is trained. We can now use it to make predictions on an
    unseen dataset that has not been used for training the network.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了网络中权重和偏差的最佳值，我们就说我们的网络已经训练好了。现在我们可以用它来对未用于训练网络的未见数据集进行预测。
- en: 8.5 How deep learning works in an unsupervised manner
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 无监督方式下深度学习的工作原理
- en: We know that unsupervised learning solutions work on unlabeled datasets; thus,
    for deep learning in unsupervised settings, the training dataset is unlabeled.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道无监督学习解决方案适用于未标记的数据集；因此，在无监督设置中的深度学习，训练数据集是无标签的。
- en: As compared to supervised datasets where we have tags, unsupervised methods
    have to self-organize themselves to get densities, probabilities’ distributions,
    preferences, and groupings. We can solve a similar problem using supervised and
    unsupervised methods. For example, a supervised deep learning method can be used
    to identify dogs versus cats while an unsupervised deep learning method might
    be used to cluster the pictures of dogs and cats into different groups. In machine
    learning, a lot of solutions that were initially conceived as supervised learning
    ones, over a period of time, employed unsupervised learning methods to enrich
    the data and hence improve the supervised learning solution.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们有标签的监督数据集相比，无监督方法必须自我组织以获取密度、概率分布、偏好和分组。我们可以使用监督和无监督方法解决类似的问题。例如，可以使用监督深度学习方法来识别狗与猫，而无监督深度学习方法可能用于将狗和猫的图片聚类到不同的组中。在机器学习中，许多最初被认为是监督学习解决方案的解决方案，在一段时间内，采用了无监督学习方法来丰富数据，从而提高监督学习解决方案。
- en: During the learning phase in unsupervised deep learning, it is expected that
    the network will mimic the data and then improve itself based on the errors. In
    the supervised learning algorithm, other methods play the same part as the backpropagation
    algorithm. These include, among others,
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督深度学习的学习阶段，预期网络将模仿数据，然后根据错误进行自我改进。在监督学习算法中，其他方法与反向传播算法起着相同的作用。这包括，但不仅限于，
- en: Boltzmann learning rule
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boltzmann学习规则
- en: Contrastive divergence
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比散度
- en: Maximum likelihood
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大似然
- en: Hopfield learning rule
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfield学习规则
- en: GAN
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN
- en: Deep belief network (DBN)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度信念网络（DBN）
- en: In this book, we cover autoencoders and GAN in depth in separate chapters. The
    rest of the methods are covered in this chapter.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将在单独的章节中深入探讨自编码器和GAN。其余的方法在本章中介绍。
- en: 'Next, we study the two most widely used types of neural networks in supervised
    learning settings: the convolutional neural network (CNN) and the recurrent neural
    network (RNN).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究在监督学习设置中最广泛使用的两种神经网络类型：卷积神经网络（CNN）和循环神经网络（RNN）。
- en: Exercise 8.4
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习8.4
- en: 'Answer these questions to check your understanding:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: Write in a simple form the major steps in a backpropagation technique.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以简单形式写出反向传播技术的主要步骤。
- en: Backpropagation is preferred in unsupervised learning. True or False?
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在无监督学习中，反向传播算法是首选。对或错？
- en: The objective of deep learning is to maximize the loss function. True or False?
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度学习的目标是最大化损失函数。对或错？
- en: 8.6 Convolutional neural networks
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 卷积神经网络
- en: CNNs are a class of deep learning models that are primarily used for image and
    video processing tasks. They have become a powerful tool in the field of computer
    vision due to their ability to automatically detect and learn the pattern from
    raw images and, hence, are used for several use cases across multiple domains
    and functions. We provide only a brief overview, as there can be an entire book
    on different types of CNN solutions.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是一类主要用于图像和视频处理任务的深度学习模型。由于它们能够自动从原始图像中检测和学习模式，因此已成为计算机视觉领域的一种强大工具。因此，它们被用于多个领域和功能中的多个用例。我们只提供简要概述，因为关于不同类型的CNN解决方案可以有一整本书。
- en: 8.6.1 Key concepts of CNN
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.1 CNN的关键概念
- en: 'The following are the key concepts of CNN:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为CNN的关键概念：
- en: '*Input layer*—The input to the CNN is generally a tensor representing an image.
    As we know, an image is made up of pixels, and each pixel is made up of RGB channels.
    An image is represented by a 3D matrix, which is a width × height channel.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入层*——CNN的输入通常是一个表示图像的张量。正如我们所知，图像由像素组成，每个像素由RGB通道组成。图像由一个3D矩阵表示，即宽度 × 高度通道。'
- en: '*Convolution layer*—This is the core building layer of a CNN. It applies a
    filter to the input data, which scans over the image to detect patterns like lines,
    curves, texture, edges, etc. The filter size is generally small and usually 3
    × 3 or 5 × 5\. As the kernel slides over the input, it performs an element-wise
    multiplication and sum, creating a feature map. Multiple filters can be applied
    to learn different features, generating multiple feature maps. The entire process
    is illustrated in figure 8.13.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积层*——这是CNN的核心构建层。它对输入数据应用滤波器，扫描图像以检测线条、曲线、纹理、边缘等模式。滤波器大小通常较小，通常是3 × 3或5 ×
    5。当核在输入上滑动时，它执行逐元素乘法和求和，创建一个特征图。可以应用多个滤波器来学习不同的特征，生成多个特征图。整个过程如图8.13所示。'
- en: '![figure](../Images/CH08_F13_Verdhan.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F13_Verdhan.png)'
- en: Figure 8.13 CNN process. The original data is 6 × 6, and the filter applied
    is 3 × 3, which results in a 4 × 4 output.
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.13 CNN过程。原始数据是6 × 6，应用的是3 × 3的滤波器，结果输出为4 × 4。
- en: '*ReLU activation function*—This is applied to add nonlinearity. It helps the
    network to understand and model more complex and difficult patterns that are present
    in the data.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ReLU激活函数*——这是用来增加非线性。它帮助网络理解和建模数据中存在的更复杂和困难的模式。'
- en: '*Polling layer*—This is used to reduce the spatial dimensions of images while
    preserving the most significant details. The most common type of pulling is called
    max pulling. It takes the maximum value from a region of input. The major function
    of the pooling layer is to reduce the computation load and also reduce overfitting
    by providing a form of translation in variance.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*池化层*——这是用来减少图像的空间维度同时保留最重要的细节。最常见的一种池化称为最大池化。它从一个输入区域中取最大值。池化层的主要功能是减少计算负载，并通过提供一种形式上的平移不变性来减少过拟合。'
- en: '*Output*—After we have created several convolutional and pooling layers, we
    receive the output. It is generally flattened into a 1D vector and the output
    is then passed to the fully connected layer. The main task of the fully connected
    layer is to perform high-level classification of the image based on the features
    extracted by the previous layers.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出*—在我们创建了几个卷积层和池化层之后，我们得到输出。输出通常被展平成一个一维向量，然后输出被传递到全连接层。全连接层的主要任务是根据前一层提取的特征对图像进行高级分类。'
- en: '*Output layer*—If the solution is for classification of data points, the output
    layer would contain a function like softmax. The softmax function gives respective
    probabilities for different classes. For example, if you are trying to predict
    that a given picture is a cat or a dog, the softmax function will give the probability
    of the picture being a dog or a cat.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出层*—如果解决方案是用于数据点的分类，输出层将包含一个如softmax之类的函数。softmax函数为不同的类别提供相应的概率。例如，如果你试图预测一个给定的图片是猫还是狗，softmax函数将给出图片是狗或猫的概率。'
- en: In CNNs, the same filter is applied across different regions of the image. Thus
    the number of parameters is reduced as compared to a traditional fully connected
    network. Each neuron in the convolutional layer is connected only to a small region
    of the input, and so the complexity of the network is also reduced. The network
    also automatically trains and learns to detect low-level patterns. An example
    of a low-level pattern is edges. The network subsequently progresses to learn
    more complex patterns like shapes in the deeper layers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，相同的过滤器被应用于图像的不同区域。因此，与传统的全连接网络相比，参数数量减少了。卷积层中的每个神经元仅连接到输入的一个小区域，因此网络的复杂性也降低了。网络还自动训练和学习检测低级模式。低级模式的一个例子是边缘。网络随后进步到学习更复杂的模式，如深层中的形状。
- en: 8.6.2 Use of CNN
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.2 CNN的使用
- en: Call networks are fundamental and foundational to the modern-day competition
    solutions. They are heavily used for image classification, image processing, speech
    recognition, developing computer board games, and various other video processing
    solutions. Many solutions are developed using CNN—for example, automatic detection
    of vehicle license plates, detection of cancerous cells from scans, detection
    of broken bones from x-rays, facial recognition solutions, automatic entry handwriting,
    recognition solutions, and many other solutions that are having an amazing affect
    across our lives.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 网络调用是现代竞争解决方案的基本和基础。它们被大量用于图像分类、图像处理、语音识别、开发计算机棋类游戏以及各种其他视频处理解决方案。许多解决方案都是使用CNN开发的——例如，自动检测车辆牌照、从扫描中检测癌细胞、从X光片中检测骨折、面部识别解决方案、自动输入手写识别解决方案，以及许多对我们生活产生巨大影响的解决方案。
- en: 'There are quite a few CNN architectures available, like Inception, ResNet,
    LeNet, VGG-16, etc., that are useful for creating computer vision solutions. We
    now move on to the second common type of neural network: RNN.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的CNN架构有很多，如Inception、ResNet、LeNet、VGG-16等，这些架构对于创建计算机视觉解决方案非常有用。我们现在转向第二种常见的神经网络类型：RNN。
- en: 8.7 Recurrent neural networks
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 循环神经网络
- en: RNNs are quite a popular class of networks that are designed to recognize patterns
    in a sequence of data—for example, time service data or videos, natural languages,
    or any other kind of data with this sequence of information. Here RNNs are very
    useful. The most significant feature of RNNs is their ability to maintain a memory
    about the previous input, which they capture using temporal dependencies and the
    order in the dataset. This augments their capability to recognize patterns in
    the sequential datasets, and hence RNN has been found to be a parting solution
    in multiple domains.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是一类相当流行的网络，旨在识别数据序列中的模式——例如，时间服务数据或视频、自然语言或任何其他具有这种信息序列的数据。在这里，RNN非常有用。RNN最显著的特征是它们能够维持对先前输入的记忆，它们通过时间依赖性和数据集中的顺序来捕捉这种记忆。这增强了它们在顺序数据集中识别模式的能力，因此RNN被发现是多个领域的解决方案。
- en: 8.7.1 Key concepts of RNN
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1 RNN的关键概念
- en: RNNs are especially designed for sequential datasets, and here the order of
    the input display plays a pivotal role. Hence, RNNs are the go-to solution for
    sequential data handling.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: RNN特别设计用于顺序数据集，在这里输入显示的顺序起着关键作用。因此，RNN是处理顺序数据的首选解决方案。
- en: Unlike a regular neural network, which is also known as a feed-forward neural
    network, RNNs have recurrent connections. This means that the output from one
    time step is fed back as the input to the next time step. This information is
    persistent across the sequence. At the same time, the same weight is used across
    different time steps. This makes them very efficient in terms of the number of
    parameters, as the same network can be applied to every time step of the input
    sequence.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规神经网络（也称为前馈神经网络）不同，RNNs具有循环连接。这意味着一个时间步的输出被反馈作为下一个时间步的输入。这种信息在序列中是持续的。同时，相同的权重在不同的时间步中使用。这使得它们在参数数量方面非常高效，因为相同的网络可以应用于输入序列的每个时间步。
- en: 'RNNs work in the following fashion:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs以以下方式工作：
- en: The input data is processed sequentially. At each time step *t*, the network
    receives an input *x*[*t*], which is then combined with a hidden state *h*[*t*][–1].
    This hidden state is the output from the previous time step and serves as a memory
    that carries information from one time step to the next time step.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据是顺序处理的。在每个时间步 *t*，网络接收一个输入 *x*[*t*]，然后与隐藏状态 *h*[*t*][–1] 结合。这个隐藏状态是前一个时间步的输出，并作为记忆，携带信息从一个时间步传递到下一个时间步。
- en: 'The hidden state *h*[*t*] is then updated using a nonlinear function:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏状态 *h*[*t*] 然后使用非线性函数更新：
- en: '![figure](../Images/verdhan-ch8-eqs-10x.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-10x.png)'
- en: The final output at each of the time steps can be calculated and used either
    for each individual time step or only at the final time step.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个时间步的最终输出可以计算并用于每个单独的时间步或仅在最终时间步。
- en: Figure 8.14 illustrates the RNN process.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14说明了RNN过程。
- en: '![figure](../Images/CH08_F14_Verdhan.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F14_Verdhan.png)'
- en: Figure 8.14 The RNN process. RNNs have internal memory, which allows them to
    use information from the previous inputs to influence the current input and outputs.
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.14 RNN过程。RNNs具有内部记忆，这使得它们能够使用先前输入的信息来影响当前输入和输出。
- en: 'The most basic version of an RNN is a simple recurrent network, but it struggles
    with a long-term dependency because the gradient can either vanish or explode,
    making it hard for the network to remember information from far back in the sequence;
    hence, it cannot be used for a solution like a chatbot. Long short-term memory
    (LSTM) is much more useful here. LSTM is a special type of network designed to
    mitigate the vanishing gradient problem and handle long-term dependencies better
    than plain vanilla RNNs. They achieve this feat by introducing gates. There are
    three types of gates: input, forget, and output gates. These gates regulate the
    flow of information through the network and allow it to maintain important information
    over longer periods of time. Gated recurrent units are another type of RNN, but
    LSTM and gated recurrent units are beyond the scope of this book.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: RNN最基本的形式是简单的循环网络，但它难以处理长期依赖，因为梯度可能消失或爆炸，使得网络难以记住序列中远处的信息；因此，它不能用于像聊天机器人这样的解决方案。长短期记忆（LSTM）在这里更有用。LSTM是一种特殊的网络，旨在减轻梯度消失问题，并且比普通的RNN更好地处理长期依赖。它们通过引入门控机制实现这一壮举。有三种类型的门：输入门、遗忘门和输出门。这些门控制网络中信息的流动，并允许网络在更长的时间内维持重要信息。门控循环单元是另一种类型的RNN，但LSTM和门控循环单元超出了本书的范围。
- en: RNNs are very powerful for processing sequences, and their ability to model
    time dependencies makes them indispensable in the fields of natural language processing
    and time-series analysis. Their use has been pathbreaking for many innovative
    solutions—for example, predicting the next word in a sentence; translating text
    from one language to another; processing sequences of video frames to understand
    behaviors over time; modeling temporal dependencies like audio signals, which
    can be used to recognize speech patterns over time; and many more. RNNs are the
    power engines behind GenAI solutions.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs在处理序列方面非常强大，它们建模时间依赖性的能力使它们在自然语言处理和时间序列分析领域变得不可或缺。它们的使用为许多创新解决方案开辟了道路——例如，预测句子中的下一个单词；将文本从一种语言翻译成另一种语言；处理视频帧序列以理解随时间的行为；建模如音频信号这样的时间依赖性，这些信号可以用来识别随时间变化的语音模式；等等。RNNs是GenAI解决方案背后的动力引擎。
- en: 8.8 Boltzmann learning rule
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 鲍尔兹曼学习规则
- en: The Boltzmann learning rule is an unsupervised learning rule used in neural
    networks. It is based on the principle of statistical mechanics of physical systems.
    It is seldom used in the context of Boltzmann machines. It adjusts the weights
    of a neural network with an objective to minimize the energy of the system, thereby
    ensuring the network reaches a stable state.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼学习规则是一种在神经网络中使用的无监督学习规则。它基于物理系统的统计力学原理。在玻尔兹曼机的背景下很少使用。它通过最小化系统的能量来调整神经网络的权重，从而确保网络达到稳定状态。
- en: 8.8.1 Concepts of the Boltzmann learning rule
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.1 霍尔兹曼学习规则的概念
- en: 'The following are the key concepts of the Boltzmann learning rule:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为玻尔兹曼学习规则的关键概念：
- en: It is a type of probabilistic RNN where neurons are connected in a fully connected
    graph.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一种概率性循环神经网络（RNN），其中神经元在一个全连接图中连接。
- en: The neurons in the Boltzmann machine are stochastic units that fire as per a
    probability distribution. Thus we can use the Boltzmann learning rule for dimensionality
    reduction, pattern recognition, feature extraction, and optimization tasks.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玻尔兹曼机中的神经元是随机单元，它们根据概率分布进行放电。因此，我们可以使用玻尔兹曼学习规则进行降维、模式识别、特征提取和优化任务。
- en: A Boltzmann machine has an energy function *E*(*v*,*h*) where *v* is the input
    visible unit while *h* is the hidden unit. The energy function determines the
    cost of a given state of the network. During the training of the network, we aim
    to adjust the weights in such a manner so that the energy of the system is minimized.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玻尔兹曼机有一个能量函数 *E*(*v*,*h*)，其中 *v* 是可见输入单元，而 *h* 是隐藏单元。能量函数决定了网络给定状态的代价。在网络训练过程中，我们旨在调整权重，使系统的能量最小化。
- en: 'The network models the probability of a particular state (*v*,*h*) using a
    Boltzmann distribution. It depends on the energy of the state, which is given
    by equation 8.7:'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络使用玻尔兹曼分布来模拟特定状态 (*v*,*h*) 的概率。它取决于状态的能量，由方程8.7给出：
- en: '![figure](../Images/verdhan-ch8-eqs-11x.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-11x.png)'
- en: (8.7)
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (8.7)
- en: Here, *Z* is the partition function, which ensures that the sum of probabilities
    = 1.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*Z* 是配分函数，它确保概率之和等于1。
- en: 'The rule seeks to adjust the weights to keep on decreasing the energy of the
    system during the training of the network, and it happens over time. The weights
    are updated by a rule derived from gradient of the energy function with respect
    to the weights. The weight update rule is given in equation 8.8:'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该规则旨在调整权重，在网络训练过程中持续降低系统的能量，并且这一过程是随时间发生的。权重通过从能量函数相对于权重的梯度推导出的规则进行更新。权重更新规则在方程8.8中给出：
- en: '![figure](../Images/verdhan-ch8-eqs-12x.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch8-eqs-12x.png)'
- en: (8.8)
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (8.8)
- en: Here, *h* is the learning rate, and (*v*[*i*]*h*[*j*])[data] is the correction
    between the visible unit *v*[*i*] and hidden unit *h*[*j*]. It is computed from
    the data distribution. It represents how often they are active together in the
    hidden unit. (*v*[*i*]*h*[*j*])[model] is the correction computed from the model
    distribution. It represents how often the visible unit *v*[*i*] and hidden unit
    *h*[*j*] are active together in the state generated by the network.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*h* 是学习率，而 (*v*[*i*]*h*[*j*])[data] 是可见单元 *v*[*i*] 和隐藏单元 *h*[*j*] 之间的校正，它是从数据分布中计算得出的。它表示它们在隐藏单元中一起活跃的频率。(*v*[*i*]*h*[*j*])[model]
    是从模型分布中计算得出的校正，它表示可见单元 *v*[*i*] 和隐藏单元 *h*[*j*] 在网络生成的状态中一起活跃的频率。
- en: During the training of the model, a learning rule is followed, which is to make
    the data distribution match the model distribution. Hence, it reduces the energy
    of the system and thereby increases the overall performance.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型的训练过程中，遵循一个学习规则，即使数据分布与模型分布相匹配。因此，它降低了系统的能量，从而提高了整体性能。
- en: 8.8.2 Key points
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.2 关键点
- en: 'There are certain key points we should bear in mind. Energy-based models like
    the Boltzmann machine use the Boltzmann learning rule to minimize an energy function
    by adjusting the network’s weights:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该牢记一些关键点。基于能量的模型，如玻尔兹曼机，使用玻尔兹曼学习规则通过调整网络的权重来最小化能量函数：
- en: The network strives to model the probability distribution over its inputs. The
    core objective here is to associate the higher energy with less likely configurations.
    Similarly, the lower energy is associated with more like configurations.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络努力模拟其输入的概率分布。这里的核心目标是将高能量与不太可能的配置相关联。同样，低能量与更可能的配置相关联。
- en: Boltzmann learning is an unsupervised and probabilistic method. It works on
    the concept of contrasting the model distribution and data distribution.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玻尔兹曼学习是一种无监督和概率方法。它基于对比模型分布和数据分布的概念。
- en: The rule is computationally expensive in its basic form; hence, to increase
    the training speed, sometimes we utilize methods like contrastive divergence.
    We cover contrastive divergence in the next section.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本形式的规则计算成本较高；因此，为了提高训练速度，有时我们会利用对比散度等方法。我们将在下一节介绍对比散度。
- en: The Boltzmann learning rule is primarily used for unsupervised learning tasks
    such as dimensionality reduction, feature extraction, and generative modeling.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玻尔兹曼学习规则主要用于无监督学习任务，如降维、特征提取和生成建模。
- en: The model training is sometimes slower than expected.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练有时比预期慢。
- en: In summary, the Boltzmann learning rule is a probabilistic approach to training
    neural networks by adjusting weights based on minimizing an energy function, and
    it provides a foundation for generative models like Boltzmann machines. However,
    due to computational challenges, approximations such as contrastive divergence
    are often used to make it practical for real-world applications.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，玻尔兹曼学习规则是一种通过调整权重以最小化能量函数来训练神经网络的概率方法，并为生成模型如玻尔兹曼机提供了基础。然而，由于计算挑战，通常使用对比散度等近似方法使其适用于实际应用。
- en: 8.9 Deep belief networks
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.9 深度信念网络
- en: A DBN is a type of GAN made up of multiple layers of stochastic, binary latent
    variables (hidden units), where each layer is a restricted Boltzmann machine (RBM)
    or a variant of it. DBNs were popularized by Geoffrey Hinton (who was awarded
    the Nobel Prize in Physics in 2024, shared with John Hopfield) and his collaborators
    in the mid-2000s for pretraining deep networks in an unsupervised way.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: DBN是一种由多层随机二进制潜在变量（隐藏单元）组成的GAN，其中每一层都是受限玻尔兹曼机（RBM）或其变体。DBN在2000年代中期由杰弗里·辛顿（2024年获得诺贝尔物理学奖，与约翰·霍普菲尔德共享）及其合作者推广，用于以无监督方式预训练深度网络。
- en: 8.9.1 Key points of DBN
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.9.1 DBN的关键点
- en: 'The key points of a DBN are as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: DBN的关键点如下：
- en: RBM
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBM
- en: A DBN consists of several layers of RBMs. A RBM contains a visible layer and
    a hidden layer. The visible layer represents the observed data while the hidden
    layer captures the hidden features.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBN由多个RBMs层组成。一个RBM包含一个可见层和一个隐藏层。可见层代表观察到的数据，而隐藏层则捕捉隐藏特征。
- en: Each DBN is trained independently with an objective to model the underlying
    structure of the data.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个DBN都是独立训练的，目的是模拟数据的潜在结构。
- en: The objective of the training in DBN is to optimize the log-likelihood of the
    data under the network’s generative model. For each layer, the contrastive divergence
    algorithm is used to approximate the gradient of the log-likelihood with respect
    to the weights. This allows the network to learn a good set of weights for each
    layer.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBN训练的目的是优化网络生成模型下数据的对数似然。对于每一层，使用对比散度算法来近似对数似然相对于权重的梯度。这允许网络为每一层学习一组良好的权重。
- en: The contrastive divergence algorithm is a stochastic approximation method used
    to estimate the gradient of the log-likelihood of the model. The algorithm starts
    with a sample from the visible layer and then performs Gibbs sampling to update
    the hidden layer and visible layer iteratively. Contrastive divergence ensures
    that the network learns to model the input data distribution efficiently.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比散度算法是一种用于估计模型对数似然梯度的随机近似方法。算法从可见层的一个样本开始，然后执行吉布斯采样以迭代地更新隐藏层和可见层。对比散度确保网络能够有效地学习输入数据分布。
- en: 'Layer-based pretraining:'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于层的预训练：
- en: DBNs are typically trained in a layer-wise manner, where each layer is pretrained
    as an RBM. The first RBM has an objective to learn to capture low-level features
    from the data.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBN通常以分层的方式进行训练，其中每个层都作为RBM进行预训练。第一个RBM的目标是从数据中学习捕捉低级特征。
- en: Based on this knowledge, each subsequent RBM then learns increasingly complex,
    abstract features from the representations learned by the previous layers. In
    this manner, the cycle continues.
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于这一知识，每个后续的RBM随后从先前层学习到的表示中学习越来越复杂、抽象的特征。以此类推，循环继续。
- en: This phase involves training each RBM individually using contrastive divergence.
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个阶段涉及使用对比散度分别训练每个RBM。
- en: This process tunes the weights to capture relevant patterns and features in
    the input data, without the need for labeled data.
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此过程调整权重以捕捉输入数据中的相关模式和特征，无需标记数据。
- en: Since each layer learns features at increasing levels of abstraction and complexity,
    it makes the overall solution good enough for complex tasks like image or speech
    recognition.
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于每一层都在不断抽象和复杂化的层次上学习特征，这使得整体解决方案对于复杂任务（如图像或语音识别）足够好。
- en: 'Supervised fine-tuning:'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督微调：
- en: Once the pretraining is done, the entire network is fine-tuned. It is done in
    a supervised fashion using methods like backpropagation or a labeled dataset with
    an objective to optimize the network.
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练完成后，整个网络进行微调。这通过反向传播或带有优化网络目标的有标签数据集以监督方式进行。
- en: The supervised system adjusts the network weights to minimize the prediction
    error such as what is done in classification or regression tasks.
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督系统调整网络权重以最小化预测误差，如分类或回归任务中所做的那样。
- en: The unsupervised pretraining phase helps initialize the weights in such a way
    that the network is less likely to overfit during supervised fine-tuning, as it
    starts with a better understanding of the data.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督预训练阶段有助于以这种方式初始化权重，使得网络在监督微调期间不太可能过拟合，因为它从对数据的更好理解开始。
- en: They are computationally expensive and time-consuming, particularly when dealing
    with large datasets or deep architectures.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在计算上成本高昂且耗时，尤其是在处理大型数据集或深层架构时。
- en: Pretraining using RBMs is useful, but fine-tuning the entire DBN can sometimes
    be difficult, especially if we are dealing with a very deep neural network. It
    may necessitate meticulous hyperparameter training and lots of labeled datasets.
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RBMs进行预训练是有用的，但有时微调整个DBN可能很困难，尤其是当我们处理一个非常深的神经网络时。这可能需要细致的超参数训练和大量的标记数据集。
- en: Similar to other deep learning architectures, DBNs are also prone to the vanishing
    gradient problem, where gradients diminish as they are propagated backward through
    many layers. This further complicates the entire training process.
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他深度学习架构类似，DBNs也容易受到梯度消失问题的影响，即当梯度在反向传播通过多层时减小。这进一步复杂化了整个训练过程。
- en: DBNs are typically used for unsupervised learning, dimensionality reduction,
    and feature learning, but they can also be fine-tuned for supervised tasks such
    as classification. DBNs are used to improve the performance of speech recognition
    systems by learning representations of sound features that are invariant to noise
    and other distortions. As generative models, DBNs can be used to create new data
    instances that resemble the training data. For example, DBNs have been used in
    generative art, where new images are created that resemble a set of input images.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: DBNs通常用于无监督学习、降维和特征学习，但它们也可以微调以用于监督任务，如分类。DBNs通过学习对噪声和其他失真不敏感的声音特征来提高语音识别系统的性能。作为生成模型，DBNs可以用来创建类似于训练数据的新数据实例。例如，DBNs已被用于生成艺术，其中创建了类似于一组输入图像的新图像。
- en: DBNs are a significant milestone in the development of deep learning techniques.
    They combine the strengths of generative models like RBMs with deep learning principles
    to create a powerful method for learning complex representations of data. While
    newer architectures have emerged and gained prominence, DBNs remain a key historical
    and theoretical component of modern AI, influencing the development of many advanced
    models. By utilizing unsupervised learning, DBNs can be highly effective for tasks
    like dimensionality reduction, generative modeling, and classification. However,
    challenges related to training complexity and fine-tuning remain significant hurdles
    for widespread adoption.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: DBNs是深度学习技术发展中的一个重要里程碑。它们结合了生成模型（如RBMs）的优点和深度学习原理，创造了一种学习数据复杂表示的强大方法。虽然已经出现了新的架构并获得了突出地位，但DBNs仍然是现代AI的关键历史和理论组成部分，影响了众多先进模型的发展。通过利用无监督学习，DBNs在降维、生成建模和分类等任务上可以非常有效。然而，与训练复杂性和微调相关的挑战仍然是广泛采用的重要障碍。
- en: 8.10 Popular deep learning libraries
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.10 流行的深度学习库
- en: Over the last few chapters, we have used a lot of libraries and packages for
    implementing solutions. There are quite a few libraries available in the industry
    for deep learning. These packages expedite the solution building and reduce the
    efforts as most of the heavy lifting is done by these libraries.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们使用了大量库和包来实现解决方案。在行业中，有相当多的库可用于深度学习。这些包加速了解决方案的构建，并减少了工作量，因为大部分重活都是由这些库完成的。
- en: The most popular deep learning libraries are
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的深度学习库有
- en: '*TensorFlow (TF)**—*Developed by Google, this is arguably one of the most popular
    and widely used deep learning frameworks. It was launched in 2015 and since has
    been used by a number of businesses and brands across the globe.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TensorFlow (TF)**—*由 Google 开发，这可能是最受欢迎和最广泛使用的深度学习框架之一。它于 2015 年推出，自那时起已被全球众多企业和品牌使用。'
- en: Python is mostly used for TF but C++, Java, C#, Javascript, and Julia can also
    be used. You have to install the TF library on your system and import the library.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Python 主要用于 TF，但 C++、Java、C#、JavaScript 和 Julia 也可以使用。您必须在您的系统上安装 TF 库并导入该库。
- en: NOTE  Go to [www.tensorflow.org/install](http://www.tensorflow.org/install)
    and follow the instructions to install TF.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：访问 [www.tensorflow.org/install](http://www.tensorflow.org/install) 并按照说明安装
    TF。
- en: TF is one of the most popular libraries and can work on mobile devices like
    iOS and Android.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: TF 是最受欢迎的库之一，可以在移动设备如 iOS 和 Android 上运行。
- en: '*Keras*—Keras is a mature API-driven solution and quite easy to use. It is
    one of the best choices for starters and is among the best for prototyping simple
    concepts in an easy and fast manner. Keras was initially released in 2015 and
    is one of the most recommended libraries.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Keras*—Keras 是一个成熟的 API 驱动解决方案，非常易于使用。它是初学者的最佳选择之一，也是快速以简单和快捷的方式原型设计简单概念的佼佼者。Keras
    最初于 2015 年发布，是最推荐的库之一。'
- en: NOTE  Go to [https://keras.io](https://keras.io) and follow the instructions
    to install Keras. Tf.keras can be used as an API.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：访问 [https://keras.io](https://keras.io) 并按照说明安装 Keras。Tf.keras 可以用作 API。
- en: Serialization/deserialization APIs, call-backs, and data streaming using Python
    generators are very mature. Massive models in Keras are reduced to single-line
    functions, which makes it a less configurable environment and hence very convenient
    and easy to use.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 生成器进行序列化/反序列化 API、回调和数据流非常成熟。Keras 中的大规模模型简化为单行函数，这使得它成为一个不太可配置的环境，因此非常方便且易于使用。
- en: '*PyTorch*—Facebook’s brainchild PyTorch was released in 2016 and is another
    popular framework. PyTorch operates with dynamically updated graphs and allows
    data parallelism and distributed learning models. There are debuggers like pdb
    or PyCharm available in PyTorch. For small projects and prototyping, PyTorch can
    be a good choice.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PyTorch*—Facebook 的 PyTorch 于 2016 年发布，是另一个流行的框架。PyTorch 使用动态更新的图，并允许数据并行和分布式学习模型。PyTorch
    中有如 pdb 或 PyCharm 这样的调试器。对于小型项目和原型设计，PyTorch 可以是一个不错的选择。'
- en: '*Sonnet*—DeepMind’s Sonnet is developed using and on top of TF. Sonnet is designed
    for complex neural network applications and architectures. It works by creating
    primary Python objects corresponding to a particular part of the neural network.
    Then these Python objects are independently connected to the computational TF
    graph. Because of this separation (creating Python objects and associating them
    to a graph), the design is simplified.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sonnet*—DeepMind 的 Sonnet 是在 TF 的基础上开发的。Sonnet 专为复杂的神经网络应用和架构设计。它通过创建与神经网络特定部分相对应的初级
    Python 对象来实现。然后，这些 Python 对象独立连接到计算 TF 图。由于这种分离（创建 Python 对象并将它们关联到图中），设计得到了简化。'
- en: NOTE  Having high-level object-oriented libraries is very helpful, as the abstraction
    is allowed when we develop machine learning solutions.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：拥有高级面向对象库非常有帮助，因为当我们开发机器学习解决方案时，允许进行抽象。
- en: '*MXNet*—Apache’s MXNet is a highly scalable deep learning tool that is easy
    to use and has detailed documentation. A large number of languages like C ++,
    Python, R, Julia, JavaScript, Scala, Go, and Perl are supported by MXNet.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MXNet*—Apache 的 MXNet 是一个高度可扩展且易于使用的深度学习工具，具有详细的文档。MXNet 支持大量语言，如 C++、Python、R、Julia、JavaScript、Scala、Go
    和 Perl。'
- en: There are other frameworks too, like Swift, Gluon, Chainer, DL4J, etc.; however,
    we only discuss the popular ones here. We now examine a short code in TF and Keras.
    It is just to test that you have installed these libraries correctly. You can
    learn more about TF at [https://www.tensorflow.org](https://www.tensorflow.org)
    and Keras at [https://keras.io](https://keras.io).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他框架，如Swift、Gluon、Chainer、DL4J等；然而，我们在这里只讨论流行的框架。我们现在检查TF和Keras中的简短代码，只是为了测试您是否已正确安装了这些库。您可以在[https://www.tensorflow.org](https://www.tensorflow.org)了解更多关于TF的信息，在[https://keras.io](https://keras.io)了解更多关于Keras的信息。
- en: 8.10.1 Python code for Keras and TF
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.10.1 Python代码用于Keras和TF
- en: 'We implement a very simple code in TF. We simply import the TF library and
    print “hello”. We also check the version of TF:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在TF中实现了一段非常简单的代码。我们只是导入TF库并打印“hello”。我们还检查了TF的版本：
- en: '[PRE0]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If this code runs for you and prints the version of TF, it means that you have
    installed `tensorflow` correctly:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这段代码能为您运行并打印出TF的版本，这意味着您已正确安装了`tensorflow`：
- en: '[PRE1]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If this code runs for you and prints the version of Keras, it means that you
    have installed `keras` correctly.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这段代码能为您运行并打印出Keras的版本，这意味着您已正确安装了`keras`。
- en: 8.11 Concluding thoughts
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.11 结论性思考
- en: Deep learning is changing the world we live in. It is enabling us to train and
    create really complex solutions that were a mere thought earlier. The effect of
    deep learning can be witnessed across multiple domains and industries. Perhaps
    there are no industries that have been left unaffected by the marvels of deep
    learning.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习正在改变我们生活的世界。它使我们能够训练和创建以前只是想法的真正复杂的解决方案。深度学习的影响可以在多个领域和行业中观察到。或许没有任何行业能逃脱深度学习奇迹的影响。
- en: Deep learning is one of the most-sought-after fields for research and development.
    Every year, many journals and papers are published on deep learning. Researchers
    across prominent institutions and universities (like Oxford, Stanford, etc.) of
    the world are engrossed in finding improved neural network architectures. At the
    same time, professionals and engineers in reputed organizations (like Google,
    Facebook, etc.) are working hard to create sophisticated architectures to improve
    performance.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是研究和开发中最受欢迎的领域之一。每年，都有许多关于深度学习的期刊和论文发表。世界各地的著名机构（如牛津、斯坦福等）的研究人员都在努力寻找改进的神经网络架构。同时，在知名组织（如Google、Facebook等）的专业人士和工程师也在努力创造复杂的架构以提高性能。
- en: Deep learning is making our systems and machines able to solve problems typically
    assumed to be in the realm of humans only. We have improved the clinical trials
    process for the pharma sector, fraud detection software, automatic speech detection
    systems, and various image recognition solutions; and created more robust natural
    language processing solutions, targeted marketing solutions that improve customer
    relationship management and recommendation systems, better safety processes, and
    so on. The list is quite long and growing day by day.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习使我们的系统和机器能够解决通常被认为仅属于人类领域的难题。我们改进了制药行业的临床试验流程、欺诈检测软件、自动语音检测系统以及各种图像识别解决方案；并创建了更稳健的自然语言处理解决方案、针对营销解决方案，这些解决方案可以改善客户关系管理和推荐系统，更好的安全流程等等。这个列表相当长，而且每天都在增长。
- en: At the same time, there are still a few challenges. The expectations from deep
    learning continue to increase. Deep learning is not a silver bullet or a magic
    wand to resolve all problems. It is surely one of the more sophisticated solutions,
    but it is certainly not the 100% solution to all business problems. The dataset
    we need to feed the algorithms is not always available. There is a dearth of good-quality
    datasets that are representative of business problems. Often, big organizations
    like Google, Meta, or Amazon can afford to collect such massive datasets. But
    many times we do find a lot of quality problems in the data. Having the processing
    power to train these complex algorithms is also a challenge. With the advent of
    cloud computing, though, this problem has been resolved to a certain extent.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，仍然存在一些挑战。对深度学习的期望持续增加。深度学习并不是解决所有问题的银弹或魔杖。它确实是一种更复杂的解决方案，但绝对不是解决所有商业问题的100%解决方案。我们需要提供给算法的数据集并不总是可用。缺乏高质量的、能代表商业问题的数据集。通常，像Google、Meta或Amazon这样的大组织能够负担得起收集如此庞大的数据集。但很多时候，我们在数据中确实发现了很多质量问题。拥有训练这些复杂算法的处理能力也是一个挑战。然而，随着云计算的出现，这个问题在某种程度上已经得到了解决。
- en: In this chapter, we explored the basics of neural networks and deep learning.
    We covered the details around neurons, activation function, different layers of
    a network, and loss function. We also covered in detail the backpropagation algorithm—the
    central algorithm used to train a supervised deep learning solution. Then we briefly
    went through unsupervised deep learning algorithms. We will cover these unsupervised
    deep learning solutions in greater detail in the later chapters. Figure 8.15 shows
    the major activation functions.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了神经网络和深度学习的基础。我们涵盖了关于神经元、激活函数、网络的不同层以及损失函数的细节。我们还详细介绍了反向传播算法——用于训练监督深度学习解决方案的核心算法。然后我们简要介绍了无监督深度学习算法。我们将在后面的章节中更详细地介绍这些无监督深度学习解决方案。图8.15显示了主要的激活函数。
- en: '![figure](../Images/CH08_F15_Verdhan.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F15_Verdhan.png)'
- en: 'Figure 8.15 Major activation functions at a glance (Source: towardsdatascience)'
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.15 一瞥主要激活函数（来源：towardsdatascience）
- en: 8.12 Practical next steps and suggested readings
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.12 实践下一步行动和推荐阅读
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提供了一些下一步行动的建议和一些有用的阅读材料：
- en: The book *Deep Learning with Python* by François Chollet is one of the best
    resources to clarify the concepts of deep learning. It covers all the concepts
    of deep learning and neural networks and is written by the creator of Keras.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: François Chollet所著的《Python深度学习》一书是阐明深度学习概念的最好资源之一。它涵盖了深度学习和神经网络的全部概念，并由Keras的创造者撰写。
- en: 'Read the following research papers:'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读以下研究论文：
- en: Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the Knowledge in a
    Neural Network. [https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton, G., Vinyals, O., and Dean, J. (2015). 神经网络中的知识蒸馏。[https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)
- en: Srivastava, R., Greff, K., and Schmidhuber, J. (2015). Training Very Deep Networks.
    [https://arxiv.org/pdf/1507.06228](https://arxiv.org/pdf/1507.06228)
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava, R., Greff, K., and Schmidhuber, J. (2015). 训练非常深的网络。[https://arxiv.org/pdf/1507.06228](https://arxiv.org/pdf/1507.06228)
- en: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013). Distributed
    Representations of Words and Phrases and their Compositionality. [https://arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013). 词和短语的分布式表示及其组合性。[https://arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)
- en: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). Generative Adversarial
    Networks. [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). 生成对抗网络。[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
- en: He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image
    Recognition. [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, K., Zhang, X., Ren, S., and Sun, J. (2015). 用于图像识别的深度残差学习。[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning is an advanced form of machine learning based on neural networks,
    and it’s particularly effective with unstructured data like text, images, audio,
    and video.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是基于神经网络的先进机器学习形式，它特别适用于文本、图像、音频和视频等非结构化数据。
- en: Deep learning finds applications across various sectors, such as
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习在各个领域都有应用，例如
- en: '*The medical field and pharmaceuticals*—Used for diagnosing medical conditions
    and expediting drug development'
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*医疗领域和制药业*——用于诊断医疗状况和加速药物开发'
- en: '*Banking and finance*—Detects fraud and distinguishes fake signatures'
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*银行和金融*——检测欺诈并区分伪造签名'
- en: '*The automobile sector*—Powers autonomous driving by recognizing traffic elements'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*汽车行业*——通过识别交通元素来推动自动驾驶'
- en: '*Speech and image recognition*—Enables technologies like Siri and image-based
    systems for medical diagnostics and security'
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语音和图像识别*——使Siri和基于图像的医疗诊断和安全系统等技术成为可能'
- en: Key concepts for neural networks include
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的关键概念包括
- en: '*Artificial neurons (perceptrons)*—Simplified models of biological neurons.
    Weights and biases play crucial roles in the function of a perceptron.'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工神经元（感知器）*——生物神经元的简化模型。权重和偏差在感知器的功能中起着至关重要的作用。'
- en: '*Layers*—Networks are structured with input, hidden, and output layers. Hidden
    layers extract and learn features critical for decision-making.'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层*——网络由输入层、隐藏层和输出层组成。隐藏层提取并学习对决策至关重要的特征。'
- en: '*Activation functions*—Critical for neural network performance and include
    sigmoid, TANH, LeLU, and softmax.'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*激活函数*——对神经网络性能至关重要，包括sigmoid、TANH、LeLU和softmax。'
- en: Training neural networks involves processes like feed-forward propagation, calculating
    loss, and employing backpropagation for weight adjustments to maximize prediction
    accuracy.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络涉及正向传播、计算损失和利用反向传播进行权重调整以最大化预测准确性的过程。
- en: While unsupervised learning relies on unlabeled data, techniques like Boltzmann
    learning and DBNs are central to improving data organization in such settings.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然无监督学习依赖于未标记的数据，但像霍尔兹曼学习和DBNs这样的技术对于在这样环境中改善数据组织至关重要。
- en: CNNs are primarily used in image and video processing. CNNs excel in recognizing
    patterns due to their architecture, featuring layers like convolutional and polling
    layers for feature extraction.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNs（卷积神经网络）主要用于图像和视频处理。CNNs因其架构而擅长识别模式，具有卷积层和池化层等用于特征提取的层。
- en: RNNs are suitable for sequential data. RNNs maintain information across inputs
    and are enhanced by LSTMs for long-term dependency challenges. They are key in
    natural language processing and time-series analysis.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNNs（循环神经网络）适用于序列数据。RNNs在输入之间保持信息，并通过LSTMs（长短期记忆网络）增强以解决长期依赖问题。它们在自然语言处理和时间序列分析中至关重要。
- en: The Boltzmann learning rule is an unsupervised, probabilistic method used in
    neural networks to adjust weights by minimizing an energy function, often aiding
    in tasks like dimensionality reduction and feature extraction, but computational
    challenges require approximations like contrastive divergence.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍尔兹曼学习规则是一种无监督、概率性的方法，用于神经网络中通过最小化能量函数来调整权重，通常有助于降维和特征提取等任务，但计算挑战需要如对比散度这样的近似方法。
- en: DBNs are GANs consisting of layers of RBMs, utilizing unsupervised pretraining
    to learn complex data representations and supervised fine-tuning for tasks like
    classification, yet they face challenges, including computational expense and
    potential overfitting.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBNs是包含RBMs（限制性玻尔兹曼机）层的GANs（生成对抗网络），利用无监督预训练来学习复杂的数据表示，并通过监督微调进行分类等任务，但它们面临着包括计算成本和潜在过拟合在内的挑战。
- en: DBNs use layer-wise pretraining to capture abstract features, making them suitable
    for complex applications like image or speech recognition; however, problems like
    the vanishing gradient problem and intricate fine-tuning processes can impede
    performance.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBNs使用分层预训练来捕捉抽象特征，使它们适用于图像或语音识别等复杂应用；然而，梯度消失问题和复杂的微调过程可能会阻碍性能。
- en: Despite newer deep learning architectures gaining popularity, DBNs remain integral
    to the evolution of AI, playing a critical role in the development of models for
    tasks including dimensionality reduction, generative modeling, and classification,
    although training complexity continues to be a barrier.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管新的深度学习架构越来越受欢迎，但DBNs（深度信念网络）仍然是人工智能发展的重要组成部分，在包括降维、生成建模和分类等任务模型的发展中扮演着关键角色，尽管训练复杂性仍然是一个障碍。
