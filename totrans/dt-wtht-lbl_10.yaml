- en: '8 Deep learning: The foundational concepts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Core building blocks of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised and unsupervised learning approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional and recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Boltzmann learning rule and deep belief networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python coding with TensorFlow and Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of deep learning libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The art of simplicity is a puzzle of complexity.—Douglas Horton
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Welcome to the third part of the book. So far, you have covered a lot of concepts
    and case studies and Python code. From this chapter onward, the level of complexity
    will be even higher.
  prefs: []
  type: TYPE_NORMAL
- en: In the first two parts of the book, we covered various unsupervised learning
    algorithms like clustering, dimensionality reduction, etc. We discussed both simpler
    and advanced algorithms. We also covered working on text data in the second part
    of the book. Starting from this third part of the book, we will start our journey
    on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and neural networks have changed the world and the business domains.
    You have probably heard about deep learning and neural networks. Their implementations
    and sophistication result in better cancer detection, autonomous driving cars,
    improved disaster management systems, better pollution control systems, reduced
    fraud in transactions, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the third part of the book, we will explore unsupervised learning using deep
    learning. We will study what deep learning is and the basics of neural networks,
    as well as the layers in a neural network, activation functions, the process of
    deep learning, and various libraries. Then we will move to autoencoders and generative
    adversarial networks (GANs) and generative AI (GenAI). The topics are indeed complex
    and sometimes quite mathematically heavy. We will use different kinds of datasets
    for working on the problems, but primarily the datasets will be unstructured in
    nature. As always, Python will be used to generate the solutions. We also share
    a lot of external resources to complement the concepts. Please note that these
    are advanced topics, and a lot of research is still ongoing for these topics.
  prefs: []
  type: TYPE_NORMAL
- en: We have divided the third part of the book into four chapters. This chapter
    covers the foundational concepts of deep learning and neural networks. The next
    two chapters focus on autoencoders, GAN and GenAI. The final chapter of the book
    talks about the deployment of these models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we discuss the concepts of neural networks and deep learning.
    We discuss what a neural network is, its activation functions, different optimization
    functions, the neural network training process, etc. The concepts covered in this
    chapter form the base of neural networks and deep learning and subsequent learning
    in the next two chapters. Hence, it is vital that you are clear about these concepts.
    The best external resources to learn these concepts in more detail are given at
    the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the eighth chapter, and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will continue to use the same version of Python and Jupyter Notebook as
    we have used so far. The codes and datasets used in this chapter have been checked
    in at the same GitHub location. You will need to install a couple of Python libraries
    in this chapter: `tensorflow` and `keras`.'
  prefs: []
  type: TYPE_NORMAL
- en: '8.1.1 Deep learning: What is it? What does it do?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning has gathered a lot of momentum in the past few years. Neural networks
    are pushing the boundaries of machine learning solutions. Deep learning is machine
    learning only. Deep learning is based on neural networks. It utilizes a similar
    concept—that is, using historical data and understanding the attributes and the
    intelligence gathered to find patterns or predict the future, albeit deep learning
    is more complex than the algorithms we have covered so far.
  prefs: []
  type: TYPE_NORMAL
- en: Recall chapter 1, where we covered the concepts of structured and unstructured
    datasets. Unstructured datasets include text, images, audio, video, etc. Figure
    8.1 describes the major sources of text, images, audio, and video datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Unstructured datasets like text, audio, images, and video can be
    analyzed using deep learning. There are multiple sources of such datasets.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'While deep learning can be implemented for structured datasets too, it is mostly
    working wonders on unstructured datasets. One of the prime reasons is that the
    classical machine learning algorithms are sometimes not that effective on unstructured
    datasets like that of images, text, audio, and video. A few of the path-breaking
    solutions delivered by deep learning across various domains are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The medical field and pharmaceuticals*—Deep learning sees application in areas
    such as the identification of bones and joint problems or in determining if there
    are any clots in arteries or veins. In the pharmaceutical field, it expedites
    clinical trials and helps to reach the target drug faster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The banking and financial sector*—Deep learning-based algorithms are used
    to detect potential fraud in transactions. Using image recognition-based algorithms,
    we can also distinguish fake signatures on checks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The automobile sector*—You have probably heard about autonomous driving (aka
    self-driving) cars. Using deep learning, the algorithms can detect traffic signals,
    pedestrians, other vehicles on the road, their respective distances, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retail*—In the retail sector, using deep learning-based algorithms, humans
    can improve customer targeting and develop advanced and customized marketing tactics.
    The recommended models to provide next-best products to the customers have been
    improved using deep learning. We can get better returns on investments and improve
    cross-sell and upsell strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, automatic speech recognition is possible with deep learning. Using
    sophisticated neural networks, humans can create speech recognition algorithms.
    These solutions are being used across Siri, Alexa, Translator, Baidu, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image recognition is also advancing. Neural networks are improving image recognition
    techniques. This can be done using convolutional neural networks, which are improving
    computer vision. Use cases include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is quite effective for differentiation between cancerous cells
    and benign cells. Differentiation can be achieved by using the images of cancerous
    cells and benign cells.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An automated number plate reading system has been developed using neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection methods and monitor sensing and tracking systems can be developed
    using deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In disaster management systems, deep learning can detect the presence of humans
    in affected areas. Just imagine how, during rescue operations, human lives can
    be saved using better detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI is changing the world rapidly. Use cases include automating content creation,
    such as writing articles, essays, and social media posts and generating images
    and videos. It improves customer service and customer experience by providing
    chatbots that provide instant, personalized responses to the queries of the customers.
    It can be implemented in any industry. In data-heavy industries, it creates ripples
    by summarizing complex and long documents and generating insights from dashboards
    and reports. These reports can be Power BI/Tableau dashboards, PowerPoints, or
    pdf files, for example. It has also helped software developers in code generation
    and debugging and has improved software development efficiency. The use cases
    are many, ranging from retail; telecommunications; healthcare; R&D; banking, finance,
    and insurance, etc., in improving sales, reducing costs, saving time, and improving
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The use cases listed are certainly not exhaustive. Using deep learning, we can
    improve natural language processing solutions used to measure customers’ sentiments,
    language translation, text classification, named-entity recognition, etc. Across
    use cases in bioinformatics, the military, mobile advertising, technology, the
    supply chain, and so on, deep learning is paving the path for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Building blocks of a neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial neural networks (ANNs) are said to be inspired by the way the human
    brain works. The human brain is the best machine we currently have access to.
    When we see a picture or a face or hear a tune, we associate a label or a name
    with it. That allows us to train our brain and senses to recognize a picture or
    a face or a tune when we see/hear it again. ANNs learn to perform similar tasks
    by learning or getting trained.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the meaning of deep learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks cannot be used for unsupervised learning. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore more use cases for deep learning in nonconventional business domains.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 8.2.1 Neural networks for solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In deep learning, too, the concepts of supervised and unsupervised learning
    are applicable. We cover both types of training of the network: supervised and
    unsupervised. This will give you a complete picture. At the same time, to fully
    appreciate unsupervised deep learning, you should be clear on the supervised deep
    learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the deep learning process by using an example. Consider this:
    we wish to create a solution that can identify faces—a solution that can distinguish
    faces and identify the person by allocating a name to the face. For training the
    model, we will use a dataset that will have images of people’s faces and corresponding
    names. The ANN will start with no prior understanding of the image’s dataset or
    the attributes. During the process of training, it will learn the attributes and
    the identification characteristics from the training data. These learned attributes
    are then used to distinguish between faces. At this moment, we are only covering
    the process at a high level; we will cover this process in much more detail in
    subsequent sections. Figure 8.2 shows a representation of a neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F02_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 A typical neural network with neurons and various layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The process in a neural network is quite complex. We will first cover all the
    building blocks of a neural network, like neurons, activation functions, weights,
    bias terms, etc., and then move on to the process followed in a neural network.
    Let’s start with the protagonist: a neuron.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Artificial neurons and perceptrons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The human brain contains billions of neurons. The neurons are interconnected
    cells in our brains. These neurons receive signals, process them, and generate
    results. Artificial neurons are based on biological neurons only and can be considered
    simplified computational models of biological neurons.
  prefs: []
  type: TYPE_NORMAL
- en: In 1943, researchers Warren McCullock and Walter Pitts proposed the concept
    of a simplified brain cell called the McCullock-Pitts neuron. It can be thought
    of as a simple logic gate with binary outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The working methodology for artificial neurons is similar to that of biological
    neurons, albeit artificial neurons are far simpler than biological neurons. A
    perceptron is a mathematical model of a biological neuron. In the actual biological
    neurons, dendrites receive electrical signals from the axons of other neurons.
    In a perceptron, these electrical signals are represented as numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: The artificial neuron receives inputs from the previous neurons or can receive
    the input data. It then processes that input information and shares an output.
    The input can be the raw data or processed information from a preceding neuron.
    The neuron then combines the input with its own internal state, weighs them separately,
    and passes the output received through a nonlinear function to generate output.
    These nonlinear functions are also called activation functions (we will cover
    them later). You can think of an activation function as a mathematical function.
    A neuron can be represented as shown in figure 8.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F03_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 A neuron gets the inputs, processes them using mathematical functions,
    and then generates the output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In simpler terms, a neuron can be termed as a mathematical function that computes
    the weighted average of its input datasets; then this sum is passed through activation
    functions. The output of the neuron can then be the input to the next neuron,
    which will again process the input received. Let’s go a bit deeper.
  prefs: []
  type: TYPE_NORMAL
- en: In a perceptron, each input value is multiplied by a factor called the *weight*.
    Biological neurons fire once the total strength of the input signals exceeds a
    certain threshold. A similar format is followed in a perceptron. In a perceptron,
    a weighted sum of the inputs is calculated to get the total strength of the input
    data, and then an activation function is applied to each of the outputs. Each
    output can then be fed to the next layer of perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that there are two input values, *a* and *b*, for a perceptron
    *X*, which for the sake of simplicity has only one output. Let the respective
    weights for *a* and *b* be *P* and *Q*. So the weighted sum can be calculated
    as *P* * *X* + *Q* * *b*. The perceptron will fire or will have a nonzero output
    only if the weighted sum exceeds a certain threshold. Let’s call the threshold
    *C*. So, we can say the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The output of *X* will be 0 if *P* * *X* + *Q* * *y* <= *C*.
  prefs: []
  type: TYPE_NORMAL
- en: The output of *X* will be 1 if *P* * *S* + *Q* * *y* > *C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we generalize this understanding, we can represent it as follows. Representing
    a perceptron as a function maps input *x* as the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: where *x* is the vector of input values, *w* represents the vector of weights,
    and *b* is the bias term. We explain the bias and the weight terms next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the linear equation: *y* = *mx* + *c* where *m* is the slope of the
    straight line and *c* is the constant term. Both bias and weight can be defined
    using the same linear equation.'
  prefs: []
  type: TYPE_NORMAL
- en: The role of weight is similar to the slope of the line in a linear equation.
    It defines the change in the value of *f*(*x*) by a unit change in the value of
    *x*.
  prefs: []
  type: TYPE_NORMAL
- en: The role of the bias is similar to the role of a constant in a linear function.
    In case there is no bias, the input to the activation function is *x* multiplied
    by the weight.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Weights and bias terms are the parameters that get trained in a network.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the function will depend on the activation function used. We will
    cover various types of activation functions in the next section after we have
    covered different layers in a network.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Different layers in a network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A simple and effective way of organizing neurons is the following. Rather than
    allowing arbitrary neurons connected with arbitrary others, neurons are organized
    in layers. A neuron in a layer has all its inputs coming only from the previous
    layer and all its output going only to the next. There are no other connections,
    for example, between neurons of the same layer or between neurons in neurons belonging
    to distant layers (with a small exception for specialized cases, which is beyond
    the scope of this book).
  prefs: []
  type: TYPE_NORMAL
- en: We know that information flows through a neural network. That information is
    processed and passed on from one layer to another layer in a network. There are
    three layers in a neural network, as shown in figure 8.4\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F04_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 A typical neural network with neurons and input, hidden, and output
    layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The neural network shown in figure 8.4 has three input units and two hidden
    layers with four neurons each and one final output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input layer*—As the name signifies, this receives the input data and shares
    it with the hidden layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hidden layer*—This is the heart and soul of the network. The number of hidden
    layers depends on the problem at hand; the number of layers can range from a few
    to hundreds. All the processing, feature extraction, and learning of the attributes
    is done in these layers. In the hidden layers, all the input raw data is broken
    into attributes and features. This learning is useful for decision-making at a
    later stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output layer*—This is the decision layer and final piece in a network. It
    accepts the outputs from the preceding hidden layers and then makes a prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, the input training data may have raw images or processed images.
    These images will be fed to the input layer. The data then travels to the hidden
    layers where all the calculations are done. These calculations are done by neurons
    in each layer. The output is the task that needs to be accomplished—for example,
    identification of an object or classification of an image, etc.
  prefs: []
  type: TYPE_NORMAL
- en: The ANN consists of various connections. Each of the connections aims to receive
    the input and provide the output to the next neuron. This output to the next neuron
    will serve as an input to it. Also, as discussed earlier, each connection is assigned
    a weight, which is representative of its respective importance. It is important
    to note that a neuron can have multiple input and output connections, which means
    it can receive inputs and deliver multiple outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: The input data is fed to the hidden layers in a neural network. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A bias term is similar to the slope of a linear equation. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find and explore the deepest neural network ever trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So what is the role of a layer? A layer receives inputs, processes them, and
    passes the output to the next layer. Technically, it is imperative that the transformation
    implemented by a layer is parameterized by its weights, which are also referred
    to as parameters of a layer. In simple terms, to ensure a neural network is “trained”
    to a specific task, something must be changed in the network. It turns out that
    changing the architecture of the network (i.e., how neurons are connected) has
    only a small effect. On the other hand, as we will see later in this chapter,
    changing the weights is the key to the “learning” process.
  prefs: []
  type: TYPE_NORMAL
- en: We now move to the very important topic of activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Activation functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already mentioned activation functions. The primary role of an activation
    function is to decide whether a neuron/perceptron should fire or not. These functions
    play a central role in the training of the network at a later stage. They are
    sometimes referred to as *transfer functions*. It is also important to know why
    we need nonlinear activation functions. If we use only linear activation functions,
    the output will also be linear. At the same time, the derivative of a linear function
    will be constant. Hence, there will not be much learning possible. Thus, we prefer
    to have nonlinear activation functions. We study the most common activation functions
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A sigmoid is a bounded monotonic mathematical function. It always increases
    its output value when the input values increase. Its output value is always between
    –1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: A sigmoid is a differentiable function with an S-shaped curve, and its first
    derivative function is bell-shaped. It has a nonnegative derivative function and
    is defined for all real input values. The sigmoid function is used if the output
    value of a neuron is between 0 and 1\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, a sigmoid function can be represented by equation 8.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: (8.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 8.5 shows a graph of a sigmoid function. The sigmoid function finds its
    applications in complex learning systems. It is usually used for binary classification
    and in the final output layer of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F05_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 A sigmoid function. Note the shape of the function and the min/max
    values.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: TANH function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In mathematics, the tangent hyperbolic (TANH) function is a differentiable hyperbolic
    function. It is a smooth function, and its input values are in the range of –1
    to +1.
  prefs: []
  type: TYPE_NORMAL
- en: 'A TANH function is written as equation 8.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: (8.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A graphical representation of TANH is shown in figure 8.6\. It is a scaled version
    of the sigmoid function, and hence a TANH function can be derived from a sigmoid
    function and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F06_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 A TAHN function, which is a scaled version of a sigmoid function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A TANH function is generally used in the hidden layers. It makes the mean closer
    to zero, which makes the training easier for the next layer in the network. This
    is also referred to as centering the data.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified linear unit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A rectified linear unit (ReLU) is an activation function that defines the positives
    of an argument. Equation 8.3 shows the ReLU function. Note that the value is 0
    even for the negative values, and from 0 the value starts to incline.
  prefs: []
  type: TYPE_NORMAL
- en: (8.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*F*(*x*) = max (0, *x*)'
  prefs: []
  type: TYPE_NORMAL
- en: It will give the output as *x* if positive, else 0\.
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU is a simple function and hence less expensive to compute and much faster.
    It is unbounded and not centered at zero. It can be differentiated at all places
    except zero. Since the ReLU function is less complex, it is computationally less
    expensive and, hence, is widely used in the hidden layers to train the networks
    faster. Figure 8.7 is a graphical representation of a ReLU function.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F07_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 A ReLU function. It is one of the favored activation functions in
    the hidden layers of a neural network. A ReLU is simple to use and less expensive
    to train.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Softmax function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The softmax function is used in the final layer of the neural network to generate
    the output from the network. It is an activation function that is useful for multiclass
    classification problems and forces the neural network to output the sum of 1\.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, say the distinct classes for an image are cars, bikes, or trucks.
    The softmax function will generate three probabilities for each category. The
    category that has received the highest probability will be the predicted category.
  prefs: []
  type: TYPE_NORMAL
- en: There are other activation functions too, like ELU, PeLU, etc., which are beyond
    the scope of this book. We provide a summary of various activation functions at
    the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We next cover hyperparameters, which are the control levers we have while the
    network is trained.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.5 Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During training a network, the algorithm is constantly learning the attributes
    of the raw input data. At the same time, the network cannot learn everything itself;
    there are a few parameters for which initial settings must be provided. These
    are the variables that determine the structure of the neural network and the respective
    variables that are useful to train the network.
  prefs: []
  type: TYPE_NORMAL
- en: A few examples of hyperparameters are the number of hidden layers in a network,
    the number of neurons in each layer, the activation functions used in layers,
    weight initialization, etc. We have to pick the best values of the hyperparameters.
    To do so, we select some reasonable values for the hyperparameters, train the
    network, measure the performance of the network, tweak the hyperparameters and
    retrain the network, reevaluate and retweak, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Hyperparameters are controlled by us, as we input hyperparameters to improve
    the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now move to the next important component in a neural network: optimization
    functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.6 Optimization functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In deep learning, optimizers play a critical role. They minimize the loss function
    by adjusting the model parameters, which are weights and biases. The optimizers
    facilitate faster convergence and improve the overall performance of the network.
    Some of the most commonly used optimization functions are discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Batch gradient descent, stochastic gradient descent, and mini-batch stochastic
    gradient descent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In any prediction-based solution, we want to predict as best as we can; or,
    in other words, we want to reduce the error as much as possible. Error is the
    difference between the actual values and the predicted values. The purpose of
    a machine learning solution is to find the optimum value for our functions. We
    want to decrease the error or maximize the accuracy. Gradient descent can help
    to achieve this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: The batch gradient descent technique is an optimization technique used to find
    the global minima of a function. We proceed in the direction of the steepest descent
    iteratively, which is defined by the negative of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: But batch gradient descent can be slow to run on very large datasets or datasets
    with a very high number of dimensions. This is due to the fact that one iteration
    of the gradient descent algorithm predicts for every instance in the training
    dataset. Hence, it is obvious that it will take a lot of time if we have thousands
    of records. For such a situation, we have stochastic gradient descent (SGD).
  prefs: []
  type: TYPE_NORMAL
- en: In SGD, rather than at the end of the batch of the data, the coefficients are
    updated for each training instance, and hence it takes less time.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 shows the way gradient descent works. Notice how we can progress
    downward toward the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F08_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 The concept of gradient descent. It is the mechanism to minimize
    the loss function.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Mini-batch gradient descent batches gradient descent and SGD by using small
    subsets of data. They are called mini-batches. In this fashion, it can balance
    both speed and accuracy. At the same time, it adds a hyperparameter, and we have
    to carefully tune the batch size. Generally, it is kept in the power of 2 (32,
    64, 128, 256, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive optimization algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Researchers have observed that there is a need for optimization algorithms
    for more complex tasks like image, text, video, or audio analysis. Hence, adaptive
    optimization solutions like momentum, Nesterov accelerated gradient (NAG), Adagrad,
    etc., have been developed. We provide a brief summary of these solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Momentum*—This optimizer adds a fraction of the previous gradient to the current
    gradient. The idea is to give more weight to the most recent update as compared
    to the previous updates. It accelerates the convergence and achieves better accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: and hence the weights are updated by *θ* = *θ* – *V*(*t*).
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the value of the momentum term (*γ*) is set to 0.9\. With momentum,
    the convergence is faster, but at the same time, we must compute one more variable
    for each update.
  prefs: []
  type: TYPE_NORMAL
- en: '*NAG*—This is an improvement over momentum. In momentum, if the value becomes
    too large, the optimizer might miss the local minima. Hence, NAG was developed.
    It is a look-ahead method wherein the weights are modified to determine the future
    location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we discuss the most widely used optimization algorithms in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Learning and learning rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a network, we take various steps to improve the performance of the solution:
    learning rate is one of them. The learning rate will define the size of the corrective
    steps that a model takes to reduce the errors. Learning rate defines the amount
    by which we should adjust the values of weights of the network with respect to
    the loss gradients (more on this process later). If we have a higher learning
    rate, the accuracy will be lower. If we have a very low learning rate, the training
    time will increase.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Compare and contrast the sigmoid and TANH functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ReLU is generally used in the output layer of the network. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient descent is an optimization technique. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have examined the main concepts of deep learning. Now let us study how a
    neural network works. You will learn how the various layers interact with each
    other and how information is passed from one layer to another.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 How does deep learning work in a supervised manner?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have covered the major components of a neural network. It is the time for
    all the pieces to come together and orchestrate the entire learning process. The
    training of a neural network is quite a complex process and can be examined in
    a step-by-step fashion.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering what is meant by “learning” of a neural network. Learning
    is a process to find the best and most optimized values for weights and bias for
    all the layers of the network so that we can achieve the best accuracy. As deep
    neural networks can have practically infinite possibilities for weights and bias
    terms, we have to find the optimum value for all the parameters. This seems like
    a herculean task considering that changing one value affects the other values,
    and indeed, it is a process where the various parameters of the networks are changing.
  prefs: []
  type: TYPE_NORMAL
- en: Recall in the first chapter we covered the basics of supervised learning. We
    will refresh that understanding here. The reason is to ensure that you are fully
    able to appreciate the process of training the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Supervised learning algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised learning algorithms have a “guidance” or “supervision” to direct
    toward the business goal of making predictions for the future. Formally put, supervised
    models are statistical models that use both the input data and the desired output
    to predict the future. The output is the value we wish to predict and is referred
    to as the *target variable,* and the data used to make that prediction is called
    the *training data*. The target variable is sometimes referred to as the *label*.
    The various attributes or variables present in the data are called *independent
    variables*. Each of the historical data points or *training examples* contain
    these independent variables and corresponding target variables. Supervised learning
    algorithms make a prediction for the unseen future data. The accuracy of the solution
    depends on the training done and patterns learned from the labeled historical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Most deep learning solutions are based on supervised learning. Unsupervised
    deep learning is rapidly gaining traction, however, as unlabeled datasets are
    far more abundant than labeled ones.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning problems are used in demand prediction, credit card fraud
    detection, customer churn prediction, premium estimation, etc. They are heavily
    used across retail, telecom, banking and finance, aviation, insurance, and other
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now refreshed the concepts of supervised learning. We now move on to
    the first step in the training of the neural network: feed-forward propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: '8.3.2 Step 1: Feed-forward propagation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us start the process that occurs in a neural network (see figure 8.9). This
    is the basic skeleton of a network we have created to explain the process. Let’s
    say we have some input data points and the input data layer, which will consume
    the input data. The information flows from the input layer to the data transformation
    layers (hidden layers). In the hidden layers, the data is processed using the
    activation functions and based on the weights and bias terms. Then a prediction
    is made on the dataset. This is called *feed-forward propagation,* as during this
    process, the input variables are calculated in a sequence from the input layer
    to the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F09_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 The basic skeleton of a neural network training process. We have
    the input layers and data transformation layers.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For example, say we wish to create a solution that can identify the faces of
    people. In this case, we will have the training data, which is different images
    of people’s faces from various angles, and a target variable, which is the name
    of the person.
  prefs: []
  type: TYPE_NORMAL
- en: This training dataset can be fed to the algorithm. The algorithm will then understand
    the attributes of various faces or, in other words, *learn* the various attributes.
    Based on the training done, the algorithm can then make a prediction on the faces.
    The prediction will be a probability score if the face belongs to Mr. X. If the
    probability is high enough, we can safely say that the face belongs to Mr. X.
  prefs: []
  type: TYPE_NORMAL
- en: '8.3.3 Step 2: Adding the loss function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output is generated in step 1\. Now we have to gauge the accuracy of this
    network. We want our network to have the best possible accuracy in identifying
    the faces. Using the prediction made by the algorithm, we will control and improve
    the accuracy of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy measurement in the network can be achieved by the loss function, also
    called the *objective function*. The loss function compares the actual values
    and the predicted values. The loss function computes the difference score and
    hence is able to measure how well the network has done and what the error rates
    are. Let’s update the diagram we created in step 1 by adding a loss function and
    corresponding loss score, used to measure the accuracy of the network, as shown
    in figure 8.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F10_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 A loss function has been added to measure the accuracy.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '8.3.4 Step 3: Calculating the error'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We generated the predictions in step 1 of the network. In step 2, we compared
    the output with the actual values to get the error in prediction. The objective
    of our solution is to minimize this error, which is the same as maximizing the
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: To constantly lower the error, the loss score (Predictions – Actual) is then
    used as feedback to adjust the value of the weights. This task is done by the
    backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 3 of the last section, we said we use an optimizer to constantly update
    the weights to reduce the error. While the learning rate defines the size of the
    corrective steps to reduce the error, backpropagation is used to adjust the connection
    weights. These weights are updated backward based on the error. Following this,
    the errors are recalculated, the gradient descent is calculated, and the respective
    weights are adjusted. Hence, backpropagation is sometimes called the central algorithm
    in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation was originally suggested in the 1970s. Then, in 1986, David
    Rumelhartm, Geoffrey Hinton, and Ronald Williams’s paper received a lot of appreciation.
    Nowadays, backpropagation is the backbone of deep learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11 shows the process for backpropagation, where the information flows
    from the output layer back to the hidden layers. Note that the flow of information
    is backward as compared to forward propagation, where the information flows from
    left to right.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F11_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11 Backpropagation as a process: the information flows from the final
    layers to the initial layers'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First, we describe the process at a very high level. Remember that in step 1,
    at the start of the training process, some random values were assigned to the
    weights. Using these random values, an initial output is generated. Since this
    is the first attempt, the output received can be quite different from the real
    values and the loss score is accordingly very high. But this is going to improve.
    While training the neural network, the weights (and biases) are adjusted a little
    in the correct direction, and subsequently, the loss score decreases. We iterate
    this training loop many times, and it results in the optimum weight values that
    minimize the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Backpropagation allows us to iteratively reduce the error during the network
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: The following section is mathematically heavy. If you are not keen to understand
    the mathematics behind the process, you can skip it.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 The mathematics behind backpropagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we train a neural network, we calculate a loss function. The loss function
    tells us how different the predictions from the actual values are. Backpropagation
    calculates the gradient of the loss function with respect to each of the weights.
    With this information, each weight can be updated individually over iterations,
    which reduces the loss gradually.
  prefs: []
  type: TYPE_NORMAL
- en: In backpropagation, the gradient is calculated backward—that is, from the last
    layer of the network through the hidden layers to the very first layer. The gradients
    of all the layers are combined using the calculus chain rule to get the gradient
    of any particular layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We go into more details of the process next. First, let’s denote a few mathematical
    symbols:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h*^(^(*i*)^)—output of the hidden layer *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*g*^(^(*i*)^)—activation function of hidden layer *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*^(^(*i*)^)—hidden weights matrix in the layer *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b*^(^(*i*)^)—bias in layer *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*—input vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N*—total number of layers in the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W*^(^(*i*)^)[*jk*]—weight of the network from node *j* in layer (*i*–1) to
    node *k* in layer *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*δ**A*/*δ**B*—partial derivative of *A* with respect to *B*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the training of the network, the input *x* is fed to the network, and
    it passes through the layers to generate an output *y*̂. The expected output is
    *y*. Hence, the cost function or the loss function to compare *y* and *y*̂ is
    *C*(*y*, *y*̂). Also, the output for any hidden layer of the network can be represented
    as equation 8.4
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-6x.png)'
  prefs: []
  type: TYPE_IMG
- en: (8.4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: where *i* (index) can be any layer in the network.
  prefs: []
  type: TYPE_NORMAL
- en: The final layer’s output is
  prefs: []
  type: TYPE_NORMAL
- en: (8.5)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*y*(*x*) = *W*^(^(*N*)^)^(*T*) *h*^(^(*N*)^–^(1)) + *b*^(^(*N*)^)'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the training of the network, we adjust the network’s weights so that
    *C* is reduced. Hence, we calculate the derivative of *C* with respect to every
    weight in the network. The following is the derivative of *C* with respect to
    every weight in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-8x.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we know that a neural network has many layers. The backpropagation algorithm
    starts at calculating the derivatives at the last layer of the network, which
    is the N^(th) layer. Then these derivatives are fed backward. So the derivatives
    at the *N*^(th) layers will be fed to the (*N* – 1) layer of the network and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each component of the derivatives of *C* is calculated individually using the
    calculus chain rule. As per the chain rule, for a function *c* depending on *b*,
    where *b* depends on *a*, the derivative of *c* with respect to *a* can be written
    as equation 8.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-9x.png)'
  prefs: []
  type: TYPE_IMG
- en: (8.6)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Hence, in backpropagation the derivatives of the layer *N* are used in the layer
    (*N* – 1) so that they are saved and again used in the (*N* – 2) layer. We start
    with the last layer of the network, through all the layers to the first layer,
    and each time, we use the derivatives of the last calculations made to get the
    derivatives of the current layers. Hence, backpropagation turns out to be extremely
    efficient compared to a normal approach where we would have calculated each weight
    in the network individually.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have calculated the gradients, we update all the weights in the network.
    The objective is to minimize the cost function. We have already studied methods
    like gradient descent in the last section. We now continue to the next step in
    the neural network training process.
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.2 Step 4: Optimization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Backpropagation allows us to optimize our network and achieve the best accuracy
    (see figure 8.12). Notice the optimizer, which provides regular and continuous
    feedback to reach the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F12_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 Optimization is the process to minimize the loss function.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once we have achieved the best values of the weights and biases for our network,
    we say that our network is trained. We can now use it to make predictions on an
    unseen dataset that has not been used for training the network.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 How deep learning works in an unsupervised manner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know that unsupervised learning solutions work on unlabeled datasets; thus,
    for deep learning in unsupervised settings, the training dataset is unlabeled.
  prefs: []
  type: TYPE_NORMAL
- en: As compared to supervised datasets where we have tags, unsupervised methods
    have to self-organize themselves to get densities, probabilities’ distributions,
    preferences, and groupings. We can solve a similar problem using supervised and
    unsupervised methods. For example, a supervised deep learning method can be used
    to identify dogs versus cats while an unsupervised deep learning method might
    be used to cluster the pictures of dogs and cats into different groups. In machine
    learning, a lot of solutions that were initially conceived as supervised learning
    ones, over a period of time, employed unsupervised learning methods to enrich
    the data and hence improve the supervised learning solution.
  prefs: []
  type: TYPE_NORMAL
- en: During the learning phase in unsupervised deep learning, it is expected that
    the network will mimic the data and then improve itself based on the errors. In
    the supervised learning algorithm, other methods play the same part as the backpropagation
    algorithm. These include, among others,
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann learning rule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrastive divergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum likelihood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopfield learning rule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep belief network (DBN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we cover autoencoders and GAN in depth in separate chapters. The
    rest of the methods are covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we study the two most widely used types of neural networks in supervised
    learning settings: the convolutional neural network (CNN) and the recurrent neural
    network (RNN).'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Write in a simple form the major steps in a backpropagation technique.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagation is preferred in unsupervised learning. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The objective of deep learning is to maximize the loss function. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 8.6 Convolutional neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNNs are a class of deep learning models that are primarily used for image and
    video processing tasks. They have become a powerful tool in the field of computer
    vision due to their ability to automatically detect and learn the pattern from
    raw images and, hence, are used for several use cases across multiple domains
    and functions. We provide only a brief overview, as there can be an entire book
    on different types of CNN solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.1 Key concepts of CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the key concepts of CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input layer*—The input to the CNN is generally a tensor representing an image.
    As we know, an image is made up of pixels, and each pixel is made up of RGB channels.
    An image is represented by a 3D matrix, which is a width × height channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Convolution layer*—This is the core building layer of a CNN. It applies a
    filter to the input data, which scans over the image to detect patterns like lines,
    curves, texture, edges, etc. The filter size is generally small and usually 3
    × 3 or 5 × 5\. As the kernel slides over the input, it performs an element-wise
    multiplication and sum, creating a feature map. Multiple filters can be applied
    to learn different features, generating multiple feature maps. The entire process
    is illustrated in figure 8.13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F13_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 CNN process. The original data is 6 × 6, and the filter applied
    is 3 × 3, which results in a 4 × 4 output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*ReLU activation function*—This is applied to add nonlinearity. It helps the
    network to understand and model more complex and difficult patterns that are present
    in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Polling layer*—This is used to reduce the spatial dimensions of images while
    preserving the most significant details. The most common type of pulling is called
    max pulling. It takes the maximum value from a region of input. The major function
    of the pooling layer is to reduce the computation load and also reduce overfitting
    by providing a form of translation in variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output*—After we have created several convolutional and pooling layers, we
    receive the output. It is generally flattened into a 1D vector and the output
    is then passed to the fully connected layer. The main task of the fully connected
    layer is to perform high-level classification of the image based on the features
    extracted by the previous layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output layer*—If the solution is for classification of data points, the output
    layer would contain a function like softmax. The softmax function gives respective
    probabilities for different classes. For example, if you are trying to predict
    that a given picture is a cat or a dog, the softmax function will give the probability
    of the picture being a dog or a cat.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In CNNs, the same filter is applied across different regions of the image. Thus
    the number of parameters is reduced as compared to a traditional fully connected
    network. Each neuron in the convolutional layer is connected only to a small region
    of the input, and so the complexity of the network is also reduced. The network
    also automatically trains and learns to detect low-level patterns. An example
    of a low-level pattern is edges. The network subsequently progresses to learn
    more complex patterns like shapes in the deeper layers.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.2 Use of CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Call networks are fundamental and foundational to the modern-day competition
    solutions. They are heavily used for image classification, image processing, speech
    recognition, developing computer board games, and various other video processing
    solutions. Many solutions are developed using CNN—for example, automatic detection
    of vehicle license plates, detection of cancerous cells from scans, detection
    of broken bones from x-rays, facial recognition solutions, automatic entry handwriting,
    recognition solutions, and many other solutions that are having an amazing affect
    across our lives.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are quite a few CNN architectures available, like Inception, ResNet,
    LeNet, VGG-16, etc., that are useful for creating computer vision solutions. We
    now move on to the second common type of neural network: RNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.7 Recurrent neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RNNs are quite a popular class of networks that are designed to recognize patterns
    in a sequence of data—for example, time service data or videos, natural languages,
    or any other kind of data with this sequence of information. Here RNNs are very
    useful. The most significant feature of RNNs is their ability to maintain a memory
    about the previous input, which they capture using temporal dependencies and the
    order in the dataset. This augments their capability to recognize patterns in
    the sequential datasets, and hence RNN has been found to be a parting solution
    in multiple domains.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7.1 Key concepts of RNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs are especially designed for sequential datasets, and here the order of
    the input display plays a pivotal role. Hence, RNNs are the go-to solution for
    sequential data handling.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a regular neural network, which is also known as a feed-forward neural
    network, RNNs have recurrent connections. This means that the output from one
    time step is fed back as the input to the next time step. This information is
    persistent across the sequence. At the same time, the same weight is used across
    different time steps. This makes them very efficient in terms of the number of
    parameters, as the same network can be applied to every time step of the input
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs work in the following fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: The input data is processed sequentially. At each time step *t*, the network
    receives an input *x*[*t*], which is then combined with a hidden state *h*[*t*][–1].
    This hidden state is the output from the previous time step and serves as a memory
    that carries information from one time step to the next time step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The hidden state *h*[*t*] is then updated using a nonlinear function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-10x.png)'
  prefs: []
  type: TYPE_IMG
- en: The final output at each of the time steps can be calculated and used either
    for each individual time step or only at the final time step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 8.14 illustrates the RNN process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F14_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 The RNN process. RNNs have internal memory, which allows them to
    use information from the previous inputs to influence the current input and outputs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The most basic version of an RNN is a simple recurrent network, but it struggles
    with a long-term dependency because the gradient can either vanish or explode,
    making it hard for the network to remember information from far back in the sequence;
    hence, it cannot be used for a solution like a chatbot. Long short-term memory
    (LSTM) is much more useful here. LSTM is a special type of network designed to
    mitigate the vanishing gradient problem and handle long-term dependencies better
    than plain vanilla RNNs. They achieve this feat by introducing gates. There are
    three types of gates: input, forget, and output gates. These gates regulate the
    flow of information through the network and allow it to maintain important information
    over longer periods of time. Gated recurrent units are another type of RNN, but
    LSTM and gated recurrent units are beyond the scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are very powerful for processing sequences, and their ability to model
    time dependencies makes them indispensable in the fields of natural language processing
    and time-series analysis. Their use has been pathbreaking for many innovative
    solutions—for example, predicting the next word in a sentence; translating text
    from one language to another; processing sequences of video frames to understand
    behaviors over time; modeling temporal dependencies like audio signals, which
    can be used to recognize speech patterns over time; and many more. RNNs are the
    power engines behind GenAI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8 Boltzmann learning rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Boltzmann learning rule is an unsupervised learning rule used in neural
    networks. It is based on the principle of statistical mechanics of physical systems.
    It is seldom used in the context of Boltzmann machines. It adjusts the weights
    of a neural network with an objective to minimize the energy of the system, thereby
    ensuring the network reaches a stable state.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8.1 Concepts of the Boltzmann learning rule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the key concepts of the Boltzmann learning rule:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a type of probabilistic RNN where neurons are connected in a fully connected
    graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neurons in the Boltzmann machine are stochastic units that fire as per a
    probability distribution. Thus we can use the Boltzmann learning rule for dimensionality
    reduction, pattern recognition, feature extraction, and optimization tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Boltzmann machine has an energy function *E*(*v*,*h*) where *v* is the input
    visible unit while *h* is the hidden unit. The energy function determines the
    cost of a given state of the network. During the training of the network, we aim
    to adjust the weights in such a manner so that the energy of the system is minimized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The network models the probability of a particular state (*v*,*h*) using a
    Boltzmann distribution. It depends on the energy of the state, which is given
    by equation 8.7:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-11x.png)'
  prefs: []
  type: TYPE_IMG
- en: (8.7)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here, *Z* is the partition function, which ensures that the sum of probabilities
    = 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rule seeks to adjust the weights to keep on decreasing the energy of the
    system during the training of the network, and it happens over time. The weights
    are updated by a rule derived from gradient of the energy function with respect
    to the weights. The weight update rule is given in equation 8.8:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch8-eqs-12x.png)'
  prefs: []
  type: TYPE_IMG
- en: (8.8)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here, *h* is the learning rate, and (*v*[*i*]*h*[*j*])[data] is the correction
    between the visible unit *v*[*i*] and hidden unit *h*[*j*]. It is computed from
    the data distribution. It represents how often they are active together in the
    hidden unit. (*v*[*i*]*h*[*j*])[model] is the correction computed from the model
    distribution. It represents how often the visible unit *v*[*i*] and hidden unit
    *h*[*j*] are active together in the state generated by the network.
  prefs: []
  type: TYPE_NORMAL
- en: During the training of the model, a learning rule is followed, which is to make
    the data distribution match the model distribution. Hence, it reduces the energy
    of the system and thereby increases the overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8.2 Key points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are certain key points we should bear in mind. Energy-based models like
    the Boltzmann machine use the Boltzmann learning rule to minimize an energy function
    by adjusting the network’s weights:'
  prefs: []
  type: TYPE_NORMAL
- en: The network strives to model the probability distribution over its inputs. The
    core objective here is to associate the higher energy with less likely configurations.
    Similarly, the lower energy is associated with more like configurations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boltzmann learning is an unsupervised and probabilistic method. It works on
    the concept of contrasting the model distribution and data distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rule is computationally expensive in its basic form; hence, to increase
    the training speed, sometimes we utilize methods like contrastive divergence.
    We cover contrastive divergence in the next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Boltzmann learning rule is primarily used for unsupervised learning tasks
    such as dimensionality reduction, feature extraction, and generative modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model training is sometimes slower than expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, the Boltzmann learning rule is a probabilistic approach to training
    neural networks by adjusting weights based on minimizing an energy function, and
    it provides a foundation for generative models like Boltzmann machines. However,
    due to computational challenges, approximations such as contrastive divergence
    are often used to make it practical for real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 8.9 Deep belief networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A DBN is a type of GAN made up of multiple layers of stochastic, binary latent
    variables (hidden units), where each layer is a restricted Boltzmann machine (RBM)
    or a variant of it. DBNs were popularized by Geoffrey Hinton (who was awarded
    the Nobel Prize in Physics in 2024, shared with John Hopfield) and his collaborators
    in the mid-2000s for pretraining deep networks in an unsupervised way.
  prefs: []
  type: TYPE_NORMAL
- en: 8.9.1 Key points of DBN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The key points of a DBN are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: RBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A DBN consists of several layers of RBMs. A RBM contains a visible layer and
    a hidden layer. The visible layer represents the observed data while the hidden
    layer captures the hidden features.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each DBN is trained independently with an objective to model the underlying
    structure of the data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The objective of the training in DBN is to optimize the log-likelihood of the
    data under the network’s generative model. For each layer, the contrastive divergence
    algorithm is used to approximate the gradient of the log-likelihood with respect
    to the weights. This allows the network to learn a good set of weights for each
    layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The contrastive divergence algorithm is a stochastic approximation method used
    to estimate the gradient of the log-likelihood of the model. The algorithm starts
    with a sample from the visible layer and then performs Gibbs sampling to update
    the hidden layer and visible layer iteratively. Contrastive divergence ensures
    that the network learns to model the input data distribution efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Layer-based pretraining:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBNs are typically trained in a layer-wise manner, where each layer is pretrained
    as an RBM. The first RBM has an objective to learn to capture low-level features
    from the data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on this knowledge, each subsequent RBM then learns increasingly complex,
    abstract features from the representations learned by the previous layers. In
    this manner, the cycle continues.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This phase involves training each RBM individually using contrastive divergence.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This process tunes the weights to capture relevant patterns and features in
    the input data, without the need for labeled data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since each layer learns features at increasing levels of abstraction and complexity,
    it makes the overall solution good enough for complex tasks like image or speech
    recognition.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supervised fine-tuning:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the pretraining is done, the entire network is fine-tuned. It is done in
    a supervised fashion using methods like backpropagation or a labeled dataset with
    an objective to optimize the network.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The supervised system adjusts the network weights to minimize the prediction
    error such as what is done in classification or regression tasks.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The unsupervised pretraining phase helps initialize the weights in such a way
    that the network is less likely to overfit during supervised fine-tuning, as it
    starts with a better understanding of the data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They are computationally expensive and time-consuming, particularly when dealing
    with large datasets or deep architectures.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretraining using RBMs is useful, but fine-tuning the entire DBN can sometimes
    be difficult, especially if we are dealing with a very deep neural network. It
    may necessitate meticulous hyperparameter training and lots of labeled datasets.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to other deep learning architectures, DBNs are also prone to the vanishing
    gradient problem, where gradients diminish as they are propagated backward through
    many layers. This further complicates the entire training process.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DBNs are typically used for unsupervised learning, dimensionality reduction,
    and feature learning, but they can also be fine-tuned for supervised tasks such
    as classification. DBNs are used to improve the performance of speech recognition
    systems by learning representations of sound features that are invariant to noise
    and other distortions. As generative models, DBNs can be used to create new data
    instances that resemble the training data. For example, DBNs have been used in
    generative art, where new images are created that resemble a set of input images.
  prefs: []
  type: TYPE_NORMAL
- en: DBNs are a significant milestone in the development of deep learning techniques.
    They combine the strengths of generative models like RBMs with deep learning principles
    to create a powerful method for learning complex representations of data. While
    newer architectures have emerged and gained prominence, DBNs remain a key historical
    and theoretical component of modern AI, influencing the development of many advanced
    models. By utilizing unsupervised learning, DBNs can be highly effective for tasks
    like dimensionality reduction, generative modeling, and classification. However,
    challenges related to training complexity and fine-tuning remain significant hurdles
    for widespread adoption.
  prefs: []
  type: TYPE_NORMAL
- en: 8.10 Popular deep learning libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last few chapters, we have used a lot of libraries and packages for
    implementing solutions. There are quite a few libraries available in the industry
    for deep learning. These packages expedite the solution building and reduce the
    efforts as most of the heavy lifting is done by these libraries.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular deep learning libraries are
  prefs: []
  type: TYPE_NORMAL
- en: '*TensorFlow (TF)**—*Developed by Google, this is arguably one of the most popular
    and widely used deep learning frameworks. It was launched in 2015 and since has
    been used by a number of businesses and brands across the globe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python is mostly used for TF but C++, Java, C#, Javascript, and Julia can also
    be used. You have to install the TF library on your system and import the library.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Go to [www.tensorflow.org/install](http://www.tensorflow.org/install)
    and follow the instructions to install TF.
  prefs: []
  type: TYPE_NORMAL
- en: TF is one of the most popular libraries and can work on mobile devices like
    iOS and Android.
  prefs: []
  type: TYPE_NORMAL
- en: '*Keras*—Keras is a mature API-driven solution and quite easy to use. It is
    one of the best choices for starters and is among the best for prototyping simple
    concepts in an easy and fast manner. Keras was initially released in 2015 and
    is one of the most recommended libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE  Go to [https://keras.io](https://keras.io) and follow the instructions
    to install Keras. Tf.keras can be used as an API.
  prefs: []
  type: TYPE_NORMAL
- en: Serialization/deserialization APIs, call-backs, and data streaming using Python
    generators are very mature. Massive models in Keras are reduced to single-line
    functions, which makes it a less configurable environment and hence very convenient
    and easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: '*PyTorch*—Facebook’s brainchild PyTorch was released in 2016 and is another
    popular framework. PyTorch operates with dynamically updated graphs and allows
    data parallelism and distributed learning models. There are debuggers like pdb
    or PyCharm available in PyTorch. For small projects and prototyping, PyTorch can
    be a good choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sonnet*—DeepMind’s Sonnet is developed using and on top of TF. Sonnet is designed
    for complex neural network applications and architectures. It works by creating
    primary Python objects corresponding to a particular part of the neural network.
    Then these Python objects are independently connected to the computational TF
    graph. Because of this separation (creating Python objects and associating them
    to a graph), the design is simplified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE  Having high-level object-oriented libraries is very helpful, as the abstraction
    is allowed when we develop machine learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '*MXNet*—Apache’s MXNet is a highly scalable deep learning tool that is easy
    to use and has detailed documentation. A large number of languages like C ++,
    Python, R, Julia, JavaScript, Scala, Go, and Perl are supported by MXNet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other frameworks too, like Swift, Gluon, Chainer, DL4J, etc.; however,
    we only discuss the popular ones here. We now examine a short code in TF and Keras.
    It is just to test that you have installed these libraries correctly. You can
    learn more about TF at [https://www.tensorflow.org](https://www.tensorflow.org)
    and Keras at [https://keras.io](https://keras.io).
  prefs: []
  type: TYPE_NORMAL
- en: 8.10.1 Python code for Keras and TF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We implement a very simple code in TF. We simply import the TF library and
    print “hello”. We also check the version of TF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If this code runs for you and prints the version of TF, it means that you have
    installed `tensorflow` correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If this code runs for you and prints the version of Keras, it means that you
    have installed `keras` correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 8.11 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is changing the world we live in. It is enabling us to train and
    create really complex solutions that were a mere thought earlier. The effect of
    deep learning can be witnessed across multiple domains and industries. Perhaps
    there are no industries that have been left unaffected by the marvels of deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is one of the most-sought-after fields for research and development.
    Every year, many journals and papers are published on deep learning. Researchers
    across prominent institutions and universities (like Oxford, Stanford, etc.) of
    the world are engrossed in finding improved neural network architectures. At the
    same time, professionals and engineers in reputed organizations (like Google,
    Facebook, etc.) are working hard to create sophisticated architectures to improve
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is making our systems and machines able to solve problems typically
    assumed to be in the realm of humans only. We have improved the clinical trials
    process for the pharma sector, fraud detection software, automatic speech detection
    systems, and various image recognition solutions; and created more robust natural
    language processing solutions, targeted marketing solutions that improve customer
    relationship management and recommendation systems, better safety processes, and
    so on. The list is quite long and growing day by day.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, there are still a few challenges. The expectations from deep
    learning continue to increase. Deep learning is not a silver bullet or a magic
    wand to resolve all problems. It is surely one of the more sophisticated solutions,
    but it is certainly not the 100% solution to all business problems. The dataset
    we need to feed the algorithms is not always available. There is a dearth of good-quality
    datasets that are representative of business problems. Often, big organizations
    like Google, Meta, or Amazon can afford to collect such massive datasets. But
    many times we do find a lot of quality problems in the data. Having the processing
    power to train these complex algorithms is also a challenge. With the advent of
    cloud computing, though, this problem has been resolved to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explored the basics of neural networks and deep learning.
    We covered the details around neurons, activation function, different layers of
    a network, and loss function. We also covered in detail the backpropagation algorithm—the
    central algorithm used to train a supervised deep learning solution. Then we briefly
    went through unsupervised deep learning algorithms. We will cover these unsupervised
    deep learning solutions in greater detail in the later chapters. Figure 8.15 shows
    the major activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F15_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.15 Major activation functions at a glance (Source: towardsdatascience)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 8.12 Practical next steps and suggested readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  prefs: []
  type: TYPE_NORMAL
- en: The book *Deep Learning with Python* by François Chollet is one of the best
    resources to clarify the concepts of deep learning. It covers all the concepts
    of deep learning and neural networks and is written by the creator of Keras.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Read the following research papers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the Knowledge in a
    Neural Network. [https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Srivastava, R., Greff, K., and Schmidhuber, J. (2015). Training Very Deep Networks.
    [https://arxiv.org/pdf/1507.06228](https://arxiv.org/pdf/1507.06228)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013). Distributed
    Representations of Words and Phrases and their Compositionality. [https://arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). Generative Adversarial
    Networks. [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image
    Recognition. [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is an advanced form of machine learning based on neural networks,
    and it’s particularly effective with unstructured data like text, images, audio,
    and video.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning finds applications across various sectors, such as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The medical field and pharmaceuticals*—Used for diagnosing medical conditions
    and expediting drug development'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Banking and finance*—Detects fraud and distinguishes fake signatures'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The automobile sector*—Powers autonomous driving by recognizing traffic elements'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Speech and image recognition*—Enables technologies like Siri and image-based
    systems for medical diagnostics and security'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Key concepts for neural networks include
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Artificial neurons (perceptrons)*—Simplified models of biological neurons.
    Weights and biases play crucial roles in the function of a perceptron.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Layers*—Networks are structured with input, hidden, and output layers. Hidden
    layers extract and learn features critical for decision-making.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Activation functions*—Critical for neural network performance and include
    sigmoid, TANH, LeLU, and softmax.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Training neural networks involves processes like feed-forward propagation, calculating
    loss, and employing backpropagation for weight adjustments to maximize prediction
    accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While unsupervised learning relies on unlabeled data, techniques like Boltzmann
    learning and DBNs are central to improving data organization in such settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs are primarily used in image and video processing. CNNs excel in recognizing
    patterns due to their architecture, featuring layers like convolutional and polling
    layers for feature extraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs are suitable for sequential data. RNNs maintain information across inputs
    and are enhanced by LSTMs for long-term dependency challenges. They are key in
    natural language processing and time-series analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Boltzmann learning rule is an unsupervised, probabilistic method used in
    neural networks to adjust weights by minimizing an energy function, often aiding
    in tasks like dimensionality reduction and feature extraction, but computational
    challenges require approximations like contrastive divergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBNs are GANs consisting of layers of RBMs, utilizing unsupervised pretraining
    to learn complex data representations and supervised fine-tuning for tasks like
    classification, yet they face challenges, including computational expense and
    potential overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBNs use layer-wise pretraining to capture abstract features, making them suitable
    for complex applications like image or speech recognition; however, problems like
    the vanishing gradient problem and intricate fine-tuning processes can impede
    performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite newer deep learning architectures gaining popularity, DBNs remain integral
    to the evolution of AI, playing a critical role in the development of models for
    tasks including dimensionality reduction, generative modeling, and classification,
    although training complexity continues to be a barrier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
