# 第二章. 提示的艺术：与 AI 有效沟通

在 vibe coding 中，提示是新的源代码。

你向 AI 传达意图的方式直接影响到它生成的代码质量。编写一个好的提示既是艺术也是科学，通常被称为*提示工程*。本章将为你提供技巧，让你充分利用你的 AI 编码助手。我们将从为什么提示很重要的一些基本原理开始，然后深入探讨一系列提示技巧，从简单到高级。通过学习如何制作有效的提示以及如何迭代地改进它们（图 2-1），你将能够更高效、更准确地与 AI 合作。

![图片](img/bevc_0201.png)

###### 图 2-1. 一个聊天机器人协助编程的插图。开发者和 AI 进行对话：开发者提供指令或问题（提示），AI 则回应代码或答案。通过精心制作的提示与 AI 有效沟通是获得准确和有用代码生成的关键。

# 提示工程基础

如果 vibe coding 是你和 AI 模型之间的对话，那么*提示工程*就是用 AI 的语言说话以获得最佳结果的技能。一个精心制作的提示可以区分无关或带错误的代码建议和完美的解决方案。掌握提示工程意味着理解如何有效地引导 AI，如何提供上下文，以及当第一个答案不太正确时如何与 AI 迭代。

当你用 AI 编程时，你实际上是通过自然语言通过 AI 编程的。你提供的提示就像是一种高级编程语言，AI 解释器随后将其翻译成实际的代码。就像编译器的输出只取决于其输入的源代码一样，AI 的输出也只取决于提示。

为什么提示如此重要？尽管 LLMs（大型语言模型）非常复杂，但它们并不是心灵感应者。它们只对它们接收到的输入做出反应。含糊不清或措辞不当的提示可能导致无关或错误的代码，而一个清晰具体的提示则可能在第一次尝试时就提供完美的解决方案。在传统的编码中，你花费时间思考算法并编写代码；在 vibe coding 中，你花费时间思考如何向 AI 传达你的需求。这改变了“编写代码”的含义：你可能写一段文字而不是一个函数，但你仍然需要精确和逻辑。

将编写提示想象成为一位非常字面和刻板的初级开发者编写文档或用户故事，这位开发者会严格按照（并且仅）文档说明去做，他拥有大量知识，但除了他们看到过的模式之外没有常识。如果你的指令（提示）留下了可以解释的空间，AI 可能会以你未预料到的方式填补这些空白。因此，学会与 AI 进行沟通与学习编程语言的语法一样至关重要。

提示至关重要的另一个原因是可重复性和未来保障。如果你发现一个提示可以可靠地生成针对特定模式或任务的优质代码，那么这个提示就成了一项宝贵的知识（几乎像是一个片段或模板）。你可能保存它或在类似情境中重用它。在团队中，开发者可能会相互分享有效的提示模式，类似于他们分享编码最佳实践的方式。

最后，随着模型变得更好和更集成，它们可能允许更复杂的交互。擅长提示将让你能够快速利用新功能。例如，一些高级系统允许你附加广泛的指令或提供整个参考文档作为模型的上下文。了解如何构建这种输入是利用这种力量的关键。

所以，将提示写作视为一项新的基本技能。在许多方面，提示实际上就是编程。主要区别在于你使用的是一种语言（如英语），然后 AI 将其转换为代码。但你在描述中仍然需要清晰、逻辑性强，并预见到边缘情况。

# 具体性和清晰性：编写能够达到目的的提示

提示的一个黄金法则（我将在第三章（ch03.html#ch03_the_70_problem_ai_assisted_workflows_that_actual_1752630043200933）中更详细地阐述）是明确且清晰地表达你的需求。与人类合作者不同，AI 并不真正理解你提供单词之外的目标。一个常见的错误是给 AI 一个非常高级的提示，比如“制作一个网站”，并期望出现魔法。AI 更擅长处理具体细节。

总是假设它对你提供的项目一无所知。包括相关的细节，如编程语言、框架和库，以及具体的函数或代码片段。如果有错误，请提供确切的错误消息并描述代码应该做什么。任何含糊不清或解释空间都可能导致意外的输出。

例如，与其说“编写一个排序函数”，你可以说：

> 编写一个 Python 函数`sort_by_lastname(customers)`，该函数接受一个包含客户记录的列表（每个记录都有一个`first_name`和`last_name`字段）并按`last_name`字母顺序返回排序后的列表。包括一个简短的文档字符串，并处理缺少姓氏的情况，将其视为空字符串。

这个提示明确了关于语言（Python）、函数名称和目的、输入结构、排序键、额外要求（文档字符串）和边缘情况的期望。它很可能会产生你需要的或非常接近你需要的答案。本质上，要像编写规范一样：你指定的任务越精确，AI 需要猜测的就越少，你需要的修订也就越少。

提高具体性的策略包括：

提及语言或环境

如果你想要 JavaScript 解决方案，请明确指出：“写一个 JavaScript 函数...”而不是仅仅“写一个函数...”。如果你需要特定框架或版本，请包含它（“使用 React Hooks...”或“在 Python 3...”）。

定义输出范围

你只需要一个函数吗？一个完整的文件或模块？包括测试？例如，“仅提供函数实现”和“提供完整的可运行脚本”可以产生不同的响应。

包含需求和约束

在登录示例中，我们指定了密码长度和尝试限制。考虑边缘情况或约束，并将它们放入提示中。如果你需要代码针对性能进行优化或使用特定的算法，请说明：“使用 O(n)时间和 O(1)空间”或“使用二分搜索方法。”

避免含糊不清的引用

不要在没有明确先行词的情况下使用像“它”这样的词。例如，不要说“处理它并返回结果”，而要说“处理数组并返回结果数组”。

命名你期望的输出格式

如果你希望 AI 仅输出代码或带注释的代码或解释，你可以指示：“仅提供代码，无需解释”或“为每个步骤提供代码和简要注释。”

清晰的提示为 AI 的成功奠定了基础。如果你发现 AI 的答案经常需要大量纠正，请检查你的提示是否可能过于不具体。

这里是不要做的事情：

不要写一整篇小说

过于冗长且包含无关信息的提示可能会使模型困惑或导致它关注错误的事情。描述要简洁完整。例如，在编码环境中，通常不需要以“你是一位世界级的程序员...”作为前言（一些人在一般 ChatGPT 使用中这样做，但在编码任务中，这通常是多余的，可能会增加噪音）。

不要假设 AI 会自行正确填充细节

如果某些事情很重要（如线程安全、特殊字符的处理等），请提及。如果没有提及，假设 AI 可能不会处理它。

当你需要确定性输出时，避免开放式的“创意”提示

例如，说“写一些代码来分析数据”可能会让 AI 猜测你想要的分析类型。相反，请具体指定：

> 计算数字列表的平均值和标准差。

总结来说，*明确表达你的意思*。AI“了解”你真正想要的东西越多，它就能提供得越好。如果你发现自己不得不多次纠正 AI，请问：我的初始提示是否可以更清晰？

# 迭代优化：与 AI 的反馈循环

即使有清晰的提示，你也不一定能在第一次尝试时就得到完美的答案。将与 AI 的交互视为对话或迭代开发过程。这正是我在第一章中提到的反馈循环。

当 AI 给你代码时，要像审查人类编写的代码一样批判性地审查它。它是否符合要求？如果不符，确定缺少或错误的地方。然后提供反馈或优化提示。这可以通过对话式 AI 简单地继续对话来完成，或者在编辑器中为 AI 写另一个评论以供其回复。

通过向 AI 提供反馈，你可以将其引导到更接近你期望的结果。从某种意义上说，你是在实时训练它来解决你的特定问题。高级提示工程就像图 2-2 中的循环：提示 → AI 输出 → 审查 → 优化提示 → AI 输出 → ...直到满意。保持每次迭代的更改尽可能小是有用的；如果你对提示进行大规模的修改，你可能会丢失一些前一个输出的好部分。

![图片](img/bevc_0202.png)

###### 图 2-2\. 高级提示工程循环。

例如，你可能会提示：

> 编写一个函数，该函数接收一个整数列表并返回它们的总和。

AI 随后返回一个函数，但它的代码假设列表不为空，并且没有很好地处理空列表。然后你可以回复：

> 看起来不错。但是，请修改它，如果列表为空则返回 0。

AI 随后会相应地更新函数。这样，你不必从头开始提示；你只是告诉 AI 进行一些调整。AI 已经有了之前给出的代码的上下文。

如果你使用内联助手，优化可能看起来像编辑代码，并可能写一个注释如`# TODO: 处理空列表`，然后看看 AI 是否建议修复这个问题。

另一种优化方法是，如果第一次输出不正确，重新提示更多信息。假设你说，“对一组名字进行排序”，它给出了按大小写敏感排序的代码，但你想要不区分大小写的排序。你可以重新表述：

> 不区分大小写地对一组名字进行排序。

或者甚至：

> 之前的代码按大小写敏感排序。修改它使其不区分大小写。

在调试过程中，对于更复杂的逻辑错误（没有抛出明显的错误信息，但输出结果错误），你可以提示 AI 遍历代码的执行过程。例如：

> 逐行遍历这个函数，并跟踪每一步的总值。它没有正确累加——逻辑在哪里出错？

这是一个“橡皮鸭”调试提示的例子：你实际上是在要求 AI 模拟人类可能会用打印或调试器进行的调试过程。这样的提示通常会揭示微妙的问题，比如变量没有重置或不正确的条件逻辑，因为 AI 会在每个步骤中说明状态。如果你怀疑代码的某个部分，你可以聚焦于：

> 解释这里过滤器调用在做什么，以及它是否可能排除了它不应该排除的更多项。

在解释过程中与 AI 互动可以揭示错误。

在解释之后，直接询问你需要的内容通常很有效：

> 什么可能导致了这个问题，我该如何修复它？

这邀请 AI 进行诊断并提出解决方案。如果 AI 的第一个答案不清楚或只有部分帮助，不要犹豫，提出后续问题：

> 那个解释是有道理的。你能展示我如何修复代码吗？请提供修正后的代码。

在聊天环境中，AI 有聊天历史，因此它可以直接输出修改后的代码。如果你使用 VSCode 中的 Copilot 或 Cursor 这样的内联工具而没有聊天功能，你可以在代码上方写一个注释：

```py
// BUG: returns NaN, fix this function and see how it autocompletes 
```

一般而言，交互式聊天会产生更详尽的解释。

另一种后续模式：如果 AI 提供了一个修复方案但你不知道为什么，你可以问：

> 你能解释一下为什么这个改变能解决问题吗？

这样，你可以为下一次学习，并且可以再次确认 AI 的推理是合理的。

大型语言模型（LLMs）在示例和纠正中茁壮成长。如果你指出错误或给出一个快速示例，AI 可以将其纳入：

> 如果输入是[]，它应该返回 0，但现在它报错了。

这种迭代过程是正常的。实际上，试图在一个提示中塞入所有细节可能不如几次来回的交流有效。利用这一点来发挥你的优势。

在反馈时要耐心且具体。而不仅仅是说，“不，这是错误的”，而是说明哪里错了或需要什么：

> 这段代码没有正确处理负数。它应该将它们视为求和中的 0。

此外，如果 AI 偏离了轨道，你可以引导它回到正轨：有时重置或重新措辞比试图挽救一个非常错误的尝试更容易。运用你的判断力。如果 AI 的输出显示它完全误解了你，请从头开始澄清你的提示。

随着你不断优化，你也会了解到 AI 是如何解读你的提示的。这可以指导你如何撰写未来的提示。你可能会意识到，“哦，它把‘登录系统’理解为一个完整的用户界面。下次我只指定后端。”

想象它就像调试代码：如果 AI 的输出是错误的，“bug”可能在于你的提示，而不是 AI 的处理。就像你会在代码产生错误结果时检查和修复你的代码一样，当 AI 生成意外或不正确的输出时，你应该优化你的提示。你和 AI 之间的对话就像一个调试会话，每次交流都有助于你定位和修复问题。

# 比较两个提示

想象你有一个简单的 Node.js 函数，旨在通过用户 ID 将用户对象列表转换为查找映射。然而，它抛出了错误。以下是存在错误的代码：

```py
// Buggy function: converts array of users to a map by ID
function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i <= users.length; i++) {  
    // Note: using <= will go out of bounds on the last iteration
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}

// Example usage:
const result = mapUsersById([{ id: 1, name: "Alice" }]);
```

这段代码本应遍历用户的数组并构建一个对象，其中键是用户 ID。然而，存在一个错误：循环使用 `i <= users.length` 而不是 `i < users.length`。这将在最后一次迭代时导致 `users[i]` 为 undefined，很可能会引发运行时错误（例如尝试访问 `undefined` 的属性 ID）。让我们看看人工智能助手如何对关于这个问题的两个不同提示做出回应（见图 2-3）。

![](img/bevc_0203.png)

###### 图 2-3\. 提示质量影响比较：模糊的提示会产生通用或不正确的代码，而具体、结构良好的提示则会产生准确、上下文相关的解决方案。

## 糟糕的提示

这里是一个糟糕的提示示例：

> 为什么我的 mapUsersById 函数不起作用？

对糟糕提示的人工智能回应（模拟）：

> 没有更多信息很难说。你确定函数是以正确的用户数组调用的吗？也许问题是用户数组为空或不是数组，导致循环无法正确执行。也可能是设置 userMap 属性时出错。确保每个用户都有一个 id 属性。如果用户是 undefined 或 null，也会导致它无法工作。

这个答案相当模糊。提示没有提供任何细节，除了函数名，所以人工智能在猜测通用原因（“可能数组为空或不是数组”，“确保每个用户都有一个 ID”）。它没有识别出实际的错误。

当问题未具体说明时，你会得到这样的回应——模型只能重复常见的调试提示。如果你是开发者，这并不很有帮助，因为它没有解决真正的错误。提示中的上下文和具体性不足导致了一个通用的答案：本质上，人工智能的“你试过关机再开机吗？”这种回应与社区建议相符，即模糊的提示会导致无关或通用的答案。

## 改进的提示

让我们再试一次：

> 我有一个名为 `mapUsersById` 的 JavaScript 函数，它应该将用户对象的数组转换为按用户 ID 键控的 `map (对象)`。然而，当我运行它时，它会抛出错误。例如，当我传递 `[ {id: 1, name: "Alice"} ]` 时，我得到 `TypeError: Cannot read property 'id' of undefined`。以下是函数代码：
> 
> ```py
> function mapUsersById(users) {
>   const userMap = {};
>   for (let i = 0; i <= users.length; i++) {  
>     const user = users[i];
>     userMap[user.id] = user;
>   }
>   return userMap;
> }
> ```
> 
> 它应该返回 `{ "1": {id: 1, name: "Alice"} }`。错误是什么，我该如何修复它？

对改进提示的人工智能回应（模拟）：

> 错误“无法读取属性`'id'`的未定义”表明，在某个时刻，用户是未定义的，然后你尝试访问 user.id。查看你的循环，问题出在循环边界上。你正在使用`i <= users.length`。如果`users.length`是`1`（数组中的一个用户），循环将运行`i = 0`和`i = 1`。当`i`是`1`时，`users[1]`是未定义的（因为数组中只有一个索引是 0）。因此，在那个迭代中，用户变为未定义，`user.id`抛出`TypeError`。修复方法是更改循环条件为`i < users.length`，这样`i`将从`0`到`users.length-1`。例如：
> 
> ```py
> for (let i = 0; i < users.length; i++) {
>   const user = users[i];
>   userMap[user.id] = user;
> }
> ```
> 
> 这样，`i`将为第一个（也是唯一的一个）元素是`0`，然后循环停止。有了这个变化，你的函数将不再遇到未定义的用户，并应该按预期工作，对于给定的输入返回`{"1": {id: 1, name: "Alice"}}`。

这个响应直接指出了错误（在循环中使用`<=`），并解释了为什么会导致用户未定义。然后提供了正确的代码解决方案（`i < users.length`）并确认了预期的结果。改进的提示之所以成功，是因为它遵循了我们的原则：指定了语言（JavaScript）、描述了函数的目的、包含了确切的错误信息和示例输入，甚至提供了相关的代码片段。

注意 AI 如何利用错误信息作为线索来关注循环边界——一个有针对性的提示使 AI 能够参与真正的解决问题，有效地模拟人类调试员的思维方式：“未定义可能来自哪里？很可能是来自循环索引。”这是详细提示带来益处的具体体现。

# 提示技术：有效沟通的工具箱

现在让我们深入了解可以大幅提升你的提示技能的具体技术。这些就像模式或食谱，当直接的指令不够或者你想以某种方式引导 AI 时，你可以使用它们。

通过掌握这些技术，你可以处理各种情况：用简单的英语指导 AI、给它提供例子、让它解释或结构化其输出，或者将其设定为不同的思维模式或角色。所有这些都有助于你引导 AI 产生你所需的内容。

提示技术不是相互排斥的；你通常会同时使用几种技术以获得最佳效果，尤其是在复杂任务中。

# 样式注意事项

当你使用这些技术时，调整你的语气以适应模型。许多模型对礼貌或中性的指令反应良好。你不需要使用过时的或过于正式的语言。直接但礼貌通常有效：“请做 X”或“让我们做 Y”。例如，在思维链（CoT）提示中，一个流行的短语是“让我们一步步思考。”像 GPT-4 这样的模型会将这视为一个显示推理的线索。

## 零样本提示

*零样本提示*就是简单地要求模型做某事，而不提供任何示例或超出指令的额外指导。本质上，模型是从“零”个示例中解决问题。

*何时使用：* 这是最常见的场景：你只用普通语言请求你想要的东西。如果任务是标准的，提示清晰，这通常就足够了。

*示例：*

> 编写一个 Python 函数，用于检查一个数字是否为素数。

这是零样本。AI 可能会使用循环或试除法生成一个素数检查函数。

*优点：* 它很快，并且依赖于模型学习到的知识。现代模型在许多编程任务的零样本响应方面表现得惊人地好，尤其是如果它们很常见（如素数检查、排序或字符串操作）。

*缺点：* 如果任务是不同寻常的或输出格式是特定的，零样本可能第一次尝试的结果并不完全符合你的需求，因为模型可能有多种解释它的方式。

通常，对于简单的事情，先尝试零样本是一个好主意。如果结果不理想，你可能会转向细化或其他技术。

## 单次尝试和少量样本提示

*单次尝试提示*意味着你作为提示的一部分提供你想要的 exactly one 个示例（输入和期望的输出）；*少量样本提示*意味着在要求模型对新输入执行任务之前提供几个示例（通常是两个到五个）。

这就像向模型展示，“这是解决一个实例的方法。现在你以类似的方式解决下一个。”

*何时使用：* 当模型可能不知道你需要的确切格式或风格，或者任务有点不同寻常时，这种提示很有用。通过提供示例，你可以减少歧义。

*示例（单次尝试）：* 假设你使用的是模型可能没有见过很多的语言或某种风格。比如说，你想要特定格式的伪代码。你的提示可能是：

> 将以下英文指令转换为类似 Python 的伪代码。

示例指令：“计算 n 的阶乘”：

```py
Example pseudocode:

function factorial(n):

    if n <= 1:

        return 1

    else:

        return n * factorial(n-1)

Instruction: "Find the largest number in a list"

Pseudocode:
```

你提供了一个示例（阶乘）和想要的格式。现在模型更有可能以类似格式（带有函数，需要时带有 if/else 或循环逻辑）生成“最大数字”指令的伪代码。

*示例（少量样本）：* 假设你想要 AI 使用特定的算法。你可以给它一个该算法操作的较小示例作为提示。或者如果任务有多个正确答案但你更喜欢其中一个，示例可以推动它朝那个方向。

少量样本提示在格式化方面非常强大；例如：

> 将以下英文语句转换为 SQL 查询。\N1."在 2020 年之后雇佣的所有员工" → Select * From Employees Where Hire_Date > '2020-01-01';\N2. "列出在过去一个月内进行过购买的客户姓名" → Select Name From Customers Join Purchases On ... Where Purchase_Date > ...;\N3. "缺货产品的数量" →

在这里，一旦你给出两个英语到 SQL 的示例，AI 很可能会按照模式正确回答第三个查询。少样本示例也可以应用于编码：在小的样本中向 AI 展示你想要的风格，然后要求更多。这就像在你的提示中给它一个迷你训练数据集。

*优点:* 你可以实现非常具体的输出风格。这种技术还有助于模型处理涉及遵循模式或重复应用概念的任务。

# 上下文窗口

术语*上下文窗口*指的是语言模型在单次交互中可以处理的文本最大量（以标记衡量），包括输入提示和生成的响应。这代表了当前 AI 模型的限制——一旦达到这个阈值，模型就无法处理更多信息。当你制作提示时，你必须将包括在内的一切（指令、示例、数据和响应所需的空间）都放入这个固定容量中。

*缺点:* 尤其是少样本提示会使提示变得更长（这会消耗上下文窗口）。对于非常大的/复杂示例，它可能会“消耗”模型的大量容量。但通常一个小例子或两个就足够了。

*提示:* 如果你想让模型严格遵循某种输出结构，给出一个例子几乎可以保证它符合该结构，而不是提供一个需要解析的自由形式回复。

## 思维链提示

*思维链*（CoT）提示涉及要求模型在给出最终答案之前逐步思考或展示其推理。换句话说，你鼓励模型分解问题。

*何时使用:* 这对于涉及推理和多步计算的复杂问题很有用，或者当你怀疑模型如果直接跳到答案可能会出错时。如果你想在输出中看到解释，这也很有用。

# 组合数学

*组合数学* 处理根据特定规则或约束进行计数、排列和选择对象的问题。常见问题包括计算排列（顺序重要的排列）、组合（顺序不重要的选择）和其他计数原理。关键符号包括“从 n 中选 k”（写作 C(n,k)或 nCk），它表示从 n 个总项目中选择 k 个项目的不同方式，该方式使用公式 n!/(k!(n-k)!)计算。这些计算在概率、统计学和离散数学应用中经常出现。

*示例:* 与仅仅询问一个[组合数学问题](https://oreil.ly/vI3V5)如“12 选 4 是什么？”并得到答案相比，你可能会说：

> 逐步解决 12 选 4。

模型可能会先概述：

```py
12 choose 4 = 12!/(4!*8!) = ... = 495
```

在编码中，CoT 对于棘手的算法任务很有用。你可以尝试以下提示：

> 逐步解释如何合并两个有序列表，然后提供 Python 代码。

模型首先可能会概述如下：

> 我们将使用两个指针，分别从每个列表的头部开始，比较元素，将较小的元素追加到结果列表中，然后移动该指针，依此类推...

然后它可能会给出代码。这确保它在编码之前正确地构建了解决方案。

另一个用途是调试或理解输出：

> 通过逻辑分析来确定 19 是否为素数，然后给出结果。

模型可能会列出素数的除法，然后得出结论：

> 19 是素数。

*优点:* 提高了需要推理的任务的正确性。有[研究证据](https://oreil.ly/t7flF)表明，提示模型“大声思考”可以提高数学和逻辑任务的结果。这还让你了解了模型的过程，这可能是有益的或帮助你更信任答案。

*缺点:* 输出会更长（这可能在最终代码中不是你想要的）。此外，一些接口（如典型的代码补全）没有设置好以显示与代码分开的推理。这种技术在问答或聊天场景中更为常见。然而，你可以指示模型将推理作为代码中的注释包含在内，这是一种获取详尽注释代码的巧妙方法。

## 角色提示

*角色提示*意味着你要求 AI 扮演一个可能影响其响应方式的特定身份或角色。

*何时使用:* 当你想影响答案的风格或细节，或获得某种特定观点时，这很有用。例如，一个扮演“专家”角色的 AI 可能会提供一个更高级的解决方案或更多的解释，而“初学者”角色可能会让它解释更多基本概念。

*示例:*

+   你是一位 Python 讲师。解释以下代码，然后修改它使其更符合 Python 风格。

+   担任安全分析师的角色。以下是一些代码。识别任何安全漏洞。

+   假设你是一个检查代码风格问题的代码检查器。

这可以显著影响响应。将 AI 分配为安全分析师角色可能会让它关注它否则不会提及的事情（如数据验证、安全编码实践或潜在漏洞）。讲师角色可能会让它提供更清晰的解释，也许不会假设先验知识。

在编码中，你可能会在请求代码之前说：

> 你是一位精通优化的专家级 C++程序员，正在指导一位初级开发者。

结果可能会使用更多高级的 C++特性，并解释为什么做出了某些选择，在技术复杂性和教育清晰度之间取得平衡。

*优点:* 这种技术可以引导答案的语气和深度。这可以使解决方案适应一定程度的复杂性或详尽程度。如果你想要一个非常简单的解决方案（让它扮演新手角色，可能会避免复杂的技巧）或一个非常优化的解决方案（让它扮演性能专家角色），这很有用。

*缺点:* 有时模型可能会过分关注角色特征（例如，“讲师”可能会解释你已经知道的事情）。此外，一些 AI 安全系统对某些角色描述更为敏感——尤其是那些可能暗示欺骗、权威模仿或潜在有害活动的描述——尽管像“数据分析师”或“软件工程师”这样的直接技术和专业角色通常没有问题。

## 上下文提示

*上下文提示*意味着向 AI 提供超出即时任务描述的额外上下文或信息。除非你在提示中提供（或通过高级 IDE 集成中的某些集成上下文窗口），否则 AI 模型不会记住你整个项目的持久记忆。因此，如果你想 AI 编写适合你现有代码库的代码，就提供那个上下文。基本上，你将相关数据或背景作为提示的一部分提供。

*何时使用:* 当解决问题需要了解模型可能不知道或可能无法从训练中正确回忆的数据或定义时使用。或者，当您想确保与某些外部信息（如 API 规范或对话的先前部分）的一致性时使用。

*示例:*

如果你有一个数据结构，并且想要与之配合工作的代码，你可以粘贴其定义：

```py
Given the class below, implement the function X.

class Node:

    def __init__(self, value, next=None):

        self.value = value

        self.next = next

# Now write a function to count the nodes in a linked list starting at head.
```

通过包含类定义，使 AI 更可能在其代码中正确使用`Node.value`和`Node.next`。

如果你想要使用特定的 API，请在提示中包含文档片段：

> 使用 requests 库从 API 获取数据。（API 返回格式为 JSON 的格式：{...}）

如果你从文档中包含 API 使用的简短示例，AI 可以模仿它。

为了消除歧义：

> 使用术语`*学生*`来指代高中生，编写一个函数...

如果在上下文中`*学生*`可能存在歧义，你已经澄清了它。

*优点:* 你正在将 AI 扎根于你关心的上下文中。如果你提供事实，它就不太可能做出错误的假设。如果 AI 否则可能不会记住或知道你的特定用例细节，这非常有帮助。

*缺点:* 这种技术会使提示变得更长。此外，模型有时可能会将提供的上下文直接重复到答案中（如果不小心，可能会像从文档片段中复制行到代码中一样）。但通常它使用它是恰当的。

*提示:* 如果你有一个大的上下文（如大的模式或许多代码行），有时最好为模型总结关键元素，而不是包括所有内容。这种方法有助于你保持在上下文限制内，同时确保模型接收到的信息是最相关的。然而，如果内容足够小，只需直接包含它。

提及约束条件也很有用：性能约束（“优化为 O(n log n)或更好”），兼容性约束（“必须在 Python 3.8 上运行”），或库选择（“仅使用标准库，无外部依赖”）。这些就像护栏一样，确保 AI 不会建议超出可接受范围的内容。

## 元提示

*元提示*是关于输出本身的指令，而不仅仅是解决方案应该做什么。这就像告诉 AI 如何格式化或处理解决方案。

*何时使用:* 当您需要特定格式或风格的答案，或者您想通过问题控制 AI 的工作方式时，这很有用。

*示例:*

> 首先，用两句话解释方法，然后提供代码。

这确保 AI 不会直接进入代码：

> 在解决方案中不要使用任何库。

这对解决方案施加了一个约束：

> 将输出格式化为 JSON。

如果您使用 AI 生成数据而不是代码，这很有用：

> 只提供函数体，不要提供定义行。

如果您想将函数插入现有代码，这很方便：

> 如果输入无效，而不是错误，返回 None。

这并不是一个精确的输出格式，但它指导 AI 在特定情况下如何表现。

*优点:* 您可以得到您需要的，以您需要的方式，无需额外编辑。这在某些情况下至关重要。如果您计划自动将 AI 的输出用于管道，那么您真的希望格式保持一致。

*缺点:* 如果指令与模型的默认风格冲突，有时它可能部分遵循它们，或者您必须强调它们。例如，即使您说“只有代码，没有解释”，偶尔模型可能会包含一些小的注释。通常，用直接命令的措辞有助于：

> 不要包含任何解释；仅在一个代码块内输出代码。

像 GPT 这样的模型很好地遵循这一点。

## 自洽性（多个输出和多数投票）

*自洽性*更多的是一种策略，而不是提示风格。想法是获取相同提示的多个输出，然后决定最好的或最常见的。正如[Sander Schulhoff of Learn Prompting]（https://oreil.ly/fHABW）指出，自洽性利用了这样一个观点：如果您多次（带有轻微随机性）询问模型，并且许多答案一致，那么这种共识很可能是正确的。

*何时使用:* 这对于复杂问题很有用，您不确定模型的第一个答案是否正确，尤其是如果您无法轻松验证它，或者您想通过看到它是否反复给出相同答案来从 AI 那里获得信心检查。

*手动使用方法:* 在某些平台（如 ChatGPT）上，您可以点击“重新生成答案”。或者，您可以将提示复制到新会话中，看看是否得到相同的结果。如果您得到三个答案，其中两个相同，一个不同，您可能会相信那两个（假设问题只有一个正确答案）。

在编程环境中，如果它为确定性任务生成代码，通常每次都会给出非常相似的代码（变量名或风格上的小变化）。但如果是一个算法问题（如“这段代码的输出是什么？”），你可以检查多次运行。

这种技术在非编码任务（如逻辑谜题）中更强大，但值得关注。

*另一个角度——集成提示：* 你实际上可以在一个提示中让模型考虑多个可能性：

> 为这个问题提供两种不同的解决方案。

然后你可能可以看到你喜欢的哪一个，或者测试两个。这就像一次性获得多个答案的自我一致性。

*优点：* 如果多次尝试收敛，这种技术可以提高对解决方案的信心。此外，你可能会得到多样性（如果你想在多个解决方案中选择最优雅的解决方案，这很好）。

*缺点：* 进行多次调用并比较输出很耗时。

在实践中，如果我对答案不确定，我经常会以不同的方式重新提出问题，看看是否能得到相同的答案。如果我能得到相同的答案，我就更有信心它是正确的。

## ReAct (推理 + 行动) 提示

*ReAct* 是一种更高级的提示技术，它[结合了 *推理* 和 *行动*](https://arxiv.org/abs/2210.03629)。它不仅让模型像 CoT 那样思考，还能执行计算、调用 API 或使用工具等动作。（更多信息请参阅 [ReAct 提示工程指南](https://oreil.ly/P_KIV)）。在当前实践中，这通常与 LangChain 等框架一起使用，其中 AI 可以输出程序解释为动作的特殊格式（如执行命令或运行查询），然后将结果反馈回来。

对于我们的范围（在没有这种执行环境的情况下），你仍然可以通过指示 AI 首先概述一个计划，然后输出结果来进行一种形式的 ReAct。这类似于 CoT，但专门针对使用工具或执行子任务。

*示例：*

> 使用 Python，确定巴黎当前的天气并将其打印出来。

除非 AI 具有浏览能力，否则它无法真正获取当前的天气。ReAct 方法会让 AI 首先通过陈述问题来推理问题：

> 我需要获取巴黎当前的天气数据，这需要调用天气 API。

然后 AI 会尝试使用可用的工具来调用这个 API。如果成功，它会接收到实际的天气数据；如果没有这样的工具，它可能会承认限制或使用假设数据。最后，AI 会编写 Python 代码来显示天气信息，结合通过推理和行动过程获得的所有数据。

没有外部工具访问，ReAct 可能对简单的提示任务并不特别相关。然而，在评估你组织的 AI 工具时，确定它们是否可以访问互联网上的当前信息代表了一个关键的能力评估。许多 AI 模型在知识截止点下运行，这意味着它们的训练数据只延伸到特定日期，这可能导致快速变化主题的信息过时。

如果你在一个 AI 可以执行代码的环境中（例如 Jupyter 集成或类似平台），你可以通过指示系统来实现 ReAct：

> 首先为这个函数编写一个测试，运行它，然后相应地调整代码。

这通过一个推理步骤（编写测试）来展示 ReAct 模式，然后是执行测试的动作，然后根据结果调整代码。然而，通过纯提示来编排这样的工作流程需要高级提示技术和适当的技术基础设施。

*简单使用:* 你可以模拟一个问答，其中 AI 有中间步骤来模拟动作：

> 逐步思考，如果需要，进行计算。

这实际上是 CoT，但语气更加命令式。

*优点:* 当可用时，它可以解决需要外部信息或迭代试验的问题（例如，AI 可以通过实际运行代码来自我纠正）。在调试环境中，能够执行代码以进行测试的 AI 非常出色。

*缺点:* 这种技术没有特定的工具支持时并不广泛可用。而且，如果你在普通的 ChatGPT 中以这种方式进行提示，它要么会想象动作，要么只是做 CoT。

在编写提示的目的上，请记住，一些系统（如 OpenAI 的工具使用代理或其他系统）存在，但在纯提示中，我们主要做 CoT，并且我们自己处理像运行代码或测试这样的动作。

# 高级提示：结合技术和处理复杂性

提示技术可以结合使用。例如，你可能进行一个几轮提示，同时也在示例中展示 CoT。或者你可能将角色与 CoT 结合：

> 作为一名高级工程师，逐步思考问题，然后给出代码。

现在我们已经探讨了各种提示技术，让我们通过一个或两个场景来看看它们在实际中的应用，然后讨论如何审查和改进 AI 的输出（这引出了下一章关于理解和掌握生成代码的内容）。

假设你有一个不工作的函数。你可能需要结合角色和 CoT 提示：

> 你是一位 Python 调试器。让我们逐步思考，找出以下代码中的错误。

这将随后是代码。AI 可能会对每一行进行分析，并指出错误。

或者让我们假设你想要为某个相对复杂的算法生成代码，确保它有良好的注释，并且还需要为其获取测试用例。一个组合提示可能看起来像这样：

> 你是一位专家 Python 开发者。让我们一步步解决这个问题。我们需要一个函数`merge_sorted_lists(list1, list2)`，它将两个排序后的列表合并成一个排序后的列表。首先，解释方法，然后提供带有注释的 Python 代码。之后，给出 2-3 个示例测试代码来演示它的工作。

这个单一提示非常全面。第一句话设定了一个角色。第二句要求逐步推理。第三句给出了主要任务。第四句要求带有解释注释的代码，第五句甚至要求测试。

然后，AI 可能会输出一个解释，然后是带有内联注释的代码，最后是一些测试用例。这是一个高级用法，但它展示了你可以如何通过多方面的响应来指导 AI。

## 了解模型的限制

提示工程还涉及知道什么**不**要问以及如何避免陷阱。如果一个提示太长或包含太多指令，模型可能会感到困惑或截断部分输出。如果你发现它开始忽略你的提示部分，你可能需要简化或分部分来做。如果一个 AI 模型有时会产生错误的事实或代码（它“产生幻觉”），你会学会双重检查并且不将其作为事实预言家使用。如果你发现它倾向于给出过于冗长的代码，你可以通过“尽可能使解决方案简洁”来预防。如果它有时使用不存在的函数，你可以指示“只使用以下列出的 API 函数”并列出它们。你越了解 AI 的行为，你就能越能调整你的提示来克服任何弱点。

如果一个任务非常复杂，你也可以将其分解为子任务给 AI。例如，你可能会首先提示：

> 列出实现一个简单算术表达式语言的基本编译器的步骤。

一旦 AI 给出了步骤，你就用单独的提示来处理每个步骤，甚至可能在单独的文件或会话中：

> 现在实现步骤 1：分词。

这就像用 AI 做系统设计：你可以概述然后细化每个部分。它利用了 AI 在规划（不仅仅是编码）方面的辅助能力。

## 有状态对话与一次性提示

在聊天设置中，你有一个对话历史，称为**状态**。你可以通过与 AI 讨论来建立上下文。在 IDE 完成设置中，上下文主要是你的文件内容和注释。两者都以不同的方式允许累积上下文。如果你需要 AI 记住之前说过的话（比如完善答案），请使用对话。如果你想确保它只关注现在相关的信息，请使用新的提示或文件上下文。有时清除上下文可以防止模型坚持可能错误的早期假设。

通过在各种示例中练习这些技术，你会熟练地知道何时使用哪种方法：

+   如果输出格式很重要，给出示例（少样本）或明确的格式化指令。

+   如果逻辑很复杂，使用 CoT 或逐步进行。

+   如果解决方案的质量可能有所不同，设置一个角色（如“资深工程师”）以获得更好的风格。

+   如果模型不遵守，也许可以将你的提示分成几个部分，简化它们，或者使用更强的措辞来约束。

## 常见提示反模式及其避免方法

并非所有提示都是平等的。到目前为止，我们已经看到了许多有效的提示示例，但同样有教育意义的是识别反模式——这些常见的错误会导致 AI 的糟糕响应。本节涵盖了一些常见的提示失败及其修复方法。

### 模糊的提示

这是一种经典的“它不起作用，请修复它”或“写一些能做 X 的事情”的描述，但缺乏足够的细节。问题“我的函数为什么不起作用？”通常只会得到一个无用的答案。模糊的提示迫使 AI 猜测上下文，通常会导致一般性的建议或不相关的代码。

解决方法是添加上下文和具体细节。如果你发现自己提出一个问题，而答案感觉像是一个魔球（“你尝试检查 X 了吗？”），停下来，用更多细节重新构建你的查询（错误信息、代码摘录、预期与实际结果等）。一个好的做法是阅读你的提示并问自己，“这个问题是否适用于数十种不同的场景？”如果答案是肯定的，那么它太模糊了。让它变得如此具体，以至于只能适用于你的场景。

### 过载的提示

这是相反的问题：要求 AI 一次做太多事情。例如：

> 生成一个完整的 Node.js 应用程序，包括身份验证、React 前端和部署脚本。

或者，在更小的范围内：

> 一次性修复这 5 个错误并添加这 3 个功能。

AI 可能会尝试，但你很可能会得到一个混乱或不完整的结果，或者它可能会忽略请求的一些部分。即使它解决了所有问题，响应也会很长且难以验证。

解决方法是分解任务。优先考虑：一次只做一件事，正如我们之前强调的那样。这使得更容易捕捉错误并确保模型保持专注。如果你发现自己写了一个在指令中使用“和”多次的段落，考虑将其分解成单独的提示或顺序步骤。

### 缺少问题

有时用户会提供大量信息，但从未清楚地提出问题或指定他们需要什么，就像只是扔出一个大的代码片段，然后说，“这是我的代码。”这可能会让 AI 困惑——它不知道你想要什么。

总是包括一个清晰的请求：

+   识别上述代码中的任何错误。

+   解释这段代码的功能。

+   完成代码中的待办事项。

一个提示应该有一个目的。如果你只是提供文本而没有问题或指令，AI 可能会做出错误的假设（比如总结代码而不是修复它等）。确保 AI 知道你为什么展示了某些代码。即使是简单的添加，如“这段代码有什么问题？”或“请继续实现这个函数”，也能给它指明方向。

### 模糊的成功标准

这是一个微妙的问题。有时你可能会要求优化或改进，但你没有定义成功是什么样的——例如，“让这个函数更快。”更快是根据什么指标？如果 AI 不知道你的性能限制，它可能会对一些无关紧要的事情进行微观优化，或者使用理论上更快但实际上可以忽略不计的方法。或者“让这个代码更简洁”：简洁是主观的。我们通过明确陈述目标来解决此问题，例如“减少重复”或“改进变量名”等。

修复方法：量化或限定改进：

+   优化此函数以线性时间运行（当前版本是二次方）。

+   将此重构以移除全局变量并使用类代替。

基本上，在重构或功能中明确说明你正在解决的问题。如果你让它太开放，AI 可能会解决一个与你关心的不同的问题。

### 忽略 AI 的澄清或输出

有时 AI 可能会用一个澄清问题或假设来回应：

+   你使用的是 React 类组件还是函数组件？

+   我假设输入是一个字符串——请确认。

如果你忽略这些并只是重复你的请求，你错过了改进提示的机会。AI 正在发出需要更多信息的需求信号。始终回答它的问题或改进你的提示以包括这些细节。

此外，如果 AI 的输出明显错误（例如误解了问题），不要只是重复相同的提示。花点时间调整你的措辞。也许你的提示中有一个模糊的短语或遗漏了某些关键信息。把它当作对话来处理：如果一个人误解了，你会用不同的方式解释；对 AI 也这样做。

### 不一致性

如果你不断改变提问方式或一次混合不同的格式，模型可能会感到困惑。两个例子包括在说明中在第一人称和第三人称之间切换或以令人困惑的方式将伪代码与实际代码混合。

尝试在单个提示中保持一致的风格。如果你提供示例，确保它们清晰界定（使用 Markdown 三重反引号表示代码，引号表示输入/输出示例等）。一致性有助于模型正确解析你的意图。此外，如果你有首选风格（比如，ES6 与 ES5 语法），请始终一致地提及它；否则，模型可能会在一个提示中建议一种方式，而在以后的提示中建议另一种方式。

### 模糊的引用，如“上面的代码”

在使用聊天时，如果你说“上面的函数”或“前面的输出”，确保引用是清晰的。如果对话很长，你说，“重构上面的代码”，AI 可能会失去跟踪或选择错误的代码片段进行重构。

最好是再次引用代码或具体指出你想重构的函数。模型有有限的注意力窗口，尽管许多 LLM 可以参考对话的先前部分，但再次提供明确的上下文可以帮助避免混淆。这尤其适用于代码展示后经过了一些时间（或几条消息）。

# 摘要和下一步

提示的艺术是迭代和创造性的。随着模型的发展，提示的最佳实践可能会改变（例如，未来的模型可能用更少的文字就能更好地理解意图）。但基本原理保持不变：有效沟通，AI 将更好地为你服务。

从本质上讲，掌握提示工程就像掌握一门新的编程语言——AI 的指令语言。它融合了技术写作、前瞻性和对*提示*本身的交互式调试。但一旦你擅长它，AI 就会真正地感觉像是你思维的延伸，因为你可以用最小的摩擦可靠地提取你设想（甚至是你尚未完全设想但可以引导 AI 发现的）的解决方案。这项技能可能会变得和知道如何使用谷歌或如何使用调试器一样基本——它是现代开发者技能集在 vibe 编码时代的一部分。

如果 AI 可以解决大约 70%的问题，你作为编码的合作伙伴将如何处理？第三章探讨了开发者*真正*如何使用 AI，并制定了一些“黄金法则”用于 vibe 编码。
