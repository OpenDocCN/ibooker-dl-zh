- en: Chapter 5\. Introduction to Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 自然语言处理简介
- en: Natural language processing (NLP) is a technique in AI that deals with the understanding
    of language. It involves programming techniques to create models that can understand
    language, classify content, and even generate and create new compositions in language.
    It’s also the underlying foundation to large language models (LLMs) such as GPT,
    Gemini, and Claude. We’ll explore LLMs in later chapters, but first, we’ll look
    at more basic NLP over the next few chapters to equip you for what’s to come.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是人工智能（AI）中的一种技术，它处理语言的理解。它涉及编程技术来创建可以理解语言、分类内容，甚至生成和创作新语言作品的模型。它也是大型语言模型（LLMs）如GPT、Gemini和Claude的底层基础。我们将在后面的章节中探讨LLMs，但首先，在接下来的几章中，我们将探讨更基础的NLP，以便为你即将学习的内容做好准备。
- en: There are also lots of services that use NLP to create applications such as
    chatbots, but that’s not in the scope of this book—instead, we’ll be looking at
    the foundations of NLP and how to model language so that you can train neural
    networks to understand and classify text. In later chapters, you’ll also learn
    how to use the predictive elements of an ML model to write some poetry. This isn’t
    just for fun—it’s also a precursor to learning how to use the transformer-based
    models that underpin generative AI!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多服务使用自然语言处理（NLP）来创建应用程序，如聊天机器人，但这本书的范围不包括这一点——相反，我们将探讨NLP的基础以及如何建模语言，以便你可以训练神经网络来理解和分类文本。在后面的章节中，你还将学习如何使用机器学习（ML）模型的预测元素来写一些诗歌。这不仅仅是为了好玩——这也是学习如何使用支撑生成式人工智能（AI）的基于转换器的模型的一个先导。
- en: We’ll start this chapter by looking at how you can decompose language into numbers
    and how you can then use those numbers in neural networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将从这个章节开始，探讨如何将语言分解成数字，以及如何使用这些数字在神经网络中使用。  '
- en: Encoding Language into Numbers
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将语言编码成数字
- en: Ultimately, computers deal in numbers, so to handle language, you need to convert
    it into numerics in a process called *encoding.*
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，计算机处理的是数字，因此为了处理语言，你需要将其转换为数字，这个过程称为*编码*。
- en: You can encode language into numbers in many ways. The most common is to encode
    by letters, as is done naturally when strings are stored in your program. In memory,
    however, you don’t store the letter *a* but an encoding of it—perhaps an ASCII
    or Unicode value or something else. For example, consider the word *listen*. You
    can encode it with ASCII into the numbers 76, 73, 83, 84, 69, and 78\. This is
    good in that you can now use numerics to represent the word. But then consider
    the word *silent*, which is an anagram of *listen*. The same numbers represent
    that word, albeit in a different order, which might make building a model to understand
    the text much more difficult.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用许多方式将语言编码成数字。最常见的是通过字母编码，就像在程序中存储字符串时自然发生的那样。然而，在内存中，你并不存储字母*a*，而是它的编码——可能是ASCII或Unicode值或其他。例如，考虑单词*listen*。你可以用ASCII将其编码为数字76,
    73, 83, 84, 69和78。这是好的，因为现在你可以用数字来表示这个单词。但考虑单词*silent*，它是*listen*的字母表重组。相同的数字代表这个单词，尽管顺序不同，这可能会使构建一个理解文本的模型变得更加困难。
- en: A better alternative might be to use numbers to encode entire words instead
    of the letters within them. In that case, *silent* could be the number *x* and
    *listen* could be the number *y*, and they wouldn’t overlap with each other.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的选择可能是使用数字来编码整个单词，而不是单词中的字母。在这种情况下，*silent*可以是数字*x*，而*listen*可以是数字*y*，它们不会相互重叠。
- en: Using this technique, consider a sentence like “I love my dog.” You could encode
    that with the numbers [1, 2, 3, 4]. If you then wanted to encode “I love my cat,”
    you could do it with [1, 2, 3, 5]. By now, you’ve probably gotten to the point
    where you can tell that the sentences have a similar meaning because they’re similar
    numerically—in other words, [1, 2, 3, 4] looks a lot like [1, 2, 3, 5].
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，考虑一个句子如“我爱我的狗。”你可以用数字[1, 2, 3, 4]来编码它。如果你想编码“我爱我的猫”，你可以用[1, 2, 3, 5]来编码。到现在，你可能已经到了可以判断句子具有相似意义的地步，因为它们在数值上相似——换句话说，[1,
    2, 3, 4]看起来非常像[1, 2, 3, 5]。
- en: The numbers representing words are also called *tokens*, and as a result, this
    process is called *tokenization*. You’ll explore how to do that in code next.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 代表单词的数字也被称为*标记*，因此这个过程被称为*分词*。你将在下一节中探索如何在代码中实现这一点。
- en: Getting Started with Tokenization
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用分词
- en: The PyTorch ecosystem contains many libraries for tokenization, which takes
    words and turns them into tokens. A common tokenizer you might see in code samples
    is `torchtext,` but this has been deprecated since 2023\. So, be careful when
    using it, especially because PyTorch versions advance but it doesn’t. So, some
    alternatives are to use a custom tokenizer, a pretrained one from elsewhere, or
    (surprisingly) those from the Keras ecosystem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch生态系统包含许多用于标记化的库，它将单词转换为标记。你可能在代码示例中看到的一个常见标记器是`torchtext`，但自2023年以来它已经被弃用。所以，在使用它时要小心，特别是因为PyTorch版本在进步，但它没有。因此，一些替代方案是使用自定义标记器、来自其他地方的预训练标记器，或者（令人惊讶的是）来自Keras生态系统的标记器。
- en: Using a custom tokenizer
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义标记器
- en: 'To give you a simple example, here’s some code I used to create a custom tokenizer
    to turn the words of a small corpus (two sentences) into tokens:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个简单的例子，这里有一些我用来创建自定义标记器的代码，将一个小语料库（两个句子）的单词转换为标记：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The word *corpus* is commonly used to denote a set of text items that you will
    use for training. It’s literally the *body* of text that you’ll use to train the
    model and create tokenizers for.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 单词*corpus*通常用来表示一组你将用于训练的文本项。它字面上是你将用于训练模型和创建标记器的文本*主体*。
- en: 'The output of this is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出的结果如下：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see, the tokenizer did a really simple job of creating a list with
    my vocabulary, and every time it hit a unique word, it added it to the list. So
    the first sentence, “Today is a sunny day,” yielded five tokens for the five words:
    “today,” “is,” “a,” “sunny,” and “day.” The second sentence had *most* of these
    words in common, with “rainy” being the exception, so that became the sixth token.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，标记器完成了一个非常简单的任务，创建了一个包含我的词汇表的单词列表，每次它遇到一个独特的单词，它就会将其添加到列表中。所以第一个句子，“今天是个晴天”，产生了五个标记，对应五个单词：“今天”、“是”、“一个”、“晴天”和“天”。第二个句子有*大多数*这些单词是共同的，除了“雨天”，所以它成为了第六个标记。
- en: On the other hand, you can imagine that for a very large corpus, this process
    would be very slow.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你可以想象，对于一个非常大的语料库，这个过程会非常慢。
- en: Using a pretrained tokenizer from Hugging Face
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Hugging Face的预训练标记器
- en: With that in mind, I’m going to use Hugging Face’s transformers library and
    pre-built tokenizers from within it. In this case, because the transformers library
    supports many language models and these language models need tokenizers to work
    with their corpus of text, the tokenizer, which is trained on many millions of
    words, is freely available for you to use. It has bigger coverage than one you
    might create, and it’s free and easy to use!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我将使用Hugging Face的`transformers`库和其中的预构建标记器。在这种情况下，因为`transformers`库支持许多语言模型，而这些语言模型需要标记器来处理它们的文本语料库，所以这个在数百万个单词上训练的标记器是免费供你使用的。它的覆盖范围比你自己创建的更大，而且它是免费且易于使用的！
- en: 'If you don’t have this library already, you can install it with this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这个库，你可以使用以下命令安装它：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let’s see it in action with a simple example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用一个简单的例子来看看它的实际应用：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output from it looks like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它的输出看起来像这样：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, let’s break this down. We start by importing the `BertTokenizerFast` from
    the transformers library. This can be initialized with a number of pretrained
    tokenizers, and we choose the `'bert-base-uncased'` one. You might be wondering
    what on earth that is! Well, the idea here is that I wanted to take a pretrained
    tokenizer, and they are usually partnered with the model they were trained on.
    BERT (which stands for bidirectional encoder representations from transformers)
    is a model trained by Google on a large corpus, with a vocabulary of 30,000 words.
    You can find models like this in the Hugging Face model repository, and when you
    dig down into a model, you’ll often see the transformer’s code to get its tokenizer.
    For example, see [this page that I used](https://oreil.ly/Ok7L9)—and while I’m
    not using the model, I can still get its tokenizer instead of creating a custom
    one.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来分解这个过程。我们首先从`transformers`库中导入`BertTokenizerFast`。这个类可以用多个预训练的标记器进行初始化，我们选择了`'bert-base-uncased'`这个版本。你可能想知道这究竟是什么！好吧，这里的想法是我想使用一个预训练的标记器，通常这些标记器是与它们训练的模型配对的。BERT（代表来自转换器的双向编码器表示）是由谷歌在一个大型语料库上训练的模型，拥有30,000个词汇。你可以在Hugging
    Face模型仓库中找到这样的模型，当你深入一个模型时，你通常会看到获取其标记器的转换器代码。例如，看看[这个页面](https://oreil.ly/Ok7L9)——虽然我没有使用这个模型，但我仍然可以获取它的标记器而不是创建一个自定义的标记器。
- en: In this case, we create a `tokenizer` object and specify the number of words
    that it can tokenize. This will be the maximum number of tokens to generate from
    the corpus of words. We also have a very small corpus here, containing only six
    unique words, so we’ll be well under the maximum of one hundred specified.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们创建一个 `tokenizer` 对象并指定它可以分词的单词数量。这将是从单词语料库中生成的最大分词数。我们这里有一个非常小的语料库，只包含六个独特的单词，所以我们将在指定的最大值一百以下。
- en: 'Once I have the tokenizer, I can then just pass the text to it:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我有了分词器，我就可以直接将文本传递给它：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’ll explore padding and truncation a little later in this chapter, but for
    now, you should note the `return_tensors='pt'` parameter. This is a nice convenience
    for us PyTorch developers because the return values will be `torch.Tensor` objects,
    which are easy for us to handle.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后探索填充和截断，但就目前而言，你应该注意 `return_tensors='pt'` 参数。这对我们 PyTorch 开发者来说是一个很好的便利，因为返回值将是
    `torch.Tensor` 对象，这使我们很容易处理。
- en: 'The BERT model uses a number of overlays on the original tokenization, such
    as `attention_masking`, which means it works with `IDs` for each word instead
    of raw tokens. This is beyond the scope of this chapter, but where it impacts
    you right now is if you don’t need all that. If you just want the tokens, you
    have to extract the tokens in the following way, noting that your sentences were
    encoded as `input_Ids` within the BERT tokenizer:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型在原始分词上使用了一些叠加，例如 `attention_masking`，这意味着它使用每个单词的 `IDs` 而不是原始分词。这超出了本章的范围，但对你当前的影响是，如果你不需要所有这些。如果你只需要分词，你必须以以下方式提取分词，注意你的句子在
    BERT 分词器中被编码为 `input_Ids`：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once you’ve done that, you can easily print out the following `Tokens` collection:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了这个，你就可以轻松地打印出以下 `Tokens` 集合：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, you may be wondering what `[CLS]` and `[SEP]` are—and how the BERT model
    has been trained to expect sentences to begin with `[CLS]` (for *classifier*)
    and end with or be separated by `[SEP]` (for *separator*). These two expressions
    are tokenized to values 101 and 102, respectively, so when you print out the token
    values for your sentences, you’ll see this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道 `[CLS]` 和 `[SEP]` 是什么——以及 BERT 模型是如何被训练成期望句子以 `[CLS]`（用于*分类器*）开始，以
    `[SEP]`（用于*分隔符*）结束或分隔的。这两个表达式分别被分词为值 101 和 102，所以当你打印出你句子的分词值时，你会看到这个：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: From this, you can derive that *today* is token 2651 in BERT, *is* is token
    2003, etc.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个中，你可以推导出 *今天* 在 BERT 中是分词 2651，*是* 是分词 2003，等等。
- en: So, it really depends on you how you want to approach this. For learning with
    small datasets, the custom tokenizer is probably OK. But once you start getting
    into larger datasets, you may want to opt for a pretrained tokenizer. In that
    case, you may have to deal with some overhead—so for the rest of this chapter,
    I’m going to use custom code to tokenize and preprocess the text without the overhead
    of something like the BERT tokenizer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这完全取决于你如何处理这个问题。对于使用小型数据集进行学习，自定义分词器可能就足够了。但一旦你开始处理更大的数据集，你可能希望选择一个预训练的分词器。在这种情况下，你可能必须处理一些开销——所以在本章的其余部分，我将使用自定义代码来分词和预处理文本，而不需要像
    BERT 分词器那样的开销。
- en: Either way, once you have the words in your sentences tokenized, the next step
    is to convert your sentences into lists of numbers, with the number being the
    value where the word is the key. This process is called *sequencing*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，一旦你的句子中的单词被分词，下一步就是将你的句子转换成数字列表，其中数字是单词作为键的值。这个过程被称为*序列化*。
- en: Turning Sentences into Sequences
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将句子转换为序列
- en: 'Now that you’ve seen how to take words and tokenize them into numbers, the
    next step is to encode the sentences into sequences of numbers, which you can
    do as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了如何将单词转换为数字，下一步是将句子编码成数字序列，你可以这样做：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, you’ll be given the sequences representing the three sentences. Remember
    that the word index is this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你会得到表示三个句子的序列。记住，单词索引是这样的：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the output will look like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将看起来像这样：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can then substitute the words for the numbers, and you’ll see that the sentences
    make sense.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以用数字替换单词，你会发现句子是有意义的。
- en: Now, consider what happens if you are training a neural network on a set of
    data. The typical pattern is that you have a set of data that’s used for training
    but that you know won’t cover 100% of your needs, but you hope it covers as much
    as possible. In the case of NLP, you might have many thousands of words in your
    training data that are used in many different contexts, but you can’t have every
    possible word in every possible context. So when you show your neural network
    some new, previously unseen text that contains previously unseen words, what might
    happen? You guessed it—the network will get confused because it simply has no
    context for those words, and as a result, any prediction it gives will be negatively
    affected.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一下如果你在一组数据上训练神经网络会发生什么。典型的模式是，你有一组用于训练的数据，但你知道它不会覆盖100%的需求，但你希望尽可能多地覆盖。在NLP的情况下，你可能在训练数据中有成千上万的单词，它们在许多不同的上下文中使用，但你不能在每一个可能的上下文中都有每一个可能的单词。所以当你向你的神经网络展示一些新的、之前未见过的文本，其中包含之前未见过的单词时，可能会发生什么？你已经猜到了——网络会感到困惑，因为它根本没有任何这些单词的上下文，因此，它给出的任何预测都会受到负面影响。
- en: Using out-of-vocabulary tokens
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用词汇表外的标记
- en: 'One tool you can use to handle these situations is an *out-of-vocabulary* (OOV)
    *token*, which can help your neural network understand the context of the data
    containing previously unseen text. For example, given the previous small example
    corpus, suppose you want to process sentences like these:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用一个工具来处理这些情况，那就是一个*词汇表外*（OOV）*标记*，它可以帮助你的神经网络理解包含之前未见过的文本的数据的上下文。例如，给定之前的简小语料库，假设你想要处理这样的句子：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Remember that you’re not adding this input to the corpus of existing text (which
    you can think of as your training data) but you’re considering how a pretrained
    network might view this text. Say you tokenize it with the words that you’ve already
    used and your existing tokenizer, like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你并不是将这个输入添加到现有的文本语料库中（你可以将其视为你的训练数据），而是考虑一个预训练的网络可能会如何看待这段文本。比如说，你用你已经使用过的单词和现有的分词器来分词，就像这样：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, your results will look like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你的结果将看起来像这样：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So, the new sentences, swapping back tokens for words, would be “today is a
    <UNK> day” and “<UNK> <UNK> <UNK> rainy <UNK>.”
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，新的句子，将标记换回单词，将是“今天是<UNK>的一天”和“<UNK> <UNK> <UNK>雨天<UNK>。”
- en: Here I’m using the tag <UNK> (which stands for *unknown*) for token 0\. If you
    check out the `text_to_sequence` code I showed previously, it uses `0` for words
    that aren’t in its dictionary. You can, of course, use any value you like.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我使用标签<UNK>（代表*未知*）来表示标记0。如果你查看我之前展示的`text_to_sequence`代码，它使用`0`来表示其字典中不存在的单词。当然，你可以使用任何你喜欢的值。
- en: Understanding padding
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解填充
- en: When training neural networks, you typically need all your data to be in the
    same shape. Recall from earlier chapters that when you were training with images,
    you reformatted the images to be the same width and height. With text, you face
    the same issue—once you’ve tokenized your words and converted your sentences into
    sequences, they can all be different lengths. But to get them to be the same size
    and shape, you can use *padding*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，你通常需要所有数据都具有相同的形状。回想一下，在之前章节中，当你用图像训练时，你会重新格式化图像以具有相同的宽度和高度。对于文本，你面临相同的问题——一旦你将单词分词并将句子转换为序列，它们的长度都可以不同。但为了使它们具有相同的大小和形状，你可以使用*填充*。
- en: All the sentences we have used so far are composed of five words, so you can
    see that our sequences are five tokens. But what would happen if you had some
    sentences that were longer. Say a few had 5 words, some had 8 words, and some
    had 10 words. To have a neural network handle them all, they would need to be
    of the same length! You could convert everything to 10 words by lengthening the
    sentences that are shorter, convert everything to 5 words by chopping off bits
    of the longer ones, or follow some other strategy!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所使用的所有句子都是由五个单词组成的，所以你可以看到我们的序列是五个标记。但是，如果你有一些较长的句子会怎样呢？比如说，一些句子有5个单词，一些有8个单词，一些有10个单词。为了使神经网络处理它们，它们需要具有相同的长度！你可以通过延长较短的句子将所有内容转换为10个单词，通过截断较长的句子的一部分将所有内容转换为5个单词，或者遵循其他策略！
- en: 'To explore padding, let’s add another, much longer, sentence to the corpus:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索填充，让我们将另一个更长的句子添加到语料库中：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When you sequence that, you’ll see that your lists of numbers have different
    lengths. Also note that if you haven’t retokenized to build the new vocabulary,
    the latter two sentences will be full of zeros:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当你进行序列化时，你会看到你的数字列表有不同的长度。还请注意，如果你没有重新分词来构建新的词汇表，后两个句子将充满零：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So, don’t forget to call:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，别忘了调用：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And then you’ll have new tokens for the new words in your tokenizer, so the
    output will look like this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将为你的分词器中的新单词创建新的标记，输出将看起来像这样：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Remember that when you were training neural networks in earlier chapters, your
    input layers of the neural network required images to have consistent sizes and
    shapes. It’s the same with NLP, for the most part. (There’s an exception for something
    called *ragged tensors*, but that’s beyond the scope of this chapter.) So, we
    need a way to make our sentences the same length.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当你在前几章中训练神经网络时，你的神经网络输入层需要图像具有一致的大小和形状。在NLP的大部分情况下也是如此。（对于所谓的*ragged tensors*有一个例外，但这超出了本章的范围。）因此，我们需要一种方法来使我们的句子具有相同的长度。
- en: 'Here’s a simple padding function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的填充函数：
- en: '[PRE19]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This function will reshape every array in the sequence to be the same length
    as the maximum-length one. So, say we take our sentences and pad them after sequencing
    them with code like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数会将序列中的每个数组重塑为与最大长度相同的长度。所以，假设我们使用以下代码对句子进行序列化后，再进行填充：
- en: '[PRE20]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, the output will look like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，输出将看起来像这样：
- en: '[PRE21]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, each of the sequences has a length of 10 because of the `maxlen` parameter.
    It’s a pretty simple implementation that you would likely want to build on if
    you’re using this in a more serious way. For example, you might want to consider
    what would happen if you had a sequence that was longer than the maximum length.
    Right now, it would cut off everything *after* the maximum, but you might want
    it to exhibit different behavior!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于`maxlen`参数，每个序列的长度都是10。这是一个相当简单的实现，如果你更认真地使用它，你可能会想要在此基础上构建。例如，你可能想要考虑如果你有一个比最大长度更长的序列会发生什么。目前，它会在最大长度之后截断一切，但你可能希望它表现出不同的行为！
- en: Also note that if you’re using off-the-shelf tokenizers like the BERT one we
    showed you earlier, much of this functionality may already be available to you,
    so be sure to experiment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，如果你使用的是像我们之前展示的BERT这样的现成分词器，那么这些功能中的许多可能已经可用，所以请确保进行实验。
- en: Removing Stopwords and Cleaning Text
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除停用词和清洗文本
- en: In the next section you’ll look at some real-world datasets, and you’ll find
    that there’s often text that you *don’t* want in your dataset. You may also want
    to filter out so-called *stopwords*—like “the,” “and,” and “but”—that are too
    common and don’t add any meaning. You may also encounter a lot of HTML tags in
    your text, and it would be good to have a clean way to remove them. Other things
    you may want to filter out include rude words, punctuation, or names. Later, we’ll
    explore a dataset of tweets that often have somebody’s user ID in them, and we’ll
    want to filter those out.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将查看一些真实世界的数据集，你会发现数据集中往往有一些你*不希望*包含的文本。你可能还希望过滤掉所谓的*停用词*——如“the”、“and”和“but”——它们太常见且不增加任何意义。你也可能遇到很多HTML标签，有一个干净的方法来移除它们会很好。你可能还想过滤掉粗俗的词语、标点符号或名字。稍后，我们将探索一个包含用户ID的推文数据集，我们希望过滤掉这些ID。
- en: While every task is different based on your corpus of text, there are three
    main things that you can do to clean up your text programmatically.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每个任务都基于你的文本语料库而有所不同，但你可以通过以下三种主要方式来程序化地清理你的文本。
- en: Stripping Out HTML Tags
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除HTML标签
- en: 'The first thing you can do is strip out HTML tags, and fortunately, there’s
    a library called BeautifulSoup that makes this straightforward. For example, if
    your sentences contain HTML tags such as `<br>`, then you can remove them by using
    this code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的第一件事是移除HTML标签，幸运的是，有一个名为BeautifulSoup的库可以使这个过程变得简单。例如，如果你的句子包含如`<br>`这样的HTML标签，你可以通过以下代码来移除它们：
- en: '[PRE22]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Stripping Out Stopwords
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除停用词
- en: 'The second thing to do is strip out stopwords, and a common way to do it is
    to have a stopwords list and preprocess your sentences by removing instances of
    stopwords. Here’s an abbreviated example:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第二件事是移除停用词，一个常见的方法是拥有一个停用词列表，并通过移除停用词的实例来预处理你的句子。以下是一个简化的示例：
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You can find a full stopwords list in some of the [online examples for this
    chapter](https://github.com/lmoroney/PyTorch-Book-FIles).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本章的一些[在线示例](https://github.com/lmoroney/PyTorch-Book-FIles)中找到一个完整的停用词列表。
- en: 'Then, as you’re iterating through your sentences, you can use code like this
    to remove the stopwords from your sentences:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当你遍历你的句子时，你可以使用如下代码来从你的句子中去除停用词：
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Stripping Out Punctuation
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除标点符号
- en: The third thing you can do is strip out punctuation, and you’ll want to do it
    because punctuation can fool a stopword remover. The one we just showed you looks
    for words surrounded by spaces, so it won’t spot a stopword that’s immediately
    followed by a period or a comma.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的第三件事是去除标点符号，你想要这样做是因为标点符号可能会欺骗停用词过滤器。我们刚刚展示的那个就是寻找被空格包围的单词，所以它不会发现紧随句号或逗号之后的停用词。
- en: Fixing this problem is easy with the translation functions provided by the Python
    string library. But do be careful with this approach, as there are scenarios where
    it might impact NLP analysis, particularly when detecting sentiment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python字符串库提供的翻译函数可以轻松解决这个问题。但请注意这种方法，因为在某些情况下它可能会影响NLP分析，尤其是在检测情感时。
- en: 'The library also comes with a constant called `string.punctuation` that contains
    a list of common punctuation marks, so to remove them from a word, you can do
    the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该库还包含一个名为`string.punctuation`的常量，其中包含常用标点符号的列表，因此要从单词中去除它们，你可以这样做：
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, before filtering for stopwords, the constant removes punctuation from
    each word in the sentence. So, if splitting a sentence gives you the word *it*,
    the word will be converted to *it* and then stripped out as a stopword. Note,
    however, that when doing this, you may have to update your stopwords list. It’s
    also common for these lists to have abbreviated words and contractions like *you’ll*
    in them, and the translator will change *you’ll* to *youll*. So if you want to
    have those words filtered out, you’ll need to update your stopwords list to include
    them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在过滤停用词之前，常量会从句子中的每个单词去除标点符号。所以，如果分割一个句子给你的是单词*it*，这个单词将被转换为*it*，然后作为停用词被去除。然而，请注意，在这样做的时候，你可能需要更新你的停用词列表。这些列表通常包含缩写词和像*you’ll*这样的缩写，翻译器会将*you’ll*转换为*youll*。所以，如果你想过滤掉这些单词，你需要更新你的停用词列表以包括它们。
- en: Following these three steps will give you a much cleaner set of text to use.
    But of course, every dataset will have its idiosyncrasies that you’ll need to
    work with.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这三个步骤将为你提供一套更干净的文本集。但当然，每个数据集都会有其独特的特性，你需要与之打交道。
- en: Working with Real Data Sources
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用真实数据源
- en: 'Now that you’ve seen the basics of getting sentences, encoding them with a
    word index, and sequencing the results, you can take it to the next level by taking
    some well-known public datasets and using the tools Python provides to get them
    into a format where you can easily sequence them. We’ll start with a dataset in
    which a lot of the work has already been done for you: the IMDb dataset. After
    that, we’ll get a bit more hands-on by processing a JSON-based dataset and a couple
    of comma-separated values (CSV) datasets with emotion data in them.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了获取句子、用词索引编码它们以及排序结果的基础知识，你可以通过使用一些知名的公共数据集并利用Python提供的工具将它们转换成易于排序的格式来提升到下一个层次。我们将从一个已经为你做了大量工作的数据集开始：IMDb数据集。之后，我们将通过处理基于JSON的数据集和包含情感数据的几个逗号分隔值（CSV）数据集来获得更多实践经验。
- en: Getting Text Datasets
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取文本数据集
- en: We explored some datasets in [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246),
    so if you get stuck on any of the concepts in this section, you can get a quick
    review there. However, at the time of writing, accessing *text*-based datasets
    is a little unusual. Given that the torchtext library has been deprecated, it’s
    not clear what will happen with its built-in datasets, so we’ll get hands-on in
    dealing with raw data in this section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](ch04.html#ch04_using_data_with_pytorch_1748548966496246)中探索了一些数据集，所以如果你在这个部分遇到任何概念上的困难，你可以在那里快速复习。然而，在撰写本文时，访问基于文本的数据集有些不寻常。鉴于torchtext库已被弃用，其内置数据集的未来走向尚不明确，所以在本节中我们将亲自动手处理原始数据。
- en: We’ll start by exploring the IMDb reviews, which is a dataset of 50,000 labeled
    movie reviews from the Internet Movie Database (IMDb), each of which is determined
    to be positive or negative in sentiment.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探索IMDb评论数据集，这是一个包含来自互联网电影数据库（IMDb）的50,000条标记电影评论的数据集，每条评论都被判定为正面或负面情感。
- en: 'This code will download the raw dataset and unzip it into folders where training
    and test splits are already pre-made for us. These will then be stored in subdirectories,
    and there are further subdirectories called `pos` and `neg` in each that determine
    the labels of the text files they contain:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将下载原始数据集并将其解压缩到文件夹中，其中已经为我们预先制作了训练和测试分割。然后，它们将存储在子目录中，每个子目录中都有名为 `pos` 和
    `neg` 的进一步子目录，这些子目录确定了包含的文本文件的标签：
- en: '[PRE26]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The file structure will look like the one in [Figure 5-1](#ch05_figure_1_1748549080734915).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 文件结构将类似于[图5-1](#ch05_figure_1_1748549080734915)中的结构。
- en: '![](assets/aiml_0501.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0501.png)'
- en: Figure 5-1\. Exploring the IMDb dataset structure
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 探索IMDb数据集结构
- en: In this figure you can see the *test/pos* directory and the first couple of
    files in it. Note that these are text files, so to create a tokenizer and vocabulary,
    we’re going to have to read files instead of in-memory strings like in the earlier
    example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，您可以看到 *test/pos* 目录及其中的前几个文件。请注意，这些是文本文件，因此为了创建分词器和词汇表，我们必须读取文件，而不是像早期示例中那样读取内存中的字符串。
- en: 'Let’s take a look at the code for a custom tokenizer for this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看为这个自定义分词器编写的代码：
- en: '[PRE27]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It’s pretty straightforward code that just reads through each file and adds
    new words it discovers to the vocabulary, giving each word a new token value.
    Generally, you’ll only want to do this for the training data, with the understanding
    that there will be words in the test data that aren’t in the training data and
    that they would be tokenized with an OOV or unknown token.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一段非常直接的代码，它只是遍历每个文件，并将发现的每个新单词添加到词汇表中，为每个单词分配一个新的标记值。通常，您只想对训练数据进行此操作，理解测试数据中会有不在训练数据中的单词，并且它们将使用OOV（未知标记）进行分词。
- en: 'The output should look like this (truncated):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示（已截断）：
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This is a naive tokenizer in that the first word it sees gets the first token,
    the second gets the second, etc. For performance reasons, it’s often better for
    the more frequent words in the corpus to get the earlier tokens and the less frequent
    ones to get the later tokens. We’ll explore that in a moment.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的分词器，因为它看到的第一个单词获得第一个标记，第二个获得第二个，依此类推。出于性能原因，通常更好的做法是，语料库中更频繁的单词获得更早的标记，而较少频繁的单词获得较晚的标记。我们将在稍后探讨这一点。
- en: 'You can then do sequencing and padding as you did earlier:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以像之前那样进行序列化和填充：
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: So, for example, our sentence `This is an example` will output as `[30, 56,
    144, 16040]` because those are the tokens assigned to those words. The padded
    sequence would have a tensor of 256 values, with these tokens as the first 4 and
    the next 252 being zeros!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的句子 `This is an example` 将输出为 `[30, 56, 144, 16040]`，因为这些是分配给这些单词的标记。填充的序列将有一个包含256个值的张量，其中这些标记作为前4个，接下来的252个是零！
- en: 'Now, let’s update the tokenizer to do the words in order of frequency. This
    update changes the tokenizer so that we load all of the files into memory and
    count the instance of each word to get a frequency table:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更新分词器，按频率顺序处理单词。这次更新改变了分词器，以便我们将所有文件加载到内存中，并计算每个单词的实例以获取频率表：
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can then output the vocabulary as this frequency table. The vocabulary is
    too large to show the entire index, but here are the top 20 words. Note that the
    tokenizer lists them in order of frequency in the dataset, so common words like
    *the*, *and*, and *a* are indexed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以输出词汇表，如下所示的频率表。由于词汇表太大，无法显示整个索引，但这里列出了前20个单词。请注意，分词器按数据集中单词的频率顺序列出它们，因此像
    *the*、*and* 和 *a* 这样的常用词被索引：
- en: '[PRE31]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: These are stopwords, as described in the previous section. Having these present
    can impact your training accuracy because they’re the most common words and they’re
    nondistinct (i.e., they’re likely present in both positive and negative reviews),
    so they add noise to our training.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是停用词，如前文所述。这些词的存在可能会影响您的训练精度，因为它们是最常见的词，并且它们没有区分性（即，它们可能存在于正面和负面评论中），因此它们会给我们的训练添加噪声。
- en: Also note that *br* is included in this list because it’s commonly used in this
    corpus as the `<br>` HTML tag.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，*br* 包含在这个列表中，因为它在这个语料库中常用作 `<br>` HTML标签。
- en: 'You can also update the code to use `BeautifulSoup` to remove the HTML tags,
    and you can remove stopwords from the given list as follows. To do this, you can
    update the tokenizer to remove the HTML tags by using `BeautifulSoup`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以更新代码以使用 `BeautifulSoup` 来删除HTML标签，并且您可以从提供的列表中删除停用词。为此，您可以更新分词器，使用 `BeautifulSoup`
    来删除HTML标签：
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, when you print out your word index, you’ll see this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当您打印出您的单词索引时，您将看到这个：
- en: '[PRE33]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You can see that this is much cleaner than before. There’s always room to improve,
    however, and one thing I noted when looking at the full index was that some of
    the less common words toward the end were nonsensical. Often, reviewers would
    combine words, for example with a dash (as in *annoying-conclusion*) or a slash
    (as in *him/her*), and the stripping of punctuation would incorrectly turn these
    combined words into a single word. Or, as you can see in the preceding code, the
    dash (*-*) character was common enough to be tokenized. You can strip that out
    by adding it as a stopword.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这比之前干净多了。然而，总有改进的空间，当我查看完整索引时，我注意到一些不常见的单词在末尾是无意义的。通常，审稿人会合并单词，例如使用破折号（如
    *annoying-conclusion*）或斜杠（如 *him/her*），而去除标点符号会错误地将这些合并的单词变成一个单词。或者，如前述代码所示，破折号（*-*）字符足够常见，以至于被标记化。你可以通过将其添加为停用词来去除它。
- en: 'Now that you have a tokenizer for the corpus, you can encode your sentences.
    For example, the simple sentences we were looking at earlier in the chapter will
    come out like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了语料库的标记化器，你可以对句子进行编码。例如，我们在本章前面看到的简单句子将输出如下：
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If you decode these, you’ll see that the stopwords are dropped and you get the
    sentences encoded as `today sunny day`, `today rainy day`, and `sunny today`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你解码这些，你会看到停用词被删除，你得到的是编码为 `today sunny day`、`today rainy day` 和 `sunny today`
    的句子。
- en: 'If you want to do this in code, you can create a new `dict` with the reversed
    keys and values (i.e., for a key/value pair in the word index, you can make the
    value the key and the key the value) and do the lookup from that. Here’s the code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在代码中完成这个操作，你可以创建一个新的 `dict`，其中键和值的顺序是相反的（即，对于单词索引中的键/值对，你可以将值作为键，将键作为值），然后从那里进行查找。以下是代码：
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will give the following result:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下结果：
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: A common way to store labeled text data is in the comma-separated value (CSV)
    format. We’ll discuss that next.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 存储标记文本数据的一种常见方式是逗号分隔值（CSV）格式。我们将在下一节中讨论这一点。
- en: Getting Text from CSV Files
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 CSV 文件中获取文本
- en: NLP data is also commonly available in CSV file format. Over the next couple
    of chapters, you’ll use a CSV of data that I adapted from the open source [Sentiment
    Analysis in Text dataset](https://oreil.ly/7ZKEU). The creator of this dataset
    sourced it from Twitter (now called X). You will use two different datasets, one
    where the emotions have been reduced to “positive” or “negative” for binary classification
    and one where the full range of emotion labels is used. Both datasets use the
    same structure, so I’ll just show the binary version here.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 数据也通常以 CSV 文件格式提供。在接下来的几章中，你将使用我改编自开源 [Sentiment Analysis in Text 数据集](https://oreil.ly/7ZKEU)
    的 CSV 数据。该数据集的创建者从 Twitter（现在称为 X）获取了数据。你将使用两个不同的数据集，一个将情感简化为“正面”或“负面”以进行二元分类，另一个使用完整的情感标签范围。这两个数据集使用相同的结构，所以我只展示二元版本。
- en: While the name *CSV* seems to suggest a standard file format in which values
    are comma separated, there’s actually a wide diversity of formats that can be
    considered CSV, and there’s very little adherence to any particular standard.
    To solve this, the Python csv library makes handling CSV files straightforward.
    In this case, the data is stored with two values per line. The first value is
    a number (0 or 1) denoting whether the sentiment is negative or positive, and
    the second value is a string containing the text.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 *CSV* 这个名字似乎暗示了一个标准文件格式，其中值以逗号分隔，但实际上有各种各样的格式可以被认为是 CSV，而且对任何特定标准的遵守非常少。为了解决这个问题，Python
    的 csv 库使得处理 CSV 文件变得简单直接。在这种情况下，数据是按每行两个值存储的。第一个值是一个数字（0 或 1），表示情感是负面还是正面，第二个值是一个包含文本的字符串。
- en: 'The following code snippet will read the CSV and do preprocessing that’s similar
    to what we saw in the previous section. For the full code, please check the repo
    for this book. The code adds spaces around the punctuation in compound words,
    uses `BeautifulSoup` to strip HTML content, and then removes all punctuation characters:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段将读取 CSV 并执行类似于我们在上一节中看到的预处理。对于完整的代码，请查看本书的仓库。该代码在复合词的标点符号周围添加空格，使用 `BeautifulSoup`
    去除 HTML 内容，然后删除所有标点符号：
- en: '[PRE37]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This will give you a list of 35,327 sentences.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给你一个包含 35,327 个句子的列表。
- en: Note that this code is specific to this data. It’s intended to help you understand
    the types of tasks you may have to take on in order to make stuff work, and it’s
    not intended to be an exhaustive list of things that you’ll have to do for every
    task—so your mileage may vary.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，此代码针对特定数据。它的目的是帮助你了解你可能需要执行的任务类型，以便使某些东西工作，并不旨在成为你必须为每个任务执行的所有事情的详尽列表——因此你的里程可能会有所不同。
- en: Creating training and test subsets
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建训练集和测试集
- en: 'Now that the text corpus has been read into a list of sentences, you’ll need
    to split it into training and test subsets for training a model. For example,
    if you want to use 28,000 sentences for training with the rest held back for testing,
    you can use code like this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在文本语料库已经被读入一个句子列表中，你需要将其分割成训练集和测试集以训练模型。例如，如果你想用28,000个句子进行训练，其余的保留用于测试，你可以使用如下代码：
- en: '[PRE38]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now that you have a training set, you can edit the tokenizer and vocabulary
    builder to create the word index from this corpus. As the corpus is an in-memory
    array of strings (`training_sentences`), the process is a lot simpler:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了训练集，你可以编辑分词器和词汇构建器，从这个语料库中创建单词索引。由于语料库是一个字符串的内存数组（`training_sentences`），这个过程要简单得多：
- en: '[PRE39]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can use the same helper functions to turn the text into a sequence and
    then padding, like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用相同的辅助函数将文本转换为序列，然后进行填充，如下所示：
- en: '[PRE40]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The results will be as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE41]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Another common format for structured data, particularly in response to web calls,
    is JavaScript Object Notation (JSON). We’ll explore how to read JSON data next.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的结构化数据格式，尤其是在响应网络调用时，是JavaScript对象表示法（JSON）。我们将探讨如何读取JSON数据。
- en: Getting Text from JSON Files
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从JSON文件中获取文本
- en: JSON is an open standard file format that’s used often for data interchange,
    particularly with web applications. It’s human readable and designed to use name/value
    pairs, and as such, it’s particularly well suited for labeled text. A quick search
    of Kaggle datasets for JSON yields over 2,500 results. For example, popular datasets
    such as the Stanford Question Answering Dataset (SQuAD) are stored in JSON.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是一个开放标准文件格式，常用于数据交换，尤其是在网络应用程序中。它是可读的，并设计为使用名称/值对，因此特别适合用于标记文本。在Kaggle数据集中搜索JSON会得到超过2,500个结果。例如，流行的数据集，如斯坦福问答数据集（SQuAD），就是存储在JSON中的。
- en: 'JSON has very simple syntax in which objects are contained within braces as
    name/value pairs, each of which is separated by a comma. For example, a JSON object
    representing my name would be as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: JSON具有非常简单的语法，其中对象包含在大括号中，作为名称/值对，每个对之间用逗号分隔。例如，表示我名字的JSON对象如下所示：
- en: '[PRE42]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'JSON also supports arrays, which are a lot like Python lists and are denoted
    by the square bracket syntax. Here’s an example:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: JSON还支持数组，这与Python列表非常相似，并且用方括号语法表示。以下是一个示例：
- en: '[PRE43]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Objects can also contain arrays, so this is perfectly valid JSON:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对象也可以包含数组，因此这是完全有效的JSON：
- en: '[PRE44]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'A smaller dataset that’s stored in JSON and a lot of fun to work with is the
    “News Headlines Dataset for Sarcasm Detection” by [Rishabh Misra](https://oreil.ly/wZ3oD),
    which is available on [Kaggle](https://oreil.ly/_AScB). This dataset collects
    news headlines from two sources: *The Onion* for funny or sarcastic ones and the
    *HuffPost* for normal headlines.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一个存储在JSON中且非常有趣的数据集是“用于讽刺检测的新闻标题数据集”，由[Rishabh Misra](https://oreil.ly/wZ3oD)创建，可在[Kaggle](https://oreil.ly/_AScB)上找到。这个数据集收集了两个来源的新闻标题：*The
    Onion*用于搞笑或讽刺的标题，而*HuffPost*用于普通标题。
- en: 'The file structure in the sarcasm dataset is very simple:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 讽刺数据集的文件结构非常简单：
- en: '[PRE45]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The dataset consists of about 26,000 items, one per line. To make it more readable
    in Python, I’ve created a version that encloses these items in an array so the
    dataset can be read as a single list, which is used in the source code for this
    chapter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含大约26,000个项目，每个项目一行。为了在Python中使其更易于阅读，我创建了一个版本，将这些项目包含在一个数组中，这样数据集就可以作为一个单独的列表读取，该列表用于本章的源代码。
- en: Reading JSON files
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取JSON文件
- en: Python’s json library makes reading JSON files simple. Given that JSON uses
    name/value pairs, you can index the content based on the name. So, for example,
    for the sarcasm dataset, you can create a file handle to the JSON file, open it
    with the `json` library, and have an iterable go through, read each field line
    by line, and get the data item by using the name of the field.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Python的json库使得读取JSON文件变得简单。鉴于JSON使用名称/值对，你可以根据名称进行索引。因此，例如，对于讽刺数据集，你可以创建一个指向JSON文件的文件句柄，使用`json`库打开它，然后让一个可迭代对象逐行读取，通过字段名称获取数据项。
- en: 'Here’s the code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码：
- en: '[PRE46]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This makes it simple for you to create lists of sentences and labels, as you’ve
    done throughout this chapter, and then tokenize the sentences. You can also do
    preprocessing on the fly as you read a sentence, removing stopwords, HTML tags,
    punctuation, and more.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得你可以轻松地创建句子和标签的列表，就像你在本章中做的那样，然后对句子进行标记化。你还可以在阅读句子时即时进行预处理，去除停用词、HTML标签、标点符号等。
- en: 'Here’s the complete code to create lists of sentences, labels, and URLs while
    having the sentences cleaned of unwanted words and characters:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建句子、标签和URL列表的完整代码，同时确保句子被清理掉不需要的单词和字符：
- en: '[PRE47]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As before, you can split these into training and test sets. If you want to
    use 23,000 of the 26,000 items in the dataset for training, you can do the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以将这些数据分成训练集和测试集。如果你想使用数据集中的23,000个中的26,000个项进行训练，你可以这样做：
- en: '[PRE48]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now that you have them as `in_memory` string arrays, tokenizing them and sequencing
    them will work exactly the same way as tokenizing and sequencing the sarcasm dataset.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经将它们作为`in_memory`字符串数组，对它们进行标记化和序列化将完全与对讽刺数据集进行标记化和序列化的方式相同。
- en: Hopefully, the similar-looking code will help you see the pattern that you can
    follow when preparing text for neural networks to classify or generate. In the
    next chapter, you’ll learn how to build a classifier for text using embeddings,
    and in [Chapter 7](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648)
    you’ll take that a step further by exploring recurrent neural networks. Then,
    in [Chapter 8](ch08.html#ch08_using_ml_to_create_text_1748549671852453), you’ll
    learn how to further enhance the sequence data to create a neural network that
    can generate new text!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 希望类似外观的代码能帮助你看到在为神经网络进行分类或生成文本准备时可以遵循的模式。在下一章中，你将学习如何使用嵌入构建文本分类器，而在[第7章](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648)中，你将通过探索循环神经网络将这一步更进一步。然后，在第[第8章](ch08.html#ch08_using_ml_to_create_text_1748549671852453)中，你将学习如何进一步增强序列数据以创建一个能够生成新文本的神经网络！
- en: Tip
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Regular expressions (aka Regex) are terrific tools for sorting, filtering, and
    cleaning text. They have a syntax that’s often hard to understand and difficult
    to learn, but I have found that generative AI tools like Gemini, Claude, and ChatGPT
    are really useful here.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式（也称为Regex）是排序、过滤和清理文本的出色工具。它们的语法通常很难理解，学习起来也很困难，但我发现Gemini、Claude和ChatGPT等生成式AI工具在这里非常有用。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In earlier chapters, you used images to build a classifier. Images, by definition,
    have well-defined dimensions—you know their width, height, and format. Text, on
    the other hand, can be far more difficult to work with. It is often unstructured,
    can contain undesirable content such as formatting instructions, doesn’t always
    contain what you want, and often has to be filtered to remove nonsensical or irrelevant
    content.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你使用了图像来构建分类器。根据定义，图像具有明确的维度——你知道它们的宽度和高度以及格式。另一方面，文本处理起来可能要困难得多。它通常是未结构化的，可能包含不希望的内容，如格式说明，不一定包含你想要的内容，并且通常需要过滤以去除无意义或不相关的内容。
- en: In this chapter, you saw how to take text and convert it to numbers using word
    tokenization, and you then explored how to read and filter text in a variety of
    formats. With these skills in hand, you’re now ready to take the next step and
    learn how *meaning* can be inferred from words—which is the first step in understanding
    natural language.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你看到了如何使用词标记化将文本转换为数字，然后探讨了如何以各种格式读取和过滤文本。掌握这些技能后，你现在可以迈出下一步，学习如何从单词中推断出*意义*——这是理解自然语言的第一个步骤。
