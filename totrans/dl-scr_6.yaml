- en: Chapter 6\. Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover recurrent neural networks (RNNs), a class of neural
    network architectures meant for handling sequences of data. The neural networks
    we’ve seen so far treated each batch of data they received as a set of independent
    observations; there was no notion of some of the MNIST digits arriving before
    or after the other digits, in either the fully connected neural networks we saw
    in [Chapter 4](ch04.html#extensions) or the convolutional neural networks we saw
    in [Chapter 5](ch05.html#convolution). Many kinds of data, however, are intrinsically
    ordered, whether time series data, which one might deal with in an industrial
    or financial context, or language data, in which the characters, words, sentences,
    and so on are ordered. Recurrent neural networks are designed to learn how to
    take in *sequences* of such data and return a correct prediction as output, whether
    that correct prediction is of the price of a financial asset on the following
    day or of the next word in a sentence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Dealing with ordered data will require three kinds of changes from the fully
    connected neural networks we saw in the first few chapters. First, it will involve
    “adding a new dimension” to the `ndarray`s we feed our neural networks. Previously,
    the data we fed our neural networks was intrinsically two-dimensional—each `ndarray`
    had one dimension representing the number of observations and another representing
    the number of features;^([1](ch06.html#idm45732615342296)) another way to think
    of this is that *each observation* was a one-dimensional vector. With recurrent
    neural networks, each input will still have a dimension representing the number
    of observations, but each observation will be represented as a two-dimensional
    `ndarray`: one dimension will represent the length of the sequence of data, and
    a second dimension will represent the number of features present at each sequence
    element. The overall input to an RNN will thus be a three-dimensional `ndarray`
    of shape `[batch_size, sequence_length, num_features]`—a batch of sequences.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, of course, to deal with this new three-dimensional input we’ll have
    to use a new kind of neural network architecture, which will be the main focus
    of this chapter. The third change, however, is where we’ll start our discussion
    in this chapter: we’ll have to use a completely different framework with different
    abstractions to deal with this new form of data. Why? In the cases of both fully
    connected and convolutional neural networks, each “operation,” even if it in fact
    represented *many* individual additions and multiplications (as in the case of
    matrix multiplication or a convolution), could be described as a single “minifactory,”
    that on both the forward and backward passes took in one `ndarray` as input and
    produced one `ndarray` as an output (possibly using another `ndarray` representing
    the operation’s parameters as part of these computations). As it turns out, recurrent
    neural networks cannot be implemented in this way. Before reading further to find
    out why, take some time to think about it: what characteristics of a neural network
    architecture would cause the framework we’ve built so far to break down? While
    the answer is illuminating, the full solution involves concepts that go deep into
    implementation details and are beyond the scope of this book.^([2](ch06.html#idm45732615335848))
    To begin to unpack this, let’s reveal a key limitation of the framework we’ve
    been using so far.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'The Key Limitation: Handling Branching'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It turns out that our framework couldn’t train models with computational graphs
    like those depicted in [Figure 6-1](#fig_06_01).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of length one](assets/dlfs_0601.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-1\. The computational graph that causes our Operation framework to
    fail: the same quantity is repeated multiple times during the forward pass, meaning
    that we can’t simply send gradients backward in sequence during the backward pass
    as before'
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'What’s wrong with this? Translating the forward pass into code seems fine (note
    that we have written `Add` and `Multiply` operations here for illustration purposes
    only):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这有什么问题吗？将前向传递转换为代码似乎没问题（请注意，我们这里仅出于说明目的编写了“Add”和“Multiply”操作）：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The trouble begins when we start the backward pass. Let’s say we want to use
    our usual chain rule logic to calculate the derivative of `L` with respect to
    `w1`. Previously, we would simply call `backward` on each operation in reverse
    order. Here, because of the *reuse of `b2` during the forward pass*, that approach
    doesn’t work. If we began by calling `backward` on `mult3`, for example, we would
    have gradients for each of its inputs, `c1` and `b2`. However, if we then called
    `backward` on `add2`, we couldn’t just feed in the gradient for `c1`: we would
    have to also feed in the gradient for `b2` somehow, since this also affects the
    loss `L`. So, to properly perform the backward pass for this graph, we couldn’t
    just move through the operations in exactly the reverse sequence; we’d have to
    manually write something like the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 问题开始于我们开始后向传递时。假设我们想要使用我们通常的链式法则逻辑来计算`L`相对于`w1`的导数。以前，我们只需按相反顺序在每个操作上调用“backward”。在这里，由于在前向传递中*重复使用`b2`*，这种方法不起作用。例如，如果我们从`mult3`开始调用“backward”，我们将得到每个输入`c1`和`b2`的梯度。然而，如果我们接着在`add2`上调用“backward”，我们不能只传入`c1`的梯度：我们还必须以某种方式传入`b2`的梯度，因为这也会影响损失`L`。因此，为了正确执行此图的后向传递，我们不能只按照完全相反的顺序移动操作；我们必须手动编写类似以下内容的内容：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At this point we might as well skip using `Operation`s entirely; we can simply
    save all of the quantities we computed on the forward pass and reuse them on the
    backward pass, as we were doing in [Chapter 2](ch02.html#fundamentals)! We could
    *always* code arbitrarily complicated neural networks by manually defining the
    individual computations to be done on the forward and backward passes of the network,
    just as we wrote out the 17 individual operations involved in the backward pass
    of a two-layer neural network in [Chapter 2](ch02.html#fundamentals) (indeed,
    we’ll do something like this later in this chapter inside “RNN cells”). What we
    were trying to do with `Operation`s was to build a flexible framework that let
    us describe a neural network in high-level terms and have all of the low-level
    computations “just work.” While this framework illustrated many key concepts about
    neural networks, we now see its limitations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可能完全可以跳过使用“Operation”；我们可以简单地保存在前向传递中计算的所有量，并在后向传递中重复使用它们，就像我们在[第2章](ch02.html#fundamentals)中所做的那样！我们可以通过手动定义网络前向和后向传递中要执行的各个计算来始终编写任意复杂的神经网络，就像我们在[第2章](ch02.html#fundamentals)中写出了两层神经网络后向传递中涉及的17个单独操作一样（事实上，我们稍后在本章中的“RNN单元”中会做类似的事情）。我们尝试使用“Operation”构建一个灵活的框架，让我们以高层次的术语描述神经网络，并让所有低级别的计算“自动工作”。虽然这个框架展示了许多关于神经网络的关键概念，但现在我们看到了它的局限性。
- en: 'There is an elegant solution to this problem: automatic differentiation, a
    completely different way of implementing neural networks.^([3](ch06.html#idm45732614959096))
    We’ll cover just enough of this concept here to give you an idea of how it works
    but won’t go further than that building a full-featured automatic differentiation
    framework would take several chapters just on its own. Furthermore, we’ll see
    how to *use* a high-performance automatic differentiation framework in the next
    chapter when we cover PyTorch. Still, automatic differentiation is an important
    enough concept to understand from first principles that before we get into RNNs,
    we’ll design a basic framework for it and show how it solves the problem with
    reusing objects during the forward pass described in the preceding example.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个优雅的解决方案：自动微分，这是一种完全不同的实现神经网络的方式。我们将在这里涵盖这个概念的足够部分，以便让您了解它的工作原理，但不会进一步构建一个完整功能的自动微分框架将需要几章的篇幅。此外，当我们涵盖PyTorch时，我们将看到如何*使用*一个高性能的自动微分框架。尽管如此，自动微分是一个重要的概念，需要从第一原则理解，在我们深入研究RNN之前，我们将为其设计一个基本框架，并展示它如何解决在前面示例中描述的前向传递中重复使用对象的问题。
- en: Automatic Differentiation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动微分
- en: 'As we’ve seen, there are some neural network architectures for which the `Operation`
    framework we’ve been using so far can’t easily compute the gradients of the output
    with respect to the inputs, as we have to do to be able to train our models. Automatic
    differentiation allows us to compute these gradients via a completely different
    route: rather than the `Operation`s being the atomic units that make up the network,
    we define a class that wraps around the data itself and allows the data to keep
    track of the operations performed on it, so that the data can continually accumulate
    gradients as it is involved in different operations. To understand better how
    this “gradient accumulation” would work, let’s start to code it up.^([4](ch06.html#idm45732615077368))'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，有一些神经网络架构，对于我们迄今使用的“Operation”框架来说，很难轻松地计算输出相对于输入的梯度，而我们必须这样做才能训练我们的模型。自动微分允许我们通过完全不同的路径计算这些梯度：而不是“Operation”是构成网络的原子单位，我们定义一个包装在数据周围的类，允许数据跟踪在其上执行的操作，以便数据可以在参与不同操作时不断累积梯度。为了更好地理解这种“梯度累积”是如何工作的，让我们开始编码吧。
- en: Coding Up Gradient Accumulation
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写梯度累积
- en: 'To automatically keep track of gradients, we have to overwrite the Python methods
    that perform basic operations on our data. In Python, using operators such as
    `+` or `–` actually calls underlying hidden methods such as `__add__` and `__sub__`.
    For example, here’s how that works with `+`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动跟踪梯度，我们必须重写执行数据基本操作的Python方法。在Python中，使用诸如`+`或`-`之类的运算符实际上调用诸如`__add__`和`__sub__`之类的底层隐藏方法。例如，这是`+`的工作原理：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can take advantage of this to write a class that wraps around a typical
    Python “number” (`float` or `int`) and overwrites the `add` and `mul` methods:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这一点编写一个类，该类包装了典型的Python“数字”（`float`或`int`）并覆盖了`add`和`mul`方法：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There’s a lot going on here, so let’s unpack this `NumberWithGrad` class and
    see how it works. Recall that the goal of such a class is to be able to write
    simple operations and have the gradients be computed automatically; for example,
    suppose we write:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情要做，让我们解开这个`NumberWithGrad`类并看看它是如何工作的。请记住，这样一个类的目标是能够编写简单的操作并自动计算梯度；例如，假设我们写：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'At this point, how much will increasing `a` by *ϵ* increase the value of `c`?
    It should be pretty obvious that it will increase `c` by <math><mrow><mn>4</mn>
    <mo>×</mo> <mi>ϵ</mi></mrow></math> . And indeed, using the preceding class, if
    we first write:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，通过增加*ϵ*，`a`将增加多少会增加`c`的值？很明显，它将增加`c`<math><mrow><mn>4</mn> <mo>×</mo>
    <mi>ϵ</mi></mrow></math>。确实，使用前面的类，如果我们首先写：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'then, without writing a `for` loop to iterate through the `Operation`s or anything,
    we can write:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，不需要编写`for`循环来迭代`Operation`，我们可以写：
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'How does this work? The fundamental insight incorporated into the previous
    class is that every time the `+` or `*` operations are performed on a `NumberWithGrad`,
    a new `NumberWithGrad` is created, with the first `NumberWithGrad` as a dependency.
    Then, when `backward` is called on a `NumberWithGrad`, like it is called on `c`
    previously, all the gradients for all of the `NumberWithGrad`s used to create
    `c` are automatically calculated. So indeed, not only was the gradient for `a`
    calculated, but so was the gradient for `b`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的？前一个类中融入的基本见解是，每当在`NumberWithGrad`上执行`+`或`*`操作时，都会创建一个新的`NumberWithGrad`，第一个`NumberWithGrad`作为依赖项。然后，当在`NumberWithGrad`上调用`backward`时，就像之前在`c`上调用的那样，用于创建`c`的所有`NumberWithGrad`的所有梯度都会自动计算。因此，确实，不仅计算了`a`的梯度，还计算了`b`的梯度：
- en: '[PRE9]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The real beauty of this framework, however, is that it allows the `NumberWithGrad`s
    to *accumulate* gradients and thus be reused multiple times during a series of
    computations, and we still end up with the correct gradient. We’ll illustrate
    this with the same series of operations that stumped us before, using a `NumberWithGrad`
    multiple times in a series of computations, and then unpack how it works in detail.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个框架的真正美妙之处在于它允许`NumberWithGrad`*累积*梯度，从而在一系列计算中多次重复使用，并且我们最终得到正确的梯度。我们将用相同的一系列操作来说明这一点，这些操作在之前让我们困惑，使用`NumberWithGrad`多次进行一系列计算，然后详细解释它是如何工作的。
- en: Automatic differentiation illustration
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动微分示例
- en: 'Here’s the series of computations in which `a` is reused multiple times:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一系列计算，其中`a`被多次重复使用：
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can work out that if we do these operations, *d* = 75, but as we know, the
    real question is: how much will increasing the value of `a` increase the value
    of `d`? We can first work out the answer to this question mathematically. We have:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算出，如果我们进行这些操作，*d* = 75，但正如我们所知道的，真正的问题是：增加`a`的值将如何增加`d`的值？我们可以首先通过数学方法解决这个问题。我们有：
- en: <math display="block"><mrow><mi>d</mi> <mo>=</mo> <mrow><mo>(</mo> <mn>4</mn>
    <mi>a</mi> <mo>+</mo> <mn>3</mn> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mn>2</mn> <mo>)</mo></mrow> <mo>=</mo> <mn>4</mn> <msup><mi>a</mi>
    <mn>2</mn></msup> <mo>+</mo> <mn>11</mn> <mi>a</mi> <mo>+</mo> <mn>6</mn></mrow></math>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>d</mi> <mo>=</mo> <mrow><mo>(</mo> <mn>4</mn>
    <mi>a</mi> <mo>+</mo> <mn>3</mn> <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo>
    <mi>a</mi> <mo>+</mo> <mn>2</mn> <mo>)</mo></mrow> <mo>=</mo> <mn>4</mn> <msup><mi>a</mi>
    <mn>2</mn></msup> <mo>+</mo> <mn>11</mn> <mi>a</mi> <mo>+</mo> <mn>6</mn></mrow></math>
- en: 'So, using the power rule from calculus:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用微积分中的幂规则：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>d</mi></mrow> <mrow><mi>∂</mi><mi>a</mi></mrow></mfrac>
    <mo>=</mo> <mn>8</mn> <mi>a</mi> <mo>+</mo> <mn>11</mn></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>d</mi></mrow> <mrow><mi>∂</mi><mi>a</mi></mrow></mfrac>
    <mo>=</mo> <mn>8</mn> <mi>a</mi> <mo>+</mo> <mn>11</mn></mrow></math>
- en: 'For *a* = 3, therefore, the value of this derivative should be <math><mrow><mn>8</mn>
    <mo>×</mo> <mn>3</mn> <mo>+</mo> <mn>11</mn> <mo>=</mo> <mn>35</mn></mrow></math>
    . Confirming this numerically:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*a* = 3，因此，这个导数的值应该是<math><mrow><mn>8</mn> <mo>×</mo> <mn>3</mn> <mo>+</mo>
    <mn>11</mn> <mo>=</mo> <mn>35</mn></mrow></math>。通过数值确认：
- en: '[PRE12]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, observe that we get the same result when we compute the gradient with
    our automatic differentiation framework:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，观察到当我们使用自动微分框架计算梯度时，我们得到相同的结果：
- en: '[PRE14]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Explaining what happened
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释发生了什么
- en: As we can see, the goal with automatic differentiation is to make the *data
    objects themselves*—numbers, `ndarray`s, `Tensors`, and so on—the fundamental
    units of analysis, rather than the `Operation`s as before.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，自动微分的目标是使*数据对象本身*——数字、`ndarray`、`张量`等——成为分析的基本单位，而不是以前的`Operation`。
- en: 'All automatic differentiation techniques have the following in common:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所有自动微分技术都有以下共同点：
- en: Each technique includes a class that wraps around the actual data being computed.
    Here, we wrap `NumberWithGrad` around `float`s and `int`s; in PyTorch, for example,
    the analogous class is called `Tensor`.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种技术都包括一个包装实际计算数据的类。在这里，我们将`NumberWithGrad`包装在`float`和`int`周围；例如，在PyTorch中，类似的类称为`Tensor`。
- en: Common operations such as adding, multiplying, and matrix multiplication are
    redefined so that they always return members of this class; in the preceding case,
    we ensure the addition of *either* a `NumberWithGrad` and a `NumberWithGrad` *or*
    a `NumberWithGrad` and a `float` or an `int`.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新定义常见操作，如加法、乘法和矩阵乘法，以便它们始终返回该类的成员；在前面的情况下，我们确保*要么*是`NumberWithGrad`和`NumberWithGrad`的加法*要么*是`NumberWithGrad`和`float`或`int`的加法。
- en: The `NumberWithGrad` class must contain information on how to compute gradients,
    given what happens on the forward pass. Previously, we did this by including a
    `creation_op` argument in the class that simply recorded how the `NumberWithGrad`
    was created.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NumberWithGrad`类必须包含有关如何计算梯度的信息，考虑到前向传播时发生了什么。以前，我们通过在类中包含一个`creation_op`参数来实现这一点，该参数简单地记录了`NumberWithGrad`是如何创建的。'
- en: On the backward pass, gradients are passed backward using the underlying data
    type, not the wrapper. Here, this means gradients are of type `float` and `int`,
    not `NumberWithGrad`.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在反向传播过程中，梯度是使用底层数据类型而不是包装器向后传递的。这意味着梯度的类型是`float`和`int`，而不是`NumberWithGrad`。
- en: 'As mentioned at the start of this section, automatic differentiation allows
    us to reuse quantities computed during the forward pass—in the preceding example,
    we use `a` twice with no problem. The key for allowing this is these lines:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如在本节开头提到的，自动微分允许我们在前向传播期间重复使用计算的量——在前面的示例中，我们两次使用`a`而没有问题。允许这样做的关键是这些行：
- en: '[PRE16]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: These lines state that upon receiving a new gradient, `backward_grad`, a `NumberWithGrad`
    should either initialize the gradient of `NumberWithGrad` to be this value, or
    simply add the value to the `NumberWithGrad`’s existing gradient. This is what
    allows the `NumberWithGrad`s to accumulate gradients as the relevant objects are
    reused in the model.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些行表明，在接收到新的梯度`backward_grad`时，`NumberWithGrad`应该将`NumberWithGrad`的梯度初始化为这个值，或者简单地将该值添加到`NumberWithGrad`的现有梯度中。这就是允许`NumberWithGrad`在模型中重复使用相关对象时累积梯度的原因。
- en: That’s all we’ll cover of automatic differentiation. Let’s now turn to the model
    structure that motivated this digression, since it requires certain quantities
    to be reused during the forward pass to make predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们将涵盖的自动微分的全部内容。现在让我们转向激发这一偏离的模型结构，因为在前向传播过程中需要重复使用某些量来进行预测。
- en: Motivation for Recurrent Neural Networks
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络的动机
- en: 'As we discussed at the beginning of this chapter, recurrent neural networks
    are designed to handle data that appears in sequences: instead of each observation
    being a vector with, say, `n` features, it is now a two-dimensional array of dimension
    `n` features by `t` time steps. This is depicted in [Figure 6-2](#fig_06_02).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开头讨论的那样，循环神经网络旨在处理以序列形式出现的数据：每个观察结果不再是具有`n`个特征的向量，而是一个二维数组，维度为`n`个特征乘以`t`个时间步。这在[图6-2](#fig_06_02)中有所描述。
- en: '![Output of length one](assets/dlfs_0602.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![长度为一的输出](assets/dlfs_0602.png)'
- en: 'Figure 6-2\. Sequential data: at each of the t time steps we have n features'
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 顺序数据：在每个时间步中我们有n个特征
- en: In the following few sections, we’ll explain how RNNs accommodate data of this
    form, but first let’s try to understand why we need them. What would be the limitations
    of simply using a normal feed-forward neural network to deal with this kind of
    data? One way would be to represent each time step as an independent set of features.
    For example, one observation could have the features from time `t = 1` with the
    value of the target from time `t = 2`, the next observation could have the features
    from time `t = 2` with the value of the target from time `t = 3`, and so on. If
    we wanted to use data from *multiple* time steps to make each prediction rather
    than data from just one time step, we could use the features from `t = 1` and
    `t = 2` to predict the target at `t = 3`, the features from `t = 2` and `t = 3`
    to predict the target at `t = 4`, and so on.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将解释RNN如何适应这种形式的数据，但首先让我们尝试理解为什么我们需要它们。仅仅使用普通的前馈神经网络来处理这种类型的数据会有什么限制？一种方法是将每个时间步表示为一个独立的特征集。例如，一个观察结果可以具有来自时间`t
    = 1`的特征和来自时间`t = 2`的目标值，下一个观察结果可以具有来自时间`t = 2`的特征和来自时间`t = 3`的目标值，依此类推。如果我们想要使用*多个*时间步的数据来进行每次预测，而不仅仅是来自一个时间步的数据，我们可以使用`t
    = 1`和`t = 2`的特征来预测`t = 3`的目标，使用`t = 2`和`t = 3`的特征来预测`t = 4`的目标，依此类推。
- en: 'However, treating each time step as independent ignores the fact that the data
    is ordered sequentially. How would we ideally want to use the sequential nature
    of the data to make better predictions? The solution would look something like
    this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将每个时间步视为独立的方式忽略了数据是按顺序排列的事实。我们如何理想地利用数据的顺序性来做出更好的预测？解决方案看起来会像这样：
- en: Use features from time step `t = 1` to make predictions for the corresponding
    target at `t = 1`.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用时间步`t = 1`的特征来预测对应时间`t = 1`的目标。
- en: Use features from time step `t = 2` *as well as the information from `t = 1`,
    including the value of the target at `t = 1`*, to make predictions for `t = 2`.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用时间步`t = 2`的特征以及从`t = 1`包括`t = 1`的目标值的信息来预测`t = 2`。
- en: Use features from time step `t = 3` *as well as the accumulated information
    from `t = 1` and `t = 2`* to make predictions at `t = 3`.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用时间步`t = 3`的特征以及从`t = 1`和`t = 2`累积的信息来预测`t = 3`时的结果。
- en: And so on, at each step using the information from all prior time steps to make
    a prediction.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，每一步都使用所有先前时间步的信息来进行预测。
- en: To do this, it seems that we’d want to pass our data through the neural network
    one sequence element at a time, with the data from the first time step being passed
    through first, then the data from the next time step, and so on. In addition,
    we’ll want our neural network to “accumulate information” about what it has seen
    before as the new sequence elements are passed through. We’ll spend the rest of
    this chapter discussing in detail precisely how recurrent neural networks do this.
    As we’ll see, while there are several variants of recurrent neural networks, they
    all share a common underlying structure in the way they process data sequentially;
    we’ll spend most of our time discussing this structure and at the end discuss
    the ways in which the variants differ.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，似乎我们希望逐个序列元素地通过神经网络传递我们的数据，首先传递第一个时间步的数据，然后传递下一个时间步的数据，依此类推。此外，我们希望我们的神经网络在通过新的序列元素时“累积信息”关于它之前所看到的内容。我们将在本章的其余部分详细讨论循环神经网络如何做到这一点。正如我们将看到的，虽然有几种循环神经网络的变体，但它们在处理数据时都共享一个共同的基本结构；我们将大部分时间讨论这种结构，并在最后讨论这些变体的不同之处。
- en: Introduction to Recurrent Neural Networks
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络简介
- en: Let’s start our discussion of RNNs by reviewing, at a high level, how data is
    passed through a “feed-forward” neural network. In this type of network, data
    is passed forward through a series of *layers*. For a single observation, the
    output of each layer is the neural network’s “representation” of that observation
    at that layer. After the first layer, that representation consists of features
    that are combinations of the original features; after the next layer, it consists
    of combinations of these representations, or “features of features” of the original
    features, and so on for subsequent layers in the network. After each forward pass,
    then, the network contains in the outputs of each of its layers many representations
    of the original observation. This is encapsulated in [Figure 6-3](#fig_06_03).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过高层次的方式开始讨论RNN，看看数据是如何通过“前馈”神经网络传递的。在这种类型的网络中，数据通过一系列*层*向前传递。对于单个观察结果，每一层的输出是该观察结果在该层的神经网络“表示”。在第一层之后，该表示由原始特征的组合组成；在下一层之后，它由这些表示的组合或原始特征的“特征的特征”组成，依此类推，直到网络中的后续层。因此，在每次向前传递之后，网络在每个层的输出中包含许多原始观察结果的表示。这在[图6-3](#fig_06_03)中有所体现。
- en: '![Output of length one](assets/dlfs_0603.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![长度为一的输出](assets/dlfs_0603.png)'
- en: Figure 6-3\. A regular neural network passing an observation forward and transforming
    it into different representations after each layer
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3。一个常规的神经网络将观察结果向前传递，并在每一层之后将其转换为不同的表示
- en: 'When the next set of observations is passed through the network, however, these
    representations are discarded; the key innovation of recurrent neural networks
    and all of their variants is to *pass these representations back into the network*
    along with the next set of observations. Here’s how that procedure would look:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当下一组观察结果通过网络传递时，这些表示将被丢弃；循环神经网络及其所有变体的关键创新是*将这些表示传递回网络*，以及下一组观察结果。这个过程看起来是这样的：
- en: In the first time step, `t = 1`, we would pass through the observation from
    the first time step (along with randomly initialized representations, perhaps).
    We would output a prediction for `t = 1`, along with representations at each layer.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个时间步，`t = 1`，我们将通过第一个时间步的观察结果（可能还有随机初始化的表示）进行传递。我们将输出`t = 1`的预测，以及每一层的表示。
- en: In the next time step, we would pass through the observation from the second
    time step, `t = 2`, along with the representations computed during the first time
    step (which, again, are just the outputs of the neural network’s layers), and
    combine these somehow (it is in this combining step that the variants of RNNs
    we’ll learn about differ). We would use these two pieces of information to output
    a prediction for `t = 2` as well as the *updated* representations at each layer,
    which are now a function of the inputs passed in at both `t = 1` *and* `t = 2`.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个时间步中，我们将通过第二个时间步的观察结果`t = 2`，以及在第一个时间步计算的表示（再次，这些只是神经网络层的输出），并以某种方式将它们结合起来（正是在这个结合步骤中，我们将学习到的RNN的变体有所不同）。我们将使用这两个信息来输出`t
    = 2`的预测，以及每一层的*更新*表示，这些表示现在是输入在`t = 1`和`t = 2`时传递的函数。
- en: In the third time step, we would pass through the observation from `t = 3`,
    as well as the representations that now incorporate the information from `t =
    1` and `t = 2`, and use this information to make predictions for `t = 3`, as well
    as additional updated representations at each layer, which now incorporate information
    from time steps 1–3.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三个时间步中，我们将通过来自`t = 3`的观察结果，以及现在包含来自`t = 1`和`t = 2`信息的表示，利用这些信息对`t = 3`进行预测，以及每一层的额外更新的表示，现在包含时间步1-3的信息。
- en: This process is depicted in [Figure 6-4](#fig_06_04).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在[图6-4](#fig_06_04)中描述。
- en: '![Output of length one](assets/dlfs_0604.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![长度为一的输出](assets/dlfs_0604.png)'
- en: Figure 6-4\. Recurrent neural networks pass the representations of each layer
    forward into the next time step
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4。循环神经网络将每一层的表示向前传递到下一个时间步
- en: 'We see that each layer has a representation that is “persistent,” getting updated
    over time as new observations are passed through. Indeed, this fact is why RNNs
    are not amenable to the `Operation` framework we’ve written for the prior chapters:
    the `ndarray` that represents this state for each layer is continually updated
    and reused many times in order to make one set of predictions for a single sequence
    of data using an RNN. Because we can’t use the framework from the prior chapter,
    we’ll have to reason from first principles about what classes to build to handle
    RNNs.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到每一层都有一个“持久”的表示，随着时间的推移而更新，因为新的观察结果被传递。事实上，这就是为什么RNN不适用于我们为之前章节编写的`Operation`框架的原因：每一层的表示的`ndarray`会不断更新和重复使用，以便使用RNN对一系列数据进行一次预测。因为我们无法使用之前章节的框架，我们将不得不从头开始考虑如何构建处理RNN的类。
- en: 'The First Class for RNNs: RNNLayer'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN的第一个类：RNNLayer
- en: 'Based on the description of how we want RNNs to work, we know at the very least
    that we’ll need an `RNNLayer` class that passes a sequence of data forward one
    sequence element at a time. Let’s now get into the details of how such a class
    should work. As we’ve mentioned in this chapter, RNNs will deal with data in which
    each observation is two-dimensional, with dimensions `(sequence_length, num_features)`;
    and since it is always more efficient computationally to pass data forward in
    batches, `RNNLayer` will have to take in three-dimensional `ndarray`s, of size
    `(batch_size, sequence_length, num_features)`. I explained in the prior section,
    however, that we want to feed our data through our `RNNLayer`s one sequence element
    at a time; how can we do this if our input, `data`, is `(batch_size, sequence_length,
    num_features)`? Here’s how:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们希望RNN工作的描述，我们至少知道我们需要一个`RNNLayer`类，该类将逐个序列元素向前传递数据序列。现在让我们深入了解这样一个类应该如何工作的细节。正如我们在本章中提到的，RNN将处理每个观察都是二维的数据，维度为`(sequence_length,
    num_features)`；由于在计算上总是更有效率地批量传递数据，`RNNLayer`将需要接收三维的`ndarray`，大小为`(batch_size,
    sequence_length, num_features)`。然而，我在前一节中解释过，我们希望逐个序列元素通过我们的`RNNLayer`传递数据；如果我们的输入`data`是`(batch_size,
    sequence_length, num_features)`，我们如何做到这一点呢？这样做：
- en: Select a two-dimensional array from the second axis, starting with `data[:,
    0, :]`. This `ndarray` will have shape `(batch_size, num_features)`.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第二轴选择一个二维数组，从`data[:, 0, :]`开始。这个`ndarray`的形状将是`(batch_size, num_features)`。
- en: Initialize a “hidden state” for the `RNNLayer` that will continually get updated
    with each sequence element passed in, this time of shape `(batch_size, hidden_size)`.
    This `ndarray` will represent the layer’s “accumulated information” about the
    data that has been passed in during the prior time steps.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`RNNLayer`初始化一个“隐藏状态”，该状态将随着传入的每个序列元素而不断更新，这次的形状为`(batch_size, hidden_size)`。这个`ndarray`将代表层对已经在先前时间步传入的数据的“累积信息”。
- en: 'Pass these two `ndarray`s forward through the first time step in this layer.
    We’ll end up designing `RNNLayer` to output `ndarray`s of different dimensionality
    than the inputs, just like regular `Dense` layers can, so the output will be of
    shape `(batch_size, num_outputs)`. In addition, update the neural network’s representation
    for each observation: at each time step, our `RNNLayer` should *also* output an
    `ndarray` of shape `(batch_size, hidden_size)`.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这两个`ndarray`通过该层的第一个时间步向前传递。我们将设计`RNNLayer`以输出与输入不同维度的`ndarray`，就像常规的`Dense`层一样，因此输出将是形状为`(batch_size,
    num_outputs)`。此外，更新神经网络对每个观察的表示：在每个时间步，我们的`RNNLayer`还应该输出一个形状为`(batch_size, hidden_size)`的`ndarray`。
- en: 'Select the next two-dimensional array from `data`: `data[:, 1, :]`.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`data`中选择下一个二维数组：`data[:, 1, :]`。
- en: Pass this data, as well as the values of the RNN’s representations outputted
    at the first time step, into the second time step at this layer to get another
    output of shape `(batch_size, num_outputs)`, as well as updated representations
    of shape `(batch_size, hidden_size)`.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些数据以及RNN在第一个时间步输出的表示值传递到该层的第二个时间步，以获得另一个形状为`(batch_size, num_outputs)`的输出，以及形状为`(batch_size,
    hidden_size)`的更新表示。
- en: Continue until all `sequence_length` time steps have been passed through the
    layer. Then concatenate all the results together to get an output from that layer
    of shape `(batch_size, sequence_length, num_outputs)`.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一直持续到所有`sequence_length`时间步都通过该层。然后将所有结果连接在一起，以获得该层的输出形状为`(batch_size, sequence_length,
    num_outputs)`。
- en: This gives us an idea of how our `RNNLayer`s should work—and we’ll solidify
    this understanding when we code it up—but it also hints that we’ll need another
    class to handle receiving the data and updating the layer’s hidden state at each
    time step. For this we’ll use the `RNNNode`, the next class we’ll cover.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个关于我们的`RNNLayer`应该如何工作的想法——当我们编写代码时，我们将巩固这种理解——但它也暗示我们需要另一个类来处理接收数据并更新每个时间步的层隐藏状态。为此，我们将使用`RNNNode`，这是我们将要介绍的下一个类。
- en: 'The Second Class for RNNs: RNNNode'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN的第二个类：RNNNode
- en: 'Based on the description from the prior section, an `RNNNode` should have a
    `forward` method with the following inputs and outputs:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前一节的描述，`RNNNode`应该有一个`forward`方法，具有以下输入和输出：
- en: 'Two `ndarray`s as inputs:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个`ndarray`作为输入：
- en: One for the data inputs to the network, of shape `[batch_size, num_features]`
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于网络的数据输入，形状为`[batch_size, num_features]`
- en: One for the representations of the observations at that time step, of shape
    `[batch_size, hidden_size]`
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于该时间步观察的表示的形状为`[batch_size, hidden_size]`
- en: 'Two `ndarray`s as outputs:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个`ndarray`作为输出：
- en: One for the outputs of the network at that time step, or shape `[batch_size,
    num_outputs]`
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于网络在该时间步的输出的数组，形状为`[batch_size, num_outputs]`
- en: 'One for the *updated* representations of the observations at that time step,
    of shape: `[batch_size, hidden_size]`'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于该时间步观察的*更新*表示，形状为：`[batch_size, hidden_size]`
- en: Next, we’ll show how the two classes, `RNNNode` and `RNNLayer`, fit together.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展示`RNNNode`和`RNNLayer`这两个类如何配合。
- en: Putting These Two Classes Together
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将这两个类结合起来
- en: 'The `RNNLayer` class will wrap around a list of `RNNNode`s and will (at least)
    contain a `forward` method that has the following inputs and outputs:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`RNNLayer`类将包装在一个`RNNNode`列表周围，并且（至少）包含一个具有以下输入和输出的`forward`方法：'
- en: 'Input: a batch of sequences of observations of shape `[batch_size, sequence_length,
    num_features]`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：形状为`[batch_size, sequence_length, num_features]`的一批观察序列
- en: 'Output: the neural network output of those sequences of shape `[batch_size,
    sequence_length, num_outputs]`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：这些序列的神经网络输出的形状为`[batch_size, sequence_length, num_outputs]`
- en: '[Figure 6-5](#fig_06_05) shows the order that data would move forward through
    an RNN with two `RNNLayer`s with five `RNNNode`s each. At each time step, inputs
    initially of dimension `feature_size` are passed successively forward through
    the first `RNNNode` in each `RNNLayer`, with the network ultimately outputting
    a prediction at that time step of dimension `output_size`. In addition, each `RNNNode`
    passes a “hidden state” forward to the next `RNNNode` within each layer. Once
    data from each of the five time steps has been passed forward through all the
    layers, we will have a final set of predictions of shape `(5, output_size)`, where
    `output_size` should be the same dimension as the targets. These predictions would
    then be compared to the target, and the loss gradient would be computed, kicking
    off the backward pass. [Figure 6-5](#fig_06_05) summarizes this, showing the order
    in which the data would flow through the 5 × 2 `RNNNode`s from first (1) to last
    (10).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-5](#fig_06_05)展示了数据如何通过具有每个五个`RNNNode`的两个`RNNLayer`的RNN向前传播的顺序。在每个时间步，初始维度为`feature_size`的输入依次通过每个`RNNLayer`中的第一个`RNNNode`向前传递，网络最终在该时间步输出维度为`output_size`的预测。此外，每个`RNNNode`将“隐藏状态”向前传递到每层内的下一个`RNNNode`。一旦每个五个时间步的数据都通过所有层向前传递，我们将得到一个形状为`(5,
    output_size)`的最终预测集，其中`output_size`应该与目标的维度相同。然后，这些预测将与目标进行比较，并计算损失梯度，启动反向传播。[图6-5](#fig_06_05)总结了这一点，展示了数据如何从第一个（1）到最后一个（10）依次通过5×2个`RNNNode`的顺序流动。'
- en: '![Output of length one](assets/dlfs_0605.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![长度为一的输出](assets/dlfs_0605.png)'
- en: Figure 6-5\. The order in which data would flow through an RNN with two layers
    that was designed to process sequences of length 5
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。设计用于处理长度为5的序列的具有两层的RNN中数据将如何流动的顺序
- en: 'Alternatively, data could flow through the RNN in the order shown in [Figure 6-6](#fig_06_06).
    Whatever the order, the following must occur:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，数据可以按照[图6-6](#fig_06_06)中显示的顺序在RNN中流动。无论顺序如何，以下步骤必须发生：
- en: Each layer needs to process its data at a given time step before the next layer—for
    example, in [Figure 6-5](#fig_06_05), 2 can’t happen before 1, and 4 can’t happen
    before 3.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个层都需要在给定时间步处理其数据，然后才能处理下一层 - 例如，在[图6-5](#fig_06_05)中，2不能在1之前发生，4不能在3之前发生。
- en: Similarly, each layer has to process all of its time steps in order—in [Figure 6-5](#fig_06_05),
    for example, 4 can’t happen before 2, and 3 can’t happen before 1.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，每个层都必须按顺序处理其所有时间步 - 例如，在[图6-5](#fig_06_05)中，4不能在2之前发生，3不能在1之前发生。
- en: The last layer has to output dimension `feature_size` for each observation.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一层必须为每个观测输出维度`feature_size`。
- en: '![Output of length one](assets/dlfs_0606.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![长度为一的输出](assets/dlfs_0606.png)'
- en: Figure 6-6\. Another order in which data could flow through the same RNN during
    its forward pass
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6。数据在同一RNN的前向传播过程中可能流动的另一种顺序
- en: This all covers how the forward pass through an RNN would work. What about the
    backward pass?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了RNN前向传播的工作原理。那么反向传播呢？
- en: The Backward Pass
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'Backpropagation through recurrent neural networks is often described as a separate
    algorithm called “backpropagation through time.” While this does indeed describe
    what happens during backpropagation, it makes things sound a lot more complicated
    than they are. Keeping in mind the explanation of how data flows forward through
    an RNN, we can describe what happens on the backward pass this way: we pass data
    backward through the RNN by passing gradients backward through the network in
    reverse of the order that we passed inputs forward on the forward pass—which,
    indeed, is the same thing we do in regular feed-forward networks.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过递归神经网络的反向传播通常被描述为一个称为“递归神经网络的反向传播”算法。虽然这确实描述了反向传播过程中发生的事情，但这让事情听起来比实际复杂得多。牢记数据如何通过RNN向前流动的解释，我们可以这样描述反向传播过程：我们通过将梯度反向通过网络传递，以与在前向传播过程中向前传递输入的顺序相反的顺序将数据向后传递，这与我们在常规前馈网络中所做的事情是一样的。
- en: 'Looking at the diagrams in Figures [6-5](#fig_06_05) and [6-6](#fig_06_06),
    on the forward pass:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图[6-5](#fig_06_05)和[6-6](#fig_06_06)中的图表，在前向传播过程中：
- en: We start with a batch of observations, each of shape `(feature_size, sequence_length)`.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从形状为`(feature_size, sequence_length)`的一批观测开始。
- en: These inputs are broken up into the individual `sequence_length` elements and
    passed into the network one at a time.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些输入被分解为单个`sequence_length`元素，并逐个传入网络。
- en: Each element gets passed through all the layers, ultimately getting transformed
    into an output of size `output_size`.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个元素都通过所有层，最终被转换为大小为`output_size`的输出。
- en: At the same time, the layer passes the hidden state forward into the layer’s
    computation at the next time step.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，层将隐藏状态向前传递到下一个时间步的层的计算中。
- en: This continues for all `sequence_length` time steps, resulting in a total output
    of size `(output_size, sequence_length)`.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将持续进行所有`sequence_length`时间步，最终产生大小为`(output_size, sequence_length)`的总输出。
- en: 'Backpropagation simply works the same way, but in reverse:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播只是以相反的方式工作：
- en: We start with a *gradient* of shape `[output_size, sequence_length]`, representing
    how much each element of the output (also of size `[output_size, sequence_length]`)
    ultimately impacts the loss computed for that batch of observations.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从形状为`[output_size, sequence_length]`的*梯度*开始，表示输出的每个元素（也是形状为`[output_size,
    sequence_length]`的）最终对该批观测的损失产生了多大影响。
- en: These gradients are broken up into the individual `sequence_length` elements
    and passed *backward* through the layers *in reverse order*.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些梯度被分解为单个`sequence_length`元素，并*以相反顺序*通过层*向后*传递。
- en: The gradient for an individual element is passed backward through all the layers.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单个元素的梯度通过所有层向后传递。
- en: At the same, the layers pass the *gradient of the loss with respect to the hidden
    state at that time step* backward into the layers’ computations at the prior time
    steps.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，各层将“与该时间步的隐藏状态相关的损失的梯度”向后传递到先前时间步的层的计算中。
- en: This continues for all `sequence_length` time steps, until the gradients have
    been passed backward to every layer in the network, thus allowing us to compute
    the gradient of the loss with respect to each of the weights, just as we do in
    the case of regular feed-forward networks.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将持续进行所有`sequence_length`时间步，直到梯度已经向网络中的每一层传递，从而使我们能够计算出损失相对于每个权重的梯度，就像在常规前馈网络的情况下一样。
- en: This parallelism between the backward and forward pass is highlighted in [Figure 6-7](#fig_06_07),
    which shows how data flows through an RNN during the backward pass. You’ll notice,
    of course, that it is the same as [Figure 6-5](#fig_06_05) but with the arrows
    reversed and the numbers changed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这种前向传递和后向传递之间的并行性在[图6-7](#fig_06_07)中得到了突出，该图显示了数据在RNN在后向传递过程中的流动方式。当然，您会注意到，它与[图6-5](#fig_06_05)相同，但箭头方向相反，数字也有所改变。
- en: '![Output of length one](assets/dlfs_0607.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![长度为一的输出](assets/dlfs_0607.png)'
- en: Figure 6-7\. In the backward pass, RNNs pass data in the opposite direction
    of the way the data is passed during the forward pass
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7。在后向传递中，RNNs将数据传递的方向与前向传递相反
- en: 'This highlights that, at a high level, the forward and backward passes for
    an `RNNLayer` are very similar to those of a layer in a normal neural network:
    they both receive `ndarray`s of a certain shape as input, output `ndarray`s of
    another shape, and on the backward pass receive an output gradient of the same
    shape as their output and produce an input gradient of the same shape as their
    input. There is a key difference in the way the weight gradients are handled in
    `RNNLayer`s versus other layers, however, so we’ll briefly cover that before we
    shift to coding this all up.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了，在高层次上，`RNNLayer`的前向和后向传递与普通神经网络中的层非常相似：它们都接收特定形状的`ndarray`作为输入，输出另一种形状的`ndarray`，在后向传递中接收与其输出相同形状的输出梯度，并产生与其输入相同形状的输入梯度。然而，在`RNNLayer`中处理权重梯度的方式与其他层有关键差异，因此在我们开始编码之前，我们将简要介绍一下这一点。
- en: Accumulating gradients for the weights in an RNN
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在RNN中累积权重的梯度
- en: 'In recurrent neural networks, just as in regular neural networks, each layer
    will have *one set of weights*. That means that the same set of weights will affect
    the layer’s output at all `sequence_length` time steps; during backpropagation,
    therefore, the same set of weights will receive `sequence_length` different gradients.
    For example, in the circle labeled “1” in the backpropagation shown in [Figure 6-7](#fig_06_07),
    the second layer will receive a gradient for the last time step, while in the
    circle labeled “3,” the layer will receive a gradient for the second-to-last time
    step; both of these will be driven by the same set of weights. Thus, during backpropagation,
    we’ll have to *accumulate gradients* for the weights over a series of time steps,
    which means that however we choose to store the weights, we’ll have to update
    their gradients using something like the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环神经网络中，就像在常规神经网络中一样，每一层都会有*一组权重*。这意味着相同的权重集将影响所有`sequence_length`时间步的层输出；因此，在反向传播过程中，相同的权重集将接收`sequence_length`不同的梯度。例如，在[图6-7](#fig_06_07)中显示的反向传播中标记为“1”的圆圈中，第二层将接收最后一个时间步的梯度，而在标记为“3”的圆圈中，该层将接收倒数第二个时间步的梯度；这两者都将由相同的权重驱动。因此，在反向传播过程中，我们将不得不*累积权重的梯度*，这意味着无论我们选择如何存储权重，我们都将不得不使用类似以下的方法更新它们的梯度：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is different from the `Dense` and `Conv2D` layers, in which we just stored
    the parameters in a `param_grad` argument.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这与`Dense`和`Conv2D`层不同，在这些层中，我们只是将参数存储在`param_grad`参数中。
- en: We’ve laid out how RNNs work and the classes we want to build to implement them;
    now let’s start figuring out the details.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经阐明了RNN的工作原理以及我们想要构建的类来实现它们；现在让我们开始研究细节。
- en: 'RNNs: The Code'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN：代码
- en: 'Let’s start with several of the ways the implementation of RNNs will be similar
    to that of the other neural networks we’ve covered in this book:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从几种实现RNN与我们在本书中介绍的其他神经网络类似的方式开始：
- en: 'An RNN still passes data forward through a series of layers, which send outputs
    forward on the forward pass and gradients backward on the backward pass. Thus,
    for example, whatever the equivalent of our `NeuralNetwork` class ends up being
    will still have a list of `RNNLayer`s as a `layers` attribute, and the forward
    pass will consist of code like:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN仍然通过一系列层向前传递数据，这些层在前向传递时将输出向前传递，在后向传递时将梯度向后传递。因此，例如，无论我们的`NeuralNetwork`类的等价物最终是什么，它仍将具有`layers`属性作为`RNNLayer`的列表，并且前向传递将包含如下代码：
- en: '[PRE18]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `Loss` for RNNs is the same as before: an `ndarray` `output` is produced
    by the last `Layer` and compared with `y_batch`, a single value is computed, and
    a gradient of this value with respect to the input to the `Loss` is returned with
    the same shape as `output`. We’ll have to modify the softmax function to work
    appropriately with `ndarray`s of shape `[batch_size, sequence_length, feature_size]`,
    but we can handle that.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN的`Loss`与以前相同：最后一个`Layer`生成一个`ndarray` `output`，与`y_batch`进行比较，计算出一个单一值，并返回相对于`Loss`输入的该值的梯度，形状与`output`相同。我们将不得不修改softmax函数，以便与形状为`[batch_size,
    sequence_length, feature_size]`的`ndarray`适当地配合，但我们可以处理这个问题。
- en: 'The `Trainer` is mostly the same: we cycle through our training data, selecting
    batches of input data and batches of output data, and continually feed them through
    our model, producing loss values that tell us whether our model is learning and
    updating the weights after each batch has been fed through. Speaking of which…'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Trainer`大部分都是相同的：我们循环遍历我们的训练数据，选择输入数据和输出数据的批次，并不断将它们通过我们的模型，产生损失值，告诉我们我们的模型是否在学习，并在每个批次通过后更新权重。说到这里...'
- en: Our `Optimizer` remains the same as well. As we’ll see, we’ll have to update
    how we extract the `params` and `param_grads` at each time step, but the “update
    rules” (which we captured in the `_update_rule` function in our class) remain
    the same.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的`Optimizer`保持不变。正如我们将看到的，我们将不得不更新如何在每个时间步提取`params`和`param_grads`，但“更新规则”（我们在类中的`_update_rule`函数中捕获）保持不变。
- en: The `Layer`s themselves are where things get interesting.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`Layer`本身是有趣的地方。'
- en: The RNNLayer Class
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNNLayer类
- en: 'Previously, we gave `Layer`s a set of `Operation`s that passed data forward
    and sent gradients backward. `RNNLayer`s will be completely different; they now
    must maintain a “hidden state” that continually gets updated as new data gets
    fed in and gets “combined” with the data somehow at each time step. How *exactly*
    should this work? We can use Figures [6-5](#fig_06_05) and [6-6](#fig_06_06) as
    guides here: they suggest that each `RNNLayer` should have a `List` of `RNNNode`s
    as an attribute, and then each sequence element from the layer’s `input` should
    get passed through each `RNNNode`, one element at a time. Each `RNNNode` will
    take in this sequence element, as well as the “hidden state” for that layer, and
    produce an output for the layer at that time step as well as updating the layer’s
    hidden state.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们给`Layer`提供了一组`Operation`，用于向前传递数据并向后发送梯度。`RNNLayer`将完全不同；它们现在必须保持一个“隐藏状态”，随着新数据被馈送并在每个时间步骤以某种方式与数据“组合”，该状态将不断更新。这应该如何工作？我们可以使用图[6-5](#fig_06_05)和[6-6](#fig_06_06)作为指导：它们建议每个`RNNLayer`应该有一个`RNNNode`列表作为属性，然后该层的`input`中的每个序列元素应该逐个通过每个`RNNNode`传递。每个`RNNNode`将接收此序列元素，以及该层的“隐藏状态”，并在该时间步骤为该层产生一个输出，同时更新该层的隐藏状态。
- en: 'To clarify all this, let’s dive in and start coding it up: we’ll cover, in
    order, how an `RNNLayer` should be initialized, how it should send data forward
    during the forward pass, and how it should send data backward during the backward
    pass.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清所有这些，让我们深入研究并开始编码：我们将按顺序介绍`RNNLayer`的初始化方式，它在前向传递期间如何发送数据，以及在反向传递期间如何发送数据。
- en: Initialization
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化
- en: 'Each `RNNLayer` will start with:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`RNNLayer`将以以下方式开始：
- en: An `int` `hidden_size`
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`int` `hidden_size`
- en: An `int` `output_size`
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`int` `output_size`
- en: An `ndarray` `start_H` of shape `(1, hidden_size)`, representing the layer’s
    hidden state
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个形状为`(1, hidden_size)`的`ndarray` `start_H`，表示该层的隐藏状态
- en: In addition, just like in regular neural networks, we’ll set `self.first = True`
    when we initialize the layer; the first time we pass data into the `forward` method
    we’ll pass the `ndarray` we receive into an `_init_params` method, initialize
    the parameters, and set `self.first = False`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，就像在常规神经网络中一样，当我们初始化层时，我们将设置`self.first = True`；第一次将数据传递到`forward`方法时，我们将将接收到的`ndarray`传递到一个`_init_params`方法中，初始化参数，并设置`self.first
    = False`。
- en: With our layer initialized, we are ready to describe how to send data forward.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有了初始化的层，我们准备描述如何将数据发送到前面。
- en: The forward method
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向方法
- en: 'The bulk of the `forward` method will consist of taking in an `ndarray` `x_seq_in`
    of shape `(batch_size, sequence_length, feature_size)` and feeding it through
    the layer’s `RNNNode`s in sequence. In the following code, `self.nodes` are the
    `RNNNode`s for the layer, and `H_in` is the hidden state for the layer:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`方法的大部分将包括接收形状为`(batch_size, sequence_length, feature_size)`的`ndarray`
    `x_seq_in`，并按顺序通过该层的`RNNNode`。在以下代码中，`self.nodes`是该层的`RNNNode`，`H_in`是该层的隐藏状态：'
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'One note on the hidden state `H_in`: the hidden state for an `RNNLayer` is
    typically represented in a vector, but the operations in each `RNNNode` require
    the hidden state to be an `ndarray` of size `(batch_size, hidden_size)`. So, at
    the beginning of each forward pass, we simply “repeat” the hidden state:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 关于隐藏状态`H_in`的一点说明：`RNNLayer`的隐藏状态通常表示为一个向量，但每个`RNNNode`中的操作要求隐藏状态为大小为`(batch_size,
    hidden_size)`的`ndarray`。因此，在每次前向传递的开始时，我们简单地“重复”隐藏状态：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After the forward pass, we take the average value across the observations making
    up the batch to get the updated hidden state for that layer:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递之后，我们取得构成批次的观测值的平均值，以获得该层的更新隐藏状态：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Also, we can see from this code that an `RNNNode` will have to have a `forward`
    method that takes in two arrays of shapes:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以从这段代码中看到，`RNNNode`将必须具有一个接收两个形状数组的`forward`方法：
- en: '`(batch_size, feature_size)`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, feature_size)`'
- en: '`(batch_size, hidden_size)`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, hidden_size)`'
- en: 'and returns two arrays of shapes:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 并返回两个形状的数组：
- en: '`(batch_size, output_size)`'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, output_size)`'
- en: '`(batch_size, hidden_size)`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, hidden_size)`'
- en: We’ll cover `RNNNode`s (and their variants) in the next section. But first let’s
    cover the `backward` method for the `RNNLayer` class.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中介绍`RNNNode`（及其变体）。但首先让我们介绍`RNNLayer`类的`backward`方法。
- en: The backward method
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向方法
- en: 'Since the `forward` method outputted `x_seq_out`, the `backward` method will
    receive a gradient of the same shape as `x_seq_out` called `x_seq_out_grad`. Moving
    in the opposite direction from the `forward` method, we feed this gradient *backward*
    through the `RNNNode`s, ultimately returning `x_seq_in_grad` of shape `(batch_size,
    sequence_length, self.feature_size)` as the gradient for the entire layer:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`forward`方法输出了`x_seq_out`，`backward`方法将接收与`x_seq_out`形状相同的梯度，称为`x_seq_out_grad`。与`forward`方法相反，我们通过`RNNNode`将此梯度*向后*传递，最终返回整个层的形状为`(batch_size,
    sequence_length, self.feature_size)`的`x_seq_in_grad`作为梯度：
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'From this, we see that `RNNNode`s should have a `backward` method that, following
    the pattern, is the opposite of the `forward` method, taking in two arrays of
    shapes:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们看到，`RNNNode`应该有一个`backward`方法，遵循模式，与`forward`方法相反，接收两个形状的数组：
- en: '`(batch_size, output_size)`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, output_size)`'
- en: '`(batch_size, hidden_size)`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, hidden_size)`'
- en: 'and returning two arrays of shapes:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 并返回两个形状的数组：
- en: '`(batch_size, feature_size)`'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, feature_size)`'
- en: '`(batch_size, hidden_size)`'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(batch_size, hidden_size)`'
- en: 'And that’s the working of an `RNNLayer`. Now it seems like the only thing left
    is to describe the core of recurrent neural networks: the `RNNNode`s where the
    actual computations happen. Before we do, let’s clarify the role of `RNNNode`s
    and their variants within RNNs as a whole.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The Essential Elements of RNNNodes
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In most treatments of RNNs, the first thing discussed is the workings of what
    we are here calling `RNNNode`s. However, we cover these last because the most
    important concepts to understand about RNNs are those that we’ve described in
    the diagrams and code thus far in this chapter: the way the data is structured
    and the way it and the hidden states are routed between layers and through time.
    As it turns out, there are multiple ways we can implement `RNNNode`s, the actual
    processing of the data from a given time step, and updating of the layer’s hidden
    state. One way produces what is usually thought of as a “regular” recurrent neural
    network, which we’ll refer to here by another common term: a “vanilla RNN.” However,
    there are other, more complicated ways that produce different variants of RNNs;
    one of these, for example, is a variant with `RNNNode`s called GRUs, which stands
    for “Gated Recurrent Units.” Often, GRUs and other RNN variants are described
    as being significantly different from vanilla RNNs; however, it is important to
    understand that *all* RNN variants share the structure of the layers that we’ve
    seen so far—for example, they all pass data forward in time in the same way, updating
    their hidden state(s) at each time step. The only way they differ is in the internal
    workings of these “nodes.”'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'To reinforce this point: if we implemented a `GRULayer` instead of an `RNNLayer`,
    the code would be exactly the same! The following code would still form the core
    of the forward pass:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The only difference is that each “node” in `self.nodes` would be a `GRUNode`
    instead of an `RNNNode`. The `backward` method, similarly, would be identical.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also almost entirely true for the most well-known variant on vanilla
    RNNs: LSTMs, or “Long Short Term Memory” cells. The only difference with these
    is that `LSTMLayer`s require *two* quantities to be “remembered” by the layer
    and updated as sequence elements are passed forward through time: in addition
    to a “hidden state,” there is a “cell state” stored in the layer that allows it
    to better model long-term dependencies. This leads to some minor differences in
    how we would implement an `LSTMLayer` as opposed to an `RNNLayer`; for example,
    an `LSTMLayer` would have two `ndarray`s to store the layer’s state throughout
    the time steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: An `ndarray` `start_H` of shape `(1, hidden_size)`, representing the layer’s
    hidden state
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `ndarray` `start_C` of shape `(1, cell_size)`, representing the layer’s cell
    state
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each `LSTMNode`, therefore, should take in the input, as well as both the hidden
    state *and* the cell state. On the forward pass, this will look like:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'as well as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: in the `backward` method.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: There are many more variants than the three mentioned here, some of which, such
    as LSTMs with “peephole connections,” have a cell state in addition to only a
    hidden state, and some of which maintain only a hidden state.^([5](ch06.html#idm45732613324856))
    Still, a layer made up of `LSTMPeepholeConnectionNode`s would fit into an `RNNLayer`
    in the same way as the variants we’ve seen so far and would thus have the same
    `forward` and `backward` methods. This basic structure of RNN—the way data is
    routed forward through layers, as well as forward through time steps, and then
    routed in the opposite direction during the backward pass—is what makes recurrent
    neural networks unique. The actual structural differences between a vanilla RNN
    and an LSTM-based RNN, for example, are relatively minor, even though they can
    have dramatically different performance.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: With that, let’s look at the implementation of an `RNNNode`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: “Vanilla” RNNNodes
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RNNs receive data one sequence element at a time; for example, if we are predicting
    the price of oil, at each time step, the RNN will receive information about the
    features we are using to predict the price at that time step. In addition, the
    RNN will have in its “hidden state” an encoding representing cumulative information
    about what has happened at prior time steps. We want to combine these two pieces
    of data—the features at the time step, and the cumulative information from all
    the prior time steps—into a prediction at that time step, as well as an updated
    hidden state.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: RNN每次接收一个序列元素的数据；例如，如果我们正在预测石油价格，在每个时间步，RNN将接收关于我们用于预测该时间步价格的特征的信息。此外，RNN将在其“隐藏状态”中具有一个编码，表示有关先前时间步发生的事情的累积信息。我们希望将这两个数据片段——时间步的特征和所有先前时间步的累积信息——组合成该时间步的预测以及更新的隐藏状态。
- en: To understand how RNNs should accomplish this, recall what happens in a regular
    neural network. In a feed-forward neural network, each layer receives a set of
    “learned features” from the prior layer, each of which is a combination of the
    original features that the network has “learned” is useful. The layer then multiplies
    these features by a weight matrix that allows the layer to learn features that
    are combinations of the features the layer received as input. To level set and
    normalize the output, respectively, we add a “bias” to these new features and
    feed them through an activation function.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解RNN应该如何实现这一点，回想一下在常规神经网络中发生的情况。在前馈神经网络中，每一层接收来自前一层的一组“学习到的特征”，每个特征都是网络“学习到”有用的原始特征的组合。然后该层将这些特征乘以一个权重矩阵，使该层能够学习到作为输入接收的特征的组合特征。为了水平设置和规范化输出，我们向这些新特征添加一个“偏置”，并通过激活函数传递它们。
- en: 'In recurrent neural networks, we want our updated hidden state to be a combination
    of both the input and the old hidden state. Thus, similar to what happens in regular
    neural networks:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归神经网络中，我们希望我们更新的隐藏状态是输入和旧隐藏状态的组合。因此，类似于常规神经网络中发生的情况：
- en: We first concatenate the input and the hidden state. Then we multiply this value
    by a weight matrix, add a bias, and feed the result through the `Tanh` activation
    function. This is our updated hidden state.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将输入和隐藏状态连接起来。然后我们将这个值乘以一个权重矩阵，加上一个偏置，并通过`Tanh`激活函数传递结果。这就是我们更新的隐藏状态。
- en: Next, we multiply this new hidden state by a weight matrix that transforms the
    hidden state into an output with the dimension that we want. For example, if we
    are using this RNN to predict a single continuous value at each time step, we’ll
    multiply the hidden state by a weight matrix of size `(hidden_size, 1)`.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将这个新的隐藏状态乘以一个权重矩阵，将隐藏状态转换为我们想要的维度的输出。例如，如果我们使用这个RNN来预测每个时间步的单个连续值，我们将把隐藏状态乘以一个大小为`(hidden_size,
    1)`的权重矩阵。
- en: Thus, our updated hidden state will be a function of both the input received
    at that time step as well as the prior hidden state, and the output will be the
    result of feeding this updated hidden state through the operations of a fully
    connected layer.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们更新的隐藏状态将是在该时间步接收到的输入以及先前隐藏状态的函数，输出将是通过全连接层的操作将此更新的隐藏状态馈送的结果。
- en: Let’s code this up.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写代码。
- en: 'RNNNode: The code'
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNNNode：代码
- en: 'The following code implements the steps described a moment ago. Note that,
    just as we’ll do with GRUs and LSTMs a bit later (and as we did with the simple
    mathematical functions we showed in [Chapter 1](ch01.html#foundations)), we save
    all the quantities computed on the forward pass as attributes stored in the `Node`
    so we can use them to compute the backward pass:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了刚才描述的步骤。请注意，正如我们稍后将对GRUs和LSTMs做的一样（以及我们在[第1章](ch01.html#foundations)中展示的简单数学函数），我们将在`Node`中存储所有在前向传播中计算的量作为属性，以便我们可以使用它们来计算反向传播：
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Another note: since we’re not using `ParamOperation`s here, we’ll need to store
    the parameters differently. We’ll store them in a dictionary `params_dict`, which
    refers to the parameters by name. Furthermore, each parameter will have two keys:
    `value` and `deriv`, which will store the actual parameter values and their associated
    gradients, respectively. Here, in the forward pass, we simply use the `value`
    key.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个注意事项：由于我们这里没有使用`ParamOperation`，我们需要以不同的方式存储参数。我们将把它们存储在一个名为`params_dict`的字典中，通过名称引用参数。此外，每个参数将有两个键：`value`和`deriv`，分别存储实际参数值和它们关联的梯度。在前向传播中，我们只使用`value`键。
- en: 'RNNNodes: The backward pass'
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNNNodes：反向传播
- en: 'The backward pass through an `RNNNode` simply computes the value of the gradients
    of the loss with respect to the inputs to the `RNNNode`, given gradients of the
    loss with respect to the outputs of the `RNNNode`. We can do this using logic
    similar to that which we worked out in Chapters [1](ch01.html#foundations) and
    [2](ch02.html#fundamentals): since we can represent an `RNNNode` as a series of
    operations, we can simply compute the derivative of each operation evaluated at
    its input, and successively multiply these derivatives together with the ones
    that have come before (taking care to handle matrix multiplication correctly)
    to end up with `ndarray`s representing the gradients of the loss with respect
    to each of the inputs. The following code accomplishes this:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`RNNNode`的反向传播简单地计算损失相对于`RNNNode`输入的梯度值，给定损失相对于`RNNNode`输出的梯度。我们可以使用类似于我们在[第1章](ch01.html#foundations)和[第2章](ch02.html#fundamentals)中解决的逻辑来做到这一点：由于我们可以将`RNNNode`表示为一系列操作，我们可以简单地计算每个操作在其输入处的导数，并将这些导数与之前的导数逐个相乘（注意正确处理矩阵乘法），最终得到表示损失相对于每个输入的梯度的`ndarray`。以下代码实现了这一点：
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that just as in our `Operation`s from before, the shapes of the inputs
    to the `backward` function must match the shapes of the outputs of the `forward`
    function, and the shapes of the outputs of the `backward` function must match
    the shapes of the inputs to the `forward` function.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of “Vanilla” RNNNodes
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember: the purpose of RNNs is to model dependencies in sequences of data.
    Thinking of modeling the price of oil as our canonical example, this means that
    we should be able to uncover the relationship between the sequence of features
    we’ve seen in the last several time steps and what will happen with the price
    of oil in the next time step. But how long should “several” be? For the price
    of oil, we might imagine that the relationship between what happened yesterday—one
    time step before—would be most important for predicting the price of oil tomorrow,
    with the day before being less important, and the importance generally decaying
    as we move backward in time.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'While this is true for many real-world problems, there are domains to which
    we’d like to apply RNNs where we would want to learn extremely long-range dependencies.
    *Language modeling* is the canonical example here—that is, building a model that
    can predict the next character, word, or word part, given a theoretically extremely
    long series of past words or characters (since this is a particularly prevalent
    application, we’ll discuss some details specific to language modeling later in
    this chapter). For this, vanilla RNNs are usually insufficient. Now that we’ve
    seen their details, we can understand why: at each time step, the hidden state
    is multiplied by *the same weight matrix* across all time steps in the layer.
    Consider what happens when we multiply a number by a value `x` over and over again:
    if `x < 1`, the number decreases exponentially to 0, and if `x > 1`, the number
    increases exponentially to infinity. Recurrent neural networks have the same issues:
    over long time horizons, because the same set of weights is multiplied by the
    hidden state at each time step, the gradient for these weights tends to become
    either extremely small or extremely large. The former is known as the *vanishing
    gradient problem* and the latter is known as the *exploding gradient problem*.
    Both make it hard to train RNNs to model the very long term dependencies (50–100
    time steps) needed for high-quality language modeling. The two commonly used modifications
    of the vanilla RNN architectures we’ll cover next both significantly mitigate
    this problem.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'One Solution: GRUNodes'
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vanilla RNNs can be described as taking the input and hidden state, combining
    them, and using the matrix multiplication to determine how to “weigh” the information
    contained in the hidden state against the information in the new input to predict
    the output. The insight that motivates more advanced RNN variants is that to model
    long-term dependencies, such as those that exist in language, *we sometimes receive
    information that tells us we need to “forget” or “reset” our hidden state*. A
    simple example is a period “.” or a colon “:”—if a language model receives one
    of these, it knows that it should forget the characters that came before and begin
    modeling a new pattern in the sequence of characters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: A first, simple variant on vanilla RNNs that leverages this insight is GRUs
    or Gated Recurrent Units, so named because the input and the prior hidden state
    are passed through a series of “gates.”
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'The first gate is similar to the operations that take place in vanilla RNNs:
    the input and hidden state are concatenated together, multiplied by a weight matrix,
    and then passed through a `sigmoid` operation. We can think of the output of this
    as the “update” gate.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The second gate is interpreted as a “reset” gate: the input and hidden state
    are concatenated, multiplied by a weight matrix, passed through a `sigmoid` operation,
    *and then multiplied by the prior hidden state*. This allows the network to “learn
    to forget” what was in the hidden state, given the particular input that was passed
    in.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the second gate is then multiplied by another matrix and passed
    through the `Tanh` function, with the output being a “candidate” for the new hidden
    state.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the hidden state is updated to be the update gate times the “candidate”
    for the new hidden state, plus the old hidden state times 1 minus the update gate.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’ll cover two advanced variants on vanilla RNNs in this chapter: GRUs and
    LSTMs. LSTMs are more popular and were invented long before GRUs. Nevertheless,
    GRUs are a simpler version of LSTMs, and more directly illustrate how the idea
    of “gates” can enable RNNs to “learn to reset” their hidden state given the input
    they receive, which is why we cover them first.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'GRUNodes: A diagram'
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 6-8](#fig_06_08) depicts `GRUNode` as a series of gates. Each gate
    contains the operations of a `Dense` layer: multiplication by a weight matrix,
    adding a bias, and feeding the result through an activation function. The activation
    functions used are either `sigmoid`, in which case the range of the result falls
    between 0 and 1, or `Tanh`, in which case the range falls between –1 and 1; the
    range of each intermediate `ndarray` produced next is shown under the name of
    the array.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of length one](assets/dlfs_0608.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. The flow of data forward through a GRUNode, passing through gates
    and producing X_out and H_out
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Figure 6-8](#fig_06_08) as in Figures [6-9](#fig_06_09) and [6-10](#fig_06_10),
    the inputs to the node are colored green, the intermediate quantities computed
    are colored blue, and the outputs are colored red. All the weights (not directly
    shown) are contained in the gates.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Note that to backpropagate through this, we would have to represent this solely
    as a series of `Operation`s, compute the derivative of each `Operation` with respect
    to its input, and multiply the results together. We don’t show that explicitly
    here, instead showing gates (which are really groups of three operations) as a
    single block. Still, at this point, we know how to backpropagate through the `Operation`s
    that make up each gate, and the notion of “gates” is used throughout descriptions
    of recurrent neural networks and their variants, so we’ll stick with that representation
    here.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, [Figure 6-9](#fig_06_09) shows a representation of a vanilla `RNNNode`,
    using gates.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of length one](assets/dlfs_0609.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. The flow of data forward through an RNNNode, passing through just
    two gates and producing X_out and H_out
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thus, another way to think of the `Operation`s we previously described as making
    up a vanilla `RNNNode` is as passing the input and hidden state through two gates.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'GRUNodes: The code'
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code implements the forward pass for a `GRUNode` as described
    earlier:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that we don’t explicitly concatenate `X_in` and `H_in`, since—unlike in
    an `RNNNode`, where they are always used together—we use them independently in
    `GRUNode`s; specifically, we use `H_in` independently of `X_in` in the line `self.h_reset
    = self.r * H_in`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The `backward` method can be found on [the book’s website](https://oreil.ly/2P0lG1G);
    it simply steps backward through the operations that make up a `GRUNode`, calculating
    the derivative of each operation with respect to its input and multiplying the
    results together.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: LSTMNodes
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Long Short Term Memory cells, or LSTMs, are the most popular variant of vanilla
    RNN cells. Part of the reason for this is that they were invented in the early
    days of deep learning, back in 1997,^([6](ch06.html#idm45732612415128)) whereas
    investigation into LSTM alternatives such as GRUs has just accelerated in the
    last several years (GRUs were proposed in 2014, for example).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Like GRUs, LSTMs are motivated by the desire to give the RNN the ability to
    “reset” or “forget” its hidden state as it receives new input. In GRUs, this is
    achieved by feeding the input and hidden state through a series of gates, as well
    as computing a “proposed” new hidden state using these gates—`self.h_bar`, computed
    using the gate `self.r`—and then computing the final hidden state using a weighted
    average of the proposed new hidden state and the old hidden state, controlled
    by an update gate:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: LSTMs, by contrast, *use a separate “state” vector, the “cell state,” to determine
    whether to “forget” what is in the hidden state*. They then use two other gates
    to control the extent to which they should reset or update *what is in the cell
    state*, and a fourth gate to determine the extent to which the hidden state gets
    updated based on the final cell state.^([7](ch06.html#idm45732612392904))
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMNodes: Diagram'
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 6-10](#fig_06_10) shows a diagram of an `LSTMNode` with the operations
    represented as gates.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfs_0610.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. The flow of data forward through an LSTMNode, passing through
    a series of gates and outputting updated cell states and hidden states C_out and
    H_out, respectively, along with an actual output X_out
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'LSTMs: The code'
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As with `GRUNode`s, the full code for an `LSTMNode`, including the `backward`
    method and an example showing how these nodes fit into an `LSTMLayer`, is included
    on [the book’s website](https://oreil.ly/2P0lG1G). Here, we just show the `forward`
    method:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And that’s the last element of our RNN framework that we needed to start training
    models! There is one more topic we should cover: how to represent text data in
    a form that will allow us to feed it into our RNNs.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Data Representation for a Character-Level RNN-Based Language Model
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Language modeling is one of the most common tasks RNNs are used for. How can
    we reshape a sequence of characters into a training dataset so that an RNN can
    be trained to predict the next character? The simplest method is to use *one-hot
    encoding*. This works as follows: first, each letter is represented as a vector
    with dimension equal to *the size of the vocabulary* or the number of letters
    in the overall corpus of text we’ll train the network on (this is calculated beforehand
    and hardcoded as a hyperparameter in the network). Then each letter is represented
    as a vector with a 1 in the position representing that letter and 0s everywhere
    else. Finally, the vectors for each letter are simply concatenated together to
    get an overall representation for the sequence of letters.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example of how this would look with a vocabulary of four letters,
    `a`, `b`, `c`, and `d`, where we arbitrarily call `a` the first letter, `b` the
    second letter, and so on:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>a</mi> <mi>b</mi> <mi>c</mi> <mi>d</mi> <mi>b</mi>
    <mo>→</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>a</mi> <mi>b</mi> <mi>c</mi> <mi>d</mi> <mi>b</mi>
    <mo>→</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'This 2D array would take the place of one observation of shape `(sequence_length,
    num_features) = (5, 4)` in a batch of sequences. So if our text was “abcdba”—of
    length 6—and we wanted to feed sequences of length 5 into our array, the first
    sequence would be transformed into the preceding matrix, and the second sequence
    would be:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>b</mi> <mi>c</mi> <mi>d</mi> <mi>b</mi> <mi>a</mi>
    <mo>→</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>b</mi> <mi>c</mi> <mi>d</mi> <mi>b</mi> <mi>a</mi>
    <mo>→</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd>
    <mtd><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: These would be then concatenated together to create an input to the RNN of shape
    `(batch_size, sequence_length, vocab_size) = (2, 5, 4)`. Continuing in this way,
    we can take raw text and transform it into a batch of sequences to be fed into
    an RNN.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: In the [Chapter 6 notebook](https://oreil.ly/2P0lG1G) on the book’s GitHub repo,
    we code this up as part of an `RNNTrainer` class that can take in raw text, preprocess
    it using the techniques described here, and feed it into an RNN in batches.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Other Language Modeling Tasks
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他语言建模任务
- en: We didn’t emphasize this earlier in the chapter, but as you can see from the
    preceding code, all `RNNNode` variants allow an `RNNLayer` to output a different
    number of features than it received as input. The last step of all three nodes
    is to multiply the network’s final hidden state by a weight matrix we access via
    `params_dict[*W_v*]`; the second dimension of this weight matrix will determine
    the dimensionality of the `Layer`’s output. This allows us to use the same architecture
    for different language modeling tasks simply by changing an `output_size` argument
    in each `Layer`.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中没有强调这一点，但正如您从前面的代码中看到的，所有`RNNNode`变体都允许`RNNLayer`输出与其接收的特征数量不同的特征。所有三个节点的最后一步是将网络的最终隐藏状态乘以我们通过`params_dict[*W_v*]`访问的权重矩阵；这个权重矩阵的第二维将决定`Layer`的输出维度。这使我们可以通过在每个`Layer`中更改一个`output_size`参数来简单地为不同的语言建模任务使用相同的架构。
- en: 'For example, so far we’ve just considered building a language model via “next
    character prediction”; in this case, our output size will be equal to the size
    of the vocabulary: `output_size = vocab_size`. For something like sentiment analysis,
    however, sequences we pass in may simply have a label of “0” or “1”—positive or
    negative. In this case, not only will we have `output_size = 1`, but we’ll compare
    the output to the target only after we pass in the entire sequence. This will
    look like what is depicted in [Figure 6-11](#fig_06_11).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，到目前为止，我们只考虑通过“下一个字符预测”构建语言模型；在这种情况下，我们的输出大小将等于词汇表的大小：`output_size = vocab_size`。然而，对于情感分析之类的任务，我们传入的序列可能只有一个标签“0”或“1”——积极或消极。在这种情况下，我们不仅将有`output_size
    = 1`，而且只有在传入整个序列后才将输出与目标进行比较。这将看起来像[图6-11](#fig_06_11)中所示。
- en: '![Output of length one](assets/dlfs_0611.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![长度为一的输出](assets/dlfs_0611.png)'
- en: Figure 6-11\. For sentiment analysis, RNNs will compare their predictions with
    actual values and produce gradients *just for the output of the last sequence
    element*; then backpropagation will proceed as usual, with each of the nodes that
    isn’t the last one simply receiving an “X_grad_out” array of all zeros
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11。对于情感分析，RNN将将其预测与实际值进行比较，并仅为最后一个序列元素的输出产生梯度；然后反向传播将继续进行，每个不是最后一个节点的节点将简单地接收一个全零的“X_grad_out”数组
- en: Thus, this framework can easily accommodate different language modeling tasks;
    indeed, it can accommodate any modeling task in which the data is sequential and
    can be fed into the network one sequence element at a time.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个框架可以轻松适应不同的语言建模任务；实际上，它可以适应任何数据是顺序的并且可以逐个序列元素馈送到网络中的建模任务。
- en: 'Before we conclude, we’ll cover an infrequently discussed aspect of RNNs: that
    these different kinds of layers—`GRULayer`s, `LSTMLayer`s, and other variants—can
    be mixed and matched.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束之前，我们将讨论RNN中很少讨论的一个方面：这些不同类型的层——`GRULayer`、`LSTMLayer`和其他变体——可以混合和匹配。
- en: Combining RNNLayer Variants
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组合RNNLayer变体
- en: 'Stacking different kinds of `RNNLayer`s is straightforward: each RNN outputs
    an `ndarray` of shape `(batch_size, sequence_length, output_size)`, which can
    be fed into the next layer. As was the case in `Dense` layers, we don’t have to
    specify an `input_shape`; we simply set up the weights based on the first `ndarray`
    the layer receives as input to be the appropriate shape given the input. An `RNNModel`
    can thus have a `self.layers` attribute of:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠不同类型的`RNNLayer`非常简单：每个RNN输出一个形状为`(batch_size, sequence_length, output_size)`的`ndarray`，可以被馈送到下一层。就像在`Dense`层中一样，我们不需要指定`input_shape`；我们只需根据层接收的第一个`ndarray`设置权重，使其具有适当的形状。一个`RNNModel`可以具有一个`self.layers`属性：
- en: '[PRE31]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As with our fully connected neural networks, we just have to be sure that the
    last layer produces an output of the desired dimensionality; here, if we are dealing
    with a vocabulary of size 62 and doing next character prediction, our last layer
    must have an `output_size` of 62, just as the last layer in our fully connected
    neural networks dealing with the MNIST problem had to have dimension 10.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的全连接神经网络一样，我们只需要确保最后一层产生所需维度的输出；在这里，如果我们处理的词汇量为62并进行下一个字符预测，我们的最后一层必须具有62的`output_size`，就像我们处理MNIST问题的全连接神经网络中的最后一层必须具有维度10一样。
- en: 'Something that should be clear after reading this chapter but that isn’t often
    covered in treatments of RNNs is that, because each kind of layer we’ve seen has
    the same underlying structure taking in sequences of dimension `feature_size`
    and outputting sequences of dimension `output_size`, we can easily stack different
    kinds of layers. For example, on [the book’s website](https://oreil.ly/2P0lG1G),
    we train an `RNNModel` with a `self.layers` attribute of:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章后应该清楚的一点，但在处理RNN时通常不经常涉及的是，因为我们看到的每种类型的层都具有相同的基础结构，接收维度为`feature_size`的序列并输出维度为`output_size`的序列，我们可以轻松地堆叠不同类型的层。例如，在[书籍网站](https://oreil.ly/2P0lG1G)上，我们训练一个具有`self.layers`属性的`RNNModel`：
- en: '[PRE32]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In other words, the first layer passes its input forward through time using
    `GRUNode`s, and then passes an `ndarray` of shape `(batch_size, sequence_length,
    128)` into the next layer, which subsequently passes them through its `LSTMNode`s.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，第一层通过使用`GRUNode`将其输入向前传递一段时间，然后将形状为`(batch_size, sequence_length, 128)`的`ndarray`传递到下一层，随后通过其`LSTMNode`将它们传递。
- en: Putting This All Together
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: A classic exercise to illustrate the effectiveness of RNNs is to train them
    to write text in a particular style; [on the book’s website](https://oreil.ly/2P0lG1G),
    we have an end-to-end code example, with the model defined using the abstractions
    described in this chapter, that learns to write text in the style of Shakespeare.
    The only component we haven’t shown is an `RNNTrainer` class that iterates through
    the training data, preprocesses it, and feeds it through the model. The main difference
    between this and the `Trainer` we’ve seen previously is that with RNNs, once we
    select a batch of data to be fed through—with each element of the batch simply
    a string—we must first preprocess it, one-hot encoding each letter and concatenating
    the resulting vectors into a sequence to transform each string of length `sequence_length`
    into an `ndarray` of shape `(sequence_length, vocab_size)`. To form the batches
    that get will fed into our RNN, *these* `ndarray`s will then be concatenated together
    to form the batch of size `(sequence_length, vocab_size, batch_size)`.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'But once the data has been preprocessed and the model defined, RNNs are trained
    in the same way as other neural networks we’ve seen: batches are iteratively fed
    through, the model’s predictions are compared to the targets to generate the loss,
    and the loss is backpropagated through the operations that make up the model to
    update the weights.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about recurrent neural networks, a special kind
    of neural network architecture designed for processing sequences of data, rather
    than individual operations. You learned how RNNs are made up of layers that pass
    data forward in time, updating their hidden states (and their cell states, in
    the case of LSTMs) as they go. You saw the details of advanced RNN variants, GRUs
    and LSTMs, and how they pass data forward through a series of “gates” at each
    time step; nevertheless, you learned that these advanced variants fundamentally
    process the sequences of data in the same way and thus have the same structure
    to their layers, differing only in the specific operations they apply at each
    time step.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully this multifaceted topic is now less of a black box. In [Chapter 7](ch07.html#pytorch),
    I’ll conclude the book by turning to the practical side of deep learning, showing
    how to implement everything we’ve talked about thus far using the PyTorch framework,
    a high-performance, automatic differentiation–based framework for building and
    training deep learning models. Onward!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch06.html#idm45732615342296-marker)) We happened to find it convenient
    to arrange the observations along the rows and the features along the columns,
    but we didn’t necessarily have to arrange the data that way. The data does, however,
    have to be two-dimensional.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.html#idm45732615335848-marker)) Or this edition of the book, at least.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch06.html#idm45732614959096-marker)) I want to mention an alternative
    solution to this problem shared by author Daniel Sabinasz on his blog, [*deep
    ideas*](http://www.deepideas.net): he represents the operations as a graph and
    then uses breadth-first search to compute the gradients on the backward pass in
    the correct order, ultimately building a framework that mimics TensorFlow. His
    blog posts covering how he does this are extremely clear and well structured.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch06.html#idm45732615077368-marker)) For a deeper dive into how to implement
    automatic differentiation, see *Grokking Deep Learning* by Andrew Trask (Manning).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch06.html#idm45732613324856-marker)) See the Wikipedia page on LSTMs for
    more examples of [LSTM variants](https://oreil.ly/2TysrXj).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch06.html#idm45732612415128-marker)) See the original LSTM paper, [“Long
    Short-Term Memory”](https://oreil.ly/2YYZvwT), by Hochreiter et al. (1997).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch06.html#idm45732612392904-marker)) At least the standard variant of
    LSTMs; as mentioned, there are other variants such as “LSTMs with peephole connections”
    whose gates are arranged differently.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
