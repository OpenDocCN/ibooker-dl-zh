["```py\ngit clone \\\n  https://github.com/DOsinga/deep_learning_cookbook.git\ncd deep_learning_cookbook\npython3 -m venv venv3\nsource venv3/bin/activate\npip install -r requirements.txt\njupyter notebook\n```", "```py\n03.1 Using pretrained word embeddings\n03.2 Domain specific ranking using word2vec cosine distance\n```", "```py\nMODEL = 'GoogleNews-vectors-negative300.bin'\npath = get_file(MODEL + '.gz',\n    'https://s3.amazonaws.com/dl4j-distribution/%s.gz' % MODEL)\nunzipped = os.path.join('generated', MODEL)\nif not os.path.isfile(unzipped):\n    with open(unzipped, 'wb') as fout:\n        zcat = subprocess.Popen(['zcat'],\n                          stdin=open(path),\n                          stdout=fout\n                         )\n        zcat.wait()\n```", "```py\nDownloading data from GoogleNews-vectors-negative300.bin.gz\n1647050752/1647046227 [==============================] - 71s 0us/step\n```", "```py\nmodel = gensim.models.KeyedVectors.load_word2vec_format(MODEL, binary=True)\n```", "```py\nmodel.most_similar(positive=['espresso'])\n```", "```py\n[(u'cappuccino', 0.6888186931610107),\n (u'mocha', 0.6686209440231323),\n (u'coffee', 0.6616827249526978),\n (u'latte', 0.6536752581596375),\n (u'caramel_macchiato', 0.6491267681121826),\n (u'ristretto', 0.6485546827316284),\n (u'espressos', 0.6438628435134888),\n (u'macchiato', 0.6428250074386597),\n (u'chai_latte', 0.6308028697967529),\n (u'espresso_cappuccino', 0.6280542612075806)]\n```", "```py\ndef A_is_to_B_as_C_is_to(a, b, c, topn=1):\n  a, b, c = map(lambda x:x if type(x) == list else [x], (a, b, c))\n  res = model.most_similar(positive=b + c, negative=a, topn=topn)\n  if len(res):\n    if topn == 1:\n      return res[0][0]\n    return [x[0] for x in res]\n  return None\n```", "```py\nA_is_to_B_as_C_is_to('man', 'woman', 'king')\n```", "```py\nu'queen'\n```", "```py\nfor country in 'Italy', 'France', 'India', 'China':\n    print('%s is the capital of %s' %\n          (A_is_to_B_as_C_is_to('Germany', 'Berlin', country), country))\n```", "```py\nRome is the capital of Italy\nParis is the capital of France\nDelhi is the capital of India\nBeijing is the capital of China\n```", "```py\nfor company in 'Google', 'IBM', 'Boeing', 'Microsoft', 'Samsung':\n  products = A_is_to_B_as_C_is_to(\n    ['Starbucks', 'Apple'], ['Starbucks_coffee', 'iPhone'], company, topn=3)\n  print('%s -> %s' %\n        (company, ', '.join(products)))\n```", "```py\nGoogle -> personalized_homepage, app, Gmail\nIBM -> DB2, WebSphere_Portal, Tamino_XML_Server\nBoeing -> Dreamliner, airframe, aircraft\nMicrosoft -> Windows_Mobile, SyncMate, Windows\nSamsung -> MM_A###, handset, Samsung_SCH_B###\n```", "```py\nbeverages = ['espresso', 'beer', 'vodka', 'wine', 'cola', 'tea']\ncountries = ['Italy', 'Germany', 'Russia', 'France', 'USA', 'India']\nsports = ['soccer', 'handball', 'hockey', 'cycling', 'basketball', 'cricket']\n\nitems = beverages + countries + sports\n```", "```py\nitem_vectors = [(item, model[item])\n                    for item in items\n                    if item in model]\n```", "```py\nvectors = np.asarray([x[1] for x in item_vectors])\nlengths = np.linalg.norm(vectors, axis=1)\nnorm_vectors = (vectors.T / lengths).T\ntsne = TSNE(n_components=2, perplexity=10,\n            verbose=2).fit_transform(norm_vectors)\n```", "```py\nx=tsne[:,0]\ny=tsne[:,1]\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\n\nfor item, x1, y1 in zip(item_vectors, x, y):\n    ax.annotate(item[0], (x1, y1))\n\nplt.show()\n```", "```py\nmodel = gensim.models.KeyedVectors.load_word2vec_format(MODEL, binary=True)\nmodel.most_similar(positive=['Germany'])\n```", "```py\n[(u'Austria', 0.7461062073707581),\n (u'German', 0.7178748846054077),\n (u'Germans', 0.6628648042678833),\n (u'Switzerland', 0.6506867408752441),\n (u'Hungary', 0.6504981517791748),\n (u'Germnay', 0.649348258972168),\n (u'Netherlands', 0.6437495946884155),\n (u'Cologne', 0.6430779099464417)]\n```", "```py\npositive = ['Chile', 'Mauritius', 'Barbados', 'Ukraine', 'Israel',\n  'Rwanda', 'Venezuela', 'Lithuania', 'Costa_Rica', 'Romania',\n  'Senegal', 'Canada', 'Malaysia', 'South_Korea', 'Australia',\n  'Tunisia', 'Armenia', 'China', 'Czech_Republic', 'Guinea',\n  'Gambia', 'Gabon', 'Italy', 'Montenegro', 'Guyana', 'Nicaragua',\n  'French_Guiana', 'Serbia', 'Uruguay', 'Ethiopia', 'Samoa',\n  'Antarctica', 'Suriname', 'Finland', 'Bermuda', 'Cuba', 'Oman',\n  'Azerbaijan', 'Papua', 'France', 'Tanzania', 'Germany' \u2026 ]\n```", "```py\nnegative = random.sample(model.vocab.keys(), 5000)\nnegative[:4]\n```", "```py\n[u'Denys_Arcand_Les_Invasions',\n u'2B_refill',\n u'strained_vocal_chords',\n u'Manifa']\n```", "```py\nlabelled = [(p, 1) for p in positive] + [(n, 0) for n in negative]\nrandom.shuffle(labelled)\nX = np.asarray([model[w] for w, l in labelled])\ny = np.asarray([l for w, l in labelled])\n```", "```py\nTRAINING_FRACTION = 0.7\ncut_off = int(TRAINING_FRACTION * len(labelled))\nclf = svm.SVC(kernel='linear')\nclf.fit(X[:cut_off], y[:cut_off])\n```", "```py\nres = clf.predict(X[cut_off:])\n\nmissed = [country for (pred, truth, country) in\n          zip(res, y[cut_off:], labelled[cut_off:]) if pred != truth]\n100 - 100 * float(len(missed)) / len(res), missed\n```", "```py\nres = []\nfor word, pred in zip(model.index2word, all_predictions):\n  if pred:\n    res.append(word)\n    if len(res) == 150:\n      break\nrandom.sample(res, 10)\n```", "```py\n[u'Myanmar',\n u'countries',\n u'Sri_Lanka',\n u'Israelis',\n u'Australia',\n u'Pyongyang',\n u'New_Hampshire',\n u'Italy',\n u'China',\n u'Philippine']\n```", "```py\ncountry_to_idx = {country['name']: idx for idx, country in enumerate(countries)}\ncountry_vecs = np.asarray([model[c['name']] for c in countries])\ncountry_vecs.shape\n```", "```py\n(184, 300)\n```", "```py\ncountries = list(country_to_cc.keys())\ncountry_vecs = np.asarray([model[c] for c in countries])\n```", "```py\ndists = np.dot(country_vecs, country_vecs[country_to_idx['Canada']])\nfor idx in reversed(np.argsort(dists)[-8:]):\n    print(countries[idx], dists[idx])\n```", "```py\nCanada 7.5440245\nNew_Zealand 3.9619699\nFinland 3.9392405\nPuerto_Rico 3.838145\nJamaica 3.8102934\nSweden 3.8042784\nSlovakia 3.7038736\nAustralia 3.6711009\n```", "```py\ndef rank_countries(term, topn=10, field='name'):\n    if not term in model:\n        return []\n    vec = model[term]\n    dists = np.dot(country_vecs, vec)\n    return [(countries[idx][field], float(dists[idx]))\n            for idx in reversed(np.argsort(dists)[-topn:])]\n```", "```py\nrank_countries('cricket')\n```", "```py\n[('Sri_Lanka', 5.92276668548584),\n ('Zimbabwe', 5.336524486541748),\n ('Bangladesh', 5.192488670349121),\n ('Pakistan', 4.948408126831055),\n ('Guyana', 3.9162840843200684),\n ('Barbados', 3.757995128631592),\n ('India', 3.7504401206970215),\n ('South_Africa', 3.6561498641967773),\n ('New_Zealand', 3.642028331756592),\n ('Fiji', 3.608567714691162)]\n```", "```py\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld.head()\n```", "```py\ndef map_term(term):\n    d = {k.upper(): v for k, v in rank_countries(term,\n                                                 topn=0,\n                                                 field='cc3')}\n    world[term] = world['iso_a3'].map(d)\n    world[term] /= world[term].max()\n    world.dropna().plot(term, cmap='OrRd')\n\nmap_term('coffee')\n```"]