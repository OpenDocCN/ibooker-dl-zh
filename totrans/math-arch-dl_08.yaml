- en: 9 Loss, optimization, and regularization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 æŸå¤±ã€ä¼˜åŒ–å’Œæ­£åˆ™åŒ–
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æ¶µç›–äº†
- en: Geometrical and algebraic introductions to loss functions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°çš„å‡ ä½•å’Œä»£æ•°ä»‹ç»
- en: Geometrical intuitions for softmax
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: softmaxçš„å‡ ä½•ç›´è§‰
- en: Optimization techniques including momentum, Nesterov, AdaGrad, Adam, and SGD
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…æ‹¬åŠ¨é‡ã€Nesterovã€AdaGradã€Adamå’ŒSGDåœ¨å†…çš„ä¼˜åŒ–æŠ€æœ¯
- en: Regularization and its relationship to Bayesian approaches
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–åŠå…¶ä¸è´å¶æ–¯æ–¹æ³•çš„å…³ç³»
- en: Overfitting while training, and dropout
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡æ‹Ÿåˆå’Œdropout
- en: By now, it should be etched in your mind that neural networks are essentially
    function approximators. In particular, neural network classifiers model the decision
    boundaries between the classes in the feature space (a space where every input
    feature combination is a specific point). Supervised classifiers mark sample training
    data inputs in this space with aâ€”perhaps manually generatedâ€”class label (ground
    truth). The training process iteratively learns a function that essentially creates
    decision boundaries separating the sampled training data points into individual
    classes. If the training data set is a reasonable representative of the true distribution
    of possible inputs, the network (the learned function that models the class boundaries)
    will classify never-before-seen inputs with good accuracy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œä½ åº”è¯¥å·²ç»æ·±åˆ»åœ°è®¤è¯†åˆ°ç¥ç»ç½‘ç»œæœ¬è´¨ä¸Šæ˜¯ä¸€ç§å‡½æ•°é€¼è¿‘å™¨ã€‚ç‰¹åˆ«æ˜¯ï¼Œç¥ç»ç½‘ç»œåˆ†ç±»å™¨åœ¨ç‰¹å¾ç©ºé—´ï¼ˆæ¯ä¸ªè¾“å…¥ç‰¹å¾ç»„åˆéƒ½æ˜¯ä¸€ä¸ªç‰¹å®šç‚¹çš„ç©ºé—´ï¼‰ä¸­æ¨¡æ‹Ÿäº†ç±»ä¹‹é—´çš„å†³ç­–è¾¹ç•Œã€‚ç›‘ç£åˆ†ç±»å™¨åœ¨è¿™ä¸ªç©ºé—´ä¸­æ ‡è®°æ ·æœ¬è®­ç»ƒæ•°æ®è¾“å…¥ï¼Œå¹¶å¸¦æœ‰â€”â€”å¯èƒ½æ˜¯æ‰‹åŠ¨ç”Ÿæˆçš„â€”â€”ç±»æ ‡ç­¾ï¼ˆçœŸå®æ ‡ç­¾ï¼‰ã€‚è®­ç»ƒè¿‡ç¨‹è¿­ä»£åœ°å­¦ä¹ ä¸€ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°æœ¬è´¨ä¸Šåˆ›å»ºäº†å†³ç­–è¾¹ç•Œï¼Œå°†æ ·æœ¬è®­ç»ƒæ•°æ®ç‚¹åˆ†ç¦»æˆå•ä¸ªç±»åˆ«ã€‚å¦‚æœè®­ç»ƒæ•°æ®é›†æ˜¯å¯èƒ½è¾“å…¥çš„çœŸå®åˆ†å¸ƒçš„åˆç†ä»£è¡¨ï¼Œé‚£ä¹ˆç½‘ç»œï¼ˆæ¨¡å‹ç±»è¾¹ç•Œçš„å­¦åˆ°çš„å‡½æ•°ï¼‰å°†ä»¥è‰¯å¥½çš„å‡†ç¡®æ€§å¯¹ä»æœªè§è¿‡çš„è¾“å…¥è¿›è¡Œåˆ†ç±»ã€‚
- en: When we select a specific neural network architecture (with a fixed set of layers,
    each with a fixed set of perceptrons with specific connections), we have essentially
    frozen the *family* of functions we use as a function approximator. We still have
    to â€œlearnâ€ the exact weights of the connectors between various *perceptrons* aka
    *neurons*). The training process iteratively sets these weights so as to best
    classify the training data points. To do this, we design a loss function that
    measures the departure of the network output from the desired result. The network
    continually tries to minimize this loss. There are a variety of loss functions
    to choose from.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªç‰¹å®šçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆå…·æœ‰å›ºå®šçš„ä¸€ç»„å±‚ï¼Œæ¯ä¸ªå±‚å…·æœ‰å›ºå®šçš„ä¸€ç»„å…·æœ‰ç‰¹å®šè¿æ¥çš„æ„ŸçŸ¥å™¨ï¼‰æ—¶ï¼Œæˆ‘ä»¬æœ¬è´¨ä¸Šå†»ç»“äº†æˆ‘ä»¬ç”¨ä½œå‡½æ•°é€¼è¿‘å™¨çš„*å‡½æ•°æ—*ã€‚æˆ‘ä»¬ä»ç„¶éœ€è¦â€œå­¦ä¹ â€å„ç§*æ„ŸçŸ¥å™¨*ï¼ˆä¹Ÿç§°ä¸º*ç¥ç»å…ƒ*ï¼‰ä¹‹é—´çš„è¿æ¥å™¨çš„ç¡®åˆ‡æƒé‡ã€‚è®­ç»ƒè¿‡ç¨‹è¿­ä»£åœ°è®¾ç½®è¿™äº›æƒé‡ï¼Œä»¥ä¾¿æœ€å¥½åœ°åˆ†ç±»è®­ç»ƒæ•°æ®ç‚¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥è¡¡é‡ç½‘ç»œè¾“å‡ºä¸æœŸæœ›ç»“æœä¹‹é—´çš„åå·®ã€‚ç½‘ç»œæŒç»­å°è¯•æœ€å°åŒ–è¿™ä¸ªæŸå¤±ã€‚æœ‰å„ç§å„æ ·çš„æŸå¤±å‡½æ•°å¯ä¾›é€‰æ‹©ã€‚
- en: The iterative process through which loss is minimized is called *optimization*.
    We also have a multitude of optimization algorithms to choose from. In this chapter,
    we study loss functions, optimization algorithms, and associated topics like L1
    and L2 regularization and dropout. We also learn about overfitting, a potential
    pitfall to avoid while training a neural network.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿­ä»£æœ€å°åŒ–æŸå¤±çš„è¿‡ç¨‹ç§°ä¸º*ä¼˜åŒ–*ã€‚æˆ‘ä»¬è¿˜æœ‰è®¸å¤šä¼˜åŒ–ç®—æ³•å¯ä¾›é€‰æ‹©ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶æŸå¤±å‡½æ•°ã€ä¼˜åŒ–ç®—æ³•ä»¥åŠç›¸å…³çš„ä¸»é¢˜ï¼Œå¦‚L1å’ŒL2æ­£åˆ™åŒ–å’Œdropoutã€‚æˆ‘ä»¬è¿˜äº†è§£è¿‡æ‹Ÿåˆï¼Œè¿™æ˜¯åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶éœ€è¦é¿å…çš„ä¸€ä¸ªæ½œåœ¨é™·é˜±ã€‚
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng.bz/aZv9](http://mng.bz/aZv9)
    in the form of fully functional and executable Jupyter notebooks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæœ¬ç« çš„å®Œæ•´PyTorchä»£ç ä»¥å®Œå…¨åŠŸèƒ½æ€§å’Œå¯æ‰§è¡Œçš„Jupyterç¬”è®°æœ¬å½¢å¼ï¼Œå¯åœ¨[http://mng.bz/aZv9](http://mng.bz/aZv9)æ‰¾åˆ°ã€‚
- en: 9.1 Loss functions
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 æŸå¤±å‡½æ•°
- en: A loss function essentially measures the badness of the neural network output.
    In the case of a supervised network, the loss for an individual training data
    instance is the distance of the actual output of the neural network (aka prediction)
    from the desired ideal outputs known or manually labeled ground truth [GT]) on
    that particular training input instance. Total training loss is obtained by summing
    the losses from all training data instances. Training is essentially an iterative
    optimization process that minimizes the total training loss.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°æœ¬è´¨ä¸Šè¡¡é‡äº†ç¥ç»ç½‘ç»œè¾“å‡ºçš„ä¸è‰¯ç¨‹åº¦ã€‚åœ¨ç›‘ç£ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œå•ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹çš„æŸå¤±æ˜¯ç¥ç»ç½‘ç»œçš„å®é™…è¾“å‡ºï¼ˆä¹Ÿç§°ä¸ºé¢„æµ‹ï¼‰ä¸ç‰¹å®šè®­ç»ƒè¾“å…¥å®ä¾‹ä¸Šå·²çŸ¥çš„æˆ–æ‰‹åŠ¨æ ‡è®°çš„ç†æƒ³è¾“å‡ºï¼ˆçœŸå®æ ‡ç­¾[GT]ï¼‰ä¹‹é—´çš„è·ç¦»ã€‚æ€»è®­ç»ƒæŸå¤±æ˜¯é€šè¿‡å°†æ‰€æœ‰è®­ç»ƒæ•°æ®å®ä¾‹çš„æŸå¤±ç›¸åŠ å¾—åˆ°çš„ã€‚è®­ç»ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªè¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹æœ€å°åŒ–æ€»è®­ç»ƒæŸå¤±ã€‚
- en: 9.1.1 Quantification and geometrical view of loss
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 æŸå¤±çš„é‡åŒ–ä¸å‡ ä½•è§†è§’
- en: Loss surfaces and their minimization are described in detail in section [8.4.2](../Text/08.xhtml#sec-loss-surface-graddesc).
    Here we only do a quick review.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±è¡¨é¢åŠå…¶æœ€å°åŒ–åœ¨[8.4.2](../Text/08.xhtml#sec-loss-surface-graddesc)èŠ‚ä¸­è¯¦ç»†æè¿°ã€‚è¿™é‡Œæˆ‘ä»¬åªåšç®€è¦å›é¡¾ã€‚
- en: A full neural network can be described by the equation
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå®Œæ•´çš„ç¥ç»ç½‘ç»œå¯ä»¥ç”¨ä»¥ä¸‹æ–¹ç¨‹æè¿°
- en: '![](../../OEBPS/Images/eq_09-01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-01.png)'
- en: Equation 9.1
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹9.1
- en: Equation [9.1](#eq-nn-forward-pass-overall) says that given an input ![](../../OEBPS/Images/AR_x.png),
    the neural network with weights ![](../../OEBPS/Images/AR_w.png) and biases ![](../../OEBPS/Images/AR_b.png)
    emits the *output vector* or *prediction vector* ![](../../OEBPS/Images/AR_y.png).
    The weights and biases may be organized into layers; this equation does not care.
    The vectors ![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png),
    respectively, denote the sets of *all* weights and biases from all layers aggregated.
    Evaluating the function *f*(â‹…) is equivalent to performing one forward pass on
    the network. In particular, given a training input instance ![](../../OEBPS/Images/AR_x.png)^((*i*)),
    the neural network emits ![](../../OEBPS/Images/AR_y.png)^((*i*)) = *f*(![](../../OEBPS/Images/AR_x.png)^((*i*))|![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png)). We refer to ![](../../OEBPS/Images/AR_y.png)^((*i*))
    as the output of the *i*th training data instance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹[9.1](#eq-nn-forward-pass-overall)è¡¨æ˜ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥ ![](../../OEBPS/Images/AR_x.png)ï¼Œæƒé‡ä¸º
    ![](../../OEBPS/Images/AR_w.png) å’Œåç½®ä¸º ![](../../OEBPS/Images/AR_b.png) çš„ç¥ç»ç½‘ç»œä¼šå‘å‡º*è¾“å‡ºå‘é‡*æˆ–*é¢„æµ‹å‘é‡*
    ![](../../OEBPS/Images/AR_y.png)ã€‚æƒé‡å’Œåç½®å¯ä»¥ç»„ç»‡æˆå±‚ï¼›è¿™ä¸ªæ–¹ç¨‹å¹¶ä¸å…³å¿ƒè¿™ä¸€ç‚¹ã€‚å‘é‡ ![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png) åˆ†åˆ«è¡¨ç¤ºä»æ‰€æœ‰å±‚èšåˆçš„æ‰€æœ‰æƒé‡å’Œåç½®çš„é›†åˆã€‚è¯„ä¼°å‡½æ•° *f*(â‹…) ç­‰åŒäºåœ¨ç½‘ç»œä¸­è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ é€’ã€‚ç‰¹åˆ«æ˜¯ï¼Œç»™å®šä¸€ä¸ªè®­ç»ƒè¾“å…¥å®ä¾‹
    ![](../../OEBPS/Images/AR_x.png)^((*i*))ï¼Œç¥ç»ç½‘ç»œå‘å‡º ![](../../OEBPS/Images/AR_y.png)^((*i*))
    = *f*(![](../../OEBPS/Images/AR_x.png)^((*i*))|![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png))ã€‚æˆ‘ä»¬å°† ![](../../OEBPS/Images/AR_y.png)^((*i*))
    ç§°ä¸ºç¬¬ *i* ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹çš„è¾“å‡ºã€‚
- en: During supervised training, for each training input instance ![](../../OEBPS/Images/AR_x.png)^((*i*)),
    we have the GT (the known output), *È³*^((*i*)). We refer to *È³*^((*i*)) as the
    *GT vector* (as usual, we use superscript indices for training data instances).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç›‘ç£è®­ç»ƒæœŸé—´ï¼Œå¯¹äºæ¯ä¸ªè®­ç»ƒè¾“å…¥å®ä¾‹ ![](../../OEBPS/Images/AR_x.png)^((*i*))ï¼Œæˆ‘ä»¬éƒ½æœ‰GTï¼ˆå·²çŸ¥çš„è¾“å‡ºï¼‰ï¼Œ*È³*^((*i*)ï¼‰ã€‚æˆ‘ä»¬å°†
    *È³*^((*i*)ï¼‰ç§°ä¸º*GTå‘é‡*ï¼ˆå¦‚é€šå¸¸æ‰€åšï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸Šæ ‡ç´¢å¼•è¡¨ç¤ºè®­ç»ƒæ•°æ®å®ä¾‹ï¼‰ã€‚
- en: 'Ideally, the output vector ![](../../OEBPS/Images/AR_y.png)^((*i*)) should
    match the GT vector *È³*^((*i*)). The mismatch between them is the loss for that
    training data instance ğ•ƒ^((*i*))(![](../../OEBPS/Images/AR_y.png)^((*i*)), *È³*^((*i*))|![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png)), which we sometimes denote as ğ•ƒ^((*i*))(![](../../OEBPS/Images/AR_y.png)^((*i*)),
    *È³*^((*i*))). The overall training loss (to be minimized by the optimization process)
    is the sum of losses over all training data instances:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç†æƒ³æƒ…å†µä¸‹ï¼Œè¾“å‡ºå‘é‡ ![](../../OEBPS/Images/AR_y.png)^((*i*)) åº”è¯¥ä¸GTå‘é‡ *È³*^((*i*)ï¼‰ç›¸åŒ¹é…ã€‚å®ƒä»¬ä¹‹é—´çš„ä¸åŒ¹é…æ˜¯é’ˆå¯¹è¯¥è®­ç»ƒæ•°æ®å®ä¾‹
    ğ•ƒ^((*i*))(![](../../OEBPS/Images/AR_y.png)^((*i*)), *È³*^((*i*))) çš„æŸå¤±ï¼Œæˆ‘ä»¬æœ‰æ—¶å°†å…¶è¡¨ç¤ºä¸º
    ğ•ƒ^((*i*))(![](../../OEBPS/Images/AR_y.png)^((*i*)), *È³*^((*i*))). æ‰€æœ‰è®­ç»ƒæ•°æ®å®ä¾‹çš„æ€»ä½“è®­ç»ƒæŸå¤±ï¼ˆç”±ä¼˜åŒ–è¿‡ç¨‹æœ€å°åŒ–ï¼‰æ˜¯æ‰€æœ‰è®­ç»ƒæ•°æ®å®ä¾‹æŸå¤±çš„åŠ å’Œï¼š
- en: '![](../../OEBPS/Images/eq_09-02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-02.png)'
- en: Equation 9.2
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹9.2
- en: where the summation is over all training data instances, and *n* is the size
    of the training data set. Note that this summation over all training data points
    is needed to compute the loss for each training data instance. Thus an *epoch*,
    a single training loop over all training data instances, costs *O*(*n*Â²), where
    *n* is the number of training data points. Training usually requires many epochs.
    This makes the training process very expensive. In section [9.2.2](../Text/09.xhtml#sec-SGD),
    we study ways of mitigating this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ±‚å’Œæ˜¯é’ˆå¯¹æ‰€æœ‰è®­ç»ƒæ•°æ®å®ä¾‹çš„ï¼Œ*n* æ˜¯è®­ç»ƒæ•°æ®é›†çš„å¤§å°ã€‚è¯·æ³¨æ„ï¼Œå¯¹æ‰€æœ‰è®­ç»ƒæ•°æ®ç‚¹çš„è¿™ç§æ±‚å’Œæ˜¯è®¡ç®—æ¯ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹æŸå¤±æ‰€å¿…éœ€çš„ã€‚å› æ­¤ï¼Œä¸€ä¸ª*epoch*ï¼Œå³å¯¹æ‰€æœ‰è®­ç»ƒæ•°æ®å®ä¾‹çš„å•æ¬¡è®­ç»ƒå¾ªç¯ï¼Œéœ€è¦
    *O*(*n*Â²) çš„æ—¶é—´å¤æ‚åº¦ï¼Œå…¶ä¸­ *n* æ˜¯è®­ç»ƒæ•°æ®ç‚¹çš„æ•°é‡ã€‚è®­ç»ƒé€šå¸¸éœ€è¦è®¸å¤šä¸ªepochã€‚è¿™ä½¿å¾—è®­ç»ƒè¿‡ç¨‹éå¸¸æ˜‚è´µã€‚åœ¨[9.2.2](../Text/09.xhtml#sec-SGD)èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å‡è½»è¿™ç§å½±å“çš„æ–¹æ³•ã€‚
- en: '![](../../OEBPS/Images/CH09_F01_Chaudhury.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F01_Chaudhury.png)'
- en: Figure 9.1 The loss surface can be viewed as a canyon.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.1 æŸå¤±è¡¨é¢å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªå³¡è°·ã€‚
- en: NOTE In this chapter, we use *n* to indicate the number of training data points
    and *N* to indicate the dimensionality of the output vector. For classifiers,
    the dimensionality of the output vector, *N*, matches the number of classes. We
    also use superscript (*i*) to index training data points and subscript *j* to
    index output vector dimensions. For classifiers, *j* indicates the class.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šåœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ç”¨*n*è¡¨ç¤ºè®­ç»ƒæ•°æ®ç‚¹çš„æ•°é‡ï¼Œç”¨*N*è¡¨ç¤ºè¾“å‡ºå‘é‡çš„ç»´åº¦ã€‚å¯¹äºåˆ†ç±»å™¨ï¼Œè¾“å‡ºå‘é‡çš„ç»´åº¦*N*ä¸ç±»åˆ«çš„æ•°é‡ç›¸åŒ¹é…ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨ä¸Šæ ‡(*i*)æ¥ç´¢å¼•è®­ç»ƒæ•°æ®ç‚¹ï¼Œä¸‹æ ‡*j*æ¥ç´¢å¼•è¾“å‡ºå‘é‡ç»´åº¦ã€‚å¯¹äºåˆ†ç±»å™¨ï¼Œ*j*è¡¨ç¤ºç±»åˆ«ã€‚
- en: We can visualize ğ•ƒ(![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png))
    as a hypersurface in high-dimensional space. Figures [8.8](../Text/08.xhtml#fig-gradient-descent-3d)
    and [8.9](../Text/08.xhtml#fig-non-convex-local-minima) show some low-dimensional
    examples of loss surfaces. These are illustrative examples. In reality, the loss
    surface is typically high dimensional and very complex. One good mental picture
    is that of a canyon (see figure [9.1](#fig-grand-canyon)). Traveling â€œdownhillâ€
    at any point effectively follows the negative direction of the local gradient
    the gradient of a loss surface was introduced in section [8.4.2](../Text/08.xhtml#sec-loss-surface-graddesc)).
    Traveling downhill along the gradient does not always lead to the global minimum.
    For instance, going downhill following the dashed arrow will take us to a local
    minimum, whereas the global minimum is where the water is going, indicated by
    the solid arrow. (See also section [8.4.4](../Text/08.xhtml#sec-graddesc_local_minima)
    and figure [8.9](../Text/08.xhtml#fig-non-convex-local-minima).)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°† ğ•ƒ(![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png))
    è§†ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„è¶…æ›²é¢ã€‚å›¾[8.8](../Text/08.xhtml#fig-gradient-descent-3d)å’Œ[8.9](../Text/08.xhtml#fig-non-convex-local-minima)å±•ç¤ºäº†æŸå¤±è¡¨é¢çš„ä½ç»´ç¤ºä¾‹ã€‚è¿™äº›éƒ½æ˜¯è¯´æ˜æ€§çš„ä¾‹å­ã€‚å®é™…ä¸Šï¼ŒæŸå¤±è¡¨é¢é€šå¸¸æ˜¯é«˜ç»´ä¸”éå¸¸å¤æ‚çš„ã€‚ä¸€ä¸ªå¾ˆå¥½çš„å¿ƒç†å›¾åƒæ˜¯å³¡è°·ï¼ˆè§å›¾[9.1](#fig-grand-canyon)ï¼‰ã€‚åœ¨ä»»ä½•ä¸€ç‚¹â€œå‘ä¸‹èµ°â€å®é™…ä¸Šéµå¾ªå±€éƒ¨æ¢¯åº¦çš„è´Ÿæ–¹å‘ï¼Œå³æŸå¤±è¡¨é¢çš„æ¢¯åº¦åœ¨[8.4.2](../Text/08.xhtml#sec-loss-surface-graddesc)èŠ‚ä¸­å¼•å…¥ã€‚æ²¿ç€æ¢¯åº¦å‘ä¸‹èµ°å¹¶ä¸æ€»æ˜¯å¯¼è‡´å…¨å±€æœ€å°å€¼ã€‚ä¾‹å¦‚ï¼Œæ²¿ç€è™šçº¿ç®­å¤´å‘ä¸‹èµ°å°†å¸¦æˆ‘ä»¬åˆ°ä¸€ä¸ªå±€éƒ¨æœ€å°å€¼ï¼Œè€Œå…¨å±€æœ€å°å€¼æ˜¯æ°´æµçš„æ–¹å‘ï¼Œç”±å®çº¿ç®­å¤´æŒ‡ç¤ºã€‚ï¼ˆä¹Ÿå‚è§[8.4.4](../Text/08.xhtml#sec-graddesc_local_minima)èŠ‚å’Œå›¾[8.9](../Text/08.xhtml#fig-non-convex-local-minima)ã€‚ï¼‰
- en: Many loss formulations are possible, quantifying the mismatch ğ•ƒ^((*i*))(![](../../OEBPS/Images/AR_y.png)^((*i*)),
    *È³*^((*i*))); some of them are described in the following subsections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šæŸå¤±å…¬å¼éƒ½æ˜¯å¯èƒ½çš„ï¼Œé‡åŒ–äº†ä¸åŒ¹é… ğ•ƒ^((*i*))(![](../../OEBPS/Images/AR_y.png)^((*i*)), *È³*^((*i*)));
    å…¶ä¸­ä¸€äº›å°†åœ¨ä»¥ä¸‹å°èŠ‚ä¸­æè¿°ã€‚
- en: 9.1.2 Regression loss
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 å›å½’æŸå¤±
- en: 'Regression loss is the simplest loss formulation. It is the L2 norm of the
    difference between the output and GT vectors. This loss was introduced in equation
    [8.11](../Text/08.xhtml#eq-mse-loss). We restate it here: the loss on the *i*th
    training data instance is'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å›å½’æŸå¤±æ˜¯æœ€ç®€å•çš„æŸå¤±å…¬å¼ã€‚å®ƒæ˜¯è¾“å‡ºå‘é‡å’ŒGTå‘é‡ä¹‹é—´å·®å¼‚çš„L2èŒƒæ•°ã€‚è¿™ä¸ªæŸå¤±åœ¨æ–¹ç¨‹[8.11](../Text/08.xhtml#eq-mse-loss)ä¸­å¼•å…¥ã€‚æˆ‘ä»¬åœ¨æ­¤é‡ç”³ï¼šç¬¬*i*ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹çš„æŸå¤±æ˜¯
- en: '![](../../OEBPS/Images/eq_09-02-a.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-02-a.png)'
- en: where the summation is over the components of the output vector. *N* is the
    number of classes. The GT vector and output vector are both *N*-dimensional.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ±‚å’Œæ˜¯å¯¹è¾“å‡ºå‘é‡çš„åˆ†é‡è¿›è¡Œçš„ã€‚*N*æ˜¯ç±»åˆ«çš„æ•°é‡ã€‚GTå‘é‡å’Œè¾“å‡ºå‘é‡éƒ½æ˜¯*N*-ç»´çš„ã€‚
- en: NOTE Fully functional code for regression loss, executable via Jupyter Notebook,
    can be found at [http://mng.bz/g1a8](http://mng.bz/g1a8).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå®Œæ•´çš„å›å½’æŸå¤±åŠŸèƒ½ä»£ç ï¼Œå¯é€šè¿‡Jupyter Notebookæ‰§è¡Œï¼Œå¯åœ¨[http://mng.bz/g1a8](http://mng.bz/g1a8)æ‰¾åˆ°ã€‚
- en: Listing 9.1 PyTorch code for regression loss
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨9.1 PyTorchä»£ç ç”¨äºå›å½’æŸå¤±
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: â‘  Imports the regression loss (mean squared error loss)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  å¯¼å…¥å›å½’æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®æŸå¤±ï¼‰
- en: â‘¡ N-d prediction vector
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ Nç»´é¢„æµ‹å‘é‡
- en: â‘¢ N-d ground truth vector
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¢ Nç»´çœŸå®å‘é‡
- en: â‘£ Computes the regression loss
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: â‘£ è®¡ç®—å›å½’æŸå¤±
- en: 9.1.3 Cross-entropy loss
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 äº¤å‰ç†µæŸå¤±
- en: Cross-entropy loss was discussed in the context of entropy in section [6.3](../Text/06.xhtml#sec-cross-entropy).
    If necessary, this would be a great time to reread that. Here we review the idea
    quickly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰ç†µæŸå¤±åœ¨[6.3](../Text/06.xhtml#sec-cross-entropy)èŠ‚ä¸­è®¨è®ºäº†ç†µçš„ä¸Šä¸‹æ–‡ã€‚å¦‚æœéœ€è¦ï¼Œç°åœ¨é‡æ–°é˜…è¯»é‚£éƒ¨åˆ†æ˜¯ä¸ªå¥½æ—¶æœºã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹è¿™ä¸ªæƒ³æ³•ã€‚
- en: Cross-entropy loss is typically used to measure the mismatch between a classifier
    neural network output and the corresponding GT in a classification problem. Here,
    the GT is a one-hot vector whose length equals the number of classes. All but
    one of its elements are 0\. The single nonzero element is 1, and it occurs at
    the index corresponding to the correct class for that training data instance.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰ç†µæŸå¤±é€šå¸¸ç”¨äºè¡¡é‡åˆ†ç±»é—®é¢˜ä¸­åˆ†ç±»å™¨ç¥ç»ç½‘ç»œè¾“å‡ºä¸ç›¸åº” GT ä¹‹é—´çš„ä¸åŒ¹é…ã€‚åœ¨è¿™é‡Œï¼ŒGT æ˜¯ä¸€ä¸ªé•¿åº¦ç­‰äºç±»æ•°çš„ one-hot å‘é‡ã€‚é™¤äº†ä¸€ä¸ªå…ƒç´ å¤–ï¼Œå…¶ä½™å…ƒç´ éƒ½æ˜¯
    0ã€‚å”¯ä¸€çš„éé›¶å…ƒç´ æ˜¯ 1ï¼Œå®ƒå‡ºç°åœ¨å¯¹åº”äºè¯¥è®­ç»ƒæ•°æ®å®ä¾‹æ­£ç¡®ç±»åˆ«çš„ç´¢å¼•å¤„ã€‚
- en: Thus the GT vector looks like *È³*^((*i*)) = [0,â€¦,0,1,0,â€¦,0]. The prediction
    vector should have elements with values between 0 and 1. Each element of the prediction
    vector ![](../../OEBPS/Images/AR_y.png)^((*i*)) indicates the probability of a
    specific class. In other words, ![](../../OEBPS/Images/AR_y.png)^((*i*)) = [*p*[0],
    *p*[1],â€¦, *p*[*N*âˆ’1]], where *p[j]* is the probability of the input *i* belonging
    to the *j*th class. In section [6.3](../Text/06.xhtml#sec-cross-entropy), we illustrated
    with an example image classifier that predicts whether an image contains a cat
    class 0), a dog (class 1), an airplane (class 2), or an automobile (class 3).
    One of the four is always assumed to be present in the image. If, for the *i*th
    training data instance, the GT vector is an image of cat, we have *È³*^((*i*))
    = [1,0,0,0]. A prediction vector ![](../../OEBPS/Images/AR_y.png)^((*i*)) = [0.8,0.15,0.04,0.01]
    is good, while ![](../../OEBPS/Images/AR_y.png)^((*i*)) = [0.25,0.25,0.25,0.25]
    is bad. Note that sum of the elements of the GT as well as the prediction vector
    is always 1 since they are probabilities. Mathematically, given a training dataset
    *X*,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒGT å‘é‡çœ‹èµ·æ¥åƒ *È³*^((*i*)) = [0,â€¦,0,1,0,â€¦,0]ã€‚é¢„æµ‹å‘é‡åº”è¯¥å…·æœ‰ä»‹äº 0 å’Œ 1 ä¹‹é—´çš„å€¼ã€‚é¢„æµ‹å‘é‡ ![](../../OEBPS/Images/AR_y.png)^((*i*))
    çš„æ¯ä¸ªå…ƒç´ è¡¨ç¤ºç‰¹å®šç±»çš„æ¦‚ç‡ã€‚æ¢å¥è¯è¯´ï¼Œ![](../../OEBPS/Images/AR_y.png)^((*i*)) = [*p*[0], *p*[1],â€¦,
    *p*[*N*âˆ’1]]ï¼Œå…¶ä¸­ *p[j]* æ˜¯è¾“å…¥ *i* å±äºç¬¬ *j* ç±»çš„æ¦‚ç‡ã€‚åœ¨ç¬¬ [6.3](../Text/06.xhtml#sec-cross-entropy)
    èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªç¤ºä¾‹å›¾åƒåˆ†ç±»å™¨æ¥è¯´æ˜ï¼Œè¯¥åˆ†ç±»å™¨é¢„æµ‹å›¾åƒæ˜¯å¦åŒ…å«çŒ«ï¼ˆç±»åˆ« 0ï¼‰ã€ç‹—ï¼ˆç±»åˆ« 1ï¼‰ã€é£æœºï¼ˆç±»åˆ« 2ï¼‰æˆ–æ±½è½¦ï¼ˆç±»åˆ« 3ï¼‰ã€‚å››ä¸ªç±»åˆ«ä¸­æ€»æœ‰ä¸€ä¸ªå‡è®¾å­˜åœ¨äºå›¾åƒä¸­ã€‚å¦‚æœå¯¹äºç¬¬
    *i* ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹ï¼ŒGT å‘é‡æ˜¯ä¸€å¼ çŒ«çš„å›¾ç‰‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æœ‰ *È³*^((*i*)) = [1,0,0,0]ã€‚é¢„æµ‹å‘é‡ ![](../../OEBPS/Images/AR_y.png)^((*i*))
    = [0.8,0.15,0.04,0.01] æ˜¯å¥½çš„ï¼Œè€Œ ![](../../OEBPS/Images/AR_y.png)^((*i*)) = [0.25,0.25,0.25,0.25]
    æ˜¯ä¸å¥½çš„ã€‚è¯·æ³¨æ„ï¼ŒGT ä»¥åŠé¢„æµ‹å‘é‡çš„å…ƒç´ ä¹‹å’Œæ€»æ˜¯ 1ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æ¦‚ç‡ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›† *X*ï¼Œ
- en: '![](../../OEBPS/Images/eq_09-02-b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![äº¤å‰ç†µæŸå¤±å…¬å¼](../../OEBPS/Images/eq_09-02-b.png)'
- en: Given such GT and prediction vectors, the cross-entropy loss (CE loss) is
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šè¿™æ ·çš„çœŸå®æ ‡ç­¾ GT å’Œé¢„æµ‹å‘é‡ï¼Œäº¤å‰ç†µæŸå¤±ï¼ˆCE æŸå¤±ï¼‰æ˜¯
- en: '![](../../OEBPS/Images/eq_09-03.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![äº¤å‰ç†µæŸå¤±å…¬å¼](../../OEBPS/Images/eq_09-03.png)'
- en: Equation 9.3
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 9.3
- en: where the summation is over the elements of the prediction vector and *N* is
    the number of classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ±‚å’Œæ˜¯å¯¹é¢„æµ‹å‘é‡çš„å…ƒç´ è¿›è¡Œçš„ï¼Œ*N* æ˜¯ç±»æ•°ã€‚
- en: Intuitions behind cross-entropy loss
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰ç†µæŸå¤±èƒŒåçš„ç›´è§‰
- en: Note that only one elementâ€”the one corresponding to the GT classâ€”survives in
    the summation of equation [9.3](#eq-ce-loss). The other elements vanish because
    they are multiplied by the 0 GT value. The (logarithm of) the predicted probability
    of the correct GT class is multiplied by 1. Hence, the CE loss always boils down
    to âˆ’*log*(*y*[*j*^*]^((*i*))), where *j*^* is the GT class. If this probability
    is 1, the CE loss becomes 0, rightly so, as the correct class is being predicted
    with a probability of 1. If the predicted probability of the correct class is
    0, the CE loss is âˆ’*log*(0) = âˆ, again rightly so, since this is the worst possible
    prediction. The closer the prediction for the correct class is to 1, the smaller
    the loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæ–¹ç¨‹ [9.3](#eq-ce-loss) çš„æ±‚å’Œä¸­åªä¿ç•™äº†ä¸€ä¸ªå…ƒç´ â€”â€”å¯¹åº”äº GT ç±»åˆ«çš„å…ƒç´ ã€‚å…¶ä»–å…ƒç´ æ¶ˆå¤±ï¼Œå› ä¸ºå®ƒä»¬è¢«ä¹˜ä»¥ 0 çš„ GT å€¼ã€‚æ­£ç¡®çš„
    GT ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ï¼ˆå¯¹æ•°ï¼‰ä¹˜ä»¥ 1ã€‚å› æ­¤ï¼ŒCE æŸå¤±å§‹ç»ˆç®€åŒ–ä¸º âˆ’*log*(*y*[*j*^*]^((*i*)))ï¼Œå…¶ä¸­ *j*^* æ˜¯ GT ç±»åˆ«ã€‚å¦‚æœè¿™ä¸ªæ¦‚ç‡æ˜¯
    1ï¼Œé‚£ä¹ˆ CE æŸå¤±å˜ä¸º 0ï¼Œè¿™æ˜¯æ­£ç¡®çš„ï¼Œå› ä¸ºä»¥ 1 çš„æ¦‚ç‡é¢„æµ‹äº†æ­£ç¡®çš„ç±»åˆ«ã€‚å¦‚æœæ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡æ˜¯ 0ï¼Œé‚£ä¹ˆ CE æŸå¤±æ˜¯ âˆ’*log*(0) = âˆï¼Œè¿™ä¹Ÿæ˜¯æ­£ç¡®çš„ï¼Œå› ä¸ºè¿™æ˜¯æœ€ç³Ÿç³•çš„é¢„æµ‹ã€‚é¢„æµ‹å¯¹æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹è¶Šæ¥è¿‘
    1ï¼ŒæŸå¤±å°±è¶Šå°ã€‚
- en: NOTE Fully functional code for cross-entropy loss, executable via Jupyter Notebook,
    can be found at [http://mng.bz/g1a8](http://mng.bz/g1a8).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå®Œæ•´çš„äº¤å‰ç†µæŸå¤±ä»£ç ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯ä»¥åœ¨ [http://mng.bz/g1a8](http://mng.bz/g1a8)
    æ‰¾åˆ°ã€‚
- en: Listing 9.2 PyTorch code for cross-entropy loss
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 9.2 PyTorch äº¤å‰ç†µæŸå¤±ä»£ç 
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: â‘  N-d prediction vector
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  N ç»´é¢„æµ‹å‘é‡
- en: â‘¡ N-d one-hot ground truth vector
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ N ç»´ one-hot çœŸå®æ ‡ç­¾å‘é‡
- en: â‘¢ Computes the cross-entropy loss
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¢ è®¡ç®—äº¤å‰ç†µæŸå¤±
- en: Special case of two classes
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªç±»åˆ«çš„ç‰¹æ®Šæƒ…å†µ
- en: 'What happens if *N* = 2 (that is, we have only two classes)? Letâ€™s denote the
    predicted probability of class 0, for the *i*th training input, as *y*^((*i*)):
    that is, ![](../../OEBPS/Images/AR_y.png)[0]^((*i*)) = *y*^((*i*)). Then, since
    these are probabilities, the prediction on the other class ![](../../OEBPS/Images/AR_y.png)[1]^((*i*))
    = 1 âˆ’ *y*^((*i*)). Also, let *È³*^((*i*)) denote the GT probability for class 0
    on this *i*th training input. Then 1 âˆ’ *È³*^((*i*)) is the GT probability for class
    1. (We have slightly abused notationsâ€”up to this point, *È³* has denoted a vector,
    but here it denotes a scalar.)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ*N* = 2ï¼ˆå³æˆ‘ä»¬åªæœ‰ä¸¤ä¸ªç±»åˆ«ï¼‰ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬ç”¨*y*^((*i*))è¡¨ç¤ºç¬¬*i*ä¸ªè®­ç»ƒè¾“å…¥çš„ç±»åˆ«0çš„é¢„æµ‹æ¦‚ç‡ï¼šå³![](../../OEBPS/Images/AR_y.png)[0]^((*i*))
    = *y*^((*i*)ï¼‰ã€‚ç”±äºè¿™äº›æ˜¯æ¦‚ç‡ï¼Œå¯¹å¦ä¸€ä¸ªç±»åˆ«çš„é¢„æµ‹![](../../OEBPS/Images/AR_y.png)[1]^((*i*)) = 1
    âˆ’ *y*^((*i*)ï¼‰ã€‚æ­¤å¤–ï¼Œç”¨*È³*^((*i*))è¡¨ç¤ºç¬¬*i*ä¸ªè®­ç»ƒè¾“å…¥çš„ç±»åˆ«0çš„GTæ¦‚ç‡ã€‚é‚£ä¹ˆ1 âˆ’ *È³*^((*i*))æ˜¯ç±»åˆ«1çš„GTæ¦‚ç‡ã€‚ï¼ˆæˆ‘ä»¬ç¨å¾®æ»¥ç”¨äº†ç¬¦å·â€”â€”åˆ°ç›®å‰ä¸ºæ­¢ï¼Œ*È³*è¡¨ç¤ºä¸€ä¸ªå‘é‡ï¼Œä½†åœ¨è¿™é‡Œå®ƒè¡¨ç¤ºä¸€ä¸ªæ ‡é‡ã€‚ï¼‰
- en: Then, following equation [9.3](#eq-ce-loss), the CE loss on the *i*th training
    data instance becomes
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ ¹æ®æ–¹ç¨‹å¼[9.3](#eq-ce-loss)ï¼Œç¬¬*i*ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹çš„CEæŸå¤±å˜ä¸º
- en: ğ•ƒ^((*i*))(*y*^((*i*)), *È³*^((*i*))) = âˆ’*È³*^((*i*)) *log*(*y*^((*i*))) âˆ’ (1âˆ’*È³*^((*i*)))
    *log*(1âˆ’*y*^((*i*)))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ğ•ƒ^((*i*))(*y*^((*i*)), *È³*^((*i*))) = âˆ’*È³*^((*i*)) *log*(*y*^((*i*))) âˆ’ (1âˆ’*È³*^((*i*)))
    *log*(1âˆ’*y*^((*i*)))
- en: Equation 9.4
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼9.4
- en: NOTE Fully functional code for binary cross-entropy loss, executable via Jupyter
    Notebook, can be found at [http://mng.bz/g1a8](http://mng.bz/g1a8).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šäºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±çš„å®Œæ•´ä»£ç ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯åœ¨[http://mng.bz/g1a8](http://mng.bz/g1a8)æ‰¾åˆ°ã€‚
- en: Listing 9.3 PyTorch code for binary cross-entropy loss
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨9.3 PyTorchä»£ç ç”¨äºäºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: â‘  Imports the binary cross-entropy loss
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  å¯¼å…¥äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±
- en: â‘¡ Outputs the probability of class 0 - *y*[0]. A single value is sufficient
    because *y*[1] = 1 âˆ’ *y*[0].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ è¾“å‡ºç±»åˆ«0çš„æ¦‚ç‡ - *y*[0]ã€‚å•ä¸ªå€¼å°±è¶³å¤Ÿäº†ï¼Œå› ä¸º *y*[1] = 1 âˆ’ *y*[0]ã€‚
- en: â‘¢ The ground truth is either 0 or 1.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¢ çœŸå®å€¼æ˜¯0æˆ–1ã€‚
- en: â‘£ Computes the cross-entropy loss
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: â‘£ è®¡ç®—äº¤å‰ç†µæŸå¤±
- en: 9.1.4 Binary cross-entropy loss for image and vector mismatches
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.4 å›¾åƒå’Œå‘é‡ä¸åŒ¹é…çš„äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±
- en: Given a pair of normalized tensors (such as images or vectors) whose elements
    all have values between 0 and 1, a variant of the two-class CE loss can be used
    to estimate the mismatch between the tensors. Note that an image with pixel-intensity
    values between 0 and 255 can always be normalized by dividing each pixel-intensity
    value by 255, thereby converting it to the 0 to 1 range. Such a comparison of
    two images is used in image autoencoders, for example. We study autoencoders later;
    here, we provide a brief overview in the following sidebar.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€å¯¹å½’ä¸€åŒ–çš„å¼ é‡ï¼ˆä¾‹å¦‚å›¾åƒæˆ–å‘é‡ï¼‰ï¼Œå…¶å…ƒç´ å€¼å‡åœ¨0åˆ°1ä¹‹é—´ï¼Œå¯ä»¥ä½¿ç”¨ä¸¤åˆ†ç±»CEæŸå¤±çš„ä¸€ä¸ªå˜ä½“æ¥ä¼°è®¡å¼ é‡ä¹‹é—´çš„ä¸åŒ¹é…ã€‚è¯·æ³¨æ„ï¼Œåƒç´ å¼ºåº¦å€¼åœ¨0åˆ°255ä¹‹é—´çš„å›¾åƒå¯ä»¥é€šè¿‡å°†æ¯ä¸ªåƒç´ å¼ºåº¦å€¼é™¤ä»¥255æ¥å½’ä¸€åŒ–ï¼Œä»è€Œå°†å…¶è½¬æ¢ä¸º0åˆ°1çš„èŒƒå›´ã€‚è¿™ç§ä¸¤ç§å›¾åƒçš„æ¯”è¾ƒåœ¨å›¾åƒè‡ªç¼–ç å™¨ä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œä¾‹å¦‚ã€‚æˆ‘ä»¬ç¨åç ”ç©¶è‡ªç¼–ç å™¨ï¼›åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†åœ¨ä»¥ä¸‹ä¾§è¾¹æ ä¸­æä¾›ç®€è¦æ¦‚è¿°ã€‚
- en: Autoencoders
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨
- en: 'Autoencoders take an image as input, create a low-dimensional descriptor from
    the imageâ€”this descriptor is often referred to as an *embedding* of the imageâ€”and
    try to reconstruct the input image from the embedding. The image embedding is
    a compressed representation of the image. Reconstruction is a lossy process: the
    small, subtle variations in the signal are lost, and only the essential part is
    retained. The loss is the mismatch between the input image and the reconstructed
    image. By minimizing this loss, we incentivize the system to retain the essence
    of the input as much as possible within the embedding-size budget.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨ä»¥å›¾åƒä¸ºè¾“å…¥ï¼Œä»å›¾åƒä¸­åˆ›å»ºä¸€ä¸ªä½ç»´æè¿°ç¬¦â€”â€”è¿™ä¸ªæè¿°ç¬¦é€šå¸¸è¢«ç§°ä¸ºå›¾åƒçš„*åµŒå…¥*â€”â€”å¹¶å°è¯•ä»åµŒå…¥ä¸­é‡å»ºè¾“å…¥å›¾åƒã€‚å›¾åƒåµŒå…¥æ˜¯å›¾åƒçš„å‹ç¼©è¡¨ç¤ºã€‚é‡å»ºæ˜¯ä¸€ä¸ªæœ‰æŸè¿‡ç¨‹ï¼šä¿¡å·ä¸­çš„å¾®å°ã€ç»†å¾®çš„å˜åŒ–ä¼šä¸¢å¤±ï¼Œåªä¿ç•™åŸºæœ¬éƒ¨åˆ†ã€‚æŸå¤±æ˜¯è¾“å…¥å›¾åƒå’Œé‡å»ºå›¾åƒä¹‹é—´çš„ä¸åŒ¹é…ã€‚é€šè¿‡æœ€å°åŒ–è¿™ä¸ªæŸå¤±ï¼Œæˆ‘ä»¬æ¿€åŠ±ç³»ç»Ÿåœ¨åµŒå…¥å¤§å°é¢„ç®—å†…å°½å¯èƒ½å¤šåœ°ä¿ç•™è¾“å…¥çš„æœ¬è´¨ã€‚
- en: Let *È³* denote the input image. Let ![](../../OEBPS/Images/AR_y.png)^((*i*))
    denote the reconstructed image outputted by the autoencoder. The binary cross-entropy
    loss is defined as
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨*È³*è¡¨ç¤ºè¾“å…¥å›¾åƒã€‚ç”¨![](../../OEBPS/Images/AR_y.png)^((*i*))è¡¨ç¤ºç”±è‡ªç¼–ç å™¨è¾“å‡ºçš„é‡å»ºå›¾åƒã€‚äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±å®šä¹‰ä¸º
- en: '![](../../OEBPS/Images/eq_09-05.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-05.png)'
- en: Equation 9.5
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼9.5
- en: Note that here *N* is the number of pixels in the image, not the number of classes,
    as before. The summation is over the pixels in the image. Also, the GT vector
    *È³*^((*i*)) is not a one-hot vector; rather, it is the input image. These differences
    aside, equation [9.5](#eq-pixel-ce-loss) is based on the same idea as equation
    [9.4](#eq-binary-ce-loss).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œåœ¨è¿™é‡Œ *N* æ˜¯å›¾åƒä¸­çš„åƒç´ æ•°ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„ç±»åˆ«æ•°ã€‚æ±‚å’Œæ˜¯é’ˆå¯¹å›¾åƒä¸­çš„åƒç´ ã€‚æ­¤å¤–ï¼ŒGT å‘é‡ *È³*^((*i*)) ä¸æ˜¯ä¸€ä¸ª one-hot
    å‘é‡ï¼›è€Œæ˜¯è¾“å…¥å›¾åƒã€‚å°½ç®¡æœ‰è¿™äº›å·®å¼‚ï¼Œæ–¹ç¨‹ [9.5](#eq-pixel-ce-loss) çš„æ€æƒ³ä¸æ–¹ç¨‹ [9.4](#eq-binary-ce-loss)
    ç›¸åŒã€‚
- en: Why does it work?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå®ƒæœ‰æ•ˆï¼Ÿ
- en: Binary cross-entropy loss attains its minimum when the input matches the GT.
    We outline the proof next. (Note that we drop the superscripts and subscripts
    for simplicity.) We have
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¾“å…¥ä¸ GT åŒ¹é…æ—¶ï¼ŒäºŒå…ƒäº¤å‰ç†µæŸå¤±è¾¾åˆ°æœ€å°å€¼ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢æ¦‚è¿°è¯æ˜ã€‚ï¼ˆæ³¨æ„ï¼Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬çœç•¥äº†ä¸Šæ ‡å’Œä¸‹æ ‡ã€‚ï¼‰æˆ‘ä»¬æœ‰
- en: âˆ’ğ•ƒ = *È³log*(*y*) + (1 âˆ’Â *È³*)*log*(1 âˆ’Â *y*)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ’ğ•ƒ = *È³log*(*y*) + (1 âˆ’Â *È³*)*log*(1 âˆ’Â *y*)
- en: At the minimum, *âˆ‚*ğ•ƒ**/***âˆ‚y* = 0.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€å°å€¼å¤„ï¼Œ*âˆ‚*ğ•ƒ**/***âˆ‚y* = 0ã€‚
- en: '![](../../OEBPS/Images/eq_09-05-a.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-05-a.png)'
- en: Thus, the minimum of the binary cross-entropy loss occurs when the network output
    matches the GT. However, this does not mean this loss becomes zero when the output
    matches the GT.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒäºŒå…ƒäº¤å‰ç†µæŸå¤±çš„æœ€å°å€¼å‘ç”Ÿåœ¨ç½‘ç»œè¾“å‡ºä¸ GT åŒ¹é…æ—¶ã€‚ä½†è¿™å¹¶ä¸æ„å‘³ç€å½“è¾“å‡ºä¸ GT åŒ¹é…æ—¶ï¼Œè¿™ç§æŸå¤±å˜ä¸ºé›¶ã€‚
- en: NOTE Binary cross-entropy loss is not necessarily zero even in the ideal case
    of the output matching the input (although the loss is indeed *minimal* in the
    ideal case, meaning the loss is higher for non-ideal cases with mismatched input
    and output).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå³ä½¿åœ¨è¾“å‡ºä¸è¾“å…¥åŒ¹é…çš„ç†æƒ³æƒ…å†µä¸‹ï¼ŒäºŒå…ƒäº¤å‰ç†µæŸå¤±ä¹Ÿä¸ä¸€å®šæ˜¯é›¶ï¼ˆå°½ç®¡åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼ŒæŸå¤±ç¡®å®æ˜¯ *æœ€å°* çš„ï¼Œè¿™æ„å‘³ç€å¯¹äºè¾“å…¥å’Œè¾“å‡ºä¸åŒ¹é…çš„éç†æƒ³æƒ…å†µï¼ŒæŸå¤±æ›´é«˜ï¼‰ã€‚
- en: Examining equation [9.5](#eq-pixel-ce-loss), when the two inputs match, we have
    âˆ’ğ•ƒ(*È³*, ![](../../OEBPS/Images/AR_y.png))|[![](../../OEBPS/Images/AR_y.png) =
    *È³*]. We intuitively expect this loss to be zero since the output is ideal. But
    it isnâ€™t. For example, if, for *È³[j]*^((*i*)) = *y[j]*^((*i*)) = 0.25
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ–¹ç¨‹ [9.5](#eq-pixel-ce-loss)ï¼Œå½“ä¸¤ä¸ªè¾“å…¥åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬æœ‰ âˆ’ğ•ƒ(*È³*, ![](../../OEBPS/Images/AR_y.png))|[![](../../OEBPS/Images/AR_y.png)
    = *È³*]ã€‚æˆ‘ä»¬ç›´è§‚åœ°æœŸæœ›è¿™ç§æŸå¤±ä¸ºé›¶ï¼Œå› ä¸ºè¾“å‡ºæ˜¯ç†æƒ³çš„ã€‚ä½†äº‹å®å¹¶éå¦‚æ­¤ã€‚ä¾‹å¦‚ï¼Œå¦‚æœï¼Œå¯¹äº *È³[j]*^((*i*)) = *y[j]*^((*i*))
    = 0.25
- en: '![](../../OEBPS/Images/eq_09-05-b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-05-b.png)'
- en: In fact, the binary cross-entropy loss is zero only in special cases, like *y[j]*^((*i*))
    = *È³[j]*^((*i*)) = 1.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼ŒäºŒå…ƒäº¤å‰ç†µæŸå¤±ä»…åœ¨ç‰¹æ®Šæƒ…å†µä¸‹ä¸ºé›¶ï¼Œä¾‹å¦‚ *y[j]*^((*i*)) = *È³[j]*^((*i*)) = 1ã€‚
- en: 9.1.5 Softmax
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.5 Softmax
- en: 'Suppose we are building a classifier: for instance, the image classifier we
    illustrated in section [6.3](../Text/06.xhtml#sec-cross-entropy), which predicts
    whether an image contains a cat (class 0), a dog (class 1), and airplane (class
    2), or an automobile (class 3). Our classifier can emit a score vector ![](../../OEBPS/Images/AR_s.png)
    corresponding to an input image. Element *j* of the score vector corresponds to
    the *j[th]* class. We take the max of the score vector and call that the neural
    network-predicted label for the image. For instance, in the example image classifier,
    a score vector may be [9.99 Â Â 10 Â Â 0.01 Â Â -10]. Since the highest score occurs
    at index 1, we conclude that the image contains a dog class 1).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªåˆ†ç±»å™¨ï¼šä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨ç¬¬ [6.3](../Text/06.xhtml#sec-cross-entropy) èŠ‚ä¸­å±•ç¤ºçš„å›¾åƒåˆ†ç±»å™¨ï¼Œå®ƒé¢„æµ‹å›¾åƒæ˜¯å¦åŒ…å«çŒ«ï¼ˆç±»åˆ«
    0ï¼‰ã€ç‹—ï¼ˆç±»åˆ« 1ï¼‰ã€é£æœºï¼ˆç±»åˆ« 2ï¼‰æˆ–æ±½è½¦ï¼ˆç±»åˆ« 3ï¼‰ã€‚æˆ‘ä»¬çš„åˆ†ç±»å™¨å¯ä»¥å‘å‡ºä¸€ä¸ªä¸è¾“å…¥å›¾åƒå¯¹åº”çš„åˆ†æ•°å‘é‡ ![](../../OEBPS/Images/AR_s.png)ã€‚åˆ†æ•°å‘é‡ä¸­çš„å…ƒç´ 
    *j* å¯¹åº”äº *j[th]* ç±»ã€‚æˆ‘ä»¬å–åˆ†æ•°å‘é‡çš„æœ€å¤§å€¼ï¼Œå¹¶å°†å…¶ç§°ä¸ºç¥ç»ç½‘ç»œé¢„æµ‹çš„å›¾åƒæ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¤ºä¾‹å›¾åƒåˆ†ç±»å™¨ä¸­ï¼Œä¸€ä¸ªåˆ†æ•°å‘é‡å¯èƒ½æ˜¯ [9.99 Â Â 10
    Â Â 0.01 Â Â -10]ã€‚ç”±äºæœ€é«˜åˆ†æ•°å‡ºç°åœ¨ç´¢å¼• 1 å¤„ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œè¯¥å›¾åƒåŒ…å«ç‹—ç±»åˆ« 1ï¼‰ã€‚
- en: The scores are unbounded; they can be any real number in the range [âˆ’âˆ,âˆ]. In
    general, however, neural networks behave better when the loss function involves
    a bounded set of numbers in the same range. The training converges faster and
    to better minima, and the inferencing is more accurate. Consequently, it is desirable
    to convert the example scores to probabilities. These will be numbers in the range
    [0,1] (and the elements of the vector will sum to 1).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æ•°æ˜¯æ— ç•Œçš„ï¼›å®ƒä»¬å¯ä»¥æ˜¯èŒƒå›´ [âˆ’âˆ,âˆ] å†…çš„ä»»ä½•å®æ•°ã€‚ç„¶è€Œï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œå½“æŸå¤±å‡½æ•°æ¶‰åŠåŒä¸€èŒƒå›´å†…çš„æœ‰ç•Œæ•°å­—é›†æ—¶ï¼Œç¥ç»ç½‘ç»œè¡¨ç°æ›´å¥½ã€‚è®­ç»ƒä¼šæ›´å¿«åœ°æ”¶æ•›åˆ°æ›´å¥½çš„æœ€å°å€¼ï¼Œæ¨ç†ä¹Ÿæ›´å‡†ç¡®ã€‚å› æ­¤ï¼Œå°†ç¤ºä¾‹åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡æ˜¯å¯å–çš„ã€‚è¿™äº›æ•°å­—å°†åœ¨èŒƒå›´
    [0,1] å†…ï¼ˆå¹¶ä¸”å‘é‡çš„å…ƒç´ ä¹‹å’Œä¸º 1ï¼‰ã€‚
- en: The softmax function converts unbounded scores to probabilities. Given a score
    vector ![](../../OEBPS/Images/AR_s.png) = [*s*[0] Â Â *s*[1] Â Â *s*[2]Â Â â€¦Â Â *s*[*N*-1]],
    the corresponding softmax vector is
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax å‡½æ•°å°†æ— ç•Œåˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ã€‚ç»™å®šä¸€ä¸ªåˆ†æ•°å‘é‡ ![](../../OEBPS/Images/AR_s.png) = [*s*[0] Â Â *s*[1]
    Â Â *s*[2]Â Â â€¦Â Â *s*[*N*-1]]ï¼Œç›¸åº”çš„ softmax å‘é‡æ˜¯
- en: '![](../../OEBPS/Images/eq_09-06.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-06.png)'
- en: Equation 9.6
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 9.6
- en: 'A few noteworthy points:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¸ªå€¼å¾—æ³¨æ„çš„ç‚¹ï¼š
- en: The vector has as many elements as possible classes.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘é‡å…·æœ‰å°½å¯èƒ½å¤šçš„ç±»åˆ«å…ƒç´ ã€‚
- en: The sum of the elements in the previous vector is 1.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸Šä¸€ä¸ªå‘é‡ä¸­å…ƒç´ çš„æ€»å’Œä¸º 1ã€‚
- en: The *j*th element of the vector represents the predicted probability of class
    *j*.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘é‡çš„ç¬¬ *j* ä¸ªå…ƒç´ ä»£è¡¨ç±»åˆ« *j* çš„é¢„æµ‹æ¦‚ç‡ã€‚
- en: The formulation can handle arbitrary scores, including negative ones.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥å…¬å¼å¯ä»¥å¤„ç†ä»»æ„å¾—åˆ†ï¼ŒåŒ…æ‹¬è´Ÿå¾—åˆ†ã€‚
- en: So in our example classification problem with the four classes (cat, dog, airplane,
    automobile), the score vector
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„è¿™ä¸ªåŒ…å«å››ä¸ªç±»åˆ«ï¼ˆçŒ«ã€ç‹—ã€é£æœºã€æ±½è½¦ï¼‰çš„ç¤ºä¾‹åˆ†ç±»é—®é¢˜ä¸­ï¼Œå¾—åˆ†å‘é‡
- en: '![](../../OEBPS/Images/AR_s.png) = [9.99 Â Â 10 Â Â 0.01 Â Â â€“10]'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_s.png) = [9.99 Â Â 10 Â Â 0.01 Â Â â€“10]'
- en: will yield the softmax vector
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å°†äº§ç”Ÿ softmax å‘é‡
- en: '*softmax* (![](../../OEBPS/Images/AR_s.png)) = [0.497 Â Â 0.502 Â Â 2.30eâ€“5 Â Â 1.04eâ€“9].'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*softmax* (![](../../OEBPS/Images/AR_s.png)) = [0.497 Â Â 0.502 Â Â 2.30eâ€“5 Â Â 1.04eâ€“9].'
- en: 'The probability of cat is 0.497, and the probability of dog is slightly higher,
    0.502. The probabilities of airplane and automobile are much lower: the neural
    network predicts that the image is that of a dog, but it is not very confident;
    it could also be a cat.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: çŒ«çš„æ¦‚ç‡æ˜¯ 0.497ï¼Œç‹—çš„æ¦‚ç‡ç•¥é«˜ï¼Œä¸º 0.502ã€‚é£æœºå’Œæ±½è½¦çš„æ¦‚ç‡è¦ä½å¾—å¤šï¼šç¥ç»ç½‘ç»œé¢„æµ‹å›¾åƒæ˜¯ç‹—çš„å›¾åƒï¼Œä½†å®ƒå¹¶ä¸ååˆ†è‡ªä¿¡ï¼›å®ƒä¹Ÿå¯èƒ½æ˜¯çŒ«çš„ã€‚
- en: Why the name softmax?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå« softmaxï¼Ÿ
- en: 'The softmax function is a smooth (differentiable) approximation to the argmaxonehot
    function, which emits a one-hot vector corresponding to the index of the max score.
    The argmaxonehot function is *discontinuous*. To see this, consider a pair of
    two class score vectors:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax å‡½æ•°æ˜¯ argmaxonehot å‡½æ•°çš„å¹³æ»‘ï¼ˆå¯å¾®ï¼‰è¿‘ä¼¼ï¼Œå®ƒè¾“å‡ºä¸€ä¸ªå¯¹åº”äºæœ€å¤§å¾—åˆ†ç´¢å¼•çš„ one-hot å‘é‡ã€‚argmaxonehot
    å‡½æ•°æ˜¯ *ä¸è¿ç»­çš„*ã€‚ä¸ºäº†çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œè€ƒè™‘ä¸€å¯¹äºŒç±»å¾—åˆ†å‘é‡ï¼š
- en: '![](../../OEBPS/Images/eq_09-06-a.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-06-a.png)'
- en: 'Performing an argmaxonehot operation on them will yield the following one-hot
    vectors, respectively:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å…¶æ‰§è¡Œ argmaxonehot æ“ä½œå°†åˆ†åˆ«äº§ç”Ÿä»¥ä¸‹ one-hot å‘é‡ï¼š
- en: '![](../../OEBPS/Images/eq_09-06-b.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-06-b.png)'
- en: Thus we see that the vectors *argmaxonehot*(![](../../OEBPS/Images/AR_p.png))
    and *argmaxonehot*(![](../../OEBPS/Images/AR_q.png)) are significantly far from
    each other, even though the points ![](../../OEBPS/Images/AR_p.png) and ![](../../OEBPS/Images/AR_q.png)
    are very close to each other. On the other hand, the corresponding softmax vectors
    are
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤æˆ‘ä»¬çœ‹åˆ°ï¼Œå‘é‡ *argmaxonehot*(![](../../OEBPS/Images/AR_p.png)) å’Œ *argmaxonehot*(![](../../OEBPS/Images/AR_q.png))
    ç›¸å¯¹äºå½¼æ­¤éå¸¸é¥è¿œï¼Œå°½ç®¡ç‚¹ ![](../../OEBPS/Images/AR_p.png) å’Œ ![](../../OEBPS/Images/AR_q.png)
    éå¸¸æ¥è¿‘ã€‚å¦ä¸€æ–¹é¢ï¼Œç›¸åº”çš„ softmax å‘é‡æ˜¯
- en: '![](../../OEBPS/Images/eq_09-06-c.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_09-06-c.png)'
- en: Although the predicted classes still match those from the argmaxonehot vector,
    the softmax vectors are very close to each other. The closer the scores, the closer
    the softmax probabilities. In other words, the softmax is continuous.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶é¢„æµ‹çš„ç±»åˆ«ä»ç„¶ä¸ argmaxonehot å‘é‡ä¸­çš„é‚£äº›åŒ¹é…ï¼Œä½† softmax å‘é‡å½¼æ­¤éå¸¸æ¥è¿‘ã€‚å¾—åˆ†è¶Šæ¥è¿‘ï¼Œsoftmax æ¦‚ç‡ä¹Ÿè¶Šæ¥è¿‘ã€‚æ¢å¥è¯è¯´ï¼Œsoftmax
    æ˜¯è¿ç»­çš„ã€‚
- en: Figure [9.2](#fig-twoclass-softmax-argmaxonehot) depicts this geometrically.
    The argmaxonehot functions as a function of the score vector [*s*0, *s*1] (for
    selecting classes 0 and 1, respectively) are shown in figures [9.2a](#fig-argmaxonehot-class0)
    and [9.2c](#fig-argmaxonehot-class1). These are step functions on the (*s*0, *s*1)
    plane, with *s*0 = *s*1 being the decision boundary. Their softmax approximations
    are shown in figures [9.2b](#fig-softmax-argmaxonehot-class0) and [9.2d](#fig-softmax-argmaxonehot-class1).
    In section [8.1](../Text/08.xhtml#sec-sigmoid-etc), we introduced the 1D sigmoid
    function (see figure [8.1](../Text/08.xhtml#fig-sigmoid1d)), which approximates
    the 1D step function. Here we see the higher-dimensional analog of that.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ [9.2](#fig-twoclass-softmax-argmaxonehot) å‡ ä½•åœ°æè¿°äº†è¿™ä¸€ç‚¹ã€‚argmaxonehot å‡½æ•°ä½œä¸ºå¾—åˆ†å‘é‡
    [*s*0, *s*1]ï¼ˆåˆ†åˆ«ç”¨äºé€‰æ‹©ç±»åˆ« 0 å’Œ 1ï¼‰çš„å‡½æ•°ï¼Œå¦‚å›¾ [9.2a](#fig-argmaxonehot-class0) å’Œ [9.2c](#fig-argmaxonehot-class1)
    æ‰€ç¤ºã€‚è¿™äº›æ˜¯åœ¨ (*s*0, *s*1) å¹³é¢ä¸Šçš„æ­¥å‡½æ•°ï¼Œ*s*0 = *s*1 æ˜¯å†³ç­–è¾¹ç•Œã€‚å®ƒä»¬çš„ softmax è¿‘ä¼¼å¦‚å›¾ [9.2b](#fig-softmax-argmaxonehot-class0)
    å’Œ [9.2d](#fig-softmax-argmaxonehot-class1) æ‰€ç¤ºã€‚åœ¨ [8.1](../Text/08.xhtml#sec-sigmoid-etc)
    èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† 1D sigmoid å‡½æ•°ï¼ˆè§å›¾ [8.1](../Text/08.xhtml#fig-sigmoid1d)ï¼‰ï¼Œå®ƒè¿‘ä¼¼ 1D æ­¥å‡½æ•°ã€‚è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°å…¶é«˜ç»´ç±»ä¼¼ç‰©ã€‚
- en: '![](../../OEBPS/Images/CH09_F02a_Chaudhury.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F02a_Chaudhury.png)'
- en: '(a) Step function: *z* = 1 if *s*0 > = *s*1, else *z* = 0'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (a) æ­¥å‡½æ•°ï¼š*z* = 1 å¦‚æœ *s*0 >= *s*1ï¼Œå¦åˆ™ *z* = 0
- en: '![](../../OEBPS/Images/CH09_F02b_Chaudhury.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F02b_Chaudhury.png)'
- en: '(b) Softmax: differential approximation of the step function'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Softmaxï¼šæ­¥å‡½æ•°çš„å¾®åˆ†è¿‘ä¼¼
- en: '![](../../OEBPS/Images/CH09_F02c_Chaudhury.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F02c_Chaudhury.png)'
- en: '(c) Step function: *z* = 1 if *s*1 > = *s*0, else *z* = 0'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (c) æ­¥å‡½æ•°ï¼š*z* = 1å¦‚æœ*s*1 >= *s*0ï¼Œå¦åˆ™*z* = 0
- en: '![](../../OEBPS/Images/CH09_F02d_Chaudhury.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/CH09_F02d_Chaudhury.png)'
- en: '(d) Softmax: differential approximation of the step function'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Softmaxï¼šæ­¥å‡½æ•°çš„å¾®åˆ†è¿‘ä¼¼
- en: Figure 9.2 Two-class argmaxonehot and softmax (function of score vector [*s*0,
    *s*1]) on the (*s*0, *s*1) plane. The decision boundary is the 45*[o]* line *s*0
    = *s*1.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.2åœ¨(*s*0, *s*1)å¹³é¢ä¸Šï¼Œä¸¤ç±»çš„argmaxonehotå’Œsoftmaxï¼ˆåˆ†æ•°å‘é‡çš„å‡½æ•°[*s*0, *s*1]ï¼‰ã€‚å†³ç­–è¾¹ç•Œæ˜¯45*[o]*çº¿
    *s*0 = *s*1ã€‚
- en: Listing 9.4 PyTorch code for softmax
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨9.4 PyTorchä»£ç å®ç°softmax
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: â‘  Imports the softmax function
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  å¯¼å…¥softmaxå‡½æ•°
- en: â‘¡ Scores are typically raw, un-normalized outputs of a neural network.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ åˆ†æ•°é€šå¸¸æ˜¯ç¥ç»ç½‘ç»œçš„åŸå§‹ã€æœªç»å½’ä¸€åŒ–çš„è¾“å‡ºã€‚
- en: â‘¢ Computes the softmax
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¢ è®¡ç®—softmax
- en: 9.1.6 Softmax cross-entropy loss
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.6 Softmaxäº¤å‰ç†µæŸå¤±
- en: From the preceding discussion, it should be clear that it is desirable to make
    the last layer of a classifier neural network a softmax layer. Then, given an
    input, the network will emit probabilities for each class. During training, we
    can evaluate the loss on these probabilities with regard to the known GT probabilities.
    This can be done via the CE loss (see section [9.1.3](#sec-ce-loss-again)). Thus
    the softmax is often followed by the CE loss during classifier training. Consequently,
    the combination (softmax CE loss) is available as a single operation in many deep
    learning packages, such as PyTorch. This is convenient because we do not need
    to call softmax and then CE loss. But the deeper reason for combining them is
    that the combination tends to be numerically better.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å‰é¢çš„è®¨è®ºä¸­ï¼Œåº”è¯¥æ¸…æ¥šï¼Œå°†åˆ†ç±»å™¨ç¥ç»ç½‘ç»œçš„æœ€åä¸€å±‚è®¾ç½®ä¸ºsoftmaxå±‚æ˜¯å¯å–çš„ã€‚ç„¶åï¼Œç»™å®šä¸€ä¸ªè¾“å…¥ï¼Œç½‘ç»œå°†è¾“å‡ºæ¯ä¸ªç±»çš„æ¦‚ç‡ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡CEæŸå¤±ï¼ˆè§ç¬¬9.1.3èŠ‚ï¼‰æ¥è¯„ä¼°è¿™äº›æ¦‚ç‡ä¸å·²çŸ¥GTæ¦‚ç‡ä¹‹é—´çš„æŸå¤±ã€‚è¿™å¯ä»¥é€šè¿‡CEæŸå¤±ï¼ˆè§ç¬¬9.1.3èŠ‚ï¼‰æ¥å®ç°ã€‚å› æ­¤ï¼Œsoftmaxé€šå¸¸åœ¨åˆ†ç±»å™¨è®­ç»ƒè¿‡ç¨‹ä¸­è·ŸéšCEæŸå¤±ã€‚å› æ­¤ï¼Œç»„åˆï¼ˆsoftmax
    CEæŸå¤±ï¼‰åœ¨è®¸å¤šæ·±åº¦å­¦ä¹ åŒ…ä¸­ä½œä¸ºå•ä¸ªæ“ä½œæä¾›ï¼Œä¾‹å¦‚PyTorchã€‚è¿™å¾ˆæ–¹ä¾¿ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦è°ƒç”¨softmaxç„¶åCEæŸå¤±ã€‚ä½†å°†å®ƒä»¬ç»“åˆçš„æ›´æ·±å±‚æ¬¡çš„åŸå› æ˜¯ï¼Œè¿™ç§ç»„åˆåœ¨æ•°å€¼ä¸Šå¾€å¾€æ›´å¥½ã€‚
- en: 'Letâ€™s look at an example to see how the softmax CE loss changes as the output
    prediction changes. Consider the image classification problem again, where weâ€™d
    like to classify whether an image contains one of four categories: cat (class
    0), dog (class 1), airplane (class 2), or automobile (class 3). Figure [9.3](#fig-softmax-good-bad-visual)
    represents this visually. Suppose the image weâ€™re classifying actually contains
    a dog class 1). The GT is represented as a one-hot vector [0 1 0 0]. If our classifier
    predicts the vector [0.498 0.502 0 0], itâ€™s predicting both cat and dog with almost
    equal probability. This is a bad prediction because we would ideally expect it
    to confidently predict a dog (class 1). Consequently, the CE loss is high (0.688).
    On the other hand, if our classifier predicts [0.003 0.997 0 0], it is highly
    certain (with a probability of 0.997) that the image contains a dog. This is a
    good prediction, and hence the CE loss is low (0.0032). Softmax CE loss is probably
    the most popular loss method used for training in classifiers at the moment.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä¾‹å­æ¥çœ‹çœ‹softmax CEæŸå¤±æ˜¯å¦‚ä½•éšç€è¾“å‡ºé¢„æµ‹çš„å˜åŒ–è€Œå˜åŒ–çš„ã€‚å†æ¬¡è€ƒè™‘å›¾åƒåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å›¾åƒåˆ†ç±»ä¸ºå››ä¸ªç±»åˆ«ä¹‹ä¸€ï¼šçŒ«ï¼ˆç±»åˆ«0ï¼‰ã€ç‹—ï¼ˆç±»åˆ«1ï¼‰ã€é£æœºï¼ˆç±»åˆ«2ï¼‰æˆ–æ±½è½¦ï¼ˆç±»åˆ«3ï¼‰ã€‚å›¾[9.3](#fig-softmax-good-bad-visual)å±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚å‡è®¾æˆ‘ä»¬æ­£åœ¨åˆ†ç±»çš„å›¾åƒå®é™…ä¸ŠåŒ…å«ç‹—ç±»åˆ«1ï¼‰ã€‚GTè¡¨ç¤ºä¸ºä¸€ä¸ªone-hotå‘é‡[0
    1 0 0]ã€‚å¦‚æœæˆ‘ä»¬çš„åˆ†ç±»å™¨é¢„æµ‹çš„å‘é‡æ˜¯[0.498 0.502 0 0]ï¼Œå®ƒå‡ ä¹ä»¥ç›¸åŒçš„æ¦‚ç‡é¢„æµ‹äº†çŒ«å’Œç‹—ã€‚è¿™æ˜¯ä¸€ä¸ªä¸è‰¯çš„é¢„æµ‹ï¼Œå› ä¸ºæˆ‘ä»¬ç†æƒ³ä¸Šå¸Œæœ›å®ƒè‡ªä¿¡åœ°é¢„æµ‹ç‹—ï¼ˆç±»åˆ«1ï¼‰ã€‚å› æ­¤ï¼ŒCEæŸå¤±è¾ƒé«˜ï¼ˆ0.688ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬çš„åˆ†ç±»å™¨é¢„æµ‹[0.003
    0.997 0 0]ï¼Œå®ƒé«˜åº¦ç¡®å®šï¼ˆæ¦‚ç‡ä¸º0.997ï¼‰å›¾åƒåŒ…å«ç‹—ã€‚è¿™æ˜¯ä¸€ä¸ªå¥½çš„é¢„æµ‹ï¼Œå› æ­¤CEæŸå¤±è¾ƒä½ï¼ˆ0.0032ï¼‰ã€‚softmax CEæŸå¤±å¯èƒ½æ˜¯ç›®å‰ç”¨äºåˆ†ç±»å™¨è®­ç»ƒçš„æœ€æµè¡Œçš„æŸå¤±æ–¹æ³•ã€‚
- en: '![](../../OEBPS/Images/CH09_F03_Chaudhury.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/CH09_F03_Chaudhury.png)'
- en: Figure 9.3 Softmax output and cross-entropy loss for good and bad output predictions
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.3è‰¯å¥½å’Œä¸è‰¯è¾“å‡ºé¢„æµ‹çš„softmaxè¾“å‡ºå’Œäº¤å‰ç†µæŸå¤±
- en: NOTE Fully functional code for softmax CE loss, executable via Jupyter Notebook,
    can be found at [http://mng.bz/g1a8](http://mng.bz/g1a8).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å¤‡æ³¨ï¼šå®Œæ•´çš„softmax CEæŸå¤±åŠŸèƒ½ä»£ç ï¼Œå¯é€šè¿‡Jupyter Notebookæ‰§è¡Œï¼Œå¯åœ¨[http://mng.bz/g1a8](http://mng.bz/g1a8)æ‰¾åˆ°ã€‚
- en: Listing 9.5 PyTorch code for softmax cross-entropy loss
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨9.5 PyTorchä»£ç å®ç°softmaxäº¤å‰ç†µæŸå¤±
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: â‘  Ground truth class index Ranges from 0 to _ â€“1
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  çœŸå®ç±»åˆ«ç´¢å¼•èŒƒå›´ä»0åˆ°_ â€“1
- en: â‘¡ Computes the softmax cross-entropy loss
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ è®¡ç®—softmaxäº¤å‰ç†µæŸå¤±
- en: 9.1.7 Focal loss
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.7 FocalæŸå¤±
- en: As training progresses, where should we focus our attention? This question becomes
    especially significant when there is *data imbalance*, meaning the number of training
    data instances available for some classes is significantly smaller than others.
    In such cases, not all training data is equally important. We have to use our
    training data instances wisely.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the greater bang for the buck is trying to improve the training
    data instances that are not doing well. In other words, instead of trying to squeeze
    out every bit of juice from the examples in which the network is doing well (the
    so-called â€œeasyâ€ examples), it is better to focus on the examples where the network
    is not doing as well (â€œhardâ€ examples).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'To stop focusing on easy examples and focus instead on hard examples, we can
    put more weight on the loss from training data instances that are far from the
    GT, and vice versa: that is, put less weight on the loss from training data instances
    that are close to GT. Consider the binary CE loss of equation [9.4](#eq-binary-ce-loss)
    one more time. The loss for the *i*th training instance can be rewritten as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-06-d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: NOTE Going forward in this subsection, we drop the superscript (*i*) for the
    sake of notational simplification, although it remains implied.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Now, when the GT is class 1 (that is, *È³* = 1), the entity (1âˆ’*y*) measures
    the departure of the prediction from GT. We can multiply the loss by this to weigh
    down the losses from good predictions and weigh up the losses from bad predictions.
    In practice, we multiply by (1âˆ’*y*)^Î³ for some value of *Î³* (such as *Î³* = 2).
    Similarly, when the GT is class 0 (that is, *È³* = 0), the entity *y* measures
    the departure of the prediction from the GT. In this case, we multiply the loss
    by *y^Î³*. The overall loss then becomes
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-06-e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: We can have a somewhat simplified expression
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-07.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Equation 9.7
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-07-a.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: Equation [9.7](#eq-focal-loss) is the popular expression of focal loss. Its
    graph at various values of *Î³* is shown in figure [9.4](#fig-focal-loss). Note
    how the loss becomes more and more subdued as the probability of the GT increases
    toward the right until it flattens out at the bottom.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for focal loss, executable via Jupyter Notebook,
    can be found at [http://mng.bz/g1a8](http://mng.bz/g1a8).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 PyTorch code for focal loss
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: â‘  *y[t]* = *y* if *y[gt]* is 1
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '*y[t]* = 1 âˆ’ *y* if *y[gt]* is 0'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.8 Hinge loss
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The softmax CE loss becomes zero only under the ideal condition: the correct
    class has a finite score, and other classes have a score of negative infinity.
    Hence, that loss will continue to push the network toward improvement until the
    ideal is achieved (which never happens in practice). Sometimes we prefer to stop
    changing the network when the correct class has the maximum score, and we do not
    care about increasing the distance between correct and incorrect scores any further.
    This is where hinge loss comes in.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F04_Chaudhury.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 Focal loss graph (various *Î³* values)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: A hinged door opens in one direction but not in the other direction. Similarly,
    a hinge loss function increases if a certain goodness criterion is *not* satisfied
    but becomes zero (and does not reduce any further) if the criterion is satisfied.
    This is akin to saying, â€œIf you are not my friend, the distance between us can
    vary from small to large (unboundedly), but I donâ€™t distinguish between friends.
    All my friends are at a distance of zero from me.â€
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiclass support vector machine loss: Hinge loss for classification'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider again our old friend, the classifier that predicts whether an image
    contains a cat (class 0), dog class 1), airplane (class 2), or automobile (class
    3). Our classifier emits an output vector ![](../../OEBPS/Images/AR_y.png) corresponding
    to an input image. Here, the outputs are scores: *y[j]* is the score corresponding
    to the *j*th class. (In this subsection, we have dropped the superscripts indicating
    the training data index to simplify notations.)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Given a (training data instance, GT label) pair (![](../../OEBPS/Images/AR_x.png),
    *c*) (that is, the GT class corresponding to the input ![](../../OEBPS/Images/AR_x.png)
    is *c*), the multiclass support vector machine (SVM) loss is
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-08.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Equation 9.8
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: where *m* is a margin usually *m* = 1).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this, consider the equation without the margin first:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-08-a.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'In equation [9.8](#eq-muticlass-svm-loss), we are summing over all the classes
    except the one that matches the GT. In other words, we are summing over only the
    incorrect classes. For these, we want the score *y[j]* to be smaller than the
    score *y[c]* for the correct class. There are two possibilities:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '*Good output*â€”Incorrect class score less than correct class score:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y[j]* âˆ’ *y[c]* < 0 âŸ¹ *max*(0, *y[j]* âˆ’ *y[c]*) = 0'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The contribution to the loss is zero (we do not distinguish between correct
    scores: all friends are at zero distance).'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Bad output*â€”Incorrect class score more than correct class score:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y[j]* > *y[c]* âŸ¹ *max*(0, *y[j]* â€“ *y[c]*) = *y[j]* âˆ’ *y[c]*'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The contribution to the loss is positive (non-friends are at a positive distance
    that varies with the degree of non-friendness).
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In practical settings, the margin is set to a positive number usually 1) to
    penalize predictions where the score of the correct class is marginally greater
    than that of the incorrect classes. This forces the classifier to learn to predict
    the correct class with high confidence. Figure [9.8](#fig-hinge-loss-good-bad-visual)
    shows how hinge loss differs for good and bad output predictions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…è®¾ç½®ä¸­ï¼Œè¾¹ç¼˜è®¾ç½®ä¸ºæ­£æ•°ï¼Œé€šå¸¸ä¸º1ï¼Œä»¥æƒ©ç½šæ­£ç¡®ç±»åˆ«çš„åˆ†æ•°ä»…ç•¥é«˜äºé”™è¯¯ç±»åˆ«çš„é¢„æµ‹ã€‚è¿™è¿«ä½¿åˆ†ç±»å™¨å­¦ä¼šä»¥é«˜ç½®ä¿¡åº¦é¢„æµ‹æ­£ç¡®ç±»åˆ«ã€‚å›¾[9.8](#fig-hinge-loss-good-bad-visual)æ˜¾ç¤ºäº†è‰¯å¥½å’Œä¸è‰¯è¾“å‡ºé¢„æµ‹çš„hingeæŸå¤±å¦‚ä½•ä¸åŒã€‚
- en: '![](../../OEBPS/Images/CH09_F05_Chaudhury.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH09_F05_Chaudhury.png)'
- en: Figure 9.5 Hinge loss for good and bad output predictions
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.5 è‰¯å¥½å’Œä¸è‰¯è¾“å‡ºé¢„æµ‹çš„HingeæŸå¤±
- en: One mental model to have about the multiclass SVM loss is that it is lazy. It
    stops changing as soon as the correct class score exceeds the incorrect scores
    by the margin *m*. The loss does not change if the correct class score goes still
    higher, which means it does not push the machine to improve beyond that point.
    This behavior is different from the softmax CE loss, which tries to push the machine
    to achieve an infinite score for the correct class.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå¤šç±»SVMæŸå¤±çš„ä¸€ä¸ªå¿ƒç†æ¨¡å‹æ˜¯å®ƒå¾ˆæ‡’æƒ°ã€‚ä¸€æ—¦æ­£ç¡®çš„ç±»åˆ«åˆ†æ•°è¶…è¿‡é”™è¯¯åˆ†æ•°çš„è¾¹ç¼˜ *m*ï¼Œå®ƒå°±åœæ­¢å˜åŒ–ã€‚å¦‚æœæ­£ç¡®çš„ç±»åˆ«åˆ†æ•°ç»§ç»­ä¸Šå‡ï¼ŒæŸå¤±ä¸ä¼šæ”¹å˜ï¼Œè¿™æ„å‘³ç€å®ƒä¸ä¼šæ¨åŠ¨æœºå™¨åœ¨è¿™ä¸ªç‚¹ä¹‹åç»§ç»­æ”¹è¿›ã€‚è¿™ç§è¡Œä¸ºä¸softmax
    CEæŸå¤±ä¸åŒï¼Œsoftmax CEæŸå¤±è¯•å›¾æ¨åŠ¨æœºå™¨ä¸ºæ­£ç¡®çš„ç±»åˆ«è¾¾åˆ°æ— é™åˆ†æ•°ã€‚
- en: 9.2 Optimization
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 ä¼˜åŒ–
- en: Neural network models define a loss function that estimates the badness of the
    networkâ€™s output. During supervised training, the output on a particular training
    instance input is compared to a known output GT) for that particular training
    instance. The difference between the GT and the network-generated output is called
    loss. We can sum up the losses from individual training data instances and compute
    the total loss over all the training data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰äº†ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¼°è®¡ç½‘ç»œè¾“å‡ºçš„ä¸è‰¯ç¨‹åº¦ã€‚åœ¨ç›‘ç£è®­ç»ƒæœŸé—´ï¼Œç‰¹å®šè®­ç»ƒå®ä¾‹è¾“å…¥çš„è¾“å‡ºä¸è¯¥ç‰¹å®šè®­ç»ƒå®ä¾‹çš„å·²çŸ¥è¾“å‡ºï¼ˆGTï¼‰è¿›è¡Œæ¯”è¾ƒã€‚GTå’Œç½‘ç»œç”Ÿæˆçš„è¾“å‡ºä¹‹é—´çš„å·®å¼‚ç§°ä¸ºæŸå¤±ã€‚æˆ‘ä»¬å¯ä»¥å°†å•ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹çš„æŸå¤±ç›¸åŠ ï¼Œå¹¶è®¡ç®—æ‰€æœ‰è®­ç»ƒæ•°æ®ä¸Šçš„æ€»æŸå¤±ã€‚
- en: These losses are, of course, functions of the network parameters, ![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png). We can imagine a space whose dimensionality
    is *dim*(![](../../OEBPS/Images/AR_w.png)) + *dim*(![](../../OEBPS/Images/AR_b.png)).
    At each point in this space, we have a value for the total training loss. Thus,
    we can imagine a loss surfaceâ€”a surface whose height represents the loss valueâ€”defined
    over the high-dimensional domain of network parameters (weights and biases).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æŸå¤±å½“ç„¶æ˜¯ç½‘ç»œå‚æ•°çš„å‡½æ•°ï¼Œ![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png)ã€‚æˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ä¸ªç»´åº¦ä¸º
    *dim*(![](../../OEBPS/Images/AR_w.png)) + *dim*(![](../../OEBPS/Images/AR_b.png))
    çš„ç©ºé—´ã€‚åœ¨è¿™ä¸ªç©ºé—´çš„æ¯ä¸€ä¸ªç‚¹ä¸Šï¼Œæˆ‘ä»¬éƒ½æœ‰ä¸€ä¸ªæ€»è®­ç»ƒæŸå¤±çš„å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ä¸ªæŸå¤±è¡¨é¢â€”â€”ä¸€ä¸ªé«˜åº¦ä»£è¡¨æŸå¤±å€¼çš„è¡¨é¢â€”â€”å®ƒå®šä¹‰åœ¨ç½‘ç»œå‚æ•°ï¼ˆæƒé‡å’Œåå·®ï¼‰çš„é«˜ç»´åŸŸä¸Šã€‚
- en: 'Optimization is nothing but finding the lowest point on this surface. During
    training, we start with random values of network parameters: this is akin to starting
    at a random point on the surface. Then we constantly move locally downhill on
    the loss surface in the direction of the negative gradient. We hope this eventually
    takes us to the minimum or a sufficiently low point. Continuing our analogy of
    the loss surface as a ravine, the minimum is at sea level. This minimum provides
    us with the network parameter values (weights and biases) that will yield the
    least loss on the training data. If the training data set adequately represents
    the problem, the trained model will perform well on unseen data.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ä¸è¿‡æ˜¯æ‰¾åˆ°è¿™ä¸ªè¡¨é¢ä¸Šçš„æœ€ä½ç‚¹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»ç½‘ç»œå‚æ•°çš„éšæœºå€¼å¼€å§‹ï¼šè¿™å°±åƒåœ¨è¡¨é¢ä¸Šéšæœºé€‰æ‹©ä¸€ä¸ªç‚¹å¼€å§‹ã€‚ç„¶åæˆ‘ä»¬ä¸æ–­åœ°åœ¨æŸå¤±è¡¨é¢ä¸Šæ²¿ç€è´Ÿæ¢¯åº¦çš„æ–¹å‘å±€éƒ¨ä¸‹é™ã€‚æˆ‘ä»¬å¸Œæœ›è¿™æœ€ç»ˆèƒ½å¸¦æˆ‘ä»¬åˆ°æœ€å°å€¼æˆ–è¶³å¤Ÿä½çš„ä½ç½®ã€‚ç»§ç»­æˆ‘ä»¬å…³äºæŸå¤±è¡¨é¢åƒå³¡è°·çš„ç±»æ¯”ï¼Œæœ€å°å€¼åœ¨æµ·å¹³é¢ã€‚è¿™ä¸ªæœ€å°å€¼ä¸ºæˆ‘ä»¬æä¾›äº†ç½‘ç»œå‚æ•°å€¼ï¼ˆæƒé‡å’Œåå·®ï¼‰ï¼Œè¿™äº›å€¼å°†åœ¨è®­ç»ƒæ•°æ®ä¸Šäº§ç”Ÿæœ€å°çš„æŸå¤±ã€‚å¦‚æœè®­ç»ƒæ•°æ®é›†å……åˆ†ä»£è¡¨äº†é—®é¢˜ï¼Œè®­ç»ƒå¥½çš„æ¨¡å‹å°†åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚
- en: This process of traveling toward the minimum, the iterative updating of weights
    and biases to have minimal loss over the training dataset, is called *optimization*.
    The basic math was introduced in section [8.4.2](../Text/08.xhtml#sec-loss-surface-graddesc)
    (equation [8.12](../Text/08.xhtml#eq-wtsbiases-update-vector)). Here we study
    many practical nuances and variants.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'At every iteration, we update the weights and biases, so if *t* denotes the
    iteration number, ![](../../OEBPS/Images/AR_w.png)*[t]* denotes the weight values
    at iteration *t*, *Î´*![](../../OEBPS/Images/AR_w.png)*[t]* denotes the weight
    updates at iteration *t*, and so on:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-09.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Equation 9.9
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic update is along the direction of the negative gradient (see equation
    [8.12](../Text/08.xhtml#eq-wtsbiases-update-vector)):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-10.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Equation 9.10
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, ğ•ƒ(![](../../OEBPS/Images/AR_w.png)*[t]*, ![](../../OEBPS/Images/AR_b.png)*[t]*)
    denotes the loss at iteration *t*. Ideally, we should evaluate the loss on every
    training data instance and average them. But that would imply that we must process
    every training data instance for every iteration, which is prohibitively expensive.
    Instead, we use sampling see section [9.2.2](../Text/09.xhtml#sec-SGD)):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'The constant *Î·* is called the *learning rate* (LR). A larger LR results in
    bigger steps (bigger adjustments to weights and biases per update) and vice versa.
    We use larger values of LR in the beginning: when the network is completely untrained,
    we want to take large steps toward the minimum. When we are close to the minimum,
    on the other hand, we want to take smaller steps, lest we overshoot it. The LR
    is typically a small number, like *Î·* = 0.01.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In stochastic gradient descent (SGD; a popular approach), the LR *Î·* is typically
    held constant during an epoch (an *epoch* is a single pass over all the training
    data). Then the LR is decreased after one or more epochs. This process is called
    *learning rate decay*. So, the LR is not exactly a constant. We could have written
    it *Î·[t]* to indicate the temporal nature, but we chose to keep it simple because
    (in SGD, at least) it changes infrequently.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to reevaluate the loss and its gradients in each iteration since their
    values will change in every iteration, because the weights and biases of the underlying
    neural network are changing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: How many iterations are necessary? Typically, this is a large number. We iterate
    *multiple times over the entire training dataset*. A typical training session
    has multiple epochs. In this context, note that for proper convergence, it is
    *extremely important* to randomly shuffle the order of occurrence of the training
    data after every epoch. In the following sections, we look at some practical nuances
    of the process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Geometrical view of optimization
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This topic is described in detail in section [8.4.2](../Text/08.xhtml#sec-loss-surface-graddesc).
    You are encouraged to revisit that discussion if necessary.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¸»é¢˜åœ¨[8.4.2](../Text/08.xhtml#sec-loss-surface-graddesc)èŠ‚ä¸­æœ‰è¯¦ç»†æè¿°ã€‚å¦‚æœéœ€è¦ï¼Œä½ è¢«é¼“åŠ±é‡æ–°å›é¡¾é‚£ä¸ªè®¨è®ºã€‚
- en: Overall, neural network optimization is an iterative process. Ideally, in each
    iteration, we compute the gradient of the loss with respect to the current parameters
    (weights and biases) and obtain improved values for them by moving in the direction
    of the negative gradient.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“æ¥è¯´ï¼Œç¥ç»ç½‘ç»œä¼˜åŒ–æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—å½“å‰å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰ç›¸å¯¹äºæŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œå¹¶é€šè¿‡æ²¿ç€è´Ÿæ¢¯åº¦æ–¹å‘ç§»åŠ¨æ¥è·å¾—å®ƒä»¬çš„æ”¹è¿›å€¼ã€‚
- en: 9.2.2 Stochastic gradient descent and minibatches
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 éšæœºæ¢¯åº¦ä¸‹é™å’Œå°æ‰¹é‡
- en: 'How do we compute the gradient of the loss function? The loss is different
    for every training data instance. The sensible thing to do is to average them
    out. But as we mentioned earlier, that leads to a practical problem: we would
    have to process the entire training dataset on every iteration. If the size of
    the training dataset is *n*, an epoch is an *O*(*n*Â²) operation every iteration,
    we have to process all of the *n* training data instances to compute the gradient,
    and an epoch has *n* iterations). Since *n* is a large number, often in the millions,
    *O*(*n*Â²) is prohibitively expensive.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Ÿå¯¹äºæ¯ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹ï¼ŒæŸå¤±æ˜¯ä¸åŒçš„ã€‚åˆä¹é€»è¾‘çš„åšæ³•æ˜¯å¹³å‡å®ƒä»¬ã€‚ä½†å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªå®é™…çš„é—®é¢˜ï¼šæˆ‘ä»¬æ¯æ¬¡è¿­ä»£éƒ½å¿…é¡»å¤„ç†æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ã€‚å¦‚æœè®­ç»ƒæ•°æ®é›†çš„å¤§å°æ˜¯*n*ï¼Œé‚£ä¹ˆä¸€ä¸ªepochæ˜¯æ¯æ¬¡è¿­ä»£çš„*O(nÂ²)*æ“ä½œï¼Œæˆ‘ä»¬å¿…é¡»å¤„ç†æ‰€æœ‰çš„*n*ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹æ¥è®¡ç®—æ¢¯åº¦ï¼Œè€Œä¸€ä¸ªepochæœ‰*n*æ¬¡è¿­ä»£ï¼‰ã€‚ç”±äº*n*é€šå¸¸æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æ•°å­—ï¼Œç»å¸¸æ˜¯æ•°ç™¾ä¸‡ï¼Œ*O(nÂ²)*æ˜¯è¿‡äºæ˜‚è´µçš„ã€‚
- en: In SGD, we do not average over the entire training data set to produce the gradient.
    Instead, we average over a random sampled subset of the training data. This randomly
    sampled subset of training data is called a *minibatch*. The gradient is computed
    by averaging the loss over the minibatch (as opposed to the entire training dataset).
    This gradient is used to update the weight and bias parameters.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨SGDä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šå¹³å‡æ¥äº§ç”Ÿæ¢¯åº¦ã€‚ç›¸åï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæ•°æ®çš„ä¸€ä¸ªéšæœºæ ·æœ¬å­é›†ä¸Šå¹³å‡ã€‚è¿™ä¸ªéšæœºæ ·æœ¬çš„å­é›†è¢«ç§°ä¸º*å°æ‰¹é‡*ã€‚æ¢¯åº¦æ˜¯é€šè¿‡å¹³å‡å°æ‰¹é‡ä¸­çš„æŸå¤±æ¥è®¡ç®—çš„ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ï¼‰ã€‚è¿™ä¸ªæ¢¯åº¦ç”¨äºæ›´æ–°æƒé‡å’Œåç½®å‚æ•°ã€‚
- en: 9.2.3 PyTorch code for SGD
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 SGDçš„PyTorchä»£ç 
- en: Now, letâ€™s implement SGD in PyTorch.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬åœ¨PyTorchä¸­å®ç°SGDã€‚
- en: NOTE Fully functional code for SGD, executable via Jupyter Notebook, can be
    found at [http://mng.bz/ePyG](http://mng.bz/ePyG).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šSGDçš„å®Œæ•´åŠŸèƒ½ä»£ç ï¼Œå¯é€šè¿‡Jupyter Notebookæ‰§è¡Œï¼Œå¯ä»¥åœ¨[http://mng.bz/ePyG](http://mng.bz/ePyG)æ‰¾åˆ°ã€‚
- en: Letâ€™s consider the example discussed in section [6.9](../Text/06.xhtml#sec-GMM).
    Our goal is to build a model that can predict whether a Statsville resident is
    a man, woman, or child using height and weight as input data. For this purpose,
    letâ€™s assume we have a large dataset *X* containing the heights and weights of
    various Statsville residents. *X* is of shape (*num*_*samples*,2), where each
    row represents the (*height*, *weight*) pair of a single resident. The corresponding
    labels are stored in y[gt], which contains *num*_*samples* elements. Each row
    of y[gt] can be 0, 1, or 2, depending on whether the resident is a man, woman,
    or child. Figure [9.6](#fig-sgd-example-gt-dist) shows an example distribution
    of *X*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘[6.9](../Text/06.xhtml#sec-GMM)èŠ‚ä¸­è®¨è®ºçš„ä¾‹å­ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä½¿ç”¨èº«é«˜å’Œä½“é‡ä½œä¸ºè¾“å…¥æ•°æ®æ¥é¢„æµ‹Statsvilleå±…æ°‘æ˜¯ç”·æ€§ã€å¥³æ€§è¿˜æ˜¯å„¿ç«¥ã€‚ä¸ºæ­¤ï¼Œè®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«å„ç§Statsvilleå±…æ°‘èº«é«˜å’Œä½“é‡çš„åºå¤§æ•°æ®é›†*X*ã€‚*X*çš„å½¢çŠ¶æ˜¯(*num_samples*,2)ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨å•ä¸ªå±…æ°‘çš„(*height*,
    *weight*)å¯¹ã€‚ç›¸åº”çš„æ ‡ç­¾å­˜å‚¨åœ¨y[gt]ä¸­ï¼Œå®ƒåŒ…å«*num_samples*ä¸ªå…ƒç´ ã€‚y[gt]çš„æ¯ä¸€è¡Œå¯ä»¥æ˜¯0ã€1æˆ–2ï¼Œå…·ä½“å–å†³äºå±…æ°‘æ˜¯ç”·æ€§ã€å¥³æ€§è¿˜æ˜¯å„¿ç«¥ã€‚å›¾[9.6](#fig-sgd-example-gt-dist)æ˜¾ç¤ºäº†*X*çš„ä¸€ä¸ªç¤ºä¾‹åˆ†å¸ƒã€‚
- en: '![](../../OEBPS/Images/CH09_F06_Chaudhury.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/CH09_F06_Chaudhury.png)'
- en: Figure 9.6 Height and weight of various Statsville residents. Class 0 (man)
    is represented by the right-most cluster, class 1 (woman) by the middle cluster,
    and class 2 (child) by the left-most cluster.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.6 Statsvilleå±…æ°‘çš„èº«é«˜å’Œä½“é‡ã€‚ç±»åˆ«0ï¼ˆç”·æ€§ï¼‰ç”±æœ€å³ä¾§çš„ç°‡è¡¨ç¤ºï¼Œç±»åˆ«1ï¼ˆå¥³æ€§ï¼‰ç”±ä¸­é—´çš„ç°‡è¡¨ç¤ºï¼Œç±»åˆ«2ï¼ˆå„¿ç«¥ï¼‰ç”±æœ€å·¦ä¾§çš„ç°‡è¡¨ç¤ºã€‚
- en: Before training a model, we must first convert the data into a training-friendly
    format. We subclass `torch.utils.data.Dataset` to do so and implement the `__len__`
    and `__getitem__` methods. The *i*th training data instance can be accessed by
    calling `data_set[i]`. Remember that in SGD, we feed in minibatches that contain
    `batch_size` elements in every iteration. This can be achieved by calling the
    `__getitem__` method `batch_size` times. However, instead of doing this ourselves,
    we use PyTorchâ€™s `DataLoader`, which gives us a convenient wrapper. Using `DataLoader`
    is recommended in production settings because it provides a simple API through
    which we can (1) create minibatches, 2) speed up data-loading times via multiprocessing,
    and (3) randomly shuffle data in every epoch to prevent overfitting. The following
    code creates a custom PyTorch data set.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.7 PyTorch code to create a custom dataset
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: â‘  Subclasses torch.utils.data.Dataset
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Returns the size of the data set
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Returns the *i*th training data element
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Instantiates the data set
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Instantiates the data loader with a batch size of 10 and shuffle on
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next step is to create a classifier model that can take the height and
    weight data (*X*) as input and predict the output class. Here, we create a simple
    neural network model that consists of two linear layers followed by a softmax
    layer. The output of the softmax layer has three values representing the probability
    of each of the three classes (man, woman, and child), respectively. Note that
    in the forward pass, we donâ€™t call the softmax layer because our loss function,
    PyTorchâ€™s CE loss, expects raw, un-normalized scores as input. Hence we pass the
    output of the second linear layer to the loss function. However, during prediction,
    we pass the scores to the softmax layer to get a probability vector and then take
    an argmax to get the predicted class. Notice that we have a function to initialize
    the weights of the linear layers: this is important because the starting value
    of the weights can often affect convergence. If the model starts too far from
    the minimum, it may never converge.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 PyTorch code to create a custom neural network model
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: â‘  Subclasses torch.nn.Module
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Instantiates the linear layers and the softmax layer
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Feeds forward the input through the two linear layers
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Predicts the output class index
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Initializes the weights to help the model converge better
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our data set and model, letâ€™s define our loss function and
    instantiate our SGD optimizer.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.9 PyTorch code for a loss function and SGD optimizer
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: â‘  Instantiates the SGD optimizer with learning rate = 0.02
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Now we define the training loop, which is essentially one pass over the entire
    dataset. We iterate over the dataset in batches of size `batch_size`, run the
    forward pass, compute the gradients, and update the weights in the direction of
    the negative gradient. Note that we call `optimizer.zero_grad()` in every iteration
    to prevent the accumulation of gradients from the previous steps.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.10 PyTorch code for one training loop
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: â‘  Iterates through the data set in batches
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Feeds forward the model to compute scores
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Computes the cross-entropy loss
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Clears the gradients accumulated from the previous step
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Runs backpropagation and computes the gradients
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: â‘¥ Updates the weights
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: With this, we are ready to train our model. The following code shows how to
    do so. Figure [9.7](#fig-sgd-examples-model-pred) shows the output predictions
    and loss at the end of every epoch.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F07_Chaudhury.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 Model predictions at the end of every epoch. Notice how the loss
    is reduced with every epoch. In the beginning, all training data points are wrongly
    classified as class 1\. After the end of epoch 1, most of the training data is
    classified correctly, and the distribution of the classifierâ€™s output has become
    close to the ground truth. The loss continues to decrease until epoch 4 although
    the improvements are harder to see visually).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.11 PyTorch code to run the training loop `num_epochs` times
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 9.2.4 Momentum
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For real-life loss surfaces in high dimensions, the ravine analogy is quite
    appropriate. A loss surface is hardly like a porcelain cup with smooth walls;
    it is much more like the walls of the Grand Canyon (see figure [9.8](#fig-grand-canyon-momentum)).
    Furthermore, the gradient estimate (usually done over a minibatch) is noisy. Consequently,
    gradient estimates are never aligned in one directionâ€”they tend to go hither and
    thither. Nonetheless, most of them tend to have a good downhill component. The
    other (non-downhill component) is somewhat random (see figure [9.8](#fig-grand-canyon-momentum)).
    So if we average them, the downhill components reinforce each other and are strengthened,
    while the non-downhill components cancel each other and are weakened.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F08_Chaudhury.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 Momentum. Noisy stochastic gradient estimates at different points
    on the loss surface (thick solid arrows) are not aligned in direction, but they
    all have a significant downhill component (thin solid arrows). The non-downhill
    components thin dashed arrows) point in random directions. Hence, averaging tends
    to strengthen the downhill component and cancel out the non-downhill components.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, if there is a small flat region with downhill areas preceding and
    following it, the vanilla gradient-based approach will get stuck in the small
    flat region (since the gradient there is zero). But averaging with the past allows
    us to have nonzero updates that take the optimization out of the local small flat
    region so that it can continue downhill.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: A good mental picture in this context is that of a ball rolling downhill. The
    surface of the hill is rough, and the ball is not going directly downward. Rather,
    it takes a zigzag path. But as it travels, it gathers downward momentum, and its
    downward velocity becomes greater.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this theory, we take the weighted average of the gradient computed
    at this iteration with the update used in the previous iteration:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-11.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: Equation 9.11
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: where *Î³*, *Î·* are positive constants with values less than 1. The weights and
    bias parameters are updated in the usual fashion using equation [9.9](#eq-optim-updates).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Unrolling the recursion of the momentum equation
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Unraveling the recursive equation [9.11](#eq-momentum), we see
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-11-a.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Assuming *Î´*![](../../OEBPS/Images/AR_w.png)[âˆ’1] = 0, we get
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-12.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Equation 9.12
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: We are taking a weighted sum of the gradients from past iterations. This is
    not quite a weighted average, though, as explained later.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Older gradients are weighted by higher powers of *Î³*. Since *Î³* < 1, weights
    decrease with age long-past iterations have less influence).
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the weights of the gradients, going backward from now to the beginning
    of time (the 0th iteration), is
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-12-a.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: Now, using Taylor expansion,
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-12-b.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Thus the sum of the weights of the past gradients in momentum-based gradient
    descent is *Î·*/(1â€“*Î³*) â‰  1. In other words, this is not quite a weighted average
    (where the weights should sum up to 1). This is a somewhat undesirable property
    and is rectified in the Adam algorithm discussed later.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: A similar analysis can be done for the biases.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '9.2.5 Geometric view: Constant loss contours, gradient descent, and momentum'
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider a network with a tiny two-element weight vector ![](../../OEBPS/Images/eq_09-12-c2.png)
    and no bias. Further, suppose that the loss function is ğ•ƒ = ||![](../../OEBPS/Images/AR_w.png)||Â²
    = *w*[0]Â² + *w*[1]Â². The constant loss contours are concentric circles with the
    origin as the center. The radius of the circle indicates the loss magnitude.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'If we move along the circleâ€™s circumference, the loss does not change. The
    loss changes maximally along the orthogonal direction to that: the radius of the
    circle. This intuitive observation is confirmed by evaluating the gradient'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-12-d.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Thus the gradient is along the radius, and the negative gradient direction is
    radially inward. So the loss decreases most rapidly if we move radially inward.
    If we move orthogonal to the radius (that is, along the circumference), the loss
    remains unchanged; of course, we are moving along the constant loss contour.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Optimization then takes us from outer, larger-radius circles to inner, smaller-radius
    circles. The minimum is at the origin; ideally, optimization should stop once
    we reach the origin.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Figure [9.9](#fig-graddesc-momentum-constant-loss-contours) shows optimization
    for a simple loss function ğ•ƒ = ||![](../../OEBPS/Images/AR_w.png)||Â² = *w*[0]Â²
    + *w*[1]Â². We start at an arbitrary pair of weight values and repeatedly update
    them via equation [9.9](#eq-optim-updates). For figure [9.9a](#fig-graddesc-momentum-constant-loss-contours),
    we use update without momentum (equation [9.10](#eq-optim-vanilla)). For figure
    [9.9b](#fig-graddesc-momentum-constant-loss-contours), we use update with momentum
    (equation [9.11](#eq-momentum)).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F09a_Chaudhury.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: (a) A trajectory through constant loss contours for gradient descent *without*
    momentum
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F09b_Chaudhury.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: (b) A trajectory through constant loss contours for gradient descent *with*
    momentum
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 Constant loss contours and optimization trajectory for the loss function
    ğ•ƒ = ||![](../../OEBPS/Images/AR_w.png)||Â² = *w*[0]Â² + *w*[1]Â². The loss surface
    is a cone with its apex on the origin and its base a circle in a plane parallel
    to the *w*[0], *w*[1] plane. Optimization takes us down the cone toward smaller
    and smaller cross-sections as we approach the minimum at the origin. Updates are
    shown with arrows. Note how the momentum version arrives at a smaller circle in
    fewer steps.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'When the constant loss contours are concentric circles with the origin as the
    center, the loss surface is a cone with its apex on the origin. The cross sections
    are circles on planes parallel to the *w*[0], *w*[1] plane. Optimization takes
    us down the inner walls of the cone through smaller and smaller circular cross-sections
    as we approach the minimum. The global minimum is at the origin and corresponds
    to zero loss. The zero-loss contour is a circle with a zero radius, which is effectively
    a single point: the origin.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases, progress slows down (step sizes become smaller) as we approach
    the minimum. This is because the magnitude of the gradient becomes smaller and
    smaller as we get closer to the minimum (imagine a bowl: it becomes flatter as
    we get closer to the bottom). However, this effect is countered to a certain extent
    if we have momentum. So, we can see that we need fewer steps to reach a circle
    with a smaller radius when we have momentum.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.6 Nesterov accelerated gradients
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One problem with momentum-based gradient descent is that it may overshoot the
    minimum. This can be seen in figure [9.10a](#fig-nesterov-momentum-constant-loss-contours)
    where the loss decreases through a series of updates and then, when we are close
    to the minimum, an update (shown with a dotted arrow) overshoots the minimum and
    increases the loss (shown with a dotted circle).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F10a_Chaudhury.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: (a) Momentum gradient descent overshooting the minimum. Loss decreases for a
    while and then increases dotted circle and arrow)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F10b_Chaudhury.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: (b) Nesterov reduces the step size when we are about to overshoot the minimum.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 Figure (a) shows momentum-based gradient descent overshooting the
    minimum. Over-shooting the update is shown with a dotted arrow. A circle with
    a radius larger than the last step is shown with a dotted outline. Nesterov does
    better by taking smaller steps when overshooting is imminent.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: This is the phenomenon that Nesterovâ€™s accelerated gradient-based optimization
    tries to tackle.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'In Nesterovâ€™s technique, we do the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Estimate where this update will take us (that is, the destination point) by
    adding the previous stepâ€™s update to the current point.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient at the estimated destination point. This is where the approach
    differs from the vanilla momentum-based approach, which takes the gradient at
    the current point.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a weighted average of the gradient (at the estimated destination point)
    with the previous stepâ€™s update. That is the update for the current step.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mathematically speaking,
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-13.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: Equation 9.13
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: where *Î³*, *Î·* are constants with values less than 1. The weights and biases
    are updated in the usual fashion using equation [9.9](#eq-optim-updates).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Why does this help? Well, consider the following:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: When we are somewhat far away from the minimum (no possibility of overshooting),
    the gradient at the estimated destination is more or less the same as the gradient
    at the current point, so we progress toward the minimum in a fashion similar to
    momentum-based gradient descent.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But when we are close to the minimum and the current update may potentially
    take us past it (see the dotted arrow in figure [9.10a](#fig-nesterov-momentum-constant-loss-contours)),
    the gradient at the estimated destination will lie on the other side of the minimum.
    As before, imagine this loss surface as a cone with its apex at the origin and
    base above the apex. We have traveled down one of the coneâ€™s side walls and just
    started climbing back up. Thus the gradient at the estimated destination is in
    the opposite direction from the previous step. When we take their weighted average,
    they will cancel each other in some dimensions, resulting in a smaller magnitude.
    The resulting smaller step will mitigate the overshooting phenomenon.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE Fully functional code for momentum and Nesterov accelerated gradients,
    executable via Jupyter notebook, can be found at [http://mng.bz/p9KR](http://mng.bz/p9KR).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.7 AdaGrad
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The momentum-based optimization approach (equation [9.11](#eq-momentum)) and
    the Nesterov approach (equation [9.13](#eq-nag)) both suffer from a serious drawback:
    they treat all dimensions of the parameter vectors ![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png) equally. But the loss surface is *not* symmetrical
    in all dimensions. The slope can be high along some dimensions and low along others.
    We cannot control everything with a single LR for all the dimensions. If we set
    the LR high, the high-slope dimensions will exhibit too much variance. If we set
    it low, the low-slope dimensions will barely progress toward the minimum.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: What we need is *per-parameter LR*. Then each LR will adapt to the slope of
    its particular dimension. This is what AdaGrad tries to achieve. Dimensions with
    historically larger gradients have smaller LRs, while dimensions with historically
    smaller gradients have larger LRs.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we keep track of the historic magnitudes of gradients? To do this, AdaGrad
    maintains a state vector in which it accumulates the sum of the squared partial
    derivatives for each dimension of the gradient vector seen so far during training:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-13-a.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: where âˆ˜ denotes the Hadamard operator (elementwise multiplication of the two
    vectors). We can express the previous equation a bit more succinctly as
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-13-b.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: Unrolling the recursion, we get
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-13-c.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: 'assuming ![](../../OEBPS/Images/AR_s.png)[âˆ’1] = 0, *![](../../OEBPS/Images/AR_s.png)[t]*
    is a vector that holds the cumulative sum over all training iterations of the
    squared slope for each dimension. For the dimensions with historically high slopes,
    the corresponding element of *![](../../OEBPS/Images/AR_s.png)[t]* is large, and
    vice versa. The overall update vector looks like this:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-14.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: Equation 9.14
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Here *Ïµ* is a very small constant added to prevent division by zero. Then we
    use equation [9.9](#eq-optim-updates) to update the weights as usual. A parallel
    set of equations exist for the bias.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Using AdaGrad, in the loss gradient, the dimensions that have seen big slopes
    in earlier iterations are given less importance during the update. We emphasize
    the dimensions that have not seen much progress in the past. This is a bit like
    saying, â€œI will pay less attention to a person who speaks all the time, and I
    will pay more attention to someone who speaks infrequently.â€
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.8 Root-mean-squared propagation
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A significant drawback of the AdaGrad algorithm is that the magnitude of the
    vector *![](../../OEBPS/Images/AR_s.png)[t]* keeps increasing as iteration progresses.
    This causes the LR for all dimensions to become smaller and smaller. Eventually,
    when the number of iterations is high, the LR becomes close to zero, and the updates
    do hardly anything; progress toward the minimum slows to a virtual halt. Thus
    AdaGrad is an impractical algorithm to use in real life, although the idea of
    per-component LR is good.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Root mean squared propagation (RMSProp) addresses this drawback without sacrificing
    the dimension-adaptive nature of AdaGrad. Here again there is a state vector,
    but its equation is
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-14-a.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: 'Compare this with the state vector equation in AdaGrad:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-14-b.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
- en: 'They are almost the same, but the terms are weighted by (1âˆ’*Î³*) and *Î³*, where
    0 < *Î³* < 1 is a constant. This particular pair of weights in a recursive equation
    has a very interesting effect. To see it, we have to unroll the recursion:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-14-c.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: Thus *![](../../OEBPS/Images/AR_s.png)[t]* is a weighted sum of the past term-wise
    squared gradient magnitude vectors. Going back from now to the beginning of time
    (the 0th iteration), the weights are (1âˆ’*Î³*), (1âˆ’*Î³*) *Î³*, (1âˆ’*Î³*) *Î³*Â², â‹¯, (1âˆ’*Î³*)
    *Î³^t*. If we add these weights, we get
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-14-d.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: As the number of iterations becomes high (*t* â†’ âˆ), this becomes
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-14-e.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: where Taylor expansion has been used to evaluate the term in parentheses.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: For a large number of iterations, the sum of the weights approaches 1. This
    implies that after many iterations, the RMSProp state vector effectively takes
    the *weighted average of the past term-wise squared gradient magnitude vectors*.
    With more iterations, the weights are redistributed and older terms become de-emphasized,
    but the overall magnitude does not increase. This eliminates the vanishing LR
    problem from AdaGrad. RMSProp continues de-emphasizing the dimensions with high
    cumulative partial derivatives with lower LRs, but it does so without making the
    LR vanishingly small.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: The overall RMSProp update equations are
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-15.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: Equation 9.15
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: There is a parallel set of equations for the bias. The weights and bias parameters
    are updated in the usual fashion using equation [9.9](#eq-optim-updates).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.9 Adam optimizer
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Momentum-based gradient descent amplifies the downhill component more and more
    as iterations progress. On the other hand, RMSProp reduces the LR for dimensions
    that are seeing large gradients and vice versa to balance the progress rate along
    all dimensions.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Both of these are desirable properties. We want an optimization algorithm that
    combines them, and that algorithm is Adam. It is increasingly becoming the optimizer
    of choice for most deep learning researchers.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'The Adam optimization algorithm maintains two state vectors:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-16.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: Equation 9.16
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-17.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: Equation 9.17
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'where 0 < *Î²*[1] < 1 and 0 < *Î²*[2] < 1 are constants. Note the following:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Equation [9.16](#eq-adam-vt) is essentially the momentum equation of equation
    [9.11](#eq-momentum) with one significant difference. We have changed the term
    weights to *Î²*[1] and (1âˆ’*Î²*[1]). As weâ€™ve seen, with *t* â†’ âˆ, this results in
    the state vector being a weighted average of all the past gradients. This is an
    improvement over the original momentum scheme.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second state vector is basically the one from the RMSProp equation [9.15](#eq-rmsprop).
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these two state vectors, Adam creates the update vector as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-18.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
- en: Equation 9.18
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: The ![](../../OEBPS/Images/AR_v.png)*[t]* in the numerator pulls in the benefits
    of the momentum approach (with the enhancement of averaging). Otherwise, the equation
    is almost identical to the RMSProp; those benefits are also pulled in.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the weights and bias parameters are updated in the usual fashion using
    equation [9.9](#eq-optim-updates).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Bias correction
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'The sum of weights of past values in the state vectors ![](../../OEBPS/Images/AR_v.png)*[t]*,
    *![](../../OEBPS/Images/AR_s.png)[t]* will approach âˆ only at large values of
    *t*. To improve the approximation at smaller values of *t*, Adam introduces a
    bias correction:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-16.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: Equation 9.19
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-20.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: Equation 9.20
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Instead of ![](../../OEBPS/Images/AR_v.png)*[t]* and *![](../../OEBPS/Images/AR_s.png)[t]*,
    we use the bias-corrected entities *vÌ‚[t]* and *Å[t]* in equation [9.18](#eq-adam-update).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.12 PyTorch code for various optimizers
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: â‘  Sets the learning rate to 0.01
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Sets the momentum to 0.9
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Sets the Nesterov flag to True
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Sets the smoothing constant to 0.99 (*Î³* in [9.15](#eq-rmsprop))
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Regularization
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose we are teaching a baby to recognize cars. We show them red cars, blue
    cars, black cars, large cars, small cars, medium cars, cars with round tops, cars
    with rectangular tops, and so on. Soon the babyâ€™s brain realizes that there are
    too many varieties to remember them all by rote. So the brain starts forming *abstractions*:
    mysterious common features that occur together are stored in the babyâ€™s brain
    with the label *car*. The brain has learned to classify an abstract entity called
    a car. Even though it fails to *remember* every car it has seen, it can *recognize*
    cars. We can say it has developed *experience*. And so it is with neural networks.
    We do not want our network to *remember* every training instance. Rather, we want
    the network to form *abstractions* that will enable it to *recognize* an object
    during inferencing even though the exact likeness of the object instance encountered
    during inferencing was never seen during training.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and underfitting
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: If the network has too much expressive power (too many perceptrons or, equivalently,
    too many weights) relative to the number of training instances, the network can
    and often will rote-remember the training instances. This phenomenon is called
    *overfitting*. Overfitting causes the network to perform very well over the training
    data but badly during testing or real-life inferencing. Stated another way, the
    network has adjusted itself to every nook, bend, and kink of the training data
    and thereby performs great on the training dataâ€”to the detriment of test data
    performance. This is illustrated in figure [9.11](#fig-overfitting). *Regularization*
    refers to a bag of tricks that, in general, try to prevent overfitting. This is
    the topic of this section.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: There is another phenomenon called *underfitting*, where the network simply
    does not have enough expressive power to model the training data. The symptom
    of underfitting is that the network performs badly on both training and testing
    data. If we see this, we should try a more complex network with more perceptrons.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F11_Chaudhury.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11 Overfitting: data points for a binary classifier. Points belonging
    to different classes are visually demarcated as squares and circles. Filled squares/circles
    indicate training data, and unfilled squares/circles indicate test data. There
    are some anomalous training data instances (filled circles in the square zone).
    The estimated decision boundary (solid line) has become wriggly to accommodate
    them, which is causing many test points (unfilled squares/circles) to be misclassified.
    The wiggly solid curve is an example of an overfitted decision boundary. If we
    had chosen a â€œsimplerâ€ decision boundary (dashed straight line), the two anomalous
    training points would have been misclassified, but the machine would have performed
    much better in tests.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '9.3.1 Minimum descriptor length: An Occamâ€™s razor view of optimization'
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Is the set of weights and biases that minimizes a particular loss function unique?
    Letâ€™s examine a single perceptron (equation [7.3](../Text/07.xhtml#eq-perceptron))
    with the output *Ï•*(![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b*). Letâ€™s say *Ï•* is the Heaviside step function (see section [7.3.1](../Text/07.xhtml#step-func)).
    Let ![](../../OEBPS/Images/AR_w.png)[*], *b*[*] be the weights and biases minimizing
    the loss function. It is easy to see that the perceptron output remains the same
    if we scale the weights, like *Î±*![](../../OEBPS/Images/AR_w.png)[*] for all positive
    real values of *Î±*. Thus the weight vector 7![](../../OEBPS/Images/AR_w.png)[*]
    will also minimize the loss function.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'This is true in general for arbitrary neural networks (composed of many perceptrons):
    the set of weights and biases minimizing a loss function is non-unique. How does
    the neural network choose one? Which of them is correct? We can use the principle
    of Occamâ€™s razor to answer that.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Occamâ€™s razor is a philosophical principle. Its literal translation from Latin
    states, â€œEntities should not be multiplied beyond necessity.â€ This is roughly
    taken to mean *among adequate explanations, the simplest one is the best*. In
    machine learning, this principle is typically interpreted as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Among the set of candidate neural network parameter values (weights and biases)
    that minimize the loss, the â€œsimplestâ€ one should be chosen.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: The general idea is as follows. Suppose we are trying to minimize ğ•ƒ(*Î¸*) (here
    we have used *Î¸* to denote ![](../../OEBPS/Images/AR_w.png) and ![](../../OEBPS/Images/AR_b.png)
    together). We also want the solution to be as simple as possible. To achieve that,
    we add a penalty for departing from â€œsimplicityâ€ to the original loss term. Thus
    we minimize
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: ğ•ƒ(*Î¸*) + *Î»R*(*Î¸*)
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Here,
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: The expression *R*(*Î¸*) indicates a measure for un-simplicity. It is sometimes
    called the *regularization penalty*. Adding a regularization penalty to the loss
    incentivizes the network to try to minimize un-simplicity *R*(*Î¸*) (or, equivalently,
    maximize simplicity) while trying to minimize the original loss term ğ•ƒ(*Î¸*).
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Î»* is a hyperparameter. Its value should be carefully chosen via trial and
    error. If *Î»* is too low, this is akin to no regularization, and the network becomes
    prone to overfitting. If *Î»* is too high, the regularization penalty will dominate,
    and the network will not adequately minimize the actual loss term.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two popular ways of estimating *R*(*Î¸*), outlined in sections [9.3.2](#sec-l2-reg)
    and [9.3.3](#sec-l1-reg), respectively. Both try to minimize some norm (length)
    of the parameter vector (which is basically a network descriptor). This is why
    regularization can be viewed as minimizing the descriptor length.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 L2 regularization
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In L2 regularization, we posit that shorter-length vectors are simpler. In other
    words, simplicity is inversely proportional to the square of the L2 norm (aka
    Euclidean norm). Thus,
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '*R*(*Î¸*) = (||![](../../OEBPS/Images/AR_w.png)||Â²+||![](../../OEBPS/Images/AR_b.png)||Â²)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we minimize
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-21.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
- en: Equation 9.21
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Compare this with equation [9.2](#eq-loss).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization is by far the most popular form of regularization. From this
    point on, we often use ğ•ƒ(![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png))
    to mean the L2-regularized version (that is, equation [9.21](#eq-l2-reg)). The
    hyperparameter *Î»* is often called *weight decay* in PyTorch. Weight decay is
    usually set to a small number so that the second term of equation [9.21](#eq-l2-reg)
    the norm of the weight vector) does not drown the actual loss term. The following
    code shows how to instantiate an optimizer with regularization enabled.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.13 PyTorch code to enable L2 regularization
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: â‘  Sets the weight decay to 0.01
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F12a_Chaudhury.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
- en: (a) L1 regularization
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F12b_Chaudhury.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: (b) L2 regularization
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 L1 and L2 regularization
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 L1 regularization
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'L1 regularization is similar in principle to L2 regularization, but it defines
    simplicity as the sum of the absolute values of the weights and biases:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '*R*(*Î¸*) = (|![](../../OEBPS/Images/AR_w.png)|+|![](../../OEBPS/Images/AR_b.png)|)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Thus, here we minimize
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
- en: Equation 9.22
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '9.3.4 Sparsity: L1 vs. L2 regularization'
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: L1 regularization tends to create sparse models where many of the weights are
    0\. In comparison, L2 regularization tends to create models with low (but nonzero)
    weights.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: To understand this, consider figure [9.12](#fig-l1-l2-reg), which plots the
    loss function and its derivative for both L1 and L2 regularization. Let *w* be
    a single element of the weight vector ![](../../OEBPS/Images/AR_w.png). In L1
    regularization,
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-a.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
- en: Since the gradient is constant for all values of *w*, L1 regularization pushes
    the weight toward 0 with the same step size at all values of *w*. In particular,
    the step toward 0 does not reduce in magnitude when close to 0\. In L2 regularization,
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-b.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
- en: Here, the gradient keeps decreasing in magnitude as *w* approaches 0\. Hence,
    *w* comes closer to 0 but may never reach 0 since the updates take smaller and
    smaller steps as *w* approaches 0\. Therefore, L2 regularization produces more
    dense weight vectors than L1 regularization, which produces sparse weight vectors
    with many 0s.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.5 Bayesâ€™ theorem and the stochastic view of optimization
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In sections [6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation) and [6.6.3](../Text/06.xhtml#sec-MAP_estimation),
    we discussed maximum likelihood estimation (MLE) and maximum a posteriori (MAP)
    in the context of unsupervised learning (you are encouraged to revisit those sections
    if necessary). Here, we study them in the context of supervised learning.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: In the supervised optimization process, we have samples of the input and known
    output pairs (in the form of training data) âŸ¨![](../../OEBPS/Images/AR_x.png)^((*i*)),
    *È³*^((*i*))âŸ©. Of course, we can do the forward pass and generate the network output
    at each training data point
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-c.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
- en: where *Î¸* is the networkâ€™s parameter set (representing weights and biases together).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we view the sample training data generation as a stochastic process.
    We can model the probability of a training instance *T*^((*i*)) = âŸ¨*È³*^((*i*)),
    ![](../../OEBPS/Images/AR_x.png)^((*i*))âŸ© given the current network parameters
    *Î¸* as
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-d.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
- en: This makes intuitive sense. At an optimal value of *Î¸*, the network output *f*(![](../../OEBPS/Images/AR_x.png)^((*i*)),
    *Î¸*) would match the GT *È³*^((*i*)). We want our model distribution to have the
    highest probability density at the location where the network output matches the
    GT. The probability density should fall off with distance from that location.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: A little thought reveals that this Gaussian-like formulation, which leads to
    a regression-loss-like numerator, is not the only one possible. We can use any
    loss function in the numerator since all of them have the property of being minimum
    when the network output matches the GT and gradually increase as the mismatch
    grows. In general,
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-e.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
- en: Assuming the training instances are mutually independent, the probability of
    the entire training dataset occurring jointly is the product of individual instance
    probabilities. If we denote the entire training dataset as *T*,
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-f.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: Then
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-g.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
- en: At this point, we can take one of two possible approaches described in the next
    two subsections.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: MLE-based optimization
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, we choose the optimal value for the parameter set *Î¸* by maximizing
    the likelihood
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-h.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
- en: This is equivalent to saying we will choose the optimal parameters *Î¸* such
    that the probability of occurrence of the training data is maximized. Thus the
    optimal parameter set *Î¸*[*] is yielded by
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-i.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
- en: Obviously, the optimal *Î¸* that maximizes the likelihood is the one that minimizes
    ğ•ƒ(*Î¸*). So, the maximum likelihood formulation is nothing but minimizing the total
    mismatch between predicted and GT output over the entire training dataset.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: MAP optimization
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: By Bayesâ€™ theorem (equation [6.1](../Text/06.xhtml#eq-bayes-theorem-2var)),
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-j.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
- en: To estimate the optimal *Î¸*, we can also maximize the posterior probability
    on the left side of this equation. This is equivalent to saying we will choose
    the optimal parameters *Î¸* such that *Î¸* has the maximal conditional probability
    given the training dataset. Thus the optimal value for the parameter set *Î¸* is
    yielded by
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-k.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
- en: where the last equality is derived via Bayesâ€™ theorem.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Observing the previous equation, we see that the denominator *p*(*T*) in the
    rightmost term does not involve *Î¸* and hence can be dropped from the optimization.
    So,
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-l.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
- en: How do we model the *a priori* probability *p*(*Î¸*)? We can use Occamâ€™s razor
    and say that we assign a higher *a priori* probability to smaller parameter values.
    Thus, we can say
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-m.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
- en: Then the overall posterior probability maximization becomes
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-n.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
- en: NOTE Maximizing this posterior probability is equivalent to minimizing the regularized
    loss. Maximizing the likelihood is equivalent to minimizing the un-regularized
    loss.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.6 Dropout
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the introduction for section [9.3](../Text/09.xhtml#sec-regularization),
    we saw that too much expressive power (too many perceptron nodes) sometimes prevents
    the machine from developing general *abstractions* (aka *experience*). Instead,
    the machine may remember the training data instances (see figure [9.11](#fig-overfitting)).
    This phenomenon is called overfitting. We have already seen that one way to mitigate
    this problem is to add a regularization penaltyâ€”such as adding the L2 norm of
    the weightsâ€”to the loss to discourage the network from learning large weight values.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is another method of regularization. Here, somewhat crazily, we turn
    off random perceptrons in the network (set their value to 0) during training.
    To be precise, we attach a probability *p[i]^l* with the *i*th node (perceptron)
    in layer *l*. In any training iteration, the node (perceptron) is off with a probability
    of (1âˆ’*p*). Typically, dropout is only enabled during training and is turned off
    during inferencing.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: What good does it do? Well,
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Dropout prevents the network from relying too much on a small number of nodes.
    Instead, the network is forced to use all the nodes.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equivalently, dropout encourages the training process to spread the weights
    to multiple nodes instead of putting much weight on a few nodes. This makes the
    effect somewhat similar to L2 regularization.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dropout mitigates *co-adaptation*: a behavior whereby a group of nodes in the
    network behave in a highly correlated fashion, emitting similar outputs most of
    the time. This means the network could retain only one of them with no significant
    loss of accuracy.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout simulates an ensemble of subnetworks
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a small three-node intermediate layer of some neural network with
    dropout. The *k*th input to this layer can turn on with probability *p[k]*. This
    means the probability of that input node turning off is (1âˆ’*p[k]*). We can express
    this with a *binary* stochastic variable *Î´[k]* for *k* = 0 or *k* = 1 or *k*
    = 2. This variable *Î´[k]* takes one of two possible values: 0 or 1. The probability
    of it taking the value 1 is *p[k]*. In other words, *p*(*Î´[k]* = 1) = *p[k]*,
    and *p*(*Î´[k]* = 0) = 1 âˆ’ *p[k]*. The output of this small three-node layer with
    dropout can be expressed as'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-o.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
- en: 'We have three variables *Î´*[0], *Î´*[1], and *Î´*[2], each of which can take
    two values. Altogether we have 2Â³ = 8 possible combinations. Each combination
    leads to a subnode shown in figure [9.13](#fig-dropout_3layer). Each of these
    combinations has a probability of occurrence *P[i]*, also shown in the figure.
    These observations lead to a very important insight:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: The expected value of the outputâ€”that is, ğ”¼(*a[l]*)â€”is the same as the expected
    value of the output if we deployed the subnetworks in figure [9.13](#fig-dropout_3layer)
    randomly, with probabilities *P[i]*.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: Why does this matter? Well, given a problem, none of us know the right number
    of perceptrons for the network to deploy. One thing to do under these circumstances
    is to deploy networks of various strengths randomly and take an average of their
    outputs. We have just established that dropout (turning inputs off randomly) achieves
    the same thing. But deploying a network where inputs get turned on or off randomly
    is much simpler than deploying a large number of subnetworks. All we have to do
    is deploy a *dropout* layer.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch code for dropout
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: In section [9.2.3](#sec-pytorch-sgd), we created a simple two-layer neural network
    classifier to predict whether a Statsville resident is a man, woman, or child
    based on height and weight data. In this section, we show the same model with
    dropout layers added. Note that dropout should be enabled only during training,
    not during inferencing. To do this in PyTorch, you can call `model.eval()` before
    running inferencing. This way, your training and inferencing code remains the
    same, but PyTorch knows under the hood when to include the dropout layers and
    when not to.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.14 Dropout
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: â‘  Instantiates a dropout layer with a probability of dropout = 0.2
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F13a_Chaudhury.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
- en: '(a) Subnetwork 0: probability *P*[0] = (1âˆ’*p*[0]) (1âˆ’*p*[1]) (1âˆ’*p*[2])'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F13b_Chaudhury.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
- en: '(b) Subnetwork 1: probability *P*[1] = (1âˆ’*p*[0]) (1âˆ’*p*[1]) *p*[2]'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F13c_Chaudhury.png)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
- en: '(c) Subnetwork 2: probability *P*[2] = (1âˆ’*p*[0] )*p*[1] (1âˆ’*p*[2])'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F13d_Chaudhury.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
- en: '(d) Subnetwork 3: probability *P*[3] = (1âˆ’*p*[0] )*p*[1] *p*[2]'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F13e_Chaudhury.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
- en: '(e) Subnetwork 4: probability *P*[4] = *p*[0] (1âˆ’*p*[1]) (1âˆ’*p*[2])'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F13f_Chaudhury.png)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
- en: '(f) Subnetwork 5: probability *P*[5] = *p*[0] (1âˆ’*p*[1]) *p*[2]'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F13g_Chaudhury.png)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
- en: '(g) Subnetwork 6: probability *P*[6] = *p*[0] *p*[1] (1âˆ’*p*[2])'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F13h_Chaudhury.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
- en: '(h) Subnetwork 7: probability *P*[7] = *p*[0] *p*[1] *p*[2]'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.13 Dropout simulates subnetworks: illustrated with a three-node intermediate
    layer of a neural network. The probability of input node *a[k]*^((*l* âˆ’ 1)) being
    *on* is *p*(*Î´[k]* = 1) = *p[k]*.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training is the process by which a neural network identifies the optimal values
    for its parameters (weights and biases of the individual perceptrons). Training
    progresses iteratively: in each iteration, we estimate the loss and run an optimization
    step that updates the parameter values so as to decrease the loss. After doing
    this many times, we hope to arrive at optimal parameter values.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a supervised neural network, loss quantifies the mismatch between the desired
    output and the network output over sampled training data instances. The desired
    output (ground truth) is often estimated via manual effort. Training the neural
    network essentially identifies the weights and biases of the neural network that
    minimize the loss.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discrepancy between the ground truth and network output can be expressed
    in many different ways. Each corresponds to a different loss function. Denoting
    the ground truth probabilities of all the possible classes given the *i*th training
    input ![](../../OEBPS/Images/AR_x.png)^((*i*)) as a vector *È³*^((*i*)) and the
    network output on the same as *y*^((*i*)),
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression loss takes the L2 norm of the vector difference between the network
    output and ground-truth vectors and is mathematically expressed as ![](../../OEBPS/Images/eq_09-22-p1.png).
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the neural network is a classifier, it typically outputs a vector of class
    probabilities. This means
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-p2.png)'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: 'where *p[j]*(![](../../OEBPS/Images/AR_x.png)^((*i*))) is the network estimated
    probability of the input belonging to class *j*, and *N* is the number of possible
    classes. In general, given an input, a neural network computes per-class scores:
    the class with the highest score is the predicted class. The scores are unbounded
    numbers and can be arbitrarily large or small (even negative). The operation converts
    them into probabilities. If'
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-p3.png)'
  id: totrans-485
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: denotes the score vector, the corresponding softmax vector is ![](../../OEBPS/Images/eq_09-22-p4.png)
  id: totrans-486
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: where ![](../../OEBPS/Images/eq_09-22-p5.png). The softmax output vector consists
    of probabilities, meaning they are numbers between 0 and 1 and they sum to 1.
  id: totrans-487
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Given the probability of each class, classifiers can employ the cross-entropy
    loss, which can be expressed as
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_09-22-p6.png).'
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note that since the ground-truth vector is one-hot, only a single term in this
    expression survives: the one corresponding to the desired class, denoted *j*^*.
    The loss becomes the logarithm of the corresponding network output, âˆ’*log*(*y*[*j*^*]^((*i*))),
    and is zero if *y*[*j*^*]^((*i*)) = 1. This agrees with our expectation: if the
    network is predicting the GT class with probability 1, there is no loss.'
  id: totrans-490
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Since during training, softmax is almost always followed by cross-entropy loss,
    PyTorch has a combined operator called softmax cross-entropy loss. It is over
    doing these operations individually because it has better numerical properties.
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Focal loss tries to tackle the data-imbalance problem by putting more weight
    on the â€œhardâ€ examples with higher loss.
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinge loss is another popular loss that becomes zero when the correct class
    is predicted with the maximum score. Once that criterion is achieved, it no longer
    tries to improve the relative values of scores.
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total loss can be obtained by adding the losses from all the individual training
    data instances. However, this requires us to process all the training data points
    in every iteration, which is prohibitively expensive. Hence, we sample a subset
    of training data points to create a minibatch, estimate losses for each input
    data instance in the minibatch, and add them to obtain an estimate for the total
    loss. This process is known as stochastic gradient descent (SGD).
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization is the process of updating the neural network parameters (weights
    and biases of perceptrons) so as to reduce the loss. We can plot the loss against
    weights and bias values and obtain a hypersurface defined over the domain of weights
    and bias values. Our ultimate goal is to reach the bottom (minimum point) of this
    loss hypersurface. Optimization is geometrically equivalent to moving down the
    hypersurface to reduce the loss. The steepest descent (toward the nearest minimum)
    happens along a negative gradient.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can combine several other criteria with the gradient to improve the convergence
    to the minimum loss value. Each of them results in a different optimization technique:'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to noisy estimations, the local gradient may not always point toward the
    minimum, but it will have a strong component toward that minimum along with other
    noisy components. Instead of blindly following the current gradient, we can follow
    the direction corresponding to a weighted average of the current gradient estimate
    and the gradient estimate from the previous iteration. This is a recursive process,
    which effectively means the direction followed at any iteration is a weighted
    average of the current and all gradient estimates from the beginning of training.
    Recent gradients are weighted higher, and older gradients are weighed lower. All
    these gradients have strong components toward the minimum that reinforce each
    other, while the noisy components point in random directions and tend to cancel
    each other. Thus the overall weighted sum is a more reliable move toward the minimum.
    This is called momentum-based gradient descent.
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A drawback of momentum is that it can result in overshooting the minimum. Nesterov
    accelerated gradients correct this drawback by calculating gradients via one-step
    lookahead. If the current update takes us to the other side of the minimum, the
    gradient will have an opposite direction there. We take a weighted average of
    the update suggested by momentum-based gradient descent and the gradient at the
    estimated destination point. If we are far from the minimum, it will be roughly
    equivalent to momentum-based gradient descent. But if we are about to overshoot
    the minimum, there will be cancelation, and the update will be smaller than that
    of the pure momentum case. Thus we reduce the chances of overshooting the minimum.
  id: totrans-498
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaGrad is an optimization technique that imparts additional weight to infrequently
    changing axes of the loss hypersurface. The Adam optimizer combines many advantages
    of other optimizers and is often the modern optimizer of choice.
  id: totrans-499
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The principle of Occamâ€™s razor essentially says that among adequate explanations,
    the simplest one should be preferred. In machine learning, this leads to regularization.
    There are typically many solutions to the loss-minimization problem. We want to
    choose the simplest one. Accordingly, we add a penalty for departing from simplicity
    (regularization loss) to whatever other losses we have. This incentivizes the
    system to go for the simplest solution. Regularization loss is often the total
    length of the parameter vector; thus, regularization pushes us toward solutions
    with smaller absolute values for parameters.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing the loss function without the regularization term is equivalent to
    the Bayesian maximum likelihood estimation (MLE) technique. On the other hand,
    minimizing loss with the regularized term is equivalent to maximum a posteriori
    (MAP) estimation.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting is a phenomenon where the neural network has learned all the nuances
    of the training data. Since anomalous nuances in training data points are often
    caused by noise, this leads to worse performance during inferencing. Overfitting
    is symptomized by great accuracy (low loss) on training data but bad accuracy
    on test data. It often happens because the network has more expressive power than
    necessary for the problem and is trying to memorize all nooks and bends of the
    training data. Regularization mitigates the problem. Another trick is dropout,
    where we deliberately turn off a random subset of neurons during training iterations.
    This forces all the neurons to learn to do the job with a reduced set of neurons.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
