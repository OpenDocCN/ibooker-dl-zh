["```py\nfrom pydantic import BaseModel\n\nclass BatchDocumentClassification(BaseModel):\n    class Category(BaseModel):\n        document_id: str\n        category: list[str]\n\n    categories: list[Category] ![1](assets/1.png)\n```", "```py\nimport json\nfrom uuid import UUID\n\ndef create_batch_file(\n    entries: list[str],\n    system_prompt: str,\n    model: str = \"gpt-4o-mini\",\n    filepath: str = \"batch.jsonl\",\n    max_tokens: int = 1024,\n) -> None:\n    with open(filepath, \"w\") as file:\n        for _, entry in enumerate(entries, start=1):\n            request = {\n                \"custom_id\": f\"request-{UUID()}\",\n                \"method\": \"POST\",\n                \"url\": \"/v1/chat/completions\",\n                \"body\": {\n                    \"model\": model,\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": system_prompt,\n                        },\n                        {\"role\": \"user\", \"content\": entry},\n                    ],\n                    \"max_tokens\": max_tokens,\n                },\n            }\n            file.write(json.dumps(request) + \"\\n\")\n```", "```py\nfrom loguru import logger\nfrom openai import AsyncOpenAI\nfrom openai.types import Batch\n\nclient = AsyncOpenAI()\n\nasync def submit_batch_job(filepath: str) -> Batch:\n    if \".jsonl\" not in filepath:\n        raise FileNotFoundError(f\"JSONL file not provided at {filepath}\")\n\n    file_response = await client.files.create(\n        file=open(filepath, \"rb\"), purpose=\"batch\"\n    )\n\n    batch_job_response = await client.batches.create(\n        input_file_id=file_response.id,\n        endpoint=\"/v1/chat/completions\",\n        completion_window=\"24h\",\n        metadata={\"description\": \"document classification job\"},\n    )\n    return batch_job_response\n\nasync def retrieve_batch_results(batch_id: str):\n    batch = await client.batches.retrieve(batch_id)\n    if (\n        status := batch.status == \"completed\"\n        and batch.output_file_id is not None\n    ):\n        file_content = await client.files.content(batch.output_file_id)\n        return file_content\n    logger.warning(f\"Batch {batch_id} is in {status} status\")\n```", "```py\n$ pip install \"fastapi-cache2[redis]\"\n```", "```py\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\n\nfrom fastapi import FastAPI\nfrom fastapi_cache import FastAPICache\nfrom fastapi_cache.backends.redis import RedisBackend\nfrom redis import asyncio as aioredis\n\n@asynccontextmanager\nasync def lifespan(_: FastAPI) -> AsyncIterator[None]:\n    redis = aioredis.from_url(\"redis://localhost\")\n    FastAPICache.init(RedisBackend(redis), prefix=\"fastapi-cache\") ![1](assets/1.png)\n    yield\n\napp = FastAPI(lifespan=lifespan)\n```", "```py\nfrom fastapi import APIRouter\nfrom fastapi_cache.decorator import cache\n\nrouter = APIRouter(prefix=\"/generate\", tags=[\"Resource\"])\n\n@cache()\nasync def classify_document(title: str) -> str:\n    ...\n\n@router.post(\"/text\")\n@cache(expire=60) ![1](assets/1.png)\nasync def serve_text_to_text_controller():\n    ...\n```", "```py\nimport uuid\nfrom qdrant_client import AsyncQdrantClient, models\nfrom qdrant_client.http.models import Distance, PointStruct, ScoredPoint\n\nclass CacheClient:\n    def __init__(self):\n        self.db = AsyncQdrantClient(\":memory:\") ![1](assets/1.png)\n        self.cache_collection_name = \"cache\"\n\n    async def initialize_database(self) -> None:\n        await self.db.create_collection(\n            collection_name=self.cache_collection_name,\n            vectors_config=models.VectorParams(\n                size=384, distance=Distance.EUCLID\n            ),\n        )\n\n    async def insert(\n        self, query_vector: list[float], documents: list[str]\n    ) -> None:\n        point = PointStruct(\n            id=str(uuid.uuid4()),\n            vector=query_vector,\n            payload={\"documents\": documents},\n        )\n        await self.db.upload_points(\n            collection_name=self.cache_collection_name, points=[point]\n        )\n\n    async def search(self, query_vector: list[float]) -> list[ScoredPoint]:\n        return await self.db.search(\n            collection_name=self.cache_collection_name,\n            query_vector=query_vector,\n            limit=1,\n        )\n```", "```py\nfrom qdrant_client import AsyncQdrantClient, models\nfrom qdrant_client.http.models import Distance, ScoredPoint\n\ndocuments = [...] ![1](assets/1.png)\n\nclass DocumentStoreClient:\n    def __init__(self, host=\"localhost\", port=6333):\n        self.db_client = AsyncQdrantClient(host=host, port=port)\n        self.collection_name = \"docs\"\n\n    async def initialize_database(self) -> None:\n        await self.db_client.create_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(\n                size=384, distance=Distance.EUCLID\n            ),\n        )\n        await self.db_client.add(\n            documents=documents, collection_name=self.collection_name\n        )\n\n    async def search(self, query_vector: list[float]) -> list[ScoredPoint]:\n        results = await self.db_client.search(\n            query_vector=query_vector,\n            limit=3,\n            collection_name=self.collection_name,\n        )\n        return results\n```", "```py\nimport time\nfrom loguru import logger\nfrom transformers import AutoModel\n\n...\n\nclass SemanticCacheService:\n    def __init__(self, threshold: float = 0.35):\n        self.embedder = AutoModel.from_pretrained(\n            \"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True\n        )\n        self.euclidean_threshold = threshold\n        self.cache_client = CacheClient()\n        self.doc_db_client = DocumentStoreClient()\n\n    def get_embedding(self, question) -> list[float]:\n        return list(self.embedder.embed(question))[0]\n\n    async def initialize_databases(self):\n        await self.cache_client.initialize_databases()\n        await self.doc_db_client.initialize_databases()\n\n    async def ask(self, query: str) -> str:\n        start_time = time.time()\n        vector = self.get_embedding(query)\n        if search_results := await self.cache_client.search(vector):\n            for s in search_results:\n                if s.score <= self.euclidean_threshold: ![1](assets/1.png)\n                    logger.debug(f\"Found cache with score {s.score:.3f}\")\n                    elapsed_time = time.time() - start_time\n                    logger.debug(f\"Time taken: {elapsed_time:.3f} seconds\")\n                    return s.payload[\"content\"]\n\n        if db_results := await self.doc_db_client.search(vector): ![2](assets/2.png)\n            documents = [r.payload[\"content\"] for r in db_results]\n            await self.cache_client.insert(vector, documents)\n            logger.debug(\"Query context inserted to Cache.\")\n            elapsed_time = time.time() - start_time\n            logger.debug(f\"Time taken: {elapsed_time:.3f} seconds\")\n\n        logger.debug(\"No answer found in Cache or Database.\")\n        elapsed_time = time.time() - start_time\n        logger.debug(f\"Time taken: {elapsed_time:.3f} seconds\")\n        return \"No answer available.\" ![3](assets/3.png)\n```", "```py\nasync def main():\n    cache_service = SemanticCacheService()\n    query_1 = \"How to build GenAI services?\"\n    query_2 = \"What is the process for developing GenAI services?\"\n\n    cache_service.ask(query_1)\n    cache_service.ask(query_2)\n\nasyncio.run(main())\n\n# Query 1:\n# Query added to Cache.\n# Time taken: 0.822 seconds\n\n# Query 2:\n# Found cache with score 0.329\n# Time taken: 0.016 seconds\n```", "```py\n$ pip install gptcache\n```", "```py\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncIterator\n\nfrom fastapi import FastAPI\nfrom gptcache import Config, cache\nfrom gptcache.embedding import Onnx\nfrom gptcache.processor.post import random_one\nfrom gptcache.processor.pre import last_content\nfrom gptcache.similarity_evaluation import OnnxModelEvaluation\n\n@asynccontextmanager\nasync def lifespan(_: FastAPI) -> AsyncIterator[None]:\n    cache.init(\n        post_func=random_one, ![1](assets/1.png)\n        pre_embedding_func=last_content, ![2](assets/2.png)\n        embedding_func=Onnx().to_embeddings, ![3](assets/3.png)\n        similarity_evaluation=OnnxModelEvaluation(), ![4](assets/4.png)\n        config=Config(similarity_threshold=0.75), ![5](assets/5.png)\n    )\n    cache.set_openai_key() ![6](assets/6.png)\n    yield\n\napp = FastAPI(lifespan=lifespan)\n```", "```py\nimport time\nfrom openai import OpenAI\nclient = OpenAI()\n\nquestion = \"what's FastAPI\"\nfor _ in range(2):\n    start_time = time.time()\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}],\n    )\n    print(f\"Question: {question}\")\n    print(\"Time consuming: {:.2f}s\".format(time.time() - start_time))\n    print(f\"Answer: {response.choices[0].message.content}\\n\")\n```", "```py\nfrom anthropic import Anthropic\n\nclient = Anthropic()\n\nresponse = client.messages.create(\n    model=\"claude-3-7-sonnet-20250219\", ![1](assets/1.png)\n    max_tokens=1024,\n    system=[\n        {\n            \"type\": \"text\",\n            \"text\": \"You are an AI assistant\",\n        },\n        {\n            \"type\": \"text\",\n            \"text\": \"<the entire content of a large document>\",\n            \"cache_control\": {\"type\": \"ephemeral\"}, ![2](assets/2.png)\n        },\n    ],\n    messages=[{\"role\": \"user\", \"content\": \"Summarize the documents in ...\"}],\n)\nprint(response)\n```", "```py\n$ pip install google-generativeai\n```", "```py\nimport datetime\nimport google.generativeai as genai\nfrom google.generativeai import caching\n\ngenai.configure(api_key=\"your_gemini_api_key\")\n\ncorpus = genai.upload_file(path=\"corpus.txt\")\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='fastapi', ![1](assets/1.png)\n    system_instruction=(\n        \"You are an expert AI engineer, and your job is to answer \"\n        \"the user's query based on the files you have access to.\"\n    ),\n    contents=[corpus], ![2](assets/2.png)\n    ttl=datetime.timedelta(minutes=5),\n)\n\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)\nresponse = model.generate_content(\n    [\n        (\n            \"Introduce different characters in the movie by describing \"\n            \"their personality, looks, and names. Also list the timestamps \"\n            \"they were introduced for the first time.\"\n        )\n    ]\n)\n```", "```py\n>> print(response.usage_metadata)\n\nprompt_token_count: 696219\ncached_content_token_count: 696190\ncandidates_token_count: 214\ntotal_token_count: 696433\n```", "```py\n$ pip install auto-gptq optimum transformers accelerate\n```", "```py\nimport torch\nfrom optimum.gptq import GPTQQuantizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"facebook/opt-125m\" ![1](assets/1.png)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, torch_dtype=torch.float16\n)\n\nquantizer = GPTQQuantizer(\n    bits=4,\n    dataset=\"c4\", ![2](assets/2.png)\n    block_name_to_quantize=\"model.decoder.layers\", ![3](assets/3.png)\n    model_seqlen=2048, ![4](assets/4.png)\n)\nquantized_model = quantizer.quantize_model(model, tokenizer)\n```", "```py\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\n\nclient = AsyncOpenAI()\n\nclass DocumentClassification(BaseModel): ![1](assets/1.png)\n    category: str = Field(..., description=\"The category of the classification\")\n\nasync def get_document_classification(\n    title: str,\n) -> DocumentClassification | str | None:\n    response = await client.beta.chat.completions.parse(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"classify the provided document into the following: ...\",\n            },\n            {\"role\": \"user\", \"content\": title},\n        ],\n        response_format=DocumentClassification, ![2](assets/2.png)\n    )\n\n    message = response.choices[0].message\n    return message.parsed if message.parsed is not None else message.refusal\n```", "```py\nimport json\nfrom loguru import logger\nfrom openai import AsyncOpenAI\nclient = AsyncOpenAI()\n\nsystem_template = \"\"\"\nClassify the provided document into the following: ...\n\nProvide responses in the following manner json: {\"category\": \"string\"}\n\"\"\"\n\nasync def get_document_classification(title: str) -> dict:\n    response = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        max_tokens=1024, ![1](assets/1.png)\n        messages=[\n            {\"role\": \"system\", \"content\": system_template},\n            {\"role\": \"user\", \"content\": title},\n            {\n                \"role\": \"assistant\",\n                \"content\": \"The document classification JSON is {\", ![2](assets/2.png)\n            },\n        ],\n    )\n    message = response.choices[0].message.content or \"\"\n    try:\n        return json.loads(\"{\" + message[: message.rfind(\"}\") + 1]) ![3](assets/3.png)\n    except json.JSONDecodeError: ![4](assets/4.png)\n        logger.warning(f\"Failed to parse the response: {message}\")\n    return {\"error\": \"Refusal response\"}\n```", "```py\nfrom openai import OpenAI\nfrom scraper import fetch\nclient = OpenAI()\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"fetch\",\n            \"description\": \"Read the content of url and provide a summary\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"url\": {\n                        \"type\": \"string\",\n                        \"description\": \"The url to fetch\",\n                    },\n                },\n                \"required\": [\"url\"],\n                \"additionalProperties\": False,\n            },\n        },\n    }\n]\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful customer support assistant\"\n        \"Use the supplied tools to assist the user.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Summarize this paper: https://arxiv.org/abs/2207.05221\",\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    tools=tools,\n)\n```", "```py\n// training_data.jsonl\n\n{\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"<text>\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"<text>\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"<text>\"\n        }\n    ]\n}\n// more entries\n```", "```py\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.files.create(\n    file=open(\"mydata.jsonl\", \"rb\"), purpose=\"fine-tune\"\n)\n\nclient.fine_tuning.jobs.create(\n    training_file=response.id, model=\"gpt-4o-mini-2024-07-18\"\n)\n```", "```py\nfrom openai import OpenAI\nclient = OpenAI()\n\nfine_tuning_job_id = \"ftjob-abc123\"\nresponse = client.fine_tuning.jobs.retrieve(fine_tuning_job_id)\nfine_tuned_model = response.fine_tuned_model\n\nif fine_tuned_model is None:\n    raise ValueError(\n        f\"Failed to retrieve the fine-tuned model - \"\n        f\"Job ID: {fine_tuning_job_id}\"\n    )\n\ncompletion = client.chat.completions.create(\n    model=fine_tuned_model,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n)\nprint(completion.choices[0].message)\n```"]