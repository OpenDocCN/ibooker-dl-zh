- en: '6 Progression of RAG systems: Naïve, advanced, and modular RAG'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 RAG系统进展：简单、高级和模块化RAG
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Limitations of the naïve RAG approach
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单RAG方法局限性
- en: Advanced RAG strategies and techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级RAG策略和技术
- en: Modular patterns in RAG
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG中的模块化模式
- en: In the first two parts of this book, you learned about the utility of retrieval-augmented
    generation (RAG), along with the development and evaluation of a basic RAG system.
    The basic, or the naïve RAG approach that we have discussed is, generally, inadequate
    when it comes to production-grade systems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前两部分中，你学习了检索增强生成（RAG）的效用，以及基本RAG系统的开发和评估。我们讨论的基本或简单的RAG方法，在面向生产级系统时通常是不够的。
- en: This chapter focuses on more advanced concepts in RAG. We begin by revisiting
    the limitations and the points of failure of the naïve RAG approach. Next, we
    discuss the failures at the retrieval, augmentation, and generation stages. Advanced
    strategies and techniques to address these points of failure will be elaborated
    on in distinct phases of the RAG pipeline.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍RAG的更高级概念。我们首先回顾简单RAG方法的局限性和失败点。接下来，我们讨论检索、增强和生成阶段的失败。在RAG管道的不同阶段，将详细阐述解决这些失败点的先进策略和技术。
- en: Better indexing of the knowledge base leads to better RAG outcomes. We will
    look at a few data indexing strategies that build on the naïve indexing pipeline
    to improve the searchability of the knowledge base.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的知识库索引导致更好的RAG结果。我们将探讨一些基于简单索引管道构建的数据索引策略，以改善知识库的可搜索性。
- en: 'In the generation pipeline, improvements are examined in three stages: pre-retrieval,
    retrieval, and post-retrieval. Pre-retrieval techniques focus on manipulating
    and improving the input user query. Retrieval strategies focus on better matching
    of the user query to the documents in the knowledge base. Finally, in the post-retrieval
    stage, the focus is on aligning the retrieved context with the desired result
    and making it suitable for generation.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成管道中，改进被分为三个阶段：检索前、检索和检索后。检索前技术侧重于操作和改进用户查询的输入。检索策略侧重于更好地匹配用户查询与知识库中的文档。最后，在检索后阶段，重点是使检索到的上下文与期望的结果对齐，并使其适合生成。
- en: The last part of the chapter discusses a modular approach to RAG that has been
    emerging to find applicability in RAG systems. The modular approach is an architectural
    enhancement to the basic RAG system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后部分讨论了一种模块化方法，该方法正在出现并试图在RAG系统中找到适用性。模块化方法是对基本RAG系统的架构增强。
- en: Note that the strategies and techniques for RAG improvement are expansive, and
    this chapter highlights a few popular ones. The chapter is interspersed with code
    examples, but for a more exhaustive supporting code, check out the source code
    repository of this book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，RAG改进的策略和技术是广泛的，本章突出了一些流行的策略。本章穿插了代码示例，但如需更全面的辅助代码，请查看本书的源代码仓库。
- en: By the end of this chapter, you should
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该
- en: Understand why the naïve approach to RAG is not suitable for production.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解为什么简单的RAG方法不适合生产。
- en: Be aware of indexing strategies that make the RAG knowledge base more efficient.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解使RAG知识库更高效的索引策略。
- en: Know some of the popular pre-retrieval, retrieval, and post-retrieval techniques.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解一些流行的检索前、检索和检索后技术。
- en: Be familiar with the modular approach to RAG.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉RAG的模块化方法。
- en: RAG powers a variety of AI applications. However, there is a certain aspect
    of uncertainty when it comes to outcomes. Inaccuracies in retrieval, disjointed
    context, and incoherence in the LLM outputs need to be addressed before taking
    RAG to production. In a very short time, researchers and practitioners have experimented
    with innovative techniques to improve the relevance and faithfulness of RAG systems.
    But before we look at these techniques, it is important to understand why a naïve
    RAG approach often doesn’t find its way into a production environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RAG（检索增强生成）为各种人工智能应用提供动力。然而，在结果方面存在一定的不确定性。在将RAG应用于生产之前，需要解决检索中的不准确、上下文不连贯以及LLM（大型语言模型）输出中的不连贯问题。在很短的时间内，研究人员和从业者已经尝试了创新技术来提高RAG系统的相关性和忠实度。但在我们探讨这些技术之前，了解为什么简单的RAG方法通常无法进入生产环境是很重要的。
- en: 6.1 Limitations of naïve RAG
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 简单RAG局限性
- en: Naïve RAG can be thought of as the earliest form of RAG, which gained popularity
    after the release of ChatGPT and the rise of LLM technology. As we have seen so
    far, it follows a linear process of indexing, retrieving, augmenting, and generation.
    This process falls in a “retrieve then read” framework, which means that there’s
    a retriever retrieving information and that there’s an LLM reading this information
    to generate the results, as shown in figure 6.1.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 天真的RAG可以被认为是RAG的最早形式，它在ChatGPT发布和LLM技术兴起后变得流行。如我们所见，它遵循索引、检索、增强和生成的线性过程。这个过程属于“检索后读取”框架，这意味着有一个检索器检索信息，还有一个LLM读取这些信息以生成结果，如图6.1所示。
- en: '![](../Images/CH06_F01_Kimothi.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F01_Kimothi.png)'
- en: Figure 6.1  Naïve RAG is a sequential “retrieve then read” process.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1  天真的RAG是一个顺序的“检索后读取”过程。
- en: 'The naïve RAG approach is marred with drawbacks at each of the three stages:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 天真的RAG方法在三个阶段都存在缺陷：
- en: '*Retrieval*—Naïve retrieval is often observed to have low precision that leads
    to irrelevant information being retrieved. It also has a low recall, which means
    that relevant information is missed, which leads to incomplete results.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索*—天真的检索通常观察到精度低，导致检索到不相关信息。它还有低召回率，这意味着相关信息被遗漏，导致结果不完整。'
- en: '*Augmentation*—There is a real possibility of redundancy and repetition when
    multiple retrieved documents have similar information. Also, when information
    is sourced from different documents, the context becomes disjointed. There’s also
    the problem of context length of the LLMs that has an effect on the volume of
    retrieved context that can be passed on to the LLM for generation.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增强*—当多个检索到的文档包含相似信息时，存在冗余和重复的真正可能性。此外，当信息来自不同的文档时，上下文变得不连贯。还有LLM上下文长度的问题，它会影响传递给LLM用于生成的检索上下文的数量。'
- en: '*Generation*—With the inadequacies of the upstream processes, the generation
    suffers from hallucination and lack of groundedness of the generated content.
    The LLM faces challenge in reconciling information. The challenges of toxicity
    and bias also persist. It is also noticed sometimes that the LLM becomes over-reliant
    on the retrieved context and forgets to draw from its own parametric memory.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成*—由于上游过程的不足，生成过程受到幻觉和生成内容缺乏根基的影响。LLM在协调信息时面临挑战。毒性和偏见的问题也持续存在。有时也注意到LLM过度依赖检索到的上下文，并忘记从其自身的参数记忆中提取信息。'
- en: Figure 6.2 summarizes these drawbacks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2总结了这些缺陷。
- en: '![A black box with black text'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个带有黑色文字的黑盒'
- en: AI-generated content may be incorrect.](../Images/CH06_F02_Kimothi.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH06_F02_Kimothi.png)
- en: Figure 6.2  Drawbacks of naïve RAG at each stage of the process
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2  天真RAG在过程中的每个阶段的缺陷
- en: In the last few years, a lot of research and experimentation has been done to
    address these drawbacks. Early approaches involved pre-training language models.
    Techniques involving fine-tuning of the LLMs, embeddings models, and retrievers
    have also been tried. These techniques require training data and re-computation
    of model weights, generally using supervised learning techniques. Since this book
    is a foundational guide, we will not go into these complex techniques.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，进行了大量研究和实验来解决这些缺陷。早期方法包括预训练语言模型。还尝试了涉及LLM微调、嵌入模型和检索器的技术。这些技术需要训练数据和模型权重的重新计算，通常使用监督学习技术。由于本书是一本基础指南，我们将不会深入这些复杂的技术。
- en: 'This chapter covers some interventions, techniques, and strategies used at
    different stages of the two RAG pipelines: the indexing and generation pipeline.
    Although the array of such interventions is endless, some of the more popular
    ones are highlighted in the subsequent sections.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了在两个RAG管道的不同阶段使用的干预、技术和策略：索引和生成管道。尽管这样的干预措施种类繁多，但下文将突出一些更受欢迎的。
- en: 6.2 Advanced RAG techniques
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 高级RAG技术
- en: 'Advanced techniques in RAG have continued to emerge since the earliest experiments
    with naïve RAG. There are three stages in which we can discuss these techniques:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 自从最早的天真RAG实验以来，RAG的高级技术一直在不断涌现。我们可以从以下三个阶段来讨论这些技术：
- en: '*Pre-retrieval stag**e*—As the name suggests, certain interventions can be
    employed before the retriever comes into action. This broadly covers two aspects:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索前阶段*—正如其名所示，可以在检索器开始行动之前采取某些干预措施。这大致涵盖两个方面：'
- en: '*Index optimization*—The way documents are stored in the knowledge base'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*索引优化*——在知识库中存储文档的方式'
- en: '*Query optimization*—Optimizing the user query so it aligns better with the
    retrieval and generation tasks'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询优化*——优化用户查询，使其更好地与检索和生成任务对齐'
- en: '*Retrieval stag**e*—Certain strategies can improve the recall and precision
    of the retrieval process. This goes beyond the capability of the underlying retrieval
    algorithms discussed in chapter 4.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索阶段*——某些策略可以提高检索过程的召回率和精确率。这超出了第4章中讨论的底层检索算法的能力。'
- en: '*Post-retrieval stag**e*—Once the information has been retrieved, the context
    can be further optimized to better align with the generation task and the downstream
    LLM.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后检索阶段*——一旦信息被检索出来，上下文可以进一步优化，以更好地与生成任务和下游LLM对齐。'
- en: With techniques employed at these three stages, the advanced RAG process follows
    a “rewrite then retrieve then re-rank then read” frameworks. Two additional components
    of rewrite and re-rank are added, and the retrieve component is enhanced in comparison
    with naïve RAG. This structure is presented in figure 6.3.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三个阶段采用的技术，高级RAG过程遵循“重写然后检索然后重新排序然后阅读”的框架。增加了重写和重新排序的两个额外组件，并且与简单的RAG相比，检索组件得到了增强。这种结构在图6.3中展示。
- en: '![](../Images/CH06_F03_Kimothi.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_Kimothi.png)'
- en: Figure 6.3  Advanced RAG is a rewrite–retrieve–re-rank–read process, as compared
    to a retrieve–read naïve RAG process.
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 高级RAG是一个重写-检索-重新排序-阅读的过程，与简单的检索-阅读RAG过程相比。
- en: We now explore these components one by one, beginning with the pre-retrieval
    stage.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将逐一探索这些组件，从预检索阶段开始。
- en: 6.3 Pre-retrieval techniques
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 预检索技术
- en: 'The primary objective of employing pre-retrieval techniques is to facilitate
    better retrieval. We have noted that the retrieval stage of naïve RAG suffers
    from low recall and low precision—irrelevant information is retrieved, and not
    all relevant information is retrieved. This can happen mainly because of two reasons:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 采用预检索技术的首要目标是促进更好的检索。我们注意到，简单RAG的检索阶段存在召回率和精确率低的问题——检索到了不相关信息，并且没有检索到所有相关信息。这主要可能由于以下两个原因：
- en: '*Knowledge base is not suited for retrieval*. If the information in the knowledge
    base is not stored in a manner that is easy to search through, then the quality
    of retrieval will remain suboptimal. To address this problem, *index optimization*
    is done in the indexing pipeline for more efficient storage of the knowledge base.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*知识库不适合检索*。如果知识库中的信息不是以易于搜索的方式存储的，那么检索的质量将保持次优。为了解决这个问题，在索引管道中进行*索引优化*，以实现知识库的更有效存储。'
- en: '*Retriever doesn’t completely understand the input query.* In generative AI
    applications, the control over the user query is generally limited. The level
    of detail a user provides is subjective. The retriever sometimes may misunderstand
    or not completely understand the context of the user query. *Query optimization*
    addresses this aspect of the challenge with the naïve RAG.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索器不完全理解输入查询*。在生成式AI应用中，对用户查询的控制通常有限。用户提供的详细程度是主观的。检索器有时可能误解或不完全理解用户查询的上下文。*查询优化*解决了简单RAG的这一挑战方面。'
- en: Both index and query optimizations are carried out before the retriever is invoked.
    This is the only stage that recommends interventions both in the indexing and
    generation pipeline. We will look at a few techniques for each of these.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 索引和查询优化都在检索器被调用之前进行。这是唯一一个建议在索引和生成管道中采取干预的阶段。我们将探讨这些方面的几种技术。
- en: 6.3.1 Index optimization
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 索引优化
- en: Index optimization is employed in the indexing pipeline. The objective of index
    optimization is to set up the knowledge base for better retrieval. Some of the
    popular strategies are as follows.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 索引优化在索引管道中使用。索引优化的目标是建立知识库以实现更好的检索。以下是一些流行的策略。
- en: Chunk optimization
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块优化
- en: Chapter 3 discussed the significance of chunking in the indexing pipeline. Chunking
    large documents into smaller segments plays a crucial role in retrieval and handling
    the context length limits of LLMs. Certain techniques aim for better chunking
    and efficient retrieval of the chunks, such as
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章讨论了在索引管道中分块的重要性。将大型文档分成更小的段落对于检索和处理LLMs的上下文长度限制起着至关重要的作用。某些技术旨在实现更好的分块和高效的块检索，例如
- en: '*Chunk size optimizatio**n*—The size of the chunks can have a significant effect
    on the quality of the RAG system. While large-sized chunks provide better context,
    they also carry a lot of noise. Smaller chunks, however, have precise information,
    but they might miss important information. For instance, consider a legal document
    that’s 10,000 words long. If we chunk it into 1,000-word segments, each chunk
    might contain multiple legal clauses, making it hard to retrieve specific information.
    Conversely, chunking it into 200-word segments allows for more precise retrieval
    of individual clauses, but may lose the context provided by surrounding clauses.
    Experimenting with chunk sizes can help find the optimal balance for accurate
    retrieval. The processing time also depends on the chunk size. Chunk size, therefore,
    has a significant effect on retrieval accuracy, processing speed, and storage
    efficiency. The ideal chunk size varies with the use case and depends on balancing
    factors such as document types and structure, complexity of user query, and the
    desired response time. There is no one-size-fits-all approach to optimizing chunk
    sizes. Experimentation and evaluation of different chunk sizes on metrics such
    as faithfulness, relevance, and response time (as discussed in chapter 5) can
    help in identifying the optimal chunk size for the RAG system. Chunk size optimization
    may require periodic reassessment as data or requirements change.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*块大小优化*—块的大小对 RAG 系统的质量有显著影响。虽然大块提供了更好的上下文，但它们也携带了大量的噪声。然而，小块具有精确的信息，但可能会错过重要信息。例如，考虑一份10,000字的法律文件。如果我们将其分成1,000字的段落，每个块可能包含多个法律条款，这使得检索特定信息变得困难。相反，将其分成200字的段落可以更精确地检索单个条款，但可能会失去周围条款提供的上下文。通过实验块大小可以帮助找到准确检索的最佳平衡。处理时间也取决于块大小。因此，块大小对检索准确性、处理速度和存储效率有显著影响。理想的块大小因用例而异，并取决于平衡因素，如文档类型和结构、用户查询的复杂性和期望的响应时间。没有一种适用于所有情况的优化块大小的方法。通过实验和评估不同块大小在忠实度、相关性和响应时间（如第5章所述）等指标上的表现，可以帮助确定
    RAG 系统的最佳块大小。块大小优化可能需要定期重新评估，因为数据或需求发生变化。'
- en: '*Context-enriched chunkin**g*—This method adds the summary of the larger document
    to each chunk to enrich the context of the smaller chunk. This makes more context
    available to the LLM without adding too much noise. It also improves the retrieval
    accuracy and maintains semantic coherence across chunks. This feature is particularly
    useful in scenarios where a more holistic view of the information is crucial.
    While this approach enhances the understanding of the broader context, it adds
    a level of complexity and comes at the cost of higher computational requirements,
    increased storage needs, and possible latency in retrieval. Here is an example
    of how context enrichment can be done using GPT-4o-mini, OpenAI embeddings, and
    FAISS:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文丰富化块分割*—这种方法将较大文档的摘要添加到每个块中，以丰富小块的上下文。这为 LLM 提供了更多上下文，而不会添加太多噪声。它还提高了检索准确性，并保持了块之间的语义连贯性。这个特性在需要更全面的信息视图的情况下特别有用。虽然这种方法增强了更广泛上下文的理解，但它增加了复杂性，并带来了更高的计算需求、增加的存储需求和可能的检索延迟。以下是如何使用
    GPT-4o-mini、OpenAI 嵌入和 FAISS 进行上下文丰富化的一个示例：'
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Loads text from Wikipedia page'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从维基百科页面加载文本'
- en: '#2 Generates summary of the text using GPT-4o-mini model'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用 GPT-4o-mini 模型生成文本摘要'
- en: '#3 Creates chunks using recursive character splitter'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用递归字符分割器创建块'
- en: '#4 Enriches chunks with summary data'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 通过汇总数据丰富块'
- en: '#5 Creates embeddings and storing in FAISS index'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 创建嵌入并将其存储在 FAISS 索引中'
- en: '*Fetch surrounding chunk**s*—In this technique, chunks are created at a granular
    level, say, at a sentence level, and when a relevant chunk of text is found in
    response to a query, the system retrieves not only that chunk but also the surrounding
    chunks. This makes the search granular but also performs contextual expansion
    by retrieving adjacent chunks. It is useful in long-form content such as books
    and reports where information flows across paragraphs and sections. This technique
    also adds a layer of processing cost and latency to the system. Apart from that,
    there is a possibility of diluting the relevance as the neighboring chunks may
    contain noise.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*获取周围数据块**—* 在这种技术中，数据块在细粒度级别上创建，例如在句子级别，当查询响应中找到相关文本块时，系统不仅检索该文本块，还检索周围的文本块。这使得搜索更加细粒度，并通过检索相邻的文本块进行上下文扩展。这在长篇内容如书籍和报告中很有用，其中信息跨越段落和章节流动。这种技术也给系统增加了处理成本和延迟。除此之外，相邻的文本块可能包含噪声，这可能导致相关性的稀释。'
- en: Chunk optimization is an effective step toward better RAG systems. Although
    it presents challenges such as managing the costs, system latency, and storage
    efficiency, optimizing chunking can fundamentally improve the retrieval and generation
    process of the RAG system.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据块优化是向更好的RAG系统迈出的有效一步。尽管它提出了诸如管理成本、系统延迟和存储效率等挑战，但优化数据块可以从根本上改善RAG系统的检索和生成过程。
- en: '**Metadata enhancements**'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**元数据增强**'
- en: A common way of defining metadata is “data about data.” Metadata describes other
    data. It can provide information such as a description of the data, time of creation,
    author, and similar. While metadata is useful for managing and organizing data,
    in the context of RAG, metadata enhances the searchability of data. A few ways
    in which metadata is crucial in improving RAG systems are
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定义元数据的一种常见方式是“关于数据的数据。”元数据描述其他数据。它可以提供诸如数据描述、创建时间、作者和类似信息。虽然元数据对于管理和组织数据很有用，但在RAG的背景下，元数据增强了数据的可搜索性。以下是一些元数据在提高RAG系统中的关键作用：
- en: '*Metadata filtering*—Adding metadata such as timestamp, author, category, and
    similar can enhance the chunks. While retrieving, chunks can first be filtered
    by relevant metadata information before doing a similarity search. This improves
    retrieval efficiency and reduces noise in the system. For example, using the timestamp
    filters can help avoid outdated information in the knowledge base. If a user searches
    for “latest COVID-19 travel guidelines,” metadata filtering by timestamp ensures
    that only the most recent guidelines are retrieved, avoiding outdated information.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元数据过滤*—添加诸如时间戳、作者、类别等元数据可以增强数据块。在检索时，可以在进行相似度搜索之前，首先通过相关元数据信息过滤数据块。这提高了检索效率并减少了系统中的噪声。例如，使用时间戳过滤器可以帮助避免知识库中的过时信息。如果用户搜索“最新的COVID-19旅行指南”，通过时间戳进行元数据过滤确保只检索最新的指南，避免过时信息。'
- en: '*Metadata enrichment*—Timestamp, author, category, chapter, page number, and
    so forth are common metadata elements that can be extracted from documents. However,
    even more valuable metadata items can be constructed. This can be a summary of
    the chunk by extracting tags from the chunk. One particularly useful technique
    is reverse hypothetical document embeddings. It involves using a language model
    to generate potential queries that could be answered by each document or chunk.
    These synthetic queries are then added to metadata. During retrieval, the system
    compares the user’s query with these synthetic queries to find the most relevant
    chunks.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元数据丰富*—时间戳、作者、类别、章节、页码等是从文档中提取的常见元数据元素。然而，还可以构建更有价值的元数据项。这可以是通过对数据块提取标签来总结数据块。一个特别有用的技术是反向假设文档嵌入。这涉及使用语言模型生成每个文档或数据块可能回答的潜在查询。然后，将这些合成查询添加到元数据中。在检索过程中，系统将用户的查询与这些合成查询进行比较，以找到最相关的数据块。'
- en: Metadata is a great tool for improving the accuracy of the retrieval system.
    However, a degree of caution must be exercised when adding metadata to the chunks.
    Designing the metadata schema is important to avoid redundancies and managing
    processing and storage costs. Providing improved relevance and accuracy, metadata
    enhancement has become extremely popular in contemporary RAG systems.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据是提高检索系统准确性的强大工具。然而，在向数据块添加元数据时必须谨慎行事。设计元数据模式对于避免冗余和管理处理及存储成本至关重要。提供改进的相关性和准确性，元数据增强在当代RAG系统中变得极为流行。
- en: Index structures
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 索引结构
- en: 'Another important aspect of the knowledge base is how well the information
    is structured. In the naïve RAG approach, there is no structural order to documents/chunks.
    However, for a more efficient retrieval, a few indexing structures have become
    popular and effective:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库的另一个重要方面是信息结构的良好程度。在简单的RAG方法中，文档/块没有结构化的顺序。然而，为了更有效的检索，一些索引结构已经变得流行且有效：
- en: '*Parent–child document structure*—In a parent–child document structure, documents
    are organized hierarchically. The parent document contains overarching themes
    or summaries, while child documents delve into specific details. During retrieval,
    the system can first locate the most relevant child documents and then refer to
    the parent documents for additional context if needed. This approach enhances
    the precision of retrieval, while maintaining the broader context. Simultaneously,
    this hierarchical structure can present challenges in terms of memory requirements
    and computational load.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*父子文档结构*——在父子文档结构中，文档是按层次组织的。父文档包含总体主题或摘要，而子文档深入具体细节。在检索过程中，系统可以首先定位最相关的子文档，如果需要，然后参考父文档以获取更多上下文。这种方法提高了检索的精确度，同时保持了更广泛的上下文。同时，这种层次结构在内存需求和计算负载方面可能带来挑战。'
- en: '*Knowledge graph inde**x*—Knowledge graphs organize data in a structured manner
    as entities and relationships. Using knowledge graph structures not only increases
    contextual understanding but also equips the system with enhanced reasoning capabilities
    and improved explainability. Knowledge graph creation and maintenance, however,
    is an expensive process. Knowledge-graph-powered RAG, also called GraphRAG, is
    an emerging advanced RAG pattern that has demonstrated significant improvements
    in RAG performance. We will discuss GraphRAG in detail in chapter 8\.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*知识图谱索引*——知识图谱以实体和关系的方式结构化组织数据。使用知识图谱结构不仅增加了上下文理解，还使系统具备增强的推理能力和改进的可解释性。然而，知识图谱的创建和维护是一个昂贵的流程。由知识图谱驱动的RAG，也称为GraphRAG，是一种新兴的高级RAG模式，它已经在RAG性能方面展示了显著的改进。我们将在第8章中详细讨论GraphRAG。'
- en: Index structure, perhaps, has the biggest effect on index optimization for retrieval.
    It, however, introduces storage and memory burden on the system and affects search
    time performance. Index structure optimization is therefore advised in large scale
    systems where the true potential of concepts such as GraphRAG and hierarchical
    index can be realized.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 索引结构可能对索引优化检索有最大的影响。然而，它给系统带来了存储和内存负担，并影响了搜索时间性能。因此，在大型系统中建议优化索引结构，在这些系统中可以实现诸如GraphRAG和分层索引等概念的真实潜力。
- en: Note In the previous chapters, we have discussed that embeddings are a crucial
    component of RAG. They are used to calculate the semantic similarity between the
    user query and the documents stored in the knowledge base. Generally available
    embeddings models have been trained on commonly spoken language. When dealing
    with domain-specific or specialized content, these models may not yield good results.
    Fine-tuning embedding models let you optimize vector representations for your
    specific domain or task, leading to more accurate retrieval of relevant context.
    Fine-tuning is a slightly complex process since it requires curation of the training
    dataset and resources for recalculating the embeddings model. In case you’re dealing
    with highly specialized domains where the vocabulary is different from commonly
    spoken languages, you should consider fine-tuning the embedding model for your
    domain.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在前几章中，我们讨论过嵌入是RAG的关键组成部分。它们用于计算用户查询与知识库中存储的文档之间的语义相似度。通常可用的嵌入模型是在通用语言上训练的。当处理特定领域或专业内容时，这些模型可能不会产生良好的结果。微调嵌入模型可以让您优化特定领域或任务的向量表示，从而实现更精确的相关上下文检索。微调是一个稍微复杂的过程，因为它需要整理训练数据集和资源以重新计算嵌入模型。如果您处理的是与通用语言词汇不同的高度专业化的领域，您应该考虑为您的领域微调嵌入模型。
- en: Like the indexing pipeline, index optimization is a periodic process and does
    not happen in real-time. The objective of index optimization is to set up the
    knowledge base for better retrieval. One must also be mindful of the added complexity
    that leads to an increase in computational, memory, and storage requirements.
    Figure 6.4 is an illustrative workflow of an index-optimized knowledge base.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与索引管道一样，索引优化是一个周期性的过程，不会实时发生。索引优化的目标是设置知识库以更好地检索。同时，也要注意增加的计算、内存和存储需求所带来的额外复杂性。图6.4展示了索引优化知识库的说明性工作流程。
- en: '![](../Images/CH06_F04_Kimothi.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F04_Kimothi.png)'
- en: Figure 6.4  Illustration of an index-optimized knowledge base
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4 索引优化知识库的说明
- en: 6.3.2 Query optimization
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 查询优化
- en: The second stage of pre-retrieval techniques is a part of the generation pipeline.
    The objective of this stage is to optimize the input user query in a manner that
    makes it better suited for the retrieval tasks. Some of the popular query optimization
    strategies are listed in the following sections.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 预检索技术的第二阶段是生成管道的一部分。这一阶段的目标是以一种使输入用户查询更适合检索任务的方式对其进行优化。以下章节中列出了一些流行的查询优化策略。
- en: Query expansion
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查询扩展
- en: In query expansion, the original user query is enriched to retrieve more relevant
    information. This helps in increasing the recall of the system and overcomes the
    challenge of incomplete or very brief user queries. Some of the techniques that
    expand user queries are
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询扩展中，原始用户查询被丰富以检索更多相关信息。这有助于提高系统的召回率，并克服了用户查询不完整或非常简短所带来的挑战。以下是一些扩展用户查询的技术：
- en: '*Multi-query expansio**n*—In this approach, multiple variations of the original
    query are generated using an LLM, and each variant query is used to search and
    retrieve chunks from the knowledge base. For a query “How does climate change
    affect polar bears?” a multi-query expansion might generate “Impact of global
    warming on polar bears,” “What are the consequences of climate change for polar
    bear habitats?” Let’s look at a simple example of multi-query generation using
    GPT 4o-mini model:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多查询扩展*——在这种方法中，使用LLM生成原始查询的多个变体，并使用每个变体查询从知识库中搜索和检索片段。对于一个查询“气候变化如何影响北极熊？”多查询扩展可能会生成“全球变暖对北极熊的影响”，“气候变化对北极熊栖息地的后果是什么？”让我们看看使用GPT
    4o-mini模型生成多查询的一个简单例子：'
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Crafts the prompt for query expansion'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为查询扩展构建提示'
- en: '#2 Uses GPT 4o-mini to generate expanded queries'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用GPT 4o-mini生成扩展查询'
- en: '#3 Extracts the text from the response object'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从响应对象中提取文本'
- en: '*Sub-query expansion*: Subquery approach is quite like the multi-query approach.
    In this approach, instead of generating variations of the original query, a complex
    query is broken down into simpler sub-queries. This approach is inspired by the
    least-to-most prompting technique, where complex problems are broken down into
    simpler sub-problems and are solved one by one. A sub-query expansion on the same
    query—“How does climate change affect polar bears?”—may generate “How does melting
    sea ice influence polar bear hunting and feeding behaviors?” and “What are the
    physiological and health impacts of climate change on polar bears?” The approach
    to sub-query is similar to that for multi-query, except for the changes to the
    prompt:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*子查询扩展*：子查询方法与多查询方法相当类似。在这种方法中，不是生成原始查询的变体，而是将复杂查询分解成更简单的子查询。这种方法受到最少到最多提示技术的启发，其中复杂问题被分解成更简单的子问题，并逐一解决。对同一查询——“气候变化如何影响北极熊？”的子查询扩展可能会生成“融化的海冰如何影响北极熊的狩猎和觅食行为？”以及“气候变化对北极熊的生理和健康有何影响？”子查询的方法与多查询的方法类似，除了对提示的修改：'
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Step-back expansio**n*—The term comes from the step-back prompting approach
    where the original query is abstracted to a higher-level conceptual query. During
    retrieval, both the original query and the abstracted query are used to fetch
    chunks. Similar to above example, an abstracted step-back query may be “What are
    the ecological impacts of climate change on arctic ecosystems?” Here is an example
    of the prompt that can be used:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回溯扩展*——这个术语来自回溯提示方法，其中原始查询被抽象为更高层次的查询。在检索过程中，原始查询和抽象查询都用于获取片段。类似于上面的例子，一个抽象的回溯查询可能是“气候变化对北极生态系统有何生态影响？”以下是一个可以使用的提示示例：'
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: While multi-query expansion generates various rephrasing or synonyms of the
    original query to cast a wider net during retrieval, sub-query expansion breaks
    down a complex query into simpler, component queries to target specific pieces
    of information, and step-back expansion abstracts the query to a higher-level
    concept to capture broader context.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当多查询扩展在检索过程中生成原始查询的各种改写或同义词以扩大搜索范围时，子查询扩展将复杂查询分解成更简单、更组件化的查询以针对特定信息，而回溯扩展将查询抽象为更高层次的概念以捕捉更广泛的上下文。
- en: Query expansion also presents its own set of challenges that need to be considered
    while implementing this strategy. While query expansion may increase recall by
    matching more documents, it may reduce the precision. The expansion terms need
    to be carefully selected to avoid contextual drift from the original query. Overexpansion
    can dilute the focus from the original query. Despite the challenges, query expansion
    has proved to be an effective technique for improving the recall of retrieval
    and generating more context aware responses.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 查询扩展在实施此策略时也带来了一系列需要考虑的挑战。虽然查询扩展可以通过匹配更多文档来提高召回率，但它可能会降低精确度。需要仔细选择扩展词，以避免从原始查询中产生上下文漂移。过度扩展可能会分散对原始查询的注意力。尽管存在挑战，查询扩展已被证明是提高检索召回率和生成更多上下文感知响应的有效技术。
- en: Query transformation
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查询转换
- en: Compared to query expansion, in query transformation, instead of the original
    user query, retrieval happens on a transformed query, which is more suitable for
    the retriever.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与查询扩展相比，在查询转换中，检索不是在原始用户查询上进行的，而是在转换后的查询上进行的，这对于检索器来说更合适。
- en: '*Rewrit**e*—Queries are rewritten from the input. The input in quite a few
    real-world applications may not be a direct query or a query suited for retrieval.
    Based on the input, a language model can be trained to transform the input into
    a query that can be used for retrieval. A user’s statement like, “I can’t send
    emails from my phone” can be rewritten as “Troubleshooting steps for resolving
    email sending issues on smartphones,” making it more suitable for retrieval.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rewrit**e*——查询是从输入重写的。在许多现实世界的应用中，输入可能不是一个直接的查询或适合检索的查询。基于输入，可以训练一个语言模型将输入转换成一个可用于检索的查询。例如，用户的陈述“我无法从手机发送电子邮件”可以重写为“解决智能手机上电子邮件发送问题的故障排除步骤”，使其更适合检索。'
- en: '*HyD**E*—Hypothetical document embedding, or HyDE, is a technique where the
    language model first generates a hypothetical answer to the user’s query without
    accessing the knowledge base. This generated answer is then used to perform a
    similarity search against the document embeddings in the knowledge base, effectively
    retrieving documents that are similar to the hypothetical answer rather than the
    query itself. Here is an example that generates hypothetical document embeddings:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HyD**E*——假设文档嵌入，或HyDE，是一种技术，其中语言模型首先在未访问知识库的情况下为用户的查询生成一个假设答案。然后使用这个生成的答案在知识库中的文档嵌入上执行相似性搜索，有效地检索与假设答案相似而不是查询本身的文档。以下是一个生成假设文档嵌入的示例：'
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Original query'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 原始查询'
- en: '#2 Prompts for generating HyDE'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 生成HyDE的提示'
- en: '#3 Uses OpenAI to generate a hypothetical answer'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用OpenAI生成一个假设答案'
- en: '#4 Uses OpenAI Embeddings to convert Hyde into embeddings'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用OpenAI嵌入将HyDE转换为嵌入'
- en: Challenges similar to query expansion such as drift from original query and
    maintaining intent also persist in query transformation strategies. Effective
    rewriting and transformation of the query result in enhancing the context awareness
    of the system.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与查询扩展类似的一些挑战，如从原始查询中漂移和保持意图，也存在于查询转换策略中。有效的查询重写和转换可以增强系统的上下文感知能力。
- en: '**Query routing**'
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**Query routing**'
- en: 'Different queries can demand different retrieval methods. Based on criteria
    such as intent, domain, language, complexity, source of information, and so forth,
    queries need to be classified so that they can follow the appropriate retrieval
    method. This is the idea behind optimizing the user query by routing it to the
    appropriate workflow. Types of routing techniques include:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的查询可能需要不同的检索方法。根据意图、领域、语言、复杂性、信息来源等标准，需要对查询进行分类，以便它们可以遵循适当的检索方法。这是通过将用户查询路由到适当的流程来优化用户查询的想法。路由技术的类型包括：
- en: '*Intent classificatio**n*—A pre-trained classification model is used to classify
    the intent of the user query to select the appropriate retrieval method. A modification
    to this technique is prompt-based classification, where instead of a pre-trained
    classifier, an LLM is prompted to categorize the query into an intent.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*意图分类*——使用预训练的分类模型来对用户查询的意图进行分类，以选择适当的检索方法。这种技术的改进是基于提示的分类，其中不是使用预训练的分类器，而是提示一个LLM对查询进行分类。'
- en: '*Metadata routin**g*—In this approach, keywords and tags are extracted from
    the user query and then filtering is done on the chunk metadata to narrow down
    the scope of the search.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元数据路由*——在这种方法中，从用户查询中提取关键词和标签，然后对块元数据进行过滤，以缩小搜索范围。'
- en: '*Semantic routin**g*—In this approach, the user query is matched with a pre-defined
    set of queries for each retrieval method. Wherever the similarity between the
    user query and pre-defined queries is the highest, that retrieval method is invoked.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义路由*——在这种方法中，用户查询与每个检索方法的预定义查询集进行匹配。无论用户查询与预定义查询之间的相似度最高，都调用该检索方法。'
- en: In customer support chatbots, query routing ensures that technical queries are
    directed to databases with troubleshooting guides, while billing questions are
    routed to account information, enhancing user satisfaction.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户支持聊天机器人中，查询路由确保技术查询被导向包含故障排除指南的数据库，而账单问题则被导向账户信息，从而提高用户满意度。
- en: Implementing query routing takes effort and skill. It introduces a whole new
    predictive component, bringing uncertainty to the process. Therefore, it must
    be carefully crafted. Query routing is a must when dealing with source data and
    query type variability.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 实施查询路由需要努力和技能。它引入了一个全新的预测组件，给过程带来了不确定性。因此，它必须精心设计。在处理源数据和查询类型变化时，查询路由是必不可少的。
- en: Although the universe of pre-retrieval strategies and techniques is expansive
    and ever-evolving, we have looked at a few of the most popular and effective techniques
    in this section. Bear in mind that the applicability of the strategies will depend
    on the nature of the content in the knowledge base and the use case. However,
    using each of these strategies will result in incremental gains in the RAG system
    performance. Now that we have set up the knowledge base and the user query for
    better retrieval, let’s discuss important retrieval strategies in the next section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管检索前策略和技术范围广泛且不断演变，但我们在本节中查看了一些最受欢迎和有效的技术。请注意，这些策略的适用性将取决于知识库中内容的性质和用例。然而，使用这些策略中的每一个都将导致RAG系统性能的渐进式提升。既然我们已经为更好的检索设置了知识库和用户查询，那么让我们在下一节讨论重要的检索策略。
- en: 6.4 Retrieval strategies
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 检索策略
- en: Interventions in the pre-retrieval stage can bring significant improvements
    in the performance of the RAG system if the query and the knowledge base become
    well aligned with the retrieval algorithm. We have discussed quite a few retrieval
    algorithms in chapter 4\. In this section, we focus on strategies that can be
    employed for better retrieval.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索前阶段进行干预，如果查询和知识库与检索算法很好地对齐，可以显著提高RAG系统的性能。我们在第4章讨论了许多检索算法。在本节中，我们关注可以采用以改善检索的策略。
- en: 6.4.1 Hybrid retrieval
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 混合检索
- en: Hybrid retrieval strategy is an essential component of production-grade RAG
    systems. It involves combining retrieval methods for improved retrieval accuracy.
    This can mean simply using a keyword-based search along with semantic similarity.
    It can also mean combining all sparse embedding, dense embedding vector, and knowledge
    graph-based search. The retrieval can be a union or an intersection of all these
    methods, depending on the requirements of precision and recall. It generally follows
    a weighted approach to retrieval. Figure 6.5 shows the hybrid retriever querying
    graph and vector storage.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 混合检索策略是生产级RAG系统的一个基本组成部分。它涉及结合检索方法以提高检索准确性。这可能意味着简单地使用基于关键词的搜索以及语义相似度。也可能意味着结合所有稀疏嵌入、密集嵌入向量和基于知识图谱的搜索。检索可以是所有这些方法的并集或交集，具体取决于精度和召回率的要求。它通常遵循一个加权的检索方法。图6.5显示了混合检索器查询图和向量存储。
- en: '![](../Images/CH06_F05_Kimothi.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![混合检索](../Images/CH06_F05_Kimothi.png)'
- en: Figure 6.5  Hybrid retriever employs multiple querying techniques and combines
    the results.
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5 混合检索器采用多种查询技术并合并结果。
- en: 6.4.2 Iterative retrieval
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 迭代检索
- en: Instead of using a retrieve–generate linear process, the iterative retrieval
    strategy searches the knowledge base repeatedly based on the original query and
    the generated text, which allows the system to gather more information by refining
    the search based on initial results. It is useful when solving multi-hop or complex
    queries. While effective, iterative retrieval can lead to longer processing times
    and may introduce challenges in managing larger amounts of retrieved information.
    There are examples of iterative retrieval that have demonstrated remarkably improved
    performance such as Iter-RetGen, which is an iterative approach that alternates
    between retrieval and generation steps.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用检索-生成线性过程不同，迭代检索策略根据原始查询和生成的文本反复搜索知识库，这使得系统可以通过根据初始结果细化搜索来收集更多信息。这在解决多跳或复杂查询时很有用。虽然有效，但迭代检索可能导致更长的处理时间，并可能在管理大量检索信息时引入挑战。例如，Iter-RetGen等迭代检索方法已经展示了显著提高的性能，它是一种交替进行检索和生成步骤的迭代方法。
- en: 6.4.3 Recursive retrieval
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 递归检索
- en: The recursive retrieval strategy builds on the idea of iterative retrieval by
    transforming the query iteratively depending on the results obtained. While the
    initial query is used to retrieve the chunks, new focused queries are generated
    based on these chunks. It, therefore, leads to a better ability to find scattered
    information across document chunks and a more coherent and contextual response.
    Iterative retrieval chain-of-thought (IRCoT) is a recursive retrieval technique
    that combines iterative retrieval with CoT prompting.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 递归检索策略建立在迭代检索的基础上，通过根据获得的结果迭代地转换查询。虽然初始查询用于检索块，但新的聚焦查询是基于这些块生成的。因此，它导致在文档块中找到分散信息的能力更强，并且响应更加连贯和具有上下文。迭代检索思维链（IRCoT）是一种递归检索技术，它将迭代检索与CoT提示相结合。
- en: 6.4.4 Adaptive retrieval
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 自适应检索
- en: Adaptive retrieval also follows the approach of repeated retrieval cycles. In
    adaptive retrieval strategies, an LLM is enabled to determine the most appropriate
    moment and content for retrieval. The objective of adaptive retrieval is to make
    the retrieval process more personalized to users and context. It is applied in
    areas such as adapting queries depending on user behavior or adjusting retrieval
    based on user performance. FLARE and Self-RAG are two popular examples of adaptive
    retrieval. Self-RAG introduces “reflection tokens” that enable the model to introspect
    and decide when additional retrieval is necessary. FLARE (forward-looking active
    retrieval-augmented generation) predicts future content needs based on the current
    generation and retrieves relevant information proactively. Adaptive retrieval
    is a part of a broader trend of agentic AI. Agentic AI refers to AI systems that
    can make autonomous decisions during tasks, adapting their actions based on the
    context. In the context of RAG, agentic RAG involves AI agents that dynamically
    decide when and how to retrieve information, thus enhancing the flexibility and
    efficiency of the retrieval process. Agentic AI is an important emerging RAG pattern.
    We will discuss Agentic RAG in detail in chapter 8.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应检索也遵循重复检索周期的方法。在自适应检索策略中，一个LLM被启用以确定检索的最合适时机和内容。自适应检索的目的是使检索过程更加个性化，以适应用户和上下文。它应用于根据用户行为调整查询或根据用户表现调整检索等领域。FLARE和Self-RAG是自适应检索的两个流行例子。Self-RAG引入了“反思令牌”，使模型能够自我反思并决定何时需要额外的检索。FLARE（前瞻性主动检索增强生成）根据当前生成预测未来内容需求并主动检索相关信息。自适应检索是更广泛的代理人工智能趋势的一部分。代理人工智能指的是在任务过程中能够做出自主决策的人工智能系统，根据上下文调整其行为。在RAG的上下文中，代理RAG涉及动态决定何时以及如何检索信息的AI代理，从而增强了检索过程的灵活性和效率。代理人工智能是RAG中一个重要的新兴模式。我们将在第8章详细讨论代理RAG。
- en: Figure 6.6 compares the three retrieval strategies that focus on repeated retrieval
    cycles. While recursive and iterative approaches need a threshold to break out
    of the iterations, in the adaptive approach, a judge model decides on-demand retrieval
    and generation steps.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6比较了三种关注重复检索周期的检索策略。虽然递归和迭代方法需要一个阈值来跳出迭代，但在自适应方法中，一个判断模型根据需要决定检索和生成步骤。
- en: '![](../Images/CH06_F06_Kimothi.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F06_Kimothi.png)'
- en: 'Figure 6.6  Iterative, recursive, and adaptive retrieval incorporate repeated
    retrieval cycles. Source: Adapted from Gao et al., December 18, 2023\. “*Retrieval-Augmented
    Generation for Large Language Models: A Survey.”*'
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6 迭代、递归和自适应检索结合了重复检索周期。来源：改编自Gao等人，2023年12月18日。“*大型语言模型的检索增强生成：综述。”*
- en: All the advanced retrieval strategies introduce overheads in terms of computational
    complexity, and therefore the accuracy must be balanced against the cost and latency
    of the system.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有高级检索策略都会在计算复杂度方面引入开销，因此必须平衡准确性与系统的成本和延迟。
- en: By employing advanced pre-retrieval techniques and a suitable retrieval strategy,
    we can expect that richer, deeper, and more relevant context is being retrieved
    from the knowledge base. Even when the relevant context is retrieved, the LLM
    may struggle to assimilate all the information. To address this problem, in the
    next section, we discuss a couple of post-retrieval strategies that help curate
    the context before augmenting the prompt with the necessary information.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用高级预检索技术和合适的检索策略，我们可以预期从知识库中检索到更丰富、更深入和更相关的上下文。即使检索到了相关上下文，LLM也可能难以吸收所有信息。为了解决这个问题，在下一节中，我们将讨论一些检索后策略，这些策略有助于在将必要信息添加到提示之前对上下文进行整理。
- en: 6.5 Post-retrieval techniques
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 检索后技术
- en: Even if the retrieval of the chunks happens in an expected manner, a point of
    failure still remains. The LLM might not be able to process all the information.
    This may be due to redundancies or disjointed nature of the context among many
    other reasons. At the post-retrieval stage, the approaches of re-ranking and compression
    help in providing better context to the LLM for generation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 即使块检索以预期的方式进行，仍然存在一个故障点。LLM可能无法处理所有信息。这可能是由于冗余或上下文之间不连贯的性质等多种原因。在检索后阶段，重新排序和压缩的方法有助于为LLM提供更好的上下文以进行生成。
- en: 6.5.1 Compression
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 压缩
- en: Excessively long context has the potential of introducing noise into the system.
    This diminishes the LLM’s capability to process information. Consequently, hallucinations
    and irrelevant responses to the query may persist. In prompt compression, language
    models are used to detect and remove unimportant and irrelevant tokens. Apart
    from making the context more relevant, prompt compression also has a positive
    influence on cost and efficiency. Another advantage of prompt compression is being
    able to reduce the size of the prompt so that it can fit into the context window
    of the LLM. COCOM is a context compression method that compresses contexts into
    a small number of context embeddings. Similarly, xRAG is a method that uses document
    embeddings as features. Compression can lead to loss of information, and therefore,
    there needs to be a balance between compression and performance. A very simple
    prompt to compress a long-retrieved context is
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 过长的上下文可能将噪声引入系统。这降低了LLM处理信息的能力。因此，查询可能持续出现幻觉和不相关的响应。在提示压缩中，语言模型被用来检测和删除不重要的和不相关的标记。除了使上下文更相关外，提示压缩还对成本和效率产生积极影响。提示压缩的另一个优点是能够减小提示的大小，使其适合LLM的上下文窗口。COCOM是一种将上下文压缩成少量上下文嵌入的方法。同样，xRAG是一种使用文档嵌入作为特征的方法。压缩可能导致信息丢失，因此需要在压缩和性能之间取得平衡。一个简单的提示来压缩长时间检索到的上下文是
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Re-ranking
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重新排序
- en: Reordering all the retrieved documents ensures that the most relevant information
    is prioritized for the generation step. It refines retrieval results by prioritizing
    documents that are more contextually appropriate for the query, improving the
    overall quality and accuracy of information used for generation. Re-ranking also
    addresses the question of prioritization when a hybrid approach to retrieval is
    employed and improves the overall response quality. There are commonly available
    re-rankers such as multi-vector, Learning to Rank (LTR), BERT-based, and even
    hybrid re-rankers that can be employed. Specialized APIs such as Cohere Rerank
    offer pre-trained models for efficient reranking integration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有检索到的文档进行重新排序确保了最相关的信息在生成步骤中得到优先考虑。它通过优先考虑对查询更合适的文档来细化检索结果，从而提高用于生成的信息的整体质量和准确性。重新排序还解决了在采用混合检索方法时优先级的问题，并提高了整体响应质量。常见的重新排序器包括多向量、学习排序（LTR）、基于BERT的，甚至混合重新排序器，都可以使用。Cohere
    Rerank等专用API提供预训练模型，以实现高效的重新排序集成。
- en: In this section, we discuss some of the popular advanced RAG strategies and
    techniques employed at different stages of the RAG pipeline. It is important to
    also consider the tradeoffs that come with these techniques. Almost any advanced
    technique will introduce overheads to the system. These can be in the form of
    computational load, latency in the system, and increased storage and memory requirements.
    Therefore, these techniques warrant a performance versus overhead assessment catered
    to specific use cases. Table 6.1 provides a summary of the 12 strategies discussed
    so far.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了在RAG管道的不同阶段使用的一些流行的先进RAG策略和技术。同时考虑这些技术的权衡也很重要。几乎任何高级技术都会给系统带来开销。这些开销可能以计算负载、系统延迟以及增加的存储和内存需求的形式出现。因此，这些技术需要针对特定用例进行性能与开销的评估。表6.1提供了迄今为止讨论的12种策略的总结。
- en: Table 6.1 Advanced RAG strategies with their benefits and limitations
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.1 带有其优势和局限性的高级RAG策略
- en: '| Strategy | Description | Benefits | Challenges |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 描述 | 利益 | 挑战 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Chunk optimization | Adjusting document chunks for optimal size and context
    | Improves retrieval accuracy, processing speed, and storage | Requires experimentation;
    optimal chunk varies by use case |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 文档块优化 | 调整文档块以实现最佳大小和上下文 | 提高检索准确度、处理速度和存储 | 需要进行实验；最佳块大小因用例而异 |'
- en: '| Metadata enhancements | Enriching chunks with additional metadata for better
    filtering and searchability | Improves retrieval efficiency; reduces noise | Requires
    careful schema design; manages processing costs |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 元数据增强 | 通过添加额外的元数据来丰富数据块以提高过滤和可搜索性 | 提高检索效率；减少噪声 | 需要仔细的架构设计；管理处理成本 |'
- en: '| Index structures | Organizing data in structured formats for efficient retrieval
    | Enhances accuracy and context in retrieval | Increases memory and computational
    load |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 索引结构 | 以结构化格式组织数据以实现高效检索 | 提高检索的准确性和上下文 | 增加内存和计算负载 |'
- en: '| Query expansion | Enriching the user query to retrieve more relevant information
    | Increases recall; overcomes brief queries | May reduce precision; risk of contextual
    drift |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 查询扩展 | 丰富用户查询以检索更多相关信息 | 提高召回率；克服简短查询 | 可能降低精确度；风险出现上下文漂移 |'
- en: '| Query transformation | Modifying the user query for better retrieval suitability
    | Enhances context awareness; maintains intent | Potential for misinterpretation;
    drift from the original query |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 查询转换 | 修改用户查询以提高检索的适用性 | 提高上下文意识；保持意图 | 可能出现误解；偏离原始查询 |'
- en: '| Query routing | Directing queries to appropriate retrieval methods based
    on classification | Enhances retrieval by matching method to query type | Introduces
    uncertainty; requires careful crafting |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 查询路由 | 根据分类将查询引导到适当的检索方法 | 通过将方法与查询类型匹配来提高检索 | 引入不确定性；需要精心制作 |'
- en: '| Hybrid retrieval | Combining multiple retrieval methods (e.g., keyword and
    semantic) | Improves retrieval accuracy and robustness | Increased complexity;
    requires method weighting |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 混合检索 | 结合多种检索方法（例如，关键词和语义） | 提高检索准确性和鲁棒性 | 增加复杂性；需要方法加权 |'
- en: '| Iterative retrieval | Repeatedly searching based on initial results and query
    refinement | Gathers more comprehensive information; refines search | Longer processing
    times; managing more data |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 迭代检索 | 基于初始结果和查询细化重复搜索 | 收集更全面的信息；细化搜索 | 更长的处理时间；管理更多数据 |'
- en: '| Recursive retrieval | Iteratively transforming the query based on obtained
    results | Finds scattered information; provides coherent responses | Similar to
    iterative retrieval; potential for increased load |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 递归检索 | 基于获取的结果迭代地转换查询 | 找到分散的信息；提供连贯的响应 | 类似于迭代检索；可能增加负载 |'
- en: '| Adaptive retrieval | LLM decides when and what to retrieve during generation
    | Personalized and context-aware retrieval; dynamic adaptation | Increased computational
    complexity; part of agentic AI |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 自适应检索 | LLM决定在生成过程中何时以及检索什么 | 个性化且上下文感知的检索；动态适应 | 增加计算复杂性；属于代理人工智能的一部分 |'
- en: '| Compression | Reducing context length by removing irrelevant information
    | Fits within LLM context window; reduces noise and costs | Potential loss of
    important information; needs balance |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 压缩 | 通过删除无关信息来减少上下文长度 | 适应LLM上下文窗口；减少噪声和成本 | 可能丢失重要信息；需要平衡 |'
- en: '| Reranking | Reordering retrieved documents to prioritize relevance | Enhances
    response quality; ensures most relevant info is used | Requires additional models;
    may introduce overhead |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 重新排序 | 重新排序检索到的文档以优先考虑相关性 | 提高响应质量；确保使用最相关的信息 | 需要额外的模型；可能引入开销 |'
- en: Figure 6.7 is an illustrative example of what a generation pipeline looks like
    after incorporating advanced techniques.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7是采用高级技术后一代化管道的示例。
- en: '![](../Images/CH06_F07_Kimothi.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F07_Kimothi.png)'
- en: Figure 6.7  Illustrative example of advanced generation pipeline
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7 高级生成管道的示例
- en: While these advanced strategies and techniques are extremely useful in improving
    performance, a RAG system also needs to provide customization and flexibility.
    This is because we may need to quickly adopt different techniques as the nature
    of data and queries evolve. A modular RAG approach discussed in the next section
    aims to provide greater architectural flexibility over the traditional RAG system.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些高级策略和技术在提高性能方面非常有用，但RAG系统还需要提供定制和灵活性。这是因为随着数据和查询性质的变化，我们可能需要迅速采用不同的技术。下一节中讨论的模块化RAG方法旨在提供比传统RAG系统更大的架构灵活性。
- en: 6.6 Modular RAG
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 模块化RAG
- en: 'AI systems are becoming increasingly complex, demanding more customizable,
    flexible, and scalable RAG architectures. The emergence of modular RAG is a leap
    forward in the evolution of RAG systems. Modular RAG breaks down the traditional
    monolithic RAG structure into interchangeable components. This allows for tailoring
    of the system to specific use cases. The modular approach brings modularity to
    RAG components, such as retrievers, indexing, and generation, while also adding
    more modules such as search, memory, and fusion. We can think of the modular RAG
    approach in two parts:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统正变得越来越复杂，需要更多可定制、灵活和可扩展的RAG架构。模块化RAG的出现是RAG系统演变中的飞跃。模块化RAG将传统的单体RAG结构分解为可互换的组件。这使得可以根据特定用例定制系统。模块化方法将模块化引入RAG组件，如检索器、索引和生成，同时也添加了更多模块，如搜索、内存和融合。我们可以将模块化RAG方法分为两部分：
- en: Core components of RAG developed as flexible, interchangeable modules
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为灵活、可互换模块开发的RAG核心组件
- en: Specialized modules to enhance the core features of retrieval, augmentation,
    and generation
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于增强检索、增强和生成核心功能的专用模块
- en: 6.6.1 Core modules
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.1 核心模块
- en: The core components of the RAG system (i.e. indexing, retrieval, augmentation
    and generation), along with the advanced pre- and post-retrieval techniques, are
    composed as flexible, interchangeable modules in the modular RAG framework.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统（即索引、检索、增强和生成）的核心组件，以及先进的检索前和检索后技术，在模块化RAG框架中组成灵活、可互换的模块。
- en: '*Indexing modul**e*—The indexing module serves as the foundation for building
    the knowledge base. By modularizing this component, developers can choose from
    various embedding models for advanced semantic understanding. Vector stores can
    be interchanged based on scalability and performance needs. Additionally, chunking
    methods can be adapted to the data structure, whether it’s text, code, or multimedia
    content, ensuring optimal indexing for retrieval.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*索引模块*——索引模块作为构建知识库的基础。通过模块化此组件，开发者可以选择各种嵌入模型以实现高级语义理解。根据可扩展性和性能需求，可以互换向量存储。此外，可以根据数据结构（无论是文本、代码还是多媒体内容）调整分块方法，确保检索的最佳索引。'
- en: '*Retrieval modul**e*—The retrieval module enables the use of diverse retrieval
    algorithms. For instance, developers can switch between semantic similarity search
    using dense embeddings and traditional keyword-based search such as BM25\. This
    flexibility allows for tailoring retrieval methods to the specific requirements
    of the application, such as prioritizing speed, accuracy, or resource utilization.
    For example, a customer support chatbot might use semantic search during off-peak
    hours for higher accuracy and switch to keyword search during peak hours to handle
    increased load. The modular retrieval component allows this dynamic interchange
    of retrieval strategies based on real-time needs.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索模块*——检索模块允许使用多种检索算法。例如，开发者可以在使用密集嵌入的语义相似度搜索和传统的基于关键词的搜索（如BM25）之间切换。这种灵活性允许根据应用程序的具体要求定制检索方法，例如优先考虑速度、准确性或资源利用率。例如，客户支持聊天机器人可能在非高峰时段使用语义搜索以提高准确性，并在高峰时段切换到关键词搜索以处理增加的负载。模块化检索组件允许根据实时需求动态交换检索策略。'
- en: '*Generation modul**e*—In the generation module, the choice of LLM is modular.
    Developers can select from models such as GPT-4 for complex language generation
    or smaller models for cost efficiency. This module also handles prompt engineering
    for augmentation to guide the LLM in generating accurate and relevant responses.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成模块*—在生成模块中，LLM的选择是模块化的。开发者可以从GPT-4等用于复杂语言生成的模型或用于成本效益的小型模型中进行选择。此模块还处理提示工程，以引导LLM生成准确和相关的响应。'
- en: '*Pre-retrieval modul**e*—Allows flexibility of pre-retrieval techniques to
    improve the quality of indexed content and user query.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索前模块*—允许检索前技术的灵活性，以提高索引内容的质量和用户查询。'
- en: '*Post-retrieval modul**e*—Like the pre-retrieval module, this module allows
    for flexible implementation of post-retrieval techniques to refine and optimize
    the retrieved context.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索后模块*—与检索前模块类似，此模块允许灵活实现检索后技术以精炼和优化检索到的上下文。'
- en: You may note that the first three modules complete the naïve RAG approach, and
    the addition of the pre-retrieval and post-retrieval modules enhances the naïve
    RAG into an advanced RAG implementation. It can also be said that naïve RAG is
    a special (and limited) case of advanced RAG.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到前三个模块完成了原始RAG方法，而检索前和检索后模块的添加将原始RAG提升为高级RAG实现。也可以说，原始RAG是高级RAG的一个特殊（且有限）情况。
- en: 6.6.2 New modules
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.2 新模块
- en: The modular RAG framework has introduced several new components to enhance the
    retrieval and generation capabilities of naïve and advanced RAG approaches. Some
    of these components/modules are
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化的RAG框架引入了几个新组件，以增强原始和高级RAG方法检索和生成能力。其中一些组件/模块包括
- en: '*Searc**h*—The search module is aimed at performing searches on different data
    sources. It is customized to different data sources and aimed at increasing the
    source data for better response generation.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索*—搜索模块旨在对不同数据源进行搜索。它针对不同的数据源进行定制，旨在增加源数据以更好地生成响应。'
- en: '*Fusio**n*—RAG fusion improves traditional search systems by overcoming their
    limitations through a multi-query approach. The fusion module enhances retrieval
    by expanding the user’s query into multiple, diverse perspectives using an LLM.
    It then conducts parallel searches for these expanded queries, fuses the results
    by reranking and selecting the most relevant information, and presents a comprehensive
    answer. This approach captures both explicit and implicit information, uncovering
    deeper insights that might be missed with a single query.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*融合*—RAG融合通过多查询方法克服了传统搜索系统的局限性。融合模块通过使用LLM将用户的查询扩展到多个、不同的视角来增强检索。然后，它对这些扩展查询进行并行搜索，通过重新排序和选择最相关的信息来融合结果，并呈现全面的答案。这种方法捕捉了显性和隐性信息，揭示了可能因单一查询而错过的更深入的见解。'
- en: '*Memor**y*—The memory module uses the inherent memory of the LLM, meaning the
    knowledge encoded within its parameters from pre-training. This module uses the
    LLM to recall information without explicit retrieval, guiding the system on when
    to retrieve additional data and when to rely on the LLM’s internal knowledge.
    It can involve techniques such as using reflection tokens or prompts that encourage
    the model to introspect and decide if more information is needed. For example,
    when answering a query about historical events, the memory module can decide to
    rely on the LLM’s knowledge about World War II to provide context, only retrieving
    specific dates or figures as needed. This approach reduces unnecessary retrieval
    and uses the model’s pre-trained knowledge.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*记忆*—记忆模块使用LLM的固有记忆，即其参数中编码的预训练知识。此模块使用LLM回忆信息，而无需显式检索，指导系统何时检索额外数据，何时依赖LLM的内部知识。它可以涉及使用反射令牌或提示等技术，鼓励模型进行内省并决定是否需要更多信息。例如，当回答有关历史事件的查询时，记忆模块可以决定依赖LLM关于第二次世界大战的知识来提供背景，仅检索所需的特定日期或数字。这种方法减少了不必要的检索并使用了模型的预训练知识。'
- en: '*Routin**g*—Routing in the RAG system navigates through diverse data sources,
    selecting the optimal pathway for a query, whether it involves summarization,
    specific database searches, or merging different information streams.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*路由*—在RAG系统中，路由通过不同的数据源进行导航，为查询选择最佳路径，无论是涉及摘要、特定数据库搜索还是合并不同的信息流。'
- en: '*Task adapte**r*—This module makes RAG adaptable to various downstream tasks
    allowing the development of task-specific end-to-end retrievers with minimal examples,
    demonstrating flexibility in handling different tasks. The task adapter module
    allows the RAG system to be fine-tuned for specific tasks like summarization,
    translation, or sentiment analysis. By incorporating a small number of task-specific
    examples or prompts, the module adjusts the retrieval and generation components
    to produce outputs tailored to the desired task, enhancing versatility without
    extensive retraining.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任务适配器*—此模块使RAG能够适应各种下游任务，允许通过最小示例开发特定任务的端到端检索器，展示了处理不同任务的灵活性。任务适配器模块允许RAG系统针对特定任务（如摘要、翻译或情感分析）进行微调。通过结合少量特定任务的示例或提示，该模块调整检索和生成组件，以产生符合所需任务的输出，增强多功能性而无需大量重新训练。'
- en: You may observe that advanced RAG is a special case within the modular RAG framework.
    You also saw earlier that naïve RAG is a special case of advanced RAG. This means
    that the RAG approaches (i.e., naïve, advanced, and modular) are not competing
    but progressive. You may start by trying out a naïve implementation of RAG and
    move to a more modular approach. Figure 6.8 shows the progression of RAG systems.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会观察到，高级RAG是模块化RAG框架中的一个特殊情况。您也之前看到，简单RAG是高级RAG的一个特殊情况。这意味着RAG方法（即简单、高级和模块化）不是竞争关系，而是渐进关系。您可以从尝试RAG的简单实现开始，然后过渡到更模块化的方法。图6.8显示了RAG系统的演变过程。
- en: '![](../Images/CH06_F08_Kimothi.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F08_Kimothi.png)'
- en: Figure 6.8  Naïve, advanced, and modular approaches to RAG are progressive.
    Naïve RAG is a sub-component of advanced RAG, which is a sub-component of modular
    RAG.
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8  RAG的简单、高级和模块化方法都是渐进的。简单RAG是高级RAG的一个子组件，而高级RAG又是模块化RAG的一个子组件。
- en: While building a modular RAG system, remember that each module should be designed
    to work independently. This requires defining clear inputs and outputs. Along
    with the independent modules, the orchestration layer should be flexible to allow
    mixing and matching of modules. One should also bear in mind that a modular approach
    introduces complexity in the process. Managing interfaces, dependencies, configurations,
    and versions of modules can be complex. Ensuring compatibility and consistency
    between modules can be challenging. Testing each module independently and collectively
    requires a robust evaluation strategy. Extra modules may also add latency and
    inference costs to the system.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模块化RAG系统时，请记住每个模块都应该设计成可以独立工作。这需要定义清晰的输入和输出。除了独立的模块外，编排层应该灵活，以便混合和匹配模块。还应记住，模块化方法在过程中引入了复杂性。管理接口、依赖关系、配置和模块版本可能很复杂。确保模块之间的兼容性和一致性可能具有挑战性。独立和集体测试每个模块需要强大的评估策略。额外的模块也可能增加系统的延迟和推理成本。
- en: Despite the added complexities, the modular approach toward RAG is state-of-the-art
    in large-scale RAG systems. It enables rapid experimentation, efficient optimization,
    and seamless integration of new technologies as they emerge. By offering the ability
    to mix and match different modules, modular RAG empowers you to build more robust,
    accurate, and versatile AI solutions. It also facilitates easier maintenance,
    updates, and scalability, making it an ideal choice for managing complex, evolving
    knowledge bases.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管增加了复杂性，但RAG的模块化方法在大型RAG系统中仍然是处于领先地位的。它使得快速实验、高效优化以及新技术的无缝集成成为可能。通过提供混合和匹配不同模块的能力，模块化RAG使您能够构建更稳健、准确和多功能的人工智能解决方案。它还简化了维护、更新和可扩展性，使其成为管理复杂、不断发展的知识库的理想选择。
- en: This section concludes the discussion on improving RAG performance using advanced
    techniques and a modular framework. Interventions can be employed at different
    stages of the indexing and generation pipelines. Modular approaches to RAG enable
    rapid experimentation, flexibility, and scalable architecture. You will need to
    experiment to figure out the techniques that help in improving RAG for specific
    use cases. It is also important to be mindful of the tradeoffs. Advanced techniques
    introduce complexities that have an effect on computation, memory, and storage
    requirements.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了使用高级技术和模块化框架提高RAG性能。可以在索引和生成管道的不同阶段采取干预措施。RAG的模块化方法使快速实验、灵活性和可扩展架构成为可能。您需要实验以找出有助于特定用例的技巧。同时，也要注意权衡。高级技术引入的复杂性会影响计算、内存和存储需求。
- en: This is one aspect of putting RAG in production. Advanced techniques are necessary
    for RAG systems to achieve acceptable accuracy and efficiency. The other enablers
    for RAG systems in production are the tools and technologies that form the backbone
    of the RAG stack. In the next chapter, we will look at this technology infrastructure
    that enables RAG systems.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将RAG投入生产的一个方面。为了实现可接受的准确性和效率，RAG系统需要高级技术。RAG系统在生产中的其他推动因素是构成RAG堆栈骨干的工具和技术。在下一章中，我们将探讨这一技术基础设施，它使RAG系统成为可能。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Limitations of naïve RAG
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 纳维亚RAG的局限性
- en: Naïve RAG follows a simple “retrieve then read” process.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纳维亚RAG遵循简单的“检索然后阅读”过程。
- en: This approach suffers from low precision and incomplete retrieval.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法存在精度低和检索不完整的问题。
- en: Retrieval often misses relevant information and pulls in irrelevant content.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索通常会错过相关信息，并拉入不相关的内容。
- en: At the augmentation stage, there is often redundancy from similar retrieved
    documents.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在增强阶段，通常存在来自类似检索文档的冗余。
- en: Context can become disjointed when sourced from multiple documents.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当从多个文档中获取时，上下文可能会变得不连贯。
- en: The generation stage faces hallucinations and biased outputs.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成阶段面临幻觉和有偏的输出。
- en: The model can overly rely on retrieved data and ignore its internal knowledge.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可能会过度依赖检索数据，而忽略其内部知识。
- en: Advanced RAG techniques
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级RAG技术
- en: The advanced RAG process follows a “rewrite then retrieve then re-rank then
    read” framework, where the query is optimized through rewriting, retrieval is
    enhanced for better precision, results are re-ranked to prioritize relevance,
    and the most relevant information is used for generating the final response.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级RAG流程遵循“重写然后检索然后重新排序然后阅读”的框架，其中查询通过重写进行优化，检索通过增强以提高精度，结果通过重新排序以优先考虑相关性，并使用最相关的信息来生成最终响应。
- en: Pre-retrieval techniques include
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预检索技术包括
- en: '*Index optimizatio**n*—Improves document storage for better searchability'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*索引优化*—改善文档存储以提高可搜索性'
- en: '*Chunk optimizatio**n*—Balances chunk sizes to avoid losing context or introducing
    noise'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分块优化*—平衡块大小以避免丢失上下文或引入噪声'
- en: '*Context-enriched chunkin**g*—Adds summaries to each chunk to improve retrieval'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文丰富分块*—为每个块添加摘要以改善检索'
- en: '*Metadata enhancement**s*—Adds tags and metadata like timestamps or categories
    for better filtering'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元数据增强*—添加标签和元数据，如时间戳或类别，以实现更好的过滤'
- en: '*Query optimizatio**n*—Expands or rewrites user queries for improved retrieval
    accuracy'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询优化*—通过扩展或重写用户查询来提高检索准确性'
- en: Retrieval techniques include
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索技术包括
- en: '*Hybrid retrieva**l*—Combines keyword-based and semantic searches'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合检索*—结合基于关键词和语义搜索'
- en: '*Iterative retrieva**l*—Refines searches by repeatedly querying based on initial
    results'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*迭代检索*—通过反复查询初始结果来细化搜索'
- en: '*Recursive retrieva**l*—Generates new queries based on retrieved chunks to
    gather more relevant information'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*递归检索*—根据检索到的块生成新查询以收集更多相关信息'
- en: Post-retrieval techniques include
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后检索技术包括
- en: '*Compressio**n*—Reduces unnecessary context to remove noise and fit within
    the model’s context window'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*压缩*—减少不必要的上下文以去除噪声并适应模型的上下文窗口'
- en: '*Re-rankin**g*—Reorders retrieved documents to prioritize the most relevant
    ones'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重新排序*—重新排序检索到的文档以优先考虑最相关的文档'
- en: Modular RAG framework
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模块化RAG框架
- en: Core modules include
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心模块包括
- en: '*Indexing modul**e*—Allows flexible embedding models and vector store options'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*索引模块*—允许灵活的嵌入模型和向量存储选项'
- en: '*Retrieval modul**e*—Supports switching between dense and keyword-based retrieval
    methods'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检索模块*—支持在密集型和基于关键词的检索方法之间切换'
- en: '*Generation modul**e*—Offers flexibility in selecting language models based
    on complexity and cost'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成模块*—根据复杂性和成本选择语言模型的灵活性'
- en: New modules include
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新模块包括
- en: '*Search modul**e*—Tailors search to specific data sources for better results'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索模块*—针对特定数据源进行定制搜索以获得更好的结果'
- en: '*Fusion modul**e*—Expands user queries into multiple forms and combines retrieved
    results for deeper insights'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*融合模块*—将用户查询扩展到多种形式，并组合检索结果以获得更深入的见解'
- en: '*Memory modul**e*—Uses the model’s internal knowledge to reduce unnecessary
    retrieval, retrieving only when needed'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内存模块*—使用模型的内部知识来减少不必要的检索，仅在需要时检索'
- en: '*Routing modul**e*—Dynamically selects the best path for handling different
    types of queries'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*路由模块*—动态选择处理不同类型查询的最佳路径'
- en: '*Task adapter modul**e*—Adapts the system for different downstream tasks like
    summarization or translation'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任务适配模块*—适配系统以适应不同的下游任务，如摘要或翻译'
- en: Tradeoffs and best practices
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权衡和最佳实践
- en: Advanced techniques improve RAG accuracy but add complexity.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级技术可以提高RAG的准确性，但会增加复杂性。
- en: Techniques such as hybrid retrieval or re-ranking can increase computational
    costs and latency.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如混合检索或重新排序等技术可能会增加计算成本和延迟。
- en: Modular RAG offers flexibility but requires careful management of interfaces
    and module compatibility.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块化RAG提供灵活性，但需要仔细管理接口和模块兼容性。
- en: Testing each module independently and as a whole is important to ensure system
    stability and performance.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立和整体测试每个模块对于确保系统稳定性和性能很重要。
- en: Tradeoffs between performance, cost, and system complexity should be carefully
    assessed.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能、成本和系统复杂性之间的权衡应仔细评估。
