- en: '6 Progression of RAG systems: Naïve, advanced, and modular RAG'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Limitations of the naïve RAG approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced RAG strategies and techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modular patterns in RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first two parts of this book, you learned about the utility of retrieval-augmented
    generation (RAG), along with the development and evaluation of a basic RAG system.
    The basic, or the naïve RAG approach that we have discussed is, generally, inadequate
    when it comes to production-grade systems.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on more advanced concepts in RAG. We begin by revisiting
    the limitations and the points of failure of the naïve RAG approach. Next, we
    discuss the failures at the retrieval, augmentation, and generation stages. Advanced
    strategies and techniques to address these points of failure will be elaborated
    on in distinct phases of the RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Better indexing of the knowledge base leads to better RAG outcomes. We will
    look at a few data indexing strategies that build on the naïve indexing pipeline
    to improve the searchability of the knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the generation pipeline, improvements are examined in three stages: pre-retrieval,
    retrieval, and post-retrieval. Pre-retrieval techniques focus on manipulating
    and improving the input user query. Retrieval strategies focus on better matching
    of the user query to the documents in the knowledge base. Finally, in the post-retrieval
    stage, the focus is on aligning the retrieved context with the desired result
    and making it suitable for generation.'
  prefs: []
  type: TYPE_NORMAL
- en: The last part of the chapter discusses a modular approach to RAG that has been
    emerging to find applicability in RAG systems. The modular approach is an architectural
    enhancement to the basic RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the strategies and techniques for RAG improvement are expansive, and
    this chapter highlights a few popular ones. The chapter is interspersed with code
    examples, but for a more exhaustive supporting code, check out the source code
    repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should
  prefs: []
  type: TYPE_NORMAL
- en: Understand why the naïve approach to RAG is not suitable for production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware of indexing strategies that make the RAG knowledge base more efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know some of the popular pre-retrieval, retrieval, and post-retrieval techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be familiar with the modular approach to RAG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG powers a variety of AI applications. However, there is a certain aspect
    of uncertainty when it comes to outcomes. Inaccuracies in retrieval, disjointed
    context, and incoherence in the LLM outputs need to be addressed before taking
    RAG to production. In a very short time, researchers and practitioners have experimented
    with innovative techniques to improve the relevance and faithfulness of RAG systems.
    But before we look at these techniques, it is important to understand why a naïve
    RAG approach often doesn’t find its way into a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Limitations of naïve RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Naïve RAG can be thought of as the earliest form of RAG, which gained popularity
    after the release of ChatGPT and the rise of LLM technology. As we have seen so
    far, it follows a linear process of indexing, retrieving, augmenting, and generation.
    This process falls in a “retrieve then read” framework, which means that there’s
    a retriever retrieving information and that there’s an LLM reading this information
    to generate the results, as shown in figure 6.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F01_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1  Naïve RAG is a sequential “retrieve then read” process.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The naïve RAG approach is marred with drawbacks at each of the three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Retrieval*—Naïve retrieval is often observed to have low precision that leads
    to irrelevant information being retrieved. It also has a low recall, which means
    that relevant information is missed, which leads to incomplete results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Augmentation*—There is a real possibility of redundancy and repetition when
    multiple retrieved documents have similar information. Also, when information
    is sourced from different documents, the context becomes disjointed. There’s also
    the problem of context length of the LLMs that has an effect on the volume of
    retrieved context that can be passed on to the LLM for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generation*—With the inadequacies of the upstream processes, the generation
    suffers from hallucination and lack of groundedness of the generated content.
    The LLM faces challenge in reconciling information. The challenges of toxicity
    and bias also persist. It is also noticed sometimes that the LLM becomes over-reliant
    on the retrieved context and forgets to draw from its own parametric memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 6.2 summarizes these drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: '![A black box with black text'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH06_F02_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2  Drawbacks of naïve RAG at each stage of the process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the last few years, a lot of research and experimentation has been done to
    address these drawbacks. Early approaches involved pre-training language models.
    Techniques involving fine-tuning of the LLMs, embeddings models, and retrievers
    have also been tried. These techniques require training data and re-computation
    of model weights, generally using supervised learning techniques. Since this book
    is a foundational guide, we will not go into these complex techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers some interventions, techniques, and strategies used at
    different stages of the two RAG pipelines: the indexing and generation pipeline.
    Although the array of such interventions is endless, some of the more popular
    ones are highlighted in the subsequent sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Advanced RAG techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Advanced techniques in RAG have continued to emerge since the earliest experiments
    with naïve RAG. There are three stages in which we can discuss these techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pre-retrieval stag**e*—As the name suggests, certain interventions can be
    employed before the retriever comes into action. This broadly covers two aspects:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Index optimization*—The way documents are stored in the knowledge base'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Query optimization*—Optimizing the user query so it aligns better with the
    retrieval and generation tasks'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieval stag**e*—Certain strategies can improve the recall and precision
    of the retrieval process. This goes beyond the capability of the underlying retrieval
    algorithms discussed in chapter 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Post-retrieval stag**e*—Once the information has been retrieved, the context
    can be further optimized to better align with the generation task and the downstream
    LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With techniques employed at these three stages, the advanced RAG process follows
    a “rewrite then retrieve then re-rank then read” frameworks. Two additional components
    of rewrite and re-rank are added, and the retrieve component is enhanced in comparison
    with naïve RAG. This structure is presented in figure 6.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F03_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3  Advanced RAG is a rewrite–retrieve–re-rank–read process, as compared
    to a retrieve–read naïve RAG process.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We now explore these components one by one, beginning with the pre-retrieval
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Pre-retrieval techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary objective of employing pre-retrieval techniques is to facilitate
    better retrieval. We have noted that the retrieval stage of naïve RAG suffers
    from low recall and low precision—irrelevant information is retrieved, and not
    all relevant information is retrieved. This can happen mainly because of two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Knowledge base is not suited for retrieval*. If the information in the knowledge
    base is not stored in a manner that is easy to search through, then the quality
    of retrieval will remain suboptimal. To address this problem, *index optimization*
    is done in the indexing pipeline for more efficient storage of the knowledge base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retriever doesn’t completely understand the input query.* In generative AI
    applications, the control over the user query is generally limited. The level
    of detail a user provides is subjective. The retriever sometimes may misunderstand
    or not completely understand the context of the user query. *Query optimization*
    addresses this aspect of the challenge with the naïve RAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both index and query optimizations are carried out before the retriever is invoked.
    This is the only stage that recommends interventions both in the indexing and
    generation pipeline. We will look at a few techniques for each of these.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Index optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Index optimization is employed in the indexing pipeline. The objective of index
    optimization is to set up the knowledge base for better retrieval. Some of the
    popular strategies are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Chunk optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chapter 3 discussed the significance of chunking in the indexing pipeline. Chunking
    large documents into smaller segments plays a crucial role in retrieval and handling
    the context length limits of LLMs. Certain techniques aim for better chunking
    and efficient retrieval of the chunks, such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk size optimizatio**n*—The size of the chunks can have a significant effect
    on the quality of the RAG system. While large-sized chunks provide better context,
    they also carry a lot of noise. Smaller chunks, however, have precise information,
    but they might miss important information. For instance, consider a legal document
    that’s 10,000 words long. If we chunk it into 1,000-word segments, each chunk
    might contain multiple legal clauses, making it hard to retrieve specific information.
    Conversely, chunking it into 200-word segments allows for more precise retrieval
    of individual clauses, but may lose the context provided by surrounding clauses.
    Experimenting with chunk sizes can help find the optimal balance for accurate
    retrieval. The processing time also depends on the chunk size. Chunk size, therefore,
    has a significant effect on retrieval accuracy, processing speed, and storage
    efficiency. The ideal chunk size varies with the use case and depends on balancing
    factors such as document types and structure, complexity of user query, and the
    desired response time. There is no one-size-fits-all approach to optimizing chunk
    sizes. Experimentation and evaluation of different chunk sizes on metrics such
    as faithfulness, relevance, and response time (as discussed in chapter 5) can
    help in identifying the optimal chunk size for the RAG system. Chunk size optimization
    may require periodic reassessment as data or requirements change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Context-enriched chunkin**g*—This method adds the summary of the larger document
    to each chunk to enrich the context of the smaller chunk. This makes more context
    available to the LLM without adding too much noise. It also improves the retrieval
    accuracy and maintains semantic coherence across chunks. This feature is particularly
    useful in scenarios where a more holistic view of the information is crucial.
    While this approach enhances the understanding of the broader context, it adds
    a level of complexity and comes at the cost of higher computational requirements,
    increased storage needs, and possible latency in retrieval. Here is an example
    of how context enrichment can be done using GPT-4o-mini, OpenAI embeddings, and
    FAISS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads text from Wikipedia page'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generates summary of the text using GPT-4o-mini model'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates chunks using recursive character splitter'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Enriches chunks with summary data'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Creates embeddings and storing in FAISS index'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fetch surrounding chunk**s*—In this technique, chunks are created at a granular
    level, say, at a sentence level, and when a relevant chunk of text is found in
    response to a query, the system retrieves not only that chunk but also the surrounding
    chunks. This makes the search granular but also performs contextual expansion
    by retrieving adjacent chunks. It is useful in long-form content such as books
    and reports where information flows across paragraphs and sections. This technique
    also adds a layer of processing cost and latency to the system. Apart from that,
    there is a possibility of diluting the relevance as the neighboring chunks may
    contain noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunk optimization is an effective step toward better RAG systems. Although
    it presents challenges such as managing the costs, system latency, and storage
    efficiency, optimizing chunking can fundamentally improve the retrieval and generation
    process of the RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Metadata enhancements**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A common way of defining metadata is “data about data.” Metadata describes other
    data. It can provide information such as a description of the data, time of creation,
    author, and similar. While metadata is useful for managing and organizing data,
    in the context of RAG, metadata enhances the searchability of data. A few ways
    in which metadata is crucial in improving RAG systems are
  prefs: []
  type: TYPE_NORMAL
- en: '*Metadata filtering*—Adding metadata such as timestamp, author, category, and
    similar can enhance the chunks. While retrieving, chunks can first be filtered
    by relevant metadata information before doing a similarity search. This improves
    retrieval efficiency and reduces noise in the system. For example, using the timestamp
    filters can help avoid outdated information in the knowledge base. If a user searches
    for “latest COVID-19 travel guidelines,” metadata filtering by timestamp ensures
    that only the most recent guidelines are retrieved, avoiding outdated information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metadata enrichment*—Timestamp, author, category, chapter, page number, and
    so forth are common metadata elements that can be extracted from documents. However,
    even more valuable metadata items can be constructed. This can be a summary of
    the chunk by extracting tags from the chunk. One particularly useful technique
    is reverse hypothetical document embeddings. It involves using a language model
    to generate potential queries that could be answered by each document or chunk.
    These synthetic queries are then added to metadata. During retrieval, the system
    compares the user’s query with these synthetic queries to find the most relevant
    chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata is a great tool for improving the accuracy of the retrieval system.
    However, a degree of caution must be exercised when adding metadata to the chunks.
    Designing the metadata schema is important to avoid redundancies and managing
    processing and storage costs. Providing improved relevance and accuracy, metadata
    enhancement has become extremely popular in contemporary RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Index structures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another important aspect of the knowledge base is how well the information
    is structured. In the naïve RAG approach, there is no structural order to documents/chunks.
    However, for a more efficient retrieval, a few indexing structures have become
    popular and effective:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Parent–child document structure*—In a parent–child document structure, documents
    are organized hierarchically. The parent document contains overarching themes
    or summaries, while child documents delve into specific details. During retrieval,
    the system can first locate the most relevant child documents and then refer to
    the parent documents for additional context if needed. This approach enhances
    the precision of retrieval, while maintaining the broader context. Simultaneously,
    this hierarchical structure can present challenges in terms of memory requirements
    and computational load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Knowledge graph inde**x*—Knowledge graphs organize data in a structured manner
    as entities and relationships. Using knowledge graph structures not only increases
    contextual understanding but also equips the system with enhanced reasoning capabilities
    and improved explainability. Knowledge graph creation and maintenance, however,
    is an expensive process. Knowledge-graph-powered RAG, also called GraphRAG, is
    an emerging advanced RAG pattern that has demonstrated significant improvements
    in RAG performance. We will discuss GraphRAG in detail in chapter 8\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index structure, perhaps, has the biggest effect on index optimization for retrieval.
    It, however, introduces storage and memory burden on the system and affects search
    time performance. Index structure optimization is therefore advised in large scale
    systems where the true potential of concepts such as GraphRAG and hierarchical
    index can be realized.
  prefs: []
  type: TYPE_NORMAL
- en: Note In the previous chapters, we have discussed that embeddings are a crucial
    component of RAG. They are used to calculate the semantic similarity between the
    user query and the documents stored in the knowledge base. Generally available
    embeddings models have been trained on commonly spoken language. When dealing
    with domain-specific or specialized content, these models may not yield good results.
    Fine-tuning embedding models let you optimize vector representations for your
    specific domain or task, leading to more accurate retrieval of relevant context.
    Fine-tuning is a slightly complex process since it requires curation of the training
    dataset and resources for recalculating the embeddings model. In case you’re dealing
    with highly specialized domains where the vocabulary is different from commonly
    spoken languages, you should consider fine-tuning the embedding model for your
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: Like the indexing pipeline, index optimization is a periodic process and does
    not happen in real-time. The objective of index optimization is to set up the
    knowledge base for better retrieval. One must also be mindful of the added complexity
    that leads to an increase in computational, memory, and storage requirements.
    Figure 6.4 is an illustrative workflow of an index-optimized knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F04_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4  Illustration of an index-optimized knowledge base
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.3.2 Query optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second stage of pre-retrieval techniques is a part of the generation pipeline.
    The objective of this stage is to optimize the input user query in a manner that
    makes it better suited for the retrieval tasks. Some of the popular query optimization
    strategies are listed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Query expansion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In query expansion, the original user query is enriched to retrieve more relevant
    information. This helps in increasing the recall of the system and overcomes the
    challenge of incomplete or very brief user queries. Some of the techniques that
    expand user queries are
  prefs: []
  type: TYPE_NORMAL
- en: '*Multi-query expansio**n*—In this approach, multiple variations of the original
    query are generated using an LLM, and each variant query is used to search and
    retrieve chunks from the knowledge base. For a query “How does climate change
    affect polar bears?” a multi-query expansion might generate “Impact of global
    warming on polar bears,” “What are the consequences of climate change for polar
    bear habitats?” Let’s look at a simple example of multi-query generation using
    GPT 4o-mini model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Crafts the prompt for query expansion'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses GPT 4o-mini to generate expanded queries'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Extracts the text from the response object'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sub-query expansion*: Subquery approach is quite like the multi-query approach.
    In this approach, instead of generating variations of the original query, a complex
    query is broken down into simpler sub-queries. This approach is inspired by the
    least-to-most prompting technique, where complex problems are broken down into
    simpler sub-problems and are solved one by one. A sub-query expansion on the same
    query—“How does climate change affect polar bears?”—may generate “How does melting
    sea ice influence polar bear hunting and feeding behaviors?” and “What are the
    physiological and health impacts of climate change on polar bears?” The approach
    to sub-query is similar to that for multi-query, except for the changes to the
    prompt:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Step-back expansio**n*—The term comes from the step-back prompting approach
    where the original query is abstracted to a higher-level conceptual query. During
    retrieval, both the original query and the abstracted query are used to fetch
    chunks. Similar to above example, an abstracted step-back query may be “What are
    the ecological impacts of climate change on arctic ecosystems?” Here is an example
    of the prompt that can be used:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: While multi-query expansion generates various rephrasing or synonyms of the
    original query to cast a wider net during retrieval, sub-query expansion breaks
    down a complex query into simpler, component queries to target specific pieces
    of information, and step-back expansion abstracts the query to a higher-level
    concept to capture broader context.
  prefs: []
  type: TYPE_NORMAL
- en: Query expansion also presents its own set of challenges that need to be considered
    while implementing this strategy. While query expansion may increase recall by
    matching more documents, it may reduce the precision. The expansion terms need
    to be carefully selected to avoid contextual drift from the original query. Overexpansion
    can dilute the focus from the original query. Despite the challenges, query expansion
    has proved to be an effective technique for improving the recall of retrieval
    and generating more context aware responses.
  prefs: []
  type: TYPE_NORMAL
- en: Query transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared to query expansion, in query transformation, instead of the original
    user query, retrieval happens on a transformed query, which is more suitable for
    the retriever.
  prefs: []
  type: TYPE_NORMAL
- en: '*Rewrit**e*—Queries are rewritten from the input. The input in quite a few
    real-world applications may not be a direct query or a query suited for retrieval.
    Based on the input, a language model can be trained to transform the input into
    a query that can be used for retrieval. A user’s statement like, “I can’t send
    emails from my phone” can be rewritten as “Troubleshooting steps for resolving
    email sending issues on smartphones,” making it more suitable for retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HyD**E*—Hypothetical document embedding, or HyDE, is a technique where the
    language model first generates a hypothetical answer to the user’s query without
    accessing the knowledge base. This generated answer is then used to perform a
    similarity search against the document embeddings in the knowledge base, effectively
    retrieving documents that are similar to the hypothetical answer rather than the
    query itself. Here is an example that generates hypothetical document embeddings:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Original query'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Prompts for generating HyDE'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Uses OpenAI to generate a hypothetical answer'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses OpenAI Embeddings to convert Hyde into embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges similar to query expansion such as drift from original query and
    maintaining intent also persist in query transformation strategies. Effective
    rewriting and transformation of the query result in enhancing the context awareness
    of the system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Query routing**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Different queries can demand different retrieval methods. Based on criteria
    such as intent, domain, language, complexity, source of information, and so forth,
    queries need to be classified so that they can follow the appropriate retrieval
    method. This is the idea behind optimizing the user query by routing it to the
    appropriate workflow. Types of routing techniques include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Intent classificatio**n*—A pre-trained classification model is used to classify
    the intent of the user query to select the appropriate retrieval method. A modification
    to this technique is prompt-based classification, where instead of a pre-trained
    classifier, an LLM is prompted to categorize the query into an intent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metadata routin**g*—In this approach, keywords and tags are extracted from
    the user query and then filtering is done on the chunk metadata to narrow down
    the scope of the search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Semantic routin**g*—In this approach, the user query is matched with a pre-defined
    set of queries for each retrieval method. Wherever the similarity between the
    user query and pre-defined queries is the highest, that retrieval method is invoked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In customer support chatbots, query routing ensures that technical queries are
    directed to databases with troubleshooting guides, while billing questions are
    routed to account information, enhancing user satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing query routing takes effort and skill. It introduces a whole new
    predictive component, bringing uncertainty to the process. Therefore, it must
    be carefully crafted. Query routing is a must when dealing with source data and
    query type variability.
  prefs: []
  type: TYPE_NORMAL
- en: Although the universe of pre-retrieval strategies and techniques is expansive
    and ever-evolving, we have looked at a few of the most popular and effective techniques
    in this section. Bear in mind that the applicability of the strategies will depend
    on the nature of the content in the knowledge base and the use case. However,
    using each of these strategies will result in incremental gains in the RAG system
    performance. Now that we have set up the knowledge base and the user query for
    better retrieval, let’s discuss important retrieval strategies in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Retrieval strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interventions in the pre-retrieval stage can bring significant improvements
    in the performance of the RAG system if the query and the knowledge base become
    well aligned with the retrieval algorithm. We have discussed quite a few retrieval
    algorithms in chapter 4\. In this section, we focus on strategies that can be
    employed for better retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Hybrid retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hybrid retrieval strategy is an essential component of production-grade RAG
    systems. It involves combining retrieval methods for improved retrieval accuracy.
    This can mean simply using a keyword-based search along with semantic similarity.
    It can also mean combining all sparse embedding, dense embedding vector, and knowledge
    graph-based search. The retrieval can be a union or an intersection of all these
    methods, depending on the requirements of precision and recall. It generally follows
    a weighted approach to retrieval. Figure 6.5 shows the hybrid retriever querying
    graph and vector storage.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F05_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5  Hybrid retriever employs multiple querying techniques and combines
    the results.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.4.2 Iterative retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using a retrieve–generate linear process, the iterative retrieval
    strategy searches the knowledge base repeatedly based on the original query and
    the generated text, which allows the system to gather more information by refining
    the search based on initial results. It is useful when solving multi-hop or complex
    queries. While effective, iterative retrieval can lead to longer processing times
    and may introduce challenges in managing larger amounts of retrieved information.
    There are examples of iterative retrieval that have demonstrated remarkably improved
    performance such as Iter-RetGen, which is an iterative approach that alternates
    between retrieval and generation steps.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3 Recursive retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The recursive retrieval strategy builds on the idea of iterative retrieval by
    transforming the query iteratively depending on the results obtained. While the
    initial query is used to retrieve the chunks, new focused queries are generated
    based on these chunks. It, therefore, leads to a better ability to find scattered
    information across document chunks and a more coherent and contextual response.
    Iterative retrieval chain-of-thought (IRCoT) is a recursive retrieval technique
    that combines iterative retrieval with CoT prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.4 Adaptive retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adaptive retrieval also follows the approach of repeated retrieval cycles. In
    adaptive retrieval strategies, an LLM is enabled to determine the most appropriate
    moment and content for retrieval. The objective of adaptive retrieval is to make
    the retrieval process more personalized to users and context. It is applied in
    areas such as adapting queries depending on user behavior or adjusting retrieval
    based on user performance. FLARE and Self-RAG are two popular examples of adaptive
    retrieval. Self-RAG introduces “reflection tokens” that enable the model to introspect
    and decide when additional retrieval is necessary. FLARE (forward-looking active
    retrieval-augmented generation) predicts future content needs based on the current
    generation and retrieves relevant information proactively. Adaptive retrieval
    is a part of a broader trend of agentic AI. Agentic AI refers to AI systems that
    can make autonomous decisions during tasks, adapting their actions based on the
    context. In the context of RAG, agentic RAG involves AI agents that dynamically
    decide when and how to retrieve information, thus enhancing the flexibility and
    efficiency of the retrieval process. Agentic AI is an important emerging RAG pattern.
    We will discuss Agentic RAG in detail in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 compares the three retrieval strategies that focus on repeated retrieval
    cycles. While recursive and iterative approaches need a threshold to break out
    of the iterations, in the adaptive approach, a judge model decides on-demand retrieval
    and generation steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F06_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6  Iterative, recursive, and adaptive retrieval incorporate repeated
    retrieval cycles. Source: Adapted from Gao et al., December 18, 2023\. “*Retrieval-Augmented
    Generation for Large Language Models: A Survey.”*'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: All the advanced retrieval strategies introduce overheads in terms of computational
    complexity, and therefore the accuracy must be balanced against the cost and latency
    of the system.
  prefs: []
  type: TYPE_NORMAL
- en: By employing advanced pre-retrieval techniques and a suitable retrieval strategy,
    we can expect that richer, deeper, and more relevant context is being retrieved
    from the knowledge base. Even when the relevant context is retrieved, the LLM
    may struggle to assimilate all the information. To address this problem, in the
    next section, we discuss a couple of post-retrieval strategies that help curate
    the context before augmenting the prompt with the necessary information.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Post-retrieval techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even if the retrieval of the chunks happens in an expected manner, a point of
    failure still remains. The LLM might not be able to process all the information.
    This may be due to redundancies or disjointed nature of the context among many
    other reasons. At the post-retrieval stage, the approaches of re-ranking and compression
    help in providing better context to the LLM for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Excessively long context has the potential of introducing noise into the system.
    This diminishes the LLM’s capability to process information. Consequently, hallucinations
    and irrelevant responses to the query may persist. In prompt compression, language
    models are used to detect and remove unimportant and irrelevant tokens. Apart
    from making the context more relevant, prompt compression also has a positive
    influence on cost and efficiency. Another advantage of prompt compression is being
    able to reduce the size of the prompt so that it can fit into the context window
    of the LLM. COCOM is a context compression method that compresses contexts into
    a small number of context embeddings. Similarly, xRAG is a method that uses document
    embeddings as features. Compression can lead to loss of information, and therefore,
    there needs to be a balance between compression and performance. A very simple
    prompt to compress a long-retrieved context is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Re-ranking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reordering all the retrieved documents ensures that the most relevant information
    is prioritized for the generation step. It refines retrieval results by prioritizing
    documents that are more contextually appropriate for the query, improving the
    overall quality and accuracy of information used for generation. Re-ranking also
    addresses the question of prioritization when a hybrid approach to retrieval is
    employed and improves the overall response quality. There are commonly available
    re-rankers such as multi-vector, Learning to Rank (LTR), BERT-based, and even
    hybrid re-rankers that can be employed. Specialized APIs such as Cohere Rerank
    offer pre-trained models for efficient reranking integration.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discuss some of the popular advanced RAG strategies and
    techniques employed at different stages of the RAG pipeline. It is important to
    also consider the tradeoffs that come with these techniques. Almost any advanced
    technique will introduce overheads to the system. These can be in the form of
    computational load, latency in the system, and increased storage and memory requirements.
    Therefore, these techniques warrant a performance versus overhead assessment catered
    to specific use cases. Table 6.1 provides a summary of the 12 strategies discussed
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 Advanced RAG strategies with their benefits and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Strategy | Description | Benefits | Challenges |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Chunk optimization | Adjusting document chunks for optimal size and context
    | Improves retrieval accuracy, processing speed, and storage | Requires experimentation;
    optimal chunk varies by use case |'
  prefs: []
  type: TYPE_TB
- en: '| Metadata enhancements | Enriching chunks with additional metadata for better
    filtering and searchability | Improves retrieval efficiency; reduces noise | Requires
    careful schema design; manages processing costs |'
  prefs: []
  type: TYPE_TB
- en: '| Index structures | Organizing data in structured formats for efficient retrieval
    | Enhances accuracy and context in retrieval | Increases memory and computational
    load |'
  prefs: []
  type: TYPE_TB
- en: '| Query expansion | Enriching the user query to retrieve more relevant information
    | Increases recall; overcomes brief queries | May reduce precision; risk of contextual
    drift |'
  prefs: []
  type: TYPE_TB
- en: '| Query transformation | Modifying the user query for better retrieval suitability
    | Enhances context awareness; maintains intent | Potential for misinterpretation;
    drift from the original query |'
  prefs: []
  type: TYPE_TB
- en: '| Query routing | Directing queries to appropriate retrieval methods based
    on classification | Enhances retrieval by matching method to query type | Introduces
    uncertainty; requires careful crafting |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid retrieval | Combining multiple retrieval methods (e.g., keyword and
    semantic) | Improves retrieval accuracy and robustness | Increased complexity;
    requires method weighting |'
  prefs: []
  type: TYPE_TB
- en: '| Iterative retrieval | Repeatedly searching based on initial results and query
    refinement | Gathers more comprehensive information; refines search | Longer processing
    times; managing more data |'
  prefs: []
  type: TYPE_TB
- en: '| Recursive retrieval | Iteratively transforming the query based on obtained
    results | Finds scattered information; provides coherent responses | Similar to
    iterative retrieval; potential for increased load |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptive retrieval | LLM decides when and what to retrieve during generation
    | Personalized and context-aware retrieval; dynamic adaptation | Increased computational
    complexity; part of agentic AI |'
  prefs: []
  type: TYPE_TB
- en: '| Compression | Reducing context length by removing irrelevant information
    | Fits within LLM context window; reduces noise and costs | Potential loss of
    important information; needs balance |'
  prefs: []
  type: TYPE_TB
- en: '| Reranking | Reordering retrieved documents to prioritize relevance | Enhances
    response quality; ensures most relevant info is used | Requires additional models;
    may introduce overhead |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.7 is an illustrative example of what a generation pipeline looks like
    after incorporating advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F07_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7  Illustrative example of advanced generation pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While these advanced strategies and techniques are extremely useful in improving
    performance, a RAG system also needs to provide customization and flexibility.
    This is because we may need to quickly adopt different techniques as the nature
    of data and queries evolve. A modular RAG approach discussed in the next section
    aims to provide greater architectural flexibility over the traditional RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Modular RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI systems are becoming increasingly complex, demanding more customizable,
    flexible, and scalable RAG architectures. The emergence of modular RAG is a leap
    forward in the evolution of RAG systems. Modular RAG breaks down the traditional
    monolithic RAG structure into interchangeable components. This allows for tailoring
    of the system to specific use cases. The modular approach brings modularity to
    RAG components, such as retrievers, indexing, and generation, while also adding
    more modules such as search, memory, and fusion. We can think of the modular RAG
    approach in two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Core components of RAG developed as flexible, interchangeable modules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized modules to enhance the core features of retrieval, augmentation,
    and generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.6.1 Core modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core components of the RAG system (i.e. indexing, retrieval, augmentation
    and generation), along with the advanced pre- and post-retrieval techniques, are
    composed as flexible, interchangeable modules in the modular RAG framework.
  prefs: []
  type: TYPE_NORMAL
- en: '*Indexing modul**e*—The indexing module serves as the foundation for building
    the knowledge base. By modularizing this component, developers can choose from
    various embedding models for advanced semantic understanding. Vector stores can
    be interchanged based on scalability and performance needs. Additionally, chunking
    methods can be adapted to the data structure, whether it’s text, code, or multimedia
    content, ensuring optimal indexing for retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieval modul**e*—The retrieval module enables the use of diverse retrieval
    algorithms. For instance, developers can switch between semantic similarity search
    using dense embeddings and traditional keyword-based search such as BM25\. This
    flexibility allows for tailoring retrieval methods to the specific requirements
    of the application, such as prioritizing speed, accuracy, or resource utilization.
    For example, a customer support chatbot might use semantic search during off-peak
    hours for higher accuracy and switch to keyword search during peak hours to handle
    increased load. The modular retrieval component allows this dynamic interchange
    of retrieval strategies based on real-time needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generation modul**e*—In the generation module, the choice of LLM is modular.
    Developers can select from models such as GPT-4 for complex language generation
    or smaller models for cost efficiency. This module also handles prompt engineering
    for augmentation to guide the LLM in generating accurate and relevant responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pre-retrieval modul**e*—Allows flexibility of pre-retrieval techniques to
    improve the quality of indexed content and user query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Post-retrieval modul**e*—Like the pre-retrieval module, this module allows
    for flexible implementation of post-retrieval techniques to refine and optimize
    the retrieved context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may note that the first three modules complete the naïve RAG approach, and
    the addition of the pre-retrieval and post-retrieval modules enhances the naïve
    RAG into an advanced RAG implementation. It can also be said that naïve RAG is
    a special (and limited) case of advanced RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.2 New modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The modular RAG framework has introduced several new components to enhance the
    retrieval and generation capabilities of naïve and advanced RAG approaches. Some
    of these components/modules are
  prefs: []
  type: TYPE_NORMAL
- en: '*Searc**h*—The search module is aimed at performing searches on different data
    sources. It is customized to different data sources and aimed at increasing the
    source data for better response generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fusio**n*—RAG fusion improves traditional search systems by overcoming their
    limitations through a multi-query approach. The fusion module enhances retrieval
    by expanding the user’s query into multiple, diverse perspectives using an LLM.
    It then conducts parallel searches for these expanded queries, fuses the results
    by reranking and selecting the most relevant information, and presents a comprehensive
    answer. This approach captures both explicit and implicit information, uncovering
    deeper insights that might be missed with a single query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memor**y*—The memory module uses the inherent memory of the LLM, meaning the
    knowledge encoded within its parameters from pre-training. This module uses the
    LLM to recall information without explicit retrieval, guiding the system on when
    to retrieve additional data and when to rely on the LLM’s internal knowledge.
    It can involve techniques such as using reflection tokens or prompts that encourage
    the model to introspect and decide if more information is needed. For example,
    when answering a query about historical events, the memory module can decide to
    rely on the LLM’s knowledge about World War II to provide context, only retrieving
    specific dates or figures as needed. This approach reduces unnecessary retrieval
    and uses the model’s pre-trained knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Routin**g*—Routing in the RAG system navigates through diverse data sources,
    selecting the optimal pathway for a query, whether it involves summarization,
    specific database searches, or merging different information streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Task adapte**r*—This module makes RAG adaptable to various downstream tasks
    allowing the development of task-specific end-to-end retrievers with minimal examples,
    demonstrating flexibility in handling different tasks. The task adapter module
    allows the RAG system to be fine-tuned for specific tasks like summarization,
    translation, or sentiment analysis. By incorporating a small number of task-specific
    examples or prompts, the module adjusts the retrieval and generation components
    to produce outputs tailored to the desired task, enhancing versatility without
    extensive retraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may observe that advanced RAG is a special case within the modular RAG framework.
    You also saw earlier that naïve RAG is a special case of advanced RAG. This means
    that the RAG approaches (i.e., naïve, advanced, and modular) are not competing
    but progressive. You may start by trying out a naïve implementation of RAG and
    move to a more modular approach. Figure 6.8 shows the progression of RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH06_F08_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8  Naïve, advanced, and modular approaches to RAG are progressive.
    Naïve RAG is a sub-component of advanced RAG, which is a sub-component of modular
    RAG.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While building a modular RAG system, remember that each module should be designed
    to work independently. This requires defining clear inputs and outputs. Along
    with the independent modules, the orchestration layer should be flexible to allow
    mixing and matching of modules. One should also bear in mind that a modular approach
    introduces complexity in the process. Managing interfaces, dependencies, configurations,
    and versions of modules can be complex. Ensuring compatibility and consistency
    between modules can be challenging. Testing each module independently and collectively
    requires a robust evaluation strategy. Extra modules may also add latency and
    inference costs to the system.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the added complexities, the modular approach toward RAG is state-of-the-art
    in large-scale RAG systems. It enables rapid experimentation, efficient optimization,
    and seamless integration of new technologies as they emerge. By offering the ability
    to mix and match different modules, modular RAG empowers you to build more robust,
    accurate, and versatile AI solutions. It also facilitates easier maintenance,
    updates, and scalability, making it an ideal choice for managing complex, evolving
    knowledge bases.
  prefs: []
  type: TYPE_NORMAL
- en: This section concludes the discussion on improving RAG performance using advanced
    techniques and a modular framework. Interventions can be employed at different
    stages of the indexing and generation pipelines. Modular approaches to RAG enable
    rapid experimentation, flexibility, and scalable architecture. You will need to
    experiment to figure out the techniques that help in improving RAG for specific
    use cases. It is also important to be mindful of the tradeoffs. Advanced techniques
    introduce complexities that have an effect on computation, memory, and storage
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: This is one aspect of putting RAG in production. Advanced techniques are necessary
    for RAG systems to achieve acceptable accuracy and efficiency. The other enablers
    for RAG systems in production are the tools and technologies that form the backbone
    of the RAG stack. In the next chapter, we will look at this technology infrastructure
    that enables RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Limitations of naïve RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Naïve RAG follows a simple “retrieve then read” process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach suffers from low precision and incomplete retrieval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval often misses relevant information and pulls in irrelevant content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the augmentation stage, there is often redundancy from similar retrieved
    documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context can become disjointed when sourced from multiple documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generation stage faces hallucinations and biased outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can overly rely on retrieved data and ignore its internal knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced RAG techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The advanced RAG process follows a “rewrite then retrieve then re-rank then
    read” framework, where the query is optimized through rewriting, retrieval is
    enhanced for better precision, results are re-ranked to prioritize relevance,
    and the most relevant information is used for generating the final response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-retrieval techniques include
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Index optimizatio**n*—Improves document storage for better searchability'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk optimizatio**n*—Balances chunk sizes to avoid losing context or introducing
    noise'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Context-enriched chunkin**g*—Adds summaries to each chunk to improve retrieval'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metadata enhancement**s*—Adds tags and metadata like timestamps or categories
    for better filtering'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Query optimizatio**n*—Expands or rewrites user queries for improved retrieval
    accuracy'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval techniques include
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hybrid retrieva**l*—Combines keyword-based and semantic searches'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Iterative retrieva**l*—Refines searches by repeatedly querying based on initial
    results'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recursive retrieva**l*—Generates new queries based on retrieved chunks to
    gather more relevant information'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-retrieval techniques include
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Compressio**n*—Reduces unnecessary context to remove noise and fit within
    the model’s context window'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Re-rankin**g*—Reorders retrieved documents to prioritize the most relevant
    ones'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Modular RAG framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Core modules include
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Indexing modul**e*—Allows flexible embedding models and vector store options'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieval modul**e*—Supports switching between dense and keyword-based retrieval
    methods'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generation modul**e*—Offers flexibility in selecting language models based
    on complexity and cost'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: New modules include
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Search modul**e*—Tailors search to specific data sources for better results'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fusion modul**e*—Expands user queries into multiple forms and combines retrieved
    results for deeper insights'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memory modul**e*—Uses the model’s internal knowledge to reduce unnecessary
    retrieval, retrieving only when needed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Routing modul**e*—Dynamically selects the best path for handling different
    types of queries'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Task adapter modul**e*—Adapts the system for different downstream tasks like
    summarization or translation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tradeoffs and best practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Advanced techniques improve RAG accuracy but add complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques such as hybrid retrieval or re-ranking can increase computational
    costs and latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modular RAG offers flexibility but requires careful management of interfaces
    and module compatibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing each module independently and as a whole is important to ensure system
    stability and performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tradeoffs between performance, cost, and system complexity should be carefully
    assessed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
