- en: 'Chapter 6\. Text II: Word Vectors, Advanced RNN, and Embedding Visualization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we go deeper into important topics discussed in [Chapter 5](ch05.html#text_i)
    regarding working with text sequences. We first show how to train word vectors
    by using an unsupervised method known as *word2vec*, and how to visualize embeddings
    interactively with TensorBoard. We then use pretrained word vectors, trained on
    massive amounts of public data, in a supervised text-classification task, and
    also introduce more-advanced RNN components that are frequently used in state-of-the-art
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Word Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html#text_i) we introduced RNN models and working with text
    sequences in TensorFlow. As part of the supervised model training, we also trained
    word vectors—mapping from word IDs to lower-dimensional continuous vectors. The
    reasoning for this was to enable a scalable representation that can be fed into
    an RNN layer. But there are deeper reasons for the use of word vectors, which
    we discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the sentence appearing in [Figure 6-1](#generating_skip_grams_from_text):
    “Our company provides smart agriculture solutions for farms, with advanced AI,
    deep-learning.” This sentence may be taken from, say, a tweet promoting a company. As
    data scientists or engineers, we now may wish to process it as part of an advanced
    machine intelligence system, that sifts through tweets and automatically detects
    informative content (e.g., public sentiment).'
  prefs: []
  type: TYPE_NORMAL
- en: In one of the major traditional natural language processing (NLP) approaches
    to text processing, each of the words in this sentence would be represented with
    N ID—say, an integer. So, as we posited in the previous chapter, the word “agriculture”
    might be mapped to the integer 3452, the word “farm” to 12, “AI” to 150, and “deep-learning”
    to 0.
  prefs: []
  type: TYPE_NORMAL
- en: While this representation has led to excellent results in practice in some basic
    NLP tasks and is still often used in many cases (such as in bag-of-words text
    classification), it has some major inherent problems. First, by using this type
    of atomic representation, we lose all meaning encoded within the word, and crucially,
    we thus lose information on the semantic proximity between words. In our example,
    we of course know that “agriculture” and “farm” are strongly related, and so are
    “AI” and “deep-learning,” while deep learning and farms don’t usually have much
    to do with one another. This is not reflected by their arbitrary integer IDs.
  prefs: []
  type: TYPE_NORMAL
- en: Another important issue with this way of looking at data stems from the size
    of typical vocabularies, which can easily reach huge numbers. This means that
    naively, we could need to keep millions of such word identifiers, leading to great
    data sparsity and in turn, making learning harder and more expensive.
  prefs: []
  type: TYPE_NORMAL
- en: With images, such as in the MNIST data we used in the first section of [Chapter 5](ch05.html#text_i),
    this is not quite the case. While images can be high-dimensional, their natural
    representation in terms of pixel values already encodes some semantic meaning,
    and this representation is dense.  In practice, RNN models like the one we saw
    in Chapter [5](ch05.html#text_i) require dense vector representations to work
    well.
  prefs: []
  type: TYPE_NORMAL
- en: We would like, therefore, to use dense vector representations of words, which
    carry semantic meaning. But how do we obtain them?
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html#text_i) we trained supervised word vectors to solve
    a specific task, using labeled data. But it is often expensive for individuals
    and organizations to obtain labeled data, in terms of the resources, time, and
    effort involved in manually tagging texts or somehow acquiring enough labeled
    instances. Obtaining huge amounts of unlabeled data, however, is often a much
    less daunting endeavor. We thus would like a way to use this data to train word
    representations, in an unsupervised fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are actually many ways to do unsupervised training of word embeddings,
    including both more traditional approaches to NLP that can still work very well
    and newer methods, many of which use neural networks. Whether old or new, these
    all rely at their core on the *distributional hypothesis*, which is most easily
    explained by a well-known quote by linguist John Firth: “You shall know a word
    by the company it keeps.”  In other words, words that tend to appear in similar
    contexts tend to have similar semantic meanings.'
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we focus on powerful word embedding methods based on neural networks.
    In [Chapter 5](ch05.html#text_i) we saw how to train them as part of a downstream
    text-classification task. We now show how to train word vectors in an unsupervised
    manner, and then how to use pretrained vectors that were trained using huge amounts
    of text from the web.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec is a very well-known unsupervised word embedding approach. It is actually
    more like a family of algorithms, all based in some way on exploiting the context
    in which words appear to learn their representation (in the spirit of the distributional
    hypothesis). We focus on the most popular word2vec implementation, which trains
    a model that, given an input word, predicts the word’s context by using something
    known as *skip-grams*. This is actually rather simple, as the following example
    will demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider, again, our example sentence: “Our company provides smart agriculture
    solutions for farms, with advanced AI, deep-learning.” We define (for simplicity) the
    context of a word as its immediate neighbors (“the company it keeps”)—i.e., the
    word to its left and the word to its right. So, the context of “company” is [our,
    provides], the context of “AI” is [advanced, deep-learning], and so on (see [Figure 6-1](#generating_skip_grams_from_text)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Generating skip-grams from text.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the skip-gram word2vec model, we train a model to predict context based on
    an input word. All that means in this case is that we generate training instance
    and label pairs such as (our, company), (provides, company), (advanced, AI), (deep-learning,
    AI), etc.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these pairs we extract from the data, we also sample “fake” pairs—that
    is, for a given input word (such as “AI”), we also sample random noise words as
    context (such as “monkeys”), in a process known as *negative sampling*. We use
    the true pairs combined with noise pairs to build our training instances and labels,
    which we use to train a binary classifier that learns to distinguish between them.
    The trainable parameters in this classifier are the vector representations—word
    embeddings. We tune these vectors to yield a classifier able to tell the difference
    between true contexts of a word and randomly sampled ones, in a binary classification
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow enables many ways to implement the word2vec model, with increasing
    levels of sophistication and optimization, using multithreading and higher-level
    abstractions for optimized and shorter code. We present here a fundamental approach,
    which will introduce you to the core ideas and operations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive straight into implementing the core ideas in TensorFlow code.
  prefs: []
  type: TYPE_NORMAL
- en: Skip-Grams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by preparing our data and extracting skip-grams. As in [Chapter 5](ch05.html#text_i),
    our data comprises two classes of very short “sentences,” one composed of odd
    digits and the other of even digits (with numbers written in English). We make
    sentences equally sized here, for simplicity, but this doesn’t really matter for
    word2vec training. Let’s start by setting some parameters and creating sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at our sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, as in [Chapter 5](ch05.html#text_i), we map words to indices by creating
    a dictionary with words as keys and indices as values, and create the inverse
    map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To prepare the data for word2vec, let’s create skip-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Each skip-gram pair is composed of target and context word indices (given by
    the `word2index_map` dictionary, and not in correspondence to the actual digit
    each word represents). Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can generate batches of sequences of word indices, and check out the original
    sentences with the inverse dictionary we created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create our input and label placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Embeddings in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 5](ch05.html#text_i), we used the built-in `tf.nn.embedding_lookup()`
    function as part of our supervised RNN. The same functionality is used here. Here
    too, word embeddings can be viewed as lookup tables that map words to vector values,
    which are optimized as part of the training process to minimize a loss function.
    As we shall see in the next section, unlike in [Chapter 5](ch05.html#text_i),
    here we use a loss function accounting for the unsupervised nature of the task,
    but the embedding lookup, which efficiently retrieves the vectors for each word
    in a given sequence of word indices, remains the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The Noise-Contrastive Estimation (NCE) Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our introduction to skip-grams, we mentioned we create two types of context–target
    pairs of words: real ones that appear in the text, and “fake” noisy pairs that
    are generated by inserting random context words. Our goal is to learn to distinguish
    between the two, helping us learn a good word representation. We could draw random
    noisy context pairs ourselves, but luckily TensorFlow comes with a useful loss
    function designed especially for our task. `tf.nn.nce_loss()` automatically draws
    negative (“noise”) samples when we evaluate the loss (run it in a session):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We don’t go into the mathematical details of this loss function, but it is sufficient
    to think of it as a sort of efficient approximation to the ordinary softmax function
    used in classification tasks, as introduced in previous chapters. We tune our
    embedding vectors to optimize this loss function. For more details about it, see
    the official TensorFlow [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss)
    and references within.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re now ready to train. In addition to obtaining our word embeddings in TensorFlow,
    we next introduce two useful capabilities: adjustment of the optimization learning
    rate, and interactive visualization of embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning Rate Decay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in previous chapters, gradient-descent optimization adjusts weights
    by making small steps in the direction that minimizes our loss function. The `learning_rate`
    hyperparameter controls just how aggressive these steps are. During gradient-descent
    training of a model, it is common practice to gradually make these steps smaller
    and smaller, so that we allow our optimization process to “settle down” as it
    approaches good points in the parameter space. This small addition to our training
    process can actually often lead to significant boosts in performance, and is a
    good practice to keep in mind in general.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.train.exponential_decay()` applies exponential decay to the learning rate,
    with the exact form of decay controlled by a few hyperparameters, as seen in the
    following code (for exact details, see the official TensorFlow documentation at
    [http://bit.ly/2tluxP1](http://bit.ly/2tluxP1)). Here, just as an example, we
    decay every 1,000 steps, and the decayed learning rate follows a staircase function—a
    piecewise constant function that resembles a staircase, as its name implies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Training and Visualizing with TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We train our graph within a session as usual, adding some lines of code enabling
    cool interactive visualization in TensorBoard, a new tool for visualizing embeddings
    of high-dimensional data—typically images or word vectors—introduced for TensorFlow
    in late 2016.
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a TSV (tab-separated values) metadata file. This file connects
    embedding vectors with associated labels or images we may have for them. In our
    case, each embedding vector has a label that is just the word it stands for.
  prefs: []
  type: TYPE_NORMAL
- en: We then point TensorBoard to our embedding variables (in this case, only one),
    and link them to the metadata file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, after completing optimization but before closing the session, we normalize
    the word embedding vectors to unit length, a standard post-processing step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Checking Out Our Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take a quick look at the word vectors we got. We select one word (*one*)
    and sort all the other word vectors by how close they are to it, in descending
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s take a look at the word distances from the *one* vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We see that the word vectors representing odd numbers are similar (in terms
    of the dot product) to *one*, while those representing even numbers are not similar
    to it (and have a negative dot product with the *one* vector). We learned embedded
    vectors that allow us to distinguish between even and odd numbers—their respective
    vectors are far apart, and thus capture the context in which each word (odd or
    even digit) appeared.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in TensorBoard, go to the Embeddings tab. This is a three-dimensional interactive
    visualization panel, where we can move around the space of our embedded vectors
    and explore different “angles,” zoom in, and more (see Figures [6-2](#fig0602)
    and [6-3](#fig0603)). This enables us to understand our data and interpret the
    model in a visually comfortable manner. We can see, for instance, that the odd
    and even numbers occupy different areas in feature space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Interactive visualization of word embeddings.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/letf_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. We can explore our word vectors from different angles (especially
    useful in high-dimensional problems with large vocabularies).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Of course, this type of visualization really shines when we have a great number
    of embedded vectors, such as in real text classification tasks with larger vocabularies,
    as we will see in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications),
    for example, or in the Embedding Projector [TensorFlow demo](http://projector.tensorflow.org/). Here,
    we just give you a taste of how to interactively explore your data and deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained Embeddings, Advanced RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed earlier, word embeddings are a powerful component in deep learning
    models for text. A popular approach seen in many applications is to first train
    word vectors with methods such as word2vec on massive amounts of (unlabeled) text,
    and then use these vectors in a downstream task such as supervised document classification.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we trained unsupervised word vectors from scratch.
    This approach typically requires very large corpora, such as Wikipedia entries
    or web pages. In practice, we often use pretrained word embeddings, trained on
    such huge corpora and available online, in much the same manner as the pretrained
    models presented in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we show how to use pretrained word embeddings in TensorFlow
    in a simplified text-classification task. To make things more interesting, we
    also take this opportunity to introduce some more useful and powerful components
    that are frequently used in modern deep learning applications for natural language
    understanding: the bidirectional RNN layers and the gated recurrent unit (GRU)
    cell.'
  prefs: []
  type: TYPE_NORMAL
- en: We will expand and adapt our text-classification example from [Chapter 5](ch05.html#text_i),
    focusing only on the parts that have changed.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained Word Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we show how to take word vectors trained based on web data and incorporate
    them into a (contrived) text-classification task. The embedding method is known
    as *GloVe*, and while we don’t go into the details here, the overall idea is similar
    to that of word2vec—learning representations of words by the context in which
    they appear. Information on the method and its authors, and the pretrained vectors,
    is available on the project’s [website](http://nlp.stanford.edu/projects/glove/).
  prefs: []
  type: TYPE_NORMAL
- en: We download the Common Crawl vectors (840B tokens), and proceed to our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first set the path to the downloaded word vectors and some other parameters,
    as in [Chapter 5](ch05.html#text_i):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create the contrived, simple simulated data, also as in [Chapter 5](ch05.html#text_i)
    (see details there):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the word index map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s refresh our memory of its content—just a map from word to an (arbitrary)
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to get word vectors. There are 2.2 million words in the vocabulary
    of the pretrained GloVe embeddings we downloaded, and in our toy example we have
    only 9\. So, we take the GloVe vectors only for words that appear in our own tiny
    vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We go over the GloVe file line by line, take the word vectors we need, and normalize
    them. Once we have extracted the nine words we need, we stop the process and exit
    the loop. The output of our function is a dictionary, mapping from each word to
    its vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to place these vectors in a matrix, which is the required
    format for TensorFlow. In this matrix, each row index should correspond to the
    word index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that for the `PAD_TOKEN` word, we set the corresponding vector to 0\. As
    we saw in [Chapter 5](ch05.html#text_i), we ignore padded tokens in our call to
    `dynamic_rnn()` by telling it the original sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now create our training and test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And we create our input placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we created an `embedding_placeholder`, to which we feed the word
    vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our embeddings are initialized with the content of `embedding_placeholder`,
    using the `assign()` function to assign initial values to the `embeddings` variable.
    We set `trainable=True` to tell TensorFlow we want to update the values of the
    word vectors, by optimizing them for the task at hand. However, it is often useful
    to set `trainable=False` and not update these values; for example, when we do
    not have much labeled data or have reason to believe the word vectors are already
    “good” at capturing the patterns we are after.
  prefs: []
  type: TYPE_NORMAL
- en: There is one more step missing to fully incorporate the word vectors into the
    training—feeding `embedding_placeholder` with `embedding_matrix`. We will get
    to that soon, but for now we continue the graph building and introduce bidirectional
    RNN layers and GRU cells.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional RNN and GRU Cells
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bidirectional RNN layers are a simple extension of the RNN layers we saw in
    [Chapter 5](ch05.html#text_i). All they consist of, in their basic form, is two
    ordinary RNN layers: one layer that reads the sequence from left to right, and
    another that reads from right to left. Each yields a hidden representation, the
    left-to-right vector <math><mover accent="true"><mi>h</mi> <mo>→</mo></mover></math>
    , and the right-to-left vector  <math alttext="ModifyingAbove h With left-arrow"><mover
    accent="true"><mi>h</mi> <mo>←</mo></mover></math> . These are then concatenated
    into one vector. The major advantage of this representation is its ability to
    capture the context of words from both directions, which enables richer understanding
    of natural language and the underlying semantics in text. In practice, in complex
    tasks, it often leads to improved accuracy. For example, in part-of-speech (POS)
    tagging, we want to output a predicted tag for each word in a sentence (such as
    “noun,” “adjective,” etc.). In order to predict a POS tag for a given word, it
    is useful to have information on its surrounding words, from both directions.'
  prefs: []
  type: TYPE_NORMAL
- en: Gated recurrent unit (GRU) cells are a simplification of sorts of LSTM cells.
    They also have a memory mechanism, but with considerably fewer parameters than
    LSTM. They are often used when there is less available data, and are faster to
    compute. We do not go into the mathematical details here, as they are not important
    for our purposes; there are many good online resources explaining GRU and how
    it is different from LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow comes equipped with `tf.nn.bidirectional_dynamic_rnn()`, which is
    an extension of `dynamic_rnn()` for bidirectional layers. It takes `cell_fw` and
    `cell_bw` RNN cells, which are the left-to-right and right-to-left vectors, respectively.
    Here we use `GRUCell()` for our forward and backward representations and add dropout
    for regularization, using the built-in `DropoutWrapper()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We concatenate the forward and backward state vectors by using `tf.concat()`
    along the suitable axis, and then add a linear layer followed by softmax as in
    [Chapter 5](ch05.html#text_i):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to train. We initialize the `embedding_placeholder` by feeding
    it our `embedding_matrix`. It’s important to note that we do so after calling
    `tf.global_variables_initializer()`—doing this in the reverse order would overrun
    the pre-trained vectors with a default initializer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we extended our knowledge regarding working with text sequences,
    adding some important tools to our TensorFlow toolbox. We saw a basic implementation
    of word2vec, learning the core concepts and ideas, and used TensorBoard for 3D
    interactive visualization of embeddings. We then incorporated publicly available
    GloVe word vectors, and RNN components that allow for richer and more efficient
    models. In the next chapter, we will see how to use abstraction libraries, including
    for classification tasks on real text data with LSTM networks.
  prefs: []
  type: TYPE_NORMAL
