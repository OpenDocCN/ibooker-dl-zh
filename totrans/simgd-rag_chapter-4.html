<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>chapter-4</title>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css" />
 </head>
 <body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">4</span> </span><span class="chapter-title-text">Generation pipeline: Generating contextual LLM responses</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header"><span class="CharOverride-1">This chapter covers</span></h3> 
   <ul> 
    <li class="readable-text" id="p2"><span class="CharOverride-2">Retrievers and retrieval methodologies</span></li> 
    <li class="readable-text" id="p3"><span class="CharOverride-2">Augmentation using prompt engineering techniques</span></li> 
    <li class="readable-text" id="p4"><span class="CharOverride-2">Generation using LLMs</span> </li> 
    <li class="readable-text" id="p5"><span class="CharOverride-2">Basic implementation of the RAG pipeline in Python</span></li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>In chapter 3, we discussed the creation of the knowledge base, or the non-parametric memory of retrieval augmented generation (RAG)-based applications, via the indexing pipeline. To use this knowledge base for accurate and contextual responses, we need to create a generation pipeline that includes the steps of retrieval, augmentation, and generation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>This chapter elaborates on the three components of the generation pipeline. We begin by discussing the retrieval process, which primarily involves searching through the embeddings stored in vector databases of the knowledge base and returning a list of documents that closely match the input query of the user. You will also learn about the concept of retrievers and a few retrieval algorithms. Next, we move to the augmentation step. At this point, it is also beneficial to understand different prompt engineering frameworks used with RAG. Finally, as part of the generation step, we discuss a few stages of the LLM life cycle, such as using foundation models versus supervised fine-tuning, models of different sizes, and open source versus proprietary models in the RAG context. In each of these steps, we also highlight the benefits and drawbacks of different methods.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>By the end of this chapter, you will be equipped with an understanding of the two foundational pipelines of a RAG system. You should also be ready to build a basic RAG system.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>By the end of this chapter, you should</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p10">Know several retrievers used in RAG.</li> 
   <li class="readable-text" id="p11">Get an understanding of augmentation using prompt engineering. </li> 
   <li class="readable-text" id="p12">Learn some details about how LLMs are used in the context of RAG.</li> 
   <li class="readable-text" id="p13">Have an end-to-end knowledge of setting up a basic RAG system. </li> 
  </ul> 
  <div class="readable-text" id="p14"> 
   <p>Let’s get started with an overview of the generation pipeline before diving into each component.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.1</span> Generation pipeline overview</h2> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Recall the generation pipeline introduced in chapter 2. When a user provides an input, the generation pipeline is responsible for providing the contextual response. The retriever searches for the most appropriate information from the knowledge base. The user question is augmented with this information and passed as input to the LLM for generating the final response. This process is illustrated in figure 4.1.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>The generation pipeline involves three processes: retrieval, augmentation, and generation. The retrieval process is responsible for fetching the information relevant to the user query from the knowledge base. Augmentation is the process of combining the fetched information with the user query. Generation is the last step, in which the LLM generates a response based on the augmented prompt. This chapter discusses these three processes in detail. </p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.2</span> Retrieval</h2> 
  </div> 
  <div class="readable-text" id="p19"> 
   <p>Retrieval refers to the process of finding and extracting relevant pieces of information from a large corpus or knowledge base. As you saw in chapter 3, the information from various sources is parsed, chunked, and stored as embeddings in vector databases. These stored embeddings are also sometimes referred to as documents, and the knowledge base consists of several volumes of documents. Retrieval, essentially, is a search problem to find the documents that best match the input query.</p> 
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>Searching through the knowledge base and retrieving the right documents is done by a component called the <em>retriever</em>. In simple terms, retrievers accept a query as input and return a list of matching documents as output. This process is illustrated in figure 4.2. You can imagine that retrieval is a crucial step since the quality of the retrieved information directly affects the quality of the output that will be generated.  </p> 
  </div> 
  <div class="browsable-container figure-container " id="p21">  
   <img src="../Images/CH04_F01_Kimothi.png" alt="A diagram of a process

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.1</span><span class=""> </span><span class="">Generation pipeline overview with the three components (i.e., retrieval, augmentation, and generation)</span></h5>
  </div> 
  <div class="browsable-container figure-container " id="p22">  
   <img src="../Images/CH04_F02_Kimothi.png" alt="A diagram of a user query

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.2</span><span class=""> </span><span class="">A retriever searches through the knowledge base and returns the most relevant documents.</span></h5>
  </div> 
  <div class="readable-text" id="p23"> 
   <p>We have already discussed embeddings in chapter 3 while building the indexing pipeline. Using embeddings, we can find documents that match the user query. Embeddings is one method in which retrieval can happen. There are other methods, too, and it is worth spending some time understanding different types of retrieval methods and the way they calculate the results. </p> 
  </div> 
  <div class="readable-text intended-text" id="p24"> 
   <p>This section on retrievers first discusses different retrieval algorithms and their significance in the context of RAG. In RAG systems, one or more retrieval methods can be used to build the retriever component. Next, we look at a few examples of prebuilt retrievers that can be used directly through a framework (e.g., LangChain). These retrievers are integrated with services such as databases, cloud providers, or third-party information sources. Finally, we will close this section by building a very simple retriever in LangChain using Python. We will continue to demonstrate with this example the augmentation and generation steps, too, so that we have a full implementation of the generation pipeline by the end of this chapter.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p25"> 
   <p><span class="print-book-callout-head">Note</span> Chapter 3 discussed indexing and how to convert and store data in a numerical form that can be used to retrieve information later. You may recall we discussed embeddings at length in section 3.3. It should be intuitive that since we stored the data in the form of embeddings, to fetch this data, we will also have to work on the search using embeddings. Therefore, the retrieval process is tightly coupled with the indexing process. Whatever we use to index, we will have to use to retrieve.</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.2.1</span> Progression of retrieval methods</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>Information retrieval, or IR, is the science of searching. Whether you are searching for information in a document or for documents themselves, it falls under the gamut of information retrieval. IR has a rich history in computing, starting from Joseph Marie Jacquard’s invention of the Jacquard Loom, the first device that could read punched cards, back in the early 19<span class="_Superscript _idGenCharOverride-1">th</span> century. Since then, IR has evolved leaps and bounds from simple to highly sophisticated search and retrieval. <em>Boolean retrieval</em> is a simple keyword-based search (like the one you encounter when you press CTRL/CMD + F on your browser or word processor) where Boolean logic is used to match documents with queries based on the absence or presence of the words. Documents are retrieved if they contain the exact terms in the query, often combined with AND, NOT, and OR operators. <em>Bag of Words </em><em>(BoW)</em> was used quite often in the early days of NLP. It creates a vocabulary of all the words in the documents as a vector indicating the presence or absence of each word. Consider two sentences: “The cat sat on the mat” and “The cat in the hat.” The vocabulary is <code>[</code><code>&quot;</code><code>the</code><code>&quot;</code><code>,</code> <code>&quot;</code><code>cat</code><code>&quot;</code><code>,</code> <code>&quot;</code><code>in</code><code>&quot;</code><code>,</code> <code>&quot;</code><code>hat</code><code>&quot;</code><code>,</code> <code>&quot;</code><code>on</code><code>&quot;</code><code>,</code> <code>&quot;</code><code>mat</code><code>&quot;</code><code>]</code> and the first sentence is represented as a vector <code>[2,</code> <code>1,</code> <code>1,</code> <code>1,</code> <code>0,</code> <code>0]</code>, while the one is <code>[2,</code> <code>1,</code> <code>0,</code> <code>0,</code> <code>1, 1]</code>. While simple, it ignores the context, meaning, and the order of words. </p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>Some of these, although popular in ML and IR space, don’t make sense in the context of RAG for a variety of reasons. For our purpose, we focus on a few of the popular retrieval techniques that have been used in RAG.</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <h4 class=" readable-text-h4">Term Frequency-Inverse Document Frequency</h4> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Term Frequency–Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It assigns higher weights to words that appear frequently in a document but infrequently across the corpus. Figure 4.3 illustrates how TF-IDF is calculated for a unigram search term.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p31">  
   <img src="../Images/CH04_F03_Kimothi.png" alt="A screenshot of a computer

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.3</span><span class=""> </span><span class="">Calculating TF-IDF to rank documents based on search terms</span></h5>
  </div> 
  <div class="readable-text" id="p32"> 
   <p>LangChain also provides an abstract implementation of TF-IDF using retrievers from <code>langchain_community</code>, which, in turn, uses <code>scikit-learn</code>: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p33"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Install or Upgrade Scikit-learn</strong>
%pip install –-upgrade scikit-learn

<strong># Import TFIDFRetriever class from retrievers library</strong>
from langchain_community.retrievers import TFIDFRetriever

<strong># Create an instance of the TFIDFRetriever with texts</strong>
retriever = TFIDFRetriever.from_texts(
[&quot;Australia won the Cricket World Cup 2023&quot;,
 &quot;India and Australia played in the finals&quot;,
 &quot;Australia won the sixth time having last won in 2015&quot;]
)

<strong># Use the retriever using the invoke method</strong>
result=retriever.invoke(&quot;won&quot;)

<strong># Print the results</strong>
print(result)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>TF-IDF not only can be used for unigrams, but also for phrases (n-grams). However, even TF-IDF improves on simpler search methods by emphasizing unique words, it still lacks context and word-order consideration, making it less suitable for complex tasks like RAG.</p> 
  </div> 
  <div class="readable-text" id="p35"> 
   <h4 class=" readable-text-h4">Best Match 25</h4> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>Best Match 25 (BM25) is an advanced probabilistic model used to rank documents based on the query terms appearing in each document. It is part of the family of probabilistic information retrieval models and is considered an advancement over the classic TF-IDF model. The improvement that BM25 brings is that it adjusts for the length of the documents so that longer documents do not unfairly get higher scores. Figure 4.4 illustrates the BM25 calculation.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p37">  
   <img src="../Images/CH04_F04_Kimothi.png" alt="A screenshot of a computer

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.4</span><span class=""> </span><span class="">BM25 also considers the length of the documents.</span></h5>
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Like TF-IDF, LangChain also has an abstract implementation of BM25 (Okapi BM25, specifically) using the <code>rank_bm25</code> package: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p39"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Install or Upgrade rank_bm25</strong>
%pip install –-upgrade rank_bm25

<strong># Import BM25Retriever class from retrievers library</strong>
from langchain_community.retrievers import BM25Retriever

<strong># Create an instance of the TFIDFRetriever with texts</strong>
retriever = BM25Retriever.from_texts(
[&quot;Australia won the Cricket World Cup 2023&quot;,
 &quot;India and Australia played in the finals&quot;,
 &quot;Australia won the sixth time having last won in 2015&quot;]
)

<strong># Use the retriever using the invoke method</strong>
result=retriever.invoke(&quot;Who won the 2023 Cricket World Cup?&quot;)

<strong># Print the results</strong>
print(result)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>For long queries instead of single keywords, the BM25 value is calculated for each word in the query, and the final BM25 value for the query is a summation of the values for all the words. BM25 is a powerful tool in traditional IR, but it still doesn’t capture the full semantic meaning of queries and documents required for RAG applications. BM25 is generally used in RAG for quick initial retrieval, and then a more powerful retriever is used to re-rank the results. We will learn about re-ranking later in chapter 6, when we discuss advanced strategies for RAG.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <h4 class=" readable-text-h4">Static word embeddings</h4> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Static embeddings such as Word2Vec and GloVe represent words as dense vectors in a continuous vector space, capturing semantic relationships based on context. For instance, “king” − “man” + “woman” approximates “queen.” These embeddings can capture nuances such as similarity and analogy, which BoW, TF-IDF, and BM25 miss. However, while they provide a richer representation, they still lack full contextual understanding and are limited in handling polysemy (words with multiple meanings). The term <em>static</em> here highlights that the vector representation of words does not change with the context of the word in the input query.</p> 
  </div> 
  <div class="readable-text" id="p43"> 
   <h4 class=" readable-text-h4">Contextual embeddings</h4> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>Generated by models such as BERT or OpenAI’s text embeddings, contextual embeddings produce high-dimensional, context-aware representations for queries and documents. These models, based on transformers, capture deep semantic meanings and relationships. For example, a query about “apple” will retrieve documents discussing apple the fruit, or Apple the technology company, depending on the input query. Figure 4.5 illustrates the difference between static and contextual embeddings. Contextual embeddings represent a significant advancement in IR, providing the context and understanding necessary for RAG tasks. Despite being computationally intensive, contextual embeddings are the most widely used retrievers in RAG. Examples of embedding models discussed in section 3.3.2 are contextual embeddings.</p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>Methods such as TF-IDF and BM25 use frequency-based calculations to rank documents. In embeddings (both static and contextual), ranking is done based on a similarity score. Similarity is popularly calculated using the cosine of the angle between document vectors. We discussed cosine similarity calculation in section 3.3.3. Figure 4.6 illustrates the process of retrieval using embeddings.</p> 
  </div> 
  <div class="readable-text" id="p46"> 
   <h4 class=" readable-text-h4">Other retrieval methods</h4> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>While the discussed methods are most popular in the discourse, other methods are also available. These methods represent more recent developments and specialized approaches and are good to refer to if you want to dive deeper into the world of information retrieval:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p48"><em>Learned sparse retrieval</em>—Generates sparse, interpretable representations using neural networks (examples: SPLADE, DeepCT, and DocT5Quer)</li> 
   <li class="readable-text" id="p49"><em>Dense retrieval</em>—Encodes queries and documents as dense vectors for semantic matching (examples: dense passage retriever [DPR], ANCE, RepBERT)</li> 
  </ul> 
  <div class="browsable-container figure-container " id="p50">  
   <img src="../Images/CH04_F05_Kimothi.png" alt="A diagram of a diagram

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.5</span><span class=""> </span><span class="">Static vs. contextual embeddings</span> </h5>
  </div> 
  <ul> 
   <li class="readable-text" id="p51"><em>Hybrid retrieva</em><em>l</em>—Combines sparse and dense methods for balanced efficiency and effectiveness (examples: ColBERT, COIL)</li> 
   <li class="readable-text" id="p52"><em>Cross-encoder retrieva</em><em>l</em>—Directly compares query-document pairs using transformer models (example: BERT-based re-rankers)</li> 
   <li class="readable-text" id="p53"><em>Graph-based retrieva</em><em>l</em>—Uses graph structures to model relationships between documents (examples: TextGraphs, graph neural networks for IR)</li> 
   <li class="readable-text" id="p54"><em>Quantum-inspired retrieva</em><em>l</em>—Applies quantum computing principles to information retrieval (example: quantum language models [QLM])</li> 
   <li class="readable-text" id="p55"><em>Neural IR model</em><em>s</em>—Encompass various neural network-based approaches to information retrieval (examples: NPRF [neural PRF], KNRM [Kernel-based Neural Ranking Model]) </li> 
  </ul> 
  <div class="browsable-container figure-container " id="p56">  
   <img src="../Images/CH04_F06_Kimothi.png" alt="A diagram of a diagram

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.6</span><span class=""> </span><span class="">Similarity calculation and results ranking in embeddings-based retrieval technique</span></h5>
  </div> 
  <div class="readable-text" id="p57"> 
   <p>Table 4.1 notes the weaknesses and strengths of different retrievers. While contextual embeddings are the only ones you need to know to get started with RAG, it is useful to get familiar with other retrievers for further exploration and for cases where you want to improve retriever performance. As we discussed, the implementation of TF-IDF using the <code>scikit-learn</code> retriever and BM25 using <code>rank_bm25</code> retriever in LangChain, there are many others available that use one of the mentioned methodologies. We will look at some of the popular ones in the next section.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p58"> 
   <h5 class=" browsable-container-h5">Table 4.1 Comparison of different retrieval techniques for RAG</h5> 
   <table id="table001" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-2" /> 
     <col class="_idGenTableRowColumn-2" /> 
     <col class="_idGenTableRowColumn-3" /> 
     <col class="_idGenTableRowColumn-2" /> 
    </colgroup> 
    <thead> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-1" scope="col"> <p class="_TableHead">Technique</p> </td> 
      <td class="No-Table-Style CellOverride-2" scope="col"> <p class="_TableHead">Key feature</p> </td> 
      <td class="No-Table-Style CellOverride-2" scope="col"> <p class="_TableHead">Strengths</p> </td> 
      <td class="No-Table-Style CellOverride-2" scope="col"> <p class="_TableHead">Weaknesses</p> </td> 
      <td class="No-Table-Style CellOverride-3" scope="col"> <p class="_TableHead">Suitability for RAG</p> </td> 
     </tr> 
    </thead> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-5"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Boolean retrieval</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Exact matching with logical operators</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Simple, fast, and precise</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Limited relevance ranking; no partial matching</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Low: Too rigid</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-5"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">BoW</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Unordered word frequency counts</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Simple and intuitive</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Ignores word order and context</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Low: Lacks semantic understanding</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">TF-IDF</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Term weighting based on document and corpus frequency</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Improved relevance ranking over BoW</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Still ignores semantics and word relationships</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Low–medium: Better than BoW but limited; used in hybrid retrieval</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">BM25</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Advanced ranking function with length normalization</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Robust performance; industry standard</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Limited semantic understanding</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Medium: Good baseline for simple RAG; used in hybrid retrieval.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Static embeddings</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Fixed dense vector representations</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Captures some semantic relationships</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Context-independent; limited in polysemy <br />handling</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Medium: Introduces basic semantics</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-7"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Contextual embeddings</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Context-aware dense representations</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Rich semantic understanding; handles polysemy</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Computationally intensive</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High: Excellent semantic capture</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Learned sparse retrievers</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Neural-network-generated sparse representations</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Efficient, interpretable, and has some semantic understanding</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">May miss some semantic relationships</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High: Balances efficiency and semantics</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Dense retrievers</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Dense vector matching for queries and documents</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Strong semantic matching</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Computationally intensive; less interpretable</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High: Excellent for semantic search in RAG</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Hybrid retrievers</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Combination of sparse and dense methods</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Balances efficiency and effectiveness</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Complex to implement and tune</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High: Versatile for various RAG needs</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Cross-encoder retrievers</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Direct query-document comparison</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Very accurate relevance assessment</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Extremely computationally expensive</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Medium–high: Great for reranking in RAG</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Graph-based retrievers</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Graph structure for document relationships</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Captures complex relationships in data</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Can be complex to construct and query</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Medium–high: Good for structured data in RAG</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody ParaOverride-16">Quantum-inspired retrievers</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Quantum computing concepts in IR</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Potential for handling complex queries</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Emerging field; practical benefits not fully proven</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Low–medium: Potentially promising but not mature</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Neural IR models</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Various neural network approaches <br />to IR</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Flexible; can capture complex patterns</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Often require large training data; can be black-box</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">High: Adaptable to various RAG scenarios</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p59"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.2.2</span> Popular retrievers</h3> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Developers can build their retrievers based on one or a combination of multiple retrieval methodologies. Retrievers are used not just in RAG but in a variety of search-related tasks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p61"> 
   <p>For RAG, LangChain provides many integrations where the algorithms such as TF-IDF, embeddings and similarity search, and BM25 have been abstracted as retrievers for developers to use. We have already seen the ones for TF-IDF and BM25. Some of the other popular retrievers are described in the following sections.</p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <h4 class=" readable-text-h4">Vector stores and databases as retrievers</h4> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>Vector stores can act as the retrievers, taking away the responsibility from the developer to convert the query vector into embeddings by calculating similarity and ranking the results. FAISS is typically used in tandem with a contextual embedding model for retrieval. Other vector DBs such as PineCone, Milvus, and Weaviate provide hybrid search functionality by combining dense retrieval methods such as embeddings and sparse methods such as BM25 and SPLADE. </p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <h4 class=" readable-text-h4">Cloud providers</h4> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Cloud providers Azure, AWS, and Google also offer their retrievers. Integration with Amazon Kendra, Azure AI Search, AWS Bedrock, Google Drive, and Google Vertex AI Search provides developers with infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid queries at scale.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <h4 class=" readable-text-h4">Web information resources</h4> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Connections to information resources such as Wikipedia, Arxiv, and AskNews provide optimized search and retrieval from these sources. You can check these retrievers and more in the official LangChain documentation (<a href="https://mng.bz/gm4R"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/gm4R</span></a>)</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>This was a brief introduction to the world of retrievers. If you found the information slightly complex, you can always revisit it. At this stage, the understanding of contextual embeddings will suffice. Contextual embeddings are the most popular technique for basic RAG pipelines, and we will now create a simple retriever using OpenAI embeddings.</p> 
  </div> 
  <div class="readable-text" id="p69"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.2.3</span> A simple retriever implementation</h3> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>Before we move to the next step of the generation pipeline, let’s look at a simple example of a retriever. In chapter 3, we were working on indexing the Wikipedia page for the 2023 Cricket World Cup. If you recall, we used embeddings from OpenAI to encode the text and used FAISS as the vector index to store the embeddings. We also stored the FAISS index in a local directory. Let’s reuse this index: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p71"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Install the langchain openai library</strong>
%pip install langchain-openai==0.3.7
<strong># Import FAISS class from vectorstore library</strong>
from langchain_community.vectorstores import FAISS

<strong># Import OpenAIEmbeddings from the library</strong>
from langchain_openai import OpenAIEmbeddings

<strong># Set the OPENAI_API_KEY as the environment variable</strong>
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &lt;YOUR_API_KEY&gt;

<strong># Instantiate the embeddings object</strong>
embeddings=OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)

<strong># Load the database stored in the local directory</strong>
vector_store=FAISS.load_local(
folder_path=&quot;../../Assets/Data&quot;, 
index_name=&quot;CWC_index&quot;,
embeddings=embeddings, 
allow_dangerous_deserialization=True
)

<strong># Original Question</strong>
query = &quot;Who won the 2023 Cricket World Cup?&quot;

<strong># Ranking the chunks in descending order of similarity</strong>
retrieved_docs = vector_store.similarity_search(query, k=2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>This <code>similarity_search ()</code> function returns a list of matching documents ordered by a score. This score is a quantification of the similarity between the query and the document and is hence called the similarity score. In this example, the vector index’s inbuilt similarity search feature was used for retrieval. As one of the retrievers we discussed in section 4.2.2, the vector store itself acted as the retriever. <code>K=2</code> tells the function to retrieve the top two documents. This is the most basic implementation of a retriever in the generation pipeline of a RAG system, and the retrieval method is enabled by embeddings. We used the text-embedding-3-small from OpenAI. FAISS calculated the similarity score based on these embeddings. </p> 
  </div> 
  <div class="readable-text intended-text" id="p73"> 
   <p>Retrievers are the backbone of RAG systems. The quality of the retriever has a great bearing on the quality of the generated output. In this section, you learned about vanilla retrieval methods. Multiple strategies are used when designing production-grade systems. We will read about these advanced strategies in chapter 6. Now that we have gained an understanding of the retrievers, we will move on to the next important step—augmentation.</p> 
  </div> 
  <div class="readable-text" id="p74"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.3</span> Augmentation</h2> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>A retriever fetches the information (or documents) that are most relevant to the user query. But, what next? How do we use this information? The answer is quite intuitive. If you recall the discussion in chapter 1, the input to an LLM is a natural language prompt. This information fetched by the retriever should also be sent to the LLM in the form of a natural language prompt. This process of combining the user query and the retrieved information is called <em>augmentation</em>.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>The augmentation step in RAG largely falls under the discipline of prompt engineering. Prompt engineering can be defined as the technique of giving instructions to an LLM to attain a desired outcome. The goal of prompt engineering is to construct the prompts to achieve accuracy and relevance in the LLM responses to the desired <span class="CharOverride-3">outcome(s). At the first glance, augmentation is quite simple—just add the retrieved information to the query. However, some nuanced augmentation techniques help improve the quality of the generated results. See figure 4.7 for an example of simple augmentation.</span> </p> 
  </div> 
  <div class="browsable-container figure-container " id="p77">  
   <img src="../Images/CH04_F07_Kimothi.png" alt="A diagram of a diagram

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.7</span><span class=""> </span><span class="">Simple augmentation combines the user query with retrieved documents to send to the LLM.</span></h5>
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.3.1</span> RAG prompt engineering techniques</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>Prompt engineering as a discipline has, sometimes, been dismissed as being too simple to be called engineering. You may have heard the phrase, “English is the new programming language.” Interaction with LLMs is indeed in natural language. However, what is also true is that the principles of programming are not the language in which code is written but the logic in which the machine is instructed. With that in mind, let’s examine different logical approaches that can be taken to augment the user query with the retrieved information. </p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <h4 class=" readable-text-h4">Contextual prompting</h4> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>To understand a simple augmentation technique, let’s revisit chapter 1. Recall our example of “Who won the 2023 Cricket World Cup?” We copied an excerpt from the Wikipedia article. This excerpt is the retrieved information. We then added this information to the prompt and provided an extra instruction—“Answer only based on the context provided below.” Figure 4.8 illustrates this example.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>By adding this instruction, we have set up our generation to focus only on the provided information and not on LLM’s internal knowledge (or parametric knowledge). This is a simple augmentation technique that is also referred to as <em>contextual prompting</em>. Please note that the instruction can be given in any linguistic construct. For example, we could have added the instruction at the beginning of the prompt as, “Given the context below, answer the question, Who won the 2023 Cricket World Cup. Information: &lt;Wikipedia excerpt&gt;.” We can also reiterate the instruction at the end of the prompt—“Remember to answer only based on the context provided and not from any other source.</p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <h4 class=" readable-text-h4">Controlled generation prompting</h4> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Sometimes, the information might not be present in the retrieved document. This happens when the documents in the knowledge base do not have any informationrelevant to the user query. The retriever might still fetch some documents that are the closest to the user query. In these cases, the chances of hallucination increase because the LLM will still try to follow the instructions for answering the question. To avoid this scenario, an additional instruction is added, which tells the LLM not to answer if the retrieved document does not have proper information to answer the user question (something like, “If the question cannot be answered based on the provided context, say I don’t know.”). In the context of RAG, this technique is particularly valuable because it ensures that the model’s responses are grounded in the retrieved information. If the relevant information hasn’t been retrieved or isn’t present in the knowledge base, the model is instructed to acknowledge this lack of information rather than attempting to generate a potentially incorrect answer.”</p> 
  </div> 
  <div class="browsable-container figure-container " id="p85">  
   <img src="../Images/CH04_F08_Kimothi.png" alt="A screenshot of a chat

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.8</span><span class=""> </span><span class="">Information is augmented to the original question with an added instruction.</span></h5>
  </div> 
  <div class="readable-text" id="p86"> 
   <h4 class=" readable-text-h4">Few-shot prompting</h4> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>It has been observed that while generating responses, LLMs adhere quite well to the examples provided in the prompt. If you want the generation to be in a certain format or style, it is recommended to provide a few examples. In RAG, while providing the retrieved information in the prompt, we can also specify certain examples to help guide the generation in the way we need the retrieved information to be used. This technique is called <em>few-shot prompting</em>. Here “shot” refers to the examples given in the prompt. Figure 4.9 illustrates a prompt that includes two examples with the question. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p88">  
   <img src="../Images/CH04_F09_Kimothi.png" alt="A screenshot of a text box

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.9</span><span class=""> </span><span class="">Example of few-shot prompting in the context of RAG</span></h5>
  </div> 
  <div class="readable-text" id="p89"> 
   <p>You might come across terms such as <em>one-shot prompting</em> or <span class="IndexSee">two-shot prompting</span>, which replaces the word “few” with the number of examples given. Conversely, when no example is given, and the LLM is expected to answer correctly, the technique is also called <em>zero-shot prompting</em>. </p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <h4 class=" readable-text-h4">Chain of thought prompting</h4> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>It has been observed that the introduction of intermediate reasoning steps improves the performance of LLMs in tasks requiring complex reasoning, such as arithmetic, common sense, and symbolic reasoning. The same can be applied in the context of RAG. This is called <em>chain-of-thought</em>, or CoT, <em>prompting</em>. In figure 4.10, I asked ChatGPT to analyze the performance of two teams based on the retrieved information.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p92">  
   <img src="../Images/CH04_F10_Kimothi.png" alt="A paper with text and numbers

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.10</span><span class=""> </span><span class="">Chain-of-thought (CoT) prompting for reasoning tasks</span></h5>
  </div> 
  <div class="readable-text" id="p93"> 
   <p>The CoT prompting approach can also be combined with the few-shot prompting technique, where a few examples of reasoning are provided before the final question. Creating these examples is a manually intensive task. In auto-CoT, the examples are also created using an LLM.</p> 
  </div> 
  <div class="readable-text" id="p94"> 
   <h4 class=" readable-text-h4">Other advanced prompting techniques</h4> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Prompt engineering is becoming an increasingly intricate discipline. Ongoing research constantly presents new improvements in prompting techniques. To dive deeper into prompt engineering, let’s check out some of the following techniques: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p96"><em class="CharOverride-4">Self-consistency</em>—While CoT uses a single reasoning chain in CoT prompting, self-consistency aims to sample multiple diverse reasoning paths and use their respective generations to arrive at the most consistent answer.</li> 
   <li class="readable-text" id="p97"><em>Generated knowledge promptin</em><em>g</em>—This technique explores the idea of prompt-based knowledge generation by dynamically constructing relevant knowledge chains, using models’ latent knowledge to strengthen reasoning.</li> 
   <li class="readable-text" id="p98"><em>Tree-of-thoughts promptin</em><em>g</em>—This technique maintains an explorable tree structure of coherent intermediate thought steps aimed at solving problems.</li> 
   <li class="readable-text" id="p99"><em class="CharOverride-4">Automatic reasoning and tool use </em>(ART)—The ART framework automatically interleaves model generations with tool use for complex reasoning tasks. ART employs demonstrations to decompose problems and integrate tools without task-specific scripting.</li> 
   <li class="readable-text" id="p100"><em class="CharOverride-4">Automatic prompt engineer </em>(APE)—The APE framework automatically generates and selects optimal instructions to guide models. It uses an LLM to synthesize candidate prompt solutions for a task based on output demonstrations.</li> 
   <li class="readable-text" id="p101"><em>Active promp</em><em>t</em>—Active-prompt improves CoT methods by dynamically adapting language models to task-specific prompts through a process involving query, uncertainty analysis, human annotation, and enhanced inference.</li> 
   <li class="readable-text" id="p102"><em class="CharOverride-4">ReAct prompting</em>—ReAct integrates LLMs for concurrent reasoning traces and task-specific actions, improving performance by interacting with external tools for information retrieval. When combined with CoT, it optimally utilizes internal knowledge and external information, enhancing the interpretability and trustworthiness of LLMs.</li> 
   <li class="readable-text" id="p103"><em>Recursive promptin</em><em>g</em><strong>—</strong>Recursive prompting breaks down complex problems into subproblems, solving them by sequentially using prompts. This method aids compositional generalization in tasks such as math problems or question answering, with the model building on solutions from previous steps.</li> 
  </ul> 
  <div class="readable-text" id="p104"> 
   <p>Table 4.2 summarizes different prompting techniques. Prompt engineering for augmentation is an evolving discipline. It is important to note that there is a lot of scope for creativity in writing prompts for RAG applications. Efficient prompting has a significant effect on the generated output. The kind of prompts you use will depend a lot on your use case and the nature of the information in the knowledge base. </p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p105"> 
   <h5 class=" browsable-container-h5"><span class="">Table 4.2</span><span class=""> </span><span class="">Comparison of prompting techniques for augmentation </span></h5> 
   <table id="table002" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-2" /> 
     <col class="_idGenTableRowColumn-9" /> 
     <col class="_idGenTableRowColumn-9" /> 
     <col class="_idGenTableRowColumn-9" /> 
     <col class="_idGenTableRowColumn-10" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-11"> 
      <td class="No-Table-Style CellOverride-13"> <p class="_TableHead">Technique</p> </td> 
      <td class="No-Table-Style CellOverride-14"> <p class="_TableHead">Description</p> </td> 
      <td class="No-Table-Style CellOverride-14"> <p class="_TableHead">Key advantage</p> </td> 
      <td class="No-Table-Style CellOverride-14"> <p class="_TableHead">Best use case</p> </td> 
      <td class="No-Table-Style CellOverride-15"> <p class="_TableHead">Complexity</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-12"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Contextual prompting</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Adds retrieved information to the prompt with instructions to focus on the provided context</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Ensures focus on relevant information</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">General RAG queries</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Low</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-13"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Controlled generation prompting</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Instructs the model to say “I don’t know” when information is not available </p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Reduces hallucination risk</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">When accuracy is critical</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Low</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-14"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Few-shot prompting</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Provides examples in the prompt to guide response format and style</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Improves output consistency and format adherence</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">When a specific output format is required</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Medium</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-7"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Chain-of-thought (CoT) prompting</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Introduces intermediate reasoning steps</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Improves performance on complex reasoning tasks</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Complex queries requiring step-by-step analysis</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Medium</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-7"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Self-consistency</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Samples multiple diverse reasoning paths</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Improves answer consistency and accuracy</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Tasks with multiple possible reasoning approaches</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-7"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Generated knowledge prompting</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Dynamically constructs relevant knowledge chains</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Uses the model’s latent knowledge</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Tasks requiring broad knowledge application</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-15"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Tree-of-thoughts prompting</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Maintains an explorable tree structure of thought steps</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Allows for more comprehensive problem-solving</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Complex, multistep problem solving</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-7"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Automatic reasoning and tool use (ART)</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Interleaves model generations with tool use</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Enhances problem decomposition and tool integration</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Tasks requiring external tool use</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Very High</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-5"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Automatic prompt engineer (APE)</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Automatically generates and selects optimal instructions</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Optimizes prompts for specific tasks</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Prompt optimization for complex tasks</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Very High</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-16"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Active prompt</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Dynamically adapts LMs to task-specific prompts</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Improves task-specific performance</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Tasks requiring adaptive prompting</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-14"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">ReAct prompting</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Integrates reasoning traces with task-specific actions</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Improves performance and interpretability</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Tasks requiring both reasoning and action</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">High</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-16"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Recursive prompting</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Breaks down complex problems into subproblems</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Aids in compositional generalization</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Complex, multistep problems</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">High</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>We have already built a simple retriever in the previous section. We will now execute augmentation with a simple contextual prompt with controlled generation.</p> 
  </div> 
  <div class="readable-text" id="p107"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.3.2</span> A simple augmentation prompt creation</h3> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>In section 4.2.3, we were able to implement a FAISS-based retriever using OpenAI embeddings. We will now make use of this retriever and create the augmentation prompt:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p109"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Import FAISS class from vectorstore library</strong>
from langchain_community.vectorstores import FAISS

<strong># Import OpenAIEmbeddings from the library</strong>
from langchain_openai import OpenAIEmbeddings

<strong># Set the OPENAI_API_KEY as the environment variable</strong>
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &lt;YOUR_API_KEY&gt;

<strong># Instantiate the embeddings object</strong>
embeddings=OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)

<strong># Load the database stored in the local directory</strong>
vector_store=FAISS.load_local(
folder_path=&quot;../../Assets/Data&quot;, 
index_name=&quot;CWC_index&quot;,
embeddings=embeddings, 
allow_dangerous_deserialization=True
)

<strong># Original Question</strong>
query = &quot;Who won the 2023 Cricket World Cup?&quot;

<strong># Ranking the chunks in descending order of similarity</strong>
retrieved_docs = vector_store.similarity_search(query, k=2)

<strong># Selecting the first chunk as the retrieved information</strong>
retrieved_context= retrieved_docs[0].page_content

<strong># Creating the prompt</strong>
augmented_prompt=f&quot;&quot;&quot;

Given the context below, answer the question.

Question: {query} 

Context : {retrieved_context}

Remember to answer only based on the context provided and not from any other source. 

If the question cannot be answered based on the provided context, say I don't know.

&quot;&quot;&quot;</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>With the augmentation step complete, we are now ready to send the prompt to the LLM for the generation of the desired outcome. You will now learn how LLMs generate text and the nuances of generation. </p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <h2 class=" readable-text-h2"><span class="num-string">4.4</span> Generation</h2> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>Generation is the final step of this pipeline. While LLMs may be used in any of the previous steps, the generation step relies completely on the LLM. The most popular LLMs are the ones being developed by OpenAI, Anthropic, Meta, Google, Microsoft, and Mistral, among other developers. While text generation is the core capability of LLMs, we are now seeing multimodal models that can handle images and audio along with text. Simultaneously, researchers are developing faster and smaller models. </p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>In this section, we will discuss the factors that can help choose a language model for your RAG system. We will then continue with our example of the retriever and augmented prompt we have built so far and complete it by adding the generation step.</p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.4.1</span> Categorization of LLMs and suitability for RAG</h3> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>As of June 2024, there are over a hundred LLMs available to use, and new ones are coming out every week. How do we decide then which LLM to choose for our RAG system? To show you the decision-making process, let’s discuss three themes under which we can broadly categorize LLMs: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p116">How they have been trained</li> 
   <li class="readable-text" id="p117">How they can be accessed</li> 
   <li class="readable-text" id="p118">Their size</li> 
  </ul> 
  <div class="readable-text" id="p119"> 
   <p>We will discuss the LLMs under these themes and understand the factors that may influence the LLM choice for RAG. </p> 
  </div> 
  <div class="readable-text" id="p120"> 
   <h4 class=" readable-text-h4">Original vs. fine-tuned models</h4> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>Training an LLM takes massive amounts of data and computational resources. LLMs training is done through an unsupervised learning process. All modern LLMs are autoregressive models and are trained to generate the next token in a sequence. These massive pre-trained LLMs are also called <em>foundation models.</em> </p> 
  </div> 
  <div class="readable-text intended-text" id="p122"> 
   <p>The question that you may ask is, if LLMs just predict the next tokens in a sequence, how are we able to ask questions and chat with these models? The answer is in what we call <em>supervised fine-tuning</em>, or<em> SFT.</em><em></em></p> 
  </div> 
  <div class="readable-text intended-text" id="p123"> 
   <p>Supervised fine-tuning is a process used to adapt a pre-trained language model for specific tasks or behaviors such as question-answering or chat. It involves further training a pre-trained foundation model on a labeled dataset, where the model learns to map inputs to specific desired outputs. You start with a pre-trained model, prepare a labelled dataset for the target task, and train the model on this dataset, which adjusts the model parameters to perform better on the target task. Figure 4.11 gives an overview of the SFT process.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p124">  
   <img src="../Images/CH04_F11_Kimothi.png" alt="A diagram of a training program

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 4.11</span><span class=""> </span><span class="">Supervised fine-tuning is a classification mode-training process.</span></h5>
  </div> 
  <div class="readable-text" id="p125"> 
   <p>While foundation models generalize well for a wide array of tasks, there are several use cases where the need for a fine-tuned model arises. Domain adaptation for specialized fields such as law and healthcare, task specific optimization such as classification and NER (named entity recognition), and conversational AI, personalization are some use cases where you may observe a fine-tuned model performing better.</p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>Specifically, in the context of RAG, some criteria should be considered, while choosing between a foundation model and a fine-tuning one: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p127"><em>Domain specificit</em><em>y</em>—Foundation models have broader knowledge and can handle a wider range of topics and queries for general-purpose RAG systems. If your RAG application is specialized (say, dealing with patient records or instruction manuals for heavy machinery), you may find that fine-tuning the model for specific domains may improve performance.</li> 
   <li class="readable-text" id="p128"><em>Retrieval integratio</em><em>n</em>—If you observe that a foundation model you are using is not integrating the retrieved information well, a fine-tuned model trained to better utilize information can lead to better quality of generations.</li> 
   <li class="readable-text" id="p129"><em>Deployment spee</em><em>d</em>—A foundation model can be quickly deployed since there is no additional training required. To fine-tune a model, you will need to spend time in gathering training data and the actual training of the model.</li> 
   <li class="readable-text" id="p130"><em>Customization of response</em><em>s</em>—For generating results in a specific format or custom-style elements such as tone or vocabulary, a fine-tuned model may result in better adherence to the requirements compared to foundation models. </li> 
   <li class="readable-text" id="p131"><em>Resource efficienc</em><em>y</em>—Fine-tuning a model requires more storage and computational resources. Depending on the scale of deployment, the costs may be higher for a fine-tuned model.</li> 
   <li class="readable-text" id="p132"><em>Ethical alignmen</em><em>t</em>—A fine-tuned model allows for better control over the responses in adherence to ethical guidelines and even certain privacy aspects.</li> 
  </ul> 
  <div class="readable-text" id="p133"> 
   <p>A summary of the criteria is presented in table 4.3.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p134"> 
   <h5 class=" browsable-container-h5">Table 4.3 Criteria for choosing between foundation and fine-tuned models</h5> 
   <table id="table003" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-17" /> 
     <col class="_idGenTableRowColumn-18" /> 
     <col class="_idGenTableRowColumn-19" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-11"> 
      <td class="No-Table-Style CellOverride-13"> <p class="_TableHead">Criteria</p> </td> 
      <td class="No-Table-Style CellOverride-14"> <p class="_TableHead">Better suitability</p> </td> 
      <td class="No-Table-Style CellOverride-15"> <p class="_TableHead">Explanation</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-20"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Domain specificity</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Fine-tuned models</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Better performance for specialized applications (e.g., patient records and instruction manuals)</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-20"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Retrieval integration</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Fine-tuned models</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Can be trained to better utilize retrieved information</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Deployment speed</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Foundation models</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Quicker deployment with no additional training required</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-22"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Customization of responses</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Fine-tuned models</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Better adherence to specific format, style, tone, or vocabulary requirements </p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-20"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Resource efficiency</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Foundation models</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Requires less storage and computational resources</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Ethical alignment</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Fine-tuned models</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Allows better control over responses to ethical guidelines and privacy</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>Fine-tuned models give better control over your RAG systems, but they are costly. There’s also a risk of overreliance on retrieval and a potential tradeoff between RAG performance and inherent LLM language abilities. Therefore, whether to use a foundation model or fine-tuning one depends on the improvements you are targeting, availability of data, cost, and other tradeoffs. The general recommendation is to start experimenting with a foundation model and then progress to supervised fine-tuning for performance improvement.</p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <h4 class=" readable-text-h4">Open source vs. proprietary models</h4> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>Software development and distribution are represented by two fundamentally different approaches: open versus proprietary software. The world of LLMs is no different. Some LLM developers such as Meta and Mistral have made the model weights public to foster collaboration and community-driven innovation. In contrast, pioneers such as OpenAI, Anthropic, and Google have kept the models closed, offering support, managed services, and better user experience. </p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>For RAG systems, open source models provide the flexibility of customization, deployment method, and transparency, but warrant the need for the necessary infrastructure to maintain the models. Proprietary model providers might be costlier for high volumes but provide regular updates, ease of use, scalability, and faster development, among other things. Some proprietary model providers such as OpenAI have prebuilt RAG capabilities. Your choice of the type of model you choose may depend on some of the following criteria:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p139"><em>Customizatio</em><em>n</em>—Open source LLMs are generally considered better for customizations such as deep integration with custom retrieval mechanisms. A better control over fine-tuning is also something that open source LLMs allow for. Customization of proprietary models is limited to API capabilities.</li> 
   <li class="readable-text" id="p140"><em>Ease of us</em><em>e</em>—Proprietary models, however, are much easier to use. Some of the models such as OpenAI, Cohere, and similar offer optimized, prebuilt RAG solutions.</li> 
   <li class="readable-text" id="p141"><em>Deployment flexibilit</em><em>y</em>—Open source models can be deployed according to your preference (private cloud, on-premises), while proprietary models are managed by the providers. This also has a bearing on data security and privacy. Most proprietary model providers are now offering multiple deployment <br />options.</li> 
   <li class="readable-text" id="p142"><em>Cos</em><em>t</em>—Open source LLMs may come with upfront infrastructure costs, while proprietary models are priced based on usage. Long-term costs and query volumes are considerations to choose between open source and proprietary models. Large-scale deployments may favor the use of open source models.</li> 
  </ul> 
  <div class="readable-text" id="p143"> 
   <p>The choice between open source and proprietary models for RAG depends on factors such as the scale of deployment, specific domain requirements, integration needs, and the importance of customization in the retrieval and generation process. Apart from these, the need for knowledge updates, transparency, scalability, the structure of data, compliance, and the like will determine the choice of the model. A summary of the discussion is presented in table 4.4</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p144"> 
   <h5 class=" browsable-container-h5">Table 4.4 Criteria for choosing between open source and proprietary models</h5> 
   <table id="table004" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-23" /> 
     <col class="_idGenTableRowColumn-24" /> 
     <col class="_idGenTableRowColumn-25" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-11"> 
      <td class="No-Table-Style CellOverride-13"> <p class="_TableHead">Criteria</p> </td> 
      <td class="No-Table-Style CellOverride-14"> <p class="_TableHead">Better suitability</p> </td> 
      <td class="No-Table-Style CellOverride-15"> <p class="_TableHead">Explanation</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Customization</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Open source</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Allows deeper integration with custom retrieval mechanisms and better control over fine-tuning</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-22"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Ease of use</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Proprietary</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Offers optimized, prebuilt RAG solutions and are generally easier to use</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-22"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Deployment flexibility</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Open source</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Can be deployed on private cloud or on-premises, offering more options</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Cost for large-scale deployment</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Open source</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">May be more cost-effective for large-scale deployments despite upfront infrastructure costs</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Data security and privacy</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Open source</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Offers more control over data, though some private models now offer various deployment options</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Regular updates and support</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Proprietary</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Typically provides regular updates and better support</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>A hybrid approach is also not ruled out. At a PoC stage, a proprietary model may make sense for quick experimentation. </p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>Here are some examples of popular proprietary models: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p147">GPT series by OpenAI (<a href="https://platform.openai.com/docs/models"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/platform.openai.com/docs/models</span></a>)</li> 
   <li class="readable-text" id="p148">Claude series by Anthropic (<a href="https://www.anthropic.com/claude"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/www.anthropic.com/claude</span></a>)</li> 
   <li class="readable-text" id="p149">Gemini series by Google (<a href="https://mng.bz/eBnJ"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/eBnJ</span></a>)</li> 
   <li class="readable-text" id="p150">Command R series by Cohere (<a href="https://cohere.com/command"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/cohere.com/command</span></a>)</li> 
  </ul> 
  <div class="readable-text" id="p151"> 
   <p>Some of open source models are  </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p152">Llama series by Meta (<a href="https://llama.meta.com/"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/llama.meta.com/</span></a>)</li> 
   <li class="readable-text" id="p153">Mistral (<a href="https://docs.mistral.ai/getting-started/models/"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/docs.mistral.ai/getting-started/models/</span></a>) </li> 
  </ul> 
  <div class="readable-text" id="p154"> 
   <h4 class=" readable-text-h4">Model sizes</h4> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>LLMs come in various sizes, typically measured by the number of parameters they contain. The size of the model greatly affects the capabilities along with the resource requirements. </p> 
  </div> 
  <div class="readable-text intended-text" id="p156"> 
   <p>Larger models have several billion, even trillions, of parameters. These models exhibit superior performance in reasoning abilities, and language understanding, and have broader knowledge. They can generate more coherent text, and their responses are contextually more accurate. However, these larger models have significantly high computation, storage, and energy requirements.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>Smaller models with parameter sizes in millions or a few billion offer benefits such as faster inference times, lower resource usage, and easier deployment on edge devices or resource constrained environments. Researchers and developers continue to explore methods to achieve large-model performance with smaller and more efficient architectures.</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>For a RAG system, the following should be assessed: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p159"><em>Resource constraints</em>—Small models have a much lower resource usage. Lightweight RAG applications with faster inference can be built with smaller <br />models.</li> 
   <li class="readable-text" id="p160"><em>Reasoning capability</em>—On the other spectrum of resource constraints is the language-processing ability of the model. Large models are better suited for complex reasoning tasks and can deal with ambiguity in the retrieved information. Smaller models, therefore, will rely heavily on the quality of retrieved information.</li> 
   <li class="readable-text" id="p161"><em>Deployment options</em>—The size of large models makes it difficult to deploy on-edge devices. This is a flexibility that smaller models provide, bringing RAG applications to a wide range of devices and environments.</li> 
   <li class="readable-text" id="p162"><em>Context handling</em>—Large models may be better at integrating multiple pieces of retrieved information in RAG systems since they have longer context windows. Large models are also better at handling diverse queries, while small models struggle with out-of-domain queries. Large models might perform better in RAG systems with diverse or unpredictable query types. </li> 
  </ul> 
  <div class="readable-text" id="p163"> 
   <p>In practice, most RAG applications are built on large models. However, smaller models make more sense in the long-term adoption and application of the technology. The various factors are summarized in table 4.5</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p164"> 
   <h5 class=" browsable-container-h5">Table 4.5 Criteria for choosing between small and large models</h5> 
   <table id="table005" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-26" /> 
     <col class="_idGenTableRowColumn-23" /> 
     <col class="_idGenTableRowColumn-27" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-11"> 
      <td class="No-Table-Style CellOverride-13"> <p class="_TableHead">Criteria</p> </td> 
      <td class="No-Table-Style CellOverride-14"> <p class="_TableHead">Better suitability</p> </td> 
      <td class="No-Table-Style CellOverride-15"> <p class="_TableHead">Explanation</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Resource constraints</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Small models</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Lower resource usage; suitable for lightweight RAG applications</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Reasoning capability</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Large models</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Better for complex reasoning tasks and handling ambiguity in retrieved information</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-20"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Deployment options</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Small models</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">More flexible; can be deployed on edge devices and resource-constrained environments</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-20"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Context handling</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Large models</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Better at integrating multiple pieces of retrieved information; longer context windows</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-11"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Query diversity</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Large models</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Handle diverse and unpredictable query types better</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-21"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Inference speed</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Small models</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Faster inference times; suitable for applications requiring quick responses </p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>Examples of popular small language models are:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p166">Phi-3 by Microsoft (<span class="Hyperlink">h</span><a href="https://azure.microsoft.com/en-us/products/phi-3"><span class="Hyperlink">ttps:</span><span class="Hyperlink">/</span><span class="Hyperlink">/azure.microsoft.com/en-us/products/phi-3</span></a>) </li> 
   <li class="readable-text" id="p167">Gemma by Google (<a href="https://ai.google.dev/gemma"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/ai.google.dev/gemma</span></a>)</li> 
  </ul> 
  <div class="readable-text" id="p168"> 
   <p>The choice of the LLM is a core consideration in your RAG system that requires close attention and iterations. The performance of your system may require experimenting and adapting your choice of the LLM. </p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>The list of LLMs has become almost endless. What this means for developers and businesses is that the technology has truly been democratized. While all LLMs have their unique propositions and architecture, for practical applications, there are a wide array of choices available. While simple RAG applications may rely on a single LLM provider, for more complex applications, a multi-LLM strategy may be beneficial.</p> 
  </div> 
  <div class="readable-text intended-text" id="p170"> 
   <p>We have implemented a simple retriever and created an augmented prompt. In the last section of this chapter, we round up the pipeline by creating the generation step.</p> 
  </div> 
  <div class="readable-text" id="p171"> 
   <h3 class=" readable-text-h3"><span class="num-string">4.4.2</span> Completing the RAG pipeline: Generation using LLMs</h3> 
  </div> 
  <div class="readable-text" id="p172"> 
   <p>We have built a simple retriever using FAISS and OpenAI embeddings, and we created a simple augmented prompt. Now we will use OpenAI’s latest model, GPT-4o, to generate the response:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p173"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Import FAISS class from vectorstore library</strong>
from langchain_community.vectorstores import FAISS

<strong># Import OpenAIEmbeddings from the library</strong>
from langchain_openai import OpenAIEmbeddings

<strong># Set the OPENAI_API_KEY as the environment variable</strong>
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &lt;YOUR_API_KEY&gt;

<strong># Instantiate the embeddings object</strong>
embeddings=OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)

<strong># Load the database stored in the local directory</strong>
vector_store=FAISS.load_local(
    folder_path=&quot;../../Assets/Data&quot;, 
    index_name=&quot;CWC_index&quot;,
    embeddings=embeddings, 
    allow_dangerous_deserialization=True
    )

<strong># Original Question</strong>
query = &quot;Who won the 2023 Cricket World Cup?&quot;

<strong># Ranking the chunks in descending order of similarity</strong>
retrieved_docs = vector_store.similarity_search(query, k=2)

<strong># Selecting the first chunk as the retrieved information</strong>
retrieved_context= retrieved_docs[0].page_content

<strong># Creating the prompt</strong>
augmented_prompt=f&quot;&quot;&quot;

Given the context below, answer the question.

Question: {query} 

Context : {retrieved_context}

Remember to answer only based on the context provided and not from any other source. 

If the question cannot be answered based on the provided context, say I don't know.

&quot;&quot;&quot;
<strong># Importing the OpenAI library from langchain</strong>
from langchain_openai import ChatOpenAI

<strong># Instantiate the OpenAI LLM</strong>
llm = ChatOpenAI(
            model=&quot;gpt-4o-mini&quot;,
            temperature=0,
            max_tokens=None,
            timeout=None,
            max_retries=2
)
<strong># Make the API call passing the augmented prompt to the LLM</strong>
response = llm.invoke (
     [(&quot;human&quot;,augmented_prompt)]
    )

<strong># Extract the answer from the response object</strong>
answer=response.content

print(answer)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p174"> 
   <p>And there it is. We have built a generation pipeline, albeit a very simple one. It can now fetch information from the knowledge base and generate an answer pertinent to the question asked and rooted in the knowledge base. Try asking a different question to see how well the pipeline generalizes.</p> 
  </div> 
  <div class="readable-text intended-text" id="p175"> 
   <p>We have now covered all three steps—retrieval, augmentation, and generation—of the generation pipeline. With the knowledge of the indexing pipeline (covered in chapter 3) and the generation pipeline, you are now all set to create a basic RAG system. What we have discussed so far can be termed a <em>na&iuml;ve RAG implementation</em>. Na&iuml;ve RAG can be marred by inaccuracies. It can be inefficient in retrieving and ranking information correctly. The LLM can ignore the retrieved information and still hallucinate. To discuss and address these challenges, in chapter 6, we examine advanced strategies that allow for more complex and better-performing RAG systems. </p> 
  </div> 
  <div class="readable-text intended-text" id="p176"> 
   <p>But before that, the question of evaluating the system arises. Is it generating the responses on the expected lines? Is the LLM still hallucinating? Before trying to improve the performance of the system, we need to be able to measure and benchmark it. That is what we will do in chapter 5. We will look at the evaluation metrics and the popular RAG benchmarks.</p> 
  </div> 
  <div class="readable-text" id="p177"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <div class="readable-text" id="p178"> 
   <h3 class=" readable-text-h3">Retrieval</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p179">Retrieval is the process of finding relevant information from the knowledge base based on a user query. It is a search problem to match documents with input queries.</li> 
   <li class="readable-text buletless-item" id="p180">The popular retrieval methods for RAG include 
    <ul> 
     <li><em>TF-IDF (Term Frequency-Inverse Document Frequency</em><em>)</em>—Statistical measure of word importance in a document relative to a corpus. It can be implemented using LangChain’s TFIDFRetriever.</li> 
     <li><em>BM25 (Best Match 25</em><em>)</em>—Advanced probabilistic model, an improvement over TF-IDF. It adjusts for document length and can be implemented using Lang&shy;Chain’s BM25Retriever.</li> 
     <li><em>Static word embedding</em><em>s</em>—Represent words as dense vectors (e.g., Word2Vec, GloVe) and capture semantic relationships but lack full contextual under&shy;standing.</li> 
     <li><em>Contextual embedding</em><em>s</em>—Produced by models like BERT or OpenAI’s text embeddings. They provide context-aware representations and are most widely used in RAG, despite being computationally intensive.</li> 
     <li><em>Advanced retrieval method</em><em>s</em>—They include learned sparse retrieval, dense retrieval, hybrid retrieval, cross-encoder retrieval, graph-based retrieval, quantum-inspired retrieval, and neural IR models.</li> 
    </ul> </li> 
   <li class="readable-text" id="p181">Most advanced implementations will include a hybrid approach. </li> 
   <li class="readable-text" id="p182">Vector stores and databases (e.g., FAISS, PineCone, Milvus, Weaviate), cloud provider solutions (e.g., Amazon Kendra, Azure AI Search, Google Vertex AI Search), and web information resources (e.g., Wikipedia, Arxiv, AskNews) are some of the popular retriever integrations provided by LangChain.</li> 
   <li class="readable-text" id="p183">The choice of retriever depends on factors such as accuracy, speed, and compatibility with the indexing method.</li> 
  </ul> 
  <div class="readable-text" id="p184"> 
   <h3 class=" readable-text-h3">Augmentation</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p185">Augmentation combines the user query with retrieved information to create a prompt for the LLM.</li> 
   <li class="readable-text" id="p186">Prompt engineering is crucial for effective augmentation, aiming for accuracy and relevance in LLM responses.</li> 
   <li class="readable-text buletless-item" id="p187">Key prompt engineering techniques for RAG include 
    <ul> 
     <li><em>Contextual promptin</em><em>g</em>—Adding retrieved information with instructions to focus on the provided context.</li> 
     <li><em>Controlled generation promptin</em><em>g</em>—Instructing the LLM to admit lack of knowledge when information is insufficient.</li> 
     <li><em>Few-shot promptin</em><em>g</em>—Providing examples to guide the LLM’s response format or style.</li> 
     <li><em>Chain-of-thought (CoT) promptin</em><em>g</em>—Introducing intermediate reasoning steps for complex tasks.</li> 
     <li><span class="CharOverride-4">Advanced technique</span><span class="CharOverride-4">s</span>—These include self-consistency, generated knowledge prompting, and tree of thought.</li> 
    </ul> </li> 
   <li class="readable-text" id="p188">The choice of augmentation technique depends on the task complexity, desired output format, and LLM capabilities.</li> 
  </ul> 
  <div class="readable-text" id="p189"> 
   <h3 class=" readable-text-h3">Generation</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p190">Generation is the final step in which the LLM produces the response based on the augmented prompt.</li> 
   <li class="readable-text" id="p191">LLMs can be categorized based on how they’ve been trained, how they can be accessed, and the number of parameters they have.</li> 
   <li class="readable-text" id="p192">Supervised fine-tuning, or SFT, improves context use and domain optimization, enhances coherence, and enables source attribution; however, it comes with challenges such as cost, risk of overreliance on retrieval, and potential tradeoffs with inherent LLM abilities.</li> 
   <li class="readable-text" id="p193">The choice between open source and proprietary LLMs depends on customization needs, long-term costs, and data sensitivity.</li> 
   <li class="readable-text" id="p194">Larger models come with superior reasoning, language understanding, and broader knowledge, and generate more coherent and contextually accurate responses but come with high computational and resource requirements. Smaller models allow faster inference, lower resource usage, and are easier to deploy on edge devices or resource-constrained environments but do not have the same language understanding abilities as large models.</li> 
   <li class="readable-text" id="p195">Popular LLMs include offerings from OpenAI, Anthropic, Google, and similar, and open source models are available through platforms such as Hugging Face.</li> 
   <li class="readable-text" id="p196">The choice of LLM depends on factors such as performance requirements, resource constraints, deployment environment, and data sensitivity.</li> 
   <li class="readable-text" id="p197">The choice of LLM for RAG systems requires careful consideration, experimentation, and potential adaptation based on performance.</li> 
  </ul>
 </body>
</html>