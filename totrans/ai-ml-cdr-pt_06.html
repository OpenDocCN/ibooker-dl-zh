<html><head></head><body><section data-pdf-bookmark="Chapter 5. Introduction to Natural Language Processing" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch05_introduction_to_natural_language_processing_1748549080743759">
      <h1><span class="label">Chapter 5. </span>Introduction to Natural <span class="keep-together">Language Processing</span></h1>
      <p>Natural language processing (NLP) is a technique<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="about" data-type="indexterm" id="id1055"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="about natural language processing" data-type="indexterm" id="id1056"/> in AI that deals with the understanding of language. It involves programming techniques to create models that can understand language, classify content, and even generate and create new compositions in language. It’s also the underlying foundation to large language models (LLMs) such as GPT, Gemini, and Claude. We’ll explore LLMs in later chapters, but first, we’ll look at more basic NLP over the next few chapters to equip you for what’s to come.</p>
      <p>There are also lots of services that use NLP to create applications such as chatbots, but that’s not in the scope of this book—instead, we’ll be looking at the foundations of NLP and how to model language so that you can train neural networks to understand and classify text. In later chapters, you’ll also learn how to use the predictive elements of an ML model to write some poetry. This isn’t just for fun—it’s also a precursor to learning how to use the transformer-based models that underpin generative AI!</p>
      <p>We’ll start this chapter by looking at how you can decompose language into numbers and how you can then use those numbers in neural networks.</p>
      <section data-pdf-bookmark="Encoding Language into Numbers" data-type="sect1"><div class="sect1" id="ch05_encoding_language_into_numbers_1748549080743987">
        <h1>Encoding Language into Numbers</h1>
        <p>Ultimately, computers deal in numbers,<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="encoding language into numbers" data-type="indexterm" id="ch5enco"/><a contenteditable="false" data-primary="encoding language into numbers" data-type="indexterm" id="ch5enco2"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="encoding language into numbers" data-tertiary="about" data-type="indexterm" id="id1057"/> so to handle language, you need to convert it into numerics in a process called <em>encoding.</em></p>
        <p>You can encode language into numbers in many ways. The most common is to encode by letters, as is done naturally when strings are stored in your program. In memory, however, you don’t store the letter <em>a</em> but an encoding of it—perhaps an ASCII or Unicode value or something else. For example, consider the word <em>listen</em>. You can encode it with ASCII into the numbers 76, 73, 83, 84, 69, and 78. This is good in that you can now use numerics to represent the word. But then consider the word <em>silent</em>, which is an anagram of <em>listen</em>. The same numbers represent that word, albeit in a different order, which might make building a model to understand the text much more difficult.</p>
        <p>A better alternative might be to use numbers to encode entire words instead of the letters within them. In that case, <em>silent</em> could be the number <em>x</em> and <em>listen</em> could be the number <em>y</em>, and they wouldn’t overlap with each other.</p>
        <p>Using this technique, consider a sentence like “I love my dog.” You could encode that with the numbers [1, 2, 3, 4]. If you then wanted to encode “I love my cat,” you could do it with [1, 2, 3, 5]. By now, you’ve probably gotten to the point where you can tell that the sentences have a similar meaning because they’re similar numerically—in other words, [1, 2, 3, 4] looks a lot like [1, 2, 3, 5].</p>
        <p>The numbers representing words<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="encoding language into numbers" data-tertiary="tokens" data-type="indexterm" id="id1058"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="tokens" data-type="indexterm" id="id1059"/><a contenteditable="false" data-primary="tokens" data-type="indexterm" id="id1060"/> are also called <em>tokens</em>, and as a result, this process is called <em>tokenization</em>. You’ll explore how to do that in code next.</p>
        <section data-pdf-bookmark="Getting Started with Tokenization" data-type="sect2"><div class="sect2" id="ch05_getting_started_with_tokenization_1748549080744057">
          <h2>Getting Started with Tokenization</h2>
          <p>The PyTorch ecosystem contains<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="encoding language into numbers" data-tertiary="tokenization" data-type="indexterm" id="ch5iza"/><a contenteditable="false" data-primary="tokenization" data-type="indexterm" id="ch5iza2"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="tokenization" data-type="indexterm" id="ch5iza3"/><a contenteditable="false" data-primary="text datasets for natural language processing" data-secondary="torchtext deprecated" data-type="indexterm" id="id1061"/><a contenteditable="false" data-primary="torchtext deprecated" data-type="indexterm" id="id1062"/> many libraries for tokenization, which takes words and turns them into tokens. A common tokenizer you might see in code samples is <code>torchtext,</code> but this has been deprecated since 2023. So, be careful when using it, especially because PyTorch versions advance but it doesn’t. So, some alternatives are to use a custom tokenizer, a pretrained one from elsewhere, or (surprisingly) those from the Keras ecosystem.</p>
          <section data-pdf-bookmark="Using a custom tokenizer" data-type="sect3"><div class="sect3" id="ch05_using_a_custom_tokenizer_1748549080744110">
            <h3>Using a custom tokenizer</h3>
            <p>To give you a simple example,<a contenteditable="false" data-primary="encoding language into numbers" data-secondary="tokenization" data-tertiary="custom tokenizers" data-type="indexterm" id="id1063"/><a contenteditable="false" data-primary="tokenization" data-secondary="custom tokenizers" data-type="indexterm" id="id1064"/> here’s some code I used to create a custom tokenizer to turn the words of a small corpus (two sentences) into tokens:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
 
<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s1">'Today is a sunny day'</code><code class="p">,</code>
    <code class="s1">'Today is a rainy day'</code>
<code class="p">]</code>
 
<code class="c1"># Tokenization function</code>
<code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">text</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
 
<code class="c1"># Build the vocabulary</code>
<code class="k">def</code> <code class="nf">build_vocab</code><code class="p">(</code><code class="n">sentences</code><code class="p">):</code>
    <code class="n">vocab</code> <code class="o">=</code> <code class="p">{}</code>
    <code class="k">for</code> <code class="n">sentence</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">:</code>
        <code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">tokens</code><code class="p">:</code>
            <code class="k">if</code> <code class="n">token</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">vocab</code><code class="p">:</code>
                <code class="n">vocab</code><code class="p">[</code><code class="n">token</code><code class="p">]</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">vocab</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code> 
    <code class="k">return</code> <code class="n">vocab</code>
 
<code class="c1"># Create the vocabulary index</code>
<code class="n">vocab</code> <code class="o">=</code> <code class="n">build_vocab</code><code class="p">(</code><code class="n">sentences</code><code class="p">)</code>
 
<code class="nb">print</code><code class="p">(</code><code class="s2">"Vocabulary Index:"</code><code class="p">,</code> <code class="n">vocab</code><code class="p">)</code></pre>
            <div data-type="note" epub:type="note"><h6>Note</h6>
              <p>The word <em>corpus</em> is commonly used<a contenteditable="false" data-primary="corpus" data-type="indexterm" id="id1065"/> to denote a set of text items that you will use for training. It’s literally the <em>body</em> of text that you’ll use to train the model and create tokenizers for. </p>
            </div>
            <p>The output of this is as follows:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">Vocabulary</code> <code class="n">Index</code><code class="p">:</code> <code class="p">{</code><code class="s1">'today'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'is'</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'sunny'</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s1">'day'</code><code class="p">:</code> <code class="mi">5</code><code class="p">,</code> <code class="s1">'rainy'</code><code class="p">:</code> <code class="mi">6</code><code class="p">}</code></pre>
            <p>As you can see, the tokenizer did a really simple job of creating a list with my vocabulary, and every time it hit a unique word, it added it to the list. So the first sentence, “Today is a sunny day,” yielded five tokens for the five words: “today,” “is,” “a,” “sunny,” and “day.” The second sentence had <em>most</em> of these words in common, with “rainy” being the exception, so that became the sixth token. </p>
            <p>On the other hand, you can imagine that for a very large corpus, this process would be very slow. </p>
          </div></section>
          <section data-pdf-bookmark="Using a pretrained tokenizer from Hugging Face" data-type="sect3"><div class="sect3" id="ch05_using_a_pre_trained_tokenizer_from_hugging_face_1748549080744162">
            <h3>Using a pretrained tokenizer from Hugging Face</h3>
            <p>With that in mind, I’m going to<a contenteditable="false" data-primary="tokenization" data-secondary="pretrained tokenizers" data-type="indexterm" id="ch5pre"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="tokenization" data-tertiary="pretrained tokenizers" data-type="indexterm" id="ch5pre2"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="transformers library" data-tertiary="pretrained tokenizers" data-type="indexterm" id="ch5pre3"/><a contenteditable="false" data-primary="BertTokenizerFast" data-type="indexterm" id="ch5pre4"/><a contenteditable="false" data-primary="transformers library (Hugging Face)" data-secondary="pretrained tokenizers" data-type="indexterm" id="ch5pre5"/><a contenteditable="false" data-primary="generative AI" data-secondary="transformers library from Hugging Face" data-tertiary="pretrained tokenizers" data-type="indexterm" id="ch5pre6"/><a contenteditable="false" data-primary="tokenization" data-secondary="pretrained tokenizers" data-tertiary="BertTokenizerFast" data-type="indexterm" id="id1066"/> use Hugging Face’s transformers  library and pre-built tokenizers from within it. In this case, because the  transformers library supports many language models and these language models need tokenizers to work with their corpus of text, the tokenizer, which is trained on many millions of words, is freely available for you to use. It has bigger coverage than one you might create, and it’s free and easy to use!</p>
            <p>If you don’t have this library already, you can install it with this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">transformers</code></pre>
            <p>Now, let’s see it in action with a simple example:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">BertTokenizerFast</code>
 
<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s1">'Today is a sunny day'</code><code class="p">,</code>
    <code class="s1">'Today is a rainy day'</code>
<code class="p">]</code>
 
<code class="c1"># Initialize the tokenizer</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">BertTokenizerFast</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s1">'bert-base-uncased'</code><code class="p">)</code>
 
<code class="c1"># Tokenize the sentences and encode them</code>
<code class="n">encoded_inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">sentences</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> 
                           <code class="n">return_tensors</code><code class="o">=</code><code class="s1">'pt'</code><code class="p">)</code>
 
<code class="c1"># To see the tokens for each input (helpful for understanding the output)</code>
<code class="n">tokens</code> <code class="o">=</code> <code class="p">[</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">convert_ids_to_tokens</code><code class="p">(</code><code class="n">ids</code><code class="p">)</code> 
           <code class="k">for</code> <code class="n">ids</code> <code class="ow">in</code> <code class="n">encoded_inputs</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">]]</code>
 
<code class="c1"># To get the word index similar to Keras' tokenizer</code>
<code class="n">word_index</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">get_vocab</code><code class="p">()</code>
 
<code class="nb">print</code><code class="p">(</code><code class="s2">"Tokens:"</code><code class="p">,</code> <code class="n">tokens</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Token IDs:"</code><code class="p">,</code> <code class="n">encoded_inputs</code><code class="p">[</code><code class="s1">'input_ids'</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Word Index:"</code><code class="p">,</code> <code class="nb">dict</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">word_index</code><code class="o">.</code><code class="n">items</code><code class="p">())[:</code><code class="mi">10</code><code class="p">]))</code>  
<code class="c1"># show only the first 10 for brevity</code>
 </pre>
            <p>The output from it looks like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">Tokens</code><code class="p">:</code> <code class="p">[[</code><code class="s1">'[CLS]'</code><code class="p">,</code> <code class="s1">'today'</code><code class="p">,</code> <code class="s1">'is'</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">,</code> <code class="s1">'sunny'</code><code class="p">,</code> <code class="s1">'day'</code><code class="p">,</code> <code class="s1">'[SEP]'</code><code class="p">],</code> 
         <code class="p">[</code><code class="s1">'[CLS]'</code><code class="p">,</code> <code class="s1">'today'</code><code class="p">,</code> <code class="s1">'is'</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">,</code> <code class="s1">'rainy'</code><code class="p">,</code> <code class="s1">'day'</code><code class="p">,</code> <code class="s1">'[SEP]'</code><code class="p">]]</code>
 
<code class="n">Token</code> <code class="n">IDs</code><code class="p">:</code> <code class="n">tensor</code><code class="p">([</code>
        <code class="p">[</code>  <code class="mi">101</code><code class="p">,</code>  <code class="mi">2651</code><code class="p">,</code>  <code class="mi">2003</code><code class="p">,</code>  <code class="mi">1037</code><code class="p">,</code> <code class="mi">11559</code><code class="p">,</code>  <code class="mi">2154</code><code class="p">,</code>   <code class="mi">102</code><code class="p">],</code>
        <code class="p">[</code>  <code class="mi">101</code><code class="p">,</code>  <code class="mi">2651</code><code class="p">,</code>  <code class="mi">2003</code><code class="p">,</code>  <code class="mi">1037</code><code class="p">,</code> <code class="mi">16373</code><code class="p">,</code>  <code class="mi">2154</code><code class="p">,</code>   <code class="mi">102</code><code class="p">]])</code>
 
<code class="n">Word</code> <code class="n">Index</code><code class="p">:</code> <code class="p">{</code><code class="s1">'protestant'</code><code class="p">:</code> <code class="mi">8330</code><code class="p">,</code> <code class="s1">'initial'</code><code class="p">:</code> <code class="mi">3988</code><code class="p">,</code> <code class="s1">'##pt'</code><code class="p">:</code> <code class="mi">13876</code><code class="p">,</code> 
             <code class="s1">'charters'</code><code class="p">:</code> <code class="mi">23010</code><code class="p">,</code> <code class="s1">'243'</code><code class="p">:</code> <code class="mi">22884</code><code class="p">,</code> <code class="s1">'ref'</code><code class="p">:</code> <code class="mi">25416</code><code class="p">,</code> <code class="s1">'##dies'</code><code class="p">:</code> <code class="mi">18389</code><code class="p">,</code> 
             <code class="s1">'##uchi'</code><code class="p">:</code> <code class="mi">15217</code><code class="p">,</code> <code class="s1">'sainte'</code><code class="p">:</code> <code class="mi">16947</code><code class="p">,</code> <code class="s1">'annette'</code><code class="p">:</code> <code class="mi">22521</code><code class="p">}</code></pre>
            <p>Now, let’s break this down. We start by importing the <code>BertTokenizerFast</code> from the transformers library. This can be initialized with a number of pretrained tokenizers, and we choose the <code>'bert-base-uncased' </code>one. You might be wondering what on earth that is! Well, the idea here is that I wanted to take a pretrained tokenizer, and they are usually partnered with the model they were trained on. BERT (which stands for bidirectional encoder representations from transformers) is a model trained by Google on a large corpus, with a vocabulary of 30,000 words. You can find models like this in the Hugging Face model repository, and when you dig down into a model, you’ll often see the transformer’s code to get its tokenizer. For example, see <a href="https://oreil.ly/Ok7L9">this page that I used</a>—and while I’m not using the model, I can still get its tokenizer instead of creating a custom one.</p>
            <p>In this case, we create a <code>tokenizer</code> object and specify the number of words that it can tokenize. This will be the maximum number of tokens to generate from the corpus of words. We also have a very small corpus here, containing only six unique words, so we’ll be well under the maximum of one hundred specified.</p>
            <p>Once I have the tokenizer, I can then just pass the text to it:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="c1"># Tokenize the sentences and encode them</code>
<code class="n">encoded_inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">sentences</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> 
                           <code class="n">return_tensors</code><code class="o">=</code><code class="s1">'pt'</code><code class="p">)</code>
 </pre>
            <p>We’ll explore padding and truncation a little later in this chapter, but for now, you should note the <code>return_tensors='pt'</code> parameter. This is a nice convenience for us PyTorch developers because the return values will be <code>torch.Tensor</code> objects, which are easy for us to handle. </p>
            <p>The BERT model uses a number of overlays on the original tokenization, such as <code>attention_masking</code>, which means it works with <code>IDs</code> for each word instead of raw tokens. This is beyond the scope of this chapter, but where it impacts you right now is if you don’t need all that. If you just want the tokens, you have to extract the tokens in the following way, noting that your sentences were encoded as <code>input_Ids</code> within the BERT tokenizer:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="c1"># To see the tokens for each input (helpful for understanding the output)</code>
<code class="n">tokens</code> <code class="o">=</code> <code class="p">[</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">convert_ids_to_tokens</code><code class="p">(</code><code class="n">ids</code><code class="p">)</code> 
           <code class="k">for</code> <code class="n">ids</code> <code class="ow">in</code> <code class="n">encoded_inputs</code><code class="p">[</code><code class="s2">"input_ids"</code><code class="p">]]</code></pre>
            <p>Once you’ve done that, you can easily print out the following <code>Tokens</code> collection:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">Tokens</code><code class="p">:</code> <code class="p">[[</code><code class="s1">'[CLS]'</code><code class="p">,</code> <code class="s1">'today'</code><code class="p">,</code> <code class="s1">'is'</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">,</code> <code class="s1">'sunny'</code><code class="p">,</code> <code class="s1">'day'</code><code class="p">,</code> <code class="s1">'[SEP]'</code><code class="p">],</code> 
         <code class="p">[</code><code class="s1">'[CLS]'</code><code class="p">,</code> <code class="s1">'today'</code><code class="p">,</code> <code class="s1">'is'</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">,</code> <code class="s1">'rainy'</code><code class="p">,</code> <code class="s1">'day'</code><code class="p">,</code> <code class="s1">'[SEP]'</code><code class="p">]]</code></pre>
            <p>Now, you may be wondering what <code>[CLS]</code> and <code>[SEP]</code> are—and how the BERT model has been trained to expect sentences to begin with <code>[CLS]</code> (for <em>classifier</em>) and end with or be separated by <code>[SEP]</code> (for <em>separator</em>). These two expressions are tokenized to values 101 and 102, respectively, so when you print out the token values for your sentences, you’ll see this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">Token</code> <code class="n">IDs</code><code class="p">:</code> <code class="n">tensor</code><code class="p">([</code>
        <code class="p">[</code>  <code class="mi">101</code><code class="p">,</code>  <code class="mi">2651</code><code class="p">,</code>  <code class="mi">2003</code><code class="p">,</code>  <code class="mi">1037</code><code class="p">,</code> <code class="mi">11559</code><code class="p">,</code>  <code class="mi">2154</code><code class="p">,</code>   <code class="mi">102</code><code class="p">],</code>
        <code class="p">[</code>  <code class="mi">101</code><code class="p">,</code>  <code class="mi">2651</code><code class="p">,</code>  <code class="mi">2003</code><code class="p">,</code>  <code class="mi">1037</code><code class="p">,</code> <code class="mi">16373</code><code class="p">,</code>  <code class="mi">2154</code><code class="p">,</code>   <code class="mi">102</code><code class="p">]])</code></pre>
            <p>From this, you can derive that <em>today</em> is token 2651 in BERT, <em>is</em> is token 2003, etc.</p>
            <p>So, it really depends on you how you want to approach this. For learning with small datasets, the custom tokenizer is probably OK. But once you start getting into larger datasets, you may want to opt for a pretrained tokenizer. In that case, you may have to deal with some overhead—so for the rest of this chapter, I’m going to use custom code to tokenize and preprocess the text without the overhead of something like the BERT tokenizer.</p>
            <p>Either way, once you have the words in your sentences tokenized, the next step is to convert your sentences into lists of numbers, with the number being the value where the word is the key. This process is called <em>sequencing</em>.<a contenteditable="false" data-primary="" data-startref="ch5iza" data-type="indexterm" id="id1067"/><a contenteditable="false" data-primary="" data-startref="ch5iza2" data-type="indexterm" id="id1068"/><a contenteditable="false" data-primary="" data-startref="ch5iza3" data-type="indexterm" id="id1069"/><a contenteditable="false" data-primary="" data-startref="ch5pre" data-type="indexterm" id="id1070"/><a contenteditable="false" data-primary="" data-startref="ch5pre2" data-type="indexterm" id="id1071"/><a contenteditable="false" data-primary="" data-startref="ch5pre3" data-type="indexterm" id="id1072"/><a contenteditable="false" data-primary="" data-startref="ch5pre4" data-type="indexterm" id="id1073"/><a contenteditable="false" data-primary="" data-startref="ch5pre5" data-type="indexterm" id="id1074"/><a contenteditable="false" data-primary="" data-startref="ch5pre6" data-type="indexterm" id="id1075"/></p>
          </div></section>
        </div></section>
        <section data-pdf-bookmark="Turning Sentences into Sequences" data-type="sect2"><div class="sect2" id="ch05_turning_sentences_into_sequences_1748549080744214">
          <h2>Turning Sentences into Sequences</h2>
          <p>Now that you’ve seen how to take<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="encoding language into numbers" data-tertiary="sentences into sequences" data-type="indexterm" id="s2s"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="sentences into sequences" data-type="indexterm" id="s2s2"/><a contenteditable="false" data-primary="sequences from sentences" data-secondary="encoding language into numbers" data-type="indexterm" id="s2s3"/><a contenteditable="false" data-primary="sentences" data-secondary="sequences from" data-type="indexterm" id="s2s4"/> words and tokenize them into numbers, the <span class="keep-together">next step</span> is to encode the sentences into sequences of numbers, which you can do as follows: </p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">text_to_sequence</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">vocab</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">vocab</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">token</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code> <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)]</code>  
<code class="c1"># 0 for unknown words</code>
 </pre>
          <p>Then, you’ll be given the sequences representing the three sentences. Remember that the word index is this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">Vocabulary</code> <code class="n">Index</code><code class="p">:</code> <code class="p">{</code><code class="s1">'today'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'is'</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'sunny'</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s1">'day'</code><code class="p">:</code> <code class="mi">5</code><code class="p">,</code> <code class="s1">'rainy'</code><code class="p">:</code> <code class="mi">6</code><code class="p">}</code></pre>
          <p>And the output will look like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
<code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code></pre>
          <p>You can then substitute the words for the numbers, and you’ll see that the sentences make sense.</p>
          <p>Now, consider what happens if you are training a neural network on a set of data. The typical pattern is that you have a set of data that’s used for training but that you know won’t cover 100% of your needs, but you hope it covers as much as possible. In the case of NLP, you might have many thousands of words in your training data that are used in many different contexts, but you can’t have every possible word in every possible context. So when you show your neural network some new, previously unseen text that contains previously unseen words, what might happen? You guessed it—the network will get confused because it simply has no context for those words, and as a result, any prediction it gives will be negatively affected.</p>
          <section data-pdf-bookmark="Using out-of-vocabulary tokens" data-type="sect3"><div class="sect3" id="ch05_using_out_of_vocabulary_tokens_1748549080744265">
            <h3>Using out-of-vocabulary tokens</h3>
            <p>One tool you can use to handle these<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="encoding language into numbers" data-tertiary="out-of-vocabulary tokens" data-type="indexterm" id="id1076"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="sentences into sequences" data-tertiary="out-of-vocabulary tokens" data-type="indexterm" id="id1077"/><a contenteditable="false" data-primary="sequences from sentences" data-secondary="out-of-vocabulary tokens" data-type="indexterm" id="id1078"/> situations is an <em>out-of-vocabulary</em> (OOV) <em>token</em>, which can help your neural network understand the context of the data containing previously unseen text. For example, given the previous small example corpus, suppose you want to process sentences like these:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">test_data</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s1">'Today is a snowy day'</code><code class="p">,</code>
    <code class="s1">'Will it be rainy tomorrow?'</code>
<code class="p">]</code></pre>
            <p>Remember that you’re not adding this input to the corpus of existing text (which you can think of as your training data) but you’re considering how a pretrained network might view this text. Say you tokenize it with the words that you’ve already used and your existing tokenizer, like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="k">for</code> <code class="n">test_sentence</code> <code class="ow">in</code> <code class="n">test_data</code><code class="p">:</code>
  <code class="n">test_seq</code> <code class="o">=</code> <code class="n">text_to_sequence</code><code class="p">(</code><code class="n">test_sentence</code><code class="p">,</code> <code class="n">vocab</code><code class="p">)</code>
  <code class="nb">print</code><code class="p">(</code><code class="n">test_seq</code><code class="p">)</code></pre>
            <p>Then, your results will look like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
<code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code></pre>
            <p>So, the new sentences, swapping back tokens for words, would be “today is a &lt;UNK&gt; day” and “&lt;UNK&gt; &lt;UNK&gt; &lt;UNK&gt; rainy &lt;UNK&gt;.”</p>
            <p>Here I’m using the tag &lt;UNK&gt; (which stands for <em>unknown</em>) for token 0. If you check out the <code>text_to_sequence</code> code I showed previously, it uses <code>0</code> for words that aren’t in its dictionary. You can, of course, use any value you like.</p>
          </div></section>
          <section data-pdf-bookmark="Understanding padding" data-type="sect3"><div class="sect3" id="ch05_understanding_padding_1748549080744314">
            <h3>Understanding padding</h3>
            <p>When training neural networks, you typically<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="encoding language into numbers" data-tertiary="padding" data-type="indexterm" id="ch5pad"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="sentences into sequences" data-tertiary="padding" data-type="indexterm" id="ch5pad2"/><a contenteditable="false" data-primary="padding" data-type="indexterm" id="ch5pad3"/> need all your data to be in the same shape. Recall from earlier chapters that when you were training with images, you reformatted the images to be the same width and height. With text, you face the same issue—once you’ve tokenized your words and converted your sentences into sequences, they can all be different lengths. But to get them to be the same size and shape, you can use <em>padding</em>.</p>
            <p>All the sentences we have used so far are composed of five words, so you can see that our sequences are five tokens. But what would happen if you had some sentences that were longer. Say a few had 5 words, some had 8 words, and some had 10 words. To have a neural network handle them all, they would need to be of the same length! You could convert everything to 10 words by lengthening the sentences that are shorter, convert everything to 5 words by chopping off bits of the longer ones, or follow some other strategy!</p>
            <p>To explore padding, let’s add another, much longer, sentence to the corpus:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s1">'Today is a sunny day'</code><code class="p">,</code>
    <code class="s1">'Today is a rainy day'</code><code class="p">,</code>
    <code class="s1">'Is it sunny today?'</code><code class="p">,</code>
    <code class="s1">'I really enjoyed walking in the snow today'</code>
<code class="p">]</code></pre>
            <p>When you sequence that, you’ll see that your lists of numbers have different lengths. Also note that if you haven’t retokenized to build the new vocabulary, the latter two sentences will be full of zeros:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
<code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
<code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>
<code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code></pre>
            <p>So, don’t forget to call:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">vocab</code> <code class="o">=</code> <code class="n">build_vocab</code><code class="p">(</code><code class="n">sentences</code><code class="p">)</code></pre>
            <p>And then you’ll have new tokens for the new words in your tokenizer, so the output will look like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
<code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
<code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">]</code>
<code class="p">[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">11</code><code class="p">,</code> <code class="mi">12</code><code class="p">,</code> <code class="mi">13</code><code class="p">,</code> <code class="mi">14</code><code class="p">,</code> <code class="mi">15</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code></pre>
            <p>Remember that when you were training neural networks in earlier chapters, your input layers of the neural network required images to have consistent sizes and shapes. It’s the same with NLP, for the most part. (There’s an exception for something called <em>ragged tensors</em>, but that’s beyond the scope of this chapter.) So, we need a way to make our sentences the same length.</p>
            <p>Here’s a simple padding function:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">pad_sequences</code><code class="p">(</code><code class="n">sequences</code><code class="p">,</code> <code class="n">maxlen</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">seq</code> <code class="o">+</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="p">(</code><code class="n">maxlen</code> <code class="o">-</code> <code class="nb">len</code><code class="p">(</code><code class="n">seq</code><code class="p">))</code> <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code> <code class="o">&lt;</code> <code class="n">maxlen</code> 
            <code class="k">else</code> <code class="n">seq</code><code class="p">[:</code><code class="n">maxlen</code><code class="p">]</code> <code class="k">for</code> <code class="n">seq</code> <code class="ow">in</code> <code class="n">sequences</code><code class="p">]</code></pre>
            <p>This function will reshape every array in the sequence to be the same length as the maximum-length one. So, say we take our sentences and pad them after sequencing them with code like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="k">for</code> <code class="n">sentence</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">:</code>
  <code class="n">seq</code> <code class="o">=</code> <code class="n">text_to_sequence</code><code class="p">(</code><code class="n">sentence</code><code class="p">,</code> <code class="n">vocab</code><code class="p">)</code>
  <code class="n">padded_seq</code> <code class="o">=</code> <code class="n">pad_sequences</code><code class="p">([</code><code class="n">seq</code><code class="p">],</code> <code class="n">maxlen</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>  <code class="c1"># Example maxlen</code>
  <code class="nb">print</code><code class="p">(</code><code class="n">padded_seq</code><code class="p">)</code></pre>
            <p>Then, the output will look like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="p">[[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>
<code class="p">[[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>
<code class="p">[[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>
<code class="p">[[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">11</code><code class="p">,</code> <code class="mi">12</code><code class="p">,</code> <code class="mi">13</code><code class="p">,</code> <code class="mi">14</code><code class="p">,</code> <code class="mi">15</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code></pre>
            <p>Now, each of the sequences has a length of 10 because of the <code>maxlen</code> parameter. It’s a pretty simple implementation that you would likely want to build on if you’re using this in a more serious way. For example, you might want to consider what would happen if you had a sequence that was longer than the maximum length. Right now, it would cut off everything <em>after</em> the maximum, but you might want it to exhibit different behavior! </p>
            <p class="pagebreak-before less_space">Also note that if you’re using off-the-shelf tokenizers like the BERT one we showed you earlier, much of this functionality may already be available to you, so be sure to experiment.<a contenteditable="false" data-primary="" data-startref="ch5enco" data-type="indexterm" id="id1079"/><a contenteditable="false" data-primary="" data-startref="ch5enco2" data-type="indexterm" id="id1080"/><a contenteditable="false" data-primary="" data-startref="ch5pad" data-type="indexterm" id="id1081"/><a contenteditable="false" data-primary="" data-startref="ch5pad2" data-type="indexterm" id="id1082"/><a contenteditable="false" data-primary="" data-startref="ch5pad3" data-type="indexterm" id="id1083"/></p>
          </div></section>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Removing Stopwords and Cleaning Text" data-type="sect1"><div class="sect1" id="ch05_removing_stopwords_and_cleaning_text_1748549080744364">
        <h1>Removing Stopwords and Cleaning Text</h1>
        <p>In the next section you’ll look at <a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="cleaning text" data-tertiary="about" data-type="indexterm" id="id1084"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="cleaning text" data-type="indexterm" id="id1085"/>some real-world datasets, and you’ll find that there’s often text that you <em>don’t</em> want in your dataset. You may also want to filter out so-called <em>stopwords</em>—like “the,” “and,” and “but”—that are too common and don’t add any meaning. You may also encounter a lot of HTML tags in your text, and it would be good to have a clean way to remove them. Other things you may want to filter out include rude words, punctuation, or names. Later, we’ll explore a dataset of tweets that often have somebody’s user ID in them, and we’ll want to filter those out.</p>
        <p>While every task is different based on your corpus of text, there are three main things that you can do to clean up your text programmatically.<a contenteditable="false" data-primary="" data-startref="s2s" data-type="indexterm" id="id1086"/><a contenteditable="false" data-primary="" data-startref="s2s2" data-type="indexterm" id="id1087"/><a contenteditable="false" data-primary="" data-startref="s2s3" data-type="indexterm" id="id1088"/><a contenteditable="false" data-primary="" data-startref="s2s4" data-type="indexterm" id="id1089"/></p>
        <section data-pdf-bookmark="Stripping Out HTML Tags" data-type="sect2"><div class="sect2" id="ch05_stripping_out_html_tags_1748549080744420">
          <h2>Stripping Out HTML Tags</h2>
          <p>The first thing you can do<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="cleaning text" data-tertiary="HTML tags stripped out" data-type="indexterm" id="id1090"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="cleaning text" data-tertiary="HTML tags stripped out" data-type="indexterm" id="id1091"/><a contenteditable="false" data-primary="HTML tags stripped out of text" data-type="indexterm" id="id1092"/> is strip out HTML tags, and fortunately, there’s a library called  BeautifulSoup that makes this straightforward. For example, if your sentences contain HTML tags such as <code>&lt;br&gt;</code>, then you can remove them by using this code:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>
<code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code>
<code class="n">sentence</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">get_text</code><code class="p">()</code></pre>
        </div></section>
        <section data-pdf-bookmark="Stripping Out Stopwords" data-type="sect2"><div class="sect2" id="ch05_stripping_out_stopwords_1748549080744466">
          <h2>Stripping Out Stopwords</h2>
          <p>The second thing to do is<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="cleaning text" data-tertiary="stopwords stripped out" data-type="indexterm" id="id1093"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="cleaning text" data-tertiary="stopwords stripped out" data-type="indexterm" id="id1094"/><a contenteditable="false" data-primary="stopwords stripped out of text" data-type="indexterm" id="id1095"/> strip out stopwords, and a common way to do it is to have a stopwords list and preprocess your sentences by removing instances of stopwords. Here’s an abbreviated example:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">stopwords</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"a"</code><code class="p">,</code> <code class="s2">"about"</code><code class="p">,</code> <code class="s2">"above"</code><code class="p">,</code> <code class="o">...</code> <code class="s2">"yours"</code><code class="p">,</code> <code class="s2">"yourself"</code><code class="p">,</code> <code class="s2">"yourselves"</code><code class="p">]</code></pre>
          <p>You can find a full stopwords<a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="book chapters online" data-type="indexterm" id="id1096"/><a contenteditable="false" data-primary="online resources" data-secondary="book chapters text" data-type="indexterm" id="id1097"/> list in some of the <a href="https://github.com/lmoroney/PyTorch-Book-FIles">online examples for this chapter</a>.</p>
          <p>Then, as you’re iterating through your sentences, you can use code like this to remove the stopwords from your sentences:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">words</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
<code class="n">filtered_sentence</code> <code class="o">=</code> <code class="s2">""</code>
<code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">:</code>
    <code class="k">if</code> <code class="n">word</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">:</code>
        <code class="n">filtered_sentence</code> <code class="o">=</code> <code class="n">filtered_sentence</code> <code class="o">+</code> <code class="n">word</code> <code class="o">+</code> <code class="s2">" "</code>
<code class="n">sentences</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">filtered_sentence</code><code class="p">)</code></pre>
        </div></section>
        <section data-pdf-bookmark="Stripping Out Punctuation" data-type="sect2"><div class="sect2" id="ch05_stripping_out_punctuation_1748549080744510">
          <h2>Stripping Out Punctuation</h2>
          <p>The third thing you can do<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="cleaning text" data-tertiary="punctuation stripped out" data-type="indexterm" id="id1098"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="cleaning text" data-tertiary="punctuation stripped out" data-type="indexterm" id="id1099"/><a contenteditable="false" data-primary="punctuation stripped out of text" data-type="indexterm" id="id1100"/> is strip out punctuation, and you’ll want to do it because punctuation can fool a stopword remover. The one we just showed you looks for words surrounded by spaces, so it won’t spot a stopword that’s immediately followed by a period or a comma.</p>
          <p>Fixing this problem is easy with the translation functions provided by the Python string library. <a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="sentiment" data-tertiary="punctuation affecting detection" data-type="indexterm" id="id1101"/><a contenteditable="false" data-primary="sentiment" data-secondary="punctuation affecting detection" data-type="indexterm" id="id1102"/>But do be careful with this approach, as there are scenarios where it might impact NLP analysis, particularly when detecting sentiment.</p>
          <p>The library also comes with a constant called <code>string.punctuation</code> that contains a <span class="keep-together">list of    common</span>  punctuation marks, so to remove them from a word, you can do the following:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">string</code>
<code class="n">table</code> <code class="o">=</code> <code class="nb">str</code><code class="o">.</code><code class="n">maketrans</code><code class="p">(</code><code class="s1">''</code><code class="p">,</code> <code class="s1">''</code><code class="p">,</code> <code class="n">string</code><code class="o">.</code><code class="n">punctuation</code><code class="p">)</code>
<code class="n">words</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
<code class="n">filtered_sentence</code> <code class="o">=</code> <code class="s2">""</code>
<code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">:</code>
    <code class="n">word</code> <code class="o">=</code> <code class="n">word</code><code class="o">.</code><code class="n">translate</code><code class="p">(</code><code class="n">table</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">word</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">:</code>
        <code class="n">filtered_sentence</code> <code class="o">=</code> <code class="n">filtered_sentence</code> <code class="o">+</code> <code class="n">word</code> <code class="o">+</code> <code class="s2">" "</code>
<code class="n">sentences</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">filtered_sentence</code><code class="p">)</code></pre>
          <p>Here, before filtering for stopwords, the constant removes punctuation from each word in the sentence. So, if splitting a sentence gives you the word <em>it</em>, the word will be converted to <em>it</em> and then stripped out as a stopword. Note, however, that when doing this, you may have to update your stopwords list. It’s also common for these lists to have abbreviated words and contractions like <em>you’ll</em> in them, and the translator will change <em>you’ll</em> to <em>youll</em>. So if you want to have those words filtered out, you’ll need to update your stopwords list to include them.</p>
          <p>Following these three steps will give you a much cleaner set of text to use. But of course, every dataset will have its idiosyncrasies that you’ll need to work with.</p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Working with Real Data Sources" data-type="sect1"><div class="sect1" id="ch05_working_with_real_data_sources_1748549080744564">
        <h1>Working with Real Data Sources</h1>
        <p>Now that you’ve seen the basics<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="real data sources" data-type="indexterm" id="ch5src"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="real data sources" data-type="indexterm" id="ch5src2"/><a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-type="indexterm" id="ch5src3"/> of getting sentences, encoding them with a word index, and sequencing the results, you can take it to the next level by taking some well-known public datasets and using the tools Python provides to get them into a format where you can easily sequence them. We’ll start with a dataset in which a lot of the work has already been done for you: the IMDb dataset. After that, we’ll get a bit more hands-on by processing a JSON-based dataset and a couple of comma-separated values (CSV) datasets with emotion data in them.</p>
        <section data-pdf-bookmark="Getting Text Datasets" data-type="sect2"><div class="sect2" id="ch05_getting_text_datasets_1748549080744619">
          <h2>Getting Text Datasets</h2>
          <p>We explored some datasets in <a data-type="xref" href="ch04.html#ch04_using_data_with_pytorch_1748548966496246">Chapter 4</a>, so if you get stuck<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="real data sources" data-tertiary="text datasets" data-type="indexterm" id="ch5txt"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="real data sources" data-tertiary="text datasets" data-type="indexterm" id="ch5txt2"/><a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="text datasets" data-type="indexterm" id="ch5txt3"/><a contenteditable="false" data-primary="text datasets for natural language processing" data-type="indexterm" id="ch5txt4"/><a contenteditable="false" data-primary="text datasets for natural language processing" data-secondary="IMDb reviews" data-type="indexterm" id="ch5txt5"/><a contenteditable="false" data-primary="datasets" data-secondary="IMDb reviews text dataset" data-type="indexterm" id="ch5txt6"/><a contenteditable="false" data-primary="torchtext deprecated" data-type="indexterm" id="id1103"/><a contenteditable="false" data-primary="text datasets for natural language processing" data-secondary="torchtext deprecated" data-type="indexterm" id="id1104"/> on any of the concepts in this section, you can get a quick review there. However, at the time of writing, accessing <em>text</em>-based datasets is a little unusual. Given that the torchtext library has been deprecated, it’s not clear what will happen with its built-in datasets, so we’ll get hands-on in dealing with raw data in this section.</p>
          <p>We’ll start by exploring the IMDb reviews, which is a dataset of 50,000 labeled movie reviews from the Internet Movie Database (IMDb), each of which is determined to be positive or negative in sentiment.</p>
          <p>This code will download the raw dataset and unzip it into folders where training and test splits are already pre-made for us. These will then be stored in subdirectories, and there are further subdirectories called <code>pos</code> and <code>neg</code> in each that determine the labels of the text files they contain:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">os</code>
<code class="kn">import</code> <code class="nn">urllib.request</code>
<code class="kn">import</code> <code class="nn">tarfile</code>
 
<code class="k">def</code> <code class="nf">download_and_extract</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">destination</code><code class="p">):</code>
    <code class="k">if</code> <code class="ow">not</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">exists</code><code class="p">(</code><code class="n">destination</code><code class="p">):</code>
        <code class="n">os</code><code class="o">.</code><code class="n">makedirs</code><code class="p">(</code><code class="n">destination</code><code class="p">,</code> <code class="n">exist_ok</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="n">file_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">destination</code><code class="p">,</code> <code class="s2">"aclImdb_v1.tar.gz"</code><code class="p">)</code>
 
    <code class="k">if</code> <code class="ow">not</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">exists</code><code class="p">(</code><code class="n">file_path</code><code class="p">):</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"Downloading the dataset..."</code><code class="p">)</code>
        <code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlretrieve</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">file_path</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"Download complete."</code><code class="p">)</code>
 
    <code class="k">if</code> <code class="s2">"aclImdb"</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">os</code><code class="o">.</code><code class="n">listdir</code><code class="p">(</code><code class="n">destination</code><code class="p">):</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"Extracting the dataset..."</code><code class="p">)</code>
        <code class="k">with</code> <code class="n">tarfile</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">file_path</code><code class="p">,</code> <code class="s1">'r:gz'</code><code class="p">)</code> <code class="k">as</code> <code class="n">tar</code><code class="p">:</code>
            <code class="n">tar</code><code class="o">.</code><code class="n">extractall</code><code class="p">(</code><code class="n">path</code><code class="o">=</code><code class="n">destination</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"Extraction complete."</code><code class="p">)</code>
 
<code class="c1"># URL for the dataset</code>
<code class="n">dataset_url</code> <code class="o">=</code> <code class="s2">"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"</code>
<code class="n">download_and_extract</code><code class="p">(</code><code class="n">dataset_url</code><code class="p">,</code> <code class="s2">"./data"</code><code class="p">)</code></pre>
          <p>The file structure will look like the one in <a data-type="xref" href="#ch05_figure_1_1748549080734915">Figure 5-1</a>.</p>
          <figure><div class="figure" id="ch05_figure_1_1748549080734915">
            <img src="assets/aiml_0501.png"/>
            <h6><span class="label">Figure 5-1. </span>Exploring the IMDb dataset structure</h6>
          </div></figure>
          <p>In this figure you can see the <em>test/pos</em> directory and the first couple of files in it. Note that these are text files, so to create a tokenizer and vocabulary, we’re going to have to read files instead of in-memory strings like in the earlier example. </p>
          <p>Let’s take a look at the code for a custom tokenizer for this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">Counter</code>
<code class="kn">import</code> <code class="nn">os</code>
 
<code class="c1"># Simple tokenizer</code>
<code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">text</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
 
<code class="c1"># Build vocabulary</code>
<code class="k">def</code> <code class="nf">build_vocab</code><code class="p">(</code><code class="n">path</code><code class="p">):</code>
    <code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">folder</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"pos"</code><code class="p">,</code> <code class="s2">"neg"</code><code class="p">]:</code>
        <code class="n">folder_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">path</code><code class="p">,</code> <code class="n">folder</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">filename</code> <code class="ow">in</code> <code class="n">os</code><code class="o">.</code><code class="n">listdir</code><code class="p">(</code><code class="n">folder_path</code><code class="p">):</code>
            <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">folder_path</code><code class="p">,</code> <code class="n">filename</code><code class="p">),</code> <code class="s1">'r'</code><code class="p">,</code> 
                                   <code class="n">encoding</code><code class="o">=</code><code class="s1">'utf-8'</code><code class="p">)</code> <code class="k">as</code> <code class="n">file</code><code class="p">:</code>
                <code class="n">counter</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">tokenize</code><code class="p">(</code><code class="n">file</code><code class="o">.</code><code class="n">read</code><code class="p">()))</code>
    <code class="k">return</code> <code class="p">{</code><code class="n">word</code><code class="p">:</code> <code class="n">i</code><code class="o">+</code><code class="mi">1</code> <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">word</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">counter</code><code class="p">)}</code> <code class="c1"># Starting index from 1</code>
 
<code class="n">vocab</code> <code class="o">=</code> <code class="n">build_vocab</code><code class="p">(</code><code class="s2">"./data/aclImdb/train/"</code><code class="p">)</code></pre>
          <p>It’s pretty straightforward code that just reads through each file and adds new words it discovers to the vocabulary, giving each word a new token value. Generally, you’ll only want to do this for the training data, with the understanding that there will be words in the test data that aren’t in the training data and that they would be tokenized with an OOV or unknown token.</p>
          <p>The output should look like this (truncated):</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">{</code><code class="s1">'a'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'year'</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'or'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'so'</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s1">'ago,'</code><code class="p">:</code> <code class="mi">5</code><code class="p">,</code> <code class="s1">'i'</code><code class="p">:</code> <code class="mi">6</code><code class="p">,</code> <code class="s1">'was'</code><code class="p">:</code> <code class="mi">7</code><code class="err">…</code></pre>
          <p class="pagebreak-before less_space">This is a naive tokenizer in that the first word it sees gets the first token, the second gets the second, etc. For performance reasons, it’s often better for the more frequent words in the corpus to get the earlier tokens and the less frequent ones to get the later tokens. We’ll explore that in a moment.</p>
          <p>You can then do sequencing and padding as you did earlier:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">text_to_sequence</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">vocab</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">vocab</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">token</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code> <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)]</code>  <code class="c1"># 0 for unknown</code>
 
<code class="k">def</code> <code class="nf">pad_sequences</code><code class="p">(</code><code class="n">sequences</code><code class="p">,</code> <code class="n">maxlen</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">seq</code> <code class="o">+</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="p">(</code><code class="n">maxlen</code> <code class="o">-</code> <code class="nb">len</code><code class="p">(</code><code class="n">seq</code><code class="p">))</code> 
           <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code> <code class="o">&lt;</code> <code class="n">maxlen</code> <code class="k">else</code> <code class="n">seq</code><code class="p">[:</code><code class="n">maxlen</code><code class="p">]</code> <code class="k">for</code> <code class="n">seq</code> <code class="ow">in</code> <code class="n">sequences</code><code class="p">]</code>
 
<code class="c1"># Example use</code>
<code class="n">text</code> <code class="o">=</code> <code class="s2">"This is an example."</code>
<code class="n">seq</code> <code class="o">=</code> <code class="n">text_to_sequence</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">vocab</code><code class="p">)</code>
<code class="n">padded_seq</code> <code class="o">=</code> <code class="n">pad_sequences</code><code class="p">([</code><code class="n">seq</code><code class="p">],</code> <code class="n">maxlen</code><code class="o">=</code><code class="mi">256</code><code class="p">)</code>  <code class="c1"># Example maxlen</code>
<code class="nb">print</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code></pre>
          <p>So, for example, our sentence <code>This is an example</code> will output as <code>[30, 56, 144, 16040]</code> because those are the tokens assigned to those words. The padded sequence would have a tensor of 256 values, with these tokens as the first 4 and the next 252 being zeros!</p>
          <p>Now, let’s update the tokenizer to do the words in order of frequency. This update changes the tokenizer so that we load all of the files into memory and count the instance of each word to get a frequency table: </p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Build vocabulary</code>
<code class="k">def</code> <code class="nf">build_vocab</code><code class="p">(</code><code class="n">path</code><code class="p">):</code>
    <code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">folder</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"pos"</code><code class="p">,</code> <code class="s2">"neg"</code><code class="p">]:</code>
        <code class="n">folder_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">path</code><code class="p">,</code> <code class="n">folder</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">filename</code> <code class="ow">in</code> <code class="n">os</code><code class="o">.</code><code class="n">listdir</code><code class="p">(</code><code class="n">folder_path</code><code class="p">):</code>
            <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">folder_path</code><code class="p">,</code> <code class="n">filename</code><code class="p">),</code> <code class="s1">'r'</code><code class="p">,</code> 
                                   <code class="n">encoding</code><code class="o">=</code><code class="s1">'utf-8'</code><code class="p">)</code> <code class="k">as</code> <code class="n">file</code><code class="p">:</code>
                <code class="n">counter</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">tokenize</code><code class="p">(</code><code class="n">file</code><code class="o">.</code><code class="n">read</code><code class="p">()))</code>
 
    <code class="c1"># Sort words by frequency in descending order</code>
    <code class="n">sorted_words</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="n">counter</code><code class="o">.</code><code class="n">items</code><code class="p">(),</code> <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
 
    <code class="c1"># Create vocabulary with indices starting from 1</code>
    <code class="n">vocab</code> <code class="o">=</code> <code class="p">{</code><code class="n">word</code><code class="p">:</code> <code class="n">idx</code> <code class="o">+</code> <code class="mi">1</code> <code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">_</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">sorted_words</code><code class="p">)}</code>
    <code class="n">vocab</code><code class="p">[</code><code class="s1">'&lt;pad&gt;'</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>  <code class="c1"># Add padding token with index 0</code>
    <code class="k">return</code> <code class="n">vocab</code></pre>
          <p class="pagebreak-before less_space">We can then output the vocabulary as this frequency table. The vocabulary is too large to show the entire index, but here are the top 20 words. Note that the tokenizer lists them in order of frequency in the dataset, so common words like <em>the</em>, <em>and</em>, and <em>a</em> are indexed:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="p">{</code><code class="s1">'the'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'and'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'of'</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s1">'to'</code><code class="p">:</code> <code class="mi">5</code><code class="p">,</code> <code class="s1">'is'</code><code class="p">:</code> <code class="mi">6</code><code class="p">,</code> <code class="s1">'in'</code><code class="p">:</code> <code class="mi">7</code><code class="p">,</code> <code class="s1">'i'</code><code class="p">:</code> <code class="mi">8</code><code class="p">,</code> 
<code class="s1">'this'</code><code class="p">:</code> <code class="mi">9</code><code class="p">,</code> <code class="s1">'that'</code><code class="p">:</code> <code class="mi">10</code><code class="p">,</code> <code class="s1">'it'</code><code class="p">:</code> <code class="mi">11</code><code class="p">,</code> <code class="s1">'/&gt;&lt;br'</code><code class="p">:</code> <code class="mi">12</code><code class="p">,</code> <code class="s1">'was'</code><code class="p">:</code> <code class="mi">13</code><code class="p">,</code> <code class="s1">'as'</code><code class="p">:</code> <code class="mi">14</code><code class="p">,</code>
<code class="s1">'for'</code><code class="p">:</code> <code class="mi">15</code><code class="p">,</code> <code class="s1">'with'</code><code class="p">:</code> <code class="mi">16</code><code class="p">,</code> <code class="s1">'but'</code><code class="p">:</code> <code class="mi">17</code><code class="p">,</code> <code class="s1">'on'</code><code class="p">:</code> <code class="mi">18</code><code class="p">,</code> <code class="s1">'movie'</code><code class="p">:</code> <code class="mi">19</code><code class="p">,</code> <code class="s1">'his'</code><code class="p">:</code> <code class="mi">20</code><code class="p">,</code></pre>
          <p>These are stopwords, as described in the previous section. Having these present can impact your training accuracy because they’re the most common words and they’re nondistinct (i.e., they’re likely present in both positive and negative reviews), so they add noise to our training.</p>
          <p>Also note that <em>br</em> is included in this list because it’s commonly used in this corpus as the <code>&lt;br&gt;</code> HTML tag.</p>
          <p>You can also update the code to use <code>BeautifulSoup</code> to remove the HTML tags, and you can remove stopwords from the given list as follows. To do this, you can update the tokenizer to remove the HTML tags by using <code>BeautifulSoup</code>:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Simple tokenizer</code>
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>
 
<code class="c1"># Note that the list of stopwords is defined in the source code. </code>
<code class="c1"># It’s an array of words. You can define your own or just get the one from </code>
<code class="c1"># the book’s github.</code>
 
<code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="s2">"html.parser"</code><code class="p">)</code>
    <code class="n">cleaned_text</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">get_text</code><code class="p">()</code>  <code class="c1"># Extract text from HTML</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">word</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">cleaned_text</code><code class="o">.</code><code class="n">split</code><code class="p">()</code> <code class="k">if</code> <code class="n">word</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code> 
            <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">]</code></pre>
          <p>Now, when you print out your word index, you’ll see this:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="p">{</code><code class="s1">'movie'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'not'</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'film'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'one'</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s1">'like'</code><code class="p">:</code> <code class="mi">5</code><code class="p">,</code> <code class="s1">'just'</code><code class="p">:</code> <code class="mi">6</code><code class="p">,</code> <code class="s2">"it's"</code><code class="p">:</code> <code class="mi">7</code><code class="p">,</code> 
 <code class="s1">'even'</code><code class="p">:</code> <code class="mi">8</code><code class="p">,</code> <code class="s1">'good'</code><code class="p">:</code> <code class="mi">9</code><code class="p">,</code> <code class="s1">'no'</code><code class="p">:</code> <code class="mi">10</code><code class="p">,</code> <code class="s1">'really'</code><code class="p">:</code> <code class="mi">11</code><code class="p">,</code> <code class="s1">'can'</code><code class="p">:</code> <code class="mi">12</code><code class="p">,</code> <code class="s1">'see'</code><code class="p">:</code> <code class="mi">13</code><code class="p">,</code> <code class="s1">'-'</code><code class="p">:</code> <code class="mi">14</code><code class="p">,</code> 
 <code class="s1">'get'</code><code class="p">:</code> <code class="mi">15</code><code class="p">,</code> <code class="s1">'will'</code><code class="p">:</code> <code class="mi">16</code><code class="p">,</code> <code class="s1">'much'</code><code class="p">:</code> <code class="mi">17</code><code class="p">,</code> <code class="s1">'story'</code><code class="p">:</code> <code class="mi">18</code><code class="p">,</code> <code class="s1">'also'</code><code class="p">:</code> <code class="mi">19</code><code class="p">,</code> <code class="s1">'first'</code><code class="p">:</code> <code class="mi">20</code></pre>
          <p>You can see that this is much cleaner than before. There’s always room to improve, however, and one thing I noted when looking at the full index was that some of the less common words toward the end were nonsensical. Often, reviewers would combine words, for example with a dash (as in <em>annoying-conclusion</em>) or a slash (as in <em>him/her</em>), and the stripping of punctuation would incorrectly turn these combined words into a single word. Or, as you can see in the preceding code, the dash (<em>-</em>) character was common enough to be tokenized. You can strip that out by adding it as a <span class="keep-together">stopword.</span></p>
          <p>Now that you have a tokenizer for the corpus, you can encode your sentences. For example, the simple sentences we were looking at earlier in the chapter will come out like this:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s1">'Today is a sunny day'</code><code class="p">,</code>
    <code class="s1">'Today is a rainy day'</code><code class="p">,</code>
    <code class="s1">'Is it sunny today?'</code>
<code class="p">]</code>

<code class="p">[[</code><code class="mi">1094</code><code class="p">,</code> <code class="mi">6112</code><code class="p">,</code> <code class="mi">246</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>
<code class="p">[[</code><code class="mi">1094</code><code class="p">,</code> <code class="mi">6730</code><code class="p">,</code> <code class="mi">246</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>
<code class="p">[[</code><code class="mi">6112</code><code class="p">,</code> <code class="mi">25065</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>
 </pre>
          <p>If you decode these, you’ll see that the stopwords are dropped and you get the sentences encoded as <code>today sunny day</code>, <code>today rainy day</code>, and <code>sunny today</code>.</p>
          <p>If you want to do this in code, you can create a new <code>dict</code> with the reversed keys and values (i.e., for a key/value pair in the word index, you can make the value the key and the key the value) and do the lookup from that. Here’s the code:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">reverse_word_index</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code>
    <code class="p">[(</code><code class="n">value</code><code class="p">,</code> <code class="n">key</code><code class="p">)</code> <code class="k">for</code> <code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="n">value</code><code class="p">)</code> <code class="ow">in</code> <code class="n">vocab</code><code class="o">.</code><code class="n">items</code><code class="p">()])</code>
 
<code class="n">decoded_review</code> <code class="o">=</code> <code class="s1">' '</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">reverse_word_index</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">i</code><code class="p">,</code> <code class="s1">'?'</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">seq</code><code class="p">])</code>
 
<code class="nb">print</code><code class="p">(</code><code class="n">decoded_review</code><code class="p">)</code>
 </pre>
          <p>This will give the following result:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">today</code> <code class="n">sunny</code> <code class="n">day</code></pre>
          <p>A common way to store labeled text data is in the comma-separated value (CSV) format. We’ll discuss that next.<a contenteditable="false" data-primary="" data-startref="ch5txt" data-type="indexterm" id="id1105"/><a contenteditable="false" data-primary="" data-startref="ch5txt2" data-type="indexterm" id="id1106"/><a contenteditable="false" data-primary="" data-startref="ch5txt3" data-type="indexterm" id="id1107"/><a contenteditable="false" data-primary="" data-startref="ch5txt4" data-type="indexterm" id="id1108"/><a contenteditable="false" data-primary="" data-startref="ch5txt5" data-type="indexterm" id="id1109"/><a contenteditable="false" data-primary="" data-startref="ch5txt6" data-type="indexterm" id="id1110"/></p>
        </div></section>
        <section data-pdf-bookmark="Getting Text from CSV Files" data-type="sect2"><div class="sect2" id="ch05_getting_text_from_csv_files_1748549080744667">
          <h2>Getting Text from CSV Files</h2>
          <p>NLP data is also commonly available<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="real data sources" data-tertiary="text from CSV files" data-type="indexterm" id="ch5csv"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="real data sources" data-tertiary="text from CSV files" data-type="indexterm" id="ch5csv2"/><a contenteditable="false" data-primary="text datasets for natural language processing" data-secondary="text from CSV files" data-type="indexterm" id="ch5csv3"/><a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="text from CSV files" data-type="indexterm" id="ch5csv4"/><a contenteditable="false" data-primary="CSV files as text data sources" data-type="indexterm" id="ch5csv5"/><a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="Sentiment Analysis in Text" data-type="indexterm" id="id1111"/><a contenteditable="false" data-primary="Sentiment Analysis in Text dataset" data-type="indexterm" id="id1112"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="real data sources" data-tertiary="Sentiment Analysis in Text dataset" data-type="indexterm" id="id1113"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="real data sources" data-tertiary="Sentiment Analysis in Text dataset" data-type="indexterm" id="id1114"/><a contenteditable="false" data-primary="sentiment" data-secondary="Sentiment Analysis in Text dataset" data-type="indexterm" id="id1115"/> in CSV file format. Over the next couple of chapters, you’ll use a CSV of data that I adapted from the open source <a href="https://oreil.ly/7ZKEU">Sentiment Analysis in Text dataset</a>. The creator of this dataset sourced it from Twitter (now called X). You will use two different datasets, one where the emotions have been reduced to “positive” or “negative” for binary classification and one where the full range of emotion labels is used. Both datasets use the same structure, so I’ll just show the binary version here.</p>
          <p>While the name <em>CSV</em> seems to<a contenteditable="false" data-primary="CSV files as text data sources" data-secondary="about CSV files" data-type="indexterm" id="id1116"/> suggest a standard file format in which values are comma separated, there’s actually a wide diversity of formats that can be considered CSV, and there’s very little adherence to any particular standard. To solve this, the Python csv library makes handling CSV files straightforward. In this case, the data is stored with two values per line. The first value is a number (0 or 1) denoting whether the sentiment is negative or positive, and the second value is a string containing the text.</p>
          <p>The following code snippet will read the CSV and do preprocessing that’s similar to what we saw in the previous section. For the full code, please check the repo for this book. <a contenteditable="false" data-primary="encoding language into numbers" data-secondary="cleaning text" data-tertiary="HTML tags stripped via BeautifulSoup" data-type="indexterm" id="id1117"/><a contenteditable="false" data-primary="HTML tags stripped out of text" data-secondary="BeautifulSoup for" data-type="indexterm" id="id1118"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="cleaning text" data-tertiary="HTML tags stripped via BeautifulSoup" data-type="indexterm" id="id1119"/><a contenteditable="false" data-primary="BeautifulSoup for stripping HTML tags from text" data-type="indexterm" id="id1120"/>The code adds spaces around the punctuation in compound words, uses <span class="keep-together"><code>BeautifulSoup</code></span> to strip HTML content, and then removes all punctuation characters:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">csv</code>
<code class="n">sentences</code><code class="o">=</code><code class="p">[]</code>
<code class="n">labels</code><code class="o">=</code><code class="p">[]</code>
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s1">'/tmp/binary-emotion.csv'</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'UTF-8'</code><code class="p">)</code> <code class="k">as</code> <code class="n">csvfile</code><code class="p">:</code>
    <code class="n">reader</code> <code class="o">=</code> <code class="n">csv</code><code class="o">.</code><code class="n">reader</code><code class="p">(</code><code class="n">csvfile</code><code class="p">,</code> <code class="n">delimiter</code><code class="o">=</code><code class="s2">","</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">reader</code><code class="p">:</code>
        <code class="n">labels</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">row</code><code class="p">[</code><code class="mi">0</code><code class="p">]))</code>
        <code class="n">sentence</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code>
        <code class="n">sentence</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">","</code><code class="p">,</code> <code class="s2">" , "</code><code class="p">)</code>
        <code class="n">sentence</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"."</code><code class="p">,</code> <code class="s2">" . "</code><code class="p">)</code>
        <code class="n">sentence</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"-"</code><code class="p">,</code> <code class="s2">" - "</code><code class="p">)</code>
        <code class="n">sentence</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"/"</code><code class="p">,</code> <code class="s2">" / "</code><code class="p">)</code>
        <code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code>
        <code class="n">sentence</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">get_text</code><code class="p">()</code>
        <code class="n">words</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
        <code class="n">filtered_sentence</code> <code class="o">=</code> <code class="s2">""</code>
        <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">:</code>
            <code class="n">word</code> <code class="o">=</code> <code class="n">word</code><code class="o">.</code><code class="n">translate</code><code class="p">(</code><code class="n">table</code><code class="p">)</code>
            <code class="k">if</code> <code class="n">word</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">:</code>
                <code class="n">filtered_sentence</code> <code class="o">=</code> <code class="n">filtered_sentence</code> <code class="o">+</code> <code class="n">word</code> <code class="o">+</code> <code class="s2">" "</code>
        <code class="n">sentences</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">filtered_sentence</code><code class="p">)</code></pre>
          <p>This will give you a list of 35,327 sentences.</p>
          <p>Note that this code is specific to this data. It’s intended to help you understand the types of tasks you may have to take on in order to make stuff work, and it’s not intended to be an exhaustive list of things that you’ll have to do for every task—so your mileage may vary. </p>
          <section data-pdf-bookmark="Creating training and test subsets" data-type="sect3"><div class="sect3" id="ch05_creating_training_and_test_subsets_1748549080744712">
            <h3>Creating training and test subsets</h3>
            <p>Now that the text corpus has been read into a list of sentences, you’ll need to split it into training and test subsets for training a model. For example, if you want to use 28,000 sentences for training with the rest held back for testing, you can use code like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">training_size</code> <code class="o">=</code> <code class="mi">28000</code>
 
<code class="n">training_sentences</code> <code class="o">=</code> <code class="n">sentences</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">training_size</code><code class="p">]</code>
<code class="n">testing_sentences</code> <code class="o">=</code> <code class="n">sentences</code><code class="p">[</code><code class="n">training_size</code><code class="p">:]</code>
<code class="n">training_labels</code> <code class="o">=</code> <code class="n">labels</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">training_size</code><code class="p">]</code>
<code class="n">testing_labels</code> <code class="o">=</code> <code class="n">labels</code><code class="p">[</code><code class="n">training_size</code><code class="p">:]</code></pre>
            <p>Now that you have a training set, you can edit the tokenizer and vocabulary builder to create the word index from this corpus. As the corpus is an in-memory array of strings (<code>training_sentences</code>), the process is a lot simpler:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">Counter</code>
 
<code class="c1"># Assuming the tokenize function is defined elsewhere</code>
<code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="c1"># Tokenization logic, removing HTML and stopwords as discussed earlier</code>
    <code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="s2">"html.parser"</code><code class="p">)</code>
    <code class="n">cleaned_text</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">get_text</code><code class="p">()</code>
    <code class="n">tokens</code> <code class="o">=</code> <code class="n">cleaned_text</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
    <code class="n">filtered_tokens</code> <code class="o">=</code> <code class="p">[</code><code class="n">token</code> <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">tokens</code> <code class="k">if</code> <code class="n">token</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">filtered_tokens</code>
 
<code class="k">def</code> <code class="nf">build_vocab</code><code class="p">(</code><code class="n">sentences</code><code class="p">):</code>
    <code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">text</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">:</code>
        <code class="n">counter</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">))</code>
 
    <code class="c1"># Sort words by frequency in descending order</code>
    <code class="n">sorted_words</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="n">counter</code><code class="o">.</code><code class="n">items</code><code class="p">(),</code> <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
 
    <code class="c1"># Create vocabulary with indices starting from 1</code>
    <code class="n">vocab</code> <code class="o">=</code> <code class="p">{</code><code class="n">word</code><code class="p">:</code> <code class="n">idx</code> <code class="o">+</code> <code class="mi">1</code> <code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">_</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">sorted_words</code><code class="p">)}</code>
    <code class="n">vocab</code><code class="p">[</code><code class="s1">'&lt;pad&gt;'</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>  <code class="c1"># Add padding token with index 0</code>
    <code class="k">return</code> <code class="n">vocab</code>
 
 
<code class="n">vocab</code> <code class="o">=</code> <code class="n">build_vocab</code><code class="p">(</code><code class="n">training_sentences</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">vocab</code><code class="p">)</code></pre>
            <p>You can use the same helper functions to turn the text into a sequence and then padding, like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="n">testing_sentences</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="n">seq</code> <code class="o">=</code> <code class="n">text_to_sequence</code><code class="p">(</code><code class="n">testing_sentences</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">vocab</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code></pre>
            <p>The results will be as follows:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">made</code> <code class="n">many</code> <code class="n">new</code> <code class="n">friends</code> <code class="n">twitter</code> <code class="n">around</code> <code class="n">usa</code> <code class="n">another</code> <code class="n">bike</code> <code class="n">across</code> <code class="n">usa</code> <code class="n">trip</code> <code class="n">amazing</code> 
 <code class="n">see</code> <code class="n">people</code>
<code class="p">[</code><code class="mi">146</code><code class="p">,</code> <code class="mi">259</code><code class="p">,</code> <code class="mi">30</code><code class="p">,</code> <code class="mi">110</code><code class="p">,</code> <code class="mi">53</code><code class="p">,</code> <code class="mi">198</code><code class="p">,</code> <code class="mi">2161</code><code class="p">,</code> <code class="mi">111</code><code class="p">,</code> <code class="mi">752</code><code class="p">,</code> <code class="mi">970</code><code class="p">,</code> <code class="mi">2161</code><code class="p">,</code> <code class="mi">407</code><code class="p">,</code> <code class="mi">217</code><code class="p">,</code> <code class="mi">26</code><code class="p">,</code> <code class="mi">73</code><code class="p">]</code></pre>
            <p>Another common format for structured data, particularly in response to web calls, is JavaScript Object Notation (JSON). We’ll explore how to read JSON data next.<a contenteditable="false" data-primary="" data-startref="ch5csv" data-type="indexterm" id="id1121"/><a contenteditable="false" data-primary="" data-startref="ch5csv2" data-type="indexterm" id="id1122"/><a contenteditable="false" data-primary="" data-startref="ch5csv3" data-type="indexterm" id="id1123"/><a contenteditable="false" data-primary="" data-startref="ch5csv4" data-type="indexterm" id="id1124"/><a contenteditable="false" data-primary="" data-startref="ch5csv5" data-type="indexterm" id="id1125"/></p>
          </div></section>
        </div></section>
        <section data-pdf-bookmark="Getting Text from JSON Files" data-type="sect2"><div class="sect2" id="ch05_getting_text_from_json_files_1748549080744760">
          <h2>Getting Text from JSON Files</h2>
          <p>JSON is an open standard file format<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="real data sources" data-tertiary="JSON files" data-type="indexterm" id="ch5json"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="real data sources" data-tertiary="JSON files" data-type="indexterm" id="ch5json2"/><a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="JSON files" data-type="indexterm" id="ch5json3"/><a contenteditable="false" data-primary="JSON" data-secondary="data sources for natural language processing" data-type="indexterm" id="ch5json4"/> that’s used often for data interchange, particularly with web applications. It’s human readable and designed to use name/value pairs, and as such, it’s particularly well suited for labeled text. A quick search of Kaggle datasets for JSON yields over 2,500 results. <a contenteditable="false" data-primary="Stanford Question Answering Dataset (SQuAD)" data-type="indexterm" id="id1126"/><a contenteditable="false" data-primary="SQuAD (Stanford Question Answering Dataset)" data-type="indexterm" id="id1127"/><a contenteditable="false" data-primary="JSON" data-secondary="data sources for natural language processing" data-tertiary="Stanford Question Answering Dataset" data-type="indexterm" id="id1128"/>For example, popular datasets such as the Stanford Question Answering Dataset (SQuAD) are stored in JSON.</p>
          <p>JSON has very simple syntax<a contenteditable="false" data-primary="JSON" data-secondary="syntax basics" data-type="indexterm" id="id1129"/> in which objects are contained within braces as name/value pairs, each of which is separated by a comma. For example, a JSON object representing my name would be as follows:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">{</code><code class="s2">"firstName"</code> <code class="p">:</code> <code class="s2">"Laurence"</code><code class="p">,</code>
 <code class="s2">"lastName"</code> <code class="p">:</code> <code class="s2">"Moroney"</code><code class="p">}</code></pre>
          <p>JSON also supports arrays,<a contenteditable="false" data-primary="JSON" data-secondary="arrays" data-type="indexterm" id="id1130"/> which are a lot like Python lists and are denoted by the square bracket syntax. Here’s an example:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">[</code>
 <code class="p">{</code><code class="s2">"firstName"</code> <code class="p">:</code> <code class="s2">"Laurence"</code><code class="p">,</code>
 <code class="s2">"lastName"</code> <code class="p">:</code> <code class="s2">"Moroney"</code><code class="p">},</code>
 <code class="p">{</code><code class="s2">"firstName"</code> <code class="p">:</code> <code class="s2">"Sharon"</code><code class="p">,</code>
 <code class="s2">"lastName"</code> <code class="p">:</code> <code class="s2">"Agathon"</code><code class="p">}</code>
<code class="p">]</code></pre>
          <p>Objects can also contain arrays,<a contenteditable="false" data-primary="JSON" data-secondary="arrays" data-tertiary="objects containing arrays" data-type="indexterm" id="id1131"/> so this is perfectly valid JSON:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">[</code>
 <code class="p">{</code><code class="s2">"firstName"</code> <code class="p">:</code> <code class="s2">"Laurence"</code><code class="p">,</code>
 <code class="s2">"lastName"</code> <code class="p">:</code> <code class="s2">"Moroney"</code><code class="p">,</code>
 <code class="s2">"emails"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"lmoroney@gmail.com"</code><code class="p">,</code> <code class="s2">"lmoroney@galactica.net"</code><code class="p">]</code>
 <code class="p">},</code>
 <code class="p">{</code><code class="s2">"firstName"</code> <code class="p">:</code> <code class="s2">"Sharon"</code><code class="p">,</code>
 <code class="s2">"lastName"</code> <code class="p">:</code> <code class="s2">"Agathon"</code><code class="p">,</code>
 <code class="s2">"emails"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"sharon@galactica.net"</code><code class="p">,</code> <code class="s2">"boomer@cylon.org"</code><code class="p">]</code>
 <code class="p">}</code>
<code class="p">]</code></pre>
          <p>A smaller dataset that’s stored<a contenteditable="false" data-primary="JSON" data-secondary="data sources for natural language processing" data-tertiary="News Headlines Dataset for Sarcasm Detection" data-type="indexterm" id="id1132"/><a contenteditable="false" data-primary="News Headlines Dataset for Sarcasm Detection dataset (Misra)" data-type="indexterm" id="id1133"/><a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="News Headlines Dataset for Sarcasm Detection" data-type="indexterm" id="id1134"/><a contenteditable="false" data-primary="Misra, Rishabh" data-type="indexterm" id="id1135"/><a contenteditable="false" data-primary="online resources" data-secondary="News Headlines Dataset for Sarcasm Detection dataset" data-type="indexterm" id="id1136"/><a contenteditable="false" data-primary="sarcasm detector" data-secondary="sarcasm dataset" data-type="indexterm" id="id1137"/> in JSON and a lot of fun to work with is the “News Headlines Dataset for Sarcasm Detection” by <a href="https://oreil.ly/wZ3oD">Rishabh Misra</a>, which is available on <a href="https://oreil.ly/_AScB">Kaggle</a>. This dataset collects news headlines from two sources: <em>The Onion</em> for funny or sarcastic ones and the <em>HuffPost</em> for normal headlines.</p>
          <p>The file structure in the sarcasm dataset is very simple:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">{</code><code class="s2">"is_sarcastic"</code><code class="p">:</code> <code class="mi">1</code> <code class="ow">or</code> <code class="mi">0</code><code class="p">,</code> 
 <code class="s2">"headline"</code><code class="p">:</code> <code class="n">String</code> <code class="n">containing</code> <code class="n">headline</code><code class="p">,</code> 
 <code class="s2">"article_link"</code><code class="p">:</code> <code class="n">String</code> <code class="n">Containing</code> <code class="n">link</code><code class="p">}</code></pre>
          <p>The dataset consists of about 26,000 items, one per line. To make it more readable in Python, I’ve created a version that encloses these items in an array so the dataset can be read as a single list, which is used in the source code for this chapter.</p>
          <section data-pdf-bookmark="Reading JSON files" data-type="sect3"><div class="sect3" id="ch05_reading_json_files_1748549080744807">
            <h3>Reading JSON files</h3>
            <p>Python’s json library makes<a contenteditable="false" data-primary="JSON" data-secondary="reading JSON files" data-type="indexterm" id="id1138"/><a contenteditable="false" data-primary="Python" data-secondary="json library" data-type="indexterm" id="id1139"/> reading JSON files simple. Given that JSON uses name/value pairs, you can index the content based on the name. So, for example, for the sarcasm dataset, you can create a file handle to the JSON file, open it with the <code>json</code> library, and have an iterable go through, read each field line by line, and get the data item by using the name of the field.</p>
            <p>Here’s the code:<a contenteditable="false" data-primary="JSON" data-secondary="reading JSON files" data-tertiary="code for text cleanup" data-type="indexterm" id="id1140"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="cleaning text" data-tertiary="code for JSON file text cleanup" data-type="indexterm" id="id1141"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="cleaning text" data-tertiary="code for JSON file text cleanup" data-type="indexterm" id="id1142"/></p>
            <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">json</code>
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"/tmp/sarcasm.json"</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">datastore</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">datastore</code><code class="p">:</code>
        <code class="n">sentence</code> <code class="o">=</code> <code class="n">item</code><code class="p">[</code><code class="s1">'headline'</code><code class="p">]</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code>
        <code class="n">label</code><code class="o">=</code> <code class="n">item</code><code class="p">[</code><code class="s1">'is_sarcastic'</code><code class="p">]</code>
        <code class="n">link</code> <code class="o">=</code> <code class="n">item</code><code class="p">[</code><code class="s1">'article_link'</code><code class="p">]</code></pre>
            <p>This makes it simple for you to create lists of sentences and labels, as you’ve done throughout this chapter, and then tokenize the sentences. You can also do preprocessing on the fly as you read a sentence, removing stopwords, HTML tags, punctuation, and more. </p>
            <p>Here’s the complete code to create lists of sentences, labels, and URLs while having the sentences cleaned of unwanted words and characters:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"/tmp/sarcasm.json"</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">datastore</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">)</code>
 
<code class="n">sentences</code> <code class="o">=</code> <code class="p">[]</code> 
<code class="n">labels</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">urls</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">datastore</code><code class="p">:</code>
    <code class="n">sentence</code> <code class="o">=</code> <code class="n">item</code><code class="p">[</code><code class="s1">'headline'</code><code class="p">]</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code>
    <code class="n">sentence</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">","</code><code class="p">,</code> <code class="s2">" , "</code><code class="p">)</code>
    <code class="n">sentence</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"."</code><code class="p">,</code> <code class="s2">" . "</code><code class="p">)</code>
    <code class="n">sentence</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"-"</code><code class="p">,</code> <code class="s2">" - "</code><code class="p">)</code>
    <code class="n">sentence</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"/"</code><code class="p">,</code> <code class="s2">" / "</code><code class="p">)</code>
    <code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">sentence</code><code class="p">)</code>
    <code class="n">sentence</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">get_text</code><code class="p">()</code>
    <code class="n">words</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
    <code class="n">filtered_sentence</code> <code class="o">=</code> <code class="s2">""</code>
    <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">:</code>
        <code class="n">word</code> <code class="o">=</code> <code class="n">word</code><code class="o">.</code><code class="n">translate</code><code class="p">(</code><code class="n">table</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">word</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">:</code>
            <code class="n">filtered_sentence</code> <code class="o">=</code> <code class="n">filtered_sentence</code> <code class="o">+</code> <code class="n">word</code> <code class="o">+</code> <code class="s2">" "</code>
    <code class="n">sentences</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">filtered_sentence</code><code class="p">)</code>
    <code class="n">labels</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">item</code><code class="p">[</code><code class="s1">'is_sarcastic'</code><code class="p">])</code>
    <code class="n">urls</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">item</code><code class="p">[</code><code class="s1">'article_link'</code><code class="p">])</code></pre>
            <p>As before, you can split these into training and test sets. If you want to use 23,000 of the 26,000 items in the dataset for training, you can do the following:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">training_size</code> <code class="o">=</code> <code class="mi">23000</code>
 
<code class="n">training_sentences</code> <code class="o">=</code> <code class="n">sentences</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">training_size</code><code class="p">]</code>
<code class="n">testing_sentences</code> <code class="o">=</code> <code class="n">sentences</code><code class="p">[</code><code class="n">training_size</code><code class="p">:]</code>
<code class="n">training_labels</code> <code class="o">=</code> <code class="n">labels</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">training_size</code><code class="p">]</code>
<code class="n">testing_labels</code> <code class="o">=</code> <code class="n">labels</code><code class="p">[</code><code class="n">training_size</code><code class="p">:]</code></pre>
            <p>Now that you have them as <code>in_memory</code> string arrays, tokenizing them and sequencing them will work exactly the same way as tokenizing and sequencing the sarcasm <span class="keep-together">dataset.</span></p>
            <p>Hopefully, the similar-looking code will help you see the pattern that you can follow when preparing text for neural networks to classify or generate. In the next chapter, you’ll learn how to build a classifier for text using embeddings, and in <a data-type="xref" href="ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648">Chapter 7</a> you’ll take that a step further by exploring recurrent neural networks. Then, in <a data-type="xref" href="ch08.html#ch08_using_ml_to_create_text_1748549671852453">Chapter 8</a>, you’ll learn how to further enhance the sequence data to create a neural network that can generate new text!</p>
            <div data-type="tip"><h6>Tip</h6>
              <p>Regular expressions (aka Regex) are terrific tools for sorting, filtering, and cleaning text. They have a syntax that’s often hard to understand and difficult to learn, but I have found that generative AI tools like Gemini, Claude, and ChatGPT are really useful here.<a contenteditable="false" data-primary="" data-startref="ch5src" data-type="indexterm" id="id1143"/><a contenteditable="false" data-primary="" data-startref="ch5src2" data-type="indexterm" id="id1144"/><a contenteditable="false" data-primary="" data-startref="ch5src3" data-type="indexterm" id="id1145"/><a contenteditable="false" data-primary="" data-startref="ch5json" data-type="indexterm" id="id1146"/><a contenteditable="false" data-primary="" data-startref="ch5json2" data-type="indexterm" id="id1147"/><a contenteditable="false" data-primary="" data-startref="ch5json3" data-type="indexterm" id="id1148"/><a contenteditable="false" data-primary="" data-startref="ch5json4" data-type="indexterm" id="id1149"/></p>
            </div>
          </div></section>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch05_summary_1748549080744852">
        <h1>Summary</h1>
        <p>In earlier chapters, you used images to build a classifier. Images, by definition, have well-defined dimensions—you know their width, height, and format. Text, on the other hand, can be far more difficult to work with. It is often unstructured, can contain undesirable content such as formatting instructions, doesn’t always contain what you want, and often has to be filtered to remove nonsensical or irrelevant content. </p>
        <p>In this chapter, you saw how to take text and convert it to numbers using word tokenization, and you then explored how to read and filter text in a variety of formats. With these skills in hand, you’re now ready to take the next step and learn how <span class="keep-together"><em>meaning</em></span> can be inferred from words—which is the first step in understanding natural language.</p>
      </div></section>
    </div></section></body></html>