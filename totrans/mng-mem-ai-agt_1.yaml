- en: Chapter 1\. A Deep Dive into Agent Memory Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章. 深入探讨智能体记忆系统
- en: An agent’s *memory* is, at its core, synonymous with data, storage, and retrieval.
    Anytime you hear “memory management,” you would be right to interpret this as
    “data management.” And data management, it turns out, is a concept we have a great
    deal of knowledge about in the world of software engineering. So why is it then
    that we need entire reports on memory management in AI agents? The answer, it
    turns out, is that while data management is an understood entity, the *usage*
    of data for AI agents is fundamentally different from anything that tool engineers
    have encountered before.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的“记忆”在本质上等同于数据、存储和检索。每当听到“内存管理”时，你正确地将其解释为“数据管理”。事实上，在软件工程的世界里，我们对数据管理有相当多的了解。那么，为什么我们还需要关于AI智能体内存管理的完整报告呢？答案是，虽然数据管理是一个理解的概念，但AI智能体对数据的“使用”在本质上与传统工具工程师之前遇到的一切都不同。
- en: Recall that agents are nondeterministic systems, programmed with the ability
    to use tools and constraints that generally guide them—but nondeterministic all
    the same. To these agents, some data is more relevant than others, all data takes
    up space, and all agents have a limited amount of space—or context windows—in
    which to process. This fundamental insight shapes everything about how we design,
    implement, and manage agent memory systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，智能体是非确定性系统，它们被编程使用工具和约束，这些工具和约束通常引导它们——但仍然是非确定性的。对这些智能体来说，某些数据比其他数据更相关，所有数据都占用空间，所有智能体都有有限的容量——或者说上下文窗口——来处理这些数据。这一基本洞察塑造了我们设计、实施和管理智能体记忆系统的方方面面。
- en: Because the way in which agents use data differs fundamentally from traditional
    software programs, memory is often used as a stand-in for human cognition. The
    thinking is that if agents use information to generate nondeterministically—weighing
    recent information more heavily than older information, taking into account user
    context and preferences, and adapting to new inputs dynamically—then perhaps memory
    is the appropriate analogy for how agents work.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于智能体使用数据的方式与传统软件程序有根本性的不同，因此记忆通常被用作人类认知的替代品。这种想法是，如果智能体使用信息以非确定性方式生成——比旧信息更重视新信息，考虑用户上下文和偏好，并动态适应新输入——那么记忆可能是智能体工作方式的适当类比。
- en: This tension between humanlike behavior and fundamentally different architectures
    defines the current state of agent memory systems. In this chapter, we’ll explore
    how the industry is navigating these challenges through short-term memory management,
    long-term persistence strategies, emerging technologies, and enhancement techniques
    like named entity recognition.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在类似人类行为和根本不同的架构之间的张力定义了当前智能体记忆系统的状态。在本章中，我们将探讨行业如何通过短期记忆管理、长期持久化策略、新兴技术和诸如命名实体识别等增强技术来应对这些挑战。
- en: Understanding Agent Memory Systems
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解智能体记忆系统
- en: Not every system will be the same for agents, and they are certainly going to
    change in the future—they’re changing as we speak! That’s why focusing on specific
    architectures is much less important than understanding higher concepts. Although
    agents aren’t classically programmable, they follow classic computer science principles
    when it comes to memory management.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个系统对智能体来说都是相同的，它们在未来肯定会发生变化——它们正在变化！这就是为什么关注特定架构远不如理解更高层次的概念重要。尽管智能体不是传统可编程的，但在内存管理方面，它们遵循经典计算机科学原理。
- en: Think of agent memory as like RAM in a computer. Even as context windows expand,
    it’s generally true that the more applicable and concise the information, and
    the more direct the query, the better the results will be. This isn’t just about
    efficiency; it’s about the fundamental nature of language paired with the systems
    that process this information.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 将智能体记忆类比为计算机中的RAM。即使上下文窗口扩大，通常情况下，信息越适用、越简洁，查询越直接，结果就会越好。这不仅仅关乎效率；这是关于语言的基本性质以及处理这些信息的系统的本质。
- en: As there are different architectures for agent systems, there are also different
    classifications of memory. Some distinguish between sensory memory (ingesting
    information like images, audio, and haptic feedback), short-term or working memory
    (an active memory buffer of conversation history), and long-term memory (storage
    relevant to the agent’s or user’s life or work).^([1](ch01.html#id57)) Others
    focus on splitting between short-term and long-term memory, with subcategories
    for long-term memory including episodic (specific past events), procedural (contextual
    working knowledge and learned skills), and semantic (general world knowledge).^([2](ch01.html#id58))
    Much of this thinking is influenced by the field of psychology, which can categorize
    human memory similarly. These distinctions help tailor how agents interact with
    humans, with systems, and even with other agents. In particular, they influence
    how memory is stored and retrieved.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代理系统有不同的架构，因此也有不同的内存分类。有些区分感觉记忆（如图像、音频和触觉反馈等信息摄入）、短期或工作记忆（对话历史的活跃记忆缓冲区）和长期记忆（与代理或用户的生活或工作相关的存储）。^([1](ch01.html#id57))
    另一些人专注于短期和长期记忆之间的划分，长期记忆的子类别包括情景记忆（特定过去事件）、程序记忆（情境工作知识和学习技能）和语义记忆（一般世界知识）。^([2](ch01.html#id58))
    这些思考很大程度上受到心理学领域的影响，心理学可以以类似的方式对人类记忆进行分类。这些区分有助于定制代理与人类、系统和甚至与其他代理的交互方式。特别是，它们影响记忆的存储和检索方式。
- en: Memory Storage and Representation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存存储和表示
- en: Almost all memory is embedded into vectors of continuous numbers meaningful
    to specific large language models (LLMs) and then stored in vector databases.
    Some of this information can be considered knowledge (embedded documents or working
    context), while other information is memory in the traditional sense (user preferences,
    standing instructions, or relevant past answers). Then there are episodic memories
    that don’t need to be kept, such as random interactions or questions that don’t
    appear to have lasting relevance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的记忆都嵌入到对特定大型语言模型（LLMs）有意义的连续数字向量中，然后存储在向量数据库中。其中一些信息可以被认为是知识（嵌入的文档或工作上下文），而其他信息是传统意义上的记忆（用户偏好、持续指令或相关过去答案）。然后还有不需要保留的情景记忆，例如随机交互或似乎没有持久相关性的问题。
- en: The process of deciding what constitutes each type is where the complexity begins.
    Unlike traditional databases where you explicitly define schemas and relationships,
    agent memory systems must make these determinations dynamically, often with imperfect
    information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决定每种类型构成的过程是复杂性的开始。与明确定义模式和关系的传统数据库不同，代理记忆系统必须动态地做出这些决定，通常是在不完整信息的情况下。
- en: The Challenge of Nondeterministic Systems
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非确定性系统的挑战
- en: 'Those of us who work with agents every day know the frustration: instructions
    explicitly given in the past that are no longer retained in a new session or,
    worse, instructions neglected during a longer session with the same agent. But
    even if memories are retained, that doesn’t mean the agent will act as you’d expect.
    Because agents are not programmable in the traditional sense, giving one the same
    task twice may yield markedly different results.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这些每天与代理打交道的人都知道这种挫败感：过去明确给出的指令在新会话中不再保留，或者更糟糕的是，在较长的会话中忽略了相同的指令。即使记忆被保留，这也并不意味着代理会按照你的预期行事。因为代理不是以传统方式可编程的，所以重复相同的任务可能会得到明显不同的结果。
- en: Consider a research assistant that pulls different sources for the same question.
    This variability extends to coding agents, too. After all, asking a coding agent
    to scan and iteratively improve a large legacy codebase is a far more difficult
    task than asking one to build a new repository from scratch. Instructing an agent
    to “alter the search bar” in a system that could have many search bars leaves
    significant room for interpretation. What agents retrieve for you largely depends
    on factors such as the size of the context the agent must ingest, a complex interplay
    of their embeddings, their similarity metrics, and the specific phrasing of your
    query.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个研究助理为同一个问题检索不同来源的情况。这种可变性也扩展到编码代理。毕竟，要求编码代理扫描和迭代改进大型遗留代码库的任务，比要求其从头开始构建新仓库要困难得多。指示代理在一个可能有许多搜索栏的系统中的“修改搜索栏”留下了很大的解释空间。代理为你检索的内容很大程度上取决于因素，如代理必须摄入的上下文大小、它们嵌入的复杂相互作用、它们的相似性指标以及你查询的具体措辞。
- en: 'Storage and Retrieval: The Core Challenge'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储和检索：核心挑战
- en: 'This bears repeating because it is the most crucial aspect of memory management
    in AI agent systems: the retention of knowledge is dynamic and stochastic, not
    just on the storage side but on the retrieval side as well.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要反复强调，因为这是人工智能代理系统中内存管理最关键的一个方面：知识的保留是动态和随机的，不仅是在存储方面，在检索方面也是如此。
- en: How do you decide what should be stored? What instructions are comprehensive
    enough to give a system? Do you store everything? Conversations can range from
    a few sentences to dozens of pages of text, depending on the user and use case.
    When storage gets tight, how do you flush the system?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何决定应该存储什么？哪些指令足够全面以提供给系统？你是否存储一切？对话可能从几句话到几十页的文本不等，这取决于用户和用例。当存储空间紧张时，你如何清除系统？
- en: 'There are many strategies to address these challenges. Some popular methods
    include:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多策略可以应对这些挑战。一些流行的方法包括：
- en: Importance scoring
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性评分
- en: Calculating memory importance based on recency, frequency of reference, user
    engagement metrics, and keyword relevance^([3](ch01.html#id59))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基于近期、引用频率、用户参与度指标和关键词相关性计算内存重要性^([3](ch01.html#id59))
- en: Cascading memory systems
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 级联内存系统
- en: Allowing the agent itself to choose what to promote to long-term storage and
    what to retrieve^([4](ch01.html#id60))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 允许代理本身选择什么应该提升到长期存储，什么应该检索^([4](ch01.html#id60))
- en: Intelligent compression
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 智能压缩
- en: Using specialized models to condense conversation history into key details,
    events, and decisions^([5](ch01.html#id61))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用专用模型将对话历史压缩为关键细节、事件和决策^([5](ch01.html#id61))
- en: Vector store offloading
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 向量存储卸载
- en: Moving older messages from short-term memory into vector stores, often with
    summarization^([6](ch01.html#id62))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将较旧的消息从短期记忆移动到向量存储中，通常伴随着摘要^([6](ch01.html#id62))
- en: Engineers typically compress information by instructing an LLM to summarize
    to the best of its ability. But summaries are not the same thing as the original.
    There is, by definition, loss of information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师通常通过指示大型语言模型（LLM）尽可能地进行总结来压缩信息。但摘要与原文不同。根据定义，信息会有所损失。
- en: The Imprecision of Retrieval
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索的不精确性
- en: 'Retrieval makes everything more complicated. Information retrieval is often
    based on similarity between texts, and because language is imprecise, retrieval
    will be fuzzy, too. The classic example is *bank*: it can be a financial institution
    or an aspect of a river.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 检索使一切变得更加复杂。信息检索通常基于文本之间的相似性，由于语言的不精确性，检索也会变得模糊。经典的例子是*银行*：它可以是一个金融机构或河流的一个方面。
- en: 'Depending on the model choice and embeddings, different algorithms must be
    used: cosine similarity, Euclidean distance, or even older methods like term frequency–inverse
    document frequency (TF-IDF). There’s no single way to search and retrieve information,
    with plenty of trade-offs between speed and accuracy.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型选择和嵌入，必须使用不同的算法：余弦相似度、欧几里得距离，甚至像词频-逆文档频率（TF-IDF）这样的旧方法。没有单一的方式来搜索和检索信息，速度和准确性之间有很多权衡。
- en: We’re certainly improving in the vector database space—local options like ChromaDB,
    Redis, PostgreSQL with pgvector, and Qdrant exist—but there’s still plenty of
    room for further improvement.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在向量数据库空间中确实在进步——本地选项如ChromaDB、Redis、带有pgvector的PostgreSQL和Qdrant存在，但仍有很大的改进空间。
- en: Managing Context Window Limitations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理上下文窗口限制
- en: All models have context windows containing information provided as context for
    generating answers. There are different methods for managing these limitations.
    With FIFO (first in, first out), the earliest information received over a long
    conversation may be least accurately recalled, meaning that more recent information
    gets prioritized.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型都包含包含用于生成答案的上下文信息的上下文窗口。管理这些限制有不同的方法。使用FIFO（先进先出），在长时间对话中最早接收到的信息可能最不准确，这意味着最近的信息被优先考虑。
- en: 'Strategies to address this include intelligent pruning, where a model removes
    superfluous information. But there are consequences to this. Consider summarized
    information about a legal text: you might get the broad strokes of the legal argument
    and topics but lose critical information, like a negation or case reference that
    completely changes the content. Summarization by definition means losing detail
    and taking a higher abstract perspective.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一问题的策略包括智能修剪，其中模型会删除多余的信息。但这也带来了一些后果。考虑关于法律文本的摘要信息：你可能会得到法律论点和主题的大致轮廓，但会失去关键信息，比如否定或案例引用，这会完全改变内容。根据定义，摘要意味着失去细节并采取更高的抽象视角。
- en: Compare this to larger context windows like those in Gemini 2.5, which can handle
    millions of tokens. We can stuff more information into a model, but we may not
    have effective recall for the first-passed information relative to the last. It’d
    be nice if we lived in a world where models had perfect recall, but the architecture
    of transformers’ self-attention mechanisms requires quadratically more processing
    as context increases.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与 Gemini 2.5 中的更大上下文窗口进行比较，它可以处理数百万个标记。我们可以将更多信息放入模型，但相对于最后的信息，我们可能没有有效的召回能力。如果我们可以生活在一个模型具有完美召回能力的世界里，那将很棒，但变压器自注意力机制的架构要求随着上下文的增加而进行二次方处理。
- en: Algorithms like FlashAttention attempt to work around this, but a more direct
    approach might be retrieval-augmented generation (RAG), which limits the corpus
    of documents and forces the LLM to return sourced information rather than stuffing
    superfluous information inside.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 FlashAttention 这样的算法试图解决这个问题，但更直接的方法可能是检索增强生成（RAG），它限制了文档的语料库并迫使大型语言模型返回有源信息，而不是在信息中填充多余的内容。
- en: 'An even more refined approach—one I believe will become more popular—is semantic
    caching. What if we retained the relative context of an information retrieval
    system over time by processing the semantics of the content being passed? Frequently
    retrieved information gets prioritized. For systems like internal LLMs or RAG
    where many users talk to the same corpus of information, it may be both more computationally
    effective and cost-effective to semantically cache that information. Semantic
    caching isn’t without its drawbacks—it works exceptionally well for single-shot
    questions but breaks down in multiturn conversations. These methods will continue
    to be refined, but the fundamental problems will remain the same: how can we most
    efficiently and effectively store and retrieve the information we desire?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更为精细的方法——我相信它将变得更加流行——是语义缓存。如果我们通过处理传递内容的意义来保留信息检索系统随时间推移的相对上下文，会怎样？频繁检索到的信息得到优先处理。对于像内部大型语言模型或
    RAG 这样的系统，其中许多用户与相同的信息语料库进行交流，可能更有效率和成本效益的是在语义上缓存这些信息。语义缓存并非没有缺点——它对于单次问题处理得非常好，但在多轮对话中会崩溃。这些方法将继续得到改进，但基本问题将保持不变：我们如何最有效地存储和检索我们所需的信息？
- en: Persistence via Checkpointing
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过检查点实现持久性
- en: Checkpointing is a crucial step for agents, especially those that engage in
    multiturn conversations. As agents interact with a user or a system, they periodically
    save their internal states (their memory) in order to persist information across
    sessions or long conversations. Different organizations, products, and systems
    approach this process differently. The key insight is that checkpointing isn’t
    just about saving state—it’s about making that state retrievable and actionable
    in the dynamic, nondeterministic world of agent interactions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点对于代理来说是一个关键步骤，尤其是那些参与多轮对话的代理。随着代理与用户或系统交互，它们会定期保存其内部状态（它们的记忆），以便在会话或长对话中持久化信息。不同的组织、产品和系统以不同的方式处理这个过程。关键洞察是，检查点不仅仅是保存状态——它是在动态、非确定性的代理交互世界中使状态可检索和可操作。
- en: There are many ways to handle checkpointing, and different teams use different
    tools. For example, Redis is a popular choice because it’s fast and works well
    for real-time applications. Some setups use Redis to save conversation threads
    or agent state, making it easy to restore context across sessions or even share
    memory between different parts of a system. Features like automatic cleanup (time
    to live) help keep things tidy, so you’re not stuck with a bunch of old, irrelevant
    data.^([7](ch01.html#id63))
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 处理检查点的方式有很多，不同的团队使用不同的工具。例如，Redis 是一个流行的选择，因为它速度快，非常适合实时应用。一些配置使用 Redis 来保存会话线程或代理状态，这使得跨会话恢复上下文或在不同系统部分之间共享内存变得容易。像自动清理（生存时间）这样的功能有助于保持事物整洁，因此你不会陷入一大堆过时、无关的数据。[7](ch01.html#id63)
- en: Basically, checkpointing is about making sure agents don’t lose their place
    and that their memory is both persistent and practical in the unpredictable world
    of AI interactions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，检查点是为了确保代理不会失去位置，并且它们的记忆在不可预测的 AI 交互世界中既持久又实用。
- en: ^([1](ch01.html#id57-marker)) Michael Lanham, *AI Agents in Action* (Manning
    Publications, 2024), 200.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch01.html#id57-marker) Michael Lanham, 《AI Agents in Action》 (Manning Publications,
    2024), 200.'
- en: '^([2](ch01.html#id58-marker)) Manvinder Singh and Andrew Brookins, “Build Smarter
    AI Agents: Manage Short-Term and Long-Term Memory with Redis,” Redis Blog, April
    29, 2025, [*https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/*](https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.html#id58-marker)) Manvinder Singh 和 Andrew Brookins, “构建更智能的 AI
    代理：使用 Redis 管理短期和长期内存,” Redis 博客, 2025年4月29日, [*https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/*](https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/).
- en: ^([3](ch01.html#id59-marker)) Singh and Brookins, “Build Smarter AI Agents.”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.html#id59-marker)) Singh 和 Brookins, “构建更智能的 AI 代理。”
- en: ^([4](ch01.html#id60-marker)) “Memory for Agents,” LangChain Blog, October 19,
    2024, [*https://blog.langchain.com/memory-for-agents*](https://blog.langchain.com/memory-for-agents).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.html#id60-marker)) “代理的内存,” LangChain 博客, 2024年10月19日, [*https://blog.langchain.com/memory-for-agents*](https://blog.langchain.com/memory-for-agents).
- en: ^([5](ch01.html#id61-marker)) “How to Migrate to LangGraph Memory,” LangChain
    Documentation, [*https://python.langchain.com/docs/versions/migrating_memory*](https://python.langchain.com/docs/versions/migrating_memory).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch01.html#id61-marker)) “如何迁移到 LangGraph 内存,” LangChain 文档, [*https://python.langchain.com/docs/versions/migrating_memory*](https://python.langchain.com/docs/versions/migrating_memory).
- en: ^([6](ch01.html#id62-marker)) Singh and Brookins, “Build Smarter AI Agents.”
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch01.html#id62-marker)) Singh 和 Brookins, “构建更智能的 AI 代理。”
- en: '^([7](ch01.html#id63-marker)) Brian Sam-Bodden, “LangGraph & Redis: Build Smarter
    AI Agents with Memory & Persistence,” Redis Blog, March 28, 2025, [*https://redis.io/blog/langgraph-redis-build-smarter-ai-agents-with-memory-persistence/*](https://redis.io/blog/langgraph-redis-build-smarter-ai-agents-with-memory-persistence/);
    Redis Official Documentation, Redis, [*https://redis.io/docs/*](https://redis.io/docs/).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch01.html#id63-marker)) Brian Sam-Bodden, “LangGraph 与 Redis：使用内存和持久性构建更智能的
    AI 代理,” Redis 博客, 2025年3月28日, [*https://redis.io/blog/langgraph-redis-build-smarter-ai-agents-with-memory-persistence/*](https://redis.io/blog/langgraph-redis-build-smarter-ai-agents-with-memory-persistence/);
    Redis 官方文档, Redis, [*https://redis.io/docs/*](https://redis.io/docs/).
