- en: Chapter 1\. A Deep Dive into Agent Memory Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An agent’s *memory* is, at its core, synonymous with data, storage, and retrieval.
    Anytime you hear “memory management,” you would be right to interpret this as
    “data management.” And data management, it turns out, is a concept we have a great
    deal of knowledge about in the world of software engineering. So why is it then
    that we need entire reports on memory management in AI agents? The answer, it
    turns out, is that while data management is an understood entity, the *usage*
    of data for AI agents is fundamentally different from anything that tool engineers
    have encountered before.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that agents are nondeterministic systems, programmed with the ability
    to use tools and constraints that generally guide them—but nondeterministic all
    the same. To these agents, some data is more relevant than others, all data takes
    up space, and all agents have a limited amount of space—or context windows—in
    which to process. This fundamental insight shapes everything about how we design,
    implement, and manage agent memory systems.
  prefs: []
  type: TYPE_NORMAL
- en: Because the way in which agents use data differs fundamentally from traditional
    software programs, memory is often used as a stand-in for human cognition. The
    thinking is that if agents use information to generate nondeterministically—weighing
    recent information more heavily than older information, taking into account user
    context and preferences, and adapting to new inputs dynamically—then perhaps memory
    is the appropriate analogy for how agents work.
  prefs: []
  type: TYPE_NORMAL
- en: This tension between humanlike behavior and fundamentally different architectures
    defines the current state of agent memory systems. In this chapter, we’ll explore
    how the industry is navigating these challenges through short-term memory management,
    long-term persistence strategies, emerging technologies, and enhancement techniques
    like named entity recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Agent Memory Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not every system will be the same for agents, and they are certainly going to
    change in the future—they’re changing as we speak! That’s why focusing on specific
    architectures is much less important than understanding higher concepts. Although
    agents aren’t classically programmable, they follow classic computer science principles
    when it comes to memory management.
  prefs: []
  type: TYPE_NORMAL
- en: Think of agent memory as like RAM in a computer. Even as context windows expand,
    it’s generally true that the more applicable and concise the information, and
    the more direct the query, the better the results will be. This isn’t just about
    efficiency; it’s about the fundamental nature of language paired with the systems
    that process this information.
  prefs: []
  type: TYPE_NORMAL
- en: As there are different architectures for agent systems, there are also different
    classifications of memory. Some distinguish between sensory memory (ingesting
    information like images, audio, and haptic feedback), short-term or working memory
    (an active memory buffer of conversation history), and long-term memory (storage
    relevant to the agent’s or user’s life or work).^([1](ch01.html#id57)) Others
    focus on splitting between short-term and long-term memory, with subcategories
    for long-term memory including episodic (specific past events), procedural (contextual
    working knowledge and learned skills), and semantic (general world knowledge).^([2](ch01.html#id58))
    Much of this thinking is influenced by the field of psychology, which can categorize
    human memory similarly. These distinctions help tailor how agents interact with
    humans, with systems, and even with other agents. In particular, they influence
    how memory is stored and retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Storage and Representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost all memory is embedded into vectors of continuous numbers meaningful
    to specific large language models (LLMs) and then stored in vector databases.
    Some of this information can be considered knowledge (embedded documents or working
    context), while other information is memory in the traditional sense (user preferences,
    standing instructions, or relevant past answers). Then there are episodic memories
    that don’t need to be kept, such as random interactions or questions that don’t
    appear to have lasting relevance.
  prefs: []
  type: TYPE_NORMAL
- en: The process of deciding what constitutes each type is where the complexity begins.
    Unlike traditional databases where you explicitly define schemas and relationships,
    agent memory systems must make these determinations dynamically, often with imperfect
    information.
  prefs: []
  type: TYPE_NORMAL
- en: The Challenge of Nondeterministic Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Those of us who work with agents every day know the frustration: instructions
    explicitly given in the past that are no longer retained in a new session or,
    worse, instructions neglected during a longer session with the same agent. But
    even if memories are retained, that doesn’t mean the agent will act as you’d expect.
    Because agents are not programmable in the traditional sense, giving one the same
    task twice may yield markedly different results.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a research assistant that pulls different sources for the same question.
    This variability extends to coding agents, too. After all, asking a coding agent
    to scan and iteratively improve a large legacy codebase is a far more difficult
    task than asking one to build a new repository from scratch. Instructing an agent
    to “alter the search bar” in a system that could have many search bars leaves
    significant room for interpretation. What agents retrieve for you largely depends
    on factors such as the size of the context the agent must ingest, a complex interplay
    of their embeddings, their similarity metrics, and the specific phrasing of your
    query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Storage and Retrieval: The Core Challenge'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This bears repeating because it is the most crucial aspect of memory management
    in AI agent systems: the retention of knowledge is dynamic and stochastic, not
    just on the storage side but on the retrieval side as well.'
  prefs: []
  type: TYPE_NORMAL
- en: How do you decide what should be stored? What instructions are comprehensive
    enough to give a system? Do you store everything? Conversations can range from
    a few sentences to dozens of pages of text, depending on the user and use case.
    When storage gets tight, how do you flush the system?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many strategies to address these challenges. Some popular methods
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: Importance scoring
  prefs: []
  type: TYPE_NORMAL
- en: Calculating memory importance based on recency, frequency of reference, user
    engagement metrics, and keyword relevance^([3](ch01.html#id59))
  prefs: []
  type: TYPE_NORMAL
- en: Cascading memory systems
  prefs: []
  type: TYPE_NORMAL
- en: Allowing the agent itself to choose what to promote to long-term storage and
    what to retrieve^([4](ch01.html#id60))
  prefs: []
  type: TYPE_NORMAL
- en: Intelligent compression
  prefs: []
  type: TYPE_NORMAL
- en: Using specialized models to condense conversation history into key details,
    events, and decisions^([5](ch01.html#id61))
  prefs: []
  type: TYPE_NORMAL
- en: Vector store offloading
  prefs: []
  type: TYPE_NORMAL
- en: Moving older messages from short-term memory into vector stores, often with
    summarization^([6](ch01.html#id62))
  prefs: []
  type: TYPE_NORMAL
- en: Engineers typically compress information by instructing an LLM to summarize
    to the best of its ability. But summaries are not the same thing as the original.
    There is, by definition, loss of information.
  prefs: []
  type: TYPE_NORMAL
- en: The Imprecision of Retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Retrieval makes everything more complicated. Information retrieval is often
    based on similarity between texts, and because language is imprecise, retrieval
    will be fuzzy, too. The classic example is *bank*: it can be a financial institution
    or an aspect of a river.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the model choice and embeddings, different algorithms must be
    used: cosine similarity, Euclidean distance, or even older methods like term frequency–inverse
    document frequency (TF-IDF). There’s no single way to search and retrieve information,
    with plenty of trade-offs between speed and accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: We’re certainly improving in the vector database space—local options like ChromaDB,
    Redis, PostgreSQL with pgvector, and Qdrant exist—but there’s still plenty of
    room for further improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Context Window Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All models have context windows containing information provided as context for
    generating answers. There are different methods for managing these limitations.
    With FIFO (first in, first out), the earliest information received over a long
    conversation may be least accurately recalled, meaning that more recent information
    gets prioritized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategies to address this include intelligent pruning, where a model removes
    superfluous information. But there are consequences to this. Consider summarized
    information about a legal text: you might get the broad strokes of the legal argument
    and topics but lose critical information, like a negation or case reference that
    completely changes the content. Summarization by definition means losing detail
    and taking a higher abstract perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: Compare this to larger context windows like those in Gemini 2.5, which can handle
    millions of tokens. We can stuff more information into a model, but we may not
    have effective recall for the first-passed information relative to the last. It’d
    be nice if we lived in a world where models had perfect recall, but the architecture
    of transformers’ self-attention mechanisms requires quadratically more processing
    as context increases.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms like FlashAttention attempt to work around this, but a more direct
    approach might be retrieval-augmented generation (RAG), which limits the corpus
    of documents and forces the LLM to return sourced information rather than stuffing
    superfluous information inside.
  prefs: []
  type: TYPE_NORMAL
- en: 'An even more refined approach—one I believe will become more popular—is semantic
    caching. What if we retained the relative context of an information retrieval
    system over time by processing the semantics of the content being passed? Frequently
    retrieved information gets prioritized. For systems like internal LLMs or RAG
    where many users talk to the same corpus of information, it may be both more computationally
    effective and cost-effective to semantically cache that information. Semantic
    caching isn’t without its drawbacks—it works exceptionally well for single-shot
    questions but breaks down in multiturn conversations. These methods will continue
    to be refined, but the fundamental problems will remain the same: how can we most
    efficiently and effectively store and retrieve the information we desire?'
  prefs: []
  type: TYPE_NORMAL
- en: Persistence via Checkpointing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Checkpointing is a crucial step for agents, especially those that engage in
    multiturn conversations. As agents interact with a user or a system, they periodically
    save their internal states (their memory) in order to persist information across
    sessions or long conversations. Different organizations, products, and systems
    approach this process differently. The key insight is that checkpointing isn’t
    just about saving state—it’s about making that state retrievable and actionable
    in the dynamic, nondeterministic world of agent interactions.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to handle checkpointing, and different teams use different
    tools. For example, Redis is a popular choice because it’s fast and works well
    for real-time applications. Some setups use Redis to save conversation threads
    or agent state, making it easy to restore context across sessions or even share
    memory between different parts of a system. Features like automatic cleanup (time
    to live) help keep things tidy, so you’re not stuck with a bunch of old, irrelevant
    data.^([7](ch01.html#id63))
  prefs: []
  type: TYPE_NORMAL
- en: Basically, checkpointing is about making sure agents don’t lose their place
    and that their memory is both persistent and practical in the unpredictable world
    of AI interactions.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch01.html#id57-marker)) Michael Lanham, *AI Agents in Action* (Manning
    Publications, 2024), 200.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch01.html#id58-marker)) Manvinder Singh and Andrew Brookins, “Build Smarter
    AI Agents: Manage Short-Term and Long-Term Memory with Redis,” Redis Blog, April
    29, 2025, [*https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/*](https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch01.html#id59-marker)) Singh and Brookins, “Build Smarter AI Agents.”
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch01.html#id60-marker)) “Memory for Agents,” LangChain Blog, October 19,
    2024, [*https://blog.langchain.com/memory-for-agents*](https://blog.langchain.com/memory-for-agents).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch01.html#id61-marker)) “How to Migrate to LangGraph Memory,” LangChain
    Documentation, [*https://python.langchain.com/docs/versions/migrating_memory*](https://python.langchain.com/docs/versions/migrating_memory).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch01.html#id62-marker)) Singh and Brookins, “Build Smarter AI Agents.”
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch01.html#id63-marker)) Brian Sam-Bodden, “LangGraph & Redis: Build Smarter
    AI Agents with Memory & Persistence,” Redis Blog, March 28, 2025, [*https://redis.io/blog/langgraph-redis-build-smarter-ai-agents-with-memory-persistence/*](https://redis.io/blog/langgraph-redis-build-smarter-ai-agents-with-memory-persistence/);
    Redis Official Documentation, Redis, [*https://redis.io/docs/*](https://redis.io/docs/).'
  prefs: []
  type: TYPE_NORMAL
