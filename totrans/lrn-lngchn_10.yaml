- en: 'Chapter 10\. Testing: Evaluation, Monitoring, and Continuous Improvement'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 9](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604),
    you learned how to deploy your AI application into production and utilize LangGraph
    Platform to host and debug your app.
  prefs: []
  type: TYPE_NORMAL
- en: Although your app can respond to user inputs and execute complex tasks, its
    underlying LLM is nondeterministic and prone to hallucination. As discussed in
    previous chapters, LLMs can generate inaccurate and outdated outputs due to a
    variety of reasons including the prompt, format of user’s input, and retrieved
    context. In addition, harmful or misleading LLM outputs can significantly damage
    a company’s brand and customer loyalty.
  prefs: []
  type: TYPE_NORMAL
- en: To combat this tendency toward hallucination, you need to build an efficient
    system to test, evaluate, monitor, and continuously improve your LLM applications’
    performance. This robust testing process will enable you to quickly debug and
    fix AI-related issues before and after your app is in production.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to build an iterative testing system across
    the key stages of the LLM app development life-cycle and maintain high performance
    of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Techniques Across the LLM App Development Cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we construct the testing system, let’s briefly review how testing can
    be applied across the three key stages of LLM app development:'
  prefs: []
  type: TYPE_NORMAL
- en: Design
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, LLM tests are applied directly to your application. These tests
    can be assertions executed at runtime that feed failures back to the LLM for self-correction.
    The purpose of testing at this stage is error handling within your app before
    it affects users.
  prefs: []
  type: TYPE_NORMAL
- en: Preproduction
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, tests are run right before deployment into production. The purpose
    of testing at this stage is to catch and fix any regressions before the app is
    released to real users.
  prefs: []
  type: TYPE_NORMAL
- en: Production
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, tests are run while your application is in production to help
    monitor and catch errors affecting real users. The purpose is to identify issues
    and feed them back into the design or preproduction phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The combination of testing across these stages creates a continuous improvement
    cycle where these steps are repeated: design, test, deploy, monitor, fix, and
    redesign. See [Figure 10-1](#ch10_figure_1_1736545678095728).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](assets/lelc_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. The three key stages of the LLM app development cycle
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In essence, this cycle helps you to identify and fix production issues in an
    efficient and quick manner.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deeper into testing techniques across each of these stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Design Stage: Self-Corrective RAG'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed previously, your application can incorporate error handling at
    runtime that feeds errors to the LLM for self-correction. Let’s explore a RAG
    use case using LangGraph as the framework to orchestrate error handling.
  prefs: []
  type: TYPE_NORMAL
- en: Basic RAG-driven AI applications are prone to hallucination due to inaccurate
    or incomplete retrieval of relevant context to generate outputs. But you can utilize
    an LLM to grade retrieval relevance and fix hallucination issues.
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph enables you to effectively implement the control flow of this process,
    as shown in [Figure 10-2](#ch10_figure_2_1736545678095764).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Self-corrective RAG control flow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The control flow steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the routing step, each question is routed to the relevant retrieval method,
    that is, vector store and web search.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If, for example, the question is routed to a vector store for retrieval, the
    LLM in the control flow will retrieve and grade the documents for relevancy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the document is relevant, the LLM proceeds to generate an answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM will check the answer for hallucinations and only proceed to display
    the answer to the user if the output is accurate and relevant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a fallback, if the retrieved document is irrelevant or the generated answer
    doesn’t answer the user’s question, the flow utilizes web search to retrieve relevant
    information as context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process enables your app to iteratively generate answers, self-correct
    errors and hallucinations, and improve the quality of outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run through an example code implementation of this control flow. First,
    download the required packages and initialize relevant API keys. For these examples,
    you’ll need to set your OpenAI and LangSmith API keys as environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll create an index of three blog posts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed previously, the LLM will grade the relevancy of the retrieved
    documents from the index. We can construct this instruction in a system prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice the use of Pydantic/Zod to help model the binary decision output in a
    format that can be used to programmatically decide which node in the control flow
    to move toward.
  prefs: []
  type: TYPE_NORMAL
- en: In LangSmith, you can see a trace of the logic flow across the nodes discussed
    previously (see [Figure 10-3](#ch10_figure_3_1736545678095796)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. LangSmith trace results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s test to see what happens when the input question cannot be answered by
    the retrieved documents in the index.
  prefs: []
  type: TYPE_NORMAL
- en: First, utilize LangGraph to make it easier to construct, execute, and debug
    the full control flow. See the full graph definition in the book’s [GitHub repository](https://oreil.ly/v63Vr).
    Notice that we’ve added a `transform_query` node to help rewrite the input query
    in a format that web search can use to retrieve higher-quality results.
  prefs: []
  type: TYPE_NORMAL
- en: As a final step, we set up our web search tool and execute the graph using the
    out-of-context question. The LangSmith trace shows that the web search tool was
    used as a fallback to retrieve relevant information prior to the final LLM generated
    answer (see [Figure 10-4](#ch10_figure_4_1736545678095829)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. LangSmith trace of self-corrective RAG utilizing web search as
    a fallback
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s move on to the next stage in LLM app testing: preproduction.'
  prefs: []
  type: TYPE_NORMAL
- en: The Preproduction Stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of the preproduction stage of testing is to measure and evaluate
    the performance of your application prior to production. This will enable you
    to efficiently assess the accuracy, latency, and cost of utilizing the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prior to testing, you need to define a set of scenarios you’d like to test and
    evaluate. A *dataset* is a collection of examples that provide inputs and expected
    outputs used to evaluate your LLM app.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are three common methods to build datasets for valuation:'
  prefs: []
  type: TYPE_NORMAL
- en: Manually curated examples
  prefs: []
  type: TYPE_NORMAL
- en: These are handwritten examples based on expected user inputs and ideal generated
    outputs. A small dataset consists of between 10 and 50 quality examples. Over
    time, more examples can be added to the dataset based on edge cases that emerge
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: Application logs
  prefs: []
  type: TYPE_NORMAL
- en: Once the application is in production, you can store real-time user inputs and
    later add them to the dataset. This will help ensure the dataset is realistic
    and covers the most common user questions.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data
  prefs: []
  type: TYPE_NORMAL
- en: These are artificially generated examples that simulate various scenarios and
    edge cases. This enables you to generate new inputs by sampling existing inputs,
    which is useful when you don’t have enough real data to test on.
  prefs: []
  type: TYPE_NORMAL
- en: In LangSmith, you can create a new dataset by selecting Datasets and Testing
    in the sidebar and clicking the “+ New Dataset” button on the top right of the
    app, as shown in [Figure 10-5](#ch10_figure_5_1736545678095862).
  prefs: []
  type: TYPE_NORMAL
- en: In the opened window, enter the relevant dataset details, including a name,
    description, and dataset type. If you’d like to use your own dataset, click the
    “Upload a CSV dataset” button.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Creating a new dataset in the LangSmith UI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'LangSmith offers three different dataset types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kv` (key-value) dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '*Inputs* and *outputs* are represented as arbitrary key-value pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `kv` dataset is the most versatile, and it is the default type. The `kv`
    dataset is suitable for a wide range of evaluation scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset type is ideal for evaluating chains and agents that require multiple
    inputs or generate multiple outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm` (large language model) dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The `llm` dataset is designed for evaluating completion style language models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs dictionary contains a single input key mapped to the prompt string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs dictionary contains a single output key mapped to the corresponding
    response string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset type simplifies evaluation for LLMs by providing a standardized
    format for inputs and outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chat` dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The `chat` dataset is designed for evaluating LLM structured chat messages as
    inputs and outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *inputs* dictionary contains a single *input* key mapped to a list of serialized
    chat messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *outputs* dictionary contains a single *output* key mapped to a list of
    serialized chat messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset type is useful for evaluating conversational AI systems or chatbots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most flexible option is the key-value data type (see [Figure 10-6](#ch10_figure_6_1736545678095893)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Selecting a dataset type in the LangSmith UI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, add examples to the dataset by clicking Add Example. Provide the input
    and output examples as JSON objects, as shown in [Figure 10-7](#ch10_figure_7_1736545678095923).
  prefs: []
  type: TYPE_NORMAL
- en: '![A white background with black lines  Description automatically generated](assets/lelc_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Add key-value dataset examples in the LangSmith UI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also define a schema for your dataset in the “Dataset schema” section,
    as shown in [Figure 10-8](#ch10_figure_8_1736545678095955).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. Adding a dataset schema in the LangSmith UI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Defining Your Evaluation Criteria
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After creating your dataset, you need to define evaluation metrics to assess
    your application’s outputs before deploying into production. This batch evaluation
    on a predetermined test suite is often referred to as*offline evaluation***.**
  prefs: []
  type: TYPE_NORMAL
- en: For offline evaluation, you can optionally label expected outputs (that is,
    ground truth references) for the data points you are testing on. This enables
    you to compare your application’s response with the ground truth references, as
    shown in [Figure 10-9](#ch10_figure_9_1736545678095982).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of an application process  Description automatically generated](assets/lelc_1009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. AI evaluation diagram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are three main evaluators to score your LLM app performance:'
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluators
  prefs: []
  type: TYPE_NORMAL
- en: If you can’t express your testing requirements as code, you can use human feedback
    to express qualitative characteristics and label app responses with scores. LangSmith
    speeds up the process of collecting and incorporating human feedback with annotation
    queues.
  prefs: []
  type: TYPE_NORMAL
- en: Heuristic evaluators
  prefs: []
  type: TYPE_NORMAL
- en: These are hardcoded functions and assertions that perform computations to determine
    a score. You can use reference-free heuristics (for example, checking whether
    output is valid JSON) or reference-based heuristics such as accuracy. Reference-based
    evaluation compares an output to a predefined ground truth, whereas reference-free
    evaluation assesses qualitative characteristics without a ground truth. Custom
    heuristic evaluators are useful for code-generation tasks such as schema checking
    and unit testing with hardcoded evaluation logic.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-as-a-judge evaluators
  prefs: []
  type: TYPE_NORMAL
- en: This evaluator integrates human grading rules into an LLM prompt to evaluate
    whether the output is correct relative to the reference answer supplied from the
    dataset output. As you iterate in preproduction, you’ll need to audit the scores
    and tune the LLM-as-a-judge to produce reliable scores.
  prefs: []
  type: TYPE_NORMAL
- en: To get started with evaluation, start simple with heuristic evaluators. Then
    implement human evaluators before moving on to LLM-as-a-judge to automate your
    human review. This enables you to add depth and scale once your criteria are well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using LLM-as-a-judge evaluators, use straightforward prompts that can easily
    be replicated and understood by a human. For example, avoid asking an LLM to produce
    scores on a range of 0 to 10 with vague distinctions between scores.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-10](#ch10_figure_10_1736545678096002) illustrates LLM-as-a-judge
    evaluator in the context of a RAG use case. Note that the reference answer is
    the ground truth.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a brain  Description automatically generated](assets/lelc_1010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. LLM-as-a-judge evaluator used in a RAG use case
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Improving LLM-as-a-judge evaluators performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using an LLM-as-a-judge is an effective method to grade natural language outputs
    from LLM applications. This involves passing the generated output to a separate
    LLM for judgment and evaluation. But how can you trust the results of LLM-as-a-judge
    evaluation?
  prefs: []
  type: TYPE_NORMAL
- en: Often, rounds of prompt engineering are required to improve accuracy, which
    is cumbersome and time-consuming. Fortunately, LangSmith provides a *few-shot*
    prompt solution whereby human corrections to LLM-as-a-judge outputs are stored
    as few-shot examples, which are then fed back into the prompt in future iterations.
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing few-shot learning, the LLM can improve accuracy and align outputs
    with human preferences by providing examples of correct behavior. This is especially
    useful when it’s difficult to construct instructions on how the LLM should behave
    or be formatted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The few-shot evaluator follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The LLM evaluator provides feedback on generated outputs, assessing factors
    such as correctness, relevance, or other criteria.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It adds human corrections to modify or correct the LLM evaluator’s feedback
    in LangSmith. This is where human preferences and judgment are captured.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These corrections are stored as few-shot examples in LangSmith, with an option
    to leave explanations for corrections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The few-shot examples are incorporated into future prompts as subsequent evaluation
    runs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Over time, the few-shot evaluator will become increasingly aligned with human
    preferences. This self-improving mechanism reduces the need for time-consuming
    prompt engineering, while improving the accuracy and relevance of LLM-as-a-judge
    evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how to easily set up the LLM-as-a-judge evaluator in LangSmith for offline
    evaluation. First, navigate to the “Datasets and Testing” section in the sidebar
    and select the dataset you want to configure the evaluator for. Click the Add
    Auto-Evaluator button at the top right of the dashboard to add an evaluator to
    the dataset. This will open a modal you can use to configure the evaluator.
  prefs: []
  type: TYPE_NORMAL
- en: Select the LLM-as-a-judge option and give your evaluator a name. You will now
    have the option to set an inline prompt or load a prompt from the prompt hub that
    will be used to evaluate the results of the runs in the experiment. For the sake
    of this example, choose the Create Few-Shot Evaluator option, as shown in [Figure 10-11](#ch10_figure_11_1736545678096023).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a survey  Description automatically generated](assets/lelc_1011.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-11\. LangSmith UI options for the LLM-as-a-judge evaluator
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This option will create a dataset that holds few-shot examples that will autopopulate
    when you make corrections on the evaluator feedback. The examples in this dataset
    will be inserted in the system prompt message.
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify the scoring criteria in the Schema field and toggle between
    primitive types—for example, integer and Boolean (see [Figure 10-12](#ch10_figure_12_1736545678096045)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a quiz  Description automatically generated](assets/lelc_1012.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-12\. LLM-as-a-judge evaluator scoring criteria
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Save the evaluator and navigate back to the dataset details page. Moving forward,
    each subsequent experiment run from the dataset will be evaluated by the evaluator
    you configured.
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ranking LLM outputs by preference can be less cognitively demanding for human
    or LLM-as-a-judge evaluators. For example, assessing which output is more informative,
    specific, or safe. Pairwise evaluation compares two outputs simultaneously from
    different versions of an application to determine which version better meets evaluation
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangSmith natively supports running and visualizing pairwise LLM app generations,
    highlighting preference for one generation over another based on guidelines set
    by the pairwise evaluator. LangSmith’s pairwise evaluation enables you to do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a custom pairwise LLM-as-a-judge evaluator using any desired criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare two LLM generations using this evaluator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As per the LangSmith [docs](https://oreil.ly/ruFvy), you can use custom pairwise
    evaluators in the LangSmith SDK and visualize the results of pairwise evaluations
    in the LangSmith UI.
  prefs: []
  type: TYPE_NORMAL
- en: After creating an evaluation experiment, you can navigate to the Pairwise Experiments
    tab in the Datasets & Experiments section. The UI enables you to dive into each
    pairwise experiment, showing which LLM generation is preferred based upon our
    criteria. If you click the RANKED_PREFERENCE score under each answer, you can
    dive deeper into each evaluation trace (see [Figure 10-13](#ch10_figure_13_1736545678096064)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1013.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-13\. Pairwise experiment UI evaluation trace
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Regression Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In traditional software development, tests are expected to pass 100% based on
    functional requirements. This ensures stable behavior once the test is validated.
    In contrast, however, AI models’ output performances can vary significantly due
    to model *drift* (degradation due to changes in data distribution or updates to
    the model). As a result, testing AI applications may not always lead to a perfect
    score on the evaluation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This has several implications. First, it’s important to track results and performance
    of your tests over time to prevent regression of your app’s performance. *Regression*
    testing ensures that the latest updates or changes of the LLM model of your app
    do not *regress* (perform worse) relative to the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Second, it’s crucial to compare the individual data points between two or more
    experimental runs to see where the model got it right or wrong.
  prefs: []
  type: TYPE_NORMAL
- en: LangSmith’s comparison view has native support for regression testing, allowing
    you to quickly see examples that have changed relative to the baseline. Runs that
    regressed or improved are highlighted differently in the LangSmith dashboard (see
    [Figure 10-14](#ch10_figure_14_1736545678096104)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1014.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-14\. LangSmith’s experiments comparison view
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In LangSmith’s Comparing Experiments dashboard, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Compare multiple experiments and runs associated with a dataset. Aggregate stats
    of runs is useful for migrating models or prompts, which may result in performance
    improvements or regression on specific examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set a baseline run and compare it against prior app versions to detect unexpected
    regressions. If a regression occurs, you can isolate both the app version and
    the specific examples that contain performance changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drill into data points that behaved differently between compared experiments
    and runs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This regression testing is crucial to ensure that your application maintains
    high performance over time regardless of updates and LLM changes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered various preproduction testing strategies, let’s explore
    a specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating an Agent’s End-to-End Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although agents show a lot of promise in executing autonomous tasks and workflows,
    testing an agent’s performance can be challenging. In previous chapters, you learned
    how agents use tool calling with planning and memory to generate responses. In
    particular, tool calling enables the model to respond to a given prompt by generating
    a tool to invoke and the input arguments required to execute the tool.
  prefs: []
  type: TYPE_NORMAL
- en: Since agents use an LLM to decide the control flow of the application, each
    agent run can have significantly different outcomes. For example, different tools
    might be called, agents might get stuck in a loop, or the number of steps from
    start to finish can vary significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, agents should be tested at three different levels of granularity:'
  prefs: []
  type: TYPE_NORMAL
- en: Response
  prefs: []
  type: TYPE_NORMAL
- en: The agent’s final response to focus on the end-to-end performance. The inputs
    are a prompt and an optional list of tools, whereas the output is the final agent
    response.
  prefs: []
  type: TYPE_NORMAL
- en: Single step
  prefs: []
  type: TYPE_NORMAL
- en: Any single, important step of the agent to drill into specific tool calls or
    decisions. In this case, the output is a tool call.
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory
  prefs: []
  type: TYPE_NORMAL
- en: The full trajectory of the agent. In this case, the output is the list of tool
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-15](#ch10_figure_15_1736545678096126) illustrates these levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a tool call  Description automatically generated](assets/lelc_1015.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-15\. An example of an agentic app’s flow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s dive deeper into each of these three agent-testing granularities.
  prefs: []
  type: TYPE_NORMAL
- en: Testing an agent’s final response
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to assess the overall performance of an agent on a task, you can treat
    the agent as a black box and define success based on whether or not it completes
    the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing for the agent’s final response typically involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: User input and (optionally) predefined tools
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: Agent’s final response
  prefs: []
  type: TYPE_NORMAL
- en: Evaluator
  prefs: []
  type: TYPE_NORMAL
- en: LLM-as-a-judge
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this in a programmatic manner, first create a dataset that includes
    questions and expected answers from the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, as discussed earlier, we can utilize the LLM to compare the generated
    answer with the reference answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Testing a single step of an agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Testing an agent’s individual action or decision enables you to identify and
    analyze specifically where your application is underperforming. Testing for a
    single step of an agent involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: User input to a single step (for example, user prompt, set of tools). This can
    also include previously completed steps.
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: LLM response from the inputs step, which often contains tool calls indicating
    what action the agent should take next.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluator
  prefs: []
  type: TYPE_NORMAL
- en: Binary score for correct tool selection and heuristic assessment of the tool
    input’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example checks a specific tool call using a custom evaluator:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block implements these distinct components:'
  prefs: []
  type: TYPE_NORMAL
- en: Invoke the assistant, `assistant_runnable`, with a prompt and check if the resulting
    tool call is as expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize a specialized agent where the tools are hardcoded rather than passed
    with the dataset input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the reference tool call for the step that we are evaluating for `expected_tool_call`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing an agent’s trajectory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to look back on the steps an agent took in order to assess whether
    or not the trajectory lined up with expectations of the agent—that is, the number
    of steps or sequence of steps taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing an agent’s trajectory involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs: []
  type: TYPE_NORMAL
- en: User input and (optionally) predefined tools.
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: Expected sequence of tool calls or a list of tool calls in any order.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluator
  prefs: []
  type: TYPE_NORMAL
- en: Function over the steps taken. To test the outputs, you can look at an exact
    match binary score or metrics that focus on the number of incorrect steps. You’d
    need to evaluate the full agent’s trajectory against a reference trajectory and
    then compile as a set of messages to pass into the LLM-as-a-judge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example assesses the trajectory of tool calls using custom evaluators:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation example includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Invoking a precompiled LangGraph agent `graph.invoke` with a prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing a specialized agent where the tools are hardcoded rather than passed
    with the dataset input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting of the list of tools called using the function `find_tool_calls`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking if all expected tools are called in any order using the function `contains_all_tool_calls_any_order`
    or called in order using `contains_all_tool_calls_in_order`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking whether all expected tools are called in the exact order using `contains_all_tool_calls_in_order_exact_match`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three of these agent evaluation methods can be observed and debugged in
    LangSmith’s experimentation UI (see [Figure 10-16](#ch10_figure_16_1736545678096148)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1016.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-16\. Example of an agent evaluation test in the LangSmith UI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general, these tests are a solid starting point to help mitigate an agent’s
    cost and unreliability due to LLM invocations and variability in tool calling.
  prefs: []
  type: TYPE_NORMAL
- en: Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although testing in the preproduction phase is useful, certain bugs and edge
    cases may not emerge until your LLM application interacts with live users. These
    issues can affect latency, as well as the relevancy and accuracy of outputs. In
    addition, observability and the process of *online evaluation* can help ensure
    that there are guardrails for LLM inputs or outputs. These guardrails can provide
    much-needed protection from prompt injection and toxicity.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in this process is to set up LangSmith’s tracing feature.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *trace* is a series of steps that your application takes to go from input
    to output. LangSmith makes it easy to visualize, debug, and test each trace generated
    from your app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve installed the relevant LangChain and LLM dependencies, all you
    need to do is configure the tracing environment variables based on your LangSmith
    account credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: After the environment variables are set, no other code is required to enable
    tracing. Traces will be automatically logged to their specific project in the
    “Tracing projects” section of the LangSmith dashboard. The metrics provided include
    trace volume, success and failure rates, latency, token count and cost, and more—as
    shown in [Figure 10-17](#ch10_figure_17_1736545678096172).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1017.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-17\. An example of LangSmith’s trace performance metrics
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can review a variety of strategies to implement tracing based on your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Collect Feedback in Production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the preproduction phase, evaluators for production testing don’t have
    grounded reference responses for the LLM to compare against. Instead, evaluators
    need to score performance in real time as your application processes user inputs.
    This reference-free, real-time evaluation is often referred to as *online evaluation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are at least two types of feedback you can collect in production to improve
    app performance:'
  prefs: []
  type: TYPE_NORMAL
- en: Feedback from users
  prefs: []
  type: TYPE_NORMAL
- en: You can directly collect user feedback explicitly or implicitly. For example,
    giving users the ability to click a like and dislike button or provide detailed
    feedback based on the application’s output is an effective way to track user satisfaction.
    In LangSmith, you can attach user feedback to any trace or intermediate run (that
    is, span) of a trace, including annotating traces inline or reviewing runs together
    in an annotation queue.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback from LLM-as-a judge evaluators
  prefs: []
  type: TYPE_NORMAL
- en: As discussed previously, these evaluators can be implemented directly on traces
    to identify hallucination and toxic responses.
  prefs: []
  type: TYPE_NORMAL
- en: The earlier preproduction section already discussed how to set up LangSmith’s
    auto evaluation in the Datasets & Experiments section of the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Classification and Tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement effective guardrails against toxicity or gather insights
    on user sentiment analysis, we need to build an effective system for labeling
    user inputs and generated outputs.
  prefs: []
  type: TYPE_NORMAL
- en: This system is largely dependent on whether or not you have a dataset that contains
    reference labels. If you don’t have preset labels, you can use the LLM-as-a-judge
    evaluator to assist in performing classification and tagging based upon specified
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: If, however, ground truth classification labels are provided, then a custom
    heuristic evaluator can be used to score the chain’s output relative to the ground
    truth class labels.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and Fixing Errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once your application is in production, LangSmith’s tracing will catch errors
    and edge cases. You can add these errors into your test dataset for offline evaluation
    in order to prevent recurrences of the same issues.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful strategy is to release your app in phases to a small group of
    beta users before a larger audience can access its features. This will enable
    you to uncover crucial bugs, develop a solid evaluation dataset with ground truth
    references, and assess the general performance of the app including cost, latency,
    and quality of outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in this chapter, robust testing is crucial to ensure that your
    LLM application is accurate, reliable, fast, toxic-free, and cost-efficient. The
    three key stages of LLM app development create a data cycle that helps to ensure
    high performance throughout the lifetime of the application.
  prefs: []
  type: TYPE_NORMAL
- en: During the design phase, in-app error handling enables self-correction before
    the error reaches the user. Preproduction testing ensures each of your app’s updates
    avoids regression in performance metrics. Finally, production monitoring gathers
    real-time insights and application errors that inform the subsequent design process
    and the cycle repeats.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, this process of testing, evaluation, monitoring, and continuous
    improvement, will help you fix issues and iterate faster, and most importantly,
    deliver a product that users can trust to consistently deliver their desired results.
  prefs: []
  type: TYPE_NORMAL
