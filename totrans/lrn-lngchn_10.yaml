- en: 'Chapter 10\. Testing: Evaluation, Monitoring, and Continuous Improvement'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 9](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604),
    you learned how to deploy your AI application into production and utilize LangGraph
    Platform to host and debug your app.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Although your app can respond to user inputs and execute complex tasks, its
    underlying LLM is nondeterministic and prone to hallucination. As discussed in
    previous chapters, LLMs can generate inaccurate and outdated outputs due to a
    variety of reasons including the prompt, format of user’s input, and retrieved
    context. In addition, harmful or misleading LLM outputs can significantly damage
    a company’s brand and customer loyalty.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: To combat this tendency toward hallucination, you need to build an efficient
    system to test, evaluate, monitor, and continuously improve your LLM applications’
    performance. This robust testing process will enable you to quickly debug and
    fix AI-related issues before and after your app is in production.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to build an iterative testing system across
    the key stages of the LLM app development life-cycle and maintain high performance
    of your application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Testing Techniques Across the LLM App Development Cycle
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we construct the testing system, let’s briefly review how testing can
    be applied across the three key stages of LLM app development:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Design
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, LLM tests are applied directly to your application. These tests
    can be assertions executed at runtime that feed failures back to the LLM for self-correction.
    The purpose of testing at this stage is error handling within your app before
    it affects users.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Preproduction
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, tests are run right before deployment into production. The purpose
    of testing at this stage is to catch and fix any regressions before the app is
    released to real users.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Production
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, tests are run while your application is in production to help
    monitor and catch errors affecting real users. The purpose is to identify issues
    and feed them back into the design or preproduction phases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'The combination of testing across these stages creates a continuous improvement
    cycle where these steps are repeated: design, test, deploy, monitor, fix, and
    redesign. See [Figure 10-1](#ch10_figure_1_1736545678095728).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](assets/lelc_1001.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. The three key stages of the LLM app development cycle
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In essence, this cycle helps you to identify and fix production issues in an
    efficient and quick manner.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deeper into testing techniques across each of these stages.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The Design Stage: Self-Corrective RAG'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed previously, your application can incorporate error handling at
    runtime that feeds errors to the LLM for self-correction. Let’s explore a RAG
    use case using LangGraph as the framework to orchestrate error handling.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Basic RAG-driven AI applications are prone to hallucination due to inaccurate
    or incomplete retrieval of relevant context to generate outputs. But you can utilize
    an LLM to grade retrieval relevance and fix hallucination issues.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph enables you to effectively implement the control flow of this process,
    as shown in [Figure 10-2](#ch10_figure_2_1736545678095764).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_1002.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Self-corrective RAG control flow
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The control flow steps are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In the routing step, each question is routed to the relevant retrieval method,
    that is, vector store and web search.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If, for example, the question is routed to a vector store for retrieval, the
    LLM in the control flow will retrieve and grade the documents for relevancy.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the document is relevant, the LLM proceeds to generate an answer.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM will check the answer for hallucinations and only proceed to display
    the answer to the user if the output is accurate and relevant.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a fallback, if the retrieved document is irrelevant or the generated answer
    doesn’t answer the user’s question, the flow utilizes web search to retrieve relevant
    information as context.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process enables your app to iteratively generate answers, self-correct
    errors and hallucinations, and improve the quality of outputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run through an example code implementation of this control flow. First,
    download the required packages and initialize relevant API keys. For these examples,
    you’ll need to set your OpenAI and LangSmith API keys as environment variables.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll create an index of three blog posts:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*JavaScript*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As discussed previously, the LLM will grade the relevancy of the retrieved
    documents from the index. We can construct this instruction in a system prompt:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*JavaScript*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*The output:*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice the use of Pydantic/Zod to help model the binary decision output in a
    format that can be used to programmatically decide which node in the control flow
    to move toward.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: In LangSmith, you can see a trace of the logic flow across the nodes discussed
    previously (see [Figure 10-3](#ch10_figure_3_1736545678095796)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1003.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. LangSmith trace results
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s test to see what happens when the input question cannot be answered by
    the retrieved documents in the index.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: First, utilize LangGraph to make it easier to construct, execute, and debug
    the full control flow. See the full graph definition in the book’s [GitHub repository](https://oreil.ly/v63Vr).
    Notice that we’ve added a `transform_query` node to help rewrite the input query
    in a format that web search can use to retrieve higher-quality results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: As a final step, we set up our web search tool and execute the graph using the
    out-of-context question. The LangSmith trace shows that the web search tool was
    used as a fallback to retrieve relevant information prior to the final LLM generated
    answer (see [Figure 10-4](#ch10_figure_4_1736545678095829)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1004.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. LangSmith trace of self-corrective RAG utilizing web search as
    a fallback
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s move on to the next stage in LLM app testing: preproduction.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The Preproduction Stage
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of the preproduction stage of testing is to measure and evaluate
    the performance of your application prior to production. This will enable you
    to efficiently assess the accuracy, latency, and cost of utilizing the LLM.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Creating Datasets
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prior to testing, you need to define a set of scenarios you’d like to test and
    evaluate. A *dataset* is a collection of examples that provide inputs and expected
    outputs used to evaluate your LLM app.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'These are three common methods to build datasets for valuation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Manually curated examples
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: These are handwritten examples based on expected user inputs and ideal generated
    outputs. A small dataset consists of between 10 and 50 quality examples. Over
    time, more examples can be added to the dataset based on edge cases that emerge
    in production.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Application logs
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Once the application is in production, you can store real-time user inputs and
    later add them to the dataset. This will help ensure the dataset is realistic
    and covers the most common user questions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: These are artificially generated examples that simulate various scenarios and
    edge cases. This enables you to generate new inputs by sampling existing inputs,
    which is useful when you don’t have enough real data to test on.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: In LangSmith, you can create a new dataset by selecting Datasets and Testing
    in the sidebar and clicking the “+ New Dataset” button on the top right of the
    app, as shown in [Figure 10-5](#ch10_figure_5_1736545678095862).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: In the opened window, enter the relevant dataset details, including a name,
    description, and dataset type. If you’d like to use your own dataset, click the
    “Upload a CSV dataset” button.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1005.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Creating a new dataset in the LangSmith UI
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'LangSmith offers three different dataset types:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '`kv` (key-value) dataset'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '*Inputs* and *outputs* are represented as arbitrary key-value pairs.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `kv` dataset is the most versatile, and it is the default type. The `kv`
    dataset is suitable for a wide range of evaluation scenarios.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset type is ideal for evaluating chains and agents that require multiple
    inputs or generate multiple outputs.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm` (large language model) dataset'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: The `llm` dataset is designed for evaluating completion style language models.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs dictionary contains a single input key mapped to the prompt string.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs dictionary contains a single output key mapped to the corresponding
    response string.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset type simplifies evaluation for LLMs by providing a standardized
    format for inputs and outputs.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chat` dataset'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The `chat` dataset is designed for evaluating LLM structured chat messages as
    inputs and outputs.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *inputs* dictionary contains a single *input* key mapped to a list of serialized
    chat messages.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *outputs* dictionary contains a single *output* key mapped to a list of
    serialized chat messages.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset type is useful for evaluating conversational AI systems or chatbots.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most flexible option is the key-value data type (see [Figure 10-6](#ch10_figure_6_1736545678095893)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](assets/lelc_1006.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Selecting a dataset type in the LangSmith UI
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, add examples to the dataset by clicking Add Example. Provide the input
    and output examples as JSON objects, as shown in [Figure 10-7](#ch10_figure_7_1736545678095923).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![A white background with black lines  Description automatically generated](assets/lelc_1007.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Add key-value dataset examples in the LangSmith UI
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also define a schema for your dataset in the “Dataset schema” section,
    as shown in [Figure 10-8](#ch10_figure_8_1736545678095955).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1008.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. Adding a dataset schema in the LangSmith UI
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Defining Your Evaluation Criteria
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After creating your dataset, you need to define evaluation metrics to assess
    your application’s outputs before deploying into production. This batch evaluation
    on a predetermined test suite is often referred to as*offline evaluation***.**
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: For offline evaluation, you can optionally label expected outputs (that is,
    ground truth references) for the data points you are testing on. This enables
    you to compare your application’s response with the ground truth references, as
    shown in [Figure 10-9](#ch10_figure_9_1736545678095982).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of an application process  Description automatically generated](assets/lelc_1009.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. AI evaluation diagram
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are three main evaluators to score your LLM app performance:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluators
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: If you can’t express your testing requirements as code, you can use human feedback
    to express qualitative characteristics and label app responses with scores. LangSmith
    speeds up the process of collecting and incorporating human feedback with annotation
    queues.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Heuristic evaluators
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: These are hardcoded functions and assertions that perform computations to determine
    a score. You can use reference-free heuristics (for example, checking whether
    output is valid JSON) or reference-based heuristics such as accuracy. Reference-based
    evaluation compares an output to a predefined ground truth, whereas reference-free
    evaluation assesses qualitative characteristics without a ground truth. Custom
    heuristic evaluators are useful for code-generation tasks such as schema checking
    and unit testing with hardcoded evaluation logic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: LLM-as-a-judge evaluators
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: This evaluator integrates human grading rules into an LLM prompt to evaluate
    whether the output is correct relative to the reference answer supplied from the
    dataset output. As you iterate in preproduction, you’ll need to audit the scores
    and tune the LLM-as-a-judge to produce reliable scores.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: To get started with evaluation, start simple with heuristic evaluators. Then
    implement human evaluators before moving on to LLM-as-a-judge to automate your
    human review. This enables you to add depth and scale once your criteria are well-defined.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using LLM-as-a-judge evaluators, use straightforward prompts that can easily
    be replicated and understood by a human. For example, avoid asking an LLM to produce
    scores on a range of 0 to 10 with vague distinctions between scores.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-10](#ch10_figure_10_1736545678096002) illustrates LLM-as-a-judge
    evaluator in the context of a RAG use case. Note that the reference answer is
    the ground truth.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a brain  Description automatically generated](assets/lelc_1010.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. LLM-as-a-judge evaluator used in a RAG use case
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Improving LLM-as-a-judge evaluators performance
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using an LLM-as-a-judge is an effective method to grade natural language outputs
    from LLM applications. This involves passing the generated output to a separate
    LLM for judgment and evaluation. But how can you trust the results of LLM-as-a-judge
    evaluation?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Often, rounds of prompt engineering are required to improve accuracy, which
    is cumbersome and time-consuming. Fortunately, LangSmith provides a *few-shot*
    prompt solution whereby human corrections to LLM-as-a-judge outputs are stored
    as few-shot examples, which are then fed back into the prompt in future iterations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing few-shot learning, the LLM can improve accuracy and align outputs
    with human preferences by providing examples of correct behavior. This is especially
    useful when it’s difficult to construct instructions on how the LLM should behave
    or be formatted.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The few-shot evaluator follows these steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: The LLM evaluator provides feedback on generated outputs, assessing factors
    such as correctness, relevance, or other criteria.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It adds human corrections to modify or correct the LLM evaluator’s feedback
    in LangSmith. This is where human preferences and judgment are captured.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These corrections are stored as few-shot examples in LangSmith, with an option
    to leave explanations for corrections.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The few-shot examples are incorporated into future prompts as subsequent evaluation
    runs.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Over time, the few-shot evaluator will become increasingly aligned with human
    preferences. This self-improving mechanism reduces the need for time-consuming
    prompt engineering, while improving the accuracy and relevance of LLM-as-a-judge
    evaluations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how to easily set up the LLM-as-a-judge evaluator in LangSmith for offline
    evaluation. First, navigate to the “Datasets and Testing” section in the sidebar
    and select the dataset you want to configure the evaluator for. Click the Add
    Auto-Evaluator button at the top right of the dashboard to add an evaluator to
    the dataset. This will open a modal you can use to configure the evaluator.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Select the LLM-as-a-judge option and give your evaluator a name. You will now
    have the option to set an inline prompt or load a prompt from the prompt hub that
    will be used to evaluate the results of the runs in the experiment. For the sake
    of this example, choose the Create Few-Shot Evaluator option, as shown in [Figure 10-11](#ch10_figure_11_1736545678096023).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a survey  Description automatically generated](assets/lelc_1011.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 10-11\. LangSmith UI options for the LLM-as-a-judge evaluator
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This option will create a dataset that holds few-shot examples that will autopopulate
    when you make corrections on the evaluator feedback. The examples in this dataset
    will be inserted in the system prompt message.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify the scoring criteria in the Schema field and toggle between
    primitive types—for example, integer and Boolean (see [Figure 10-12](#ch10_figure_12_1736545678096045)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a quiz  Description automatically generated](assets/lelc_1012.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Figure 10-12\. LLM-as-a-judge evaluator scoring criteria
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Save the evaluator and navigate back to the dataset details page. Moving forward,
    each subsequent experiment run from the dataset will be evaluated by the evaluator
    you configured.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise evaluation
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ranking LLM outputs by preference can be less cognitively demanding for human
    or LLM-as-a-judge evaluators. For example, assessing which output is more informative,
    specific, or safe. Pairwise evaluation compares two outputs simultaneously from
    different versions of an application to determine which version better meets evaluation
    criteria.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'LangSmith natively supports running and visualizing pairwise LLM app generations,
    highlighting preference for one generation over another based on guidelines set
    by the pairwise evaluator. LangSmith’s pairwise evaluation enables you to do the
    following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Define a custom pairwise LLM-as-a-judge evaluator using any desired criteria
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare two LLM generations using this evaluator
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As per the LangSmith [docs](https://oreil.ly/ruFvy), you can use custom pairwise
    evaluators in the LangSmith SDK and visualize the results of pairwise evaluations
    in the LangSmith UI.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: After creating an evaluation experiment, you can navigate to the Pairwise Experiments
    tab in the Datasets & Experiments section. The UI enables you to dive into each
    pairwise experiment, showing which LLM generation is preferred based upon our
    criteria. If you click the RANKED_PREFERENCE score under each answer, you can
    dive deeper into each evaluation trace (see [Figure 10-13](#ch10_figure_13_1736545678096064)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1013.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 10-13\. Pairwise experiment UI evaluation trace
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Regression Testing
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In traditional software development, tests are expected to pass 100% based on
    functional requirements. This ensures stable behavior once the test is validated.
    In contrast, however, AI models’ output performances can vary significantly due
    to model *drift* (degradation due to changes in data distribution or updates to
    the model). As a result, testing AI applications may not always lead to a perfect
    score on the evaluation dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: This has several implications. First, it’s important to track results and performance
    of your tests over time to prevent regression of your app’s performance. *Regression*
    testing ensures that the latest updates or changes of the LLM model of your app
    do not *regress* (perform worse) relative to the baseline.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Second, it’s crucial to compare the individual data points between two or more
    experimental runs to see where the model got it right or wrong.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: LangSmith’s comparison view has native support for regression testing, allowing
    you to quickly see examples that have changed relative to the baseline. Runs that
    regressed or improved are highlighted differently in the LangSmith dashboard (see
    [Figure 10-14](#ch10_figure_14_1736545678096104)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1014.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: Figure 10-14\. LangSmith’s experiments comparison view
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In LangSmith’s Comparing Experiments dashboard, you can do the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Compare multiple experiments and runs associated with a dataset. Aggregate stats
    of runs is useful for migrating models or prompts, which may result in performance
    improvements or regression on specific examples.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set a baseline run and compare it against prior app versions to detect unexpected
    regressions. If a regression occurs, you can isolate both the app version and
    the specific examples that contain performance changes.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drill into data points that behaved differently between compared experiments
    and runs.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This regression testing is crucial to ensure that your application maintains
    high performance over time regardless of updates and LLM changes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered various preproduction testing strategies, let’s explore
    a specific use case.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating an Agent’s End-to-End Performance
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although agents show a lot of promise in executing autonomous tasks and workflows,
    testing an agent’s performance can be challenging. In previous chapters, you learned
    how agents use tool calling with planning and memory to generate responses. In
    particular, tool calling enables the model to respond to a given prompt by generating
    a tool to invoke and the input arguments required to execute the tool.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Since agents use an LLM to decide the control flow of the application, each
    agent run can have significantly different outcomes. For example, different tools
    might be called, agents might get stuck in a loop, or the number of steps from
    start to finish can vary significantly.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, agents should be tested at three different levels of granularity:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Response
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The agent’s final response to focus on the end-to-end performance. The inputs
    are a prompt and an optional list of tools, whereas the output is the final agent
    response.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Single step
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Any single, important step of the agent to drill into specific tool calls or
    decisions. In this case, the output is a tool call.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The full trajectory of the agent. In this case, the output is the list of tool
    calls.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-15](#ch10_figure_15_1736545678096126) illustrates these levels:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a tool call  Description automatically generated](assets/lelc_1015.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 10-15\. An example of an agentic app’s flow
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s dive deeper into each of these three agent-testing granularities.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Testing an agent’s final response
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to assess the overall performance of an agent on a task, you can treat
    the agent as a black box and define success based on whether or not it completes
    the task.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing for the agent’s final response typically involves the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: User input and (optionally) predefined tools
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Output
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Agent’s final response
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Evaluator
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: LLM-as-a-judge
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this in a programmatic manner, first create a dataset that includes
    questions and expected answers from the agent:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*JavaScript*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, as discussed earlier, we can utilize the LLM to compare the generated
    answer with the reference answer:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*JavaScript*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Testing a single step of an agent
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Testing an agent’s individual action or decision enables you to identify and
    analyze specifically where your application is underperforming. Testing for a
    single step of an agent involves the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: User input to a single step (for example, user prompt, set of tools). This can
    also include previously completed steps.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Output
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: LLM response from the inputs step, which often contains tool calls indicating
    what action the agent should take next.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Evaluator
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Binary score for correct tool selection and heuristic assessment of the tool
    input’s accuracy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example checks a specific tool call using a custom evaluator:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*JavaScript*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code block implements these distinct components:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Invoke the assistant, `assistant_runnable`, with a prompt and check if the resulting
    tool call is as expected.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize a specialized agent where the tools are hardcoded rather than passed
    with the dataset input.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the reference tool call for the step that we are evaluating for `expected_tool_call`.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing an agent’s trajectory
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to look back on the steps an agent took in order to assess whether
    or not the trajectory lined up with expectations of the agent—that is, the number
    of steps or sequence of steps taken.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing an agent’s trajectory involves the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: User input and (optionally) predefined tools.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Output
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Expected sequence of tool calls or a list of tool calls in any order.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Evaluator
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Function over the steps taken. To test the outputs, you can look at an exact
    match binary score or metrics that focus on the number of incorrect steps. You’d
    need to evaluate the full agent’s trajectory against a reference trajectory and
    then compile as a set of messages to pass into the LLM-as-a-judge.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example assesses the trajectory of tool calls using custom evaluators:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*JavaScript*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This implementation example includes the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Invoking a precompiled LangGraph agent `graph.invoke` with a prompt
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing a specialized agent where the tools are hardcoded rather than passed
    with the dataset input
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting of the list of tools called using the function `find_tool_calls`
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking if all expected tools are called in any order using the function `contains_all_tool_calls_any_order`
    or called in order using `contains_all_tool_calls_in_order`
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking whether all expected tools are called in the exact order using `contains_all_tool_calls_in_order_exact_match`
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three of these agent evaluation methods can be observed and debugged in
    LangSmith’s experimentation UI (see [Figure 10-16](#ch10_figure_16_1736545678096148)).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1016.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: Figure 10-16\. Example of an agent evaluation test in the LangSmith UI
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general, these tests are a solid starting point to help mitigate an agent’s
    cost and unreliability due to LLM invocations and variability in tool calling.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Production
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although testing in the preproduction phase is useful, certain bugs and edge
    cases may not emerge until your LLM application interacts with live users. These
    issues can affect latency, as well as the relevancy and accuracy of outputs. In
    addition, observability and the process of *online evaluation* can help ensure
    that there are guardrails for LLM inputs or outputs. These guardrails can provide
    much-needed protection from prompt injection and toxicity.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The first step in this process is to set up LangSmith’s tracing feature.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *trace* is a series of steps that your application takes to go from input
    to output. LangSmith makes it easy to visualize, debug, and test each trace generated
    from your app.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve installed the relevant LangChain and LLM dependencies, all you
    need to do is configure the tracing environment variables based on your LangSmith
    account credentials:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After the environment variables are set, no other code is required to enable
    tracing. Traces will be automatically logged to their specific project in the
    “Tracing projects” section of the LangSmith dashboard. The metrics provided include
    trace volume, success and failure rates, latency, token count and cost, and more—as
    shown in [Figure 10-17](#ch10_figure_17_1736545678096172).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/lelc_1017.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: Figure 10-17\. An example of LangSmith’s trace performance metrics
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can review a variety of strategies to implement tracing based on your needs.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Collect Feedback in Production
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the preproduction phase, evaluators for production testing don’t have
    grounded reference responses for the LLM to compare against. Instead, evaluators
    need to score performance in real time as your application processes user inputs.
    This reference-free, real-time evaluation is often referred to as *online evaluation*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'There are at least two types of feedback you can collect in production to improve
    app performance:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Feedback from users
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: You can directly collect user feedback explicitly or implicitly. For example,
    giving users the ability to click a like and dislike button or provide detailed
    feedback based on the application’s output is an effective way to track user satisfaction.
    In LangSmith, you can attach user feedback to any trace or intermediate run (that
    is, span) of a trace, including annotating traces inline or reviewing runs together
    in an annotation queue.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Feedback from LLM-as-a judge evaluators
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: As discussed previously, these evaluators can be implemented directly on traces
    to identify hallucination and toxic responses.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The earlier preproduction section already discussed how to set up LangSmith’s
    auto evaluation in the Datasets & Experiments section of the dashboard.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Classification and Tagging
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement effective guardrails against toxicity or gather insights
    on user sentiment analysis, we need to build an effective system for labeling
    user inputs and generated outputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: This system is largely dependent on whether or not you have a dataset that contains
    reference labels. If you don’t have preset labels, you can use the LLM-as-a-judge
    evaluator to assist in performing classification and tagging based upon specified
    criteria.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: If, however, ground truth classification labels are provided, then a custom
    heuristic evaluator can be used to score the chain’s output relative to the ground
    truth class labels.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and Fixing Errors
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once your application is in production, LangSmith’s tracing will catch errors
    and edge cases. You can add these errors into your test dataset for offline evaluation
    in order to prevent recurrences of the same issues.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Another useful strategy is to release your app in phases to a small group of
    beta users before a larger audience can access its features. This will enable
    you to uncover crucial bugs, develop a solid evaluation dataset with ground truth
    references, and assess the general performance of the app including cost, latency,
    and quality of outputs.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的策略是在更大范围的受众可以访问其功能之前，将你的应用分阶段发布给一小群测试用户。这将使你能够发现关键的错误，开发一个包含真实参考的可靠评估数据集，并评估应用的整体性能，包括成本、延迟和输出质量。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: As discussed in this chapter, robust testing is crucial to ensure that your
    LLM application is accurate, reliable, fast, toxic-free, and cost-efficient. The
    three key stages of LLM app development create a data cycle that helps to ensure
    high performance throughout the lifetime of the application.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章所述，稳健的测试对于确保你的LLM应用准确、可靠、快速、无毒性且成本效益至关重要。LLM应用开发的三个关键阶段创建了一个数据循环，有助于确保应用整个生命周期内的高性能。
- en: During the design phase, in-app error handling enables self-correction before
    the error reaches the user. Preproduction testing ensures each of your app’s updates
    avoids regression in performance metrics. Finally, production monitoring gathers
    real-time insights and application errors that inform the subsequent design process
    and the cycle repeats.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计阶段，应用内错误处理能够在错误到达用户之前进行自我纠正。预生产测试确保你的应用每个更新都不会在性能指标上出现回归。最后，生产监控收集实时洞察和应用程序错误，这些信息将指导后续的设计过程，并使循环重复。
- en: Ultimately, this process of testing, evaluation, monitoring, and continuous
    improvement, will help you fix issues and iterate faster, and most importantly,
    deliver a product that users can trust to consistently deliver their desired results.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这个测试、评估、监控和持续改进的过程将帮助你修复问题并更快地迭代，最重要的是，交付一个用户可以信赖的、能够持续提供预期结果的产物。
