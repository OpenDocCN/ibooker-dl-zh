- en: 4 Testing the DAG with causal constraints
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 使用因果约束测试DAG
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using d-separation to reason about how causality constrains conditional independence
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用d分离来推理因果关系如何约束条件独立性
- en: Using NetworkX and pgmpy to do d-separation analysis
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NetworkX和pgmpy进行d分离分析
- en: Refuting a causal DAG using conditional independence tests
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用条件独立性测试反驳因果DAG
- en: Refuting a causal DAG when there are latent variables
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在存在潜在变量时反驳因果DAG
- en: Using and applying causal discovery algorithm constraints
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用和应用因果发现算法约束
- en: Our causal DAG, or any causal model, captures a set of assumptions about the
    real world. Often, those assumptions are testable with data. If we test an assumption,
    and it turns out not to hold, then our causal model is wrong. In other words,
    our test has “falsified” or “refuted” our model. When this happens, we go back
    to the drawing board, come up with a better model, and try to refute it again.
    We repeat this loop until we get a model that is robust to our attempts to refute
    it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的因果有向无环图（DAG）或任何因果模型，都捕捉了关于现实世界的一组假设。通常，这些假设可以通过数据来检验。如果我们检验一个假设，结果发现它不成立，那么我们的因果模型就是错误的。换句话说，我们的检验“证伪”或“反驳”了我们的模型。当这种情况发生时，我们回到起点，提出一个更好的模型，并再次尝试反驳它。我们重复这个循环，直到我们得到一个能够抵抗我们反驳尝试的模型。
- en: In this chapter, we’ll focus on using statistical conditional independence-based
    testing to test our causal DAG. As you learn more about the assumptions we can
    pack into a causal model, and the inferences those assumptions allow you to make,
    you’ll learn new ways to test and refute your model. The workflow you’ll learn
    for running conditional independence tests in this chapter can be applied to new
    tests you may come up with.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于使用基于统计条件独立性的测试来检验我们的因果DAG。随着你更多地了解我们可以放入因果模型中的假设，以及这些假设允许你做出的推断，你将学会新的测试和反驳模型的方法。本章中你将学习的运行条件独立性测试的工作流程可以应用于你可能提出的新的测试。
- en: 4.1 How causality induces conditional independence
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 因果关系如何导致条件独立
- en: Causal relationships constrain the data in certain ways, one of which is by
    forcing variables to be conditionally independent. This forced conditional independence
    gives us a way to test our model with data using statistical tests for independence;
    if we find strong evidence that two variables are dependent when the DAG says
    they shouldn’t be, our DAG is wrong.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因果关系以某种方式约束数据，其中之一是通过迫使变量条件独立。这种强制条件独立性为我们提供了一种使用独立性统计测试来用数据检验模型的方法；如果我们发现当DAG表明它们不应该依赖时，两个变量之间存在强烈的依赖证据，那么我们的DAG就是错误的。
- en: In this chapter, we’ll test our causal DAG using these statistical independence
    tests, including independence tests on *functions* of observed variables that
    we can run when other variables are latent in the data. At the end, we’ll look
    at how these ideas enable *causal discovery algorithms* that try to learn the
    causal DAG directly from data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用这些统计独立性测试来测试我们的因果DAG，包括当数据中的其他变量是潜在变量时，我们可以运行的关于观测变量*函数*的独立性测试。最后，我们将探讨这些想法如何使*因果发现算法*能够直接从数据中学习因果DAG。
- en: But before that, let’s see how causality induces conditional independence. Consider
    again our blood type example, shown in figure 4.1\. Your father’s blood type is
    a direct cause of yours, and your paternal grandfather’s blood type is an indirect
    cause. Despite being a cause of your blood type, your paternal grandfather’s blood
    type is conditionally independent of your blood type, given your father’s.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但在那之前，让我们看看因果关系是如何导致条件独立的。再次考虑我们的血型示例，如图4.1所示。你父亲的血型是你血型的直接原因，而你父亲的父亲的血型是间接原因。尽管是导致你血型的原因，但你的父亲的父亲的血型在给定你父亲血型的情况下与你血型条件独立。
- en: '![figure](../Images/CH04_F01_Ness.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F01_Ness.png)'
- en: Figure 4.1 Causality induces conditional independence. Your blood type is conditionally
    independent of your paternal grandfather’s blood type (an indirect cause), given
    your father’s blood type (a direct cause).
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1 因果关系导致条件独立。你的血型在给定你父亲血型（直接原因）的情况下与你父亲的父亲的血型（间接原因）条件独立。
- en: We know this from causality; the parents’ blood types completely determine the
    blood type of the child. Your paternal grandfather’s and grandmother’s blood types
    completely determined your father’s blood type, but your father’s and mother’s
    blood types completely determined yours. Once we know your father’s blood type,
    there is nothing more your paternal grandfather’s blood type can tell us. In other
    words, your grandparent’s blood type is independent of yours, given your parents.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从因果关系中得知这一点；父母的血型完全决定了孩子的血型。你父亲的和外祖母的血型完全决定了你父亲的血型，但你的父亲和母亲的血型完全决定了你的血型。一旦我们知道了你父亲的血型，就没有更多你外祖父的血型可以告诉我们了。换句话说，你的祖父母的血型在你父母的情况下是独立的。
- en: 4.1.1 Colliders
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 碰撞器
- en: Now we’ll consider the *collider,* an interesting way in which causality induces
    cases of dependence between variables that are typically independent. Consider
    the canonical example in figure 4.2\. Whether the sprinkler is on or off, and
    whether it is raining or not, are causes of whether the grass is wet, but knowing
    that the sprinkler is off won’t help you predict whether it’s raining. In other
    words, the state of the sprinkler and whether it’s raining are independent. But
    when you know the grass is wet, also knowing that the sprinkler is off tells you
    it *must* be raining. So while the state of the sprinkler and the presence or
    absence of rain are independent, they become conditionally dependent, given the
    state of the grass.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来考虑*碰撞器*，这是一种因果如何导致通常独立的变量之间产生依赖关系的有趣方式。考虑图4.2中的典型例子。水龙头开启或关闭，以及是否下雨，是草地湿润与否的原因，但知道水龙头关闭并不能帮助你预测是否在下雨。换句话说，水龙头的状态和是否下雨是独立的。但是，当你知道草地是湿的，同时知道水龙头关闭就能告诉你*一定*是在下雨。所以，虽然水龙头的状态和下雨的有无是独立的，但在给定草地状态的情况下，它们变成了条件依赖的。
- en: '![figure](../Images/CH04_F02_Ness.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F02_Ness.png)'
- en: Figure 4.2 The sprinkler being on or off and whether or not it rains causes
    the grass to be wet or not. Knowing that the sprinkler is off won’t help you predict
    whether it’s raining—the sprinkler state and rain state are independent. But given
    that the grass is wet, knowing the sprinkler is off tells you it must be raining—the
    sprinkler state and rain state are conditionally dependent, given the state of
    the grass.
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2 水龙头开启或关闭以及是否下雨会导致草地湿润或不湿润。知道水龙头关闭并不能帮助你预测是否在下雨——水龙头状态和下雨状态是独立的。但是，如果知道草地是湿的，那么知道水龙头关闭就能告诉你一定是下雨了——在水龙头状态和下雨状态是条件依赖的，给定草地的状态。
- en: 'In this case “wet grass” is a *collide**r*: an effect with at least two independent
    causes. Colliders are interesting because they illustrate how causal variables
    can be independent but then become dependent if we condition on a shared effect
    variable. In conditional independence terms, the parent causes are independent
    (sprinkler ⊥ rain) but become dependent after we observe (condition on) the child
    (sprinkler ⟂̷ rain | wet grass).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，“湿草地”是一个*碰撞器*：至少有两个独立原因的效果。碰撞器之所以有趣，是因为它们说明了因果变量可以是独立的，但如果我们基于一个共享的效果变量进行条件化，它们就会变得依赖。在条件独立术语中，父母的因果是独立的（水龙头
    ⊥ 雨），但在观察到孩子（基于孩子进行条件化）之后，它们变得依赖（水龙头 ⟂̷ 雨 | 湿草地）。
- en: For another example, let’s look at blood type again, as shown in figure 4.3.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，让我们再次看看血型，如图4.3所示。
- en: '![figure](../Images/CH04_F03_Ness.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F03_Ness.png)'
- en: Figure 4.3 Mothers and fathers are usually unrelated, so knowing mother’s blood
    type can’t help predict the father’s blood type. But if we know the mother’s blood
    type and the child’s blood type, it narrows down the possible blood types of the
    father.
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3 母亲和父亲通常是无关的，所以知道母亲的血型并不能帮助预测父亲的血型。但是，如果我们知道母亲的血型和孩子的血型，就能缩小父亲可能的血型范围。
- en: If we assume the mother and father are unrelated, the mother’s blood type tells
    us nothing about the father’s blood type—(mother’s blood type ⊥ father’s blood
    type). But suppose we know the child’s blood type is B. Does that help us use
    the mother’s blood type to predict the father’s blood type?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设母亲和父亲没有关系，母亲的血型并不能告诉我们父亲的血型——(母亲的血型 ⊥ 父亲的血型)。但是，如果我们知道孩子的血型是B型，这能帮助我们使用母亲的血型来预测父亲的血型吗？
- en: To answer this, examine the standard blood type table in figure 4.4\. We see
    that if mother has blood type A and the child has blood type B, then possibly
    blood types for the father are B and AB.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，请查看图4.4中的标准血型表。我们看到，如果母亲血型是A型，孩子血型是B型，那么父亲可能的血型可能是B型和AB型。
- en: '![figure](../Images/CH04_F04_Ness.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F04_Ness.png)'
- en: Figure 4.4 Knowing the mother’s blood type can help you narrow down the father’s
    blood type if you know the child’s blood type.
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4 知道母亲的血型可以帮助你在知道孩子的血型的情况下缩小父亲的血型范围。
- en: Knowing the mother’s blood type alone doesn’t tell us anything about the father’s
    blood type. But if we add information about the child’s blood type (the collider),
    we can narrow down the father’s blood type from four to two possibilities. In
    other words, (mother’s blood type ⊥ father’s blood type), but the mother’s and
    father’s blood type become dependent once we condition on the child’s blood type.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 仅知道母亲的血型并不能告诉我们关于父亲血型的任何信息。但如果我们添加关于孩子血型（碰撞体）的信息，我们可以将父亲的血型从四种可能性缩小到两种。换句话说，(母亲的血型
    ⊥ 父亲的血型)，但一旦我们对孩子的血型进行条件化，母亲和父亲的血型就会变得相关。
- en: Colliders show up in various parts of causal inference. In section 4.6, we’ll
    see that colliders are important in the task of causal discovery, where we try
    to learn a causal DAG from data. When we look at causal effects in chapters 7
    and 11, we’ll see how accidentally “adjusting for” colliders can introduce unwanted
    “collider bias” when inferring causal effects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 碰撞体出现在因果推断的各个部分。在第4.6节中，我们将看到碰撞体在因果发现任务中的重要性，我们试图从数据中学习一个因果DAG。当我们查看第7章和第11章中的因果效应时，我们将看到在推断因果效应时意外“调整”碰撞体如何引入不受欢迎的“碰撞体偏差”。
- en: For now, we’ll note that colliders can be at odds with our statistical intuition,
    because they describe how causal logic leads to situations where two things are
    independent but “suddenly” become dependent when you condition on a third or more
    variables.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们只需注意，碰撞体可能与我们的统计直觉相矛盾，因为它们描述了因果逻辑如何导致两种事物独立，但在对第三个或更多变量进行条件化时“突然”变得相关。
- en: 4.1.2 Abstracting independence with a causal graph
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 使用因果图抽象独立性
- en: '![figure](../Images/CH04_F05_Ness.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F05_Ness.png)'
- en: Figure 4.5 In causal effect inference, we are interested in statistically quantifying
    how much a cause (treatment) affects an effect (outcome). *Confounders* are common
    causes that are a source of non-causal correlation between treatment and outcome.
    Causal effect inference requires “adjusting” for confounders. D-separation is
    the backbone of the theory that tells us how.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5 在因果效应推断中，我们感兴趣的是在统计上量化一个原因（治疗）对效应（结果）的影响程度。*混杂因素*是常见的共同原因，是治疗和结果之间非因果相关性的来源。因果效应推断需要“调整”混杂因素。D分离是告诉我们如何做到这一点的理论的核心。
- en: In the previous section, we used the basic rules of blood type heredity to show
    how causality induces conditional independence. If we want to write code that
    can help us make causal inferences across different domains, we’ll need an abstraction
    for mapping causal relationships to conditional independence that doesn’t rely
    on the rules of a particular domain. “D-separation” solves this problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了血型遗传的基本规则来展示因果关系如何导致条件独立性。如果我们想要编写能够帮助我们跨不同领域进行因果推断的代码，我们需要一个将因果关系映射到条件独立性的抽象，这个抽象不依赖于特定领域的规则。“D分离”解决了这个问题。
- en: '*D-separation* and *d-connection* refer to how we use graphs to reason about
    conditional independence. The concepts are novel at first glance, but they will
    be some of your most important tools for graph-based causal reasoning. As a bit
    of a spoiler for chapter 7, consider the problem of causal effect inference, illustrated
    in figure 4.5\. In causal inference, you are interested in statistically quantifying
    how much a cause (often called a “treatment”) affects an effect (an “outcome”).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*D分离*和*d-连接*指的是我们如何使用图来推理条件独立性。这些概念乍一看是新颖的，但它们将成为你在基于图的因果推理中最重要的工具之一。作为第7章的一个小小的预告，考虑一下图4.5中展示的因果效应推断问题。在因果推断中，你感兴趣的是在统计上量化一个原因（通常称为“治疗”）对效应（“结果”）的影响程度。'
- en: As you saw in chapter 3, you can describe variables in a DAG in terms of their
    role in a causal inference task. One role in the task of causal effect inference
    is the *confounder*. Confounders are common causes that are a source of non-causal
    correlation between the treatment and the effect. To estimate the causal effect
    of the treatment on the outcome, we have to “adjust” for the confounder. The theoretical
    justification for doing so is based on “d-separating” the path {treatment ← confounder
    → outcome} and zooming in on the path {treatment → outcome}.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第3章中看到的，你可以用因果推断任务中变量的角色来描述DAG中的变量。在因果效应推断的任务中，一个角色是*混杂因素*。混杂因素是治疗和效果之间非因果相关性的共同原因。为了估计治疗对结果的影响，我们必须“调整”混杂因素。这样做理论上的依据是基于“d分离”路径{治疗
    ← 混杂因素 → 结果}并聚焦于路径{治疗 → 结果}。
- en: 4.2 D-separation and conditional independence
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 D分离和条件独立性
- en: 'Recall the following ideas from previous chapters:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下前几章中的以下思想：
- en: A causal DAG is a model of the data generating process (DGP).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果DAG是数据生成过程（DGP）的一个模型。
- en: The DGP entails a joint probability distribution.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DGP包含一个联合概率分布。
- en: Causal relationships induce independence and conditional independence between
    variables in the joint probability distribution.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果关系在联合概率分布中诱导变量之间的独立性和条件独立性。
- en: D-separation and d-connection are graphical abstractions for reasoning about
    the conditional independence in the joint probability distribution that a causal
    DAG models. The concept refers to nodes and paths between nodes in the causal
    DAG; the nodes and paths are “d-connected” or “d-separated,” where the “d” stands
    for “directional.” The idea is for a statement like “these nodes are d-separated
    in the graph” to correspond to a statement like “these variables are conditionally
    independent.” D-separation is not about stating what causes what; it is about
    whether paths between variables in the DAG indicate the absence or presence of
    dependence between those variables in the joint probability distribution.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: D分离和d连接是用于推理因果DAG模型中联合概率分布的条件的独立性的图形抽象。这个概念指的是因果DAG中的节点和节点之间的路径；节点和路径是“d连接”或“d分离”的，其中“d”代表“方向”。这个想法是使一个像“这些节点在图中是d分离的”这样的陈述与一个像“这些变量在条件独立下”这样的陈述相对应。D分离不是关于陈述什么导致什么的；它是关于DAG中变量之间的路径是否表明了在联合概率分布中这些变量之间是否存在依赖。
- en: We want to make this correspondence because reasoning about graphs is easier
    than reasoning about probability distributions directly; tracing paths between
    nodes is easier than taking graduate-level classes in probability theory. Also,
    recall from chapter 2 that graphs are fundamental to algorithms and data structures,
    and that statistical modeling benefits from making conditional independence assumptions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要建立这种对应关系，因为推理图形比直接推理概率分布更容易；追踪节点之间的路径比上概率理论的高级课程更容易。此外，回想一下第2章的内容，图形是算法和数据结构的基础，统计建模从做出条件独立性假设中受益。
- en: '4.2.1 D-separation: A gateway to simplified causal analysis'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 D分离：简化因果分析的门户
- en: Suppose we have a statement that *U* and *V* are conditionally independent given
    *Z* (i.e., *U*⊥*V*|*Z*). Our task is to define a corresponding statement purely
    in graphical terms. We’ll write this statement as *U*⊥[*G*]*V*|*Z* and read it
    as “*U* and *V* are d-separated by *Z* in graph *G.*”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个陈述，即*U*和*V*在给定*Z*的情况下是条件独立的（即*U*⊥*V*|*Z*）。我们的任务是定义一个纯粹以图形术语相对应的陈述。我们将这个陈述写成*U*⊥[*G*]*V*|*Z*，并读作“*U*和*V*在图*G*中由*Z*进行d分离。”
- en: Let *Z* represent a set of nodes called the d-separating set or “blockers.”
    In terms of conditional independence, *Z* corresponds to a set of variables we
    condition on. Our goal is to define d-separation such that the nodes in *Z* in
    some sense “block” the dependence between *U* and *V* that is implied by the causal
    structure of our DAG.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 用*Z*代表一组称为d分离集或“阻塞者”的节点。在条件独立性的术语中，*Z*对应于一组我们对其条件化的变量。我们的目标是定义d分离，使得*Z*中的节点在某种意义上“阻塞”了由我们的DAG的因果结构所暗示的*U*和*V*之间的依赖。
- en: Next, let *P* be a *path*, meaning a series of connected edges (and nodes) between
    two nodes. It does not matter if the nodes on the paths are observed or not in
    your data (we’ll see how the data factors in later). Our definition of “path”
    does not depend on the orientation of the edges; for example, {*x* → *y* → *z*},
    {*x* ← *y* → *z*}, {*x* ← *y* ← *z*}, and {*x* → *y* ← *z*} are all paths between
    *x* and *z*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让*P*成为一个*路径*，意味着两个节点之间的一系列连接边（和节点）。路径上的节点是否在你的数据中被观察到并不重要（我们将在后面看到数据是如何起作用的）。我们“路径”的定义不依赖于边的方向；例如，{*x*
    → *y* → *z*}，{*x* ← *y* → *z*}，{*x* ← *y* ← *z*}和{*x* → *y* ← *z*}都是*x*和*z*之间的路径。
- en: Finally, let’s revisit the collider. A collider structure refers to a motif
    like *x* → *y* ← *z* where the middle node *y* (the collider) has incoming edges.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们重新审视碰撞器。碰撞器结构指的是像*x* → *y* ← *z*这样的模式，其中中间节点*y*（碰撞器）有入边。
- en: We’ll define d-separation now. First, two nodes *u* and *v* are said to be d-separated
    (blocked) by *Z* if all *paths* between them are d-separated by *Z*. If any of
    those paths between *u* and *v* are not d-separated, then *u* and *v* are d-connected.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义d分离。首先，如果两个节点*u*和*v*之间的所有*路径*都被*Z*分离，那么这两个节点*u*和*v*被认为是d分离（阻塞）的。如果*u*和*v*之间的任何路径没有被d分离，那么*u*和*v*是d连接的。
- en: Let’s define d-separation for a path. A path *P* is d-separated by node set
    *Z* if any of four criteria are met.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义路径的d分离。一个路径*P*如果满足以下四个标准之一，则被认为是被节点集*Z*分离的。
- en: '*P* contains a chain, *i* → *m* → *j*, such that the middle node *m* is in
    *Z.*'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*P*包含一个链*i* → *m* → *j*，其中中间节点*m*在*Z*中。'
- en: '*P* contains a chain, *i* ← *m* ← *j*, such that the middle node *m* is in
    *Z.*'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*P*包含一个链*i* ← *m* ← *j*，其中中间节点*m*在*Z*中。'
- en: '*P* contains a child-parent-child structure *i* ← *m* → *j*, such that the
    middle (parent) node *m* is in *Z.*'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*P*包含一个子父子结构*i* ← *m* → *j*，其中中间（父）节点*m*在*Z*中。'
- en: Let’s pause. Criteria 1–3 are just walking through the ways we can orient edges
    between three nodes. If this keeps up, then *P* is always d-separated if a node
    on *P* is in set *Z*. That would be nice, because it would mean that two nodes
    are d-connected (i.e., dependent) if there are any paths between them in the DAG,
    and they are d-separated if all those paths are blocked by nodes in set *Z*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下。标准1-3只是通过三种节点之间边的方向来分析。如果这样继续下去，那么如果*P*上的一个节点在集合*Z*中，那么*P*总是d分离的。那将会很棒，因为这将意味着如果DAG中存在任何路径，那么两个节点就是d连接的（即，相关的），如果所有这些路径都被集合*Z*中的节点阻塞，那么它们就是d分离的。
- en: 'Unfortunately, colliders make the fourth criterion contrary to the others:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，碰撞器使得第四个标准与其他标准相反：
- en: 4\. *P* contains a *collider* *structure*, *i* → *m* ← *j*, such that the middle
    node *m* is not in *Z,* and no descendant of *m* is in *Z.*
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. *P*包含一个*碰撞器*结构*i* → *m* ← *j*，其中中间节点*m*不在*Z*中，并且*m*的任何后代都不在*Z*中。
- en: This fourth criterion is how d-separation captures the way two independent (d-separated)
    items can become dependent when conditioning on a collider.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这第四个标准是d分离如何捕捉两个独立（d分离）的项目在条件化一个碰撞器时如何变得相关。
- en: 'Many writers conflate d-separation and conditional independence. Keep the distinction
    clear in your mind: ⊥*[G]* speaks of graphs, whereas ⊥ speaks of distributions.
    It matters because, as you’ll see later in this chapter, we’ll use d-separation
    to test our causal assumptions against statistical evidence of conditional independence
    in the data.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 许多作家混淆了d分离和条件独立性。请清晰地记住这种区别：⊥*[G]*指的是图，而⊥指的是分布。这很重要，因为正如你将在本章后面看到的，我们将使用d分离来测试我们的因果假设与数据中条件独立性的统计证据。
- en: '![figure](../Images/CH04_F06_Ness.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F06_Ness.png)'
- en: Figure 4.6 Does the set {*m*, *k*} d-separate path *u* → *i* → *m* → *j* → *v*?
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.6集合{*m*, *k*}是否d分离路径*u* → *i* → *m* → *j* → *v*？
- en: Let’s work through a few examples.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几个例子来分析。
- en: Example with chain i → m → j
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 以链i → m → j为例
- en: Consider the DAG in figure 4.6, where *P* is *u* → *i* → *m* → *j* → *v*. This
    path is d-connected by default. Now let *Z* be the set {*m*, *k*}. *P* contains
    a chain *i* → *m* → *j*, and *m* is in *Z*. If we block on *Z*, the first criterion
    is satisfied, and *u* and *v* are d-separated.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图4.6中的DAG，其中*P*是*u* → *i* → *m* → *j* → *v*。这条路径默认是d连接的。现在让*Z*是集合{*m*, *k*}。*P*包含一个链*i*
    → *m* → *j*，并且*m*在*Z*中。如果我们阻塞在*Z*上，第一个标准就得到了满足，并且*u*和*v*是d分离的。
- en: For some (but not all), a helpful analogy for understanding d-separation is
    an electronic circuit. Paths without colliders are d-connected and are like closed
    circuits, where electrical current flows uninhibited. “Blocking” on a node on
    that path d-separates the path and will “break the circuit” so current can’t flow.
    Blocking on *Z* (specifically, blocking on *m*, which is in *Z*) “breaks the circuit”
    as shown in figure 4.7.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F07_Ness.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 The path is d-connected by default, but blocking on m **∈** *Z* d-separates
    the path and figuratively breaks the circuit (“**∈**” means “in”).
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Example with chain i ← m → j
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now consider the DAG in figure 4.8, where *P* is *u* ← *i* ← *m* → *j* → *v*.
    This path is also d-connected by default. Note that d-connection can go against
    the grain of causality. In figure 4.7, the d-connected path from *u* to *v* takes
    steps in the direction of causality: *u* to *i* (*u* ← *i*), then *i* to *m* (*i*
    ← *m*), then *m* to *j* (*m* → *j*), and then *j* to *v* (*j* → *v*). But here,
    we have two *anticausal* (meaning against the direction of causality) steps, namely
    the step from *u* to *i* (*u* ← *i*) and *i* to *m* (*i* ← *m*).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F08_Ness.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 Does the set {*m*} d-separate path *u* ← *i* ← *m* → *j* → *v*?
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Suppose we block on set *Z*, and *Z* contains only the node *m*. Then condition
    3 is satisfied and the path is d-separated, as illustrated in figure 4.9.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F09_Ness.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 This path from *u* to *v* is also d-connected by default, even though
    it has some steps (*u* to *i* and *i* to *m*) that go against the direction of
    causality. Again, blocking on *m* **∈** *Z* d-separates the path and figuratively
    breaks the circuit.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Colliders make d-separation weird
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The fourth criterion focuses on the collider motif *i* → *m* ← *j*: *P* contains
    a *collider* *structure*, *i* → *m* ← *j*, such that the middle node *m* is not
    in *Z,* and no descendant of *m* is in *Z.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Let’s relate this back to our blood type example. Here *i* and *j* are the parents’
    blood types and *m* is the child’s blood type. We saw that colliders are a bit
    odd, because conditioning on the collider (the child’s blood type) induces dependence
    between two independent things (like the parents’ blood types). This oddness makes
    d-separation a bit tricky to understand at first glance. Figure 4.10 illustrates
    how colliders affect d-separation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F10_Ness.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 Colliders make d-connection tricky. Given a node *m* on a path,
    if *m* is not a collider, the path is d-connected by default and d-separated when
    you block on *m*. If *m* is a collider, the path is d-separated by default and
    d-connected when you block on *m*.
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following is true of colliders:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: All paths between two nodes d-connect by default *unless that path has a collider
    motif*. A path with a collider is d-separated by default.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocking with any node on a d-connected path will d-separate that path *unless
    that node is a collider*. Blocking on a collider will d-connect a path by default,
    as
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在d连接路径上的任何节点进行阻塞都将d分离该路径 *除非该节点是碰撞节点*。在碰撞节点上进行阻塞将默认情况下通过d连接路径，如下所示
- en: will blocking with a descendant of that collider.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将阻塞应用于该碰撞的后代。
- en: '![figure](../Images/CH04_F11_Ness.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F11_Ness.png)'
- en: Figure 4.11 Does the set {*m*} (or {*k*} or {*m*, *k*}) d-separate path *u*
    → *i* → *m* ← *j* → *v*?
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.11 集合{*m*}（或{*k*}或{*m*, *k*}）是否d分离路径 *u* → *i* → *m* ← *j* → *v*？
- en: In terms of the circuit analogy, colliders are like an open switch, which prevents
    current flow in an electronic circuit. When a path has a collider, the collider
    stops all current from passing through it. Colliders break the circuit. Blocking
    on a collider is like closing the switch, and the current that couldn’t pass through
    before now can pass through (d-connection).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在电路类比中，碰撞节点类似于一个开开关，它阻止电子电路中的电流流动。当路径有一个碰撞节点时，该节点会阻止所有电流通过。碰撞节点会中断电路。在碰撞节点上进行阻塞就像关闭开关一样，之前无法通过的电流现在可以通过了（d连接）。
- en: In the DAG in figure 4.11, is the path *u* → *i* → *m* ← *j* → *v* d-connected
    by default? No, because the path contains a collider structure *m* (*i* → *m*
    ← *j*).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.11的DAG中，路径 *u* → *i* → *m* ← *j* → *v* 是否默认情况下是d连接的？不，因为该路径包含一个碰撞结构 *m*
    (*i* → *m* ← *j*）。
- en: Now consider what would happen if the blocking set *Z* included *m*. In this
    case, condition 4 is violated and the path *becomes d-connected*, as in figure
    4.12\.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑如果阻塞集 *Z* 包含 *m* 会发生什么。在这种情况下，条件4被违反，路径 *变为d连接*，如图4.12所示。
- en: '![figure](../Images/CH04_F12_Ness.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F12_Ness.png)'
- en: Figure 4.12 This path from *u* to *v* is d-separated by default because it contains
    a collider *m*. The collider is analogous to an open circuit. Blocking on *m*
    or any of its descendants d-connects the path and figuratively closes the circuit.
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.12 这个从 *u* 到 *v* 的路径默认情况下是d分离的，因为它包含一个碰撞节点 *m*。碰撞节点类似于一个开路。在 *m* 或其任何后代上进行阻塞将连接路径，并形象地闭合电路。
- en: The path would also become d-connected if *Z* didn’t have *m* but just had *k*
    (or if *Z* included both *m* and *k*). Blocking on a descendant of a collider
    d-connects in the same manner as blocking on a collider.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *Z* 没有包含 *m* 但只包含 *k*（或者如果 *Z* 包含了 *m* 和 *k*），路径也会变为d连接。在碰撞节点的后代上进行阻塞与在碰撞节点上进行阻塞的方式相同。
- en: Can you guess why? It’s because the collider’s descendant is *d-connected to
    the collider*. In causal terms, we saw how, given a mother’s blood type, observing
    the child’s blood type (the collider) might reveal the father’s blood type. Suppose
    that if instead of observing the child’s blood type, we observed the child’s child’s
    blood type (call it the grandchild’s blood type). That grandchild’s blood type
    could help narrow down the child’s blood type and thus narrow down the father’s
    blood type. In other words, if the mother’s and father’s blood types are dependent,
    given the child’s blood type, and the grandchild’s blood type gives you information
    about the child’s blood type, then the mother’s and father’s blood types are dependent
    given the grandchild’s blood type.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜到为什么吗？这是因为碰撞的后代与碰撞节点 *d连接*。在因果术语中，我们看到了给定母亲的血型，观察孩子的血型（碰撞节点）可能会揭示父亲的血型。假设如果我们不是观察孩子的血型，而是观察孩子的孩子的血型（称为孙子的血型）。这个孙子的血型可以帮助缩小孩子的血型，从而缩小父亲的血型。换句话说，如果母亲和父亲的血型在孩子的血型给定的情况下是相关的，并且孙子的血型为你提供了关于孩子血型的信息，那么在孙子的血型给定的情况下，母亲和父亲的血型也是相关的。
- en: D-separation and sets of nodes
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D分离和节点集
- en: D-separation doesn’t just apply to pairs of nodes, it applies to pairs of sets
    of nodes. In the notation *u*⊥*v*|*Z*, *Z* can be a set of blockers, and *u* and
    *v* can be sets as well. We d-separate two sets by blocking all d-connected paths
    between members of each set. Other graph-based causal ideas, such as the do-calculus,
    also generalize to sets of nodes. If you remember that fact, we can build intuition
    on individual nodes, and that intuition will generalize to sets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: D分离不仅适用于节点对，也适用于节点集对。在符号 *u*⊥*v*|*Z* 中，*Z* 可以是阻塞集，而 *u* 和 *v* 也可以是集。我们通过阻塞每个集合成员之间的所有d连接路径来d分离两个集合。其他基于图的因果概念，如do-calculus，也推广到节点集。如果你记住这个事实，我们就可以在单个节点上建立直觉，而这种直觉可以推广到集合。
- en: When the blocking set *Z* is the singleton set {*m*}, this set is sufficient
    to block the paths *u* → *i* → *m* → *j* → *v* in figure 4.7 and *u* ← *i* ← *m*
    → *j* → *v* in figure 4.8\. Altogether, the sets {*i*}, {*m*}, {*j*}, {*i*, *m*},
    {*i*, *j*}, {*m*, *j*}, and {*i*, *m*, *j*} all d-separate *u* and *v* on these
    two paths. However, {*i*}, {*m*}, and {*j*} are the *minimal d-separating sets*,
    meaning that all the other d-separating sets include at least one of these sets.
    The minimal d-separation sets are sufficient to d-separate the two nodes. When
    reasoning about d-separation and when implementing it in algorithms, we want to
    focus on finding minimal d-separating sets; if *U*⊥*V*|*Z* and *U*⊥*V*|*Z*, *W*
    are both true, we don’t want to waste effort on *U*⊥*V*|*Z*, *W*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Examples of d-separating multiple paths
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we want to d-separate two nodes. Often there are multiple d-connected
    paths between those nodes. To d-separate those nodes, we need to find blockers
    that d-separate each of those paths. Let’s walk through some examples.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Finding a minimal d-separating set
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a bigger graph with more edges, the number of paths between two nodes can
    be quite large. But often longer paths often get blocked as a side-effect of blocking
    shorter paths. So we can start with shorter paths, and work our way to longer
    paths that haven’t been blocked yet, until no unblocked paths remain.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: For example, *U* and *V* are d-connected in figure 4.13\. What sets of nodes
    are fully required to d-separate them?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 4.13, *U* and *V* are d-connected through these paths:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '*U* → *I* → *V*'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* → *J* → *V*'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* → *J* → *I* → *V*'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we can d-separate *U* → *I* → *V* by blocking on *I*. Then, we d-separate
    *U* → *J* → *V* *by blo*cking on *J*. At this point, we see that our blocking
    set {*I*, *J*} already d-separates *U* → *J* → *I* → *V*, so we are done.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: In another example, how do we d-separate *U* and *V* in figure 4.14?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F13_Ness.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 We can d-separate *U* and *V* with {*I*, *J*}.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F14_Ness.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 We can d-separate *U* and *V* with sets {*I*, *M*, *K*, *J*} or
    {*I*, *M*, *K*, *L*}.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are many paths between *U* and *V*. Let’s first enumerate three of the
    shortest paths:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '*U* ← *I* → *V*'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* ← *M* → *V*'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* ← *K* → *V*'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll need to block on at least on {*I*, *M*, *K*} to d-separate these three
    paths. Note that *U* has another parent *J*, and there are several paths from
    *U* to *V* through *J*, but there are only two paths we haven’t already d-separated;
    *U* ← *J* → *L* → *V* and *U* ← *J* → *K* ← *L* → *V*. Both *J* and *L* will block
    these paths, so we could d-separate *U* and V with minimal sets {*I*, *M*, *K*,
    *J*} or {*I*, *M*, *K*, *L*}. Note that *U* ← *J* → *K* ← *L* → *V* was d-connected
    because we initially added *K*, a collider on this path, to our blocking set.
    Next, we look at another example of this phenomenon.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: When d-separating one path d-connects another
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When you attempt to d-separate a path between *U* and *V* by blocking on a node
    that is a collider on another path, you potentially d-connect that other path.
    That is fine, as long as you take additional steps to d-separate that path as
    well. To illustrate, consider the graph in figure 4.15\. This graph is simple
    enough that we can enumerate all of the paths.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F15_Ness.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 Blocking with *M* will block the path *U* ← *M* → *V* but would
    d-connect the path *U* ← *I* → *M* ← *J* → *V* because *M* is a collider between
    *I* and *J*. So we need to additionally block on either *I* or *J* to d-separate
    *U* ← *I* → *M* ← *J* → *V*.
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s start with the three d-connecting paths:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '*U* ← *M* → *V*'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* ← *I* → *M* → *V*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* ← *M* ← *J* → *V*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also have a path *U* ← *I* → *M* ← *J* → *V*, but that is not a d-connecting
    path because *M* is a collider on that path.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to block all three of these d-connected paths with one node
    is to block on *M*. However, if we block on that collider, the path *U* ← *I*
    → *M* ← *J* → *V* d-connects. So we need to additionally block on *I* or *J*.
    In other words, our minimal d-separating sets are {*I*, *M*} and {*J*, *M*}.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 D-separation in code
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don’t fret if you are still hazy on d-separation. We’ve defined four criteria
    for describing paths between nodes on a graph, which is just the sort of thing
    we can implement in a graph library. In Python, the graph library NetworkX already
    has a utility that checks for d-separation. You can experiment with these tools
    to build an intuition for d-separation on different graphs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This code was written with pgmpy version 0.1.24\. The pandas version was 2.0.3.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Let’s verify our d-separation analysis of the causal DAG shown previously in
    figure 4.15.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 D-separation analysis of the DAG in figure 4.15
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 The graph library NetworkX implements the d-separation algorithm for NetworkX
    graph objects, such as ΔiGraph (directed graph).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '#2 ΔAG is a base class for the BayesianNetwork class. The base class for ΔAG
    is NetworkX’s ΔiGraph. So is_d_separator will work on objects of the class ΔAG
    (and BayesianNetwork).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Build the graph in figure 4.11\. Blocking on a collider M blocks the path
    U ← M → V but will d-connect the path U ← I → M ← J → V, so this will print False.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Blocking on M will block U ← M → V and open (d-connect) U ← I → M ← J →
    V, but we can block that path with I and J, so this evaluates to True.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Blocking on both I and J is overkill. The minimal d-separating sets are
    {“M”, “I”} and {“M”, “J”}.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: pgmpy also has a `get_independencies` method in the `DAG` class that enumerates
    minimal d-separating states that are true given a graph.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Enumerating d-separations in pgmpy
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Obtain all the minimal d-separation statements that are true in the ΔAG.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The `get_independencies` method returns the following results. (You might see
    a slight difference in the ordering of the output depending on your environment.)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that the `get_independencies` function name is a misnomer; it does not
    “get independencies”; it gets d-separations. Again, don’t conflate d-separation
    in the causal graph with conditional independence in the joint probability distribution
    entailed by the DGP the graph is meant to model. Keeping this distinction in your
    mind will help you with the next task: using d-separation to test a DAG against
    evidence of conditional independence in the data.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Refuting a causal DAG
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen how to build a causal DAG. Of course, we want to find a causal
    model that fits the data well, so now we’ll evaluate the causal DAG against the
    data. We could use standard goodness-of-fit and predictive statistics to evaluate
    fit, but here we’re going to focus on *refuting* our causal DAG, using data to
    show that our model is wrong.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Statistical models fit curves and patterns in the data. There is no “right”
    statistical model; there are just models that fit the data well. In contrast,
    causal models go beyond the data to make causal assertions about the DGP, and
    those assertions are either true or false. As modelers of causality, we try to
    find a model that fits well, but we also try to *refute* our model’s causal assertions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Refutation and Popper
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The approach to building DAGs by refutation aligns with Karl Popper’s falsifiable
    theories framework. Karl Popper was a 20th-century philosopher known for his contributions
    to the philosophy of science, particularly his theory of falsification. Popper
    argued that scientific theories cannot be proven true, but they can be tested
    and potentially falsified, or in other words, *refuted*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: We take a “Popperesque” approach to model building, meaning that we don’t merely
    want to find a model that fits the evidence. Rather, we actively search for evidence
    that refutes our model. When we find it, we reject our model, build a better one,
    and repeat.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: D-separation is our first tool for refutation. Suppose you build a causal DAG
    and it implies conditional independence. You then look for evidence in the data
    of dependence, where your DAG says there should be conditional independence. If
    you find that evidence, you have refuted your DAG. You then go back and iterate
    on the causal DAG, until you can no longer refute it, given your data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve done that, you move on to your downstream causal inference workflow.
    But keep this refutation mentality in mind. If you work with the same causal DAG
    repeatedly, you should always be seeking new ways to refute and iterate upon it.
    Practically, your goal is not getting the true DAG, but getting a hard-to-refute
    DAG.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Revisiting the causal Markov property
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall that we saw two aspects of the causal Markov property:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '*Local Markov property*—A node is conditionally independent of its non-descendants,
    given its parents.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Markov factorization property*—The joint probability distribution factorizes
    into conditional distributions of variables, given their direct parents in the
    causal DAG.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we’ll introduce a third face of this property called the *global Markov
    property*. This property states that d-separation in the causal DAG implies conditional
    independence in the joint probability distribution. In notation, we write
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch4-eqs-0x.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: In plain words, that notation reads as “If *U* and *V* are d-separated by *Z*
    in graph *G*, they are conditionally independent given *Z*.” Note that if any
    of the three facets of the causal Markov property are true, they are all true.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The global Markov property gives us a straightforward way to refute our causal
    model. We can use d-separations to specify statistical tests for the presence
    of conditional independence. Failing tests refute the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Refutation using conditional independence tests
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are multiple ways to statistically evaluate conditional independence,
    and the most obvious is with a statistical test for conditional independence.
    pgmpy and other libraries make it relatively easy to run conditional independence
    tests. Let’s revisit the transportation model, shown again in figure 4.16.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F16_Ness.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 The transportation model. Age (*A*) and gender (*S*) determine education
    (*E*). Education causes occupation (*O*) and residence (*R*). Occupation and residence
    cause transportation (*T*).
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Recall that for our transportation model we were able to collect the following
    observations:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '*Age (A)*—Recorded as young (“young”) for individuals up to and including 29
    years, adult (“adult”) for individuals from 30 to 60 years old (inclusive), and
    old (“old”) for people 61 and over.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gender (S)*—The self-reported gender of an individual, recorded as male (“M”),
    female (“F”), or other (“O”).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Education (E)*—The highest level of education or training completed by the
    individual, recorded as either high school (“high”) or university degree (“uni”).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Occupation (O)*—Employee (“emp”) or a self-employed worker (“self”).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Residence (R)*—The population size of the city the individual lives in, recorded
    as small (“small”) or big (“big”).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Travel (T)*—The means of transport favored by the individual, recorded as
    car (“car”), train (“train”), or other (“other”).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the graph, *E* ⊥[*G*] *T* | *O*, *R*. So let’s test the conditional independence
    statement *E* ⊥ *T* | *O*, *R*. Statistical hypothesis tests have a *null hypothesis*
    (denoted *H*[0]*)* and an *alternative hypothesis* (denoted *H*[*a*]*)*. For statistical
    hypothesis tests of conditional independence, it is standard that the null hypothesis
    *H*[0] is the hypothesis of conditional independence, and *H*[*a*] is the hypothesis
    that the variables are not conditionally independent.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: A statistical hypothesis test uses the *N* data points of observed values of
    *U*, *V*, and *Z* (from an exploratory dataset) to calculate a statistic. The
    following code loads the transportation data. After loading, it creates two DataFrames,
    one with all the data and one with just the first 30 rows so we can see how sample
    size affects the significance test.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Loading the transportation data
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Subsetting the data to only 30 datapoints for explanation'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The line `print(data[0:5])` prints the first five rows of the DataFrame.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Most conditional independence testing libraries will implement frequentist hypothesis
    tests. These tests will conclude in favor of *H*[*0*] or *H*[*a*] depending on
    whether a given statistic falls above or below a certain threshold. “Frequentist,”
    in this context, means that the statistic produced by the test is called a *p*-value,
    and the threshold is called a significance level, which by convention is usually
    .05 or .01.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The test favors the null hypothesis *H*[0] of conditional independence if the
    *p*-value falls above the significance threshold and the alternative hypothesis
    H[*a*] if it falls below the threshold. This frequentist approach is an optimization
    that guarantees the significance level is an upper bound on the chances of concluding
    in favor of dependence when *E* and *T* are actually conditionally independent.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Most software libraries provide conditional independence testing utilities that
    make specific mathematical assumptions when calculating a *p*-value. For example,
    we can run a specific conditional independence test that derives a test statistic
    that theoretically follows the chi-squared probability distribution, and then
    use this assumption to derive a *p*-value. The following code runs the test.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Chi-squared test of conditional independence
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Import the chi_square test function.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set the significance level to .05.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '#3 When the boolean argument is set to False, the test returns a tuple of three
    elements. The first two are the chi-square statistic and the corresponding p-value
    of 0.56\. The last element is a chi-squares distribution parameter called degrees
    of freedom, which is needed to calculate the p-value.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: This prints the tuple `(1.1611111111111112, 0.5595873983053805, 2)`, where the
    values are chi-squared test statistic, *p*-value, and degrees of freedom respectively.
    The *p*-value is greater than the significance level, so this test favors the
    null hypothesis of conditional independence. In other words, this particular test
    did not offer falsifying evidence against our model.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: We can jump directly to the result of the test by setting the `chi_square` function’s
    `boolean` argument to `True`. The function will then return `True` if the *p*-value
    is greater than the significance value (favoring conditional independence) and
    `False` otherwise (favoring dependence).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 Chi-squared test with Boolean outcome
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Import the chi_square test function.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set the significance level to .05.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '#3 When the boolean argument is set to True, the test returns a simple True
    or False outcome. It will return True if the p-value is greater than the significance
    value, which favors conditional independence. It returns False otherwise, favoring
    dependence.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: This prints the result `True`. Now let’s iterate through all the d-separation
    statements we can derive from the transportation graph, and test them one by one.
    The following script will print each d-separation statement along with the outcome
    of the corresponding conditional independence test.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 Run a chi-squared test for each d-separation statement
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The result is a list of d-separation statements and whether the evidence in
    the data supports (or fails to refute) that statement.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can count the number of tests that pass.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 Calculate the proportion of d-separations with passing tests
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here we get `0.2875`. This implies that 29% of the d-separations lack corresponding
    evidence of conditional independence in the data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: This number seems high, but as we’ll see in section 4.4, this statistic depends
    on the size of the data and other factors. We’ll want to compare it to the result
    for other candidate DAGs. For now, the next step is to inspect these cases of
    apparent dependence where our DAG says there should be conditional independence.
    If the evidence of dependence is strong, we need to think about how to improve
    our causal DAG to explain it.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, I used the `chi_square` function, which constructs a specific test
    statistic with a chi-squared test distribution—the distribution used to calculate
    the *p*-value. The chi-squared distribution is just another canonical distribution,
    like the normal or Bernoulli distributions. The chi-squared distribution comes
    up frequently for discrete variables, because there are several test statistics
    in the discrete setting that either have a chi-squared distribution or get closer
    to one as the size of the data increases. Overall, independence tests have a variety
    of test statistics with different test distributions. pgmpy provides several options
    by way of calls to SciPy’s stats library.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: One common concern is that the test makes strong assumptions. For example, some
    conditional independence tests between continuous variables assume any dependence
    between the variables would be *linear*. An alternative approach is to use a *permutation*
    test, which is an algorithm that constructs the *p*-value without relying on a
    canonical test distribution*.* Permutation tests make fewer assumptions but are
    computationally expensive.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Some tests are more important than others
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous analysis tested all the d-separations implied by a causal DAG.
    But some d-separations might be more important to you than others. Some dependence
    relations and conditional independence relations are pivotal to a downstream causal
    inference analysis, while others don’t affect that analysis at all.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider figure 4.17, which we looked at earlier in section 3.3\.
    We added the variable *Z* to the graph because we might want to use it as an “instrumental
    variable” in the estimation of the causal effect.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F17_Ness.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 *Z*, *X*[0], and *X*[1] were included in the DAG because they play
    a useful role in analyzing the causal effect of *U* on *Y*. Their role depends
    on conditional independence, and it is important to test that they can indeed
    serve those roles.
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll discuss instrumental variables in depth in chapter 11\. For now, suffice
    it to say that for *Z* to be an instrument, it must be independent of *W*[0],
    *W*[1], and *W*[2]. So we’d pay special attention to testing that assumption.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Caveats with conditional independence testing
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I mentioned, conditional independence tests are perhaps the most obvious
    way to test the conditional independence constraints implied by your proposed
    causal DAG. However, there are several caveats with using statistical tests to
    test a causal DAG. In my experience, these issues can distract analysts from their
    ultimate goal of answering a causal question. In this section, I’ll highlight
    some of these caveats and propose some alternatives to conditional independence
    testing. The main takeaway is that statistical testing is an important tool for
    building your DAG, but as with any statistical methodology, it is not a panacea
    (and that’s fine).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Statistical tests always have some chance of error
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I mentioned that with d-separation, we should not “confuse the map for the terrain”;
    d-separation is not the same thing as conditional independence. Rather, if your
    model is a good representation of causality, d-separation *implies* conditional
    independence.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, conditional independence is not the same as *statistical evidence*
    of conditional independence. The causal structure of the DGP imposes conditional
    independence constraints on the joint probability distribution. But you can’t
    “see” the joint distribution and the independencies it contains; you can only
    “see” (and run statistical tests on) the data sampled from that distribution.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Just like with prediction, classification, or any other statistical pattern
    recognition procedure, the procedure for detecting these independencies in data
    can get it wrong. You can get false negatives, where a pair of variables are truly
    conditionally independent but the statistical independence test concludes they
    are dependent. You can have false positives, where a statistical independence
    test finds a pair of variables to be conditionally independent when they are not.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Testing causal DAGs with traditional CI tests is flawed
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I say that the proposed conditional independence tests for refutation are “flawed”
    because they violate the spirit of statistical hypothesis testing in science.
    Suppose you think you have discovered some pattern in stock prices. You are biased
    to think the pattern is more than coincidence because, if it is, you can make
    money. To be rigorous and not fall prey to your biases, your alternative hypothesis
    says the pattern is real and exploitable, whereas the null hypothesis is that
    it is just random noise. The frequentist test assumes the null hypothesis is true
    and gives you a *p*-value, which quantifies the chances that random noise could
    form a pattern at least as strong as the one you found. The test forces you to
    reject the pattern as real unless that *p*-value is really small. Most mainstream
    statistical testing libraries are designed for this use case.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: When you propose a causal model, you are also biased to believe it is true.
    But causal models induce conditional independences, which by definition are the
    *absences* of patterns. In this case the null and alternative hypotheses should
    switch; the alternative should be that your model is right and there isn’t a pattern
    (and any evidence of patterns in the data is just spurious correlation), and the
    null should be that there is a pattern. It is possible to implement such a hypothesis
    test, but it is not mathematically trivial, and most mainstream statistical libraries
    like SciPy do not support this use case.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The compromise is using the traditional tests, where the null hypothesis specifies
    conditional independence less as a theoretically rigorous analysis and more as
    a *heuristic*—an empirical problem solving technique that can be suboptimal but
    sufficient to reach a good enough solution.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 p-values vary with the size of the data
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The conclusion of a traditional conditional independence test depends on a significance
    threshold. If the *p*-value falls below this threshold, you favor dependence,
    and if it falls above, you favor conditional independence. The choice of threshold
    is a bit arbitrary; people tend to go with commonly selected values like .1 or
    .05 or .01.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the *p*-value statistic varies with the size of the data.
    All else equal, as the size of the data increases, the *p*-value decreases. In
    other words, the larger the data, the more that things start to look dependent.
    If you have a large dataset, it is more likely that *p*-values will fall below
    that arbitrary threshold, and the data will look like it’s refuting the conditional
    independence implied by your DAG, even when that conditional independence is true.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, the test of *E* ⊥ *T* | *O*, *R* in section 4.3.2 had 30 data
    points and produced a *p*-value of 0.56\. In our data, *E* ⊥ *T* | *O*, *R* is
    ground truth (via simulation), so if a test concludes against *E* ⊥ *T* | *O*,
    *R*, it is because of statistical issues with the test, not the quality of the
    data. The following bootstrap statistical analysis will show how the estimate
    of the *p*-value falls as the size of the data increases.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll write a `sample_p_value` function that samples a *p*-value for
    a given data size. The next function, `estimate_p_value`, will do this sampling
    repeatedly and calculate a mean *p*-value, a 90% confidence interval, and the
    probability that the *p*-value falls below the significance threshold, which is
    the probability of rejecting the correct conclusion that *E* ⊥ *T* | *O*, *R*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 Bootstrap analysis of sensitivity of test of *E* **⊥** *T* | *O*,
    *R* to sample size
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Given a certain data size, this function randomly samples that number of
    rows from the full dataset. It then runs the chi-squared independence test and
    returns the p-value.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '#2 This function conducts a “bootstrap” procedure that samples 1,000 p-values
    for a given data size and calculates the mean p-value and 90% p-value confidence
    interval.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Calculate the probability of a test concluding in favor of conditional independence.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Calculate the mean of the p-values to get the bootstrap mean.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Calculate the 5th and 95th percentiles to get a 90% bootstrap confidence
    interval.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Run the bootstrap analysis.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll visualize the results. We’ll plot the size of the data against
    the mean and 90% confidence intervals for the *p*-values we get for that given
    data size. We’ll also plot how the probability of concluding in favor of the true
    hypothesis (*E* ⊥ *T* | *O*, *R*) for a significance level of .05 depends on data
    size.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.9 Visualize dependence of conditional independence testing on data
    size
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Run the bootstrap analysis to get quantiles of p-values and probability
    of concluding in favor of independence.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Plot the data size vs. p-value. At larger data sizes, the expected p-value
    falls below a threshold.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Plot data size vs. the probability of concluding in favor of independence,
    given .05 significance.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.18 shows the first plot. The descending curve is the expected *p*-values
    at different data sizes, the vertical lines are error bars showing a 90% bootstrap
    confidence interval. By the time we get to a dataset of size 1,000, the expected
    *p*-value is below the threshold, meaning that the test favors the conclusion
    that *E* ⊥ *T* | *O*, *R* is false.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F18_Ness.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 Sample size vs. expected *p*-value of the conditional independence
    test for *E***⊥** *T* | *O*, *R* (solid line). The vertical lines are the error
    bars; they show the 90% bootstrap confidence intervals. The horizontal dashed
    line is a .05 significance level, above which we favor the null hypothesis of
    conditional independence and below which we reject it. As the sample size increases,
    we eventually cross the line. Thus, the result of our refutation analysis depends
    on the size of the data.
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that the lower bound of the confidence interval crosses the significance
    threshold well before 1,000, suggesting that at even lower data sizes, we have
    a good chance of rejecting the true conclusion of *E* ⊥ *T* | *O*, *R*. This becomes
    clearer in figure 4.19, where the probability of concluding in favor of the true
    conclusion decreases as the size of the data increases.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F19_Ness.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 As the size of the data increases, the probability of concluding
    in favor of this (true) instance of the conditional independence relation *E*
    **⊥** *T* | *O*, *R* decreases.
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You might think that as the size of the data increases, the algorithm is detecting
    subtle dependencies between *E* and *T* that were undetectable with less data.
    Not so, for this transportation data is simulated in such a way that *E* ⊥ *T*
    | *O*, *R* is definitely true. This is a case where more data leads us to rejecting
    independence because more data leads to more spurious correlations—patterns that
    aren’t really there.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: A causal model is either right or wrong about causality in the DGP it describes.
    The conditional independence the model implies is either there or it’s not. Yet
    if that conditional independence is there, the test can still conclude in favor
    of dependence when the data is arbitrarily large.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Again, if we view conditional independence testing as a heuristic for refuting
    our DAG, then this sensitivity to the size of the data shouldn’t upset us. Regardless
    of the data size and the significance thresholds, the *relative* differences between
    *p*-values when there is no conditional independence and when there is will be
    large and obvious.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 The problem of multiple comparisons
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In statistical hypothesis testing, the more tests you run, the more testing
    errors you rack up. The same is true when running a test for each d-separation
    implied by a causal DAG. In statistics, this problem is called the *multiple comparisons
    problem*. There are solutions to dealing with multiple comparisons problems, such
    as using *false discovery rates*. If you are familiar with such methods, applying
    them won’t hurt. If you want to learn more, see the chapter’s notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references to false discovery rates in the context of causal modeling. But
    again, I encourage you to view traditional conditional independence testing as
    a heuristic that helps with the ultimate goal of building a good causal DAG. Focus
    on this goal and on the subsequent causal inference analysis you will conduct
    using your DAG, and avoid rabbit holes of statistical testing rigor.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.5 Conditional independence testing struggles in machine learning settings
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Commonly used libraries for conditional independence testing are generally limited
    to one-dimensional variables with fairly simple patterns of correlation between
    them. pgmpy’s conditional independence tests, which are imported from SciPy, are
    no exception. In recent years, several nonparametric tests have been developed
    for more nuanced distributions, such as kernel-based conditional independence
    tests. Tests in the PyWhy library PyWhy-Stats are a good place to start if you
    are interested in such tests.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: However, in machine learning, it is common for variables to have more than one
    dimension such as vectors, matrices, and tensors. For example, one variable in
    a causal DAG might represent a matrix of pixels constituting an image. Further,
    the statistical associations between these variables can be nonlinear.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: One solution is to focus on prediction. If two things are independent, they
    have no ability to predict one another. Suppose we have two predictive models
    *M*[1] and *M*[2]. *M*[1] predicts *Y* using *Z* as a predictor. *M*[2] predicts
    *Y* using *X* and *Z* as a predictor. Predictors can have dimensions greater than
    one. If *X* ⊥ *Y* | *Z*, then any *X* has no predictive information about *Y*
    beyond what is already provided by *Z*. So you can test *X* ⊥ *Y* | *Z* by comparing
    the model predictive accuracy of *M*[2]to *M*[1]. When the models perform similarly,
    we have evidence of conditional independence. Note that you’d want to prevent
    *M*[2] from “cheating” on its predictive accuracy by taking steps to avoid overfitting—yet
    another way spurious correlation can creep into our analysis.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.6 Final thoughts
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Conditional independence testing is an extensive and nuanced subject. Your goal
    with this testing is to refute your causal DAG, not to create the Platonic ideal
    of a conditional independence testing suite. I recommend getting a testing workflow
    that is *good enough*, and then focusing on building your DAG and using that DAG
    in downstream causal inferences. For example, if I had a mix of continuous and
    discrete variables, then rather than implementing a test that could accommodate
    my different data types, I would discretize my continuous variables (for example,
    turning age as time since birth into age brackets) and use a vanilla chi-squared
    test, to keep things moving along.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Refuting a causal DAG given latent variables
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The method of testing DAGs with conditional independence has a latent variable
    problem. If a variable in our causal DAG is latent (not observed in the data),
    we can’t run any conditional independence tests involving that variable. That
    is a major problem; if a variable is an important part of the DGP, we can’t exclude
    it from our DAG simply because we can’t test independence assertions with that
    variable.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, consider the causal DAG in figure 4.20\. This figure represents
    how smoking behavior (*S*) is influenced both by the cost of cigarettes (*C*)
    as well as genetic factors (denoted *D* as in “DNA”) that make one more or less
    prone to nicotine addiction. Those same genetic factors influence one’s likelihood
    of getting lung cancer (*L*). In this model, smoking’s effect on cancer is *mediated*
    through tar buildup (*T*) in the lungs.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F20_Ness.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 A causal DAG representing smoking’s effect on cancer. The variable
    for genetics (*D*) is gray because it is unobserved in the data, so we can’t run
    tests for conditional independencies involving *D*. However, we can test other
    types of constraints.
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'If we have data observing all these variables, we can run conditional independence
    tests targeting the following d-separations: (*C* ⊥*[G]* *T* | *S*), (*C* ⊥*[G]*
    *L* | *D*, *T*), (*C* ⊥*[G]* *L* | *D*, *S*), (*C* ⊥*[G]* *D*), (*S* ⊥*[G]* *L*
    | *D*, *T*), and (*T* ⊥*[G]* *D* | *S*). But suppose we don’t have data on the
    genetics variable (*D*). For example, perhaps measuring this genetics feature
    requires an infeasibly expensive and invasive laboratory test. Of all the d-separations
    we listed, the only one not involving *D* is (*C* ⊥*[G]* *T* | *S*). We are down
    from six to one feasible conditional independence test with which to test our
    DAG.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: In general, a proposed causal model can have various implications for the joint
    probability distributions that are testable with data. The conditional independence
    implied by the graph structure is one type of testable implication. But some of
    the model’s implications are testable in cases of latent variables. In this section,
    we’re going to look at how we can test a DAG with one of these latent variable–related
    constraints.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 An example of a testable implication that works with latent variables
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The causal Markov assumption says d-separations imply conditional independence
    in the data. So far, we’ve explored direct conditional independence between variables,
    but when some variables are latent, the graph can imply conditional independence
    *between functions of observed variables*. These implications are called “Verma
    constraints” in the literature, though I will use the less jargony “functional
    constraints.”
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, the DAG in figure 4.20 with latent variable *D* has the following
    functional constraint (for now, don’t worry about how its derived):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch4-eqs-1x.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Just as the d-separation (*C* ⊥*[G]* *T* | *S*) implies that the conditional
    independence statement (*C* ⊥ *T* | *S*) should hold for the observational joint
    distribution, the functional constraint (*C* ⊥*[G]* *h*(*L*, *C*, *T*)) implies
    that *C* is independent of some function *h*(.) of variables *L*, *C*, and *T*
    in the observational joint distribution. Both implications are testable since
    they don’t involve *D*. We now have two tests we can run instead of one.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '*h*(.) has two components:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*l*|*c*, *s*, *t*) is a function that returns the probability that *L*
    = *l* (suppose *l* is “true” for “has lung cancer” and “false” for “no lung cancer”),
    given *C* = *c*, *S* = *s*, and *T* = *t*.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*s*|*c*) is a function that returns the probability that *S* = *s* (suppose
    *s* is “low,” “medium,” or “high” depending on how heavily a smoker smokes) given
    the cost of cigarettes *C* = *c*.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h*(.) then sums over all values of *S*. The function’s output is a random
    variable that, according to the DAG, should be independent of *C*. *h*(*l*, *c*,
    *t*) is a function of *P*(*l*|*c*, *s*, *t*) and *P*(*s*|*c*), and it may feel
    odd thinking about independence in terms of probability functions. Remember that
    the independence relation is itself just a function of joint probability distributions.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll fit models of *P*(*l*|*c*, *s*, *t*) and *P*(*s*|*c*) from data
    and test this independence relation. But first, we’ll look at libraries that let
    us enumerate functional constraints like (*C* ⊥[*G*] *h*(*L*, *C*, *T*)) from
    a DAG just like we could enumerate d-separations with pgmpy’s `get_independencies`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Libraries and perspectives on testing functional constraints
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How do we derive functional constraints like *C* ⊥[*G*] *h*(*L*, *C*, *T*)?
    Like d-separation, we can derive this type of constraint algorithmically from
    the graph. One implementation is in the `verma.constraints` function in the causaleffect
    R library. This function takes in the DAG with nodes labeled as latent and returns
    a set of testable constraints just like pgmpy’s `get_independencies`. For Python,
    the library Y0 (pronounced “why-not”) has a `r_get_verma_constraints` function
    (as of version 0.2.10), which is a wrapper that calls causaleffect’s R code. I’ll
    omit the Python code here because it requires installing R, but visit www.altdeep.ai/causalAIbook
    for links to libraries and references.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical intuition for functional constraints, and some advice
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our goal for this section is only to show that there are ways to test your causal
    model even when there are latent variables. Functional constraints are one way
    to do this, but we don’t want to over-index on this particular flavor of testable
    implication. It is more important to avoid the dangerous mindset of limiting ourselves
    only to DAGs that are fully observed in the data.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: That said, for the curious, I’ll offer a very high-level intuition for the math.
    Recall that the local Markov property says that *a* *node is conditionally independent
    of its non-descendants, given its parents*. From there, we derive graphical criteria
    called d-separation that lets us find sets of nodes where this applies, we write
    a graph algorithm that uses those criteria to enumerate d-separations, and we
    use that algorithm to enumerate some conditional independence tests we can run.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given node *X*, let’s say “orphaned cousins” means non-descendants of
    *X* that share a latent ancestor of *X*. Here is, in informal terms, a latent
    variable analog to the local Markov property: *A node is conditionally independent
    of its non-descendants given its nearest observed ancestors, its orphaned cousins,
    and other nearest observed ancestors of those cousins*. Just as with d-separations,
    we can derive graphical criteria to identify individual cases where this applies.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Recall that we can factorize the joint probability distribution such that each
    factor is the conditional probability of a node’s outcome, given its parents.
    The probability functions in the functional constraint (like the *P*(*l*|*c*,
    *s*, *t*) and *P*(*s*|*c*) terms in *h*(*l*,*c*,*t*)) come into the picture once
    we start marginalizing that factorization over the latent variables and doing
    subsequent probability math.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: See the references listed at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    if you want to deep dive. But my warning from the previous section holds here—*our
    goal is to falsify our DAG and move on to our target causal inference**.* Beware
    of falling down statistical, mathematical, and theoretical rabbit holes on the
    way to that goal.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a new testable implication in the form of *C* ⊥*[G]* *h*(*L*,
    *C*, *T*), let’s test it out.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3 Testing a functional constraint
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To test (*C* ⊥*[G]* *h*(*L*, *C*, *T*)), we have to calculate *h*(*l*, *c*,
    *t*) = *∑*[S]*P*(*l*|*c*, *s*, *t*)*P*(*s*|*c*) for each item in our data. That
    requires us to model *P*(*l*| *c*, *s*, *t*) and *P*(*s*|*c*). There are several
    modeling approaches we could go with, but we’ll use a naive Bayes classifier for
    this example so we can stick with using the pgmpy and pandas libraries. We’ll
    take the following steps:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Discretize cost (*C*) so we can treat it as a discrete variable.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use pgmpy to fit a naive Bayes classifier to *P*(*l*| *c*, *s*, *t*) and *P*(*s*|*c*).
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a function that takes in values of *L*, *C*, *T* and calculates *h*(*L*,
    *C*, *T*).
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply that function to each row in the data to get a new column of *h*(*L*,
    *C*, *T*) values.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run an independence test between that column and the *C* column.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up your environment
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The following code uses pgmpy version 0.1.19 because versions up to 0.1.24 (current
    at the time of writing) have a bug (already reported) that can cause issues with
    some of the naive Bayes classifier inference code. You don’t need to do this if
    you use another method of calculating *P*(*l*|*c*, *s*, *t*) and *P*(*s*|*c*).
    For stability, we’ll also use pandas version 1.4.3, which was the version when
    pgmpy 0.1.19 was current. Note that if you have installed later versions of pgmpy
    and pandas, you might have to uninstall those versions before installing these,
    or you could just spin up a new Python environment. Visit [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for links to the Jupyter notebooks with the code and notes on setting up a working
    environment.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll import the data. We’ll also discretize the cost of cigarettes (*C*)
    so it is more amenable to modeling with pgmpy.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.10 Importing and formatting cigarette and cancer data
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Load the CSV file into a pandas ΔataFrame.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Δiscretize cost (C) into a discrete variable with three levels to facilitate
    conditional impendence tests.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Δiscretize cost (C) into a discrete variable with three levels to facilitate
    conditional impendence tests.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Turn lung cancer (L) from a Boolean to a string, so the conditional independence
    test will treat it as a discrete variable.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The `print(data_disc)` line prints out the elements of the `data_disc` DataFrame.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we need to model *P*(*l*| *c*, *s*, *t*) and *P*(*s*|*c*). We’ll opt for
    a naive Bayes classifier, a probabilistic model that “naively” assumes that, in
    the case of *P*(*l*| *c*, *s*, *t*), cost (*C*), smoking (*S*), and tar (*T*)
    are conditionally independent given lung cancer status (*L*). According to our
    causal DAG, that is clearly not true, but that doesn’t matter if all we want is
    a good way to calculate probability values for *L* given *C*, *S*, and *T*. A
    naive Bayes classifier will do that well enough.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.11 Fit naive Bayes classifier of *P*(*l*| *c*, *s*, *t*)
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 We’ll use a naive Bayes classifier in pgmpy to calculate the probability
    value for a given value of L given values of C, S, and T. In this case, we’ll
    use variable elimination.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We’ll use a naive Bayes classifier in pgmpy to calculate the probability
    value for a given value of L given values of C, S, and T. In this case, we’ll
    use variable elimination.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll do the same for *P*(*s*|*c*).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.12 Fit naive Bayes classifier of *P*(*s*|*c*)
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now we’ll bring these together to implement the *h*(*L*, *T*, *C*) function.
    The following code uses a `for` loop to do the summation over *S*.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.13 Combine models to create *h*(*L*, *T*, *C*)
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Implement h(L, C, T).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Implement the summation of P(l|c,s,t) * P(s|c) over s.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll calculate the full set of outcomes for set {*C*, *T*, *L*}. Given
    these outcomes, we can calculate the *h*(*L*, *C*, *T*) for each of these combinations
    using the preceding function.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.14 Calculate the outcome combinations of *C*, *T*, and *L*
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Calculate these values for each possible combination of outcomes of L, C,
    and T. First, we use list comprehensions to make a ΔataFrame containing all the
    combinations.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Printing this shows all combinations of outcomes for *C*, *T*, and *L*.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: For each of these outcomes, we’ll apply *h*(*L*, *C*, *T*).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.15 Calculate *h*(*L*, *C*, *T*) for each outcome of C, T, L
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now for each joint outcome of *C*, *T*, and *L*, we have a value of *h*(*L*,
    *C*, *T*).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Finally, we’ll merge this `h_func` distribution into the dataset such that for
    each row of our data, we get a value of *h*(*L*, *C*, *T*).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.16 Merge to get a value of *h*(*L*, *C*, *T*) for each row in the
    data
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 Add a column representing the variable h(L, C, T).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'We see the result with `print(df_mod)`:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The functional constraint says that *C* and *h*(*L*, *C*, *T*) should be independent,
    so we can look at the evidence of independence between the `h_func` column and
    the *C* column. Since we discretized *C*, our calculated outcomes for *h*(*L*,
    *C*, *T*) are technically discrete, so we could use a chi-squared test. But *h*(*L*,
    *C*, *T*) is continuous in theory, so instead we’ll use a box plot to visualize
    dependence between the two variables. The functional constraint says *C* and *h*(*L*,
    *C*, *T*) should be independent, so we’ll use a box plot that plots values of
    *h*(*L*, *C*, *T*) against values of *C* to visually inspect whether *C* and *h*(*L*,
    *C*, *T*) look independent.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.17 Box plot visualizing independence between *C* and *h*(*L*, *C*,
    *T*)
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This produces figure 4.21.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F21_Ness.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 A box plot visualization of cost (*C*) on the *x*-axis and the function
    *h*(*L*, *C*, *T*) on the *y*-axis (labeled “Sum product”). The overlap of the
    distributions of the sum product for each value of *C* supports the functional
    constraints assertion that these two quantities are independent.
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The *x*-axis in figure 4.21 is different levels of cost (low, medium, and high).
    The *y*-axis represents values of the sum. Figure 4.21 is a box wand whiskers
    plot; each box is a representation of the distribution of the sum product for
    a given value of *C*. The top and bottom of the boxes are the quartiles of the
    distribution, the lines in the middle of the boxes are the median, and the shorter
    horizonal lines are the max and min values (for low cost, the median, upper quartile,
    and max are quite close). In summary, it looks as though the distributions of
    the sum product don’t change much across the different levels of cost; that’s
    what independence is supposed to look like.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: We can also derive a *p*-value using an *analysis of variance* (ANOVA) approach,
    this time using an F-test rather than a chi-squared test. The following code uses
    the statsmodels library to run an ANOVA test.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Note  “PR( >F)” means the probability of seeing an F-statistic for a given variable
    (in our case, *C*) is at least as large as the F-statistic calculated from the
    data, assuming that the variable is independent of `sum_product` (i.e., the *p*-value).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.18 Using ANOVA to evaluate independence
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 A recipe for doing ANOVA using the statmodels library'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Returns a high p-value, which supports (fails to falsify) the assertion
    that h(L, C, T) and C are independent'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Just as a sanity check, we run the same test to see whether h(L, C, T) looks
    independent of T and L. Unlike C, T and L should not be independent of h(L, C,
    T) and as expected, these tests return much smaller p-values, indicating dependence.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: We print the *p*-value for *C* with `print(aov_table["PR(>F)"]["C"])` and get
    ~0.1876\. That *p*-value indicates we can’t reject the null hypothesis of independence,
    so it looks like the data supports the constraint. We also run the same test for
    *T* and *L* and, as expected, these are much smaller, indicating evidence of dependence.
    They are lower, both falling below the common .1 threshold where a standard hypothesis
    test would reject the hypothesis that *h*(*L*, *C*, *T*) is independent of *T*
    and *L*.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.4 Final thoughts on testable implications
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A DAG’s d-separation and functional constraints imply that certain conditional
    independencies should hold in the joint probability distribution *if* the DAG
    is a good causal model of the DGP. We can falsify the DAG by running statistical
    tests for conditional independence.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: More generally, a causal model can have different mathematical implications
    for the underlying joint probability distribution, and some of these can be tested.
    For example, if your model assumed the relationship between a cause and effect
    was linear, you could look for evidence of nonlinearity in the data (we’ll see
    more about functional causal assumptions in chapter 6). And, of course, you can
    falsify your model’s implications with experiments (as we’ll see in chapter 7).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: The better we get at causal modeling, the better we get at testing and falsifying
    our causal models. But remember, don’t let the statistical and mathematical nuances
    of testing distract you from your goal of getting a good enough model and moving
    on to your target causal inference.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Primer on (the perils of) causal discovery
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous workflow, we proposed a causal DAG, considered what implications
    (like conditional independence) the DAG had for the observational joint distribution,
    and then tested those implications with the data. What if we went in the other
    direction? What if we analyzed the data for statistical evidence of causality
    induced constraints, and then constructed a causal DAG that is consistent with
    those constraints?
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'This describes the task of *causal discovery*: statistical learning of causal
    DAGs from data. In this section, I’ll provide a brief primer on causal discovery
    and cover what you need to know to make use of this class of algorithms.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Beware the false promises of causal discovery
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Causal discovery algorithms are often presented as magical tools that convert
    any dataset, no matter how limited in quality, into a causal DAG. That false promise
    discourages the mindset of modeling the DGP (rather than the data) and falsifying
    candidate models. It is also why it is hard to find consistent use cases for discovery
    in practice. This section takes the approach of framing how discovery algorithms
    work and where they fail, rather than going through a list of algorithms. I’ll
    conclude with advice about how to effectively incorporate these algorithms into
    your analysis workflow.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with an overview of key ideas that underpin discovery algorithms.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1 Approaches to causal discovery
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several approaches to causal discovery. Some algorithms (often called
    *constraint-based* algorithms) do what I just suggested—reverse engineer a graph
    from evidence of conditional independence in the data. Other algorithms (often
    called *score-based* algorithms) turn the causal DAG into an explanatory model
    of the data and find causal DAGs that have a high goodness-of-fit score. Yet another
    approach is to assume additional constraints on the functional relationships between
    parents and children in the causal DAG, as we’ll see with structural causal models
    in chapter 6.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: The space of possible DAGs is a discrete space. One class of approaches tries
    to soften this space into a continuous space and use continuous optimization techniques.
    The popularity of automatic differentiation libraries for deep learning have accelerated
    this trend.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Because the space of DAGs can be quite large, it is useful to incorporate prior
    knowledge to constrain the size of that space. This often takes the form of specifying
    what edges must be present or what must be absent, or of using Bayesian priors
    on graph structure.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Some causal discovery algorithms can work with experimental data. This requires
    telling the algorithm which variables were set by the experimenter (or as we’ll
    say starting in chapter 7, which were “intervened upon”).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: To get started with causal discovery using Python, I recommend the PyWhy libraries
    for causal discovery such as causal-learn and DoDiscover.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2 Causal discovery, causal faithfulness, and latent variable assumptions
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The causal Markov property assumes that if our DAG is true, d-separations in
    that DAG imply conditional independence statements in the joint probability of
    the variables:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch4-eqs-2x.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
- en: '*Causal faithfulness* (or just “faithfulness”) is the converse statement—conditional
    independence in the joint distribution implies d-separation in the graph:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch4-eqs-3x.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: Many causal discovery algorithms rely on an assumption that faithfulness holds.
    It may not.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Discovery and faithfulness violations
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In section 4.4, we used the Markov property to test a candidate DAG; given a
    d- separation statement that held for the DAG, we ran a statistical test to check
    for empirical evidence of the conditional independence implied by that d-separation.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you wanted to build your graph by going in reverse. You detect evidence
    of an instance of conditional independence in your data, and then you limit your
    space of candidate DAGs to those consistent with the implied d-separation. You
    do this iteratively until you’ve narrowed down the space of candidate DAGs. Some
    discovery algorithms do some version of this procedure, and those that do are
    relying on a faithfulness assumption.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Note  Algorithms that match evidence of conditional independence to d-separation
    are often called “constraint-based” discovery algorithms. A well-known example
    is the PC algorithm. Constraint-based algorithms find DAGs that are *constrained*
    to be consistent with the empirical evidence of causality.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'The trouble comes from “faithfulness violations”—special cases where conditional
    independence in a joint probability distribution does not map to d-separation
    statements in a ground truth DAG. A simple example of a faithfulness violation
    is the case of a three-variable system that can decompose as follows: *P*(*x*,
    *y*, *z*) = *P*(*x*, *y*)*P*(*y*, *z*)*P*(*x*, *z*). That is, for any value of
    one variable, the association between the other two variables is always the same.
    You could detect this peculiar form of independence in data, but you can’t represent
    it with d-separation in a DAG. (If you don’t believe me, try.)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Researchers worry about these special cases because they mean a discovery algorithm
    that relies on faithfulness doesn’t generalize to all distributions. When you
    use these algorithms, you are assuming faithfulness holds for you problem domain,
    and that’s not something you can test. However, violations of causal faithfulness
    are not typically the biggest source of headaches in practical causal discovery.
    That honor is reserved for latent variables.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of latent variables
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The bigger pain is that most causal discovery algorithms, yet again, have a
    latent variable problem. To illustrate, suppose the true causal DAG was the DAG
    in figure 4.22.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F22_Ness.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 Assume this is the true causal DAG. Here, *B*, *C*, and *D* are
    conditionally independent, given *A*.
  id: totrans-377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this DAG, variables *B*, *C*, and *D* are conditionally independent of one
    another, given *A*. Now suppose that *A* were not observed in the data. With *A*
    as a latent variable, the discovery algorithm can’t run tests like *B* ⊥ *C* |
    *A*. The algorithm will detect a dependence between *B*, *C*, and *D* but will
    not find conditional independence between the three given *A*, and it might possibly
    return a DAG like figure 4.23, which reflects these results.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F23_Ness.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 If *A* is latent, conditional independence tests that condition
    on *A* can’t be run. The algorithm would detect dependence between *B*, *C*, and
    *D* but no conditional independence given *A*, and it might possibly return a
    graph such as this.
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The remedy for this problem is to provide strong domain-specific assumptions
    about the latent variable structure in the discovery algorithm. A few generic
    discovery algorithms provide some accommodation for latent variable assumptions
    (the causal-learn library has a few). But this is rare, because it is hard to
    make it easy for users to specify domain-specific assumptions while still generalizing
    across domains.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.3 Equivalence classes and PDAGs
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s suppose our algorithm were to correctly recover all the true conditional
    independence statements from data and map them back to a true set of d-separation
    statements (causal faithfulness holds). The problem we face now is that multiple
    causal DAGs may have the same set of d-separation statements. This set of candidate
    DAGs is called a *Markov equivalence class*. The true causal DAG would be one
    of a possibly large set of members of this class.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose the DAG on the left of figure 4.24 were the ground truth
    DAG. The DAG on the right of the graph differs from the correct graph in the edge
    between *A* and *T*. The two graphs have the same set of d-separation. In fact,
    we can also change the directions of the edges between {*L*, *S*} and {*B*, *S*}
    and still be in the same equivalence class, except for introducing a collider
    {*L* → *S* ← *B*}, because a new collider would change the set of d-separations.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F24_Ness.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 Supposing the DAG on the left is the ground truth DAG, the (wrong)
    DAG on the right is in the same Markov equivalence class. The PDAG in the middle
    represents the equivalence class, where undirected edges represent edges where
    members disagree on direction.
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some discovery algorithms will return a partially directed acyclic graph (PDAG),
    such as the DAG in the center of figure 4.24\. In the PDAG, undirected edges correspond
    to edges where there is disagreement on the edge’s direction between members of
    the Markov equivalence class. This is nice, because we get a graphical representation
    of the equivalence class, and the algorithm can potentially search through the
    space of PDAGs instead of the larger space of DAGs.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Colliders and discovery
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Colliders feature prominently in causal discovery because they allow us to orient
    edges in the DAG from evidence of statistical dependence alone. Suppose we are
    using data to attempt to construct the ground-truth DAG in figure 4.24\. We find
    evidence of dependence in the data of an edge between *A* and *T*. The idea of
    Markov equivalence means that evidence is not enough to determine the direction
    of that edge. Generally, evidence of dependence and independence in the data can
    imply the presence of edges but not their direction.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Colliders are the exception. It is possible to detect colliders like {*T* →
    *E* ← *L*} from evidence of independence and dependence alone; if the data suggests
    *T* and *L* are independent, but become dependent when conditioning on *E*, you
    have evidence of a collider with directed edges {*T* → *E* ← *L*}.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Colliders can also force orientation of edges outside of the collider. For
    example, consider the edge between *E* and *X* in the ground-truth DAG in figure
    4.23\. We might infer the existence of that edge from the following evidence in
    the data:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '*E* and *X* are dependent.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T* and *X* are dependent.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T* and *X* are independent, given *E*.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An edge between *E* and *X* is consistent with that evidence, but should we
    go with *E* → *X* or *E* ← *X*? Here, the collider {*T* → *E* ← *L*} helps; it
    already oriented the edge *T* → *E*, so adding *E* ← *X* would induce another
    collider {*T* → *E* ← *X*}. That collider would suggest *T* and *X* are independent
    but become dependent when conditioning on *E*, which violates the second and third
    observed items of evidence. So we conclude the edge is oriented as *E* → *X* by
    process of elimination.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Some causal discovery algorithms essentially algorithmicize this kind of logic.
    But remember, this logic breaks down when latent variables induce dependence between
    observed variables.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: That said, PDAGs and Markov equivalence classes only capture equivalence between
    DAGs encoding the same set of conditional independence constraints. If you want
    to find all graphs that satisfy an additional layer of constraining assumptions,
    such as all graphs that have the same posterior probability given a certain prior,
    then the PDAG might not be sufficient.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: If we go only on conditional independence, data can’t distinguish between members
    of the Markov equivalence class, because having the same set of d-separations
    means having the same evidence of conditional independence in the data. This is
    an example of a *lack of causal identification*—when our data and a set of causal
    assumptions are not sufficient to disambiguate between possible answers to a causal
    question (in this case “what is the right causal DAG?”). We’ll explore causal
    identification in depth in chapter 10.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.4 How to think about causal discovery
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In section 4.3, I argued that testing for causality induced constraints like
    conditional independence using off-the-shelf hypothesis testing libraries should
    be viewed more as a heuristic approach to refuting your causal DAG than a rigorous
    statistical procedure for validating the DAG. Similarly, I argued that for the
    practical user, off-the-shelf causal discovery algorithms should be viewed as
    a tool for exploratory data analysis during a human-driven causal DAG building
    process. The more you can input various types of domain knowledge and knowledge
    of latent variables into these algorithms, the better. But even then, they will
    produce obvious errors. Just as with the hypothesis testing case, avoid rabbit
    holes of trying to “fix” the discovery algorithm so it doesn’t make these errors.
    Use causal discovery as one imperfect tool in your broader project of building
    a good causal DAG and running the subsequent causal inference analysis.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Causal modeling induces conditional independence constraints on the joint probability
    distribution. D-separation provides a graphical representation of conditional
    independence constraints.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an intuition for d-separation is important for reasoning about causal
    effect inference and other queries.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The colliders might make d-separation confusing, but you can build intuition
    by using d-separation functions in NetworkX and pgmpy.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using traditional conditional independence testing libraries to test d-separation
    has its challenges. The tests are sensitive to sample size, they don’t work well
    in many machine learning settings, and their hypotheses are misaligned.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of these challenges, it is best to view the attempts to falsify the
    DAG using off-the-shelf conditional independence testing libraries as more of
    a heuristic. Focus on the overall goal of building a good (i.e., hard to refute)
    causal DAG and moving on to your downstream causal inference task. Avoid fixating
    on theoretical rigor in statistical hypothesis testing.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there are latent variables, a causal DAG may still have testable implications
    for functions of the observed variables.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal discovery refers to the use of statistical algorithms to recover a causal
    DAG from data.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causal faithfulness property assumes conditional independence in the joint
    probability distribution maps to a true set of d-separations that hold in the
    ground truth causal DAG.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Markov equivalence class of DAGs is a set of DAGs with the same set of d-separations.
    Assuming you have the true set of d-separations, the ground truth causal DAG generally
    shares that set with other (wrong) DAGs.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal discovery is especially vulnerable to latent variables.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The more you can constrain causal inference with prior assumptions, such as
    latent structure and which edges cannot possibly exist and which must exist, the
    better.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal discovery algorithms are useful exploratory data analysis tools in the
    process of building a causal DAG, but they are not reliable replacements for that
    process. Again, focus on the overall goal of building a good causal DAG and moving
    on to the downstream causal inference analysis. Avoid trying to “fix” causal discovery
    algorithms so they don’t produce obvious errors in your domain.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
