["```py\nimport numpy as np\nimport pandas as pd\nimport dowhy\nfrom dowhy import CausalModel\nfrom dowhy.datasets import linear_dataset\nfrom dowhy.do_why import CausalModel    #1\n\n# Let's create a sample dataset for illustration purposes\n# Normally, you'd import your own dataset: data = pd.read_csv('your_dataset.csv')\nn_points = 1000\ndata = pd.DataFrame({\n    \"S\": np.random.binomial(n=1, p=0.5, size=n_points),     #2\n    \"LC\": np.random.binomial(n=1, p=0.5, size=n_points),    #3\n    \"Price\": np.random.normal(loc=5, scale=1, size=n_points),     #4\n})\n\n# Create a causal DAG\nmodel=CausalModel(\n        data = data,\n        treatment='S',\n        outcome='LC',\n        common_causes=['G', 'A', 'E', 'O'],  # Potential confounders\n        instruments=['Price']  # Instrumental Variable\n)\n\n# Identify the causal effect\nidentified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n\n# Estimate the causal effect using Instrumental Variable method\nestimate = model.estimate_effect(identified_estimand,\n                                 method_name=\"iv.instrumental_variable\",\n                                 method_params={'iv_instrument_name': 'Price'})\n\n# Print the causal effect estimate\nprint(estimate)\n```", "```py\nAug 22 14:02:34 User \"Jake\" logged in from 192.168.0.105.\nAug 22 14:03:12 Shared folder accessed by user \"Jake\" at //SERVER/shared-folder.\nAug 22 14:03:20 File transfer initiated by \"Jake\" from local: /home/jake/documents to remote: //SERVER/shared-folder/documents.\nAug 22 14:03:25 WARNING: Unusual network activity detected. User \"Jake\" launched file \"heavy_process.exe\" on //SERVER/shared-folder.\nAug 22 14:03:30 Network performance alert: Bandwidth usage spikes at 95%.\nAug 22 14:04:00 Network slowdown detected: Packet loss rate 35%.\n```", "```py\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')   #1\ntokens = tokenizer.tokenize(\"Can LLMs reason counterfactually?\")    #2\nprint(tokens)     #3\n```", "```py\n['Can', 'ĠLL', 'Ms', 'Ġreason', 'Ġcounter', 'fact', 'ually', '?']\n```", "```py\ninput_ids = tokenizer.encode(     #1\n    \"Can LLMs reason counterfactually?\",    #1\n    return_tensors='pt'    #1\n)   #1\nprint(input_ids)\n```", "```py\ntensor([[ 6090, 27140, 10128,  1738,  3753, 22584,   935,    30]])\n```", "```py\nimport torch\nfrom transformers import GPT2LMHeadModel\n\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')    #1\nmodel.eval()    #1\n\ninput_text = \"Can LLMs reason counterfactually?<|endoftext|>\"     #2\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')    #2\n\nwith torch.no_grad():    #3\n    outputs = model(input_ids)    #3\n    logits = outputs.logits    #3\n\nlog_probs = torch.nn.functional.log_softmax(logits, dim=-1)     #4\nfor idx, token in enumerate(input_ids[0]):    #4\n    token_log_prob = log_probs[0][idx][token].item()    #4\n    print(f\"Token: {tokenizer.decode(token)}\" + #4\n           \" | Log Probability: {token_log_prob}\")     #4\n```", "```py\nToken: Can | Log Probability: -10.451835632324219\nToken:  LL | Log Probability: -9.275650978088379\nToken: Ms | Log Probability: -14.926365852355957\nToken:  reason | Log Probability: -10.416162490844727\nToken:  counter | Log Probability: -8.359155654907227\nToken: fact | Log Probability: -22.62082290649414\nToken: ually | Log Probability: -11.302435874938965\nToken: ? | Log Probability: -10.131906509399414\nToken: <|endoftext|> | Log Probability: -11.475025177001953\n```", "```py\nprompt = \"Counterfactual reasoning would enable AI to\"     #1\ninput_ids = tokenizer.encode(prompt, return_tensors='pt')    #1\n\noutput = model.generate(    #2\n    input_ids,     #2\n    max_length=25,    #2\n    do_sample=True,    #2\n    pad_token_id=tokenizer.eos_token_id     #2\n)\n\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)    #3\nprint(generated_text)   #3\n```", "```py\nCounterfactual reasoning would enable AI to figure out what people want before they ask them. It would also enable self-awareness\n```", "```py\nimport pandas as pd\nurl = (\"https://raw.githubusercontent.com/altdeep/\"\n       \"causalML/master/book/chapter%2013/\"\n       \"king-prince-kingdom-updated.csv\")\ndf = pd.read_csv(url)\nprint(df.shape[0])    #1\n\nprint(df[\"King\"][0] + \"\\n\")     #2\nprint(df[\"King\"][1] + \"\\n\")     #2\nprint(df[\"King\"][2])     #2\n\nprint(\"----\")\nprint(df[\"Prince\"][0] + \"\\n\")     #3\nprint(df[\"Prince\"][1] + \"\\n\")     #3\nprint(df[\"Prince\"][2])    #3\n\nprint(\"----\")\nprint(df[\"Kingdom\"][0] + \"\\n\")    #4\nprint(df[\"Kingdom\"][1] + \"\\n\")     #4\nprint(df[\"Kingdom\"][2])     #4\n```", "```py\n21000\n----\nKing brokers a peace treaty with a rival kingdom, putting an end to years of bloody conflict\n\nA wise king successfully negotiates peace with a rival nation\n\nA wise king successfully negotiates peace between his kingdom and a long-time enemy\n----\nhowever, his son, the Prince, falls in love and marries a foreigner, causing political unrest\n\nPrince falls in love with and marries a foreign princess, forging a strong alliance\n\nbut when a new threat emerges, the Prince leads the army to defend their realm\n----\ndespite efforts, the ongoing war results in both kingdoms falling into poverty.\"\n\nthe alliance strengthens their forces, leading the kingdom to a victorious battle.\"\n\nhowever, a series of misfortunes and disastrous decisions plunge their once prosperous kingdom into poverty.\"\n```", "```py\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n    AutoTokenizer, DataCollatorForLanguageModeling,\n    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n    Trainer, TrainingArguments)\nurl = (\"https://raw.githubusercontent.com/altdeep/\"\n       \"causalML/master/book/chapter%2013/\"\n       \"king-prince-kingdom-updated.csv\")\ndf = pd.read_csv(url)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")     #1\ntokenizer.pad_token = tokenizer.eos_token    #2\ndef tokenize_phrases(phrases, max_length=40):  #3\n    return tokenizer(\n        phrases,\n        truncation=True,\n        padding='max_length',\n        max_length=max_length\n    )\n```", "```py\nclass ModelDataset(Dataset):    #1\n    def __init__(self, encodings, labels):   #1\n        self.encodings = encodings    #1\n        self.labels = labels    #1\n        #1\n    def __getitem__(self, idx):   #1\n        item = {    #A    #1\n            key: torch.tensor(val[idx])    #1\n            for key, val in self.encodings.items()    #1\n        }    #A   #1\n        item['labels'] = torch.tensor(self.labels[idx])    #1\n        return item     #1\n        #1\n    def __len__(self):   #1\n        return len(self.encodings.input_ids)   #1\n\ndef create_king_dataset(input_phrases):     #2\n    king_phrases = input_phrases.tolist()   #2\n    king_encodings = tokenize_phrases(king_phrases)    #2\n    king_dataset = ModelDataset(    #2\n        king_encodings,   #2\n        king_encodings['input_ids']) #2\n    return king_dataset    #2\n```", "```py\ndef create_seq2seq_datasets(input_phrases, target_phrases):\n    input_phrases_list = input_phrases.tolist()\n    target_phrases_list = target_phrases.tolist()\n    spit = train_test_split(     #1\n        input_phrases_list,     #1\n        target_phrases_list,    #1\n        test_size=0.1    #1\n    )    #1\n    train_inputs, val_inputs, train_targets, val_targets = spit     #1\n    train_input_encodings = tokenize_phrases(train_inputs)    #2\n    val_input_encodings = tokenize_phrases(val_inputs)    #2\n    train_target_encodings = tokenize_phrases(train_targets)    #2\n    val_target_encodings = tokenize_phrases(val_targets)     #2\n    train_dataset = ModelDataset(\n        train_input_encodings, train_target_encodings['input_ids']\n    )\n    val_dataset = ModelDataset(\n        val_input_encodings, val_target_encodings['input_ids']\n    )\n    return train_dataset, val_dataset\n```", "```py\ndef train_king_model(output_dir, train_dataset,\n                     model_name=\"gpt2-medium\", epochs=4):\n    king_model = AutoModelForCausalLM.from_pretrained(model_name)     #1\n    training_args_king = TrainingArguments(     #1\n      output_dir=output_dir,    #1\n      per_device_train_batch_size=32,     #1\n      overwrite_output_dir=True,     #1\n      num_train_epochs=epochs,     #1\n      save_total_limit=1,    #1\n      save_steps=len(train_dataset) // 16, #1\n      max_grad_norm=1.0 #1\n    )    #1\n    data_collator = DataCollatorForLanguageModeling(    #1\n        tokenizer=tokenizer,     #1\n        mlm=False)    #1\n    trainer_king = Trainer(     #2\n        model=king_model,     #2\n        args=training_args_king,     #2\n        data_collator=data_collator,    #2\n        train_dataset=train_dataset,     #2\n    )     #2\n    trainer_king.train()   #3\n    king_model.save_pretrained(output_dir)\n    return king_model\n```", "```py\ndef train_seq2seq_model(output_dir, train_dataset, val_dataset,\n                        model_name=\"facebook/bart-base\",\n                        epochs=4):\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n    training_args = Seq2SeqTrainingArguments(     #1\n        output_dir=output_dir,     #1\n        per_device_train_batch_size=16,     #1\n        predict_with_generate=True,    #1\n        logging_dir=f\"{output_dir}/logs\",     #1\n        save_total_limit=1,    #1\n        save_steps=len(train_dataset) // 16,     #1\n        learning_rate=3e-5,     #1\n        num_train_epochs=epochs,    #1\n        warmup_steps=500,   #1\n        weight_decay=0.01,     #1\n    )    #1\n    trainer = Seq2SeqTrainer(     #2\n        model=model,    #2\n        args=training_args,   #2\n        train_dataset=train_dataset,     #2\n        eval_dataset=val_dataset,    #2\n    )     #2\n    trainer.train()     #3\n    model.save_pretrained(output_dir)\n    return model\n```", "```py\nimport os\n\nking_model_path = os.path.join(os.getcwd(), 'king_model')     #1\nprince_model_path = os.path.join(os.getcwd(), 'prince_model')     #1\nkingdom_model_path = os.path.join(os.getcwd(), 'kingdom_model')     #1\nprince2king_model_path = os.path.join(     #1\n    os.getcwd(), 'prince2king_model')     #1\n\nking_dataset = create_king_dataset(df[\"King\"])     #2\nking_model = train_king_model(king_model_path, king_dataset)     #2\n\ndatasets = create_seq2seq_datasets(df[\"King\"], df[\"Prince\"])    #3\ntrain_dataset_prince, val_dataset_prince = datasets #3\nprince_model = train_seq2seq_model(    #3\n    prince_model_path,     #3\n    train_dataset_prince,     #3\n    val_dataset_prince, #3\n    epochs=6   #3\n)    #3\n\nking_and_prince = [f\"{k} {p}\" for k, p in zip(df[\"King\"], df[\"Prince\"])]     #4\ndf[\"King and Prince\"] = king_and_prince    #4\ntrain_dataset_kingdom, val_dataset_kingdom = create_seq2seq_datasets(    #4\n    df[\"King and Prince\"], df[\"Kingdom\"]    #4\n)    #4\nkingdom_model = train_seq2seq_model(    #4\n    kingdom_model_path,    #4\n    train_dataset_kingdom,     #4\n    val_dataset_kingdom,     #4\n   epochs=6    #4\n)  #4\n```", "```py\np2k_data = create_seq2seq_datasets(    \n    df[\"Prince\"], df[\"King\"])    \ntrain_dataset_prince2king, val_dataset_prince2king = p2k_data    \nprince2king_model = train_seq2seq_model(    \n    prince2king_model_path,    \n    train_dataset_prince2king,    \n    val_dataset_prince2king,    \n    epochs=6    \n)\n```", "```py\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import (\n    AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n    AutoTokenizer, GPT2LMHeadModel,\n    PreTrainedModel, BartForConditionalGeneration)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nking_model = AutoModelForCausalLM.from_pretrained(     #1\n    \"osazuwa/causalLLM-king\").to(DEVICE)    #1\nprince_model = AutoModelForSeq2SeqLM.from_pretrained(    #1\n    \"osazuwa/causalLLM-prince\").to(DEVICE)    #1\nkingdom_model = AutoModelForSeq2SeqLM.from_pretrained(    #1\n    \"osazuwa/causalLLM-kingdom\").to(DEVICE)     #1\nprince2king_model = AutoModelForSeq2SeqLM.from_pretrained(     #1\n    \"osazuwa/causalLLM-prince2king\").to(DEVICE)     #1\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")    #2\ntokenizer.pad_token = tokenizer.eos_token  #2\n```", "```py\ndef encode(text:str, device=DEVICE) -> torch.tensor:    #1\n    input_ids = tokenizer.encode(text, return_tensors=\"pt\")   #1\n    input_ids = input_ids.to(device)    #1\n    return input_ids   #1\n\ndef decode(text_ids: torch.tensor) -> str:    #2\n    output = tokenizer.decode(text_ids, skip_special_tokens=True)     #2\n    return output     #2\n\nEMPTY_TEXT = torch.tensor(tokenizer.encode(\"\")).unsqueeze(0).to(DEVICE)   #3\n\ndef generate_from_model(model: PreTrainedModel,     #4\n                        input_sequence: torch.tensor = EMPTY_TEXT,   #4\n                        max_length: int = 25,    #4\n                        temperature=1.0):   #4\n    output = model.generate(    #4\n        input_sequence,    #4\n        max_length=max_length,    #4\n        do_sample=True,   #4\n        pad_token_id=tokenizer.pad_token_id, #4\n        eos_token_id=tokenizer.pad_token_id,   #4\n        temperature=temperature,   #4\n        top_p=0.9,   #4\n    )   #4\n    return output    #4\n\ndef convert_to_text(output):\n    return decode(output[0]).strip().capitalize()\n```", "```py\ndef compute_log_probs(model, output_sequence):\n    if isinstance(model, GPT2LMHeadModel):     #1\n        outputs = model(     #1\n            input_ids=output_sequence,     #1\n            labels=output_sequence    #1\n        )     #1\n        log_softmax = torch.nn.functional.log_softmax(     #1\n            outputs.logits, dim=-1)     #1\n        log_probs = log_softmax.gather(2, output_sequence.unsqueeze(-1))     #1\n        log_probs = log_probs.squeeze(-1).sum(dim=-1)     #1\n    elif isinstance(model, BartForConditionalGeneration):\n        outputs = model(    #2\n            input_ids=output_sequence,    #2\n            labels=output_sequence)     #2\n        loss = outputs.loss  #2\n        log_probs = -loss * output_sequence.size(1)    #2\n    else:\n        raise ValueError(\"Unsupported model type\")\n    return torch.tensor(log_probs.item())\n```", "```py\nking_output = generate_from_model(king_model)   #1\nking_statement = convert_to_text(king_output)   #1\nprint(\"Generated from king_nodel:\", king_statement)    #1\nlog_prob_king = compute_log_probs(king_model, king_output)    #1\nprint(\"Log prob of generated king text:\", log_prob_king)    #1\n\nprince_output = generate_from_model(prince_model, king_output)     #2\nprince_statement = convert_to_text(prince_output)   #2\nprint(\"Generated from prince_model:\", prince_statement)    #2\nlog_prob_prince = compute_log_probs(prince_model, prince_output)    #2\nprint(\"Log prob of generated prince text:\", log_prob_prince)     #2\n\nking_prince_statement = king_statement + \". \" + prince_statement     #3\nking_prince_output = encode(king_prince_statement)    #3\nkingdom_output = generate_from_model(kingdom_model, king_prince_output)     #3\nkingdom_statement = convert_to_text(kingdom_output)    #3\n\nprint(\"Generated from kingdom model:\", kingdom_statement)   #3\nlog_prob_kingdom = compute_log_probs(kingdom_model, kingdom_output)    #3\nprint(\"Log prob of generated kingdom text:\", log_prob_kingdom)    #3\n\nking_output_infer = generate_from_model(prince2king_model, prince_output)     #4\nking_statement_infer = convert_to_text(king_output_infer)   #4\nprint(\"Generated statement from prince2king:\", king_statement_infer)    #4\nlog_prob_prince2king = compute_log_probs(prince2king_model, prince_output)    #4\nprint(\"Log prob of generated inference text:\", log_prob_prince2king)    #4\n```", "```py\nGenerated statement from king_model: The king, driven by ambition, declares war on a neighboring nation to expand his kingdom's territories, declares war on. \nLog probability of generated king_model: tensor(-325.8379)\nGenerated statement from prince_model: The prince, disillusioned by his father's actions, abdicates the throne in protest. \nLog probability of generated prince text: tensor(-18.2486)\nGenerated statement from kingdom model: As the war drags on, resources are depleted, and the once-prosperous kingdom falls. \nLog probability of generated kingdom text: tensor(-38.3716)\nGenerated statement from prince2king: A king, driven by greed, declares war on a neighboring kingdom. \nLog probability of generated inference text: tensor(-297.3446)\n```", "```py\nimport pyro\nfrom pyro.distributions.torch_distribution \\\nimport TorchDistributionMixin\n\nclass TransformerModelDistribution(TorchDistributionMixin):\n    def __init__(self, model: PreTrainedModel,\n                 input_encoding: torch.tensor = EMPTY_TEXT,\n                ):\n        super().__init__()\n        self.model = model\n        self.input_encoding = input_encoding\n\n    def sample(self, sample_shape=torch.Size()):     #1\n        output = generate_from_model(    #1\n            self.model, self.input_encoding     #1\n        )    #1\n        return output     #1\n\n    def log_prob(self, value):     #2\n        return compute_log_probs(self.model, value)     #2\n```", "```py\ndef causalLLM():  #1\n    king = pyro.sample(     #2\n        \"King\", TransformerModelDistribution(king_model)     #2\n    )    #2\n    prince = pyro.sample(     #3\n        \"Prince\", TransformerModelDistribution(   #3\n            prince_model, king)   #3\n    )     #3\n    king_and_prince = torch.cat([king, prince], dim=1)     #4\n    kingdom = pyro.sample(    #4\n        \"Kingdom\", TransformerModelDistribution(    #4\n            kingdom_model, king_and_prince)     #4\n    )    #4\n    king_text = convert_to_text(king)     #5\n    prince_text = convert_to_text(prince)    #5\n    kingdom_text = convert_to_text(kingdom)     #5\n    return king_text, prince_text, kingdom_text     #5\n\nfor _ in range(2):     #6\n    king, prince, kingdom = causalLLM()    #6\n    vignette = \" \".join([king, prince, kingdom])     #6\n    print(vignette)    #6\n```", "```py\nAnd beloved king falls gravely ill, leaving the kingdom in despair in uncertainty over the inexperienced prince to lead the kingdom. The young prince, eager to prove himself, leads the army into a costly and ill-advised war. As a result, the kingdom's resources are depleted, plunging the once-prosperous land into.\nKing, fueled by ambition, declares war on a neighboring realm, leaving his subjects anxious and. The prince, disillusioned by his father's actions, abdicates the throne in search of a new life. Without strong leadership, the kingdom spirals into poverty and despair.\n```", "```py\nimport pyro.poutine as poutine\nfrom pyro.distributions import Categorical\n\nPRINCE_STORY = (     #1\n    \"His courageous Prince takes command, leading \"  #1\n    \"the kingdom's army to victory in battle after battle\")     #1\ncond_model = pyro.condition(    #1\n    causalLLM, {\"Prince\": encode(PRINCE_STORY)})   #1\n\ndef proposal_given_prince():    #2\n    prince = encode(PRINCE_STORY) #2\n    king = pyro.sample(   #3\n        \"King\",     #3\n        TransformerModelDistribution(prince2king_model, prince)   #3\n    )    #3\n    king_and_prince = torch.cat([king, prince], dim=1)   #3\n    kingdom = pyro.sample(     #4\n        \"Kingdom\",    #4\n        TransformerModelDistribution(kingdom_model, king_and_prince)    #4\n    )    #4\n    vignette = (convert_to_text(king) +    #5\n        PRINCE_STORY +  #5\n        convert_to_text(kingdom))  #5\n    return vignette\n```", "```py\ndef process_sample(model, proposal):\n    sample_trace = poutine.trace(proposal).get_trace()    #1\n    king_text = convert_to_text(sample_trace.nodes['King']['value'])  #1\n    kingdom_text = convert_to_text( #1\n        sample_trace.nodes['Kingdom']['value'])     #1\n    proposal_log_prob = sample_trace.log_prob_sum()     #2\n    replay = poutine.replay(model, trace=sample_trace)    #3\n    model_trace = poutine.trace(replay).get_trace()     #3\n    model_log_prob = model_trace.log_prob_sum()   #3\n    log_importance_weight = model_log_prob - proposal_log_prob    #4\n    sample = (king_text, kingdom_text, log_importance_weight)\n    return sample\n```", "```py\ndef do_importance_resampling(model, proposal, num_samples):    #1\n    original_samples = []\n    for _ in range(num_samples):\n        sample = process_sample(model, proposal)\n        original_samples.append(sample)\n    unique_samples = list(set(original_samples))     #2\n    log_importance_weights = torch.tensor(   #2\n        [sample[2] for sample in original_samples]) #2\n    resampling_dist = Categorical(logits=log_importance_weights)     #2\n    resampled_indices = resampling_dist.sample_n(num_samples)     #2\n    samples = pd.DataFrame(   #2\n        [unique_samples[i] for i in resampled_indices],      #2\n        columns=[\"King\", \"Kingdom\", \"log_importance_weight\"]    #2\n    )     #2\n    samples[\"Prince\"] = PRINCE_STORY\n    samples[\"Distribution\"] = \"observational\"\n    return samples[['King', 'Prince', 'Kingdom', 'Distribution']]\n\nnum_samples = 1000\nposterior_samples = do_importance_resampling(\n    cond_model, proposal_given_prince, num_samples)\n```", "```py\nintervention_model = pyro.do(     #1\n    causalLLM, {\"Prince\": encode(PRINCE_STORY)})   #1\nintervention_samples = pd.DataFrame(     #2\n    [intervention_model() for _ in range(num_samples)],     #2\n    columns=[\"King\", \"Prince\", \"Kingdom\"]    #2\n)   #2\nintervention_samples[\"Distribution\"] = \"interventional\"     #2\nall_samples = pd.concat(   #2\n    [posterior_samples, intervention_samples],    #2\n    ignore_index=True     #2\n)    #2\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nkingdom_samples_url = (\n    \"https://raw.githubusercontent.com/altdeep/causalML/\"\n    \"master/book/chapter%2013/kingdom_samples.csv\")\nall_samples = pd.read_csv(kingdom_samples_url)\n\nobservational_texts = all_samples[     #1\n    all_samples[\"Distribution\"] == \"observational\"][\"Kingdom\"]   #1\ninterventional_texts = all_samples[all_samples[  #1\n    \"Distribution\"] == \"interventional\"][\"Kingdom\"]   #1\n\n    vectorizer = TfidfVectorizer(stop_words='english')     #2\nX_obs = vectorizer.fit_transform(observational_texts)    #2\nX_int = vectorizer.transform(interventional_texts)     #2\n\nk = 10     #3\nfeature_names = vectorizer.get_feature_names_out()     #3\nobs_indices = X_obs.sum(axis=0).argsort()[0, -k:][::-1]    #3\nint_indices = X_int.sum(axis=0).argsort()[0, -k:][::-1]    #3\ncombined_indices = np.concatenate((obs_indices, int_indices))    #3\ncombined_indices = np.unique(combined_indices)     #3\n```", "```py\nimport matplotlib.pyplot as plt\n\nlabels = [feature_names[i] for i in combined_indices]    #1\nlabels, indices = np.unique(labels, return_index=True)     #1\nobs_values = np.array(X_obs.sum(axis=0))[0, combined_indices]     #1\nint_values = np.array(X_int.sum(axis=0))[0, combined_indices]     #1\nobs_values = [obs_values[0][i] for i in indices]     #1\nint_values = [int_values[0][i] for i in indices]     #1\ncombined = list(zip(labels, obs_values, int_values))    #1\nsorted_combined = sorted(combined, key=lambda x: (-x[1], x[2]))    #1\nlabels, obs_values, int_values = zip(*sorted_combined)    #1\n\nwidth = 0.35    #2\nx = np.arange(len(labels))     #2\nfig, ax = plt.subplots()    #2\nrects1 = ax.bar(x - width/2, obs_values, width, #2\n                label='Observational', alpha=0.7)    #2\nrects2 = ax.bar(x + width/2, int_values, width, #2\n                label='Interventional', alpha=0.7)    #2\nax.set_xlabel('Words')     #2\nax.set_ylabel('TF-IDF Values')     #2\nax.set_title(    #2\n    'Top Words in Generated Kingdom Vignettes by TF-IDF Value')    #2\nax.set_xticks(x)     #2\nax.set_xticklabels(labels)    #2\nax.legend()    #2\nfig.tight_layout()    #2\nplt.xticks(rotation=45)   #2\nplt.show()     #2\n```"]