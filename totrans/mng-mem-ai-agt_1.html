<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" class="pagenumrestart" data-pdf-bookmark="Chapter 1. A Deep Dive into Agent Memory Systems"><div class="chapter" id="ch01_a_deep_dive_into_agent_memory_systems_1758256567644223">
      <h1><span class="label">Chapter 1. </span>A Deep Dive into Agent <span class="keep-together">Memory Systems</span></h1>
      <p>An agent’s <em>memory</em> is, at its core, synonymous with data, storage, and retrieval. Anytime you hear “memory management,” you would be right to interpret this as “data management.” And data management, it turns out, is a concept we have a great deal of knowledge about in the world of software engineering. So why is it then that we need entire reports on memory management in AI agents? The answer, it turns out, is that while data management is an understood entity, the <em>usage</em> of data for AI agents is fundamentally different from anything that tool engineers have encountered before.</p>
      <p>Recall that agents are nondeterministic systems, programmed with the ability to use tools and constraints that generally guide them—but nondeterministic all the same. To these agents, some data is more relevant than others, all data takes up space, and all agents have a limited amount of space—or context windows—in which to process. This fundamental insight shapes everything about how we design, implement, and manage agent memory systems.</p>
      <p>Because the way in which agents use data differs fundamentally from traditional software programs, memory is often used as a stand-in for human cognition. The thinking is that if agents use information to generate nondeterministically—weighing recent information more heavily than older information, taking into account user context and preferences, and adapting to new inputs dynamically—then perhaps memory is the appropriate analogy for how agents work.</p>
      <p>This tension between humanlike behavior and fundamentally different architectures defines the current state of agent memory systems. In this chapter, we’ll explore how the industry is navigating these challenges through short-term memory management, long-term persistence strategies, emerging technologies, and enhancement techniques like named entity recognition.</p>
      <section data-type="sect1" data-pdf-bookmark="Understanding Agent Memory Systems"><div class="sect1" id="ch01_understanding_agent_memory_systems_1758256567644405">
        <h1>Understanding Agent Memory Systems</h1>
        <p>Not every system will be the same for agents, and they are certainly going to change in the future—they’re changing as we speak! That’s why focusing on specific architectures is much less important than understanding higher concepts. Although agents aren’t classically programmable, they follow classic computer science principles when it comes to memory management.</p>
        <p>Think of agent memory as like RAM in a computer. Even as context windows expand, it’s generally true that the more applicable and concise the information, and the more direct the query, the better the results will be. This isn’t just about efficiency; it’s about the fundamental nature of language paired with the systems that process this information.</p>
        <p>As there are different architectures for agent systems, there are also different classifications of memory. Some distinguish between sensory memory (ingesting information like images, audio, and haptic feedback), short-term or working memory (an active memory buffer of conversation history), and long-term memory (storage relevant to the agent’s or user’s life or work).<sup><a data-type="noteref" id="id57-marker" href="ch01.html#id57">1</a></sup> Others focus on splitting between short-term and long-term memory, with subcategories for long-term memory including episodic (specific past events), procedural (contextual working knowledge and learned skills), and semantic (general world knowledge).<sup><a data-type="noteref" id="id58-marker" href="ch01.html#id58">2</a></sup> Much of this thinking is influenced by the field of psychology, which can categorize human <span class="keep-together">memory</span> similarly. These distinctions help tailor how agents interact with humans, with systems, and even with other agents. In particular, they influence how memory is stored and retrieved.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Memory Storage and Representation"><div class="sect1" id="ch01_memory_storage_and_representation_1758256567644473">
        <h1>Memory Storage and Representation</h1>
        <p>Almost all memory is embedded into vectors of continuous numbers meaningful to specific large language models (LLMs) and then stored in vector databases. Some of this information can be considered knowledge (embedded documents or working context), while other information is memory in the traditional sense (user preferences, standing instructions, or relevant past answers). Then there are episodic memories that don’t need to be kept, such as random interactions or questions that don’t appear to have lasting relevance.</p>
        <p>The process of deciding what constitutes each type is where the complexity begins. Unlike traditional databases where you explicitly define schemas and relationships, agent memory systems must make these determinations dynamically, often with imperfect <span class="keep-together">information</span>.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="The Challenge of Nondeterministic Systems"><div class="sect1" id="ch01_the_challenge_of_nondeterministic_systems_1758256567644523">
        <h1>The Challenge of Nondeterministic Systems</h1>
        <p>Those of us who work with agents every day know the frustration: instructions explicitly given in the past that are no longer retained in a new session or, worse, instructions neglected during a longer session with the same agent. But even if memories are retained, that doesn’t mean the agent will act as you’d expect. Because agents are not programmable in the traditional sense, giving one the same task twice may yield markedly different results.</p>
        <p>Consider a research assistant that pulls different sources for the same question. This variability extends to coding agents, too. After all, asking a coding agent to scan and iteratively improve a large legacy codebase is a far more difficult task than asking one to build a new repository from scratch. Instructing an agent to “alter the search bar” in a system that could have many search bars leaves significant room for interpretation. What agents retrieve for you largely depends on factors such as the size of the context the agent must ingest, a complex interplay of their embeddings, their similarity metrics, and the specific phrasing of your query.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Storage and Retrieval: The Core Challenge"><div class="sect1" id="ch01_storage_and_retrieval_the_core_challenge_1758256567644572">
        <h1>Storage and Retrieval: The Core Challenge</h1>
        <p>This bears repeating because it is the most crucial aspect of memory management in AI agent systems: the retention of knowledge is dynamic and stochastic, not just on the storage side but on the retrieval side as well.</p>
        <p>How do you decide what should be stored? What instructions are comprehensive enough to give a system? Do you store everything? Conversations can range from a few sentences to dozens of pages of text, depending on the user and use case. When storage gets tight, how do you flush the system?</p>
        <p>There are many strategies to address these challenges. Some popular methods include:</p>
        <dl>
          <dt>Importance scoring</dt>
          <dd>
            <p>Calculating memory importance based on recency, frequency of reference, user engagement metrics, and keyword relevance<sup><a data-type="noteref" id="id59-marker" href="ch01.html#id59">3</a></sup></p>
          </dd>
          <dt>Cascading memory systems</dt>
          <dd>
            <p>Allowing the agent itself to choose what to promote to long-term storage and what to retrieve<sup><a data-type="noteref" id="id60-marker" href="ch01.html#id60">4</a></sup></p>
          </dd>
          <dt>Intelligent compression</dt>
          <dd>
            <p>Using specialized models to condense conversation history into key details, events, and decisions<sup><a data-type="noteref" id="id61-marker" href="ch01.html#id61">5</a></sup></p>
          </dd>
          <dt>Vector store offloading</dt>
          <dd>
            <p>Moving older messages from short-term memory into vector stores, often with summarization<sup><a data-type="noteref" id="id62-marker" href="ch01.html#id62">6</a></sup></p>
          </dd>
        </dl>
        <p>Engineers typically compress information by instructing an LLM to summarize to the best of its ability. But summaries are not the same thing as the original. There is, by definition, loss of information.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="The Imprecision of Retrieval"><div class="sect1" id="ch01_the_imprecision_of_retrieval_1758256567644620">
        <h1>The Imprecision of Retrieval</h1>
        <p>Retrieval makes everything more complicated. Information retrieval is often based on similarity between texts, and because language is imprecise, retrieval will be fuzzy, too. The classic example is <em>bank</em>: it can be a financial institution or an aspect of a river. </p>
        <p>Depending on the model choice and embeddings, different algorithms must be used: cosine similarity, Euclidean distance, or even older methods like term frequency–inverse document frequency (TF-IDF). There’s no single way to search and retrieve information, with plenty of trade-offs between speed and accuracy.</p>
        <p class="loosen-letter-spacing">We’re certainly improving in the vector database space—local options like ChromaDB, Redis, PostgreSQL with pgvector, and Qdrant exist—but there’s still plenty of room for further <span class="keep-together">improvement</span>.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Managing Context Window Limitations"><div class="sect1" id="ch01_managing_context_window_limitations_1758256567644664">
        <h1>Managing Context Window Limitations</h1>
        <p>All models have context windows containing information provided as context for generating answers. There are different methods for managing these limitations. With FIFO (first in, first out), the earliest information received over a long conversation may be least accurately recalled, meaning that more recent information gets <span class="keep-together">prioritized</span>.</p>
        <p>Strategies to address this include intelligent pruning, where a model removes superfluous information. But there are consequences to this. Consider summarized information about a legal text: you might get the broad strokes of the legal argument and topics but lose critical information, like a negation or case reference that completely changes the content. Summarization by definition means losing detail and taking a higher abstract perspective.</p>
        <p>Compare this to larger context windows like those in Gemini 2.5, which can handle millions of tokens. We can stuff more information into a model, but we may not have effective recall for the first-passed information relative to the last. It’d be nice if we lived in a world where models had perfect recall, but the architecture of transformers’ self-attention mechanisms requires quadratically more processing as context increases.</p>
        <p>Algorithms like FlashAttention attempt to work around this, but a more direct approach might be retrieval-augmented generation (RAG), which limits the corpus of documents and forces the LLM to return sourced information rather than stuffing superfluous information inside.</p>
        <p>An even more refined approach—one I believe will become more popular—is semantic caching. What if we retained the relative context of an information retrieval system over time by processing the semantics of the content being passed? Frequently retrieved information gets prioritized. For systems like internal LLMs or RAG where many users talk to the same corpus of information, it may be both more computationally effective and cost-effective to semantically cache that information. Semantic caching isn’t without its drawbacks—it works exceptionally well for single-shot questions but breaks down in multiturn conversations. These methods will continue to be refined, but the fundamental problems will remain the same: how can we most efficiently and effectively store and retrieve the information we desire?</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Persistence via Checkpointing"><div class="sect1" id="ch01_persistence_via_checkpointing_1758256567644709">
        <h1>Persistence via Checkpointing</h1>
        <p class="fix_tracking">Checkpointing is a crucial step for agents, especially those that engage in multiturn conversations. As agents interact with a user or a system, they periodically save their internal states (their memory) in order to persist information across sessions or long conversations. Different organizations, products, and systems approach this process differently. The key insight is that checkpointing isn’t just about saving state—it’s about making that state retrievable and actionable in the dynamic, nondeterministic world of agent <span class="keep-together">interactions</span>.</p>
        <p>There are many ways to handle checkpointing, and different teams use different tools. For example, Redis is a popular choice because it’s fast and works well for real-time applications. Some setups use Redis to save conversation threads or agent state, making it easy to restore context across sessions or even share memory between different parts of a system. Features like automatic cleanup (time to live) help keep things tidy, so you’re not stuck with a bunch of old, irrelevant data.<sup><a data-type="noteref" id="id63-marker" href="ch01.html#id63">7</a></sup></p>
        <p>Basically, checkpointing is about making sure agents don’t lose their place and that their memory is both persistent and practical in the unpredictable world of AI interactions.</p>
      </div></section>
    <div data-type="footnotes"><p data-type="footnote" id="id57"><sup><a href="ch01.html#id57-marker">1</a></sup> Michael Lanham, <em>AI Agents in Action</em> (Manning Publications, 2024), 200.</p><p data-type="footnote" id="id58"><sup><a href="ch01.html#id58-marker">2</a></sup> Manvinder Singh and Andrew Brookins, “Build Smarter AI Agents: Manage Short-Term and Long-Term Memory with Redis,” Redis Blog, April 29, 2025, <a href="https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/"><em class="hyperlink">https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis/</em></a>. </p><p data-type="footnote" id="id59"><sup><a href="ch01.html#id59-marker">3</a></sup> Singh and Brookins, “Build Smarter AI Agents.”</p><p data-type="footnote" id="id60"><sup><a href="ch01.html#id60-marker">4</a></sup> “Memory for Agents,” LangChain Blog, October 19, 2024, <a href="https://blog.langchain.com/memory-for-agents"><em class="hyperlink">https://blog.langchain.com/memory-for-agents</em></a>. </p><p data-type="footnote" id="id61"><sup><a href="ch01.html#id61-marker">5</a></sup> “How to Migrate to LangGraph Memory,” LangChain Documentation, <a href="https://python.langchain.com/docs/versions/migrating_memory"><em class="hyperlink"><span class="keep-together">https://python.langchain.com/docs/versions/migrating_memory</span></em></a>.</p><p data-type="footnote" id="id62"><sup><a href="ch01.html#id62-marker">6</a></sup> Singh and Brookins, “Build Smarter AI Agents.”</p><p data-type="footnote" id="id63"><sup><a href="ch01.html#id63-marker">7</a></sup> Brian Sam-Bodden, “LangGraph &amp; Redis: Build Smarter AI Agents with Memory &amp; Persistence,” Redis Blog, March 28, 2025, <a href="https://redis.io/blog/langgraph-redis-build-smarter-ai-agents-with-memory-persistence/"><em class="hyperlink">https://redis.io/blog/langgraph-redis-build-smarter-ai-agents-with-memory-persistence/</em></a>; Redis Official Documentation, Redis, <a href="https://redis.io/docs/"><em class="hyperlink">https://redis.io/docs/</em></a>.</p></div></div></section></div></div></body></html>