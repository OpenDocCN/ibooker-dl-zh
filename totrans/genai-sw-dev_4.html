<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Automated Testing and Quality Assurance"><div class="chapter" id="ch04_automated_testing_and_quality_assurance_1749441076310969">
      <h1><span class="label">Chapter 4. </span>Automated Testing and Quality Assurance</h1>
      <p>Testing and quality assurance (QA) are usually the last gates that new software code must pass through before it gets deployed in production. Their ultimate goal is to find costly bugs or other standout issues that may have made it through code review (as covered in the previous chapter) to avoid putting them into production. </p>
      <p>The QA process happens after code has been developed, reviewed, and accepted to merge into the codebase. There <a contenteditable="false" data-type="indexterm" data-primary="QA (quality assurance)" data-seealso="testing" id="id375"/><a contenteditable="false" data-type="indexterm" data-primary="testing" id="id376"/>is occasional confusion between testing and QA as concepts, perhaps because the stakeholders traditionally involved are called either testing engineers or QA engineers at different companies. Whatever the title, though, they are usually in charge of the process covered in this chapter.</p>
      <p>Typically, the QA process consists of conducting manual and/or automated tests in an environment that closely matches production and mimics user behavior, to catch any bugs that escaped the code-review process.</p>
      <p>When such bugs are found during testing/QA, the feature is regressed back to development status. The original software engineer in charge of implementation must fix the issues before pushing the feature to review and QA again. These regression loops aim to guarantee that the code that ultimately gets deployed to production is indeed bug-free.</p>
      <p>These processes are critical to any software development team. We can break them into two main categories: automated and manual. </p>
      <dl class="pagebreak-before less_space">
        <dt>Automated tests</dt>
        <dd>
          <p>Automated testing employs specialized <a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="automated tests" id="id377"/><a contenteditable="false" data-type="indexterm" data-primary="automated tests" id="id378"/>software tools to execute pre-scripted tests on the application. This method is highly efficient for repetitive and regression tests, because it reduces the time needed to validate new code changes. Automation ensures consistency and precision, minimizes the risk of human error, and enables extensive test coverage. Automated tests can run around the clock, providing rapid feedback and <a contenteditable="false" data-type="indexterm" data-primary="CI/CD (continuous integration and continuous delivery), testing and" id="id379"/><a contenteditable="false" data-type="indexterm" data-primary="continuous integration" id="id380"/><a contenteditable="false" data-type="indexterm" data-primary="continuous delivery" id="id381"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="CI/CD and" id="id382"/>allowing for continuous integration and continuous delivery (CI/CD) pipelines. Although initially setting up automated tests requires effort, as does maintaining them, the long-term benefits include faster release cycles, improved accuracy, and the ability to quickly detect and address defects.</p>
        </dd>
        <dt>Manual tests</dt>
        <dd>
          <p class="fix_tracking">In manual testing, human testers meticulously execute test cases without the assistance of <a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="manual testing" id="id383"/><a contenteditable="false" data-type="indexterm" data-primary="manual tests" id="id384"/>automated tools. They simulate end-user behavior to identify defects, ensuring that the software behaves as expected in real-world scenarios. This approach allows for nuanced understanding and adaptability, often catching issues that automated scripts might miss, such as user-interface glitches and usability concerns. While manual testing can be time-consuming and labor-intensive, it remains essential for exploratory testing, where creativity and intuition are crucial in uncovering unexpected bugs and ensuring a seamless user experience.</p>
        </dd>
      </dl>
      <p>QA is a meticulous, careful process by its nature, which often makes it a bottleneck that delays features going live. As such, there’s a market for AI tools that propose to accelerate different parts of this process. This chapter will focus on two of those tools in particular.</p>
      <p>AI is changing every aspect of automated testing. For example, until very recently, automating testing involved writing complex scripts. Now, however, many automated testing tools provide ways to create tests without writing a single line of code. With simple, plain English, you can create automated tests that check every component and functionality in your software application. Visual testing has also been simplified with AI-powered tools that automatically detect visual bugs, ensuring that your user interface looks and works as intended. These improvements make the testing process more effective and efficient, which allows testers to focus on improving the overall quality of the software. </p>
      <section data-type="sect1" data-pdf-bookmark="Types of AI Testing Tools"><div class="sect1" id="ch04_types_of_ai_testing_tools_1749441076311089">
        <h1>Types of AI Testing Tools</h1>
        <p>In addition to the automated/manual divide, we can also classify AI tools for software testing and quality assurance as <em>functional</em> and <em>nonfunctional</em>, based on the specific areas they target within the testing lifecycle. </p>
        <dl>
          <dt>Functional AI testing tools</dt>
          <dd>
            <p>As the name implies, <em>functional</em> testing tools verify that a software application <a contenteditable="false" data-type="indexterm" data-primary="tools" data-secondary="testing tools" data-tertiary="functional" id="id385"/><a contenteditable="false" data-type="indexterm" data-primary="testing tools" data-secondary="functional" id="id386"/>performs all of its intended functions accurately. These tools focus on what the system does. Their goal is to test whether the application’s internal components deliver the expected output. Functional testing tools handle unit tests, integration tests, visual tests, regression tests, and smoke tests, for example. </p>
          </dd>
          <dt>Nonfunctional AI testing tools</dt>
          <dd>
            <p><em>Nonfunctional</em> AI testing tools assess aspects of software that go beyond its <a contenteditable="false" data-type="indexterm" data-primary="tools" data-secondary="testing tools" data-tertiary="nonfunctional" id="id387"/><a contenteditable="false" data-type="indexterm" data-primary="testing tools" data-secondary="nonfunctional" id="id388"/>functional behavior, such as its performance, compatibility, usability, security, and reliability. These tools focus on evaluating the software’s <em>performance</em> rather than its behavior. They measure speed, response time, and resource utilization, to name a few. </p>
          </dd>
        </dl>
        <p>Tools in both categories aim to identify potential performance issues and security vulnerabilities. They use deep learning models trained on customer usage data, internal company documents, or even industry regulatory norms or standards. These algorithms can learn to identify patterns that may indicate performance bottlenecks or security risks. This underlying “intelligence” makes these AI tools important peers of humans in the QA stage of the software development lifecycle. The biggest gain to be reaped from using these tools is that they can apply their intrinsic testing acumen on large codebases in near-real time.</p>
        <p>A common frustration is that QA takes a long time, since complex products and extensive codebases usually have hundreds of different user journeys to test, and doing this manually is very time-consuming. Automated tools do not reduce the value of having a human in the loop, but they can certainly automate a lot of repetitive work, freeing human QA professionals to focus on the critical flows, ones that were changed in the last pull request, or whatever makes up the 20% of work that has 80% of the impact (as per the <a href="https://oreil.ly/qCXc5">Pareto principle</a>, so often used in software <span class="keep-together">development).</span> </p>
        <p class="fix_tracking">Many of the prominent tools I evaluate in this chapter combine functional and nonfunctional testing abilities, as they aim to integrate into various development environments. These tools can be used in different ways, depending on each team’s context and preferences. For instance, testing is one of the most significant aspects of the CI/CD process. Thanks to CI/CD-integrated <a contenteditable="false" data-type="indexterm" data-primary="tools" data-secondary="testing tools" data-tertiary="CI/CD processing" id="id389"/><a contenteditable="false" data-type="indexterm" data-primary="testing tools" data-secondary="CI/CD process" id="id390"/><a contenteditable="false" data-type="indexterm" data-primary="CI/CD (continuous integration and continuous delivery), testing and" id="id391"/><a contenteditable="false" data-type="indexterm" data-primary="continuous integration" id="id392"/><a contenteditable="false" data-type="indexterm" data-primary="continuous delivery" id="id393"/>testing tools, we now conduct tests continuously during development rather than waiting until after development. This continuous integration approach provides real-time feedback about your software’s performance and internal functioning.</p>
        <p>CI/CD-integrated AI testing tools automatically test changes made to your code after every build. Continuous testing ensures that issues are identified and addressed early in the development cycle, reducing the risk of defects in production. This approach promotes a culture of quality and allows for faster, more reliable software releases.</p>
        <p>In contrast,<em> </em>browser and cloud-based<em> </em>tools run tests in web browsers or the cloud, providing flexibility and accessibility. They allow testing on different devices and environments, without complex setups like IDEs and CI/CD-integrated tools.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Use Cases"><div class="sect1" id="ch04_use_cases_1749441076311144">
        <h1>Use Cases</h1>
        <p>Software developers and  engineering teams across various industries are integrating AI testing and QA tools into their <a contenteditable="false" data-type="indexterm" data-primary="tools" data-secondary="testing tools" data-tertiary="use cases" id="id394"/><a contenteditable="false" data-type="indexterm" data-primary="testing tools" data-secondary="use cases" id="id395"/>processes. Here are some of the prominent use cases that we’ve seen:</p>
        <dl>
          <dt>Automated test creation </dt>
          <dd>
            <p>Building test automations used to be <a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="automated tests" data-tertiary="creating" id="id396"/><a contenteditable="false" data-type="indexterm" data-primary="automated tests" data-secondary="creating" id="id397"/>very slow and time-consuming. It takes a lot of time and mental bandwidth to design and write test scripts, run regression tests, and do everything in between. This is what many AI-driven testing tools aim to help with, by generating comprehensive test scripts from plain English prompts within seconds. This natural language processing (NLP) method of scripting makes it easy to automate complex workflows. This, in turn, makes testing accessible to both technical and nontechnical stakeholders. AI-generated test scripts are usually based on user behavior and existing patterns in previous test data, which makes the tests more relevant and closer to what a human QA tester would create.</p>
          </dd>
          <dt>Improved test accuracy </dt>
          <dd>
            <p>Improving accuracy means <a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="accuracy" id="id398"/>fewer code bugs slip through the QA stage to production. AI algorithms’ superpower is that, unlike manual testers, they can capture patterns and anomalies at scale. Being trained on extensive codebases and past testing data helps them more easily spot nuances that might indicate an issue requiring the feature to be regressed.</p>
          </dd>
          <dt>Self-healing capabilities after encountering errors </dt>
          <dd>
            <p>AI testing tools with <em>self-healing</em> capabilities automatically detect and fix <a contenteditable="false" data-type="indexterm" data-primary="tools" data-secondary="testing tools" data-tertiary="self-healing capability" id="id399"/><a contenteditable="false" data-type="indexterm" data-primary="testing tools" data-secondary="self-healing capability" id="id400"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="errors, self-healing" id="id401"/>issues in test scripts when changes in the application’s UI or code cause tests to fail. This ensures that all tests remain functional and up to date without manual intervention. Historically, updates are one of the biggest challenges for QA teams, since a change in the UI means many tests written in the past also need to be changed. These AI tools can significantly reduce the maintenance burden on QA teams and make the QA process faster and more reliable.</p>
          </dd>
          <dt>Faster software-release cycles</dt>
          <dd>
            <p>By automating repetitive <a contenteditable="false" data-type="indexterm" data-primary="software release, testing and" id="id402"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="software release and" id="id403"/>tasks using AI testing tools, we can speed the release cycle of software applications tenfold. Developers can concentrate more of their time on innovating new features and enhancing the product instead of spending the entire day trying to catch bugs or write test scripts. Companies can also respond faster to market demands and user feedback.</p>
          </dd>
        </dl>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="The Need for Human Testers"><div class="sect1" id="ch04_the_need_for_human_testers_1749441076311195">
        <h1>The Need for Human Testers</h1>
        <p>It is important to remember that while these AI tools can do a great job catching issues and bugs that would eventually break production, the human instinct is still crucial during <a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="human testers" id="id404"/>testing. This is not just about the limitations of the tools reviewed here, nor their underlying AI algorithms. It goes beyond that. Software development teams don’t write 100% of their requirements and edge cases in an absolutely perfect way. </p>
        <p>I can speak from my own experience leading software teams for more than a decade: there are <em>always</em> changes and caveats based on last-minute user feedback, an ad hoc request from sales, or even a phone call from the CEO with a specific exception. While teams try hard to properly document all requirements and capture edge cases and test plans in the software development task descriptions, the result is never perfect. There are gaps. And because these are the written materials on which AI tools are trained, and they take project requirements as the ultimate instructions to test against, they’ll eventually miss some nuances of those requirements or ad hoc <span class="keep-together">exceptions.</span></p>
        <p>Even beyond that, frankly, there’s often specific context awareness that only humans can have. We need humans in order to adapt to industry-wide events or sensitive user concerns. Software development is a complex matter, and the more extensive a product and codebase are, the more likely it is that a purely AI-driven QA process will show its limitations and gaps.</p>
        <p>AI algorithms are only as good as the data used in training them. They can absolutely help a lot, as this chapter shows—especially with the repetitive grunt work, like testing an extensive list of user journeys and application flows. But human monitoring, review, and intervention are still needed for the critical parts of the process.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Evaluation Process"><div class="sect1" id="ch04_evaluation_process_1749441076311251">
        <h1>Evaluation Process</h1>
        <p>Most companies in the QA automation space cater to enterprise clients. This makes sense, given that enterprise companies tend to have larger teams, more extensive products, and <a contenteditable="false" data-type="indexterm" data-primary="tools" data-secondary="testing tools" data-tertiary="evaluating" id="aitttlvl"/><a contenteditable="false" data-type="indexterm" data-primary="testing tools" data-secondary="evaluating" id="tlaivlua"/>much higher quality-control standards. While this is totally fine and expected, it affected my selection process for tools to showcase here, since I gave preference to tools that can be accessed via a simple self-service sign-up process and that offer a free trial. This is a deliberate choice to make it easier for readers to act on what they read here, though it certainly leaves out some tools that required me to speak with their sales teams to negotiate a price package. I decided those tools were out of scope for this book.</p>
        <p>Even with that limitation, as I researched this chapter, I reviewed more than 20 automated testing tools (many of which fell into that enterprise sales category). I shortlisted the two tools highlighted next.</p>
        <p>To evaluate and compare AI-powered testing tools for this chapter, I used each tool to write and run test cases for <a href="https://katalon-demo-cura.herokuapp.com">a simple, straightforward test site</a>: a basic web application for booking appointments with a <span class="keep-together">medical</span> doctor. Since developing a comprehensive, end-to-end automated testing framework is a substantial undertaking, I focused on evaluating the specific AI features these testing tools offer, to demonstrate their potential for integrating AI into software testing.</p>
        <p>The examples in this book are not intended to represent a complete testing framework, but rather to demonstrate how to use AI-integrated features in automated testing tools. The primary objective of this chapter is to showcase AI’s possibilities and simplicity in the software testing domain, not to provide a production-ready solution.</p>
        <p>I evaluated how the AI features in these tools enhance various aspects of the testing process, such as generating test cases, creating test data, executing tests, and analyzing results. </p>
        <dl>
          <dt>Test site</dt>
          <dd>
            <p><em>https://katalon-demo-cura.herokuapp.com</em></p>
          </dd>
          <dt>App description</dt>
          <dd>
            <p>Web app with a login page for booking appointments with a medical doctor </p>
          </dd>
          <dt>Test description</dt>
          <dd>
            <p>Automate a series of actions on a healthcare service website. This test ensures that a patient can navigate the app and use it to successfully book an appointment to meet with the doctor. We want to see if everything works as it should on the app.</p>
          </dd>
          <dt>Steps</dt>
          <dd>
            <p>We intend to generate/create test cases that automatically evaluate whether: </p>
          
          <ul>
            <li>
              <p>The login page works perfectly </p>
            </li>
            <li>
              <p>Users can successfully book appointments if all the required fields are <span class="keep-together">updated</span></p>
            </li>
            <li>
              <p>The booking history records every booking made </p>
            </li>
          </ul>
        </dd>
          <dt>Test case 1</dt>
          <dd>
          <ol>
            <li>
              <p>Navigate to <em>https://katalon-demo-cura.herokuapp.com</em>.</p>
            </li>
            <li>
              <p>Click on the Make Appointment button.</p>
            </li>
            <li>
              <p>Set the text John Doe in the username field.</p>
            </li>
            <li>
              <p>Set encrypted text in the password field.</p>
            </li>
            <li>
              <p>Click on the Login button.</p>
            </li>
            <li>
              <p>Check if the user can successfully log in when the correct information is entered.</p>
            </li>
          </ol>
       </dd>
          <dt>Test case 2</dt>
          <dd>
            <p>On the Make Appointment page:</p>
          
          <ol>
            <li>
              <p>Select a Visit Date.</p>
            </li>
            <li>
              <p>Select the Medicare option.</p>
            </li>
            <li>
              <p>Select the “Apply for hospital readmission” option.</p>
            </li>
            <li>
              <p>Enter a comment in the text area.</p>
            </li>
            <li>
              <p>Book an appointment.</p>
            </li>
            <li>
              <p>Check that the user can successfully book an appointment 10 seconds after submitting the booking form with all the correct information.</p>
            </li>
          </ol>
          </dd>
      
          <dt>Test case 3 </dt>
          <dd>
          <ol>
            <li>
              <p>Toggle the menu.</p>
            </li>
            <li>
              <p>Access the history page by clicking on History.</p>
            </li>
            <li>
              <p>Confirm that the appointment just booked appears in the history.</p>
            </li>
          </ol>
        </dd>
        </dl>
        <p>Now, let’s examine the top-performing AI testing tools I found and see how they followed these instructions and evaluated the website using their AI features. </p>
        <section data-type="sect2" data-pdf-bookmark="Katalon Studio"><div class="sect2" id="ch04_katalon_studio_1749441076311312">
          <h2>Katalon Studio</h2>
          <p><a href="https://oreil.ly/xREhC">Katalon Studio</a>, launched by Katalon Inc. in 2015, is an automated <a contenteditable="false" data-type="indexterm" data-primary="Katalon Studio" id="kltnot"/><a contenteditable="false" data-type="indexterm" data-primary="QA (quality assurance)" data-secondary="Katalon Studio" id="aqktstd"/>software QA tool that supports testing for mobile applications, web apps, desktop apps, and APIs. The company’s website highlights that it has “embedded AI across our entire platform to test faster, see clearer, and streamline test automation with fewer bottlenecks.”</p>
          <p class="fix_tracking">Katalon Studio was the first tool in Katalon’s ecosystem. Since then, two additional tools have been added. Katalon Recorder is a browser automation extension for creating and running Firefox, Edge, and Chrome tests. Katalon TestOps is a test-orchestration platform that centralizes test planning and management activities, streamlining DevOps processes and enhancing cross-team collaboration.</p>
          <p>The AI-augmented testing features in Katalon include: </p>
          <ul>
            <li>
              <p>Generating Groovy code from plain English instructions (Groovy is the scripting language used for writing test cases in Katalon)</p>
            </li>
            <li>
              <p>Automatically generating test scripts based on prompts </p>
            </li>
            <li>
              <p>A Virtual Data Analyst feature that analyzes all your TestOps data and generates reports </p>
            </li>
            <li>
              <p>Self-healing capabilities </p>
            </li>
          </ul>
          <p>Katalon’s self-healing AI, as noted previously, automatically helps you fix tests that break during runs. You don’t have to manually maintain existing test scripts when you ship a new feature or change a component. Regression test plans are also handled automatically: the AI engine instantly reruns your existing functional and nonfunctional tests to ensure that your software’s previously developed and tested components still perform correctly even after you’ve added new changes. </p>
          <p>To create test cases in Katalon, you typically either record tests and playback or write test scripts with Groovy.</p>
          <section data-type="sect3" data-pdf-bookmark="Practical example"><div class="sect3" id="ch04_practical_example_1749441076311364">
            <h3>Practical example</h3>
            <p>In this example, I used StudioAssist AI, Katalon’s generative AI, which helps programmers write test cases from plain-language prompts. I used it to write test cases for the healthcare service website. For the sake of this test, I acted as a stakeholder who doesn’t know the Groovy syntax. I used the StudioAssist AI feature in Katalon to generate Groovy scripts, which set up my tests. I wrote the test I wanted in the prompt, and it created a test script for me in Groovy, which I then ran to evaluate the software. StudioAssist also helps explain the function of each line of code it generates.</p>
            <p>I created a new test project, set up a test folder, and navigated to the script tab to begin writing my tests. Here is the prompt I gave StudioAssist AI:</p>
            <p>Prompt: </p>
            <pre data-type="programlisting">I want to write a test case performing the following steps: 
1. Open the browser to <a href="https://katalon-demo-cura.herokuapp.com">https://katalon-demo-cura.herokuapp.com</a>
2. Click the make appointment button named 
'Page_CuraHomepage/btn_MakeAppointment' 
3. Fill username in the 'Page_Login/txt_Username' object with the value 
in the 'Username' variable
4. Fill the password in the Page_Login/txt_Password' object with the 
value in the 'Password' variable
5. Verify that the appointment div 'Page_CuraAppointment/div_Appointment' 
exists within 10 seconds. 
6. Close the browser  </pre>
            <p>Katalon StudioAssist generated <a contenteditable="false" data-type="indexterm" data-primary="Katalon Studio" data-secondary="Katalon StudioAssist" id="id405"/><a contenteditable="false" data-type="indexterm" data-primary="QA (quality assurance)" data-secondary="Katalon Studio" data-tertiary="Katalon StudioAssist" id="id406"/>test cases written in the correct Groovy syntax (see it in full in <a data-type="xref" href="#ch04_example_1_1749441076308740">Example 4-1</a>) that executed the test script when it was run (see Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch04_figure_1_1749441076304400">4-1</a>, <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch04_figure_2_1749441076304430">4-2</a>, and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch04_figure_3_1749441076304450">4-3</a>). </p>
            <figure><div id="ch04_figure_1_1749441076304400" class="figure">
              <img alt="" src="assets/gasd_0401.png" width="600" height="338"/>
              <h6><span class="label">Figure 4-1. </span>Generating tests with Katalon is intuitive when using the StudioAssist option in the UI</h6>
            </div></figure>
            <figure><div id="ch04_figure_2_1749441076304430" class="figure">
              <img alt="" src="assets/gasd_0402.png" width="600" height="281"/>
              <h6><span class="label">Figure 4-2. </span>Generating tests with Katalon takes a few seconds</h6>
            </div></figure>
            <figure><div id="ch04_figure_3_1749441076304450" class="figure">
              <img alt="" src="assets/gasd_0403.png" width="600" height="328"/>
              <h6><span class="label">Figure 4-3. </span>Tests generated by Katalon and executed on the StudioAssist UI</h6>
            </div></figure>
            <div data-type="example" id="ch04_example_1_1749441076308740">
              <h5><span class="label">Example 4-1. </span>Full code of the tests generated by Katalon</h5>
              <pre data-type="programlisting"><code>/* I want to write a Katalon Studio test case to perform the following steps.</code>
* 1. Open browser to the URL  'https://katalon-demo-cura.herokuapp.com/'
* 2. Click the make appointment button
* 3. Fill in the username and password fields based on the variables
* 4. Click the login button
* 5. Verify that the appointment div exists
* 6. Close the browser
* */
// Open browser to the URL 'https://katalon-demo-cura.herokuapp.com/'
WebUI.openBrowser('https://katalon-demo-cura.herokuapp.com/')
 
// Click the make appointment button
WebUI.click(findTestObject('Page_CURA Healthcare Service/a_Make Appointment'))
 
// Fill in the username and password fields based on the variables
WebUI.setText(findTestObject('Page_CURA Healthcare Service/input_Username_username'), 
                             'John Doe')
WebUI.setText(findTestObject('Page_CURA Healthcare Service/input_Password_password'), 
                             'ThisIsNotAPassword')
 
// Click the login button
WebUI.click(findTestObject('Page_CURA Healthcare Service/button_Login'))
 
// Verify that the appointment div exists
WebUI.verifyElementPresent(
      findTestObject('Page_CURA Healthcare Service/div_Appointment'))
 
// Close the browser
WebUI.closeBrowser()
 </pre>
            </div>
            <p>As you can see, the generated test fulfills the instructions I provided, and the code is written in the correct syntax.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Pros"><div class="sect3" id="ch04_pros_1749441076311409">
            <h3>Pros</h3>
            <ul>
              <li>
                <p>StudioAssist is easy for nontechnical users to use and debug, since it transforms natural-language prompts into the correct Groovy testing syntax.</p>
              </li>
              <li>
                <p>Built-in keywords and templates speed up the test-creation process and reduce the need for extensive coding.</p>
              </li>
              <li>
                <p>Its self-healing capabilities automatically update test scripts when there are changes to the application’s UI. </p>
              </li>
              <li>
                <p>StudioAssist integrates with popular CI/CD tools and testing frameworks like Jenkins, Git, and Jira.</p>
              </li>
            </ul>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Cons"><div class="sect3" id="ch04_cons_1749441076311455">
            <h3>Cons</h3>
            <ul>
              <li>
                <p>Katalon requires you to download and install StudioAssist (shown in the preceding screenshots). This adds some additional setup work.</p>
              </li>
              <li>
                <p>Katalon can sometimes be slow, particularly when dealing with large test suites or complex test scenarios.</p>
              </li>
              <li>
                <p>There is a bit of a learning curve with the Katalon StudioAssist UI. Some options are buried inside the cascade options from the top bar, and you’ll need to learn keyboard shortcuts.</p>
              </li>
            </ul>
            <p>I rate Katalon a 9 out of 10. It helps a lot with writing tests from plain English text prompts and executing them against the application I want to test, both within the same UI. The only <a contenteditable="false" data-type="indexterm" data-primary="Katalon Studio" data-startref="kltnot" id="id407"/><a contenteditable="false" data-type="indexterm" data-primary="QA (quality assurance)" data-secondary="Katalon Studio" data-startref="aqktstd" id="id408"/>reason I won’t rate it 10/10 is the learning curve pointed out in the cons list. It could certainly be more intuitive, although this is a typical UX for complex enterprise products, which Katalon already is.</p>
            <p>Let’s turn now to the second tool.</p>
          </div></section>
        </div></section>
        <section data-type="sect2" class="pagebreak-before" data-pdf-bookmark="testRigor "><div class="sect2" id="ch04_testrigor_1749441076311501">
          <h2 class="less_space">testRigor </h2>
          <p>The next tool I tested is <a href="https://oreil.ly/OM9lJ">testRigor</a>, an AI-driven automated <a contenteditable="false" data-type="indexterm" data-primary="testRigor" id="tstrgrio"/><a contenteditable="false" data-type="indexterm" data-primary="QA (quality assurance)" data-secondary="testRigor" id="aqttrgr"/>tool designed to streamline software testing. Unlike traditional testing tools, testRigor allows developers to create and execute tests without writing code. Its NLP capabilities allow you to describe your application’s functionality in plain English. The AI then generates, executes, and reports on test cases, significantly reducing the time and technical expertise required for comprehensive software testing. </p>
          <section data-type="sect3" data-pdf-bookmark="Practical example"><div class="sect3" id="ch04_practical_example_1749441076311549">
            <h3>Practical example</h3>
            <p>In my evaluation of testRigor, one feature that really stood out was its completely codeless test-creation process. I did not have to write a single line of test code. Instead, I provided my test site URL and a brief description of my application and how it should behave. I also provided my test goals and specified the number of test cases to generate. The AI handled everything, from generating tests to executing them to generating a detailed test report (see Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch04_figure_4_1749441076304466">4-4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch04_figure_5_1749441076304482">4-5</a>). </p>
            <figure><div id="ch04_figure_4_1749441076304466" class="figure">
              <img alt="" src="assets/gasd_0404.png" width="600" height="520"/>
              <h6><span class="label">Figure 4-4. </span>Prompt and description to generate test case</h6>
            </div></figure>
            <figure><div id="ch04_figure_5_1749441076304482" class="figure">
              <img alt="" src="assets/gasd_0405.png" width="600" height="279"/>
              <h6><span class="label">Figure 4-5. </span>Tests were executed against the testing app and passed successfully </h6>
            </div></figure>
            <p>The goal of the testing, as you may recall, was to check whether a user can log in in less than 10 seconds and successfully book an appointment. </p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Pros"><div class="sect3" id="ch04_pros_1749441076311594">
            <h3>Pros</h3>
            <ul>
              <li>
                <p>testRigor uses Behavior-Driven Test Case Creation, which allows for the creation of tests based on how users interact with the application. This bypasses the technicalities of testing syntax, which can prevent attrition for nontechnical users or smaller teams.</p>
              </li>
              <li>
                <p>testRigor’s testing product is very accessible, which makes it stand out from the crowd. It’s fully cloud-based, which eliminates the need to install additional software (unlike Katalon). This makes it easy to access and use from anywhere. </p>
              </li>
              <li>
                <p>It integrates with popular CI/CD pipelines like Jenkins and CircleCI and supports bug-tracking tools like Jira, which make it seamless to integrate with the tools that teams are already using.</p>
              </li>
              <li>
                <p>The self-healing functionality, just like Katalon’s, reduces the maintenance burden on the testing team whenever existing application workflows are changed.</p>
              </li>
            </ul>
          </div></section>
          <section data-type="sect3" class="pagebreak-before" data-pdf-bookmark="Cons"><div class="sect3" id="ch04_cons_1749441076311639">
            <h3 class="less_space">Cons</h3>
            <ul>
              <li>
                <p>Bypassing actual test writing is great for smaller teams and occasional users, but I doubt it would be practical for larger teams that already have a large testing infrastructure in place. For those software teams (which are the majority), the value of automated testing is to generate the tests in correct syntax.</p>
              </li>
              <li>
                <p>A cascade con of this bypass is that testRigor doesn’t offer the same flexibility and control as traditional testing languages and frameworks. It would not work well for complex test scenarios or extensive application workflows.</p>
              </li>
            </ul>
            <p>Due to these limitations, I rate testRigor a 7 out of 10. Beyond that, it’s a great UX that “just works,” and it’s a perfect fit for smaller teams that don’t have a complex testing infrastructure in place already, or teams whose testing needs are occasional and who just <a contenteditable="false" data-type="indexterm" data-primary="tools" data-secondary="testing tools" data-tertiary="evaluating" data-startref="aitttlvl" id="id409"/><a contenteditable="false" data-type="indexterm" data-primary="testing tools" data-secondary="evaluating" data-startref="tlaivlua" id="id410"/><a contenteditable="false" data-type="indexterm" data-primary="testRigor" data-startref="tstrgrio" id="id411"/><a contenteditable="false" data-type="indexterm" data-primary="QA (quality assurance)" data-secondary="testRigor" data-startref="aqttrgr" id="id412"/>want to check that the product is working as per the requirements.</p>
          </div></section>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Tool Comparison"><div class="sect1" id="ch04_tool_comparison_1749441076311684">
        <h1>Tool Comparison</h1>
        <p>Katalon and testRigor have strengths that cater to different testing needs, though both leverage AI and machine learning to enhance their functionalities. <a data-type="xref" href="#ch04_table_1_1749441076306640">Table 4-1</a> provides a comparison.</p>
        <dl>
          <dt>Katalon</dt>
          <dd>
            <p class="fix_tracking">Katalon offers a robust <a contenteditable="false" data-type="indexterm" data-primary="Katalon" id="id413"/>suite of features designed to handle complex test scenarios. It is particularly useful for large-scale testing projects where comprehensive test coverage is critical, and for software development teams that already have a testing infrastructure, team, and processes in place. While the learning curve is steeper than with testRigor, Katalon’s depth of features and flexibility in handling diverse testing requirements make it a powerful tool for a larger number of software development teams, especially larger ones or those working on complex products.</p>
          </dd>
          <dt>testRigor</dt>
          <dd>
            <p class="fix_tracking">I was impressed with testRigor’s simplicity and ease of use. The learning curve is notably short, and I found it remarkable how fast I went from signup to actual test results. This tool excels in environments where product features change frequently, requiring rapid and continuous testing. I’d say testRigor is best suited for startup teams or occasional one-off users who don’t have an existing testing infrastructure in place and whose product requirements may change too often for them to even set up such a robust testing environment. On the other hand, testRigor poses limitations for those teams where Katalon excels; that is, for larger teams and more complex product workflows.</p>
          </dd>
        </dl>
        <table id="ch04_table_1_1749441076306640">
          <caption><span class="label">Table 4-1. </span>AI testing tools overview</caption>
          <thead>
            <tr>
              <th>Tool</th>
              <th>UX</th>
              <th>Test performance</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Katalon</td>
              <td>Repository</td>
              <td>9/10</td>
            </tr>
            <tr>
              <td>testRigor</td>
              <td>Browser </td>
              <td>7/10</td>
            </tr>
          </tbody>
        </table>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="ch04_conclusion_1749441076311732">
        <h1>Conclusion</h1>
        <p>Of the tools analyzed in this chapter, Katalon emerged as a good pick for larger teams and enterprise products, while testRigor proved to be a winner for startups and side products. That covers the software-development market nicely, and showcases how teams with different types of products and levels of maturity can benefit from using AI testing tools.</p>
        <p>If you’ve ever worked in software testing or QA, or if you’ve simply written unit tests for any code you wrote, you’ll know how laborious it is to write tests and keep them updated as an application evolves and gets extended.</p>
        <p>I’ve often been part of conversations about budget planning and roadmap discussions where robust testing was postponed, or outdated tests were simply framed as technical debt that should be phased out. It’s very common for both technical and nontechnical stakeholders to have biases against proper testing practices, and one of the key reasons for that is how significant an investment it has been, historically, to have them.</p>
        <p>That brings us to the bulk of the value that AI testing tools can bring to the table. In software development, we’re constantly looking for occurrences of the Pareto principle: “What’s the 20% of effort that will return 80% of this roadmap item’s value?” As a CTO, I’ve been in the center of these discussions many times. In QA, the 20% of effort that creates 80% of value involves defining the application workflow properly; talking with users and clients about the issues and edge cases; going the extra mile to map out nuances for the software developers who will implement the requirements; and, ultimately, conducting user acceptance testing as a final gatekeeper before going live.</p>
        <p>The other 80% of effort, which creates 20% of value, is the actual grunt work of writing and executing each test to verify if the code fulfills the requirements. AI tools excel at this task. Being able to provide instructions in natural language and get back tests written in proper syntax, ready to execute, is a huge time-saving use case. Having those self-healing capabilities to update tests whenever application code is changed is a great backstop for when tests become deprecated and are simply commented out, as pressing priorities emerge to get a release to production.</p>
        <p class="pagebreak-before less_space">These are the day-to-day decisions that so often relegate proper software testing to a second-order priority. AI tools can help alleviate those concerns and contribute to ensuring that software running in production is properly tested and bug-free. This can’t be done by AI tools <em>alone</em>, since these tools won’t replace humans. Quite the opposite: the human tasks in QA are critical, as they define the scope of testing and serve as key guidelines for the AI tools to do the grunt work at a high quality <span class="keep-together">standard.</span></p>
        <p>Once again, “AI + human” is a combination that improves an often-frustrating process to produce a higher-quality output.</p>
      </div></section>
    </div></section></div></div></body></html>