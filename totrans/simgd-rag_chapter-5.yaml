- en: '5 RAG evaluation: Accuracy, relevance, and faithfulness'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The need and requirements for evaluating RAG pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics, frameworks, and benchmarks for RAG evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current limitations and future course of RAG evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapters 3 and 4 discussed the development of retrieval-augmented generation
    (RAG) systems using the indexing and generation pipelines. RAG promises to reduce
    hallucinations and ground the large language model (LLM) responses in the provided
    context, which is done by creating a non-parametric memory or knowledge base for
    the system and then retrieving information from it.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers the methods used to evaluate how well the RAG system is
    functioning. We need to make sure that the components of the two RAG pipelines
    are performing per the expectations. At a high level, we need to ensure that the
    information being retrieved is relevant to the input query and that the LLM is
    generating responses grounded in the retrieved context. To this end, there have
    been several frameworks developed over time. Here we discuss some popular frameworks
    and the metrics they calculate.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a second aspect to evaluation. While the frameworks allow for
    the calculation of metrics, how do you make sure that your RAG pipelines are working
    better than those developed by other developers? The evaluations cannot be done
    in isolation. For this purpose, several benchmarks have been established. These
    benchmarks evaluate the RAG systems on preset data, such as question–answer sets,
    for accurate comparison of different RAG pipelines. These benchmarks help developers
    evaluate the performance of their systems vis-à-vis those developed by other developers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, like RAG techniques, the research on RAG evaluations is still in progress.
    There are still some limitations in the current set of evaluation parameters.
    We discuss these limitations and some ideas on the way forward for RAG evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should
  prefs: []
  type: TYPE_NORMAL
- en: Know the fundamentals of RAG evaluations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware of the popular frameworks, metrics, and benchmarks for RAG evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the limitations and best practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to evaluate the RAG pipeline in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For RAG to live up to the promise of grounding the LLM responses in data, you
    will need to go beyond the simple implementation of indexing, retrieval, augmentation,
    and generation. We will discuss these advanced strategies in chapter 6\. However,
    to improve something, you need to first measure the performance. RAG evaluations
    help in setting up the baseline of your RAG system performance for you to then
    improve it. First, we look at the fundamental aspects of RAG systems evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Key aspects of RAG evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building a PoC RAG pipeline is not overtly complex. It is achievable through
    brief training and verification of a limited set of examples. However, to enhance
    its robustness, thorough testing on a dataset that accurately mirrors the production
    use case is imperative. RAG pipelines can suffer from hallucinations of their
    own. This can be because
  prefs: []
  type: TYPE_NORMAL
- en: The retriever fails to retrieve the entire context or retrieves irrelevant context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite being provided the context, the LLM does not consider it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM picks irrelevant information from the context instead of answering the
    query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Retrieval and generation are two processes that need special focus from an
    evaluation perspective. This is because these two steps produce outputs that can
    be evaluated. (While indexing and augmentation will have a bearing on the outputs,
    they do not produce measurable outcomes). Here are several questions we need to
    ask ourselves about these two processes:'
  prefs: []
  type: TYPE_NORMAL
- en: How good is the retrieval of the context from the knowledge base?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it relevant to the query?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much noise (irrelevant information) is present?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How good is the generated response?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the response grounded in the provided context?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the response relevant to the query?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can ask many more questions such as these to assess the performance of your
    RAG system. Contemporary research has discovered certain scores to assess the
    quality and abilities of a RAG system. The following sections discuss three predominant
    quality scores and four main abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Quality scores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are three quality score dimensions prevalent in the discourse on RAG
    evaluation. They measure the quality of retrieval and generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Context relevanc**e*—This dimension evaluates how relevant the retrieved information
    or context is to the user query. It calculates metrics such as the precision and
    recall with which context is retrieved from the knowledge base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Answer faithfulness (also called groundedness**)*—This dimension evaluates
    whether the answer generated by the system is using the retrieved information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Answer relevanc**e*—This dimension evaluates how relevant the answer generated
    by the system is to the original user query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss how these scores are calculated in section 5.2
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Required abilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The quality scores are important for measuring how well the retrieval and the
    generation components of the RAG system are performing. At an overall level, there
    are certain critical abilities that a RAG system should possess:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Noise robustnes**s*—It is impractical to assume that the information stored
    in the knowledge base for RAG systems is perfectly curated to answer the questions
    that can be potentially asked. It is very probable that a document is related
    to the user query but does not have any meaningful information to answer it. The
    ability of the RAG system to separate these noisy documents from the relevant
    ones is termed noise robustness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Negative rejectio**n*—By nature, LLMs always generate text. There may be no
    information about the user query in the documents in the knowledge base. The ability
    of the RAG system not to give an answer when there is no relevant information
    is called negative rejection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Information integratio**n*—To obtain a comprehensive answer to a user query,
    it is also very likely the information must be retrieved from multiple documents.
    This ability of the system to assimilate information from multiple documents is
    called information integration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Counterfactual robustnes**s*—Sometimes the information in the knowledge base
    might itself be inaccurate. A high-quality RAG system should be able to address
    this problem and reject known inaccuracies in the retrieved information. This
    ability is known as counterfactual robustness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noise robustness is an ability that the retrieval component should possess,
    and other abilities are largely related to the generation component.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these, *latency* is another often-mentioned capability. Although
    it is a non-functional requirement, it is quite critical in generative AI applications.
    Latency is the delay that happens between the user query and the response. You
    may have observed that LLMs themselves have considerable latency before the final
    response is generated. Add to it the task of retrieval and augmentation, and the
    latency is bound to increase. Therefore, it is important to monitor how much time
    your RAG system takes from user input to response.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations are also at the forefront of generative AI adoption.
    For some RAG applications, it is important to measure the degree of *bias* and
    *toxicity*in the system responses. This is also influenced by the underlying data
    in the knowledge base. While it is not specific to RAG, it is important to evaluate
    the outputs for bias and toxicity.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect to check is the *robustness* of the system, that is, its ability
    to handle different types of queries. Some queries may be simple, while others
    may involve complex reasoning. Some queries may require comparing two pieces of
    information, while others may involve complex post-processing, like mathematical
    calculations. We will look at some types of queries when we discuss CRAG, a benchmark,
    in section 5.4.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is important to mention that these are scores and abilities that
    approach RAG at the core technique level. RAG, after all, is a means to solving
    the end use case. Therefore, you may have to build a *use case-specific*evaluation
    criteria for your RAG system. For example, a question-answering system may use
    an exact match (EM) or F1 score as a metric, and a summarization service may use
    ROUGE scores. Modern search engines using RAG may look at user interaction metrics,
    accuracy of source attribution, and similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the main idea behind evaluating RAG pipelines. The quality scores and
    the abilities that we discussed before need to be measured and benchmarked. There
    are two critical enablers of RAG evaluations: frameworks and benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Frameworks* are tools designed to facilitate evaluation, offering automation
    of the evaluation process and data generation. They are used to streamline the
    evaluation process by providing a structured environment for testing different
    aspects of RAG systems. They are flexible and can be adapted to different datasets
    and metrics. We will discuss the popular evaluation frameworks in section 5.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Benchmarks* are standardized datasets and their evaluation metrics used to
    measure the performance of RAG systems. Benchmarks provide a common ground for
    comparing different RAG approaches. They ensure consistency across the evaluations
    by considering a fixed set of tasks and their evaluation criteria. For example,
    HotpotQA focuses on multi-hop reasoning and retrieval capabilities using metrics
    such as Exact Match and F1 scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks are used to establish a baseline for performance and identify strengths/weaknesses
    in specific tasks or domains. We will discuss a few benchmarks and their characteristics
    in section 5.4
  prefs: []
  type: TYPE_NORMAL
- en: Developers can use frameworks to integrate evaluation in their development process
    and use benchmarks to compare their development with established standards. The
    frameworks and benchmarks both calculate *metrics* that focus on retrieval and
    the RAG quality scores. We will begin our discussion about the metrics in the
    next section before moving on to the popular benchmarks and frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Metrics quantify the assessment of the RAG system performance. We will classify
    the evaluation metrics into two broad groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval metrics that are commonly used in information retrieval tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG-specific metrics that have evolved as RAG has found more application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is noteworthy that there are natural-language-generation-specific metrics
    such as BLEU, ROUGE, and METEOR that focus on fluency and measure relevance and
    semantic similarity. They play an important role in analyzing and benchmarking
    the performance of LLMs. This book discusses metrics specific to retrieval and
    RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Retrieval metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The retrieval component of RAG can be evaluated independently to determine how
    well the retrievers are satisfying the user query. The primary retrieval evaluation
    metrics include accuracy, precision, recall, F1-score, mean reciprocal rank (MRR),
    mean average precision (MAP), and normalized discounted cumulative gain (nDCG).
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Accuracy is typically defined as the proportion of correct predictions (both
    true positives and true negatives) among the total number of cases examined. In
    the context of information retrieval, it could be interpreted as
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-0x.png)
  prefs: []
  type: TYPE_NORMAL
- en: Although accuracy is a simple, intuitive metric, it is not the primary metric
    for retrieval. In a large knowledge base, a majority of documents are usually
    irrelevant to any given query, which can lead to misleadingly high accuracy scores.
    It does not consider the ranking of the retrieved results.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Precision focuses on the quality of the retrieved results. It measures the proportion
    of retrieved documents relevant to the user query. It answers the question, “Of
    all the documents that were retrieved, how many were relevant?”
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-1x.png)
  prefs: []
  type: TYPE_NORMAL
- en: A higher precision means that the retriever is performing well and retrieving
    mostly relevant documents.
  prefs: []
  type: TYPE_NORMAL
- en: Precision@k
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Precision@k is a variation of precision that measures the proportion of relevant
    documents among the top ‘k’ retrieved results. It is particularly important because
    it focuses on the top results rather than all the retrieved documents. For RAG,
    it is important because only the top results are most likely to be used for augmentation.
    For example, if you restrict your RAG system to use only the top five retrieved
    documents for context augmentation, Precision@5 will be the metric to calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-2x.png)
  prefs: []
  type: TYPE_NORMAL
- en: where ‘k’ is a chosen cut-off point. A precision@5 of .8 means that out of the
    top five retrieved documents, four were relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Precision@k is also useful to compare systems when the total number of results
    retrieved may be different in different systems. However, the limitation is that
    the choice of ‘k’ can be arbitrary, and this metric doesn’t look beyond the chosen
    ‘k’.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recall focuses on the coverage that the retriever provides. It measures the
    proportion of the relevant documents retrieved from all the relevant documents
    in the corpus. It answers the question, “Of all the relevant documents, how many
    were retrieved?”
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-3x.png)
  prefs: []
  type: TYPE_NORMAL
- en: Note that, unlike precision, calculation of recall requires prior knowledge
    of the total number of relevant documents. This requirement can become challenging
    in large-scale systems, which have many documents in the knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Like precision, recall also doesn’t consider the ranking of the retrieved documents.
    It can also be misleading as retrieving all documents in the knowledge base will
    result in a perfect recall value. Figure 5.1 visualizes various precision and
    recall scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F01_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1  Precision and recall
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: F1-score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'F1-score is the harmonic mean of precision and recall. It provides a single
    metric that balances both the quality and coverage of the retriever:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-4x.png)
  prefs: []
  type: TYPE_NORMAL
- en: The equation is such that the F1-score penalizes either variable having a low
    score; a high F1 score is only possible when both recall and precision values
    are high. This means that the score cannot be positively skewed by a single variable.
    Figure 5.2 illustrates how the F1-score balances precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a graph'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F02_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2  F1-score balances precision and recall. A medium value of both precision
    and recall gets a higher F1-score than if one value is very high and the other
    is very low.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: F1-score provides a single, balanced measure that can be used to easily compare
    different systems. However, it does not take ranking into account and gives equal
    weight to precision and recall, which might not always be ideal.
  prefs: []
  type: TYPE_NORMAL
- en: Mean reciprocal rank
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Mean reciprocal rank, or MRR, is particularly useful in evaluating the rank
    of the relevant document. It measures the reciprocal of the ranks of the first
    relevant document in the list of results. MRR is calculated over a set of queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-5x.png)
  prefs: []
  type: TYPE_NORMAL
- en: where N is the total number of queries, and ranki  is the rank of the first
    relevant document of the i-th query.
  prefs: []
  type: TYPE_NORMAL
- en: MRR is particularly useful when you’re interested in how quickly the system
    can find a relevant document and consider the ranking of the results. However,
    since it doesn’t look at anything beyond the first relevant result, it may not
    be useful when multiple relevant results are important. Figure 5.3 shows how the
    mean reciprocal rank is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '![A paper with a number of results'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F03_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3  MRR considers the ranking but doesn’t consider all the documents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Mean average precision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Mean average precision, or MAP, is a metric that combines precision and recall
    at different cut-off levels of ‘k’, that is, the cut-off number for the top results.
    It calculates a measure called average precision and then averages it across all
    queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-6x.png)
  prefs: []
  type: TYPE_NORMAL
- en: where Ri is the number of relevant documents for query i, Precision@k is the
    precision at cut-off ‘k’, and rel@k is a binary flag indicating the relevance
    of the document at rank k.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean average precision is the mean of the average precision over all the N
    queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-7x.png)
  prefs: []
  type: TYPE_NORMAL
- en: MAP provides a single measure of quality across recall levels. It is quite suitable
    when result ranking is important but complex to calculate. Let’s look at an example
    MAP calculation in figure 5.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F04_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4  MAP considers all the retrieved documents and gives a higher score
    for better ranking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Normalized discounted cumulative gain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Normalized discounted cumulative gain (nDCG) evaluates the ranking quality
    by considering the position of relevant documents in the result list and assigning
    higher scores to relevant documents appearing earlier. It is particularly effective
    for scenarios where documents have varying degrees of relevance. To calculate
    discounted cumulative gain (DCG), each document in the retrieved list is assigned
    a relevance score, rel, and a discount factor reduces the weight of documents
    as their rank position increases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-8x.png)
  prefs: []
  type: TYPE_NORMAL
- en: where reli is the graded relevance of the document at position I, and IDCG is
    the ideal DCG, which is the DCG for perfect ranking.
  prefs: []
  type: TYPE_NORMAL
- en: 'nDCG is calculated as the ratio between actual DCG and the IDCG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-9x.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 shows an example of nDCG calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '![A paper with numbers and letters'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F05_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5  nDCG addresses degrees of relevance in documents and penalizes incorrect
    ranking.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: nDCG is a complex metric to calculate. It requires documents to have a relevance
    score, which may lead to subjectivity, and the choice of the discount factor affects
    the values significantly, but it accounts for varying degrees of relevance in
    documents and gives more weight to higher-ranked items.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval systems are not just used in RAG but also in a variety of other application
    areas such as web and enterprise search engines, e-commerce product search and
    personalized recommendations, social media ad retrieval, archival systems, databases,
    virtual assistants, and more. The retrieval metrics help in assessing and improving
    the performance to effectively meet user needs. Table 5.1 summarizes different
    retrieval metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 Retrieval metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Metric | What it measures | Strengths | Use cases | Considerations |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | Overall correctness of retrieval | Simple to understand; includes
    true negatives | General performance in balanced datasets | Can be misleading
    in imbalanced datasets; doesn’t consider ranking |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | Quality of retrieved results | Easy to understand and calculate
    | General retrieval quality assessment | Doesn’t consider ranking or completeness
    of retrieval |'
  prefs: []
  type: TYPE_TB
- en: '| Precision@k | Quality of top k retrieved results | Focuses on most relevant
    results for RAG | When only top k results are used for augmentation | Choose k
    based on your RAG system’s usage |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | Coverage of relevant documents | Measures completeness of retrieval
    | Assessing if important information is missed | Requires knowing all relevant
    documents in the corpus |'
  prefs: []
  type: TYPE_TB
- en: '| F1-score | Balance between precision and recall | Single metric combining
    quality and coverage | Overall retrieval performance | May obscure tradeoffs between
    precision and recall |'
  prefs: []
  type: TYPE_TB
- en: '| Mean reciprocal rank (MRR) | How quickly a relevant document is found | Emphasizes
    finding at least one relevant result quickly | When finding one good result is
    sufficient | Less useful when multiple relevant results are needed |'
  prefs: []
  type: TYPE_TB
- en: '| Mean average precision (MAP) | Precision at different recall levels | Considers
    both precision and ranking | Comprehensive evaluation of ranked retrieval results
    | More complex to calculate and interpret |'
  prefs: []
  type: TYPE_TB
- en: '| Normalized discounted cumulative gain (nDCG) | Ranking quality with graded
    relevance | Accounts for varying degrees of relevance and ranking | When documents
    have different levels of relevance | Requires relevance scoring for documents
    |'
  prefs: []
  type: TYPE_TB
- en: Not all retrieval metrics are popular for evaluation. Often, the more complex
    metrics are overlooked for the sake of explainability. The usage of these metrics
    depends on the stage of improvement in the evolution of system performance you
    are in. For example, to start with, you may just be trying to improve precision,
    while at an evolved stage, you may be looking for better ranking.
  prefs: []
  type: TYPE_NORMAL
- en: While these metrics focus on retrieval in general, some metrics have been created
    specifically for RAG applications. These metrics focus on the three quality scores
    discussed in section 5.1\.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 RAG-specific metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The three quality scores used to evaluate RAG applications are context relevance,
    answer relevance, and answer faithfulness. These scores specifically answer the
    following three questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the information retrieval relevant to the user query?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the generated answer rooted in the retrieved information?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the generated answer relevant to the user query?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now take a look at each of these scores.
  prefs: []
  type: TYPE_NORMAL
- en: Context relevance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Context relevance evaluates how well the retrieved documents relate to the original
    query. The key aspects are topical alignment, information usefulness, and redundancy.
    There are human evaluation methods, as well as semantic similarity measures to
    calculate context relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 'One such measure is employed by the Retrieval-Augmented Generation Assessment
    (RAGAs) framework (further discussed in section 5.3). The retrieved context should
    contain information only relevant to the query or the prompt. For context relevance,
    a metric S is estimated, where S is the number of sentences in the retrieved context
    relevant for responding to the query or the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-10x.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 is an illustrative example of high and low context relevance.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a test'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F06_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6  Context relevance evaluates the degree to which the retrieved information
    is relevant
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: to the query.
  prefs: []
  type: TYPE_NORMAL
- en: The number of relevant sentences is also sometimes customized to the sum of
    similarity scores of each of the sentences with the query. Context relevance ensures
    that the generation component has access to appropriate information.
  prefs: []
  type: TYPE_NORMAL
- en: Answer faithfulness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Answer faithfulness is the measure of the extent to which the response is factually
    grounded in the retrieved context. Faithfulness ensures that the facts in the
    response do not contradict the context and can be traced back to the source. It
    also ensures that the LLM is not hallucinating. In the RAGAs framework, faithfulness
    first identifies the number of claims made in the response and calculates the
    proportion of those claims present in the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-11x.png)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example in figure 5.7
  prefs: []
  type: TYPE_NORMAL
- en: '![A white paper with text and numbers'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F07_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7  Answer faithfulness evaluates the closeness of the generated response
    to the retrieved context.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Faithfulness is not a complete measure of factual accuracy but only evaluates
    the groundedness to the context. An inverse metric for faithfulness is also the
    *hallucination rate***,** which can calculate the proportion of generated claims
    in the response that are not present in the retrieved context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another related metric to faithfulness is *coverage*. Coverage measures the
    number of relevant claims in the context and calculates the proportion of relevant
    claims present in the generated response. It measures how much of the relevant
    information from the retrieved passages is included in the generated answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-12x.png)
  prefs: []
  type: TYPE_NORMAL
- en: Answer relevance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like context relevance measures the relevance of the retrieved context to the
    query, answer relevance is the measure of the extent to which the response is
    relevant to the query. This metric focuses on key aspects such as the system’s
    ability to comprehend the query, the response being pertinent to the query, and
    the completeness of the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'In RAGAs, for this metric, a response is generated for the initial query or
    prompt. To compute the score, the LLM is then prompted to generate questions for
    the generated response several times. The mean cosine similarity between these
    questions and the original one is then calculated. The concept is that if the
    answer addresses the initial question correctly, the LLM should generate questions
    from it that match the original question:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-13x.png)
  prefs: []
  type: TYPE_NORMAL
- en: where N is the number of queries generated by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that answer relevance is not a measure of truthfulness but only of relevance.
    The response may or may not be factually accurate, but it may be relevant. Figure
    5.8 is an illustration of the answer relevance calculation. Can you find the reason
    why the relevance is not very high? (Hint: The answer may have some irrelevant
    facts.) Answer relevance ensures that the RAG system provides useful and appropriate
    responses, enhancing user satisfaction and the system’s practical utility.'
  prefs: []
  type: TYPE_NORMAL
- en: Tradeoffs and other considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These three metrics and their derivatives form the core of RAG quality evaluation.
    Furthermore, these metrics are interconnected and sometimes involve tradeoffs.
    High context relevance usually leads to better faithfulness, as the system has
    access to more pertinent information. However, high faithfulness doesn’t always
    guarantee high answer relevance. A system might faithfully reproduce information
    from the retrieved passages but fail to directly address the query. Optimizing
    for answer relevance without considering faithfulness might lead to responses
    that seem appropriate but contain hallucinated or incorrect information.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed quite a few metrics in this section. Effective interpretation
    of these metrics is crucial for performance improvement. As creators of RAG systems,
    you should use these metrics to compare with similar systems. You can also look
    at consistent trends to identify the strengths and weaknesses of your system.
    A low-precision high-recall system may indicate that your system is retrieving
    a lot of documents, and you may need to make your retriever more selective. A
    low-precision low-recall system points out fundamental problems with retrieval,
    and you may need to reassess the indexing pipeline itself. The same problem may
    be indicated by a low MAP or a low context-relevance score. Similarly, a low MRR
    or a low nDCG value may indicate a problem with the ranking algorithm of the retriever.
    To address low-answer faithfulness or low-answer relevance, you may need to improve
    your prompts or fine-tune the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a diagram'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F08_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8  Answer relevance is calculated as the mean of cosine similarity
    between the original and synthetic questions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There may also exist some tradeoffs that you will need to balance. Improving
    precision often reduces recall and vice-versa. Highly relevant but brief contexts
    may lead to incomplete answers, and high answer faithfulness may sometimes come
    at the cost of answer relevance.
  prefs: []
  type: TYPE_NORMAL
- en: The relative importance of each metric will depend on your use case and user
    requirements. You may need to include other metrics specific to your downstream
    use case, such as summarization to measure conciseness, and chatbots to emphasize
    conversation coherence.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can code these metrics from scratch and integrate them in the development
    and deployment process of their RAG system. However, you’ll find evaluation frameworks
    that are readily available quite handy. We discuss three popular frameworks in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluations and ground truth data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most of the metrics we discussed talk about a concept of relevant documents.
    For example, precision is calculated as the number of relevant documents retrieved,
    divided by the total number of retrieved documents. The question that arises is,
    how does one establish that a document is relevant?
  prefs: []
  type: TYPE_NORMAL
- en: The simple answer is a human evaluation approach. A subject matter expert looks
    at the documents and determines the relevance. Human evaluation brings in subjectivity,
    and therefore, human evaluations are done by a panel of experts rather than an
    individual. But human evaluations are restrictive from a scale and a cost perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Any data that can reliably establish relevance becomes extremely useful consequently.
    Ground truth is information known to be real or true. In RAG, and the generative
    AI domain in general, ground truth is a prepared set of prompt–context–response
    or question–context–response examples, akin to labeled data in supervised machine
    learning parlance. Ground truth data created for your knowledge base can be used
    for the evaluation of your RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: How does one go about creating the ground truth data? It can be viewed as a
    one-time exercise where a group of experts creates this data. However, generating
    hundreds of QCA (question–context–answer) samples from documents manually can
    be a time-consuming and labor-intensive task. Additionally, if the knowledge base
    is dynamic, the ground truth data will also need updates. Questions created by
    humans may face challenges in achieving the necessary level of complexity for
    a comprehensive evaluation, potentially affecting the overall quality of the assessment.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can be used to address these challenges. Synthetic data generation uses
    LLMs to generate diverse questions and answers from the documents in the knowledge
    base. LLMs can be prompted to create questions such as simple questions, multi-context
    questions, conditional questions, reasoning questions, and similar using the documents
    from the knowledge base as context.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Frameworks provide a structured approach to RAG evaluations. They can be used
    to automate the evaluation process. Some go beyond and assist in the synthetic
    ground truth data generation. While new evaluation frameworks continue to be introduced,
    there are two popular ones that we discuss here:'
  prefs: []
  type: TYPE_NORMAL
- en: RAGAs (Retrieval-Augmented Generation Assessment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARES (Automated RAG Evaluation System)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.3.1 RAGAs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation Assessment, or RAGAs, is a framework developed
    by Exploding Gradients that assesses the retrieval and generation components of
    RAG systems without relying on extensive human annotations. RAGAs
  prefs: []
  type: TYPE_NORMAL
- en: Synthetically generate a test dataset that can be used to evaluate a RAG pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use metrics to measure the performance of the pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the quality of the application in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will continue with our example of the Wikipedia page of the 2023 Cricket
    World Cup, but we first create a synthetic test dataset using RAGAs and then use
    the RAGAs metrics to evaluate the performance of the RAG pipeline we created in
    chapters 3 and 4.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic test dataset generation (ground truths)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Section 5.2 pointed out that ground truths data is necessary to calculate evaluation
    metrics for assessing the quality of RAG pipelines. While this data can be manually
    curated, RAGAs provides the functionality of generating this dataset from the
    documents in the knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: RAGAs does this using an LLM. It analyses the documents in the knowledge base
    and uses an LLM to generate seed questions from chunks in the knowledge base.
    These questions are based on the document chunks from the knowledge base. These
    chunks act as the context for the questions. Another LLM is used to generate the
    answer to these questions. This is how it generates a question–context–answer
    data based on the documents in the knowledge base. RAGAs also has an evolver module
    that creates more difficult questions (e.g., multi-context, reasoning, and conditional)
    for a more comprehensive evaluation. Figure 5.9 illustrates the process of synthetic
    data generation using RAGAs.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of questions and arrows'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F09_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9  Synthetic ground truths data generation using RAGAs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To evaluate our RAG pipeline, let’s recreate the documents from the Wikipedia
    page like we did in chapter 3\. Note that we will have to install the packages
    used in the previous chapters to continue with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `html_data_transformed` contains the necessary document format of the Wikipedia
    page. We will use RAGAs library to generate the dataset from these documents.
    For that, we will first need to install the RAGAs library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `testset` that we created contains 20 questions based on our document, along
    with the chunk of the document that the question was based on, and the ground
    truth answer. A screenshot of the dataset is shown in figure 5.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a table'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F10_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10  Synthetic test data generated using RAGAs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will use this dataset to evaluate our RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Recreating the RAG pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the created test dataset, we use the `question` and the `ground_truth`
    information. We pass the questions to our RAG pipeline and generate answers. We
    compare these answers with the `ground_truth` to calculate the evaluation metrics.
    First, we recreate our RAG pipeline. Again, it is important to note that we will
    have to install the packages we used in the previous chapters to continue with
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can try this pipeline to generate answers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the RAG pipeline function, we can evaluate this pipeline using
    the questions that have been synthetically generated.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first generate answers to the questions in the synthetic test data using
    our RAG pipeline. We then compare the answers to the ground truth answers. We
    first generate the answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For RAGAs, the evaluation set needs to be in the `Dataset` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the complete evaluation dataset, we can invoke the metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can also check the official documentation of RAGAs for more information
    ([https://docs.ragas.io/en/stable/](https://docs.ragas.io/en/stable/)). RAGAs
    calculates a bunch of metrics that are useful for assessing the quality of the
    RAG pipeline. RAGAs uses an LLM to do this, somewhat subjective, task. For example,
    to calculate faithfulness for a given question–context–answer record, RAGAs first
    breaks down the answer into simple statements. Then, for each statement, it asks
    the LLM whether the statement can be inferred from the context. The LLM provides
    a 0 or 1 response along with a reason. This process is repeated a couple of times.
    Finally, faithfulness is calculated as the proportion of statements judged by
    the LLM as faithful (i.e., 1). Several other metrics are calculated using this
    LLM-based approach. This approach, where an LLM is used in evaluating a task,
    is also popularly called *LLM as a judge* approach. An important point to note
    here is that the accuracy of this evaluation is also dependent on the quality
    of the LLM being used as the judge.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Automated RAG evaluation system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Automated RAG evaluation system, or ARES, is a framework developed by researchers
    at Stanford University and Databricks. Like RAGAs, ARES uses an LLM as a judge
    approach for evaluations. Both request a language model to classify answer relevance,
    context relevance, and faithfulness for a given query. However, there are some
    differences:'
  prefs: []
  type: TYPE_NORMAL
- en: RAGAs relies on heuristically written prompts sent to the LLM for evaluation.
    ARES, in contrast, trains a classifier using a language model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAGAs aggregates the responses from the LLM to arrive at a score. ARES provides
    confidence intervals for the scores using a framework called Prediction-Powered
    Inference (PPI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAGAs generates a simple synthetic question–context–answer dataset for evaluation
    from the documents. ARES generate synthetic datasets comprising both positive
    and negative examples of query–passage–answer triples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ARES requires more data than RAGAs. To use ARES, you need the following three
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In-domain passage se**t*—This is a collection of passages relevant to the
    specific domain being evaluated. The passages should be suitable for generating
    queries and answers. In our case, it will be the documents that we created from
    the Wikipedia article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human preference validation se**t*—A minimum of approximately 150 annotated
    data points is required. This set is used to validate the preferences of human
    annotators regarding the relevance of the generated query-passage–answer triples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Few-shot example**s*—At least five examples of in-domain queries and answers
    are needed. These examples help prompt the LLMs during the synthetic data generation
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for a human-preference validation set and fine-tuning of language models
    for classification makes applying ARES more complex. The application of ARES is
    out of the scope of this book. However, ARES is a robust framework. It provides
    a detailed analysis of system performance with statistical confidence intervals,
    making it suitable for in-depth RAG system evaluations. RAGAs promises a faster
    evaluation cycle without reliance on human annotations. More details on the ARES
    application can be found in the official GitHub repository ([https://github.com/stanford-futuredata/ARES](https://github.com/stanford-futuredata/ARES)).
  prefs: []
  type: TYPE_NORMAL
- en: While RAGAs and ARES have gained popularity, there are other frameworks, such
    as TruLens, DeepEval, and RAGChecker, that have also gotten acceptance amongst
    RAG developers.
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks provide a standardized method of automating the evaluation of your
    RAG pipelines. Your choice of the evaluation framework should depend on your use
    case requirements. For quick and easy evaluations that are widely understood,
    RAGAs may be your choice. For robustness across diverse domains and question types,
    ARES might suit better. Most of the proprietary service providers (vector DBs,
    LLMs, etc.) have their evaluation features you may use. You can also develop your
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we look at benchmarks. Benchmarks are used to compare competing RAG systems
    with one another.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmarks provide a standard point of reference to evaluate the quality and
    performance of a system. RAG benchmarks are a set of standardized tasks, and a
    dataset used to compare the efficiency of different RAG systems in retrieving
    relevant information and generating accurate responses. There has been a surge
    in creating benchmarks since 2023, when RAG started gaining popularity, but there
    have been benchmarks on question-answering tasks that were introduced before that.
    Benchmarks such as Stanford Question Answering Dataset (SQuAD), WikiQA, Natural
    Question (NQ), and HotpotQA are open domain question-answering datasets that primarily
    evaluate the retriever component using metrics such as Exact Match (EM) and F1-score.
    BEIR or benchmarking information retrieval is a comprehensive, heterogeneous benchmark
    based on 9 IR tasks and 19 question–answer datasets. This section discusses three
    of the popular RAG-specific benchmarks and their evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 RGB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Retrieval-augmented generation benchmark (RGB) was introduced in a December
    2023 paper ([https://arxiv.org/pdf/2309.01431](https://arxiv.org/pdf/2309.01431)).
    It comprises 600 base questions and 400 additional questions, evenly split between
    English and Chinese. The corpus was constructed using a multistep process that
    involved collecting recent news articles, generating questions and answers using
    ChatGPT, retrieving relevant web pages through Google’s API, and selecting the
    most pertinent text chunks using a dense retrieval model. It is a benchmark that
    focuses on four key abilities of a RAG system: noise robustness, negative rejection,
    information integration, and counterfactual robustness, as illustrated in figure
    5.11.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RGB focuses on the following metrics for evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accurac**y*—Used for noise robustness and information integration. It is based
    on the exact matching of the generated text with the correct answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rejection rat**e*—Used for negative rejection. It is measured by exact matching
    of the model’s output with a specific rejection phrase. The rejection rate is
    also'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A group of text boxes'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F11_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.11  Four abilities required of RAG systems. Source: Benchmarking Large
    Language Models in Retrieval-Augmented Generation by Chen et al., [https://arxiv.org/pdf/2309.0143](https://arxiv.org/pdf/2309.0143).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: evaluated using ChatGPT to determine whether the responses contain rejection
    information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Error detection rat**e*—Used for counterfactual robustness. It is measured
    by exact matching of the model’s output with a specific error-detection phrase
    and is also evaluated using ChatGPT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Error correction rat**e*—Used for counterfactual robustness. It measures whether
    the model can provide the correct answer after identifying errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the GitHub repository to implement RGB ([https://github.com/chen700564/RGB](https://github.com/chen700564/RGB)).
  prefs: []
  type: TYPE_NORMAL
- en: Multi-hop RAG
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Curated by researchers at HKUST, multi-hop RAG contains 2556 queries, with
    evidence for each query distributed across two to four documents. The queries
    also involve document metadata, reflecting complex scenarios commonly found in
    real-world RAG applications. It contains four types of queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Inferenc**e*—Synthesizing information across multiple sources (e.g., Which
    report discusses the supply chain risk of Apple—the 2019 annual report or the
    2020 annual report?)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Compariso**n*—Comparing facts from different sources (e.g., Did Netflix or
    Google report higher revenue for the year 2023?)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tempora**l*—Analyzing the temporal ordering of events (e.g., e.g. Did Apple
    introduce the AirTag tracking device before or after the launch of the 5th generation
    iPad Pro?)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nul**l*—Queries not answerable from the knowledge base'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full implementation code is available at [https://github.com/yixuantt/MultiHop-RAG](https://github.com/yixuantt/MultiHop-RAG).
  prefs: []
  type: TYPE_NORMAL
- en: CRAG
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Comprehensive RAG benchmark (CRAG), curated by Meta and HKUST, is a factual
    question-answering benchmark of 4,409 question–answer pairs and mock APIs to simulate
    web and knowledge graph (KG) search. It contains eight types (simple, conditions,
    comparison questions, aggregation questions, multi-hop questions, set queries,
    post-processing-heavy questions, and false-premise questions, as illustrated in
    figure 5.12) of queries across five domains (finance, sports, music, movie, and
    open domain).
  prefs: []
  type: TYPE_NORMAL
- en: 'For each question in the evaluation set, CRAG labels the answer with one of
    four classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Perfec**t*—The response correctly answers the user’s question and contains
    no hallucinated content (scored as +1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Acceptabl**e*—The response provides a useful answer to the user’s question
    but may contain minor errors that do not harm the usefulness of the answer (scored
    as +0.5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Missin**g*—The response is “I don’t know”, “I’m sorry I can’t find ...”, a
    system error such as an empty response, or a request from the system to clarify
    the original question (scored as 0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Incorrec**t*—The response provides wrong or irrelevant information to answer
    the user’s question (scored as −1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A table of questions with text'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F12_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12  Eight question types in CRAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For automatic evaluation, CRAG classifies an answer as perfect if it exactly
    matches the ground truth. If not, then it asks an LLM to do the classification.
    It uses two LLM evaluators. You can read more about CRAG at [https://arxiv.org/pdf/2406.04744](https://arxiv.org/pdf/2406.04744).
  prefs: []
  type: TYPE_NORMAL
- en: Other noteworthy benchmark datasets are MedRAG ([https://github.com/Teddy-XiongGZ/MedRAG](https://github.com/Teddy-XiongGZ/MedRAG)),
    which focuses on Medical Information, CRUD-RAG ([https://arxiv.org/pdf/2401.17043](https://arxiv.org/pdf/2401.17043)),
    which focuses on the Chinese language, and FeB4RAG ([https://arxiv.org/abs/2402.11891](https://arxiv.org/abs/2402.11891)),
    which focuses on federated search. If you’re developing an LLM application that
    has accurate and contextual generation as its core proposition, you’ll be able
    to communicate the quality of your application by showing how it performs on different
    benchmarks. Table 5.2 compares the different benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.2 RAG benchmarks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Benchmark | Dataset | Task | Metrics | Applicability |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SQuAD | Stanford Question Answering Dataset | Open domain QA | Exact match
    (EM), F1-score | General QA tasks, model evaluation on comprehension accuracy
    |'
  prefs: []
  type: TYPE_TB
- en: '| Natural questions | Real Google search queries | Open domain QA | F1-score
    | Real-world QA, information retrieval from large corpora |'
  prefs: []
  type: TYPE_TB
- en: '| HotpotQA | Wikipedia-based QA | Multi-hop QA | EM, F1-score | QA involving
    multiple documents, complex reasoning tasks |'
  prefs: []
  type: TYPE_TB
- en: '| BEIR | Multiple datasets | Information retrieval | nDCG@10 | Comprehensive
    IR model evaluation across multiple domains |'
  prefs: []
  type: TYPE_TB
- en: '| RGB | News articles, ChatGPT-generated QA | Robust QA | Accuracy, rejection
    rate, error detection rate, error correction rate | Robustness and reliability
    of RAG systems |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-hop RAG | HKUST-curated queries | Complex QA | Various | RAG applications
    requiring multi-source synthesis |'
  prefs: []
  type: TYPE_TB
- en: '| CRAG | Multiple sources (finance, sports, music, etc.) | Factual QA | Four-class
    evaluation (perfect, acceptable, missing, and incorrect) | Evaluating factual
    QA with diverse question types |'
  prefs: []
  type: TYPE_TB
- en: We have looked frameworks that help in automating the calculation of evaluation
    metrics and benchmarks that enable comparisons across different implementations
    and approaches. Frameworks will assist you in improving the performance of your
    system, and benchmarks will facilitate comparing it with other systems available
    in the market.
  prefs: []
  type: TYPE_NORMAL
- en: However, as with any evolving field, there are some limitations and challenges
    to consider. The next section examines these limitations and discusses best practices
    that have emerged to address them, ensuring a more holistic and nuanced approach
    to RAG evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Limitations and best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There has been a lot of progress made in the frameworks and benchmarks used
    for RAG evaluation. The complexity in evaluation arises due to the interplay between
    the retrieval and generation components. In practice, there’s a significant reliance
    on human judgements, which are subjective and difficult to scale. What follows
    are a few common challenges and some guidelines to navigate them.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of standardized metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There’s no consensus on what the best metrics are to evaluate RAG systems. Precision,
    recall, and F1-score are commonly measured for retrieval but do not fully capture
    the nuances of generative response. Similarly, commonly used generation metrics
    such as BLEU, ROUGE, and similar do not fully capture the context awareness required
    for RAG. Using RAG-specific metrics such as answer relevance, context relevance,
    and faithfulness for evaluation brings in the necessary nuances required for RAG
    evaluation. However, even for these metrics, there’s no standard way of calculation
    and each framework brings in its methodology.
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Compare the results on RAG specific metrics from different
    frameworks. Sometimes, it may be warranted to change the calculation method with
    respect to the use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Overreliance on LLM as a judge
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The evaluation of RAG-specific metrics (in RAGAs, ARES, etc.) relies on using
    an LLM as a judge. An LLM is prompted or fine-tuned to classify a response as
    relevant or not. This adds to the complexity of the LLMs’ ability to do this task.
    It may be possible that the LLM may not be very accurate in judging for your specific
    documents and knowledge bases. Another problem that arises is that of self-reference.
    It is possible that if the judge LLM is the same as the generation LLM in your
    system, you will get a more favorable evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Sample a few results from the judge LLM and evaluate whether
    the results are in line with commonly understood business practice. To avoid the
    self-reference problem, make sure to use a judge LLM different from the generation
    LLM. It may also help if you use multiple judge LLMs and aggregate their results.'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of use case subjectivity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most frameworks have a generalized approach to evaluation. They may not capture
    the subjective nature of the task relevant to your use case (content generation
    versus chatbot versus question-answering, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Focus on use-case-specific metrics to assess quality, coherence,
    usefulness, and similar. Incorporate human judgements in your workflow with techniques
    such as user feedback, crowd-sourcing, or expert ratings.'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks are static
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most benchmarks are static and do not account for the evolving nature of information.
    RAG systems need to adapt to real-time information changes, which are not currently
    tested effectively. There is a lack of evaluation for how well RAG models learn
    and adapt to new data over time. Most benchmarks are domain-agnostic, which may
    not reflect the performance of RAG systems in your specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Use a benchmark that is tailored to your domain. The static
    nature of benchmarks is limiting. Do not overly rely on benchmarks, and augment
    the use of benchmarks with regularly updating data.'
  prefs: []
  type: TYPE_NORMAL
- en: Scalability and cost
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluating large-scale RAG systems is more complex than evaluating basic RAG
    pipelines. It requires significant computational resources. Benchmarks and frameworks
    also generally do not account for metrics such as latency and efficiency, which
    are critical for real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Employ careful sampling of test cases for evaluation. Incorporate
    workflows to measure latency and efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these, you should also carefully consider the aspects of bias and
    toxicity, focusing on information integration and negative rejection, which the
    frameworks do not evaluate well. It is also important to keep an eye on how these
    evaluation frameworks and benchmarks evolve.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we comprehensively examined the evaluation metrics, frameworks,
    and benchmarks that will help you evaluate your RAG pipelines. We used RAGAs to
    evaluate the pipeline that we have been building.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we have looked at building and evaluating a simple RAG system. This
    also marks the second part 2 of this book. You are now familiar with the creation
    of the RAG knowledge brain using the indexing pipeline, enabling real-time interaction
    using the generation pipeline and evaluating your RAG system using frameworks
    and benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next part, we will move toward discussing the production aspects of RAG
    systems. In chapter 6, we will look at strategies and advanced techniques to improve
    our RAG pipeline, which should also reflect in better evaluation metrics. In chapter
    7, we will look at the LLMOps stack that enables RAG in production.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG evaluation fundamentals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG evaluation assesses how well systems reduce hallucinations and ground responses
    in the provided context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three key quality scores for RAG evaluation are context relevance, answer faithfulness,
    and answer relevance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four critical abilities required of RAG systems include noise robustness, negative
    rejection, information integration, and counterfactual robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional considerations include latency, robustness, bias, and toxicity of
    responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom use-case-specific metrics should be developed to evaluate performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval metrics include precision, recall, F1-score, mean reciprocal rank
    (MRR), mean average precision (MAP), and normalized discounted cumulative gain
    (nDCG).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy, precision, recall, and F1-score do not consider the ranking order
    of the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG-specific metrics focus on context relevance, answer faithfulness, and answer
    relevance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human evaluations and ground truth data play a crucial role in RAG assessment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAGAs is an easy-to-implement framework that can be used for quick evaluation
    of RAG pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARES uses a more complex approach, including classifier training and confidence
    interval calculations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Benchmarks provide standardized datasets and metrics for comparing different
    RAG implementations on specific tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular benchmarks such as SQuAD, natural questions, HotpotQA, and BEIR focus
    on retrieval quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recent benchmarks such as RGB, multi-hop RAG, and CRAG are more holistic from
    a RAG perspective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarks focus on different aspects of RAG performance, such as multi-hop
    reasoning or specific domains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations and best practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Challenges in RAG evaluation include lack of standardized metrics, overreliance
    on LLMs as judges, and static nature of benchmarks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices include using multiple frameworks, incorporating use-case-specific
    metrics, and regularly updating evaluation data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing automated metrics with human judgment and considering use-case-specific
    requirements is crucial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The field of RAG evaluation is evolving, with new frameworks and benchmarks
    constantly emerging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers should stay informed about new developments and adapt their evaluation
    strategies accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
