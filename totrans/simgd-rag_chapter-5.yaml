- en: '5 RAG evaluation: Accuracy, relevance, and faithfulness'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 RAG评估：准确性、相关性和忠实度。
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The need and requirements for evaluating RAG pipelines
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估RAG管道的需求和需求。
- en: Metrics, frameworks, and benchmarks for RAG evaluation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估的指标、框架和基准。
- en: Current limitations and future course of RAG evaluation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估的当前局限性和未来发展方向。
- en: Chapters 3 and 4 discussed the development of retrieval-augmented generation
    (RAG) systems using the indexing and generation pipelines. RAG promises to reduce
    hallucinations and ground the large language model (LLM) responses in the provided
    context, which is done by creating a non-parametric memory or knowledge base for
    the system and then retrieving information from it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章和第4章讨论了使用索引和生成管道开发检索增强生成（RAG）系统的进展。RAG承诺减少幻觉，并将大型语言模型（LLM）的响应基于提供的环境，这是通过为系统创建一个非参数化记忆或知识库，然后从中检索信息来实现的。
- en: This chapter covers the methods used to evaluate how well the RAG system is
    functioning. We need to make sure that the components of the two RAG pipelines
    are performing per the expectations. At a high level, we need to ensure that the
    information being retrieved is relevant to the input query and that the LLM is
    generating responses grounded in the retrieved context. To this end, there have
    been several frameworks developed over time. Here we discuss some popular frameworks
    and the metrics they calculate.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了评估RAG系统功能良好程度的方法。我们需要确保两个RAG管道的组件按预期运行。在较高层次上，我们需要确保检索到的信息与输入查询相关，并且LLM生成的响应基于检索到的环境。为此，随着时间的推移已经开发出几个框架。在这里，我们讨论了一些流行的框架以及它们计算的指标。
- en: There is also a second aspect to evaluation. While the frameworks allow for
    the calculation of metrics, how do you make sure that your RAG pipelines are working
    better than those developed by other developers? The evaluations cannot be done
    in isolation. For this purpose, several benchmarks have been established. These
    benchmarks evaluate the RAG systems on preset data, such as question–answer sets,
    for accurate comparison of different RAG pipelines. These benchmarks help developers
    evaluate the performance of their systems vis-à-vis those developed by other developers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 评估还有第二个方面。虽然框架允许计算指标，但如何确保你的RAG管道比其他开发者开发的管道表现更好？评估不能孤立进行。为此，已经建立了一些基准。这些基准评估了RAG系统在预设数据上的表现，如问答集，以便对不同RAG管道进行准确比较。这些基准帮助开发者评估他们的系统与其他开发者开发的系统相比的性能。
- en: Finally, like RAG techniques, the research on RAG evaluations is still in progress.
    There are still some limitations in the current set of evaluation parameters.
    We discuss these limitations and some ideas on the way forward for RAG evaluations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，像RAG技术一样，RAG评估的研究仍在进行中。当前评估参数集中仍存在一些局限性。我们讨论了这些局限性以及RAG评估未来发展的想法。
- en: By the end of this chapter, you should
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该
- en: Know the fundamentals of RAG evaluations.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解RAG评估的基本原理。
- en: Be aware of the popular frameworks, metrics, and benchmarks for RAG evaluation.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解RAG评估的流行框架、指标和基准。
- en: Understand the limitations and best practices.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解局限性和最佳实践。
- en: Be able to evaluate the RAG pipeline in Python.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够在Python中评估RAG管道。
- en: For RAG to live up to the promise of grounding the LLM responses in data, you
    will need to go beyond the simple implementation of indexing, retrieval, augmentation,
    and generation. We will discuss these advanced strategies in chapter 6\. However,
    to improve something, you need to first measure the performance. RAG evaluations
    help in setting up the baseline of your RAG system performance for you to then
    improve it. First, we look at the fundamental aspects of RAG systems evaluation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让RAG实现将LLM响应基于数据的承诺，你需要超越简单的索引、检索、增强和生成实现。我们将在第6章中讨论这些高级策略。然而，要改进某物，你首先需要衡量其性能。RAG评估有助于为你设置RAG系统性能的基线，然后你可以对其进行改进。首先，我们来看RAG系统评估的基本方面。
- en: 5.1 Key aspects of RAG evaluation
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 RAG评估的关键方面
- en: Building a PoC RAG pipeline is not overtly complex. It is achievable through
    brief training and verification of a limited set of examples. However, to enhance
    its robustness, thorough testing on a dataset that accurately mirrors the production
    use case is imperative. RAG pipelines can suffer from hallucinations of their
    own. This can be because
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个PoC RAG管道并不特别复杂。通过简短的培训和验证一组有限的示例即可实现。然而，为了增强其鲁棒性，在准确反映生产用例的数据集上进行彻底测试是必不可少的。RAG管道可能会出现自己的幻觉。这可能是由于
- en: The retriever fails to retrieve the entire context or retrieves irrelevant context.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索器未能检索到完整上下文，或者检索到了无关的上下文。
- en: Despite being provided the context, the LLM does not consider it.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管提供了上下文，但大型语言模型（LLM）并没有考虑它。
- en: The LLM picks irrelevant information from the context instead of answering the
    query.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM从上下文中选择无关信息而不是回答查询。
- en: 'Retrieval and generation are two processes that need special focus from an
    evaluation perspective. This is because these two steps produce outputs that can
    be evaluated. (While indexing and augmentation will have a bearing on the outputs,
    they do not produce measurable outcomes). Here are several questions we need to
    ask ourselves about these two processes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 检索和生成是两个需要从评估角度特别关注的过程。这是因为这两个步骤会产生可评估的输出。（虽然索引和增强会影响输出，但它们不会产生可测量的结果）。以下是关于这两个过程我们需要问自己的几个问题：
- en: How good is the retrieval of the context from the knowledge base?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从知识库中检索上下文的效果如何？
- en: Is it relevant to the query?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否与查询相关？
- en: How much noise (irrelevant information) is present?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在多少噪声（无关信息）？
- en: How good is the generated response?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的响应有多好？
- en: Is the response grounded in the provided context?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应是否基于提供的上下文？
- en: Is the response relevant to the query?
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应是否与查询相关？
- en: You can ask many more questions such as these to assess the performance of your
    RAG system. Contemporary research has discovered certain scores to assess the
    quality and abilities of a RAG system. The following sections discuss three predominant
    quality scores and four main abilities.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以问更多类似的问题来评估你的RAG系统的性能。当代研究已经发现某些评分来评估RAG系统的质量和能力。以下几节将讨论三个主要的质量评分和四种主要能力。
- en: 5.1.1 Quality scores
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 质量评分
- en: 'There are three quality score dimensions prevalent in the discourse on RAG
    evaluation. They measure the quality of retrieval and generation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG评估的讨论中，有三个质量评分维度很常见。它们衡量检索和生成的质量：
- en: '*Context relevanc**e*—This dimension evaluates how relevant the retrieved information
    or context is to the user query. It calculates metrics such as the precision and
    recall with which context is retrieved from the knowledge base.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文相关性*—这个维度评估检索到的信息或上下文与用户查询的相关性。它计算从知识库中检索上下文的精确度和召回率等指标。'
- en: '*Answer faithfulness (also called groundedness**)*—This dimension evaluates
    whether the answer generated by the system is using the retrieved information.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*答案忠实度（也称为扎根性**）*—这个维度评估系统生成的答案是否使用了检索到的信息。'
- en: '*Answer relevanc**e*—This dimension evaluates how relevant the answer generated
    by the system is to the original user query.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*答案相关性*—这个维度评估系统生成的答案与原始用户查询的相关性。'
- en: We discuss how these scores are calculated in section 5.2
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第5.2节讨论这些评分是如何计算的。
- en: 5.1.2 Required abilities
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 必需的能力
- en: 'The quality scores are important for measuring how well the retrieval and the
    generation components of the RAG system are performing. At an overall level, there
    are certain critical abilities that a RAG system should possess:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 质量评分对于衡量RAG系统的检索和生成组件表现如何非常重要。在整体层面上，RAG系统应该具备某些关键能力：
- en: '*Noise robustnes**s*—It is impractical to assume that the information stored
    in the knowledge base for RAG systems is perfectly curated to answer the questions
    that can be potentially asked. It is very probable that a document is related
    to the user query but does not have any meaningful information to answer it. The
    ability of the RAG system to separate these noisy documents from the relevant
    ones is termed noise robustness.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*噪声鲁棒性*—假设RAG系统存储的知识库中的信息被完美整理以回答可能提出的问题是不切实际的。很可能一个文档与用户查询相关，但没有任何有意义的答案信息。RAG系统从相关文档中分离出这些噪声文档的能力被称为噪声鲁棒性。'
- en: '*Negative rejectio**n*—By nature, LLMs always generate text. There may be no
    information about the user query in the documents in the knowledge base. The ability
    of the RAG system not to give an answer when there is no relevant information
    is called negative rejection.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*负拒绝*—本质上，LLMs总是生成文本。知识库中的文档可能没有关于用户查询的信息。RAG系统在没有相关信息时不给出答案的能力被称为负拒绝。'
- en: '*Information integratio**n*—To obtain a comprehensive answer to a user query,
    it is also very likely the information must be retrieved from multiple documents.
    This ability of the system to assimilate information from multiple documents is
    called information integration.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*信息整合*—为了对用户查询给出全面的答案，很可能需要从多个文档中检索信息。系统从多个文档中吸收信息的能力被称为信息整合。'
- en: '*Counterfactual robustnes**s*—Sometimes the information in the knowledge base
    might itself be inaccurate. A high-quality RAG system should be able to address
    this problem and reject known inaccuracies in the retrieved information. This
    ability is known as counterfactual robustness.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反事实鲁棒性*—有时知识库中的信息本身可能是不准确的。一个高质量的RAG系统应该能够解决这个问题，并拒绝检索到的信息中的已知错误。这种能力被称为反事实鲁棒性。'
- en: Noise robustness is an ability that the retrieval component should possess,
    and other abilities are largely related to the generation component.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声鲁棒性是检索组件应该具备的能力，而其他能力大多与生成组件相关。
- en: Apart from these, *latency* is another often-mentioned capability. Although
    it is a non-functional requirement, it is quite critical in generative AI applications.
    Latency is the delay that happens between the user query and the response. You
    may have observed that LLMs themselves have considerable latency before the final
    response is generated. Add to it the task of retrieval and augmentation, and the
    latency is bound to increase. Therefore, it is important to monitor how much time
    your RAG system takes from user input to response.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，*延迟*是另一个经常提到的能力。尽管它是一个非功能性需求，但在生成AI应用中却非常关键。延迟是用户查询和响应之间的延迟。你可能已经注意到，LLMs在生成最终响应之前有相当大的延迟。再加上检索和增强的任务，延迟肯定会增加。因此，重要的是要监控你的RAG系统从用户输入到响应所需的时间。
- en: Ethical considerations are also at the forefront of generative AI adoption.
    For some RAG applications, it is important to measure the degree of *bias* and
    *toxicity*in the system responses. This is also influenced by the underlying data
    in the knowledge base. While it is not specific to RAG, it is important to evaluate
    the outputs for bias and toxicity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理考量也是生成AI应用的前沿。对于某些RAG应用，测量系统响应中的*偏差*和*毒性*的程度很重要。这也会受到知识库中底层数据的影响。虽然这并不是RAG特有的，但评估输出中的偏差和毒性是很重要的。
- en: Another aspect to check is the *robustness* of the system, that is, its ability
    to handle different types of queries. Some queries may be simple, while others
    may involve complex reasoning. Some queries may require comparing two pieces of
    information, while others may involve complex post-processing, like mathematical
    calculations. We will look at some types of queries when we discuss CRAG, a benchmark,
    in section 5.4.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要检查的方面是系统的*鲁棒性*，即其处理不同类型查询的能力。一些查询可能很简单，而另一些可能涉及复杂的推理。一些查询可能需要比较两块信息，而另一些可能涉及复杂的后处理，如数学计算。当我们讨论第5.4节中的基准CRAG时，我们将探讨一些查询类型。
- en: Finally, it is important to mention that these are scores and abilities that
    approach RAG at the core technique level. RAG, after all, is a means to solving
    the end use case. Therefore, you may have to build a *use case-specific*evaluation
    criteria for your RAG system. For example, a question-answering system may use
    an exact match (EM) or F1 score as a metric, and a summarization service may use
    ROUGE scores. Modern search engines using RAG may look at user interaction metrics,
    accuracy of source attribution, and similar.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重要的是要提到，这些是接近RAG核心技术层面的分数和能力。毕竟，RAG是一种解决最终用例的手段。因此，你可能需要为你的RAG系统构建*特定用例*的评估标准。例如，问答系统可能使用精确匹配（EM）或F1分数作为指标，而摘要服务可能使用ROUGE分数。使用RAG的现代搜索引擎可能会考虑用户交互指标、源归属的准确性以及类似指标。
- en: 'This is the main idea behind evaluating RAG pipelines. The quality scores and
    the abilities that we discussed before need to be measured and benchmarked. There
    are two critical enablers of RAG evaluations: frameworks and benchmarks.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '*Frameworks* are tools designed to facilitate evaluation, offering automation
    of the evaluation process and data generation. They are used to streamline the
    evaluation process by providing a structured environment for testing different
    aspects of RAG systems. They are flexible and can be adapted to different datasets
    and metrics. We will discuss the popular evaluation frameworks in section 5.3.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '*Benchmarks* are standardized datasets and their evaluation metrics used to
    measure the performance of RAG systems. Benchmarks provide a common ground for
    comparing different RAG approaches. They ensure consistency across the evaluations
    by considering a fixed set of tasks and their evaluation criteria. For example,
    HotpotQA focuses on multi-hop reasoning and retrieval capabilities using metrics
    such as Exact Match and F1 scores.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks are used to establish a baseline for performance and identify strengths/weaknesses
    in specific tasks or domains. We will discuss a few benchmarks and their characteristics
    in section 5.4
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Developers can use frameworks to integrate evaluation in their development process
    and use benchmarks to compare their development with established standards. The
    frameworks and benchmarks both calculate *metrics* that focus on retrieval and
    the RAG quality scores. We will begin our discussion about the metrics in the
    next section before moving on to the popular benchmarks and frameworks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Evaluation metrics
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Metrics quantify the assessment of the RAG system performance. We will classify
    the evaluation metrics into two broad groups:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval metrics that are commonly used in information retrieval tasks
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG-specific metrics that have evolved as RAG has found more application
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is noteworthy that there are natural-language-generation-specific metrics
    such as BLEU, ROUGE, and METEOR that focus on fluency and measure relevance and
    semantic similarity. They play an important role in analyzing and benchmarking
    the performance of LLMs. This book discusses metrics specific to retrieval and
    RAG.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Retrieval metrics
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The retrieval component of RAG can be evaluated independently to determine how
    well the retrievers are satisfying the user query. The primary retrieval evaluation
    metrics include accuracy, precision, recall, F1-score, mean reciprocal rank (MRR),
    mean average precision (MAP), and normalized discounted cumulative gain (nDCG).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Accuracy is typically defined as the proportion of correct predictions (both
    true positives and true negatives) among the total number of cases examined. In
    the context of information retrieval, it could be interpreted as
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-0x.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Although accuracy is a simple, intuitive metric, it is not the primary metric
    for retrieval. In a large knowledge base, a majority of documents are usually
    irrelevant to any given query, which can lead to misleadingly high accuracy scores.
    It does not consider the ranking of the retrieved results.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Precision focuses on the quality of the retrieved results. It measures the proportion
    of retrieved documents relevant to the user query. It answers the question, “Of
    all the documents that were retrieved, how many were relevant?”
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-1x.png)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: A higher precision means that the retriever is performing well and retrieving
    mostly relevant documents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Precision@k
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Precision@k is a variation of precision that measures the proportion of relevant
    documents among the top ‘k’ retrieved results. It is particularly important because
    it focuses on the top results rather than all the retrieved documents. For RAG,
    it is important because only the top results are most likely to be used for augmentation.
    For example, if you restrict your RAG system to use only the top five retrieved
    documents for context augmentation, Precision@5 will be the metric to calculate:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-2x.png)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: where ‘k’ is a chosen cut-off point. A precision@5 of .8 means that out of the
    top five retrieved documents, four were relevant.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Precision@k is also useful to compare systems when the total number of results
    retrieved may be different in different systems. However, the limitation is that
    the choice of ‘k’ can be arbitrary, and this metric doesn’t look beyond the chosen
    ‘k’.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recall focuses on the coverage that the retriever provides. It measures the
    proportion of the relevant documents retrieved from all the relevant documents
    in the corpus. It answers the question, “Of all the relevant documents, how many
    were retrieved?”
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-3x.png)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Note that, unlike precision, calculation of recall requires prior knowledge
    of the total number of relevant documents. This requirement can become challenging
    in large-scale systems, which have many documents in the knowledge base.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Like precision, recall also doesn’t consider the ranking of the retrieved documents.
    It can also be misleading as retrieving all documents in the knowledge base will
    result in a perfect recall value. Figure 5.1 visualizes various precision and
    recall scenarios.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH05_F01_Kimothi.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1  Precision and recall
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: F1-score
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'F1-score is the harmonic mean of precision and recall. It provides a single
    metric that balances both the quality and coverage of the retriever:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-4x.png)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The equation is such that the F1-score penalizes either variable having a low
    score; a high F1 score is only possible when both recall and precision values
    are high. This means that the score cannot be positively skewed by a single variable.
    Figure 5.2 illustrates how the F1-score balances precision and recall.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程式使得F1分数惩罚任何得分低的变量；只有当召回率和精确度值都很高时，才能得到高F1分数。这意味着分数不能由单个变量正偏斜。图5.2说明了F1分数如何平衡精确度和召回率。
- en: '![A diagram of a graph'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图的示意图'
- en: AI-generated content may be incorrect.](../Images/CH05_F02_Kimothi.png)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能不正确。](../Images/CH05_F02_Kimothi.png)
- en: Figure 5.2  F1-score balances precision and recall. A medium value of both precision
    and recall gets a higher F1-score than if one value is very high and the other
    is very low.
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2  F1分数平衡了精确度和召回率。精确度和召回率的中间值比一个非常高而另一个非常低的值得到的F1分数要高。
- en: F1-score provides a single, balanced measure that can be used to easily compare
    different systems. However, it does not take ranking into account and gives equal
    weight to precision and recall, which might not always be ideal.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数提供了一个单一、平衡的度量，可以用来轻松比较不同的系统。然而，它不考虑排名，并且对精确度和召回率给予相同的权重，这可能并不总是理想的。
- en: Mean reciprocal rank
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平均倒数排名
- en: 'Mean reciprocal rank, or MRR, is particularly useful in evaluating the rank
    of the relevant document. It measures the reciprocal of the ranks of the first
    relevant document in the list of results. MRR is calculated over a set of queries:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 平均倒数排名，或MRR，在评估相关文档排名时特别有用。它测量列表中第一个相关文档的排名的倒数。MRR在查询集上计算：
- en: '![A diagram of a machine'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-5x.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-5x.png)
- en: where N is the total number of queries, and ranki  is the rank of the first
    relevant document of the i-th query.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中N是查询总数，ranki是第i个查询的第一个相关文档的排名。
- en: MRR is particularly useful when you’re interested in how quickly the system
    can find a relevant document and consider the ranking of the results. However,
    since it doesn’t look at anything beyond the first relevant result, it may not
    be useful when multiple relevant results are important. Figure 5.3 shows how the
    mean reciprocal rank is calculated.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: MRR在你想了解系统如何快速找到相关文档并考虑结果排名时特别有用。然而，由于它没有考虑第一个相关结果之外的内容，当多个相关结果都很重要时，它可能没有用。图5.3展示了平均倒数排名的计算方法。
- en: '![A paper with a number of results'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![带有多个结果的论文'
- en: AI-generated content may be incorrect.](../Images/CH05_F03_Kimothi.png)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能不正确。](../Images/CH05_F03_Kimothi.png)
- en: Figure 5.3  MRR considers the ranking but doesn’t consider all the documents.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3  MRR考虑了排名，但没有考虑所有文档。
- en: Mean average precision
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均值平均精度
- en: 'Mean average precision, or MAP, is a metric that combines precision and recall
    at different cut-off levels of ‘k’, that is, the cut-off number for the top results.
    It calculates a measure called average precision and then averages it across all
    queries:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 均值平均精度，或MAP，是一个度量，它结合了不同“k”截止水平的精确度和召回率，即顶部结果的截止数。它计算一个称为平均精度的度量，然后对所有查询进行平均：
- en: '![A diagram of a machine'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-6x.png)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-6x.png)
- en: where Ri is the number of relevant documents for query i, Precision@k is the
    precision at cut-off ‘k’, and rel@k is a binary flag indicating the relevance
    of the document at rank k.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Ri是查询i的相关文档数，Precision@k是截止“k”处的精确度，rel@k是一个二进制标志，表示排名k的文档的相关性。
- en: 'Mean average precision is the mean of the average precision over all the N
    queries:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 均值平均精度是所有N个查询的平均精度：
- en: '![A diagram of a machine'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-7x.png)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-7x.png)
- en: MAP provides a single measure of quality across recall levels. It is quite suitable
    when result ranking is important but complex to calculate. Let’s look at an example
    MAP calculation in figure 5.4.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: MAP提供了一种在召回率水平上的单一质量度量。当结果排序很重要但计算复杂时，它非常合适。让我们看看图5.4中的MAP计算示例。
- en: '![A screenshot of a computer'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机屏幕截图'
- en: AI-generated content may be incorrect.](../Images/CH05_F04_Kimothi.png)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能不正确。](../Images/CH05_F04_Kimothi.png)
- en: Figure 5.4  MAP considers all the retrieved documents and gives a higher score
    for better ranking
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4  MAP考虑了所有检索到的文档，并为更好的排名给出更高的分数
- en: Normalized discounted cumulative gain
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 归一化折现累积增益
- en: 'Normalized discounted cumulative gain (nDCG) evaluates the ranking quality
    by considering the position of relevant documents in the result list and assigning
    higher scores to relevant documents appearing earlier. It is particularly effective
    for scenarios where documents have varying degrees of relevance. To calculate
    discounted cumulative gain (DCG), each document in the retrieved list is assigned
    a relevance score, rel, and a discount factor reduces the weight of documents
    as their rank position increases:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化折现累积增益（nDCG）通过考虑相关文档在结果列表中的位置并给予较早出现的相关文档更高的分数来评估排名质量。它在文档具有不同相关度级别的场景中特别有效。为了计算折现累积增益（DCG），检索列表中的每个文档都被分配一个相关性分数
    rel，以及一个折扣因子减少文档的权重，随着其排名位置的上升：
- en: '![A diagram of a machine'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-8x.png)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-8x.png)
- en: where reli is the graded relevance of the document at position I, and IDCG is
    the ideal DCG, which is the DCG for perfect ranking.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 reli 是文档在位置 I 的分级相关性，IDCG 是理想DCG，即完美排名的DCG。
- en: 'nDCG is calculated as the ratio between actual DCG and the IDCG:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: nDCG是实际DCG与IDCG的比率：
- en: '![A diagram of a machine'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-9x.png)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-9x.png)
- en: Figure 5.5 shows an example of nDCG calculation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5展示了nDCG计算的示例。
- en: '![A paper with numbers and letters'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![带有数字和字母的论文'
- en: AI-generated content may be incorrect.](../Images/CH05_F05_Kimothi.png)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH05_F05_Kimothi.png)
- en: Figure 5.5  nDCG addresses degrees of relevance in documents and penalizes incorrect
    ranking.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5  nDCG处理文档中的相关度级别，并惩罚错误的排名。
- en: nDCG is a complex metric to calculate. It requires documents to have a relevance
    score, which may lead to subjectivity, and the choice of the discount factor affects
    the values significantly, but it accounts for varying degrees of relevance in
    documents and gives more weight to higher-ranked items.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: nDCG是一个复杂的指标，计算起来很复杂。它需要文档具有相关性分数，这可能导致主观性，折扣因子的选择对值有显著影响，但它考虑了文档中不同级别的相关性，并给予排名较高的项目更高的权重。
- en: Retrieval systems are not just used in RAG but also in a variety of other application
    areas such as web and enterprise search engines, e-commerce product search and
    personalized recommendations, social media ad retrieval, archival systems, databases,
    virtual assistants, and more. The retrieval metrics help in assessing and improving
    the performance to effectively meet user needs. Table 5.1 summarizes different
    retrieval metrics.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 检索系统不仅用于RAG，还用于各种其他应用领域，如网络和搜索引擎、电子商务产品搜索和个性化推荐、社交媒体广告检索、归档系统、数据库、虚拟助手等。检索指标有助于评估和改进性能，以有效满足用户需求。表5.1总结了不同的检索指标。
- en: Table 5.1 Retrieval metrics
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.1 检索指标
- en: '| Metric | What it measures | Strengths | Use cases | Considerations |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 衡量内容 | 优点 | 用例 | 考虑事项 |'
- en: '| Accuracy | Overall correctness of retrieval | Simple to understand; includes
    true negatives | General performance in balanced datasets | Can be misleading
    in imbalanced datasets; doesn’t consider ranking |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Accuracy | 检索的整体正确性 | 容易理解；包括真实负例 | 平衡数据集的一般性能 | 在不平衡数据集中可能具有误导性；不考虑排名 |'
- en: '| Precision | Quality of retrieved results | Easy to understand and calculate
    | General retrieval quality assessment | Doesn’t consider ranking or completeness
    of retrieval |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Precision | 检索结果的品质 | 容易理解和计算 | 通用检索质量评估 | 不考虑排名或检索的完整性 |'
- en: '| Precision@k | Quality of top k retrieved results | Focuses on most relevant
    results for RAG | When only top k results are used for augmentation | Choose k
    based on your RAG system’s usage |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Precision@k | 前 k 个检索结果的品质 | 专注于RAG中最相关的结果 | 仅使用前 k 个结果进行增强 | 根据你的RAG系统使用情况选择
    k |'
- en: '| Recall | Coverage of relevant documents | Measures completeness of retrieval
    | Assessing if important information is missed | Requires knowing all relevant
    documents in the corpus |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Recall | 相关文档的覆盖率 | 衡量检索的完整性 | 评估是否遗漏了重要信息 | 需要知道语料库中所有相关文档 |'
- en: '| F1-score | Balance between precision and recall | Single metric combining
    quality and coverage | Overall retrieval performance | May obscure tradeoffs between
    precision and recall |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| F1-score | 精确率和召回率的平衡 | 结合质量和覆盖率的单一指标 | 通用检索性能 | 可能会模糊精确率和召回率之间的权衡 |'
- en: '| Mean reciprocal rank (MRR) | How quickly a relevant document is found | Emphasizes
    finding at least one relevant result quickly | When finding one good result is
    sufficient | Less useful when multiple relevant results are needed |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 平均倒数排名（MRR） | 快速找到相关文档的速度 | 强调快速找到至少一个相关结果 | 当找到一个好结果就足够时 | 当需要多个相关结果时不太有用
    |'
- en: '| Mean average precision (MAP) | Precision at different recall levels | Considers
    both precision and ranking | Comprehensive evaluation of ranked retrieval results
    | More complex to calculate and interpret |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 平均平均精度（MAP） | 不同召回率水平的精度 | 考虑精度和排名 | 对排名检索结果的全面评估 | 计算和解释更复杂 |'
- en: '| Normalized discounted cumulative gain (nDCG) | Ranking quality with graded
    relevance | Accounts for varying degrees of relevance and ranking | When documents
    have different levels of relevance | Requires relevance scoring for documents
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 标准化折现累积增益（nDCG） | 带有分级相关性的排名质量 | 考虑不同程度的相关性和排名 | 当文档具有不同级别的相关性时 | 需要对文档进行相关性评分
    |'
- en: Not all retrieval metrics are popular for evaluation. Often, the more complex
    metrics are overlooked for the sake of explainability. The usage of these metrics
    depends on the stage of improvement in the evolution of system performance you
    are in. For example, to start with, you may just be trying to improve precision,
    while at an evolved stage, you may be looking for better ranking.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有检索指标都适用于评估。通常，为了可解释性，更复杂的指标会被忽视。这些指标的使用取决于你在系统性能演变过程中的改进阶段。例如，一开始你可能只是试图提高精确度，而在一个进化的阶段，你可能正在寻找更好的排名。
- en: While these metrics focus on retrieval in general, some metrics have been created
    specifically for RAG applications. These metrics focus on the three quality scores
    discussed in section 5.1\.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些指标主要关注检索，但一些指标是专门为RAG应用创建的。这些指标专注于第5.1节中讨论的三个质量评分。
- en: 5.2.2 RAG-specific metrics
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 RAG特定指标
- en: 'The three quality scores used to evaluate RAG applications are context relevance,
    answer relevance, and answer faithfulness. These scores specifically answer the
    following three questions:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估RAG应用的三个质量评分是语境相关性、答案相关性和答案忠实度。这些评分具体回答以下三个问题：
- en: Is the information retrieval relevant to the user query?
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息检索是否与用户查询相关？
- en: Is the generated answer rooted in the retrieved information?
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的答案是否基于检索到的信息？
- en: Is the generated answer relevant to the user query?
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的答案是否与用户查询相关？
- en: Let’s now take a look at each of these scores.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些评分中的每一个。
- en: Context relevance
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语境相关性
- en: Context relevance evaluates how well the retrieved documents relate to the original
    query. The key aspects are topical alignment, information usefulness, and redundancy.
    There are human evaluation methods, as well as semantic similarity measures to
    calculate context relevance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 语境相关性评估检索到的文档与原始查询的相关程度。关键方面包括主题一致性、信息有用性和冗余。存在人类评估方法和语义相似度度量来计算语境相关性。
- en: 'One such measure is employed by the Retrieval-Augmented Generation Assessment
    (RAGAs) framework (further discussed in section 5.3). The retrieved context should
    contain information only relevant to the query or the prompt. For context relevance,
    a metric S is estimated, where S is the number of sentences in the retrieved context
    relevant for responding to the query or the prompt:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标被检索增强生成评估（RAGAs）框架采用（将在第5.3节中进一步讨论）。检索到的上下文应只包含与查询或提示相关的信息。对于语境相关性，估计一个指标S，其中S是检索上下文中与回答查询或提示相关的句子数量：
- en: '![A diagram of a machine'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器图示'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-10x.png)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成描述](../Images/kimothi-ch5-eqs-10x.png)
- en: Figure 5.6 is an illustrative example of high and low context relevance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6是高语境和低语境相关性的一个示例。
- en: '![A screenshot of a test'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![测试截图'
- en: AI-generated content may be incorrect.](../Images/CH05_F06_Kimothi.png)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH05_F06_Kimothi.png)
- en: Figure 5.6  Context relevance evaluates the degree to which the retrieved information
    is relevant
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6  语境相关性评估检索信息的相关程度
- en: to the query.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与查询的相关性。
- en: The number of relevant sentences is also sometimes customized to the sum of
    similarity scores of each of the sentences with the query. Context relevance ensures
    that the generation component has access to appropriate information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 相关句子的数量有时也会根据每个句子与查询的相似度得分之和进行定制。语境相关性确保生成组件可以访问适当的信息。
- en: Answer faithfulness
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 答案忠实度
- en: 'Answer faithfulness is the measure of the extent to which the response is factually
    grounded in the retrieved context. Faithfulness ensures that the facts in the
    response do not contradict the context and can be traced back to the source. It
    also ensures that the LLM is not hallucinating. In the RAGAs framework, faithfulness
    first identifies the number of claims made in the response and calculates the
    proportion of those claims present in the context:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-11x.png)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example in figure 5.7
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![A white paper with text and numbers'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F07_Kimothi.png)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7  Answer faithfulness evaluates the closeness of the generated response
    to the retrieved context.
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Faithfulness is not a complete measure of factual accuracy but only evaluates
    the groundedness to the context. An inverse metric for faithfulness is also the
    *hallucination rate***,** which can calculate the proportion of generated claims
    in the response that are not present in the retrieved context.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Another related metric to faithfulness is *coverage*. Coverage measures the
    number of relevant claims in the context and calculates the proportion of relevant
    claims present in the generated response. It measures how much of the relevant
    information from the retrieved passages is included in the generated answer:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-12x.png)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Answer relevance
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like context relevance measures the relevance of the retrieved context to the
    query, answer relevance is the measure of the extent to which the response is
    relevant to the query. This metric focuses on key aspects such as the system’s
    ability to comprehend the query, the response being pertinent to the query, and
    the completeness of the response.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'In RAGAs, for this metric, a response is generated for the initial query or
    prompt. To compute the score, the LLM is then prompted to generate questions for
    the generated response several times. The mean cosine similarity between these
    questions and the original one is then calculated. The concept is that if the
    answer addresses the initial question correctly, the LLM should generate questions
    from it that match the original question:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a machine'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](../Images/kimothi-ch5-eqs-13x.png)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: where N is the number of queries generated by the LLM.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that answer relevance is not a measure of truthfulness but only of relevance.
    The response may or may not be factually accurate, but it may be relevant. Figure
    5.8 is an illustration of the answer relevance calculation. Can you find the reason
    why the relevance is not very high? (Hint: The answer may have some irrelevant
    facts.) Answer relevance ensures that the RAG system provides useful and appropriate
    responses, enhancing user satisfaction and the system’s practical utility.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Tradeoffs and other considerations
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权衡和其他考虑因素
- en: These three metrics and their derivatives form the core of RAG quality evaluation.
    Furthermore, these metrics are interconnected and sometimes involve tradeoffs.
    High context relevance usually leads to better faithfulness, as the system has
    access to more pertinent information. However, high faithfulness doesn’t always
    guarantee high answer relevance. A system might faithfully reproduce information
    from the retrieved passages but fail to directly address the query. Optimizing
    for answer relevance without considering faithfulness might lead to responses
    that seem appropriate but contain hallucinated or incorrect information.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个指标及其导数构成了RAG质量评估的核心。此外，这些指标相互关联，有时还涉及权衡。高上下文相关性通常会导致更高的忠实度，因为系统可以访问更多相关信息。然而，高忠实度并不总是保证高答案相关性。一个系统可能忠实于检索到的段落中的信息，但未能直接回答查询。在不考虑忠实度的情况下优化答案相关性可能会导致看似合适的响应，但包含幻觉或错误信息。
- en: We have discussed quite a few metrics in this section. Effective interpretation
    of these metrics is crucial for performance improvement. As creators of RAG systems,
    you should use these metrics to compare with similar systems. You can also look
    at consistent trends to identify the strengths and weaknesses of your system.
    A low-precision high-recall system may indicate that your system is retrieving
    a lot of documents, and you may need to make your retriever more selective. A
    low-precision low-recall system points out fundamental problems with retrieval,
    and you may need to reassess the indexing pipeline itself. The same problem may
    be indicated by a low MAP or a low context-relevance score. Similarly, a low MRR
    or a low nDCG value may indicate a problem with the ranking algorithm of the retriever.
    To address low-answer faithfulness or low-answer relevance, you may need to improve
    your prompts or fine-tune the LLM.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中讨论了许多指标。有效解释这些指标对于性能提升至关重要。作为RAG系统的创建者，您应该使用这些指标来与其他类似系统进行比较。您还可以观察一致的趋势，以识别您系统的优势和劣势。低精确度高召回率的系统可能表明您的系统检索了大量文档，您可能需要使检索器更加选择性地检索。低精确度低召回率的系统指出检索存在根本问题，您可能需要重新评估索引管道本身。低MAP或低上下文相关性分数可能表明存在相同的问题。同样，低MRR或低nDCG值可能表明检索器的排名算法存在问题。为了解决低答案忠实度或低答案相关性，您可能需要改进您的提示或微调LLM。
- en: '![A screenshot of a diagram'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的截图'
- en: AI-generated content may be incorrect.](../Images/CH05_F08_Kimothi.png)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是错误的。](../Images/CH05_F08_Kimothi.png)
- en: Figure 5.8  Answer relevance is calculated as the mean of cosine similarity
    between the original and synthetic questions.
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.8 答案相关性是通过计算原始问题和合成问题之间的余弦相似度的平均值来计算的。
- en: There may also exist some tradeoffs that you will need to balance. Improving
    precision often reduces recall and vice-versa. Highly relevant but brief contexts
    may lead to incomplete answers, and high answer faithfulness may sometimes come
    at the cost of answer relevance.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能存在一些需要您权衡的权衡。提高精确度通常会降低召回率，反之亦然。高度相关但简短的上下文可能导致答案不完整，而高答案忠实度有时可能以牺牲答案相关性为代价。
- en: The relative importance of each metric will depend on your use case and user
    requirements. You may need to include other metrics specific to your downstream
    use case, such as summarization to measure conciseness, and chatbots to emphasize
    conversation coherence.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 每个指标相对的重要性将取决于您的用例和用户需求。您可能需要包括针对您的下游用例特定的其他指标，例如用于衡量简洁性的摘要，以及用于强调对话连贯性的聊天机器人。
- en: Developers can code these metrics from scratch and integrate them in the development
    and deployment process of their RAG system. However, you’ll find evaluation frameworks
    that are readily available quite handy. We discuss three popular frameworks in
    the next section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以从头开始编写这些指标并将其集成到他们RAG系统的开发和部署过程中。然而，您会发现现成的评估框架非常方便。我们将在下一节讨论三个流行的框架。
- en: Human evaluations and ground truth data
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人工评估和真实数据
- en: Most of the metrics we discussed talk about a concept of relevant documents.
    For example, precision is calculated as the number of relevant documents retrieved,
    divided by the total number of retrieved documents. The question that arises is,
    how does one establish that a document is relevant?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中讨论的大多数指标都涉及相关文档的概念。例如，精确度是检索到的相关文档数量除以检索到的总文档数量。出现的问题是，一个人如何确定一个文档是相关的？
- en: The simple answer is a human evaluation approach. A subject matter expert looks
    at the documents and determines the relevance. Human evaluation brings in subjectivity,
    and therefore, human evaluations are done by a panel of experts rather than an
    individual. But human evaluations are restrictive from a scale and a cost perspective.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的答案是采用人工评估方法。主题专家查看文档并确定相关性。人工评估引入了主观性，因此，人工评估是由专家小组而不是个人进行的。但人工评估在规模和成本方面存在限制。
- en: Any data that can reliably establish relevance becomes extremely useful consequently.
    Ground truth is information known to be real or true. In RAG, and the generative
    AI domain in general, ground truth is a prepared set of prompt–context–response
    or question–context–response examples, akin to labeled data in supervised machine
    learning parlance. Ground truth data created for your knowledge base can be used
    for the evaluation of your RAG system.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 任何可以可靠地建立相关性的数据都变得极其有用。地面实据是已知为真实或正确的信息。在RAG和生成式AI领域，地面实据是一组准备好的提示-上下文-响应或问题-上下文-响应示例，类似于监督机器学习中的标记数据。为您的知识库创建的地面实据数据可以用于评估您的RAG系统。
- en: How does one go about creating the ground truth data? It can be viewed as a
    one-time exercise where a group of experts creates this data. However, generating
    hundreds of QCA (question–context–answer) samples from documents manually can
    be a time-consuming and labor-intensive task. Additionally, if the knowledge base
    is dynamic, the ground truth data will also need updates. Questions created by
    humans may face challenges in achieving the necessary level of complexity for
    a comprehensive evaluation, potentially affecting the overall quality of the assessment.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如何创建地面实据数据？这可以被视为一次性的练习，其中一组专家创建这些数据。然而，从文档中手动生成数百个QCA（问题-上下文-答案）样本可能是一个耗时且劳动密集型的任务。此外，如果知识库是动态的，地面实据数据也需要更新。由人类创建的问题可能面临达到全面评估所需复杂性的挑战，这可能会影响评估的整体质量。
- en: LLMs can be used to address these challenges. Synthetic data generation uses
    LLMs to generate diverse questions and answers from the documents in the knowledge
    base. LLMs can be prompted to create questions such as simple questions, multi-context
    questions, conditional questions, reasoning questions, and similar using the documents
    from the knowledge base as context.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）可以用来解决这些挑战。合成数据生成使用LLMs从知识库中的文档生成多样化的问题和答案。LLMs可以被提示创建问题，如简单问题、多上下文问题、条件问题、推理问题等，使用知识库中的文档作为上下文。
- en: 5.3 Frameworks
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 框架
- en: 'Frameworks provide a structured approach to RAG evaluations. They can be used
    to automate the evaluation process. Some go beyond and assist in the synthetic
    ground truth data generation. While new evaluation frameworks continue to be introduced,
    there are two popular ones that we discuss here:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 框架为RAG评估提供了一个结构化的方法。它们可以用来自动化评估过程。一些框架甚至超越了这一点，并帮助生成合成地面实据数据。虽然新的评估框架仍在不断推出，但这里我们讨论两个流行的框架：
- en: RAGAs (Retrieval-Augmented Generation Assessment)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAGAs（检索增强生成评估）
- en: ARES (Automated RAG Evaluation System)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ARES（自动RAG评估系统）
- en: 5.3.1 RAGAs
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 RAGAs
- en: Retrieval-Augmented Generation Assessment, or RAGAs, is a framework developed
    by Exploding Gradients that assesses the retrieval and generation components of
    RAG systems without relying on extensive human annotations. RAGAs
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Retrieval-Augmented Generation Assessment（RAGAs，检索增强生成评估），是由Exploding Gradients开发的一个框架，它评估RAG系统的检索和生成组件，而不依赖于大量的人工标注。RAGAs
- en: Synthetically generate a test dataset that can be used to evaluate a RAG pipeline.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过合成生成一个可用于评估RAG管道的测试数据集。
- en: Use metrics to measure the performance of the pipeline.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用指标来衡量管道的性能。
- en: Monitor the quality of the application in production.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控生产中的应用质量。
- en: We will continue with our example of the Wikipedia page of the 2023 Cricket
    World Cup, but we first create a synthetic test dataset using RAGAs and then use
    the RAGAs metrics to evaluate the performance of the RAG pipeline we created in
    chapters 3 and 4.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续以2023年板球世界杯的维基百科页面为例，但首先使用RAGAs创建一个合成测试数据集，然后使用RAGAs指标来评估我们在第3章和第4章中创建的RAG管道的性能。
- en: Synthetic test dataset generation (ground truths)
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 合成测试数据集生成（地面实据）
- en: Section 5.2 pointed out that ground truths data is necessary to calculate evaluation
    metrics for assessing the quality of RAG pipelines. While this data can be manually
    curated, RAGAs provides the functionality of generating this dataset from the
    documents in the knowledge base.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: RAGAs does this using an LLM. It analyses the documents in the knowledge base
    and uses an LLM to generate seed questions from chunks in the knowledge base.
    These questions are based on the document chunks from the knowledge base. These
    chunks act as the context for the questions. Another LLM is used to generate the
    answer to these questions. This is how it generates a question–context–answer
    data based on the documents in the knowledge base. RAGAs also has an evolver module
    that creates more difficult questions (e.g., multi-context, reasoning, and conditional)
    for a more comprehensive evaluation. Figure 5.9 illustrates the process of synthetic
    data generation using RAGAs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of questions and arrows'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F09_Kimothi.png)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9  Synthetic ground truths data generation using RAGAs
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To evaluate our RAG pipeline, let’s recreate the documents from the Wikipedia
    page like we did in chapter 3\. Note that we will have to install the packages
    used in the previous chapters to continue with the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `html_data_transformed` contains the necessary document format of the Wikipedia
    page. We will use RAGAs library to generate the dataset from these documents.
    For that, we will first need to install the RAGAs library:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `testset` that we created contains 20 questions based on our document, along
    with the chunk of the document that the question was based on, and the ground
    truth answer. A screenshot of the dataset is shown in figure 5.10.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a table'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F10_Kimothi.png)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10  Synthetic test data generated using RAGAs
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will use this dataset to evaluate our RAG pipeline.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Recreating the RAG pipeline
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the created test dataset, we use the `question` and the `ground_truth`
    information. We pass the questions to our RAG pipeline and generate answers. We
    compare these answers with the `ground_truth` to calculate the evaluation metrics.
    First, we recreate our RAG pipeline. Again, it is important to note that we will
    have to install the packages we used in the previous chapters to continue with
    the code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can try this pipeline to generate answers.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have the RAG pipeline function, we can evaluate this pipeline using
    the questions that have been synthetically generated.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Evaluations
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first generate answers to the questions in the synthetic test data using
    our RAG pipeline. We then compare the answers to the ground truth answers. We
    first generate the answers:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For RAGAs, the evaluation set needs to be in the `Dataset` format:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have the complete evaluation dataset, we can invoke the metrics:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can also check the official documentation of RAGAs for more information
    ([https://docs.ragas.io/en/stable/](https://docs.ragas.io/en/stable/)). RAGAs
    calculates a bunch of metrics that are useful for assessing the quality of the
    RAG pipeline. RAGAs uses an LLM to do this, somewhat subjective, task. For example,
    to calculate faithfulness for a given question–context–answer record, RAGAs first
    breaks down the answer into simple statements. Then, for each statement, it asks
    the LLM whether the statement can be inferred from the context. The LLM provides
    a 0 or 1 response along with a reason. This process is repeated a couple of times.
    Finally, faithfulness is calculated as the proportion of statements judged by
    the LLM as faithful (i.e., 1). Several other metrics are calculated using this
    LLM-based approach. This approach, where an LLM is used in evaluating a task,
    is also popularly called *LLM as a judge* approach. An important point to note
    here is that the accuracy of this evaluation is also dependent on the quality
    of the LLM being used as the judge.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Automated RAG evaluation system
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Automated RAG evaluation system, or ARES, is a framework developed by researchers
    at Stanford University and Databricks. Like RAGAs, ARES uses an LLM as a judge
    approach for evaluations. Both request a language model to classify answer relevance,
    context relevance, and faithfulness for a given query. However, there are some
    differences:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: RAGAs relies on heuristically written prompts sent to the LLM for evaluation.
    ARES, in contrast, trains a classifier using a language model.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAGAs aggregates the responses from the LLM to arrive at a score. ARES provides
    confidence intervals for the scores using a framework called Prediction-Powered
    Inference (PPI).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAGAs generates a simple synthetic question–context–answer dataset for evaluation
    from the documents. ARES generate synthetic datasets comprising both positive
    and negative examples of query–passage–answer triples.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ARES requires more data than RAGAs. To use ARES, you need the following three
    datasets:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '*In-domain passage se**t*—This is a collection of passages relevant to the
    specific domain being evaluated. The passages should be suitable for generating
    queries and answers. In our case, it will be the documents that we created from
    the Wikipedia article.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human preference validation se**t*—A minimum of approximately 150 annotated
    data points is required. This set is used to validate the preferences of human
    annotators regarding the relevance of the generated query-passage–answer triples.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Few-shot example**s*—At least five examples of in-domain queries and answers
    are needed. These examples help prompt the LLMs during the synthetic data generation
    process.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for a human-preference validation set and fine-tuning of language models
    for classification makes applying ARES more complex. The application of ARES is
    out of the scope of this book. However, ARES is a robust framework. It provides
    a detailed analysis of system performance with statistical confidence intervals,
    making it suitable for in-depth RAG system evaluations. RAGAs promises a faster
    evaluation cycle without reliance on human annotations. More details on the ARES
    application can be found in the official GitHub repository ([https://github.com/stanford-futuredata/ARES](https://github.com/stanford-futuredata/ARES)).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: While RAGAs and ARES have gained popularity, there are other frameworks, such
    as TruLens, DeepEval, and RAGChecker, that have also gotten acceptance amongst
    RAG developers.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks provide a standardized method of automating the evaluation of your
    RAG pipelines. Your choice of the evaluation framework should depend on your use
    case requirements. For quick and easy evaluations that are widely understood,
    RAGAs may be your choice. For robustness across diverse domains and question types,
    ARES might suit better. Most of the proprietary service providers (vector DBs,
    LLMs, etc.) have their evaluation features you may use. You can also develop your
    metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Next, we look at benchmarks. Benchmarks are used to compare competing RAG systems
    with one another.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Benchmarks
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmarks provide a standard point of reference to evaluate the quality and
    performance of a system. RAG benchmarks are a set of standardized tasks, and a
    dataset used to compare the efficiency of different RAG systems in retrieving
    relevant information and generating accurate responses. There has been a surge
    in creating benchmarks since 2023, when RAG started gaining popularity, but there
    have been benchmarks on question-answering tasks that were introduced before that.
    Benchmarks such as Stanford Question Answering Dataset (SQuAD), WikiQA, Natural
    Question (NQ), and HotpotQA are open domain question-answering datasets that primarily
    evaluate the retriever component using metrics such as Exact Match (EM) and F1-score.
    BEIR or benchmarking information retrieval is a comprehensive, heterogeneous benchmark
    based on 9 IR tasks and 19 question–answer datasets. This section discusses three
    of the popular RAG-specific benchmarks and their evaluation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 RGB
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Retrieval-augmented generation benchmark (RGB) was introduced in a December
    2023 paper ([https://arxiv.org/pdf/2309.01431](https://arxiv.org/pdf/2309.01431)).
    It comprises 600 base questions and 400 additional questions, evenly split between
    English and Chinese. The corpus was constructed using a multistep process that
    involved collecting recent news articles, generating questions and answers using
    ChatGPT, retrieving relevant web pages through Google’s API, and selecting the
    most pertinent text chunks using a dense retrieval model. It is a benchmark that
    focuses on four key abilities of a RAG system: noise robustness, negative rejection,
    information integration, and counterfactual robustness, as illustrated in figure
    5.11.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'RGB focuses on the following metrics for evaluation:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '*Accurac**y*—Used for noise robustness and information integration. It is based
    on the exact matching of the generated text with the correct answer.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rejection rat**e*—Used for negative rejection. It is measured by exact matching
    of the model’s output with a specific rejection phrase. The rejection rate is
    also'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A group of text boxes'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F11_Kimothi.png)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.11  Four abilities required of RAG systems. Source: Benchmarking Large
    Language Models in Retrieval-Augmented Generation by Chen et al., [https://arxiv.org/pdf/2309.0143](https://arxiv.org/pdf/2309.0143).'
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: evaluated using ChatGPT to determine whether the responses contain rejection
    information.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Error detection rat**e*—Used for counterfactual robustness. It is measured
    by exact matching of the model’s output with a specific error-detection phrase
    and is also evaluated using ChatGPT.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Error correction rat**e*—Used for counterfactual robustness. It measures whether
    the model can provide the correct answer after identifying errors.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the GitHub repository to implement RGB ([https://github.com/chen700564/RGB](https://github.com/chen700564/RGB)).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Multi-hop RAG
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Curated by researchers at HKUST, multi-hop RAG contains 2556 queries, with
    evidence for each query distributed across two to four documents. The queries
    also involve document metadata, reflecting complex scenarios commonly found in
    real-world RAG applications. It contains four types of queries:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '*Inferenc**e*—Synthesizing information across multiple sources (e.g., Which
    report discusses the supply chain risk of Apple—the 2019 annual report or the
    2020 annual report?)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Compariso**n*—Comparing facts from different sources (e.g., Did Netflix or
    Google report higher revenue for the year 2023?)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tempora**l*—Analyzing the temporal ordering of events (e.g., e.g. Did Apple
    introduce the AirTag tracking device before or after the launch of the 5th generation
    iPad Pro?)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nul**l*—Queries not answerable from the knowledge base'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full implementation code is available at [https://github.com/yixuantt/MultiHop-RAG](https://github.com/yixuantt/MultiHop-RAG).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: CRAG
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Comprehensive RAG benchmark (CRAG), curated by Meta and HKUST, is a factual
    question-answering benchmark of 4,409 question–answer pairs and mock APIs to simulate
    web and knowledge graph (KG) search. It contains eight types (simple, conditions,
    comparison questions, aggregation questions, multi-hop questions, set queries,
    post-processing-heavy questions, and false-premise questions, as illustrated in
    figure 5.12) of queries across five domains (finance, sports, music, movie, and
    open domain).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'For each question in the evaluation set, CRAG labels the answer with one of
    four classes:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '*Perfec**t*—The response correctly answers the user’s question and contains
    no hallucinated content (scored as +1).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Acceptabl**e*—The response provides a useful answer to the user’s question
    but may contain minor errors that do not harm the usefulness of the answer (scored
    as +0.5).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Missin**g*—The response is “I don’t know”, “I’m sorry I can’t find ...”, a
    system error such as an empty response, or a request from the system to clarify
    the original question (scored as 0).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Incorrec**t*—The response provides wrong or irrelevant information to answer
    the user’s question (scored as −1).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A table of questions with text'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH05_F12_Kimothi.png)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12  Eight question types in CRAG
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For automatic evaluation, CRAG classifies an answer as perfect if it exactly
    matches the ground truth. If not, then it asks an LLM to do the classification.
    It uses two LLM evaluators. You can read more about CRAG at [https://arxiv.org/pdf/2406.04744](https://arxiv.org/pdf/2406.04744).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Other noteworthy benchmark datasets are MedRAG ([https://github.com/Teddy-XiongGZ/MedRAG](https://github.com/Teddy-XiongGZ/MedRAG)),
    which focuses on Medical Information, CRUD-RAG ([https://arxiv.org/pdf/2401.17043](https://arxiv.org/pdf/2401.17043)),
    which focuses on the Chinese language, and FeB4RAG ([https://arxiv.org/abs/2402.11891](https://arxiv.org/abs/2402.11891)),
    which focuses on federated search. If you’re developing an LLM application that
    has accurate and contextual generation as its core proposition, you’ll be able
    to communicate the quality of your application by showing how it performs on different
    benchmarks. Table 5.2 compares the different benchmarks.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.2 RAG benchmarks
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Benchmark | Dataset | Task | Metrics | Applicability |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| SQuAD | Stanford Question Answering Dataset | Open domain QA | Exact match
    (EM), F1-score | General QA tasks, model evaluation on comprehension accuracy
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| Natural questions | Real Google search queries | Open domain QA | F1-score
    | Real-world QA, information retrieval from large corpora |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| HotpotQA | Wikipedia-based QA | Multi-hop QA | EM, F1-score | QA involving
    multiple documents, complex reasoning tasks |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| BEIR | Multiple datasets | Information retrieval | nDCG@10 | Comprehensive
    IR model evaluation across multiple domains |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| RGB | News articles, ChatGPT-generated QA | Robust QA | Accuracy, rejection
    rate, error detection rate, error correction rate | Robustness and reliability
    of RAG systems |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| Multi-hop RAG | HKUST-curated queries | Complex QA | Various | RAG applications
    requiring multi-source synthesis |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| CRAG | Multiple sources (finance, sports, music, etc.) | Factual QA | Four-class
    evaluation (perfect, acceptable, missing, and incorrect) | Evaluating factual
    QA with diverse question types |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: We have looked frameworks that help in automating the calculation of evaluation
    metrics and benchmarks that enable comparisons across different implementations
    and approaches. Frameworks will assist you in improving the performance of your
    system, and benchmarks will facilitate comparing it with other systems available
    in the market.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: However, as with any evolving field, there are some limitations and challenges
    to consider. The next section examines these limitations and discusses best practices
    that have emerged to address them, ensuring a more holistic and nuanced approach
    to RAG evaluation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Limitations and best practices
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There has been a lot of progress made in the frameworks and benchmarks used
    for RAG evaluation. The complexity in evaluation arises due to the interplay between
    the retrieval and generation components. In practice, there’s a significant reliance
    on human judgements, which are subjective and difficult to scale. What follows
    are a few common challenges and some guidelines to navigate them.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Lack of standardized metrics
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There’s no consensus on what the best metrics are to evaluate RAG systems. Precision,
    recall, and F1-score are commonly measured for retrieval but do not fully capture
    the nuances of generative response. Similarly, commonly used generation metrics
    such as BLEU, ROUGE, and similar do not fully capture the context awareness required
    for RAG. Using RAG-specific metrics such as answer relevance, context relevance,
    and faithfulness for evaluation brings in the necessary nuances required for RAG
    evaluation. However, even for these metrics, there’s no standard way of calculation
    and each framework brings in its methodology.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Compare the results on RAG specific metrics from different
    frameworks. Sometimes, it may be warranted to change the calculation method with
    respect to the use case.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Overreliance on LLM as a judge
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The evaluation of RAG-specific metrics (in RAGAs, ARES, etc.) relies on using
    an LLM as a judge. An LLM is prompted or fine-tuned to classify a response as
    relevant or not. This adds to the complexity of the LLMs’ ability to do this task.
    It may be possible that the LLM may not be very accurate in judging for your specific
    documents and knowledge bases. Another problem that arises is that of self-reference.
    It is possible that if the judge LLM is the same as the generation LLM in your
    system, you will get a more favorable evaluation.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Sample a few results from the judge LLM and evaluate whether
    the results are in line with commonly understood business practice. To avoid the
    self-reference problem, make sure to use a judge LLM different from the generation
    LLM. It may also help if you use multiple judge LLMs and aggregate their results.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Lack of use case subjectivity
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most frameworks have a generalized approach to evaluation. They may not capture
    the subjective nature of the task relevant to your use case (content generation
    versus chatbot versus question-answering, etc.)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Focus on use-case-specific metrics to assess quality, coherence,
    usefulness, and similar. Incorporate human judgements in your workflow with techniques
    such as user feedback, crowd-sourcing, or expert ratings.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks are static
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most benchmarks are static and do not account for the evolving nature of information.
    RAG systems need to adapt to real-time information changes, which are not currently
    tested effectively. There is a lack of evaluation for how well RAG models learn
    and adapt to new data over time. Most benchmarks are domain-agnostic, which may
    not reflect the performance of RAG systems in your specific domain.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Use a benchmark that is tailored to your domain. The static
    nature of benchmarks is limiting. Do not overly rely on benchmarks, and augment
    the use of benchmarks with regularly updating data.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Scalability and cost
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluating large-scale RAG systems is more complex than evaluating basic RAG
    pipelines. It requires significant computational resources. Benchmarks and frameworks
    also generally do not account for metrics such as latency and efficiency, which
    are critical for real-world applications.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '*Best practice*: Employ careful sampling of test cases for evaluation. Incorporate
    workflows to measure latency and efficiency.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these, you should also carefully consider the aspects of bias and
    toxicity, focusing on information integration and negative rejection, which the
    frameworks do not evaluate well. It is also important to keep an eye on how these
    evaluation frameworks and benchmarks evolve.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we comprehensively examined the evaluation metrics, frameworks,
    and benchmarks that will help you evaluate your RAG pipelines. We used RAGAs to
    evaluate the pipeline that we have been building.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we have looked at building and evaluating a simple RAG system. This
    also marks the second part 2 of this book. You are now familiar with the creation
    of the RAG knowledge brain using the indexing pipeline, enabling real-time interaction
    using the generation pipeline and evaluating your RAG system using frameworks
    and benchmarks.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: In the next part, we will move toward discussing the production aspects of RAG
    systems. In chapter 6, we will look at strategies and advanced techniques to improve
    our RAG pipeline, which should also reflect in better evaluation metrics. In chapter
    7, we will look at the LLMOps stack that enables RAG in production.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG evaluation fundamentals
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG evaluation assesses how well systems reduce hallucinations and ground responses
    in the provided context.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three key quality scores for RAG evaluation are context relevance, answer faithfulness,
    and answer relevance.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four critical abilities required of RAG systems include noise robustness, negative
    rejection, information integration, and counterfactual robustness.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional considerations include latency, robustness, bias, and toxicity of
    responses.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom use-case-specific metrics should be developed to evaluate performance.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation metrics
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval metrics include precision, recall, F1-score, mean reciprocal rank
    (MRR), mean average precision (MAP), and normalized discounted cumulative gain
    (nDCG).
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy, precision, recall, and F1-score do not consider the ranking order
    of the results.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG-specific metrics focus on context relevance, answer faithfulness, and answer
    relevance.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human evaluations and ground truth data play a crucial role in RAG assessment.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation frameworks
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAGAs is an easy-to-implement framework that can be used for quick evaluation
    of RAG pipelines.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARES uses a more complex approach, including classifier training and confidence
    interval calculations.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarks
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Benchmarks provide standardized datasets and metrics for comparing different
    RAG implementations on specific tasks.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular benchmarks such as SQuAD, natural questions, HotpotQA, and BEIR focus
    on retrieval quality.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recent benchmarks such as RGB, multi-hop RAG, and CRAG are more holistic from
    a RAG perspective.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarks focus on different aspects of RAG performance, such as multi-hop
    reasoning or specific domains.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations and best practices
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Challenges in RAG evaluation include lack of standardized metrics, overreliance
    on LLMs as judges, and static nature of benchmarks.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices include using multiple frameworks, incorporating use-case-specific
    metrics, and regularly updating evaluation data.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing automated metrics with human judgment and considering use-case-specific
    requirements is crucial.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The field of RAG evaluation is evolving, with new frameworks and benchmarks
    constantly emerging.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers should stay informed about new developments and adapt their evaluation
    strategies accordingly.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
