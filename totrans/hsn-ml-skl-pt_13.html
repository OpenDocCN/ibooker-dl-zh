<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 11. Training Deep Neural Networks"><div class="chapter" id="deep_chapter">
<h1><span class="label">Chapter 11. </span>Training Deep Neural Networks</h1>


<p>In<a data-type="indexterm" data-primary="deep neural networks (DNNs)" id="xi_deepneuralnetworksDNNs1133_1"/> <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a> you built, trained, and fine-tuned several artificial neural networks using PyTorch. But they were shallow nets with just a few hidden layers. What if you need to tackle a complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper ANN, perhaps with dozens or even hundreds of layers, each containing hundreds of neurons, linked by hundreds of thousands of connections. Training a deep neural network isn’t a walk in the park. Here are some of the problems you could run into:</p>

<ul>
<li>
<p>You may be faced with the problem of gradients growing ever smaller or larger when flowing backward through the DNN during training. Both of these problems make lower layers very hard to train.</p>
</li>
<li>
<p>You might not have enough training data for such a large network, or it might be too costly to label.</p>
</li>
<li>
<p>Training may be extremely slow.</p>
</li>
<li>
<p>A model with millions of parameters risks severely overfitting the training set, especially if there are not enough training instances or if they are too noisy.</p>
</li>
</ul>

<p>In this chapter we will go through each of these problems and present various techniques to solve them. We will start by exploring the vanishing and exploding gradients problems and some of their most popular solutions, including smart weight initialization, better activation functions, batch-norm, layer-norm, and gradient clipping. Next, we will look at transfer learning and unsupervised pretraining, which can help you tackle complex tasks even when you have little labeled data. Then we will discuss a variety of optimizers that can speed up training large models tremendously. We will also discuss how you can tweak the learning rate during training to speed up training and produce better models. Finally, we will cover a few popular 
<span class="keep-together">regularization</span> techniques for large neural networks: ℓ<sub>1</sub> and ℓ<sub>2</sub> regularization, dropout, Monte Carlo dropout, and max-norm regularization.</p>

<p>With these tools, you will be able to train all sorts of deep nets. Welcome to <em>deep</em> learning!</p>






<section data-type="sect1" data-pdf-bookmark="The Vanishing/Exploding Gradients Problems"><div class="sect1" id="id176">
<h1>The Vanishing/Exploding Gradients Problems</h1>

<p>As<a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="vanishing and exploding gradients" data-see="vanishing and exploding gradients" id="id2442"/><a data-type="indexterm" data-primary="vanishing and exploding gradients" id="xi_vanishingandexplodinggradients11153_1"/> discussed in <a data-type="xref" href="ch09.html#ann_chapter">Chapter 9</a>, the backpropagation<a data-type="indexterm" data-primary="backpropagation" id="id2443"/><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="backpropagation" id="id2444"/> algorithm’s second phase works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function<a data-type="indexterm" data-primary="cost function" data-secondary="gradient descent" id="id2445"/> with regard to each parameter in the network, it uses these gradients to update each parameter with a gradient descent step.</p>

<p>Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the gradient descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. This is called the <em>vanishing gradients</em> problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the <em>exploding gradients</em><a data-type="indexterm" data-primary="exploding gradients" data-seealso="vanishing and exploding gradients" id="id2446"/> problem, which surfaces most often in recurrent neural networks (see <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>). More generally, deep neural networks suffer from unstable gradients<a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="unstable gradients" id="id2447"/><a data-type="indexterm" data-primary="unstable gradients" data-seealso="vanishing and exploding gradients" id="id2448"/>; different layers may learn at widely different speeds.</p>

<p>This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. It wasn’t clear what caused the gradients to be so unstable when training a DNN, but some light was shed in a  <a href="https://homl.info/47">2010 paper</a> by Xavier Glorot and Yoshua Bengio.⁠<sup><a data-type="noteref" id="id2449-marker" href="ch11.html#id2449">1</a></sup> The authors found a few suspects, including the combination of the popular sigmoid (logistic) activation<a data-type="indexterm" data-primary="sigmoid activation function" data-secondary="and vanishing/exploding gradients problems" data-secondary-sortas="vanishing/exploding gradients problems" id="id2450"/> function and the weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the sigmoid function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the sigmoid function in deep networks).</p>

<p>Looking at the sigmoid activation function (see <a data-type="xref" href="#sigmoid_saturation_plot">Figure 11-1</a>), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0 (i.e., the curve is flat at both extremes). Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network, and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers.</p>

<figure class="width-70"><div id="sigmoid_saturation_plot" class="figure">
<img src="assets/hmls_1101.png" alt="Diagram illustrating the sigmoid activation function, showing how it saturates at 0 for large negative inputs and 1 for large positive inputs, with a linear region in the middle." width="1792" height="1299"/>
<h6><span class="label">Figure 11-1. </span>Sigmoid activation function saturation</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Glorot Initialization and He Initialization"><div class="sect2" id="id177">
<h2>Glorot Initialization and He Initialization</h2>

<p>In<a data-type="indexterm" data-primary="Glorot initialization" id="xi_Glorotinitialization11293_1"/><a data-type="indexterm" data-primary="He initialization" id="xi_Heinitialization11293_1"/><a data-type="indexterm" data-primary="Kaiming initialization" id="xi_Kaiminginitialization11293_1"/><a data-type="indexterm" data-primary="vanishing and exploding gradients" data-secondary="Glorot and He initialization" id="xi_vanishingandexplodinggradientsGlorotandHeinitialization11293_1"/> their paper, Glorot and Bengio proposed a way to significantly alleviate the unstable gradients problem. They pointed out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argued that we need the variance of the outputs of each layer to be equal to the variance of its inputs,⁠<sup><a data-type="noteref" id="id2451-marker" href="ch11.html#id2451">2</a></sup> and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction (please check out the paper if you are interested in the mathematical details). It is actually not possible to guarantee both unless the layer has an equal number of inputs and outputs (these numbers are called the <em>fan-in</em> and <em>fan-out</em><a data-type="indexterm" data-primary="fan-in/fan-out" id="id2452"/> of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly, as described in <a data-type="xref" href="#xavier_initialization_equation">Equation 11-1</a>, where <em>fan</em><sub>avg</sub> = (<em>fan</em><sub>in</sub> + <em>fan</em><sub>out</sub>) / 2. This initialization strategy is called <em>Xavier initialization</em><a data-type="indexterm" data-primary="Xavier initialization" data-see="Glorot initialization" id="id2453"/> or <em>Glorot initialization</em>, after the paper’s first author.</p>
<div data-type="equation" id="xavier_initialization_equation">
<h5 class="less_space"><span class="label">Equation 11-1. </span>Glorot initialization (when using the sigmoid activation function)</h5>
<math display="block"><mtable columnalign="left"><mtr><mtd><mtext>Normal distribution with mean 0 and variance </mtext><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><msub><mi mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></mtd></mtr><mtr><mtd><mtext>Or a uniform distribution between </mtext><mo>-</mo><mi>r</mi><mtext> and </mtext><mo>+</mo><mi>r</mi><mtext>, with </mtext><mi>r</mi><mo>=</mo><msqrt><mfrac><mn>3</mn><msub><mi mathvariant="italic">fan</mi><mtext>avg</mtext></msub></mfrac></msqrt></mtd></mtr></mtable></math>
</div>

<p>If you replace <em>fan</em><sub>avg</sub> with <em>fan</em><sub>in</sub> in <a data-type="xref" href="#xavier_initialization_equation">Equation 11-1</a>, you get an initialization strategy that Yann LeCun proposed in the 1990s. He called it <em>LeCun initialization</em>.<a data-type="indexterm" data-primary="LeCun initialization" id="id2454"/> Genevieve Orr and Klaus-Robert Müller even recommended it in their 1998 book <em>Neural Networks: Tricks of the Trade</em> (Springer). LeCun initialization is equivalent to Glorot initialization when <em>fan</em><sub>in</sub> = <em>fan</em><sub>out</sub>. It took over a decade for researchers to realize how important this trick is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of deep learning.</p>

<p>Some papers have provided similar strategies for different activation functions<a data-type="indexterm" data-primary="activation functions" data-secondary="initialization parameters" id="id2455"/>, most notably a <a href="https://homl.info/48">2015 paper by Kaiming He et al</a>.⁠<sup><a data-type="noteref" id="id2456-marker" href="ch11.html#id2456">3</a></sup> These strategies differ only by the scale of the variance and whether they use <em>fan</em><sub>avg</sub> or <em>fan</em><sub>in</sub>, as shown in <a data-type="xref" href="#initialization_table">Table 11-1</a> (for the uniform distribution, just use <math><mi>r</mi><mo>=</mo><msqrt><mn>3</mn><msup><mi>σ</mi><mn>2</mn></msup></msqrt></math>). The initialization strategy proposed for the ReLU activation function and its variants is called <em>He initialization</em> or <em>Kaiming initialization</em>, after the paper’s first author. For SELU, use Yann LeCun’s initialization method, preferably with a normal distribution. We will cover all these activation functions shortly.</p>
<table id="initialization_table">
<caption><span class="label">Table 11-1. </span>Initialization parameters for each type of activation function</caption>
<thead>
<tr>
<th>Initialization</th>
<th>Activation functions</th>
<th><em>σ</em>² (Normal)</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Xavier Glorot</p></td>
<td><p>None, tanh, sigmoid, softmax</p></td>
<td><p>1 / <em>fan</em><sub>avg</sub></p></td>
</tr>
<tr>
<td><p>Kaiming He</p></td>
<td><p>ReLU, Leaky ReLU, ELU, GELU, Swish, Mish, SwiGLU, ReLU<sup>2</sup></p></td>
<td><p>2 / <em>fan</em><sub>in</sub></p></td>
</tr>
<tr>
<td><p>Yann LeCun</p></td>
<td><p>SELU</p></td>
<td><p>1 / <em>fan</em><sub>in</sub></p></td>
</tr>
</tbody>
</table>

<p>For historical reasons, <a data-type="indexterm" data-primary="PyTorch" data-secondary="initialization handling" id="xi_PyTorchinitializationhandling115225_1"/>PyTorch’s <code translate="no">nn.Linear</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.Linear" id="xi_torchnnLinear115246_1"/> module initializes its weights using Kaiming uniform initialization, except the weights are scaled down by a factor of <math alttext="StartRoot 6 EndRoot">
  <msqrt>
    <mn>6</mn>
  </msqrt>
</math> (and the bias terms are also initialized randomly). Sadly, this is not the optimal scale for any common activation function.⁠<sup><a data-type="noteref" id="id2457-marker" href="ch11.html#id2457">4</a></sup> One solution is to simply multiply the weights by <math alttext="StartRoot 6 EndRoot">
  <msqrt>
    <mn>6</mn>
  </msqrt>
</math> (i.e., 6<sup>0.5</sup>) just after creating the <code translate="no">nn.Linear</code> layer to get proper Kaiming initialization. To do this, we can update the parameter’s <code translate="no">data</code> attribute. We will also zero out the biases, as there’s no benefit in randomly initializing them:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="n">layer</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">40</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">layer</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">data</code> <code class="o">*=</code> <code class="mi">6</code> <code class="o">**</code> <code class="mf">0.5</code>  <code class="c1"># Kaiming init (or 3 ** 0.5 for LeCun init)</code>
<code class="n">torch</code><code class="o">.</code><code class="n">zero_</code><code class="p">(</code><code class="n">layer</code><code class="o">.</code><code class="n">bias</code><code class="o">.</code><code class="n">data</code><code class="p">)</code></pre>

<p>This works, but it’s clearer and less error-prone to use one of the initialization functions available in the <code translate="no">torch.nn.init</code> module:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">nn</code><code class="o">.</code><code class="n">init</code><code class="o">.</code><code class="n">kaiming_uniform_</code><code class="p">(</code><code class="n">layer</code><code class="o">.</code><code class="n">weight</code><code class="p">)</code>
<code class="n">nn</code><code class="o">.</code><code class="n">init</code><code class="o">.</code><code class="n">zeros_</code><code class="p">(</code><code class="n">layer</code><code class="o">.</code><code class="n">bias</code><code class="p">)</code></pre>

<p>If you want to apply the same initialization method to the weights of every <code translate="no">nn.Linear</code> layer in a model, you can do so in the model’s constructor, after creating each <code translate="no">nn.Linear</code> layer. Alternatively, you can write a subclass of the <code translate="no">nn.Linear</code> class and tweak its constructor to initialize the weights as you wish. But arguably the simplest option is to write a little function that takes a module, checks whether it’s an instance of the <code translate="no">nn.Linear</code> class, and if so, applies the desired initialization function to its weights. You can then apply this function to the model and all of its submodules by passing it to the model’s <code translate="no">apply()</code> method. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">use_he_init</code><code class="p">(</code><code class="n">module</code><code class="p">):</code>
    <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">module</code><code class="p">,</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">):</code>
        <code class="n">nn</code><code class="o">.</code><code class="n">init</code><code class="o">.</code><code class="n">kaiming_uniform_</code><code class="p">(</code><code class="n">module</code><code class="o">.</code><code class="n">weight</code><code class="p">)</code>
        <code class="n">nn</code><code class="o">.</code><code class="n">init</code><code class="o">.</code><code class="n">zeros_</code><code class="p">(</code><code class="n">module</code><code class="o">.</code><code class="n">bias</code><code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="mi">40</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">40</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">())</code>
<code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">use_he_init</code><code class="p">)</code></pre>

<p>The<a data-type="indexterm" data-startref="xi_torchnnLinear115246_1" id="id2458"/> <code translate="no">torch.nn.init</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.init" id="id2459"/> module also contains an <code translate="no">orthogonal_()</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.init.orthogonal_()" id="id2460"/> function which initializes the weights using a random orthogonal matrix, as proposed in a <a href="https://homl.info/ortho-init">2014 paper</a> by Andrew Saxe et al.⁠<sup><a data-type="noteref" id="id2461-marker" href="ch11.html#id2461">5</a></sup> Orthogonal matrices<a data-type="indexterm" data-primary="orthogonal matrices" id="id2462"/> have a number of useful mathematical properties, including the fact that they preserve norms: given an orthogonal matrix <strong>W</strong> and an input vector <strong>x</strong>, the norm of <strong>Wx</strong> is equal to the norm of <strong>x</strong>, and therefore the magnitude of the inputs is preserved in the outputs. When the inputs are standardized, this results in a stable variance through the layer, which prevents the activations and gradients from vanishing or exploding in a deep network (at least at the beginning of training). This initialization technique is much less common than the initialization techniques discussed earlier, but it can work well in recurrent neural nets (<a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>) or generative adversarial networks (<a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter 18</a>).</p>

<p>And that’s it! Scaling the weights properly will give a deep neural net a much better starting point for training.<a data-type="indexterm" data-startref="xi_PyTorchinitializationhandling115225_1" id="id2463"/></p>
<div data-type="tip"><h6>Tip</h6>
<p>In a classifier<a data-type="indexterm" data-primary="classifiers, initialization" id="id2464"/>, it’s generally a good idea to scale down the weights of the output layer during initialization (e.g., by a factor of 10). Indeed, this will result in smaller logits at the beginning of training, which means they will be closer together, and hence the estimated probabilities will also be closer together. In other words, it encourages the model to be less confident about its predictions when training starts: this will avoid extreme losses and huge gradients that can often make the model’s weights bounce around randomly at the start of training, losing time and potentially preventing the model from learning anything.<a data-type="indexterm" data-startref="xi_Glorotinitialization11293_1" id="id2465"/><a data-type="indexterm" data-startref="xi_Heinitialization11293_1" id="id2466"/><a data-type="indexterm" data-startref="xi_Kaiminginitialization11293_1" id="id2467"/><a data-type="indexterm" data-startref="xi_vanishingandexplodinggradientsGlorotandHeinitialization11293_1" id="id2468"/></p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Better Activation Functions"><div class="sect2" id="id178">
<h2>Better Activation Functions</h2>

<p>One<a data-type="indexterm" data-primary="vanishing and exploding gradients" data-secondary="activation function improvements" id="xi_vanishingandexplodinggradientsactivationfunctionimprovements11944_1"/> of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients<a data-type="indexterm" data-primary="unstable gradients" id="id2469"/> were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use something pretty close to sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks—in particular, the ReLU activation function, mostly because it does not saturate for positive values, and also because it is very fast to compute.</p>

<p>Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the <em>dying ReLUs</em><a data-type="indexterm" data-primary="activation functions" data-secondary="ReLU" id="id2470"/><a data-type="indexterm" data-primary="dying ReLU problem" id="id2471"/><a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="dying ReLU problem" id="id2472"/>: during training, some neurons effectively “die”, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the input of the ReLU function (i.e., the weighted sum of the neuron’s inputs plus its bias term) is negative for all instances in the training set. When this happens, it just keeps outputting zeros, and gradient descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.⁠<sup><a data-type="noteref" id="id2473-marker" href="ch11.html#id2473">6</a></sup></p>

<p>To solve this problem, you may want to use a variant of the ReLU function, such as the <em>leaky ReLU</em>.</p>










<section data-type="sect3" data-pdf-bookmark="Leaky ReLU"><div class="sect3" id="id179">
<h3>Leaky ReLU</h3>

<p>The<a data-type="indexterm" data-primary="activation functions" data-secondary="leaky ReLU" id="xi_activationfunctionsleakyReLU111014_1"/><a data-type="indexterm" data-primary="leaky ReLU" id="xi_leakyReLU111014_1"/><a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="leaky ReLU" id="xi_ReLUrectifiedlinearunitsleakyReLU111014_1"/> leaky ReLU activation function is defined as LeakyReLU<sub><em>α</em></sub>(<em>z</em>) = max(<em>αz</em>, <em>z</em>) (see <a data-type="xref" href="#leaky_relu_plot">Figure 11-2</a>). The hyperparameter<a data-type="indexterm" data-primary="alpha (α) hyperparameter" id="id2474"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="α (alpha)" data-secondary-sortas="aaa" id="id2475"/><a data-type="indexterm" data-primary="α (alpha) hyperparameter" id="id2476"/> <em>α</em> defines how much the function “leaks”: it is the slope of the function for <em>z</em> &lt; 0. Having a slope for <em>z</em> &lt; 0 ensures that leaky ReLUs never actually die; they can go into a long coma, but they have a chance to eventually wake up. A <a href="https://homl.info/49">2015 paper</a> by Bing Xu et al.⁠<sup><a data-type="noteref" id="id2477-marker" href="ch11.html#id2477">7</a></sup> compared several variants of the ReLU activation function, and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting <em>α</em> = 0.2 (a huge leak) seemed to result in better performance than <em>α</em> = 0.01 (a small leak). The paper also evaluated the <em>randomized leaky ReLU</em> (RReLU)<a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="RReLU" id="id2478"/><a data-type="indexterm" data-primary="RReLU (randomized leaky ReLU)" id="id2479"/>, where <em>α</em> is picked randomly in a given range during training and is fixed to an average value during testing. RReLU<a data-type="indexterm" data-primary="activation functions" data-secondary="RReLU" id="id2480"/><a data-type="indexterm" data-primary="randomized leaky ReLU (RReLU)" id="id2481"/> also performed fairly well and seemed to act as a regularizer, reducing the risk of overfitting. Finally, the paper evaluated the <em>parametric leaky ReLU</em><a data-type="indexterm" data-primary="parametric leaky ReLU (PReLU)" id="id2482"/><a data-type="indexterm" data-primary="PReLU (parametric leaky ReLU)" id="id2483"/> <a data-type="indexterm" data-primary="activation functions" data-secondary="PReLU" id="id2484"/>(PReLU), where <em>α</em> is authorized to be learned during training: instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter. PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.</p>

<figure class="width-80"><div id="leaky_relu_plot" class="figure">
<img src="assets/hmls_1102.png" alt="Diagram of the leaky ReLU activation function illustrating its slope for negative values, demonstrating the &quot;leak.&quot;" width="1792" height="933"/>
<h6><span class="label">Figure 11-2. </span>Leaky ReLU: like ReLU, but with a small slope for negative values</h6>
</div></figure>

<p>As you might expect, PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="activation function support" id="id2485"/> includes modules for each of these activation functions: <code translate="no">nn.LeakyReLU</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.LeakyReLU" id="id2486"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.PReLU" id="id2487"/><a data-type="indexterm" data-primary="torch" data-secondary="nn.RReLU" id="id2488"/>, <code translate="no">nn.RReLU</code>, and <code translate="no">nn.PReLU</code>. Just like for other ReLU variants, you should use these along with Kaiming initialization, but the variance should be slightly smaller due to the negative slope: it should be scaled down by a factor of 1 + <em>α</em><sup>2</sup>. PyTorch supports this: you can pass the <em>α</em> hyperparameter to the <code translate="no">kaiming_uniform_()</code> and <code translate="no">kaiming_normal_()</code> functions, along with <code translate="no">nonlinearity="leaky_relu"</code> to get the appropriately adjusted Kaiming initialization:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">alpha</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="mi">40</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">LeakyReLU</code><code class="p">(</code><code class="n">negative_slope</code><code class="o">=</code><code class="n">alpha</code><code class="p">))</code>
<code class="n">nn</code><code class="o">.</code><code class="n">init</code><code class="o">.</code><code class="n">kaiming_uniform_</code><code class="p">(</code><code class="n">model</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">weight</code><code class="p">,</code> <code class="n">alpha</code><code class="p">,</code> <code class="n">nonlinearity</code><code class="o">=</code><code class="s2">"leaky_relu"</code><code class="p">)</code></pre>

<p>ReLU, leaky ReLU, and PReLU all suffer from the fact that they are not smooth functions: their slopes abruptly change at <em>z</em> = 0. As we saw in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a> when we discussed lasso, this sort of discontinuity in the derivatives can make gradient descent bounce around the optimum and slow down convergence. So now we will look at some smooth variants of the ReLU activation function, starting with ELU and SELU.<a data-type="indexterm" data-startref="xi_activationfunctionsleakyReLU111014_1" id="id2489"/><a data-type="indexterm" data-startref="xi_leakyReLU111014_1" id="id2490"/><a data-type="indexterm" data-startref="xi_ReLUrectifiedlinearunitsleakyReLU111014_1" id="id2491"/></p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="ELU and SELU"><div class="sect3" id="id180">
<h3>ELU and SELU</h3>

<p>In<a data-type="indexterm" data-primary="activation functions" data-secondary="ELU" id="xi_activationfunctionsELU111203_1"/><a data-type="indexterm" data-primary="ELU (exponential linear unit)" id="xi_ELUexponentiallinearunit111203_1"/><a data-type="indexterm" data-primary="exponential linear unit (ELU)" id="xi_exponentiallinearunitELU111203_1"/> 2015, a <a href="https://homl.info/50">paper</a> by Djork-Arné Clevert et al.⁠<sup><a data-type="noteref" id="id2492-marker" href="ch11.html#id2492">8</a></sup> proposed a new activation function, called the <em>exponential linear unit</em> (ELU), that outperformed all the ReLU variants in the authors’ experiments: training time was reduced, and the neural network performed better on the test set. <a data-type="xref" href="#elu_activation_equation">Equation 11-2</a> shows this activation function’s definition.</p>
<div data-type="equation" id="elu_activation_equation">
<h5><span class="label">Equation 11-2. </span>ELU activation function</h5>
<math display="block">
  <mrow>
    <msub><mo form="prefix">ELU</mo> <mi>α</mi> </msub>
    <mrow>
      <mo>(</mo>
      <mi>z</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfenced separators="" open="{" close="">
      <mtable>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <mi>α</mi>
              <mo>(</mo>
              <mtext>exp</mtext>
              <mo>(</mo>
              <mi>z</mi>
              <mo>)</mo>
              <mo>-</mo>
              <mn>1</mn>
              <mo>)</mo>
            </mrow>
          </mtd>
          <mtd columnalign="left">
            <mrow>
              <mtext>if</mtext>
              <mspace width="4.pt"/>
              <mi>z</mi>
              <mo>&lt;</mo>
              <mn>0</mn>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd columnalign="left">
            <mi>z</mi>
          </mtd>
          <mtd columnalign="left">
            <mrow>
              <mtext>if</mtext>
              <mspace width="4.pt"/>
              <mi>z</mi>
              <mo>≥</mo>
              <mn>0</mn>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math>
</div>

<p>The ELU activation function looks a lot like the ReLU function (see <a data-type="xref" href="#elu_and_selu_activation_plot">Figure 11-3</a>), with a few major <span class="keep-together">differences</span>:</p>

<ul>
<li>
<p>It takes on negative values when <em>z</em> &lt; 0, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter <em>α</em><a data-type="indexterm" data-primary="alpha (α) hyperparameter" id="id2493"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="α (alpha) hyperparameter" data-secondary-sortas="aaa" id="id2494"/><a data-type="indexterm" data-primary="α (alpha) hyperparameter" id="id2495"/> defines the opposite of the value that the ELU function approaches when <em>z</em> is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter.</p>
</li>
<li>
<p>It has a nonzero gradient for <em>z</em> &lt; 0, which avoids the dead neurons problem.</p>
</li>
<li>
<p>If <em>α</em> is equal to 1, then the function is smooth everywhere, including around <em>z</em> = 0, which helps speed up gradient descent since it does not bounce as much to the left and right of <em>z</em> = 0.</p>
</li>
</ul>

<p>Using ELU with PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="ELU" id="id2496"/> is as easy as using the <code translate="no">nn.ELU</code> module, along with Kaiming initialization. The main drawback of the ELU activation function is that it is slower to compute than the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training may compensate for that slow computation, but still, at test time an ELU network will be a bit slower than a ReLU network.</p>

<figure class="width-80"><div id="elu_and_selu_activation_plot" class="figure">
<img src="assets/hmls_1103.png" alt="Graph comparing ELU and SELU activation functions, showing SELU is scaled higher than ELU." width="1792" height="1053"/>
<h6><span class="label">Figure 11-3. </span>ELU and SELU activation functions</h6>
</div></figure>

<p>Not<a data-type="indexterm" data-startref="xi_activationfunctionsELU111203_1" id="id2497"/><a data-type="indexterm" data-primary="activation functions" data-secondary="SELU" id="xi_activationfunctionsSELU111944_1"/><a data-type="indexterm" data-startref="xi_ELUexponentiallinearunit111203_1" id="id2498"/><a data-type="indexterm" data-startref="xi_exponentiallinearunitELU111203_1" id="id2499"/><a data-type="indexterm" data-primary="SELU (scaled ELU) activation function" id="xi_SELUscaledELUactivationfunction111944_1"/> long after, a <a href="https://homl.info/selu">2017 paper</a> by Günter Klambauer et al.⁠<sup><a data-type="noteref" id="id2500-marker" href="ch11.html#id2500">9</a></sup> introduced the <em>scaled ELU</em> (SELU) activation function: as its name suggests, it is a scaled variant of the ELU activation function (about 1.05 times ELU, using <em>α</em> ≈ 1.67). The authors showed that if you build a neural network composed exclusively of a stack of dense layers (i.e., an MLP), and if all hidden layers use the SELU activation function, then the network will <em>self-normalize</em>:<a data-type="indexterm" data-primary="self-normalization" id="xi_selfnormalization11194655_1"/> the output of each layer will tend to preserve a mean of 0 and a standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function may outperform other activation functions for MLPs, especially deep ones. To use it with PyTorch, just use <code translate="no">nn.SELU</code>. There are, however, a few conditions for self-normalization to happen (see the paper for the mathematical justification):</p>

<ul>
<li>
<p>The input features must be standardized: mean 0 and standard deviation 1.</p>
</li>
<li>
<p>Every hidden layer’s weights must be initialized using LeCun normal 
<span class="keep-together">initialization.</span></p>
</li>
<li>
<p>The self-normalizing property is only guaranteed with plain MLPs. If you try to use SELU in other architectures, like recurrent networks (see <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>) or networks with <em>skip connections</em><a data-type="indexterm" data-primary="skip connections" id="id2501"/> (i.e., connections that skip layers, such as in Wide &amp; Deep neural networks), it will probably not outperform ELU.</p>
</li>
<li>
<p>You cannot use regularization techniques like ℓ<sub>1</sub> or ℓ<sub>2</sub> regularization, batch-norm, layer-norm, max-norm, or regular dropout (these are discussed later in this chapter).</p>
</li>
</ul>

<p>These are significant constraints, so despite its promises, SELU did not gain a lot of traction. Moreover, other activation functions seem to outperform it quite consistently on most tasks. Let’s look at some of the most popular ones.<a data-type="indexterm" data-startref="xi_activationfunctionsSELU111944_1" id="id2502"/><a data-type="indexterm" data-startref="xi_selfnormalization11194655_1" id="id2503"/><a data-type="indexterm" data-startref="xi_SELUscaledELUactivationfunction111944_1" id="id2504"/></p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="GELU, Swish, SwiGLU, Mish, and RELU2"><div class="sect3" id="id181">
<h3>GELU, Swish, SwiGLU, Mish, and RELU<sup>2</sup></h3>

<p>The <em>Gaussian Error Linear Unit</em> (<em>GELU</em>)<a data-type="indexterm" data-primary="activation functions" data-secondary="GELU" id="xi_activationfunctionsGELU1120442_1"/><a data-type="indexterm" data-primary="GELU" id="xi_GELU1120442_1"/> was introduced in a <a href="https://homl.info/gelu">2016 paper</a> by Dan Hendrycks and Kevin Gimpel.<sup><a data-type="noteref" id="id2505-marker" href="ch11.html#id2505">10</a></sup> Once again, you can think of it as a smooth variant of the ReLU activation function. Its definition is given in <a data-type="xref" href="#gelu_activation_equation">Equation 11-3</a>, where Φ is the standard Gaussian cumulative distribution function (CDF): Φ(<em>z</em>) corresponds to the probability that a value sampled randomly from a normal distribution of mean 0 and variance 1 is lower than <em>z</em>.</p>
<div data-type="equation" id="gelu_activation_equation">
<h5><span class="label">Equation 11-3. </span>GELU activation function</h5>
<math display="block"><mrow><mi>GELU</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mi>z</mi><mo> </mo><mi mathvariant="normal">Φ</mi><mo>(</mo><mi>z</mi><mo>)</mo></math>
</div>

<p>As you can see in <a data-type="xref" href="#gelu_swish_mish_plot">Figure 11-4</a>, GELU resembles ReLU: it approaches 0 when its input <em>z</em> is very negative, and it approaches <em>z</em> when <em>z</em> is very positive. However, whereas all the activation functions we’ve discussed so far were both convex and monotonic,<sup><a data-type="noteref" id="id2506-marker" href="ch11.html#id2506">11</a></sup> the GELU activation function is neither: from left to right, it starts by going straight, then it wiggles down, reaches a low point around –0.17 (near z ≈ –0.75), and finally bounces up and ends up going straight toward the top right. This fairly complex shape and the fact that it has a curvature at every point may explain why it works so well, especially for complex tasks: gradient descent may find it easier to fit complex patterns. In practice, it often outperforms every other activation function discussed so far. However, it is a bit more computationally intensive, and the performance boost it provides is not always sufficient to justify the extra cost. That said, it is possible to show that it is approximately equal to <em>z</em>σ(1.702 <em>z</em>), where σ is the sigmoid function: using this approximation also works very well, and it has the advantage of being much faster to compute.</p>

<figure class="width-80"><div id="gelu_swish_mish_plot" class="figure">
<img src="assets/hmls_1104.png" alt="Plot comparing the GELU, Swish, parametrized Swish, Mish, and ReLU² activation functions, illustrating their differences in behavior across input values." width="1381" height="753"/>
<h6><span class="label">Figure 11-4. </span>GELU, Swish, parametrized Swish, Mish, and ReLU<sup>2</sup> activation functions</h6>
</div></figure>

<p>The GELU paper also introduced the <em>sigmoid linear unit</em> (SiLU)<a data-type="indexterm" data-primary="activation functions" data-secondary="SiLU" id="id2507"/><a data-type="indexterm" data-primary="SiLU activation function" id="id2508"/> activation function, which is equal to <em>z</em>σ(<em>z</em>), but it was outperformed by GELU in the authors’ tests. Interestingly, a <a href="https://homl.info/swish">2017 paper</a> by Prajit Ramachandran et al.<sup><a data-type="noteref" id="id2509-marker" href="ch11.html#id2509">12</a></sup> rediscovered the SiLU function by automatically searching for good activation functions. The authors named it <em>Swish</em>,<a data-type="indexterm" data-primary="activation functions" data-secondary="Swish" id="id2510"/><a data-type="indexterm" data-primary="Swish activation function" id="id2511"/> and the name caught on. In their paper, Swish outperformed every other function, including GELU. Ramachandran et al. later generalized Swish by adding an extra scalar hyperparameter<a data-type="indexterm" data-primary="hyperparameters" data-secondary="β (beta) scale" data-secondary-sortas="aab" id="id2512"/><a data-type="indexterm" data-primary="β (beta) hyperparameter" id="id2513"/><a data-type="indexterm" data-primary="beta (β) hyperparameter" id="id2514"/> <em>β</em> to scale the sigmoid function’s input. The generalized Swish function is Swish<sub><em>β</em></sub>(<em>z</em>) = <em>z</em>σ(<em>βz</em>), so GELU is approximately equal to the generalized Swish function using <em>β</em> = 1.702. You can tune <em>β</em> like any other hyperparameter. Alternatively, it’s also possible to make <em>β</em> trainable and let gradient descent optimize it (a bit like PReLU): there is typically a single trainable <em>β</em> parameter for the whole model, or just one per layer, to keep the model efficient and avoid overfitting.<a data-type="indexterm" data-startref="xi_activationfunctionsGELU1120442_1" id="id2515"/><a data-type="indexterm" data-startref="xi_GELU1120442_1" id="id2516"/></p>

<p>A popular Swish variant is <a href="https://homl.info/swiglu"><em>SwiGLU</em></a><a data-type="indexterm" data-primary="activation functions" data-secondary="SwiGLU" id="id2517"/><a data-type="indexterm" data-primary="SwiGLU activation function" id="id2518"/>:⁠<sup><a data-type="noteref" id="id2519-marker" href="ch11.html#id2519">13</a></sup> the inputs go through the Swish activation function, and in parallel through a linear layer, then both outputs are multiplied itemwise. That’s SwiGLU(<strong>z</strong>) = Swish<sub><em>β</em></sub>(<strong>z</strong>) ⊗ Linear(<strong>z</strong>). This is often implemented by doubling the output dimensions of the previous linear layer, then splitting the outputs in two along the feature dimension to get <strong>z</strong><sub>1</sub> and <strong>z</strong><sub>2</sub>, and finally applying: SwiGLU<sub><em>β</em></sub>(<strong>z</strong>) = Swish<sub><em>β</em></sub>(<strong>z</strong><sub>1</sub>) ⊗ <strong>z</strong><sub>2</sub>. This is a variant of the <a href="https://homl.info/glu"><em>gated linear unit</em> (GLU)</a>⁠<sup><a data-type="noteref" id="id2520-marker" href="ch11.html#id2520">14</a></sup> introduced by Facebook researchers in 2016. The itemwise multiplication gives the model more expressive power, allowing it to learn when to turn off (i.e., multiply by 0) or amplify specific features: this is called a <em>gating mechanism</em>.<a data-type="indexterm" data-primary="gating mechanisms" id="id2521"/> SwiGLU is very common in modern transformers (see <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>).</p>

<p>Another GELU-like activation function is <em>Mish</em><a data-type="indexterm" data-primary="Mish activation function" id="id2522"/>,<a data-type="indexterm" data-primary="activation functions" data-secondary="Mish" id="id2523"/> which was introduced in a <a href="https://homl.info/mish">2019 paper</a> by Diganta Misra.<sup><a data-type="noteref" id="id2524-marker" href="ch11.html#id2524">15</a></sup> It is defined as mish(<em>z</em>) = <em>z</em>tanh(softplus(<em>z</em>)), where softplus(<em>z</em>) = log(1 + exp(<em>z</em>)). Just like GELU and Swish, it is a smooth, nonconvex, and nonmonotonic variant of ReLU, and once again the author ran many experiments and found that Mish generally outperformed other activation functions—even Swish and GELU, by a tiny margin. <a data-type="xref" href="#gelu_swish_mish_plot">Figure 11-4</a> shows GELU<a data-type="indexterm" data-primary="activation functions" data-secondary="GELU" id="id2525"/><a data-type="indexterm" data-primary="GELU" id="id2526"/>, Swish<a data-type="indexterm" data-primary="activation functions" data-secondary="Swish" id="id2527"/><a data-type="indexterm" data-primary="Swish activation function" id="id2528"/> (both with the default <em>β</em> = 1 and with <em>β</em> = 0.6), and lastly Mish. As you can see, Mish overlaps almost perfectly with Swish when <em>z</em> is negative, and almost perfectly with GELU when <em>z</em> is positive.</p>

<p>Lastly, in 2021, Google researchers ran an automated architecture search to improve large transformers, and the search found a very simple yet effective activation function: <a href="https://homl.info/relu2">ReLU<sup>2</sup></a><a data-type="indexterm" data-primary="activation functions" data-secondary="ReLU²" id="id2529"/><a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="ReLU²" id="id2530"/><a data-type="indexterm" data-primary="ReLU² activation function" id="id2531"/>.⁠<sup><a data-type="noteref" id="id2532-marker" href="ch11.html#id2532">16</a></sup> As its name suggests, it’s simply ReLU squared: ReLU<sup>2</sup>(<em>z</em>) = (max(0, <em>z</em>))^2. It has all the qualities of ReLU (simplicity, computational efficiency, sparse output, no saturation on the positive side) but it also has smooth gradients at <em>z</em> = 0, and it often outperforms other activation functions, especially for sparse models. However, training can be less stable, in part because of its increased sensitivity to outliers and dying ReLUs.</p>
<div data-type="tip"><h6>Tip</h6>
<p>So, which activation function should you use for the hidden layers of your deep neural networks? ReLU<a data-type="indexterm" data-primary="activation functions" data-secondary="ReLU" id="id2533"/><a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="as default for simple tasks" data-secondary-sortas="default" id="id2534"/> remains a good default for most tasks: it’s often just as good as the more sophisticated activation functions, plus it’s very fast to compute, and many libraries and hardware accelerators provide ReLU-specific optimizations. However, Swish is probably a better default for complex tasks, and you can even try parametrized Swish with a learnable <em>β</em> parameter for the most complex tasks. Mish and SwiGLU<a data-type="indexterm" data-primary="activation functions" data-secondary="SwiGLU" id="id2535"/><a data-type="indexterm" data-primary="SwiGLU activation function" id="id2536"/> may give you slightly better results, but they require a bit more compute. If you care a lot about runtime latency, then you may prefer leaky<a data-type="indexterm" data-primary="activation functions" data-secondary="leaky ReLU" id="id2537"/><a data-type="indexterm" data-primary="activation functions" data-secondary="PReLU" id="id2538"/><a data-type="indexterm" data-primary="leaky ReLU" id="id2539"/><a data-type="indexterm" data-primary="PReLU (parametric leaky ReLU)" id="id2540"/><a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="leaky ReLU" id="id2541"/><a data-type="indexterm" data-primary="parametric leaky ReLU (PReLU)" id="id2542"/> ReLU, or parametrized leaky ReLU for complex tasks, or even ReLU<sup>2</sup>, especially for sparse models.</p>
</div>

<p>PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="activation function support" id="id2543"/> supports GELU, Mish, and Swish out of the box (using <code translate="no">nn.GELU</code>, <code translate="no">nn.Mish</code>, and <code translate="no">nn.SiLU</code>, respectively). To implement SwiGLU, double the previous linear layer’s output dimension, then use <code translate="no">z1, z2 = z.chunk(2, dim=-1)</code> to split its output in two, and compute <code translate="no">F.silu(beta * z1) * z2</code> (where <code translate="no">F</code> is <code translate="no">torch.nn.functional</code>). For ReLU<sup>2</sup><a data-type="indexterm" data-primary="activation functions" data-secondary="ReLU²" id="id2544"/><a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="ReLU²" id="id2545"/><a data-type="indexterm" data-primary="ReLU² activation function" id="id2546"/>, simply compute <code translate="no">F.relu(z).square()</code>. PyTorch also includes simplified and approximated versions of several activation functions, which are much faster to compute and often more stable during training. These simplified versions have names starting with “Hard”, such as <code translate="no">nn.Hardsigmoid</code>, <code translate="no">nn.Hardtanh</code>, and <code translate="no">nn.Hardswish</code>, and they are often used on mobile devices.</p>

<p>That’s all for activation functions! Now, let’s look at a completely different way to solve the unstable gradients problem: batch normalization.<a data-type="indexterm" data-startref="xi_vanishingandexplodinggradientsactivationfunctionimprovements11944_1" id="id2547"/></p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Batch Normalization"><div class="sect2" id="id182">
<h2>Batch Normalization</h2>

<p>Although<a data-type="indexterm" data-primary="batch normalization (BN)" id="xi_batchnormalizationBN112359_1"/><a data-type="indexterm" data-primary="BN (batch normalization)" id="xi_BNbatchnormalization112359_1"/><a data-type="indexterm" data-primary="normalization" id="xi_normalization112359_1"/><a data-type="indexterm" data-primary="vanishing and exploding gradients" data-secondary="batch normalization" id="xi_vanishingandexplodinggradientsbatchnormalization112359_1"/> using Kaiming initialization along with ReLU (or any of its variants) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.</p>

<p>In a <a href="https://homl.info/51">2015 paper</a>,⁠<sup><a data-type="noteref" id="id2548-marker" href="ch11.html#id2548">17</a></sup> Sergey Ioffe and Christian Szegedy proposed a technique called <em>batch normalization</em> (BN) that addresses these problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (no need for <code translate="no">StandardScaler</code>); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).</p>

<p>In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “batch normalization”). The whole operation is summarized step by step in <a data-type="xref" href="#batch_normalization_algorithm">Equation 11-4</a>.</p>
<div id="batch_normalization_algorithm" data-type="equation" class="less_space pagebreak-before"><h5><span class="label">Equation 11-4. </span>Batch normalization algorithm</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msub><mi mathvariant="bold">μ</mi> <mi>B</mi> </msub>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi> </msub></mfrac>
          </mstyle>
            <mspace width="0.25em"/>
          <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi> <mi>B</mi> </msub> </munderover>
          <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msup><mrow><msub><mi mathvariant="bold">σ</mi> <mi>B</mi> </msub></mrow> <mn>2</mn> </msup>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi> </msub></mfrac>
          </mstyle>
            <mspace width="0.25em"/>
          <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi> <mi>B</mi> </msub> </munderover>
          <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><msub><mi mathvariant="bold">μ</mi> <mi>B</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>3</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msup><mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><msub><mi mathvariant="bold">μ</mi> <mi>B</mi> </msub></mrow> <msqrt><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi> <mi>B</mi> </msub></mrow> <mn>2</mn> </msup><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac>
          </mstyle>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>4</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msup><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>=</mo>
          <mi mathvariant="bold">γ</mi>
          <mo>⊗</mo>
          <msup><mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>+</mo>
          <mi mathvariant="bold">β</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></div>

<p>In this algorithm:</p>

<ul>
<li>
<p><strong>μ</strong><sub><em>B</em></sub> is the vector of input means, evaluated over the whole mini-batch <em>B</em> (it contains one mean per input).</p>
</li>
<li>
<p><em>m</em><sub><em>B</em></sub> is the number of instances in the mini-batch.</p>
</li>
<li>
<p><strong>x</strong><sup>(<em>i</em>)</sup> is the input vector of the batch-norm layer for instance <em>i</em>.</p>
</li>
<li>
<p><strong>σ</strong><sub><em>B</em></sub> is the vector of input standard deviations, also evaluated over the whole mini-batch (it contains one standard deviation per input).</p>
</li>
<li>
<p><math>
  <mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover>
</math><sup>(<em>i</em>)</sup> is the vector of zero-centered and normalized inputs for instance <em>i</em>.</p>
</li>
<li>
<p><em>ε</em> is a tiny number that avoids division by zero and ensures the gradients don’t grow too large (typically 10<sup>–5</sup>). This is called a <em>smoothing term</em>.<a data-type="indexterm" data-primary="smoothing terms" id="id2549"/></p>
</li>
<li>
<p><strong>γ</strong> is the output scale parameter vector for the layer (it contains one scale parameter per input).</p>
</li>
<li>
<p>⊗ represents element-wise multiplication (each input is multiplied by its corresponding output scale parameter).</p>
</li>
<li>
<p><strong>β</strong> is the output shift (offset) parameter vector for the layer (it contains one shift parameter per input). Each input is offset by its corresponding shift parameter.</p>
</li>
<li>
<p><strong>z</strong><sup>(<em>i</em>)</sup> is the output of the BN operation. It is a rescaled and shifted version of the inputs.</p>
</li>
</ul>

<p>So during training, BN standardizes its inputs, then rescales and offsets them. Good! What about at test time? Well, it’s not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable. One solution is to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations can then be used instead of the batch input means and standard deviations when making predictions.</p>

<p>However, most implementations of batch norm estimate these final statistics during training by using a moving average of the layer’s batch input means and variances. This is what PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="batch norm" id="id2550"/> does automatically when you use its batch-norm layers, such as <code translate="no">nn.BatchNorm1d</code> (which we will discuss in the next section). To sum up, four parameter vectors are learned in each batch-norm layer: <strong>γ</strong> (the output scale vector) and <strong>β</strong> (the output offset vector) are learned through regular backpropagation, and <strong>μ</strong> (the final input mean vector) and <strong>σ</strong><sup>2</sup> (the final input variance vector) are estimated using an exponential moving average. Note that <strong>μ</strong> and <strong>σ</strong><sup>2</sup> are estimated during training, but they are used only after training, once you switch the model to evaluation mode using <code translate="no">model.eval()</code>: <strong>μ</strong> and <strong>σ</strong><sup>2</sup> then replace <strong>μ</strong><sub><em>B</em></sub> and <strong>σ</strong><sub><em>B</em></sub><sup>2</sup> in <a data-type="xref" href="#batch_normalization_algorithm">Equation 11-4</a>.</p>

<p>Ioffe and Szegedy demonstrated that batch norm considerably improved all the deep neural networks they experimented with, leading to a huge improvement in the ImageNet classification task<a data-type="indexterm" data-primary="ImageNet classification task" id="id2551"/> (ImageNet is a large database of images classified into many classes, commonly used to evaluate computer vision systems). The vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions such as tanh and even sigmoid. The networks were also much less sensitive to the weight initialization. The authors were able to use much larger learning rates, significantly speeding up the learning process. Specifically, they note that:</p>
<blockquote>
<p>Applied to a state-of-the-art image classification model, batch norm achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. […​] Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.</p></blockquote>

<p>Finally, like a gift that keeps on giving, batch norm acts like a regularizer, reducing the need for other regularization techniques (such as dropout, described later in this chapter).</p>

<p>Batch normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as discussed earlier). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it’s often possible to fuse the BN layer with the previous layer after training, thereby avoiding the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. For example, if the previous layer computes <strong>XW</strong> + <strong>b</strong>, then the BN layer will compute <strong>γ</strong> ⊗ (<strong>XW</strong> + <strong>b</strong> – <strong>μ</strong>) / <strong>σ</strong> + <strong>β</strong> (ignoring the smoothing term <em>ε</em> in the denominator). If we define <strong>W</strong>′ = <strong>γ</strong>⊗<strong>W</strong> / <strong>σ</strong> and <strong>b</strong>′ = <strong>γ</strong> ⊗ (<strong>b</strong> – <strong>μ</strong>) / <strong>σ</strong> + <strong>β</strong>, the equation simplifies to <strong>XW</strong>′ + <strong>b</strong>′. So, if we replace the previous layer’s weights and biases (<strong>W</strong> and <strong>b</strong>) with the updated weights and biases (<strong>W</strong>′ and <strong>b</strong>′), we can get rid of the BN layer. This is one of the optimizations performed by <code translate="no">optimize_for_inference()</code> (see <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You may find that training is rather slow, because each epoch takes much more time when you use batch norm. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same performance. All in all, <em>wall time</em><a data-type="indexterm" data-primary="wall time" id="id2552"/> will usually be shorter (this is the time measured by the clock on your wall).</p>
</div>










<section data-type="sect3" data-pdf-bookmark="Implementing batch norm with PyTorch"><div class="sect3" id="id183">
<h3>Implementing batch norm with PyTorch</h3>

<p>As<a data-type="indexterm" data-primary="PyTorch" data-secondary="batch norm" id="xi_PyTorchbatchnorm113623_1"/> with most things with PyTorch, implementing batch norm is straightforward and intuitive. Just add an <code translate="no">nn.BatchNorm1d</code> layer before or after each hidden layer’s activation function, and specify the number of inputs of each BN layer. You may also add a BN layer as the first layer in your model, which removes the need to standardize the inputs manually. For example, let’s create a Fashion MNIST image classifier (similar to the one we built in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>) using BN as the first layer in the model (after flattening the input images), then again after each hidden layer:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm1d</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">300</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm1d</code><code class="p">(</code><code class="mi">300</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm1d</code><code class="p">(</code><code class="mi">100</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="p">)</code></pre>

<p>You can now train the model normally (as you learned in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>), and that’s it! In this tiny example with just two hidden layers, batch norm is unlikely to have a large impact, but for deeper networks it can make a tremendous difference.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Since batch norm behaves differently during training and during evaluation, it’s critical to switch to training mode during training (using <code translate="no">model.train()</code>), and switch to evaluation mode during evaluation (using <code translate="no">model.eval()</code>). Forgetting to do so is one of the most common mistakes.</p>
</div>

<p>If you look at the parameters of the first BN layer, you will find two: <code translate="no">weight</code> and <code translate="no">bias</code>, which correspond to <strong>γ</strong> and <strong>β</strong> in <a data-type="xref" href="#batch_normalization_algorithm">Equation 11-4</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">dict</code><code class="p">(</code><code class="n">model</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">named_parameters</code><code class="p">())</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code><code class="w"/>
<code class="go">dict_keys(['weight', 'bias'])</code></pre>

<p>And if you look at the buffers of this same BN layer, you will find three: <code>run⁠ning_​mean</code>, <code translate="no">running_var</code>, and <code translate="no">num_batches_tracked</code>. The first two correspond to the running means <strong>μ</strong> and <strong>σ</strong><sup>2</sup> discussed earlier, and <code translate="no">num_batches_tracked</code> simply counts the number of batches seen during training:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">dict</code><code class="p">(</code><code class="n">model</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">named_buffers</code><code class="p">())</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code><code class="w"/>
<code class="go">dict_keys(['running_mean', 'running_var', 'num_batches_tracked'])</code></pre>

<p>The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after (as we just did). There is some debate about this, and it seems to depend on the task, so you can experiment with this to see which option works best on your dataset. If you move the BN layers before the activation functions, you can also remove the bias term from the previous <code translate="no">nn.Linear</code> layers by setting their <code translate="no">bias</code> hyperparameter to <code translate="no">False</code>. Indeed, a batch-norm layer already includes one bias term per input. You can also drop the first BN layer to avoid sandwiching the first hidden layer between two BN layers, but this means you should normalize the training set before training. The updated code looks like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">300</code><code class="p">,</code> <code class="n">bias</code><code class="o">=</code><code class="kc">False</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm1d</code><code class="p">(</code><code class="mi">300</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="n">bias</code><code class="o">=</code><code class="kc">False</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm1d</code><code class="p">(</code><code class="mi">100</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="p">)</code></pre>

<p>The <code translate="no">nn.BatchNorm1d</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.BatchNorm1d" id="xi_torchnnBatchNorm1d1141521_1"/> class has a few hyperparameters you can tweak. The defaults will usually be fine, but you may occasionally need to tweak the <code translate="no">momentum</code>.<a data-type="indexterm" data-primary="hyperparameters" data-secondary="momentum" id="xi_hyperparametersmomentum11415158_1"/><a data-type="indexterm" data-primary="momentum hyperparameter" id="xi_momentumhyperparameter11415158_1"/> This hyperparameter is used by the <code translate="no">BatchNorm1d</code> layer when it updates the exponential moving averages; given a new value <strong>v</strong> (i.e., a new vector of input means or variances computed over the current batch), the layer updates the running average <math><mover><mi mathvariant="bold">v</mi><mo>^</mo></mover></math> using the following equation:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover>
    <mo>←</mo>
    <mi mathvariant="bold">v</mi>
    <mo>×</mo>
    <mtext>momentum</mtext>
    <mo>+</mo>
    <mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover>
    <mo>×</mo>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>-</mo>
      <mtext>momentum</mtext>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>A good momentum value is typically close to 0; for example, 0.01 or 0.001. You want more 0s for smaller mini-batches, and fewer for larger mini-batches. The default is 0.1, which is good for large batch sizes, but not great for small batch sizes such as 32 or 64.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>When people talk about “momentum” in the context of a running mean, they usually refer to the weight of the current running mean in the update equation. Sadly, for historical reasons, PyTorch uses the opposite meaning in the BN layers. However, other parts of PyTorch use the conventional meaning (e.g., in optimizers), so don’t get confused.<a data-type="indexterm" data-startref="xi_hyperparametersmomentum11415158_1" id="id2553"/><a data-type="indexterm" data-startref="xi_momentumhyperparameter11415158_1" id="id2554"/></p>
</div>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Batch norm 1D, 2D, and 3D"><div class="sect3" id="id184">
<h3>Batch norm 1D, 2D, and 3D</h3>

<p>In the previous examples, we flattened the input images before sending them through the first <code translate="no">nn.BatchNorm1d</code> layer. This is because an <code translate="no">nn.BatchNorm1d</code> layer works on batches of shape <code translate="no">[batch_size, num_features]</code> (just like the <code translate="no">nn.Linear</code> layer does), so you would get an error if you moved it before the <code translate="no">nn.Flatten</code> layer.</p>

<p>However, you could use an <code translate="no">nn.BatchNorm2d</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.BatchNorm2d" id="id2555"/> layer before the <code translate="no">nn.Flatten</code> layer: indeed, it expects its inputs to be image batches of shape <code translate="no">[batch_size, channels, height, width]</code>, and it computes the batch mean and variance across both the batch dimension (dimension 0) and the spatial dimensions (dimensions 2 and 3). This means that all pixels in the same batch and channel get normalized using the same mean and variance: the <code translate="no">nn.BatchNorm2d</code> layer only has one weight per channel and one bias per channel (e.g., three weights and three bias terms for color images with three channels for red, green, and blue). This generally works better when dealing with image datasets.</p>

<p>There’s also an <code translate="no">nn.BatchNorm3d</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.BatchNorm3d" id="id2556"/> layer which expects batches of shape <code translate="no">[batch_size, channels, depth, height, width]</code>: this is useful for datasets of 3D images, such as CT scans.</p>

<p>The <code translate="no">nn.BatchNorm1d</code><a data-type="indexterm" data-startref="xi_torchnnBatchNorm1d1141521_1" id="id2557"/> layer can also work on batches of sequences. The convention in PyTorch is to represent batches of sequences as 3D tensors of shape <code translate="no">[batch_size, sequence_length, num_features]</code>. For example, suppose you work on particle physics and you have a dataset of particle trajectories, where each trajectory is composed of a sequence of 100 points in 3D space, then a batch of 32 trajectories will have a shape of <code translate="no">[32, 100, 3]</code>. However, the <code translate="no">nn.BatchNorm1d</code> layer expects the shape to be <code translate="no">[batch_size, num_features, sequence_length]</code>, and it computes the batch mean and variance across the first and last dimensions to get one mean and variance per feature. So you must permute the last two dimensions of the data using <code translate="no">X.permute(0, 2, 1)</code> before letting it go through the <code translate="no">nn.BatchNorm1d</code> layer. We will discuss sequences further in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>.</p>

<p>Batch normalization has become one of the most-used layers in deep neural networks, especially deep convolutional neural networks discussed in <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>, to the point that it is often omitted in the architecture diagrams: it is assumed that BN is added after every layer. That said, it is not perfect. In particular, the computed statistics for an instance are biased by the other samples in a batch, which may reduce performance (especially for small batch sizes). Moreover, BN struggles with some architectures, such as recurrent nets, as we will see in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>. For these reasons, batch-norm is more and more often replaced by layer-norm.<a data-type="indexterm" data-startref="xi_batchnormalizationBN112359_1" id="id2558"/><a data-type="indexterm" data-startref="xi_BNbatchnormalization112359_1" id="id2559"/><a data-type="indexterm" data-startref="xi_PyTorchbatchnorm113623_1" id="id2560"/><a data-type="indexterm" data-startref="xi_vanishingandexplodinggradientsbatchnormalization112359_1" id="id2561"/></p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Layer Normalization"><div class="sect2" id="id185">
<h2>Layer Normalization</h2>

<p>Layer normalization (LN)<a data-type="indexterm" data-primary="layer normalization (LN)" id="xi_layernormalizationLN1145725_1"/><a data-type="indexterm" data-primary="LN (layer normalization)" id="xi_LNlayernormalization1145725_1"/><a data-type="indexterm" data-primary="vanishing and exploding gradients" data-secondary="layer normalization" id="xi_vanishingandexplodinggradientslayernormalization1145725_1"/> is very similar to batch norm, but instead of normalizing across the batch dimension, LN normalizes across the feature dimensions. This simple idea was introduced by Jimmy Lei Ba et al. in a <a href="https://homl.info/layernorm">2016 paper</a>,⁠<sup><a data-type="noteref" id="id2562-marker" href="ch11.html#id2562">18</a></sup> and initially applied mostly to recurrent nets. However, in recent years it has been successfully applied to many other architectures, such as convolutional nets, transformers, diffusion nets, and more.</p>

<p>One advantage is that LN can compute the required statistics on the fly, at each time step, independently for each instance. This also means that it behaves the same way during training and testing (as opposed to BN), and it does not need to use exponential <span class="keep-together">moving</span> averages to estimate the feature statistics across all instances in the training set, like BN does. Lastly, LN learns a scale and an offset parameter for each input feature, just like BN does.</p>

<p>PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="layer normalization" id="id2563"/> includes an <code translate="no">nn.LayerNorm</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.LayerNorm" id="id2564"/> module. To create an instance, you must simply indicate the size of the dimensions that you want to normalize over. These must be the last dimension(s) of the inputs. For example, if the inputs are batches of 100 × 200 RGB images of shape <code translate="no">[3, 100, 200]</code>, and you want to normalize each image over each of the three color channels separately, you would use the following <code translate="no">nn.LayerNorm</code> module:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">inputs</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">200</code><code class="p">)</code>  <code class="c1"># a batch of random RGB images</code>
<code class="n">layer_norm</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LayerNorm</code><code class="p">([</code><code class="mi">100</code><code class="p">,</code> <code class="mi">200</code><code class="p">])</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">layer_norm</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>  <code class="c1"># normalizes over the last two dimensions</code></pre>

<p>The following code produces the same result:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">means</code> <code class="o">=</code> <code class="n">inputs</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>  <code class="c1"># shape: [32, 3, 1, 1]</code>
<code class="n">vars_</code> <code class="o">=</code> <code class="n">inputs</code><code class="o">.</code><code class="n">var</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">unbiased</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>  <code class="c1"># shape: same</code>
<code class="n">stds</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">vars_</code> <code class="o">+</code> <code class="n">layer_norm</code><code class="o">.</code><code class="n">eps</code><code class="p">)</code>  <code class="c1"># eps is a smoothing term (1e-5)</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">layer_norm</code><code class="o">.</code><code class="n">weight</code> <code class="o">*</code> <code class="p">(</code><code class="n">inputs</code> <code class="o">-</code> <code class="n">means</code><code class="p">)</code> <code class="o">/</code> <code class="n">stds</code> <code class="o">+</code> <code class="n">layer_norm</code><code class="o">.</code><code class="n">bias</code>
<code class="c1"># result shape: [32, 3, 100, 200]</code></pre>

<p>However, most computer vision architectures that use LN normalize over all channels at once. For this, you must include the size of the channel dimension when creating the <code translate="no">nn.LayerNorm</code> module:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">layer_norm</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LayerNorm</code><code class="p">([</code><code class="mi">3</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">200</code><code class="p">])</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">layer_norm</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>  <code class="c1"># normalizes over the last three dimensions</code></pre>

<p>And that’s all there is to it! Now let’s look at one last technique to stabilize gradients during training: gradient clipping.<a data-type="indexterm" data-startref="xi_layernormalizationLN1145725_1" id="id2565"/><a data-type="indexterm" data-startref="xi_LNlayernormalization1145725_1" id="id2566"/><a data-type="indexterm" data-startref="xi_normalization112359_1" id="id2567"/><a data-type="indexterm" data-startref="xi_vanishingandexplodinggradientslayernormalization1145725_1" id="id2568"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Gradient Clipping"><div class="sect2" id="id186">
<h2>Gradient Clipping</h2>

<p>Another<a data-type="indexterm" data-primary="gradient clipping" id="id2569"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="gradient clipping" id="id2570"/><a data-type="indexterm" data-primary="vanishing and exploding gradients" data-secondary="gradient clipping" id="id2571"/> technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This is called <a href="https://homl.info/52"><em>gradient clipping</em></a>.⁠<sup><a data-type="noteref" id="id2572-marker" href="ch11.html#id2572">19</a></sup> This technique is generally used in recurrent neural networks, where using batch norm is tricky (as you will see in <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>).</p>

<p>In PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="gradient clipping" id="id2573"/>, gradient clipping is generally implemented by calling either <code>torch.​nn.utils.clip_grad_norm_()</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.utils.clip_grad_norm_()" id="id2574"/> or <code translate="no">torch.nn.utils.clip_grad_value_()</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.utils.clip_grad_value_()" id="id2575"/> at each iteration during training, right after the gradients are computed (i.e., after <code translate="no">loss.backward()</code>). Both functions take as a first argument the list of model parameters whose gradients must be clipped—typically all of them (<code translate="no">model.parameters()</code>). The <code translate="no">clip_grad_norm_()</code> function clips each gradient vector’s norm if it exceeds the given <code translate="no">max_norm</code> argument. This is a hyperparameter you can tune (a typical default value is 1.0). The <code translate="no">clip_grad_value_()</code> function independently clips the individual components of the gradient vector between <code translate="no">-clip_value</code> and <code translate="no">+clip_value</code>, where <code translate="no">clip_value</code> is a hyperparameter you can tune. For example, this training loop clips the norm of each gradient vector to 1.0:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
        <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="o">=</code> <code class="n">X_batch</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="n">y_batch</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
        <code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X_batch</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">y_pred</code><code class="p">,</code> <code class="n">y_batch</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">nn</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">clip_grad_norm_</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">max_norm</code><code class="o">=</code><code class="mf">1.0</code><code class="p">)</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code></pre>

<p>Note that <code translate="no">clip_grad_value_()</code> will change the orientation of the gradient vector when its components are clipped. For instance, if the original gradient vector is <code translate="no">[0.9, 100.0]</code>, it points mostly in the direction of the second dimension; but once you clip it by value, you get <code translate="no">[0.9, 1.0]</code>, which points roughly at the diagonal between the two axes. Despite this reorientation, this approach actually works quite well in practice. If you clipped the same vector by norm, the result would be <code translate="no">[0.00899964, 0.9999595]</code>: this would preserve the vector’s orientation, but almost eliminate the first component. The best clipping function to use depends on the dataset.<a data-type="indexterm" data-startref="xi_vanishingandexplodinggradients11153_1" id="id2576"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Reusing Pretrained Layers"><div class="sect1" id="id187">
<h1>Reusing Pretrained Layers</h1>

<p>It<a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="reusing pretrained layers" id="xi_deepneuralnetworksDNNsreusingpretrainedlayers115123_1"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="reusing layers" id="xi_pretrainingandpretrainedlayersreusinglayers115123_1"/> is generally not a good idea to train a very large DNN from scratch without first trying to find an existing neural network that accomplishes a similar task to the one you are trying to tackle (I will discuss how to find them in <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>). If you find such a neural network, then you can generally reuse most of its layers, except for the top ones. This technique is called <em>transfer learning</em>.<a data-type="indexterm" data-primary="transfer learning" id="id2577"/> It will not only speed up training considerably, but also require significantly less training data.</p>

<p>Suppose you have access to a DNN that was trained to classify pictures into one hundred different categories, including animals, plants, vehicles, and everyday objects, and you now want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network (see <a data-type="xref" href="#reuse_pretrained_diagram">Figure 11-5</a>).</p>

<figure><div id="reuse_pretrained_diagram" class="figure">
<img src="assets/hmls_1105.png" alt="Diagram illustrating the reuse of pretrained layers from an existing deep neural network to train a new model for a similar task, with some weights fixed and others trainable." width="996" height="824"/>
<h6><span class="label">Figure 11-5. </span>Reusing pretrained layers</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If the input pictures for your new task don’t have the same size as the ones used in the original task, you will usually have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will work best when the inputs have similar low-level features. For example, a neural net trained on regular pictures taken from mobile phones will help with many other tasks on mobile phone pictures, but it will likely not help at all on satellite images or medical images.</p>
</div>

<p>The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs.</p>

<p>Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The more similar the tasks are, the more layers you will want to reuse (starting with the lower layers). For very similar tasks, try to keep all the hidden layers and just replace the output layer.</p>
</div>

<p>Try freezing all the reused layers first (i.e., make their parameters nontrainable by setting <code translate="no">requires_grad</code> to <code translate="no">False</code> so that gradient descent won’t modify them and they will remain fixed), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.<a data-type="indexterm" data-primary="weights" data-secondary="freezing reused layers" id="id2578"/></p>

<p>If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers.</p>








<section data-type="sect2" data-pdf-bookmark="Transfer Learning with PyTorch"><div class="sect2" id="id188">
<h2>Transfer Learning with PyTorch</h2>

<p>Let’s<a data-type="indexterm" data-primary="PyTorch" data-secondary="transfer learning" id="xi_PyTorchtransferlearning115356_1"/><a data-type="indexterm" data-primary="transfer learning" id="xi_transferlearning115356_1"/> look at an example. Suppose the Fashion MNIST dataset only contained eight classes—for example, all the classes except for Pullover and T-shirt/top. Someone built and trained a PyTorch model on that set and got reasonably good performance (~92% accuracy). Let’s call this model A. You now want to tackle a different task: you have images of T-shirts and pullovers, and you want to train a binary classifier: positive for T-shirt/top, negative for Pullover. Your dataset is tiny; you only have 20 labeled images! When you train a new model for this task (let’s call it model B) with the same architecture as model A, you get 71.6% test accuracy. While drinking your morning coffee, you realize that your task is quite similar to task A, so perhaps transfer learning can help? Let’s find out!</p>

<p class="pagebreak-before">First, let’s look at model A:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>

<code class="n">model_A</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code>
<code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># train this model or load pretrained weights</code></pre>

<p>We can now reuse the layers we want, for example, all layers except for the output layer:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">copy</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">reused_layers</code> <code class="o">=</code> <code class="n">copy</code><code class="o">.</code><code class="n">deepcopy</code><code class="p">(</code><code class="n">model_A</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code>
<code class="n">model_B_on_A</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="o">*</code><code class="n">reused_layers</code><code class="p">,</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># new output layer for task B</code>
<code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>In this code, we use Python’s <code translate="no">copy.deepcopy()</code> function to copy all the modules in the <code translate="no">nn.Sequential</code> module (along with all their data and submodules), except for the last layer. Since we’re making a deep copy<a data-type="indexterm" data-primary="deep copies" id="id2579"/>, all the submodules are copied as well. Then we create <code translate="no">model_B_on_A</code>, which is an <code translate="no">nn.Sequential</code> model based on the reused layers of model A, plus a new output layer for task B: it has a single output since task B is binary classification.</p>

<p>You could start training <code translate="no">model_B_on_A</code> for task B now, but since the new output layer was initialized randomly, it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="n">model_B_on_A</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]:</code>
    <code class="k">for</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">layer</code><code class="o">.</code><code class="n">parameters</code><code class="p">():</code>
        <code class="n">param</code><code class="o">.</code><code class="n">requires_grad</code> <code class="o">=</code> <code class="kc">False</code></pre>

<p>Now you can train <code translate="no">model_B_on_A</code>. But don’t forget that task B is binary classification, so you must switch the loss to <code translate="no">nn.BCEWithLogitsLoss</code> (or to <code translate="no">nn.BCELoss</code> if you prefer to add an <code translate="no">nn.Sigmoid</code> activation function on the output layer), as we discussed in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>. Also, if you are using <code translate="no">torchmetrics</code><a data-type="indexterm" data-primary="TorchMetrics library" id="id2580"/>, make sure to set <code translate="no">task="binary"</code> when creating the <code translate="no">Accuracy</code> metric:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">xentropy</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCEWithLogitsLoss</code><code class="p">()</code>
<code class="n">accuracy</code> <code class="o">=</code> <code class="n">torchmetrics</code><code class="o">.</code><code class="n">Accuracy</code><code class="p">(</code><code class="n">task</code><code class="o">=</code><code class="s2">"binary"</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># train model_B_on_A</code></pre>

<p>After you have trained the model for a few epochs, you can unfreeze the reused layers (setting <code translate="no">param.requires_grad = True</code> for all parameters), reduce the learning rate, and continue training to fine-tune the reused layers for task B.</p>

<p>So, what’s the final verdict? Well, this model’s test accuracy is 92.5%, which is much better than the 71.6% accuracy we reached without pretraining!</p>

<p>Are you convinced? Well, you shouldn’t be; I cheated! I tried many configurations until I found one that demonstrated a strong improvement. If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses. What I did is called “torturing the data<a data-type="indexterm" data-primary="torturing the data" id="id2581"/> until it confesses”. When a paper looks too positive, you should be suspicious. Perhaps the flashy new technique does not actually help much (in fact, it may even degrade performance), but the authors tried many variants and reported only the best results—which may be due to sheer luck—without mentioning how many failures they encountered along the way. That’s called <em>p-hacking</em>.<a data-type="indexterm" data-primary="p-hacking" id="id2582"/> Most of the time, this is not malicious, but it is part of the reason why so many results in science can never be reproduced.</p>

<p>But why did I cheat? It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful for other tasks. Transfer learning works best with deep convolutional neural networks and with Transformer architectures. We will revisit transfer learning in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch12.html#cnn_chapter">12</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch15.html#transformer_chapter">15</a>, using the techniques we just discussed (and this time it will work fine without cheating, I promise!).<a data-type="indexterm" data-startref="xi_PyTorchtransferlearning115356_1" id="id2583"/><a data-type="indexterm" data-startref="xi_transferlearning115356_1" id="id2584"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Unsupervised Pretraining"><div class="sect2" id="id189">
<h2>Unsupervised Pretraining</h2>

<p>Suppose<a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="in unsupervised learning" data-secondary-sortas="unsupervised learning" id="xi_pretrainingandpretrainedlayersinunsupervisedlearning115998_1"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="pretraining" id="xi_unsupervisedlearningpretraining115998_1"/> you want to tackle a complex task for which you don’t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don’t lose hope! First, you should try to gather more labeled training data, but if you can’t, you may still be able to perform <em>unsupervised pretraining</em> (see <a data-type="xref" href="#unsupervised_pretraining_diagram">Figure 11-6</a>). Indeed, it is often cheap to gather unlabeled training examples, but expensive to label them. If you can gather plenty of unlabeled training data, you can try to use it to train an unsupervised model<a data-type="indexterm" data-primary="greedy layer-wise pretraining" id="id2585"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="greedy layer-wise pretraining" id="id2586"/>, such as an autoencoder (see <a data-type="xref" href="ch18.html#autoencoders_chapter">Chapter 18</a>). Then you can reuse the lower layers of the autoencoder, add the output layer for your task on top, and fine-tune the final network using supervised learning (i.e., with the labeled training <span class="keep-together">examples</span>).</p>

<figure><div id="unsupervised_pretraining_diagram" class="figure">
<img src="assets/hmls_1106.png" alt="Diagram illustrating greedy layer-wise pretraining in early deep learning, showing the sequential training of layers with unlabeled and labeled data." width="1165" height="841"/>
<h6><span class="label">Figure 11-6. </span>Greedy layer-wise pretraining used in the early days of deep learning; nowadays the unsupervised part is typically done in one shot on all the data rather than one layer at a time</h6>
</div></figure>

<p>It is this technique that Geoffrey Hinton and his team used in 2006, and which led to the revival of neural networks and the success of deep learning. Until 2010, unsupervised pretraining—typically with restricted Boltzmann machines (RBMs; see the notebook at <a href="https://homl.info/extra-anns" class="bare"><em class="hyperlink">https://homl.info/extra-anns</em></a>)—was the norm for deep nets, and only after the vanishing gradients problem was alleviated did it become much more common to train DNNs purely using supervised learning. Unsupervised pretraining (today typically using autoencoders or diffusion models rather than RBMs) is still a good option when you have a complex task to solve, no similar model you can reuse, and little labeled training data, but plenty of unlabeled training data.</p>

<p>Note that in the early days of deep learning it was difficult to train deep models, so people would use a technique called <em>greedy layer-wise pretraining</em> (depicted in <a data-type="xref" href="#unsupervised_pretraining_diagram">Figure 11-6</a>). They would first train an unsupervised model with a single layer, typically an RBM, then they would freeze that layer and add another one on top of it, then train the model again (effectively just training the new layer), then freeze the new layer and add another layer on top of it, train the model again, and so on. Nowadays, things are much simpler: people generally train the full unsupervised model in one shot and use models such as autoencoders or diffusion models rather than RBMs.<a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersinunsupervisedlearning115998_1" id="id2587"/><a data-type="indexterm" data-startref="xi_unsupervisedlearningpretraining115998_1" id="id2588"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Pretraining on an Auxiliary Task"><div class="sect2" id="id190">
<h2>Pretraining on an Auxiliary Task</h2>

<p>If<a data-type="indexterm" data-primary="auxiliary task, pretraining on" id="id2589"/><a data-type="indexterm" data-primary="pretraining and pretrained layers" data-secondary="on auxiliary task" data-secondary-sortas="auxiliary" id="id2590"/> you do not have much labeled training data, one last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network’s lower layers will learn feature detectors that will likely be reusable by the second neural network.</p>

<p>For example, if you want to build a system to recognize faces, you may only have a few pictures of each individual—clearly not enough to train a good classifier. Gathering hundreds of pictures of each person would not be practical. You could, however, use a public dataset containing millions of pictures of people (such as VGGFace2) and train a first neural network to detect whether two different pictures feature the same person. Such a <span class="keep-together">network</span> would learn good feature detectors for faces, so reusing its lower layers would allow you to train a good face classifier that uses little training data.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>You could also just scrape pictures<a data-type="indexterm" data-primary="pictures, and copyright" id="id2591"/> of random people from the web, but this would probably be illegal. Firstly, photos<a data-type="indexterm" data-primary="photos, and copyright" id="id2592"/> are usually copyrighted by their creators, and websites like Instagram or Facebook enforce these copyright protections through their terms of service, which prohibit scraping and unauthorized use. Secondly, over 40 countries require explicit consent for collecting and processing personal data, including facial images<a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="copyright issue" id="id2593"/>.</p>
</div>

<p>For natural language processing (NLP) applications<a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="applications" id="id2594"/><a data-type="indexterm" data-primary="NLP" data-see="natural language processing" id="id2595"/>, you can download a corpus of millions of text documents and automatically generate labeled data from it. For example, you could randomly mask out some words and train a model to predict what the missing words are (e.g., it should predict that the missing word in the sentence “What ___ you saying?” is probably “are” or “were”). If you can train a model to reach good performance on this task, then it will already know quite a lot about language, and you can certainly reuse it for your actual task and fine-tune it on your labeled data (this is basically how large language models are trained and fine-tuned, as we will see in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><em>Self-supervised learning</em><a data-type="indexterm" data-primary="self-supervised learning" id="id2596"/> is when you automatically generate the labels from the data itself, as in the text-masking example, then you train a model on the resulting “labeled” dataset using supervised learning techniques.<a data-type="indexterm" data-startref="xi_deepneuralnetworksDNNsreusingpretrainedlayers115123_1" id="id2597"/><a data-type="indexterm" data-startref="xi_pretrainingandpretrainedlayersreusinglayers115123_1" id="id2598"/></p>
</div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Faster Optimizers"><div class="sect1" id="id191">
<h1>Faster Optimizers</h1>

<p>Training<a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="faster optimizers for" id="xi_deepneuralnetworksDNNsfasteroptimizersfor116239_1"/><a data-type="indexterm" data-primary="gradient descent (GD)" data-secondary="with optimizers" data-secondary-sortas="optimizers" id="xi_gradientdescentGDwithoptimizers116239_1"/><a data-type="indexterm" data-primary="optimizers" id="xi_optimizers116239_1"/> a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): applying a good initialization strategy for the connection weights, using a good activation function, using batch-norm or layer-norm, and reusing parts of a pretrained network (possibly built for an auxiliary task or using unsupervised learning). Another huge speed boost comes from using a faster optimizer than the regular gradient descent optimizer. In this section we will present the most popular optimization algorithms: momentum, Nesterov accelerated gradient, AdaGrad, RMSProp, and finally, Adam and its variants.</p>








<section data-type="sect2" data-pdf-bookmark="Momentum"><div class="sect2" id="id192">
<h2>Momentum</h2>

<p>Imagine<a data-type="indexterm" data-primary="gradient descent (GD)" data-secondary="versus momentum optimization" data-secondary-sortas="momentum" id="xi_gradientdescentGDversusmomentumoptimization116268_1"/><a data-type="indexterm" data-primary="momentum optimization" id="xi_momentumoptimization116268_1"/><a data-type="indexterm" data-primary="optimizers" data-secondary="momentum optimization" id="xi_optimizersmomentumoptimization116268_1"/> a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the core idea behind <em>momentum optimization</em>, <a href="https://homl.info/54">proposed by Boris Polyak in 1964</a>.⁠<sup><a data-type="noteref" id="id2599-marker" href="ch11.html#id2599">20</a></sup> In contrast, regular gradient descent will take small steps when the slope is gentle and big steps when the slope is steep, but it will never pick up speed. As a result, regular gradient descent is generally much slower to reach the minimum than momentum optimization.</p>

<p>As we saw in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>, gradient descent updates the weights <strong>θ</strong> by directly subtracting the gradient of the cost function <em>J</em>(<strong>θ</strong>) with regard to the weights (∇<sub><strong>θ</strong></sub><em>J</em>(<strong>θ</strong>)) multiplied by the learning rate <em>η</em>. The equation is <strong>θ</strong> ← <strong>θ</strong> – <em>η</em>∇<sub><strong>θ</strong></sub><em>J</em>(<strong>θ</strong>). It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.</p>

<p>Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the <em>momentum vector</em> <strong>m</strong> (multiplied by the learning rate <em>η</em>), and it updates the weights by adding this momentum vector (see <a data-type="xref" href="#momentum_equation">Equation 11-5</a>). In other words, the gradient is used as a force learning to an acceleration, not as a speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter <em>β</em>,<a data-type="indexterm" data-primary="hyperparameters" data-secondary="β momentum" data-secondary-sortas="aab" id="id2600"/><a data-type="indexterm" data-primary="momentum coefficient β" id="id2601"/><a data-type="indexterm" data-primary="β momentum coefficient" id="id2602"/> called the <em>momentum coefficient</em>, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.</p>
<div class="fifty-percent less_space pagebreak-before" id="momentum_equation" data-type="equation"><h5><span class="label">Equation 11-5. </span>Momentum algorithm</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">m</mi>
          <mo>←</mo>
          <mi>β</mi>
          <mi mathvariant="bold">m</mi>
          <mo>-</mo>
          <mi>η</mi>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>+</mo>
          <mi mathvariant="bold">m</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></div>

<p>You can verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate <em>η</em> multiplied by 1 / (1 – <em>β</em>) (ignoring the sign). For example, if <em>β</em> = 0.9, then the terminal velocity is equal to 10 times the gradient times the learning rate, so momentum optimization ends up going 10 times faster than gradient descent! In practice, the gradients are not constant, so the speedup is not always as dramatic, but momentum optimization can escape from plateaus much faster than regular gradient descent. We saw in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a> that when the inputs have very different scales, the cost function<a data-type="indexterm" data-primary="cost function" data-secondary="momentum optimization" id="id2603"/> will look like an elongated bowl (see <a data-type="xref" href="ch04.html#elongated_bowl_diagram">Figure 4-7</a>). Gradient descent goes down the steep slope quite fast, but then it takes a very long time to go down the valley. In contrast, momentum optimization will roll down the valley faster and faster until it reaches the bottom (the optimum). In deep neural networks that don’t use batch-norm or layer-norm, the upper layers will often end up having inputs with very different scales, so using momentum optimization helps a lot. It can also help roll past local optima.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons why it’s good to have a bit of friction in the system: it reduces these oscillations and thus speeds up convergence.</p>
</div>

<p>Implementing momentum optimization in PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="momentum optimization in" id="id2604"/> is a no-brainer: just use the <code translate="no">SGD</code> optimizer<a data-type="indexterm" data-primary="torch" data-secondary="optim.SGD" id="id2605"/> and set its <code translate="no">momentum</code> hyperparameter, then sit back and profit!</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">,</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code></pre>

<p>The one drawback of momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular gradient descent<a data-type="indexterm" data-startref="xi_gradientdescentGDversusmomentumoptimization116268_1" id="id2606"/><a data-type="indexterm" data-startref="xi_momentumoptimization116268_1" id="id2607"/><a data-type="indexterm" data-startref="xi_optimizersmomentumoptimization116268_1" id="id2608"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Nesterov Accelerated Gradient"><div class="sect2" id="id193">
<h2>Nesterov Accelerated Gradient</h2>

<p>One<a data-type="indexterm" data-primary="NAG (Nesterov accelerated gradient)" id="xi_NAGNesterovacceleratedgradient116984_1"/><a data-type="indexterm" data-primary="Nesterov accelerated gradient (NAG)" id="xi_NesterovacceleratedgradientNAG116984_1"/><a data-type="indexterm" data-primary="Nesterov momentum optimization" id="xi_Nesterovmomentumoptimization116984_1"/><a data-type="indexterm" data-primary="optimizers" data-secondary="Nesterov accelerated gradient" id="xi_optimizersNesterovacceleratedgradient116984_1"/> small variant to momentum optimization, proposed by <a href="https://homl.info/55">Yurii Nesterov in 1983</a>,⁠<sup><a data-type="noteref" id="id2609-marker" href="ch11.html#id2609">21</a></sup> is almost always faster than regular momentum optimization. The <em>Nesterov accelerated gradient</em> (NAG) method, also known as <em>Nesterov momentum optimization</em>, 
<span class="keep-together">measures</span> the gradient of the cost function<a data-type="indexterm" data-primary="cost function" data-secondary="Nesterov Accelerated Gradient" id="id2610"/> not at the local position <strong>θ</strong> but slightly ahead in the direction of the momentum, at <strong>θ</strong> + <em>β</em><strong>m</strong> (see <a data-type="xref" href="#nesterov_momentum_equation">Equation 11-6</a>).</p>
<div class="fifty-percent" id="nesterov_momentum_equation" data-type="equation"><h5><span class="label">Equation 11-6. </span>Nesterov accelerated gradient algorithm</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">m</mi>
          <mo>←</mo>
          <mi>β</mi>
          <mi mathvariant="bold">m</mi>
          <mo>-</mo>
          <mi>η</mi>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>+</mo>
            <mi>β</mi>
            <mi mathvariant="bold">m</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>+</mo>
          <mi mathvariant="bold">m</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></div>

<p>This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position, as you can see in <a data-type="xref" href="#nesterov_momentum_diagram">Figure 11-7</a> (where ∇<sub>1</sub> represents the gradient of the cost function measured at the starting point <strong>θ</strong>, and ∇<sub>2</sub> represents the gradient at the point located at <strong>θ</strong> + <em>β</em><strong>m</strong>).</p>

<figure class="smallerseventy"><div id="nesterov_momentum_diagram" class="figure">
<img src="assets/hmls_1107.png" alt="Diagram illustrating regular versus Nesterov momentum optimization, showing that Nesterov updates by applying gradients after a momentum step, leading to improved convergence towards the optimum." width="1203" height="713"/>
<h6><span class="label">Figure 11-7. </span>Regular versus Nesterov momentum optimization: the former applies the gradients computed before the momentum step, while the latter applies the gradients computed after</h6>
</div></figure>

<p>As you can see, the Nesterov update ends up closer to the optimum. After a while, these small improvements add up and NAG ends up being significantly faster than regular momentum optimization. Moreover, note that when the momentum pushes the weights across a valley, ∇<sub>1</sub> continues to push farther across the valley, while ∇<sub>2</sub> pushes back toward the bottom of the valley. This helps reduce oscillations and thus NAG converges faster.</p>

<p>To use NAG, simply set <code translate="no">nesterov=True</code> when creating the <code translate="no">SGD</code> optimizer:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code>
                            <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">,</code> <code class="n">nesterov</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="AdaGrad"><div class="sect2" id="id194">
<h2>AdaGrad</h2>

<p>Consider<a data-type="indexterm" data-primary="Adagrad" id="xi_Adagrad117739_1"/><a data-type="indexterm" data-startref="xi_NAGNesterovacceleratedgradient116984_1" id="id2611"/><a data-type="indexterm" data-startref="xi_NesterovacceleratedgradientNAG116984_1" id="id2612"/><a data-type="indexterm" data-startref="xi_Nesterovmomentumoptimization116984_1" id="id2613"/><a data-type="indexterm" data-primary="optimizers" data-secondary="AdaGrad" id="xi_optimizersAdaGrad117739_1"/><a data-type="indexterm" data-startref="xi_optimizersNesterovacceleratedgradient116984_1" id="id2614"/> the elongated bowl problem again: gradient descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The <a href="https://homl.info/56"><em>AdaGrad</em> algorithm</a>⁠<sup><a data-type="noteref" id="id2615-marker" href="ch11.html#id2615">22</a></sup> achieves this correction by scaling down the gradient vector along the steepest dimensions (see <a data-type="xref" href="#adagrad_algorithm">Equation 11-7</a>).</p>
<div id="adagrad_algorithm" data-type="equation"><h5><span class="label">Equation 11-7. </span>AdaGrad algorithm</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">s</mi>
          <mo>←</mo>
          <mi mathvariant="bold">s</mi>
          <mo>+</mo>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊗</mo>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>-</mo>
          <mi>η</mi>
          <mspace width="0.166667em"/>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊘</mo>
          <msqrt>
            <mrow>
              <mi mathvariant="bold">s</mi>
              <mo>+</mo>
              <mi>ε</mi>
            </mrow>
          </msqrt>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></div>

<p>The first step accumulates the square of the gradients into the vector <strong>s</strong> (recall that the ⊗ symbol represents the element-wise multiplication). This vectorized form is equivalent to computing <em>s</em><sub><em>i</em></sub> ← <em>s</em><sub><em>i</em></sub> + (∂<em>J</em>(<strong>θ</strong>) / ∂<em>θ</em><sub><em>i</em></sub>)<sup>2</sup> for each element <em>s</em><sub><em>i</em></sub> of the vector <strong>s</strong>; in other words, each <em>s</em><sub><em>i</em></sub> accumulates the squares of the partial derivative of the cost function with regard to parameter <em>θ</em><sub><em>i</em></sub>. If the cost function<a data-type="indexterm" data-primary="cost function" data-secondary="Adagrad" id="id2616"/> is steep along the <em>i</em><sup>th</sup> dimension, then <em>s</em><sub><em>i</em></sub> will get larger and larger at each iteration.</p>

<p>The second step is almost identical to gradient descent, but with one big difference: the gradient vector is scaled down by a factor of <math><msqrt><mrow><mi mathvariant="bold">s</mi><mo>+</mo><mi>ε</mi></mrow></msqrt></math> (the ⊘ symbol represents the element-wise division, the square root is also computed element-wise, and <em>ε</em> is a smoothing term to avoid division by zero, typically set to 10<sup>–10</sup>). This vectorized form is equivalent to simultaneously computing <math><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>-</mo><mi>η</mi><mo> </mo><mo>∂</mo><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo><mo>/</mo><mo>∂</mo><msub><mi>θ</mi><mi>i</mi></msub><mo lspace="0.25em" rspace="0.25em">/</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mi>ε</mi></msqrt></math> for all parameters <em>θ</em><sub><em>i</em></sub>.</p>

<p>In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an <em>adaptive learning rate</em>.<a data-type="indexterm" data-primary="adaptive learning rate algorithms" id="xi_adaptivelearningratealgorithms11853179_1"/><a data-type="indexterm" data-primary="learning rate" data-secondary="adaptive learning rate algorithms" id="xi_adaptivelearningratealgorithms11853179_2"/> It helps point the resulting updates more directly toward the global optimum (see <a data-type="xref" href="#adagrad_diagram">Figure 11-8</a>). One additional benefit is that it requires much less tuning of the learning rate hyperparameter <em>η</em>.</p>

<p>AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks: the learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum.</p>

<figure><div id="adagrad_diagram" class="figure">
<img src="assets/hmls_1108.png" alt="Diagram comparing AdaGrad and gradient descent, showing AdaGrad adjusting direction earlier towards the optimum in steep dimensions." width="1251" height="579"/>
<h6><span class="label">Figure 11-8. </span>AdaGrad versus gradient descent: the former can correct its direction earlier to point to the optimum</h6>
</div></figure>

<p>So even though PyTorch has an <code translate="no">Adagrad</code> optimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as linear regression, though). Still, understanding AdaGrad is helpful to comprehend the other adaptive learning rate <span class="keep-together">optimizers</span>.<a data-type="indexterm" data-startref="xi_Adagrad117739_1" id="id2617"/><a data-type="indexterm" data-startref="xi_optimizersAdaGrad117739_1" id="id2618"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="RMSProp"><div class="sect2" id="id195">
<h2>RMSProp</h2>

<p>As<a data-type="indexterm" data-primary="optimizers" data-secondary="RMSProp" id="xi_optimizersRMSProp118623_1"/><a data-type="indexterm" data-primary="RMSProp" id="xi_RMSProp118623_1"/> we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum. The <em>RMSProp</em> algorithm⁠<sup><a data-type="noteref" id="id2619-marker" href="ch11.html#id2619">23</a></sup> fixes this by accumulating only the gradients from the most recent iterations, as opposed to all the gradients since the beginning of training. It does so by using exponential decay in the first step (see <a data-type="xref" href="#rmsprop_algorithm">Equation 11-8</a>).</p>
<div id="rmsprop_algorithm" data-type="equation"><h5><span class="label">Equation 11-8. </span>RMSProp algorithm</h5><math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">s</mi>
          <mo>←</mo>
          <mi>α</mi>
          <mi mathvariant="bold">s</mi>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <mi>α</mi>
            <mo>)</mo>
          </mrow>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊗</mo>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>-</mo>
          <mi>η</mi>
          <mspace width="0.166667em"/>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊘</mo>
          <msqrt>
            <mrow>
              <mi mathvariant="bold">s</mi>
              <mo>+</mo>
              <mi>ε</mi>
            </mrow>
          </msqrt>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></div>

<p>The decay rate <em>α</em> is typically set to 0.9. Yes, it is once again a new hyperparameter<a data-type="indexterm" data-primary="decay rate hyperparameter" id="id2620"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="decay rate" id="id2621"/><a data-type="indexterm" data-primary="PyTorch" data-secondary="RMSprop optimizer" id="id2622"/>, but this default value often works well, so you may not need to tune it at all.</p>

<p class="pagebreak-before">As you might expect, PyTorch has an <code translate="no">RMSprop</code> optimizer<a data-type="indexterm" data-primary="torch" data-secondary="optim.RMSprop" id="id2623"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">RMSprop</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.9</code><code class="p">,</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code></pre>

<p>Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around.<a data-type="indexterm" data-startref="xi_optimizersRMSProp118623_1" id="id2624"/><a data-type="indexterm" data-startref="xi_RMSProp118623_1" id="id2625"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Adam"><div class="sect2" id="id196">
<h2>Adam</h2>

<p><a href="https://homl.info/59"><em>Adam</em></a>,⁠<sup><a data-type="noteref" id="id2626-marker" href="ch11.html#id2626">24</a></sup> which<a data-type="indexterm" data-primary="Adam optimization" id="id2627"/><a data-type="indexterm" data-primary="optimizers" data-secondary="Adam" id="id2628"/> stands for <em>adaptive moment estimation</em>,<a data-type="indexterm" data-primary="adaptive moment estimation (Adam)" id="id2629"/> combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients (see <a data-type="xref" href="#adam_algorithm">Equation 11-9</a>). These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the <em>first moment</em>,<a data-type="indexterm" data-primary="first moment (mean of gradient)" id="id2630"/> while the variance is often called the <em>second moment</em><a data-type="indexterm" data-primary="second moment (variance of gradient)" id="id2631"/>, hence the name of the algorithm.</p>
<div id="adam_algorithm" data-type="equation"><h5><span class="label">Equation 11-9. </span>Adam algorithm</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
       </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">m</mi>
          <mo>←</mo>
          <msub><mi>β</mi> <mn>1</mn> </msub>
          <mi mathvariant="bold">m</mi>
          <mo>-</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <msub><mi>β</mi> <mn>1</mn> </msub>
            <mo>)</mo>
          </mrow>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">s</mi>
          <mo>←</mo>
          <msub><mi>β</mi> <mn>2</mn> </msub>
          <mi mathvariant="bold">s</mi>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <msub><mi>β</mi> <mn>2</mn> </msub>
            <mo>)</mo>
          </mrow>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊗</mo>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>3</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
          <mo>←</mo>
          <mstyle scriptlevel="0" displaystyle="true">
<mfrac>
<mi mathvariant="bold">m</mi>
<mrow>
<mn>1</mn>
<mo>-</mo>
<msup>
<mrow>
<msub><mi>β</mi> <mn>1</mn> </msub>
</mrow>
<mi>t</mi>
</msup>
</mrow>
</mfrac>
          </mstyle>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>4</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
          <mo>←</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mi mathvariant="bold">s</mi> <mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi> <mn>2</mn> </msub></mrow> <mi>t</mi> </msup></mrow></mfrac>
          </mstyle>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>5</mn>
          <mo lspace="0%" rspace="0%">.</mo>
          <mspace width="1.em"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>+</mo>
          <mi>η</mi>
          <mspace width="0.166667em"/>
          <mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
          <mo>⊘</mo>
          <msqrt>
            <mrow>
              <mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
              <mo>+</mo>
              <mi>ε</mi>
            </mrow>
          </msqrt>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>In this equation, <em>t</em> represents the iteration number (starting at 1).</p>

<p>If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both momentum optimization and RMSProp: <em>β</em><sub>1</sub> corresponds to <em>β</em> in momentum optimization, and <em>β</em><sub>2</sub> corresponds to <em>α</em> in RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just 1 – <em>β</em><sub>1</sub> times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since <strong>m</strong> and <strong>s</strong> are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost <strong>m</strong> and <strong>s</strong> at the beginning of training.</p>

<p>The momentum decay<a data-type="indexterm" data-primary="hyperparameters" data-secondary="β momentum decay" data-secondary-sortas="aab" id="id2632"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="scaling decay" id="id2633"/><a data-type="indexterm" data-primary="momentum decay hyperparameter β" id="id2634"/><a data-type="indexterm" data-primary="β momentum decay hyperparameter" id="id2635"/> hyperparameter <em>β</em><sub>1</sub> is typically initialized to 0.9, while the scaling decay hyperparameter<a data-type="indexterm" data-primary="scaling decay hyperparameter" id="id2636"/> <em>β</em><sub>2</sub> is often initialized to 0.999. As earlier, the smoothing term<a data-type="indexterm" data-primary="smoothing terms" id="id2637"/> <em>ε</em> is usually initialized to a tiny number such as 10<sup>–8</sup>. These are the default values for the <code translate="no">Adam</code> class. Here is how to create an Adam optimizer using PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="Adam optimizer" id="id2638"/><a data-type="indexterm" data-primary="torch" data-secondary="optim.Adam" id="id2639"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">betas</code><code class="o">=</code><code class="p">(</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.999</code><code class="p">),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code></pre>

<p>Since Adam is an adaptive learning rate algorithm, like AdaGrad and RMSProp, it requires less tuning of the learning rate hyperparameter <em>η</em>. You can often use the default value <em>η</em> = 0.001, making Adam even easier to use than gradient descent.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you are starting to feel overwhelmed by all these different techniques and are wondering how to choose the right ones for your task, don’t worry: some practical guidelines are provided at the end of this chapter.</p>
</div>

<p>Finally, three variants of Adam are worth mentioning: AdaMax, NAdam, and AdamW.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="AdaMax"><div class="sect2" id="id197">
<h2>AdaMax</h2>

<p>The Adam paper also introduced AdaMax<a data-type="indexterm" data-primary="AdaMax" id="id2640"/><a data-type="indexterm" data-primary="optimizers" data-secondary="AdaMax" id="id2641"/>. Notice that in step 2 of <a data-type="xref" href="#adam_algorithm">Equation 11-9</a>, Adam accumulates the squares of the gradients in <strong>s</strong> (with a greater weight for more recent gradients). In step 5, if we ignore <em>ε</em> and steps 3 and 4 (which are technical details anyway), Adam scales down the parameter updates by the square root of <strong>s</strong>. In short, Adam scales down the parameter updates by the ℓ<sub>2</sub> norm of the time-decayed gradients (recall that the ℓ<sub>2</sub> norm is the square root of the sum of squares).</p>

<p>AdaMax replaces the ℓ<sub>2</sub> norm with the ℓ<sub>∞</sub> norm (a fancy way of saying the max). Specifically, it replaces step 2 in <a data-type="xref" href="#adam_algorithm">Equation 11-9</a> with <math><mi mathvariant="bold">s</mi><mo>←</mo><mpadded lspace="-1px"><mi>max</mi><mo>(</mo><msub><mi>β</mi><mn>2</mn></msub><mi mathvariant="bold">s</mi><mo lspace="0%" rspace="0%">,</mo><mtext> abs(</mtext><msub><mo mathvariant="bold">∇</mo><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo><mo>)</mo><mo>)</mo></mpadded></math>, it drops step 4, and in step 5 it scales down the gradient updates by a factor of <strong>s</strong>, which is the max of the absolute value of the time-decayed gradients.</p>

<p>In practice, this can make AdaMax more stable than Adam, but it really depends on the dataset, and in <span class="keep-together">general</span> Adam performs better. So, this is just one more optimizer you can try if you experience problems with Adam on some task.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="NAdam"><div class="sect2" id="id198">
<h2>NAdam</h2>

<p>NAdam<a data-type="indexterm" data-primary="Nadam" id="id2642"/><a data-type="indexterm" data-primary="optimizers" data-secondary="Nadam" id="id2643"/> optimization is Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. In his report introducing this technique,⁠<sup><a data-type="noteref" id="id2644-marker" href="ch11.html#id2644">25</a></sup> the researcher Timothy Dozat compares many different optimizers on various tasks and finds that NAdam generally outperforms Adam but is sometimes outperformed by RMSProp.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="AdamW"><div class="sect2" id="id199">
<h2>AdamW</h2>

<p><a href="https://homl.info/adamw">AdamW</a>⁠<sup><a data-type="noteref" id="id2645-marker" href="ch11.html#id2645">26</a></sup> <a data-type="indexterm" data-primary="AdamW" id="id2646"/><a data-type="indexterm" data-primary="optimizers" data-secondary="AdamW" id="id2647"/>is a variant of Adam that integrates a regularization technique called <em>weight decay</em>.<a data-type="indexterm" data-primary="regularization" data-secondary="weight decay" id="id2648"/><a data-type="indexterm" data-primary="weight decay" id="id2649"/> Weight decay reduces the size of the model’s weights at each training iteration by multiplying them by a decay factor such as 0.99. This may remind you of ℓ<sub>2</sub> regularization (introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>), which also aims to keep the weights small, and indeed it can be shown mathematically that ℓ<sub>2</sub> regularization is equivalent to weight decay when using SGD. However, when using Adam or its variants, ℓ<sub>2</sub> regularization and weight decay are <em>not</em> equivalent: in practice, combining Adam with ℓ<sub>2</sub> regularization results in models that often don’t generalize as well as those produced by SGD. AdamW fixes this issue by properly combining Adam with weight decay.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Adaptive optimization methods (including RMSProp, Adam, AdaMax, NAdam, and AdamW optimization) are often great, converging fast to a good solution. However, a <a href="https://homl.info/60">2017 paper</a>⁠<sup><a data-type="noteref" id="id2650-marker" href="ch11.html#id2650">27</a></sup> by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model’s performance, try using NAG instead: your dataset may just be allergic to adaptive gradients.</p>
</div>

<p>To use NAdam, AdaMax, or AdamW in PyTorch, replace <code translate="no">torch.optim.Adam</code> with <code translate="no">torch.optim.NAdam</code>, <code translate="no">torch.optim.Adamax</code>, or <code translate="no">torch.optim.AdamW</code>. For <a data-type="indexterm" data-primary="torch" data-secondary="optim.AdamW" id="id2651"/>AdamW, you probably want to tune the <code translate="no">weight_decay</code> hyperparameter.</p>

<p>All the optimization techniques discussed so far only rely on the <em>first-order partial derivatives</em><a data-type="indexterm" data-primary="first-order partial derivatives (Jacobians)" id="id2652"/> (<em>Jacobians</em>,<a data-type="indexterm" data-primary="Jacobians" id="id2653"/> which measure the slope of the loss function along each axis). The optimization literature also contains amazing algorithms based on the <em>second-order partial derivatives</em> (the <em>Hessians</em>,<a data-type="indexterm" data-primary="Hessians" id="id2654"/> which are the partial derivatives of the Jacobians,<a data-type="indexterm" data-primary="second-order partial derivatives (Hessians)" id="id2655"/> measuring how each Jacobian changes along each axis; in other words, measuring the loss function’s curvature).</p>

<p>Unfortunately, these Hessian-based algorithms are hard to apply directly to deep neural networks because there are <em>n</em><sup>2</sup> second-order derivatives per output (where <em>n</em> is the number of parameters), as opposed to just <em>n</em> first-order derivatives per output. Since DNNs typically have hundreds of thousands of parameters or more, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the <em>Hessian matrix</em> is just too slow.⁠<sup><a data-type="noteref" id="id2656-marker" href="ch11.html#id2656">28</a></sup></p>

<p>Luckily, it is possible to use stochastic methods that can efficiently approximate second-order information. One such algorithm is Shampoo<a data-type="indexterm" data-primary="Shampoo" id="id2657"/>,⁠<sup><a data-type="noteref" id="id2658-marker" href="ch11.html#id2658">29</a></sup> which uses accumulated gradient information to approximate the second-order terms, similar to how Adam accumulates first-order statistics. It is not included in the PyTorch library, but you can get it in the PyTorch-Optimizer library (<code translate="no">pip install torch_optimizer</code>).<a data-type="indexterm" data-startref="xi_adaptivelearningratealgorithms11853179_1" id="id2659"/><a data-type="indexterm" data-startref="xi_adaptivelearningratealgorithms11853179_2" id="id2660"/><a data-type="indexterm" data-primary="sparse models" id="id2661"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id2662">
<h1>Training Sparse Models</h1>
<p>All the optimization algorithms we just discussed produce dense models, meaning that most parameters will be nonzero. If you need a blazingly fast model at runtime, or if you need it to take up less memory, you may prefer to end up with a sparse model instead.</p>

<p>One way to achieve this is to train the model as usual, then get rid of the tiny weights (set them to zero) using <code translate="no">torch.nn.utils.prune.l1_unstructured()</code>. Or you can get rid of entire neurons, channels, or layers, not just individual weights, using <code translate="no">torch.nn.utils.prune.ln_structured()</code>, or other functions in the <code translate="no">torch.nn.utils.prune</code> 
<span class="keep-together">package.</span></p>

<p>You should generally also apply fairly strong sparsity inducing regularization during training, such as ℓ<sub>1</sub> regularization (you’ll see how later in this chapter), since it pushes the optimizer to zero out as many weights as it can (see <a data-type="xref" href="ch04.html#lasso_regression">“Lasso Regression”</a>). You can also try scaling down random weights during initialization to encourage sparsity.</p>
</div></aside>

<p><a data-type="xref" href="#optimizer_summary_table">Table 11-2</a> compares all the optimizers we’ve discussed so far (<img src="assets/star_2b50.png" width="160" height="160"/> is bad, <img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/> is average, and <img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/> is good).<a data-type="indexterm" data-startref="xi_deepneuralnetworksDNNsfasteroptimizersfor116239_1" id="id2663"/><a data-type="indexterm" data-startref="xi_gradientdescentGDwithoptimizers116239_1" id="id2664"/><a data-type="indexterm" data-startref="xi_optimizers116239_1" id="id2665"/></p>
<table id="optimizer_summary_table">
<caption><span class="label">Table 11-2. </span>Optimizer comparison</caption>
<thead>
<tr>
<th>Class</th>
<th>Convergence speed</th>
<th>Convergence quality</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><code translate="no">SGD</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
</tr>
<tr>
<td><p><code translate="no">SGD(momentum=...)</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
</tr>
<tr>
<td><p><code translate="no">SGD(momentum=..., nesterov=True)</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
</tr>
<tr>
<td><p><code translate="no">Adagrad</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/> (stops too early)</p></td>
</tr>
<tr>
<td><p><code translate="no">RMSprop</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/>  or  <img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
</tr>
<tr>
<td><p><code translate="no">Adam</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/>  or  <img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
</tr>
<tr>
<td><p><code translate="no">AdaMax</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/>  or  <img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
</tr>
<tr>
<td><p><code translate="no">NAdam</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/>  or  <img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
</tr>
<tr>
<td><p><code translate="no">AdamW</code></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
<td><p><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/>  or  <img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/><img src="assets/star_2b50.png" width="160" height="160"/></p></td>
</tr>
</tbody>
</table>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Learning Rate Scheduling"><div class="sect1" id="id200">
<h1>Learning Rate Scheduling</h1>

<p>Finding<a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="learning rate scheduling" id="xi_deepneuralnetworksDNNslearningratescheduling1111858_1"/><a data-type="indexterm" data-primary="learning schedules" id="xi_learningschedules1111858_1"/><a data-type="indexterm" data-primary="PyTorch" data-secondary="learning schedules" id="xi_PyTorchlearningschedules1111858_1"/> a good learning rate is very important. If you set it too high, training will diverge (as discussed in <a data-type="xref" href="ch04.html#gradientDescent4">“Gradient Descent”</a>). If you set it too low, then training will be painfully slow, and it may also get stuck in a local optimum and produce a suboptimal model. If you set the learning rate fairly high (but not high enough to diverge), then training will often make rapid progress at first, but it will end up dancing around the optimum toward the end of training and thereby produce a suboptimal model. If you find a really good learning rate, you can end up with an excellent model, but training will generally be a bit too slow. Luckily, you can do better than a constant learning rate. In particular, it’s a good idea to start with a fairly high learning rate and then reduce it toward the end of training (or whenever progress stops): this ensures that training starts fast, while also allowing backprop to settle down toward the end to really fine-tune the model parameters (see <a data-type="xref" href="#learning_schedule_diagram">Figure 11-9</a>).</p>

<p>There are various other strategies to tweak the learning rate during training. These are called <em>learning schedules</em> (I briefly introduced this concept in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>). The <code translate="no">torch.optim.lr_scheduler</code> module provides several implementations of common learning schedules. Let’s look at the most important ones, starting with exponential scheduling.</p>

<figure class="width-85"><div id="learning_schedule_diagram" class="figure">
<img src="assets/hmls_1109.png" alt="Diagram illustrating the effects of different learning rates on loss over epochs, showing that starting with a high rate and reducing it achieves optimal training results." width="1324" height="721"/>
<h6><span class="label">Figure 11-9. </span>Learning curves for various learning rates η</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Exponential Scheduling"><div class="sect2" id="id201">
<h2>Exponential Scheduling</h2>

<p>The<a data-type="indexterm" data-primary="exponential scheduling" id="id2666"/><a data-type="indexterm" data-primary="torch" data-secondary="optim.lr_scheduler.ExponentialLR" id="xi_torchoptimlr_scheduleExponentialLR1111944_1"/> <code translate="no">ExponentialLR</code> class implements <em>exponential scheduling</em>, whereby the learning rate is multiplied by a constant factor <code translate="no">gamma</code> at some regular interval, typically at every epoch. As a result, after the <em>n</em><sup>th</sup> epoch, the learning rate will be equal to the initial learning rate times <code translate="no">gamma</code> to the power of <em>n</em>. This factor <code translate="no">gamma</code> is yet another hyperparameter you can tune. In general, you will want to set <code translate="no">gamma</code> to a value lower than 1, but fairly close to 1 to avoid decreasing the learning rate too fast. For example, if <code translate="no">gamma</code> is set to 0.9, then after 10 epochs the learning rate will be about 35% of the initial learning rate, and after 20 epochs it will be about 12%.</p>

<p>The <code translate="no">ExponentialLR</code> constructor expects at least two arguments—the optimizer whose learning rate will be tweaked during training, and the factor <code translate="no">gamma</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># build the model</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>  <code class="c1"># or any other optim.</code>
<code class="n">scheduler</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">lr_scheduler</code><code class="o">.</code><code class="n">ExponentialLR</code><code class="p">(</code><code class="n">optimizer</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code></pre>

<p>Next, you must update the training loop to call <code translate="no">scheduler.step()</code> at the end of each epoch to tweak the optimizer’s learning rate:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># the rest of the training loop remains unchanged</code>

    <code class="n">scheduler</code><code class="o">.</code><code class="n">step</code><code class="p">()</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you interrupt training and you later want to resume it where you left off, you should set the <code translate="no">last_epoch</code> argument of the scheduler’s constructor to the last epoch you ran (zero-indexed). The default is –1, which makes the scheduler start training from scratch.<a data-type="indexterm" data-startref="xi_torchoptimlr_scheduleExponentialLR1111944_1" id="id2667"/></p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Cosine Annealing"><div class="sect2" id="id202">
<h2>Cosine Annealing</h2>

<p>Instead<a data-type="indexterm" data-primary="cosine annealing" id="xi_cosineannealing1112198_1"/> of decreasing the learning rate exponentially, you can use the cosine function to go from the maximum learning rate <em>η</em><sub>max</sub> at the start of training, down to the minimum learning rate <em>η</em><sub>min</sub> at the end. This is called <em>cosine annealing</em>. Compared to exponential scheduling, cosine annealing ensures that the learning rate remains fairly high during most of training, while getting closer to the minimum near the end (see <a data-type="xref" href="#cosine_annealing_diagram">Figure 11-10</a>). All in all, cosine annealing generally performs better. The learning rate at epoch <em>t</em> (zero-indexed) is given by <a data-type="xref" href="#cosine_annealing_equation">Equation 11-10</a>, where <em>T</em><sub>max</sub> is the maximum number of epochs.</p>

<figure><div id="cosine_annealing_diagram" class="figure">
<img src="assets/hmls_1110.png" alt="A diagram comparing cosine and exponential learning rate schedules, showing the cosine schedule maintaining a higher rate over more epochs before decreasing." width="2272" height="774"/>
<h6><span class="label">Figure 11-10. </span>Cosine annealing learning schedule</h6>
</div></figure>
<div id="cosine_annealing_equation" data-type="equation">
<h5><span class="label">Equation 11-10. </span>Cosine annealing equation</h5>
<math display="block">
  <mrow>
    <msub><mi>η</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <msub><mi>η</mi> <mtext>min</mtext> </msub>
    <mo>+</mo>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <mrow>
      <mo>(</mo>
      <msub><mi>η</mi> <mtext>max</mtext> </msub>
      <mo>-</mo>
      <msub><mi>η</mi> <mtext>min</mtext> </msub>
      <mo>)</mo>
    </mrow>
    <mfenced separators="" open="(" close=")">
      <mn>1</mn>
      <mo>+</mo>
      <mo form="prefix">cos</mo>
      <mfenced separators="" open="(" close=")">
        <mfrac><mi>t</mi> <msub><mi>T</mi> <mtext>max</mtext> </msub></mfrac>
        <mi>π</mi>
      </mfenced>
    </mfenced>
  </mrow>
</math>
</div>

<p>PyTorch includes the <code translate="no">CosineAnnealingLR</code> scheduler, which you can create as follows (<code translate="no">T_max</code> is <em>T</em><sub>max</sub> and <code translate="no">eta_min</code> is <em>η</em><sub>min</sub>). You can then use it just like the <code translate="no">ExponentialLR</code> scheduler:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">cosine_scheduler</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">lr_scheduler</code><code class="o">.</code><code class="n">CosineAnnealingLR</code><code class="p">(</code>
    <code class="n">optimizer</code><code class="p">,</code> <code class="n">T_max</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">eta_min</code><code class="o">=</code><code class="mf">0.001</code><code class="p">)</code></pre>

<p>One problem with cosine annealing is that you have to set two new hyperparameters, <em>T</em><sub>max</sub> and <em>η</em><sub>min</sub>, and it’s not easy to know in advance how many epochs to train and when to stop decreasing the learning rate. This is why I generally prefer to use the performance scheduling technique.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Performance Scheduling"><div class="sect2" id="id203">
<h2>Performance Scheduling</h2>

<p><em>Performance scheduling</em>,<a data-type="indexterm" data-primary="performance scheduling" id="id2668"/> also called <em>adaptive scheduling</em>, is implemented by PyTorch’s<a data-type="indexterm" data-primary="torch" data-secondary="optim.lr_scheduler.ReduceLROnPlateau" id="id2669"/> <code translate="no">ReduceLROnPlateau</code> scheduler: it keeps track of a given metric during training—typically the validation loss—and if this metric stops improving for some time, it multiplies the learning rate by some factor. This scheduler has quite a few hyperparameters, but the default values work well for most of them. You may occasionally need to<a data-type="indexterm" data-primary="hyperparameters" data-secondary="performance scheduling" id="id2670"/> tweak the following (see the documentation for information on the other hyperparameters):</p>
<dl>
<dt><code translate="no">mode</code></dt>
<dd>
<p>If the tracked metric must be maximized (such as the validation accuracy), then you must set the <code translate="no">mode</code> to <code translate="no">'max'</code>. The default is <code translate="no">'min'</code>, which is fine if the tracked metric must be minimized (such as the validation loss).</p>
</dd>
<dt><code translate="no">patience</code></dt>
<dd>
<p>The number of consecutive steps (typically epochs) to wait for improvement in the monitored metric before reducing the learning rate. It defaults to 10, which is generally fine. If each epoch is very long, then you may want to reduce this value.</p>
</dd>
<dt><code translate="no">factor</code></dt>
<dd>
<p>The factor by which the learning rate will be multiplied whenever the monitored metric fails to improve for too long. It defaults to 0.1, again a reasonable default, but perhaps a bit small in some cases.</p>
</dd>
</dl>

<p>For example, let’s implement performance scheduling based on the validation accuracy (i.e., which we want to maximize):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># build the model and optimizer</code>
<code class="n">scheduler</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">lr_scheduler</code><code class="o">.</code><code class="n">ReduceLROnPlateau</code><code class="p">(</code>
  <code class="n">optimizer</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"max"</code><code class="p">,</code> <code class="n">patience</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">factor</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code></pre>

<p>The training loop needs to be tweaked again because we must evaluate the desired metric at each epoch (in this example, we are using the <code translate="no">evaluate_tm()</code> function that we defined in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>), and we must then pass the result to the scheduler’s <code translate="no">step()</code> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">metric</code> <code class="o">=</code> <code class="n">torchmetrics</code><code class="o">.</code><code class="n">Accuracy</code><code class="p">(</code><code class="n">task</code><code class="o">=</code><code class="s2">"multiclass"</code><code class="p">,</code> <code class="n">num_classes</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># the rest of the training loop remains unchanged</code>
    <code class="n">val_metric</code> <code class="o">=</code> <code class="n">evaluate_tm</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">valid_loader</code><code class="p">,</code> <code class="n">metric</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
    <code class="n">scheduler</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">val_metric</code><code class="p">)</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Warming Up the Learning Rate"><div class="sect2" id="id204">
<h2>Warming Up the Learning Rate</h2>

<p>So<a data-type="indexterm" data-primary="learning rate" data-secondary="warming up" id="xi_learningratewarmingup1112763_1"/> far, we have always started training with the maximum learning rate. However, this can sometimes cause gradient descent to bounce around randomly at the beginning of training, neither exploding nor making any significant progress. This typically happens with sensitive models, such as recurrent neural networks (<a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>), or when using a very large batch size. In such cases, one solution is to “warm up” the learning rate, starting close to zero and gradually increasing the learning rate over a few epochs, up to the maximum learning rate. During this warm-up phase, gradient descent has time to stabilize into a better region of the loss landscape, where it can then make quick progress using a high learning rate.</p>

<p>Why does this work? Well, the loss landscape sometimes resembles the Himalayas: it’s very high up and full of gigantic spikes. If you start with a high learning rate, you might jump from one mountain peak to the next for a very long time. If instead you start with a small learning rate, you will just walk down the mountain and valleys and escape the spiky mountain range altogether until you reach flatter lands. From then on, you can use a large learning rate for the rest of your journey, slowing down only toward the end.</p>

<p>A common way to implement learning rate warm up using PyTorch is to use a <code translate="no">LinearLR</code><a data-type="indexterm" data-primary="torch" data-secondary="optim.lr_scheduler.LinearLR" id="id2671"/> scheduler to increase the learning rate linearly over a few epochs. For example, the following scheduler will increase the learning rate from 10% to 100% of the optimizer’s original learning rate over 3 epochs (i.e., 10% during the first epoch, 40% during the second epoch, 70% during the third epoch, and 100% after that):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">warmup_scheduler</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">lr_scheduler</code><code class="o">.</code><code class="n">LinearLR</code><code class="p">(</code>
    <code class="n">optimizer</code><code class="p">,</code> <code class="n">start_factor</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">end_factor</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">total_iters</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code></pre>

<p>If you would like more flexibility, you can write your own custom function and wrap it in a <code translate="no">LambdaLR</code><a data-type="indexterm" data-primary="torch" data-secondary="optim.lr_scheduler.LambdaLR" id="id2672"/> scheduler. For example, the following scheduler is equivalent to the <code translate="no">LinearLR</code> scheduler we just defined:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">warmup_scheduler</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">lr_scheduler</code><code class="o">.</code><code class="n">LambdaLR</code><code class="p">(</code>
    <code class="n">optimizer</code><code class="p">,</code>
    <code class="k">lambda</code> <code class="n">epoch</code><code class="p">:</code> <code class="p">(</code><code class="nb">min</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code> <code class="o">/</code> <code class="mi">3</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="mf">1.0</code> <code class="o">-</code> <code class="mf">0.1</code><code class="p">)</code> <code class="o">+</code> <code class="mf">0.1</code><code class="p">)</code></pre>

<p>You must then insert <code translate="no">warmup_scheduler.step()</code> at the beginning of each epoch, and make sure you deactivate the scheduler(s) you are using for the rest of training during the warm-up phase. And that’s all!</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
    <code class="n">warmup_scheduler</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># the rest of the training loop is unchanged</code>
    <code class="k">if</code> <code class="n">epoch</code> <code class="o">&gt;=</code> <code class="mi">3</code><code class="p">:</code>  <code class="c1"># deactivate other scheduler(s) during warmup</code>
        <code class="n">scheduler</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">val_metric</code><code class="p">)</code></pre>

<p>In short, you pretty much always want to cool down the learning rate at the end of training, and you may also want to warm it up at the beginning if gradient descent needs a bit of help getting started. But are there any cases where you may want to tweak the learning rate in the middle of training? Well yes, there are; for example, if gradient descent gets stuck in a local optimum or a high plateau. Gradient descent could remain stuck here for a long time, or even forever. Luckily, there’s a way to escape this trap: just increase the learning rate for a little while.</p>

<p>You could spend your time staring at the learning curves during training, and manually interrupting it to tweak the learning rate when needed, but you probably have better things to do. Alternatively, you could implement a custom scheduler that monitors the validation metric—much like the <code translate="no">ReduceLROnPlateau</code> scheduler—and increases the learning rate for a while if the validation metric is stuck in a bad plateau. For this, you could subclass the <code translate="no">LRScheduler</code> base class. This is beyond the scope of this book, but you can take inspiration from the <code translate="no">ReduceLROnPlateau</code> scheduler’s source code (and get a little bit of help from your favorite AI assistant). But a much simpler option is to use the cosine annealing with warm restarts learning schedule. Let’s look at it now.<a data-type="indexterm" data-startref="xi_learningratewarmingup1112763_1" id="id2673"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Cosine Annealing with Warm Restarts"><div class="sect2" id="id205">
<h2>Cosine Annealing with Warm Restarts</h2>

<p><em>Cosine annealing with warm restarts</em> was introduced in a <a href="https://homl.info/coslr">2016 paper</a> by Ilya Loshchilov and Frank Hutter.⁠<sup><a data-type="noteref" id="id2674-marker" href="ch11.html#id2674">30</a></sup> This schedule just repeats the cosine annealing schedule over and over again. Since the learning rate regularly shoots back up, this schedule allows gradient descent to escape local optima and plateaus automatically. The authors recommend starting with a fairly short round of cosine annealing, but then doubling <em>T</em><sub>max</sub> after each round (see <a data-type="xref" href="#cosine_annealing_warm_restarts_diagram">Figure 11-11</a>). This allows gradient descent to do a lot of quick explorations at the start of training, while also taking the time to properly optimize the model later during training, possibly escaping a plateau or two along the way.</p>

<figure><div id="cosine_annealing_warm_restarts_diagram" class="figure">
<img src="assets/hmls_1111.png" alt="Diagram illustrating the cosine annealing with warm restarts schedule, showing fluctuations in learning rate with repeated cycles increasing in length." width="2278" height="774"/>
<h6><span class="label">Figure 11-11. </span>Cosine annealing with warm restarts</h6>
</div></figure>

<p>Conveniently, PyTorch includes a <code translate="no">CosineAnnealingWarmRestarts</code><a data-type="indexterm" data-primary="torch" data-secondary="optim.lr_scheduler.CosineAnnealingWarmRestarts" id="id2675"/> scheduler. You must set <code translate="no">T_0</code>, which is the value of <em>T</em><sub>max</sub> for the first round of cosine annealing. You may also set <code translate="no">T_mult</code> to 2 if you want to double <em>T</em><sub>max</sub> at each round (the default is 1, meaning <em>T</em><sub>max</sub> stays constant and all rounds have the same length). Finally, you can set <code translate="no">eta_min</code> (it defaults to 0):<a data-type="indexterm" data-startref="xi_cosineannealing1112198_1" id="id2676"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">cosine_repeat_scheduler</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">lr_scheduler</code><code class="o">.</code><code class="n">CosineAnnealingWarmRestarts</code><code class="p">(</code>
    <code class="n">optimizer</code><code class="p">,</code> <code class="n">T_0</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">T_mult</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">eta_min</code><code class="o">=</code><code class="mf">0.001</code><code class="p">)</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="1cycle Scheduling"><div class="sect2" id="id206">
<h2>1cycle Scheduling</h2>

<p>Yet<a data-type="indexterm" data-primary="1cycle scheduling" data-primary-sortas="one-cycle scheduling" id="xi_1cyclescheduling1113294_1"/> another popular learning schedule is <em>1cycle</em>, introduced in a <a href="https://homl.info/1cycle">2018 paper</a> by Leslie Smith.⁠<sup><a data-type="noteref" id="id2677-marker" href="ch11.html#id2677">31</a></sup> It starts by warming up the learning rate, starting at <em>η</em><sub>0</sub> and growing linearly up to <em>η</em><sub>1</sub> halfway through training. Then it decreases the learning rate linearly down to <em>η</em><sub>0</sub> again during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly). The maximum learning rate <em>η</em><sub>1</sub> is chosen using the same approach we used to find the optimal learning rate, and the initial learning rate <em>η</em><sub>0</sub> is usually 10 times lower. When using a momentum, we start with a high momentum first (e.g., 0.95), then drop it down to a lower momentum during the first half of training (e.g., down to 0.85, linearly), and then bring it back up to the maximum value (e.g., 0.95) during the second half of training, finishing the last few epochs with that maximum value. Smith did many experiments showing that this approach was often able to speed up training considerably and reach better performance. For example, on the popular CIFAR10 image dataset, this approach reached 91.9% validation accuracy in just 100 epochs, compared to 90.3% accuracy in 800 epochs through a standard approach (using the same neural network architecture). This feat was dubbed <em>super-convergence</em>.<a data-type="indexterm" data-primary="super-convergence" id="id2678"/> PyTorch implements this schedule in the <code translate="no">OneCycleLR</code> scheduler.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you are not sure which learning schedule to use, 1cycle can be a good default, but I tend to have more luck with performance scheduling<a data-type="indexterm" data-primary="performance scheduling" id="id2679"/>. If you run into instabilities at the start of training, try adding learning rate warm-up. And if training gets stuck on plateaus, try cosine annealing with warm restarts.</p>
</div>

<p>We have now covered the most popular learning schedules, but PyTorch offers a few extra schedulers (e.g., a polynomial scheduler, a cyclic scheduler, a scheduler that makes it easy to chain other schedulers, and a few more), so make sure to check out the documentation.</p>

<p>Now let’s move on to one final topic before we complete this chapter on deep learning training techniques: regularization. Deep learning is highly prone to overfitting, so regularization is key!<a data-type="indexterm" data-startref="xi_1cyclescheduling1113294_1" id="id2680"/><a data-type="indexterm" data-startref="xi_deepneuralnetworksDNNslearningratescheduling1111858_1" id="id2681"/><a data-type="indexterm" data-startref="xi_learningschedules1111858_1" id="id2682"/><a data-type="indexterm" data-startref="xi_PyTorchlearningschedules1111858_1" id="id2683"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Avoiding Overfitting Through Regularization"><div class="sect1" id="id407">
<h1>Avoiding Overfitting Through Regularization</h1>
<blockquote>
  <p>With four parameters I can fit an elephant and with five I can make him wiggle his trunk.</p>
  <p data-type="attribution">John von Neumann, cited by Enrico Fermi in <em>Nature</em> 427</p>
</blockquote>

<p>With<a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="regularization" id="xi_deepneuralnetworksDNNsregularization1113455_1"/><a data-type="indexterm" data-primary="overfitting of data" data-secondary="avoiding through regularization" id="xi_overfittingofdataavoidingthroughregularization1113455_1"/> thousands of parameters, you can fit the whole zoo. Deep neural networks typically have tens of thousands of parameters, sometimes even millions or billions. This gives them an incredible amount of freedom and means they can fit a huge variety of complex datasets. But this great flexibility also makes the network prone to overfitting the training set. Regularization is often needed to prevent this.</p>

<p>We already implemented a common regularization technique in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a>: early stopping. Moreover, even though batch-norm and layer-norm were designed to solve the unstable gradients problems, they also act like pretty good regularizers. In this section we will examine other popular regularization techniques for neural networks: ℓ<sub>1</sub> and ℓ<sub>2</sub> regularization, dropout, MC dropout, and max-norm regularization.</p>








<section data-type="sect2" data-pdf-bookmark="ℓ1 and ℓ2 Regularization"><div class="sect2" id="id207">
<h2>ℓ<sub>1</sub> and ℓ<sub>2</sub> Regularization</h2>

<p>Just<a data-type="indexterm" data-primary="ℓ₁ and ℓ₂ regularization" id="xi_1and2regularization1113505_1"/><a data-type="indexterm" data-primary="ℓ₁ and ℓ₂ regularization" data-primary-sortas="l1 and l2 regularization" id="xi_1and2regularization1113505_1_sort"/><a data-type="indexterm" data-primary="regularization" data-secondary="ℓ₁ and ℓ₂ regularization" data-secondary-sortas="aaa" id="xi_regularization1and2regularization1113505_1"/> like you did in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter 4</a> for simple linear models, you can use ℓ<sub>2</sub> regularization to constrain a neural network’s connection weights, and/or ℓ<sub>1</sub> regularization if you want a sparse model (with many weights equal to 0). As we saw earlier (when discussing the AdamW optimizer), ℓ<sub>2</sub> regularization is mathematically equivalent to weight decay when using an <code translate="no">SGD</code> optimizer (with or without momentum), so if that’s the case you can implement ℓ<sub>2</sub> regularization by simply setting the optimizer’s <code translate="no">weight_decay</code> argument. For example, here is how to apply ℓ<sub>2</sub> regularization to the connection weights of a PyTorch model trained using <code translate="no">SGD</code>, with a regularization factor of 10<sup>–4</sup>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">weight_decay</code><code class="o">=</code><code class="mf">1e-4</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># use the optimizer normally during training</code></pre>

<p>If instead you are using an Adam optimizer<a data-type="indexterm" data-primary="Adam optimization" id="id2684"/><a data-type="indexterm" data-primary="adaptive moment estimation (Adam)" id="id2685"/>, you should switch to AdamW<a data-type="indexterm" data-primary="AdamW" id="id2686"/> and set the <code translate="no">weight_decay</code> argument. This is not exactly equivalent to ℓ<sub>2</sub> regularization, but as we saw earlier it’s pretty close and it works better.</p>

<p>Note that weight decay is applied to every model parameter, including bias terms, and even parameters of batch-norm and layer-norm layers. Generally that’s not a big deal, but penalizing these parameters does not contribute much to regularization and it may sometimes negatively impact training performance. So how can we apply weight decay<a data-type="indexterm" data-primary="regularization" data-secondary="weight decay" id="id2687"/><a data-type="indexterm" data-primary="weight decay" id="id2688"/> to some model parameters and not others? One approach is to implement ℓ<sub>2</sub> regularization manually, without relying on the optimizer’s weight decay feature. For this, you must tweak the training loop to manually compute the ℓ<sub>2</sub> loss based only on the parameters you want, and add this ℓ<sub>2</sub> loss to the main loss:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>
<code class="n">params_to_regularize</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">param</code> <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">named_parameters</code><code class="p">()</code>
    <code class="k">if</code> <code class="ow">not</code> <code class="s2">"bias"</code> <code class="ow">in</code> <code class="n">name</code> <code class="ow">and</code> <code class="ow">not</code> <code class="s2">"bn"</code> <code class="ow">in</code> <code class="n">name</code><code class="p">]</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># the rest of the training loop is unchanged</code>
        <code class="n">main_loss</code> <code class="o">=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">y_pred</code><code class="p">,</code> <code class="n">y_batch</code><code class="p">)</code>
        <code class="n">l2_loss</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">param</code><code class="o">.</code><code class="n">pow</code><code class="p">(</code><code class="mf">2.0</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code> <code class="k">for</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">params_to_regularize</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">main_loss</code> <code class="o">+</code> <code class="mf">1e-4</code> <code class="o">*</code> <code class="n">l2_loss</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code></pre>

<p>Another approach is to use PyTorch’s<a data-type="indexterm" data-primary="PyTorch" data-secondary="parameter groups" id="id2689"/> <em>parameter groups</em><a data-type="indexterm" data-primary="hyperparameters" data-secondary="parameter groups" id="id2690"/><a data-type="indexterm" data-primary="parameter groups" id="id2691"/> feature, which lets the optimizer apply different hyperparameters to different groups of model parameters. So far, we have always created optimizers by passing them the full list of model parameters: PyTorch automatically put them all in a single parameter group, sharing the same hyperparameters. Instead, we can pass a list of dictionaries to the optimizer, each with a <code translate="no">"params"</code> entry containing a list of parameters, and (optionally) some hyperparameter key/value pairs specific to this group of parameters. The group-specific hyperparameters take precedence over the optimizer’s global hyperparameters. For example, let’s create an optimizer with two parameter groups: the first group will contain all the parameters we want to regularize and it will use weight decay, while the second group will contain all the bias terms and BN parameters, and it will not use weight decay at all.</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">params_bias_and_bn</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">param</code> <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">named_parameters</code><code class="p">()</code>
    <code class="k">if</code> <code class="s2">"bias"</code> <code class="ow">in</code> <code class="n">name</code> <code class="ow">or</code> <code class="s2">"bn"</code> <code class="ow">in</code> <code class="n">name</code><code class="p">]</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">([</code>
    <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params_to_regularize</code><code class="p">,</code> <code class="s2">"weight_decay"</code><code class="p">:</code> <code class="mf">1e-4</code><code class="p">},</code>
    <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params_bias_and_bn</code><code class="p">},</code>
    <code class="p">],</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># use the optimizer normally during training</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>Parameter groups also allow you to apply different learning rates to different parts of your model. This is most common for transfer learning, when you want new layers to be updated faster than reused ones.</p>
</div>

<p>Now how about ℓ<sub>1</sub> regularization? Well unfortunately PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="ℓ₁ and ℓ₂ regularization" data-secondary-sortas="aaa" id="id2692"/> does not provide any helper for this, so you need to implement it manually, much like we did for ℓ<sub>2</sub> regularization. This means tweaking the training loop to compute the ℓ<sub>1</sub> loss and adding it to the main loss:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">l1_loss</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">param</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code> <code class="k">for</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">params_to_regularize</code><code class="p">)</code>
<code class="n">loss</code> <code class="o">=</code> <code class="n">main_loss</code> <code class="o">+</code> <code class="mf">1e-4</code> <code class="o">*</code> <code class="n">l1_loss</code></pre>

<p>That’s all there is to it! Now let’s move on to Dropout, which is one of the most popular regularization techniques for deep neural networks.<a data-type="indexterm" data-startref="xi_1and2regularization1113505_1" id="id2693"/><a data-type="indexterm" data-startref="xi_1and2regularization1113505_1_sort" id="id2694"/><a data-type="indexterm" data-startref="xi_regularization1and2regularization1113505_1" id="id2695"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Dropout"><div class="sect2" id="id208">
<h2>Dropout</h2>

<p><em>Dropout</em><a data-type="indexterm" data-primary="dropout" id="xi_Dropout11140410_1"/><a data-type="indexterm" data-primary="regularization" data-secondary="dropout" id="xi_regularizationDropout11140410_1"/> was <a href="https://homl.info/64">proposed in a paper</a>⁠<sup><a data-type="noteref" id="id2696-marker" href="ch11.html#id2696">32</a></sup> by Geoffrey Hinton et al. in 2012 and further detailed in a <a href="https://homl.info/65">2014 paper</a>⁠<sup><a data-type="noteref" id="id2697-marker" href="ch11.html#id2697">33</a></sup> by Nitish Srivastava et al., and it has proven to be highly successful: many state-of-the-art neural networks use dropout, as it gives them a 1%–2% accuracy boost. This may not sound like a lot, but when a model already has 95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%).</p>

<p>It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability <em>p</em> of being temporarily “dropped out”, meaning it will be entirely ignored during this training step, but it may be active during the next step (see <a data-type="xref" href="#dropout_diagram">Figure 11-12</a>). The hyperparameter <em>p</em> is called the <em>dropout rate</em>,<a data-type="indexterm" data-primary="dropout rate" id="id2698"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="dropout rate (p)" id="xi_hyperparametersdropoutratep111406384_1"/> and it is typically set between 10% and 50%: closer to 20%–30% in recurrent neural nets (see <a data-type="xref" href="ch13.html#rnn_chapter">Chapter 13</a>), and closer to 40%–50% in convolutional neural networks (see <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>). After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss shortly).</p>

<p>It’s surprising at first that this destructive technique works at all. Would a company perform better if its employees were told to toss a coin every morning to decide whether to go to work? Well, who knows; perhaps it would! The company would be forced to adapt its organization; it could not rely on any single person to work the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them. The company would become much more resilient. If one person quit, it wouldn’t make much of a difference. It’s unclear whether this idea would actually work for companies, but it certainly does for neural networks. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to all of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better.</p>

<figure class="width-40"><div id="dropout_diagram" class="figure">
<img src="assets/hmls_1112.png" alt="Diagram illustrating dropout regularization in a neural network where random neurons are deactivated at each step, represented by dashed arrows and red crosses." width="689" height="870"/>
<h6><span class="label">Figure 11-12. </span>With dropout regularization, at each training iteration a random subset of all neurons in one or more layers—except the output layer—are “dropped out”; these neurons output 0 at this iteration (represented by the dashed arrows)</h6>
</div></figure>

<p>Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there are a total of 2<sup><em>N</em></sup> possible networks (where <em>N</em> is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run 10,000 training steps, you have essentially trained 10,000 different neural networks, each with just one training instance. These neural networks are obviously not independent because they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an averaging ensemble of all these smaller neural networks.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Higher layers, which learn more complex feature combinations, benefit more from dropout because they are more prone to overfitting. So you can usually apply dropout only to the neurons of the top hidden layers (e.g., one to three hidden layers). However, you should avoid dropping the output neurons, as this would be like changing the task during training: it wouldn’t help.</p>
</div>

<p>There is one small but important technical detail. Suppose <em>p</em> = 75%: on average only 25% of all neurons are active at each step during training. This means that after training, each neuron receives four times more inputs than during training, on average. This discrepancy is so large that the model is unlikely to work well. To avoid this issue, a simple solution is to multiply the inputs by 4 during training, which is the same as dividing them by 25%. More generally, we need to divide the inputs by the <em>keep probability</em> 
<span class="keep-together">(1 – <em>p</em>)</span> during training.<a data-type="indexterm" data-startref="xi_hyperparametersdropoutratep111406384_1" id="id2699"/></p>

<p>To implement dropout using PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="dropout implementation" id="id2700"/>, you can use the <code translate="no">nn.Dropout</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.Dropout" id="id2701"/> layer. It’s important to switch to training mode during training, and to evaluation mode during evaluation (just like for batch norm). In training mode, the layer randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability. In evaluation mode, it does nothing at all; it just passes the inputs to the next layer. The following code applies dropout regularization before every <code translate="no">nn.Linear</code> layer, using a dropout rate of 0.2:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">p</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">p</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">p</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">p</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training).</p>
</div>

<p>If you observe that the model is overfitting<a data-type="indexterm" data-primary="overfitting of data" data-secondary="and dropout regularization" data-secondary-sortas="dropout" id="id2702"/>, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art architectures only apply dropout to the last few hidden layers, so you may want to try this if full dropout is too strong.</p>

<p>Dropout does tend to significantly slow down convergence, but it often results in a better model when tuned properly. So it is generally well worth the extra time and effort, especially for large models.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you want to regularize a self-normalizing network based on the SELU<a data-type="indexterm" data-primary="activation functions" data-secondary="SELU" id="id2703"/><a data-type="indexterm" data-primary="alpha dropout" id="id2704"/><a data-type="indexterm" data-primary="SELU (scaled ELU) activation function" id="id2705"/> activation function (as discussed earlier), you should use <em>alpha dropout</em>: this is a variant of dropout that preserves the mean and standard deviation of its inputs. It was introduced in the same paper as SELU, as regular dropout would break self-normalization<a data-type="indexterm" data-primary="self-normalization" id="id2706"/>. PyTorch implements it in the <code translate="no">nn.AlphaDropout</code> layer.<a data-type="indexterm" data-startref="xi_Dropout11140410_1" id="id2707"/><a data-type="indexterm" data-startref="xi_regularizationDropout11140410_1" id="id2708"/></p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Monte Carlo Dropout"><div class="sect2" id="id209">
<h2>Monte Carlo Dropout</h2>

<p>In 2016<a data-type="indexterm" data-primary="MC (Monte Carlo) dropout" id="xi_MCMonteCarlodropout1114438_1"/><a data-type="indexterm" data-primary="Monte Carlo (MC) dropout" id="xi_MonteCarloMCdropout1114438_1"/><a data-type="indexterm" data-primary="regularization" data-secondary="MC Dropout" id="xi_regularizationMCDropout1114438_1"/>, a <a href="https://homl.info/mcdropout">paper</a>⁠<sup><a data-type="noteref" id="id2709-marker" href="ch11.html#id2709">34</a></sup> by Yarin Gal and Zoubin Ghahramani added a few more good reasons to use dropout:</p>

<ul>
<li>
<p>First, the paper established a profound connection between dropout networks (i.e., neural networks containing <code translate="no">Dropout</code> layers) and approximate Bayesian inference,⁠<sup><a data-type="noteref" id="id2710-marker" href="ch11.html#id2710">35</a></sup> giving dropout a solid mathematical justification.</p>
</li>
<li>
<p>Second, the authors introduced a powerful technique called <em>Monte Carlo (MC) dropout</em>, which can boost the performance of any trained dropout model without having to retrain it or even modify it at all. It also provides a much better measure of the model’s uncertainty, and it can be implemented in just a few lines of code.</p>
</li>
</ul>

<p>This description of MC dropout sounds like some “one weird trick” clickbait, so let me explain: it is just like regular dropout, except it is active not only during training, but also during evaluation. This means that the predictions are always a bit random (hence the name Monte Carlo). But instead of making a single prediction, you make many predictions and average them out. It turns out that this produces better predictions than the original model.</p>

<p>Following is a full implementation of MC dropout<a data-type="indexterm" data-primary="PyTorch" data-secondary="MC dropout" id="xi_PyTorchMCdropout11145045_1"/>, using the model we trained in the previous section to make predictions for a batch of images:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="k">for</code> <code class="n">module</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">modules</code><code class="p">():</code>
    <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">module</code><code class="p">,</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">):</code>
        <code class="n">module</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>

<code class="n">X_new</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># some new images, e.g., the first 3 images of the test set</code>
<code class="n">X_new</code> <code class="o">=</code> <code class="n">X_new</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">X_new_repeated</code> <code class="o">=</code> <code class="n">X_new</code><code class="o">.</code><code class="n">repeat_interleave</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
    <code class="n">y_logits_all</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X_new_repeated</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
    <code class="n">y_probas_all</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">y_logits_all</code><code class="p">,</code> <code class="n">dim</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">y_probas</code> <code class="o">=</code> <code class="n">y_probas_all</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p class="pagebreak-before">Let’s go through this code:</p>

<ul>
<li>
<p>First, we switch the model to evaluation mode as we always do before making predictions, but this time we immediately switch all the dropout layers back to training mode, so they will behave just like during training (i.e., randomly dropping out some of their inputs). In other words, we convert the dropout layers to MC dropout layers.</p>
</li>
<li>
<p>Next we load a new batch of images <code translate="no">X_new</code>, and we move it to the GPU. In this example, let’s assume <code translate="no">X_new</code> contains three images.</p>
</li>
<li>
<p>We then use the <code translate="no">repeat_interleave()</code><a data-type="indexterm" data-primary="torch" data-secondary="repeat_interleave()" id="id2711"/> method to create a batch containing 100 copies of each image in <code translate="no">X_new</code>. The images are repeated along the first dimension (<code translate="no">dim=0</code>) so <code translate="no">X_new_repeated</code> has a shape of <code translate="no">[300, 1, 28, 28]</code>.</p>
</li>
<li>
<p>Next, we pass this big batch to the model, which predicts 10 logits per image, as usual. This tensor’s shape is <code translate="no">[300, 10]</code>, but we reshape it to <code translate="no">[3, 100, 10]</code> to group the predictions for each image. Remember that the dropout layers are active, which means that there’s some variability across the predictions, even for copies of the same image.</p>
</li>
<li>
<p>Then we convert these logits to estimated probabilities using the softmax 
<span class="keep-together">function.</span></p>
</li>
<li>
<p>Lastly, we compute the mean over the second dimension (<code translate="no">dim=1</code>) to get the average estimated probability for each class and each image, across all 100 predictions. The result is a tensor of shape <code translate="no">[3, 10]</code>. These are our final predictions:</p>
</li>
</ul>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_probas</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">decimals</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">tensor([[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.010, 0.000, 0.990],</code>
<code class="go">        [0.990, 0.000, 0.000, 0.000, 0.000, 0.000, 0.010, 0.000, 0.000, 0.000],</code>
<code class="go">        [0.410, 0.040, 0.040, 0.230, 0.040, 0.000, 0.230, 0.000, 0.010, 0.000]],</code>
<code class="go">       device='cuda:0')</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Rather than converting the logits<a data-type="indexterm" data-primary="logits" id="id2712"/> to probabilities and then computing the mean probabilities, you may be tempted to do the reverse: first average over the logits and <em>then</em> convert the mean logits to probabilities. This is faster but it does not properly reflect the model’s uncertainty, so it tends to produce overconfident models.</p>
</div>

<p>MC dropout tends to improve the reliability of the model’s probability estimates. This means that it’s less likely to be confidently wrong, making it safer (you don’t want a self-driving car confidently ignoring a stop sign). It’s also useful when you’re interested in the top <em>k</em> classes, not just the most likely. Additionally, you can take a look at the <a href="https://xkcd.com/2110">standard deviation of each class probability</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_std</code> <code class="o">=</code> <code class="n">y_probas_all</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_std</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">decimals</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">tensor([[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.000, 0.020],</code>
<code class="go">        [0.020, 0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.000, 0.000, 0.000],</code>
<code class="go">        [0.170, 0.030, 0.030, 0.130, 0.050, 0.000, 0.090, 0.000, 0.010, 0.000]],</code>
<code class="go">       device='cuda:0')</code></pre>

<p>There’s a standard deviation of 0.02 for the probability estimate of class 9 (ankle boot) for the first image. This adds a grain of salt to the estimated probability of 99% for this class: in fact, the model is really saying “mmh, I’m guessing over 95%”. If you were building a risk-sensitive system (e.g., a medical or financial system), you may want to consider only the predictions with both a high estimated probability <em>and</em> a low standard deviation.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The number of Monte Carlo samples you use (100 in this example) is a hyperparameter<a data-type="indexterm" data-primary="hyperparameters" data-secondary="Monte Carlo samples" id="id2713"/> you can tweak. The higher it is, the more accurate the predictions and their uncertainty estimates are, but also the slower the predictions are. Moreover, above a certain number of samples, you will notice little improvement. Your job is to find the right trade-off among latency, throughput, and accuracy, depending on your application.</p>
</div>

<p>If you want to train an MC dropout model from scratch rather than reuse an existing dropout model, you should probably use a custom <code translate="no">McDropout</code> module rather than using <code translate="no">nn.Dropout</code> and hacking around with <code translate="no">train()</code> and <code translate="no">eval()</code>, as this is a bit brittle (e.g., it won’t play nicely with the evaluation function). Here is a three-line implementation:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">McDropout</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">F</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">p</code><code class="p">,</code> <code class="n">training</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>In short, MC dropout is a great technique that boosts dropout models and provides better uncertainty estimates. And of course, since it is just regular dropout during training, it also acts like a regularizer.<a data-type="indexterm" data-startref="xi_MCMonteCarlodropout1114438_1" id="id2714"/><a data-type="indexterm" data-startref="xi_MonteCarloMCdropout1114438_1" id="id2715"/><a data-type="indexterm" data-startref="xi_PyTorchMCdropout11145045_1" id="id2716"/><a data-type="indexterm" data-startref="xi_regularizationMCDropout1114438_1" id="id2717"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Max-Norm Regularization"><div class="sect2" id="id210">
<h2>Max-Norm Regularization</h2>

<p>Another<a data-type="indexterm" data-primary="max-norm regularization" id="id2718"/><a data-type="indexterm" data-primary="regularization" data-secondary="max-norm" id="id2719"/> fairly popular regularization technique for neural networks is called <em>max-norm regularization</em>: for each neuron, it constrains the weights <strong>w</strong> of the incoming connections such that ∥ <strong>w</strong> ∥<sub>2</sub> ≤ <em>r</em>, where <em>r</em> is the max-norm hyperparameter and <span class="keep-together">∥ · ∥<sub>2</sub></span> is the ℓ<sub>2</sub> norm.</p>

<p>Reducing <em>r</em> increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the unstable gradients problems (if you are not using batch-norm or layer-norm).</p>

<p>Rather than adding a regularization loss term to the overall loss function, max-norm regularization is typically implemented by computing ∥ <strong>w</strong> ∥<sub>2</sub> after each training step and rescaling <strong>w</strong> if needed (<strong>w</strong> ← <strong>w</strong> <em>r</em> / ∥ <strong>w</strong> ∥<sub>2</sub>). Here’s a common way to implement this in PyTorch:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">apply_max_norm</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">max_norm</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">epsilon</code><code class="o">=</code><code class="mf">1e-8</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">named_parameters</code><code class="p">():</code>
            <code class="k">if</code> <code class="s1">'bias'</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">name</code><code class="p">:</code>
                <code class="n">actual_norm</code> <code class="o">=</code> <code class="n">param</code><code class="o">.</code><code class="n">norm</code><code class="p">(</code><code class="n">p</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="n">dim</code><code class="p">,</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
                <code class="n">target_norm</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">clamp</code><code class="p">(</code><code class="n">actual_norm</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">max_norm</code><code class="p">)</code>
                <code class="n">param</code> <code class="o">*=</code> <code class="n">target_norm</code> <code class="o">/</code> <code class="p">(</code><code class="n">epsilon</code> <code class="o">+</code> <code class="n">actual_norm</code><code class="p">)</code></pre>

<p>This function iterates through all of the model’s weight matrices (i.e., all parameters except for the bias terms), and for each one of them it uses the <code translate="no">norm()</code> method to compute the ℓ<sub>2</sub> norm of each row (<code translate="no">dim=1</code>). A <code translate="no">nn.Linear</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.Linear" id="id2720"/> layer has weights of shape [<em>number of neurons</em>, <em>number of inputs</em>], so using <code translate="no">dim=1</code> means that we will get one norm per neuron, as desired. Then the function uses <code translate="no">torch.clamp()</code><a data-type="indexterm" data-primary="torch" data-secondary="clamp()" id="id2721"/> to compute the target norm for each neuron’s weights: this creates a copy of the <code translate="no">actual_norm</code> tensor, except that all values greater than <code translate="no">max_norm</code> are replaced by <code translate="no">max_norm</code> (this corresponds to <em>r</em> in the previous equation). Lastly, we rescale the weight matrix so that each column ends up with the target norm. Note that the smoothing term <code translate="no">epsilon</code> is used to avoid division by zero in case some columns have a norm equal to zero.</p>

<p>Next, all you need to do is call <code translate="no">apply_max_norm(model)</code> in the training loop, right after calling the optimizer’s <code translate="no">step()</code> method. And of course you probably want to fine-tune the <code translate="no">max_norm</code> hyperparameter<a data-type="indexterm" data-primary="hyperparameters" data-secondary="max_norm" id="id2722"/><a data-type="indexterm" data-primary="max_norm hyperparameter" id="id2723"/>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When using max-norm with layers other than <code translate="no">nn.Linear</code>, you may need to tweak the <code translate="no">dim</code> argument. For example, when using convolutional layers (see <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>), you generally want to set <code translate="no">dim=[1, 2, 3]</code> to limit the norm of each convolutional kernel.<a data-type="indexterm" data-startref="xi_deepneuralnetworksDNNsregularization1113455_1" id="id2724"/><a data-type="indexterm" data-startref="xi_overfittingofdataavoidingthroughregularization1113455_1" id="id2725"/></p>
</div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Practical Guidelines"><div class="sect1" id="id211">
<h1>Practical Guidelines</h1>

<p>In this chapter we have covered a wide range of techniques, and you may be wondering which ones you should use. This depends on the task, and there is no clear consensus yet, but I have found the configuration in <a data-type="xref" href="#default_deep_neural_network_config">Table 11-3</a> to work fine in most cases, without requiring much hyperparameter tuning. That said, please do not consider these defaults as hard rules!<a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="default configuration" id="id2726"/></p>
<table id="default_deep_neural_network_config">
<caption><span class="label">Table 11-3. </span>Default DNN configuration</caption>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Default value</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Kernel initializer</p></td>
<td><p>He initialization</p></td>
</tr>
<tr>
<td><p>Activation function</p></td>
<td><p>ReLU if shallow; Swish if deep</p></td>
</tr>
<tr>
<td><p>Normalization</p></td>
<td><p>None if shallow; batch-norm or layer-norm if deep</p></td>
</tr>
<tr>
<td><p>Regularization</p></td>
<td><p>Early stopping; weight decay if needed</p></td>
</tr>
<tr>
<td><p>Optimizer</p></td>
<td><p>Nesterov accelerated gradients or AdamW</p></td>
</tr>
<tr>
<td><p>Learning rate schedule</p></td>
<td><p>Performance scheduling or 1cycle</p></td>
</tr>
</tbody>
</table>

<p>You should also try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an auxiliary task if you have a lot of labeled data for a similar task.</p>

<p>While the previous guidelines should cover most cases, there are some exceptions:</p>

<ul>
<li>
<p>If you need a sparse model<a data-type="indexterm" data-primary="sparse models" id="id2727"/>, use ℓ<sub>1</sub> regularization. You can also try zeroing out the smallest weights after training (for example, using the <code>torch.​nn.utils.prune.l1_unstructured()</code> function). This will break self-normalization, so you should use the default configuration in this case.</p>
</li>
<li>
<p>If you need a low-latency model<a data-type="indexterm" data-primary="low-latency models" id="id2728"/> (one that performs lightning-fast predictions), you may need to use fewer layers; use a fast activation function such as <code translate="no">nn.ReLU</code>, <code translate="no">nn.LeakyReLU</code>, or <code translate="no">nn.Hardswish</code>; and fold the batch-norm and layer-norm layers into the previous layers after training. Having a sparse model will also help. Finally, you may want to reduce the float precision from 32 bits to 16 or even 8 bits. <a data-type="xref" href="app02.html#precision_appendix">Appendix B</a> covers several techniques to make models smaller and faster, including reduced precision models, mixed precision models, and quantization.</p>
</li>
<li>
<p>If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates.</p>
</li>
</ul>

<p>Over the last three chapters, we have learned what artificial neural nets are, how to build and train them using Scikit-Learn and PyTorch, and a variety of techniques that make it possible to train deep and complex nets. In the next chapter, all of this will come together as we dive into one of the most important applications of deep learning: computer vision.<a data-type="indexterm" data-startref="xi_deepneuralnetworksDNNs1133_1" id="id2729"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id735">
<h1>Exercises</h1>
<ol>
<li>
<p>What is the problem that Glorot initialization and He initialization aim to fix?</p>
</li>
<li>
<p>Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?</p>
</li>
<li>
<p>Is it OK to initialize the bias terms to 0?</p>
</li>
<li>
<p>In which cases would you want to use each of the activation functions we discussed in this chapter?</p>
</li>
<li>
<p>What may happen if you set the <code translate="no">momentum</code> hyperparameter too close to 1 (e.g., 0.99999) when using an <code translate="no">SGD</code> optimizer?</p>
</li>
<li>
<p>Name three ways you can produce a sparse model.</p>
</li>
<li>
<p>Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC dropout?</p>
</li>
<li>
<p>Practice training a deep neural network on the CIFAR10 image dataset:</p>
<ol>
<li>
<p>Load CIFAR10 just like you loaded the FashionMNIST dataset in <a data-type="xref" href="ch10.html#pytorch_chapter">Chapter 10</a>, but using <code translate="no">torchvision.datasets.CIFAR10</code> instead of <code translate="no">FashionMNIST</code>. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes.</p>
</li>
<li>
<p>Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function (using <code translate="no">nn.SiLU</code>). Since this is a classification task, you will need an output layer with one neuron per class.</p>
</li>
<li>
<p>Using NAdam optimization and early stopping, train the network on the CIFAR10 dataset. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.</p>
</li>
<li>
<p>Now try adding batch-norm and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it affect training speed?</p>
</li>
<li>
<p>Try replacing batch-norm with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).</p>
</li>
<li>
<p>Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC dropout.</p>
</li>
<li>
<p>Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.</p>
</li>

</ol>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id2449"><sup><a href="ch11.html#id2449-marker">1</a></sup> Xavier Glorot and Yoshua Bengio, “Understanding the Difficulty of Training Deep Feedforward Neural Networks”, <em>Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</em> (2010):  <span class="keep-together">249–256.</span></p><p data-type="footnote" id="id2451"><sup><a href="ch11.html#id2451-marker">2</a></sup> Here’s an analogy: if you set a microphone amplifier’s volume knob too close to zero, people won’t hear your voice, but if you set it too close to the max, your voice will be saturated and people won’t understand what you are saying. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude as it came in.</p><p data-type="footnote" id="id2456"><sup><a href="ch11.html#id2456-marker">3</a></sup> Kaiming He et al., “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification”, <em>Proceedings of the 2015 IEEE International Conference on Computer Vision</em> (2015): 1026–1034.</p><p data-type="footnote" id="id2457"><sup><a href="ch11.html#id2457-marker">4</a></sup> A PyTorch issue (#18182) has been open since 2019 to update the weight initialization to use the current best practices.</p><p data-type="footnote" id="id2461"><sup><a href="ch11.html#id2461-marker">5</a></sup> Andrew Saxe et al., “Exact solutions to the nonlinear dynamics of learning in deep linear neural networks”, ICLR (2014).</p><p data-type="footnote" id="id2473"><sup><a href="ch11.html#id2473-marker">6</a></sup> A dead neuron may come back to life if its inputs evolve over time and eventually return within a range where the ReLU activation function gets a positive input again. For example, this may happen if gradient descent tweaks the neurons in the layers below the dead neuron.</p><p data-type="footnote" id="id2477"><sup><a href="ch11.html#id2477-marker">7</a></sup> Bing Xu et al., “Empirical Evaluation of Rectified Activations in Convolutional Network”, arXiv preprint arXiv:1505.00853 (2015).</p><p data-type="footnote" id="id2492"><sup><a href="ch11.html#id2492-marker">8</a></sup> Djork-Arné Clevert et al., “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)”, <em>Proceedings of the International Conference on Learning Representations</em>, arXiv preprint (2015).</p><p data-type="footnote" id="id2500"><sup><a href="ch11.html#id2500-marker">9</a></sup> Günter Klambauer et al., “Self-Normalizing Neural Networks”, <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em> (2017): 972–981.</p><p data-type="footnote" id="id2505"><sup><a href="ch11.html#id2505-marker">10</a></sup> Dan Hendrycks and Kevin Gimpel, “Gaussian Error Linear Units (GELUs)”, arXiv preprint arXiv:1606.08415 (2016).</p><p data-type="footnote" id="id2506"><sup><a href="ch11.html#id2506-marker">11</a></sup> A function is convex<a data-type="indexterm" data-primary="convex functions" id="id2730"/> if the line segment between any two points on the curve never lies below the curve. A monotonic<a data-type="indexterm" data-primary="monotonic functions" id="id2731"/> function only increases, or only decreases.</p><p data-type="footnote" id="id2509"><sup><a href="ch11.html#id2509-marker">12</a></sup> Prajit Ramachandran et al., “Searching for Activation Functions”, arXiv preprint arXiv:1710.05941 (2017).</p><p data-type="footnote" id="id2519"><sup><a href="ch11.html#id2519-marker">13</a></sup> Noam Shazeer, “GLU Variants Improve Transformer”, arXiv preprint arXiv:2002.05202 (2020).</p><p data-type="footnote" id="id2520"><sup><a href="ch11.html#id2520-marker">14</a></sup> Yann Dauphin et al., “Language Modeling with Gated Convolutional Networks”, arXiv preprint arXiv:1612.08083 (2016).</p><p data-type="footnote" id="id2524"><sup><a href="ch11.html#id2524-marker">15</a></sup> Diganta Misra, “Mish: A Self Regularized Non-Monotonic Activation Function”, arXiv preprint arXiv:1908.08681 (2019).</p><p data-type="footnote" id="id2532"><sup><a href="ch11.html#id2532-marker">16</a></sup> So et al., “Primer: Searching for Efficient Transformers for Language Modeling”, arXiv preprint arXiv:2109.08668 (2021).</p><p data-type="footnote" id="id2548"><sup><a href="ch11.html#id2548-marker">17</a></sup> Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”, <em>Proceedings of the 32nd International Conference on Machine Learning</em> (2015):  <span class="keep-together">448–456.</span></p><p data-type="footnote" id="id2562"><sup><a href="ch11.html#id2562-marker">18</a></sup> Jimmy Lei Ba et al., “Layer Normalization”, arXiv preprint arXiv:1607.06450 (2016).</p><p data-type="footnote" id="id2572"><sup><a href="ch11.html#id2572-marker">19</a></sup> Razvan Pascanu et al., “On the Difficulty of Training Recurrent Neural Networks”, <em>Proceedings of the 30th International Conference on Machine Learning</em> (2013): 1310–1318.</p><p data-type="footnote" id="id2599"><sup><a href="ch11.html#id2599-marker">20</a></sup> Boris T. Polyak, “Some Methods of Speeding Up the Convergence of Iteration Methods”, <em>USSR Computational Mathematics and Mathematical Physics</em> 4, no. 5 (1964): 1–17.</p><p data-type="footnote" id="id2609"><sup><a href="ch11.html#id2609-marker">21</a></sup> Yurii Nesterov, “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence <em>O</em>(1/<em>k</em><sup>2</sup>)”, <em>Doklady AN USSR</em> 269 (1983): 543–547.</p><p data-type="footnote" id="id2615"><sup><a href="ch11.html#id2615-marker">22</a></sup> John Duchi et al., “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization”, <em>Journal of Machine Learning Research</em> 12 (2011): 2121–2159.</p><p data-type="footnote" id="id2619"><sup><a href="ch11.html#id2619-marker">23</a></sup> This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey Hinton in his Coursera class on neural networks (slides: <a href="https://homl.info/57" class="bare"><em class="hyperlink">https://homl.info/57</em></a>, video: <a href="https://homl.info/58" class="bare"><em class="hyperlink">https://homl.info/58</em></a>). Amusingly, since the authors did not write a paper to describe the algorithm, researchers often cite “slide 29 in lecture 6e” in their papers.</p><p data-type="footnote" id="id2626"><sup><a href="ch11.html#id2626-marker">24</a></sup> Diederik P. Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization”, arXiv preprint arXiv:1412.6980 (2014).</p><p data-type="footnote" id="id2644"><sup><a href="ch11.html#id2644-marker">25</a></sup> Timothy Dozat, <a href="https://homl.info/nadam">“Incorporating Nesterov Momentum into Adam”</a>, (2016).</p><p data-type="footnote" id="id2645"><sup><a href="ch11.html#id2645-marker">26</a></sup> Ilya Loshchilov, and Frank Hutter, “Decoupled Weight Decay Regularization”, arXiv preprint arXiv:1711.05101 (2017).</p><p data-type="footnote" id="id2650"><sup><a href="ch11.html#id2650-marker">27</a></sup> Ashia C. Wilson et al., “The Marginal Value of Adaptive Gradient Methods in Machine Learning”, <em>Advances in Neural Information Processing Systems</em> 30 (2017): 4148–4158.</p><p data-type="footnote" id="id2656"><sup><a href="ch11.html#id2656-marker">28</a></sup> The <em>Jacobian matrix</em> contains all the first-order partial derivatives of a function with multiple parameters and multiple outputs: one column per parameter, and one row per output. When training a neural net with gradient descent, there’s a single output—the loss—so the matrix contains a single row, and there’s one column per model parameter, so it’s a 1×<em>n</em> matrix. The <em>Hessian matrix</em> contains all the second-order derivatives of a single-output function with multiple parameters: for each model parameter it contains one row and one column, so it’s an <em>n</em>×<em>n</em> matrix. The informal names <em>Jacobians</em> and <em>Hessians</em> refer to the elements of these matrices.</p><p data-type="footnote" id="id2658"><sup><a href="ch11.html#id2658-marker">29</a></sup> V. Gupta et al., <a href="https://homl.info/shampoo">“Shampoo: Preconditioned Stochastic Tensor Optimization”</a>, arXiv preprint arXiv:1802.09568 (2018).</p><p data-type="footnote" id="id2674"><sup><a href="ch11.html#id2674-marker">30</a></sup> Ilya Loshchilov and Frank Hutter, “SGDR: Stochastic Gradient Descent With Warm Restarts”, arXiv preprint arXiv:1608.03983 (2016).</p><p data-type="footnote" id="id2677"><sup><a href="ch11.html#id2677-marker">31</a></sup> Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch Size, Momentum, and Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).</p><p data-type="footnote" id="id2696"><sup><a href="ch11.html#id2696-marker">32</a></sup> Geoffrey E. Hinton et al., “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”, arXiv preprint arXiv:1207.0580 (2012).</p><p data-type="footnote" id="id2697"><sup><a href="ch11.html#id2697-marker">33</a></sup> Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting”, <em>Journal of Machine Learning Research</em> 15 (2014): 1929–1958.</p><p data-type="footnote" id="id2709"><sup><a href="ch11.html#id2709-marker">34</a></sup> Yarin Gal and Zoubin Ghahramani, “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning”, <em>Proceedings of the 33rd International Conference on Machine Learning</em> (2016): 1050–1059.</p><p data-type="footnote" id="id2710"><sup><a href="ch11.html#id2710-marker">35</a></sup> Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian inference in a specific type of probabilistic model called a <em>deep Gaussian process</em>.<a data-type="indexterm" data-primary="deep Gaussian process" id="id2732"/></p></div></div></section></div></div></body></html>