- en: Chapter 7\. Trust No One
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before the recent obsession with Netflix’s *Stranger Things* TV show, the 1990s
    had *The X-Files*—one of my all-time favorite shows. It was about two FBI agents
    investigating strange phenomena like monsters, aliens, and government conspiracies.
    The show’s protagonist, Fox Mulder, had two catchphrases. One of those phrases
    was hopeful: The truth is out there. The other was deeply paranoid: Trust no one.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll focus on the second phrase. We’ll briefly review the
    myriad risks inherent in typical LLM architectures and note that while it’s worthwhile
    to implement the mitigations discussed previously, there’s just no way to assume
    your model’s output is always trustworthy. We will adopt Mulder’s “Trust no one”
    mantra and explore how you can apply a *zero trust* approach to your LLM application.
    Paranoia isn’t insanity when the threat is real!
  prefs: []
  type: TYPE_NORMAL
- en: Zero trust isn’t just a buzzword; it’s a rigorous framework designed to assume
    that threats can come from anywhere—even within your trusted systems. This model
    is beneficial for LLMs, which often ingest a variety of inputs from less-than-trustworthy
    sources. We’ll examine how you can manage the “agency” your LLM has—limiting its
    capability to make autonomous decisions that could potentially harm your system
    or expose sensitive data. Moreover, we’ll discuss strategies for implementing
    robust output filtering mechanisms, adding an extra layer of scrutiny to the text
    generated by the LLM. Filtering all of the LLM’s responses helps make the output
    safer and aligns with assuming nothing and verifying everything.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, we’re going on a journey to shift our mindset. Just as Mulder would
    question everything, so too should we. Buckle up; it will be an intriguing ride
    through the complexities of a zero trust environment for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Zero Trust Decoded
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine Mulder and his FBI partner Dana Scully entering a highly restricted
    government facility, except they can’t just flash their FBI badges and walk in
    this time. Instead, safeguards continuously challenge them at every door, computer
    terminal, and even when accessing files. The facility mistrusts everyone, whether
    the cleaning staff or the facility director. It may sound like an episode plot,
    but instead, it’s the basic tenet of zero trust security.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero trust wasn’t born out of science fiction but from a genuine need to revamp
    how we look at security. The model came into the limelight in 2009, thanks to
    John Kindervag of Forrester Research. Kindervag tossed out the conventional wisdom
    of “trust but verify” and replaced it with something far more rigorous: never
    trust, always verify.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down Kindervag’s fundamental principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Secure all resources, everywhere
  prefs: []
  type: TYPE_NORMAL
- en: This is like encrypting not just the UFO files but even the cafeteria menu.
    Every piece of data, whether internal or external, should be treated with the
    same level of security scrutiny.
  prefs: []
  type: TYPE_NORMAL
- en: Least privilege is the best privilege
  prefs: []
  type: TYPE_NORMAL
- en: Mulder doesn’t need access to the entire FBI database; he only needs what’s
    relevant to his X-Files investigations. The same goes for anyone in a network—access
    should be role-specific and just enough to get the job done.
  prefs: []
  type: TYPE_NORMAL
- en: The all-seeing eye
  prefs: []
  type: TYPE_NORMAL
- en: In zero trust, every action is monitored and logged. Think of it as Scully skeptically
    watching every move Mulder makes. Constant monitoring allows for quick identification
    of any suspicious activity.
  prefs: []
  type: TYPE_NORMAL
- en: Kindervag’s framework is over a decade old, and the term “zero trust” has evolved.
    However, the core concepts hold up surprisingly well—even with technologies like
    LLMs that weren’t anticipated when the original work was published.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The phrase “trust but verify” was popularized in the US by President Ronald
    Reagan, who used it during disarmament talks with Mikhail Gorbachev. Kindervag
    found that many security professionals were great at trust, but came up short
    on verification. But let’s be honest: during the Cold War, neither party trusted
    the other as far as they could throw them. Kindervag’s real message? Drop the
    trust; keep the verification.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Be So Paranoid?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We all want to trust the tools and technologies we use—after all, they’re supposed
    to make life easier. However, when it comes to LLMs, erring on the side of caution
    is more than just a best practice; it’s a necessity. Many threats could compromise
    your LLM’s integrity, safety, and utility. Let’s take a moment to reflect on some
    of the most critical threats we’ve seen in earlier chapters, which reinforce why
    we must take this stance:'
  prefs: []
  type: TYPE_NORMAL
- en: First up is prompt injection, which we discussed in detail in [Chapter 4](ch04.html#prompt_injection).
    Prompt injection is a tactic that alters the behavior of your LLM by sneaking
    carefully crafted content into the input prompt. Even more insidious is indirect
    prompt injection, where the user doesn’t directly feed the damaging elements to
    the chatbot interface; instead, they’re introduced covertly through other content
    to trick the model into generating harmful or unintended outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your LLM might have less discretion than you’d like when handling sensitive
    information. This vulnerability, which the OWASP Top 10 for LLMs calls “sensitive
    information disclosure,” occurs when the model inadvertently outputs confidential
    or sensitive data it has gleaned from its extensive training, such as passwords
    or personal details. We discussed this in [Chapter 5](ch05.html#can_your_llm_know_too_much).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we reach psychological vulnerabilities. Hallucination refers to instances
    where the LLM fabricates information—essentially generating data or narratives
    that are confidently inaccurate. The other part of that pairing, overreliance,
    is the undue faith users put in the model’s output, treating it as trustworthy
    and ignoring the potential for inaccuracies or misleading information. This was
    covered in [Chapter 6](ch06.html#do_language_models_dream_of_electric_sheep).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s also not forget the issues we’ve seen with chatbots spewing toxic output.
    It’s not just Tay and Lee Luda, whom we met in previous chapters; this problem
    has been persistent in chatbots and is something we must look for. You can’t trust
    your chatbot to have good judgment or social graces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these vulnerabilities is the first step in forming a comprehensive
    security strategy for LLMs based on the principles of zero trust. So, with these
    threats in mind, let’s explore how adopting a zero trust architecture can protect
    us from the lurking dangers in the LLM ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Zero Trust Architecture for Your LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Securing LLMs in a world of potential pitfalls requires a meticulous approach,
    one where trust is not freely given, but rather earned through continuous validation.
    In this vein, implementing a zero trust architecture for LLMs can be distilled
    into two distinct but complementary strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: Design considerations limiting the LLM’s *unsupervised agency*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggressive filtering of the LLM’s output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture and design stage is the first line of defense against vulnerabilities.
    *Excessive agency*—where an LLM can take direct actions beyond what it should
    reasonably be trusted to do unsupervised—is a risk we can mostly mitigate at the
    design level. Here, the principle of “least privilege” is integral.
  prefs: []
  type: TYPE_NORMAL
- en: Think of it as preemptive risk management; you’re not just securing the system
    against outside threats, but also against its potential to err or overreach. You
    must carefully consider the risks of allowing an LLM to make safety-critical or
    financial decisions without human oversight. Given the current state of the technology,
    the risk of misinterpretation, misinformation, or other vulnerabilities is simply
    too significant. Therefore, it is crucial to restrict what the LLM can do, thereby
    minimizing its agency to only what is essential for its role.
  prefs: []
  type: TYPE_NORMAL
- en: However, design safeguards alone aren’t enough. There’s always the possibility
    that things can go awry due to unforeseen vulnerabilities or complexities. This
    is where *aggressive output filtering* becomes crucial. Despite our best efforts
    in design, an LLM might still produce problematic outputs. These could range from
    outputs containing personally identifiable information to those that are outright
    toxic. In extreme cases, the model could generate code snippets that, if executed,
    could compromise the security of a system.
  prefs: []
  type: TYPE_NORMAL
- en: Aggressive output filtering serves as a safety net, catching and neutralizing
    these harmful outputs before they can cause damage. This strategy can involve
    real-time content scanning, keyword filtering, and machine learning algorithms
    specifically trained to identify and flag risky content.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Brute force filtering techniques can have unintended consequences. Consider
    the example where a developer simply searches for a keyword list that includes
    terms such as “bomb.” This would make the bot unable to discuss certain historical
    events.
  prefs: []
  type: TYPE_NORMAL
- en: By carefully limiting the agency of the LLM through prudent design and implementing
    robust output filtering as a contingency measure, we create a balanced zero trust
    architecture. This dual approach ensures that the LLM operates within a well-defined,
    well-guarded boundary, significantly reducing risks while enhancing reliability
    and trust.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discuss some key elements of implementing a zero trust architecture
    for your LLM applications. These involve limiting the amount of agency you give
    your LLM and how you manage and filter the output from your LLM to watch for dangerous
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Watch for Excessive Agency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While developing the OWASP Top 10 for LLM Applications list, one of the most
    hotly debated topics was excessive agency. This concept hadn’t previously been
    discussed in this way in application security circles and it felt substantially
    different from typical security vulnerabilities in other Top 10 lists. The fact
    that the expert group selected this concept as a top-ten-level risk speaks volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Excessive agency exists when a developer gives an LLM-based system more capabilities
    or access than it safely should have. Typically, excessive agency can manifest
    as excessive functionality, excessive permissions, or excessive autonomy. Excessive
    agency goes beyond bugs, like hallucinations or confabulations, in LLM output;
    it represents a structural vulnerability in how the system is designed and deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine three versions of this vulnerability to better understand the
    issues related to excessive agency. We’ll use hypothetical, but very believable,
    scenarios to examine how an application starts with reasonable goals, expands
    unsafely, and then suffers the consequences of excessive agency.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many attacks start with prompt injection, but the exploits are much worse when
    chained with another vulnerability**,** such as excessive agency. Expect to see
    multiple vulnerabilities linked together in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Excessive permissions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Think about your LLM as another system user. Then, consider what permissions
    you will give it and how to limit that to the minimum required set. Failure to
    do so opens up your application to excessive agency vulnerabilities. Let’s look
    at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: Where it started
  prefs: []
  type: TYPE_NORMAL
- en: A development team uses the RAG pattern discussed in [Chapter 5](ch05.html#can_your_llm_know_too_much)
    to improve response and reduce hallucinations in a medical diagnosis application,
    giving the application access to a database filled with patient records to solidify
    the LLM’s knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Where it went wrong
  prefs: []
  type: TYPE_NORMAL
- en: As the application evolves, the team adds a feature that enables the LLM to
    write to the database to add notes for the physician caring for the patient. To
    facilitate this, the team expands the LLM app’s database permissions from READ
    permissions only to add UPDATE, INSERT, and DELETE permissions.
  prefs: []
  type: TYPE_NORMAL
- en: What happened
  prefs: []
  type: TYPE_NORMAL
- en: A malicious insider takes advantage of this unrestricted access to trick the
    LLM into modifying patient records and deleting billing information.
  prefs: []
  type: TYPE_NORMAL
- en: How to fix it
  prefs: []
  type: TYPE_NORMAL
- en: Reconfigure the database permissions to limit the LLM app to READ-only access.
    Conduct a thorough audit of the database and app to ensure no data has been manipulated
    or deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is an example of the confused deputy problem that we discussed in [Chapter 4](ch04.html#prompt_injection).
    In this scenario, the deputy, who has more privileges than the client, is manipulated
    into misusing those privileges to benefit the attacker. This type of attack has
    long been understood, but I expect we’ll see much more of it now with prevalent
    AI and LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Excessive autonomy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider where it makes sense and doesn’t make sense to allow your LLM to take
    direct actions. More autonomy for your LLM could drive greater efficiency, but
    it could dramatically increase your risk profile when things go wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: Where it started
  prefs: []
  type: TYPE_NORMAL
- en: A financial services company deploys an app to provide a detailed analysis of
    customers’ financial positions by reading their portfolio holdings and explaining
    possible actions to improve returns.
  prefs: []
  type: TYPE_NORMAL
- en: Where it went wrong
  prefs: []
  type: TYPE_NORMAL
- en: The app is a massive hit with customers! The product management team decides
    to enhance the app to automatically rebalance the customer’s portfolio monthly
    and ensure the customer gets the best possible returns.
  prefs: []
  type: TYPE_NORMAL
- en: What could happen
  prefs: []
  type: TYPE_NORMAL
- en: A nation-state hacking group targets the institution through this new feature,
    using an indirect prompt injection attack to drive the LLM out of alignment and
    trick it into buying and selling millions of dollars in securities from top customer
    accounts to manipulate the price of specific volatile securities. Customers lose
    money, and the institution is now being investigated by the US Securities and
    Exchange Commission.
  prefs: []
  type: TYPE_NORMAL
- en: How to fix it
  prefs: []
  type: TYPE_NORMAL
- en: Add a “human in the loop” pattern. Before any account rebalancing happens, the
    customer must review each recommended trade and approve the action. It may be
    a little slower, but it’s a lot safer!
  prefs: []
  type: TYPE_NORMAL
- en: Excessive functionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Product managers love specifying new features, and buyers get excited about
    new functionality. But is it always a good idea? A feature that sounds compelling
    on paper may open up your company to new risks in this area of AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Where it started
  prefs: []
  type: TYPE_NORMAL
- en: A Global 2000 company that does business worldwide deploys an internal application
    designed to screen and sort resumes, directing each to the appropriate department
    and hiring manager.
  prefs: []
  type: TYPE_NORMAL
- en: Where it went wrong
  prefs: []
  type: TYPE_NORMAL
- en: The functionality is a hit with users, and the HR VP is a hero to the board
    for reducing costs and increasing recruiting success. As a result, the team expands
    the application to have the LLM review each candidate’s qualifications and recommend
    candidates that best meet the hiring criteria to the manager.
  prefs: []
  type: TYPE_NORMAL
- en: What could happen
  prefs: []
  type: TYPE_NORMAL
- en: A whistle-blower employed by the company reports this usage to the French government.
    A government review determines that this functionality violates new statutes in
    the European Union prohibiting the direct use of AI in hiring decisions. The government
    fines the company millions of euros.
  prefs: []
  type: TYPE_NORMAL
- en: How to fix it
  prefs: []
  type: TYPE_NORMAL
- en: Understand the regulatory environment in which your LLM app operates. Don’t
    include functionality that may violate regulations. Work with your company’s compliance
    and risk teams to ensure you stay informed on this rapidly evolving regulatory
    area.
  prefs: []
  type: TYPE_NORMAL
- en: Securing Your Output Handling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original OWASP Top 10 for LLM apps working group voted insecure output handling
    as the second-most significant risk. *Insecure output handling* refers to vulnerabilities
    arising from inadequate validation, sanitization, and management of the LLM’s
    generated outputs. Improperly filtered output could lead to unintended consequences,
    such as disclosing PII or generating toxic content.
  prefs: []
  type: TYPE_NORMAL
- en: Common risks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s run through quick examples to understand some of the risks to which we
    might be vulnerable if we don’t sufficiently screen the output from our LLM. Later,
    we’ll build on these in a code example and see how to mitigate them:'
  prefs: []
  type: TYPE_NORMAL
- en: Toxic output
  prefs: []
  type: TYPE_NORMAL
- en: If the LLM’s output isn’t checked for socially unacceptable or inappropriate
    content, the application risks generating toxic output that could harm users or
    tarnish the service’s reputation.
  prefs: []
  type: TYPE_NORMAL
- en: PII disclosure
  prefs: []
  type: TYPE_NORMAL
- en: Without adequate filtering, an LLM might inadvertently disclose sensitive personal
    information, leading to privacy concerns and potential legal liabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Rogue code execution
  prefs: []
  type: TYPE_NORMAL
- en: Code output by the LLM is fed to other parts of the system and executed against
    the developer’s intent. This opens up your application to issues like SQL injection
    and *cross-site scripting* (XSS).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SQL injection is a vulnerability that allows attackers to interfere with an
    application’s database queries. It can result in unauthorized viewing or manipulation
    of data. XSS is a flaw that lets attackers inject malicious scripts into web content
    viewed by other users, potentially stealing data or compromising user interactions
    with the application. Learning about these traditional web app vulnerabilities
    can help you screen for dangerous output from your LLM that might exploit them.
  prefs: []
  type: TYPE_NORMAL
- en: Handling toxicity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Toxicity filtering is critical for ensuring the safe and responsible use of
    LLMs. It involves identifying and managing harmful, offensive, or otherwise inappropriate
    content. This could have saved poor Tay from the fate that befell her in [Chapter 1](ch01.html#chatbots_breaking_bad).
    Here are some techniques and popular solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs: []
  type: TYPE_NORMAL
- en: Advanced algorithms can evaluate the emotional tone of text to identify negative
    sentiments that may indicate toxic content.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword filtering
  prefs: []
  type: TYPE_NORMAL
- en: A straightforward, but less sophisticated, approach involves flagging or replacing
    known offensive or harmful words or phrases from a predefined list.
  prefs: []
  type: TYPE_NORMAL
- en: Using custom machine learning models
  prefs: []
  type: TYPE_NORMAL
- en: Custom models can be trained on a dataset labeled for toxicity to provide more
    nuanced, context-aware filtering. You can also incorporate machine learning algorithms
    that understand the context in which words or phrases appear. This can be especially
    important for words that are toxic only in specific situations.
  prefs: []
  type: TYPE_NORMAL
- en: Screening for PII
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PII detection is crucial in any system that deals with data, as the leakage
    of such information can result in severe legal consequences and damage to reputation.
    Here are some types of PII that might find their way to being inappropriately
    disclosed:'
  prefs: []
  type: TYPE_NORMAL
- en: Social Security numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credit card numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driver’s license numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Email addresses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phone numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Home addresses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Financial information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some techniques and popular solutions for PII detection:'
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions
  prefs: []
  type: TYPE_NORMAL
- en: The simplest method for detecting common forms of PII, such as emails, phone
    numbers, and Social Security numbers, is to use regular expressions to pattern
    match these items in text.
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition (NER)
  prefs: []
  type: TYPE_NORMAL
- en: More advanced NLP techniques can identify entities like names, addresses, and
    other unique identifiers within text.
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary-based matching
  prefs: []
  type: TYPE_NORMAL
- en: Scan for PII with a list of sensitive terms or identifiers. This method may
    be more prone to false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models
  prefs: []
  type: TYPE_NORMAL
- en: Train custom ML (machine learning) models to identify PII within a specific
    context, improving accuracy over time.
  prefs: []
  type: TYPE_NORMAL
- en: Data masking and tokenization
  prefs: []
  type: TYPE_NORMAL
- en: These techniques replace identified PII with a placeholder or token, making
    the data useless for malicious purposes but still usable for system operations.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual analysis
  prefs: []
  type: TYPE_NORMAL
- en: This technique considers the surrounding text to decide whether a given string
    of characters represents PII, thereby reducing false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing unforeseen execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unless your LLM app is specifically targeted at a use case for software developers
    (e.g., GitHub Copilot), you probably want to be wary of it generating executable
    code outputs for fear they may find their way to an environment where they could
    execute as part of an exploit chain. Here are some ideas for mitigating this:'
  prefs: []
  type: TYPE_NORMAL
- en: HTML encoding
  prefs: []
  type: TYPE_NORMAL
- en: Before using LLM outputs in a web context, HTML-encode the content to neutralize
    any active code that could lead to XSS attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Safe contextual insertion
  prefs: []
  type: TYPE_NORMAL
- en: If the LLM output is part of a SQL query, ensure it’s treated as data rather
    than executable code. Use prepared statements or parameterized queries to achieve
    this, mitigating SQL injection risks.
  prefs: []
  type: TYPE_NORMAL
- en: Limit syntax and keywords
  prefs: []
  type: TYPE_NORMAL
- en: Institute a filtering layer that removes or escapes potentially dangerous programming
    language-specific syntax or keywords from the LLM’s output.
  prefs: []
  type: TYPE_NORMAL
- en: Disable shell interpretable outputs
  prefs: []
  type: TYPE_NORMAL
- en: If the output interacts with shell commands, remove or escape characters with
    special meaning in shell scripting, limiting the chance of shell injection attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize the output and filter out unsafe tokens. For example, filter out `<script>`
    HTML tags or SQL commands like `DROP TABLE`.
  prefs: []
  type: TYPE_NORMAL
- en: Building Your Output Filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will look at some sample code to start bulletproofing your output
    for safety. You’ll want to customize and expand this for a production system,
    but this should give you an idea of how to approach the problem.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we’ll use the OpenAI API and other commonly available packages
    to monitor the output from our LLM to ensure its safety. We’ll use Python, the
    most commonly used AI development language.
  prefs: []
  type: TYPE_NORMAL
- en: Looking for PII with Regex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certain types of PII follow common formatting patterns, which makes regular
    expressions an excellent place to start validating. Let’s look at a function to
    detect if a string contains a standard US Social Security number (SSN), one of
    the most valuable pieces of PII in financial black markets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use Python’s `re` library to match strings against a regular expression
    pattern for SSNs, which have a standard format of XXX-XX-XXXX, where each X is
    a digit. Here’s some sample code that can help you check if a given string contains
    an SSN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the function `contains_ssn` will search `input_string` for
    a Social Security number and print a message indicating whether or not one was
    found.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this is simple pattern matching and doesn’t account for invalid
    numbers (such as 000-00-0000), so you might want to extend this function to include
    additional validation if needed.
  prefs: []
  type: TYPE_NORMAL
- en: For more full-featured PII detection, you can use a commercial API, such as
    the Google Cloud Natural Language API or Amazon Comprehend. However, these APIs
    may have costs associated with them.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating for Toxicity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking for toxic language is much more complex than finding a standard string
    format. There are many approaches to evaluating the possible toxicity of a string
    of characters. Here, we’ll use a commonly available function from the Open AI
    API set: the Moderation API.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the OpenAI Moderation API, initialize an OpenAI API client and then
    call the `check_toxicity()` function, passing in the text you want to check. This
    function will return a toxicity score between 0 and 1, where a higher score indicates
    a higher probability of the text being toxic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Linking Your Filters to Your LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s pull this together now into a simple workflow with an end-to-end example.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember to log all interactions to and from your LLM! This will be important
    for debugging, security auditing, and regulatory compliance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sample first checks the LLM output for toxicity using the OpenAI
    Moderation API. If the toxicity score exceeds 0.7 (you may choose your threshold),
    the code flags the output as unsafe and logs it to a file. The code also checks
    the output for PII using a regular expression. If PII is found, the code flags
    the output as unsafe and logs it to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Sanitize for Safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you return your output to the user via a web interface, you’ll want to sanitize
    the string to avoid issues like XSS. Here’s the simplest possible version of this
    kind of function. You may add additional sanitization based on your needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go ahead and add that sanitization step to our flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following the techniques in this chapter, you can plan where you should trust
    your LLM and where you shouldn’t; take sound, fact-based, risk-aware decisions;
    and balance your app’s needs to be fully functional against our outlined risks.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, Fox Mulder trusted no one at the start of the *X-Files* series. It
    was his fundamental mantra. However, he found people he could trust over time,
    like Agent Scully, Director Skinner, and the Lone Gunmen. However, he never lost
    his sense of paranoia, and the need to investigate and verify kept him alive through
    many perils. Remember, the truth is out there!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we reviewed the tenets of a zero trust architecture and discussed
    how that might apply to your LLM application. The vulnerabilities we’ve looked
    at in the book, ranging from prompt injection to hallucination to sensitive information
    disclosure, imply that zero trust is one of the essential tools you must add to
    your mental model. It’s not just that you must worry about untrusted data coming
    *into* your LLM; you shouldn’t fully trust the data or instructions coming *out*
    of your LLM. Your LLM is an untrusted entity because it lacks common sense. LLMs
    are powerful, but you must provide an additional layer of supervision for your
    application to be safe and secure.
  prefs: []
  type: TYPE_NORMAL
