- en: 8 Chatting with your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How bringing your data benefits enterprises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and using a vector database and vector index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning and retrieving your proprietary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a vector database to conduct searches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement an end-to-end chat powered by RAG using a vector database and
    an LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefits of bringing your data and RAG jointly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How RAG benefits AI safety for enterprises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing large language models (LLMs) for a chat-with-data implementation is
    a promising strategy uniquely suitable for enterprises seeking to harness the
    power of generative artificial intelligence (AI) for their specific business requirements.
    By synergizing the LLM capabilities with enterprise-specific data sources and
    tools, businesses can forge intelligent and context-aware chatbots that deliver
    invaluable insights and recommendations to their clientele and stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, there are two ways to chat with your data using an LLM—one
    is by employing a retrieval engine as implemented using the retrieval-augmented
    generation (RAG) pattern, and another is to custom-train the LLM on your data.
    The latter is more involved and complex and not available to most users.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter builds on the RAG pattern from the last chapter used to enhance
    LLMs with your data, especially when enterprises want to implement it at the scale
    for production workloads. When enterprises integrate their data using a RAG pattern
    with LLMs, they unlock many advantages, enhancing the functionality and applicability
    of these AI systems in their unique business contexts. The chapter outlines how
    these are different and, in many cases, better than larger context windows. Let’s
    start by identifying the advantages enterprises can get when wanting to bring
    in their data.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Advantages to enterprises using their data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the dynamic realm of business technology, integrating LLMs into enterprise
    data systems heralds a transformative era of interactive and intuitive processes.
    As we explored earlier, these cutting-edge AI-driven tools are reshaping how businesses
    engage with their data, thus opening up unprecedented avenues of efficiency and
    accessibility.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have achieved impressive results in various natural language processing
    (NLP) tasks, such as answering questions, summarization, translation, and dialogue.
    However, LLMs have limitations and challenges, such as data quality, ethical problems,
    and scalability. Therefore, many enterprises are interested in implementing a
    chat with their data implementation using LLMs, which offer several advantages
    for their business goals.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of using LLMs for this purpose is that they can provide
    intelligent and context-aware chatbots that can handle customer queries and concerns
    with human-like proficiency. LLMs can understand the meaning and intent of the
    user’s input, generate relevant and coherent responses, and even take action by
    invoking APIs as needed. This improves customer satisfaction and frees human agents
    to focus on more complex tasks. Another advantage of using LLMs for chat with
    data implementation is that they can be customized with enterprise-specific data,
    which leads to more accurate and relevant AI-generated insights and recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, using LLMs for chat with data implementation can enable more efficient
    and effective data analysis. LLMs can generate natural language summaries or explanations
    of the data analysis results, which can help users understand the key findings
    and implications. In addition, LLMs can generate interactive charts or graphs
    highlighting the patterns or trends in the data. These features can enhance the
    user experience and facilitate data-driven decision-making across the organization.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 What about large context windows?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most recent models from OpenAI—for example, the GPT-4 Turbo with a 128K
    context window and Google’s newest Gemini Pro 1.5 with 1.5 million token content
    windows—have generated much enthusiasm and interest. However, a bigger context
    window alone is not enough. Training an LLM on your data has the following benefits
    over just using an LLM with a larger context window:'
  prefs: []
  type: TYPE_NORMAL
- en: '*More accurate and informative answers*—When chatting with your data, the LLM
    can access much more information than it would with a larger context window alone.
    This allows the LLM to provide more accurate and informative answers to your questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*More personalized answers*—The LLM can also learn to personalize its answers
    based on your data. For example, if you chat with an LLM that has been fine-tuned
    on your customer data, it can learn to provide more relevant answers to your specific
    customers and their needs. For example, we can use a retrieval engine to index
    its customer data and then connect the retrieval engine to an LLM. This would
    allow the company to chat with its customers in a more personalized and informative
    way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*More creative answers*—The LLM can also use your data to generate more creative
    and interesting answers to your questions. For example, if you chat with an LLM
    that has fine-tuned your product data, the LLM can learn to generate new product
    ideas or marketing campaigns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, LLMs with a larger context window have their own benefits, but they
    can be a double-edged sword with some limitations. Larger context windows allow
    us to pass in more information in one API call and worry less about chunking up
    the application. For example, the recently announced GPT-4.5 Turbo has a 128K
    context window, allowing for approximately 300 pages of text in a single prompt,
    compared to approximately 75 pages from the earlier GPT-4 32K model.
  prefs: []
  type: TYPE_NORMAL
- en: On the flip side, having a larger context window has its challenges. For example,
    larger context window LLMs can be more computationally expensive to train and
    deploy. They can also be more prone to generating hallucinations or incorrect
    answers, as large context windows increase the complexity and uncertainty of the
    model’s output. LLMs are trained on large, diverse datasets that may contain incomplete,
    contradictory, or noisy information. When the model is given a long context window,
    it must process more information and decide what to generate next, which can lead
    to errors, inconsistencies, or fabrications in the output, especially if the model
    relies on heuristics or memorization rather than reasoning or understanding.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, chatting with your data can be more efficient and less prone to
    errors, mainly because when chatting with our data, we are grounding on that data
    and steering the model to use. The LLM can access a wider range of information
    and learn to personalize its answers based on your data. Ultimately, the best
    way to choose between a larger context window LLM and chatting with your data
    will depend on your specific needs and resources.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Building a chat application using our data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will build on the RAG implementation from the last chapter and build a chat
    application that we can use to chat our data. As we saw before, vector databases
    are key for enterprises, enabling them to manage, secure, and scale embeddings
    in a production environment. For many enterprises, vector databases for semantic
    search use cases solve the performance and security requirements needed for production
    systems. Figure 8.1 shows the approach at a high level for incorporating LLM on
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Azure OpenAI on your data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For example, we will use my blog ([https://blog.desigeek.com](https://blog.desigeek.com))
    as the proprietary data source. It has posts going back 20 years across various
    topics and technologies. If for every question a user asks we go back to the blog,
    load up all the posts, create embeddings, search through those, and then use RAG
    to answer the question, the process will be very time-consuming and not scalable.
    In addition, there will be added costs, as we will be using many more tokens on
    each conversation turn or for the new set of conversations. A better approach
    would be to set the following four stages we will go through:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading and injecting the information (i.e., retrieval)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the embeddings and saving the details to Redis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searches against the saved details for a Q&A implementation using the blog posts
    (i.e., augmenting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plugging this into the LLM generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by setting up a vector database.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Using a vector database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw earlier, a vector database has been designed to operate on embedding
    vectors. For most enterprise use cases, they are a great addition to RAG implementations
    and allow us to use our data. Many vector databases are available today, and with
    the increasing popularity of LLMs and generative AI, there is more support for
    semantic search each day. Let’s see how we can implement this.
  prefs: []
  type: TYPE_NORMAL
- en: In our learning context, we want something quick and easy to set up and run,
    mainly to understand the different concepts and steps required to deploy a vector
    database for embeddings and how to integrate it into our RAG implementation. For
    this purpose, we will use Redis as a vector database and run it locally in a Docker
    container.
  prefs: []
  type: TYPE_NORMAL
- en: Redis is an open source, in-memory, key–value data store that can be used as
    a database, cache, message broker, and more. It supports data structures such
    as strings, lists, sets, hashes, and streams. Redis is fast, scalable, and reliable,
    which makes it popular for many use cases that require low latency and high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Redis expands its core capabilities using the concept of modules. Redis Search
    is a module that extends Redis with powerful text search and secondary indexing
    capabilities. It lets you create indexes on your Redis data and query them using
    a rich query language. You can also use Redis Search for vector similarity search,
    which enables semantic search based on embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to deploy Redis. For local development, the quickest
    method is to use the Redis Stack Docker container, which we will use. Redis Stack
    contains several Redis modules that, for our purpose, can be used together to
    create a fast, multimodel data store and query engine. More details on the Redis
    Stack Docker container are available at [https://hub.docker.com/r/redis/redis-stack](https://hub.docker.com/r/redis/redis-stack).
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  The main prerequisite here is that Docker should already be installed
    and configured for you to use. The details of Docker installations are outside
    the book’s scope, as there are books dedicated to Docker and its management. If
    you don’t have Docker installed, please see the documentation for installing Docker
    Desktop for a more manageable experience or, at a minimum, the Docker engine.
    More details can be found at [https://docs.docker.com/desktop/](https://docs.docker.com/desktop/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the OpenAI packages, the following prerequisites are needed
    for us to get Redis running:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker must be installed and running locally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using conda, the `redis-py` package can be installed with `conda` `install
    -c` `conda-forge` `redis-py`. If we are using pip, then use `pip` `install` `redis`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use a docker-compose file for Docker, as shown in listing 8.1\. We have
    not changed the default ports, but you can configure them as you see fit for your
    environment. In this example, we pull the latest `redis-stack` image from the
    Docker registry and expose two ports—6379 and 8001\. We also set up a data volume
    to persist the information populated in the database. And finally, we set up some
    initial health checks to check basic things, such as that the service is up and
    running and reachable at the configured ports. If you change the ports, ensure
    this is updated in the test as part of the health check.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 docker-compose file for `redis-stack`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For Docker to run, as per convention, we must ensure this file is saved as
    a `docker-compose.yml` file. You can start this by entering the following commands
    from the same location where the file is saved: `docker compose up -d`. In our
    example, the container runs via the Docker Desktop GUI, as shown in figure 8.2.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Docker Desktop running Redis container
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This also includes Redis Insight, a GUI for managing our Redis database. Once
    the Docker container runs, we can access it locally at `http://localhost:8001`.
    If everything is set up correctly, we can see the database and installed modules
    (figure 8.3).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Redis database with search running locally in a container
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we have our vector database up and running, let us work through the
    next step of retrieving the information.
  prefs: []
  type: TYPE_NORMAL
- en: Note  We use Redis as an example in this chapter, as it is relatively easy to
    run it locally in a container for enterprises to try out the concepts and get
    a handle on the associated complexities. Given that it runs locally in a container,
    it also helps alleviate any initial matters about data going into the cloud, which
    might be a concern, at least in the early days of development. In addition to
    Redis, a few other vector databases are becoming increasingly popular. Some of
    the more popular vector databases are Azure AI Search, Pinecone, and Milvus.
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI Search
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although we are using Redis running locally, enterprises that need to scale
    to a larger corpus of data, indexes, and product-scale workloads and availability
    start getting much more complex. For such scenarios, Azure AI Search is a good
    choice.
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI Search is a cloud-based service that provides various features for
    building search applications. In addition to offering a vector search, which allows
    you to find and retrieve data objects that are semantically similar to a given
    query based on their vector embeddings, it also supports hybrid search. Hybrid
    search combines full-text and vector queries that execute against a search index
    containing searchable plain text content and generated embeddings. In a single-search
    request, hybrid queries can use existing functionality, such as filtering, faceting,
    sorting, scoring profiles, and semantic ranking. The query response provides just
    one result set, using reciprocal rank fusion (RRF) to determine which matches
    are included.
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI Search offers several benefits over Redis for vector searches with
    LLMs. It is a fully managed search service that can index and search structured,
    semi-structured, and unstructured data. Azure AI Search is highly scalable and
    can easily handle large amounts of data. It supports more robust security features
    that enterprises require, such as rest and transit encryption, role-based access
    control (RBAC), and more. You can find more details at [https://learn.microsoft.com/en-us/azure/search/](https://learn.microsoft.com/en-us/azure/search/).
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Planning for retrieving the information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we must understand what we are trying to retrieve and index. This helps
    us formulate the approach and determine which pieces of information are essential
    and which are redundant and can be ignored. As part of this exercise, we also
    need to factor in the technical aspects, such as how we connect to the source
    system and any technical or practical limitations. We must also understand the
    data format and engineering requirements (including data cleaning and conversions).
  prefs: []
  type: TYPE_NORMAL
- en: Before we get the data from the blog, take a look at the details outlined in
    table 8.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Data items for blog posts we are interested in
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Data | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| URL  | The URL to the individual blog post  |'
  prefs: []
  type: TYPE_TB
- en: '| Title  | Title of the blog post  |'
  prefs: []
  type: TYPE_TB
- en: '| Description  | A couple of sentences describing what that specific blog post
    is about  |'
  prefs: []
  type: TYPE_TB
- en: '| Publish date  | Date when the post was published  |'
  prefs: []
  type: TYPE_TB
- en: '| Content  | The actual content of the blog post  |'
  prefs: []
  type: TYPE_TB
- en: Although we are using a blog post as a source system, it is a holistic example
    representing most of the RAG aspects and helping us to understand the best practices
    and how to approach them. We are retrieving the information from a remote system
    to read the blog posts. This is fundamentally similar to enterprises reading information
    for various line-of-business systems. Depending on the source system, they read
    this via APIs, exported files, or connecting to various databases and data sources.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will read all the posts using the blog’s RSS feed. RSS stands
    for really simple syndication, a standard website content distribution method
    often used to publish changes. The blog can be found at [https://blog.desigeek.com/](https://blog.desigeek.com/),
    and the corresponding RSS feed is available at [https://blog.desigeek.com/index.xml](https://blog.desigeek.com/index.xml).
  prefs: []
  type: TYPE_NORMAL
- en: First, we assume Redis runs locally in a container, as shown earlier. We will
    connect to Redis and create a new index called `posts`. The schema for the index
    is shown in the next listing and represents the structure of our data that we
    saw earlier. In addition to the main content of the blog post, we also capture
    associated metadata that will help us answer questions or understand the context
    better.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Redis index schema
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This schema contains the following types of fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TagField`—Used to store tags, which are short, descriptive keywords that can
    be employed to categorize and organize data. Tags are typically stored as a list
    of strings, and Redis search supports searching for tags with Boolean operators
    such as `AND`, `OR`, and `NOT`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TextField`—Used to store text data, such as the title, description, and content
    of a blog post. Redis search supports full-text search on `TextField`s, meaning
    you can search for words and phrases in the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VectorField`—Stores vectors’ mathematical representations of data that can
    be used to perform machine learning tasks, such as image classification and natural
    language processing. Redis search supports vector similarity search, meaning you
    can search for vectors similar to a given vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the field names are self-explanatory, except the field called `"embedding"`
    of the `VectorField` type, which is used to store high-dimensional vectors. Redis
    supports two similarity search algorithms, FLAT and HNSW; in our example, we use
    HNSW.
  prefs: []
  type: TYPE_NORMAL
- en: HSNW stands for *hierarchical navigable small world*. It’s an algorithm used
    for nearest neighbor search in multidimensional spaces and is used here as the
    embedding type. The HNSW algorithm is particularly useful for tasks such as similarity
    search or clustering in high-dimensional spaces. It is known for its efficiency
    and accuracy with lower computational overhead. HNSW organizes vectors into a
    graph structure.
  prefs: []
  type: TYPE_NORMAL
- en: FLAT stands for *fast linear approximation transformation*. It is a brute-force
    algorithm and straightforward approach in which all vectors are indexed in a single
    tree or list structure. Finding the nearest neighbors of a query point is typically
    a brute-force search implemented by computing the distance from the query point
    and other indexes. This makes it much more accurate but computationally intensive
    and slower.
  prefs: []
  type: TYPE_NORMAL
- en: The embeddings are float numbers, as denoted by FLOAT32\. We set the dimensions
    to match the Azure OpenAI models’ 1536 dimensions, which must match the LLM’s
    architecture. Finally, we use the COSINE distance metric to measure similarity.
    Redis supports the three types of distance metrics (see table 8.2).
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.2 HNSW distance metric options
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| HNSW distance metric | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EUCLIDEAN  | The straight-line distance between two points in Euclidean space.
    It’s a good choice when all dimensions are similar (e.g., all distances measured
    in meters).  |'
  prefs: []
  type: TYPE_TB
- en: '| DOTPRODUCT  | Calculates the dot product between two vectors. The dot product
    is the sum of the products of the corresponding entries of the two sequence numbers.  |'
  prefs: []
  type: TYPE_TB
- en: '| COSINE  | Calculates the cosine of the angle between two vectors. Regardless
    of their magnitude, it measures how similar the vectors are. This is often used
    in text analysis, where the direction of the vector (the angle) is more important
    than the length of the vector.  |'
  prefs: []
  type: TYPE_TB
- en: TagField vs. TextField
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `URL` field is a `TagField` instead of a `TextField`. While this might seem
    odd at first, there is a good reason. With `TagField`, the entire URL is treated
    as a single tag. This property is useful if you want to search for documents using
    the exact URL. However, searching for documents containing certain words in their
    URL would be useless because the URL is not tokenized.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if the URL field were defined as a `TextField`, it would be tokenized,
    and each part of the URL would be indexed separately. This would be useful if
    you searched for documents containing certain words in their URL. However, it
    would not be useful if you wanted to search for documents by exact URL because
    the URL would be tokenized.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, if we ran a search that required tokenization (i.e., searching
    for documents that contain a certain word in their URL), the search would not
    return the expected results. Similarly, if you define the `URL` as a `TextField`
    and then try to perform a search that requires exact matching (i.e., searching
    for documents by exact URL), the search will not return the expected results.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the data that we need and the associated schema, let’s
    create the index to begin within Redis. We start by connecting to the Redis database,
    which, in our case, is running locally on Docker and reachable over port 6379,
    as shown in listing 8.3.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need the following environment variables pointing to the server host, the
    port, and the password to set, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: For Windows, use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note  You must restart your terminal to read the new variables.
  prefs: []
  type: TYPE_NORMAL
- en: On Linux/Mac, use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We first need to establish a connection with the Redis server, which is quite
    straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we already have our schema defined, as shown in listing 8.2, creating
    a vector index is straightforward. We call the function `create_index` and pass
    it a name, schema, and optional prefix. Only two indexes are supported—`HASH`
    (the default) or `JSON`—for which we need a separate module. In our case, we will
    use the default `HASH`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Of course, we can delete the index and view its details. The full code for this
    helper function is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Redis search index operations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Redis connection details'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Connects to the Redis server'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets the dimensions to match the LLM design'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Function to delete index'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Function to delete the keys from the index'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Function to create an index'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Function to run the main loop'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 shows this code running locally as an example. The index type is
    HASH, and the keys’ prefix starts with “post.”
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F04_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 Redis Insight running locally as an example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our case, we already have the index populated, and when we execute this to
    see the index, we obtain an output similar to the following listing. Note that
    the output has been truncated for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4 Redis search index details
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Figure 8.5 shows the details of one of the index items using Redis Insight—the
    GUI that allows us to do some basic database management. We can see the fields
    we identified when setting up the index. The embeddings are a binary representation,
    so they appear to be gibberish.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F05_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 Index details
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we have an index set, let’s see how we can retrieve the data (i.e.,
    the blog posts), chunk it, populate the vector database, and finally update the
    index we created.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Retrieving the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, the process is quite simple. We start loading RSS feeds using
    the `feedparser` library; then, we retrieve each blog post found, parse it for
    the content we are interested in, create the corresponding embedding, and save
    all the details in Redis. Listing 8.5 shows this flow.
  prefs: []
  type: TYPE_NORMAL
- en: Because each blog post is an HTML page, we use `BeautifulSoup`, a Python library,
    to parse the HTML page, allowing us to select the content we need. As shown in
    listing 8.5, we need to clean up some things and parse the content by matching
    the style of the blog post and the HTML generated. The search for various attributes
    and classes (such as `post-title`, etc.) depends on the shape of the incoming
    data and the use case we are trying to solve. In this example, the code must be
    updated if the blog changes its theme or rendering.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.5 Extracting content from HTML
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For real-world enterprise use cases, the retriever must be aware of the source
    system’s content and structure, which can be quite complex and daunting. In most
    cases, this would need to run through a data pipeline. This data pipeline would
    help address any data engineering aspects needed—all in the context of the associated
    use cases. See section 8.4.1 for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We create a new index hash, adding details of the information we are interested
    in as embeddings—URL, title, publish date, and blog post. We also correlate the
    different chunks that are created with the same context.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we only show the key aspect of the code in the next listing, and for
    severity, we avoid the helper functions we have seen before. The complete code
    samples are in the book’s GitHub code repository ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.6 Retrieving blog posts and saving them in Redis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once we get the blog post’s content, we need to chunk it up, as discussed in
    the previous chapter. For this example, we use spaCy to chunk the blog post and
    also have some overlap between different chunks.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Retriever pipeline best practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When implementing a RAG pattern, it’s crucial to have a deep understanding of
    the source system’s content and structure. The success of a RAG model hinges on
    its ability to access and interpret the right data, which necessitates a well-architected
    data pipeline. This pipeline is not just a conduit for data flow, but a sophisticated
    framework that ensures data is extracted, transformed, indexed, and stored to
    align with the model’s requirements and the defined use case.
  prefs: []
  type: TYPE_NORMAL
- en: The first step toward implementing GPTs and LLMs in enterprises is a deep understanding
    of the source system. This involves thoroughly analyzing the data structure, including
    entity-relationship diagrams, data types, and data distribution. Data profiling
    tools can be instrumental in understanding the nature of the content.
  prefs: []
  type: TYPE_NORMAL
- en: Note  For RAG to work well, it is important to carefully plan the preprocessing
    one needs to do in the retriever pipeline and not just use everything without
    considering whether it is better. If not planned well, this will create problems
    when using search as part of a RAG implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The next phase defines the use case, which entails creating a detailed requirement
    document outlining the problem, potential solutions, expected results, and success
    metrics. This document should also detail the users’ informational needs and the
    scenarios in which the RAG model will be applied.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, the focus shifts to data extraction and transformation. This
    process involves using ETL (extract, transform, load) tools to extract data from
    the source system and transform it into a format the RAG model can understand.
    It may involve NLP techniques such as tokenization, stop-word removal, and lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been transformed, it needs to be indexed for efficient retrieval.
    Azure AI Search, Elasticsearch, Solr, and Lucene are ideal for this purpose, as
    they provide full-text search capabilities and can handle large datasets effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel to data indexing, selecting a suitable data storage solution is important.
    Depending on the specific needs of the data size, speed, and type, this could
    be a traditional SQL database, a NoSQL database such as Cosmos DB, or a distributed
    file system such as Hadoop HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most critical phases is preprocessing planning. This involves careful
    planning of preprocessing steps, which could involve techniques such as noise
    removal, normalization, and dimensionality reduction. The goal is to retain information
    relevant to the use case while reducing the model’s complexity.
  prefs: []
  type: TYPE_NORMAL
- en: The next phase is model integration, which involves using APIs or SDKs provided
    by the AI model vendor to integrate the RAG model into the application. The retriever
    must be configured with the correct query parameters, and the generator should
    be set up with the desired output structure.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning and monitoring are crucial for enhancing the model’s performance
    and ensuring the system’s health. This involves using a validation dataset for
    fine-tuning and application performance management (APM) tools for monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding scalability and reliability, cloud platforms such as AWS, Google Cloud,
    or Azure should be used to scale the system as needed. Containerization platforms
    such as Docker and Kubernetes can assist in scaling and managing the application.
    Redundancy and failover strategies are crucial to ensuring system reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, security and compliance cannot be overlooked. Implementing data
    encryption, user authentication, access control, and regular system audits can
    ensure data security and compliance with data protection regulations such as GDPR
    or CCPA.
  prefs: []
  type: TYPE_NORMAL
- en: Before deployment, rigorous testing and validation are imperative to ensure
    that the pipeline and the RAG model meet the expectations outlined by the use
    case. Once the system is live, comprehensive documentation and technical training
    should be provided to the team for effective management, maintenance, and troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it’s crucial to ensure the quality control of the retrieval corpus,
    implement measures for information security and privacy, regularly update the
    retrieval corpus, and efficiently allocate resources. By following these steps,
    enterprises can effectively build and maintain AI-powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Search using Redis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the data ingested and the index ready, we can search against
    it. We create a simple console app that accepts a user’s query, vectorizes it,
    and searches based on the top three similar posts to return to the user. This
    is a semantic search. The following listing shows the output generated as an example
    when we ask about “Longhorn.”
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.7 Search results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note  Windows Longhorn used to be the codename for the operating system that
    eventually became Windows Vista.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s check out the code for implementing the search using Redis. We first take
    a user query such as “Tell me about Longhorn,” create a vector, and use cosine
    similarity to obtain a list of comparable results.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.8 Searching using Redis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A base query that prefilters fields and is implemented as a KNN search'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Selects the different fields we are interested in searching'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sorts by cosine similarity in descending order'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Executes the query'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Captures the query from the user'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Vectorizes the input'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Converts the vector to a NumPy array'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Performs the similarity search'
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, the `hybrid_search()` function does the heavy lifting
    of running the hybrid search query. A hybrid search query combines multiple types
    of searches into a single query. This can include combining text-based searches
    with other types, such as numerical, categorical, or even vector-based searches.
    Note that the exact search type would depend on the information and the requirement.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we combine a K-Nearest Neighbors (KNN) search on an embedding
    vector with other search fields. The KNN search finds the most related items to
    a given item, in this case, the most similar posts to a given query vector. The
    query results are sorted by vector score, which means a high to low ordering based
    on cosine similarity. In other words, the results with the highest similarity
    are shown first. We also restrict this to the top three items, as depicted by
    the `top_k` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the exact nature of the search and type also depends on the search
    engine and the data type. For more details on Redis search types and KNN, see
    the documentation at [https://mng.bz/o0Gp](https://mng.bz/o0Gp).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen the search, let’s combine all the dimensions and integrate
    them into a chat experience using an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 An end-to-end chat implementation powered by RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this and the previous chapter, we have discussed and examined all
    the pieces to help us understand some of the core concepts; now, we can bring
    it all together and build an end-to-end chat application. In the application,
    we can ask questions to get details about our data (i.e., the blog posts). Figure
    8.6 shows the application flow.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F06_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 End-to-end chat application
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The question the user asks first gets converted into embeddings and then searched
    in Redis using a hybrid search index to find similar chunks, which are returned
    as search results. As we saw earlier, the blog posts have already been injected
    into the Redis database and indexed. Once we have the results, we formulate the
    LLM prompt by combining the original questions and the chunks retrieved to answer
    from. These are passed into the prompt itself before finally calling the LLM to
    generate a response.
  prefs: []
  type: TYPE_NORMAL
- en: On the search front, we deployed Redis running locally and created a vector
    index. We read all the blog posts going back nearly 20 years. We created the relevant
    chunks for these posts and their corresponding embeddings and populated our vector
    database. We also implemented a vector search on those embeddings. The only piece
    left is to integrate all of this into our application and hook it up with an LLM
    to complete the last stage of our RAG implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 shows exactly how to do this. Several helper functions, such as
    `get_ search_results()`, take the user’s query, call another helper function to
    search Redis, and return any results found. The actual API call that calls the
    GPT is in the `ask_gpt()` function, and it is a `ChatCompletion()` API, just like
    we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: As with previous examples, we leave out the code’s helper functions and other
    aspects for brevity. The complete code samples are available in the GitHub code
    repository accompanying the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 End-to-end RAG-powered chat
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Vectorizes the query'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Converts the vector to a numpy array'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Performs the similarity search'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Manages token budget'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Loops through the results while still keeping within the token budget'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Runs a vector search to get embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Sets up the chat completion calls'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Calls the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: We can see all this coming together when we run it and chat with the blog. It
    understands the query, creates embeddings, uses the vector database and the associated
    vector indexes to retrieve the top five matching results, adds that to the prompt,
    and uses the LLM to generate the response (figure 8.7).
  prefs: []
  type: TYPE_NORMAL
- en: In the example we have seen thus far, we are responsible for everything—from
    setting up the Docker containers to deploying Redis and ingesting the data. This
    is not enough for enterprises to go into production. More system engineering is
    required, such as setting up various clusters of machines, scaling them up or
    down as needed, managing Redis, security requirements, overall operations, and
    so forth. This takes a significant amount of time, effort, cost, and skills that
    not every organization might have. Another option is to use Azure OpenAI, which
    can do much of this out of the box and allows organizations a quicker time to
    market, potentially at a lower cost. Let’s see how Azure OpenAI can achieve the
    same result but much faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F07_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 Q&A using blog data with GPT-3.5 Turbo
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 8.7 Using Azure OpenAI on your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many enterprises use Azure, and incorporating Azure OpenAI as part of their
    data strategy represents a pivotal step in employing the power of generative AI
    for business transformation. Azure OpenAI provides an enterprise-grade platform
    to integrate advanced AI models such as ChatGPT into your data workflows.
  prefs: []
  type: TYPE_NORMAL
- en: '“Azure OpenAI on your data” is the service that enables running these powerful
    chat models on your data and getting out-of-the-box features that enterprises
    require for production workloads: scalability, security, refreshes, and integration
    into others. You can connect your data source using Azure OpenAI Studio (figure
    8.8) or the REST API.'
  prefs: []
  type: TYPE_NORMAL
- en: Note Azure AI Studio is a platform that combines capabilities across multiple
    Azure AI services. It is designed for developers to build generative AI applications
    on an enterprise-grade platform. You can first interact with a project code via
    the Azure AI SDK and Azure AI CLI and seamlessly explore, build, test, and deploy
    using cutting-edge AI tools and ML models.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of Azure OpenAI’s appeal is its seamless integration with the broader
    Azure ecosystem. Connecting these powerful AI models to your data repositories
    unlocks the potential for more sophisticated data analysis, natural language processing,
    and predictive insights. This integration is particularly beneficial for enterprises
    with a significant footprint in Azure, enabling them to enhance their existing
    infrastructure with minimal disruption.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F08_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 Adding your data to Azure OpenAI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Azure AI Studio supports multiple options from existing Azure AI Search indexes,
    Blob storage, Cosmos DB, and so forth. One of these options is a URL, which we
    will use to ingest blog posts (see figure 8.9). We can also save the RSS feed
    locally and upload it as a file. One of the advantages of using our own Azure
    AI Search index is that it does the heavy lifting of keeping the data ingestion
    up to date from the source systems. This replaces Redis and can be globally distributed
    to a cloud-scale if required.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F09_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9 Azure AI Studio: Adding a data source'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can configure and set up most things here, including a storage resource where
    this data will be saved, an Azure AI Search resource, the index details, embedding
    details, and so forth (see figure 8.10). With a few clicks, all of this is set
    up and ready for us to use.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH08_F10_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 Configure details for data ingestion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'On the information security front, this process is streamlined by Azure’s robust
    security and compliance framework, ensuring that your data remains protected throughout
    its interaction with AI models. Azure OpenAI supports two key features on your
    data: role-based and document-level access controls. This feature, working alongside
    Azure AI Search security filters, can be used to limit access to only those users
    who should have access based on their permitted groups and LDAP memberships, which
    is a critical requirement for many enterprises, especially in regulated industries.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Azure’s ability to process and analyze large cloud-scale volumes of
    unstructured data scalability is another significant advantage. For example, OpenAI’s
    ChatGPT internally uses Azure AI Search, and that workload is 100+ million users
    per day. Azure’s cloud infrastructure allows for the easy scaling of AI capabilities
    as your data needs grow. More details on Azure OpenAI can be found at [https://mng.bz/n022](https://mng.bz/n022).
  prefs: []
  type: TYPE_NORMAL
- en: 8.8 Benefits of bringing your data using RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enterprises often struggle to extract meaningful insights from unstructured
    data sources such as emails, customer feedback, or social media interactions.
    When enterprises integrate their data using RAG in LLMs, they unlock many advantages,
    enhancing the functionality and applicability of these AI systems in their unique
    business contexts.
  prefs: []
  type: TYPE_NORMAL
- en: This feature offers distinct advantages over merely expanding the context window
    of these models. The pattern enhances the relevance and accuracy of LLM outputs
    and provides strategic benefits that a larger context window alone cannot match.
    LLMs can analyze this data, interpret it in a human-like manner, and provide actionable
    insights, all in a fraction of the time it would take using traditional methods.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating RAG with real-time enterprise data ensures that the information
    retrieved and included in responses is relevant and current, a critical factor
    in rapidly evolving industries. This customization leads to more precise and applicable
    answers, which is especially beneficial for sectors with specialized knowledge,
    such as legal, medical, or technical fields.
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of using enterprise-specific data in conjunction with RAG
    models lies in the tailored accuracy and applicability of the model’s responses.
    LLMs with a larger context window can process more information in a single instance,
    but they may still lack the depth of knowledge in specialized domains. When enterprises
    introduce their data, the LLMs can generate responses intricately aligned with
    the organization’s specific industry, jargon, and operational intricacies. This
    specificity is crucial for industries where specialized knowledge is paramount
    and goes beyond the scope of what a larger context window can provide.
  prefs: []
  type: TYPE_NORMAL
- en: While a larger context window allows for a broader range of preexisting information
    to be considered in the model’s responses, it does not necessarily incorporate
    the most current or enterprise-specific data. In addition, the larger the context
    window, the more the model has to process and the slower it is.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, integrating proprietary data enhances decision-making processes
    more effectively than simply expanding the context window. This integration enables
    LLMs to offer insights and analysis deeply rooted in the enterprise’s historical
    data and strategic objectives. In contrast, a larger context window might provide
    broader information but lacks precision and direct relevance to enterprises’ strategic
    questions and challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding data security and privacy, bringing proprietary data under enterprise
    control is more manageable than relying on public or generalized data that a larger
    context window might access. By controlling data inputs, enterprises can more
    effectively ensure compliance with data privacy regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RAG with your data offers significant advantages for AI safety
    in enterprise environments, primarily by enhancing the accuracy and reliability
    of information. This fusion of generative capabilities of LLMs with a comprehensive
    corpus of data allows the model to access up-to-date, factual data, crucial for
    enterprises dealing with time-sensitive and accuracy-critical information. Moreover,
    by retrieving from a diverse set of sources, RAG can mitigate biases inherent
    in the training data of LLMs, a vital feature for making unbiased, data-driven
    decisions. Enterprises can customize the retrieval corpus, ensuring alignment
    with industry regulations and internal policies. Furthermore, incorporating the
    latest information and providing sources for generated content offers improved
    transparency and decision-making support.
  prefs: []
  type: TYPE_NORMAL
- en: While expanding the context window of LLMs offers certain benefits, integrating
    proprietary data with RAG models provides specificity, current relevance, strategic
    alignment, personalization, data security, and innovation potential that a mere
    increase in the context window cannot match. This approach enables enterprises
    to use LLMs more effectively for their unique business needs and objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The benefits of integrating proprietary data with RAG models are enhancing AI
    systems’ specificity, relevance, strategic alignment, personalization, data security,
    and innovation potential.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using proprietary data over merely expanding the context window of LLMs offers
    multiple advantages, as the former provides more accurate, relevant, and personalized
    answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a production environment, using a vector database and vector index to manage,
    secure, and scale embeddings is crucial for performance and cost reasons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process of retrieving proprietary data, chunking it, creating embeddings,
    and saving the details in a vector database depends on the shape of the data at
    hand. It can require significant planning and data engineering effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration of a RAG pattern with a source system is complex, requiring planning,
    robust engineering, and an understanding of the data structure details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An end-to-end application using RAG, prompt engineering, embeddings, and search
    can be very powerful for organizations. Still, it is also complex, and if not
    designed properly, it will slow things down when deploying to production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chapter highlights how to conduct search using a vector database, retrieving
    the most similar items to a given item based on their vector embeddings. It also
    shows how incorporating the vector databases and RAG is key for implementing an
    end-to-end chat application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Azure OpenAI on your data” is a PaaS service that enables enterprises to run
    AI models on their data with out-of-the-box features such as scalability, security,
    and integration into other Azure services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
