- en: Chapter 1\. The Machine Learning Landscape
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 机器学习的景观
- en: 'Not so long ago, if you had picked up your phone and asked it the way home,
    it would have ignored you—and people would have questioned your sanity. But machine
    learning is no longer science fiction: billions of people use it every day. And
    the truth is it has actually been around for decades in some specialized applications,
    such as optical character recognition (OCR). The first ML application that really
    became mainstream, improving the lives of hundreds of millions of people, took
    over the world back in the 1990s: the *spam filter*. It’s not exactly a self-aware
    robot, but it does technically qualify as machine learning: it has actually learned
    so well that you seldom need to flag an email as spam anymore. It was followed
    by hundreds of ML applications that now quietly power hundreds of products and
    features that you use regularly: voice prompts, automatic translation, image search,
    product recommendations, and many more.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 不久前，如果您拿起手机问路回家，它会无视您，人们会质疑您的理智。但是机器学习不再是科幻：数十亿人每天都在使用它。事实上，它实际上已经存在几十年，用于一些专业应用，比如光学字符识别（OCR）。第一个真正成为主流的机器学习应用是在上世纪90年代席卷全球的：*垃圾邮件过滤器*。它并不是一个自我意识的机器人，但从技术上讲它确实算是机器学习：它学习得如此出色，以至于您很少需要将电子邮件标记为垃圾邮件。它之后又出现了数百个机器学习应用，现在悄悄地支持着您经常使用的数百种产品和功能：语音提示、自动翻译、图像搜索、产品推荐等等。
- en: Where does machine learning start and where does it end? What exactly does it
    mean for a machine to *learn* something? If I download a copy of all Wikipedia
    articles, has my computer really learned something? Is it suddenly smarter? In
    this chapter I will start by clarifying what machine learning is and why you may
    want to use it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习从何开始，到何结束？机器*学习*某事究竟意味着什么？如果我下载了所有维基百科文章的副本，我的计算机真的学到了什么吗？它突然变聪明了吗？在本章中，我将首先澄清机器学习是什么，以及为什么您可能想要使用它。
- en: 'Then, before we set out to explore the machine learning continent, we will
    take a look at the map and learn about the main regions and the most notable landmarks:
    supervised versus unsupervised learning and their variants, online versus batch
    learning, instance-based versus model-based learning. Then we will look at the
    workflow of a typical ML project, discuss the main challenges you may face, and
    cover how to evaluate and fine-tune a machine learning system.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在我们开始探索机器学习大陆之前，我们将看一看地图，了解主要区域和最显著的地标：监督学习与无监督学习及其变体，在线学习与批量学习，基于实例与基于模型的学习。然后我们将看一看典型机器学习项目的工作流程，讨论您可能面临的主要挑战，并介绍如何评估和微调机器学习系统。
- en: This chapter introduces a lot of fundamental concepts (and jargon) that every
    data scientist should know by heart. It will be a high-level overview (it’s the
    only chapter without much code), all rather simple, but my goal is to ensure everything
    is crystal clear to you before we continue on to the rest of the book. So grab
    a coffee and let’s get started!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了许多数据科学家应该牢记的基本概念（和行话）。这将是一个高层次的概述（这是唯一一个没有太多代码的章节），都相当简单，但我的目标是确保在继续阅读本书的其余部分之前，一切对您都是清晰的。所以泡杯咖啡，让我们开始吧！
- en: Tip
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are already familiar with machine learning basics, you may want to skip
    directly to [Chapter 2](ch02.html#project_chapter). If you are not sure, try to
    answer all the questions listed at the end of the chapter before moving on.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经熟悉机器学习基础知识，您可能想直接跳到[第2章](ch02.html#project_chapter)。如果您不确定，尝试在继续之前回答本章末尾列出的所有问题。
- en: What Is Machine Learning?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Machine learning is the science (and art) of programming computers so they can
    *learn from data*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是编程计算机以便它们可以*从数据中学习*的科学（和艺术）。
- en: 'Here is a slightly more general definition:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个稍微更一般的定义：
- en: '[Machine learning is the] field of study that gives computers the ability to
    learn without being explicitly programmed.'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[机器学习是]一门研究领域，赋予计算机学习的能力，而无需明确编程。'
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Arthur Samuel, 1959
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 亚瑟·塞缪尔，1959
- en: 'And a more engineering-oriented one:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个更加工程导向的定义：
- en: A computer program is said to learn from experience *E* with respect to some
    task *T* and some performance measure *P*, if its performance on *T*, as measured
    by *P*, improves with experience *E*.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果一个计算机程序在某个任务*T*上通过经验*E*，根据性能度量*P*的表现随着经验*E*的增加而改善，那么就说它从经验*E*中学习。
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tom Mitchell, 1997
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 汤姆·米切尔，1997
- en: Your spam filter is a machine learning program that, given examples of spam
    emails (flagged by users) and examples of regular emails (nonspam, also called
    “ham”), can learn to flag spam. The examples that the system uses to learn are
    called the *training set*. Each training example is called a *training instance*
    (or *sample*). The part of a machine learning system that learns and makes predictions
    is called a *model*. Neural networks and random forests are examples of models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您的垃圾邮件过滤器是一个机器学习程序，通过用户标记的垃圾邮件示例和常规邮件示例（非垃圾邮件，也称为“正常邮件”），可以学习如何标记垃圾邮件。系统用于学习的示例称为*训练集*。每个训练示例称为*训练实例*（或*样本*）。学习和预测的机器学习系统的部分称为*模型*。神经网络和随机森林是模型的示例。
- en: In this case, the task *T* is to flag spam for new emails, the experience *E*
    is the *training data*, and the performance measure *P* needs to be defined; for
    example, you can use the ratio of correctly classified emails. This particular
    performance measure is called *accuracy*, and it is often used in classification
    tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，任务*T*是为新邮件标记垃圾邮件，经验*E*是*训练数据*，性能度量*P*需要定义；例如，您可以使用正确分类的邮件比率。这种特定的性能度量称为*准确度*，在分类任务中经常使用。
- en: If you just download a copy of all Wikipedia articles, your computer has a lot
    more data, but it is not suddenly better at any task. This is not machine learning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只是下载了所有维基百科文章的副本，您的计算机拥有了更多数据，但它并不会突然在任何任务上变得更好。这不是机器学习。
- en: Why Use Machine Learning?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用机器学习？
- en: 'Consider how you would write a spam filter using traditional programming techniques
    ([Figure 1-1](#traditional_approach_diagram)):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑如何使用传统编程技术编写垃圾邮件过滤器（[图1-1](#traditional_approach_diagram)）：
- en: First you would examine what spam typically looks like. You might notice that
    some words or phrases (such as “4U”, “credit card”, “free”, and “amazing”) tend
    to come up a lot in the subject line. Perhaps you would also notice a few other
    patterns in the sender’s name, the email’s body, and other parts of the email.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您会检查垃圾邮件通常是什么样子的。您可能会注意到一些单词或短语（如“4U”、“信用卡”、“免费”和“惊人”）在主题行中经常出现。也许您还会注意到发件人姓名、电子邮件正文和其他部分中的一些其他模式。
- en: You would write a detection algorithm for each of the patterns that you noticed,
    and your program would flag emails as spam if a number of these patterns were
    detected.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会为你注意到的每个模式编写一个检测算法，如果检测到这些模式中的一些，你的程序将会标记电子邮件为垃圾邮件。
- en: You would test your program and repeat steps 1 and 2 until it was good enough
    to launch.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您会测试您的程序，并重复步骤1和2，直到它足够好以启动。
- en: '![mls3 0101](assets/mls3_0101.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0101](assets/mls3_0101.png)'
- en: Figure 1-1\. The traditional approach
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 传统方法
- en: Since the problem is difficult, your program will likely become a long list
    of complex rules—pretty hard to maintain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题很难，您的程序很可能会变成一长串复杂规则——相当难以维护。
- en: In contrast, a spam filter based on machine learning techniques automatically
    learns which words and phrases are good predictors of spam by detecting unusually
    frequent patterns of words in the spam examples compared to the ham examples ([Figure 1-2](#ml_approach_diagram)).
    The program is much shorter, easier to maintain, and most likely more accurate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于机器学习技术的垃圾邮件过滤器会自动学习哪些单词和短语是垃圾邮件的良好预测器，通过检测垃圾邮件示例中单词的异常频繁模式与正常邮件示例进行比较（[图1-2](#ml_approach_diagram)）。该程序更短，更易于维护，而且很可能更准确。
- en: '![mls3 0102](assets/mls3_0102.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0102](assets/mls3_0102.png)'
- en: Figure 1-2\. The machine learning approach
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 机器学习方法
- en: What if spammers notice that all their emails containing “4U” are blocked? They
    might start writing “For U” instead. A spam filter using traditional programming
    techniques would need to be updated to flag “For U” emails. If spammers keep working
    around your spam filter, you will need to keep writing new rules forever.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果垃圾邮件发送者注意到他们所有包含“4U”的电子邮件都被阻止了呢？他们可能会开始写“For U”代替。使用传统编程技术的垃圾邮件过滤器需要更新以标记“For
    U”电子邮件。如果垃圾邮件发送者不断绕过您的垃圾邮件过滤器，您将需要永远编写新规则。
- en: In contrast, a spam filter based on machine learning techniques automatically
    notices that “For U” has become unusually frequent in spam flagged by users, and
    it starts flagging them without your intervention ([Figure 1-3](#adapting_to_change_diagram)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于机器学习技术的垃圾邮件过滤器会自动注意到“For U”在用户标记的垃圾邮件中变得异常频繁，并开始在没有您干预的情况下标记它们（[图1-3](#adapting_to_change_diagram)）。
- en: '![mls3 0103](assets/mls3_0103.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0103](assets/mls3_0103.png)'
- en: Figure 1-3\. Automatically adapting to change
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 自动适应变化
- en: Another area where machine learning shines is for problems that either are too
    complex for traditional approaches or have no known algorithm. For example, consider
    speech recognition. Say you want to start simple and write a program capable of
    distinguishing the words “one” and “two”. You might notice that the word “two”
    starts with a high-pitch sound (“T”), so you could hardcode an algorithm that
    measures high-pitch sound intensity and use that to distinguish ones and twos⁠—but
    obviously this technique will not scale to thousands of words spoken by millions
    of very different people in noisy environments and in dozens of languages. The
    best solution (at least today) is to write an algorithm that learns by itself,
    given many example recordings for each word.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习表现出色的另一个领域是对于那些对传统方法来说要么太复杂，要么没有已知算法的问题。例如，考虑语音识别。假设你想从简单开始，编写一个能够区分“one”和“two”这两个词的程序。你可能会注意到，“two”这个词以高音（“T”）开头，因此你可以硬编码一个算法来测量高音强度，并使用它来区分“one”和“two”——但显然，这种技术无法扩展到成千上万个单词，由数百万个非常不同的人在嘈杂环境中以及数十种语言中说出。最好的解决方案（至少是今天）是编写一个算法，通过给定每个单词的许多示例录音来自学习。
- en: Finally, machine learning can help humans learn ([Figure 1-4](#data_mining_diagram)).
    ML models can be inspected to see what they have learned (although for some models
    this can be tricky). For instance, once a spam filter has been trained on enough
    spam, it can easily be inspected to reveal the list of words and combinations
    of words that it believes are the best predictors of spam. Sometimes this will
    reveal unsuspected correlations or new trends, and thereby lead to a better understanding
    of the problem. Digging into large amounts of data to discover hidden patterns
    is called *data mining*, and machine learning excels at it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，机器学习可以帮助人类学习（[图1-4](#data_mining_diagram)）。机器学习模型可以被检查以查看它们学到了什么（尽管对于某些模型来说可能有些棘手）。例如，一旦一个垃圾邮件过滤器已经在足够多的垃圾邮件上训练过，就可以轻松地检查它，以显示它认为是最佳垃圾邮件预测器的单词和单词组合的列表。有时这会揭示出意想不到的相关性或新趋势，从而更好地理解问题。挖掘大量数据以发现隐藏模式被称为*数据挖掘*，而机器学习在这方面表现出色。
- en: '![mls3 0104](assets/mls3_0104.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0104](assets/mls3_0104.png)'
- en: Figure 1-4\. Machine learning can help humans learn
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. 机器学习可以帮助人类学习
- en: 'To summarize, machine learning is great for:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，机器学习非常适用于：
- en: Problems for which existing solutions require a lot of fine-tuning or long lists
    of rules (a machine learning model can often simplify code and perform better
    than the traditional approach)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要大量微调或长列表规则的现有解决方案的问题（机器学习模型通常可以简化代码并比传统方法表现更好）
- en: Complex problems for which using a traditional approach yields no good solution
    (the best machine learning techniques can perhaps find a solution)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于使用传统方法得不到好解决方案的复杂问题（最好的机器学习技术或许可以找到解决方案）
- en: Fluctuating environments (a machine learning system can easily be retrained
    on new data, always keeping it up to date)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波动的环境（机器学习系统可以轻松地在新数据上重新训练，始终保持最新状态）
- en: Getting insights about complex problems and large amounts of data
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取关于复杂问题和大量数据的见解
- en: Examples of Applications
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用示例
- en: 'Let’s look at some concrete examples of machine learning tasks, along with
    the techniques that can tackle them:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些机器学习任务的具体示例，以及可以解决它们的技术：
- en: Analyzing images of products on a production line to automatically classify
    them
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分析生产线上产品的图像以自动分类它们
- en: This is image classification, typically performed using convolutional neural
    networks (CNNs; see [Chapter 14](ch14.html#cnn_chapter)) or sometimes transformers
    (see [Chapter 16](ch16.html#nlp_chapter)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图像分类，通常使用卷积神经网络（CNN；参见[第14章](ch14.html#cnn_chapter)）或有时使用变压器（参见[第16章](ch16.html#nlp_chapter)）进行处理。
- en: Detecting tumors in brain scans
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 检测脑部扫描中的肿瘤
- en: This is semantic image segmentation, where each pixel in the image is classified
    (as we want to determine the exact location and shape of tumors), typically using
    CNNs or transformers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是语义图像分割，其中图像中的每个像素都被分类（因为我们想要确定肿瘤的确切位置和形状），通常使用CNN或变压器。
- en: Automatically classifying news articles
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自动分类新闻文章
- en: This is natural language processing (NLP), and more specifically text classification,
    which can be tackled using recurrent neural networks (RNNs) and CNNs, but transformers
    work even better (see [Chapter 16](ch16.html#nlp_chapter)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然语言处理（NLP），更具体地说是文本分类，可以使用循环神经网络（RNN）和CNN来解决，但变压器效果更好（参见[第16章](ch16.html#nlp_chapter)）。
- en: Automatically flagging offensive comments on discussion forums
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 自动标记讨论论坛上的攻击性评论
- en: This is also text classification, using the same NLP tools.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是文本分类，使用相同的NLP工具。
- en: Summarizing long documents automatically
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 自动总结长文档
- en: This is a branch of NLP called text summarization, again using the same tools.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种称为文本摘要的NLP分支，再次使用相同的工具。
- en: Creating a chatbot or a personal assistant
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个聊天机器人或个人助手
- en: This involves many NLP components, including natural language understanding
    (NLU) and question-answering modules.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及许多NLP组件，包括自然语言理解（NLU）和问答模块。
- en: Forecasting your company’s revenue next year, based on many performance metrics
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据许多绩效指标预测公司明年的收入
- en: This is a regression task (i.e., predicting values) that may be tackled using
    any regression model, such as a linear regression or polynomial regression model
    (see [Chapter 4](ch04.html#linear_models_chapter)), a regression support vector
    machine (see [Chapter 5](ch05.html#svm_chapter)), a regression random forest (see
    [Chapter 7](ch07.html#ensembles_chapter)), or an artificial neural network (see
    [Chapter 10](ch10.html#ann_chapter)). If you want to take into account sequences
    of past performance metrics, you may want to use RNNs, CNNs, or transformers (see
    Chapters [15](ch15.html#rnn_chapter) and [16](ch16.html#nlp_chapter)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个回归任务（即预测值），可以使用任何回归模型来解决，例如线性回归或多项式回归模型（参见[第4章](ch04.html#linear_models_chapter)）、回归支持向量机（参见[第5章](ch05.html#svm_chapter)）、回归随机森林（参见[第7章](ch07.html#ensembles_chapter)）或人工神经网络（参见[第10章](ch10.html#ann_chapter)）。如果您想考虑过去绩效指标的序列，可能需要使用RNN、CNN或变压器（参见第[15章](ch15.html#rnn_chapter)和第[16章](ch16.html#nlp_chapter)）。
- en: Making your app react to voice commands
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使您的应用程序对语音命令做出反应
- en: 'This is speech recognition, which requires processing audio samples: since
    they are long and complex sequences, they are typically processed using RNNs,
    CNNs, or transformers (see Chapters [15](ch15.html#rnn_chapter) and [16](ch16.html#nlp_chapter)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是语音识别，需要处理音频样本：由于它们是长而复杂的序列，通常使用RNN、CNN或变压器进行处理（参见第[15章](ch15.html#rnn_chapter)和第[16章](ch16.html#nlp_chapter)）。
- en: Detecting credit card fraud
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 检测信用卡欺诈
- en: This is anomaly detection, which can be tackled using isolation forests, Gaussian
    mixture models (see [Chapter 9](ch09.html#unsupervised_learning_chapter)), or
    autoencoders (see [Chapter 17](ch17.html#autoencoders_chapter)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是异常检测，可以使用隔离森林、高斯混合模型或自编码器来解决（参见[第9章](ch09.html#unsupervised_learning_chapter)）。
- en: Segmenting clients based on their purchases so that you can design a different
    marketing strategy for each segment
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据客户的购买情况对客户进行分段，以便为每个细分设计不同的营销策略
- en: This is clustering, which can be achieved using *k*-means, DBSCAN, and more
    (see [Chapter 9](ch09.html#unsupervised_learning_chapter)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是聚类，可以使用*k*-means、DBSCAN等方法来实现（参见[第9章](ch09.html#unsupervised_learning_chapter)）。
- en: Representing a complex, high-dimensional dataset in a clear and insightful diagram
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在清晰而富有洞察力的图表中表示复杂的高维数据集
- en: This is data visualization, often involving dimensionality reduction techniques
    (see [Chapter 8](ch08.html#dimensionality_chapter)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据可视化，通常涉及降维技术（参见[第8章](ch08.html#dimensionality_chapter)）。
- en: Recommending a product that a client may be interested in, based on past purchases
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据过去的购买记录推荐客户可能感兴趣的产品
- en: This is a recommender system. One approach is to feed past purchases (and other
    information about the client) to an artificial neural network (see [Chapter 10](ch10.html#ann_chapter)),
    and get it to output the most likely next purchase. This neural net would typically
    be trained on past sequences of purchases across all clients.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个推荐系统。一种方法是将过去的购买记录（以及有关客户的其他信息）输入到人工神经网络中（参见[第10章](ch10.html#ann_chapter)），并让其输出最可能的下一个购买。这种神经网络通常会在所有客户的过去购买序列上进行训练。
- en: Building an intelligent bot for a game
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为游戏构建一个智能机器人
- en: This is often tackled using reinforcement learning (RL; see [Chapter 18](ch18.html#rl_chapter)),
    which is a branch of machine learning that trains agents (such as bots) to pick
    the actions that will maximize their rewards over time (e.g., a bot may get a
    reward every time the player loses some life points), within a given environment
    (such as the game). The famous AlphaGo program that beat the world champion at
    the game of Go was built using RL.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是通过强化学习（RL）来解决的（请参阅[第18章](ch18.html#rl_chapter)），这是机器学习的一个分支，训练代理人（如机器人）选择能够随着时间最大化奖励的动作（例如，每次玩家失去一些生命值时机器人都会获得奖励），在给定环境（例如游戏）中。击败围棋世界冠军的著名AlphaGo程序就是使用RL构建的。
- en: This list could go on and on, but hopefully it gives you a sense of the incredible
    breadth and complexity of the tasks that machine learning can tackle, and the
    types of techniques that you would use for each task.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表可以继续下去，但希望它能让您感受到机器学习可以处理的任务的广泛和复杂性，以及您将为每个任务使用的技术类型。
- en: Types of Machine Learning Systems
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统的类型
- en: 'There are so many different types of machine learning systems that it is useful
    to classify them in broad categories, based on the following criteria:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同类型的机器学习系统，将它们根据以下标准进行广泛分类是有用的：
- en: How they are supervised during training (supervised, unsupervised, semi-supervised,
    self-supervised, and others)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在训练过程中如何受监督（监督、无监督、半监督、自监督等）
- en: Whether or not they can learn incrementally on the fly (online versus batch
    learning)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论它们是否可以在飞行中逐步学习（在线与批量学习）
- en: Whether they work by simply comparing new data points to known data points,
    or instead by detecting patterns in the training data and building a predictive
    model, much like scientists do (instance-based versus model-based learning)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是通过简单地将新数据点与已知数据点进行比较，还是通过检测训练数据中的模式并构建预测模型，就像科学家所做的那样（基于实例与基于模型的学习）
- en: These criteria are not exclusive; you can combine them in any way you like.
    For example, a state-of-the-art spam filter may learn on the fly using a deep
    neural network model trained using human-provided examples of spam and ham; this
    makes it an online, model-based, supervised learning system.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标准并不是互斥的；您可以以任何您喜欢的方式组合它们。例如，一款最先进的垃圾邮件过滤器可能会使用深度神经网络模型进行在线学习，该模型是使用人类提供的垃圾邮件和正常邮件示例进行训练的；这使其成为一个在线、基于模型的监督学习系统。
- en: Let’s look at each of these criteria a bit more closely.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些标准中的每一个。
- en: Training Supervision
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练监督
- en: 'ML systems can be classified according to the amount and type of supervision
    they get during training. There are many categories, but we’ll discuss the main
    ones: supervised learning, unsupervised learning, self-supervised learning, semi-supervised
    learning, and reinforcement learning.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ML系统可以根据训练过程中获得的监督量和类型进行分类。有许多类别，但我们将讨论主要的类别：监督学习、无监督学习、自监督学习、半监督学习和强化学习。
- en: Supervised learning
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: In *supervised learning*, the training set you feed to the algorithm includes
    the desired solutions, called *labels* ([Figure 1-5](#supervised_learning_diagram)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在*监督学习*中，您向算法提供的训练集包括所需的解决方案，称为*标签*（[图1-5](#supervised_learning_diagram)）。
- en: '![mls3 0105](assets/mls3_0105.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0105](assets/mls3_0105.png)'
- en: Figure 1-5\. A labeled training set for spam classification (an example of supervised
    learning)
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5\. 用于垃圾邮件分类的带标签训练集（监督学习的示例）
- en: 'A typical supervised learning task is *classification*. The spam filter is
    a good example of this: it is trained with many example emails along with their
    *class* (spam or ham), and it must learn how to classify new emails.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的监督学习任务是*分类*。垃圾邮件过滤器就是一个很好的例子：它通过许多示例电子邮件及其*类别*（垃圾邮件或正常邮件）进行训练，并且必须学会如何对新邮件进行分类。
- en: Another typical task is to predict a *target* numeric value, such as the price
    of a car, given a set of *features* (mileage, age, brand, etc.). This sort of
    task is called *regression* ([Figure 1-6](#regression_diagram)).⁠^([1](ch01.html#idm45720250412768))
    To train the system, you need to give it many examples of cars, including both
    their features and their targets (i.e., their prices).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个典型的任务是预测一个*目标*数值，例如一辆汽车的价格，给定一组*特征*（里程、年龄、品牌等）。这种类型的任务被称为*回归*（[图1-6](#regression_diagram)）。为了训练系统，您需要提供许多汽车的示例，包括它们的特征和目标（即它们的价格）。
- en: Note that some regression models can be used for classification as well, and
    vice versa. For example, *logistic regression* is commonly used for classification,
    as it can output a value that corresponds to the probability of belonging to a
    given class (e.g., 20% chance of being spam).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一些回归模型也可以用于分类，反之亦然。例如，*逻辑回归*通常用于分类，因为它可以输出与属于给定类别的概率相对应的值（例如，属于垃圾邮件的概率为20%）。
- en: '![mls3 0106](assets/mls3_0106.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0106](assets/mls3_0106.png)'
- en: 'Figure 1-6\. A regression problem: predict a value, given an input feature
    (there are usually multiple input features, and sometimes multiple output values)'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6\. 一个回归问题：根据输入特征预测一个值（通常有多个输入特征，有时有多个输出值）
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The words *target* and *label* are generally treated as synonyms in supervised
    learning, but *target* is more common in regression tasks and *label* is more
    common in classification tasks. Moreover, *features* are sometimes called *predictors*
    or *attributes*. These terms may refer to individual samples (e.g., “this car’s
    mileage feature is equal to 15,000”) or to all samples (e.g., “the mileage feature
    is strongly correlated with price”).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，*目标*和*标签*通常被视为同义词，但*目标*在回归任务中更常见，*标签*在分类任务中更常见。此外，*特征*有时被称为*预测变量*或*属性*。这些术语可能指个别样本（例如，“这辆车的里程特征等于15,000”）或所有样本（例如，“里程特征与价格强相关”）。
- en: Unsupervised learning
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In *unsupervised learning*, as you might guess, the training data is unlabeled
    ([Figure 1-7](#unsupervised_learning_diagram)). The system tries to learn without
    a teacher.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在*无监督学习*中，正如您可能猜到的那样，训练数据是未标记的（[图1-7](#unsupervised_learning_diagram)）。系统试图在没有老师的情况下学习。
- en: 'For example, say you have a lot of data about your blog’s visitors. You may
    want to run a *clustering* algorithm to try to detect groups of similar visitors
    ([Figure 1-8](#clustering_diagram)). At no point do you tell the algorithm which
    group a visitor belongs to: it finds those connections without your help. For
    example, it might notice that 40% of your visitors are teenagers who love comic
    books and generally read your blog after school, while 20% are adults who enjoy
    sci-fi and who visit during the weekends. If you use a *hierarchical clustering*
    algorithm, it may also subdivide each group into smaller groups. This may help
    you target your posts for each group.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您有关于博客访问者的大量数据。您可能希望运行一个*聚类*算法来尝试检测相似访问者的群组。在任何时候，您都不告诉算法访问者属于哪个群组：它会在没有您帮助的情况下找到这些连接。例如，它可能注意到您的40%访问者是喜欢漫画书并且通常在放学后阅读您的博客的青少年，而20%是喜欢科幻并且在周末访问的成年人。如果使用*层次聚类*算法，它还可以将每个群组细分为更小的群组。这可能有助于您为每个群体定位您的帖子。
- en: '![mls3 0107](assets/mls3_0107.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0107](assets/mls3_0107.png)'
- en: Figure 1-7\. An unlabeled training set for unsupervised learning
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7\. 无监督学习的未标记训练集
- en: '![mls3 0108](assets/mls3_0108.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0108](assets/mls3_0108.png)'
- en: Figure 1-8\. Clustering
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8\. 聚类
- en: '*Visualization* algorithms are also good examples of unsupervised learning:
    you feed them a lot of complex and unlabeled data, and they output a 2D or 3D
    representation of your data that can easily be plotted ([Figure 1-9](#socher_ganjoo_manning_ng_2013_paper)).
    These algorithms try to preserve as much structure as they can (e.g., trying to
    keep separate clusters in the input space from overlapping in the visualization)
    so that you can understand how the data is organized and perhaps identify unsuspected
    patterns.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*可视化*算法也是无监督学习的很好的例子：你向它们提供大量复杂且未标记的数据，它们会输出数据的二维或三维表示，可以轻松绘制出来。这些算法试图尽可能保留数据的结构（例如，尝试在可视化中保持输入空间中的不同簇不重叠），以便您可以了解数据的组织方式，也许还可以识别出意想不到的模式。'
- en: A related task is *dimensionality reduction*, in which the goal is to simplify
    the data without losing too much information. One way to do this is to merge several
    correlated features into one. For example, a car’s mileage may be strongly correlated
    with its age, so the dimensionality reduction algorithm will merge them into one
    feature that represents the car’s wear and tear. This is called *feature extraction*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的任务是*降维*，其目标是简化数据而不丢失太多信息。一种方法是将几个相关特征合并为一个。例如，一辆汽车的里程可能与其年龄强相关，因此降维算法将它们合并为一个代表汽车磨损程度的特征。这被称为*特征提取*。
- en: '![mls3 0109](assets/mls3_0109.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0109](assets/mls3_0109.png)'
- en: Figure 1-9\. Example of a t-SNE visualization highlighting semantic clusters⁠^([2](ch01.html#idm45720242786608))
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. t-SNE可视化示例，突出显示语义簇⁠^([2](ch01.html#idm45720242786608))
- en: Tip
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: It is often a good idea to try to reduce the number of dimensions in your training
    data using a dimensionality reduction algorithm before you feed it to another
    machine learning algorithm (such as a supervised learning algorithm). It will
    run much faster, the data will take up less disk and memory space, and in some
    cases it may also perform better.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在将训练数据提供给另一个机器学习算法（如监督学习算法）之前，尝试使用降维算法减少训练数据的维数通常是一个好主意。这样可以使运行速度更快，数据占用的磁盘和内存空间更少，并且在某些情况下可能表现更好。
- en: 'Yet another important unsupervised task is *anomaly detection*—for example,
    detecting unusual credit card transactions to prevent fraud, catching manufacturing
    defects, or automatically removing outliers from a dataset before feeding it to
    another learning algorithm. The system is shown mostly normal instances during
    training, so it learns to recognize them; then, when it sees a new instance, it
    can tell whether it looks like a normal one or whether it is likely an anomaly
    (see [Figure 1-10](#anomaly_detection_diagram)). A very similar task is *novelty
    detection*: it aims to detect new instances that look different from all instances
    in the training set. This requires having a very “clean” training set, devoid
    of any instance that you would like the algorithm to detect. For example, if you
    have thousands of pictures of dogs, and 1% of these pictures represent Chihuahuas,
    then a novelty detection algorithm should not treat new pictures of Chihuahuas
    as novelties. On the other hand, anomaly detection algorithms may consider these
    dogs as so rare and so different from other dogs that they would likely classify
    them as anomalies (no offense to Chihuahuas).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的无监督任务是*异常检测*——例如，检测异常的信用卡交易以防止欺诈，捕捉制造缺陷，或在将数据提供给另一个学习算法之前自动删除异常值。系统在训练期间主要展示正常实例，因此学会了识别它们；然后，当它看到一个新实例时，它可以判断它是否看起来像一个正常实例，或者它很可能是一个异常（见[图1-10](#anomaly_detection_diagram)）。一个非常相似的任务是*新颖性检测*：它旨在检测看起来与训练集中所有实例都不同的新实例。这需要一个非常“干净”的训练集，不包含您希望算法检测的任何实例。例如，如果您有成千上万张狗的图片，其中1%的图片代表吉娃娃，那么新颖性检测算法不应该将新的吉娃娃图片视为新颖。另一方面，异常检测算法可能认为这些狗非常罕见，与其他狗有很大不同，因此很可能将它们分类为异常（对吉娃娃没有冒犯意图）。
- en: '![mls3 0110](assets/mls3_0110.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0110](assets/mls3_0110.png)'
- en: Figure 1-10\. Anomaly detection
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10\. 异常检测
- en: Finally, another common unsupervised task is *association rule learning*, in
    which the goal is to dig into large amounts of data and discover interesting relations
    between attributes. For example, suppose you own a supermarket. Running an association
    rule on your sales logs may reveal that people who purchase barbecue sauce and
    potato chips also tend to buy steak. Thus, you may want to place these items close
    to one another.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一个常见的无监督任务是*关联规则学习*，其目标是挖掘大量数据并发现属性之间的有趣关系。例如，假设您拥有一家超市。在销售日志上运行关联规则可能会发现购买烧烤酱和薯片的人也倾向于购买牛排。因此，您可能希望将这些物品放在彼此附近。
- en: Semi-supervised learning
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Since labeling data is usually time-consuming and costly, you will often have
    plenty of unlabeled instances, and few labeled instances. Some algorithms can
    deal with data that’s partially labeled. This is called *semi-supervised learning*
    ([Figure 1-11](#semi_supervised_learning_diagram)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标记数据通常耗时且昂贵，您通常会有大量未标记的实例和少量标记的实例。一些算法可以处理部分标记的数据。这被称为*半监督学习*（[图1-11](#semi_supervised_learning_diagram)）。
- en: '![mls3 0111](assets/mls3_0111.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0111](assets/mls3_0111.png)'
- en: 'Figure 1-11\. Semi-supervised learning with two classes (triangles and squares):
    the unlabeled examples (circles) help classify a new instance (the cross) into
    the triangle class rather than the square class, even though it is closer to the
    labeled squares'
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-11\. 两类（三角形和正方形）的半监督学习：未标记的示例（圆圈）有助于将新实例（十字）分类为三角形类，而不是正方形类，即使它更接近标记的正方形
- en: Some photo-hosting services, such as Google Photos, are good examples of this.
    Once you upload all your family photos to the service, it automatically recognizes
    that the same person A shows up in photos 1, 5, and 11, while another person B
    shows up in photos 2, 5, and 7\. This is the unsupervised part of the algorithm
    (clustering). Now all the system needs is for you to tell it who these people
    are. Just add one label per person⁠^([3](ch01.html#idm45720251565632)) and it
    is able to name everyone in every photo, which is useful for searching photos.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一些照片托管服务，如Google相册，就是很好的例子。一旦您将所有家庭照片上传到该服务，它会自动识别出同一个人A出现在照片1、5和11中，而另一个人B出现在照片2、5和7中。这是算法的无监督部分（聚类）。现在系统只需要您告诉它这些人是谁。只需为每个人添加一个标签⁠^([3](ch01.html#idm45720251565632))，它就能够为每张照片中的每个人命名，这对于搜索照片很有用。
- en: Most semi-supervised learning algorithms are combinations of unsupervised and
    supervised algorithms. For example, a clustering algorithm may be used to group
    similar instances together, and then every unlabeled instance can be labeled with
    the most common label in its cluster. Once the whole dataset is labeled, it is
    possible to use any supervised learning algorithm.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数半监督学习算法是无监督和监督算法的组合。例如，可以使用聚类算法将相似的实例分组在一起，然后每个未标记的实例可以用其簇中最常见的标签进行标记。一旦整个数据集被标记，就可以使用任何监督学习算法。
- en: Self-supervised learning
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自监督学习
- en: Another approach to machine learning involves actually generating a fully labeled
    dataset from a fully unlabeled one. Again, once the whole dataset is labeled,
    any supervised learning algorithm can be used. This approach is called *self-supervised
    learning*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种机器学习方法涉及从完全未标记的数据集中生成一个完全标记的数据集。再次，一旦整个数据集被标记，就可以使用任何监督学习算法。这种方法被称为*自监督学习*。
- en: For example, if you have a large dataset of unlabeled images, you can randomly
    mask a small part of each image and then train a model to recover the original
    image ([Figure 1-12](#self_supervised_learning_diagram)). During training, the
    masked images are used as the inputs to the model, and the original images are
    used as the labels.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您有一个大量未标记的图像数据集，您可以随机遮挡每个图像的一小部分，然后训练一个模型恢复原始图像（[图1-12](#self_supervised_learning_diagram)）。在训练过程中，遮挡的图像被用作模型的输入，原始图像被用作标签。
- en: '![mls3 0112](assets/mls3_0112.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0112](assets/mls3_0112.png)'
- en: 'Figure 1-12\. Self-supervised learning example: input (left) and target (right)'
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-12\. 自监督学习示例：输入（左）和目标（右）
- en: The resulting model may be quite useful in itself—for example, to repair damaged
    images or to erase unwanted objects from pictures. But more often than not, a
    model trained using self-supervised learning is not the final goal. You’ll usually
    want to tweak and fine-tune the model for a slightly different task—one that you
    actually care about.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的模型本身可能非常有用，例如用于修复损坏的图像或从图片中删除不需要的物体。但通常情况下，使用自监督学习训练的模型并不是最终目标。通常您会想要微调和调整模型以执行一个略有不同的任务，一个您真正关心的任务。
- en: 'For example, suppose that what you really want is to have a pet classification
    model: given a picture of any pet, it will tell you what species it belongs to.
    If you have a large dataset of unlabeled photos of pets, you can start by training
    an image-repairing model using self-supervised learning. Once it’s performing
    well, it should be able to distinguish different pet species: when it repairs
    an image of a cat whose face is masked, it must know not to add a dog’s face.
    Assuming your model’s architecture allows it (and most neural network architectures
    do), it is then possible to tweak the model so that it predicts pet species instead
    of repairing images. The final step consists of fine-tuning the model on a labeled
    dataset: the model already knows what cats, dogs, and other pet species look like,
    so this step is only needed so the model can learn the mapping between the species
    it already knows and the labels we expect from it.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您真正想要的是一个宠物分类模型：给定任何宠物的图片，它将告诉您它属于哪个物种。如果您有一个大量未标记的宠物照片数据集，您可以通过使用自监督学习训练一个图像修复模型来开始。一旦表现良好，它应该能够区分不同的宠物物种：当它修复一个脸部被遮盖的猫的图像时，它必须知道不要添加狗的脸。假设您的模型架构允许这样做（大多数神经网络架构都允许），那么就可以调整模型，使其预测宠物物种而不是修复图像。最后一步是在一个标记的数据集上对模型进行微调：模型已经知道猫、狗和其他宠物物种的外观，因此这一步只是为了让模型学习它已经知道的物种与我们期望从中得到的标签之间的映射。
- en: Note
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Transferring knowledge from one task to another is called *transfer learning*,
    and it’s one of the most important techniques in machine learning today, especially
    when using *deep neural networks* (i.e., neural networks composed of many layers
    of neurons). We will discuss this in detail in [Part II](part02.html#neural_nets_part).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个任务中转移知识到另一个任务被称为*迁移学习*，这是当今机器学习中最重要的技术之一，特别是在使用*深度神经网络*（即由许多层神经元组成的神经网络）时。我们将在[第二部分](part02.html#neural_nets_part)中详细讨论这个问题。
- en: 'Some people consider self-supervised learning to be a part of unsupervised
    learning, since it deals with fully unlabeled datasets. But self-supervised learning
    uses (generated) labels during training, so in that regard it’s closer to supervised
    learning. And the term “unsupervised learning” is generally used when dealing
    with tasks like clustering, dimensionality reduction, or anomaly detection, whereas
    self-supervised learning focuses on the same tasks as supervised learning: mainly
    classification and regression. In short, it’s best to treat self-supervised learning
    as its own category.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为自监督学习是无监督学习的一部分，因为它处理完全未标记的数据集。但是自监督学习在训练过程中使用（生成的）标签，因此在这方面更接近于监督学习。而“无监督学习”这个术语通常用于处理聚类、降维或异常检测等任务，而自监督学习侧重于与监督学习相同的任务：主要是分类和回归。简而言之，最好将自监督学习视为其自己的类别。
- en: Reinforcement learning
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习
- en: '*Reinforcement learning* is a very different beast. The learning system, called
    an *agent* in this context, can observe the environment, select and perform actions,
    and get *rewards* in return (or *penalties* in the form of negative rewards, as
    shown in [Figure 1-13](#reinforcement_learning_diagram)). It must then learn by
    itself what is the best strategy, called a *policy*, to get the most reward over
    time. A policy defines what action the agent should choose when it is in a given
    situation.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*是一种非常不同的学习方式。在这种情况下，学习系统被称为*代理*，它可以观察环境，选择和执行动作，并获得*奖励*（或以负奖励形式的*惩罚*，如[图1-13](#reinforcement_learning_diagram)所示）。然后，它必须自己学习什么是最佳策略，称为*策略*，以获得最大的奖励。策略定义了代理在特定情况下应该选择什么动作。'
- en: '![mls3 0113](assets/mls3_0113.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0113](assets/mls3_0113.png)'
- en: Figure 1-13\. Reinforcement learning
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-13。强化学习
- en: 'For example, many robots implement reinforcement learning algorithms to learn
    how to walk. DeepMind’s AlphaGo program is also a good example of reinforcement
    learning: it made the headlines in May 2017 when it beat Ke Jie, the number one
    ranked player in the world at the time, at the game of Go. It learned its winning
    policy by analyzing millions of games, and then playing many games against itself.
    Note that learning was turned off during the games against the champion; AlphaGo
    was just applying the policy it had learned. As you will see in the next section,
    this is called *offline learning*.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，许多机器人实现强化学习算法来学习如何行走。DeepMind的AlphaGo程序也是强化学习的一个很好的例子：2017年5月，它在围棋比赛中击败了当时世界排名第一的柯洁，成为头条新闻。它通过分析数百万场比赛学习了其获胜策略，然后对自己进行了许多场比赛。请注意，在与冠军对战时学习被关闭；AlphaGo只是应用了它学到的策略。正如您将在下一节中看到的那样，这被称为*离线学习*。
- en: Batch Versus Online Learning
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量学习与在线学习
- en: Another criterion used to classify machine learning systems is whether or not
    the system can learn incrementally from a stream of incoming data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分类机器学习系统的另一个标准是系统是否能够从不断涌入的数据流中逐步学习。
- en: Batch learning
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量学习
- en: 'In *batch learning*, the system is incapable of learning incrementally: it
    must be trained using all the available data. This will generally take a lot of
    time and computing resources, so it is typically done offline. First the system
    is trained, and then it is launched into production and runs without learning
    anymore; it just applies what it has learned. This is called *offline learning*.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在*批量学习*中，系统无法逐步学习：它必须使用所有可用数据进行训练。这通常需要大量时间和计算资源，因此通常在离线状态下进行。首先训练系统，然后将其投入生产并在不再学习的情况下运行；它只是应用它所学到的知识。这被称为*离线学习*。
- en: 'Unfortunately, a model’s performance tends to decay slowly over time, simply
    because the world continues to evolve while the model remains unchanged. This
    phenomenon is often called *model rot* or *data drift*. The solution is to regularly
    retrain the model on up-to-date data. How often you need to do that depends on
    the use case: if the model classifies pictures of cats and dogs, its performance
    will decay very slowly, but if the model deals with fast-evolving systems, for
    example making predictions on the financial market, then it is likely to decay
    quite fast.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，模型的性能往往会随着时间的推移而缓慢下降，仅仅因为世界在不断发展，而模型保持不变。这种现象通常被称为*模型腐烂*或*数据漂移*。解决方案是定期使用最新数据对模型进行重新训练。您需要多久才能做到这一点取决于用例：如果模型对猫和狗的图片进行分类，其性能将会缓慢下降，但如果模型处理快速演变的系统，例如在金融市场上进行预测，那么它可能会迅速下降。
- en: Warning
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Even a model trained to classify pictures of cats and dogs may need to be retrained
    regularly, not because cats and dogs will mutate overnight, but because cameras
    keep changing, along with image formats, sharpness, brightness, and size ratios.
    Moreover, people may love different breeds next year, or they may decide to dress
    their pets with tiny hats—who knows?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是训练用于分类猫和狗图片的模型，也可能需要定期重新训练，不是因为猫和狗会在一夜之间发生变异，而是因为相机不断变化，图像格式、清晰度、亮度和大小比例也在变化。此外，人们可能会在明年喜欢不同的品种，或者决定给他们的宠物戴上小帽子——谁知道呢？
- en: If you want a batch learning system to know about new data (such as a new type
    of spam), you need to train a new version of the system from scratch on the full
    dataset (not just the new data, but also the old data), then replace the old model
    with the new one. Fortunately, the whole process of training, evaluating, and
    launching a machine learning system can be automated fairly easily (as we saw
    in [Figure 1-3](#adapting_to_change_diagram)), so even a batch learning system
    can adapt to change. Simply update the data and train a new version of the system
    from scratch as often as needed.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望批量学习系统了解新数据（例如新类型的垃圾邮件），您需要从头开始对完整数据集进行新版本系统的训练（不仅仅是新数据，还包括旧数据），然后用新模型替换旧模型。幸运的是，整个机器学习系统的训练、评估和启动过程可以相当容易地自动化（正如我们在[图1-3](#adapting_to_change_diagram)中看到的那样），因此即使是批量学习系统也可以适应变化。只需根据需要更新数据并从头开始训练新版本的系统。
- en: This solution is simple and often works fine, but training using the full set
    of data can take many hours, so you would typically train a new system only every
    24 hours or even just weekly. If your system needs to adapt to rapidly changing
    data (e.g., to predict stock prices), then you need a more reactive solution.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案简单且通常效果良好，但使用完整数据集进行训练可能需要很多小时，因此您通常只会每24小时或甚至每周训练一个新系统。如果您的系统需要适应快速变化的数据（例如，预测股票价格），那么您需要一个更具反应性的解决方案。
- en: Also, training on the full set of data requires a lot of computing resources
    (CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a lot
    of data and you automate your system to train from scratch every day, it will
    end up costing you a lot of money. If the amount of data is huge, it may even
    be impossible to use a batch learning algorithm.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对完整数据集进行训练需要大量的计算资源（CPU、内存空间、磁盘空间、磁盘I/O、网络I/O等）。如果您有大量数据并且自动化系统每天从头开始训练，那么最终会花费很多钱。如果数据量很大，甚至可能无法使用批量学习算法。
- en: Finally, if your system needs to be able to learn autonomously and it has limited
    resources (e.g., a smartphone application or a rover on Mars), then carrying around
    large amounts of training data and taking up a lot of resources to train for hours
    every day is a showstopper.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您的系统需要能够自主学习并且资源有限（例如，智能手机应用程序或火星车），那么携带大量训练数据并每天花费大量资源进行训练是一个障碍。
- en: A better option in all these cases is to use algorithms that are capable of
    learning incrementally.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，更好的选择是使用能够增量学习的算法。
- en: Online learning
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线学习
- en: In *online learning*, you train the system incrementally by feeding it data
    instances sequentially, either individually or in small groups called *mini-batches*.
    Each learning step is fast and cheap, so the system can learn about new data on
    the fly, as it arrives (see [Figure 1-14](#online_learning_diagram)).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在*在线学习*中，您通过顺序地逐个或以小组（称为*小批量*）的方式向系统提供数据实例来逐步训练系统。每个学习步骤都很快且便宜，因此系统可以在数据到达时即时学习新数据（参见[图1-14](#online_learning_diagram)）。
- en: '![mls3 0114](assets/mls3_0114.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0114](assets/mls3_0114.png)'
- en: Figure 1-14\. In online learning, a model is trained and launched into production,
    and then it keeps learning as new data comes in
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-14。在在线学习中，模型经过训练并投入生产，然后随着新数据的到来而不断学习
- en: Online learning is useful for systems that need to adapt to change extremely
    rapidly (e.g., to detect new patterns in the stock market). It is also a good
    option if you have limited computing resources; for example, if the model is trained
    on a mobile device.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习对需要极快适应变化的系统非常有用（例如，检测股市中的新模式）。如果您的计算资源有限，例如在移动设备上训练模型，这也是一个不错的选择。
- en: Additionally, online learning algorithms can be used to train models on huge
    datasets that cannot fit in one machine’s main memory (this is called *out-of-core*
    learning). The algorithm loads part of the data, runs a training step on that
    data, and repeats the process until it has run on all of the data (see [Figure 1-15](#ol_for_huge_datasets_diagram)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以使用在线学习算法在无法适应一台机器的主内存的大型数据集上训练模型（这称为*离线*学习）。该算法加载部分数据，在该数据上运行训练步骤，并重复该过程，直到在所有数据上运行完毕（参见[图1-15](#ol_for_huge_datasets_diagram)）。
- en: '![mls3 0115](assets/mls3_0115.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0115](assets/mls3_0115.png)'
- en: Figure 1-15\. Using online learning to handle huge datasets
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-15。使用在线学习处理大型数据集
- en: 'One important parameter of online learning systems is how fast they should
    adapt to changing data: this is called the *learning rate*. If you set a high
    learning rate, then your system will rapidly adapt to new data, but it will also
    tend to quickly forget the old data (and you don’t want a spam filter to flag
    only the latest kinds of spam it was shown). Conversely, if you set a low learning
    rate, the system will have more inertia; that is, it will learn more slowly, but
    it will also be less sensitive to noise in the new data or to sequences of nonrepresentative
    data points (outliers).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习系统的一个重要参数是它们应该如何快速适应变化的数据：这被称为“学习率”。如果设置较高的学习率，那么您的系统将迅速适应新数据，但也会很快忘记旧数据（您不希望垃圾邮件过滤器只标记它所展示的最新类型的垃圾邮件）。相反，如果设置较低的学习率，系统将具有更多的惯性；也就是说，它将学习得更慢，但也会对新数据中的噪声或非代表性数据点序列（异常值）不太敏感。
- en: Warning
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Out-of-core learning is usually done offline (i.e., not on the live system),
    so *online learning* can be a confusing name. Think of it as *incremental learning*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 离线学习通常是在离线系统上完成的（即不在实时系统上），因此“在线学习”可能是一个令人困惑的名称。将其视为“增量学习”。
- en: A big challenge with online learning is that if bad data is fed to the system,
    the system’s performance will decline, possibly quickly (depending on the data
    quality and learning rate). If it’s a live system, your clients will notice. For
    example, bad data could come from a bug (e.g., a malfunctioning sensor on a robot),
    or it could come from someone trying to game the system (e.g., spamming a search
    engine to try to rank high in search results). To reduce this risk, you need to
    monitor your system closely and promptly switch learning off (and possibly revert
    to a previously working state) if you detect a drop in performance. You may also
    want to monitor the input data and react to abnormal data; for example, using
    an anomaly detection algorithm (see [Chapter 9](ch09.html#unsupervised_learning_chapter)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习的一个重大挑战是，如果向系统提供了错误数据，系统的性能将下降，可能会很快下降（取决于数据质量和学习率）。如果这是一个实时系统，您的客户会注意到。例如，错误的数据可能来自错误（例如，机器人上的传感器故障），也可能来自试图操纵系统的人（例如，通过垃圾信息搜索引擎以在搜索结果中排名靠前）。为了降低这种风险，您需要密切监控系统，并在检测到性能下降时及时关闭学习（并可能恢复到先前工作状态）。您还可能希望监控输入数据并对异常数据做出反应；例如，使用异常检测算法（参见[第9章](ch09.html#unsupervised_learning_chapter)）。
- en: Instance-Based Versus Model-Based Learning
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于实例与基于模型的学习
- en: One more way to categorize machine learning systems is by how they *generalize*.
    Most machine learning tasks are about making predictions. This means that given
    a number of training examples, the system needs to be able to make good predictions
    for (generalize to) examples it has never seen before. Having a good performance
    measure on the training data is good, but insufficient; the true goal is to perform
    well on new instances.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习系统分类的另一种方法是通过它们的*泛化*方式。大多数机器学习任务都是关于进行预测。这意味着给定一些训练示例，系统需要能够对它以前从未见过的示例进行良好的预测（泛化）。在训练数据上有一个良好的性能度量是好的，但不足够；真正的目标是在新实例上表现良好。
- en: 'There are two main approaches to generalization: instance-based learning and
    model-based learning.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化有两种主要方法：基于实例的学习和基于模型的学习。
- en: Instance-based learning
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于实例的学习
- en: Possibly the most trivial form of learning is simply to learn by heart. If you
    were to create a spam filter this way, it would just flag all emails that are
    identical to emails that have already been flagged by users—not the worst solution,
    but certainly not the best.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最琐碎的学习形式就是纯粹靠记忆学习。如果您按照这种方式创建垃圾邮件过滤器，它将只标记所有与用户已标记的电子邮件相同的电子邮件，这并不是最糟糕的解决方案，但肯定不是最好的解决方案。
- en: Instead of just flagging emails that are identical to known spam emails, your
    spam filter could be programmed to also flag emails that are very similar to known
    spam emails. This requires a *measure of similarity* between two emails. A (very
    basic) similarity measure between two emails could be to count the number of words
    they have in common. The system would flag an email as spam if it has many words
    in common with a known spam email.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 而不仅仅是标记与已知垃圾邮件相同的电子邮件，您的垃圾邮件过滤器还可以被编程为标记与已知垃圾邮件非常相似的电子邮件。这需要两封电子邮件之间的*相似度度量*。两封电子邮件之间的（非常基本的）相似度度量可以是计算它们共同拥有的单词数量。如果一封电子邮件与已知的垃圾邮件有许多共同单词，系统将标记该电子邮件为垃圾邮件。
- en: 'This is called *instance-based learning*: the system learns the examples by
    heart, then generalizes to new cases by using a similarity measure to compare
    them to the learned examples (or a subset of them). For example, in [Figure 1-16](#instance_based_learning_diagram)
    the new instance would be classified as a triangle because the majority of the
    most similar instances belong to that class.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这称为*基于实例的学习*：系统通过记忆示例，然后使用相似度度量将新案例泛化到学习示例（或其中的一个子集）。例如，在[图1-16](#instance_based_learning_diagram)中，新实例将被分类为三角形，因为大多数最相似的实例属于该类。
- en: '![mls3 0116](assets/mls3_0116.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0116](assets/mls3_0116.png)'
- en: Figure 1-16\. Instance-based learning
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-16\. 基于实例的学习
- en: Model-based learning and a typical machine learning workflow
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于模型的学习和典型的机器学习工作流程
- en: Another way to generalize from a set of examples is to build a model of these
    examples and then use that model to make *predictions*. This is called *model-based
    learning* ([Figure 1-17](#model_based_learning_diagram)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 从一组示例中泛化的另一种方法是构建这些示例的模型，然后使用该模型进行*预测*。这称为*基于模型的学习*（[图1-17](#model_based_learning_diagram)）。
- en: '![mls3 0117](assets/mls3_0117.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0117](assets/mls3_0117.png)'
- en: Figure 1-17\. Model-based learning
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-17\. 基于模型的学习
- en: For example, suppose you want to know if money makes people happy, so you download
    the Better Life Index data from the [OECD’s website](https://www.oecdbetterlifeindex.org)
    and [World Bank stats](https://ourworldindata.org) about gross domestic product
    (GDP) per capita. Then you join the tables and sort by GDP per capita. [Table 1-1](#life_satisfaction_table_excerpt)
    shows an excerpt of what you get.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您想知道金钱是否让人们快乐，因此您从[OECD网站](https://www.oecdbetterlifeindex.org)下载更美好生活指数数据和[世界银行统计数据](https://ourworldindata.org)关于人均国内生产总值（GDP）。然后您连接这些表格并按人均GDP排序。[表1-1](#life_satisfaction_table_excerpt)显示了您获得的摘录。
- en: Table 1-1\. Does money make people happier?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-1\. 金钱让人更快乐吗？
- en: '| Country | GDP per capita (USD) | Life satisfaction |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 国家 | 人均GDP（美元） | 生活满意度 |'
- en: '| --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Turkey | 28,384 | 5.5 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 土耳其 | 28,384 | 5.5 |'
- en: '| Hungary | 31,008 | 5.6 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 匈牙利 | 31,008 | 5.6 |'
- en: '| France | 42,026 | 6.5 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 法国 | 42,026 | 6.5 |'
- en: '| United States | 60,236 | 6.9 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 美国 | 60,236 | 6.9 |'
- en: '| New Zealand | 42,404 | 7.3 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 新西兰 | 42,404 | 7.3 |'
- en: '| Australia | 48,698 | 7.3 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 48,698 | 7.3 |'
- en: '| Denmark | 55,938 | 7.6 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 丹麦 | 55,938 | 7.6 |'
- en: Let’s plot the data for these countries ([Figure 1-18](#money_happy_scatterplot)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为这些国家的数据绘制图表（[图1-18](#money_happy_scatterplot)）。
- en: '![mls3 0118](assets/mls3_0118.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0118](assets/mls3_0118.png)'
- en: Figure 1-18\. Do you see a trend here?
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-18\. 你看到这里有一个趋势吗？
- en: 'There does seem to be a trend here! Although the data is *noisy* (i.e., partly
    random), it looks like life satisfaction goes up more or less linearly as the
    country’s GDP per capita increases. So you decide to model life satisfaction as
    a linear function of GDP per capita. This step is called *model selection*: you
    selected a *linear model* of life satisfaction with just one attribute, GDP per
    capita ([Equation 1-1](#a_simple_linear_model)).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里似乎有一个趋势！尽管数据是*嘈杂*的（即部分随机），但看起来生活满意度随着国家人均GDP的增加而更多或更少地呈线性增长。因此，您决定将生活满意度建模为国家人均GDP的线性函数。这一步称为*模型选择*：您选择了一个只有一个属性，即国家人均GDP的生活满意度的*线性模型*（[方程1-1](#a_simple_linear_model)）。
- en: Equation 1-1\. A simple linear model
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程1-1\. 一个简单的线性模型
- en: <math display="block"><mrow><mtext>life_satisfaction</mtext> <mo>=</mo> <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo>×</mo> <mtext>GDP_per_capita</mtext></mrow></math>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>life_satisfaction</mtext> <mo>=</mo> <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo>×</mo> <mtext>GDP_per_capita</mtext></mrow></math>
- en: This model has two *model parameters*, *θ*[0] and *θ*[1].⁠^([4](ch01.html#idm45720251146336))
    By tweaking these parameters, you can make your model represent any linear function,
    as shown in [Figure 1-19](#tweaking_model_params_plot).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有两个*模型参数*，*θ*[0]和*θ*[1]。⁠^([4](ch01.html#idm45720251146336)) 通过调整这些参数，您可以使您的模型表示任何线性函数，如[图1-19](#tweaking_model_params_plot)所示。
- en: '![mls3 0119](assets/mls3_0119.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0119](assets/mls3_0119.png)'
- en: Figure 1-19\. A few possible linear models
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-19\. 几种可能的线性模型
- en: Before you can use your model, you need to define the parameter values *θ*[0]
    and *θ*[1]. How can you know which values will make your model perform best? To
    answer this question, you need to specify a performance measure. You can either
    define a *utility function* (or *fitness function*) that measures how *good* your
    model is, or you can define a *cost function* that measures how *bad* it is. For
    linear regression problems, people typically use a cost function that measures
    the distance between the linear model’s predictions and the training examples;
    the objective is to minimize this distance.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用模型之前，您需要定义参数值*θ*[0]和*θ*[1]。您如何知道哪些值会使您的模型表现最佳？要回答这个问题，您需要指定一个性能度量。您可以定义一个度量模型*好坏*的*效用函数*（或*适应函数*），也可以定义一个度量模型*坏*的*成本函数*。对于线性回归问题，人们通常使用一个测量线性模型预测与训练示例之间距离的成本函数；目标是最小化这个距离。
- en: 'This is where the linear regression algorithm comes in: you feed it your training
    examples, and it finds the parameters that make the linear model fit best to your
    data. This is called *training* the model. In our case, the algorithm finds that
    the optimal parameter values are *θ*[0] = 3.75 and *θ*[1] = 6.78 × 10^(–5).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是线性回归算法的作用：您将训练示例提供给它，它会找到使线性模型最适合您的数据的参数。这称为*训练*模型。在我们的情况下，算法发现最佳参数值为*θ*[0]
    = 3.75和*θ*[1] = 6.78 × 10^(–5)。
- en: Warning
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Confusingly, the word “model” can refer to a *type of model* (e.g., linear regression),
    to a *fully specified model architecture* (e.g., linear regression with one input
    and one output), or to the *final trained model* ready to be used for predictions
    (e.g., linear regression with one input and one output, using *θ*[0] = 3.75 and
    *θ*[1] = 6.78 × 10^(–5)). Model selection consists in choosing the type of model
    and fully specifying its architecture. Training a model means running an algorithm
    to find the model parameters that will make it best fit the training data, and
    hopefully make good predictions on new data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 令人困惑的是，“模型”这个词可以指一个*模型类型*（例如线性回归），也可以指一个*完全指定的模型架构*（例如具有一个输入和一个输出的线性回归），或者指准备用于预测的*最终训练好的模型*（例如具有一个输入和一个输出的线性回归，使用*θ*[0]
    = 3.75和*θ*[1] = 6.78 × 10^(–5)）。模型选择包括选择模型类型和完全指定其架构。训练模型意味着运行算法以找到使其最佳拟合训练数据的模型参数，并希望在新数据上做出良好的预测。
- en: Now the model fits the training data as closely as possible (for a linear model),
    as you can see in [Figure 1-20](#best_fit_model_plot).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型尽可能地拟合训练数据（对于线性模型），如您在[图1-20](#best_fit_model_plot)中所见。
- en: '![mls3 0120](assets/mls3_0120.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0120](assets/mls3_0120.png)'
- en: Figure 1-20\. The linear model that fits the training data best
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-20。最佳拟合训练数据的线性模型
- en: 'You are finally ready to run the model to make predictions. For example, say
    you want to know how happy Cypriots are, and the OECD data does not have the answer.
    Fortunately, you can use your model to make a good prediction: you look up Cyprus’s
    GDP per capita, find $37,655, and then apply your model and find that life satisfaction
    is likely to be somewhere around 3.75 + 37,655 × 6.78 × 10^(–5) = 6.30.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在准备运行模型进行预测。例如，假设您想知道塞浦路斯人有多幸福，而OECD数据没有答案。幸运的是，您可以使用您的模型进行良好的预测：查找塞浦路斯的人均GDP，找到37655美元，然后应用您的模型，发现生活满意度可能在3.75
    + 37655 × 6.78 × 10^(–5) = 6.30左右。
- en: To whet your appetite, [Example 1-1](#example_scikit_code) shows the Python
    code that loads the data, separates the inputs `X` from the labels `y`, creates
    a scatterplot for visualization, and then trains a linear model and makes a prediction.⁠^([5](ch01.html#idm45720251254096))
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激起您的兴趣，[示例1-1](#example_scikit_code)展示了加载数据、将输入`X`与标签`y`分开、创建散点图进行可视化、然后训练线性模型并进行预测的Python代码。⁠^([5](ch01.html#idm45720251254096))
- en: Example 1-1\. Training and running a linear model using Scikit-Learn
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例1-1。使用Scikit-Learn训练和运行线性模型
- en: '[PRE0]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you had used an instance-based learning algorithm instead, you would have
    found that Israel has the closest GDP per capita to that of Cyprus ($38,341),
    and since the OECD data tells us that Israelis’ life satisfaction is 7.2, you
    would have predicted a life satisfaction of 7.2 for Cyprus. If you zoom out a
    bit and look at the two next-closest countries, you will find Lithuania and Slovenia,
    both with a life satisfaction of 5.9\. Averaging these three values, you get 6.33,
    which is pretty close to your model-based prediction. This simple algorithm is
    called *k-nearest neighbors* regression (in this example, *k* = 3).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用了基于实例的学习算法，您会发现以色列的人均GDP最接近塞浦路斯的人均GDP（38341美元），由于OECD数据告诉我们以色列人的生活满意度为7.2，您可能会预测塞浦路斯的生活满意度为7.2。如果您稍微放大一点并查看两个最接近的国家，您会发现立陶宛和斯洛文尼亚，两者的生活满意度都为5.9。将这三个值平均，您会得到6.33，这与基于模型的预测非常接近。这个简单的算法称为*k-最近邻*回归（在这个例子中，*k*
    = 3）。
- en: 'Replacing the linear regression model with *k*-nearest neighbors regression
    in the previous code is as easy as replacing these lines:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中用*k*-最近邻回归替换线性回归模型就像替换这些行一样容易：
- en: '[PRE1]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'with these two:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个：
- en: '[PRE2]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If all went well, your model will make good predictions. If not, you may need
    to use more attributes (employment rate, health, air pollution, etc.), get more
    or better-quality training data, or perhaps select a more powerful model (e.g.,
    a polynomial regression model).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您的模型将做出良好的预测。如果不是，您可能需要使用更多属性（就业率、健康、空气污染等）、获取更多或更高质量的训练数据，或者选择一个更强大的模型（例如多项式回归模型）。
- en: 'In summary:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: You studied the data.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您研究了数据。
- en: You selected a model.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您选择了一个模型。
- en: You trained it on the training data (i.e., the learning algorithm searched for
    the model parameter values that minimize a cost function).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在训练数据上对其进行了训练（即，学习算法搜索使模型参数值最小化成本函数）。
- en: Finally, you applied the model to make predictions on new cases (this is called
    *inference*), hoping that this model will generalize well.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，您应用模型对新案例进行预测（这称为*推断*），希望这个模型能很好地泛化。
- en: This is what a typical machine learning project looks like. In [Chapter 2](ch02.html#project_chapter)
    you will experience this firsthand by going through a project end to end.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个典型的机器学习项目的样子。在[第2章](ch02.html#project_chapter)中，您将通过从头到尾完成一个项目来亲身体验这一过程。
- en: 'We have covered a lot of ground so far: you now know what machine learning
    is really about, why it is useful, what some of the most common categories of
    ML systems are, and what a typical project workflow looks like. Now let’s look
    at what can go wrong in learning and prevent you from making accurate predictions.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了很多内容：您现在知道机器学习真正关注的是什么，为什么它有用，一些最常见的ML系统类别是什么，以及典型项目工作流程是什么样的。现在让我们看看在学习过程中可能出现的问题，阻止您做出准确的预测。
- en: Main Challenges of Machine Learning
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的主要挑战
- en: In short, since your main task is to select a model and train it on some data,
    the two things that can go wrong are “bad model” and “bad data”. Let’s start with
    examples of bad data.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，由于您的主要任务是选择一个模型并在一些数据上进行训练，可能出错的两个因素是“坏模型”和“坏数据”。让我们从坏数据的例子开始。
- en: Insufficient Quantity of Training Data
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据数量不足
- en: For a toddler to learn what an apple is, all it takes is for you to point to
    an apple and say “apple” (possibly repeating this procedure a few times). Now
    the child is able to recognize apples in all sorts of colors and shapes. Genius.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个幼儿来学习什么是苹果，只需要您指向一个苹果并说“苹果”（可能需要重复几次这个过程）。现在孩子能够识别各种颜色和形状的苹果了。天才。
- en: Machine learning is not quite there yet; it takes a lot of data for most machine
    learning algorithms to work properly. Even for very simple problems you typically
    need thousands of examples, and for complex problems such as image or speech recognition
    you may need millions of examples (unless you can reuse parts of an existing model).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习还没有完全成熟；大多数机器学习算法需要大量数据才能正常工作。即使对于非常简单的问题，您通常也需要成千上万的示例，而对于像图像或语音识别这样的复杂问题，您可能需要数百万的示例（除非您可以重用现有模型的部分）。
- en: Nonrepresentative Training Data
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非代表性训练数据
- en: In order to generalize well, it is crucial that your training data be representative
    of the new cases you want to generalize to. This is true whether you use instance-based
    learning or model-based learning.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了很好地泛化，您的训练数据必须代表您想要泛化到的新案例。无论您使用基于实例的学习还是基于模型的学习，这一点都是真实的。
- en: For example, the set of countries you used earlier for training the linear model
    was not perfectly representative; it did not contain any country with a GDP per
    capita lower than $23,500 or higher than $62,500\. [Figure 1-22](#representative_training_data_scatterplot)
    shows what the data looks like when you add such countries.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您之前用于训练线性模型的国家集合并不完全代表性；它不包含人均GDP低于23,500美元或高于62,500美元的任何国家。[图1-22](#representative_training_data_scatterplot)展示了当您添加这样的国家时数据的样子。
- en: If you train a linear model on this data, you get the solid line, while the
    old model is represented by the dotted line. As you can see, not only does adding
    a few missing countries significantly alter the model, but it makes it clear that
    such a simple linear model is probably never going to work well. It seems that
    very rich countries are not happier than moderately rich countries (in fact, they
    seem slightly unhappier!), and conversely some poor countries seem happier than
    many rich countries.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在这些数据上训练线性模型，您将得到实线，而旧模型由虚线表示。正如您所看到的，不仅添加一些缺失的国家会显著改变模型，而且清楚地表明这样一个简单的线性模型可能永远不会很好地工作。似乎非常富裕的国家并不比中等富裕的国家更幸福（事实上，它们似乎稍微不那么幸福！），反之，一些贫穷的国家似乎比许多富裕国家更幸福。
- en: By using a nonrepresentative training set, you trained a model that is unlikely
    to make accurate predictions, especially for very poor and very rich countries.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用非代表性的训练集，您训练了一个不太可能做出准确预测的模型，特别是对于非常贫穷和非常富裕的国家。
- en: '![mls3 0122](assets/mls3_0122.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0122](assets/mls3_0122.png)'
- en: Figure 1-22\. A more representative training sample
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-22。更具代表性的训练样本
- en: 'It is crucial to use a training set that is representative of the cases you
    want to generalize to. This is often harder than it sounds: if the sample is too
    small, you will have *sampling noise* (i.e., nonrepresentative data as a result
    of chance), but even very large samples can be nonrepresentative if the sampling
    method is flawed. This is called *sampling bias*.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用代表您想要泛化到的案例的训练集至关重要。这通常比听起来更难：如果样本太小，您将会有*抽样误差*（即由于偶然性导致的非代表性数据），但即使是非常大的样本也可能是非代表性的，如果抽样方法有缺陷的话。这被称为*抽样偏差*。
- en: Poor-Quality Data
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低质量数据
- en: 'Obviously, if your training data is full of errors, outliers, and noise (e.g.,
    due to poor-quality measurements), it will make it harder for the system to detect
    the underlying patterns, so your system is less likely to perform well. It is
    often well worth the effort to spend time cleaning up your training data. The
    truth is, most data scientists spend a significant part of their time doing just
    that. The following are a couple examples of when you’d want to clean up training
    data:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果您的训练数据中充满错误、异常值和噪音（例如，由于质量不佳的测量），这将使系统更难检测到潜在的模式，因此您的系统更不可能表现良好。花时间清理训练数据通常是非常值得的。事实上，大多数数据科学家花费大部分时间就是在做这件事。以下是您需要清理训练数据的几个例子：
- en: If some instances are clearly outliers, it may help to simply discard them or
    try to fix the errors manually.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一些实例明显是异常值，简单地丢弃它们或尝试手动修复错误可能有所帮助。
- en: If some instances are missing a few features (e.g., 5% of your customers did
    not specify their age), you must decide whether you want to ignore this attribute
    altogether, ignore these instances, fill in the missing values (e.g., with the
    median age), or train one model with the feature and one model without it.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一些实例缺少一些特征（例如，5%的客户没有指定年龄），您必须决定是否要完全忽略这个属性，忽略这些实例，填补缺失值（例如，用中位数年龄），或者训练一个带有该特征的模型和一个不带该特征的模型。
- en: Irrelevant Features
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无关特征
- en: 'As the saying goes: garbage in, garbage out. Your system will only be capable
    of learning if the training data contains enough relevant features and not too
    many irrelevant ones. A critical part of the success of a machine learning project
    is coming up with a good set of features to train on. This process, called *feature
    engineering*, involves the following steps:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 俗话说：垃圾进，垃圾出。只有当训练数据包含足够相关的特征而不包含太多无关的特征时，您的系统才能学习。机器学习项目成功的关键部分之一是提出一组良好的特征进行训练。这个过程被称为*特征工程*，包括以下步骤：
- en: '*Feature selection* (selecting the most useful features to train on among existing
    features)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征选择*（在现有特征中选择最有用的特征进行训练）'
- en: '*Feature extraction* (combining existing features to produce a more useful
    one⁠—as we saw earlier, dimensionality reduction algorithms can help)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征提取*（将现有特征组合以生成更有用的特征——正如我们之前看到的，降维算法可以帮助）'
- en: Creating new features by gathering new data
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过收集新数据创建新特征
- en: Now that we have looked at many examples of bad data, let’s look at a couple
    examples of bad algorithms.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看了很多坏数据的例子，让我们看一看一些坏算法的例子。
- en: Overfitting the Training Data
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合训练数据
- en: 'Say you are visiting a foreign country and the taxi driver rips you off. You
    might be tempted to say that *all* taxi drivers in that country are thieves. Overgeneralizing
    is something that we humans do all too often, and unfortunately machines can fall
    into the same trap if we are not careful. In machine learning this is called *overfitting*:
    it means that the model performs well on the training data, but it does not generalize
    well.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您正在访问一个外国国家，出租车司机宰客。您可能会说那个国家的*所有*出租车司机都是小偷。过度概括是我们人类经常做的事情，不幸的是，如果我们不小心，机器也会陷入同样的陷阱。在机器学习中，这被称为*过拟合*：这意味着模型在训练数据上表现良好，但在泛化上表现不佳。
- en: '[Figure 1-23](#overfitting_model_plot) shows an example of a high-degree polynomial
    life satisfaction model that strongly overfits the training data. Even though
    it performs much better on the training data than the simple linear model, would
    you really trust its predictions?'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-23](#overfitting_model_plot)显示了一个高次多项式生活满意度模型的例子，它在训练数据上过拟合。尽管它在训练数据上的表现比简单线性模型要好得多，但您真的会相信它的预测吗？'
- en: '![mls3 0123](assets/mls3_0123.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0123](assets/mls3_0123.png)'
- en: Figure 1-23\. Overfitting the training data
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-23\. 过拟合训练数据
- en: 'Complex models such as deep neural networks can detect subtle patterns in the
    data, but if the training set is noisy, or if it is too small, which introduces
    sampling noise, then the model is likely to detect patterns in the noise itself
    (as in the taxi driver example). Obviously these patterns will not generalize
    to new instances. For example, say you feed your life satisfaction model many
    more attributes, including uninformative ones such as the country’s name. In that
    case, a complex model may detect patterns like the fact that all countries in
    the training data with a *w* in their name have a life satisfaction greater than
    7: New Zealand (7.3), Norway (7.6), Sweden (7.3), and Switzerland (7.5). How confident
    are you that the *w*-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously
    this pattern occurred in the training data by pure chance, but the model has no
    way to tell whether a pattern is real or simply the result of noise in the data.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 像深度神经网络这样的复杂模型可以检测数据中的微妙模式，但如果训练集存在噪声，或者太小，引入了抽样误差，那么模型很可能会检测到噪声本身中的模式（就像出租车司机的例子）。显然，这些模式不会推广到新实例。例如，假设您向生活满意度模型提供了更多属性，包括无信息的属性，如国家名称。在这种情况下，复杂模型可能会检测到模式，比如训练数据中所有带有*W*的国家名称的生活满意度大于7：新西兰（7.3）、挪威（7.6）、瑞典（7.3）和瑞士（7.5）。您有多大信心认为*W*-满意度规则适用于卢旺达或津巴布韦？显然，这种模式纯粹是偶然出现在训练数据中，但模型无法判断一个模式是真实的还是仅仅是数据中的噪声所致。
- en: Warning
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Overfitting happens when the model is too complex relative to the amount and
    noisiness of the training data. Here are possible solutions:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合发生在模型相对于训练数据的数量和噪声过多时。以下是可能的解决方案：
- en: Simplify the model by selecting one with fewer parameters (e.g., a linear model
    rather than a high-degree polynomial model), by reducing the number of attributes
    in the training data, or by constraining the model.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择具有较少参数的模型（例如，线性模型而不是高次多项式模型）、减少训练数据中的属性数量或约束模型来简化模型。
- en: Gather more training data.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多的训练数据。
- en: Reduce the noise in the training data (e.g., fix data errors and remove outliers).
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少训练数据中的噪声（例如，修复数据错误和删除异常值）。
- en: 'Constraining a model to make it simpler and reduce the risk of overfitting
    is called *regularization*. For example, the linear model we defined earlier has
    two parameters, *θ*[0] and *θ*[1]. This gives the learning algorithm two *degrees
    of freedom* to adapt the model to the training data: it can tweak both the height
    (*θ*[0]) and the slope (*θ*[1]) of the line. If we forced *θ*[1] = 0, the algorithm
    would have only one degree of freedom and would have a much harder time fitting
    the data properly: all it could do is move the line up or down to get as close
    as possible to the training instances, so it would end up around the mean. A very
    simple model indeed! If we allow the algorithm to modify *θ*[1] but we force it
    to keep it small, then the learning algorithm will effectively have somewhere
    in between one and two degrees of freedom. It will produce a model that’s simpler
    than one with two degrees of freedom, but more complex than one with just one.
    You want to find the right balance between fitting the training data perfectly
    and keeping the model simple enough to ensure that it will generalize well.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对模型进行约束使其变得更简单并减少过拟合风险称为*正则化*。例如，我们之前定义的线性模型有两个参数，*θ*[0]和*θ*[1]。这给学习算法两个*自由度*来调整模型以适应训练数据：它可以调整线的高度（*θ*[0]）和斜率（*θ*[1]）。如果我们强制*θ*[1]
    = 0，算法将只有一个自由度，并且更难正确拟合数据：它只能上下移动线以尽可能接近训练实例，因此最终会在均值附近。一个非常简单的模型！如果允许算法修改*θ*[1]但强制保持较小，则学习算法实际上将在一个自由度和两个自由度之间。它将产生一个比具有两个自由度的模型更简单，但比只有一个自由度的模型更复杂的模型。您希望找到完全拟合训练数据并保持模型足够简单以确保其良好泛化的正确平衡。
- en: '[Figure 1-24](#ridge_model_plot) shows three models. The dotted line represents
    the original model that was trained on the countries represented as circles (without
    the countries represented as squares), the solid line is our second model trained
    with all countries (circles and squares), and the dashed line is a model trained
    with the same data as the first model but with a regularization constraint. You
    can see that regularization forced the model to have a smaller slope: this model
    does not fit the training data (circles) as well as the first model, but it actually
    generalizes better to new examples that it did not see during training (squares).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-24](#ridge_model_plot)展示了三个模型。虚线代表原始模型，该模型是在以圆圈表示的国家上进行训练的（不包括以方块表示的国家），实线是我们的第二个模型，训练了所有国家（圆圈和方块），虚线是一个使用与第一个模型相同数据进行训练但带有正则化约束的模型。您可以看到正则化强制模型具有较小的斜率：这个模型不像第一个模型那样很好地拟合训练数据（圆圈），但实际上更好地泛化到在训练过程中没有见过的新示例（方块）。'
- en: '![mls3 0124](assets/mls3_0124.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0124](assets/mls3_0124.png)'
- en: Figure 1-24\. Regularization reduces the risk of overfitting
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-24\. 正则化减少过拟合风险
- en: The amount of regularization to apply during learning can be controlled by a
    *hyperparameter*. A hyperparameter is a parameter of a learning algorithm (not
    of the model). As such, it is not affected by the learning algorithm itself; it
    must be set prior to training and remains constant during training. If you set
    the regularization hyperparameter to a very large value, you will get an almost
    flat model (a slope close to zero); the learning algorithm will almost certainly
    not overfit the training data, but it will be less likely to find a good solution.
    Tuning hyperparameters is an important part of building a machine learning system
    (you will see a detailed example in the next chapter).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中应用的正则化量可以通过*超参数*来控制。超参数是学习算法的参数（而不是模型的参数）。因此，它不受学习算法本身的影响；它必须在训练之前设置，并在训练过程中保持不变。如果将正则化超参数设置为非常大的值，您将得到一个几乎平坦的模型（斜率接近零）；学习算法几乎肯定不会过度拟合训练数据，但更不太可能找到一个好的解决方案。调整超参数是构建机器学习系统的重要部分（您将在下一章中看到一个详细的示例）。
- en: Underfitting the Training Data
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据欠拟合
- en: 'As you might guess, *underfitting* is the opposite of overfitting: it occurs
    when your model is too simple to learn the underlying structure of the data. For
    example, a linear model of life satisfaction is prone to underfit; reality is
    just more complex than the model, so its predictions are bound to be inaccurate,
    even on the training examples.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能猜到的，*欠拟合*是过拟合的相反：当您的模型过于简单而无法学习数据的基本结构时，就会发生欠拟合。例如，对生活满意度的线性模型容易发生欠拟合；现实比模型更复杂，因此其预测很可能不准确，即使在训练示例上也是如此。
- en: 'Here are the main options for fixing this problem:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解决此问题的主要选项：
- en: Select a more powerful model, with more parameters.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个更强大的模型，具有更多参数。
- en: Feed better features to the learning algorithm (feature engineering).
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向学习算法提供更好的特征（特征工程）。
- en: Reduce the constraints on the model (for example by reducing the regularization
    hyperparameter).
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型的约束（例如通过减少正则化超参数）。
- en: Stepping Back
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 退一步
- en: 'By now you know a lot about machine learning. However, we went through so many
    concepts that you may be feeling a little lost, so let’s step back and look at
    the big picture:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经了解了很多关于机器学习的知识。然而，我们讨论了许多概念，您可能感到有点迷茫，所以让我们退一步，看看整体情况：
- en: Machine learning is about making machines get better at some task by learning
    from data, instead of having to explicitly code rules.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是通过从数据中学习来使机器在某些任务上变得更好，而不是必须明确编写规则。
- en: 'There are many different types of ML systems: supervised or not, batch or online,
    instance-based or model-based.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多不同类型的ML系统：监督或非监督，批处理或在线，基于实例或基于模型。
- en: In an ML project you gather data in a training set, and you feed the training
    set to a learning algorithm. If the algorithm is model-based, it tunes some parameters
    to fit the model to the training set (i.e., to make good predictions on the training
    set itself), and then hopefully it will be able to make good predictions on new
    cases as well. If the algorithm is instance-based, it just learns the examples
    by heart and generalizes to new instances by using a similarity measure to compare
    them to the learned instances.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个机器学习项目中，你在训练集中收集数据，并将训练集提供给学习算法。如果算法是基于模型的，它会调整一些参数以使模型适应训练集（即，在训练集上做出良好的预测），然后希望它也能在新案例上做出良好的预测。如果算法是基于实例的，它只是通过记忆例子并使用相似度度量来将它们与学习的实例进行比较，从而推广到新实例。
- en: The system will not perform well if your training set is too small, or if the
    data is not representative, is noisy, or is polluted with irrelevant features
    (garbage in, garbage out). Lastly, your model needs to be neither too simple (in
    which case it will underfit) nor too complex (in which case it will overfit).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的训练集太小，或者数据不具代表性，噪音太大，或者包含无关特征（垃圾进，垃圾出），系统将无法表现良好。最后，你的模型既不能太简单（这样它会欠拟合），也不能太复杂（这样它会过拟合）。
- en: 'There’s just one last important topic to cover: once you have trained a model,
    you don’t want to just “hope” it generalizes to new cases. You want to evaluate
    it and fine-tune it if necessary. Let’s see how to do that.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个重要的主题要讨论：一旦你训练了一个模型，你不希望只是“希望”它能推广到新案例。你希望对其进行评估，并在必要时进行微调。让我们看看如何做到这一点。
- en: Testing and Validating
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和验证
- en: The only way to know how well a model will generalize to new cases is to actually
    try it out on new cases. One way to do that is to put your model in production
    and monitor how well it performs. This works well, but if your model is horribly
    bad, your users will complain—not the best idea.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 了解一个模型将如何推广到新案例的唯一方法是实际在新案例上尝试。一种方法是将你的模型投入生产并监控其表现。这种方法效果很好，但如果你的模型非常糟糕，用户会抱怨——这不是最好的主意。
- en: 'A better option is to split your data into two sets: the *training set* and
    the *test set*. As these names imply, you train your model using the training
    set, and you test it using the test set. The error rate on new cases is called
    the *generalization error* (or *out-of-sample error*), and by evaluating your
    model on the test set, you get an estimate of this error. This value tells you
    how well your model will perform on instances it has never seen before.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的选择是将数据分为两组：*训练集*和*测试集*。正如这些名称所暗示的，你使用训练集训练你的模型，并使用测试集测试它。新案例的错误率称为*泛化误差*（或*样本外误差*），通过在测试集上评估你的模型，你可以得到这个误差的估计。这个值告诉你你的模型在它从未见过的实例上的表现如何。
- en: If the training error is low (i.e., your model makes few mistakes on the training
    set) but the generalization error is high, it means that your model is overfitting
    the training data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练误差很低（即，你的模型在训练集上犯了很少的错误），但泛化误差很高，这意味着你的模型正在过拟合训练数据。
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'It is common to use 80% of the data for training and *hold out* 20% for testing.
    However, this depends on the size of the dataset: if it contains 10 million instances,
    then holding out 1% means your test set will contain 100,000 instances, probably
    more than enough to get a good estimate of the generalization error.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用80%的数据进行训练，*保留*20%用于测试。但是，这取决于数据集的大小：如果包含1000万个实例，那么保留1%意味着你的测试集将包含100,000个实例，可能足够得到泛化误差的良好估计。
- en: Hyperparameter Tuning and Model Selection
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整和模型选择
- en: 'Evaluating a model is simple enough: just use a test set. But suppose you are
    hesitating between two types of models (say, a linear model and a polynomial model):
    how can you decide between them? One option is to train both and compare how well
    they generalize using the test set.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个模型很简单：只需使用一个测试集。但是假设你在两种类型的模型之间犹豫不决（比如，线性模型和多项式模型）：你如何决定呢？一种选择是训练两种模型，并比较它们在测试集上的泛化效果。
- en: Now suppose that the linear model generalizes better, but you want to apply
    some regularization to avoid overfitting. The question is, how do you choose the
    value of the regularization hyperparameter? One option is to train 100 different
    models using 100 different values for this hyperparameter. Suppose you find the
    best hyperparameter value that produces a model with the lowest generalization
    error⁠—say, just 5% error. You launch this model into production, but unfortunately
    it does not perform as well as expected and produces 15% errors. What just happened?
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设线性模型泛化效果更好，但你想应用一些正则化来避免过拟合。问题是，你如何选择正则化超参数的值？一种选择是使用100个不同的值训练100个不同的模型。假设你找到了一个最佳的超参数值，可以产生泛化误差最低的模型——比如，只有5%的误差。你将这个模型投入生产，但不幸的是它的表现并不如预期，产生了15%的错误。发生了什么？
- en: The problem is that you measured the generalization error multiple times on
    the test set, and you adapted the model and hyperparameters to produce the best
    model *for that particular set*. This means the model is unlikely to perform as
    well on new data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于你在测试集上多次测量了泛化误差，并且调整了模型和超参数以产生最佳模型*针对那个特定集合*。这意味着该模型不太可能在新数据上表现得很好。
- en: 'A common solution to this problem is called *holdout validation* ([Figure 1-25](#hyperparameter_tuning_diagram)):
    you simply hold out part of the training set to evaluate several candidate models
    and select the best one. The new held-out set is called the *validation set* (or
    the *development set*, or *dev set*). More specifically, you train multiple models
    with various hyperparameters on the reduced training set (i.e., the full training
    set minus the validation set), and you select the model that performs best on
    the validation set. After this holdout validation process, you train the best
    model on the full training set (including the validation set), and this gives
    you the final model. Lastly, you evaluate this final model on the test set to
    get an estimate of the generalization error.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的常见方法称为*留出验证*（[图1-25](#hyperparameter_tuning_diagram)）：您只需留出部分训练集来评估几个候选模型并选择最佳模型。新的留出集称为*验证集*（或*开发集*，或*开发集*）。更具体地说，您在减少的训练集上（即完整训练集减去验证集）上训练多个具有不同超参数的模型，并选择在验证集上表现最佳的模型。在进行留出验证过程之后，您在完整训练集上（包括验证集）训练最佳模型，这将为您提供最终模型。最后，您评估这个最终模型在测试集上，以获得泛化误差的估计。
- en: '![mls3 0125](assets/mls3_0125.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0125](assets/mls3_0125.png)'
- en: Figure 1-25\. Model selection using holdout validation
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-25\. 使用留出验证进行模型选择
- en: 'This solution usually works quite well. However, if the validation set is too
    small, then the model evaluations will be imprecise: you may end up selecting
    a suboptimal model by mistake. Conversely, if the validation set is too large,
    then the remaining training set will be much smaller than the full training set.
    Why is this bad? Well, since the final model will be trained on the full training
    set, it is not ideal to compare candidate models trained on a much smaller training
    set. It would be like selecting the fastest sprinter to participate in a marathon.
    One way to solve this problem is to perform repeated *cross-validation*, using
    many small validation sets. Each model is evaluated once per validation set after
    it is trained on the rest of the data. By averaging out all the evaluations of
    a model, you get a much more accurate measure of its performance. There is a drawback,
    however: the training time is multiplied by the number of validation sets.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案通常效果很好。然而，如果验证集太小，则模型评估将不够精确：您可能会错误地选择次优模型。相反，如果验证集太大，则剩余的训练集将比完整训练集小得多。为什么这样不好呢？因为最终模型将在完整训练集上训练，所以将候选模型训练在一个小得多的训练集上进行比较并不理想。这就好比选择最快的短跑选手参加马拉松比赛。解决这个问题的一种方法是执行重复的*交叉验证*，使用许多小的验证集。每个模型在其余数据上训练后，每个验证集对其进行一次评估。通过对模型的所有评估进行平均，您将获得更准确的性能度量。然而，这种方法的一个缺点是：训练时间将乘以验证集的数量。
- en: Data Mismatch
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据不匹配
- en: In some cases, it’s easy to get a large amount of data for training, but this
    data probably won’t be perfectly representative of the data that will be used
    in production. For example, suppose you want to create a mobile app to take pictures
    of flowers and automatically determine their species. You can easily download
    millions of pictures of flowers on the web, but they won’t be perfectly representative
    of the pictures that will actually be taken using the app on a mobile device.
    Perhaps you only have 1,000 representative pictures (i.e., actually taken with
    the app).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，很容易获得大量用于训练的数据，但这些数据可能不会完全代表将在生产中使用的数据。例如，假设您想创建一个移动应用程序来拍摄花朵并自动确定它们的种类。您可以轻松地在网上下载数百万张花朵图片，但它们不会完全代表实际使用移动设备上应用程序拍摄的图片。也许您只有1,000张代表性图片（即实际使用应用程序拍摄的图片）。
- en: 'In this case, the most important rule to remember is that both the validation
    set and the test set must be as representative as possible of the data you expect
    to use in production, so they should be composed exclusively of representative
    pictures: you can shuffle them and put half in the validation set and half in
    the test set (making sure that no duplicates or near-duplicates end up in both
    sets). After training your model on the web pictures, if you observe that the
    performance of the model on the validation set is disappointing, you will not
    know whether this is because your model has overfit the training set, or whether
    this is just due to the mismatch between the web pictures and the mobile app pictures.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最重要的规则是记住验证集和测试集必须尽可能代表您预期在生产中使用的数据，因此它们应该完全由代表性图片组成：您可以对它们进行洗牌，并将一半放入验证集，一半放入测试集（确保没有重复或近似重复的图片同时出现在两个集合中）。在对网络图片训练模型后，如果您观察到模型在验证集上的表现令人失望，您将不知道这是因为您的模型已经过度拟合训练集，还是仅仅是由于网络图片和移动应用程序图片之间的不匹配。
- en: One solution is to hold out some of the training pictures (from the web) in
    yet another set that Andrew Ng dubbed the *train-dev set* ([Figure 1-26](#train_dev_diagram)).
    After the model is trained (on the training set, *not* on the train-dev set),
    you can evaluate it on the train-dev set. If the model performs poorly, then it
    must have overfit the training set, so you should try to simplify or regularize
    the model, get more training data, and clean up the training data. But if it performs
    well on the train-dev set, then you can evaluate the model on the dev set. If
    it performs poorly, then the problem must be coming from the data mismatch. You
    can try to tackle this problem by preprocessing the web images to make them look
    more like the pictures that will be taken by the mobile app, and then retraining
    the model. Once you have a model that performs well on both the train-dev set
    and the dev set, you can evaluate it one last time on the test set to know how
    well it is likely to perform in production.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是在另一个由Andrew Ng命名为*训练-开发集*的集合中保留一些训练图片（来自网络）（[图1-26](#train_dev_diagram)）。在模型训练完成后（在训练集上，*不是*在训练-开发集上），您可以在训练-开发集上评估它。如果模型表现不佳，则必须过度拟合训练集，因此应尝试简化或正则化模型，获取更多训练数据，并清理训练数据。但如果模型在训练-开发集上表现良好，则可以在开发集上评估模型。如果模型在开发集上表现不佳，则问题可能来自数据不匹配。您可以尝试通过预处理网络图像使其看起来更像移动应用程序将拍摄的图片，然后重新训练模型来解决此问题。一旦您有一个在训练-开发集和开发集上表现良好的模型，您可以最后一次在测试集上评估它，以了解它在生产中的表现如何。
- en: '![mls3 0126](assets/mls3_0126.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0126](assets/mls3_0126.png)'
- en: Figure 1-26\. When real data is scarce (right), you may use similar abundant
    data (left) for training and hold out some of it in a train-dev set to evaluate
    overfitting; the real data is then used to evaluate data mismatch (dev set) and
    to evaluate the final model’s performance (test set)
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-26。当真实数据稀缺时（右侧），您可以使用类似丰富的数据（左侧）进行训练，并在训练-开发集中保留一些数据以评估过拟合；然后使用真实数据来评估数据不匹配（开发集）并评估最终模型的性能（测试集）
- en: Exercises
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'In this chapter we have covered some of the most important concepts in machine
    learning. In the next chapters we will dive deeper and write more code, but before
    we do, make sure you can answer the following questions:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经介绍了机器学习中一些最重要的概念。在接下来的章节中，我们将深入探讨并编写更多代码，但在此之前，请确保您能回答以下问题：
- en: How would you define machine learning?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您如何定义机器学习？
- en: Can you name four types of applications where it shines?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您能否列出四种应用程序的类型，它们在哪些方面表现出色？
- en: What is a labeled training set?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是标记训练集？
- en: What are the two most common supervised tasks?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最常见的两种监督任务是什么？
- en: Can you name four common unsupervised tasks?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您能否列出四种常见的无监督任务？
- en: What type of algorithm would you use to allow a robot to walk in various unknown
    terrains?
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您会使用什么类型的算法来允许机器人在各种未知地形中行走？
- en: What type of algorithm would you use to segment your customers into multiple
    groups?
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您会使用什么类型的算法将客户分成多个群组？
- en: Would you frame the problem of spam detection as a supervised learning problem
    or an unsupervised learning problem?
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您会将垃圾邮件检测问题框定为监督学习问题还是无监督学习问题？
- en: What is an online learning system?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是在线学习系统？
- en: What is out-of-core learning?
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是离线学习？
- en: What type of algorithm relies on a similarity measure to make predictions?
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 依赖相似度度量进行预测的算法类型是什么？
- en: What is the difference between a model parameter and a model hyperparameter?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型参数和模型超参数之间有什么区别？
- en: What do model-based algorithms search for? What is the most common strategy
    they use to succeed? How do they make predictions?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于模型的算法搜索什么？它们成功的最常见策略是什么？它们如何进行预测？
- en: Can you name four of the main challenges in machine learning?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您能否列出机器学习中的四个主要挑战？
- en: If your model performs great on the training data but generalizes poorly to
    new instances, what is happening? Can you name three possible solutions?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的模型在训练数据上表现良好，但对新实例的泛化能力差，那么发生了什么？您能否列出三种可能的解决方案？
- en: What is a test set, and why would you want to use it?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是测试集，为什么要使用它？
- en: What is the purpose of a validation set?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证集的目的是什么？
- en: What is the train-dev set, when do you need it, and how do you use it?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是训练-开发集，何时需要它，以及如何使用它？
- en: What can go wrong if you tune hyperparameters using the test set?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果使用测试集调整超参数会出现什么问题？
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: '^([1](ch01.html#idm45720250412768-marker)) Fun fact: this odd-sounding name
    is a statistics term introduced by Francis Galton while he was studying the fact
    that the children of tall people tend to be shorter than their parents. Since
    the children were shorter, he called this *regression to the mean*. This name
    was then applied to the methods he used to analyze correlations between variables.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.html#idm45720250412768-marker)) 有趣的事实：这个听起来奇怪的名字是由弗朗西斯·高尔顿引入的统计术语，当时他正在研究高个子父母的孩子往往比父母矮的事实。由于孩子们较矮，他将此称为*回归到平均值*。然后，这个名字被应用于他用来分析变量之间相关性的方法。
- en: '^([2](ch01.html#idm45720242786608-marker)) Notice how animals are rather well
    separated from vehicles and how horses are close to deer but far from birds. Figure
    reproduced with permission from Richard Socher et al., “Zero-Shot Learning Through
    Cross-Modal Transfer”, *Proceedings of the 26th International Conference on Neural
    Information Processing Systems* 1 (2013): 935–943.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.html#idm45720242786608-marker)) 请注意，动物与车辆相当分离，马与鹿接近但与鸟类相距甚远。图由Richard
    Socher等人允许复制，来源于“通过跨模态转移实现零样本学习”，*第26届国际神经信息处理系统会议论文集* 1（2013）：935-943。
- en: ^([3](ch01.html#idm45720251565632-marker)) That’s when the system works perfectly.
    In practice it often creates a few clusters per person, and sometimes mixes up
    two people who look alike, so you may need to provide a few labels per person
    and manually clean up some clusters.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.html#idm45720251565632-marker)) 这就是系统完美运行的时候。在实践中，它经常为每个人创建几个簇，并有时会混淆看起来相似的两个人，因此您可能需要为每个人提供一些标签，并手动清理一些簇。
- en: ^([4](ch01.html#idm45720251146336-marker)) By convention, the Greek letter *θ*
    (theta) is frequently used to represent model parameters.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.html#idm45720251146336-marker)) 按照惯例，希腊字母*θ*（theta）经常用于表示模型参数。
- en: ^([5](ch01.html#idm45720251254096-marker)) It’s OK if you don’t understand all
    the code yet; I will present Scikit-Learn in the following chapters.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch01.html#idm45720251254096-marker)) 如果您还不理解所有的代码，没关系；我将在接下来的章节中介绍Scikit-Learn。
- en: ^([6](ch01.html#idm45720251896896-marker)) For example, knowing whether to write
    “to”, “two”, or “too”, depending on the context.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch01.html#idm45720251896896-marker)) 例如，根据上下文知道是写“to”、“two”还是“too”。
- en: '^([7](ch01.html#idm45720251893776-marker)) Peter Norvig et al., “The Unreasonable
    Effectiveness of Data”, *IEEE Intelligent Systems* 24, no. 2 (2009): 8–12.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch01.html#idm45720251893776-marker)) Peter Norvig等，“数据的不合理有效性”，《IEEE智能系统》24卷2期（2009年）：8–12。
- en: '^([8](ch01.html#idm45720251890560-marker)) Figure reproduced with permission
    from Michele Banko and Eric Brill, “Scaling to Very Very Large Corpora for Natural
    Language Disambiguation”, *Proceedings of the 39th Annual Meeting of the Association
    for Computational Linguistics* (2001): 26–33.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch01.html#idm45720251890560-marker)) 图片经Michele Banko和Eric Brill许可重印，“用于自然语言消歧的非常大的语料库的扩展”，《计算语言学协会第39届年会论文集》（2001年）：26–33。
- en: '^([9](ch01.html#idm45720245579888-marker)) David Wolpert, “The Lack of A Priori
    Distinctions Between Learning Algorithms”, *Neural Computation* 8, no. 7 (1996):
    1341–1390.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch01.html#idm45720245579888-marker)) David Wolpert，“学习算法之间缺乏先验区别”，《神经计算》8卷7期（1996年）：1341–1390。
