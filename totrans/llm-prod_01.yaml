- en: '2 Large language models: A deep dive into language modeling'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 大型语言模型：深入语言建模
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The linguistic background for understanding meaning and interpretation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解意义和解释的语言背景
- en: A comparative study of language modeling techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模技术的比较研究
- en: Attention and the transformer architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力和transformer架构
- en: How large language models both fit into and build upon these histories
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型如何融入并建立在这些历史之上
- en: If you know the enemy and know yourself, you need not fear the result of a hundred
    battles.—Sun Tzu
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你了解敌人，也了解自己，你就无需害怕百战百胜的结果。——孙子
- en: This chapter delves into linguistics as it relates to the development of LLMs,
    exploring the foundations of semiotics, linguistic features, and the progression
    of language modeling techniques that have shaped the field of natural language
    processing (NLP). We will begin by studying the basics of linguistics and its
    relevance to LLMs, highlighting key concepts such as syntax, semantics, and pragmatics
    that form the basis of natural language and play a crucial role in the functioning
    of LLMs. We will delve into semiotics, the study of signs and symbols, and explore
    how its principles have informed the design and interpretation of LLMs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨与LLMs发展相关的语言学，探索符号学的基础、语言特征以及塑造自然语言处理（NLP）领域的语言建模技术的进展。我们将从研究语言学的基础及其与LLMs的相关性开始，强调语法、语义和语用学等关键概念，这些概念构成了自然语言的基础，并在LLMs的功能中发挥着至关重要的作用。我们将深入研究符号学，即对符号和符号的研究，并探讨其原则如何指导LLMs的设计和解释。
- en: We will then trace the evolution of language modeling techniques, providing
    an overview of early approaches, including N-grams, naive Bayes classifiers, and
    neural network-based methods such as multilayer perceptrons (MLPs), recurrent
    neural networks (RNNs), and long short-term memory (LSTM) networks. We will also
    discuss the groundbreaking shift to transformer-based models that laid the foundation
    for the emergence of LLMs, which are really just big transformer-based models.
    Finally, we will introduce LLMs and their distinguishing features, discussing
    how they have built upon and surpassed earlier language modeling techniques to
    revolutionize the field of NLP.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将追溯语言建模技术的演变，概述早期方法，包括N-gram、朴素贝叶斯分类器和基于神经网络的多种方法，如多层感知器（MLPs）、循环神经网络（RNNs）和长短期记忆（LSTM）网络。我们还将讨论基于transformer架构的突破性转变，这为大型语言模型（LLMs）的出现奠定了基础，而LLMs实际上只是大型基于transformer的模型。最后，我们将介绍LLMs及其独特特征，讨论它们如何建立在并超越早期的语言建模技术，从而彻底改变自然语言处理（NLP）领域。
- en: This book is about LLMs in production. We firmly believe that if you want to
    turn an LLM into an actual product, understanding the technology better will improve
    your results and save you from making costly and time-consuming mistakes. Any
    engineer can figure out how to lug a big model into production and throw a ton
    of resources at it to make it run, but that brute-force strategy completely misses
    the lessons people have already learned trying to do the same thing before, which
    is what we are trying to solve with LLMs in the first place. Having a grasp of
    these fundamentals will better prepare you for the tricky parts, the gotchas,
    and the edge cases you are going to run into when working with LLMs. By understanding
    the context in which LLMs emerged, we can appreciate their transformative impact
    on NLP and how to enable them to create a myriad of applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书关于生产中的LLMs。我们坚信，如果你想将LLM转化为实际产品，更好地理解技术将提高你的成果并帮助你避免代价高昂且耗时耗力的错误。任何工程师都可以弄清楚如何将大型模型拖入生产并投入大量资源使其运行，但这种蛮力策略完全忽略了人们之前尝试做同样事情时已经学到的教训，这正是我们最初尝试使用LLMs的原因。掌握这些基础知识将更好地为你准备那些棘手的部分、陷阱和边缘情况，你将在与LLMs合作时遇到这些情况。通过理解LLMs出现的背景，我们可以欣赏它们对NLP的变革性影响以及如何使它们能够创造无数的应用。
- en: 2.1 Language modeling
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 语言建模
- en: It would be a great disservice to address LLMs in any depth without first addressing
    language. To that end, we will start with a brief but comprehensive overview of
    language modeling, focusing on the lessons that can help us with modern LLMs.
    Let’s first discuss levels of abstraction, as this will help us garner an appreciation
    for language modeling.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不首先解决语言问题，就深入探讨LLMs，那将是一种极大的失职。为此，我们将从对语言建模的简要但全面的概述开始，重点关注有助于我们理解现代LLMs的教训。让我们首先讨论抽象层次，因为这将帮助我们理解语言建模。
- en: Language, as a concept, is an abstraction of the feelings and thoughts that
    occur to us in our heads. Feelings come first in the process of generating language,
    but that’s not the only thing we’re trying to highlight here. We’re also looking
    at language as being unable to capture the full extent of what we are able to
    feel, which is why we’re calling it an abstraction. It moves away from the source
    material and loses information. Math is an abstraction of language, focusing on
    logic and provability, but as any mathematician will tell you, it is a subset
    of language used to describe and define in an organized and logical way. From
    math comes another abstraction, the language of binary, a base-2 system of numerical
    notation consisting of either on or off.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语言作为一个概念，是我们头脑中产生的情感和思想的抽象。在生成语言的过程中，情感首先出现，但这并不是我们在这里想要强调的唯一事物。我们还在观察语言作为无法捕捉我们能够感受到的全部范围的能力，这就是为什么我们称之为抽象。它远离了原始材料并丢失了信息。数学是语言的抽象，专注于逻辑和可证明性，但正如任何数学家都会告诉你的，它是一种用于以有组织和逻辑的方式描述和定义的语言的子集。从数学中又产生了另一个抽象，二进制语言，这是一个由开或关组成的二进制数值表示系统。
- en: This is not a commentary on usefulness, as binary and math are just as useful
    as the lower-level aspects of language, nor is it commenting on order, as we said
    before. With math and binary, the order coincidentally lines up with the layer
    of abstraction. Computers can’t do anything on their own and need to take commands
    to be useful. Binary, unfortunately, ends up taking too long for humans to communicate
    important things in it, so binary was also abstracted to assembly, a more human-comprehensible
    language for communicating with computers. This was further abstracted to the
    high-level assembly language C, which has been even further abstracted to object-oriented
    languages like Python or Java (which one doesn’t matter—we’re just measuring distance
    from binary). The flow we just discussed is outlined in figure 2.1.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是对有用性的评论，因为二进制和数学与语言的低级方面一样有用，也不是在评论顺序，因为我们之前已经说过。在数学和二进制中，顺序偶然地与抽象层相吻合。计算机不能独立做任何事情，需要接收命令才能变得有用。不幸的是，二进制对于人类来说太耗时了，以至于无法在其中传达重要的事情，因此二进制也被抽象为汇编，这是一种更易于人类理解的语言，用于与计算机通信。这进一步抽象为高级汇编语言C，它又被进一步抽象为面向对象的语言，如Python或Java（哪一个不重要——我们只是在衡量与二进制的距离）。我们刚才讨论的流程在图2.1中有所概述。
- en: '![figure](../Images/2-1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-1.png)'
- en: Figure 2.1 We compare cognitive layers of abstraction to programming layers
    of abstraction down to the logical binary abstraction. Python doesn’t come from
    C, nor does it compile into C. Python is, however, another layer of abstraction
    distant from binary. Language follows a similar path. Each layer of abstraction
    creates a potential point of failure. There are also several layers of abstraction
    to creating a model, and each is important in seeing the full path from our feelings
    to a working model.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 我们将认知抽象层与编程抽象层进行比较，直到逻辑二进制抽象。Python不是来自C，也不是编译成C。然而，Python是另一个远离二进制的抽象层。语言遵循相似的路径。每一层抽象都创造了一个潜在的错误点。在创建模型的过程中也有几个抽象层，每一层在看到从我们的感受到一个工作模型的全路径时都很重要。
- en: This is obviously a reduction; however, it’s useful to understand that the feelings
    you have in your head are the same number of abstractions away from binary, the
    language the computer actually reads, as the languages most people use to program
    in. Some people might argue that there are more steps between Python and binary,
    such as compilers or using assembly to support the C language, and that’s true,
    but there are more steps on the language side too, such as morphology, syntax,
    logic, dialogue, and agreement.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是一种简化；然而，了解你头脑中的感受与计算机实际读取的语言——即大多数人用来编程的语言——在抽象层面上距离二进制相同，是有用的。有些人可能会争论Python和二进制之间有更多的步骤，比如编译器或使用汇编来支持C语言，这是真的，但在语言方面也有更多的步骤，比如形态学、句法、逻辑、对话和一致性。
- en: This reduction can help us understand how difficult the process of getting what
    we want to be understood by an LLM actually is and even help us understand language
    modeling techniques better. We focus on binary here to illustrate that there are
    a similar number of abstract layers to get from an idea you have or from one of
    our code samples to a working model. Like the children’s telephone game where
    participants whisper into each other’s ears, each abstraction layer creates a
    disconnect point or barrier where mistakes can be made.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简化可以帮助我们理解我们想要被LLM理解的过程实际上有多么困难，甚至有助于我们更好地理解语言建模技术。在这里我们关注二进制，以说明从你拥有的想法或我们的代码示例到工作模型之间有相似数量的抽象层。就像孩子们玩电话游戏，参与者互相耳语，每个抽象层都创造了一个断开点或障碍，错误可能在这里发生。
- en: Figure 2.1 is also meant not only to illustrate the difficulty in creating reliable
    code and language input but also to draw attention to how important the intermediary
    abstraction steps, like tokenization and embeddings, are for the model itself.
    Even if you have perfectly reliable code and perfectly expressed ideas, the meaning
    may be fumbled by one of those processes before it ever reaches the LLM.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1不仅旨在说明创建可靠代码和语言输入的难度，而且还强调中间抽象步骤，如分词和嵌入，对模型本身的重要性。即使你拥有完全可靠的代码和完美表达的思想，这些过程之一也可能在它到达LLM之前就弄错了意义。
- en: In this chapter, we will try to help you understand what you can do to reduce
    the risks of these failure points, whether on the language, coding, or modeling
    side. Unfortunately, it’s a bit tricky to strike a balance between giving you
    too much linguistics that doesn’t immediately matter for the task at hand versus
    giving you too much technical knowledge that, while useful, doesn’t help you develop
    an intuition for language modeling as a practice. With this in mind, you should
    know that linguistics can be traced back thousands of years in our history, and
    there’s lots to learn from it. We’ve included a brief overview of how language
    modeling has progressed over time in appendix A, and we encourage you to take
    a look.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将尝试帮助你了解你可以在语言、编码或建模方面做些什么来降低这些失败点的风险。不幸的是，在给你太多不立即与当前任务相关的语言学知识，以及给你太多虽然有用但不会帮助你培养对语言建模实践直觉的技术知识之间取得平衡是有点棘手的。考虑到这一点，你应该知道语言学可以追溯到我们历史数千年前，并且有很多东西可以从中学习。我们在附录A中简要概述了语言建模随时间的发展，并鼓励你查看。
- en: Let’s start with our focus on the building blocks that constitute language itself.
    We expect our readers to have at least attempted language modeling before and
    to have heard of libraries like PyTorch and TensorFlow, but we do not expect most
    of our readers to have considered the language side of things before. By understanding
    the essential features that make up language, we can better appreciate the complexities
    involved in creating effective language models and how these features interact
    with one another to form the intricate web of communication that connects us all.
    In the following section, we will examine the various components of language,
    such as phonetics, pragmatics, morphology, syntax, and semantics, as well as the
    role they play in shaping our understanding and usage of languages around the
    world. Let’s take a moment to explore how we currently understand language, along
    with the challenges we face that LLMs are meant to solve.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从构成语言本身的基本构建块开始。我们希望我们的读者至少尝试过语言建模，并且听说过像PyTorch和TensorFlow这样的库，但我们并不期望大多数读者在之前考虑过语言方面的问题。通过理解构成语言的本质特征，我们可以更好地欣赏创建有效语言模型所涉及的复杂性，以及这些特征如何相互作用，形成连接我们所有人的复杂沟通网络。在下一节中，我们将检查语言的各个组成部分，例如语音学、语用学、形态学、句法和语义学，以及它们在塑造我们对世界各地语言的理解和使用中所起的作用。让我们花点时间来探讨我们目前对语言的理解，以及我们面临的挑战，这些挑战正是LLMs旨在解决的。
- en: 2.1.1 Linguistic features
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 语言特征
- en: 'Our current understanding of language is that language is made up of at least
    five parts: phonetics, syntax, semantics, pragmatics, and morphology. Each of
    these portions contributes significantly to the overall experience and meaning
    being ingested by the listener in any conversation. Not all of our communication
    uses all of these forms; for example, the book you’re currently reading is devoid
    of phonetics, which is one of the reasons why so many people think text messages
    are unsuited for more serious or complex conversations. Let’s work through each
    of these five parts to figure out how to present them to a language model for
    a full range of communicative power.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对语言的理解是，语言至少由五个部分组成：语音学、句法、语义、语用和形态学。这些部分中的每一个都对任何对话中听众所吸收的整体体验和意义有显著的贡献。并不是所有的交流都使用所有这些形式；例如，你现在正在阅读的这本书没有语音学，这也是许多人认为短信不适合更严肃或复杂对话的原因之一。让我们逐一探讨这五个部分，以了解如何将它们呈现给语言模型，以实现全面的沟通能力。
- en: Phonetics
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语音学
- en: Phonetics is probably the easiest for a language model to ingest, as it involves
    the actual sound of the language. This is where accent manifests and deals with
    the production and perception of speech sounds, with phonology focusing on the
    way sounds are organized within a particular language system. Similarly to computer
    vision, while a sound isn’t necessarily easy to deal with as a whole, there’s
    no ambiguity in how to parse, vectorize, or tokenize the actual sound waves. They
    have a numerical value attached to each part, the crest, the trough, and the slope
    during each frequency cycle. It is vastly easier than text to tokenize and process
    by a computer while being no less complex.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言模型来说，语音学可能是最容易吸收的，因为它涉及到语言的实际声音。这就是口音显现和处理语音声音生产与感知的地方，语音学专注于特定语言系统中声音的组织方式。与计算机视觉类似，虽然处理整个声音可能并不容易，但在解析、矢量化或标记实际声音波方面没有歧义。每个部分（如每个频率周期的峰值、谷值和斜率）都附有数值。与文本相比，语音学在计算机标记和处理方面要容易得多，尽管它并不简单。
- en: Sound inherently also contains more encoded meaning than text. For example,
    imagine someone saying the words “Yeah, right,” to you. It could be sarcastic,
    or it could be congratulatory, depending on the tone—and English isn’t even tonal!
    Phonetics, unfortunately, doesn’t have terabyte-sized datasets commonly associated
    with it, and performing data acquisition and cleaning on phonetic data, especially
    on the scale needed to train an LLM, is difficult at best. In an alternate world
    where audio data was more prevalent than text data and took up a smaller memory
    footprint, phonetic-based or phonetic-aware LLMs would be much more sophisticated,
    and creating that world is a solid goal to work toward.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 声音本身也包含比文本更多的编码意义。例如，想象有人对你说“是啊，没错”。这可能是一种讽刺，也可能是一种祝贺，这取决于语气——而英语甚至不是音调语言！不幸的是，语音学没有与它通常相关的大型数据集，对语音数据进行数据采集和清洗，尤其是在训练大型语言模型所需的规模上，是极其困难的。在一个音频数据比文本数据更普遍且占用更小内存的世界里，基于语音或对语音敏感的大型语言模型将会更加复杂，创造这样一个世界是一个值得努力的目标。
- en: 'Anticipating this phonetical problem, a system was created in 1888 called the
    International Phonetic Alphabet (IPA). It has been revised in both the 20th and
    21st centuries to be more concise, more consistent, and clearer and could be a
    way to insert phonetic awareness into text data. IPA functions as an internationally
    standardized version of every language’s sound profile. A sound profile is the
    set of sounds that a language uses; for example, in English, we never have the
    /ʃ/ (she, shirt, sh) next to the /v/ sound. IPA is used to write sounds, rather
    than writing an alphabet or logograms, as most languages do. For example, you
    could describe how to pronounce the word “cat” using these symbols: /k/, /æ/,
    and /t/. Of course, that’s a very simplified version of it, but for models, it
    doesn’t have to be. You can describe tone and aspiration as well. This could be
    a happy medium between text and speech, capturing some phonetic information. Think
    of the phrase “What’s up?” Your pronunciation and tone can drastically change
    how you understand that phrase, sometimes sounding like a friendly “Wazuuuuup?”
    and other times an almost threatening “‘Sup?” which IPA would fully capture. IPA
    isn’t a perfect solution, though; for example, it doesn’t solve the problem of
    replicating tone very well.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 预见到这个语音问题，1888年创建了一个名为国际音标（IPA）的系统。它在20世纪和21世纪都进行了修订，以使其更加简洁、一致、清晰，并且可以作为一种将语音意识插入文本数据的方法。IPA作为每个语言声音轮廓的国际标准化版本。声音轮廓是语言使用的声音集合；例如，在英语中，我们永远不会把/ʃ/（she,
    shirt, sh）和/v/声音放在一起。IPA用于书写声音，而不是像大多数语言那样书写字母或象形文字。例如，你可以使用这些符号来描述如何发音单词“cat”：/k/，/æ/，和/t/。当然，这是一个非常简化的版本，但对于模型来说，并不需要这样。你还可以描述音调和送气。这可能是文本和语音之间的一个折中方案，捕捉一些语音信息。想想短语“What’s
    up？”你的发音和音调可以极大地改变你对这个短语的理解，有时听起来像友好的“Wazuuuuup？”有时则像几乎威胁的“‘Sup？”而IPA可以完全捕捉到这一点。尽管如此，IPA并不是一个完美的解决方案；例如，它并不能很好地解决复制音调的问题。
- en: Phonetics is listed first here because it’s the place where LLMs have been applied
    to the least out of all the features and, therefore, has the largest space for
    improvement. Even modern text-to-speech (TTS) and voice-cloning models, for the
    most part, end up converting the sound to a spectrogram and analyzing that image
    rather than incorporating any type of phonetic language modeling. Improving phonetic
    data and representation in LLMs is something to look for as far as research goes
    in the coming months and years.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 声音学在这里被列为首位，因为相较于所有其他特征，声音学在大型语言模型（LLM）中的应用最少，因此具有最大的改进空间。即使是现代的文本到语音（TTS）和声音克隆模型，大部分情况下，最终都是将声音转换成频谱图并分析该图像，而不是结合任何类型的语音语言模型。在未来的几个月和几年里，改善LLM中的语音数据和表示将是研究的一个方向。
- en: Syntax
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语法
- en: Syntax is the place where current LLMs are highest-performing, both in parsing
    syntax from the user and in generating their own. Syntax is generally what we
    think of as grammar and word order; it is the study of how words can combine to
    form phrases, clauses, and sentences. Syntax is also the first place language-learning
    programs start to help people acquire new languages, especially based on where
    they are coming from natively. For example, it is important for a native English
    speaker learning Turkish to know that the syntax is completely different, and
    you can often build entire sentences in Turkish that are just one long compound
    word, whereas in English, we never put our subject and verb together into one
    word.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 语法是当前LLM表现最出色的领域，无论是从用户那里解析语法还是生成自己的语法。语法通常是我们所认为的语法和词序；它是研究单词如何组合成短语、从句和句子的学科。语法也是语言学习程序开始帮助人们学习新语言的第一步，尤其是基于他们本来的语言背景。例如，对于学习土耳其语的英语母语者来说，了解语法完全不同是很重要的，在土耳其语中，你可以构建整个句子，而英语中我们永远不会把主语和动词放在一起成为一个单词。
- en: 'Syntax is largely separate from meaning in language, as the famous sentence
    from Noam Chomsky, the so-called father of syntax, demonstrates: “Colorless green
    ideas sleep furiously.” Everything about that sentence is both grammatically correct
    and semantically understandable. The problem isn’t that it doesn’t make sense;
    it’s that it does, and the encoded meanings of those words conflict. This is a
    reduction; however, you can think of all the times LLMs give nonsense answers
    as this phenomenon manifests. Unfortunately, the syntax is also where ambiguity
    is most commonly found. Consider the sentence, “I saw an old man and woman.” Now
    answer this question: Is the woman also old? This is syntactic ambiguity, where
    we aren’t sure whether the modifier “old” applies to all people in the following
    phrase or just the one it immediately precedes. This is less consequential than
    the fact that semantic and pragmatic ambiguity also show up in syntax. Consider
    this sentence: “I saw a man on a hill with a telescope,” and answer these questions:
    Where is the speaker, and what are they doing? Is the speaker on the hill cutting
    a man in half using a telescope? Likely, you didn’t even consider this option
    when you read the sentence because when we interpret syntax, all of our interpretations
    are at least semantically and pragmatically informed. We know from lived experience
    that that interpretation isn’t at all likely, so we throw it out immediately,
    usually without even taking time to process that we’re eliminating it from the
    pool of probable meanings. Single-modality LLMs will always have this problem,
    and multimodal LLMs can (so far) only asymptote toward the solution.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 语法在语言中很大程度上与意义是分开的，正如著名的句子来自诺姆·乔姆斯基，被称为语法的“之父”所展示的那样：“无色的绿色想法疯狂地睡觉。”这个句子的每一部分在语法上都是正确的，在语义上也是可以理解的。问题不在于它没有意义；而在于它有意义，这些词的编码意义是冲突的。这是一种简化；然而，你可以把LLMs给出无意义答案的所有时间都看作是这个现象的表现。不幸的是，语法也是歧义最常见的地方。考虑这个句子，“我看到一个老人和一个女人。”现在回答这个问题：这个女人也是老人吗？这是句法歧义，我们不确定修饰语“老人”是适用于后面短语中的所有人，还是仅仅适用于它直接前面的那个人。这比语义和语用歧义也出现在句法中的事实要轻微得多。考虑这个句子：“我看到一个在山上的男人，他手里拿着望远镜。”回答这些问题：说话者在哪里，他们在做什么？说话者是在山上用望远镜把一个男人切成两半吗？很可能，你在读这个句子的时候甚至没有考虑这个选项，因为当我们解释句法时，我们所有的解释至少在语义和语用上都是有所依据的。我们从生活经验中知道那种解释根本不可能，所以我们立即将其排除，通常甚至没有花时间去处理我们正在从可能的含义池中排除它。单模态LLMs将始终存在这个问题，而多模态LLMs（到目前为止）只能趋近于解决方案。
- en: It shouldn’t take any logical leap to understand why LLMs need to be syntax-aware
    to be high-performing. LLMs that don’t get word order correct or generate nonsense
    aren’t usually described as “good.” LLMs being syntax-dependent has prompted even
    Chomsky to call LLMs “stochastic parrots.” In our opinion, GPT-2 in 2018 was when
    language modeling solved syntax as a completely meaning-independent demonstration,
    and we’ve been happy to see the more recent attempts to combine the syntax that
    GPT-2 outputs so well with encoded and entailed meaning, which we’ll get into
    now.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么LLMs需要具备语法意识才能表现出色，不应该需要任何逻辑上的跳跃。那些没有正确处理词序或生成无意义输出的LLMs通常不会被描述为“好”。LLMs对语法的依赖甚至促使乔姆斯基将LLMs称为“随机的鹦鹉”。在我们看来，2018年的GPT-2是语言模型将语法作为一个完全独立于意义的演示解决的时候，我们很高兴看到最近尝试将GPT-2输出的语法与编码和蕴涵的意义相结合的尝试，我们现在将深入探讨这一点。
- en: Semantics
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义
- en: 'Semantics are the literal encoded meaning of words in utterances, which changes
    at breakneck speed in waves. People automatically optimize semantic meaning by
    only using words they consider meaningful in the current language epoch. If you’ve
    ever created or used an embedding with language models (word2vec, ELMo, BERT,
    MUSE [the E is for embedding], etc.), you’ve used a semantic approximation. Words
    often go through semantic shifts, and while we won’t cover this topic completely
    or go into depth, here are some common ones you may already be familiar with:
    narrowing, a broader meaning to a more specific one; broadening, the inverse of
    narrowing going from a specific meaning to a broad one; and reinterpretations,
    going through whole or partial transformations. These shifts do not have some
    grand logical underpinning. They don’t even have to correlate with reality, nor
    do speakers of a language often consciously think about the changes as they’re
    happening. That doesn’t stop the change from occurring, and in the context of
    language modeling, it doesn’t stop us from having to keep up with that change.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 语义是话语中词语的直译意义，它以惊人的速度在波浪中变化。人们会自动优化语义意义，只使用他们认为在当前语言时代有意义的词语。如果你曾经创建或使用过语言模型（word2vec、ELMo、BERT、MUSE
    [E代表嵌入]，等等）的嵌入，你就已经使用了语义近似。词语经常经历语义变化，虽然我们不会完全涵盖这个主题或深入探讨，但这里有一些你可能已经熟悉的常见变化：缩小，从更广泛的意义到更具体的一个；扩大，与缩小相反，从具体意义到广泛意义；以及重新解释，通过整体或部分转换。这些变化并没有某种伟大的逻辑基础。它们甚至不需要与现实相关联，语言的使用者通常在变化发生时也不会有意识地思考这些变化。这并不能阻止变化的发生，在语言建模的背景下，这也不能阻止我们必须跟上这种变化。
- en: Let’s look at some examples. Narrowing includes “deer,” which in Old and Middle
    English just meant any wild animal, even a bear or a cougar, and now means only
    one kind of forest animal. For broadening, we have “dog,” which used to refer
    to only one canine breed from England and now can be used to refer to any domesticated
    canine. One fun tangent about dog-broadening is in the FromSoft game *Elden Ring,*
    where because of a limited message system between players, “dog” will be used
    to refer to anything from a turtle to a giant spider and literally everything
    in between. For reinterpretation, we can consider “pretty,” which used to mean
    clever or well-crafted, not visually attractive. Another good example is “bikini,”
    which went from referring to a particular atoll to referring to clothing you might
    have worn when visiting that atoll to people acting as if the “bi-” was referring
    to the two-piece structure of the clothing, thus implying the tankini and monokini.
    Based on expert research and decades of study, we can think of language as being
    constantly compared and re-evaluated by native language speakers, out of which
    common patterns emerge. The spread of those patterns is closely studied in sociolinguistics
    and is largely out of the scope of the current purpose but can quickly come into
    scope as localization (l10n) or internationalization (i18n) for LLMs arises as
    a project requirement. Sociolinguistic phenomena such as prestige can help design
    systems that work well for everyone.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些例子。缩小包括“鹿”，在古英语和中古英语中这个词仅仅指任何野生动物，甚至包括熊或美洲狮，而现在只指一种森林动物。对于扩大，我们有“狗”，它曾经只指来自英格兰的一种犬种，而现在可以用来指任何家养犬。关于狗的扩大有一个有趣的小插曲，在FromSoft游戏《艾尔登法环》中，由于玩家之间有限的消息系统，"狗"这个词会被用来指从乌龟到巨型蜘蛛以及介于两者之间的所有事物。对于重新解释，我们可以考虑“漂亮”，它曾经意味着聪明或工艺精湛，而不是视觉上吸引人。另一个很好的例子是“比基尼”，它从指一个特定的环礁，到指在访问那个环礁时可能穿的衣服，再到人们好像“bi-”指的是衣服的两件式结构，从而暗示了坦克尼和单件比基尼。基于专家研究和数十年的研究，我们可以认为语言是不断被母语使用者比较和重新评估的，从中产生了常见的模式。这些模式在语言社会学的传播被密切研究，但很大程度上超出了当前目的的范围，但当地化（l10n）或国际化（i18n）作为LLMs项目需求出现时，可以迅速进入范围。像声望这样的社会语言学现象可以帮助设计出对每个人都适用的系统。
- en: In the context of LLMs, so-called semantic embeddings are vectorized versions
    of text that attempt to mimic semantic meaning. Currently, the most popular way
    of doing this is by tokenizing or assigning an arbitrary number in a dictionary
    to each subword in an utterance (think prefixes, suffixes, and morphemes generally),
    applying a continuous language model to increase the dimensionality of each token
    within the vector so that there’s a larger vector representing each index of the
    tokenized vector, and then applying a positional encoding to each of those vectors
    to capture word order. Each subword ends up being compared to other words in the
    larger dictionary based on how it’s used. We’ll show you an example of this later.
    Something to consider when thinking about word embeddings is that they struggle
    to capture the deep, encoded meaning of those tokens, and simply adding more dimensions
    to the embeddings hasn’t shown marked improvement. Evidence that embeddings work
    similarly to humans is that you can apply a distance function to related words
    and see that they are closer together than unrelated words. How to capture and
    represent meaning more completely is another area in which to expect groundbreaking
    research in the coming years and months.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs（大型语言模型）的背景下，所谓的语义嵌入是文本的向量化版本，试图模仿语义含义。目前，最流行的方法是通过分词或将字典中的任意数字分配给每个话语中的子词（例如前缀、后缀和一般词素），应用连续语言模型来增加向量中每个标记的维度，以便有一个更大的向量代表每个标记向量的索引，然后对每个这些向量应用位置编码以捕捉词序。每个子词最终都会根据其使用方式与其他字典中的单词进行比较。我们稍后会展示一个例子。在思考词嵌入时，需要考虑的是，它们难以捕捉那些标记的深层编码含义，而简单地增加嵌入的维度并没有显示出显著的改进。嵌入与人类工作方式相似的证据是，你可以应用距离函数到相关单词上，并看到它们比无关单词更接近。如何更完整地捕捉和表示意义是未来几年和几个月内有望出现突破性研究的另一个领域。
- en: Pragmatics
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 话语学
- en: Pragmatics is sometimes omitted from linguistics due to its referent being all
    the nonlinguistic context affecting a listener’s interpretation and the speaker’s
    decision to express things in a certain way. Pragmatics refers in large part to
    dogmas followed in cultures, regions, socio-economic classes, and shared lived
    experiences, which are played off of to take shortcuts in conversations using
    entailment.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 话语学有时被省略在语言学中，因为它的参照物是所有影响听者解释和说话者决定以某种方式表达的非语言环境。话语学在很大程度上指的是文化、地区、社会经济阶层和共同生活经历中遵循的教条，这些教条在对话中通过蕴涵来走捷径。
- en: If we were to say, “A popular celebrity was just taken into the ICU,” your pragmatic
    interpretation based on lived experience might be to assume that a well-beloved
    person has been badly injured and is now undergoing medical treatment in a well-equipped
    hospital. You may wonder about which celebrity it is, whether they will have to
    pay for the medical bills, or if the injury was self-inflicted, also based on
    your lived experience. None of these things can be inferred directly from the
    text and its encoded meaning by itself. You would need to know that ICU stands
    for a larger set of words and what those words are. You would need to know what
    a hospital is and why someone would need to be taken there instead of going there
    themselves. If any of these feel obvious, good. You live in a society, and your
    pragmatic knowledge of that society overlaps well with the example provided. If
    we share an example from a less-populated society, “Janka got her grand-night
    lashings yesterday; she’s gonna get Peter tomorrow,” you might be left scratching
    your head. If you are, realize this probably looks like how a lot of text data
    ends up looking to an LLM (anthropomorphization acknowledged). For those wondering,
    this sentence comes from Slovak Easter traditions. A lot of meaning here will
    be missed and go unexplained if you are unaccustomed to these particular traditions
    as they stand in that culture. This author personally has had the pleasure of
    trying to explain the Easter Bunny and its obsession with eggs to foreign colleagues
    and enjoyed the satisfaction of looking like I’m off my rocker.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们说，“一位受欢迎的明星刚刚被送进了ICU”，根据你的生活经验，你的语用解释可能是假设一个深受爱戴的人受到了严重伤害，现在正在一家设备齐全的医院接受治疗。你可能想知道这是哪位明星，他们是否需要支付医疗费用，或者伤害是否是自残的，这也基于你的生活经验。这些都不能直接从文本及其编码的意义中推断出来。你需要知道ICU代表一组更大的词汇以及这些词汇是什么。你需要知道医院是什么，以及为什么有人需要被送到那里而不是自己去。如果这些感觉很明显，很好。你生活在一个社会中，你对这个社会的语用知识很好地与提供的例子重叠。如果我们分享一个来自人口较少社会的例子，“Janka昨天受到了严厉的惩罚；她明天会得到Peter的惩罚”，你可能会感到困惑。如果你感到困惑，意识到这可能就是许多文本数据对LLMs（承认拟人化）看起来像的样子。对于那些想知道的人，这个句子来自斯洛伐克的复活节传统。如果你不习惯这些特定的传统，那么这里很多含义都会被错过，无法解释。这位作者个人很享受尝试向外国同事解释复活节兔子和它对鸡蛋的迷恋，并享受着看起来像是脱离了现实的感觉。
- en: In the context of LLMs, we can effectively group all out-of-text contexts into
    pragmatics. This means LLMs start without any knowledge of the outside world and
    do not gain it during training. They only gain a knowledge of how humans respond
    to particular pragmatic stimuli. LLMs do not understand social class or race or
    gender or presidential candidates, or anything else that might spark some type
    of emotion in you based on your life experience. Pragmatics isn’t something that
    we expect will be able to be directly incorporated into a model at any point because
    models cannot live in society. Yet we have already seen the benefits of incorporating
    it indirectly through data engineering and curation, prompting mechanisms like
    RAG, and supervised finetuning on instruction datasets. In the future, we expect
    great improvements in incorporating pragmatics into LLMs, but we emphasize that
    it’s an asymptotic solution because language is ultimately still an abstraction.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs（大型语言模型）的背景下，我们可以有效地将所有非文本上下文归入语用学。这意味着LLMs在开始时对现实世界没有任何知识，并且在训练过程中也不会获得这些知识。它们只获得对人类如何对特定的语用刺激做出反应的知识。LLMs不理解社会阶层、种族、性别或总统候选人，或者任何可能基于你的生活经历激发某种情感的其他事物。我们并不期望语用学能够在任何时刻直接被纳入模型中，因为模型无法生活在社会中。然而，我们已经看到了通过数据工程和整理、RAG（阅读-询问-生成）提示机制以及指令数据集上的监督微调间接纳入语用学的益处。在未来，我们预计在将语用学纳入LLMs方面将取得重大进步，但我们强调这是一个渐近解，因为语言最终仍然是一种抽象。
- en: Pragmatic structure gets added, whether you mean to add it or not, as soon as
    you acquire the data you are going to train on. You can think of this type of
    pragmatic structure as bias, not inherently good or bad, but impossible to get
    rid of. Later down the line, you get to pick the types of bias you’d like your
    data to keep by normalizing and curating, augmenting particular underrepresented
    points, and cutting overrepresented or noisy examples. Instruction datasets show
    us how you can harness pragmatic structure in your training data to create incredibly
    useful bias, like biasing your model to answer a question when asked instead of
    attempting to categorize the sentiment of the question.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是你有意还是无意，一旦你获取了将要训练的数据，实用结构就会被添加进去。你可以将这种类型的实用结构视为偏见，它本身并不一定是好是坏，但却是无法摆脱的。在后续过程中，你可以通过标准化和整理，增强特定代表性不足的点，以及削减过度代表或噪声示例，来选择你希望数据保持的偏见类型。指令数据集展示了你如何在训练数据中利用实用结构来创建极其有用的偏见，例如，当被要求回答问题时，让模型偏向于回答问题而不是尝试对问题的情感进行分类。
- en: Pragmatics and context all revolve around entailment. An entailment is a pragmatic
    marker within your data, as opposed to the literal text your dataset contains.
    For example, let’s say you have a model attempting to take an input like “Write
    me a speech about frogs eating soggy socks that doesn’t rhyme and where the first
    letters of each line spell amphibian” and actually follow that instruction. You
    can immediately tell that this input is asking for a lot. The balance for you
    as a data engineer would be to make sure that everything the input is asking for
    is explicitly accounted for in your data. You need examples of speeches, examples
    of what frogs and socks are and how they behave, and examples of acrostic poems.
    If you don’t have them, the model might be able to understand just from whatever
    entailments exist in your dataset, but it’s pretty up in the air. If you go the
    extra mile and keep track of entailed versus explicit information and tasks in
    your dataset, along with data distributions, you’ll have examples to answer, “What
    is the garbage-in resulting in our garbage-out?”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 语境和上下文都围绕着蕴涵。蕴涵是数据中的实用标记，与数据集中包含的文本内容相对。例如，假设你有一个模型试图处理一个输入，如“为我写一篇关于青蛙吃湿袜子且不押韵的演讲稿，每行的第一个字母拼写出两栖动物”，并真正遵循这个指令。你可以立即判断这个输入要求很多。作为数据工程师的你，需要确保输入要求的所有内容都在你的数据中得到明确考虑。你需要演讲的例子，青蛙和袜子的例子以及它们的行为，以及首字母诗的例子。如果你没有这些例子，模型可能只能从数据集中存在的蕴涵中理解，但这很不确定。如果你更进一步，跟踪数据集中蕴涵与显性信息以及任务，以及数据分布，你将会有例子来回答“垃圾输入导致垃圾输出是什么？”
- en: LLMs struggle to pick up on pragmatics, even more so than people, but they do
    pick up on the things that your average standard deviation of people would. They
    can even replicate responses from people outside that standard deviation, but
    pretty inconsistently without the exact right stimulus. That means it’s difficult
    for a model to give you an expert answer on a problem the average person doesn’t
    know without providing the correct bias and entailment during training and in
    the prompt. For example, including “masterpiece” at the beginning of an image-generation
    prompt will elicit different and usually higher-quality generations, but only
    if that distinction was present in the training set and only if you’re asking
    for an image where “masterpiece” is a compliment. Instruction-based datasets attempt
    to manufacture those stimuli during training by asking questions and giving instructions
    that entail representative responses. It is impossible to account for every possible
    situation in training, and you may inadvertently create new types of responses
    from your end users by trying to account for everything. After training, you can
    coax particular information from your model through prompting, which has a skill
    ceiling based on what your data originally entailed.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在捕捉语用学方面比人更困难，但它们确实能捕捉到普通人平均标准差范围内的东西。它们甚至可以复制超出那个标准差的人的回答，但如果没有精确的刺激，这种复制通常是不一致的。这意味着，如果没有在训练期间和提示中提供正确的偏见和蕴涵，模型很难在没有提供正确偏见和蕴涵的情况下给出一个普通人不知道的问题的专家答案。例如，在图像生成提示的开头包含“杰作”一词将引发不同且通常质量更高的生成，但只有当这种区别存在于训练集中，并且只有当你要求的是一幅“杰作”是赞美之词的图像时。基于指令的数据集试图在训练期间通过提问和给出包含代表性回答的指令来制造这些刺激。在训练中考虑到每一种可能的情况是不可能的，并且你可能会在试图考虑到一切时无意中从你的最终用户那里创造出新的回答类型。训练后，你可以通过提示从你的模型中诱导出特定的信息，这有一个基于你的数据最初包含的内容的技能上限。
- en: Morphology
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 形态学
- en: Morphology is the study of word structures and how they are formed from smaller
    units called morphemes. Morphemes are the smallest units of meaning, like the
    “re-” in “redo” or “relearn.” However, not all parts of words are morphemes, such
    as “ra-” in “ration” or “na-” in “nation,” and some can be unexpected, like “helico-”
    as in “helicoid” and “-pter” as in “pterodactyl.”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 形态学是研究单词结构和它们是如何由称为词素的小单元构成的学科。词素是意义的最小单位，比如“re-”在“redo”或“relearn”中。然而，并不是单词的所有部分都是词素，例如“ra-”在“ration”或“na-”在“nation”中，有些可能是出乎意料的，比如“helico-”在“helicoid”和“-pter”在“pterodactyl”中。
- en: Understanding how words are constructed helps create better language models
    and parsing algorithms, which are essential for tasks like tokenization. Tokens
    are the basic units used in NLP; they can be words, subwords, characters, or whole
    utterances and do not have to correspond to existing morphemes. People do not
    consciously decide what their units of meaning are going to be, and as such, they
    are often illogical. The effectiveness of a language model can depend on how well
    it can understand and process these tokens. For instance, in tokenization, a model
    needs to store a set of dictionaries to convert between words and their corresponding
    indices. One of these tokens is usually an `/<UNK/>` token, which represents any
    word that the model does not recognize. If this token is used too frequently,
    it can hinder the model’s performance, either because the model’s vocabulary is
    too small or because the tokenizer is not using the right algorithm for the task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 理解单词是如何构成的有助于创建更好的语言模型和解析算法，这对于像分词这样的任务至关重要。分词是自然语言处理中使用的最基本单位；它们可以是单词、子词、字符或整个话语，并且不必与现有的词素相对应。人们并不自觉地决定他们的意义单位是什么，因此它们往往是逻辑不合理的。语言模型的有效性可能取决于它理解和处理这些分词的能力有多强。例如，在分词过程中，模型需要存储一组字典来在单词及其对应的索引之间进行转换。其中这些分词之一通常是一个`/<UNK/>`分词，它代表模型不认识的任何单词。如果这个分词使用得太频繁，可能会阻碍模型的表现，要么是因为模型词汇量太小，要么是因为分词器没有使用正确的算法来完成这项任务。
- en: Consider a scenario where you want to build a code completion model, but you’re
    using a tokenizer that only recognizes words separated by whitespace, like the
    NLTK `punkt` tokenizer. When it encounters the string `def` `add_two_numbers_together(x,`
    `y):`, it will pass `[def,` `[UNK],` `y]` to the model. This causes the model
    to lose valuable information, not only because it doesn’t recognize the punctuation
    but also because the important part of the function’s purpose is replaced with
    an unknown token due to the tokenizer’s morphological algorithm. A better understanding
    of word structure and the appropriate parsing algorithms is needed to improve
    the model’s performance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一个场景，你想要构建一个代码补全模型，但你使用的是一个只能识别由空格分隔的单词的标记器，比如NLTK的`punkt`标记器。当它遇到字符串`def`
    `add_two_numbers_together(x,` `y):`时，它将把`[def,` `[UNK],` `y]`传递给模型。这导致模型丢失了有价值的信息，不仅因为它不识别标点符号，而且还因为函数目的的重要部分被标记器形态算法替换成了未知标记。为了提高模型的表现，需要更好地理解词的结构和适当的解析算法。
- en: 2.1.2 Semiotics
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 符号学
- en: After exploring the fundamental features of language and examining their significance
    in the context of LLMs, it is important to consider the broader perspective of
    meaning-making and interpretation in human communication. Semiotics, the study
    of signs and symbols, offers a valuable lens through which we can better understand
    how people interpret and process language. We will delve into semiotics, examining
    the relationship between signs, signifiers, and abstractions and how LLMs utilize
    these elements to generate meaningful output. This discussion will provide a deeper
    understanding of the intricate processes through which LLMs manage to mimic human-like
    understanding of language while also shedding light on the challenges and limitations
    they face in this endeavor. We do not necessarily believe that mimicking human
    behavior is the right answer for LLM improvement, only that mimicry is how the
    field has evaluated itself so far.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索语言的基本特征并考察它们在LLMs（大型语言模型）背景下的重要性之后，考虑人类沟通中意义构建和解释的更广泛视角是至关重要的。符号学，即对符号和象征的研究，提供了一个有价值的视角，通过它我们可以更好地理解人们如何解释和加工语言。我们将深入探讨符号学，考察符号、符号指示物和抽象之间的关系，以及LLMs如何利用这些元素来生成有意义的输出。这次讨论将提供对LLMs如何模仿人类对语言的理解的复杂过程的更深入理解，同时也会揭示它们在这一努力中所面临的挑战和限制。我们并不一定认为模仿人类行为是LLM改进的正确答案，只是认为模仿是迄今为止该领域自我评估的方式。
- en: To introduce semiotics, let’s consider figure 2.2, an adapted Peircean semiotic
    triangle. These triangles are used to organize base ideas into sequences of firstness,
    secondness, and thirdness, with firstness being at the top left, secondness at
    the bottom, and thirdness at the top right. If you’ve ever seen a semiotic triangle
    before, you may be surprised at the number of corners and orientation. To explain,
    we’ve turned them upside down to make it slightly easier to read. Also, because
    the system is recursive, we’re showing you how the system can simultaneously model
    the entire process and each piece individually. While the whole concept of these
    ideas is very cool, it’s outside of the scope of this book to delve into the philosophy
    fully. Instead, we can focus on the cardinal parts of those words (first, second,
    third) to show the sequence of how meaning is processed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍符号学，让我们考虑图2.2，一个改编的皮尔士符号三角形。这些三角形被用来将基本思想组织成第一性、第二性和第三性的序列，其中第一性位于左上角，第二性位于底部，第三性位于右上角。如果你以前见过符号三角形，你可能会对角的数量和方向感到惊讶。为了解释，我们将它们倒置，使其稍微容易阅读一些。此外，因为系统是递归的，我们展示了系统如何同时模拟整个过程和每个部分。虽然这些想法的整体概念非常酷，但深入探讨哲学超出了本书的范围。相反，我们可以专注于那些词（第一、第二、第三）的基本部分，以展示意义处理的过程。
- en: '![figure](../Images/2-2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-2.png)'
- en: Figure 2.2 A recursive Peircean semiotic triangle is a system of organizing
    the process of extracting meaning from anything—in our case, from language. Each
    point on the triangle illustrates one of the minimal parts needed to synthesize
    meaning within whatever the system is being used to describe, so each point is
    a minimal unit in meaning for language. Firstness, secondness, and thirdness are
    not points on the triangle; instead, they are more like markers for the people
    versed in semiotics to be able to orient themselves in this diagram.
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 一个递归的皮尔士符号三角形是一个组织从任何事物中提取意义的过程的系统——在我们的例子中，是从语言中提取。三角形上的每一个点都说明了在系统被用来描述的任何事物中合成意义所需的最低限度的部分，因此每个点都是语言意义的最小单位。第一性、第二性和第三性不是三角形上的点；相反，它们更像是符号学家用来在这个图表中定位自己的标记。
- en: We can also look at each intersection of the triangles to understand why things
    are presented in the order they are. Feelings can be attached to images and encodings
    long before they can be attached to words and tables. Ritual and common scripts
    give a space for interpreted action that’s second nature and doesn’t have to be
    thought about, similar to how most phrases just come together from words without
    the native speaker needing to perform metacognition about each word individually.
    All of these eventually lead to an interpretation or a document (a collection
    of utterances); in our case, that interpretation should be reached by the LLM.
    This is why, for example, prompt engineering can boost model efficacy. Foundation
    LLMs trained on millions of examples of ritual scripts can replicate the type
    of script significantly better when you explicitly tell the model in the prompt
    which script needs to be followed. Try asking the model for a step-by-step explanation—maybe
    prepend your generation with “Let’s think about this step-by-step.” The model
    will generate step-by-step scripts based on previous scripts it’s seen play out.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以观察三角形的每一个交点，以了解为什么事物以这种顺序呈现。情感可以在它们能够附加到单词和表格之前就附加到图像和编码上。仪式和常见的脚本为解释性行动提供了一个空间，这种行动是第二本能的，不需要思考，就像大多数短语只是从单词中组合起来，母语使用者不需要对每个单词进行元认知一样。所有这些最终都会导致一种解释或一份文档（一系列话语）；在我们的例子中，这种解释应该由LLM得出。这就是为什么，例如，提示工程可以提高模型的有效性。在数百万个仪式脚本示例上训练的基础LLM，当你明确告诉模型在提示中需要遵循哪个脚本时，可以显著更好地复制脚本类型。试着要求模型给出逐步解释——也许在你的生成前加上“让我们一步步来考虑这个问题。”模型将根据它之前看到的脚本生成逐步脚本。
- en: For those interested, there are specific ways of reading these figures and a
    whole field of semiotics to consider; however, it’s not guaranteed that you’ll
    be able to create the best LLMs by understanding the whole thing. Instead of diving
    deeply into this, we’ll consider the bare minimum that can help you build the
    best models, UX, and UI for everyone to interact with. For example, one aspect
    of the process of creating meaning is recursiveness. When someone is talking to
    you and they say something that doesn’t make sense (is “meaningless” to you),
    what do you do? Generally, people will ask one or more clarifying questions to
    figure out the meaning, and the process will start over and over until the meaning
    is clear to you. The most state-of-the-art models currently on the market do not
    do this, but they can be made to do it through very purposeful prompting. Many
    people wouldn’t even know to do that without having it pointed out to them. In
    other words, this is a brief introduction to semiotics. You don’t need to be able
    to give in-depth and accurate coordinate-specific explanations to experts in the
    semiotic field by the end of this section. The point we are trying to make is
    that this is an organizational system showcasing the minimum number of things
    you need to create a full picture of meaning for another person to interpret.
    We are not giving the same amount of the same kinds of information to our models
    during training, but if we did, it would result in a marked improvement in model
    behavior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于感兴趣的人来说，有特定的方法来阅读这些图表，以及一个完整的符号学领域需要考虑；然而，并不能保证你通过理解这一切就能创造出最佳的LLMs。我们不会深入探讨这一点，而是考虑最基本的可以帮助你构建最佳模型、用户体验和用户界面的要素。例如，创建意义的过程中的一个方面是递归性。当有人和你交谈，他们说的话对你来说没有意义（对你来说是“无意义的”），你会怎么做？通常，人们会提出一个或多个澄清问题来弄清楚意义，这个过程会一次又一次地重复，直到意义对你来说变得清晰。目前市场上最先进的模型并不这样做，但通过非常目的性的提示，它们可以被训练去做这件事。许多人甚至不知道该怎么做，除非有人指出。换句话说，这只是一个关于符号学的简要介绍。你不需要在阅读本节结束时能够向符号学领域的专家提供深入和准确的坐标特定解释。我们试图说明的是，这是一个展示你需要创建一个完整意义图景的最小要素的组织系统。我们在训练模型时并没有提供相同数量和类型的相同信息，但如果我们这样做，将会显著改善模型的行为。
- en: Figures 2.2 and 2.3 are meant to represent a minimal organizational model, where
    each of these pieces is essential. Let’s consider figure 2.3, which walks through
    an example of using a semiotic triangle. Consider images, pictures, and memories
    and think about what it would be like to try to absorb the knowledge in this book
    without your eyes to process images and without orthography (a writing system)
    to abstract the knowledge. Looking at the bullet points, etc., how could you read
    this book without sections, whitespace between letters, and bullet points to show
    you the order and structure to process information? Look at semantics and literal
    encoded meaning, and imagine the book without diagrams or with words that didn’t
    have dictionary definitions. The spreadsheets in the middle could be a book without
    any tables or comparative informational organizers, including these figures. What
    would it be like to read this book without a culture or society with habits and
    dogma to use as a lens for our interpretations? All these points form our ability
    to interpret information, along with the lens through which we pass our information
    to recognize patterns.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2和图2.3旨在表示一个最小化的组织模型，其中每个部分都是至关重要的。让我们考虑图2.3，它通过一个使用符号三角形的示例来展示。考虑图像、图片和记忆，想想在没有眼睛处理图像和没有正字法（一种书写系统）来抽象知识的情况下，尝试吸收这本书中的知识会是什么样子。看看项目符号等，如果没有章节、字母之间的空白和项目符号来显示顺序和结构，你将如何阅读这本书？看看语义和字面编码的意义，想象一下没有图表或没有字典定义的单词的书。中间的表格可能是一本书，没有任何表格或比较信息组织者，包括这些图表。在没有文化或社会习惯和教条作为我们解释的透镜的情况下，阅读这本书会是什么样子？所有这些点构成了我们解读信息的能力，以及我们通过透镜传递信息以识别模式的能力。
- en: '![figure](../Images/2-3.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图表](../Images/2-3.png)'
- en: Figure 2.3 Starting at the top-left corner, follow the arrows to see the general
    order we use to build our interpretations and extract meaning from things we interact
    with. Here, we’ve replaced the descriptive words with some examples of each point.
    Try to imagine interpreting this diagram without any words, examples, arrows,
    or even the pragmatic context of knowing what a figure in a book like this is
    supposed to be for.
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 从左上角开始，按照箭头指示的顺序，查看我们构建解释和从我们与之互动的事物中提取意义的一般顺序。在这里，我们用每个点的例子替换了描述性词汇。试着想象在没有任何文字、例子、箭头，甚至知道这本书中的图例应该用于什么目的的实用语境下解释这个图。
- en: 'So these are the important questions: How many of these things do you see LLMs
    having access to in order to return meaningful interpretations? Does an LLM have
    access to feelings or societal rituals? Currently, they do not, but as we go through
    traditional and newer techniques for NLP inference, think about what different
    models have access to.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些问题很重要：LLMs能够访问多少这些事物以返回有意义的解释？LLMs能够访问情感或社会仪式吗？目前，它们还不能，但随着我们通过传统的和更新的NLP推理技术进行探索，想想不同模型能够访问什么。
- en: 2.1.3 Multilingual NLP
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 多语言NLP
- en: The last challenge that we need to touch on before we evaluate previous NLP
    techniques and current-generation LLMs is the foundation of linguistics and the
    reason LLMs even exist. People have wanted to understand or exploit each other
    since the first civilizations made contact. These cases have resulted in the need
    for translators, and this need has only exponentially increased as the global
    economy has grown and flourished.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们评估之前的NLP技术和当前一代LLMs之前，我们需要触及的最后一个挑战是语言学的基础，以及LLMs甚至存在的理由。自从最早的人类文明开始接触以来，人们就想要理解或利用彼此。这些案例导致了翻译的需求，随着全球经济的发展和繁荣，这种需求呈指数级增长。
- en: It’s pretty simple math for business as well. Did you know that there are almost
    as many native speakers of Bengali as there are native speakers of English? If
    this is the first time you’ve heard of the Bengali language, this should hopefully
    color your perception that there is a valuable market for multilingual models.
    There are billions of people in the world, but only about a third of 1 billion
    speak English natively. If your model is Anglocentric, like most are, you are
    missing out on 95% of the people in the world as customers and users. Spanish
    and Mandarin Chinese are easy wins in this area, but most people don’t even go
    that far.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业来说，这也很简单。你知道孟加拉语母语者的人数几乎和英语母语者一样多吗？如果你第一次听说孟加拉语，这应该有助于你认识到多语言模型有一个有价值的市场。世界上有数十亿人，但只有大约三分之一的人母语是英语。如果你的模型以英语为中心，就像大多数模型一样，你正在错过世界上95%的人作为客户和用户。西班牙语和普通话在这个领域很容易取得成功，但大多数人甚至没有走那么远。
- en: There are many more politically charged examples of calling things, including
    different languages, the same that are out of the scope of this book. These are
    most often because of external factors like government involvement. Keeping these
    two points in mind—that a monolingual system focusing on English doesn’t have
    the coverage or profit potential that many businesses act like it does and that
    the boundaries between languages and dialects are unreliable at best and systematically
    harmful at worst—should highlight the dangerous swamp of opinions. Many businesses
    and research scientists don’t even pretend to want to touch this swamp with a
    50-foot pole when designing a product or system.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书的范围之外，有许多政治上充满争议的例子，包括不同语言中称呼同一事物的情况。这些情况通常是因为外部因素，如政府干预。牢记这两点——一个以英语为中心的单语系统并没有许多企业所声称的覆盖面或盈利潜力，而语言和方言之间的界限在最坏的情况下是有害的——应该会凸显出意见的险恶沼泽。许多企业和研究科学家在设计产品或系统时甚至不愿假装想要用50英尺长的杆子去触碰这个沼泽。
- en: Unfortunately, no easy solutions exist at this time. However, considering these
    factors can help you as a scientist or engineer (and hopefully an ethical person)
    to design LLMs that, at the very least, don’t exacerbate and negatively contribute
    to the existing problems. The first step in this process is deciding on a directional
    goal from the beginning of the project, either toward localization (l10n) or internationalization
    (i18n). Localization is an approach exemplified by Mozilla, which has a different
    version of its browser available through crowdsourced l10n in over 90 languages
    with no indications of stopping that effort. Internationalization is similar,
    but in the opposite direction; for example, Ikea tries to put as few words as
    possible in their instructional booklets, opting instead for internationally recognized
    symbols and pictures to help customers navigate the DIY projects. Deciding at
    the beginning of the project cuts down on the effort required to expand to either
    solution exponentially. It is large enough to switch the perception of translation
    and formatting from a cost to an investment. In the context of LLMs and their
    rapid expansion across the public consciousness, it becomes even more important
    to make that consideration early. Hitting the market with a world-changing technology
    that automatically disallows most of the world from interacting with it devalues
    those voices. Having to wait jeopardizes businesses’ economic prospects.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，没有简单的解决方案。然而，考虑到这些因素可以帮助你作为一个科学家或工程师（以及希望是一个有道德的人）设计LLMs，至少它们不会加剧现有问题并产生负面影响。这个过程的第一步是在项目开始时就决定一个方向目标，要么是本地化（l10n），要么是国际化（i18n）。本地化是一个由Mozilla所体现的方法，它在超过90种语言中通过众包l10n提供了其浏览器的不同版本，并且没有迹象表明会停止这项努力。国际化与此类似，但方向相反；例如，宜家试图在其说明书上使用尽可能少的文字，而是选择使用国际上公认的符号和图片来帮助客户导航DIY项目。在项目开始时做出决定可以减少扩展到任何解决方案所需的努力。它足够大，足以将翻译和格式化的感知从成本转变为投资。在LLMs及其在公众意识中的快速扩张的背景下，尽早考虑这一点变得更加重要。带着一种自动禁止世界上大多数人与之互动的世界改变技术进入市场，贬低了那些声音。需要等待则危及企业的经济前景。
- en: Before continuing, let’s take a moment to reflect on what we’ve discussed so
    far. We’ve hit important points in linguistics, illustrating concepts for us to
    consider, such as understanding that the structure of language is separate from
    its meaning. We have demonstrated quite a journey that each of us takes, both
    personally and as a society, toward having the metacognition to understand and
    represent language in a coherent way for computers to work with. This understanding
    will only improve as we deepen our knowledge of cognitive fields and solve for
    the linguistic features we encounter. Going along with figure 2.1, we will now
    demonstrate the computational path for language modeling that we have followed
    and explore how it has and hasn’t solved for any of those linguistic features
    or strived to create meaning. Let’s move into evaluating the various techniques
    for representing a language algorithmically.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们花一点时间回顾一下到目前为止我们所讨论的内容。我们在语言学方面触及了重要观点，为我们提供了需要考虑的概念，例如理解语言的结构与其意义是分开的。我们已经展示了一个旅程，每个人作为个人以及作为社会，都在朝着拥有元认知来以连贯的方式理解和表示语言给计算机使用前进。随着我们加深对认知领域的了解并解决我们遇到的语用特征，这种理解将得到改善。跟随图2.1，我们现在将展示我们所遵循的语言建模的计算路径，并探讨它如何以及如何没有解决那些语用特征或努力创造意义。让我们进入评估表示语言的各种算法技术。
- en: 2.2 Language modeling techniques
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 语言建模技术
- en: Having delved into the fundamental features of language, the principles of semiotics,
    and how LLMs interpret and process linguistic information, we now transition into
    a more practical realm. We will explore the various NLP techniques developed and
    employed to create these powerful language models. By examining the strengths
    and weaknesses of each approach, we will gain valuable insights into the effectiveness
    of these techniques in capturing the essence of human language and communication.
    This knowledge will not only help us appreciate the advancements made in the field
    of NLP but also enable us to better understand the current limitations of these
    models and the challenges that lie ahead for future research and development.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究语言的基本特征、符号学原理以及LLMs如何解释和处理语言信息之后，我们现在过渡到一个更实际的领域。我们将探讨为创建这些强大的语言模型而开发和应用的多种NLP技术。通过检查每种方法的优缺点，我们将获得宝贵的见解，了解这些技术在捕捉人类语言和交流本质方面的有效性。这种知识不仅将帮助我们欣赏NLP领域取得的进步，而且使我们能够更好地理解这些模型的当前局限性以及未来研究和开发面临的挑战。
- en: Let’s take a second to go over some data processing that will be universal to
    all language modeling. First, we’ll need to decide how to break up the words and
    symbols we’ll be passing into our model, effectively deciding what a token will
    be in our model. We’ll need a way to convert those tokens to numerical values
    and back again. Then, we’ll need to pick how our model will process the tokenized
    inputs. Each of the following techniques will build upon the previous techniques
    in at least one of these ways.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间回顾一下在所有语言模型中都会通用的数据处理方法。首先，我们需要决定如何将我们传递给模型的单词和符号进行拆分，实际上就是决定在我们模型中一个标记将是什么。我们需要一种方法将这些标记转换为数值，然后再转换回来。然后，我们需要选择我们的模型将如何处理这些标记化的输入。以下的所有技术至少以一种方式建立在之前的技术之上。
- en: 'The first of these techniques is called a bag-of-words (BoW) model, and it
    consists of simply counting words as they appear in text. You could import the
    CountVectorizer class from sklearn to use it, but it’s more instructive if we
    show you with a small snippet. It can be accomplished very easily with a dictionary
    that scans through text, creating a new vocabulary entry for each new word as
    a key and an incrementing value starting at 1:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术中的第一个被称为词袋（BoW）模型，它简单地由文本中出现的单词计数组成。你可以从sklearn导入CountVectorizer类来使用它，但如果我们用一个小的代码片段来展示，可能会更有教育意义。这可以通过一个扫描文本的字典非常容易地完成，为每个新单词创建一个新的词汇条目作为键，并从1开始递增的值：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Considering its simplicity, even this model, based entirely on frequency, can
    be quite powerful when trying to gain insight into a speaker’s intentions or at
    least their idiosyncrasies. For example, you could run a simple BoW model on inaugural
    speeches of US presidents, searching for the words “freedom,” “economy,” and “enemy”
    to gain a pretty good insight about which presidents assumed office under peacetime,
    during wartime, and during times of monetary strife, just based on how many times
    each word was mentioned. The BoW model’s weaknesses are many, however, as the
    model provides no images, semantics, pragmatics, phrases, or feelings. In our
    example, there are two instances of “words,” but because our tokenization strategy
    is just whitespace, it didn’t increment the key in the model. It doesn’t have
    any mechanisms to evaluate context or phonetics, and because it divides words
    by default on whitespace (you can obviously tokenize however you want, but try
    tokenizing on subwords and see what happens with this model—spoiler: it is bad),
    it doesn’t account for morphology either. Altogether, it should be considered
    a weak model for representing language but a strong baseline for evaluating other
    models against it. To solve the problem of BoW models not capturing any sequence
    data, N-gram models were conceived.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 N-gram and corpus-based techniques
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: N-gram models represent a marked and efficient improvement to BoW by allowing
    you to give the model a sort of context, represented by N. They are relatively
    simple statistical models that enable you to generate words based on the N = 1
    context space. Listing 2.1 uses trigrams, which means N = 3\. We clean the text
    and give it minimal padding/formatting to help the model, and then we train using
    everygrams, which prioritizes flexibility over efficiency so that we can train
    a pentagram (N = 5) or a septagram (N = 7) model if we want. At the end of the
    listing, where we are generating, we can give the model up to two tokens to help
    it figure out how to generate further. N-gram models were not created for and
    have never claimed to attempt complete modeling systems of linguistic knowledge,
    but they are widely useful in practical applications. They ignore all linguistic
    features, including syntax, and only attempt to draw probabilistic connections
    between words appearing in an N-length phrase.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  All assets necessary to run the code—including text and data files—can
    be found in the code repository accompanying this book: [https://github.com/IMJONEZZ/LLMs-in-Production/](https://github.com/IMJONEZZ/LLMs-in-Production/).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 A generative N-grams language model implementation
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Creates a corpus from any number of plain .txt files'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Pads each side of every line in the corpus with <s> and </s> to indicate
    the start and end of utterances'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Allows everygrams to create a training set and a vocab object from the data'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Instantiates and trains the model we’ll use for N-grams, a maximum likelihood
    estimator (MLE)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 实例化和训练我们将用于N-grams的最大似然估计器（MLE）的模型'
- en: '#5 This model will take the everygrams vocabulary, including the <UNK> token
    used for out-of-vocabulary.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 此模型将采用everygrams词汇表，包括用于未知词汇的<UNK>标记。'
- en: '#6 Language can be generated with this model and conditioned with n-1 tokens
    preceding.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 可以使用此模型生成语言，并使用n-1个标记进行条件化。'
- en: 'This code is all that you need to create a generative N-gram model. For those
    interested in being able to evaluate that model further, we’ve included the following
    code so you can grab probabilities and log scores or analyze the entropy and perplexity
    of a particular phrase. Because this is all frequency-based, even though it’s
    mathematically significant, it still does a pretty bad job of describing how perplexing
    or frequent real-world language actually is:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码就是您需要创建生成性N-gram模型的所有内容。对于那些有兴趣进一步评估该模型的人来说，我们包括了以下代码，以便您可以获取概率和对数分数，或者分析特定短语的熵和困惑度。因为这一切都是基于频率的，尽管它在数学上很重要，但它仍然无法很好地描述现实世界语言的实际困惑度或频率：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Any set of tokens up to length = n can be counted easily to determine frequency.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 可以轻松计数长度不超过n的任何一组标记，以确定频率。'
- en: '#2 Any token can be given a probability of occurrence and augmented with up
    to n-1 tokens to precede it.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 任何标记都可以赋予一个发生概率，并可以增加最多n-1个标记来先于它。'
- en: '#3 This can be done as a log score as well to avoid very big and very small
    numbers.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 这也可以作为一个对数分数来完成，以避免非常大的和非常小的数字。'
- en: '#4 Sets of tokens can be tested for entropy and perplexity as well.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 可以测试标记集的熵和困惑度。'
- en: 'While this code example illustrates creating a trigram language model, unfortunately,
    not all phrases needing to be captured are only three tokens long. For example,
    from Hamlet, “To be or not to be” consists of one phrase with two words and one
    phrase with four words. Note that even though N-grams are typically very small
    language models, it is possible to make an N-gram LLM by making N=1,000,000,000
    or higher, but don’t expect to get even one ounce of use out of it. Just because
    we made it big doesn’t make it better or mean it’ll have any practical application:
    99.9% of all text and 100% of all meaningful text contains fewer than 1 billion
    tokens appearing more than once, and that computational power can be much better
    spent elsewhere.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个代码示例说明了创建三元语言模型，但不幸的是，并非所有需要捕获的短语都只有三个标记长。例如，从《哈姆雷特》中，“To be or not to be”由一个包含两个单词和一个包含四个单词的短语组成。请注意，尽管N-grams通常是较小的语言模型，但通过将N设置为10亿或更高，可以制作一个N-gram
    LLM，但不要期望从中得到任何实际用途。仅仅因为我们做得很大，并不意味着它会更好，或者意味着它将具有任何实际应用：99.9%的所有文本和100%的有意义文本中，出现次数超过一次的标记少于10亿，而且这种计算能力可以更好地用于其他地方。
- en: N-grams only use static signals (whitespace, orthography) and words to extract
    meaning (figure 2.2). They try to measure phrases manually, assuming all phrases
    will be the same length. That said, N-grams can be used to create powerful baselines
    for text analysis. In addition, if the analyst already knows the pragmatic context
    of the utterance, N-grams can give quick and accurate insight into real-world
    scenarios. Nonetheless, this type of phrasal modeling fails to capture any semantic
    encodings that individual words could have. To solve this problem, Bayesian statistics
    were applied to language modeling.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: N-grams仅使用静态信号（空白字符、正字法）和单词来提取意义（图2.2）。它们试图手动测量短语，假设所有短语都将具有相同的长度。话虽如此，N-grams可以用于创建文本分析的强大基线。此外，如果分析师已经知道话语的语用背景，N-grams可以快速准确地洞察现实世界场景。尽管如此，这种短语建模无法捕捉到单个单词可能具有的任何语义编码。为了解决这个问题，将贝叶斯统计应用于语言建模。
- en: 2.2.2 Bayesian techniques
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 贝叶斯技术
- en: Bayes’ theorem is one of the most mathematically sound and simple theories for
    describing the occurrence of your output within your input space. Essentially,
    it calculates the probability of an event occurring based on prior knowledge.
    The theorem posits that the probability of a hypothesis being true given evidence—for
    example, that a sentence has a positive sentiment—is equal to the probability
    of the evidence occurring given the hypothesis is true multiplied by the probability
    of the hypothesis occurring, all divided by the probability of the evidence being
    true. It can be expressed mathematically as
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理是描述你的输出在输入空间中发生的一种最数学上合理且简单的理论。本质上，它基于先验知识计算事件发生的概率。该定理提出，给定证据的假设为真的概率——例如，一个句子具有积极情感的概率——等于给定假设为真的证据发生的概率乘以假设发生的概率，所有这些除以证据为真的概率。它可以表示为数学公式
- en: '*P*(*hypothesis* | *evidence*) = (*P*(*evidence* | *hypothesis*) × *P*(*hypothesis*))
    / *P*(*evidence*)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*hypothesis* | *evidence*) = (*P*(*evidence* | *hypothesis*) × *P*(*hypothesis*))
    / *P*(*evidence*)'
- en: or
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '*P*(*A*|*B*) × *P*(*B*) = *P*(*B*|*A*) × *P*(*A*)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A*|*B*) × *P*(*B*) = *P*(*B*|*A*) × *P*(*A*)'
- en: Because this isn’t a math book, we’ll dive into Bayes’ theorem to the exact
    same depth we dove into other linguistics concepts and trust the interested reader
    to search for more.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这不是一本数学书，我们将深入探讨贝叶斯定理，就像我们深入研究其他语言学概念一样，并相信感兴趣的读者会去寻找更多。
- en: Unfortunately, even though the theorem represents the data in a mathematically
    sound way, it doesn’t account for any stochasticity or multiple meanings of words.
    One word you can always throw at a Bayesian model to confuse it is “it.” Any demonstrative
    pronoun ends up getting assigned values in the same `LogPrior` and `LogLikelihood`
    way as all other words, and it gets a static value, which is antithetical to the
    usage of those words. For example, if you’re trying to perform sentiment analysis
    on an utterance, assigning all pronouns a null value would be better than letting
    them go through the Bayesian training. Note also that Bayesian techniques don’t
    create generative language models the way the rest of these techniques will. Because
    of the nature of Bayes’ theorem validating a hypothesis, these models work for
    classification and can bring powerful augmentation to a generative language model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，尽管这个定理以数学上合理的方式表示数据，但它没有考虑到任何随机性或单词的多种含义。你可以向贝叶斯模型投掷的一个总是能使其混淆的词是“它”。任何指示代词最终都会以与所有其他单词相同的`LogPrior`和`LogLikelihood`方式分配值，并且它得到一个静态值，这与这些词的使用是相反的。例如，如果你正在尝试对一个话语进行情感分析，将所有代词分配一个空值会比让它们通过贝叶斯训练更好。请注意，贝叶斯技术不会像其他技术那样创建生成语言模型。由于贝叶斯定理验证假设的性质，这些模型适用于分类，并且可以为生成语言模型带来强大的增强。
- en: Listing 2.2 shows you how to create a naive Bayes classification language model,
    or a system that performs classification on text based on a prior-learned internal
    language model. Instead of using a package like sklearn or something that would
    make writing the code a little easier, we opted to write out what we were doing,
    so it’s a bit longer, but it should be more information about how it works. We
    are using the least-complex version of a naive Bayes model. We haven’t made it
    multinomial or added anything fancy; obviously, it would work better if you opted
    to upgrade it for any problem you want. And we highly recommend you do.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2展示了如何创建一个朴素贝叶斯分类语言模型，或者是一个基于先前学习内部语言模型对文本进行分类的系统。我们选择编写我们正在做的事情，而不是使用像sklearn这样的包或使编写代码变得容易的东西，所以它会更长，但它应该提供更多关于它是如何工作的信息。我们使用的是朴素贝叶斯模型的最简单版本。我们没有将其改为多项式或添加任何花哨的东西；显然，如果你选择升级它以解决任何问题，它会工作得更好。我们强烈建议你这样做。
- en: 'NOTE  To make the code easier to understand and help highlight the portions
    we wanted to focus on, we have simplified some of our code listings by extracting
    portions to utility helpers. If you are seeing import errors, this is why. These
    helper methods can be found in the code repository accompanying this book: [https://github.com/IMJONEZZ/LLMs-in-Production/](https://github.com/IMJONEZZ/LLMs-in-Production/)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了使代码更容易理解并帮助突出我们想要关注的部分，我们通过提取部分代码到实用辅助函数中，简化了一些我们的代码列表。如果你看到导入错误，这就是原因。这些辅助方法可以在本书附带的代码库中找到：[https://github.com/IMJONEZZ/LLMs-in-Production/](https://github.com/IMJONEZZ/LLMs-in-Production/)
- en: Listing 2.2 Categorical naive Bayes language model implementation
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2 分类朴素贝叶斯语言模型实现
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Δefines the key, which is the word and label tuple'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 Δ 定义了键，即单词和标签元组'
- en: '#2 If the key exists in the dictionary, increments the count'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 如果键存在于字典中，则增加计数'
- en: '#3 If the key is new, adds it to the dict and sets the count to 1'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 如果键是新的，则将其添加到字典中并将计数设置为 1'
- en: '#4 Calculates V, the number of unique words in the vocabulary'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 计算 V，词汇表中的唯一单词数量'
- en: '#5 Calculates N_pos and N_neg'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 计算 N_pos 和 N_neg'
- en: '#6 If the label is positive (greater than zero) . . .'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 如果标签是正面的（大于零） . . .'
- en: '#7 . . . increments the number of positive words (word, label)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 . . . 增加正面单词的数量（单词，标签）'
- en: '#8 Else, the label is negative.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 否则，标签是负面的。'
- en: '#9 Increments the number of negative words (word, label)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 增加负面单词的数量（单词，标签）'
- en: '#10 Calculates Δ, the number of documents'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 计算 Δ，文档数量'
- en: '#11 Calculates the number of positive documents'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 计算正面文档的数量'
- en: '#12 Calculates the number of negative documents'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 计算负面文档的数量'
- en: '#13 Calculates logprior'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 计算对数先验概率'
- en: '#14 For each word in the vocabulary . . .'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#14 对于词汇表中的每个单词 . . .'
- en: '#15 . . . calculates the probability that each word is positive or negative'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#15 . . . 计算每个单词是正面还是负面的概率'
- en: '#16 Calculates the log likelihood of the word'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#16 计算单词的对数似然值'
- en: '#17 Processes the utt to get a list of words'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#17 处理语音以获取单词列表'
- en: '#18 Initializes probability to zero'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#18 将概率初始化为零'
- en: '#19 Adds the logprior'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#19 添加对数先验概率'
- en: '#20 Checks if the word exists in the loglikelihood dictionary'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#20 检查单词是否存在于对数似然字典中'
- en: '#21 Adds the log likelihood of that word to the probability'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#21 将该单词的对数似然值添加到概率中'
- en: '#22 Returns this properly'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#22 正确返回'
- en: '#23 If the prediction is &gt; 0 . . .'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#23 如果预测值大于 0 . . .'
- en: '#24 . . . the predicted class is 1.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#24 . . . 预测的类别是 1。'
- en: '#25 Otherwise, the predicted class is 0.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#25 否则，预测的类别是 0。'
- en: '#26 Appends the predicted class to the list y_hats'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#26 将预测的类别添加到列表 y_hats 中'
- en: '#27 Error = avg of the abs vals of the diffs between y_hats and test_y.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#27 误差 = y_hats 和 test_y 之间差的绝对值的平均值。'
- en: '#28 Accuracy is 1 minus the error.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#28 准确率是 1 减去误差。'
- en: This theorem doesn’t create the same type of language model but one with a list
    of probabilities associated with one hypothesis. As such, Bayesian language models
    can’t be used effectively to generate language, but they can be very powerfully
    implemented for classification tasks. In our opinion, though, Bayesian models
    are often overhyped for even this task. One of the crowning achievements of one
    author’s career was replacing and removing a Bayesian model from production.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理并没有创建同类型的语言模型，而是一个与一个假设相关联的概率列表。因此，贝叶斯语言模型不能有效地用于生成语言，但它们在分类任务中可以非常强大地实现。然而，我们认为，贝叶斯模型对于这项任务通常被过度炒作。一位作者职业生涯的辉煌成就之一就是从生产中替换并移除了一个贝叶斯模型。
- en: In Bayesian models, one big problem is that all sequences are completely unconnected,
    like BoW models, moving us to the opposite end of sequence modeling from N-grams.
    Like a pendulum, language modeling swings back toward sequence modeling and language
    generation with Markov chains.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯模型中，一个主要问题就是所有序列都是完全未连接的，就像词袋模型一样，将我们推向序列建模和语言生成的对立面，从 N-gram 开始。就像摆锤一样，语言建模又回到了序列建模和语言生成，使用马尔可夫链。
- en: 2.2.3 Markov chains
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 马尔可夫链
- en: Often called hidden Markov models (HMMs), Markov chains essentially add state
    to the N-gram models, storing probabilities using hidden states. They are often
    used to help parse text data for even larger models, doing things like part-of-speech
    (PoS) tagging (marking words with their parts of speech) and named entity recognition
    (NER; marking identifying words with their referent and usually type; e.g., LA
    – Los Angeles – City) on textual data. Building on the previous Bayesian models,
    Markov models rely completely on stochasticity (predictable randomness) in the
    tokens encountered. The idea that the probability of anything happening *next*
    depends completely upon the state of *now* is, like Bayes’ theorem, mathematically
    sound. So instead of modeling words based solely on their historical occurrence
    and drawing a probability from that, we model their future and past collocation
    based on what is currently occurring. So the probability of “happy” occurring
    goes down to almost zero if “happy” was just output but goes up significantly
    if “am” has just occurred. Markov chains are so intuitive that they were incorporated
    into later iterations of Bayesian statistics and are still used in production
    systems today.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通常被称为隐马尔可夫模型（HMMs），马尔可夫链本质上是在N-gram模型中添加状态，使用隐藏状态存储概率。它们常用于帮助解析文本数据，用于更大的模型，如进行词性标注（PoS；标记单词的词性）和命名实体识别（NER；标记具有其指代和通常类型的识别词；例如，LA
    – 洛杉矶 – 城市）等。建立在之前的贝叶斯模型之上，马尔可夫模型完全依赖于遇到的标记中的随机性（可预测的随机性）。任何发生的事情的概率完全取决于现在的状态，这一观点与贝叶斯定理一样，在数学上是合理的。因此，我们不是仅仅基于单词的历史出现来建模单词，并从中抽取概率，而是根据当前发生的情况来建模它们的未来和过去搭配。因此，“快乐”发生的概率如果“快乐”刚刚输出就会下降到几乎为零，但如果“am”刚刚出现，概率就会显著上升。马尔可夫链如此直观，以至于它们被纳入了贝叶斯统计学的后续迭代中，并且至今仍在生产系统中使用。
- en: In listing 2.3, we train a Markov chain generative language model. This is the
    first model where we’ve used a specific tokenizer, which, in this case, will tokenize
    based on the whitespace between words. This is also only the second time we’ve
    referred to a collection of utterances meant to be viewed together as a document.
    As you play around with this one, pay close attention and make some comparisons
    yourself of how well the HMM generates compared to even a large N-gram model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表2.3中，我们训练了一个马尔可夫链生成语言模型。这是我们第一次使用特定的标记器，在这种情况下，它将基于单词之间的空白进行标记。这也是我们第二次将一组旨在一起查看的陈述视为文档。当您玩弄这个模型时，请密切关注，并自己做一些比较，看看HMM的生成效果是否比大型N-gram模型好。
- en: Listing 2.3 Generative hidden Markov language model implementation
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.3 生成隐马尔可夫语言模型实现
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code shows a basic implementation of a Markov model for generation, and
    we encourage you to experiment with it. Give it text from songs from your favorite
    musicians or books from your favorite authors, and see whether what comes out
    sounds like them. HMMs are incredibly fast and are often used in predictive text
    or predictive search applications. Markov models represent the first comprehensive
    attempt to model language from a descriptive linguistic perspective, as opposed
    to a prescriptive one. The perspective is interesting because Markov did not originally
    intend to use linguistic modeling, only to win an argument about continuous independent
    states. Later, Markov used Markov chains to model vowel distribution in a Pushkin
    novel, so he was at least aware of the possible applications.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码展示了生成马尔可夫模型的基本实现，我们鼓励您对其进行实验。用您最喜欢的音乐家的歌曲或您最喜欢的作者的书中的文本进行尝试，看看输出的内容是否像他们。隐马尔可夫模型（HMMs）非常快速，常用于预测文本或预测搜索应用。马尔可夫模型代表了从描述性语言学的角度对语言进行建模的第一个全面尝试，而不是规范性建模。这种视角很有趣，因为马尔可夫最初并没有打算使用语言建模，只是想在一个关于连续独立状态的争论中获胜。后来，马尔可夫使用马尔可夫链来模拟普希金小说中的元音分布，所以他至少意识到了可能的用途。
- en: 'The difference between descriptive and prescriptive linguistics is that the
    latter focuses on how things *ought* to be, while the former focuses on how things
    *are*. From a language modeling perspective, it has proven vastly more effective
    to describe what language is doing from a corpus or Markov perspective rather
    than to attempt to prescribe how language ought to behave. Unfortunately, a current
    state by itself cannot be used to give context beyond the now, so historical or
    societal context cannot be represented effectively in a Markov model. The semantic
    encoding of words also becomes problematic, as represented in the code example:
    Markov chains will output syntactically correct chains of words that are nonsense
    semantically, similar to “colorless green ideas sleep furiously.” To solve this
    problem, “continuous” models were developed to allow for a “semantic embedding”
    representation of tokens.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性语言学和规范性语言学的区别在于，后者关注事物应该如何，而前者关注事物实际上是如何的。从语言建模的角度来看，从语料库或马尔可夫视角描述语言所做的事情，比试图规定语言应该如何表现要有效得多。不幸的是，当前状态本身不能用来提供超越现在的语境，因此历史或社会语境在马尔可夫模型中无法有效表示。单词的语义编码也变得有问题，如代码示例所示：马尔可夫链会输出语法上正确但语义上无意义的单词序列，类似于“无色的绿色想法疯狂地睡觉。”为了解决这个问题，开发了“连续”模型，以允许对标记进行“语义嵌入”表示。
- en: 2.2.4 Continuous language modeling
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 连续语言建模
- en: A continuous bag-of-words (CBoW) model—much like its namesake, the BoW model—is
    a frequency-based approach to analyzing language, meaning that it models words
    based on how often they occur. The next word in a human utterance has never been
    determined based on probability or frequency. Consequently, we provide an example
    of creating word embeddings to be ingested or compared by other models using a
    CBoW. We’ll use a neural network to provide you with a good methodology.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 连续词袋模型（CBoW）——就像其同名的词袋模型一样——是一种基于频率分析语言的方法，这意味着它根据单词出现的频率来建模单词。人类话语中的下一个单词从未根据概率或频率来确定。因此，我们提供了一个使用CBoW创建词嵌入的例子，以便其他模型可以摄取或比较。我们将使用神经网络为您提供一种良好的方法。
- en: This is the first language modeling technique we’ll see that essentially slides
    a context window over a given utterance (the context window is an N-gram model)
    and attempts to guess the word in the middle based on the surrounding words in
    the window. For example, let’s say your window has a length of 5, and your sentence
    is “Learning about linguistics makes me happy.” You would give the CBoW `['learning',`
    `'about',` `'makes',` `'me']` to try to get the model to guess “linguistics” based
    on how many times the model has previously seen that word occur in similar places.
    This example shows you why generation is difficult for models trained like this.
    Say you give the model `['makes',` `'me',` `'</s>']` as input. Now the model only
    has three pieces of information, instead of four, to use to try to figure out
    the answer; it also will be biased toward only guessing words it has seen before
    at the end of sentences, as opposed to getting ready to start new clauses. It’s
    not all bad, though. One feature that makes continuous models stand out for embeddings
    is that they don’t have to look at only words before the target word; they can
    also use words that come after the target to gain some semblance of context.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将看到的第一种语言建模技术，它本质上是在给定的话语上滑动一个上下文窗口（上下文窗口是一个N-gram模型），并试图根据窗口周围的单词来猜测中间的单词。例如，假设你的窗口长度为5，你的句子是“学习语言学让我很高兴。”你会给CBoW
    `['learning',` `'about',` `'makes',` `'me']`，试图让模型根据模型之前在类似位置看到该单词出现的次数来猜测“linguistics”。这个例子说明了为什么对于像这样训练的模型来说生成是困难的。比如说你给模型输入
    `['makes',` `'me',` `'</s>
- en: In listing 2.4, we create our first continuous model. In our case, to keep things
    as simple as possible, we use a BoW model for the language processing and a one-layer
    neural network with two parameters for the embedding estimation, although both
    could be substituted for any other models. For example, you could substitute N-grams
    for the BoW and a naive Bayes model for the neural network to get a continuous
    naive N-gram model. The point is that the actual models used in this technique
    are a bit arbitrary; it’s the continuous technique that’s important. To illustrate
    this further, we don’t use any packages other than `numpy` to do the math for
    the neural network, even though it’s the first one appearing in this section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '在列表2.4中，我们创建了我们的第一个连续模型。在我们的情况下，为了尽可能保持简单，我们使用BoW模型进行语言处理，并使用一个层和两个参数的神经网络进行嵌入估计，尽管两者都可以替换为任何其他模型。例如，你可以用N-gram替换BoW，用朴素贝叶斯模型替换神经网络，以获得连续的朴素N-gram模型。重点是，在这个技术中实际使用的模型有点任意；重要的是连续技术。为了进一步说明这一点，我们除了`numpy`之外不使用任何其他包来为神经网络进行数学运算，尽管它是本节中第一个出现的。 '
- en: Pay special attention to the steps—initializing the model weights, the rectified
    linear unit (ReLU) activation function, the final softmax layer, and forward and
    backpropagation—and how it all fits together in the `gradient_descent` function.
    These are pieces of the puzzle that you will see crop up again and again, regardless
    of programming language or framework. You will need to initialize models, pick
    activation functions, pick final layers, and define forward and backward propagation
    in TensorFlow, PyTorch, and Hugging Face, as well as if you ever start creating
    your own models instead of using someone else’s.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细注意以下步骤——初始化模型权重、修正线性单元（ReLU）激活函数、最终的softmax层以及前向和反向传播——以及它们如何在`gradient_descent`函数中相互配合。这些是拼图中反复出现的部分，无论编程语言或框架如何。你将需要在TensorFlow、PyTorch、Hugging
    Face中初始化模型、选择激活函数、选择最终层以及定义前向和反向传播，如果你开始创建自己的模型而不是使用别人的。
- en: Listing 2.4 Generative CBoW language model implementation
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4 生成式CBoW语言模型实现
- en: '[PRE5]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Creates our corpus for training'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建我们的训练语料库'
- en: '#2 Slightly cleans the data by removing punctuation, tokenizing by word, and
    converting to lowercase alpha characters'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 通过删除标点符号、按单词标记化并转换为小写字母字符来稍微清理数据'
- en: '#3 Gets our bag of words, along with a distribution'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取我们的词袋，以及一个分布'
- en: '#4 Creates two dictionaries to speed up time-to-convert and keep track of vocabulary'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 创建两个字典以加快转换时间和跟踪词汇'
- en: '#5 Here, we create our neural network with one layer and two parameters.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 在这里，我们使用一个层和两个参数创建我们的神经网络。'
- en: '#6 Creates our final classification layer, which makes all possibilities add
    up to 1'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 创建我们的最终分类层，使得所有可能性加起来等于1'
- en: '#7 Δefines the behavior for moving forward through our model, along with an
    activation function'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 定义了通过我们的模型向前移动的行为，以及一个激活函数'
- en: '#8 Δefine how we determine the distance between ground truth and model predictions'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 定义了如何确定真实值和模型预测之间的距离'
- en: '#9 Δefines how we move backward through the model and collect gradients'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 定义了如何通过模型向后移动并收集梯度'
- en: '#10 Puts it all together and trains'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 将所有内容组合在一起并进行训练'
- en: '#11 Trains the model'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 训练模型'
- en: The CBoW example is our first code example to showcase a full and effective
    training loop in machine learning. Within all of that, pay special attention to
    the steps in a training loop, especially the activation function, ReLU. As we
    expect you to be at least familiar with various ML paradigms, including different
    activations, we won’t explain the ReLU here. We will address when you should use
    it and when you shouldn’t. ReLUs, while solving the vanishing gradient problem,
    don’t solve the exploding gradient problem, and they destroy all negative comparisons
    within the model. Better situational variants include the Exponential linear unit
    (ELU), which allows negative numbers to normalize to alpha, and the generalized
    Gaussian linear units (GEGLU)/Swish-gated linear unit (SWIGLU), which works well
    in increasingly perplexing scenarios, like language. However, people often use
    ReLUs, not because they are the best in a situation, but because they are easy
    to understand and code and intuitive, even more so than the activations they were
    created to replace, the sigmoid or tanh.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: CBoW示例是我们第一个展示机器学习中完整且有效训练循环的代码示例。在这个过程中，请特别注意训练循环中的步骤，特别是激活函数ReLU。由于我们预期你对各种ML范式至少是熟悉的，包括不同的激活函数，我们在这里不会解释ReLU。我们将讨论何时应该使用它以及何时不应该使用它。ReLU虽然解决了梯度消失问题，但并没有解决梯度爆炸问题，并且会破坏模型中所有的负比较。更好的情境变体包括指数线性单元（ELU），它允许负数归一化到alpha，以及广义高斯线性单元（GEGLU）/Swish门控线性单元（SWIGLU），它们在越来越复杂的场景中表现良好，如语言。然而，人们经常使用ReLU，并不是因为它们在某个情境下是最好的，而是因为它们易于理解、编码和直观，甚至比它们所取代的激活函数（如sigmoid或tanh）更直观。
- en: A lot of this ends up being abstracted with packages and the like, but knowing
    what’s going on under the hood will be very helpful for you as someone putting
    LLMs in production. You should be able to predict with some certainty how different
    models will behave in various situations. The next section will dive into one
    of those abstractions—in this case, the abstraction created by the continuous
    modeling technique.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 许多内容最终都会通过包和其他类似工具进行抽象化，但了解底层发生的事情对于将LLMs投入生产的你来说将非常有帮助。你应该能够以某种确定性预测不同模型在不同情况下的行为。下一节将深入探讨这些抽象化之一——在这种情况下，是连续建模技术创建的抽象化。
- en: 2.2.5 Embeddings
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 嵌入
- en: Hearkening back to our features of language, it should be easy to connect why
    continuous-style language modeling was such a breakthrough. Embeddings take the
    tokenized vectors we’ve created that don’t contain any meaning and attempt to
    insert that meaning based on observations that can be made about the text, such
    as word order and subwords appearing in similar contexts. Despite the primary
    mode of meaning being collocation (co-located, words that appear next to each
    other), they prove useful and even show some similarities to human-encoded word
    meaning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾到我们关于语言的特征，应该很容易理解为什么连续风格的文本建模是一个突破。嵌入将我们创建的、不包含任何意义的标记化向量，尝试根据对文本的观察插入意义，例如词序和出现在相似上下文中的子词。尽管主要的意义模式是搭配（共定位，相邻出现的词），但它们证明是有用的，甚至显示出与人类编码的词义的一些相似性。
- en: 'The quintessential example from Word2Vec, one of the first pretrained vector
    embeddings, was taking the vector for “king,” subtracting the vector for “man,”
    adding the vector for “woman,” and finding the nearest neighbor to the sum was
    the vector for the word “queen.” This makes sense to us, as it mimics human semantics.
    One of the major differences is one that’s already been mentioned a couple of
    times: pragmatics. Humans use pragmatic context to inform semantic meaning, understanding
    that just because you said, “I need food,” doesn’t mean you are actually in physical
    danger without it. Embeddings are devoid of any influence outside of pure usage,
    which feels like it could be how humans learn as well, and there are good arguments
    on all sides here. The one thing holding is that if we can somehow give models
    more representative data, that may open the door to more effective embeddings,
    but it’s a chicken-and-egg problem because more effective embeddings give better
    model performance.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 中的一个经典示例，也是最早的预训练向量嵌入之一，就是取“king”（国王）的向量，减去“man”（男人）的向量，加上“woman”（女人）的向量，然后找到与这个和最近的邻居向量，就是“queen”（王后）这个词的向量。这在我们看来是有道理的，因为它模仿了人类的语义。其中一个已经被提到几次的主要区别是：语用学。人类使用语用上下文来告知语义意义，理解到你说“我需要食物”并不意味着你实际上没有食物就会处于身体危险之中。嵌入没有受到任何纯使用之外的影响，这感觉就像人类学习的方式一样，而且在这里有很好的论据。唯一确定的是，如果我们能以某种方式给模型提供更具代表性的数据，这可能为更有效的嵌入打开大门，但这是一个鸡生蛋、蛋生鸡的问题，因为更有效的嵌入会带来更好的模型性能。
- en: In listing 2.5, we dive into how to visualize embeddings using `pyplot`. We
    will be going more in depth into embeddings in later chapters. This is helpful
    for model explainability and also for validation during your pretraining step.
    If you see that your semantically similar embeddings are relatively close to each
    other on the graph, you’re likely going in the right direction.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在列出2.5时，我们深入探讨了如何使用`pyplot`可视化嵌入。我们将在后面的章节中更深入地探讨嵌入。这对于模型可解释性和在预训练步骤中的验证都很有帮助。如果你看到你的语义相似的嵌入在图上相对较近，你很可能是在正确的方向上。
- en: Listing 2.5 Embedding visualization
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列出2.5：嵌入可视化
- en: '[PRE6]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 After listing 2.4 is done and gradient descent has been executed'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 列出2.4完成后，并执行了梯度下降'
- en: As shown in figure 2.4, this code is a successful but very sparse embedding
    representation that we trained from our CBoW model. Getting those semantic representations
    (embeddings) to be denser is the main place we can see improvement in this field,
    although many successful experiments have been run where denser semantic meaning
    has been supplanted with greater pragmatic context through instruct and different
    thought-chaining techniques. We will address chain of thought (CoT) and other
    techniques later. For now, let’s pivot to discussing why our continuous embedding
    technique can even be successful, given that frequency-based models are characteristically
    difficult to correlate with reality. All of this started with the MLP more than
    half a century ago.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2.4所示，这是我们从CBoW模型中训练出的一个成功但非常稀疏的嵌入表示。使这些语义表示（嵌入）更密集是我们在这个领域可以看到的主要改进之处，尽管已经进行了许多实验，其中更密集的语义意义被通过指导和不同的思维链技术用更大的语用上下文所取代。我们将在后面讨论思维链（CoT）和其他技术。现在，让我们转向讨论为什么我们的连续嵌入技术甚至可以成功，鉴于基于频率的模型通常很难与现实相关联。所有这一切都始于半个多世纪前的MLP。
- en: '![figure](../Images/2-4.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-4.png)'
- en: Figure 2.4 A visualization technique for word embeddings. Visualizing embeddings
    can be important for model explainability.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4：词嵌入的可视化技术。可视化嵌入对于模型可解释性很重要。
- en: 2.2.6  Multilayer perceptrons
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6  多层感知器
- en: MLPs are the embodiment of the sentiment, “Machines are really good at doing
    one thing, so I wish we could just use a bunch of machines that are really good
    at the one thing to make one that’s good at a lot of things.” Every weight and
    bias in the neural network of the MLP is good at doing one thing, which could
    be detecting one or more features. So we bind a bunch of them together to detect
    larger, more complex features. MLPs serve as the primary building block in most
    neural network architectures. The key distinctions between architectures, such
    as convolutional neural networks and recurrent neural networks, mainly arise from
    data loading methods and the handling of tokenized and embedded data as it flows
    through the layers of the model rather than the functionality of individual layers,
    particularly the fully connected layers.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs是“机器擅长做一件事，所以希望我们能够只用一些擅长这件事的机器来做出擅长很多事情的机器”这一观点的体现。MLP神经网络中的每一个权重和偏差都擅长做一件事，这可能是检测一个或多个特征。因此，我们将它们绑定在一起以检测更大、更复杂的特征。MLPs是大多数神经网络架构中的主要构建块。架构之间的关键区别，如卷积神经网络和循环神经网络，主要源于数据加载方法以及处理在模型层中流动的标记化和嵌入数据，而不是单个层的功能，尤其是全连接层。
- en: Listing 2.6 provides a more dynamic class of neural networks that can have as
    many layers and parameters as deemed necessary for your task. We give a more defined
    and explicit class using PyTorch to give you the tools to implement the MLP in
    whatever way you’d like, both from scratch and in a popular framework.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.6提供了一个更动态的神经网络类，它可以拥有你任务所需的所有层和参数。我们使用PyTorch提供了一个更定义明确和显式的类，以便你能够以任何你想要的方式实现MLP，无论是从头开始还是在一个流行的框架中。
- en: Listing 2.6 Multilayer perceptron PyTorch class implementation
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6 多层感知器PyTorch类实现
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From this code, we can see, as opposed to the CBoW implementation, which had
    two static layers, that this MLP is not static in size until it has been instantiated.
    If you wanted to give this model 1 million layers, you would have to put `num_hidden_layers=
    1000000` when you instantiate the class. However, just because you give a model
    that many parameters doesn’t mean that will make it immediately better. LLMs are
    more than just a lot of layers. Like RNNs and CNNs, the magic of LLMs is in how
    data goes in and moves through the model. To illustrate, let’s look at the RNN
    and one of its variations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从这段代码中，我们可以看出，与具有两个静态层的CBoW实现不同，这个MLP在实例化之前并不是静态的尺寸。如果你想给这个模型一百万层，你必须在实例化类时将`num_hidden_layers=1000000`。然而，仅仅因为给模型那么多参数并不意味着它会立即变得更好。LLMs不仅仅是很多层。就像RNNs和CNNs一样，LLMs的魔力在于数据如何进入模型并在其中移动。为了说明这一点，让我们看看RNN及其变体。
- en: 2.2.7 Recurrent neural networks and long short-term memory networks
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.7 循环神经网络和长短期记忆网络
- en: RNNs are a class of neural networks designed to analyze sequences based on the
    weaknesses in previous language modeling techniques. A sequence can be thought
    of as an ordered array, where the sum of the whole array changes value if any
    of the parts are moved around. The logic goes that if language is presented in
    a sequence, then maybe it should be processed in a sequence instead of one token
    at a time. RNNs accomplish this by using logic we’ve seen before, both in MLPs
    and Markov chains, where an internal state or memory is referred to when new inputs
    are processed and by creating cycles when connections between nodes are detected
    as useful.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs是一类神经网络，旨在根据先前语言建模技术的弱点来分析序列。可以将序列视为一个有序数组，其中整个数组的总和在任何一个部分被移动时都会改变值。逻辑是这样的：如果语言以序列的形式呈现，那么也许它应该以序列的形式进行处理，而不是一次处理一个标记。RNNs通过使用我们在MLPs和马尔可夫链中看到过的逻辑来实现这一点，即当处理新的输入时，会引用内部状态或记忆，并在检测到节点之间的连接有用时创建循环。
- en: In fully recurrent networks, like the one in listing 2.7, all nodes start out
    initially connected to all subsequent nodes, but those connections can be set
    to zero to simulate them breaking if they are not useful. This solves one of the
    biggest problems that earlier models suffered from, static input size, and enables
    an RNN and its variants to process variable length inputs. Unfortunately, longer
    sequences create a new problem. Because each neuron in the network connects to
    subsequent neurons, longer sequences create smaller changes to the overall sum,
    making the gradients smaller until they eventually vanish, even with important
    words; this is called a vanishing gradient. Other problems exist too, such as
    exploding and diminishing gradients.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全循环网络中，如列表2.7中的那种，所有节点最初都连接到所有后续节点，但这些连接可以设置为零，以模拟它们在不适用时断开。这解决了早期模型所面临的最大问题之一，即静态输入大小，并使RNN及其变体能够处理可变长度的输入。不幸的是，较长的序列会带来新的问题。因为网络中的每个神经元都连接到后续的神经元，较长的序列会导致整体总和的变化更小，使得梯度更小，直到最终消失，即使对于重要的单词；这被称为梯度消失。还存在其他问题，如梯度爆炸和梯度减小。
- en: 'For example, let’s consider the following sentences with the task sentiment
    analysis: “I loved the movie last night” and “The movie I went to see last night
    was the very best I had ever expected to see.” These sentences can be considered
    semantically similar, even if they aren’t exactly the same. When moving through
    an RNN, each word in the first sentence is worth more, and the consequence is
    that the first sentence has a higher positive rating than the second sentence
    just because the first sentence is shorter. The inverse is also true: exploding
    gradients are a consequence of this sequence processing, which makes training
    deep RNNs difficult.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑以下用于情感分析的句子：“我昨晚非常喜欢这部电影”和“我昨晚去看的电影是我曾经期望看到的最棒的一部。”这些句子在语义上可以被认为是相似的，即使它们并不完全相同。当通过RNN移动时，第一句中的每个单词都更有价值，结果是第一句的正面评分比第二句高，仅仅因为第一句更短。反之亦然：梯度爆炸是这种序列处理的结果，这使得训练深层RNN变得困难。
- en: To solve this problem, LSTMs, a type of RNN, use memory cells and gating mechanisms
    to process sequences of variable length but without the problem of comprehending
    longer and shorter sequences differently. Anticipating multilingual scenarios
    and understanding that people don’t think about language in only one direction,
    LSTMs can also process sequences bidirectionally by concatenating the outputs
    of two RNNs, one reading the sequence from left to right and the other from right
    to left. This bidirectionality improves results, allowing information to be seen
    and remembered even after thousands of tokens have passed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，LSTMs（一种RNN），通过使用记忆单元和门控机制来处理可变长度的序列，但避免了理解长序列和短序列不同的难题。考虑到多语言场景，并理解人们不会只从单一方向思考语言，LSTMs还可以通过连接两个RNN的输出（一个从左到右读取序列，另一个从右到左读取）来双向处理序列。这种双向性提高了结果，即使在成千上万的标记之后，信息也能被看到并记住。
- en: In listing 2.7, we give classes for both an RNN and an LSTM. In the code in
    the repo associated with this book ([https://github.com/IMJONEZZ/LLMs-in-Production](https://github.com/IMJONEZZ/LLMs-in-Production)),
    you can see the results of training both the RNN and LSTM. The takeaway is that
    the LSTM achieves better accuracy on both training and validation sets in half
    as many epochs (25 versus 50 with RNN). One of the innovations to note is that
    the packed embeddings utilize padding to extend all variable-length sequences
    to the maximum length. Thus, LSTMs can process input of any length as long as
    it is shorter than the maximum. To set up the LSTM effectively, we’ll do some
    classical NLP on the dataset (a Twitter sentiment analysis dataset). That workflow
    will tokenize with the Natural Language Toolkit Regex. It looks for words and
    nothing else, passing into a spacy lemmatizer to get a list of lists containing
    only the base unconjugated forms of words.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表2.7中，我们给出了RNN和LSTM的类。在与此书相关的代码库（[https://github.com/IMJONEZZ/LLMs-in-Production](https://github.com/IMJONEZZ/LLMs-in-Production)）中，你可以看到训练RNN和LSTM的结果。关键点是，LSTM在训练和验证集上都能在更少的epoch（25个与RNN的50个相比）内达到更好的准确率。值得注意的一项创新是，打包嵌入利用填充将所有可变长度的序列扩展到最大长度。因此，只要输入长度小于最大长度，LSTMs就可以处理任何长度的输入。为了有效地设置LSTM，我们将在数据集（一个Twitter情感分析数据集）上做一些经典的NLP工作。该工作流程将使用自然语言工具包正则表达式进行标记化。它只查找单词，其他什么也不找，然后传递给spacy词形还原器，以获得只包含单词基本非屈折形式的列表。
- en: Listing 2.7 RNN and LSTM PyTorch class implementations
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.7 RNN和LSTM PyTorch类实现。
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Creates our corpus for training and performs some classic NLP preprocessing'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建我们的训练语料库并执行一些经典的NLP预处理。'
- en: '#2 Embeddings are needed to give semantic value to the inputs of an LSTM.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 需要嵌入来为LSTM的输入赋予语义价值。'
- en: '#3 Usually should be a power of 2 because it’s the easiest for computer memory'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 通常应该是2的幂，因为这是计算机内存中最容易处理的形式。'
- en: '#4 You''ve got to determine some labels for whatever you''re training on.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 你必须为你在训练的东西确定一些标签。'
- en: Looking at our classes and instantiations, you should see that the LSTM is not
    vastly different from the RNN. The only difference is that the `init` input variables
    are `n_layers` (for convenience, you can also specify it with RNNs), `bidirectional`,
    and `dropout`. `bidirectional` allows LSTMs to look ahead in sequences to help
    with meaning and context. It also helps immensely with multilingual scenarios,
    as left-to-right languages like English are not the only format for orthography.
    `dropout`, another huge innovation, changes the paradigm of overfitting from being
    data dependent and helps the model not overfit by turning off random nodes layer
    by layer during training to force all nodes not to correlate with each other and
    preventing complex co-adaptations. The only difference in the out-of-model parameters
    is that the optimizer used for an RNN is stochastic gradient descent (SGD), like
    our CBoW; the LSTM uses Adam (although either could use any, depending on performance,
    including AdamW). Next, we define our training loop and train the LSTM. Compare
    this training loop to the one defined in listing 2.4 in the `gradient_descent`
    function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 看看我们的类和实例化，你应该会看到LSTM与RNN并没有太大区别。唯一的区别是`init`输入变量是`n_layers`（为了方便，你也可以用RNN指定它），`bidirectional`和`dropout`。`bidirectional`允许LSTM在序列中向前看，以帮助理解意义和上下文。它还在多语言场景中帮助很大，因为像英语这样的从左到右的语言并不是正文的唯一格式。`dropout`是另一个巨大的创新，它改变了过拟合的模式，从数据依赖转变为帮助模型不过拟合，通过在训练过程中逐层关闭随机节点来强制所有节点不相互关联，并防止复杂的共适应。模型外的参数唯一的不同之处在于，用于RNN的优化器是随机梯度下降（SGD），就像我们的CBoW一样；LSTM使用Adam（尽管两者都可以使用任何，取决于性能，包括AdamW）。接下来，我们定义我们的训练循环并训练LSTM。将这个训练循环与`gradient_descent`函数中定义的列表2.4进行比较。
- en: One of the amazing things demonstrated in the code here is how much quicker
    the LSTM can learn compared to previous model iterations, thanks to both `bidirectionality`
    and `dropout`. Although the previous models train faster than the LSTM, they take
    hundreds of epochs to get the same performance as an LSTM in just 25 epochs. As
    its name implies, the performance on the validation set adds validity to the architecture,
    performing inference during training on examples it has not trained on and keeping
    accuracy fairly close to the training set.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里展示的代码中，令人惊叹的一点是，与之前的模型迭代相比，LSTM可以学得更快，这得益于`bidirectionality`和`dropout`。尽管之前的模型训练速度比LSTM快，但它们需要数百个epoch才能达到LSTM在25个epoch内达到的性能。正如其名所示，验证集上的性能增加了架构的有效性，在它未训练的示例上进行推理，并保持准确度与训练集相当。
- en: The problems with these models are not as pronounced, manifesting primarily
    as being incredibly resource-heavy, especially when applied to longer, more detail-oriented
    problems like healthcare and law. Despite the incredible advantages of `dropout`
    and `bidirectional` processing, they both at least double the amount of processing
    power required to train. So while inference ends up being only 2 to 3 times as
    expensive as an MLP of the same size, training becomes 10 to 12 times as expensive.
    That is, `dropout` and `bidirectional` solve exploding gradients nicely but explode
    the compute required to train. To combat this problem, a shortcut was devised
    and implemented that allows any model, including an LSTM, to figure out which
    parts of a sequence are the most influential and which parts can be safely ignored,
    known as *attention*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的问题并不那么明显，主要表现为资源消耗极其巨大，尤其是在应用于像医疗和法律这样更长、更注重细节的问题时。尽管`dropout`和`bidirectional`处理具有令人难以置信的优势，但它们至少将训练所需的处理能力翻倍。因此，虽然推理最终只比相同大小的MLP贵2到3倍，但训练成本却高达10到12倍。也就是说，`dropout`和`bidirectional`很好地解决了梯度爆炸问题，但同时也增加了训练所需的计算量。为了解决这个问题，设计并实施了一种捷径，允许任何模型，包括LSTM，找出序列中哪些部分是最有影响力的，哪些部分可以安全忽略，这被称为*注意力*。
- en: 2.2.8 Attention
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.8 注意力
- en: Attention is a mathematical shortcut that gives the model a mechanism for solving
    larger context windows faster by telling the model through an emergent mathematical
    formula which parts of an input to consider and how much. Attention is based upon
    an upgraded version of a dictionary, where instead of just key–value pairs, a
    contextual query is added. Simply know that the following code is the big differentiator
    between older NLP techniques and more modern ones.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是一种数学捷径，它通过一个突现的数学公式告诉模型考虑输入的哪些部分以及多少，从而为模型提供一个机制，以更快的速度解决更大的上下文窗口。注意力基于字典的升级版本，其中不仅包含键值对，还添加了上下文查询。简单来说，以下代码是较老的自然语言处理技术与更现代技术之间的主要区别。
- en: Attention solves the slowness of training LSTMs yet keeps high performance on
    a low number of epochs. There are multiple types of attention as well. The dot
    product attention method captures the relationships between each word (or embedding)
    in your query and every word in your key. When queries and keys are part of the
    same sentences, this is known as *bi-directional self-attention*. However, in
    certain cases, it is more suitable to only focus on words that precede the current
    one. This type of attention, especially when queries and keys come from the same
    sentences, is referred to as *causal attention*. Language modeling further improves
    by masking parts of a sequence and forcing the model to guess what should be behind
    the mask. The functions in the following listing demonstrate both dot product
    attention and masked attention.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力解决了训练LSTMs的缓慢问题，同时保持了在少量epoch上的高性能。还有多种类型的注意力。点积注意力方法捕捉了查询中的每个词（或嵌入）与键中的每个词之间的关系。当查询和键是同一句子的部分时，这被称为*双向自注意力*。然而，在某些情况下，只关注当前词之前的内容可能更合适。这种类型的注意力，尤其是当查询和键来自同一句子时，被称为*因果注意力*。通过掩码序列的部分并迫使模型猜测掩码后面的内容，语言模型进一步得到改进。以下列表中的函数展示了点积注意力和掩码注意力。
- en: Listing 2.8 Multihead attention implementation
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.8 多头注意力实现
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Step 1: Input: three inputs, d_model=4'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第1步：输入：三个输入，d_model=4'
- en: '#2 Step 2: Weights three dimensions x d_model=4'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第2步：权重三个维度 x d_model=4'
- en: '#3 Step 3: Matrix multiplication to obtain Q,K,V; query: x * w_query; key:
    x * w_key; value: x * w_value'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 第3步：矩阵乘法以获得Q，K，V；查询：x * w_query；键：x * w_key；值：x * w_value'
- en: '#4 Step 4: Scaled attention scores; square root of the dimensions'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 第4步：缩放后的注意力分数；维度的平方根'
- en: '#5 Step 5: Scaled softmax attention scores for each vector'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 第5步：每个向量的缩放softmax注意力分数'
- en: '#6 Step 6: Attention value obtained by score1/k_d * V'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 第6步：通过score1/k_d * V获得的注意力值'
- en: '#7 Step 7: Sums the results to create the first line of the output matrix'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 第7步：将结果相加以创建输出矩阵的第一行'
- en: '#8 Step 8: Steps 1 to 7 for inputs 1 to 3; because this is just a demo, we’ll
    do a random matrix of the right dimensions.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 第8步：对输入1到3执行步骤1到7；因为这只是个演示，我们将使用正确维度的随机矩阵。'
- en: '#9 Step 9: We train all eight heads of the attention sublayer using steps 1
    to 7.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 第9步：我们使用步骤1到7训练注意力子层的所有八个头。'
- en: '#10 Step 10: Concatenates heads 1 to 8 to get the original 8 × 64 output dimension
    of the model'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 第10步：将头1到8连接起来，以获得模型原始的8 × 64输出维度。'
- en: '#11 This function performs all of these steps.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 此函数执行所有这些步骤。'
- en: '#12 This function performs the previous steps but adds causality in masking.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 此函数执行前面的步骤，但增加了掩码中的因果关系。'
- en: In the full implementation of attention, you may have noticed some terminology
    you’re familiar with—namely `Key` and `Value`, but you may not have been introduced
    to `Query` before. `Key` and `Value` pairs are familiar because of dictionaries
    and lookup tables, where we map a set of keys to an array of values. `Query` should
    feel intuitive as a sort of search for retrieval. The `Query` is compared to the
    `Key`s from which a `Value` is retrieved in a normal operation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力的完整实现中，你可能注意到了一些你熟悉的术语——即`键`和`值`，但你可能之前没有接触过`查询`。`键`和`值`对之所以熟悉，是因为字典和查找表，其中我们将一组键映射到一个值数组。`查询`应该感觉直观，就像是一种检索的搜索。`查询`与从正常操作中检索`值`的`键`进行比较。
- en: In attention, the `Query` and `Key`s undergo dot product similarity comparison
    to obtain an attention score, which is later multiplied by the `Value` to get
    an ultimate score for how much attention the model should pay to that portion
    of the sequence. This can get more complex, depending upon your model’s architecture,
    because both encoder and decoder sequence lengths have to be accounted for, but
    suffice it to say for now that the most efficient way to model in this space is
    to project all input sources into a common space and compare using dot product
    for efficiency.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力机制中，`Query`和`Key`通过点积相似度比较来获得注意力分数，随后这个分数会被乘以`Value`以得到模型应该关注序列该部分的最终分数。这可能会根据你的模型架构变得更加复杂，因为必须考虑到编码器和解码器的序列长度，但在此我们可以简单地说，在这个空间中建模的最有效方法是投影所有输入源到一个公共空间，并使用点积进行高效比较。
- en: 'This code explanation was a bit more math-heavy than the previous examples,
    but it is needed to illustrate the concept. The math behind attention is truly
    innovative and has rocketed the field forward. Unfortunately, even with the advantages
    attention brings to the process of sequence modeling, with LSTMs and RNNs, there
    were still problems with speed and memory size. You may notice from the code and
    the math that a square root is taken, meaning that attention, as we use it, is
    quadratic. Various techniques, including subquadratics like Hyena and the Recurrent
    Memory Transformer (RMT, basically an RNN combined with a transformer), have been
    developed to combat these problems, which we will cover in more detail later.
    For now, let’s move on to the ultimate application of attention: the transformer.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码解释比之前的例子数学性更强，但这是为了说明概念。注意力的数学原理真正具有创新性，并推动了该领域的发展。不幸的是，即使注意力为序列建模过程带来了优势，在LSTMs和RNNs中，仍然存在速度和内存大小的问题。你可能从代码和数学中注意到取了平方根，这意味着我们使用的注意力是二次的。包括像Hyena和循环记忆转换器（RMT，基本上是一个结合了转换器的RNN）这样的亚二次技术在内的各种技术已经被开发出来以解决这些问题，我们将在稍后更详细地介绍。现在，让我们继续探讨注意力的最终应用：转换器。
- en: 2.3 Attention is all you need
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 注意力即一切
- en: In the seminal paper, “Attention Is All You Need,”[¹](#footnote-250) Vaswani
    et al. take the mathematical shortcut several steps further, positing that for
    performance, absolutely no recurrence (the “R” in RNN) or any convolutions are
    needed at all.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在开创性的论文“Attention Is All You Need”中[¹](#footnote-250)，Vaswani等人将数学捷径推进了几步，提出为了性能，根本不需要任何循环（RNN中的“R”）或任何卷积。
- en: NOTE  We don’t go over convolutions because they aren’t good for NLP, but they
    are popular, especially in computer vision.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：我们不讨论卷积，因为它们对NLP来说并不好，但在计算机视觉中却很受欢迎。
- en: Instead, Vaswani et al. opted to use only attention and specify where Q, K,
    and V were taken from much more carefully. We’ll dive into this presently. In
    our review of this diverse range of NLP techniques, we have observed their evolution
    over time and the ways in which each approach has sought to improve upon its predecessors.
    From rule-based methods to statistical models and neural networks, the field has
    continually strived for more efficient and accurate ways to process and understand
    natural language.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，Vaswani等人选择只使用注意力，并更加仔细地指定Q、K和V的来源。我们将在稍后深入探讨这一点。在我们对这一系列多样化的NLP技术的回顾中，我们观察到了它们随时间的发展以及每种方法如何寻求改进其前辈。从基于规则的方法到统计模型和神经网络，该领域一直在努力寻找更高效、更准确处理和理解自然语言的方法。
- en: 'Now we turn our attention to a groundbreaking innovation that has revolutionized
    the field of NLP: the transformer architecture. In the following section, we will
    explore the key concepts and mechanisms that underpin transformers and how they
    have enabled the development of state-of-the-art language models that surpass
    the performance of previous techniques. We will also discuss the effect of transformers
    on the broader NLP landscape and consider the potential for further advancements
    in this exciting area of research.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将注意力转向一项具有革命性的创新，它彻底改变了NLP领域：转换器架构。在下一节中，我们将探讨支撑转换器的关键概念和机制，以及它们如何使得开发超越先前技术的最先进语言模型成为可能。我们还将讨论转换器对更广泛的NLP领域的影响，并考虑在这一激动人心的研究领域中进一步进步的潜力。
- en: 2.3.1 Encoders
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 编码器
- en: Encoders are the first half of a full transformer model, excelling in the areas
    of classification and feature engineering. Vaswani et al. figured out that after
    the embedding layer inside the encoder, any additional transformations done to
    the tensors could end up harming their ability to be compared “semantically,”
    which was the point of the embedding layer. These models rely heavily upon self-attention
    and clever positional encoding to manipulate those vectors without significantly
    decreasing the similarity expressed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器是完整变压器模型的前半部分，在分类和特征工程领域表现出色。Vaswani 等人发现，在编码器内部的嵌入层之后，对张量进行的任何额外转换都可能损害它们进行“语义”比较的能力，而这正是嵌入层的目的。这些模型严重依赖于自注意力和巧妙的位置编码来操纵这些向量，而不会显著降低所表达的相似度。
- en: Again, a key characteristic of embeddings is that they are vector representations
    of data—in our case, tokens. Tokens are whatever you pick to represent language.
    We recommend subwords as a general rule, but you will get a feel for where and
    which types of tokens work well. Consider the sentence, “The cat in the hat rapidly
    leapt above the red fox and the brown unmotivated dog.” “Red” and “brown” are
    semantically similar, and both are similarly represented after the embedding layer.
    However, they fall on positions 10 and 14, respectively, in the utterance, assuming
    that we’re tokenizing by word. Therefore, the positional encoding puts distance
    between them, also adding the ability to distinguish between the same tokens at
    different positions in an utterance. However, once the sine and cosine functions
    are applied, it brings their meaning back to only a little further apart than
    they were after the encoding, and this encoding mechanism scales brilliantly with
    recurrence and more data. To illustrate, let’s say there was a 99% cosine similarity
    between [red] and [brown] after embedding. Encoding would drastically reduce that
    to around 85% to 86% similarity. Applying sine and cosine methodologies as described
    brings their similarity back up to around 96%.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，嵌入的一个关键特性是它们是数据的向量表示——在我们的案例中，是标记。标记是你选择的代表语言的任何东西。我们建议按照一般规则使用子词，但你将逐渐了解哪些位置和哪些类型的标记效果良好。考虑以下句子：“The
    cat in the hat rapidly leapt above the red fox and the brown unmotivated dog。”
    “Red” 和 “brown” 在语义上是相似的，并且在嵌入层之后有类似的表示。然而，它们在句子中的位置分别是第 10 和第 14 位，假设我们是按单词进行分词。因此，位置编码在它们之间引入了距离，同时也增加了区分句子中不同位置相同标记的能力。然而，一旦应用正弦和余弦函数，它们的意义就只比编码后稍微远一点，并且这种编码机制在递归和更多数据的情况下表现出色。为了说明这一点，让我们假设在嵌入后
    [red] 和 [brown] 之间有 99% 的余弦相似度。编码将大大将其降低到大约 85% 到 86% 的相似度。按照描述应用正弦和余弦方法将它们的相似度恢复到大约
    96%。
- en: BERT was one of the first architectures after Vaswani et al.’s original paper
    and is an example of encoder-only transformers. BERT is such an incredibly powerful
    model architecture, given how small it is, that it is still used in production
    systems today. BERT was the first encoder-only transformer to surge in popularity,
    showcasing that performing continuous or sequential (they’re the same) modeling
    using a transformer results in much better embeddings than Word2Vec. We can see
    that these embeddings are better because they can be very quickly applied to new
    tasks and data with minimal training, with human-preferred results versus Word2Vec
    embeddings. For a while, most people were using BERT-based models for few-shot
    learning tasks on smaller datasets. BERT puts state-of-the-art performance within
    arm’s reach for most researchers and businesses with minimal effort required.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是在 Vaswani 等人原始论文之后出现的首批架构之一，并且是仅使用编码器的变压器的一个例子。鉴于其体积之小，BERT 是一个极其强大的模型架构，至今仍在生产系统中使用。BERT
    是第一个仅使用编码器的变压器，其受欢迎程度激增，展示了使用变压器进行连续或顺序（它们是相同的）建模，其嵌入效果比 Word2Vec 更好。我们可以看到，这些嵌入效果更好，因为它们可以非常快速地应用于新任务和数据，并且只需要最少的训练，就能得到比
    Word2Vec 嵌入更符合人类偏好的结果。在一段时间内，大多数人使用基于 BERT 的模型在较小的数据集上进行少样本学习任务。BERT 使大多数研究人员和企业能够以最小的努力获得最先进的性能。
- en: '![figure](../Images/2-5.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-5.png)'
- en: Figure 2.5 An encoder visualized. Encoders are the first half of the full transformer
    architecture and excel in natural language understanding tasks like classification
    or named entity recognition. Encoder models improve upon previous designs by not
    requiring priors or recurrence and using clever positional encoding and multihead
    attention to create a vector embedding of each token.
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 一个编码器的可视化。编码器是完整传输器架构的前半部分，在自然语言理解任务（如分类或命名实体识别）中表现出色。编码器模型通过不需要先验或递归，并使用巧妙的位置编码和多头注意力来创建每个标记的向量嵌入，从而改进了先前设计。
- en: 'The strengths of encoders (visualized in figure 2.5) include the following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的优势（如图2.5所示）包括以下内容：
- en: Classification and hierarchical tasks showcasing understanding
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示理解的分类和层次任务
- en: Blazing fast, considering the long-range dependency modeling
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到长距离依赖建模，速度极快
- en: Builds off of known models, CBoW in embedding, MLP in feed forward, etc.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立在已知模型之上，如嵌入中的CBoW，前馈中的MLP等。
- en: Parallel
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行
- en: 'Encoders weaknesses include the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的弱点包括以下内容：
- en: As suggested, requires lots of data (although less than RNNs) to be effective
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如建议所示，需要大量数据（尽管少于RNNs）才能有效
- en: Even more complex architecture
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更加复杂的架构
- en: 2.3.2 Decoders
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 解码器
- en: Decoder models, as shown in figure 2.6, are larger versions of encoders that
    have two multihead attention blocks and three sum and normalize layers in their
    base form. They are the second half of a transformer behind an encoder. Decoders
    are very good at masked language modeling and learning and applying syntax very
    quickly, leading to the almost immediate idea that decoder-only models are needed
    to achieve artificial general intelligence. A useful reduction of encoder versus
    decoder tasks is that encoders excel in natural language understanding (NLU) tasks,
    while decoders excel in natural language generation (NLG) tasks. An example of
    decoder-only transformer architectures is the Generative Pre-trained Transformer
    (GPT) family of models. These models follow the logic of transformational generative
    grammar being completely syntax based, allowing for infinite generation of all
    possible sentences in a language (see appendix A).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器模型，如图2.6所示，是编码器的大版本，其基本形式包含两个多头注意力块和三个求和归一化层。它们是编码器之后的传输器的一半。解码器在掩码语言建模和学习以及快速应用语法方面非常出色，几乎立即产生了只需要解码器模型才能实现人工通用智能的想法。编码器与解码器任务的一个有用简化是，编码器在自然语言理解（NLU）任务中表现出色，而解码器在自然语言生成（NLG）任务中表现出色。解码器仅传输器架构的例子是生成预训练传输器（GPT）模型系列。这些模型遵循转换生成语法的逻辑，完全基于语法，允许无限生成一种语言中所有可能的句子（见附录A）。
- en: '![figure](../Images/2-6.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-6.png)'
- en: Figure 2.6 A decoder visualized. Decoders are the second half of a full transformer,
    and they excel at NLG tasks like chatbots and storytelling. Decoders improve upon
    previous architectures in the same way as encoders, but they shift their output
    one space to the right for next-word generation to help utilize the advantages
    of multihead self-attention.
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 一个解码器的可视化。解码器是完整传输器的后半部分，在NLG任务（如聊天机器人和讲故事）中表现出色。解码器与编码器一样改进了先前架构，但它们将输出向右移动一个空格以帮助下一词生成，从而利用多头自注意力的优势。
- en: 'The strengths of decoders include the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的优势包括以下内容：
- en: Generates the next token in a sequence (shifted right means taking already-generated
    tokens into account)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在序列中生成下一个标记（向右移动表示考虑已生成的标记）
- en: Builds off of both known models and encoders
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立在已知模型和编码器之上
- en: Can be streamed during generation for great UX
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成过程中可以流式传输，提供良好的用户体验
- en: 'Their weaknesses include the following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的弱点包括以下内容：
- en: Syntax-only models can often struggle to insert the expected or intended meaning
    (see all “I forced an AI to watch 1000 hours of x and generated” memes from 2018–present).
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅语法模型往往难以插入预期的或意图的含义（参见所有“我强迫一个AI观看1000小时的x并生成”的2018-至今的meme）。
- en: Hallucinations.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉。
- en: 2.3.3 Transformers
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 传输器
- en: The full transformer architecture takes advantage of both encoders and decoders,
    passing the understanding of the encoder into the second multihead attention block
    of the decoder before giving output. As each piece of the transformer has a specialty
    in either understanding or generation, it should feel intuitive for the full product
    to be best at conditional generation tasks like translation or summarization,
    where some level of understanding is required before generation occurs. Encoders
    are geared toward processing input at a high level, and decoders focus more on
    generating coherent output. The full transformer architecture can successfully
    understand the data and then generate the output based on that understanding,
    as shown in figure 2.7\. The Text-To-Text Transfer Transformer (T5) family of
    models is an example of transformers.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的Transformer架构同时利用了编码器和解码器，在输出之前，将编码器的理解传递给解码器的第二个多头注意力块。由于Transformer的每一部分在理解或生成方面都有其专长，因此整个产品在需要生成之前有一定理解条件的条件生成任务，如翻译或摘要，上表现得最好，这一点应该是直观的。编码器侧重于在高级别处理输入，而解码器则更专注于生成连贯的输出。完整的Transformer架构能够成功理解数据，然后根据这种理解生成输出，如图2.7所示。文本到文本迁移Transformer（T5）模型系列是Transformer的一个例子。
- en: '![figure](../Images/2-7.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-7.png)'
- en: Figure 2.7 A full transformer visualized. A full transformer combines the encoder
    and the decoder and does well on all of the tasks of each, as well as conditional
    generation tasks such as summarization and translation. Because transformers are
    bulkier and slower than each of their halves, researchers and businesses have
    generally opted to use those halves over the whole transformer.
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 一个完整的Transformer可视化。完整的Transformer结合了编码器和解码器，在所有任务上都做得很好，包括摘要和翻译等条件生成任务。由于Transformer比其每个部分都要庞大和缓慢，研究人员和企业通常选择使用这些部分而不是整个Transformer。
- en: NOTE  Transformer models have an advantage in that they are built around the
    parallelization of inputs, which adds speed that LSTMs can’t currently replicate.
    If LSTMs ever get to a point where they can run as quickly as transformers, they
    may become competitive in the state-of-the-art field.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：Transformer模型的优势在于它们围绕输入的并行化构建，这增加了LSTM目前无法复制的速度。如果LSTM达到可以像Transformer一样快速运行的程度，它们可能在最先进的领域中具有竞争力。
- en: 'The strengths of a transformer are as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的优势如下：
- en: Includes both an encoder and decoder, so it’s good at everything they are good
    at
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含编码器和解码器，因此它在它们擅长的所有事情上都做得很好
- en: Highly parallelized for speed and efficiency
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度并行化以提高速度和效率
- en: 'Weaknesses include the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 弱点包括以下：
- en: Memory intensive, but still less than LSTMs of the same size
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆密集型，但仍然小于相同大小的LSTM
- en: Requires large amounts of data and VRAM for training
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练需要大量的数据和VRAM
- en: As you’ve probably noticed, most of the models we’ve discussed aren’t at all
    linguistically focused, being heavily syntax-focused, if they even attempt to
    model real language at all. Models, even state-of-the-art transformers, only have
    semantic approximations—no pragmatics, no phonetics—and only really utilize a
    mathematical model of morphology during tokenization without context. This doesn’t
    mean the models can’t learn these, nor does it mean that, for example, transformers
    can’t take audio as an input; it just means that the average usage doesn’t. With
    this in mind, it is nothing short of a miracle that they work as well as they
    do, and they really should be appreciated for what they can do.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经注意到的，我们讨论的大多数模型都不是语言导向的，它们高度关注语法，即使它们试图模拟真实语言。模型，即使是最先进的Transformer，也只有语义近似——没有语用学，没有语音学——并且在标记化过程中，只有在没有上下文的情况下才真正利用形态学的数学模型。这并不意味着模型不能学习这些，也不意味着例如，Transformer不能将音频作为输入；这仅仅意味着平均使用情况不是这样。考虑到这一点，它们能像现在这样工作，简直是个奇迹，它们真正应该得到赞赏。
- en: 'So far, we’ve attempted to highlight the current limitations in models, and
    we will dive into where to improve upon them in the remainder of this book. One
    such route is one that’s already been, and is still being, explored to great success:
    transfer learning and finetuning large foundational models. This technique came
    about soon after BERT’s initial release. Researchers discovered that although
    BERT generally performed well on a large number of tasks, if they wanted it to
    perform better on a particular task or data domain, they simply needed to retrain
    the model on data representative of the task or domain but not from scratch. Given
    all of the pretrained weights BERT learned while creating the semantic approximation
    embeddings on a much larger dataset, significantly less data is required to get
    state-of-the-art performance on the portion you need. We’ve seen this with BERT
    and the GPT family of models as they’ve come out, and now we’re seeing it again
    to solve exactly the challenges we discussed: semantic approximation coverage,
    domain expertise, and data availability.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们试图强调模型当前的局限性，并在本书的剩余部分深入探讨如何改进它们。其中一条途径已经取得并且仍在取得巨大成功的探索：迁移学习和微调大型基础模型。这种技术是在BERT最初发布后不久出现的。研究人员发现，尽管BERT在大量任务上表现良好，但如果他们希望它在特定任务或数据领域上表现更好，他们只需在代表该任务或领域的但不是从头开始的数据上重新训练模型即可。鉴于BERT在创建语义近似嵌入时在更大的数据集上学习到的所有预训练权重，所需的训练数据量显著减少，就可以在所需的部分上获得最先进的性能。我们已经在BERT和GPT系列模型中看到了这一点，现在我们再次看到这一点来解决我们讨论的
    exactly the challenges：语义近似覆盖、领域专业知识和数据可用性。
- en: 2.4 Really big transformers
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 非常大的转换器
- en: Enter LLMs. Since their introduction, transformer-based models have continued
    to get larger and larger, not just in their size and number of parameters but
    also in the size and length of their training datasets and training cycles. If
    you studied machine learning or deep learning during the 2010s, you likely heard
    the moniker, “Adding more layers doesn’t make the model better.” LLMs prove this
    both wrong and right—wrong because their performance is unparalleled, often matching
    smaller models that have been meticulously finetuned on a particular domain and
    dataset, even those trained on proprietary data, and right because of the challenges
    that come with both training and deploying LLMs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 进入大型语言模型（LLMs）。自从它们被引入以来，基于转换器的模型一直在不断增大，不仅是在规模和参数数量上，还包括它们的训练数据集和训练周期的大小和长度。如果你在2010年代学习机器学习或深度学习，你很可能听说过这样的说法：“增加更多层并不会使模型变得更好。”LLMs既证明了这一点是错误的，也证明了这一点是正确的——错误是因为它们的性能无与伦比，通常与经过精心微调的较小模型相匹配，这些模型在特定领域和数据集上进行了训练，甚至是在专有数据上训练的，正确是因为训练和部署LLMs所带来的挑战。
- en: One of the major differences between LLMs and language models involves transfer
    learning and finetuning. Like previous language models, LLMs are pretrained on
    massive text corpora, enabling them to learn general language features and representations
    that can be finetuned for specific tasks. Because LLMs are so massive and their
    training datasets are so large, they are able to achieve better performance with
    less labeled data, which was a significant limitation of earlier language models.
    Often, you can finetune an LLM to do highly specialized tasks with only a dozen
    or so examples.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs与语言模型之间的一大区别在于迁移学习和微调。与之前的语言模型一样，LLMs在庞大的文本语料库上进行预训练，使它们能够学习通用的语言特征和表示，这些特征和表示可以针对特定任务进行微调。由于LLMs规模庞大，其训练数据集也很大，因此它们能够在更少的标记数据的情况下实现更好的性能，这是早期语言模型的一个重大限制。通常，你只需用十几个示例就可以微调一个LLM来执行高度专业化的任务。
- en: However, what makes LLMs so powerful and has opened the door to widespread business
    use cases is their ability to do specialized tasks using simple prompting without
    any finetuning. Just give a few examples of what you want in your query, and the
    LLM can produce results. Training an LLM on a smaller set of labeled data is called
    few-shot prompting. It’s referred to as one-shot prompting when only one example
    is given and zero-shot when the task is totally novel. LLMs, especially those
    trained using reinforcement learning from human feedback and prompt engineering
    methodologies, can perform few-shot learning, where they can generalize and solve
    tasks with only a few examples, at a whole new level. This ability is a significant
    advancement over earlier models that required extensive finetuning or large amounts
    of labeled data for each specific task.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使大型语言模型（LLMs）如此强大并为其在商业领域的广泛应用打开了大门的是它们能够通过简单的提示执行专门任务，而无需任何微调的能力。只需在查询中给出你想要的一些示例，LLM就能生成结果。在较小的标记数据集上训练LLM被称为少样本提示。当只给出一个示例时，被称为单样本提示，而当任务是全新的时，则称为零样本提示。LLMs，尤其是那些通过人类反馈和提示工程方法进行强化学习训练的LLMs，能够进行少样本学习，这意味着它们能够通过仅几个示例进行泛化和解决任务，达到全新的水平。这种能力是相对于早期模型的一个重大进步，早期模型需要针对每个特定任务进行大量的微调或大量的标记数据。
- en: LMs previously have shown promise in the few and zero-shot learning domains,
    and LLMs have proven that promise to be true. As models have gotten larger, we
    find they are capable of accomplishing tasks smaller models can’t. We call this
    *emergent behavior*.[²](#footnote-251) Figure 2.8 illustrates eight different
    tasks previous language models couldn’t perform better than at random, and then
    once the models got large enough, they could.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，语言模型在少样本和零样本学习领域已经显示出潜力，而LLMs已经证明这种潜力是真实的。随着模型规模的增大，我们发现它们能够完成小模型无法完成的任务。我们称之为*涌现行为*。[²](#footnote-251)
    图2.8展示了八个先前语言模型在随机情况下表现不佳的任务，然后一旦模型足够大，它们就能完成这些任务。
- en: '![figure](../Images/2-8.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-8.png)'
- en: Figure 2.8 Examples of LLMs demonstrating emergent behaviors when given few-shot
    prompting tasks after the model scale reaches a certain size
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8 LLMs在模型规模达到一定大小后，执行少样本提示任务时展现出的涌现行为示例
- en: LLMs also have demonstrably great zero-shot capabilities due to their vast parameter
    sizes, which is the main reason for their popularity and viability in the business
    world. LLMs also exhibit improved handling of ambiguity due to their large size
    and capacity. They are better at disambiguating words with multiple meanings and
    understanding the nuances of language, resulting in more accurate predictions
    and responses. This improvement isn’t because of better ability or architecture,
    as they share their architecture with smaller transformers, but because they have
    vastly more examples of how people generally disambiguate. LLMs, therefore, respond
    with the same disambiguation as is generally represented in the dataset. Thanks
    to the diverseness of the text data on which LLMs are trained, they exhibit increased
    robustness in handling various input styles, noisy text, and grammatical errors.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs具有庞大的参数规模，它们在零样本能力方面也表现出显著的优势，这是它们在商业世界中受欢迎和可行的最主要原因。LLMs也由于规模大和容量大，在处理歧义方面表现出改进。它们在区分具有多个含义的单词和理解语言细微差别方面做得更好，从而产生更准确的预测和响应。这种改进并不是因为它们的能力或架构更好，因为它们与较小的Transformer共享相同的架构，而是因为它们拥有大量关于人们通常如何消除歧义的事例。因此，LLMs以与数据集中通常表示的相同方式消除歧义。多亏了LLMs训练所依据的文本数据的多样性，它们在处理各种输入风格、噪声文本和语法错误方面表现出更高的鲁棒性。
- en: Another key difference between LLMs and language models is input space. A larger
    input space is important since it makes few-shot prompting tasks that much more
    viable. Many LLMs have max input sizes of 8,000+ tokens (originally 32K, GPT-4
    has sported 128K since November 2023), and while all the previously discussed
    models could also have input spaces that high, they generally don’t. We have recently
    seen a boom in this field, with techniques like Recurrent Memory Transformer (RMT)
    allowing 1M+ token context spaces, which rocket LLMs even more toward proving
    that bigger models are always better. LLMs are designed to capture long-range
    dependencies within text, allowing them to understand context more effectively
    than their predecessors. This improved understanding enables LLMs to generate
    more coherent and contextually relevant responses in tasks like machine translation,
    summarization, and conversational AI.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）与语言模型之间的另一个关键区别是输入空间。更大的输入空间非常重要，因为它使得少样本提示任务变得更加可行。许多LLMs的最大输入大小为8,000+个token（最初为32K，GPT-4自2023年11月以来已达到128K），尽管所有之前讨论的模型也可以有如此高的输入空间，但它们通常并不具备。我们最近在这个领域看到了爆炸式增长，例如循环记忆转换器（RMT）技术允许1M+个token的上下文空间，这进一步推动了LLMs向证明更大的模型总是更好的方向迈进。LLMs被设计用来捕捉文本中的长距离依赖关系，这使得它们比前辈们更有效地理解上下文。这种改进的理解能力使得LLMs在机器翻译、摘要和对话式AI等任务中能够生成更连贯和上下文相关的响应。
- en: LLMs have revolutionized NLP by offering powerful solutions to problems that
    were challenging for earlier language models. They bring substantial improvements
    in contextual understanding, transfer learning, and few-shot learning. As the
    field of NLP continues to evolve, researchers are actively working to maximize
    the benefits of LLMs while mitigating all potential risks. Because a better way
    to approximate semantics hasn’t been found, they make bigger and more dimensional
    approximations. Because a good way of storing pragmatic context hasn’t been found,
    LLMs often allow inserting context into the prompt directly, into a part of the
    input set aside for context, or even through sharing databases with the LLM at
    inference. This capability doesn’t create pragmatics or a pragmatic system within
    the models, in the same way that embeddings don’t create semantics, but it allows
    the model to correctly generate syntax that mimics how humans respond to those
    pragmatic and semantic stimuli. Phonetics is a place where LLMs could likely make
    gigantic strides, either as completely text-free models or as a text-phonetic
    hybrid model, maybe utilizing the IPA in addition to or instead of text. It is
    exciting to consider the possible developments that we are watching sweep across
    this field right now.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs通过提供早期语言模型难以解决的问题的强大解决方案，已经彻底改变了NLP（自然语言处理）。它们在上下文理解、迁移学习和少样本学习方面带来了显著的改进。随着NLP领域的持续发展，研究人员正在积极努力最大化LLMs的益处，同时减轻所有潜在的风险。因为还没有找到更好的近似语义的方法，所以他们做出了更大和更多维度的近似。因为还没有找到存储语用上下文的好方法，LLMs通常允许直接将上下文插入到提示中，或者插入到为上下文保留的输入部分，甚至通过在推理时与LLM共享数据库。这种能力不会在模型内创建语用或语用系统，就像嵌入不会创建语义一样，但它允许模型正确生成模仿人类对那些语用和语义刺激做出反应的语法。语音学是LLMs可能取得巨大进步的地方，无论是作为完全无文本的模型，还是作为文本-语音混合模型，也许会利用国际音标（IPA）作为文本的补充或替代。现在我们正在观察这个领域的可能发展，这令人兴奋。
- en: 'At this point, you should have a pretty good understanding of what LLMs are
    and some key principles of linguistics that will come in handy when putting LLMs
    in production. You should now be able to start reasoning about what type of products
    will be easier or harder to build. Consider figure 2.9: tasks in the lower left-hand
    corner, like writing assistants and chatbots, are LLMs’ bread and butter. Text
    generation based on a little context from a prompt is a strictly syntax-based
    problem; with a large enough model trained on enough data, we can do this pretty
    easily. A shopping assistant is pretty similar and rather easy to build as well;
    we are just missing pragmatics. The assistant needs to know a bit more about the
    world, such as products, stores, and prices. With a little engineering, we can
    add this information to a database and give this context to the model through
    prompting.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经对LLMs（大型语言模型）有了相当好的理解，以及一些在将LLMs投入生产时将非常有用的语言学关键原则。现在，你应该能够开始推理哪些产品更容易或更难构建。考虑图2.9：左下角的任务，如写作助手和聊天机器人，是LLMs的拿手好戏。基于提示中少量上下文的文本生成是一个严格基于语法的难题；只要模型足够大，训练数据足够多，我们就可以相对容易地做到这一点。购物助手与此类似，并且也相对容易构建；我们只是缺少了语用学。助手需要了解更多关于世界的信息，比如产品、商店和价格。通过一点工程，我们可以将这些信息添加到数据库中，并通过提示将这些上下文提供给模型。
- en: '![figure](../Images/2-9.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-9.png)'
- en: Figure 2.9 How difficult or easy certain tasks are for LLMs and what approaches
    to take to solve them
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9：某些任务对LLMs的难易程度以及解决这些任务的方法
- en: On the other end, consider a chess bot. LLMs *can* play chess, but they aren’t
    any good. They have been trained on chess games and understand that E4 is a common
    first move, but their understanding is completely syntactical. LLMs only understand
    that the text they generate should contain a letter between A and H and a number
    between 1 and 8\. Like the shopping assistant, they are missing pragmatics and
    don’t have a clear model of the game of chess. In addition, they are also missing
    semantics. Encoders might help us understand that the words “king” and “queen”
    are similar, but they don’t help us understand that E4 is a great move one moment
    for one player and that same E4 move is a terrible move the very next moment for
    a different player. LLMs also lack knowledge based on phonetics and morphology
    for chess, although they are not as important in this case. Either way, we hope
    this exercise will better inform you and your team on your next project.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一端，考虑一下棋类机器人。LLMs*可以*下棋，但它们并不擅长。它们在棋局上接受过训练，并理解E4是一个常见的第一步，但它们的理解完全是基于语法的。LLMs只理解它们生成的文本应该包含A到H之间的字母和1到8之间的数字。就像购物助手一样，它们缺少语用学，并且没有清晰的棋局模型。此外，它们还缺少语义学。编码器可能有助于我们理解“国王”和“王后”这两个词是相似的，但它们不能帮助我们理解E4对于某个玩家来说是一步好棋，而对于另一个玩家来说却是非常糟糕的一步。LLMs在棋类上还缺乏基于语音学和形态学的知识，尽管在这个案例中它们并不那么重要。无论如何，我们希望这个练习能更好地让你和你的团队了解你们下一个项目。
- en: LLMs have amazing benefits, but with all of these capabilities come some limitations.
    Foundational LLMs require vast computational resources for training, making them
    less accessible for individual researchers and smaller organizations. This problem
    is being remedied with techniques we’ll talk about throughout the book, like quantization,
    textual embeddings, low-rank adaptation, parameter-efficient finetuning, and graph
    optimization. Still, foundation models are currently solidly outside the average
    individual’s ability to train effectively. Beyond that, there are concerns that
    the energy consumption associated with training LLMs could have significant environmental
    effects and cause problems associated with sustainability. These problems are
    complex and largely out of the scope of this book, but we would be remiss not
    to bring them up.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs具有惊人的好处，但所有这些能力也带来了一些限制。基础LLMs需要大量的计算资源进行训练，这使得它们对个人研究人员和小型组织来说不太容易获得。这个问题正在通过本书中将要讨论的技术得到解决，比如量化、文本嵌入、低秩自适应、参数高效微调和图优化。然而，基础模型目前仍然超出了普通个人有效训练的能力范围。除此之外，还有关于与训练LLMs相关的能源消耗可能对环境产生重大影响并引起可持续性问题担忧。这些问题很复杂，很大程度上超出了本书的范围，但我们不能不提及它们。
- en: Last but not least, since LLMs are trained on large-scale datasets containing
    real-world text, they may learn and perpetuate biases present in the data, leading
    to ethical concerns because real-world people don’t censor themselves to provide
    optimal unbiased data. Also, knowing much about what data you’re training on is
    not a widespread practice. For example, if you ask a text-to-image diffusion LLM
    to generate 1,000 images of “leader,” 99% of the images feature men, and 95% of
    the images feature people with white skin. The concern here isn’t that men or
    white people shouldn’t be depicted as leaders, but that the model isn’t representing
    the world accurately, and it’s showing.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，由于LLM是在包含真实世界文本的大规模数据集上训练的，它们可能会学习和延续数据中存在的偏见，这引发了道德上的担忧，因为现实世界的人不会自我审查以提供最佳的无偏见数据。此外，了解你正在训练的数据的很多信息并不是一种普遍的做法。例如，如果你要求一个文本到图像的扩散LLM生成1000张“领导者”的图像，99%的图像都是男性，95%的图像都是白人。这里的担忧并不是说男性和白人不应该被描绘为领导者，而是模型没有准确地代表世界，并且正在展示这一点。
- en: Sometimes, more nuanced biases are brought out. For example, in the Midjourney
    example in figure 2.10, the model, without being prompted (the only prompt given
    was the word “leader”), changed the popular feminist icon Rosie the Riveter to
    a man. The model didn’t think about this change; it just determined during its
    sampling steps that the prompt “leader” had more male-looking depictions in the
    training set. Many people will argue about what “good” and “bad” mean in this
    context, and instead of going for a moral ought, we’ll talk about what accuracy
    means. LLMs are trained on a plethora of data with the purpose of returning the
    most accurate representations possible. When they cannot return accurate representations,
    especially with their heightened abilities to disambiguate, we can view that as
    a bias that harms the model’s ability to fulfill its purpose. Later, we will discuss
    techniques to combat harmful bias to allow you, as an LLM creator, to get the
    exact outputs you intend and minimize the number of outputs you do not intend.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，更细微的偏见会被揭示出来。例如，在图2.10中的Midjourney示例中，模型在没有被提示的情况下（唯一的提示是单词“领导者”），将流行的女权主义图标罗斯·拉弗特变成了男性。模型并没有考虑这种变化；它只是在采样步骤中确定，提示“领导者”在训练集中有更多看起来像男性的描绘。许多人会争论在这个背景下“好”和“坏”的含义，而不会追求道德上的义务，我们将讨论准确性的含义。LLM是在大量数据上训练的，目的是返回尽可能准确的表现。当它们无法返回准确的表现时，特别是当它们具有高度的能力来消除歧义时，我们可以将其视为损害模型实现其目的的偏见。稍后，我们将讨论对抗有害偏见的技术，以便您作为LLM的创建者，能够得到您期望的确切输出，并最大限度地减少不期望的输出数量。
- en: '![figure](../Images/2-10.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-10.png)'
- en: Figure 2.10 Midjourney 5, which is, at the time of this writing, the most popular
    text2img model on the market, when prompted with only one token, “leader” (left),
    changed a well-known popular feminist icon, Rosie the Riveter, into a male depiction.
    ChatGPT (right) writes a function to place you in your job based on race, gender,
    and age. These are examples of unintended outputs.
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10中的Midjourney 5，在撰写本文时，是市场上最受欢迎的文本到图像模型。当只提示一个标记“领导者”（左）时，将一个著名的流行女权主义图标，罗斯·拉弗特，变成了男性的描绘。ChatGPT（右）编写了一个函数，根据种族、性别和年龄为你安排工作。这些都是意外输出的例子。
- en: Alright, we’ve been building up to this moment the entire chapter. Let’s go
    ahead and run our first LLM! In listing 2.9, we download the Bloom model, one
    of the first open source LLMs to be created, and generate text! We are using Hugging
    Face’s Transformers library, which takes care of all the heavy lifting for us.
    Very exciting stuff!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们整个章节都在为这一刻做准备。现在我们就来运行我们的第一个大型语言模型（LLM）吧！在列表2.9中，我们下载了Bloom模型，这是最早创建的开源LLM之一，并生成文本！我们使用的是Hugging
    Face的Transformers库，它为我们处理了所有繁重的工作。这真是太激动人心了！
- en: Listing 2.9 Running our first LLM
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.9 运行我们的第一个LLM
- en: '[PRE10]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Did you try to run it?!? If you did, you probably just crashed your laptop.
    Oopsie! Forgive me for a little harmless MLOps hazing, but getting some first-hand
    experience on how large these models can get and how difficult they can be to
    run is a helpful experience to have. In the next chapter, we will talk more about
    the difficulties of running LLMs and some of the tools you need to run this code.
    If you don’t want to wait and would like to get a similar but much smaller LLM
    running, change the model name to `"bigscience/bloom-3b"`, and run it again. It
    should work just fine this time on most hardware.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你尝试运行它了吗？！？如果你尝试了，你可能刚刚让你的笔记本电脑崩溃了。哎呀！请原谅我这个小无害的MLOps折磨，但了解这些模型有多大以及它们有多难运行是一段有用的经验。在下一章中，我们将更多地讨论运行LLMs的困难以及你需要运行此代码的一些工具。如果你不想等待，并希望运行一个类似但规模小得多的LLM，将模型名称更改为`"bigscience/bloom-3b"`，然后再次运行。这次在大多数硬件上应该可以正常运行。
- en: All in all, LLMs are an amazing technology that allows our imaginations to run
    wild with possibility, and deservedly so. The number-one use case for considering
    an LLM over a smaller language model is when few-shot capabilities come into play
    for whoever the model will be helping, such as a CEO when raising funds or a software
    engineer when writing code. LLMs have these abilities precisely because of their
    size. The larger number of parameters in LLMs directly enables their ability to
    generalize over smaller spaces in larger dimensions. In this chapter, we’ve hit
    the lesser-known side of LLMs, the linguistic and language modeling side. In the
    next chapter, we’ll cover the other half, the MLOps side, where we dive into exactly
    how that large parameter size affects the model and the systems designed to support
    that model and makes it accessible to the customers or employees the model is
    intended for.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，LLMs是一项惊人的技术，它让我们的想象力在可能性中自由驰骋，这是理所当然的。考虑LLM而不是较小的语言模型的首要用例是当模型将帮助的人需要少样本能力时，例如，当CEO在筹集资金或软件工程师在编写代码时。LLMs具有这些能力正是因为它们的规模。LLMs中参数数量的增加直接使它们能够在更大维度上的较小空间中进行泛化。在本章中，我们触及了LLMs不太为人所知的方面，即语言学和语言建模方面。在下一章中，我们将涵盖另一半，即MLOps方面，我们将深入了解大量参数大小如何影响模型以及设计来支持该模型和使其对目标客户或员工可访问的系统。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The five components of linguistics are phonetics, syntax, semantics, pragmatics,
    and morphology:'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言学五大组成部分是语音学、句法、语义、语用学和形态学：
- en: Phonetics can be added through a multimodal model that processes audio files
    and is likely to improve LLMs in the future, but current datasets are too small.
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音学可以通过处理音频文件的多模态模型来添加，这可能会在未来提高大型语言模型（LLMs），但当前的语料库数据集太小。
- en: Syntax is what current models are good at.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句法是当前模型擅长的。
- en: Semantics is added through the embedding layer.
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义是通过嵌入层添加的。
- en: Pragmatics can be added through engineering efforts.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语用学可以通过工程努力来添加。
- en: Morphology is added in the tokenization layer.
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形态学是在标记化层中添加的。
- en: Language does not necessarily correlate with reality. Understanding the process
    people use to create meaning outside of reality is useful in training meaningful
    (to people) models.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言不一定与现实相关。了解人们如何在外部现实之外创造意义的过程对于训练对人们有意义的模型是有用的。
- en: Proper tokenization can be a major hurdle due to too many `<UNK>` tokens, especially
    when it comes to specialized problems like code or math.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确的标记化可能是一个主要障碍，因为存在过多的 `<UNK>` 标记，尤其是在处理像代码或数学这样的专业问题时。
- en: Multilingual processing has always outperformed monolingual processing, even
    on monolingual tasks without models.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语言处理始终优于单语言处理，甚至在没有模型的单语言任务中也如此。
- en: Each language model type in sequence shows a natural and organic growth of the
    LLM field as more and more linguistic concepts are added that make the models
    better.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按顺序展示的语言模型类型显示了LLM领域的自然和有机增长，因为越来越多的语言学概念被添加，使模型变得更好。
- en: Language modeling has seen an exponential increase in efficacy, correlating
    to how linguistics-focused the modeling has been.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模在效率上经历了指数级增长，这与建模的语言学重点相关。
- en: 'Attention is a mathematical shortcut for solving larger context windows faster
    and is the backbone of modern architectures—encoders, decoders, and transformers:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力是解决更大上下文窗口的数学捷径，是现代架构（编码器、解码器和转换器）的支柱：
- en: Encoders improve the semantic approximations in embeddings.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器提高了嵌入中的语义近似。
- en: Decoders are best at text generation.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器在文本生成方面表现最佳。
- en: Transformers combine the two.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器结合了这两者。
- en: Larger models demonstrate emergent behavior, suddenly being able to accomplish
    tasks they couldn’t before.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的模型表现出出现行为，突然能够完成之前无法完成的任务。
- en: '[[1]](#footnote-source-1) Vaswani et al., 2017, Attention Is All You Need,”
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#footnote-source-1) Vaswani等人，2017年，《Attention Is All You Need》，[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)。'
- en: '[[2]](#footnote-source-2) J. Wei et al., “Emergent abilities of large language
    models,” Transactions on Machine Learning Research, Aug. 2022, [https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#footnote-source-2) J. Wei等人，“大型语言模型的出现能力”，机器学习研究交易，2022年8月，[https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD)。'
