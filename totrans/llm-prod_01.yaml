- en: '2 Large language models: A deep dive into language modeling'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 大型语言模型：深入语言建模
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The linguistic background for understanding meaning and interpretation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解意义和解释的语言背景
- en: A comparative study of language modeling techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模技术的比较研究
- en: Attention and the transformer architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力和transformer架构
- en: How large language models both fit into and build upon these histories
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型如何融入并建立在这些历史之上
- en: If you know the enemy and know yourself, you need not fear the result of a hundred
    battles.—Sun Tzu
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你了解敌人，也了解自己，你就无需害怕百战百胜的结果。——孙子
- en: This chapter delves into linguistics as it relates to the development of LLMs,
    exploring the foundations of semiotics, linguistic features, and the progression
    of language modeling techniques that have shaped the field of natural language
    processing (NLP). We will begin by studying the basics of linguistics and its
    relevance to LLMs, highlighting key concepts such as syntax, semantics, and pragmatics
    that form the basis of natural language and play a crucial role in the functioning
    of LLMs. We will delve into semiotics, the study of signs and symbols, and explore
    how its principles have informed the design and interpretation of LLMs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨与LLMs发展相关的语言学，探索符号学的基础、语言特征以及塑造自然语言处理（NLP）领域的语言建模技术的进展。我们将从研究语言学的基础及其与LLMs的相关性开始，强调语法、语义和语用学等关键概念，这些概念构成了自然语言的基础，并在LLMs的功能中发挥着至关重要的作用。我们将深入研究符号学，即对符号和符号的研究，并探讨其原则如何指导LLMs的设计和解释。
- en: We will then trace the evolution of language modeling techniques, providing
    an overview of early approaches, including N-grams, naive Bayes classifiers, and
    neural network-based methods such as multilayer perceptrons (MLPs), recurrent
    neural networks (RNNs), and long short-term memory (LSTM) networks. We will also
    discuss the groundbreaking shift to transformer-based models that laid the foundation
    for the emergence of LLMs, which are really just big transformer-based models.
    Finally, we will introduce LLMs and their distinguishing features, discussing
    how they have built upon and surpassed earlier language modeling techniques to
    revolutionize the field of NLP.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将追溯语言建模技术的演变，概述早期方法，包括N-gram、朴素贝叶斯分类器和基于神经网络的多种方法，如多层感知器（MLPs）、循环神经网络（RNNs）和长短期记忆（LSTM）网络。我们还将讨论基于transformer架构的突破性转变，这为大型语言模型（LLMs）的出现奠定了基础，而LLMs实际上只是大型基于transformer的模型。最后，我们将介绍LLMs及其独特特征，讨论它们如何建立在并超越早期的语言建模技术，从而彻底改变自然语言处理（NLP）领域。
- en: This book is about LLMs in production. We firmly believe that if you want to
    turn an LLM into an actual product, understanding the technology better will improve
    your results and save you from making costly and time-consuming mistakes. Any
    engineer can figure out how to lug a big model into production and throw a ton
    of resources at it to make it run, but that brute-force strategy completely misses
    the lessons people have already learned trying to do the same thing before, which
    is what we are trying to solve with LLMs in the first place. Having a grasp of
    these fundamentals will better prepare you for the tricky parts, the gotchas,
    and the edge cases you are going to run into when working with LLMs. By understanding
    the context in which LLMs emerged, we can appreciate their transformative impact
    on NLP and how to enable them to create a myriad of applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书关于生产中的LLMs。我们坚信，如果你想将LLM转化为实际产品，更好地理解技术将提高你的成果并帮助你避免代价高昂且耗时耗力的错误。任何工程师都可以弄清楚如何将大型模型拖入生产并投入大量资源使其运行，但这种蛮力策略完全忽略了人们之前尝试做同样事情时已经学到的教训，这正是我们最初尝试使用LLMs的原因。掌握这些基础知识将更好地为你准备那些棘手的部分、陷阱和边缘情况，你将在与LLMs合作时遇到这些情况。通过理解LLMs出现的背景，我们可以欣赏它们对NLP的变革性影响以及如何使它们能够创造无数的应用。
- en: 2.1 Language modeling
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 语言建模
- en: It would be a great disservice to address LLMs in any depth without first addressing
    language. To that end, we will start with a brief but comprehensive overview of
    language modeling, focusing on the lessons that can help us with modern LLMs.
    Let’s first discuss levels of abstraction, as this will help us garner an appreciation
    for language modeling.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不首先解决语言问题，就深入探讨LLMs，那将是一种极大的失职。为此，我们将从对语言建模的简要但全面的概述开始，重点关注有助于我们理解现代LLMs的教训。让我们首先讨论抽象层次，因为这将帮助我们理解语言建模。
- en: Language, as a concept, is an abstraction of the feelings and thoughts that
    occur to us in our heads. Feelings come first in the process of generating language,
    but that’s not the only thing we’re trying to highlight here. We’re also looking
    at language as being unable to capture the full extent of what we are able to
    feel, which is why we’re calling it an abstraction. It moves away from the source
    material and loses information. Math is an abstraction of language, focusing on
    logic and provability, but as any mathematician will tell you, it is a subset
    of language used to describe and define in an organized and logical way. From
    math comes another abstraction, the language of binary, a base-2 system of numerical
    notation consisting of either on or off.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语言作为一个概念，是我们头脑中产生的情感和思想的抽象。在生成语言的过程中，情感首先出现，但这并不是我们在这里想要强调的唯一事物。我们还在观察语言作为无法捕捉我们能够感受到的全部范围的能力，这就是为什么我们称之为抽象。它远离了原始材料并丢失了信息。数学是语言的抽象，专注于逻辑和可证明性，但正如任何数学家都会告诉你的，它是一种用于以有组织和逻辑的方式描述和定义的语言的子集。从数学中又产生了另一个抽象，二进制语言，这是一个由开或关组成的二进制数值表示系统。
- en: This is not a commentary on usefulness, as binary and math are just as useful
    as the lower-level aspects of language, nor is it commenting on order, as we said
    before. With math and binary, the order coincidentally lines up with the layer
    of abstraction. Computers can’t do anything on their own and need to take commands
    to be useful. Binary, unfortunately, ends up taking too long for humans to communicate
    important things in it, so binary was also abstracted to assembly, a more human-comprehensible
    language for communicating with computers. This was further abstracted to the
    high-level assembly language C, which has been even further abstracted to object-oriented
    languages like Python or Java (which one doesn’t matter—we’re just measuring distance
    from binary). The flow we just discussed is outlined in figure 2.1.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是对有用性的评论，因为二进制和数学与语言的低级方面一样有用，也不是在评论顺序，因为我们之前已经说过。在数学和二进制中，顺序偶然地与抽象层相吻合。计算机不能独立做任何事情，需要接收命令才能变得有用。不幸的是，二进制对于人类来说太耗时了，以至于无法在其中传达重要的事情，因此二进制也被抽象为汇编，这是一种更易于人类理解的语言，用于与计算机通信。这进一步抽象为高级汇编语言C，它又被进一步抽象为面向对象的语言，如Python或Java（哪一个不重要——我们只是在衡量与二进制的距离）。我们刚才讨论的流程在图2.1中有所概述。
- en: '![figure](../Images/2-1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-1.png)'
- en: Figure 2.1 We compare cognitive layers of abstraction to programming layers
    of abstraction down to the logical binary abstraction. Python doesn’t come from
    C, nor does it compile into C. Python is, however, another layer of abstraction
    distant from binary. Language follows a similar path. Each layer of abstraction
    creates a potential point of failure. There are also several layers of abstraction
    to creating a model, and each is important in seeing the full path from our feelings
    to a working model.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 我们将认知抽象层与编程抽象层进行比较，直到逻辑二进制抽象。Python不是来自C，也不是编译成C。然而，Python是另一个远离二进制的抽象层。语言遵循相似的路径。每一层抽象都创造了一个潜在的错误点。在创建模型的过程中也有几个抽象层，每一层在看到从我们的感受到一个工作模型的全路径时都很重要。
- en: This is obviously a reduction; however, it’s useful to understand that the feelings
    you have in your head are the same number of abstractions away from binary, the
    language the computer actually reads, as the languages most people use to program
    in. Some people might argue that there are more steps between Python and binary,
    such as compilers or using assembly to support the C language, and that’s true,
    but there are more steps on the language side too, such as morphology, syntax,
    logic, dialogue, and agreement.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是一种简化；然而，了解你头脑中的感受与计算机实际读取的语言——即大多数人用来编程的语言——在抽象层面上距离二进制相同，是有用的。有些人可能会争论Python和二进制之间有更多的步骤，比如编译器或使用汇编来支持C语言，这是真的，但在语言方面也有更多的步骤，比如形态学、句法、逻辑、对话和一致性。
- en: This reduction can help us understand how difficult the process of getting what
    we want to be understood by an LLM actually is and even help us understand language
    modeling techniques better. We focus on binary here to illustrate that there are
    a similar number of abstract layers to get from an idea you have or from one of
    our code samples to a working model. Like the children’s telephone game where
    participants whisper into each other’s ears, each abstraction layer creates a
    disconnect point or barrier where mistakes can be made.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简化可以帮助我们理解我们想要被LLM理解的过程实际上有多么困难，甚至有助于我们更好地理解语言建模技术。在这里我们关注二进制，以说明从你拥有的想法或我们的代码示例到工作模型之间有相似数量的抽象层。就像孩子们玩电话游戏，参与者互相耳语，每个抽象层都创造了一个断开点或障碍，错误可能在这里发生。
- en: Figure 2.1 is also meant not only to illustrate the difficulty in creating reliable
    code and language input but also to draw attention to how important the intermediary
    abstraction steps, like tokenization and embeddings, are for the model itself.
    Even if you have perfectly reliable code and perfectly expressed ideas, the meaning
    may be fumbled by one of those processes before it ever reaches the LLM.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1不仅旨在说明创建可靠代码和语言输入的难度，而且还强调中间抽象步骤，如分词和嵌入，对模型本身的重要性。即使你拥有完全可靠的代码和完美表达的思想，这些过程之一也可能在它到达LLM之前就弄错了意义。
- en: In this chapter, we will try to help you understand what you can do to reduce
    the risks of these failure points, whether on the language, coding, or modeling
    side. Unfortunately, it’s a bit tricky to strike a balance between giving you
    too much linguistics that doesn’t immediately matter for the task at hand versus
    giving you too much technical knowledge that, while useful, doesn’t help you develop
    an intuition for language modeling as a practice. With this in mind, you should
    know that linguistics can be traced back thousands of years in our history, and
    there’s lots to learn from it. We’ve included a brief overview of how language
    modeling has progressed over time in appendix A, and we encourage you to take
    a look.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将尝试帮助你了解你可以在语言、编码或建模方面做些什么来降低这些失败点的风险。不幸的是，在给你太多不立即与当前任务相关的语言学知识，以及给你太多虽然有用但不会帮助你培养对语言建模实践直觉的技术知识之间取得平衡是有点棘手的。考虑到这一点，你应该知道语言学可以追溯到我们历史数千年前，并且有很多东西可以从中学习。我们在附录A中简要概述了语言建模随时间的发展，并鼓励你查看。
- en: Let’s start with our focus on the building blocks that constitute language itself.
    We expect our readers to have at least attempted language modeling before and
    to have heard of libraries like PyTorch and TensorFlow, but we do not expect most
    of our readers to have considered the language side of things before. By understanding
    the essential features that make up language, we can better appreciate the complexities
    involved in creating effective language models and how these features interact
    with one another to form the intricate web of communication that connects us all.
    In the following section, we will examine the various components of language,
    such as phonetics, pragmatics, morphology, syntax, and semantics, as well as the
    role they play in shaping our understanding and usage of languages around the
    world. Let’s take a moment to explore how we currently understand language, along
    with the challenges we face that LLMs are meant to solve.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从构成语言本身的基本构建块开始。我们希望我们的读者至少尝试过语言建模，并且听说过像PyTorch和TensorFlow这样的库，但我们并不期望大多数读者在之前考虑过语言方面的问题。通过理解构成语言的本质特征，我们可以更好地欣赏创建有效语言模型所涉及的复杂性，以及这些特征如何相互作用，形成连接我们所有人的复杂沟通网络。在下一节中，我们将检查语言的各个组成部分，例如语音学、语用学、形态学、句法和语义学，以及它们在塑造我们对世界各地语言的理解和使用中所起的作用。让我们花点时间来探讨我们目前对语言的理解，以及我们面临的挑战，这些挑战正是LLMs旨在解决的。
- en: 2.1.1 Linguistic features
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 语言特征
- en: 'Our current understanding of language is that language is made up of at least
    five parts: phonetics, syntax, semantics, pragmatics, and morphology. Each of
    these portions contributes significantly to the overall experience and meaning
    being ingested by the listener in any conversation. Not all of our communication
    uses all of these forms; for example, the book you’re currently reading is devoid
    of phonetics, which is one of the reasons why so many people think text messages
    are unsuited for more serious or complex conversations. Let’s work through each
    of these five parts to figure out how to present them to a language model for
    a full range of communicative power.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对语言的理解是，语言至少由五个部分组成：语音学、句法、语义、语用和形态学。这些部分中的每一个都对任何对话中听众所吸收的整体体验和意义有显著的贡献。并不是所有的交流都使用所有这些形式；例如，你现在正在阅读的这本书没有语音学，这也是许多人认为短信不适合更严肃或复杂对话的原因之一。让我们逐一探讨这五个部分，以了解如何将它们呈现给语言模型，以实现全面的沟通能力。
- en: Phonetics
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语音学
- en: Phonetics is probably the easiest for a language model to ingest, as it involves
    the actual sound of the language. This is where accent manifests and deals with
    the production and perception of speech sounds, with phonology focusing on the
    way sounds are organized within a particular language system. Similarly to computer
    vision, while a sound isn’t necessarily easy to deal with as a whole, there’s
    no ambiguity in how to parse, vectorize, or tokenize the actual sound waves. They
    have a numerical value attached to each part, the crest, the trough, and the slope
    during each frequency cycle. It is vastly easier than text to tokenize and process
    by a computer while being no less complex.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言模型来说，语音学可能是最容易吸收的，因为它涉及到语言的实际声音。这就是口音显现和处理语音声音生产与感知的地方，语音学专注于特定语言系统中声音的组织方式。与计算机视觉类似，虽然处理整个声音可能并不容易，但在解析、矢量化或标记实际声音波方面没有歧义。每个部分（如每个频率周期的峰值、谷值和斜率）都附有数值。与文本相比，语音学在计算机标记和处理方面要容易得多，尽管它并不简单。
- en: Sound inherently also contains more encoded meaning than text. For example,
    imagine someone saying the words “Yeah, right,” to you. It could be sarcastic,
    or it could be congratulatory, depending on the tone—and English isn’t even tonal!
    Phonetics, unfortunately, doesn’t have terabyte-sized datasets commonly associated
    with it, and performing data acquisition and cleaning on phonetic data, especially
    on the scale needed to train an LLM, is difficult at best. In an alternate world
    where audio data was more prevalent than text data and took up a smaller memory
    footprint, phonetic-based or phonetic-aware LLMs would be much more sophisticated,
    and creating that world is a solid goal to work toward.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 声音本身也包含比文本更多的编码意义。例如，想象有人对你说“是啊，没错”。这可能是一种讽刺，也可能是一种祝贺，这取决于语气——而英语甚至不是音调语言！不幸的是，语音学没有与它通常相关的大型数据集，对语音数据进行数据采集和清洗，尤其是在训练大型语言模型所需的规模上，是极其困难的。在一个音频数据比文本数据更普遍且占用更小内存的世界里，基于语音或对语音敏感的大型语言模型将会更加复杂，创造这样一个世界是一个值得努力的目标。
- en: 'Anticipating this phonetical problem, a system was created in 1888 called the
    International Phonetic Alphabet (IPA). It has been revised in both the 20th and
    21st centuries to be more concise, more consistent, and clearer and could be a
    way to insert phonetic awareness into text data. IPA functions as an internationally
    standardized version of every language’s sound profile. A sound profile is the
    set of sounds that a language uses; for example, in English, we never have the
    /ʃ/ (she, shirt, sh) next to the /v/ sound. IPA is used to write sounds, rather
    than writing an alphabet or logograms, as most languages do. For example, you
    could describe how to pronounce the word “cat” using these symbols: /k/, /æ/,
    and /t/. Of course, that’s a very simplified version of it, but for models, it
    doesn’t have to be. You can describe tone and aspiration as well. This could be
    a happy medium between text and speech, capturing some phonetic information. Think
    of the phrase “What’s up?” Your pronunciation and tone can drastically change
    how you understand that phrase, sometimes sounding like a friendly “Wazuuuuup?”
    and other times an almost threatening “‘Sup?” which IPA would fully capture. IPA
    isn’t a perfect solution, though; for example, it doesn’t solve the problem of
    replicating tone very well.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 预见到这个语音问题，1888年创建了一个名为国际音标（IPA）的系统。它在20世纪和21世纪都进行了修订，以使其更加简洁、一致、清晰，并且可以作为一种将语音意识插入文本数据的方法。IPA作为每个语言声音轮廓的国际标准化版本。声音轮廓是语言使用的声音集合；例如，在英语中，我们永远不会把/ʃ/（she,
    shirt, sh）和/v/声音放在一起。IPA用于书写声音，而不是像大多数语言那样书写字母或象形文字。例如，你可以使用这些符号来描述如何发音单词“cat”：/k/，/æ/，和/t/。当然，这是一个非常简化的版本，但对于模型来说，并不需要这样。你还可以描述音调和送气。这可能是文本和语音之间的一个折中方案，捕捉一些语音信息。想想短语“What’s
    up？”你的发音和音调可以极大地改变你对这个短语的理解，有时听起来像友好的“Wazuuuuup？”有时则像几乎威胁的“‘Sup？”而IPA可以完全捕捉到这一点。尽管如此，IPA并不是一个完美的解决方案；例如，它并不能很好地解决复制音调的问题。
- en: Phonetics is listed first here because it’s the place where LLMs have been applied
    to the least out of all the features and, therefore, has the largest space for
    improvement. Even modern text-to-speech (TTS) and voice-cloning models, for the
    most part, end up converting the sound to a spectrogram and analyzing that image
    rather than incorporating any type of phonetic language modeling. Improving phonetic
    data and representation in LLMs is something to look for as far as research goes
    in the coming months and years.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 声音学在这里被列为首位，因为相较于所有其他特征，声音学在大型语言模型（LLM）中的应用最少，因此具有最大的改进空间。即使是现代的文本到语音（TTS）和声音克隆模型，大部分情况下，最终都是将声音转换成频谱图并分析该图像，而不是结合任何类型的语音语言模型。在未来的几个月和几年里，改善LLM中的语音数据和表示将是研究的一个方向。
- en: Syntax
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语法
- en: Syntax is the place where current LLMs are highest-performing, both in parsing
    syntax from the user and in generating their own. Syntax is generally what we
    think of as grammar and word order; it is the study of how words can combine to
    form phrases, clauses, and sentences. Syntax is also the first place language-learning
    programs start to help people acquire new languages, especially based on where
    they are coming from natively. For example, it is important for a native English
    speaker learning Turkish to know that the syntax is completely different, and
    you can often build entire sentences in Turkish that are just one long compound
    word, whereas in English, we never put our subject and verb together into one
    word.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 语法是当前LLM表现最出色的领域，无论是从用户那里解析语法还是生成自己的语法。语法通常是我们所认为的语法和词序；它是研究单词如何组合成短语、从句和句子的学科。语法也是语言学习程序开始帮助人们学习新语言的第一步，尤其是基于他们本来的语言背景。例如，对于学习土耳其语的英语母语者来说，了解语法完全不同是很重要的，在土耳其语中，你可以构建整个句子，而英语中我们永远不会把主语和动词放在一起成为一个单词。
- en: 'Syntax is largely separate from meaning in language, as the famous sentence
    from Noam Chomsky, the so-called father of syntax, demonstrates: “Colorless green
    ideas sleep furiously.” Everything about that sentence is both grammatically correct
    and semantically understandable. The problem isn’t that it doesn’t make sense;
    it’s that it does, and the encoded meanings of those words conflict. This is a
    reduction; however, you can think of all the times LLMs give nonsense answers
    as this phenomenon manifests. Unfortunately, the syntax is also where ambiguity
    is most commonly found. Consider the sentence, “I saw an old man and woman.” Now
    answer this question: Is the woman also old? This is syntactic ambiguity, where
    we aren’t sure whether the modifier “old” applies to all people in the following
    phrase or just the one it immediately precedes. This is less consequential than
    the fact that semantic and pragmatic ambiguity also show up in syntax. Consider
    this sentence: “I saw a man on a hill with a telescope,” and answer these questions:
    Where is the speaker, and what are they doing? Is the speaker on the hill cutting
    a man in half using a telescope? Likely, you didn’t even consider this option
    when you read the sentence because when we interpret syntax, all of our interpretations
    are at least semantically and pragmatically informed. We know from lived experience
    that that interpretation isn’t at all likely, so we throw it out immediately,
    usually without even taking time to process that we’re eliminating it from the
    pool of probable meanings. Single-modality LLMs will always have this problem,
    and multimodal LLMs can (so far) only asymptote toward the solution.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 语法在语言中很大程度上与意义是分开的，正如著名的句子来自诺姆·乔姆斯基，被称为语法的“之父”所展示的那样：“无色的绿色想法疯狂地睡觉。”这个句子的每一部分在语法上都是正确的，在语义上也是可以理解的。问题不在于它没有意义；而在于它有意义，这些词的编码意义是冲突的。这是一种简化；然而，你可以把LLMs给出无意义答案的所有时间都看作是这个现象的表现。不幸的是，语法也是歧义最常见的地方。考虑这个句子，“我看到一个老人和一个女人。”现在回答这个问题：这个女人也是老人吗？这是句法歧义，我们不确定修饰语“老人”是适用于后面短语中的所有人，还是仅仅适用于它直接前面的那个人。这比语义和语用歧义也出现在句法中的事实要轻微得多。考虑这个句子：“我看到一个在山上的男人，他手里拿着望远镜。”回答这些问题：说话者在哪里，他们在做什么？说话者是在山上用望远镜把一个男人切成两半吗？很可能，你在读这个句子的时候甚至没有考虑这个选项，因为当我们解释句法时，我们所有的解释至少在语义和语用上都是有所依据的。我们从生活经验中知道那种解释根本不可能，所以我们立即将其排除，通常甚至没有花时间去处理我们正在从可能的含义池中排除它。单模态LLMs将始终存在这个问题，而多模态LLMs（到目前为止）只能趋近于解决方案。
- en: It shouldn’t take any logical leap to understand why LLMs need to be syntax-aware
    to be high-performing. LLMs that don’t get word order correct or generate nonsense
    aren’t usually described as “good.” LLMs being syntax-dependent has prompted even
    Chomsky to call LLMs “stochastic parrots.” In our opinion, GPT-2 in 2018 was when
    language modeling solved syntax as a completely meaning-independent demonstration,
    and we’ve been happy to see the more recent attempts to combine the syntax that
    GPT-2 outputs so well with encoded and entailed meaning, which we’ll get into
    now.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么LLMs需要具备语法意识才能表现出色，不应该需要任何逻辑上的跳跃。那些没有正确处理词序或生成无意义输出的LLMs通常不会被描述为“好”。LLMs对语法的依赖甚至促使乔姆斯基将LLMs称为“随机的鹦鹉”。在我们看来，2018年的GPT-2是语言模型将语法作为一个完全独立于意义的演示解决的时候，我们很高兴看到最近尝试将GPT-2输出的语法与编码和蕴涵的意义相结合的尝试，我们现在将深入探讨这一点。
- en: Semantics
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义
- en: 'Semantics are the literal encoded meaning of words in utterances, which changes
    at breakneck speed in waves. People automatically optimize semantic meaning by
    only using words they consider meaningful in the current language epoch. If you’ve
    ever created or used an embedding with language models (word2vec, ELMo, BERT,
    MUSE [the E is for embedding], etc.), you’ve used a semantic approximation. Words
    often go through semantic shifts, and while we won’t cover this topic completely
    or go into depth, here are some common ones you may already be familiar with:
    narrowing, a broader meaning to a more specific one; broadening, the inverse of
    narrowing going from a specific meaning to a broad one; and reinterpretations,
    going through whole or partial transformations. These shifts do not have some
    grand logical underpinning. They don’t even have to correlate with reality, nor
    do speakers of a language often consciously think about the changes as they’re
    happening. That doesn’t stop the change from occurring, and in the context of
    language modeling, it doesn’t stop us from having to keep up with that change.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 语义是话语中词语的直译意义，它以惊人的速度在波浪中变化。人们会自动优化语义意义，只使用他们认为在当前语言时代有意义的词语。如果你曾经创建或使用过语言模型（word2vec、ELMo、BERT、MUSE
    [E代表嵌入]，等等）的嵌入，你就已经使用了语义近似。词语经常经历语义变化，虽然我们不会完全涵盖这个主题或深入探讨，但这里有一些你可能已经熟悉的常见变化：缩小，从更广泛的意义到更具体的一个；扩大，与缩小相反，从具体意义到广泛意义；以及重新解释，通过整体或部分转换。这些变化并没有某种伟大的逻辑基础。它们甚至不需要与现实相关联，语言的使用者通常在变化发生时也不会有意识地思考这些变化。这并不能阻止变化的发生，在语言建模的背景下，这也不能阻止我们必须跟上这种变化。
- en: Let’s look at some examples. Narrowing includes “deer,” which in Old and Middle
    English just meant any wild animal, even a bear or a cougar, and now means only
    one kind of forest animal. For broadening, we have “dog,” which used to refer
    to only one canine breed from England and now can be used to refer to any domesticated
    canine. One fun tangent about dog-broadening is in the FromSoft game *Elden Ring,*
    where because of a limited message system between players, “dog” will be used
    to refer to anything from a turtle to a giant spider and literally everything
    in between. For reinterpretation, we can consider “pretty,” which used to mean
    clever or well-crafted, not visually attractive. Another good example is “bikini,”
    which went from referring to a particular atoll to referring to clothing you might
    have worn when visiting that atoll to people acting as if the “bi-” was referring
    to the two-piece structure of the clothing, thus implying the tankini and monokini.
    Based on expert research and decades of study, we can think of language as being
    constantly compared and re-evaluated by native language speakers, out of which
    common patterns emerge. The spread of those patterns is closely studied in sociolinguistics
    and is largely out of the scope of the current purpose but can quickly come into
    scope as localization (l10n) or internationalization (i18n) for LLMs arises as
    a project requirement. Sociolinguistic phenomena such as prestige can help design
    systems that work well for everyone.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些例子。缩小包括“鹿”，在古英语和中古英语中这个词仅仅指任何野生动物，甚至包括熊或美洲狮，而现在只指一种森林动物。对于扩大，我们有“狗”，它曾经只指来自英格兰的一种犬种，而现在可以用来指任何家养犬。关于狗的扩大有一个有趣的小插曲，在FromSoft游戏《艾尔登法环》中，由于玩家之间有限的消息系统，"狗"这个词会被用来指从乌龟到巨型蜘蛛以及介于两者之间的所有事物。对于重新解释，我们可以考虑“漂亮”，它曾经意味着聪明或工艺精湛，而不是视觉上吸引人。另一个很好的例子是“比基尼”，它从指一个特定的环礁，到指在访问那个环礁时可能穿的衣服，再到人们好像“bi-”指的是衣服的两件式结构，从而暗示了坦克尼和单件比基尼。基于专家研究和数十年的研究，我们可以认为语言是不断被母语使用者比较和重新评估的，从中产生了常见的模式。这些模式在语言社会学的传播被密切研究，但很大程度上超出了当前目的的范围，但当地化（l10n）或国际化（i18n）作为LLMs项目需求出现时，可以迅速进入范围。像声望这样的社会语言学现象可以帮助设计出对每个人都适用的系统。
- en: In the context of LLMs, so-called semantic embeddings are vectorized versions
    of text that attempt to mimic semantic meaning. Currently, the most popular way
    of doing this is by tokenizing or assigning an arbitrary number in a dictionary
    to each subword in an utterance (think prefixes, suffixes, and morphemes generally),
    applying a continuous language model to increase the dimensionality of each token
    within the vector so that there’s a larger vector representing each index of the
    tokenized vector, and then applying a positional encoding to each of those vectors
    to capture word order. Each subword ends up being compared to other words in the
    larger dictionary based on how it’s used. We’ll show you an example of this later.
    Something to consider when thinking about word embeddings is that they struggle
    to capture the deep, encoded meaning of those tokens, and simply adding more dimensions
    to the embeddings hasn’t shown marked improvement. Evidence that embeddings work
    similarly to humans is that you can apply a distance function to related words
    and see that they are closer together than unrelated words. How to capture and
    represent meaning more completely is another area in which to expect groundbreaking
    research in the coming years and months.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs（大型语言模型）的背景下，所谓的语义嵌入是文本的向量化版本，试图模仿语义含义。目前，最流行的方法是通过分词或将字典中的任意数字分配给每个话语中的子词（例如前缀、后缀和一般词素），应用连续语言模型来增加向量中每个标记的维度，以便有一个更大的向量代表每个标记向量的索引，然后对每个这些向量应用位置编码以捕捉词序。每个子词最终都会根据其使用方式与其他字典中的单词进行比较。我们稍后会展示一个例子。在思考词嵌入时，需要考虑的是，它们难以捕捉那些标记的深层编码含义，而简单地增加嵌入的维度并没有显示出显著的改进。嵌入与人类工作方式相似的证据是，你可以应用距离函数到相关单词上，并看到它们比无关单词更接近。如何更完整地捕捉和表示意义是未来几年和几个月内有望出现突破性研究的另一个领域。
- en: Pragmatics
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 话语学
- en: Pragmatics is sometimes omitted from linguistics due to its referent being all
    the nonlinguistic context affecting a listener’s interpretation and the speaker’s
    decision to express things in a certain way. Pragmatics refers in large part to
    dogmas followed in cultures, regions, socio-economic classes, and shared lived
    experiences, which are played off of to take shortcuts in conversations using
    entailment.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 话语学有时被省略在语言学中，因为它的参照物是所有影响听者解释和说话者决定以某种方式表达的非语言环境。话语学在很大程度上指的是文化、地区、社会经济阶层和共同生活经历中遵循的教条，这些教条在对话中通过蕴涵来走捷径。
- en: If we were to say, “A popular celebrity was just taken into the ICU,” your pragmatic
    interpretation based on lived experience might be to assume that a well-beloved
    person has been badly injured and is now undergoing medical treatment in a well-equipped
    hospital. You may wonder about which celebrity it is, whether they will have to
    pay for the medical bills, or if the injury was self-inflicted, also based on
    your lived experience. None of these things can be inferred directly from the
    text and its encoded meaning by itself. You would need to know that ICU stands
    for a larger set of words and what those words are. You would need to know what
    a hospital is and why someone would need to be taken there instead of going there
    themselves. If any of these feel obvious, good. You live in a society, and your
    pragmatic knowledge of that society overlaps well with the example provided. If
    we share an example from a less-populated society, “Janka got her grand-night
    lashings yesterday; she’s gonna get Peter tomorrow,” you might be left scratching
    your head. If you are, realize this probably looks like how a lot of text data
    ends up looking to an LLM (anthropomorphization acknowledged). For those wondering,
    this sentence comes from Slovak Easter traditions. A lot of meaning here will
    be missed and go unexplained if you are unaccustomed to these particular traditions
    as they stand in that culture. This author personally has had the pleasure of
    trying to explain the Easter Bunny and its obsession with eggs to foreign colleagues
    and enjoyed the satisfaction of looking like I’m off my rocker.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们说，“一位受欢迎的明星刚刚被送进了ICU”，根据你的生活经验，你的语用解释可能是假设一个深受爱戴的人受到了严重伤害，现在正在一家设备齐全的医院接受治疗。你可能想知道这是哪位明星，他们是否需要支付医疗费用，或者伤害是否是自残的，这也基于你的生活经验。这些都不能直接从文本及其编码的意义中推断出来。你需要知道ICU代表一组更大的词汇以及这些词汇是什么。你需要知道医院是什么，以及为什么有人需要被送到那里而不是自己去。如果这些感觉很明显，很好。你生活在一个社会中，你对这个社会的语用知识很好地与提供的例子重叠。如果我们分享一个来自人口较少社会的例子，“Janka昨天受到了严厉的惩罚；她明天会得到Peter的惩罚”，你可能会感到困惑。如果你感到困惑，意识到这可能就是许多文本数据对LLMs（承认拟人化）看起来像的样子。对于那些想知道的人，这个句子来自斯洛伐克的复活节传统。如果你不习惯这些特定的传统，那么这里很多含义都会被错过，无法解释。这位作者个人很享受尝试向外国同事解释复活节兔子和它对鸡蛋的迷恋，并享受着看起来像是脱离了现实的感觉。
- en: In the context of LLMs, we can effectively group all out-of-text contexts into
    pragmatics. This means LLMs start without any knowledge of the outside world and
    do not gain it during training. They only gain a knowledge of how humans respond
    to particular pragmatic stimuli. LLMs do not understand social class or race or
    gender or presidential candidates, or anything else that might spark some type
    of emotion in you based on your life experience. Pragmatics isn’t something that
    we expect will be able to be directly incorporated into a model at any point because
    models cannot live in society. Yet we have already seen the benefits of incorporating
    it indirectly through data engineering and curation, prompting mechanisms like
    RAG, and supervised finetuning on instruction datasets. In the future, we expect
    great improvements in incorporating pragmatics into LLMs, but we emphasize that
    it’s an asymptotic solution because language is ultimately still an abstraction.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs（大型语言模型）的背景下，我们可以有效地将所有非文本上下文归入语用学。这意味着LLMs在开始时对现实世界没有任何知识，并且在训练过程中也不会获得这些知识。它们只获得对人类如何对特定的语用刺激做出反应的知识。LLMs不理解社会阶层、种族、性别或总统候选人，或者任何可能基于你的生活经历激发某种情感的其他事物。我们并不期望语用学能够在任何时刻直接被纳入模型中，因为模型无法生活在社会中。然而，我们已经看到了通过数据工程和整理、RAG（阅读-询问-生成）提示机制以及指令数据集上的监督微调间接纳入语用学的益处。在未来，我们预计在将语用学纳入LLMs方面将取得重大进步，但我们强调这是一个渐近解，因为语言最终仍然是一种抽象。
- en: Pragmatic structure gets added, whether you mean to add it or not, as soon as
    you acquire the data you are going to train on. You can think of this type of
    pragmatic structure as bias, not inherently good or bad, but impossible to get
    rid of. Later down the line, you get to pick the types of bias you’d like your
    data to keep by normalizing and curating, augmenting particular underrepresented
    points, and cutting overrepresented or noisy examples. Instruction datasets show
    us how you can harness pragmatic structure in your training data to create incredibly
    useful bias, like biasing your model to answer a question when asked instead of
    attempting to categorize the sentiment of the question.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Pragmatics and context all revolve around entailment. An entailment is a pragmatic
    marker within your data, as opposed to the literal text your dataset contains.
    For example, let’s say you have a model attempting to take an input like “Write
    me a speech about frogs eating soggy socks that doesn’t rhyme and where the first
    letters of each line spell amphibian” and actually follow that instruction. You
    can immediately tell that this input is asking for a lot. The balance for you
    as a data engineer would be to make sure that everything the input is asking for
    is explicitly accounted for in your data. You need examples of speeches, examples
    of what frogs and socks are and how they behave, and examples of acrostic poems.
    If you don’t have them, the model might be able to understand just from whatever
    entailments exist in your dataset, but it’s pretty up in the air. If you go the
    extra mile and keep track of entailed versus explicit information and tasks in
    your dataset, along with data distributions, you’ll have examples to answer, “What
    is the garbage-in resulting in our garbage-out?”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: LLMs struggle to pick up on pragmatics, even more so than people, but they do
    pick up on the things that your average standard deviation of people would. They
    can even replicate responses from people outside that standard deviation, but
    pretty inconsistently without the exact right stimulus. That means it’s difficult
    for a model to give you an expert answer on a problem the average person doesn’t
    know without providing the correct bias and entailment during training and in
    the prompt. For example, including “masterpiece” at the beginning of an image-generation
    prompt will elicit different and usually higher-quality generations, but only
    if that distinction was present in the training set and only if you’re asking
    for an image where “masterpiece” is a compliment. Instruction-based datasets attempt
    to manufacture those stimuli during training by asking questions and giving instructions
    that entail representative responses. It is impossible to account for every possible
    situation in training, and you may inadvertently create new types of responses
    from your end users by trying to account for everything. After training, you can
    coax particular information from your model through prompting, which has a skill
    ceiling based on what your data originally entailed.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Morphology
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Morphology is the study of word structures and how they are formed from smaller
    units called morphemes. Morphemes are the smallest units of meaning, like the
    “re-” in “redo” or “relearn.” However, not all parts of words are morphemes, such
    as “ra-” in “ration” or “na-” in “nation,” and some can be unexpected, like “helico-”
    as in “helicoid” and “-pter” as in “pterodactyl.”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how words are constructed helps create better language models
    and parsing algorithms, which are essential for tasks like tokenization. Tokens
    are the basic units used in NLP; they can be words, subwords, characters, or whole
    utterances and do not have to correspond to existing morphemes. People do not
    consciously decide what their units of meaning are going to be, and as such, they
    are often illogical. The effectiveness of a language model can depend on how well
    it can understand and process these tokens. For instance, in tokenization, a model
    needs to store a set of dictionaries to convert between words and their corresponding
    indices. One of these tokens is usually an `/<UNK/>` token, which represents any
    word that the model does not recognize. If this token is used too frequently,
    it can hinder the model’s performance, either because the model’s vocabulary is
    too small or because the tokenizer is not using the right algorithm for the task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where you want to build a code completion model, but you’re
    using a tokenizer that only recognizes words separated by whitespace, like the
    NLTK `punkt` tokenizer. When it encounters the string `def` `add_two_numbers_together(x,`
    `y):`, it will pass `[def,` `[UNK],` `y]` to the model. This causes the model
    to lose valuable information, not only because it doesn’t recognize the punctuation
    but also because the important part of the function’s purpose is replaced with
    an unknown token due to the tokenizer’s morphological algorithm. A better understanding
    of word structure and the appropriate parsing algorithms is needed to improve
    the model’s performance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Semiotics
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After exploring the fundamental features of language and examining their significance
    in the context of LLMs, it is important to consider the broader perspective of
    meaning-making and interpretation in human communication. Semiotics, the study
    of signs and symbols, offers a valuable lens through which we can better understand
    how people interpret and process language. We will delve into semiotics, examining
    the relationship between signs, signifiers, and abstractions and how LLMs utilize
    these elements to generate meaningful output. This discussion will provide a deeper
    understanding of the intricate processes through which LLMs manage to mimic human-like
    understanding of language while also shedding light on the challenges and limitations
    they face in this endeavor. We do not necessarily believe that mimicking human
    behavior is the right answer for LLM improvement, only that mimicry is how the
    field has evaluated itself so far.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: To introduce semiotics, let’s consider figure 2.2, an adapted Peircean semiotic
    triangle. These triangles are used to organize base ideas into sequences of firstness,
    secondness, and thirdness, with firstness being at the top left, secondness at
    the bottom, and thirdness at the top right. If you’ve ever seen a semiotic triangle
    before, you may be surprised at the number of corners and orientation. To explain,
    we’ve turned them upside down to make it slightly easier to read. Also, because
    the system is recursive, we’re showing you how the system can simultaneously model
    the entire process and each piece individually. While the whole concept of these
    ideas is very cool, it’s outside of the scope of this book to delve into the philosophy
    fully. Instead, we can focus on the cardinal parts of those words (first, second,
    third) to show the sequence of how meaning is processed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 A recursive Peircean semiotic triangle is a system of organizing
    the process of extracting meaning from anything—in our case, from language. Each
    point on the triangle illustrates one of the minimal parts needed to synthesize
    meaning within whatever the system is being used to describe, so each point is
    a minimal unit in meaning for language. Firstness, secondness, and thirdness are
    not points on the triangle; instead, they are more like markers for the people
    versed in semiotics to be able to orient themselves in this diagram.
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can also look at each intersection of the triangles to understand why things
    are presented in the order they are. Feelings can be attached to images and encodings
    long before they can be attached to words and tables. Ritual and common scripts
    give a space for interpreted action that’s second nature and doesn’t have to be
    thought about, similar to how most phrases just come together from words without
    the native speaker needing to perform metacognition about each word individually.
    All of these eventually lead to an interpretation or a document (a collection
    of utterances); in our case, that interpretation should be reached by the LLM.
    This is why, for example, prompt engineering can boost model efficacy. Foundation
    LLMs trained on millions of examples of ritual scripts can replicate the type
    of script significantly better when you explicitly tell the model in the prompt
    which script needs to be followed. Try asking the model for a step-by-step explanation—maybe
    prepend your generation with “Let’s think about this step-by-step.” The model
    will generate step-by-step scripts based on previous scripts it’s seen play out.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: For those interested, there are specific ways of reading these figures and a
    whole field of semiotics to consider; however, it’s not guaranteed that you’ll
    be able to create the best LLMs by understanding the whole thing. Instead of diving
    deeply into this, we’ll consider the bare minimum that can help you build the
    best models, UX, and UI for everyone to interact with. For example, one aspect
    of the process of creating meaning is recursiveness. When someone is talking to
    you and they say something that doesn’t make sense (is “meaningless” to you),
    what do you do? Generally, people will ask one or more clarifying questions to
    figure out the meaning, and the process will start over and over until the meaning
    is clear to you. The most state-of-the-art models currently on the market do not
    do this, but they can be made to do it through very purposeful prompting. Many
    people wouldn’t even know to do that without having it pointed out to them. In
    other words, this is a brief introduction to semiotics. You don’t need to be able
    to give in-depth and accurate coordinate-specific explanations to experts in the
    semiotic field by the end of this section. The point we are trying to make is
    that this is an organizational system showcasing the minimum number of things
    you need to create a full picture of meaning for another person to interpret.
    We are not giving the same amount of the same kinds of information to our models
    during training, but if we did, it would result in a marked improvement in model
    behavior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Figures 2.2 and 2.3 are meant to represent a minimal organizational model, where
    each of these pieces is essential. Let’s consider figure 2.3, which walks through
    an example of using a semiotic triangle. Consider images, pictures, and memories
    and think about what it would be like to try to absorb the knowledge in this book
    without your eyes to process images and without orthography (a writing system)
    to abstract the knowledge. Looking at the bullet points, etc., how could you read
    this book without sections, whitespace between letters, and bullet points to show
    you the order and structure to process information? Look at semantics and literal
    encoded meaning, and imagine the book without diagrams or with words that didn’t
    have dictionary definitions. The spreadsheets in the middle could be a book without
    any tables or comparative informational organizers, including these figures. What
    would it be like to read this book without a culture or society with habits and
    dogma to use as a lens for our interpretations? All these points form our ability
    to interpret information, along with the lens through which we pass our information
    to recognize patterns.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-3.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 Starting at the top-left corner, follow the arrows to see the general
    order we use to build our interpretations and extract meaning from things we interact
    with. Here, we’ve replaced the descriptive words with some examples of each point.
    Try to imagine interpreting this diagram without any words, examples, arrows,
    or even the pragmatic context of knowing what a figure in a book like this is
    supposed to be for.
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'So these are the important questions: How many of these things do you see LLMs
    having access to in order to return meaningful interpretations? Does an LLM have
    access to feelings or societal rituals? Currently, they do not, but as we go through
    traditional and newer techniques for NLP inference, think about what different
    models have access to.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Multilingual NLP
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last challenge that we need to touch on before we evaluate previous NLP
    techniques and current-generation LLMs is the foundation of linguistics and the
    reason LLMs even exist. People have wanted to understand or exploit each other
    since the first civilizations made contact. These cases have resulted in the need
    for translators, and this need has only exponentially increased as the global
    economy has grown and flourished.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: It’s pretty simple math for business as well. Did you know that there are almost
    as many native speakers of Bengali as there are native speakers of English? If
    this is the first time you’ve heard of the Bengali language, this should hopefully
    color your perception that there is a valuable market for multilingual models.
    There are billions of people in the world, but only about a third of 1 billion
    speak English natively. If your model is Anglocentric, like most are, you are
    missing out on 95% of the people in the world as customers and users. Spanish
    and Mandarin Chinese are easy wins in this area, but most people don’t even go
    that far.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: There are many more politically charged examples of calling things, including
    different languages, the same that are out of the scope of this book. These are
    most often because of external factors like government involvement. Keeping these
    two points in mind—that a monolingual system focusing on English doesn’t have
    the coverage or profit potential that many businesses act like it does and that
    the boundaries between languages and dialects are unreliable at best and systematically
    harmful at worst—should highlight the dangerous swamp of opinions. Many businesses
    and research scientists don’t even pretend to want to touch this swamp with a
    50-foot pole when designing a product or system.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, no easy solutions exist at this time. However, considering these
    factors can help you as a scientist or engineer (and hopefully an ethical person)
    to design LLMs that, at the very least, don’t exacerbate and negatively contribute
    to the existing problems. The first step in this process is deciding on a directional
    goal from the beginning of the project, either toward localization (l10n) or internationalization
    (i18n). Localization is an approach exemplified by Mozilla, which has a different
    version of its browser available through crowdsourced l10n in over 90 languages
    with no indications of stopping that effort. Internationalization is similar,
    but in the opposite direction; for example, Ikea tries to put as few words as
    possible in their instructional booklets, opting instead for internationally recognized
    symbols and pictures to help customers navigate the DIY projects. Deciding at
    the beginning of the project cuts down on the effort required to expand to either
    solution exponentially. It is large enough to switch the perception of translation
    and formatting from a cost to an investment. In the context of LLMs and their
    rapid expansion across the public consciousness, it becomes even more important
    to make that consideration early. Hitting the market with a world-changing technology
    that automatically disallows most of the world from interacting with it devalues
    those voices. Having to wait jeopardizes businesses’ economic prospects.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing, let’s take a moment to reflect on what we’ve discussed so
    far. We’ve hit important points in linguistics, illustrating concepts for us to
    consider, such as understanding that the structure of language is separate from
    its meaning. We have demonstrated quite a journey that each of us takes, both
    personally and as a society, toward having the metacognition to understand and
    represent language in a coherent way for computers to work with. This understanding
    will only improve as we deepen our knowledge of cognitive fields and solve for
    the linguistic features we encounter. Going along with figure 2.1, we will now
    demonstrate the computational path for language modeling that we have followed
    and explore how it has and hasn’t solved for any of those linguistic features
    or strived to create meaning. Let’s move into evaluating the various techniques
    for representing a language algorithmically.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Language modeling techniques
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having delved into the fundamental features of language, the principles of semiotics,
    and how LLMs interpret and process linguistic information, we now transition into
    a more practical realm. We will explore the various NLP techniques developed and
    employed to create these powerful language models. By examining the strengths
    and weaknesses of each approach, we will gain valuable insights into the effectiveness
    of these techniques in capturing the essence of human language and communication.
    This knowledge will not only help us appreciate the advancements made in the field
    of NLP but also enable us to better understand the current limitations of these
    models and the challenges that lie ahead for future research and development.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a second to go over some data processing that will be universal to
    all language modeling. First, we’ll need to decide how to break up the words and
    symbols we’ll be passing into our model, effectively deciding what a token will
    be in our model. We’ll need a way to convert those tokens to numerical values
    and back again. Then, we’ll need to pick how our model will process the tokenized
    inputs. Each of the following techniques will build upon the previous techniques
    in at least one of these ways.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'The first of these techniques is called a bag-of-words (BoW) model, and it
    consists of simply counting words as they appear in text. You could import the
    CountVectorizer class from sklearn to use it, but it’s more instructive if we
    show you with a small snippet. It can be accomplished very easily with a dictionary
    that scans through text, creating a new vocabulary entry for each new word as
    a key and an incrementing value starting at 1:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Considering its simplicity, even this model, based entirely on frequency, can
    be quite powerful when trying to gain insight into a speaker’s intentions or at
    least their idiosyncrasies. For example, you could run a simple BoW model on inaugural
    speeches of US presidents, searching for the words “freedom,” “economy,” and “enemy”
    to gain a pretty good insight about which presidents assumed office under peacetime,
    during wartime, and during times of monetary strife, just based on how many times
    each word was mentioned. The BoW model’s weaknesses are many, however, as the
    model provides no images, semantics, pragmatics, phrases, or feelings. In our
    example, there are two instances of “words,” but because our tokenization strategy
    is just whitespace, it didn’t increment the key in the model. It doesn’t have
    any mechanisms to evaluate context or phonetics, and because it divides words
    by default on whitespace (you can obviously tokenize however you want, but try
    tokenizing on subwords and see what happens with this model—spoiler: it is bad),
    it doesn’t account for morphology either. Altogether, it should be considered
    a weak model for representing language but a strong baseline for evaluating other
    models against it. To solve the problem of BoW models not capturing any sequence
    data, N-gram models were conceived.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 N-gram and corpus-based techniques
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: N-gram models represent a marked and efficient improvement to BoW by allowing
    you to give the model a sort of context, represented by N. They are relatively
    simple statistical models that enable you to generate words based on the N = 1
    context space. Listing 2.1 uses trigrams, which means N = 3\. We clean the text
    and give it minimal padding/formatting to help the model, and then we train using
    everygrams, which prioritizes flexibility over efficiency so that we can train
    a pentagram (N = 5) or a septagram (N = 7) model if we want. At the end of the
    listing, where we are generating, we can give the model up to two tokens to help
    it figure out how to generate further. N-gram models were not created for and
    have never claimed to attempt complete modeling systems of linguistic knowledge,
    but they are widely useful in practical applications. They ignore all linguistic
    features, including syntax, and only attempt to draw probabilistic connections
    between words appearing in an N-length phrase.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  All assets necessary to run the code—including text and data files—can
    be found in the code repository accompanying this book: [https://github.com/IMJONEZZ/LLMs-in-Production/](https://github.com/IMJONEZZ/LLMs-in-Production/).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 A generative N-grams language model implementation
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Creates a corpus from any number of plain .txt files'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Pads each side of every line in the corpus with <s> and </s> to indicate
    the start and end of utterances'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Allows everygrams to create a training set and a vocab object from the data'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Instantiates and trains the model we’ll use for N-grams, a maximum likelihood
    estimator (MLE)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '#5 This model will take the everygrams vocabulary, including the <UNK> token
    used for out-of-vocabulary.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Language can be generated with this model and conditioned with n-1 tokens
    preceding.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'This code is all that you need to create a generative N-gram model. For those
    interested in being able to evaluate that model further, we’ve included the following
    code so you can grab probabilities and log scores or analyze the entropy and perplexity
    of a particular phrase. Because this is all frequency-based, even though it’s
    mathematically significant, it still does a pretty bad job of describing how perplexing
    or frequent real-world language actually is:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Any set of tokens up to length = n can be counted easily to determine frequency.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Any token can be given a probability of occurrence and augmented with up
    to n-1 tokens to precede it.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '#3 This can be done as a log score as well to avoid very big and very small
    numbers.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sets of tokens can be tested for entropy and perplexity as well.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'While this code example illustrates creating a trigram language model, unfortunately,
    not all phrases needing to be captured are only three tokens long. For example,
    from Hamlet, “To be or not to be” consists of one phrase with two words and one
    phrase with four words. Note that even though N-grams are typically very small
    language models, it is possible to make an N-gram LLM by making N=1,000,000,000
    or higher, but don’t expect to get even one ounce of use out of it. Just because
    we made it big doesn’t make it better or mean it’ll have any practical application:
    99.9% of all text and 100% of all meaningful text contains fewer than 1 billion
    tokens appearing more than once, and that computational power can be much better
    spent elsewhere.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: N-grams only use static signals (whitespace, orthography) and words to extract
    meaning (figure 2.2). They try to measure phrases manually, assuming all phrases
    will be the same length. That said, N-grams can be used to create powerful baselines
    for text analysis. In addition, if the analyst already knows the pragmatic context
    of the utterance, N-grams can give quick and accurate insight into real-world
    scenarios. Nonetheless, this type of phrasal modeling fails to capture any semantic
    encodings that individual words could have. To solve this problem, Bayesian statistics
    were applied to language modeling.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Bayesian techniques
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bayes’ theorem is one of the most mathematically sound and simple theories for
    describing the occurrence of your output within your input space. Essentially,
    it calculates the probability of an event occurring based on prior knowledge.
    The theorem posits that the probability of a hypothesis being true given evidence—for
    example, that a sentence has a positive sentiment—is equal to the probability
    of the evidence occurring given the hypothesis is true multiplied by the probability
    of the hypothesis occurring, all divided by the probability of the evidence being
    true. It can be expressed mathematically as
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*hypothesis* | *evidence*) = (*P*(*evidence* | *hypothesis*) × *P*(*hypothesis*))
    / *P*(*evidence*)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: or
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*A*|*B*) × *P*(*B*) = *P*(*B*|*A*) × *P*(*A*)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Because this isn’t a math book, we’ll dive into Bayes’ theorem to the exact
    same depth we dove into other linguistics concepts and trust the interested reader
    to search for more.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, even though the theorem represents the data in a mathematically
    sound way, it doesn’t account for any stochasticity or multiple meanings of words.
    One word you can always throw at a Bayesian model to confuse it is “it.” Any demonstrative
    pronoun ends up getting assigned values in the same `LogPrior` and `LogLikelihood`
    way as all other words, and it gets a static value, which is antithetical to the
    usage of those words. For example, if you’re trying to perform sentiment analysis
    on an utterance, assigning all pronouns a null value would be better than letting
    them go through the Bayesian training. Note also that Bayesian techniques don’t
    create generative language models the way the rest of these techniques will. Because
    of the nature of Bayes’ theorem validating a hypothesis, these models work for
    classification and can bring powerful augmentation to a generative language model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 shows you how to create a naive Bayes classification language model,
    or a system that performs classification on text based on a prior-learned internal
    language model. Instead of using a package like sklearn or something that would
    make writing the code a little easier, we opted to write out what we were doing,
    so it’s a bit longer, but it should be more information about how it works. We
    are using the least-complex version of a naive Bayes model. We haven’t made it
    multinomial or added anything fancy; obviously, it would work better if you opted
    to upgrade it for any problem you want. And we highly recommend you do.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  To make the code easier to understand and help highlight the portions
    we wanted to focus on, we have simplified some of our code listings by extracting
    portions to utility helpers. If you are seeing import errors, this is why. These
    helper methods can be found in the code repository accompanying this book: [https://github.com/IMJONEZZ/LLMs-in-Production/](https://github.com/IMJONEZZ/LLMs-in-Production/)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Categorical naive Bayes language model implementation
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Δefines the key, which is the word and label tuple'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '#2 If the key exists in the dictionary, increments the count'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '#3 If the key is new, adds it to the dict and sets the count to 1'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Calculates V, the number of unique words in the vocabulary'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Calculates N_pos and N_neg'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '#6 If the label is positive (greater than zero) . . .'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '#7 . . . increments the number of positive words (word, label)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Else, the label is negative.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Increments the number of negative words (word, label)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Calculates Δ, the number of documents'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Calculates the number of positive documents'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Calculates the number of negative documents'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Calculates logprior'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '#14 For each word in the vocabulary . . .'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '#15 . . . calculates the probability that each word is positive or negative'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '#16 Calculates the log likelihood of the word'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '#17 Processes the utt to get a list of words'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '#18 Initializes probability to zero'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '#19 Adds the logprior'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '#20 Checks if the word exists in the loglikelihood dictionary'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '#21 Adds the log likelihood of that word to the probability'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '#22 Returns this properly'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '#23 If the prediction is &gt; 0 . . .'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '#24 . . . the predicted class is 1.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '#25 Otherwise, the predicted class is 0.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '#26 Appends the predicted class to the list y_hats'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '#27 Error = avg of the abs vals of the diffs between y_hats and test_y.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '#28 Accuracy is 1 minus the error.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: This theorem doesn’t create the same type of language model but one with a list
    of probabilities associated with one hypothesis. As such, Bayesian language models
    can’t be used effectively to generate language, but they can be very powerfully
    implemented for classification tasks. In our opinion, though, Bayesian models
    are often overhyped for even this task. One of the crowning achievements of one
    author’s career was replacing and removing a Bayesian model from production.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian models, one big problem is that all sequences are completely unconnected,
    like BoW models, moving us to the opposite end of sequence modeling from N-grams.
    Like a pendulum, language modeling swings back toward sequence modeling and language
    generation with Markov chains.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Markov chains
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often called hidden Markov models (HMMs), Markov chains essentially add state
    to the N-gram models, storing probabilities using hidden states. They are often
    used to help parse text data for even larger models, doing things like part-of-speech
    (PoS) tagging (marking words with their parts of speech) and named entity recognition
    (NER; marking identifying words with their referent and usually type; e.g., LA
    – Los Angeles – City) on textual data. Building on the previous Bayesian models,
    Markov models rely completely on stochasticity (predictable randomness) in the
    tokens encountered. The idea that the probability of anything happening *next*
    depends completely upon the state of *now* is, like Bayes’ theorem, mathematically
    sound. So instead of modeling words based solely on their historical occurrence
    and drawing a probability from that, we model their future and past collocation
    based on what is currently occurring. So the probability of “happy” occurring
    goes down to almost zero if “happy” was just output but goes up significantly
    if “am” has just occurred. Markov chains are so intuitive that they were incorporated
    into later iterations of Bayesian statistics and are still used in production
    systems today.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.3, we train a Markov chain generative language model. This is the
    first model where we’ve used a specific tokenizer, which, in this case, will tokenize
    based on the whitespace between words. This is also only the second time we’ve
    referred to a collection of utterances meant to be viewed together as a document.
    As you play around with this one, pay close attention and make some comparisons
    yourself of how well the HMM generates compared to even a large N-gram model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Generative hidden Markov language model implementation
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code shows a basic implementation of a Markov model for generation, and
    we encourage you to experiment with it. Give it text from songs from your favorite
    musicians or books from your favorite authors, and see whether what comes out
    sounds like them. HMMs are incredibly fast and are often used in predictive text
    or predictive search applications. Markov models represent the first comprehensive
    attempt to model language from a descriptive linguistic perspective, as opposed
    to a prescriptive one. The perspective is interesting because Markov did not originally
    intend to use linguistic modeling, only to win an argument about continuous independent
    states. Later, Markov used Markov chains to model vowel distribution in a Pushkin
    novel, so he was at least aware of the possible applications.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between descriptive and prescriptive linguistics is that the
    latter focuses on how things *ought* to be, while the former focuses on how things
    *are*. From a language modeling perspective, it has proven vastly more effective
    to describe what language is doing from a corpus or Markov perspective rather
    than to attempt to prescribe how language ought to behave. Unfortunately, a current
    state by itself cannot be used to give context beyond the now, so historical or
    societal context cannot be represented effectively in a Markov model. The semantic
    encoding of words also becomes problematic, as represented in the code example:
    Markov chains will output syntactically correct chains of words that are nonsense
    semantically, similar to “colorless green ideas sleep furiously.” To solve this
    problem, “continuous” models were developed to allow for a “semantic embedding”
    representation of tokens.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Continuous language modeling
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A continuous bag-of-words (CBoW) model—much like its namesake, the BoW model—is
    a frequency-based approach to analyzing language, meaning that it models words
    based on how often they occur. The next word in a human utterance has never been
    determined based on probability or frequency. Consequently, we provide an example
    of creating word embeddings to be ingested or compared by other models using a
    CBoW. We’ll use a neural network to provide you with a good methodology.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: This is the first language modeling technique we’ll see that essentially slides
    a context window over a given utterance (the context window is an N-gram model)
    and attempts to guess the word in the middle based on the surrounding words in
    the window. For example, let’s say your window has a length of 5, and your sentence
    is “Learning about linguistics makes me happy.” You would give the CBoW `['learning',`
    `'about',` `'makes',` `'me']` to try to get the model to guess “linguistics” based
    on how many times the model has previously seen that word occur in similar places.
    This example shows you why generation is difficult for models trained like this.
    Say you give the model `['makes',` `'me',` `'</s>']` as input. Now the model only
    has three pieces of information, instead of four, to use to try to figure out
    the answer; it also will be biased toward only guessing words it has seen before
    at the end of sentences, as opposed to getting ready to start new clauses. It’s
    not all bad, though. One feature that makes continuous models stand out for embeddings
    is that they don’t have to look at only words before the target word; they can
    also use words that come after the target to gain some semblance of context.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.4, we create our first continuous model. In our case, to keep things
    as simple as possible, we use a BoW model for the language processing and a one-layer
    neural network with two parameters for the embedding estimation, although both
    could be substituted for any other models. For example, you could substitute N-grams
    for the BoW and a naive Bayes model for the neural network to get a continuous
    naive N-gram model. The point is that the actual models used in this technique
    are a bit arbitrary; it’s the continuous technique that’s important. To illustrate
    this further, we don’t use any packages other than `numpy` to do the math for
    the neural network, even though it’s the first one appearing in this section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Pay special attention to the steps—initializing the model weights, the rectified
    linear unit (ReLU) activation function, the final softmax layer, and forward and
    backpropagation—and how it all fits together in the `gradient_descent` function.
    These are pieces of the puzzle that you will see crop up again and again, regardless
    of programming language or framework. You will need to initialize models, pick
    activation functions, pick final layers, and define forward and backward propagation
    in TensorFlow, PyTorch, and Hugging Face, as well as if you ever start creating
    your own models instead of using someone else’s.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 Generative CBoW language model implementation
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Creates our corpus for training'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Slightly cleans the data by removing punctuation, tokenizing by word, and
    converting to lowercase alpha characters'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Gets our bag of words, along with a distribution'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates two dictionaries to speed up time-to-convert and keep track of vocabulary'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Here, we create our neural network with one layer and two parameters.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Creates our final classification layer, which makes all possibilities add
    up to 1'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Δefines the behavior for moving forward through our model, along with an
    activation function'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Δefine how we determine the distance between ground truth and model predictions'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Δefines how we move backward through the model and collect gradients'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Puts it all together and trains'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Trains the model'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: The CBoW example is our first code example to showcase a full and effective
    training loop in machine learning. Within all of that, pay special attention to
    the steps in a training loop, especially the activation function, ReLU. As we
    expect you to be at least familiar with various ML paradigms, including different
    activations, we won’t explain the ReLU here. We will address when you should use
    it and when you shouldn’t. ReLUs, while solving the vanishing gradient problem,
    don’t solve the exploding gradient problem, and they destroy all negative comparisons
    within the model. Better situational variants include the Exponential linear unit
    (ELU), which allows negative numbers to normalize to alpha, and the generalized
    Gaussian linear units (GEGLU)/Swish-gated linear unit (SWIGLU), which works well
    in increasingly perplexing scenarios, like language. However, people often use
    ReLUs, not because they are the best in a situation, but because they are easy
    to understand and code and intuitive, even more so than the activations they were
    created to replace, the sigmoid or tanh.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: A lot of this ends up being abstracted with packages and the like, but knowing
    what’s going on under the hood will be very helpful for you as someone putting
    LLMs in production. You should be able to predict with some certainty how different
    models will behave in various situations. The next section will dive into one
    of those abstractions—in this case, the abstraction created by the continuous
    modeling technique.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 Embeddings
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hearkening back to our features of language, it should be easy to connect why
    continuous-style language modeling was such a breakthrough. Embeddings take the
    tokenized vectors we’ve created that don’t contain any meaning and attempt to
    insert that meaning based on observations that can be made about the text, such
    as word order and subwords appearing in similar contexts. Despite the primary
    mode of meaning being collocation (co-located, words that appear next to each
    other), they prove useful and even show some similarities to human-encoded word
    meaning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'The quintessential example from Word2Vec, one of the first pretrained vector
    embeddings, was taking the vector for “king,” subtracting the vector for “man,”
    adding the vector for “woman,” and finding the nearest neighbor to the sum was
    the vector for the word “queen.” This makes sense to us, as it mimics human semantics.
    One of the major differences is one that’s already been mentioned a couple of
    times: pragmatics. Humans use pragmatic context to inform semantic meaning, understanding
    that just because you said, “I need food,” doesn’t mean you are actually in physical
    danger without it. Embeddings are devoid of any influence outside of pure usage,
    which feels like it could be how humans learn as well, and there are good arguments
    on all sides here. The one thing holding is that if we can somehow give models
    more representative data, that may open the door to more effective embeddings,
    but it’s a chicken-and-egg problem because more effective embeddings give better
    model performance.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.5, we dive into how to visualize embeddings using `pyplot`. We
    will be going more in depth into embeddings in later chapters. This is helpful
    for model explainability and also for validation during your pretraining step.
    If you see that your semantically similar embeddings are relatively close to each
    other on the graph, you’re likely going in the right direction.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Embedding visualization
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 After listing 2.4 is done and gradient descent has been executed'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 2.4, this code is a successful but very sparse embedding
    representation that we trained from our CBoW model. Getting those semantic representations
    (embeddings) to be denser is the main place we can see improvement in this field,
    although many successful experiments have been run where denser semantic meaning
    has been supplanted with greater pragmatic context through instruct and different
    thought-chaining techniques. We will address chain of thought (CoT) and other
    techniques later. For now, let’s pivot to discussing why our continuous embedding
    technique can even be successful, given that frequency-based models are characteristically
    difficult to correlate with reality. All of this started with the MLP more than
    half a century ago.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-4.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 A visualization technique for word embeddings. Visualizing embeddings
    can be important for model explainability.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 2.2.6  Multilayer perceptrons
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLPs are the embodiment of the sentiment, “Machines are really good at doing
    one thing, so I wish we could just use a bunch of machines that are really good
    at the one thing to make one that’s good at a lot of things.” Every weight and
    bias in the neural network of the MLP is good at doing one thing, which could
    be detecting one or more features. So we bind a bunch of them together to detect
    larger, more complex features. MLPs serve as the primary building block in most
    neural network architectures. The key distinctions between architectures, such
    as convolutional neural networks and recurrent neural networks, mainly arise from
    data loading methods and the handling of tokenized and embedded data as it flows
    through the layers of the model rather than the functionality of individual layers,
    particularly the fully connected layers.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 provides a more dynamic class of neural networks that can have as
    many layers and parameters as deemed necessary for your task. We give a more defined
    and explicit class using PyTorch to give you the tools to implement the MLP in
    whatever way you’d like, both from scratch and in a popular framework.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 Multilayer perceptron PyTorch class implementation
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From this code, we can see, as opposed to the CBoW implementation, which had
    two static layers, that this MLP is not static in size until it has been instantiated.
    If you wanted to give this model 1 million layers, you would have to put `num_hidden_layers=
    1000000` when you instantiate the class. However, just because you give a model
    that many parameters doesn’t mean that will make it immediately better. LLMs are
    more than just a lot of layers. Like RNNs and CNNs, the magic of LLMs is in how
    data goes in and moves through the model. To illustrate, let’s look at the RNN
    and one of its variations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.7 Recurrent neural networks and long short-term memory networks
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs are a class of neural networks designed to analyze sequences based on the
    weaknesses in previous language modeling techniques. A sequence can be thought
    of as an ordered array, where the sum of the whole array changes value if any
    of the parts are moved around. The logic goes that if language is presented in
    a sequence, then maybe it should be processed in a sequence instead of one token
    at a time. RNNs accomplish this by using logic we’ve seen before, both in MLPs
    and Markov chains, where an internal state or memory is referred to when new inputs
    are processed and by creating cycles when connections between nodes are detected
    as useful.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: In fully recurrent networks, like the one in listing 2.7, all nodes start out
    initially connected to all subsequent nodes, but those connections can be set
    to zero to simulate them breaking if they are not useful. This solves one of the
    biggest problems that earlier models suffered from, static input size, and enables
    an RNN and its variants to process variable length inputs. Unfortunately, longer
    sequences create a new problem. Because each neuron in the network connects to
    subsequent neurons, longer sequences create smaller changes to the overall sum,
    making the gradients smaller until they eventually vanish, even with important
    words; this is called a vanishing gradient. Other problems exist too, such as
    exploding and diminishing gradients.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s consider the following sentences with the task sentiment
    analysis: “I loved the movie last night” and “The movie I went to see last night
    was the very best I had ever expected to see.” These sentences can be considered
    semantically similar, even if they aren’t exactly the same. When moving through
    an RNN, each word in the first sentence is worth more, and the consequence is
    that the first sentence has a higher positive rating than the second sentence
    just because the first sentence is shorter. The inverse is also true: exploding
    gradients are a consequence of this sequence processing, which makes training
    deep RNNs difficult.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, LSTMs, a type of RNN, use memory cells and gating mechanisms
    to process sequences of variable length but without the problem of comprehending
    longer and shorter sequences differently. Anticipating multilingual scenarios
    and understanding that people don’t think about language in only one direction,
    LSTMs can also process sequences bidirectionally by concatenating the outputs
    of two RNNs, one reading the sequence from left to right and the other from right
    to left. This bidirectionality improves results, allowing information to be seen
    and remembered even after thousands of tokens have passed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.7, we give classes for both an RNN and an LSTM. In the code in
    the repo associated with this book ([https://github.com/IMJONEZZ/LLMs-in-Production](https://github.com/IMJONEZZ/LLMs-in-Production)),
    you can see the results of training both the RNN and LSTM. The takeaway is that
    the LSTM achieves better accuracy on both training and validation sets in half
    as many epochs (25 versus 50 with RNN). One of the innovations to note is that
    the packed embeddings utilize padding to extend all variable-length sequences
    to the maximum length. Thus, LSTMs can process input of any length as long as
    it is shorter than the maximum. To set up the LSTM effectively, we’ll do some
    classical NLP on the dataset (a Twitter sentiment analysis dataset). That workflow
    will tokenize with the Natural Language Toolkit Regex. It looks for words and
    nothing else, passing into a spacy lemmatizer to get a list of lists containing
    only the base unconjugated forms of words.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.7 RNN and LSTM PyTorch class implementations
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Creates our corpus for training and performs some classic NLP preprocessing'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Embeddings are needed to give semantic value to the inputs of an LSTM.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Usually should be a power of 2 because it’s the easiest for computer memory'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '#4 You''ve got to determine some labels for whatever you''re training on.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Looking at our classes and instantiations, you should see that the LSTM is not
    vastly different from the RNN. The only difference is that the `init` input variables
    are `n_layers` (for convenience, you can also specify it with RNNs), `bidirectional`,
    and `dropout`. `bidirectional` allows LSTMs to look ahead in sequences to help
    with meaning and context. It also helps immensely with multilingual scenarios,
    as left-to-right languages like English are not the only format for orthography.
    `dropout`, another huge innovation, changes the paradigm of overfitting from being
    data dependent and helps the model not overfit by turning off random nodes layer
    by layer during training to force all nodes not to correlate with each other and
    preventing complex co-adaptations. The only difference in the out-of-model parameters
    is that the optimizer used for an RNN is stochastic gradient descent (SGD), like
    our CBoW; the LSTM uses Adam (although either could use any, depending on performance,
    including AdamW). Next, we define our training loop and train the LSTM. Compare
    this training loop to the one defined in listing 2.4 in the `gradient_descent`
    function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: One of the amazing things demonstrated in the code here is how much quicker
    the LSTM can learn compared to previous model iterations, thanks to both `bidirectionality`
    and `dropout`. Although the previous models train faster than the LSTM, they take
    hundreds of epochs to get the same performance as an LSTM in just 25 epochs. As
    its name implies, the performance on the validation set adds validity to the architecture,
    performing inference during training on examples it has not trained on and keeping
    accuracy fairly close to the training set.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The problems with these models are not as pronounced, manifesting primarily
    as being incredibly resource-heavy, especially when applied to longer, more detail-oriented
    problems like healthcare and law. Despite the incredible advantages of `dropout`
    and `bidirectional` processing, they both at least double the amount of processing
    power required to train. So while inference ends up being only 2 to 3 times as
    expensive as an MLP of the same size, training becomes 10 to 12 times as expensive.
    That is, `dropout` and `bidirectional` solve exploding gradients nicely but explode
    the compute required to train. To combat this problem, a shortcut was devised
    and implemented that allows any model, including an LSTM, to figure out which
    parts of a sequence are the most influential and which parts can be safely ignored,
    known as *attention*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.8 Attention
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention is a mathematical shortcut that gives the model a mechanism for solving
    larger context windows faster by telling the model through an emergent mathematical
    formula which parts of an input to consider and how much. Attention is based upon
    an upgraded version of a dictionary, where instead of just key–value pairs, a
    contextual query is added. Simply know that the following code is the big differentiator
    between older NLP techniques and more modern ones.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Attention solves the slowness of training LSTMs yet keeps high performance on
    a low number of epochs. There are multiple types of attention as well. The dot
    product attention method captures the relationships between each word (or embedding)
    in your query and every word in your key. When queries and keys are part of the
    same sentences, this is known as *bi-directional self-attention*. However, in
    certain cases, it is more suitable to only focus on words that precede the current
    one. This type of attention, especially when queries and keys come from the same
    sentences, is referred to as *causal attention*. Language modeling further improves
    by masking parts of a sequence and forcing the model to guess what should be behind
    the mask. The functions in the following listing demonstrate both dot product
    attention and masked attention.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.8 Multihead attention implementation
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Step 1: Input: three inputs, d_model=4'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Step 2: Weights three dimensions x d_model=4'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Step 3: Matrix multiplication to obtain Q,K,V; query: x * w_query; key:
    x * w_key; value: x * w_value'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Step 4: Scaled attention scores; square root of the dimensions'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Step 5: Scaled softmax attention scores for each vector'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Step 6: Attention value obtained by score1/k_d * V'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Step 7: Sums the results to create the first line of the output matrix'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Step 8: Steps 1 to 7 for inputs 1 to 3; because this is just a demo, we’ll
    do a random matrix of the right dimensions.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Step 9: We train all eight heads of the attention sublayer using steps 1
    to 7.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Step 10: Concatenates heads 1 to 8 to get the original 8 × 64 output dimension
    of the model'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '#11 This function performs all of these steps.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '#12 This function performs the previous steps but adds causality in masking.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: In the full implementation of attention, you may have noticed some terminology
    you’re familiar with—namely `Key` and `Value`, but you may not have been introduced
    to `Query` before. `Key` and `Value` pairs are familiar because of dictionaries
    and lookup tables, where we map a set of keys to an array of values. `Query` should
    feel intuitive as a sort of search for retrieval. The `Query` is compared to the
    `Key`s from which a `Value` is retrieved in a normal operation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: In attention, the `Query` and `Key`s undergo dot product similarity comparison
    to obtain an attention score, which is later multiplied by the `Value` to get
    an ultimate score for how much attention the model should pay to that portion
    of the sequence. This can get more complex, depending upon your model’s architecture,
    because both encoder and decoder sequence lengths have to be accounted for, but
    suffice it to say for now that the most efficient way to model in this space is
    to project all input sources into a common space and compare using dot product
    for efficiency.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'This code explanation was a bit more math-heavy than the previous examples,
    but it is needed to illustrate the concept. The math behind attention is truly
    innovative and has rocketed the field forward. Unfortunately, even with the advantages
    attention brings to the process of sequence modeling, with LSTMs and RNNs, there
    were still problems with speed and memory size. You may notice from the code and
    the math that a square root is taken, meaning that attention, as we use it, is
    quadratic. Various techniques, including subquadratics like Hyena and the Recurrent
    Memory Transformer (RMT, basically an RNN combined with a transformer), have been
    developed to combat these problems, which we will cover in more detail later.
    For now, let’s move on to the ultimate application of attention: the transformer.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Attention is all you need
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the seminal paper, “Attention Is All You Need,”[¹](#footnote-250) Vaswani
    et al. take the mathematical shortcut several steps further, positing that for
    performance, absolutely no recurrence (the “R” in RNN) or any convolutions are
    needed at all.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  We don’t go over convolutions because they aren’t good for NLP, but they
    are popular, especially in computer vision.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Vaswani et al. opted to use only attention and specify where Q, K,
    and V were taken from much more carefully. We’ll dive into this presently. In
    our review of this diverse range of NLP techniques, we have observed their evolution
    over time and the ways in which each approach has sought to improve upon its predecessors.
    From rule-based methods to statistical models and neural networks, the field has
    continually strived for more efficient and accurate ways to process and understand
    natural language.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we turn our attention to a groundbreaking innovation that has revolutionized
    the field of NLP: the transformer architecture. In the following section, we will
    explore the key concepts and mechanisms that underpin transformers and how they
    have enabled the development of state-of-the-art language models that surpass
    the performance of previous techniques. We will also discuss the effect of transformers
    on the broader NLP landscape and consider the potential for further advancements
    in this exciting area of research.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Encoders
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encoders are the first half of a full transformer model, excelling in the areas
    of classification and feature engineering. Vaswani et al. figured out that after
    the embedding layer inside the encoder, any additional transformations done to
    the tensors could end up harming their ability to be compared “semantically,”
    which was the point of the embedding layer. These models rely heavily upon self-attention
    and clever positional encoding to manipulate those vectors without significantly
    decreasing the similarity expressed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Again, a key characteristic of embeddings is that they are vector representations
    of data—in our case, tokens. Tokens are whatever you pick to represent language.
    We recommend subwords as a general rule, but you will get a feel for where and
    which types of tokens work well. Consider the sentence, “The cat in the hat rapidly
    leapt above the red fox and the brown unmotivated dog.” “Red” and “brown” are
    semantically similar, and both are similarly represented after the embedding layer.
    However, they fall on positions 10 and 14, respectively, in the utterance, assuming
    that we’re tokenizing by word. Therefore, the positional encoding puts distance
    between them, also adding the ability to distinguish between the same tokens at
    different positions in an utterance. However, once the sine and cosine functions
    are applied, it brings their meaning back to only a little further apart than
    they were after the encoding, and this encoding mechanism scales brilliantly with
    recurrence and more data. To illustrate, let’s say there was a 99% cosine similarity
    between [red] and [brown] after embedding. Encoding would drastically reduce that
    to around 85% to 86% similarity. Applying sine and cosine methodologies as described
    brings their similarity back up to around 96%.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: BERT was one of the first architectures after Vaswani et al.’s original paper
    and is an example of encoder-only transformers. BERT is such an incredibly powerful
    model architecture, given how small it is, that it is still used in production
    systems today. BERT was the first encoder-only transformer to surge in popularity,
    showcasing that performing continuous or sequential (they’re the same) modeling
    using a transformer results in much better embeddings than Word2Vec. We can see
    that these embeddings are better because they can be very quickly applied to new
    tasks and data with minimal training, with human-preferred results versus Word2Vec
    embeddings. For a while, most people were using BERT-based models for few-shot
    learning tasks on smaller datasets. BERT puts state-of-the-art performance within
    arm’s reach for most researchers and businesses with minimal effort required.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-5.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 An encoder visualized. Encoders are the first half of the full transformer
    architecture and excel in natural language understanding tasks like classification
    or named entity recognition. Encoder models improve upon previous designs by not
    requiring priors or recurrence and using clever positional encoding and multihead
    attention to create a vector embedding of each token.
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The strengths of encoders (visualized in figure 2.5) include the following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Classification and hierarchical tasks showcasing understanding
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blazing fast, considering the long-range dependency modeling
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Builds off of known models, CBoW in embedding, MLP in feed forward, etc.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encoders weaknesses include the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: As suggested, requires lots of data (although less than RNNs) to be effective
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even more complex architecture
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.3.2 Decoders
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decoder models, as shown in figure 2.6, are larger versions of encoders that
    have two multihead attention blocks and three sum and normalize layers in their
    base form. They are the second half of a transformer behind an encoder. Decoders
    are very good at masked language modeling and learning and applying syntax very
    quickly, leading to the almost immediate idea that decoder-only models are needed
    to achieve artificial general intelligence. A useful reduction of encoder versus
    decoder tasks is that encoders excel in natural language understanding (NLU) tasks,
    while decoders excel in natural language generation (NLG) tasks. An example of
    decoder-only transformer architectures is the Generative Pre-trained Transformer
    (GPT) family of models. These models follow the logic of transformational generative
    grammar being completely syntax based, allowing for infinite generation of all
    possible sentences in a language (see appendix A).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-6.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 A decoder visualized. Decoders are the second half of a full transformer,
    and they excel at NLG tasks like chatbots and storytelling. Decoders improve upon
    previous architectures in the same way as encoders, but they shift their output
    one space to the right for next-word generation to help utilize the advantages
    of multihead self-attention.
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The strengths of decoders include the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Generates the next token in a sequence (shifted right means taking already-generated
    tokens into account)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Builds off of both known models and encoders
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be streamed during generation for great UX
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Their weaknesses include the following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Syntax-only models can often struggle to insert the expected or intended meaning
    (see all “I forced an AI to watch 1000 hours of x and generated” memes from 2018–present).
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucinations.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.3.3 Transformers
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The full transformer architecture takes advantage of both encoders and decoders,
    passing the understanding of the encoder into the second multihead attention block
    of the decoder before giving output. As each piece of the transformer has a specialty
    in either understanding or generation, it should feel intuitive for the full product
    to be best at conditional generation tasks like translation or summarization,
    where some level of understanding is required before generation occurs. Encoders
    are geared toward processing input at a high level, and decoders focus more on
    generating coherent output. The full transformer architecture can successfully
    understand the data and then generate the output based on that understanding,
    as shown in figure 2.7\. The Text-To-Text Transfer Transformer (T5) family of
    models is an example of transformers.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-7.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 A full transformer visualized. A full transformer combines the encoder
    and the decoder and does well on all of the tasks of each, as well as conditional
    generation tasks such as summarization and translation. Because transformers are
    bulkier and slower than each of their halves, researchers and businesses have
    generally opted to use those halves over the whole transformer.
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: NOTE  Transformer models have an advantage in that they are built around the
    parallelization of inputs, which adds speed that LSTMs can’t currently replicate.
    If LSTMs ever get to a point where they can run as quickly as transformers, they
    may become competitive in the state-of-the-art field.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'The strengths of a transformer are as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Includes both an encoder and decoder, so it’s good at everything they are good
    at
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly parallelized for speed and efficiency
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weaknesses include the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Memory intensive, but still less than LSTMs of the same size
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires large amounts of data and VRAM for training
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you’ve probably noticed, most of the models we’ve discussed aren’t at all
    linguistically focused, being heavily syntax-focused, if they even attempt to
    model real language at all. Models, even state-of-the-art transformers, only have
    semantic approximations—no pragmatics, no phonetics—and only really utilize a
    mathematical model of morphology during tokenization without context. This doesn’t
    mean the models can’t learn these, nor does it mean that, for example, transformers
    can’t take audio as an input; it just means that the average usage doesn’t. With
    this in mind, it is nothing short of a miracle that they work as well as they
    do, and they really should be appreciated for what they can do.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we’ve attempted to highlight the current limitations in models, and
    we will dive into where to improve upon them in the remainder of this book. One
    such route is one that’s already been, and is still being, explored to great success:
    transfer learning and finetuning large foundational models. This technique came
    about soon after BERT’s initial release. Researchers discovered that although
    BERT generally performed well on a large number of tasks, if they wanted it to
    perform better on a particular task or data domain, they simply needed to retrain
    the model on data representative of the task or domain but not from scratch. Given
    all of the pretrained weights BERT learned while creating the semantic approximation
    embeddings on a much larger dataset, significantly less data is required to get
    state-of-the-art performance on the portion you need. We’ve seen this with BERT
    and the GPT family of models as they’ve come out, and now we’re seeing it again
    to solve exactly the challenges we discussed: semantic approximation coverage,
    domain expertise, and data availability.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Really big transformers
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enter LLMs. Since their introduction, transformer-based models have continued
    to get larger and larger, not just in their size and number of parameters but
    also in the size and length of their training datasets and training cycles. If
    you studied machine learning or deep learning during the 2010s, you likely heard
    the moniker, “Adding more layers doesn’t make the model better.” LLMs prove this
    both wrong and right—wrong because their performance is unparalleled, often matching
    smaller models that have been meticulously finetuned on a particular domain and
    dataset, even those trained on proprietary data, and right because of the challenges
    that come with both training and deploying LLMs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: One of the major differences between LLMs and language models involves transfer
    learning and finetuning. Like previous language models, LLMs are pretrained on
    massive text corpora, enabling them to learn general language features and representations
    that can be finetuned for specific tasks. Because LLMs are so massive and their
    training datasets are so large, they are able to achieve better performance with
    less labeled data, which was a significant limitation of earlier language models.
    Often, you can finetune an LLM to do highly specialized tasks with only a dozen
    or so examples.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: However, what makes LLMs so powerful and has opened the door to widespread business
    use cases is their ability to do specialized tasks using simple prompting without
    any finetuning. Just give a few examples of what you want in your query, and the
    LLM can produce results. Training an LLM on a smaller set of labeled data is called
    few-shot prompting. It’s referred to as one-shot prompting when only one example
    is given and zero-shot when the task is totally novel. LLMs, especially those
    trained using reinforcement learning from human feedback and prompt engineering
    methodologies, can perform few-shot learning, where they can generalize and solve
    tasks with only a few examples, at a whole new level. This ability is a significant
    advancement over earlier models that required extensive finetuning or large amounts
    of labeled data for each specific task.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: LMs previously have shown promise in the few and zero-shot learning domains,
    and LLMs have proven that promise to be true. As models have gotten larger, we
    find they are capable of accomplishing tasks smaller models can’t. We call this
    *emergent behavior*.[²](#footnote-251) Figure 2.8 illustrates eight different
    tasks previous language models couldn’t perform better than at random, and then
    once the models got large enough, they could.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-8.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 Examples of LLMs demonstrating emergent behaviors when given few-shot
    prompting tasks after the model scale reaches a certain size
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs also have demonstrably great zero-shot capabilities due to their vast parameter
    sizes, which is the main reason for their popularity and viability in the business
    world. LLMs also exhibit improved handling of ambiguity due to their large size
    and capacity. They are better at disambiguating words with multiple meanings and
    understanding the nuances of language, resulting in more accurate predictions
    and responses. This improvement isn’t because of better ability or architecture,
    as they share their architecture with smaller transformers, but because they have
    vastly more examples of how people generally disambiguate. LLMs, therefore, respond
    with the same disambiguation as is generally represented in the dataset. Thanks
    to the diverseness of the text data on which LLMs are trained, they exhibit increased
    robustness in handling various input styles, noisy text, and grammatical errors.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Another key difference between LLMs and language models is input space. A larger
    input space is important since it makes few-shot prompting tasks that much more
    viable. Many LLMs have max input sizes of 8,000+ tokens (originally 32K, GPT-4
    has sported 128K since November 2023), and while all the previously discussed
    models could also have input spaces that high, they generally don’t. We have recently
    seen a boom in this field, with techniques like Recurrent Memory Transformer (RMT)
    allowing 1M+ token context spaces, which rocket LLMs even more toward proving
    that bigger models are always better. LLMs are designed to capture long-range
    dependencies within text, allowing them to understand context more effectively
    than their predecessors. This improved understanding enables LLMs to generate
    more coherent and contextually relevant responses in tasks like machine translation,
    summarization, and conversational AI.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have revolutionized NLP by offering powerful solutions to problems that
    were challenging for earlier language models. They bring substantial improvements
    in contextual understanding, transfer learning, and few-shot learning. As the
    field of NLP continues to evolve, researchers are actively working to maximize
    the benefits of LLMs while mitigating all potential risks. Because a better way
    to approximate semantics hasn’t been found, they make bigger and more dimensional
    approximations. Because a good way of storing pragmatic context hasn’t been found,
    LLMs often allow inserting context into the prompt directly, into a part of the
    input set aside for context, or even through sharing databases with the LLM at
    inference. This capability doesn’t create pragmatics or a pragmatic system within
    the models, in the same way that embeddings don’t create semantics, but it allows
    the model to correctly generate syntax that mimics how humans respond to those
    pragmatic and semantic stimuli. Phonetics is a place where LLMs could likely make
    gigantic strides, either as completely text-free models or as a text-phonetic
    hybrid model, maybe utilizing the IPA in addition to or instead of text. It is
    exciting to consider the possible developments that we are watching sweep across
    this field right now.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you should have a pretty good understanding of what LLMs are
    and some key principles of linguistics that will come in handy when putting LLMs
    in production. You should now be able to start reasoning about what type of products
    will be easier or harder to build. Consider figure 2.9: tasks in the lower left-hand
    corner, like writing assistants and chatbots, are LLMs’ bread and butter. Text
    generation based on a little context from a prompt is a strictly syntax-based
    problem; with a large enough model trained on enough data, we can do this pretty
    easily. A shopping assistant is pretty similar and rather easy to build as well;
    we are just missing pragmatics. The assistant needs to know a bit more about the
    world, such as products, stores, and prices. With a little engineering, we can
    add this information to a database and give this context to the model through
    prompting.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-9.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 How difficult or easy certain tasks are for LLMs and what approaches
    to take to solve them
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: On the other end, consider a chess bot. LLMs *can* play chess, but they aren’t
    any good. They have been trained on chess games and understand that E4 is a common
    first move, but their understanding is completely syntactical. LLMs only understand
    that the text they generate should contain a letter between A and H and a number
    between 1 and 8\. Like the shopping assistant, they are missing pragmatics and
    don’t have a clear model of the game of chess. In addition, they are also missing
    semantics. Encoders might help us understand that the words “king” and “queen”
    are similar, but they don’t help us understand that E4 is a great move one moment
    for one player and that same E4 move is a terrible move the very next moment for
    a different player. LLMs also lack knowledge based on phonetics and morphology
    for chess, although they are not as important in this case. Either way, we hope
    this exercise will better inform you and your team on your next project.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have amazing benefits, but with all of these capabilities come some limitations.
    Foundational LLMs require vast computational resources for training, making them
    less accessible for individual researchers and smaller organizations. This problem
    is being remedied with techniques we’ll talk about throughout the book, like quantization,
    textual embeddings, low-rank adaptation, parameter-efficient finetuning, and graph
    optimization. Still, foundation models are currently solidly outside the average
    individual’s ability to train effectively. Beyond that, there are concerns that
    the energy consumption associated with training LLMs could have significant environmental
    effects and cause problems associated with sustainability. These problems are
    complex and largely out of the scope of this book, but we would be remiss not
    to bring them up.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, since LLMs are trained on large-scale datasets containing
    real-world text, they may learn and perpetuate biases present in the data, leading
    to ethical concerns because real-world people don’t censor themselves to provide
    optimal unbiased data. Also, knowing much about what data you’re training on is
    not a widespread practice. For example, if you ask a text-to-image diffusion LLM
    to generate 1,000 images of “leader,” 99% of the images feature men, and 95% of
    the images feature people with white skin. The concern here isn’t that men or
    white people shouldn’t be depicted as leaders, but that the model isn’t representing
    the world accurately, and it’s showing.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, more nuanced biases are brought out. For example, in the Midjourney
    example in figure 2.10, the model, without being prompted (the only prompt given
    was the word “leader”), changed the popular feminist icon Rosie the Riveter to
    a man. The model didn’t think about this change; it just determined during its
    sampling steps that the prompt “leader” had more male-looking depictions in the
    training set. Many people will argue about what “good” and “bad” mean in this
    context, and instead of going for a moral ought, we’ll talk about what accuracy
    means. LLMs are trained on a plethora of data with the purpose of returning the
    most accurate representations possible. When they cannot return accurate representations,
    especially with their heightened abilities to disambiguate, we can view that as
    a bias that harms the model’s ability to fulfill its purpose. Later, we will discuss
    techniques to combat harmful bias to allow you, as an LLM creator, to get the
    exact outputs you intend and minimize the number of outputs you do not intend.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-10.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 Midjourney 5, which is, at the time of this writing, the most popular
    text2img model on the market, when prompted with only one token, “leader” (left),
    changed a well-known popular feminist icon, Rosie the Riveter, into a male depiction.
    ChatGPT (right) writes a function to place you in your job based on race, gender,
    and age. These are examples of unintended outputs.
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Alright, we’ve been building up to this moment the entire chapter. Let’s go
    ahead and run our first LLM! In listing 2.9, we download the Bloom model, one
    of the first open source LLMs to be created, and generate text! We are using Hugging
    Face’s Transformers library, which takes care of all the heavy lifting for us.
    Very exciting stuff!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.9 Running our first LLM
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Did you try to run it?!? If you did, you probably just crashed your laptop.
    Oopsie! Forgive me for a little harmless MLOps hazing, but getting some first-hand
    experience on how large these models can get and how difficult they can be to
    run is a helpful experience to have. In the next chapter, we will talk more about
    the difficulties of running LLMs and some of the tools you need to run this code.
    If you don’t want to wait and would like to get a similar but much smaller LLM
    running, change the model name to `"bigscience/bloom-3b"`, and run it again. It
    should work just fine this time on most hardware.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: All in all, LLMs are an amazing technology that allows our imaginations to run
    wild with possibility, and deservedly so. The number-one use case for considering
    an LLM over a smaller language model is when few-shot capabilities come into play
    for whoever the model will be helping, such as a CEO when raising funds or a software
    engineer when writing code. LLMs have these abilities precisely because of their
    size. The larger number of parameters in LLMs directly enables their ability to
    generalize over smaller spaces in larger dimensions. In this chapter, we’ve hit
    the lesser-known side of LLMs, the linguistic and language modeling side. In the
    next chapter, we’ll cover the other half, the MLOps side, where we dive into exactly
    how that large parameter size affects the model and the systems designed to support
    that model and makes it accessible to the customers or employees the model is
    intended for.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The five components of linguistics are phonetics, syntax, semantics, pragmatics,
    and morphology:'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phonetics can be added through a multimodal model that processes audio files
    and is likely to improve LLMs in the future, but current datasets are too small.
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Syntax is what current models are good at.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantics is added through the embedding layer.
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pragmatics can be added through engineering efforts.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Morphology is added in the tokenization layer.
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Language does not necessarily correlate with reality. Understanding the process
    people use to create meaning outside of reality is useful in training meaningful
    (to people) models.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proper tokenization can be a major hurdle due to too many `<UNK>` tokens, especially
    when it comes to specialized problems like code or math.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilingual processing has always outperformed monolingual processing, even
    on monolingual tasks without models.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each language model type in sequence shows a natural and organic growth of the
    LLM field as more and more linguistic concepts are added that make the models
    better.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language modeling has seen an exponential increase in efficacy, correlating
    to how linguistics-focused the modeling has been.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention is a mathematical shortcut for solving larger context windows faster
    and is the backbone of modern architectures—encoders, decoders, and transformers:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoders improve the semantic approximations in embeddings.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoders are best at text generation.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers combine the two.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger models demonstrate emergent behavior, suddenly being able to accomplish
    tasks they couldn’t before.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) Vaswani et al., 2017, Attention Is All You Need,”
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) J. Wei et al., “Emergent abilities of large language
    models,” Transactions on Machine Learning Research, Aug. 2022, [https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
