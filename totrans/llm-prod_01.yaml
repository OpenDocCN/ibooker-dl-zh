- en: '2 Large language models: A deep dive into language modeling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linguistic background for understanding meaning and interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparative study of language modeling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention and the transformer architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large language models both fit into and build upon these histories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you know the enemy and know yourself, you need not fear the result of a hundred
    battles.—Sun Tzu
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This chapter delves into linguistics as it relates to the development of LLMs,
    exploring the foundations of semiotics, linguistic features, and the progression
    of language modeling techniques that have shaped the field of natural language
    processing (NLP). We will begin by studying the basics of linguistics and its
    relevance to LLMs, highlighting key concepts such as syntax, semantics, and pragmatics
    that form the basis of natural language and play a crucial role in the functioning
    of LLMs. We will delve into semiotics, the study of signs and symbols, and explore
    how its principles have informed the design and interpretation of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: We will then trace the evolution of language modeling techniques, providing
    an overview of early approaches, including N-grams, naive Bayes classifiers, and
    neural network-based methods such as multilayer perceptrons (MLPs), recurrent
    neural networks (RNNs), and long short-term memory (LSTM) networks. We will also
    discuss the groundbreaking shift to transformer-based models that laid the foundation
    for the emergence of LLMs, which are really just big transformer-based models.
    Finally, we will introduce LLMs and their distinguishing features, discussing
    how they have built upon and surpassed earlier language modeling techniques to
    revolutionize the field of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: This book is about LLMs in production. We firmly believe that if you want to
    turn an LLM into an actual product, understanding the technology better will improve
    your results and save you from making costly and time-consuming mistakes. Any
    engineer can figure out how to lug a big model into production and throw a ton
    of resources at it to make it run, but that brute-force strategy completely misses
    the lessons people have already learned trying to do the same thing before, which
    is what we are trying to solve with LLMs in the first place. Having a grasp of
    these fundamentals will better prepare you for the tricky parts, the gotchas,
    and the edge cases you are going to run into when working with LLMs. By understanding
    the context in which LLMs emerged, we can appreciate their transformative impact
    on NLP and how to enable them to create a myriad of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Language modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It would be a great disservice to address LLMs in any depth without first addressing
    language. To that end, we will start with a brief but comprehensive overview of
    language modeling, focusing on the lessons that can help us with modern LLMs.
    Let’s first discuss levels of abstraction, as this will help us garner an appreciation
    for language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Language, as a concept, is an abstraction of the feelings and thoughts that
    occur to us in our heads. Feelings come first in the process of generating language,
    but that’s not the only thing we’re trying to highlight here. We’re also looking
    at language as being unable to capture the full extent of what we are able to
    feel, which is why we’re calling it an abstraction. It moves away from the source
    material and loses information. Math is an abstraction of language, focusing on
    logic and provability, but as any mathematician will tell you, it is a subset
    of language used to describe and define in an organized and logical way. From
    math comes another abstraction, the language of binary, a base-2 system of numerical
    notation consisting of either on or off.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a commentary on usefulness, as binary and math are just as useful
    as the lower-level aspects of language, nor is it commenting on order, as we said
    before. With math and binary, the order coincidentally lines up with the layer
    of abstraction. Computers can’t do anything on their own and need to take commands
    to be useful. Binary, unfortunately, ends up taking too long for humans to communicate
    important things in it, so binary was also abstracted to assembly, a more human-comprehensible
    language for communicating with computers. This was further abstracted to the
    high-level assembly language C, which has been even further abstracted to object-oriented
    languages like Python or Java (which one doesn’t matter—we’re just measuring distance
    from binary). The flow we just discussed is outlined in figure 2.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 We compare cognitive layers of abstraction to programming layers
    of abstraction down to the logical binary abstraction. Python doesn’t come from
    C, nor does it compile into C. Python is, however, another layer of abstraction
    distant from binary. Language follows a similar path. Each layer of abstraction
    creates a potential point of failure. There are also several layers of abstraction
    to creating a model, and each is important in seeing the full path from our feelings
    to a working model.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is obviously a reduction; however, it’s useful to understand that the feelings
    you have in your head are the same number of abstractions away from binary, the
    language the computer actually reads, as the languages most people use to program
    in. Some people might argue that there are more steps between Python and binary,
    such as compilers or using assembly to support the C language, and that’s true,
    but there are more steps on the language side too, such as morphology, syntax,
    logic, dialogue, and agreement.
  prefs: []
  type: TYPE_NORMAL
- en: This reduction can help us understand how difficult the process of getting what
    we want to be understood by an LLM actually is and even help us understand language
    modeling techniques better. We focus on binary here to illustrate that there are
    a similar number of abstract layers to get from an idea you have or from one of
    our code samples to a working model. Like the children’s telephone game where
    participants whisper into each other’s ears, each abstraction layer creates a
    disconnect point or barrier where mistakes can be made.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 is also meant not only to illustrate the difficulty in creating reliable
    code and language input but also to draw attention to how important the intermediary
    abstraction steps, like tokenization and embeddings, are for the model itself.
    Even if you have perfectly reliable code and perfectly expressed ideas, the meaning
    may be fumbled by one of those processes before it ever reaches the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will try to help you understand what you can do to reduce
    the risks of these failure points, whether on the language, coding, or modeling
    side. Unfortunately, it’s a bit tricky to strike a balance between giving you
    too much linguistics that doesn’t immediately matter for the task at hand versus
    giving you too much technical knowledge that, while useful, doesn’t help you develop
    an intuition for language modeling as a practice. With this in mind, you should
    know that linguistics can be traced back thousands of years in our history, and
    there’s lots to learn from it. We’ve included a brief overview of how language
    modeling has progressed over time in appendix A, and we encourage you to take
    a look.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with our focus on the building blocks that constitute language itself.
    We expect our readers to have at least attempted language modeling before and
    to have heard of libraries like PyTorch and TensorFlow, but we do not expect most
    of our readers to have considered the language side of things before. By understanding
    the essential features that make up language, we can better appreciate the complexities
    involved in creating effective language models and how these features interact
    with one another to form the intricate web of communication that connects us all.
    In the following section, we will examine the various components of language,
    such as phonetics, pragmatics, morphology, syntax, and semantics, as well as the
    role they play in shaping our understanding and usage of languages around the
    world. Let’s take a moment to explore how we currently understand language, along
    with the challenges we face that LLMs are meant to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Linguistic features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our current understanding of language is that language is made up of at least
    five parts: phonetics, syntax, semantics, pragmatics, and morphology. Each of
    these portions contributes significantly to the overall experience and meaning
    being ingested by the listener in any conversation. Not all of our communication
    uses all of these forms; for example, the book you’re currently reading is devoid
    of phonetics, which is one of the reasons why so many people think text messages
    are unsuited for more serious or complex conversations. Let’s work through each
    of these five parts to figure out how to present them to a language model for
    a full range of communicative power.'
  prefs: []
  type: TYPE_NORMAL
- en: Phonetics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Phonetics is probably the easiest for a language model to ingest, as it involves
    the actual sound of the language. This is where accent manifests and deals with
    the production and perception of speech sounds, with phonology focusing on the
    way sounds are organized within a particular language system. Similarly to computer
    vision, while a sound isn’t necessarily easy to deal with as a whole, there’s
    no ambiguity in how to parse, vectorize, or tokenize the actual sound waves. They
    have a numerical value attached to each part, the crest, the trough, and the slope
    during each frequency cycle. It is vastly easier than text to tokenize and process
    by a computer while being no less complex.
  prefs: []
  type: TYPE_NORMAL
- en: Sound inherently also contains more encoded meaning than text. For example,
    imagine someone saying the words “Yeah, right,” to you. It could be sarcastic,
    or it could be congratulatory, depending on the tone—and English isn’t even tonal!
    Phonetics, unfortunately, doesn’t have terabyte-sized datasets commonly associated
    with it, and performing data acquisition and cleaning on phonetic data, especially
    on the scale needed to train an LLM, is difficult at best. In an alternate world
    where audio data was more prevalent than text data and took up a smaller memory
    footprint, phonetic-based or phonetic-aware LLMs would be much more sophisticated,
    and creating that world is a solid goal to work toward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anticipating this phonetical problem, a system was created in 1888 called the
    International Phonetic Alphabet (IPA). It has been revised in both the 20th and
    21st centuries to be more concise, more consistent, and clearer and could be a
    way to insert phonetic awareness into text data. IPA functions as an internationally
    standardized version of every language’s sound profile. A sound profile is the
    set of sounds that a language uses; for example, in English, we never have the
    /ʃ/ (she, shirt, sh) next to the /v/ sound. IPA is used to write sounds, rather
    than writing an alphabet or logograms, as most languages do. For example, you
    could describe how to pronounce the word “cat” using these symbols: /k/, /æ/,
    and /t/. Of course, that’s a very simplified version of it, but for models, it
    doesn’t have to be. You can describe tone and aspiration as well. This could be
    a happy medium between text and speech, capturing some phonetic information. Think
    of the phrase “What’s up?” Your pronunciation and tone can drastically change
    how you understand that phrase, sometimes sounding like a friendly “Wazuuuuup?”
    and other times an almost threatening “‘Sup?” which IPA would fully capture. IPA
    isn’t a perfect solution, though; for example, it doesn’t solve the problem of
    replicating tone very well.'
  prefs: []
  type: TYPE_NORMAL
- en: Phonetics is listed first here because it’s the place where LLMs have been applied
    to the least out of all the features and, therefore, has the largest space for
    improvement. Even modern text-to-speech (TTS) and voice-cloning models, for the
    most part, end up converting the sound to a spectrogram and analyzing that image
    rather than incorporating any type of phonetic language modeling. Improving phonetic
    data and representation in LLMs is something to look for as far as research goes
    in the coming months and years.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Syntax is the place where current LLMs are highest-performing, both in parsing
    syntax from the user and in generating their own. Syntax is generally what we
    think of as grammar and word order; it is the study of how words can combine to
    form phrases, clauses, and sentences. Syntax is also the first place language-learning
    programs start to help people acquire new languages, especially based on where
    they are coming from natively. For example, it is important for a native English
    speaker learning Turkish to know that the syntax is completely different, and
    you can often build entire sentences in Turkish that are just one long compound
    word, whereas in English, we never put our subject and verb together into one
    word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Syntax is largely separate from meaning in language, as the famous sentence
    from Noam Chomsky, the so-called father of syntax, demonstrates: “Colorless green
    ideas sleep furiously.” Everything about that sentence is both grammatically correct
    and semantically understandable. The problem isn’t that it doesn’t make sense;
    it’s that it does, and the encoded meanings of those words conflict. This is a
    reduction; however, you can think of all the times LLMs give nonsense answers
    as this phenomenon manifests. Unfortunately, the syntax is also where ambiguity
    is most commonly found. Consider the sentence, “I saw an old man and woman.” Now
    answer this question: Is the woman also old? This is syntactic ambiguity, where
    we aren’t sure whether the modifier “old” applies to all people in the following
    phrase or just the one it immediately precedes. This is less consequential than
    the fact that semantic and pragmatic ambiguity also show up in syntax. Consider
    this sentence: “I saw a man on a hill with a telescope,” and answer these questions:
    Where is the speaker, and what are they doing? Is the speaker on the hill cutting
    a man in half using a telescope? Likely, you didn’t even consider this option
    when you read the sentence because when we interpret syntax, all of our interpretations
    are at least semantically and pragmatically informed. We know from lived experience
    that that interpretation isn’t at all likely, so we throw it out immediately,
    usually without even taking time to process that we’re eliminating it from the
    pool of probable meanings. Single-modality LLMs will always have this problem,
    and multimodal LLMs can (so far) only asymptote toward the solution.'
  prefs: []
  type: TYPE_NORMAL
- en: It shouldn’t take any logical leap to understand why LLMs need to be syntax-aware
    to be high-performing. LLMs that don’t get word order correct or generate nonsense
    aren’t usually described as “good.” LLMs being syntax-dependent has prompted even
    Chomsky to call LLMs “stochastic parrots.” In our opinion, GPT-2 in 2018 was when
    language modeling solved syntax as a completely meaning-independent demonstration,
    and we’ve been happy to see the more recent attempts to combine the syntax that
    GPT-2 outputs so well with encoded and entailed meaning, which we’ll get into
    now.
  prefs: []
  type: TYPE_NORMAL
- en: Semantics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Semantics are the literal encoded meaning of words in utterances, which changes
    at breakneck speed in waves. People automatically optimize semantic meaning by
    only using words they consider meaningful in the current language epoch. If you’ve
    ever created or used an embedding with language models (word2vec, ELMo, BERT,
    MUSE [the E is for embedding], etc.), you’ve used a semantic approximation. Words
    often go through semantic shifts, and while we won’t cover this topic completely
    or go into depth, here are some common ones you may already be familiar with:
    narrowing, a broader meaning to a more specific one; broadening, the inverse of
    narrowing going from a specific meaning to a broad one; and reinterpretations,
    going through whole or partial transformations. These shifts do not have some
    grand logical underpinning. They don’t even have to correlate with reality, nor
    do speakers of a language often consciously think about the changes as they’re
    happening. That doesn’t stop the change from occurring, and in the context of
    language modeling, it doesn’t stop us from having to keep up with that change.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some examples. Narrowing includes “deer,” which in Old and Middle
    English just meant any wild animal, even a bear or a cougar, and now means only
    one kind of forest animal. For broadening, we have “dog,” which used to refer
    to only one canine breed from England and now can be used to refer to any domesticated
    canine. One fun tangent about dog-broadening is in the FromSoft game *Elden Ring,*
    where because of a limited message system between players, “dog” will be used
    to refer to anything from a turtle to a giant spider and literally everything
    in between. For reinterpretation, we can consider “pretty,” which used to mean
    clever or well-crafted, not visually attractive. Another good example is “bikini,”
    which went from referring to a particular atoll to referring to clothing you might
    have worn when visiting that atoll to people acting as if the “bi-” was referring
    to the two-piece structure of the clothing, thus implying the tankini and monokini.
    Based on expert research and decades of study, we can think of language as being
    constantly compared and re-evaluated by native language speakers, out of which
    common patterns emerge. The spread of those patterns is closely studied in sociolinguistics
    and is largely out of the scope of the current purpose but can quickly come into
    scope as localization (l10n) or internationalization (i18n) for LLMs arises as
    a project requirement. Sociolinguistic phenomena such as prestige can help design
    systems that work well for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, so-called semantic embeddings are vectorized versions
    of text that attempt to mimic semantic meaning. Currently, the most popular way
    of doing this is by tokenizing or assigning an arbitrary number in a dictionary
    to each subword in an utterance (think prefixes, suffixes, and morphemes generally),
    applying a continuous language model to increase the dimensionality of each token
    within the vector so that there’s a larger vector representing each index of the
    tokenized vector, and then applying a positional encoding to each of those vectors
    to capture word order. Each subword ends up being compared to other words in the
    larger dictionary based on how it’s used. We’ll show you an example of this later.
    Something to consider when thinking about word embeddings is that they struggle
    to capture the deep, encoded meaning of those tokens, and simply adding more dimensions
    to the embeddings hasn’t shown marked improvement. Evidence that embeddings work
    similarly to humans is that you can apply a distance function to related words
    and see that they are closer together than unrelated words. How to capture and
    represent meaning more completely is another area in which to expect groundbreaking
    research in the coming years and months.
  prefs: []
  type: TYPE_NORMAL
- en: Pragmatics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pragmatics is sometimes omitted from linguistics due to its referent being all
    the nonlinguistic context affecting a listener’s interpretation and the speaker’s
    decision to express things in a certain way. Pragmatics refers in large part to
    dogmas followed in cultures, regions, socio-economic classes, and shared lived
    experiences, which are played off of to take shortcuts in conversations using
    entailment.
  prefs: []
  type: TYPE_NORMAL
- en: If we were to say, “A popular celebrity was just taken into the ICU,” your pragmatic
    interpretation based on lived experience might be to assume that a well-beloved
    person has been badly injured and is now undergoing medical treatment in a well-equipped
    hospital. You may wonder about which celebrity it is, whether they will have to
    pay for the medical bills, or if the injury was self-inflicted, also based on
    your lived experience. None of these things can be inferred directly from the
    text and its encoded meaning by itself. You would need to know that ICU stands
    for a larger set of words and what those words are. You would need to know what
    a hospital is and why someone would need to be taken there instead of going there
    themselves. If any of these feel obvious, good. You live in a society, and your
    pragmatic knowledge of that society overlaps well with the example provided. If
    we share an example from a less-populated society, “Janka got her grand-night
    lashings yesterday; she’s gonna get Peter tomorrow,” you might be left scratching
    your head. If you are, realize this probably looks like how a lot of text data
    ends up looking to an LLM (anthropomorphization acknowledged). For those wondering,
    this sentence comes from Slovak Easter traditions. A lot of meaning here will
    be missed and go unexplained if you are unaccustomed to these particular traditions
    as they stand in that culture. This author personally has had the pleasure of
    trying to explain the Easter Bunny and its obsession with eggs to foreign colleagues
    and enjoyed the satisfaction of looking like I’m off my rocker.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, we can effectively group all out-of-text contexts into
    pragmatics. This means LLMs start without any knowledge of the outside world and
    do not gain it during training. They only gain a knowledge of how humans respond
    to particular pragmatic stimuli. LLMs do not understand social class or race or
    gender or presidential candidates, or anything else that might spark some type
    of emotion in you based on your life experience. Pragmatics isn’t something that
    we expect will be able to be directly incorporated into a model at any point because
    models cannot live in society. Yet we have already seen the benefits of incorporating
    it indirectly through data engineering and curation, prompting mechanisms like
    RAG, and supervised finetuning on instruction datasets. In the future, we expect
    great improvements in incorporating pragmatics into LLMs, but we emphasize that
    it’s an asymptotic solution because language is ultimately still an abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Pragmatic structure gets added, whether you mean to add it or not, as soon as
    you acquire the data you are going to train on. You can think of this type of
    pragmatic structure as bias, not inherently good or bad, but impossible to get
    rid of. Later down the line, you get to pick the types of bias you’d like your
    data to keep by normalizing and curating, augmenting particular underrepresented
    points, and cutting overrepresented or noisy examples. Instruction datasets show
    us how you can harness pragmatic structure in your training data to create incredibly
    useful bias, like biasing your model to answer a question when asked instead of
    attempting to categorize the sentiment of the question.
  prefs: []
  type: TYPE_NORMAL
- en: Pragmatics and context all revolve around entailment. An entailment is a pragmatic
    marker within your data, as opposed to the literal text your dataset contains.
    For example, let’s say you have a model attempting to take an input like “Write
    me a speech about frogs eating soggy socks that doesn’t rhyme and where the first
    letters of each line spell amphibian” and actually follow that instruction. You
    can immediately tell that this input is asking for a lot. The balance for you
    as a data engineer would be to make sure that everything the input is asking for
    is explicitly accounted for in your data. You need examples of speeches, examples
    of what frogs and socks are and how they behave, and examples of acrostic poems.
    If you don’t have them, the model might be able to understand just from whatever
    entailments exist in your dataset, but it’s pretty up in the air. If you go the
    extra mile and keep track of entailed versus explicit information and tasks in
    your dataset, along with data distributions, you’ll have examples to answer, “What
    is the garbage-in resulting in our garbage-out?”
  prefs: []
  type: TYPE_NORMAL
- en: LLMs struggle to pick up on pragmatics, even more so than people, but they do
    pick up on the things that your average standard deviation of people would. They
    can even replicate responses from people outside that standard deviation, but
    pretty inconsistently without the exact right stimulus. That means it’s difficult
    for a model to give you an expert answer on a problem the average person doesn’t
    know without providing the correct bias and entailment during training and in
    the prompt. For example, including “masterpiece” at the beginning of an image-generation
    prompt will elicit different and usually higher-quality generations, but only
    if that distinction was present in the training set and only if you’re asking
    for an image where “masterpiece” is a compliment. Instruction-based datasets attempt
    to manufacture those stimuli during training by asking questions and giving instructions
    that entail representative responses. It is impossible to account for every possible
    situation in training, and you may inadvertently create new types of responses
    from your end users by trying to account for everything. After training, you can
    coax particular information from your model through prompting, which has a skill
    ceiling based on what your data originally entailed.
  prefs: []
  type: TYPE_NORMAL
- en: Morphology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Morphology is the study of word structures and how they are formed from smaller
    units called morphemes. Morphemes are the smallest units of meaning, like the
    “re-” in “redo” or “relearn.” However, not all parts of words are morphemes, such
    as “ra-” in “ration” or “na-” in “nation,” and some can be unexpected, like “helico-”
    as in “helicoid” and “-pter” as in “pterodactyl.”
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how words are constructed helps create better language models
    and parsing algorithms, which are essential for tasks like tokenization. Tokens
    are the basic units used in NLP; they can be words, subwords, characters, or whole
    utterances and do not have to correspond to existing morphemes. People do not
    consciously decide what their units of meaning are going to be, and as such, they
    are often illogical. The effectiveness of a language model can depend on how well
    it can understand and process these tokens. For instance, in tokenization, a model
    needs to store a set of dictionaries to convert between words and their corresponding
    indices. One of these tokens is usually an `/<UNK/>` token, which represents any
    word that the model does not recognize. If this token is used too frequently,
    it can hinder the model’s performance, either because the model’s vocabulary is
    too small or because the tokenizer is not using the right algorithm for the task.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where you want to build a code completion model, but you’re
    using a tokenizer that only recognizes words separated by whitespace, like the
    NLTK `punkt` tokenizer. When it encounters the string `def` `add_two_numbers_together(x,`
    `y):`, it will pass `[def,` `[UNK],` `y]` to the model. This causes the model
    to lose valuable information, not only because it doesn’t recognize the punctuation
    but also because the important part of the function’s purpose is replaced with
    an unknown token due to the tokenizer’s morphological algorithm. A better understanding
    of word structure and the appropriate parsing algorithms is needed to improve
    the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Semiotics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After exploring the fundamental features of language and examining their significance
    in the context of LLMs, it is important to consider the broader perspective of
    meaning-making and interpretation in human communication. Semiotics, the study
    of signs and symbols, offers a valuable lens through which we can better understand
    how people interpret and process language. We will delve into semiotics, examining
    the relationship between signs, signifiers, and abstractions and how LLMs utilize
    these elements to generate meaningful output. This discussion will provide a deeper
    understanding of the intricate processes through which LLMs manage to mimic human-like
    understanding of language while also shedding light on the challenges and limitations
    they face in this endeavor. We do not necessarily believe that mimicking human
    behavior is the right answer for LLM improvement, only that mimicry is how the
    field has evaluated itself so far.
  prefs: []
  type: TYPE_NORMAL
- en: To introduce semiotics, let’s consider figure 2.2, an adapted Peircean semiotic
    triangle. These triangles are used to organize base ideas into sequences of firstness,
    secondness, and thirdness, with firstness being at the top left, secondness at
    the bottom, and thirdness at the top right. If you’ve ever seen a semiotic triangle
    before, you may be surprised at the number of corners and orientation. To explain,
    we’ve turned them upside down to make it slightly easier to read. Also, because
    the system is recursive, we’re showing you how the system can simultaneously model
    the entire process and each piece individually. While the whole concept of these
    ideas is very cool, it’s outside of the scope of this book to delve into the philosophy
    fully. Instead, we can focus on the cardinal parts of those words (first, second,
    third) to show the sequence of how meaning is processed.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 A recursive Peircean semiotic triangle is a system of organizing
    the process of extracting meaning from anything—in our case, from language. Each
    point on the triangle illustrates one of the minimal parts needed to synthesize
    meaning within whatever the system is being used to describe, so each point is
    a minimal unit in meaning for language. Firstness, secondness, and thirdness are
    not points on the triangle; instead, they are more like markers for the people
    versed in semiotics to be able to orient themselves in this diagram.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can also look at each intersection of the triangles to understand why things
    are presented in the order they are. Feelings can be attached to images and encodings
    long before they can be attached to words and tables. Ritual and common scripts
    give a space for interpreted action that’s second nature and doesn’t have to be
    thought about, similar to how most phrases just come together from words without
    the native speaker needing to perform metacognition about each word individually.
    All of these eventually lead to an interpretation or a document (a collection
    of utterances); in our case, that interpretation should be reached by the LLM.
    This is why, for example, prompt engineering can boost model efficacy. Foundation
    LLMs trained on millions of examples of ritual scripts can replicate the type
    of script significantly better when you explicitly tell the model in the prompt
    which script needs to be followed. Try asking the model for a step-by-step explanation—maybe
    prepend your generation with “Let’s think about this step-by-step.” The model
    will generate step-by-step scripts based on previous scripts it’s seen play out.
  prefs: []
  type: TYPE_NORMAL
- en: For those interested, there are specific ways of reading these figures and a
    whole field of semiotics to consider; however, it’s not guaranteed that you’ll
    be able to create the best LLMs by understanding the whole thing. Instead of diving
    deeply into this, we’ll consider the bare minimum that can help you build the
    best models, UX, and UI for everyone to interact with. For example, one aspect
    of the process of creating meaning is recursiveness. When someone is talking to
    you and they say something that doesn’t make sense (is “meaningless” to you),
    what do you do? Generally, people will ask one or more clarifying questions to
    figure out the meaning, and the process will start over and over until the meaning
    is clear to you. The most state-of-the-art models currently on the market do not
    do this, but they can be made to do it through very purposeful prompting. Many
    people wouldn’t even know to do that without having it pointed out to them. In
    other words, this is a brief introduction to semiotics. You don’t need to be able
    to give in-depth and accurate coordinate-specific explanations to experts in the
    semiotic field by the end of this section. The point we are trying to make is
    that this is an organizational system showcasing the minimum number of things
    you need to create a full picture of meaning for another person to interpret.
    We are not giving the same amount of the same kinds of information to our models
    during training, but if we did, it would result in a marked improvement in model
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Figures 2.2 and 2.3 are meant to represent a minimal organizational model, where
    each of these pieces is essential. Let’s consider figure 2.3, which walks through
    an example of using a semiotic triangle. Consider images, pictures, and memories
    and think about what it would be like to try to absorb the knowledge in this book
    without your eyes to process images and without orthography (a writing system)
    to abstract the knowledge. Looking at the bullet points, etc., how could you read
    this book without sections, whitespace between letters, and bullet points to show
    you the order and structure to process information? Look at semantics and literal
    encoded meaning, and imagine the book without diagrams or with words that didn’t
    have dictionary definitions. The spreadsheets in the middle could be a book without
    any tables or comparative informational organizers, including these figures. What
    would it be like to read this book without a culture or society with habits and
    dogma to use as a lens for our interpretations? All these points form our ability
    to interpret information, along with the lens through which we pass our information
    to recognize patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 Starting at the top-left corner, follow the arrows to see the general
    order we use to build our interpretations and extract meaning from things we interact
    with. Here, we’ve replaced the descriptive words with some examples of each point.
    Try to imagine interpreting this diagram without any words, examples, arrows,
    or even the pragmatic context of knowing what a figure in a book like this is
    supposed to be for.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'So these are the important questions: How many of these things do you see LLMs
    having access to in order to return meaningful interpretations? Does an LLM have
    access to feelings or societal rituals? Currently, they do not, but as we go through
    traditional and newer techniques for NLP inference, think about what different
    models have access to.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Multilingual NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last challenge that we need to touch on before we evaluate previous NLP
    techniques and current-generation LLMs is the foundation of linguistics and the
    reason LLMs even exist. People have wanted to understand or exploit each other
    since the first civilizations made contact. These cases have resulted in the need
    for translators, and this need has only exponentially increased as the global
    economy has grown and flourished.
  prefs: []
  type: TYPE_NORMAL
- en: It’s pretty simple math for business as well. Did you know that there are almost
    as many native speakers of Bengali as there are native speakers of English? If
    this is the first time you’ve heard of the Bengali language, this should hopefully
    color your perception that there is a valuable market for multilingual models.
    There are billions of people in the world, but only about a third of 1 billion
    speak English natively. If your model is Anglocentric, like most are, you are
    missing out on 95% of the people in the world as customers and users. Spanish
    and Mandarin Chinese are easy wins in this area, but most people don’t even go
    that far.
  prefs: []
  type: TYPE_NORMAL
- en: There are many more politically charged examples of calling things, including
    different languages, the same that are out of the scope of this book. These are
    most often because of external factors like government involvement. Keeping these
    two points in mind—that a monolingual system focusing on English doesn’t have
    the coverage or profit potential that many businesses act like it does and that
    the boundaries between languages and dialects are unreliable at best and systematically
    harmful at worst—should highlight the dangerous swamp of opinions. Many businesses
    and research scientists don’t even pretend to want to touch this swamp with a
    50-foot pole when designing a product or system.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, no easy solutions exist at this time. However, considering these
    factors can help you as a scientist or engineer (and hopefully an ethical person)
    to design LLMs that, at the very least, don’t exacerbate and negatively contribute
    to the existing problems. The first step in this process is deciding on a directional
    goal from the beginning of the project, either toward localization (l10n) or internationalization
    (i18n). Localization is an approach exemplified by Mozilla, which has a different
    version of its browser available through crowdsourced l10n in over 90 languages
    with no indications of stopping that effort. Internationalization is similar,
    but in the opposite direction; for example, Ikea tries to put as few words as
    possible in their instructional booklets, opting instead for internationally recognized
    symbols and pictures to help customers navigate the DIY projects. Deciding at
    the beginning of the project cuts down on the effort required to expand to either
    solution exponentially. It is large enough to switch the perception of translation
    and formatting from a cost to an investment. In the context of LLMs and their
    rapid expansion across the public consciousness, it becomes even more important
    to make that consideration early. Hitting the market with a world-changing technology
    that automatically disallows most of the world from interacting with it devalues
    those voices. Having to wait jeopardizes businesses’ economic prospects.
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing, let’s take a moment to reflect on what we’ve discussed so
    far. We’ve hit important points in linguistics, illustrating concepts for us to
    consider, such as understanding that the structure of language is separate from
    its meaning. We have demonstrated quite a journey that each of us takes, both
    personally and as a society, toward having the metacognition to understand and
    represent language in a coherent way for computers to work with. This understanding
    will only improve as we deepen our knowledge of cognitive fields and solve for
    the linguistic features we encounter. Going along with figure 2.1, we will now
    demonstrate the computational path for language modeling that we have followed
    and explore how it has and hasn’t solved for any of those linguistic features
    or strived to create meaning. Let’s move into evaluating the various techniques
    for representing a language algorithmically.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Language modeling techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having delved into the fundamental features of language, the principles of semiotics,
    and how LLMs interpret and process linguistic information, we now transition into
    a more practical realm. We will explore the various NLP techniques developed and
    employed to create these powerful language models. By examining the strengths
    and weaknesses of each approach, we will gain valuable insights into the effectiveness
    of these techniques in capturing the essence of human language and communication.
    This knowledge will not only help us appreciate the advancements made in the field
    of NLP but also enable us to better understand the current limitations of these
    models and the challenges that lie ahead for future research and development.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a second to go over some data processing that will be universal to
    all language modeling. First, we’ll need to decide how to break up the words and
    symbols we’ll be passing into our model, effectively deciding what a token will
    be in our model. We’ll need a way to convert those tokens to numerical values
    and back again. Then, we’ll need to pick how our model will process the tokenized
    inputs. Each of the following techniques will build upon the previous techniques
    in at least one of these ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first of these techniques is called a bag-of-words (BoW) model, and it
    consists of simply counting words as they appear in text. You could import the
    CountVectorizer class from sklearn to use it, but it’s more instructive if we
    show you with a small snippet. It can be accomplished very easily with a dictionary
    that scans through text, creating a new vocabulary entry for each new word as
    a key and an incrementing value starting at 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Considering its simplicity, even this model, based entirely on frequency, can
    be quite powerful when trying to gain insight into a speaker’s intentions or at
    least their idiosyncrasies. For example, you could run a simple BoW model on inaugural
    speeches of US presidents, searching for the words “freedom,” “economy,” and “enemy”
    to gain a pretty good insight about which presidents assumed office under peacetime,
    during wartime, and during times of monetary strife, just based on how many times
    each word was mentioned. The BoW model’s weaknesses are many, however, as the
    model provides no images, semantics, pragmatics, phrases, or feelings. In our
    example, there are two instances of “words,” but because our tokenization strategy
    is just whitespace, it didn’t increment the key in the model. It doesn’t have
    any mechanisms to evaluate context or phonetics, and because it divides words
    by default on whitespace (you can obviously tokenize however you want, but try
    tokenizing on subwords and see what happens with this model—spoiler: it is bad),
    it doesn’t account for morphology either. Altogether, it should be considered
    a weak model for representing language but a strong baseline for evaluating other
    models against it. To solve the problem of BoW models not capturing any sequence
    data, N-gram models were conceived.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 N-gram and corpus-based techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: N-gram models represent a marked and efficient improvement to BoW by allowing
    you to give the model a sort of context, represented by N. They are relatively
    simple statistical models that enable you to generate words based on the N = 1
    context space. Listing 2.1 uses trigrams, which means N = 3\. We clean the text
    and give it minimal padding/formatting to help the model, and then we train using
    everygrams, which prioritizes flexibility over efficiency so that we can train
    a pentagram (N = 5) or a septagram (N = 7) model if we want. At the end of the
    listing, where we are generating, we can give the model up to two tokens to help
    it figure out how to generate further. N-gram models were not created for and
    have never claimed to attempt complete modeling systems of linguistic knowledge,
    but they are widely useful in practical applications. They ignore all linguistic
    features, including syntax, and only attempt to draw probabilistic connections
    between words appearing in an N-length phrase.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  All assets necessary to run the code—including text and data files—can
    be found in the code repository accompanying this book: [https://github.com/IMJONEZZ/LLMs-in-Production/](https://github.com/IMJONEZZ/LLMs-in-Production/).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 A generative N-grams language model implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a corpus from any number of plain .txt files'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Pads each side of every line in the corpus with <s> and </s> to indicate
    the start and end of utterances'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Allows everygrams to create a training set and a vocab object from the data'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Instantiates and trains the model we’ll use for N-grams, a maximum likelihood
    estimator (MLE)'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 This model will take the everygrams vocabulary, including the <UNK> token
    used for out-of-vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Language can be generated with this model and conditioned with n-1 tokens
    preceding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code is all that you need to create a generative N-gram model. For those
    interested in being able to evaluate that model further, we’ve included the following
    code so you can grab probabilities and log scores or analyze the entropy and perplexity
    of a particular phrase. Because this is all frequency-based, even though it’s
    mathematically significant, it still does a pretty bad job of describing how perplexing
    or frequent real-world language actually is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Any set of tokens up to length = n can be counted easily to determine frequency.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Any token can be given a probability of occurrence and augmented with up
    to n-1 tokens to precede it.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 This can be done as a log score as well to avoid very big and very small
    numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sets of tokens can be tested for entropy and perplexity as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While this code example illustrates creating a trigram language model, unfortunately,
    not all phrases needing to be captured are only three tokens long. For example,
    from Hamlet, “To be or not to be” consists of one phrase with two words and one
    phrase with four words. Note that even though N-grams are typically very small
    language models, it is possible to make an N-gram LLM by making N=1,000,000,000
    or higher, but don’t expect to get even one ounce of use out of it. Just because
    we made it big doesn’t make it better or mean it’ll have any practical application:
    99.9% of all text and 100% of all meaningful text contains fewer than 1 billion
    tokens appearing more than once, and that computational power can be much better
    spent elsewhere.'
  prefs: []
  type: TYPE_NORMAL
- en: N-grams only use static signals (whitespace, orthography) and words to extract
    meaning (figure 2.2). They try to measure phrases manually, assuming all phrases
    will be the same length. That said, N-grams can be used to create powerful baselines
    for text analysis. In addition, if the analyst already knows the pragmatic context
    of the utterance, N-grams can give quick and accurate insight into real-world
    scenarios. Nonetheless, this type of phrasal modeling fails to capture any semantic
    encodings that individual words could have. To solve this problem, Bayesian statistics
    were applied to language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Bayesian techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bayes’ theorem is one of the most mathematically sound and simple theories for
    describing the occurrence of your output within your input space. Essentially,
    it calculates the probability of an event occurring based on prior knowledge.
    The theorem posits that the probability of a hypothesis being true given evidence—for
    example, that a sentence has a positive sentiment—is equal to the probability
    of the evidence occurring given the hypothesis is true multiplied by the probability
    of the hypothesis occurring, all divided by the probability of the evidence being
    true. It can be expressed mathematically as
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*hypothesis* | *evidence*) = (*P*(*evidence* | *hypothesis*) × *P*(*hypothesis*))
    / *P*(*evidence*)'
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*A*|*B*) × *P*(*B*) = *P*(*B*|*A*) × *P*(*A*)'
  prefs: []
  type: TYPE_NORMAL
- en: Because this isn’t a math book, we’ll dive into Bayes’ theorem to the exact
    same depth we dove into other linguistics concepts and trust the interested reader
    to search for more.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, even though the theorem represents the data in a mathematically
    sound way, it doesn’t account for any stochasticity or multiple meanings of words.
    One word you can always throw at a Bayesian model to confuse it is “it.” Any demonstrative
    pronoun ends up getting assigned values in the same `LogPrior` and `LogLikelihood`
    way as all other words, and it gets a static value, which is antithetical to the
    usage of those words. For example, if you’re trying to perform sentiment analysis
    on an utterance, assigning all pronouns a null value would be better than letting
    them go through the Bayesian training. Note also that Bayesian techniques don’t
    create generative language models the way the rest of these techniques will. Because
    of the nature of Bayes’ theorem validating a hypothesis, these models work for
    classification and can bring powerful augmentation to a generative language model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 shows you how to create a naive Bayes classification language model,
    or a system that performs classification on text based on a prior-learned internal
    language model. Instead of using a package like sklearn or something that would
    make writing the code a little easier, we opted to write out what we were doing,
    so it’s a bit longer, but it should be more information about how it works. We
    are using the least-complex version of a naive Bayes model. We haven’t made it
    multinomial or added anything fancy; obviously, it would work better if you opted
    to upgrade it for any problem you want. And we highly recommend you do.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  To make the code easier to understand and help highlight the portions
    we wanted to focus on, we have simplified some of our code listings by extracting
    portions to utility helpers. If you are seeing import errors, this is why. These
    helper methods can be found in the code repository accompanying this book: [https://github.com/IMJONEZZ/LLMs-in-Production/](https://github.com/IMJONEZZ/LLMs-in-Production/)'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Categorical naive Bayes language model implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Δefines the key, which is the word and label tuple'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 If the key exists in the dictionary, increments the count'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 If the key is new, adds it to the dict and sets the count to 1'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Calculates V, the number of unique words in the vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Calculates N_pos and N_neg'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 If the label is positive (greater than zero) . . .'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 . . . increments the number of positive words (word, label)'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Else, the label is negative.'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Increments the number of negative words (word, label)'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Calculates Δ, the number of documents'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Calculates the number of positive documents'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Calculates the number of negative documents'
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Calculates logprior'
  prefs: []
  type: TYPE_NORMAL
- en: '#14 For each word in the vocabulary . . .'
  prefs: []
  type: TYPE_NORMAL
- en: '#15 . . . calculates the probability that each word is positive or negative'
  prefs: []
  type: TYPE_NORMAL
- en: '#16 Calculates the log likelihood of the word'
  prefs: []
  type: TYPE_NORMAL
- en: '#17 Processes the utt to get a list of words'
  prefs: []
  type: TYPE_NORMAL
- en: '#18 Initializes probability to zero'
  prefs: []
  type: TYPE_NORMAL
- en: '#19 Adds the logprior'
  prefs: []
  type: TYPE_NORMAL
- en: '#20 Checks if the word exists in the loglikelihood dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '#21 Adds the log likelihood of that word to the probability'
  prefs: []
  type: TYPE_NORMAL
- en: '#22 Returns this properly'
  prefs: []
  type: TYPE_NORMAL
- en: '#23 If the prediction is &gt; 0 . . .'
  prefs: []
  type: TYPE_NORMAL
- en: '#24 . . . the predicted class is 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '#25 Otherwise, the predicted class is 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '#26 Appends the predicted class to the list y_hats'
  prefs: []
  type: TYPE_NORMAL
- en: '#27 Error = avg of the abs vals of the diffs between y_hats and test_y.'
  prefs: []
  type: TYPE_NORMAL
- en: '#28 Accuracy is 1 minus the error.'
  prefs: []
  type: TYPE_NORMAL
- en: This theorem doesn’t create the same type of language model but one with a list
    of probabilities associated with one hypothesis. As such, Bayesian language models
    can’t be used effectively to generate language, but they can be very powerfully
    implemented for classification tasks. In our opinion, though, Bayesian models
    are often overhyped for even this task. One of the crowning achievements of one
    author’s career was replacing and removing a Bayesian model from production.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian models, one big problem is that all sequences are completely unconnected,
    like BoW models, moving us to the opposite end of sequence modeling from N-grams.
    Like a pendulum, language modeling swings back toward sequence modeling and language
    generation with Markov chains.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Markov chains
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often called hidden Markov models (HMMs), Markov chains essentially add state
    to the N-gram models, storing probabilities using hidden states. They are often
    used to help parse text data for even larger models, doing things like part-of-speech
    (PoS) tagging (marking words with their parts of speech) and named entity recognition
    (NER; marking identifying words with their referent and usually type; e.g., LA
    – Los Angeles – City) on textual data. Building on the previous Bayesian models,
    Markov models rely completely on stochasticity (predictable randomness) in the
    tokens encountered. The idea that the probability of anything happening *next*
    depends completely upon the state of *now* is, like Bayes’ theorem, mathematically
    sound. So instead of modeling words based solely on their historical occurrence
    and drawing a probability from that, we model their future and past collocation
    based on what is currently occurring. So the probability of “happy” occurring
    goes down to almost zero if “happy” was just output but goes up significantly
    if “am” has just occurred. Markov chains are so intuitive that they were incorporated
    into later iterations of Bayesian statistics and are still used in production
    systems today.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.3, we train a Markov chain generative language model. This is the
    first model where we’ve used a specific tokenizer, which, in this case, will tokenize
    based on the whitespace between words. This is also only the second time we’ve
    referred to a collection of utterances meant to be viewed together as a document.
    As you play around with this one, pay close attention and make some comparisons
    yourself of how well the HMM generates compared to even a large N-gram model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Generative hidden Markov language model implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code shows a basic implementation of a Markov model for generation, and
    we encourage you to experiment with it. Give it text from songs from your favorite
    musicians or books from your favorite authors, and see whether what comes out
    sounds like them. HMMs are incredibly fast and are often used in predictive text
    or predictive search applications. Markov models represent the first comprehensive
    attempt to model language from a descriptive linguistic perspective, as opposed
    to a prescriptive one. The perspective is interesting because Markov did not originally
    intend to use linguistic modeling, only to win an argument about continuous independent
    states. Later, Markov used Markov chains to model vowel distribution in a Pushkin
    novel, so he was at least aware of the possible applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between descriptive and prescriptive linguistics is that the
    latter focuses on how things *ought* to be, while the former focuses on how things
    *are*. From a language modeling perspective, it has proven vastly more effective
    to describe what language is doing from a corpus or Markov perspective rather
    than to attempt to prescribe how language ought to behave. Unfortunately, a current
    state by itself cannot be used to give context beyond the now, so historical or
    societal context cannot be represented effectively in a Markov model. The semantic
    encoding of words also becomes problematic, as represented in the code example:
    Markov chains will output syntactically correct chains of words that are nonsense
    semantically, similar to “colorless green ideas sleep furiously.” To solve this
    problem, “continuous” models were developed to allow for a “semantic embedding”
    representation of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Continuous language modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A continuous bag-of-words (CBoW) model—much like its namesake, the BoW model—is
    a frequency-based approach to analyzing language, meaning that it models words
    based on how often they occur. The next word in a human utterance has never been
    determined based on probability or frequency. Consequently, we provide an example
    of creating word embeddings to be ingested or compared by other models using a
    CBoW. We’ll use a neural network to provide you with a good methodology.
  prefs: []
  type: TYPE_NORMAL
- en: This is the first language modeling technique we’ll see that essentially slides
    a context window over a given utterance (the context window is an N-gram model)
    and attempts to guess the word in the middle based on the surrounding words in
    the window. For example, let’s say your window has a length of 5, and your sentence
    is “Learning about linguistics makes me happy.” You would give the CBoW `['learning',`
    `'about',` `'makes',` `'me']` to try to get the model to guess “linguistics” based
    on how many times the model has previously seen that word occur in similar places.
    This example shows you why generation is difficult for models trained like this.
    Say you give the model `['makes',` `'me',` `'</s>']` as input. Now the model only
    has three pieces of information, instead of four, to use to try to figure out
    the answer; it also will be biased toward only guessing words it has seen before
    at the end of sentences, as opposed to getting ready to start new clauses. It’s
    not all bad, though. One feature that makes continuous models stand out for embeddings
    is that they don’t have to look at only words before the target word; they can
    also use words that come after the target to gain some semblance of context.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.4, we create our first continuous model. In our case, to keep things
    as simple as possible, we use a BoW model for the language processing and a one-layer
    neural network with two parameters for the embedding estimation, although both
    could be substituted for any other models. For example, you could substitute N-grams
    for the BoW and a naive Bayes model for the neural network to get a continuous
    naive N-gram model. The point is that the actual models used in this technique
    are a bit arbitrary; it’s the continuous technique that’s important. To illustrate
    this further, we don’t use any packages other than `numpy` to do the math for
    the neural network, even though it’s the first one appearing in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Pay special attention to the steps—initializing the model weights, the rectified
    linear unit (ReLU) activation function, the final softmax layer, and forward and
    backpropagation—and how it all fits together in the `gradient_descent` function.
    These are pieces of the puzzle that you will see crop up again and again, regardless
    of programming language or framework. You will need to initialize models, pick
    activation functions, pick final layers, and define forward and backward propagation
    in TensorFlow, PyTorch, and Hugging Face, as well as if you ever start creating
    your own models instead of using someone else’s.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 Generative CBoW language model implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates our corpus for training'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Slightly cleans the data by removing punctuation, tokenizing by word, and
    converting to lowercase alpha characters'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Gets our bag of words, along with a distribution'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates two dictionaries to speed up time-to-convert and keep track of vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Here, we create our neural network with one layer and two parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Creates our final classification layer, which makes all possibilities add
    up to 1'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Δefines the behavior for moving forward through our model, along with an
    activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Δefine how we determine the distance between ground truth and model predictions'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Δefines how we move backward through the model and collect gradients'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Puts it all together and trains'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Trains the model'
  prefs: []
  type: TYPE_NORMAL
- en: The CBoW example is our first code example to showcase a full and effective
    training loop in machine learning. Within all of that, pay special attention to
    the steps in a training loop, especially the activation function, ReLU. As we
    expect you to be at least familiar with various ML paradigms, including different
    activations, we won’t explain the ReLU here. We will address when you should use
    it and when you shouldn’t. ReLUs, while solving the vanishing gradient problem,
    don’t solve the exploding gradient problem, and they destroy all negative comparisons
    within the model. Better situational variants include the Exponential linear unit
    (ELU), which allows negative numbers to normalize to alpha, and the generalized
    Gaussian linear units (GEGLU)/Swish-gated linear unit (SWIGLU), which works well
    in increasingly perplexing scenarios, like language. However, people often use
    ReLUs, not because they are the best in a situation, but because they are easy
    to understand and code and intuitive, even more so than the activations they were
    created to replace, the sigmoid or tanh.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of this ends up being abstracted with packages and the like, but knowing
    what’s going on under the hood will be very helpful for you as someone putting
    LLMs in production. You should be able to predict with some certainty how different
    models will behave in various situations. The next section will dive into one
    of those abstractions—in this case, the abstraction created by the continuous
    modeling technique.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hearkening back to our features of language, it should be easy to connect why
    continuous-style language modeling was such a breakthrough. Embeddings take the
    tokenized vectors we’ve created that don’t contain any meaning and attempt to
    insert that meaning based on observations that can be made about the text, such
    as word order and subwords appearing in similar contexts. Despite the primary
    mode of meaning being collocation (co-located, words that appear next to each
    other), they prove useful and even show some similarities to human-encoded word
    meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quintessential example from Word2Vec, one of the first pretrained vector
    embeddings, was taking the vector for “king,” subtracting the vector for “man,”
    adding the vector for “woman,” and finding the nearest neighbor to the sum was
    the vector for the word “queen.” This makes sense to us, as it mimics human semantics.
    One of the major differences is one that’s already been mentioned a couple of
    times: pragmatics. Humans use pragmatic context to inform semantic meaning, understanding
    that just because you said, “I need food,” doesn’t mean you are actually in physical
    danger without it. Embeddings are devoid of any influence outside of pure usage,
    which feels like it could be how humans learn as well, and there are good arguments
    on all sides here. The one thing holding is that if we can somehow give models
    more representative data, that may open the door to more effective embeddings,
    but it’s a chicken-and-egg problem because more effective embeddings give better
    model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.5, we dive into how to visualize embeddings using `pyplot`. We
    will be going more in depth into embeddings in later chapters. This is helpful
    for model explainability and also for validation during your pretraining step.
    If you see that your semantically similar embeddings are relatively close to each
    other on the graph, you’re likely going in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Embedding visualization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 After listing 2.4 is done and gradient descent has been executed'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 2.4, this code is a successful but very sparse embedding
    representation that we trained from our CBoW model. Getting those semantic representations
    (embeddings) to be denser is the main place we can see improvement in this field,
    although many successful experiments have been run where denser semantic meaning
    has been supplanted with greater pragmatic context through instruct and different
    thought-chaining techniques. We will address chain of thought (CoT) and other
    techniques later. For now, let’s pivot to discussing why our continuous embedding
    technique can even be successful, given that frequency-based models are characteristically
    difficult to correlate with reality. All of this started with the MLP more than
    half a century ago.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 A visualization technique for word embeddings. Visualizing embeddings
    can be important for model explainability.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 2.2.6  Multilayer perceptrons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLPs are the embodiment of the sentiment, “Machines are really good at doing
    one thing, so I wish we could just use a bunch of machines that are really good
    at the one thing to make one that’s good at a lot of things.” Every weight and
    bias in the neural network of the MLP is good at doing one thing, which could
    be detecting one or more features. So we bind a bunch of them together to detect
    larger, more complex features. MLPs serve as the primary building block in most
    neural network architectures. The key distinctions between architectures, such
    as convolutional neural networks and recurrent neural networks, mainly arise from
    data loading methods and the handling of tokenized and embedded data as it flows
    through the layers of the model rather than the functionality of individual layers,
    particularly the fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 provides a more dynamic class of neural networks that can have as
    many layers and parameters as deemed necessary for your task. We give a more defined
    and explicit class using PyTorch to give you the tools to implement the MLP in
    whatever way you’d like, both from scratch and in a popular framework.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 Multilayer perceptron PyTorch class implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: From this code, we can see, as opposed to the CBoW implementation, which had
    two static layers, that this MLP is not static in size until it has been instantiated.
    If you wanted to give this model 1 million layers, you would have to put `num_hidden_layers=
    1000000` when you instantiate the class. However, just because you give a model
    that many parameters doesn’t mean that will make it immediately better. LLMs are
    more than just a lot of layers. Like RNNs and CNNs, the magic of LLMs is in how
    data goes in and moves through the model. To illustrate, let’s look at the RNN
    and one of its variations.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.7 Recurrent neural networks and long short-term memory networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs are a class of neural networks designed to analyze sequences based on the
    weaknesses in previous language modeling techniques. A sequence can be thought
    of as an ordered array, where the sum of the whole array changes value if any
    of the parts are moved around. The logic goes that if language is presented in
    a sequence, then maybe it should be processed in a sequence instead of one token
    at a time. RNNs accomplish this by using logic we’ve seen before, both in MLPs
    and Markov chains, where an internal state or memory is referred to when new inputs
    are processed and by creating cycles when connections between nodes are detected
    as useful.
  prefs: []
  type: TYPE_NORMAL
- en: In fully recurrent networks, like the one in listing 2.7, all nodes start out
    initially connected to all subsequent nodes, but those connections can be set
    to zero to simulate them breaking if they are not useful. This solves one of the
    biggest problems that earlier models suffered from, static input size, and enables
    an RNN and its variants to process variable length inputs. Unfortunately, longer
    sequences create a new problem. Because each neuron in the network connects to
    subsequent neurons, longer sequences create smaller changes to the overall sum,
    making the gradients smaller until they eventually vanish, even with important
    words; this is called a vanishing gradient. Other problems exist too, such as
    exploding and diminishing gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s consider the following sentences with the task sentiment
    analysis: “I loved the movie last night” and “The movie I went to see last night
    was the very best I had ever expected to see.” These sentences can be considered
    semantically similar, even if they aren’t exactly the same. When moving through
    an RNN, each word in the first sentence is worth more, and the consequence is
    that the first sentence has a higher positive rating than the second sentence
    just because the first sentence is shorter. The inverse is also true: exploding
    gradients are a consequence of this sequence processing, which makes training
    deep RNNs difficult.'
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, LSTMs, a type of RNN, use memory cells and gating mechanisms
    to process sequences of variable length but without the problem of comprehending
    longer and shorter sequences differently. Anticipating multilingual scenarios
    and understanding that people don’t think about language in only one direction,
    LSTMs can also process sequences bidirectionally by concatenating the outputs
    of two RNNs, one reading the sequence from left to right and the other from right
    to left. This bidirectionality improves results, allowing information to be seen
    and remembered even after thousands of tokens have passed.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.7, we give classes for both an RNN and an LSTM. In the code in
    the repo associated with this book ([https://github.com/IMJONEZZ/LLMs-in-Production](https://github.com/IMJONEZZ/LLMs-in-Production)),
    you can see the results of training both the RNN and LSTM. The takeaway is that
    the LSTM achieves better accuracy on both training and validation sets in half
    as many epochs (25 versus 50 with RNN). One of the innovations to note is that
    the packed embeddings utilize padding to extend all variable-length sequences
    to the maximum length. Thus, LSTMs can process input of any length as long as
    it is shorter than the maximum. To set up the LSTM effectively, we’ll do some
    classical NLP on the dataset (a Twitter sentiment analysis dataset). That workflow
    will tokenize with the Natural Language Toolkit Regex. It looks for words and
    nothing else, passing into a spacy lemmatizer to get a list of lists containing
    only the base unconjugated forms of words.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.7 RNN and LSTM PyTorch class implementations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates our corpus for training and performs some classic NLP preprocessing'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Embeddings are needed to give semantic value to the inputs of an LSTM.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Usually should be a power of 2 because it’s the easiest for computer memory'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 You''ve got to determine some labels for whatever you''re training on.'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at our classes and instantiations, you should see that the LSTM is not
    vastly different from the RNN. The only difference is that the `init` input variables
    are `n_layers` (for convenience, you can also specify it with RNNs), `bidirectional`,
    and `dropout`. `bidirectional` allows LSTMs to look ahead in sequences to help
    with meaning and context. It also helps immensely with multilingual scenarios,
    as left-to-right languages like English are not the only format for orthography.
    `dropout`, another huge innovation, changes the paradigm of overfitting from being
    data dependent and helps the model not overfit by turning off random nodes layer
    by layer during training to force all nodes not to correlate with each other and
    preventing complex co-adaptations. The only difference in the out-of-model parameters
    is that the optimizer used for an RNN is stochastic gradient descent (SGD), like
    our CBoW; the LSTM uses Adam (although either could use any, depending on performance,
    including AdamW). Next, we define our training loop and train the LSTM. Compare
    this training loop to the one defined in listing 2.4 in the `gradient_descent`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: One of the amazing things demonstrated in the code here is how much quicker
    the LSTM can learn compared to previous model iterations, thanks to both `bidirectionality`
    and `dropout`. Although the previous models train faster than the LSTM, they take
    hundreds of epochs to get the same performance as an LSTM in just 25 epochs. As
    its name implies, the performance on the validation set adds validity to the architecture,
    performing inference during training on examples it has not trained on and keeping
    accuracy fairly close to the training set.
  prefs: []
  type: TYPE_NORMAL
- en: The problems with these models are not as pronounced, manifesting primarily
    as being incredibly resource-heavy, especially when applied to longer, more detail-oriented
    problems like healthcare and law. Despite the incredible advantages of `dropout`
    and `bidirectional` processing, they both at least double the amount of processing
    power required to train. So while inference ends up being only 2 to 3 times as
    expensive as an MLP of the same size, training becomes 10 to 12 times as expensive.
    That is, `dropout` and `bidirectional` solve exploding gradients nicely but explode
    the compute required to train. To combat this problem, a shortcut was devised
    and implemented that allows any model, including an LSTM, to figure out which
    parts of a sequence are the most influential and which parts can be safely ignored,
    known as *attention*.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.8 Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention is a mathematical shortcut that gives the model a mechanism for solving
    larger context windows faster by telling the model through an emergent mathematical
    formula which parts of an input to consider and how much. Attention is based upon
    an upgraded version of a dictionary, where instead of just key–value pairs, a
    contextual query is added. Simply know that the following code is the big differentiator
    between older NLP techniques and more modern ones.
  prefs: []
  type: TYPE_NORMAL
- en: Attention solves the slowness of training LSTMs yet keeps high performance on
    a low number of epochs. There are multiple types of attention as well. The dot
    product attention method captures the relationships between each word (or embedding)
    in your query and every word in your key. When queries and keys are part of the
    same sentences, this is known as *bi-directional self-attention*. However, in
    certain cases, it is more suitable to only focus on words that precede the current
    one. This type of attention, especially when queries and keys come from the same
    sentences, is referred to as *causal attention*. Language modeling further improves
    by masking parts of a sequence and forcing the model to guess what should be behind
    the mask. The functions in the following listing demonstrate both dot product
    attention and masked attention.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.8 Multihead attention implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Step 1: Input: three inputs, d_model=4'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Step 2: Weights three dimensions x d_model=4'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Step 3: Matrix multiplication to obtain Q,K,V; query: x * w_query; key:
    x * w_key; value: x * w_value'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Step 4: Scaled attention scores; square root of the dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Step 5: Scaled softmax attention scores for each vector'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Step 6: Attention value obtained by score1/k_d * V'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Step 7: Sums the results to create the first line of the output matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Step 8: Steps 1 to 7 for inputs 1 to 3; because this is just a demo, we’ll
    do a random matrix of the right dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Step 9: We train all eight heads of the attention sublayer using steps 1
    to 7.'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Step 10: Concatenates heads 1 to 8 to get the original 8 × 64 output dimension
    of the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 This function performs all of these steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 This function performs the previous steps but adds causality in masking.'
  prefs: []
  type: TYPE_NORMAL
- en: In the full implementation of attention, you may have noticed some terminology
    you’re familiar with—namely `Key` and `Value`, but you may not have been introduced
    to `Query` before. `Key` and `Value` pairs are familiar because of dictionaries
    and lookup tables, where we map a set of keys to an array of values. `Query` should
    feel intuitive as a sort of search for retrieval. The `Query` is compared to the
    `Key`s from which a `Value` is retrieved in a normal operation.
  prefs: []
  type: TYPE_NORMAL
- en: In attention, the `Query` and `Key`s undergo dot product similarity comparison
    to obtain an attention score, which is later multiplied by the `Value` to get
    an ultimate score for how much attention the model should pay to that portion
    of the sequence. This can get more complex, depending upon your model’s architecture,
    because both encoder and decoder sequence lengths have to be accounted for, but
    suffice it to say for now that the most efficient way to model in this space is
    to project all input sources into a common space and compare using dot product
    for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code explanation was a bit more math-heavy than the previous examples,
    but it is needed to illustrate the concept. The math behind attention is truly
    innovative and has rocketed the field forward. Unfortunately, even with the advantages
    attention brings to the process of sequence modeling, with LSTMs and RNNs, there
    were still problems with speed and memory size. You may notice from the code and
    the math that a square root is taken, meaning that attention, as we use it, is
    quadratic. Various techniques, including subquadratics like Hyena and the Recurrent
    Memory Transformer (RMT, basically an RNN combined with a transformer), have been
    developed to combat these problems, which we will cover in more detail later.
    For now, let’s move on to the ultimate application of attention: the transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Attention is all you need
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the seminal paper, “Attention Is All You Need,”[¹](#footnote-250) Vaswani
    et al. take the mathematical shortcut several steps further, positing that for
    performance, absolutely no recurrence (the “R” in RNN) or any convolutions are
    needed at all.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  We don’t go over convolutions because they aren’t good for NLP, but they
    are popular, especially in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Vaswani et al. opted to use only attention and specify where Q, K,
    and V were taken from much more carefully. We’ll dive into this presently. In
    our review of this diverse range of NLP techniques, we have observed their evolution
    over time and the ways in which each approach has sought to improve upon its predecessors.
    From rule-based methods to statistical models and neural networks, the field has
    continually strived for more efficient and accurate ways to process and understand
    natural language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we turn our attention to a groundbreaking innovation that has revolutionized
    the field of NLP: the transformer architecture. In the following section, we will
    explore the key concepts and mechanisms that underpin transformers and how they
    have enabled the development of state-of-the-art language models that surpass
    the performance of previous techniques. We will also discuss the effect of transformers
    on the broader NLP landscape and consider the potential for further advancements
    in this exciting area of research.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Encoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encoders are the first half of a full transformer model, excelling in the areas
    of classification and feature engineering. Vaswani et al. figured out that after
    the embedding layer inside the encoder, any additional transformations done to
    the tensors could end up harming their ability to be compared “semantically,”
    which was the point of the embedding layer. These models rely heavily upon self-attention
    and clever positional encoding to manipulate those vectors without significantly
    decreasing the similarity expressed.
  prefs: []
  type: TYPE_NORMAL
- en: Again, a key characteristic of embeddings is that they are vector representations
    of data—in our case, tokens. Tokens are whatever you pick to represent language.
    We recommend subwords as a general rule, but you will get a feel for where and
    which types of tokens work well. Consider the sentence, “The cat in the hat rapidly
    leapt above the red fox and the brown unmotivated dog.” “Red” and “brown” are
    semantically similar, and both are similarly represented after the embedding layer.
    However, they fall on positions 10 and 14, respectively, in the utterance, assuming
    that we’re tokenizing by word. Therefore, the positional encoding puts distance
    between them, also adding the ability to distinguish between the same tokens at
    different positions in an utterance. However, once the sine and cosine functions
    are applied, it brings their meaning back to only a little further apart than
    they were after the encoding, and this encoding mechanism scales brilliantly with
    recurrence and more data. To illustrate, let’s say there was a 99% cosine similarity
    between [red] and [brown] after embedding. Encoding would drastically reduce that
    to around 85% to 86% similarity. Applying sine and cosine methodologies as described
    brings their similarity back up to around 96%.
  prefs: []
  type: TYPE_NORMAL
- en: BERT was one of the first architectures after Vaswani et al.’s original paper
    and is an example of encoder-only transformers. BERT is such an incredibly powerful
    model architecture, given how small it is, that it is still used in production
    systems today. BERT was the first encoder-only transformer to surge in popularity,
    showcasing that performing continuous or sequential (they’re the same) modeling
    using a transformer results in much better embeddings than Word2Vec. We can see
    that these embeddings are better because they can be very quickly applied to new
    tasks and data with minimal training, with human-preferred results versus Word2Vec
    embeddings. For a while, most people were using BERT-based models for few-shot
    learning tasks on smaller datasets. BERT puts state-of-the-art performance within
    arm’s reach for most researchers and businesses with minimal effort required.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 An encoder visualized. Encoders are the first half of the full transformer
    architecture and excel in natural language understanding tasks like classification
    or named entity recognition. Encoder models improve upon previous designs by not
    requiring priors or recurrence and using clever positional encoding and multihead
    attention to create a vector embedding of each token.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The strengths of encoders (visualized in figure 2.5) include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification and hierarchical tasks showcasing understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blazing fast, considering the long-range dependency modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Builds off of known models, CBoW in embedding, MLP in feed forward, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encoders weaknesses include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: As suggested, requires lots of data (although less than RNNs) to be effective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even more complex architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.3.2 Decoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decoder models, as shown in figure 2.6, are larger versions of encoders that
    have two multihead attention blocks and three sum and normalize layers in their
    base form. They are the second half of a transformer behind an encoder. Decoders
    are very good at masked language modeling and learning and applying syntax very
    quickly, leading to the almost immediate idea that decoder-only models are needed
    to achieve artificial general intelligence. A useful reduction of encoder versus
    decoder tasks is that encoders excel in natural language understanding (NLU) tasks,
    while decoders excel in natural language generation (NLG) tasks. An example of
    decoder-only transformer architectures is the Generative Pre-trained Transformer
    (GPT) family of models. These models follow the logic of transformational generative
    grammar being completely syntax based, allowing for infinite generation of all
    possible sentences in a language (see appendix A).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 A decoder visualized. Decoders are the second half of a full transformer,
    and they excel at NLG tasks like chatbots and storytelling. Decoders improve upon
    previous architectures in the same way as encoders, but they shift their output
    one space to the right for next-word generation to help utilize the advantages
    of multihead self-attention.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The strengths of decoders include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Generates the next token in a sequence (shifted right means taking already-generated
    tokens into account)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Builds off of both known models and encoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be streamed during generation for great UX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Their weaknesses include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Syntax-only models can often struggle to insert the expected or intended meaning
    (see all “I forced an AI to watch 1000 hours of x and generated” memes from 2018–present).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucinations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.3.3 Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The full transformer architecture takes advantage of both encoders and decoders,
    passing the understanding of the encoder into the second multihead attention block
    of the decoder before giving output. As each piece of the transformer has a specialty
    in either understanding or generation, it should feel intuitive for the full product
    to be best at conditional generation tasks like translation or summarization,
    where some level of understanding is required before generation occurs. Encoders
    are geared toward processing input at a high level, and decoders focus more on
    generating coherent output. The full transformer architecture can successfully
    understand the data and then generate the output based on that understanding,
    as shown in figure 2.7\. The Text-To-Text Transfer Transformer (T5) family of
    models is an example of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 A full transformer visualized. A full transformer combines the encoder
    and the decoder and does well on all of the tasks of each, as well as conditional
    generation tasks such as summarization and translation. Because transformers are
    bulkier and slower than each of their halves, researchers and businesses have
    generally opted to use those halves over the whole transformer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: NOTE  Transformer models have an advantage in that they are built around the
    parallelization of inputs, which adds speed that LSTMs can’t currently replicate.
    If LSTMs ever get to a point where they can run as quickly as transformers, they
    may become competitive in the state-of-the-art field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strengths of a transformer are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Includes both an encoder and decoder, so it’s good at everything they are good
    at
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly parallelized for speed and efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weaknesses include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory intensive, but still less than LSTMs of the same size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires large amounts of data and VRAM for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you’ve probably noticed, most of the models we’ve discussed aren’t at all
    linguistically focused, being heavily syntax-focused, if they even attempt to
    model real language at all. Models, even state-of-the-art transformers, only have
    semantic approximations—no pragmatics, no phonetics—and only really utilize a
    mathematical model of morphology during tokenization without context. This doesn’t
    mean the models can’t learn these, nor does it mean that, for example, transformers
    can’t take audio as an input; it just means that the average usage doesn’t. With
    this in mind, it is nothing short of a miracle that they work as well as they
    do, and they really should be appreciated for what they can do.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we’ve attempted to highlight the current limitations in models, and
    we will dive into where to improve upon them in the remainder of this book. One
    such route is one that’s already been, and is still being, explored to great success:
    transfer learning and finetuning large foundational models. This technique came
    about soon after BERT’s initial release. Researchers discovered that although
    BERT generally performed well on a large number of tasks, if they wanted it to
    perform better on a particular task or data domain, they simply needed to retrain
    the model on data representative of the task or domain but not from scratch. Given
    all of the pretrained weights BERT learned while creating the semantic approximation
    embeddings on a much larger dataset, significantly less data is required to get
    state-of-the-art performance on the portion you need. We’ve seen this with BERT
    and the GPT family of models as they’ve come out, and now we’re seeing it again
    to solve exactly the challenges we discussed: semantic approximation coverage,
    domain expertise, and data availability.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Really big transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enter LLMs. Since their introduction, transformer-based models have continued
    to get larger and larger, not just in their size and number of parameters but
    also in the size and length of their training datasets and training cycles. If
    you studied machine learning or deep learning during the 2010s, you likely heard
    the moniker, “Adding more layers doesn’t make the model better.” LLMs prove this
    both wrong and right—wrong because their performance is unparalleled, often matching
    smaller models that have been meticulously finetuned on a particular domain and
    dataset, even those trained on proprietary data, and right because of the challenges
    that come with both training and deploying LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: One of the major differences between LLMs and language models involves transfer
    learning and finetuning. Like previous language models, LLMs are pretrained on
    massive text corpora, enabling them to learn general language features and representations
    that can be finetuned for specific tasks. Because LLMs are so massive and their
    training datasets are so large, they are able to achieve better performance with
    less labeled data, which was a significant limitation of earlier language models.
    Often, you can finetune an LLM to do highly specialized tasks with only a dozen
    or so examples.
  prefs: []
  type: TYPE_NORMAL
- en: However, what makes LLMs so powerful and has opened the door to widespread business
    use cases is their ability to do specialized tasks using simple prompting without
    any finetuning. Just give a few examples of what you want in your query, and the
    LLM can produce results. Training an LLM on a smaller set of labeled data is called
    few-shot prompting. It’s referred to as one-shot prompting when only one example
    is given and zero-shot when the task is totally novel. LLMs, especially those
    trained using reinforcement learning from human feedback and prompt engineering
    methodologies, can perform few-shot learning, where they can generalize and solve
    tasks with only a few examples, at a whole new level. This ability is a significant
    advancement over earlier models that required extensive finetuning or large amounts
    of labeled data for each specific task.
  prefs: []
  type: TYPE_NORMAL
- en: LMs previously have shown promise in the few and zero-shot learning domains,
    and LLMs have proven that promise to be true. As models have gotten larger, we
    find they are capable of accomplishing tasks smaller models can’t. We call this
    *emergent behavior*.[²](#footnote-251) Figure 2.8 illustrates eight different
    tasks previous language models couldn’t perform better than at random, and then
    once the models got large enough, they could.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 Examples of LLMs demonstrating emergent behaviors when given few-shot
    prompting tasks after the model scale reaches a certain size
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs also have demonstrably great zero-shot capabilities due to their vast parameter
    sizes, which is the main reason for their popularity and viability in the business
    world. LLMs also exhibit improved handling of ambiguity due to their large size
    and capacity. They are better at disambiguating words with multiple meanings and
    understanding the nuances of language, resulting in more accurate predictions
    and responses. This improvement isn’t because of better ability or architecture,
    as they share their architecture with smaller transformers, but because they have
    vastly more examples of how people generally disambiguate. LLMs, therefore, respond
    with the same disambiguation as is generally represented in the dataset. Thanks
    to the diverseness of the text data on which LLMs are trained, they exhibit increased
    robustness in handling various input styles, noisy text, and grammatical errors.
  prefs: []
  type: TYPE_NORMAL
- en: Another key difference between LLMs and language models is input space. A larger
    input space is important since it makes few-shot prompting tasks that much more
    viable. Many LLMs have max input sizes of 8,000+ tokens (originally 32K, GPT-4
    has sported 128K since November 2023), and while all the previously discussed
    models could also have input spaces that high, they generally don’t. We have recently
    seen a boom in this field, with techniques like Recurrent Memory Transformer (RMT)
    allowing 1M+ token context spaces, which rocket LLMs even more toward proving
    that bigger models are always better. LLMs are designed to capture long-range
    dependencies within text, allowing them to understand context more effectively
    than their predecessors. This improved understanding enables LLMs to generate
    more coherent and contextually relevant responses in tasks like machine translation,
    summarization, and conversational AI.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have revolutionized NLP by offering powerful solutions to problems that
    were challenging for earlier language models. They bring substantial improvements
    in contextual understanding, transfer learning, and few-shot learning. As the
    field of NLP continues to evolve, researchers are actively working to maximize
    the benefits of LLMs while mitigating all potential risks. Because a better way
    to approximate semantics hasn’t been found, they make bigger and more dimensional
    approximations. Because a good way of storing pragmatic context hasn’t been found,
    LLMs often allow inserting context into the prompt directly, into a part of the
    input set aside for context, or even through sharing databases with the LLM at
    inference. This capability doesn’t create pragmatics or a pragmatic system within
    the models, in the same way that embeddings don’t create semantics, but it allows
    the model to correctly generate syntax that mimics how humans respond to those
    pragmatic and semantic stimuli. Phonetics is a place where LLMs could likely make
    gigantic strides, either as completely text-free models or as a text-phonetic
    hybrid model, maybe utilizing the IPA in addition to or instead of text. It is
    exciting to consider the possible developments that we are watching sweep across
    this field right now.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you should have a pretty good understanding of what LLMs are
    and some key principles of linguistics that will come in handy when putting LLMs
    in production. You should now be able to start reasoning about what type of products
    will be easier or harder to build. Consider figure 2.9: tasks in the lower left-hand
    corner, like writing assistants and chatbots, are LLMs’ bread and butter. Text
    generation based on a little context from a prompt is a strictly syntax-based
    problem; with a large enough model trained on enough data, we can do this pretty
    easily. A shopping assistant is pretty similar and rather easy to build as well;
    we are just missing pragmatics. The assistant needs to know a bit more about the
    world, such as products, stores, and prices. With a little engineering, we can
    add this information to a database and give this context to the model through
    prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 How difficult or easy certain tasks are for LLMs and what approaches
    to take to solve them
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: On the other end, consider a chess bot. LLMs *can* play chess, but they aren’t
    any good. They have been trained on chess games and understand that E4 is a common
    first move, but their understanding is completely syntactical. LLMs only understand
    that the text they generate should contain a letter between A and H and a number
    between 1 and 8\. Like the shopping assistant, they are missing pragmatics and
    don’t have a clear model of the game of chess. In addition, they are also missing
    semantics. Encoders might help us understand that the words “king” and “queen”
    are similar, but they don’t help us understand that E4 is a great move one moment
    for one player and that same E4 move is a terrible move the very next moment for
    a different player. LLMs also lack knowledge based on phonetics and morphology
    for chess, although they are not as important in this case. Either way, we hope
    this exercise will better inform you and your team on your next project.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have amazing benefits, but with all of these capabilities come some limitations.
    Foundational LLMs require vast computational resources for training, making them
    less accessible for individual researchers and smaller organizations. This problem
    is being remedied with techniques we’ll talk about throughout the book, like quantization,
    textual embeddings, low-rank adaptation, parameter-efficient finetuning, and graph
    optimization. Still, foundation models are currently solidly outside the average
    individual’s ability to train effectively. Beyond that, there are concerns that
    the energy consumption associated with training LLMs could have significant environmental
    effects and cause problems associated with sustainability. These problems are
    complex and largely out of the scope of this book, but we would be remiss not
    to bring them up.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, since LLMs are trained on large-scale datasets containing
    real-world text, they may learn and perpetuate biases present in the data, leading
    to ethical concerns because real-world people don’t censor themselves to provide
    optimal unbiased data. Also, knowing much about what data you’re training on is
    not a widespread practice. For example, if you ask a text-to-image diffusion LLM
    to generate 1,000 images of “leader,” 99% of the images feature men, and 95% of
    the images feature people with white skin. The concern here isn’t that men or
    white people shouldn’t be depicted as leaders, but that the model isn’t representing
    the world accurately, and it’s showing.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, more nuanced biases are brought out. For example, in the Midjourney
    example in figure 2.10, the model, without being prompted (the only prompt given
    was the word “leader”), changed the popular feminist icon Rosie the Riveter to
    a man. The model didn’t think about this change; it just determined during its
    sampling steps that the prompt “leader” had more male-looking depictions in the
    training set. Many people will argue about what “good” and “bad” mean in this
    context, and instead of going for a moral ought, we’ll talk about what accuracy
    means. LLMs are trained on a plethora of data with the purpose of returning the
    most accurate representations possible. When they cannot return accurate representations,
    especially with their heightened abilities to disambiguate, we can view that as
    a bias that harms the model’s ability to fulfill its purpose. Later, we will discuss
    techniques to combat harmful bias to allow you, as an LLM creator, to get the
    exact outputs you intend and minimize the number of outputs you do not intend.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 Midjourney 5, which is, at the time of this writing, the most popular
    text2img model on the market, when prompted with only one token, “leader” (left),
    changed a well-known popular feminist icon, Rosie the Riveter, into a male depiction.
    ChatGPT (right) writes a function to place you in your job based on race, gender,
    and age. These are examples of unintended outputs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Alright, we’ve been building up to this moment the entire chapter. Let’s go
    ahead and run our first LLM! In listing 2.9, we download the Bloom model, one
    of the first open source LLMs to be created, and generate text! We are using Hugging
    Face’s Transformers library, which takes care of all the heavy lifting for us.
    Very exciting stuff!
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.9 Running our first LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Did you try to run it?!? If you did, you probably just crashed your laptop.
    Oopsie! Forgive me for a little harmless MLOps hazing, but getting some first-hand
    experience on how large these models can get and how difficult they can be to
    run is a helpful experience to have. In the next chapter, we will talk more about
    the difficulties of running LLMs and some of the tools you need to run this code.
    If you don’t want to wait and would like to get a similar but much smaller LLM
    running, change the model name to `"bigscience/bloom-3b"`, and run it again. It
    should work just fine this time on most hardware.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, LLMs are an amazing technology that allows our imaginations to run
    wild with possibility, and deservedly so. The number-one use case for considering
    an LLM over a smaller language model is when few-shot capabilities come into play
    for whoever the model will be helping, such as a CEO when raising funds or a software
    engineer when writing code. LLMs have these abilities precisely because of their
    size. The larger number of parameters in LLMs directly enables their ability to
    generalize over smaller spaces in larger dimensions. In this chapter, we’ve hit
    the lesser-known side of LLMs, the linguistic and language modeling side. In the
    next chapter, we’ll cover the other half, the MLOps side, where we dive into exactly
    how that large parameter size affects the model and the systems designed to support
    that model and makes it accessible to the customers or employees the model is
    intended for.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The five components of linguistics are phonetics, syntax, semantics, pragmatics,
    and morphology:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phonetics can be added through a multimodal model that processes audio files
    and is likely to improve LLMs in the future, but current datasets are too small.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Syntax is what current models are good at.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantics is added through the embedding layer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pragmatics can be added through engineering efforts.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Morphology is added in the tokenization layer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Language does not necessarily correlate with reality. Understanding the process
    people use to create meaning outside of reality is useful in training meaningful
    (to people) models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proper tokenization can be a major hurdle due to too many `<UNK>` tokens, especially
    when it comes to specialized problems like code or math.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilingual processing has always outperformed monolingual processing, even
    on monolingual tasks without models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each language model type in sequence shows a natural and organic growth of the
    LLM field as more and more linguistic concepts are added that make the models
    better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language modeling has seen an exponential increase in efficacy, correlating
    to how linguistics-focused the modeling has been.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention is a mathematical shortcut for solving larger context windows faster
    and is the backbone of modern architectures—encoders, decoders, and transformers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoders improve the semantic approximations in embeddings.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoders are best at text generation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers combine the two.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger models demonstrate emergent behavior, suddenly being able to accomplish
    tasks they couldn’t before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) Vaswani et al., 2017, Attention Is All You Need,”
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) J. Wei et al., “Emergent abilities of large language
    models,” Transactions on Machine Learning Research, Aug. 2022, [https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD).'
  prefs: []
  type: TYPE_NORMAL
