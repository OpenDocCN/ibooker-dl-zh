<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Model Development on Kubernetes"><div class="chapter" id="ch02_model_development_on_kubernetes_1738498450538975">
      <h1><span class="label">Chapter 2. </span>Model Development <span class="keep-together">on Kubernetes</span></h1>
      <p>In this chapter, we will provide an overview of prevailing technologies and techniques for developing machine learning models using Kubernetes as a compute platform. While we will focus on specific techniques relevant to large language models (LLMs) and generative AI, many of the techniques we discuss will apply to traditional predictive models and other architectures as well.</p>
      <p>Historically, models have required extensive data preparation to curate high-quality, labeled datasets that sufficiently capture the problem domain. Creating these datasets was very labor-intensive and expensive. More recently, advances in computational power, improved algorithms for distributing training across compute resources, and widespread open access to training data have all paved the way for extremely powerful general-purpose models to be built without heavy data curation.</p>
      <p>Generally, foundation LLMs are created via self-supervised learning, a type of unsupervised learning, on extremely large, unlabeled datasets. For LLMs, this results in a model that understands patterns in human language and can predict the most likely output that should follow a given input. These foundational models exhibit usefulness across a wide breadth of tasks, but practitioners often need to adapt these pretrained base models to some specific use case.</p>
      <p>There are several prevailing techniques for adapting these foundational models, which differ from each other in their intended use cases, ease of implementation, and costs of implementation. Collectively, we will refer to these approaches as <em>model customization techniques</em>.</p>
      <section data-type="sect1" data-pdf-bookmark="Overview of LLM Customization Techniques"><div class="sect1" id="ch02_overview_of_llm_customization_techniques_1738498450539081">
        <h1>Overview of LLM Customization Techniques</h1>
        <p>The LLM customization space, like much of generative AI, is evolving rapidly with new techniques being invented regularly. In general, customization is achieved through one or more of the following fundamental techniques:</p>
        <ul>
          <li>
            <p>Customizing an existing model’s output by leaving the model unchanged but carefully constructing the input to get a desired result. Examples of this include <em>prompt engineering</em> and <em>retrieval-augmented generation</em> (RAG).</p>
          </li>
          <li>
            <p>Combining individual models to achieve an output that is more desirable than that from a single model. One example of this is the <a href="https://oreil.ly/vw3kM"><em>mixture-of-agents</em> approach</a>.</p>
          </li>
          <li>
            <p>Retraining an existing model using curated data specific to a given task. This is <em>fine-tuning</em>.</p>
          </li>
        </ul>
        <p>Novel model customization techniques are likely to be achieved through new algorithms for implementing these fundamental techniques more efficiently or through creatively combining these techniques, as in the case of <a href="https://oreil.ly/feMXE"><em>retrieval-augmented fine-tuning</em> (RAFT)</a>.</p>
        <p>In the rest of this section, we will provide a primer on two of the most prominent approaches (as of this writing) to model customization: RAG and fine-tuning.</p>
        <section data-type="sect2" data-pdf-bookmark="Retrieval-Augmented Generation"><div class="sect2" id="ch02_retrieval_augmented_generation_rag_1738498450539148">
          <h2>Retrieval-Augmented Generation</h2>
          <p>A fundamental limitation of pretrained foundation models is that they possess “knowledge” only of the data that they were trained on. If you ask a model about a piece of data that it was not trained on, it will fail to give the desired answer. RAG is a technique that extends an existing model’s knowledge by passing relevant contextual data as input to the model at query time.</p>
          <p>So how does RAG work? Generally, when a user queries a model, a database (typically a <a href="https://oreil.ly/2L8-k">vector database</a>) is queried for information relevant to the input query. The RAG system parses the results and uses an algorithm like <a href="https://oreil.ly/7uOIh">cosine similarity</a> to choose the results most relevant to the query. Once those are chosen, they are added to the original query as contextual information and sent on to the model in a format along the lines of “using information found only in this input document, answer this question for me.” <a data-type="xref" href="#ch02_figure_1_1738498450534664">Figure 2-1</a> illustrates a hypothetical RAG system.</p>
          <figure><div id="ch02_figure_1_1738498450534664" class="figure">
            <img src="assets/skia_0201.png" width="1209" height="425"/>
            <h6><span class="label">Figure 2-1. </span>An illustration of a generalized RAG system showing the interactions between the user, retrieval system, vector database, <span class="keep-together">and model</span></h6>
          </div></figure>
          <p>The chosen retrieval and ranking algorithm is critically important to the performance of the RAG system. If no relevant contextual data is retrieved by the system, the model will lack the knowledge needed to give the desired answer to the user.</p>
          <p>Even though RAG requires a retrieval system and additional data storage between the user and the model, it has a number of benefits. Because RAG supplements the model’s knowledge at runtime, it requires less knowledge to be baked into the model and opens up the possibility of using a smaller model that is cheaper to serve to users while simultaneously allowing users to incorporate rapidly changing data like stock prices on the fly. RAG can also reduce the time to achieve value with an LLM, because it doesn’t require a lengthy retraining process to work.</p>
          <p>On the other hand, the knowledge given to a model via RAG is transient, and only exists for a single query. You also have to carefully craft the input prompt to get the kind of output you’re interested in. However, this sort of customization also has its limits. If you want to make knowledge changes persistent or fully customize the format of the model’s output, retraining the foundation model is required.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Model Fine-Tuning"><div class="sect2" id="ch02_model_fine_tuning_1738498450539206">
          <h2>Model Fine-Tuning</h2>
          <p>Training a foundation model is notoriously expensive and time-consuming, which isn’t an option for even the largest enterprises. Instead, we can make use of a technique called fine-tuning. With fine-tuning, you create a high-quality, labeled dataset that is specific to your domain-specific task, knowledge, or desired output format. You can then use that dataset to adjust a pretrained model in a fraction of the time and with a fraction of the data that would be required for training from scratch.</p>
          <p>The fine-tuned model will then have your desired knowledge and behavior baked in, allowing your production architecture to avoid the complexities required by techniques like RAG. However, fine-tuning requires knowing how to train a model, the time to curate a training dataset large enough to influence a model, and the sometimes high compute cost to perform the training itself.</p>
          <p>A number of techniques exist to optimize the compute cost of fine-tuning, such as parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA). Both of these techniques work by training only a subset of the pretrained model’s weights and biases.</p>
          <p>While this is complex, there are many tools and entire platforms available to help with training and fine-tuning models, such as  <a href="https://instructlab.ai">InstructLab</a> and Hugging Face’s <a href="https://oreil.ly/9f4mP">sft_trainer</a>, with many of them available within the open source Kubernetes ecosystem.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Kubernetes-Native Model Training Tools"><div class="sect1" id="ch02_kubernetes_native_model_training_tools_1738498450539266">
        <h1>Kubernetes-Native Model Training Tools</h1>
        <p>While many training tools and platforms are available, at a fundamental level they all provide easy access to the compute power necessary to train and fine-tune models. When evaluating a training tool or platform, the following requirements should be considered:</p>
        <ul>
          <li>
            <p>Integration with the training framework(s) (distributed or otherwise) that data scientists or data science teams use and are comfortable with (e.g., PyTorch, TensorFlow, etc.).</p>
          </li>
          <li>
            <p>Support for training/fine-tuning algorithms that your team wants to use.</p>
          </li>
          <li>
            <p>Access to hardware optimizers, such as accelerators (e.g., GPUs), specialized network devices, and specialized storage providers with multi-write-capable storage.</p>
          </li>
          <li>
            <p>Integrations with the development environments data scientists or data science teams are already using. A tool that effectively abstracts away Kubernetes so that the data scientist or team doesn’t need to manage it is ideal.</p>
          </li>
        </ul>
        <p>In the following subsections, we will explore open source tools that meet these requirements and have strong community adoption.</p>
        <section data-type="sect2" data-pdf-bookmark="Ray"><div class="sect2" id="ch02_ray_1738498450539323">
          <h2>Ray</h2>
          <p><a href="https://oreil.ly/eu-Ll">Ray</a> is a framework that enables users to scale their training and fine-tuning processes up from single machines to clusters of machines, and can run natively on Kubernetes via the <a href="https://oreil.ly/H7Xgz">KubeRay</a> operator. It seamlessly integrates with PyTorch and other frameworks via <a href="https://oreil.ly/dLTwc">Ray Train</a> and has extensive support for <a href="https://oreil.ly/TrRP-">accelerators</a>. It also comes with a dashboard that provides key monitoring information to end users.</p>
          <div data-type="note" epub:type="note"><h6>Note</h6>
            <p>An <a href="https://oreil.ly/-lPOg">operator</a> is an extension to Kubernetes that helps to manage Kubernetes applications by using custom resources to automate the application’s lifecycle.</p>
          </div>
          <p>Ray’s biggest strength is its ease of adoption by data scientists who don’t know Kubernetes well, but it comes with the downside of increased overhead through the management of Ray clusters when compared to options that have a more “raw” interface to Kubernetes. It also doesn’t always scale well to extremely large-scale training jobs.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Kubeflow Training Operator"><div class="sect2" id="ch02_kubeflow_training_operator_kfto_1738498450539380">
          <h2>Kubeflow Training Operator</h2>
          <p><a href="https://www.kubeflow.org">Kubeflow</a> is a community-managed open source ecosystem of Kubernetes components that support the full AI lifecycle. A part of that ecosystem, the Kubeflow Training Operator (KFTO) is a Kubernetes-native operator that allows users to use Kubernetes for distributed training and fine-tuning of large models. Its software development kit (SDK) allows for easy integration into existing environments and code and has extensive support for common frameworks like PyTorch. </p>
          <p>KFTO accelerator support is tied to the chosen training framework, so it supports anything that the training framework and Kubernetes support and can scale to any level that the framework and <span class="keep-together">Kubernetes</span> are capable of scaling to. Unlike Ray, KFTO is a thin layer on top of the underlying Kubernetes objects, which introduces very little compute overhead. The flipside to that, though, is that more of the Kubernetes details are exposed to the user, which may be confusing for data scientists and developers who do not need to know these details.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Native Training Framework Integration with Kubernetes"><div class="sect2" id="ch02_native_training_framework_integration_with_kuberne_1738498450539436">
          <h2>Native Training Framework Integration <span class="keep-together">with Kubernetes</span></h2>
          <p>Most training frameworks have framework-specific tooling for integrating with Kubernetes to provide computational resources. PyTorch, for example, has a universal job launcher called TorchX that includes Kubernetes support via its scheduler. While this kind of solution is the most lightweight and is the easiest for data scientists to adopt, it is less declarative and thus doesn’t lend itself as well to administration by MLOps teams.</p>
          <p>Another potential downside is that these tools are framework-specific, so usage won’t necessarily scale in large organizations with several data science teams using different frameworks. These native integrations are best suited for small teams of data scientists during experimentation phases.</p>
          <div data-type="note" epub:type="note"><h6>Note</h6>
            <p>Typically, once a model is trained or fine-tuned, you will want to evaluate its performance. Many existing model evaluation tools that data scientists use outside of Kubernetes can also be used when Kubernetes is used as a training platform. </p>
          </div>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Managing Compute Resources for Training"><div class="sect1" id="ch02_managing_compute_resources_for_training_1738498450539493">
        <h1>Managing Compute Resources for Training</h1>
        <p>While the tools described in the previous section allow you to train and fine-tune across many computational resources, this often requires extensive and costly hardware resources. Enterprises must pay particular attention to managing the cost incurred during <span class="keep-together">training</span> or fine-tuning. A robust management system should be able to do the following:</p>
        <ul>
          <li>
            <p>Facilitate the creation of job queues so that requests for compute hardware get serviced as soon as the hardware becomes available.</p>
          </li>
          <li>
            <p>Assign resource quotas to groups of users in order to constrain how many resources a given group can consume.</p>
          </li>
          <li>
            <p>Share resource quotas between groups when individual groups need to burst and there are free resources.</p>
          </li>
          <li>
            <p>Manage request priorities for resources and priority-based job preemption.</p>
          </li>
          <li>
            <p>Provide auditability and reporting on resource management at the model, job, and team levels.</p>
          </li>
          <li>
            <p>Allow all of these functions to be centrally managed by IT teams while maintaining transparency for users.</p>
          </li>
        </ul>
        <p>There are currently two major open source projects in this space: <a href="https://oreil.ly/JZQ_0">Kueue</a> and <a href="https://oreil.ly/cCQQJ">Volcano</a>. Both projects are Kubernetes native and have strong community adoption. They also have support for managing resources of various types, like Ray clusters, KFTO jobs, and PyTorch training jobs.</p>
        <p>While these projects offer similar functionality, they do have some key differences. Kueue is an official Kubernetes special interest group project and is thus “blessed” by the wider Kubernetes community. It is based on the design principle of delegating functionality to existing Kubernetes components when applicable, and because of this, Kueue is fairly lightweight. </p>
        <p>Volcano, on the other hand, replicates some existing Kubernetes functionality, giving it more overhead but allowing it to be a more holistic and better-integrated solution. It is also more mature than Kueue and as of this writing offers more capabilities.</p>
        <p>Once a data science team has a model and training procedure it is ready to send to production, it will be necessary to periodically retrain the model while keeping track of the datasets that went into each new version of the model. In <a data-type="xref" href="ch03.html#ch03_making_training_repeatable_1738498450655759">Chapter 3</a>, we will discuss why periodic retraining, model versioning, and dataset versioning are necessary along with tools to help with these production <span class="keep-together">workflows</span>.</p>
      </div></section>
    </div></section></div>
</div>
</body></html>