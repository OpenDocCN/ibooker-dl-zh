<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="appendix" epub:type="appendix" data-pdf-bookmark="Appendix B. Mixed Precision and Quantization"><div class="appendix" id="precision_appendix">
<h1><span class="label">Appendix B. </span>Mixed Precision and Quantization</h1>


<p>By<a data-type="indexterm" data-primary="mixed-precision and quantization" id="xi_mixedprecisionandquantization2243_1"/> default, PyTorch uses 32-bit floats to represent model parameters: that’s 4 bytes per parameter. If your model has 1 billion parameters, then you need at least 4 GB of RAM just to hold the model. At inference time you also need enough RAM to store the activations, and at training time you need enough RAM to store all the intermediate activations as well (for the backward pass), and to store the optimizer parameters (e.g., Adam needs two additional parameters for each model parameter—that’s an extra 8 GB). This is a lot of RAM, and it’s also plenty of time spent transferring data between the CPU and the GPU, not to mention storage space, download time, and energy consumption.</p>

<p>So how can we reduce the model’s size? A simple option is to use a reduced precision float representation—typically 16-bit floats instead of 32-bit floats. If you train a 32-bit model then shrink it to 16-bits after training, its size will be halved, with little impact on its quality. Great!</p>

<p>However, if you try to train the model using 16-bit floats, you may run into convergence issues, as we will see. So a common strategy is <em>mixed-precision training</em> (MPT)<a data-type="indexterm" data-primary="mixed-precision training (MPT)" id="id4342"/><a data-type="indexterm" data-primary="MPT (mixed-precision training)" id="id4343"/>, where we keep the weights and weight updates at 32-bit precision during training, but the rest of the computations use 16-bit precision. After training, we shrink the weights down to 16-bits.</p>

<p>Finally, to shrink the model even further, you can use <em>quantization</em><a data-type="indexterm" data-primary="quantization" id="id4344"/>: the parameters are discretized and represented as 8-bit integers, or even 4-bit integers or less. This is harder, and it degrades the model’s quality a bit more, but it reduces the model size by a factor of 4 or more, and speeds it up significantly.</p>

<p>In this appendix, we will cover reduced precision, mixed-precision training, and quantization. But to fully understand these, we must first discuss common number representations in machine learning.</p>






<section data-type="sect1" data-pdf-bookmark="Common Number Representations"><div class="sect1" id="id377">
<h1>Common Number Representations</h1>

<p>By<a data-type="indexterm" data-primary="common number representations" id="xi_commonnumberrepresentations22153_1"/><a data-type="indexterm" data-primary="mixed-precision and quantization" data-secondary="common number representations" id="xi_mixedprecisionandquantizationcommonnumberrepresentations22153_1"/> default, PyTorch represents weights and activations using 32-bit floats based on the <em>IEEE Standard for Floating-Point Arithmetic</em> (IEEE 754)<a data-type="indexterm" data-primary="IEEE Standard for Floating-Point Arithmetic (IEEE 754)" id="id4345"/>, which specifies how floating-point numbers are represented in memory. It’s a flexible and efficient format which can represent tiny values and huge values, as well as special values such as ±0,⁠<sup><a data-type="noteref" id="id4346-marker" href="app02.html#id4346">1</a></sup> ±infinity, and NaN (i.e., Not a Number).</p>

<p>The float32 data type<a data-type="indexterm" data-primary="float32 (fp32) data type" id="id4347"/><a data-type="indexterm" data-primary="fp32 (float32) data type" data-primary-sortas="fpc" id="id4348"/><a data-type="indexterm" data-primary="32-bit floats" data-primary-sortas="thirty-two bit floats" id="id4349"/><a data-type="indexterm" data-primary="floats" data-secondary="32-bit" id="id4350"/> (fp32 for short) can hold numbers as small as ±1.4e<sup>–45</sup> and as large as ±3.4e<sup>38</sup>. It is represented at the top of <a data-type="xref" href="#number_representations_diagram">Figure B-1</a>. The first bit determines the <em>sign</em> <em>S</em>: 0 means positive, 1 means negative. The next 8 bits hold the <em>exponent</em> <em>E</em>, ranging from 0 to 255. And the last 23 bits represent the <em>fraction</em> <em>F</em>, ranging from 0 to 2<sup>23</sup> – 1. Here is how to compute the value:</p>

<ul>
<li>
<p>If <em>E</em> is between 1 and 254, then the number is called <em>normalized</em><a data-type="indexterm" data-primary="normalized values" id="id4351"/>: this is the most common scenario. In this case, the value <em>v</em> can be computed using <em>v</em> = (–1)<sup><em>S</em></sup>⋅2<sup><em>E</em>–127</sup>⋅(1 + <em>F</em>⋅2<sup>–23</sup>). The last term (1 + <em>F</em>⋅2<sup>–23</sup>) corresponds to the most significant digits, so it’s called the <em>significand</em>.</p>
</li>
<li>
<p>If <em>E</em> = 0 and <em>F</em> &gt; 0, then the number is called <em>subnormal</em><a data-type="indexterm" data-primary="subnormal numbers" id="id4352"/>: it is useful to represent the tiniest values.⁠<sup><a data-type="noteref" id="id4353-marker" href="app02.html#id4353">2</a></sup> In this case, <em>v</em> = (–1)<sup><em>S</em></sup>⋅2<sup><em>E</em>+1–127</sup>⋅(0 + <em>F</em>⋅2<sup>–23</sup>) = (–1)<sup><em>S</em></sup>⋅<em>F</em>⋅2<sup>–149</sup>.</p>
</li>
<li>
<p>If <em>E</em> = 0 and <em>F</em> = 0, then <em>v</em> = ±0.</p>
</li>
<li>
<p>If <em>E</em> = 255 and <em>F</em> &gt; 0, then <em>v</em> = NaN.</p>
</li>
<li>
<p>If <em>E</em> = 255 and <em>F</em> = 0, then <em>v</em> = ±infinity.</p>
</li>
</ul>

<p>The other floating-point formats represented in <a data-type="xref" href="#number_representations_diagram">Figure B-1</a> differ only by the number of bits used for the exponent and the fraction. For example float16<a data-type="indexterm" data-primary="float16" id="id4354"/><a data-type="indexterm" data-primary="16-bit floats" data-primary-sortas="sixteen-bit floats" id="id4355"/><a data-type="indexterm" data-primary="fp16 (float16) data type" data-primary-sortas="fpb" id="id4356"/><a data-type="indexterm" data-primary="floats" data-secondary="16-bit" id="id4357"/> uses 5 bits for the exponent (i.e., it ranges from 0 to 31) and 10 bits for the fraction (ranging from 0 to 1,023), while float8<a data-type="indexterm" data-primary="float8 (fp8)" id="id4358"/><a data-type="indexterm" data-primary="8-bit floats" data-primary-sortas="eight-bit floats" id="id4359"/><a data-type="indexterm" data-primary="fp8 (float8) data type" data-primary-sortas="fpa" id="id4360"/><a data-type="indexterm" data-primary="floats" data-secondary="8-bit" id="id4361"/> uses 4 bits for the exponent (from 0 to 15) and 3 bits for the fraction, so it’s often denoted fp8 E4M3.⁠<sup><a data-type="noteref" id="id4362-marker" href="app02.html#id4362">3</a></sup> The equations to compute the value are adjusted accordingly, for example normalized float16 values are computed using <em>v</em> = (–1)<sup><em>S</em></sup>⋅2<sup><em>E</em>–15</sup>⋅(1 + <em>F</em>⋅2<sup>–10</sup>).</p>

<figure><div id="number_representations_diagram" class="figure">
<img src="assets/hmls_ab01.png" alt="Diagram illustrating common number representations in machine learning, focusing on the bit structure of various floating-point and integer formats such as float32, float16, float8, and int8, with their respective ranges and components." width="1442" height="807"/>
<h6><span class="label">Figure B-1. </span>Common number representations in machine learning</h6>
</div></figure>

<p>The bfloat16<a data-type="indexterm" data-primary="bfloat16" id="id4363"/><a data-type="indexterm" data-primary="bfloat8" id="id4364"/><a data-type="indexterm" data-primary="floats" data-secondary="bfloat" id="id4365"/> and bfloat8 formats were proposed by Google Brain (hence the <em>b</em>), and they offer a wider range for the values, at the cost of a significantly reduced precision. We will come back to that.</p>

<p>Integers<a data-type="indexterm" data-primary="integers" id="id4366"/> are often represented using 64 bits<a data-type="indexterm" data-primary="floats" data-secondary="64-bit" id="id4367"/><a data-type="indexterm" data-primary="64-bit floats" data-primary-sortas="sixty-four bit floats" id="id4368"/><a data-type="indexterm" data-primary="fp64 (float64) data type" data-primary-sortas="fpd" id="id4369"/>, with values ranging from 0 to 2<sup>64</sup> – 1 (about 1.8e<sup>19</sup>) for unsigned integers, or –2^32 to 2^32 – 1 (about ±4.3e<sup>9</sup>) for signed integers. Integers are also frequently represented using 32 bits, 16 bits, or 8 bits depending on the use case. In <a data-type="xref" href="#number_representations_diagram">Figure B-1</a>, I only represented the integer types frequently used for quantization, such as 8-bit integers (which can be unsigned or signed).</p>

<p>When quantizing<a data-type="indexterm" data-primary="quantization" id="id4370"/> down to 4 bits, we usually pack 2 weights per byte, and when quantizing down to 2 bits, we pack 4 weights per byte. It’s even possible to quantize down to ternary values, where each weight can only be equal to –1, 0, or +1. In this case, it’s common to store five weights per byte. For example, the byte 178 can be written as 20121 in base 3 (since 178 = 2⋅3<sup>4</sup> + 0⋅3<sup>3</sup> + 1⋅3<sup>2</sup> + 2⋅3<sup>1</sup> + 1⋅3<sup>0</sup>), and if we subtract 1 from each digit, we get 1, –1, 0, 1, 0: these are the 5 ternary weights stored in this single byte. Since 3<sup>5</sup> = 243, which is less than 256, we can fit five ternary values into one byte. This format only uses 1.6 bits per weight on average, which is 20 times less than using 32-bit floats!</p>

<p>It’s technically possible to quantize weights<a data-type="indexterm" data-primary="weights" data-secondary="quantization" id="id4371"/> down to a single bit each, storing 8 weights per byte: each bit represents a weight equal to either –1 or +1 (or sometimes 0 or 1). However, it’s very difficult to get reasonable accuracy using such severe quantization.</p>

<p>As you can see, PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="default weight representation" id="id4372"/>’s default weight representation (32-bit floats) takes up a <em>lot</em> of space compared to other representations: there is room for us to shrink our models quite a bit! Let’s start by reducing the precision from 32 bits down to 16 bits.<a data-type="indexterm" data-startref="xi_commonnumberrepresentations22153_1" id="id4373"/><a data-type="indexterm" data-startref="xi_mixedprecisionandquantizationcommonnumberrepresentations22153_1" id="id4374"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Reduced Precision Models"><div class="sect1" id="id378">
<h1>Reduced Precision Models</h1>

<p>If<a data-type="indexterm" data-primary="mixed-precision and quantization" data-secondary="reduced precision models" id="xi_mixedprecisionandquantizationreducedprecisionmodels22423_1"/> you have a 32-bit PyTorch model, you can convert all of its parameters to 16-bit floats<a data-type="indexterm" data-primary="fp16 (float16) data type" data-primary-sortas="fpb" id="id4375"/><a data-type="indexterm" data-primary="16-bit floats" data-primary-sortas="sixteen-bit floats" id="id4376"/><a data-type="indexterm" data-primary="floats" data-secondary="16-bit" id="id4377"/>—which is called <em>half-precision</em><a data-type="indexterm" data-primary="half-precision" id="id4378"/>—by calling the model’s <code translate="no">half()</code> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>
<code class="c1"># [...] pretend the 32-bit model is trained here</code>
<code class="n">model</code><code class="o">.</code><code class="n">half</code><code class="p">()</code>  <code class="c1"># convert the model parameters to half precision (16 bits)</code></pre>

<p>This is a quick and easy way to halve the size of a trained model, usually without much impact on its quality. Moreover, since many GPUs have 16-bit float optimizations, and since there will be less data to transfer between the CPU and the GPU, the model will typically run almost twice as fast.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When downloading a pretrained model using the Transformers<a data-type="indexterm" data-primary="Transformers library" data-secondary="downloading pretrained models" id="id4379"/> library’s <code translate="no">from_pretrained()</code> method, you can set <code translate="no">dtype="auto"</code> to let the library choose the optimal float representation for your hardware.</p>
</div>

<p>To use the model, you now need to feed it 16-bit inputs, and it will output 16-bit outputs as well:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">X</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code>  <code class="c1"># some 16-bit input</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>  <code class="c1"># 16-bit output</code></pre>

<p>But what if you want to build and train a 16-bit model<a data-type="indexterm" data-primary="16-bit models" data-primary-sortas="sixteen-bit models" id="id4380"/> right from the start? In this case, you can set <code translate="no">dtype=torch.float16</code> whenever you create a tensor or a module with parameters, for example:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
                      <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">))</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you prefer to avoid repeating <code translate="no">dtype=torch.float16</code> everywhere, then you can instead set the default data type to <code translate="no">torch.float16</code> using <code translate="no">torch.set_default_dtype(torch.float16)</code>. Be careful: this will apply to <em>all</em> tensors and modules created after that.</p>
</div>

<p>However, the reduced precision can cause some issues during training. Indeed, 16-bit floats have a limited <em>dynamic range</em><a data-type="indexterm" data-primary="dynamic range" id="id4381"/> (i.e., the ratio between the largest and smallest positive representable values): the smallest positive representable value is about 0.00000006 (i.e., 6.0e<sup>–8</sup>), while the largest is 65,504 (i.e., ~6.5e<sup>4</sup>). This implies that any gradient update smaller than ~6.0e<sup>–8</sup> will <em>underflow</em><a data-type="indexterm" data-primary="underflows" id="id4382"/>, meaning it will be rounded down to zero, and thus ignored. And conversely, any value larger than ~6.5e<sup>4</sup> will <em>overflow</em><a data-type="indexterm" data-primary="overflows" id="id4383"/>, meaning it will be rounded up to infinity, causing training to fail (once some weights are infinite, the loss will be infinite or NaN).</p>

<p>To avoid underflows, one solution is to scale up the loss by a large factor (e.g., multiply it by 256): this will automatically scale up the gradients<a data-type="indexterm" data-primary="gradients" data-secondary="in reduced-precision models" data-secondary-sortas="reduced-precision models" id="id4384"/> by the same factor during the backward pass, which will prevent them from being smaller than the smallest 16-bit representable value. However, you must scale the gradients back down before performing an optimizer step, and at this point you may get an underflow. Also, if you scale up the loss too much, you will run into overflows.</p>

<p>If you can’t find a good scaling factor that avoids both underflows and overflows, you can try to use <code translate="no">torch.bfloat16</code> rather than <code translate="no">torch.float16</code>, since bloat16 has more bits for the exponent: the smallest value is ~9.2e<sup>–41</sup>, while the largest is ~3.4e<sup>38</sup>, so there’s less risk of any significant gradient updates being ignored, or reasonable values being rounded up to infinity.</p>

<p>However, bfloat16<a data-type="indexterm" data-primary="bfloat16" id="id4385"/><a data-type="indexterm" data-primary="floats" data-secondary="bfloat" id="id4386"/> has historically had less hardware support (although this is improving), and it offers fewer bits for the fraction, which can cause some gradient updates to be ignored when the corresponding parameter values are much larger, causing training to stall. For example, if the gradient update is 4.5e<sup>–2</sup> (i.e., 0.045) and the corresponding parameter value is equal to 1.23e<sup>2</sup> (i.e., 123), then the sum should be 1.23045e<sup>2</sup> (i.e., 123.045) but bfloat16 does not have enough fraction bits to store all these digits, so it must round the result to 1.23e<sup>2</sup> (i.e., 123): as you can see, the gradient update is completely ignored. With regular 16-bit floats, the result would be 123.0625, which is not exactly right due to floating-point precision errors, but at least the parameter makes a step in the right direction. That said, if the gradient update was a bit smaller (e.g., 0.03), it would be ignored even in regular 16-bit float precision.</p>

<p>So if you try float16 and bfloat16 but you still encounter convergence issues during training, then you can try <em>mixed-precision training</em> instead.<a data-type="indexterm" data-startref="xi_mixedprecisionandquantizationreducedprecisionmodels22423_1" id="id4387"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Mixed-Precision Training"><div class="sect1" id="id379">
<h1>Mixed-Precision Training</h1>

<p><a href="https://homl.info/mpt"><em>Mixed-precision training</em> (MPT)</a> <a data-type="indexterm" data-primary="mixed-precision and quantization" data-secondary="mixed-precision training" id="xi_mixedprecisionandquantizationmixedprecisiontraining228757_1"/><a data-type="indexterm" data-primary="mixed-precision training (MPT)" id="xi_mixedprecisiontrainingMPT228757_1"/><a data-type="indexterm" data-primary="MPT (mixed-precision training)" id="xi_MPTmixedprecisiontraining228757_1"/>was proposed by Baidu and Nvidia researchers in 2017,⁠<sup><a data-type="noteref" id="id4388-marker" href="app02.html#id4388">4</a></sup> to address the issues often observed with 16-bit training. Here’s how it works:</p>

<ul>
<li>
<p>MPT stores a primary copy of the model parameters as 32-bit floats, and at each training iteration, it creates a 16-bit copy of these model parameters (see step 1 in <a data-type="xref" href="#mpt_diagram">Figure B-2</a>), and uses them for the forward pass (step 2).</p>
</li>
<li>
<p>The loss is then scaled up by a large factor (step 3) to avoid underflows, as we discussed earlier.</p>
</li>
<li>
<p>Lastly, we switch back to 32-bit precision to scale the gradients<a data-type="indexterm" data-primary="gradients" data-secondary="in mixed-precision training" data-secondary-sortas="mixed-precision" id="id4389"/> back down: this greater precision avoids the risk of underflow. Next we use the gradients to perform one optimizer step, improving the primary parameters (step 5). Performing the actual optimizer step in 32-bit precision ensures that small weight updates are not ignored when applied to much larger parameter values, since 32-bit floats have a very large fraction (23 bits).</p>
</li>
</ul>

<figure class="width-75"><div id="mpt_diagram" class="figure">
<img src="assets/hmls_ab02.png" alt="Diagram illustrating mixed-precision training, showing the process of copying 32-bit parameters to 16-bit, performing forward and backward passes, scaling losses, and completing with an optimizer step." width="898" height="635"/>
<h6><span class="label">Figure B-2. </span>Mixed-precision training</h6>
</div></figure>

<p>MPT offers almost all of the benefits of 16-bit training, without the instabilities. However, the model parameters take 50% more space than in 32-bit training because of the 16-bit copy at each training iteration, so how is this any better? Well, during training, most of the RAM is used to store the activations, not the model parameters, so in practice MPT requires just a bit more than half the RAM used by regular 32-bit training. And it typically runs twice as fast. Moreover, once training is finished, we no longer need 32-bit parameters, we can convert them to 16 bits, and we get a pure 16-bit model.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>MPT does not always accelerate training: it depends on the model, the batch size, and the hardware. That said, most large transformers are trained using MPT.</p>
</div>

<p>Rather than finding the best scaling factor by trial and error, you can run training in 32-bit precision for a little while (assuming you have enough RAM) and measure the gradient statistics to find the optimal scaling factor for your task: it should be large enough to avoid underflows, and small enough to avoid overflows.</p>

<p>Alternatively, your training script can adapt the factor dynamically during training: if some gradients<a data-type="indexterm" data-primary="gradients" data-secondary="in mixed-precision training" data-secondary-sortas="mixed-precision" id="id4390"/> are infinite or NaN, this means that an overflow occurred so the factor must be reduced (e.g., halved) and the training step must be skipped, but if no overflow is detected then the scaling factor can be gradually increased (e.g., doubled every 2,000 training steps). PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="mixed-precision training" id="id4391"/><a data-type="indexterm" data-primary="torch" data-secondary="amp.GradScaler" id="xi_torchampGradScaler22103380_1"/> provides a <code translate="no">torch.amp.GradScaler</code> class that implements this approach, and also scales down the learning rate appropriately.</p>

<p>PyTorch also provides a <code translate="no">torch.autocast()</code><a data-type="indexterm" data-primary="torch" data-secondary="autocast()" id="id4392"/> function that returns a context within which many operations will automatically run in 16-bit precision. This includes operations that typically benefit the most from 16-bit precision, such as matrix multiplication and convolutions, but it does not include operations like reductions (e.g., <code translate="no">torch.sum()</code>) since running these in half precision offers no significant benefit and can damage precision.</p>

<p>Let’s update our training function to run the forward pass within an autocast context and use a <code translate="no">GradScaler</code> to dynamically scale the loss:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">torch.amp</code> <code class="kn">import</code> <code class="n">GradScaler</code>

<code class="k">def</code> <code class="nf">train_mpt</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">,</code> <code class="n">n_epochs</code><code class="p">,</code>
              <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">,</code> <code class="n">init_scale</code><code class="o">=</code><code class="mf">2.0</code><code class="o">**</code><code class="mi">16</code><code class="p">):</code>
    <code class="n">grad_scaler</code> <code class="o">=</code> <code class="n">GradScaler</code><code class="p">(</code><code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">,</code> <code class="n">init_scale</code><code class="o">=</code><code class="n">init_scale</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
            <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="o">=</code> <code class="n">X_batch</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="n">y_batch</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
            <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">autocast</code><code class="p">(</code><code class="n">device_type</code><code class="o">=</code><code class="n">device</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">dtype</code><code class="p">):</code>
                <code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X_batch</code><code class="p">)</code>
                <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">y_pred</code><code class="p">,</code> <code class="n">y_batch</code><code class="p">)</code>
            <code class="n">grad_scaler</code><code class="o">.</code><code class="n">scale</code><code class="p">(</code><code class="n">loss</code><code class="p">)</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
            <code class="n">grad_scaler</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">optimizer</code><code class="p">)</code>
            <code class="n">grad_scaler</code><code class="o">.</code><code class="n">update</code><code class="p">()</code>
            <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>When fine-tuning a transformer using the Hugging Face Transformers library, you can set <code translate="no">fp16=True</code> or <code translate="no">bf16=True</code> in the <code translate="no">TrainingArguments</code> to activate mixed-precision training.</p>
</div>

<p>Reducing precision down to 16-bits often works great, but can we shrink our models even further? Yes, we can, using quantization.<a data-type="indexterm" data-startref="xi_mixedprecisionandquantizationmixedprecisiontraining228757_1" id="id4393"/><a data-type="indexterm" data-startref="xi_mixedprecisiontrainingMPT228757_1" id="id4394"/><a data-type="indexterm" data-startref="xi_MPTmixedprecisiontraining228757_1" id="id4395"/><a data-type="indexterm" data-startref="xi_torchampGradScaler22103380_1" id="id4396"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Quantization"><div class="sect1" id="id380">
<h1>Quantization</h1>

<p>Quantization<a data-type="indexterm" data-primary="mixed-precision and quantization" data-secondary="quantization" id="xi_mixedprecisionandquantizationquantization2213213_1"/><a data-type="indexterm" data-primary="parameters" data-seealso="quantization" id="id4397"/><a data-type="indexterm" data-primary="quantization" id="xi_quantization2213213_1"/> means mapping continuous values to discrete ones. In deep learning, this typically involves converting parameters, and often activations as well, from floats to integers—usually 32-bit floats to 8-bit integers. More generally, the goal is to shrink and speed up our model by reducing the number of bits used in parameters, and often in activations as well. Moreover, some embedded devices (e.g., ARM Cortex-M0) do not support floating-point operations at all (in part to reduce their cost and energy consumption), so models have to be quantized entirely (both weights and activations) before you can use them on the device. Modern smartphones do support floating point operations but still benefit significantly from quantization: int8 operations are 2 to 4 times faster and use 5 to 10 times less energy than FP32.</p>

<p>The simplest approach is <em>linear quantization</em>, so we’ll discuss it now. We will discuss a few nonlinear quantization methods later in this appendix.</p>








<section data-type="sect2" data-pdf-bookmark="Linear Quantization"><div class="sect2" id="id381">
<h2>Linear Quantization</h2>

<p>Linear quantization<a data-type="indexterm" data-primary="linear quantization" id="xi_linearquantization2213720_1"/> dates back to digital signal processing in the 1950s, but it has become particularly important in machine learning over the past decade since models have become gigantic, and yet we wish to run them on mobile phones and other limited devices. It has two variants: asymmetric and symmetric. In <em>asymmetric linear quantization</em><a data-type="indexterm" data-primary="asymmetric linear quantization" id="xi_asymmetriclinearquantization22137346_1"/>, float values are simply mapped linearly to unsigned bytes with values ranging from 0 to 255 (or more generally from 0 to 2<sup><em>n</em></sup> – 1 when quantizing to <em>n</em>-bit integers). For example, if the weights range between <em>a</em> = –0.1 and <em>b</em> = 0.6, then the float –0.1 will be mapped to the byte 0, the float 0.0 to integer 36, 0.1 to 72, …​, 0.6 to 255, and more generally, the float tensor <strong>w</strong> will be mapped to the integer tensor <strong>q</strong> using <a data-type="xref" href="#asymmetric_linear_quantization_equation">Equation B-1</a>.</p>
<div id="asymmetric_linear_quantization_equation" data-type="equation" class="less_space pagebreak-before">
<h5><span class="label">Equation B-1. </span>Asymmetric linear quantization</h5>
<math alttext="StartLayout 1st Row 1st Column q Subscript i 2nd Column equals round left-parenthesis StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis plus z 2nd Row 1st Column with s 2nd Column equals StartFraction b minus a Over 2 Superscript n Baseline minus 1 EndFraction and z equals minus round left-parenthesis StartFraction a Over s EndFraction right-parenthesis 3rd Row 1st Column where a 2nd Column equals min Underscript i Endscripts w Subscript i Baseline and b equals max Underscript i Endscripts w Subscript i Baseline EndLayout" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <msub><mi>q</mi> <mi>i</mi> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mtext>round</mtext>
          <mfenced separators="" open="(" close=")">
            <mfrac><msub><mi>w</mi> <mi>i</mi> </msub> <mi>s</mi></mfrac>
          </mfenced>
          <mo>+</mo>
          <mi>z</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mspace width="4.pt"/>
          <mtext>with</mtext>
          <mspace width="4.pt"/>
          <mi>s</mi>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>b</mi><mo>-</mo><mi>a</mi></mrow> <mrow><msup><mn>2</mn> <mi>n</mi> </msup><mo>-</mo><mn>1</mn></mrow></mfrac>
          </mstyle>
          <mspace width="4.pt"/>
          <mtext>and</mtext>
          <mspace width="4.pt"/>
          <mi>z</mi>
          <mo>=</mo>
          <mo>-</mo>
          <mtext>round</mtext>
          <mfenced separators="" open="(" close=")">
            <mfrac><mi>a</mi> <mi>s</mi></mfrac>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mspace width="4.pt"/>
          <mtext>where</mtext>
          <mspace width="4.pt"/>
          <mi>a</mi>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <munder><mo movablelimits="true" form="prefix">min</mo> <mi>i</mi></munder>
          <mspace width="0.166667em"/>
          <msub><mi>w</mi> <mi>i</mi> </msub>
          <mspace width="4.pt"/>
          <mtext>and</mtext>
          <mspace width="4.pt"/>
          <mi>b</mi>
          <mo>=</mo>
          <munder><mo movablelimits="true" form="prefix">max</mo> <mi>i</mi></munder>
          <mspace width="0.166667em"/>
          <msub><mi>w</mi> <mi>i</mi> </msub>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>w</em><sub><em>i</em></sub> is the <em>i</em><sup>th</sup> float in the original tensor <strong>w</strong>.</p>
</li>
<li>
<p><em>q</em><sub>i</sub> is the <em>i</em><sup>th</sup> integer in the quantized tensor <strong>q</strong>. It is clamped between 0 and 2<sup><em>n</em></sup> – 1 (e.g., 255 for 8-bit quantization).</p>
</li>
<li>
<p><em>s</em> is the <em>quantization scale</em>. Note that some authors define it as 1 / <em>s</em> and adapt the equation accordingly (i.e., they multiply rather than divide).</p>
</li>
<li>
<p><em>z</em> is the <em>quantization bias</em> or <em>zero point</em>.</p>
</li>
<li>
<p><em>a</em> is the minimum value of <strong>w</strong>, and <em>b</em> is the maximum value of <strong>w</strong>.</p>
</li>
</ul>

<p>The range [<em>a</em>, <em>b</em>] is known for weights, since their values do not change after training. However, the range of activation values depends on the inputs we feed to the model. As a result, for each activation that we want to quantize (e.g., the inputs of each layer), we will either have to compute <em>a</em> and <em>b</em> on the fly for each new input batch (this is called <em>dynamic quantization</em>)<a data-type="indexterm" data-primary="dynamic quantization" id="id4398"/> or run a calibration dataset once through the model to determine the typical range of activation values, then use this range to quantize the activations of all subsequent batches (this is called <em>static quantization</em>)<a data-type="indexterm" data-primary="static quantization" id="id4399"/>. Static quantization is a faster but less precise.</p>

<p>To approximately recover the original value <em>w</em><sub><em>i</em></sub> from a quantized value <em>q</em><sub><em>i</em></sub>, we can compute <em>w</em><sub><em>i</em></sub> ≈ s × (<em>q</em><sub><em>i</em></sub> – <em>z</em>). This is called <em>dequantization</em><a data-type="indexterm" data-primary="dequantization" id="id4400"/>. For example, if <em>q</em><sub><em>i</em></sub> = 72, then we get <em>w</em><sub><em>i</em></sub> ≈ 0.0988, which is indeed close to 0.1. The difference between the dequantized value (0.0988) and the original value (0.1) is called the <em>quantization noise</em><a data-type="indexterm" data-primary="quantization noise" id="id4401"/>: with 8-bit quantization, the quantization noise usually leads to a slightly degraded accuracy. With 6-bit, 4-bit, or less, the quantization noise can hurt even more, especially since it has a cumulative effect: the deeper the network, the stronger the impact.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a data-type="xref" href="#asymmetric_linear_quantization_equation">Equation B-1</a> guarantees that any float equal to 0.0 can be quantized and dequantized back to 0.0 exactly: indeed, if <em>w</em><sub><em>i</em></sub> = 0.0 then <em>q</em><sub><em>i</em></sub> = <em>z</em>, and dequantizing <em>q</em><sub><em>i</em></sub> gives back <em>w</em><sub><em>i</em></sub> = <em>s</em> × (<em>z</em> – <em>z</em>) = 0.0. This is particularly useful for sparse models where many weights are equal to zero. It is also important when using activations like ReLU which produce many zero activations.</p>
</div>

<p>In PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="quantized tensors" id="id4402"/>, the <code translate="no">torch.quantize_per_tensor()</code><a data-type="indexterm" data-primary="torch" data-secondary="quantize_per_tensor()" id="id4403"/> function lets you create a quantized tensor: this is a special kind of tensor that contains the quantized values (i.e., integers), as well as the <em>quantization parameters</em><a data-type="indexterm" data-primary="quantization parameters" id="id4404"/> (i.e., the scale and zero point). Let’s use this function to quantize a tensor, then dequantize it. In this example we will use the data type <code translate="no">torch.quint8</code>,<a data-type="indexterm" data-primary="torch" data-secondary="quint8" id="id4405"/> which uses 8-bit unsigned integers:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">w</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mf">0.1</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1</code><code class="p">,</code> <code class="mf">0.6</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">])</code>  <code class="c1"># 32-bit floats</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">s</code> <code class="o">=</code> <code class="p">(</code><code class="n">w</code><code class="o">.</code><code class="n">max</code><code class="p">()</code> <code class="o">-</code> <code class="n">w</code><code class="o">.</code><code class="n">min</code><code class="p">())</code> <code class="o">/</code> <code class="mf">255.</code>  <code class="c1"># compute the scale</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">z</code> <code class="o">=</code> <code class="o">-</code><code class="p">(</code><code class="n">w</code><code class="o">.</code><code class="n">min</code><code class="p">()</code> <code class="o">/</code> <code class="n">s</code><code class="p">)</code><code class="o">.</code><code class="n">round</code><code class="p">()</code>  <code class="c1"># compute the zero point</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">qw</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">quantize_per_tensor</code><code class="p">(</code><code class="n">w</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="n">s</code><code class="p">,</code> <code class="n">zero_point</code><code class="o">=</code><code class="n">z</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">quint8</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">qw</code>  <code class="c1"># this is a quantized tensor internally represented using integers</code><code class="w"/>
<code class="go">tensor([ 0.0988, -0.0988,  0.6012,  0.0000], size=(4,), dtype=torch.quint8,</code>
<code class="go">       quantization_scheme=torch.per_tensor_affine, scale=0.002745098201557994,</code>
<code class="go">       zero_point=36)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">qw</code><code class="o">.</code><code class="n">dequantize</code><code class="p">()</code>  <code class="c1"># back to 32-bit floats (close to the original tensor)</code><code class="w"/>
<code class="go">tensor([ 0.0988, -0.0988,  0.6012,  0.0000])</code></pre>

<p>Quantizing a model to 8-bits divides its size by almost 4. For example, suppose we have a convolutional layer with 64 kernels, 3 × 3 each, and it has 32 input channels. This layer requires 64 × 32 × 3 × 3 = 18,432 parameters (ignoring the bias terms). That’s 18,432 × 4 = 73,728 bytes before quantization, and just 18,432 bytes after quantization, plus 2 × 4 = 8 bytes to store <em>s</em> and <em>z</em> (indeed, they are both stored as 32-bit floats, so 4 bytes each).</p>
<div data-type="tip"><h6>Tip</h6>
<p>PyTorch also has a <code translate="no">torch.quantize_per_channel()</code><a data-type="indexterm" data-primary="torch" data-secondary="quantize_per_channel()" id="id4406"/> function which quantizes each channel separately: this offers better precision but requires a bit more space for the additional quantization parameters.<a data-type="indexterm" data-startref="xi_asymmetriclinearquantization22137346_1" id="id4407"/></p>
</div>

<p>When the float values are approximately symmetric around zero, we can use <em>symmetric linear quantization</em><a data-type="indexterm" data-primary="symmetric linear quantization" id="xi_symmetriclinearquantization22187106_1"/>, where the values are mapped between –127 and +127, or more generally between –<em>r</em> and +<em>r</em> with <em>r</em> = 2<sup><em>n</em>–1</sup> – 1, using <a data-type="xref" href="#symmetric_linear_quantization_equation">Equation B-2</a>.</p>
<div id="symmetric_linear_quantization_equation" data-type="equation">
<h5><span class="label">Equation B-2. </span>Symmetric linear quantization</h5>
<math alttext="q Subscript i Baseline equals round left-parenthesis StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis with s equals StartFraction max Underscript i Endscripts StartAbsoluteValue w Subscript i Baseline EndAbsoluteValue Over 2 Superscript n minus 1 Baseline minus 1 EndFraction" display="block">
  <mrow>
    <msub><mi>q</mi> <mi>i</mi> </msub>
    <mo>=</mo>
    <mtext>round</mtext>
    <mfenced separators="" open="(" close=")">
      <mfrac><msub><mi>w</mi> <mi>i</mi> </msub> <mi>s</mi></mfrac>
    </mfenced>
    <mspace width="4.pt"/>
    <mtext>with</mtext>
    <mspace width="4.pt"/>
    <mi>s</mi>
    <mo>=</mo>
    <mfrac><mrow><msub><mo movablelimits="true" form="prefix">max</mo> <mi>i</mi> </msub><mrow><mrow><mo>|</mo></mrow><msub><mi>w</mi> <mi>i</mi> </msub><mrow><mo>|</mo></mrow></mrow></mrow> <mrow><msup><mn>2</mn> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow> </msup><mo>-</mo><mn>1</mn></mrow></mfrac>
  </mrow>
</math>
</div>

<p>To implement symmetric linear quantization in PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="symmetric linear quantization" id="id4408"/>, we can use the <code translate="no">torch.quantize_per_tensor()</code> function again, but using a zero point equal to 0, and data type <code translate="no">qint8</code> (quantized signed 8-bit integer):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">w</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.94</code><code class="p">,</code> <code class="mf">0.92</code><code class="p">,</code> <code class="mf">0.93</code><code class="p">])</code>  <code class="c1"># 32-bit floats</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">s</code> <code class="o">=</code> <code class="n">w</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code><code class="o">.</code><code class="n">max</code><code class="p">()</code> <code class="o">/</code> <code class="mf">127.</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">qw</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">quantize_per_tensor</code><code class="p">(</code><code class="n">w</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="n">s</code><code class="p">,</code> <code class="n">zero_point</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">qint8</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">qw</code><code class="w"/>
<code class="go">tensor([ 0.0000, -0.9400,  0.9178,  0.9326], size=(4,), dtype=torch.qint8,</code>
<code class="go">       quantization_scheme=torch.per_tensor_affine, scale=0.007401574868708849,</code>
<code class="go">       zero_point=0)</code></pre>

<p><a data-type="xref" href="#symmetric_quantization_diagram">Figure B-3</a> shows some floats ranging between –0.94 and +0.93, quantized to signed bytes (i.e., 8-bits) ranging between –127 and +127,⁠<sup><a data-type="noteref" id="id4409-marker" href="app02.html#id4409">5</a></sup> using symmetric linear quantization. Notice that float 0.0 is always mapped to integer 0.</p>

<figure><div id="symmetric_quantization_diagram" class="figure">
<img src="assets/hmls_ab03.png" alt="Diagram showing the mapping of weights from floats between -0.94 and 0.93 to quantized bytes ranging from -127 to 127 using symmetric linear quantization." width="1341" height="384"/>
<h6><span class="label">Figure B-3. </span>Symmetric linear quantization</h6>
</div></figure>

<p>Symmetric mode is often a bit faster than asymmetric mode, because there’s no zero point <em>z</em> to worry about. However, if the values are not symmetric, part of the integer range will be wasted. For example, if all the weights are positive, then symmetric mode will only use bytes 0 to 127 (rather than –127 to 127). As a result, symmetric mode can be a bit less precise than asymmetric mode. In practice, symmetric mode is generally preferred for weights (which are often fairly symmetric), and asymmetric mode for activations (especially when using ReLU, since it outputs only nonnegative values).<a data-type="indexterm" data-startref="xi_symmetriclinearquantization22187106_1" id="id4410"/></p>

<p>Let’s now see how to quantize your models in practice using PyTorch’s <code>torch.​ao.quantization</code><a data-type="indexterm" data-primary="torch" data-secondary="ao.quantization" id="xi_torchaoquantization2221994_1"/> package. The first approach is to quantize a trained model, which is called <em>post-training quantization</em> (PTQ)<a data-type="indexterm" data-primary="post-training quantization (PTQ)" id="xi_PostTrainingQuantizationPTQ22219205_1"/><a data-type="indexterm" data-primary="PTQ (post-training quantization)" id="xi_PTQPostTrainingQuantization22219205_1"/>. The second is to train (or fine-tune) your model with some fake quantization to get it used to the noise: this is called <em>quantization-aware training</em> (QAT)<a data-type="indexterm" data-primary="QAT (Quantization-Aware Training)" id="id4411"/><a data-type="indexterm" data-primary="Quantization-Aware Training (QAT)" id="id4412"/>. Let’s start with PTQ.<a data-type="indexterm" data-startref="xi_linearquantization2213720_1" id="id4413"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Post-Training Quantization Using torch.ao.quantization"><div class="sect2" id="id410">
<h2>Post-Training Quantization Using torch.ao.quantization</h2>

<p>The <code translate="no">torch.ao</code><a data-type="indexterm" data-primary="torch" data-secondary="ao" id="id4414"/> package contains tools for architecture optimization (hence the name), including pruning, sparsity, and quantization. The <code translate="no">torch.ao.quantization</code> package offers two solutions to quantize trained models: dynamic quantization and static quantization. Let’s see how to implement both.</p>










<section data-type="sect3" data-pdf-bookmark="Dynamic quantization"><div class="sect3" id="id382">
<h3>Dynamic quantization</h3>

<p>Dynamic quantization<a data-type="indexterm" data-primary="dynamic quantization" id="id4415"/> is best for MLPs, RNNs, and transformers. To implement it using PyTorch’s <code translate="no">torch.ao.quantization</code> package, you must first choose a quantization engine<a data-type="indexterm" data-primary="quantization engines" id="id4416"/>: PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="quantization engine support" id="id4417"/> currently supports the <em>Facebook General Matrix Multiplication</em> (FBGEMM) engine for x86 CPUs, plus a newer x86 engine that supports recent x86 CPUs but is less battle-tested, and finally the <em>Quantized Neural Networks Package</em> (QNNPACK) engine for ARM/mobile. This code will pick the appropriate engine depending on the platform:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">platform</code>

<code class="n">machine</code> <code class="o">=</code> <code class="n">platform</code><code class="o">.</code><code class="n">machine</code><code class="p">()</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code>
<code class="n">engine</code> <code class="o">=</code> <code class="s2">"qnnpack"</code> <code class="k">if</code> <code class="p">(</code><code class="s2">"arm"</code> <code class="ow">in</code> <code class="n">machine</code> <code class="ow">or</code> <code class="s2">"aarch64"</code> <code class="ow">in</code> <code class="n">machine</code><code class="p">)</code> <code class="k">else</code> <code class="s2">"x86"</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>PyTorch does not offer an engine for CUDA<a data-type="indexterm" data-primary="CUDA library" id="id4418"/> or other hardware accelerators, but other libraries do, such as the bitsandbytes library (as we will see shortly).</p>
</div>

<p>Once you have selected an engine, you can use the <code translate="no">quantize_dynamic()</code><a data-type="indexterm" data-primary="torch" data-secondary="ao.quantization.quantize_dynamic()" id="id4419"/> function from the <code translate="no">torch.ao.quantization</code> package; just pass it your trained model, tell it the types of layers to quantize (typically just the <code translate="no">Linear</code> and RNN layers), specify the quantized data type, and boom, you have a ready-to-use quantized model:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">torch.ao.quantization</code> <code class="kn">import</code> <code class="n">quantize_dynamic</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>
<code class="c1"># [...] pretend the 32-bit model is trained here</code>
<code class="n">torch</code><code class="o">.</code><code class="n">backends</code><code class="o">.</code><code class="n">quantized</code><code class="o">.</code><code class="n">engine</code> <code class="o">=</code> <code class="n">engine</code>
<code class="n">qmodel</code> <code class="o">=</code> <code class="n">quantize_dynamic</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="p">{</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">},</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">qint8</code><code class="p">)</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">qmodel</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>  <code class="c1"># float inputs and outputs, but quantized internally</code></pre>

<p>The <code translate="no">quantize_dynamic()</code> function replaces each <code translate="no">Linear</code> layer with a <code translate="no">DynamicQuantizedLinear</code> layer, with int8 weights. This layer behaves just like a regular linear layer, with float inputs and outputs, but it quantizes its inputs on the fly (recomputing the zero points and scales for each batch), performs matrix multiplication using integers only (with 32-bit integer accumulators), and dequantizes the result so the next layer gets float inputs. Now let’s look at static quantization.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Static quantization"><div class="sect3" id="id383">
<h3>Static quantization</h3>

<p>This<a data-type="indexterm" data-primary="static quantization" id="xi_staticquantization222545_1"/> option is best for CNNs, and max inference speed. It’s also compulsory for edge devices without a <em>floating-point unit</em> (FPU), as they don’t support floats at all. Both the weights<a data-type="indexterm" data-primary="weights" data-secondary="quantization" id="id4420"/> and activations are prepared for quantization ahead of time, for all layers. As we discussed earlier, weights are constant so they can be quantized once, while activations require a calibration step to determine their typical range. The model is then converted to a fully quantized model. Here is how to implement it:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">torch.ao.quantization</code> <code class="kn">import</code> <code class="n">get_default_qconfig</code><code class="p">,</code> <code class="n">QuantStub</code><code class="p">,</code> <code class="n">DeQuantStub</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">QuantStub</code><code class="p">(),</code>
                      <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                      <code class="n">DeQuantStub</code><code class="p">())</code>
<code class="c1"># [...] pretend the 32-bit model is trained here</code>
<code class="n">model</code><code class="o">.</code><code class="n">qconfig</code> <code class="o">=</code> <code class="n">get_default_qconfig</code><code class="p">(</code><code class="n">engine</code><code class="p">)</code>
<code class="n">torch</code><code class="o">.</code><code class="n">ao</code><code class="o">.</code><code class="n">quantization</code><code class="o">.</code><code class="n">prepare</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">calibration_loader</code><code class="p">:</code>
    <code class="n">model</code><code class="p">(</code><code class="n">X_batch</code><code class="p">)</code>
<code class="n">torch</code><code class="o">.</code><code class="n">ao</code><code class="o">.</code><code class="n">quantization</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>Let’s go through this code step by step:</p>

<ul>
<li>
<p>After the imports, we create our 32-bit model, but this time we add a <code translate="no">QuantStub</code> layer as the first layer, and a <code translate="no">DeQuantStub</code> layer as the last. Both layers are just passthrough for now.</p>
</li>
<li>
<p>Next, the model can be trained normally (another option would be to take a pretrained model and place it between a <code translate="no">QuantStub</code> layer and a <code translate="no">DeQuantStub</code> layer).</p>
</li>
<li>
<p>Next, we set the model’s <code translate="no">qconfig</code> to the output of the <code translate="no">get_default_qconfig()</code> <a data-type="indexterm" data-primary="torch" data-secondary="ao.quantization.get_default_qconfig()" id="id4421"/>function: this function takes the name of the desired quantization engine and returns a <code translate="no">QConfig</code> object containing a default quantization configuration for this engine. It specifies the quantization data type (e.g., <code translate="no">torch.qint8</code>), the quantization scheme (e.g., symmetric linear quantization per tensor), and two functions that will observe the weights and activations to determine their ranges.</p>
</li>
<li>
<p>Next we call the <code translate="no">torch.ao.quantization.prepare()</code><a data-type="indexterm" data-primary="torch" data-secondary="ao.quantization.prepare()" id="id4422"/> function: it uses the weight observer specified in the configuration to determine the weights range, which it immediately uses to compute the zero points and scales for the weights. Since we don’t know what the input data looks like at this point, the function cannot compute the quantization parameters for the activations yet, so it inserts activation observers in the model itself: these are attached to the outputs of the <code translate="no">QuantStub</code> and <code translate="no">Linear</code> layers. The observer appended to the <code translate="no">QuantStub</code> layer is responsible for tracking the input range.</p>
</li>
<li>
<p>Next, we take a representative sample of input batches (i.e., the kind the model will get in production), and we pass these batches through the model: this allows the activation observers to track the activations.</p>
</li>
<li>
<p>Once we have given the model enough data, we finally call the <code>torch.ao.​quanti⁠zation.convert()</code><a data-type="indexterm" data-primary="torch" data-secondary="ao.quantization.convert()" id="id4423"/> function, which removes the observers from the model and replaces the layers with quantized versions. The <code translate="no">QuantStub</code> layer is replaced with a <code translate="no">Quantize</code> layer which will quantize the inputs. The <code translate="no">Linear</code> layers are replaced with <code translate="no">QuantizedLinear</code> layers. And the <code translate="no">DeQuantStub</code> layer is replaced with a <code translate="no">DeQuantize</code> layer which will dequantize the outputs.</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There are a few observers<a data-type="indexterm" data-primary="observers" id="id4424"/> to choose from: they can just keep track of the minimum and maximum values for each tensor (<code translate="no">MinMaxObserver</code>), or for each channel (<code translate="no">PerChannelMinMaxObserver</code>), or they can compute an exponential moving average of the min/max values, which reduces the impact of a few outliers. Finally, they can even record a histogram of the observed values (<code translate="no">HistogramObserver</code>), making it possible to find an optimal quantization range that minimizes the quantization error. That said, the default observers are usually fine.</p>
</div>

<p>We now have a model that we can use normally, with float inputs and outputs, but which works entirely with integers internally, making it lightweight and fast. To deploy it to mobile or embedded devices<a data-type="indexterm" data-primary="mobile or embedded devices, deploying to" id="id4425"/>, there are many options to choose from (which are beyond the scope of this book), including:</p>

<ul>
<li>
<p>Use ExecuTorch, which is PyTorch’s lightweight edge runtime</p>
</li>
<li>
<p>Export the model to ONNX and run it with ONNX Runtime (cross-platform)</p>
</li>
<li>
<p>Convert it to TFLite or TFLite Micro</p>
</li>
<li>
<p>Compile it for the target device using TVM or microTVM</p>
</li>
</ul>

<p>Moreover, the PyTorch team has released a separate library named <a href="https://homl.info/torchao"><em>PyTorch-native Architecture Optimization</em> (TorchAO)</a>,<a data-type="indexterm" data-primary="PyTorch-native Architecture Optimization (TorchAO)" id="id4426"/><a data-type="indexterm" data-primary="TorchAO (PyTorch-native Architecture Optimization)" id="id4427"/> designed to be a robust and extensible model optimization framework. Over time, many features in PyTorch’s <code translate="no">torch.ao</code> package are expected to be migrated to—or superseded by—TorchAO. The library already includes advanced features such as 4-bit weight support and <em>per-block quantization</em>,<a data-type="indexterm" data-primary="per-block quantization" id="id4428"/> in which each tensor is split into small blocks and each block is quantized independently, trading space for improved precision.</p>

<p>Post-training quantization (either dynamic or static) can shrink and speed up your models significantly, but it will also degrade their accuracy. This is particularly the case when quantizing down to 4 bits or less, and it’s worse for static quantization than for dynamic quantization (which can at least adapt to each input batch independently). When the accuracy drop is unacceptable, you can try quantization-aware training, as we will discuss now.<a data-type="indexterm" data-startref="xi_PostTrainingQuantizationPTQ22219205_1" id="id4429"/><a data-type="indexterm" data-startref="xi_PTQPostTrainingQuantization22219205_1" id="id4430"/><a data-type="indexterm" data-startref="xi_staticquantization222545_1" id="id4431"/><a data-type="indexterm" data-startref="xi_torchaoquantization2221994_1" id="id4432"/></p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Quantization-Aware Training (QAT)"><div class="sect2" id="id384">
<h2>Quantization-Aware Training (QAT)</h2>

<p>QAT<a data-type="indexterm" data-primary="QAT (Quantization-Aware Training)" id="xi_QATQuantizationAwareTraining222944_1"/><a data-type="indexterm" data-primary="Quantization-Aware Training (QAT)" id="xi_QuantizationAwareTrainingQAT222944_1"/> was introduced in a <a href="https://homl.info/qat">2017 paper</a> by Google researchers.⁠<sup><a data-type="noteref" id="id4433-marker" href="app02.html#id4433">6</a></sup> It rests upon a simple idea: why not introduce some fake quantization noise during training so the model can learn to cope with it? After training, we can then quantize the model for real, and it should remain fairly accurate. QAT also makes it possible to quantize more aggressively without losing too much accuracy, down to 4 bits, or even less. Sound promising? Let’s see how it can be done.</p>

<p>To add fake quantization noise to weights, we can simply quantize them and immediately dequantize them. For example, a weight equal to 0.42 might be quantized to the 4-bit integer 7, and immediately dequantized back to 0.39: we’ve successfully introduced quantization noise, and it’s precisely the quantization noise that we would get if the model were really quantized. This fake quantization operation can be executed at each training step, and it can also be applied to some of the activations (e.g., to each layer output).</p>

<p>However, there is one little problem: quantization involves rounding to the nearest integer, and the rounding operation has gradients equal to zero (or undefined at integer boundaries), so gradient descent cannot make any progress. Luckily, we can sidestep this issue by using the <em>straight-through estimator</em> (STE) trick: during the backward phase, we pretend that the fake quantization operation was just the identity function, so the gradients flow straight through it untouched. This works because the loss landscape is generally fairly smooth locally, so gradients are likely to be similar within a small region around the quantized value, including at the original value.</p>

<p>Implementing QAT in PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="Quantization-Aware Training" id="id4434"/> is fairly straightforward<a data-type="indexterm" data-primary="torch" data-secondary="ao.quantization.get_default_qat_qconfig()" id="id4435"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">torch.ao.quantization</code> <code class="kn">import</code> <code class="n">get_default_qat_qconfig</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">qconfig</code> <code class="o">=</code> <code class="n">get_default_qat_qconfig</code><code class="p">(</code><code class="n">engine</code><code class="p">)</code>
<code class="n">torch</code><code class="o">.</code><code class="n">ao</code><code class="o">.</code><code class="n">quantization</code><code class="o">.</code><code class="n">prepare_qat</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">train</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="p">[</code><code class="o">...</code><code class="p">])</code>  <code class="c1"># train the model normally</code>
<code class="n">torch</code><code class="o">.</code><code class="n">ao</code><code class="o">.</code><code class="n">quantization</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">(),</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>After the import, we create our model, set its <code translate="no">qconfig</code> attribute to the default QAT configuration object for the chosen quantization engine, then we call the <code translate="no">prepare_qat()</code><a data-type="indexterm" data-primary="torch" data-secondary="ao.quantization.prepare_qat()" id="id4436"/> function to add fake quantization operations to the model. This step also adds observers to determine the usual range of activation values. Next, we can train the model normally. Lastly, we switch the model to eval mode, and we call the <code translate="no">convert()</code> function to truly quantize it.</p>
<div data-type="tip"><h6>Tip</h6>
<p>QAT doesn’t have to be used during all of training: you can take a pretrained model and just fine-tune it for a few epochs using QAT, typically using a lower learning rate to avoid damaging the pretrained weights.</p>
</div>

<p>We’ve seen how to implement PTQ and QAT using PyTorch’s <code translate="no">torch.ao</code> package. However, it’s primarily designed for CPUs. What if you want to run an LLM on a GPU that doesn’t quite have enough RAM? One option is to use the TorchAO library, which has growing GPU support. Another is to use the bitsandbytes library: let’s discuss it now.<a data-type="indexterm" data-startref="xi_QATQuantizationAwareTraining222944_1" id="id4437"/><a data-type="indexterm" data-startref="xi_QuantizationAwareTrainingQAT222944_1" id="id4438"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Quantizing LLMs Using the bitsandbytes Library"><div class="sect2" id="id385">
<h2>Quantizing LLMs Using the bitsandbytes Library</h2>

<p>The bitsandbytes library (bnb)<a data-type="indexterm" data-primary="bnb (bitsandbytes library)" id="xi_bnbbitsandbyteslibrary2232044_1"/><a data-type="indexterm" data-primary="bitsandbytes library (bnb)" id="xi_bitsandbyteslibrarybnb2232044_1"/><a data-type="indexterm" data-primary="large language models (LLMs)" id="xi_largelanguagemodelsLLMs2232044_1"/><a data-type="indexterm" data-primary="LLMs (large language models)" id="xi_LLMslargelanguagemodels2232044_1"/>, created by Tim Dettmers, is designed to make it easier to train and run large models on GPUs with limited VRAM. For this, it offers:</p>

<ul>
<li>
<p>Quantization tools, including 4-bit quantization, block-wise quantization, and more</p>
</li>
<li>
<p>Memory-efficient versions of popular optimizers such as Adam or AdamW, that operate on 8-bit tensors</p>
</li>
<li>
<p>Custom CUDA kernels written specifically for 8-bit or 4-bit quantized models, for maximum speed</p>
</li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The bitsandbytes library is designed for Nvidia GPUs. It also has some limited support for CPUs and AMD GPUs.</p>
</div>

<p>For example, let’s see how to implement post-training static quantization down to 4 bits. If you are using Colab, you must first install the bitsandbytes library using <code translate="no">%pip install bitsandbytes</code>, then run this <a data-type="indexterm" data-primary="Transformers library" data-secondary="quantizing LLMs with bitsandbytes" id="id4439"/>code:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoModelForCausalLM</code><code class="p">,</code> <code class="n">BitsAndBytesConfig</code>

<code class="n">model_id</code> <code class="o">=</code> <code class="s2">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</code>
<code class="n">bnb_config</code> <code class="o">=</code> <code class="n">BitsAndBytesConfig</code><code class="p">(</code><code class="n">load_in_4bit</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">bnb_4bit_quant_type</code><code class="o">=</code><code class="s2">"nf4"</code><code class="p">,</code>
                                <code class="n">bnb_4bit_compute_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">bfloat16</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">AutoModelForCausalLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="n">model_id</code><code class="p">,</code> <code class="n">device_map</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">,</code>
                                             <code class="n">quantization_config</code><code class="o">=</code><code class="n">bnb_config</code><code class="p">)</code></pre>

<p class="pagebreak-before">This code starts by importing the necessary classes from the Transformers library (introduced in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter 14</a>), then it creates a <code translate="no">BitsAndBytesConfig</code><a data-type="indexterm" data-primary="BitsAndBytesConfig" id="id4440"/><a data-type="indexterm" data-primary="Transformers library" data-secondary="BitsAndBytesConfig" id="id4441"/> object, which I will explain shortly. Lastly, it downloads a pretrained model (in this case a 1.1 billion parameter version of Llama named TinyLlama<a data-type="indexterm" data-primary="TinyLlama" id="id4442"/>, fine-tuned for chat), specifying the desired quantization configuration.</p>

<p>Under the hood, the Transformers library uses the bitsandbytes library to quantize the model weights down to 4 bits just as they are loaded into the GPU: no extra step is required. You can now use this model normally to generate text (see <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>). During inference, whenever some weights are needed, they are dequantized on the fly to the type specified by the <code translate="no">bnb_4bit_compute_dtype</code> argument (<code translate="no">bfloat16</code> in this case), and the computations are performed in this higher precision. As soon as the dequantized weights are no longer needed, they are dropped, so memory usage remains low.</p>

<p>In this example, the <code translate="no">BitsAndBytesConfig</code> object specifies <em>4-bit Normal Float</em> (NF4) quantization<a data-type="indexterm" data-primary="4-bit Normal Float (NF4) quantization" data-primary-sortas="four-bit normal float" id="id4443"/><a data-type="indexterm" data-primary="NF4 (4-bit Normal Float) quantization" id="id4444"/> using <code translate="no">bfloat16</code> for computations. NF4 is a nonlinear 4-bit scheme where each of the 16 possible integer values represents a specific float value between –1 and +1. Instead of being equally spaced (as in linear quantization), these values correspond to the quantiles of the normal distribution centered on zero: this means that they are closer together near zero. This improves accuracy because model weights tend to follow a normal distribution centered on zero, so having more precision near zero is helpful.</p>

<p>NF4 was introduced as part of <a href="https://homl.info/qlora">QLoRA</a>,⁠<sup><a data-type="noteref" id="id4445-marker" href="app02.html#id4445">7</a></sup> a technique that quantizes a frozen pretrained model with NF4, then uses LoRA adapters (see <a data-type="xref" href="ch17.html#speedup_chapter">Chapter 17</a>) for fine-tuning, along with activation checkpointing (see <a data-type="xref" href="ch12.html#cnn_chapter">Chapter 12</a>). This approach drastically reduces VRAM usage and compute: the authors managed to fine-tune a 65-billion parameter model using a single GPU with 48 GB of RAM, with only a small accuracy drop. Although activation checkpointing reduces VRAM usage overall, it can lead to memory spikes when processing batches with long sequences. To deal with such spikes, the QLoRA authors also introduced <em>paged optimizers</em><a data-type="indexterm" data-primary="paged optimizers" id="id4446"/> which take advantage of Nvidia unified memory: the CUDA driver automatically moves pages of data from GPU VRAM to CPU RAM whenever needed. Lastly, the authors also used <em>double quantization</em><a data-type="indexterm" data-primary="double quantization" id="id4447"/>, meaning that the quantization parameters themselves were quantized to save a bit more VRAM.</p>

<p>For more details on 4-bit quantization in the Hugging Face ecosystem, check out this <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">great post by the QLoRA authors and other contributors</a>.<a data-type="indexterm" data-startref="xi_bnbbitsandbyteslibrary2232044_1" id="id4448"/><a data-type="indexterm" data-startref="xi_bitsandbyteslibrarybnb2232044_1" id="id4449"/><a data-type="indexterm" data-startref="xi_largelanguagemodelsLLMs2232044_1" id="id4450"/><a data-type="indexterm" data-startref="xi_LLMslargelanguagemodels2232044_1" id="id4451"/><a data-type="indexterm" data-startref="xi_mixedprecisionandquantizationquantization2213213_1" id="id4452"/><a data-type="indexterm" data-startref="xi_quantization2213213_1" id="id4453"/></p>
</div></section>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Using Pre-Quantized Models"><div class="sect1" id="id386">
<h1>Using Pre-Quantized Models</h1>

<p>Many<a data-type="indexterm" data-primary="mixed-precision and quantization" data-secondary="pre-quantized models" id="xi_mixedprecisionandquantizationprequantizedmodels223525_1"/><a data-type="indexterm" data-primary="pre-quantized models" id="xi_prequantizedmodels223525_1"/> popular pretrained models have already been quantized and published online, in particular on the Hugging Face Hub. For example, Tom Jobbins, better known by his Hugging Face username TheBloke, has published thousands of quantized models available at <a href="https://huggingface.co/TheBloke" class="bare"><em class="hyperlink">https://huggingface.co/TheBloke</em></a>. Many of these models were quantized using one of the following modern methods:</p>
<dl>
<dt><em>Generative pre-training quantization</em> (GPTQ)</dt>
<dd>
<p><a href="https://homl.info/gptq">GPTQ</a>⁠<sup><a data-type="noteref" id="id4454-marker" href="app02.html#id4454">8</a></sup> <a data-type="indexterm" data-primary="generative pre-training quantization (GPTQ)" id="id4455"/><a data-type="indexterm" data-primary="GPTQ (generative pre-training quantization)" id="id4456"/>is a post-training quantization method, usually down to 4 bits, that treats quantization as an optimization problem. GPTQ goes through each layer, one by one, and optimizes the 4-bit weights to minimize the MSE between the layer’s original outputs (i.e., using the full precision weights) and the approximate outputs (i.e., using the 4-bit weights). Once the optimal 4-bit weights are found, the approximate outputs are passed to the next layer, and the process is repeated all the way to the output layer. During inference, the weights are dequantized whenever they are needed. GPTQ only quantizes the weights, not the activations: this is called <em>weight-only quantization</em>,<a data-type="indexterm" data-primary="weight-only quantization" id="id4457"/> which is great for inference, not for training. You can use the <a href="https://huggingface.co/docs/optimum">Hugging Face Optimum library</a> or the <a href="https://github.com/ModelCloud/GPTQModel">GPTQModel library</a> to quantize your models with GPTQ.</p>
</dd>
<dt><em>Activation-aware Weight Quantization</em> (AWQ)</dt>
<dd>
<p><a href="https://homl.info/awq">AWQ</a>⁠<sup><a data-type="noteref" id="id4458-marker" href="app02.html#id4458">9</a></sup> <a data-type="indexterm" data-primary="Activation-aware Weight Quantization (AWQ)" id="id4459"/><a data-type="indexterm" data-primary="AWQ (Activation-aware Weight Quantization)" id="id4460"/>aims to improve the accuracy of block-wise weight-only quantization (typically 4-bit quantization). The idea is to preserve the precision of the most important weights. To identify these so-called <em>salient weights</em>,<a data-type="indexterm" data-primary="salient weights" id="id4461"/><a data-type="indexterm" data-primary="weights" data-secondary="salient" id="id4462"/> the algorithm runs a calibration dataset through the model and finds the largest activations for each quantization group (e.g., the largest 0.1% to 1% activations), and the corresponding weights are considered salient. The authors observed that storing the salient weights using float16 greatly reduces the model’s <em>perplexity</em> <a data-type="indexterm" data-primary="perplexity" id="id4463"/>(a common metric equal to the exponential of the cross-entropy). However, mixing 4-bit and 16-bit weights is not hardware-friendly, so AWQ uses another method to preserve the salient weight’s precision: they simply scale them up by some factor and add an operation in the model to scale down the corresponding activations (but this operation can generally be fused into the previous operation). Rather than using a fixed scaling factor, AWQ performs a search for the optimal factor, leading to the lowest quantization error. To implement AWQ, you can use the Hugging Face Optimum library.</p>
</dd>
</dl>
<dl class="less_space pagebreak-before">
<dt>Llama.cpp quantization using the <em>GPT-Generated Unified Format</em> (GGUF)</dt>
<dd>
<p><a href="https://homl.info/gguf">GGUF</a> <a data-type="indexterm" data-primary="GGUF (GPT-Generated Unified Format)" id="id4464"/><a data-type="indexterm" data-primary="GPT-Generated Unified Format (GGUF)" id="id4465"/>is a binary file format designed to store LLMs efficiently. It was introduced by Georgi Gerganov, the creator of llama.cpp, and it supersedes previous file formats such as GGML, GGMF, and GGJT. A GGUF file includes the weights, the tokenizer, special tokens, the model architecture, the vocabulary size, and other metadata. Llama.cpp offers quantizers (e.g., using the <code translate="no">quantize</code> tool) to convert the model weights to one of GGUF’s supported quantized formats, such as Q4_K_M. Q4 stands for 4-bit quantization, K stands for per-block quantization (typically 32 or 64 weights per block depending on the chosen format), and M means medium size and precision for this quantization level (other options are S = Small and L = Large). There are also more recent and efficient quantization options such as Importance-aware Quantization (IQ),<a data-type="indexterm" data-primary="Importance-aware Quantization (IQ)" id="id4466"/><a data-type="indexterm" data-primary="IQ (Importance-aware Quantization)" id="id4467"/> which uses various techniques to improve accuracy (e.g., nonlinear quantization), and Ternary Quantization (TQ).</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>On the Hugging Face Hub, every repository is backed by Git, so it has branches and commits. When you call <code translate="no">from_pretrained()</code><a data-type="indexterm" data-primary="from_pretrained()" id="id4468"/>, the model is fetched from the default branch, which is almost always <code translate="no">main</code>. But quantized models are often placed in a different branch. When calling <code translate="no">from_pretrained()</code>, you can choose a branch, a tag, or even a commit hash, by using the <code translate="no">revision</code> argument. Check the model card for the list of available files and versions. For GGUF models, you must specify the filename using the <code translate="no">gguf_file</code> argument.</p>
</div>

<p>In conclusion, reduced precision, mixed-precision training, and quantization are arguably the most important tools to allow large models to run on limited hardware. But there are many more, including the following:</p>

<ul>
<li>
<p>You could tweak the model’s architecture before training, by reducing the number of layers, or the number of neurons per layer, or by sharing weights across layers (e.g., as in the ALBERT model, introduced in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>).</p>
</li>
<li>
<p>If you have a large trained model, you can shrink it by removing some of its weights, for example the ones with the smallest magnitude, or the ones with the smallest effect on the loss. You can also remove whole channels, layers, or attention heads. This is called <em>model pruning</em><a data-type="indexterm" data-primary="model pruning" id="id4469"/>, and you can implement it using the <code translate="no">torch.nn.utils.prune</code><a data-type="indexterm" data-primary="torch" data-secondary="nn.utils.prune" id="id4470"/> module, or the Hugging Face Optimum library.</p>
</li>
<li>
<p>As we saw in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter 15</a>, you can also use a large trained model as a teacher to train a smaller model: this is called distillation<a data-type="indexterm" data-primary="distillation, model" id="id4471"/>.</p>
</li>
<li>
<p>A trained model can also be shrunk by fusing some of its layers, removing redundancy. For example, a batch-norm layer<a data-type="indexterm" data-primary="batch-norm layers" id="id4472"/> (introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>) performs a linear operation, so if it comes immediately after a linear layer, you can fuse both layers into a single linear layer. Similarly, you can fuse a convolutional layer<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="convolutional layers" id="id4473"/> followed by a batch-norm layer into a single convolutional layer. This only works after training, since the batch-norm layer must compute running averages during training. You can implement layer fusion with the <code translate="no">torch.quantization.fuse_modules()</code> function, or with the Hugging Face Optimum library. In any case, make sure to fuse layers <em>before</em> quantizing your model: less layers means less quantization noise.</p>
</li>
<li>
<p>You can use low-rank approximations<a data-type="indexterm" data-primary="low-rank approximations" id="id4474"/>, where a large matrix is replaced by the product of two smaller ones. For example, replace a large linear layer such as <code translate="no">Linear(10_000, 20_000)</code> with two linear layers <code translate="no">Linear(10_000, 100)</code> and <code translate="no">Linear(100, 20_000)</code>. This reduces the number of parameters from about 200 million down to just three million, and also drastically reduces computations. The intermediate dimensionality (100 in this example) is a hyperparameter you can tune to balance accuracy and model size. This technique can be performed after training by factorizing the weight matrix using SVD (see the notebook for an example).<a data-type="indexterm" data-startref="xi_mixedprecisionandquantization2243_1" id="id4475"/><a data-type="indexterm" data-startref="xi_mixedprecisionandquantizationprequantizedmodels223525_1" id="id4476"/><a data-type="indexterm" data-startref="xi_prequantizedmodels223525_1" id="id4477"/></p>
</li>
</ul>

<p>Give these techniques a try: shrink the models!</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Chapter 17 and Appendices C, D, and E are available online at <a href="https://homl.info" class="bare"><em class="hyperlink">https://homl.info</em></a>.</p>
</div>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id4346"><sup><a href="app02.html#id4346-marker">1</a></sup> In general, –0 and +0 are considered equal, but some operations give different results, for example 1 / –0 = –infinity, while 1 / +0 = +infinity.</p><p data-type="footnote" id="id4353"><sup><a href="app02.html#id4353-marker">2</a></sup> Some high-performance computing applications deactivate subnormal numbers because they slow down computations, and normalized numbers are generally sufficient (e.g., normalized fp32 can represent numbers as small as ±1.2e<sup>–38</sup>).</p><p data-type="footnote" id="id4362"><sup><a href="app02.html#id4362-marker">3</a></sup> The <em>M</em> stands for <em>mantissa</em>,<a data-type="indexterm" data-primary="mantissa" id="id4478"/> which is a term often used as a synonym for fraction. Unfortunately, it’s also used as a synonym for significand, leading to some confusion. This is why IEEE 754 no longer uses the term mantissa.</p><p data-type="footnote" id="id4388"><sup><a href="app02.html#id4388-marker">4</a></sup> P. Micikevicius et al., “Mixed Precision Training”, arXiv preprint 2017, ICLR (2018).</p><p data-type="footnote" id="id4409"><sup><a href="app02.html#id4409-marker">5</a></sup> PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="restricted symmetric quantization" id="id4479"/> implements <em>restricted symmetric quantization</em><a data-type="indexterm" data-primary="restricted symmetric quantization" id="id4480"/>, meaning that it excludes the lowest possible signed integer (e.g., –128 for 8-bit integers) to ensure that the range is symmetric (e.g., –127 to +127). Some other implementations allow the full signed byte range (from –128 to +127): this is called <em>unrestricted symmetric quantization</em>.<a data-type="indexterm" data-primary="unrestricted symmetric quantization" id="id4481"/> These implementations also subtract 0.5 instead of 1 in the denominator of <a data-type="xref" href="#symmetric_linear_quantization_equation">Equation B-2</a>.</p><p data-type="footnote" id="id4433"><sup><a href="app02.html#id4433-marker">6</a></sup> Benoit Jacob et al., “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference”, arXiv preprint arXiv:1712.05877 (2017)”.</p><p data-type="footnote" id="id4445"><sup><a href="app02.html#id4445-marker">7</a></sup> Tim Dettmers et al., “QLORA: Efficient Finetuning of Quantized LLMs”, arXiv preprint arXiv:2305.14314 (2023).</p><p data-type="footnote" id="id4454"><sup><a href="app02.html#id4454-marker">8</a></sup> Elias Frantar et al., “GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers”, arXiv preprint arXiv:2210.17323 (2022).</p><p data-type="footnote" id="id4458"><sup><a href="app02.html#id4458-marker">9</a></sup> Ji Lin et al., “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration”, arXiv preprint arXiv:2306.00978 (2023).</p></div></div></section></div></div></body></html>