- en: Chapter 9\. Introduction to Artificial Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 人工神经网络简介
- en: Birds inspired us to fly, burdock plants inspired Velcro, and nature has inspired
    countless more inventions. It seems only logical, then, to look at the brain’s
    architecture for inspiration on how to build an intelligent machine. This is the
    logic that sparked *artificial neural networks* (ANNs), machine learning models
    inspired by the networks of biological neurons found in our brains. However, although
    planes were inspired by birds, they don’t have to flap their wings to fly. Similarly,
    ANNs have gradually become quite different from their biological cousins. Some
    researchers even argue that we should drop the biological analogy altogether (e.g.,
    by saying “units” rather than “neurons”), lest we restrict our creativity to biologically
    plausible systems.⁠^([1](ch09.html#id2093))
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 鸟类激励我们飞翔，蒺藜植物启发了维可牢（Velcro），而自然界也启发了无数其他发明。因此，从大脑的架构中寻找灵感来构建智能机器似乎是合乎逻辑的。这就是激发*人工神经网络*（ANNs）的逻辑，这些是受我们大脑中发现的生物神经元网络启发的机器学习模型。然而，尽管飞机是受鸟类启发的，但它们不需要拍打翅膀就能飞翔。同样，ANNs逐渐与它们的生物表亲大相径庭。一些研究人员甚至认为，我们应该完全放弃生物类比（例如，用“单元”而不是“神经元”来表示），以免将我们的创造力限制在生物可能性的系统中。⁠^([1](ch09.html#id2093))
- en: ANNs are at the very core of deep learning. They are versatile, powerful, and
    scalable, making them ideal to tackle large and highly complex machine learning
    tasks such as classifying billions of images (e.g., Google Images), powering speech
    recognition services (e.g., Apple’s Siri or Google Assistant) and chatbots (e.g.,
    ChatGPT or Claude), recommending the best videos to watch to hundreds of millions
    of users every day (e.g., YouTube), or learning how proteins fold (DeepMind’s
    AlphaFold).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs是深度学习的核心。它们多功能、强大且可扩展，非常适合处理大型且高度复杂的机器学习任务，例如对数十亿张图片进行分类（例如，谷歌图片），提供语音识别服务（例如，苹果的Siri或谷歌助手）和聊天机器人（例如，ChatGPT或Claude），每天为上亿用户推荐最佳视频观看（例如，YouTube），或者学习蛋白质如何折叠（DeepMind的AlphaFold）。
- en: This chapter introduces artificial neural networks, starting with a quick tour
    of the very first ANN architectures and leading up to multilayer perceptrons (MLPs),
    which are heavily used today (many other architectures will be explored in the
    following chapters). In this chapter, we will implement simple MLPs using Scikit-Learn
    to get our feet wet, and in the next chapter we will switch to PyTorch, as it
    is a much more flexible and efficient library for neural nets.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了人工神经网络，从对第一个ANN架构的快速浏览开始，一直介绍到今天广泛使用的多层感知器（MLPs）。在本章中，我们将使用Scikit-Learn实现简单的MLPs，以熟悉相关概念，在下一章中，我们将转向PyTorch，因为它是一个更灵活、更高效的神经网络库。
- en: Now let’s go back in time to the origins of artificial neural networks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到人工神经网络起源的时代。
- en: From Biological to Artificial Neurons
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从生物神经元到人工神经元
- en: 'Surprisingly, ANNs have been around for quite a while: they were first introduced
    back in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter
    Pitts. In their [landmark paper](https://homl.info/43),⁠^([2](ch09.html#id2094))
    “A Logical Calculus of Ideas Immanent in Nervous Activity”, McCulloch and Pitts
    presented a simplified computational model of how biological neurons might work
    together in animal brains to perform complex computations using *propositional
    logic*. This was the first artificial neural network architecture. Since then
    many other architectures have been invented, as you will see.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，人工神经网络（ANNs）已经存在了相当长的时间：它们最早是在1943年由神经生理学家沃伦·麦克洛奇（Warren McCulloch）和数学家沃尔特·皮茨（Walter
    Pitts）首次提出的。在他们的[里程碑式论文](https://homl.info/43)，⁠^([2](ch09.html#id2094)) “内在于神经活动的逻辑演算”，麦克洛奇和皮茨提出了一种简化的计算模型，描述了生物神经元如何在动物大脑中协同工作以使用*命题逻辑*执行复杂计算。这是第一个人工神经网络架构。从那时起，许多其他架构也被发明出来，正如你将看到的。
- en: The early successes of ANNs led to the widespread belief that we would soon
    be conversing with truly intelligent machines. When it became clear in the 1960s
    that this promise would go unfulfilled (at least for quite a while), funding flew
    elsewhere, and ANNs entered a long winter. In the early 1980s, new architectures
    were invented and better training techniques were developed, sparking a revival
    of interest in *connectionism*, the study of neural networks. But progress was
    slow, and by the 1990s other powerful machine learning techniques had been invented,
    such as support vector machines. These techniques seemed to offer better results
    and stronger theoretical foundations than ANNs, so once again the study of neural
    networks was put on hold.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs的早期成功导致人们普遍相信我们很快就能与真正智能的机器进行对话。然而，在20世纪60年代，当人们意识到这一承诺将无法实现（至少在相当长的一段时间内）时，资金流向了其他地方，ANNs进入了漫长的寒冬。在20世纪80年代初，发明了新的架构，并开发了更好的训练技术，这激发了人们对*连接主义*（神经网络的研究）的兴趣。但进展缓慢，到20世纪90年代，其他强大的机器学习技术已被发明，例如支持向量机。这些技术似乎比ANNs提供了更好的结果和更强的理论基础，因此，对神经网络的研究再次被搁置。
- en: 'We are now witnessing yet another wave of interest in ANNs. Will this wave
    die out like the previous ones did? Well, here are a few good reasons to believe
    that this time is different and that the renewed interest in ANNs will have a
    much more profound impact on our lives:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正在见证对ANNs的又一波兴趣。这波浪潮会像之前的那样消亡吗？好吧，这里有几个很好的理由相信这次情况不同，并且对ANNs的新一轮兴趣将对我们的生活产生更深远的影响：
- en: There is now a huge quantity of data available to train neural networks, and
    ANNs frequently outperform other ML techniques on very large and complex problems.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在可供训练神经网络的可用数据量巨大，并且人工神经网络（ANNs）在处理非常庞大和复杂的问题时，通常优于其他机器学习技术。
- en: 'The tremendous increase in computing power since the 1990s now makes it possible
    to train large neural networks in a reasonable amount of time. This is in part
    due to Moore’s law (the number of components in integrated circuits has doubled
    about every 2 years over the last 50 years), but also thanks to the gaming industry,
    which has stimulated the production of powerful *graphical processing units* (GPUs)
    by the millions: GPU cards were initially designed to accelerate graphics, but
    it turns out that neural networks perform similar computations (such as large
    matrix multiplications), so they can also be accelerated using GPUs. Moreover,
    cloud platforms have made this power accessible to everyone.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自20世纪90年代以来计算能力的巨大增长，现在使得在合理的时间内训练大型神经网络成为可能。这部分得益于摩尔定律（在过去50年中，集成电路中的组件数量大约每两年翻一番），也得益于游戏行业，它通过数百万台强大的*图形处理单元*（GPUs）刺激了生产：GPU卡最初是为了加速图形而设计的，但结果证明神经网络执行类似的计算（例如大型矩阵乘法），因此它们也可以通过GPU加速。此外，云平台使这种力量对每个人都可以访问。
- en: The training algorithms have been improved. To be fair they are only slightly
    different from the ones used in the 1990s, but these relatively small tweaks have
    had a huge positive impact.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练算法已经得到了改进。公平地说，它们与20世纪90年代使用的算法只有细微的差别，但这些相对小的调整产生了巨大的积极影响。
- en: 'Some theoretical limitations of ANNs have turned out to be benign in practice.
    For example, many people thought that ANN training algorithms were doomed because
    they were likely to get stuck in local optima, but it turns out that this is not
    a big problem in practice, especially for larger neural networks: the local optima
    often perform almost as well as the global optimum.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANNs的一些理论局限性在实践中证明是无害的。例如，许多人认为ANN的训练算法注定要失败，因为它们很可能会陷入局部最优，但实际情况并非如此，尤其是在较大的神经网络中：局部最优通常表现几乎与全局最优一样好。
- en: 'The invention of the Transformer architecture in 2017 (see [Chapter 15](ch15.html#transformer_chapter))
    has been a game changer: it can process and generate all sorts of data (e.g.,
    text, images, audio) unlike earlier, more specialized, architectures, and it performs
    great across a wide variety of tasks from robotics to protein folding. Moreover,
    it scales rather well, which has made it possible to train very large *foundation
    models* that can be reused across many different tasks, possibly with a bit of
    fine-tuning (that’s transfer learning), or just by prompting the model in the
    right way (that’s *in-context learning*, or ICL). For instance, you can give it
    a few examples of the task at hand (that’s *few-shot learning*, or FSL), or ask
    it to reason step-by-step (that’s *chain-of-thought* prompting, or CoT). It’s
    a new world!'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2017年Transformer架构的发明（见第15章[Transformer章节](ch15.html#transformer_chapter)）是一场变革：它可以处理和生成各种数据（例如，文本、图像、音频），这与早期更专业化的架构不同，并且它在从机器人学到蛋白质折叠的广泛任务上表现出色。此外，它的扩展性相当好，这使得训练非常大的**基础模型**成为可能，这些模型可以在许多不同的任务中重复使用，可能只需要一点微调（这就是迁移学习），或者通过以正确的方式提示模型（这就是**情境学习**，或ICL）。例如，你可以给它一些当前任务的示例（这就是**少样本学习**，或FSL），或者要求它逐步推理（这就是**思维链提示**，或CoT）。这是一个全新的世界！
- en: 'ANNs seem to have entered a virtuous circle of funding and progress. Amazing
    products based on ANNs regularly make the headline news, which pulls more and
    more attention and funding toward them, resulting in more and more progress and
    even more amazing products. AI is no longer just powering products in the shadows:
    since chatbots such as ChatGPT were released, the general public is now directly
    interacting daily with AI assistants, and the big tech companies are competing
    fiercely to grab this gigantic market: the pace of innovation is wild.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络似乎已经进入了一个资金和进步的良性循环。基于神经网络的惊人产品经常成为头条新闻，这吸引了越来越多的关注和资金，从而带来了更多的进步和更加惊人的产品。AI不再只是默默无闻地推动产品：自从ChatGPT等聊天机器人发布以来，公众现在每天都在直接与人工智能助手互动，大型科技公司正在激烈竞争以抢占这个巨大的市场：创新的速度是疯狂的。
- en: Biological Neurons
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生物神经元
- en: Before we discuss artificial neurons, let’s take a quick look at a biological
    neuron (represented in [Figure 9-1](#biological_neuron_wikipedia)). It is an unusual-looking
    cell mostly found in animal brains. It’s composed of a *cell body* containing
    the nucleus and most of the cell’s complex components, many branching extensions
    called *dendrites*, plus one very long extension called the *axon*. The axon’s
    length may be just a few times longer than the cell body, or up to tens of thousands
    of times longer. Near its extremity the axon splits off into many branches called
    *telodendria*, and at the tip of these branches are minuscule structures called
    *synaptic terminals* (or simply *synapses*), which are connected to the dendrites
    or cell bodies of other neurons.⁠^([3](ch09.html#id2106)) Biological neurons produce
    short electrical impulses called *action potentials* (APs, or just *signals*),
    which travel along the axons and make the synapses release chemical signals called
    *neurotransmitters*. When a neuron receives a sufficient amount of these neurotransmitters
    within a few milliseconds, it fires its own electrical impulses (actually, it
    depends on the neurotransmitters, as some of them inhibit the neuron from firing).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论人工神经元之前，让我们快速看一下生物神经元（如图9-1所示[生物神经元维基百科](#biological_neuron_wikipedia)）。它是一种外形不寻常的细胞，主要存在于动物大脑中。它由一个包含细胞核和细胞大部分复杂成分的**细胞体**组成，许多分支称为**树突**，以及一个非常长的分支称为**轴突**。轴突的长度可能只是细胞体的几倍长，或者长达数万倍。在其末端，轴突分裂成许多称为**终树突**的分支，在这些分支的尖端是微小的结构，称为**突触末端**（或简称**突触**），它们连接到其他神经元的树突或细胞体。⁠^([3](ch09.html#id2106))
    生物神经元产生短暂的电脉冲，称为**动作电位**（APs，或简称**信号**），这些信号沿着轴突传播，并使突触释放称为**神经递质**的化学信号。当一个神经元在几毫秒内接收到足够的这些神经递质时，它会发出自己的电脉冲（实际上，这取决于神经递质，因为其中一些会抑制神经元放电）。
- en: '![Illustration of a biological neuron highlighting key components such as the
    cell body, dendrites, axon, telodendria, and synaptic terminals, demonstrating
    the neuron''s structure and connection points within neural networks.](assets/hmls_0901.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![生物神经元示意图，突出显示关键组件，如细胞体、树突、轴突、终树突和突触末端，展示神经元在神经网络中的结构和连接点。](assets/hmls_0901.png)'
- en: Figure 9-1\. A biological neuron⁠^([4](ch09.html#id2110))
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1. 生物神经元⁠^([4](ch09.html#id2110))
- en: Thus, individual biological neurons seem to behave in a simple way, but they’re
    organized in a vast network of billions, with each neuron typically connected
    to thousands of other neurons. Highly complex computations can be performed by
    a network of fairly simple neurons, much like a complex anthill can emerge from
    the combined efforts of simple ants. The architecture of biological neural networks
    (BNNs)⁠^([5](ch09.html#id2111)) is the subject of active research, but some parts
    of the brain have been mapped. These efforts show that neurons are often organized
    in consecutive layers, especially in the cerebral cortex (the outer layer of the
    brain), as shown in [Figure 9-2](#biological_neural_network_wikipedia).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单个生物神经元似乎以简单的方式行事，但它们被组织在一个由数十亿个神经元组成的庞大网络中，每个神经元通常连接到成千上万个其他神经元。一个由相对简单的神经元组成的网络可以执行高度复杂的计算，就像一个复杂的蚁群可以从简单蚂蚁的共同努力中产生一样。生物神经网络（BNNs）的架构是活跃研究的话题，但大脑的一些部分已经被绘制出来。这些努力表明，神经元通常按顺序组织在连续的层中，特别是在大脑皮层（大脑的外层），如图[图9-2](#biological_neural_network_wikipedia)所示。
- en: '![Illustration of layered neuron networks in the human cerebral cortex, emphasizing
    the complex organization of biological neural networks.](assets/hmls_0902.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![展示人类大脑皮层中分层神经元网络，强调生物神经网络的复杂组织。](assets/hmls_0902.png)'
- en: Figure 9-2\. Multiple layers in a biological neural network (human cortex)⁠^([6](ch09.html#id2115))
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2. 生物神经网络（人类皮层）中的多层⁠^([6](ch09.html#id2115))
- en: Logical Computations with Neurons
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元逻辑计算
- en: 'McCulloch and Pitts proposed a very simple model of the biological neuron,
    which later became known as an *artificial neuron*: it has one or more binary
    (on/off) inputs and one binary output. The artificial neuron activates its output
    when more than a certain number of its inputs are active. In their paper, McCulloch
    and Pitts showed that even with such a simplified model it is possible to build
    a network of artificial neurons that can compute any logical proposition you want.
    To see how such a network works, let’s build a few ANNs that perform various logical
    computations (see [Figure 9-3](#nn_propositional_logic_diagram)), assuming that
    a neuron is activated when at least two of its input connections are active.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 麦克洛奇和皮茨提出了一种非常简单的生物神经元模型，后来被称为**人工神经元**：它有一个或多个二进制（开/关）输入和一个二进制输出。当其输入中超过一定数量的输入活跃时，人工神经元会激活其输出。在他们的论文中，麦克洛奇和皮茨表明，即使在这种简化的模型中，也可以构建一个可以计算任何所需逻辑命题的人工神经元网络。为了了解这样一个网络是如何工作的，让我们构建几个执行各种逻辑计算的ANN（人工神经网络），假设一个神经元在其至少两个输入连接活跃时才会被激活（参见[图9-3](#nn_propositional_logic_diagram)）。
- en: '![Diagram showing artificial neural networks performing logical computations,
    including AND, OR, and NOT operations with neurons labeled A, B, and C.](assets/hmls_0903.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![展示人工神经网络执行逻辑计算，包括与、或和非操作的图，神经元标记为A、B和C。](assets/hmls_0903.png)'
- en: Figure 9-3\. ANNs performing simple logical computations
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3. 执行简单逻辑计算的ANN
- en: 'Let’s see what these networks do:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些网络能做什么：
- en: 'The first network on the left is the identity function: if neuron A is activated,
    then neuron C gets activated as well (since it receives two input signals from
    neuron A); but if neuron A is off, then neuron C is off as well.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧的第一个网络是恒等函数：如果神经元A被激活，那么神经元C也会被激活（因为它从神经元A接收两个输入信号）；但如果神经元A关闭，那么神经元C也会关闭。
- en: 'The second network performs a logical AND: neuron C is activated only when
    both neurons A and B are activated (a single input signal is not enough to activate
    neuron C).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个网络执行逻辑与操作：神经元C只有在神经元A和神经元B都被激活时才会被激活（单个输入信号不足以激活神经元C）。
- en: 'The third network performs a logical OR: neuron C gets activated if either
    neuron A or neuron B is activated (or both).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个网络执行逻辑或操作：如果神经元A或神经元B（或两者）被激活，则神经元C会被激活。
- en: 'Finally, if we suppose that an input connection can inhibit the neuron’s activity
    (which is the case with biological neurons), then the fourth network computes
    a slightly more complex logical proposition: neuron C is activated only if neuron
    A is active and neuron B is off. If neuron A is active all the time, then you
    get a logical NOT: neuron C is active when neuron B is off, and vice versa.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，如果我们假设一个输入连接可以抑制神经元的活性（这在生物神经元中是情况），那么第四个网络计算的是一个稍微复杂一些的逻辑命题：神经元C只有在神经元A活跃且神经元B关闭时才会被激活。如果神经元A始终活跃，那么你得到一个逻辑非：神经元C在神经元B关闭时活跃，反之亦然。
- en: You can imagine how these networks can be combined to compute complex logical
    expressions (see the exercises at the end of the chapter for an example).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象这些网络如何组合起来计算复杂的逻辑表达式（请参阅本章末尾的练习以获取示例）。
- en: The Perceptron
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**感知器**'
- en: 'The *perceptron* is one of the simplest ANN architectures, invented in 1957
    by Frank Rosenblatt. It is based on a slightly different artificial neuron (see
    [Figure 9-4](#artificial_neuron_diagram)) called a *threshold logic unit* (TLU),
    or sometimes a *linear threshold unit* (LTU). The inputs and output are numbers
    (instead of binary on/off values), and each input connection is associated with
    a weight. The TLU first computes a linear function of its inputs: *z* = *w*[1]
    *x*[1] + *w*[2] *x*[2] + ⋯ + *w*[*n*] *x*[*n*] + *b* = **w**^⊺ **x** + *b*. Then
    it applies a *step function* to the result: *h*[**w**](**x**) = step(*z*). So
    it’s almost like logistic regression, except it uses a step function instead of
    the logistic function.⁠^([7](ch09.html#id2122)) Just like in logistic regression,
    the model parameters are the input weights **w** and the bias term *b*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*感知器* 是最简单的ANN架构之一，由Frank Rosenblatt于1957年发明。它基于一种略微不同的人工神经元（见[图9-4](#artificial_neuron_diagram)），称为*阈值逻辑单元*（TLU），或有时称为*线性阈值单元*（LTU）。输入和输出都是数字（而不是二进制的开/关值），每个输入连接都与一个权重相关联。TLU首先计算其输入的线性函数：*z*
    = *w*[1] *x*[1] + *w*[2] *x*[2] + ⋯ + *w*[*n*] *x*[*n*] + *b* = **w**^⊺ **x**
    + *b*。然后它将一个*步函数*应用于结果：*h*[**w**](**x**) = step(*z*)。所以它几乎就像逻辑回归一样，除了它使用步函数而不是逻辑函数。⁠^([7](ch09.html#id2122))
    就像在逻辑回归中一样，模型参数是输入权重 **w** 和偏置项 *b*。'
- en: '![Diagram of a Threshold Logic Unit (TLU) showing how inputs multiplied by
    weights are summed with a bias, and a step function is applied to determine the
    output.](assets/hmls_0904.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![阈值逻辑单元（TLU）的示意图，显示输入乘以权重后与偏置相加，并应用步函数以确定输出。](assets/hmls_0904.png)'
- en: 'Figure 9-4\. TLU: an artificial neuron that computes a weighted sum of its
    inputs **w**^⊺ **x**, plus a bias term *b*, then applies a step function'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4\. TLU：一种计算输入加权总和 **w**^⊺ **x**，加上偏置项 *b*，然后应用步函数以确定输出的人工神经元
- en: The most common step function used in perceptrons is the *Heaviside step function*
    (see [Equation 9-1](#step_functions_equation)). Sometimes the sign function is
    used instead.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器中最常用的步函数是*海维塞德步函数*（见[方程9-1](#step_functions_equation)）。有时也使用符号函数。
- en: Equation 9-1\. Common step functions used in perceptrons (assuming threshold
    = 0)
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程9-1\. 感知器中使用的常见步函数（假设阈值 = 0）
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo form="prefix">heaviside</mo>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd>
    <mtd columnalign="left"><mrow><mo form="prefix">sgn</mo> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>+</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo form="prefix">heaviside</mo>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd>
    <mtd columnalign="left"><mrow><mo form="prefix">sgn</mo> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>+</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable>
- en: A single TLU can be used for simple linear binary classification. It computes
    a linear function of its inputs, and if the result exceeds a threshold, it outputs
    the positive class. Otherwise, it outputs the negative class. This may remind
    you of logistic regression ([Chapter 4](ch04.html#linear_models_chapter)) or linear
    SVM classification (see the online chapter on SVMs at [*https://homl.info*](https://homl.info)).
    You could, for example, use a single TLU to classify iris flowers based on petal
    length and width. Training such a TLU would require finding the right values for
    *w*[1], *w*[2], and *b* (the training algorithm is discussed shortly).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 单个TLU可用于简单的线性二进制分类。它计算其输入的线性函数，如果结果超过阈值，则输出正类。否则，输出负类。这可能会让你想起逻辑回归 ([第 4 章](ch04.html#linear_models_chapter))
    或线性SVM分类（请参阅关于SVMs的在线章节 [*https://homl.info*](https://homl.info)）。例如，你可以使用单个TLU根据花瓣长度和宽度对鸢尾花进行分类。训练这样的TLU需要找到
    *w*[1]、*w*[2] 和 *b*（训练算法将在稍后讨论）的正确值。
- en: A perceptron is composed of one or more TLUs organized in a single layer, where
    every TLU is connected to every input. Such a layer is called a *fully connected
    layer*, or a *dense layer*. The inputs constitute the *input layer*. And since
    the layer of TLUs produces the final outputs, it is called the *output layer*.
    For example, a perceptron with two inputs and three outputs is represented in
    [Figure 9-5](#perceptron_diagram).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个感知器由一个或多个组织在单层中的TLU（阈值逻辑单元）组成，其中每个TLU都与每个输入相连。这样的层被称为 *全连接层* 或 *密集层*。输入构成
    *输入层*。由于TLU层产生最终输出，因此它被称为 *输出层*。例如，具有两个输入和三个输出的感知器在 [图 9-5](#perceptron_diagram)
    中表示。
- en: '![Diagram of a perceptron architecture with two input neurons connected to
    three output neurons in a fully connected layer, illustrating TLUs in the output
    layer.](assets/hmls_0905.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![具有两个输入神经元连接到全连接层中三个输出神经元的感知器架构图，展示了输出层中的TLU。](assets/hmls_0905.png)'
- en: Figure 9-5\. Architecture of a perceptron with two inputs and three output neurons
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. 具有两个输入和三个输出神经元的感知器架构
- en: This perceptron can classify instances simultaneously into three different binary
    classes, which makes it a multilabel classifier. It may also be used for multiclass
    classification.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个感知器可以将实例同时分类到三个不同的二进制类别中，这使得它成为一个多标签分类器。它也可以用于多类分类。
- en: Thanks to the magic of linear algebra, [Equation 9-2](#neural_network_layer_equation)
    can be used to efficiently compute the outputs of a layer of artificial neurons
    for several instances at once.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了线性代数的魔力，[方程式 9-2](#neural_network_layer_equation) 可以用来高效地一次性计算多个实例中人工神经元层的输出。
- en: Equation 9-2\. Computing the outputs of a fully connected layer
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 9-2\. 计算全连接层的输出
- en: $ModifyingAbove bold upper Y With caret equals phi left-parenthesis bold upper
    X bold upper W plus bold b right-parenthesis$
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $ModifyingAbove bold upper Y With caret equals phi left-parenthesis bold upper
    X bold upper W plus bold b right-parenthesis$
- en: 'In this equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: $ModifyingAbove bold upper Y With caret$ is the output matrix. It has one row
    per instance and one column per neuron.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $ModifyingAbove bold upper Y With caret$ 是输出矩阵。它每一行代表一个实例，每一列代表一个神经元。
- en: '**X** is the input matrix. It has one row per instance and one column per input
    feature.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X** 是输入矩阵。它每一行代表一个实例，每一列代表一个输入特征。'
- en: The weight matrix **W** contains all the connection weights. It has one row
    per input feature and one column per neuron.⁠^([8](ch09.html#id2128))
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵 **W** 包含所有连接权重。它每一行代表一个输入特征，每一列代表一个神经元。⁠^([8](ch09.html#id2128))
- en: 'The bias vector **b** contains all the bias terms: one per neuron.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置向量 **b** 包含所有偏置项：每个神经元一个。
- en: 'The function *ϕ* is called the *activation function*: when the artificial neurons
    are TLUs, it is a step function (we will discuss other activation functions shortly).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 *ϕ* 被称为 *激活函数*：当人工神经元是TLU时，它是一个步进函数（我们将在稍后讨论其他激活函数）。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In mathematics, the sum of a matrix and a vector is undefined. However, in
    data science, we allow “broadcasting”: adding a vector to a matrix means adding
    it to every row in the matrix. So, **XW** + **b** first multiplies **X** by **W**—which
    results in a matrix with one row per instance and one column per output—then adds
    the vector **b** to every row of that matrix, which adds each bias term to the
    corresponding output, for every instance. Moreover, *ϕ* is then applied itemwise
    to each item in the resulting matrix.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，矩阵和向量的和是未定义的。然而，在数据科学中，我们允许“广播”：将向量加到矩阵上意味着将其加到矩阵的每一行。因此，**XW** + **b**
    首先将 **X** 乘以 **W**——这会产生一个矩阵，其中每行对应一个实例，每列对应一个输出——然后将向量 **b** 加到该矩阵的每一行，这样每个偏差项就加到了每个实例的相应输出上。此外，*ϕ*
    然后逐项应用于结果矩阵中的每个元素。
- en: So, how is a perceptron trained? The perceptron training algorithm proposed
    by Rosenblatt was largely inspired by *Hebb’s rule*. In his 1949 book, *The Organization
    of Behavior* (Wiley), Donald Hebb suggested that when a biological neuron triggers
    another neuron often, the connection between these two neurons grows stronger.
    Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire
    together, wire together”; that is, the connection weight between two neurons tends
    to increase when they fire simultaneously. This rule later became known as Hebb’s
    rule (or *Hebbian learning*). Perceptrons are trained using a variant of this
    rule that takes into account the error made by the network when it makes a prediction;
    the perceptron learning rule reinforces connections that help reduce the error.
    More specifically, the perceptron is fed one training instance at a time, and
    for each instance it makes its predictions. For every output neuron that produced
    a wrong prediction, it reinforces the connection weights from the inputs that
    would have contributed to the correct prediction. The rule is shown in [Equation
    9-3](#perceptron_update_rule).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，感知器是如何训练的呢？Rosenblatt 提出的感知器训练算法在很大程度上受到了 *Hebb 定律* 的启发。在1949年的著作《行为组织》（Wiley）中，Donald
    Hebb 建议，当一个生物神经元经常触发另一个神经元时，这两个神经元之间的连接会变得更强。Siegrid Löwel后来用吸引人的短语总结了Hebb的想法，“一起放电的细胞，一起连接”；也就是说，当两个神经元同时放电时，它们之间的连接权重往往会增加。这个规则后来被称为Hebb定律（或*Hebbian学习*）。感知器使用这个规则的变体进行训练，该变体考虑了网络在做出预测时犯的错误；感知器学习规则加强了有助于减少错误的连接。更具体地说，感知器一次被喂给一个训练实例，并为每个实例做出预测。对于每个产生错误预测的输出神经元，它加强了那些本应有助于正确预测的输入的连接权重。该规则在[方程9-3](#perceptron_update_rule)中显示。
- en: Equation 9-3\. Perceptron learning rule (weight update)
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程9-3\. 感知器学习规则（权重更新）
- en: $w Subscript i comma j Baseline Superscript left-parenthesis next step right-parenthesis
    Baseline equals w Subscript i comma j Baseline plus eta left-parenthesis y Subscript
    j Baseline minus ModifyingAbove y With caret Subscript j Baseline right-parenthesis
    x Subscript i$
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: $w_{i,j}^{(next)} = w_{i,j}^{(base)} + \eta (y_{j}^{(base)} - \hat{y}_{j}^{(base)})
    x_i$
- en: 'In this equation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*w*[*i*,] [*j*] is the connection weight between the *i*^(th) input and the
    *j*^(th) neuron.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[*i*,] [*j*] 是第 *i* 个输入和第 *j* 个神经元之间的连接权重。'
- en: '*x*[*i*] is the *i*^(th) input value of the current training instance.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[*i*] 是当前训练实例的第 *i* 个输入值。'
- en: $ModifyingAbove y With caret Subscript j$ is the output of the *j*^(th) output
    neuron for the current training instance.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\hat{y}_{j}^{(base)}$ 是当前训练实例的第 *j* 个输出神经元的输出。
- en: '*y*[*j*] is the target output of the *j*^(th) output neuron for the current
    training instance.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[*j*] 是当前训练实例的第 *j* 个输出神经元的期望输出。'
- en: '*η* is the learning rate (see [Chapter 4](ch04.html#linear_models_chapter)).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*η* 是学习率（见[第4章](ch04.html#linear_models_chapter)）。'
- en: The decision boundary of each output neuron is linear, so perceptrons are incapable
    of learning complex patterns (just like logistic regression classifiers). However,
    if the training instances are linearly separable, Rosenblatt demonstrated that
    this algorithm will converge to a solution.⁠^([9](ch09.html#id2132)) This is called
    the *perceptron convergence theorem*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出神经元的决策边界是线性的，因此感知器无法学习复杂模式（就像逻辑回归分类器一样）。然而，如果训练实例是线性可分的，Rosenblatt 证明了该算法将收敛到解。⁠^([9](ch09.html#id2132))
    这被称为 *感知器收敛定理*。
- en: 'Scikit-Learn provides a `Perceptron` class that can be used pretty much as
    you would expect—for example, on the iris dataset (introduced in [Chapter 4](ch04.html#linear_models_chapter)):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一个`Perceptron`类，可以像预期的那样使用——例如，在鸢尾花数据集（在第4章中介绍）上：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You may have noticed that the perceptron learning algorithm strongly resembles
    stochastic gradient descent (introduced in [Chapter 4](ch04.html#linear_models_chapter)).
    In fact, Scikit-Learn’s `Perceptron` class is equivalent to using an `SGDClassifier`
    with the following hyperparameters: `loss="perceptron"`, `learning_rate="constant"`,
    `eta0=1` (the learning rate), and `penalty=None` (no regularization).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，感知器学习算法与随机梯度下降（在第4章中介绍）非常相似。事实上，Scikit-Learn的`Perceptron`类等同于使用具有以下超参数的`SGDClassifier`：`loss="perceptron"`，`learning_rate="constant"`，`eta0=1`（学习率），和`penalty=None`（无正则化）。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Contrary to logistic regression classifiers, perceptrons do not output a class
    probability. This is one reason to prefer logistic regression over perceptrons.
    Moreover, perceptrons do not use any regularization by default, and training stops
    as soon as there are no more prediction errors on the training set, so the model
    typically does not generalize as well as logistic regression or a linear SVM classifier.
    However, perceptrons may train a bit faster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归分类器不同，感知器不会输出一个类概率。这是选择逻辑回归而不是感知器的一个原因。此外，感知器默认不使用任何正则化，并且训练会在训练集上没有更多预测错误时停止，因此模型通常不如逻辑回归或线性SVM分类器泛化得好。然而，感知器的训练可能更快一些。
- en: In their 1969 monograph, *Perceptrons*, Marvin Minsky and Seymour Papert highlighted
    a number of serious weaknesses of perceptrons—in particular, the fact that they
    are incapable of solving some trivial problems (e.g., the *exclusive OR* (XOR)
    classification problem; see the left side of [Figure 9-6](#xor_diagram)). This
    is true of any other linear classification model (such as logistic regression
    classifiers), but researchers had expected much more from perceptrons, and some
    were so disappointed that they dropped neural networks altogether in favor of
    more formal approaches such as logic, problem solving, and search. The lack of
    practical applications also didn’t help.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的1969年专著《感知器》中，Marvin Minsky和Seymour Papert强调了感知器的一些严重弱点——特别是它们无法解决一些简单问题的事实（例如，*异或*（XOR）分类问题；参见[图9-6](#xor_diagram)的左侧）。这同样适用于任何其他线性分类模型（如逻辑回归分类器），但研究人员原本对感知器寄予厚望，有些人因此大失所望，甚至完全放弃了神经网络，转而采用更正式的方法，如逻辑、问题解决和搜索。缺乏实际应用也没有帮助。
- en: It turns out that some of the limitations of perceptrons can be eliminated by
    stacking multiple perceptrons. The resulting ANN is called a *multilayer perceptron*
    (MLP).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，通过堆叠多个感知器可以消除感知器的一些局限性。由此产生的ANN被称为*多层感知器*（MLP）。
- en: The Multilayer Perceptron and Backpropagation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层感知器和反向传播
- en: 'An MLP can solve the XOR problem, as you can verify by computing the output
    of the MLP represented on the righthand side of [Figure 9-6](#xor_diagram): with
    inputs (0, 0) or (1, 1), the network outputs 0, and with inputs (0, 1) or (1,
    0) it outputs 1\. Try verifying that this network indeed solves the XOR problem!^([10](ch09.html#id2139))'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个MLP可以解决XOR问题，你可以通过计算[图9-6](#xor_diagram)右侧表示的MLP的输出来验证：当输入为(0, 0)或(1, 1)时，网络输出0，而当输入为(0,
    1)或(1, 0)时，它输出1。尝试验证这个网络确实解决了XOR问题！^([10](ch09.html#id2139))
- en: An MLP is composed of one input layer, one or more layers of artificial neurons
    (originally TLUs) called *hidden layers*, and one final layer of artificial neurons
    called the *output layer* (see [Figure 9-7](#mlp_diagram)). The layers close to
    the input layer are usually called the *lower layers*, and the ones close to the
    outputs are usually called the *upper layers*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: MLP由一个输入层、一个或多个称为*隐藏层*的人工神经元层（最初是TLU）和一个称为*输出层*的最终人工神经元层组成（参见[图9-7](#mlp_diagram)）。靠近输入层的层通常被称为*下层*，而靠近输出的层通常被称为*上层*。
- en: '![Diagram illustrating the XOR classification problem and an MLP using threshold
    logic units to solve it.](assets/hmls_0906.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用阈值逻辑单元解决XOR分类问题的MLP的示意图](assets/hmls_0906.png)'
- en: Figure 9-6\. XOR classification problem and an MLP that solves it
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6. XOR分类问题和解决它的MLP
- en: '![Diagram of a multilayer perceptron showing a feedforward neural network with
    two input neurons, a hidden layer of four neurons, and three output neurons.](assets/hmls_0907.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器的示意图，展示了一个具有两个输入神经元、一个包含四个神经元的隐藏层和三个输出神经元的正向神经网络。](assets/hmls_0907.png)'
- en: Figure 9-7\. Architecture of a multilayer perceptron with two inputs, one hidden
    layer of four neurons, and three output neurons
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. 具有两个输入、一个包含四个神经元的隐藏层和三个输出神经元的多层感知器架构
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The signal flows only in one direction (from the inputs to the outputs), so
    this architecture is an example of a *feedforward neural network* (FNN).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 信号只在一个方向上流动（从输入到输出），因此这种架构是*正向神经网络*（FNN）的一个例子。
- en: When an ANN contains a deep stack of hidden layers,⁠^([11](ch09.html#id2146))
    it is called a *deep neural network* (DNN). The field of deep learning studies
    DNNs, and more generally it is interested in models containing deep stacks of
    computations. Even so, many people talk about deep learning whenever neural networks
    are involved (even shallow ones).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个人工神经网络包含深层堆叠的隐藏层时⁠^([11](ch09.html#id2146))，它被称为*深度神经网络*（DNN）。深度学习领域研究DNN，更广泛地说，它对包含深层计算堆叠的模型感兴趣。即便如此，每当涉及到神经网络（即使是浅层神经网络）时，许多人都会谈论深度学习。
- en: For many years researchers struggled to find a way to train MLPs, without success.
    In the early 1960s several researchers discussed the possibility of using gradient
    descent to train neural networks, but as we saw in [Chapter 4](ch04.html#linear_models_chapter),
    this requires computing the gradients of the model’s error with regard to the
    model parameters; it wasn’t clear at the time how to do this efficiently with
    such a complex model containing so many parameters, especially with the computers
    they had back then.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，研究人员一直在努力寻找一种训练MLP的方法，但未能成功。在20世纪60年代初，几位研究人员讨论了使用梯度下降来训练神经网络的可行性，但正如我们在[第4章](ch04.html#linear_models_chapter)中看到的，这需要计算模型误差相对于模型参数的梯度；在当时，如何以有效的方式计算这样一个包含许多参数的复杂模型的梯度并不清楚，尤其是在他们那时的计算机条件下。
- en: Then, in 1970, a researcher named Seppo Linnainmaa introduced in his master’s
    thesis a technique to compute all the gradients automatically and efficiently.
    This algorithm is now called *reverse-mode automatic differentiation* (or *reverse-mode
    autodiff* for short). In just two passes through the network (one forward, one
    backward), it is able to compute the gradients of the neural network’s error with
    regard to every single model parameter. In other words, it can find out how each
    connection weight and each bias should be tweaked in order to reduce the neural
    network’s error. These gradients can then be used to perform a gradient descent
    step. If you repeat this process of computing the gradients automatically and
    taking a gradient descent step, the neural network’s error will gradually drop
    until it eventually reaches a minimum. This combination of reverse-mode autodiff
    and gradient descent is now called *backpropagation* (or *backprop* for short).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在1970年，一位名叫Seppo Linnainmaa的研究者在他的硕士论文中介绍了一种自动且高效地计算所有梯度的技术。这个算法现在被称为*反向模式自动微分*（或简称为*反向模式自动微分*）。在网络中仅经过两次遍历（一次正向，一次反向），它就能计算出神经网络误差相对于每个模型参数的梯度。换句话说，它能够找出每个连接权重和每个偏置应该如何调整，以减少神经网络的误差。然后，这些梯度可以被用来执行梯度下降步骤。如果你重复这个过程，自动计算梯度并执行梯度下降步骤，神经网络的误差将逐渐降低，直到最终达到最小值。这种反向模式自动微分和梯度下降的组合现在被称为*反向传播*（或简称为*反向传播*）。
- en: 'Here’s an analogy: imagine you are learning to shoot a basketball into the
    hoop. You throw the ball (that’s the forward pass), and you observe that it went
    far off to the right side (that’s the error computation), then you consider how
    you can change your body position to throw the ball a bit less to the right next
    time (that’s the backward pass): you realize that your arm will need to rotate
    a bit counterclockwise, and probably your whole upper body as well, which in turn
    means that your feet should turn too (notice how we’re going down the “layers”).
    Once you’ve thought it through, you actually move your body: that’s the gradient
    descent step. The smaller the errors, the smaller the adjustments. As you repeat
    the whole process many times, the error gradually gets smaller, and after a few
    hours of practice, you manage to get the ball through the hoop every time. Good
    job!'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个类比：想象你正在学习投篮进篮筐。你投出篮球（这就是前向传递），然后观察到篮球偏离了右侧很远（这就是错误计算），然后你考虑如何改变你的身体位置，以便下次投球时稍微向左一些（这就是反向传递）：你意识到你的手臂需要稍微逆时针旋转，可能整个上半身也需要旋转，这反过来又意味着你的脚也应该转动（注意我们是如何一层层深入下去的）。一旦你思考透彻，你实际上移动了你的身体：这就是梯度下降步骤。错误越小，调整越小。随着你重复整个过程多次，错误逐渐减小，经过几小时的练习，你每次都能成功将球投进篮筐。做得好！
- en: Note
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are various autodiff techniques, with different pros and cons. Reverse-mode
    autodiff is well suited when the function to differentiate has many variables
    (e.g., connection weights and biases) and few outputs (e.g., one loss). If you
    want to learn more about autodiff, check out [Appendix A](app01.html#autodiff_appendix).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种自动微分技术，各有优缺点。反向模式自动微分非常适合当要微分的函数有多个变量（例如，连接权重和偏差）而输出较少（例如，一个损失）时。如果你想了解更多关于自动微分的信息，请参阅[附录A](app01.html#autodiff_appendix)。
- en: 'Backpropagation can actually be applied to all sorts of computational graphs,
    not just neural networks: indeed, Linnainmaa’s master’s thesis was not about neural
    nets at all, it was more general. It was several more years before backprop started
    to be used to train neural networks, but it still wasn’t mainstream. Then, in
    1985, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a [paper](https://homl.info/44)⁠^([12](ch09.html#id2152))
    analyzing how backpropagation allows neural networks to learn useful internal
    representations. Their results were so impressive that backpropagation was quickly
    popularized in the field. Over 40 years later, it is still by far the most popular
    training technique for neural networks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播实际上可以应用于各种计算图，而不仅仅是神经网络：事实上，Linnainmaa的硕士论文根本不是关于神经网络的，它更加通用。在多年之后，反向传播才开始被用来训练神经网络，但那时它还不是主流。然后在1985年，David
    Rumelhart、Geoffrey Hinton和Ronald Williams发表了一篇[论文](https://homl.info/44)⁠^([12](ch09.html#id2152))，分析了反向传播如何使神经网络学习有用的内部表示。他们的结果非常令人印象深刻，反向传播很快就在该领域得到了普及。40多年后，它仍然是神经网络训练中最受欢迎的技术。
- en: 'Let’s run through how backpropagation works again in a bit more detail:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地回顾一下反向传播的工作原理：
- en: It handles one mini-batch at a time, and goes through the full training set
    multiple times. If each mini-batch contains 32 instances, and each instance has
    100 features, then the mini-batch will be represented as a matrix with 32 rows
    and 100 columns. Each pass through the training set is called an *epoch*.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它一次处理一个迷你批次，并多次遍历整个训练集。如果每个迷你批次包含32个实例，每个实例有100个特征，那么迷你批次将表示为一个32行100列的矩阵。每次遍历训练集称为一个*epoch*。
- en: 'For each mini-batch, the algorithm computes the output of all the neurons in
    the first hidden layer using [Equation 9-2](#neural_network_layer_equation). If
    the layer has 50 neurons, then its output is a matrix with one row per sample
    in the mini-batch (e.g., 32), and 50 columns (i.e., one per neuron). This matrix
    is then passed on to the next layer, its output is computed and passed to the
    next layer, and so on until we get the output of the last layer, the output layer.
    This is the *forward pass*: it is exactly like making predictions, except all
    intermediate results are preserved since they are needed for the backward pass.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个迷你批次，算法使用[方程9-2](#neural_network_layer_equation)计算第一隐藏层中所有神经元的输出。如果该层有50个神经元，那么它的输出是一个矩阵，每行对应迷你批次中的每个样本（例如，32），每列对应一个神经元。然后这个矩阵传递到下一层，其输出被计算并传递到下一层，依此类推，直到我们得到最后一层的输出，即输出层。这是*前向传递*：它就像做出预测一样，只不过所有中间结果都被保留，因为它们对于反向传递是必需的。
- en: Next, the algorithm measures the network’s output error (i.e., it uses a loss
    function that compares the desired output and the actual output of the network,
    and returns some measure of the error).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，算法测量网络的输出误差（即，它使用一个损失函数来比较期望的输出和网络的实际输出，并返回一些误差的度量）。
- en: Then it computes how much each output layer parameter contributed to the error.
    This is done analytically by applying the *chain rule* (one of the most fundamental
    rules in calculus), which makes this step fast and precise. The result is one
    gradient per parameter.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它计算每个输出层参数对误差的贡献有多大。这是通过应用**链式法则**（微积分中最基本的规则之一）来分析的，这使得这一步骤既快速又精确。结果是每个参数一个梯度。
- en: The algorithm then measures how much of these error contributions came from
    each connection in the layer below, again using the chain rule, working backward
    until it reaches the input layer. As explained earlier, this reverse pass efficiently
    measures the error gradient across all the connection weights and biases in the
    network by propagating the error gradient backward through the network (hence
    the name of the algorithm).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法接着测量这些误差贡献中有多少来自下一层的每个连接，再次使用链式法则，反向工作直到达到输入层。如前所述，这种反向传递通过在网络中传播误差梯度来有效地测量网络中所有连接权重和偏差的误差梯度，因此得名该算法。
- en: Finally, the algorithm performs a gradient descent step to tweak all the connection
    weights and bias terms in the network, using the error gradients it just computed.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，算法执行梯度下降步骤，调整网络中所有连接权重和偏差项，使用它刚刚计算出的误差梯度。
- en: Warning
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'It is important to initialize all the hidden layers’ connection weights randomly,
    or else training will fail. For example, if you initialize all weights and biases
    to zero, then all neurons in a given layer will be perfectly identical, and thus
    backpropagation will affect them in exactly the same way, so they will remain
    identical. In other words, despite having hundreds of neurons per layer, your
    model will act as if it had only one neuron per layer: it won’t be too smart.
    If instead you randomly initialize the weights, you *break the symmetry* and allow
    backpropagation to train a diverse team of neurons.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要随机初始化所有隐藏层的连接权重，否则训练将失败。例如，如果你将所有权重和偏差初始化为零，那么给定层的所有神经元将完全相同，因此反向传播将以完全相同的方式影响它们，因此它们将保持相同。换句话说，尽管每层有数百个神经元，但你的模型将表现得好像每层只有一个神经元：它不会太聪明。相反，如果你随机初始化权重，你将*打破对称性*，并允许反向传播训练一个多样化的神经元团队。
- en: In short, backpropagation makes predictions for a mini-batch (forward pass),
    measures the error, then goes through each layer in reverse to measure the error
    contribution from each parameter (reverse pass), and finally tweaks the connection
    weights and biases to reduce the error (gradient descent step).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，反向传播为小批量进行预测（正向传递），测量误差，然后反向遍历每一层以测量每个参数的误差贡献（反向传递），最后调整连接权重和偏差以减少误差（梯度下降步骤）。
- en: 'In order for backprop to work properly, Rumelhart and his colleagues made a
    key change to the MLP’s architecture: they replaced the step function with the
    logistic function, *σ*(*z*) = 1 / (1 + exp(–*z*)), also called the *sigmoid* function.
    This was essential because the step function contains only flat segments, so there
    is no gradient to work with (gradient descent cannot move on a flat surface),
    while the sigmoid function has a well-defined nonzero derivative everywhere, allowing
    gradient descent to make some progress at every step. In fact, the backpropagation
    algorithm works well with many other activation functions, not just the sigmoid
    function. Here are two other popular choices:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使反向传播正常工作，Rumelhart和他的同事们对MLP的架构进行了关键性的改变：他们将步骤函数替换为对数函数，σ(*z*) = 1 / (1 +
    exp(–*z*)），也称为**Sigmoid**函数。这是至关重要的，因为步骤函数只包含平坦的段，因此没有梯度可以工作（梯度下降无法在平坦表面上移动），而Sigmoid函数在所有地方都有一个定义良好的非零导数，允许梯度下降在每一步都取得一些进展。事实上，反向传播算法与许多其他激活函数一起工作得很好，而不仅仅是Sigmoid函数。这里还有两种其他流行的选择：
- en: 'The *hyperbolic tangent* function: tanh(*z*) = 2*σ*(2*z*) – 1'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数：tanh(*z*) = 2*σ*(2*z*) – 1
- en: Just like the sigmoid function, this activation function is *S*-shaped, continuous,
    and differentiable, but its output value ranges from –1 to 1 (instead of 0 to
    1 in the case of the sigmoid function). That range tends to make each layer’s
    output more or less centered around 0 at the beginning of training, which often
    helps speed up convergence.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Sigmoid函数一样，这个激活函数是*S*形，连续且可导，但其输出值范围从-1到1（与Sigmoid函数的0到1不同）。这个范围往往使得每个层的输出在训练开始时或多或少地围绕0中心，这通常有助于加快收敛速度。
- en: 'The rectified linear unit function: ReLU(*z*) = max(0, *z*)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 矩形线性单元函数：ReLU(*z*) = max(0, *z*)
- en: The ReLU function is continuous but unfortunately not differentiable at *z*
    = 0 (the slope changes abruptly, which can make gradient descent bounce around),
    and its derivative is 0 for *z* < 0\. In practice, however, it works very well
    and has the advantage of being fast to compute, so it has become the default for
    most architectures (except the Transformer architecture, as we will see in [Chapter 15](ch15.html#transformer_chapter)).⁠^([13](ch09.html#id2162))
    Importantly, the fact that it does not have a maximum output value helps reduce
    some issues during gradient descent (we will come back to this in [Chapter 11](ch11.html#deep_chapter)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数是连续的，但不幸的是在*z* = 0处不可导（斜率突然改变，这可能导致梯度下降跳动），其导数在*z* < 0时为0。然而，在实践中，它工作得非常好，并且计算速度快，因此它已成为大多数架构的默认选择（除了我们在第15章中将要看到的Transformer架构）。⁠^([13](ch09.html#id2162))
    重要的是，它没有最大输出值的事实有助于减少梯度下降期间的一些问题（我们将在第11章中回到这个问题）。
- en: 'These popular activation functions and their derivatives are represented in
    [Figure 9-8](#activation_functions_plot). But wait! Why do we need activation
    functions in the first place? Well, if you chain several linear transformations,
    all you get is a linear transformation. For example, if f(*x*) = 2*x* + 3 and
    g(*x*) = 5*x* – 1, then chaining these two linear functions gives you another
    linear function: f(g(*x*)) = 2(5*x* – 1) + 3 = 10*x* + 1\. So if you don’t have
    some nonlinearity between layers, then even a deep stack of layers is equivalent
    to a single layer, and you can’t solve very complex problems with that. Conversely,
    a large enough DNN with nonlinear activations can theoretically approximate any
    continuous function.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些流行的激活函数及其导数在[图9-8](#activation_functions_plot)中展示。但是等等！我们为什么一开始就需要激活函数呢？好吧，如果你链式连接几个线性变换，你得到的就是另一个线性变换。例如，如果
    f(*x*) = 2*x* + 3 和 g(*x*) = 5*x* – 1，那么链式连接这两个线性函数会给你另一个线性函数：f(g(*x*)) = 2(5*x*
    – 1) + 3 = 10*x* + 1。所以如果你层与层之间没有非线性，那么即使是一堆深层的层也相当于一层，你无法用这种方法解决非常复杂的问题。相反，一个足够大的具有非线性激活的DNN理论上可以逼近任何连续函数。
- en: '![Diagram illustrating four activation functions—Heaviside, ReLU, Sigmoid,
    and Tanh—alongside their respective derivatives, highlighting nonlinearity essential
    for deep neural networks.](assets/hmls_0908.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图示四个激活函数——Heaviside、ReLU、Sigmoid和Tanh——及其各自的导数，突出显示深度神经网络中必要的非线性。](assets/hmls_0908.png)'
- en: Figure 9-8\. Activation functions (left) and their derivatives (right)
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 激活函数（左）及其导数（右）
- en: OK! You know where neural nets came from, what the MLP architecture looks like,
    and how it computes its outputs. You’ve also learned about the backpropagation
    algorithm. It’s time to see MLPs in action!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！你已经知道了神经网络是从哪里来的，MLP架构看起来是什么样子，以及它是如何计算输出的。你也已经了解了反向传播算法。现在是时候看看MLP的实际应用了！
- en: Building and Training MLPs with Scikit-Learn
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn构建和训练MLP
- en: MLPs can tackle a wide range of tasks, but the most common are regression and
    classification. Scikit-Learn can help with both of these. Let’s start with regression.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: MLP可以处理各种任务，但最常见的是回归和分类。Scikit-Learn可以帮助处理这两者。让我们从回归开始。
- en: Regression MLPs
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归MLP
- en: 'How would you build an MLP for a regression task? Well, if you want to predict
    a single value (e.g., the price of a house, given many of its features), then
    you just need a single output neuron: its output is the predicted value. For multivariate
    regression (i.e., to predict multiple values at once), you need one output neuron
    per output dimension. For example, to locate the center of an object in an image,
    you need to predict 2D coordinates, so you need two output neurons. If you also
    want to place a bounding box around the object, then you need two more numbers:
    the width and the height of the object. So, you end up with four output neurons.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何构建一个用于回归任务的多层感知器（MLP）？好吧，如果你想预测单个值（例如，给定许多特征的房子价格），那么你只需要一个输出神经元：其输出是预测值。对于多元回归（即一次预测多个值），你需要每个输出维度一个输出神经元。例如，为了在图像中定位一个物体的中心，你需要预测
    2D 坐标，因此你需要两个输出神经元。如果你还想在物体周围放置一个边界框，那么你需要两个额外的数字：物体的宽度和高度。因此，你最终会有四个输出神经元。
- en: 'Scikit-Learn includes an `MLPRegressor` class, so let’s use it to build an
    MLP with three hidden layers composed of 50 neurons each, and train it on the
    California housing dataset. For simplicity, we will use Scikit-Learn’s `fetch_california_housing()`
    function to load the data. This dataset is simpler than the one we used in [Chapter 2](ch02.html#project_chapter),
    since it contains only numerical features (there is no `ocean_proximity` feature),
    and there are no missing values. The targets are also scaled down: each unit represents
    $100,000\. Let’s start by importing everything we will need:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 包含一个 `MLPRegressor` 类，因此我们可以用它来构建一个包含三个隐藏层，每个隐藏层由 50 个神经元组成的多层感知器（MLP），并在加利福尼亚住房数据集上对其进行训练。为了简化，我们将使用
    Scikit-Learn 的 `fetch_california_housing()` 函数来加载数据。这个数据集比我们在 [第 2 章](ch02.html#project_chapter)
    中使用的那个更简单，因为它只包含数值特征（没有 `ocean_proximity` 特征），并且没有缺失值。目标值也被缩小了：每个单位代表 $100,000\.
    让我们先导入我们需要的所有内容：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let’s fetch the California housing dataset and split it into a training
    set and a test set:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们获取加利福尼亚住房数据集并将其分为训练集和测试集：
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let’s create an `MLPRegressor` model with 3 hidden layers composed of 50
    neurons each. The first hidden layer’s input size (i.e., the number of rows in
    its weights matrix) and the output layer’s output size (i.e., the number of columns
    in its weights matrix) will adjust automatically to the dimensionality of the
    inputs and targets, respectively, when training starts. The model uses the ReLU
    activation function in all hidden layers, and no activation function at all on
    the output layer. We also set `verbose=True` to get details on the model’s progress
    during training:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个包含 3 个隐藏层，每个隐藏层由 50 个神经元组成的 `MLPRegressor` 模型。当训练开始时，第一个隐藏层的输入大小（即其权重矩阵中的行数）和输出层的输出大小（即其权重矩阵中的列数）将自动调整到输入和目标的维度。该模型在所有隐藏层中使用
    ReLU 激活函数，在输出层上不使用任何激活函数。我们还设置了 `verbose=True` 以在训练期间获取模型进度的详细信息：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since neural nets can have a *lot* of parameters, they have a tendency to overfit
    the training set. To reduce this risk, one option is to use early stopping (introduced
    in [Chapter 4](ch04.html#linear_models_chapter)): when we set `early_stopping=True`,
    the `MLPRegressor` class automatically sets aside 10% of the training data and
    uses it to evaluate the model at each epoch (you can adjust the validation set’s
    size by setting `validation_fraction`). If the validation score stops improving
    for 10 epochs, training automatically stops (you can tweak this number of epochs
    by setting `n_iter_no_change`).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络可能有很多参数，它们有过度拟合训练集的倾向。为了减少这种风险，一个选择是使用早停（在第 4 章 [线性模型](ch04.html#linear_models_chapter)
    中介绍）：当我们设置 `early_stopping=True` 时，`MLPRegressor` 类会自动保留 10% 的训练数据，并在每个时代（epoch）使用它来评估模型（你可以通过设置
    `validation_fraction` 来调整验证集的大小）。如果验证分数在 10 个时代后停止提高，训练将自动停止（你可以通过设置 `n_iter_no_change`
    来调整这个时代数）。
- en: 'Now let’s create a pipeline to standardize the input features before sending
    them to the `MLPRegressor`. This is very important because gradient descent does
    not converge very well when the features have very different scales, as we saw
    in [Chapter 4](ch04.html#linear_models_chapter). We can then train the model!
    The `MLPRegressor` class uses a variant of gradient descent called *Adam* (see
    [Chapter 11](ch11.html#deep_chapter)) to minimize the mean squared error. It also
    uses a tiny bit of ℓ[2] regularization (you can control its strength via the `alpha`
    hyperparameter, which defaults to 0.0001):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个管道来标准化输入特征，在将它们发送到`MLPRegressor`之前。这非常重要，因为当特征具有非常不同的尺度时，梯度下降的收敛性不好，正如我们在[第4章](ch04.html#linear_models_chapter)中看到的。然后我们可以训练模型！`MLPRegressor`类使用一种称为*Adam*的梯度下降变体来最小化均方误差（见[第11章](ch11.html#deep_chapter)）。它还使用一点ℓ[2]正则化（你可以通过`alpha`超参数控制其强度，默认为0.0001）：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And there you go, you just trained your very first MLP! It required 45 epochs,
    and as you can see, the training loss went down at each epoch. This loss corresponds
    to [Equation 4-9](ch04.html#ridge_cost_function) divided by 2, so you must multiply
    it by 2 to get the MSE (although not exactly because the loss includes the ℓ[2]
    regularization term). The validation score generally went up at each epoch. Like
    every regressor in Scikit-Learn, `MLPRegressor` uses the R² score by default for
    evaluation—that’s what the `score()` method returns. As we saw in [Chapter 2](ch02.html#project_chapter),
    the R² score measures the ratio of the variance that is explained by the model.
    In this case, it reaches close to 80% on the validation set, which is fairly good
    for this task:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，你已经训练出了你的第一个MLP！这需要45个epoch，正如你所见，每个epoch的训练损失都在下降。这个损失对应于[方程4-9](ch04.html#ridge_cost_function)除以2，所以你必须乘以2来得到MSE（尽管并不完全准确，因为损失包括ℓ[2]正则化项）。验证分数通常在每个epoch都会上升。像Scikit-Learn中的每个回归器一样，`MLPRegressor`默认使用R²分数进行评估——这就是`score()`方法返回的内容。正如我们在[第2章](ch02.html#project_chapter)中看到的，R²分数衡量的是模型解释的方差比率。在这种情况下，它在验证集上接近80%，对于这个任务来说相当不错：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s evaluate the RMSE on the test set:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试集上评估RMSE：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We get a test RMSE of about 0.53, which is comparable to what you would get
    with a random forest classifier. Not too bad for a first try! [Figure 9-9](#predictions_vs_targets_plot)
    plots the model’s predictions versus the targets (on the test set). The dashed
    red line represents the ideal predictions (i.e., equal to the targets): most of
    the predictions are close to the targets, but there are still quite a few errors,
    especially for larger targets.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了大约0.53的测试RMSE，这与随机森林分类器得到的相当。对于第一次尝试来说，还不错！[图9-9](#predictions_vs_targets_plot)绘制了模型预测与目标（在测试集上）之间的关系。虚线红色线代表理想的预测（即等于目标）：大多数预测都接近目标，但仍然有一些错误，尤其是在较大的目标上。
- en: '![A scatter plot shows the MLP regressor''s predictions versus the targets,
    with most points clustering near the dashed red line indicating ideal predictions.](assets/hmls_0909.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示了MLP回归器的预测与目标之间的关系，大多数点都聚集在虚线红色线上，表示理想的预测。](assets/hmls_0909.png)'
- en: Figure 9-9\. MLP regressor’s predictions versus the targets
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9\. MLP回归器的预测与目标之间的关系
- en: 'Note that this MLP does not use any activation function for the output layer,
    so it’s free to output any value it wants. This is generally fine, but if you
    want to guarantee that the output is always positive, then you should use the
    ReLU activation function on the output layer, or the *softplus* activation function,
    which is a smooth variant of ReLU: softplus(*z*) = log(1 + exp(*z*)). Softplus
    is close to 0 when *z* is negative, and close to *z* when *z* is positive. Finally,
    if you want to guarantee that the predictions always fall within a given range
    of values, then you should use the sigmoid function or the hyperbolic tangent,
    and scale the targets to the appropriate range: 0 to 1 for sigmoid and –1 to 1
    for tanh. Sadly, the `MLPRegressor` class does not support activation functions
    in the output layer.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个MLP没有在输出层使用任何激活函数，所以它可以输出任何它想要的值。这通常是没问题的，但如果你想保证输出总是正的，那么你应该在输出层使用ReLU激活函数，或者使用*softplus*激活函数，它是ReLU的平滑变体：softplus(*z*)
    = log(1 + exp(*z*))。当*z*为负时，Softplus接近0，当*z*为正时，接近*z*。最后，如果你想保证预测始终在一个给定的值范围内，那么你应该使用sigmoid函数或双曲正切函数，并将目标缩放到适当的范围：sigmoid为0到1，tanh为-1到1。遗憾的是，`MLPRegressor`类不支持输出层的激活函数。
- en: Warning
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Scikit-Learn does not offer GPU acceleration, and its neural net features are
    fairly limited. This is why we will switch to PyTorch starting in [Chapter 10](ch10.html#pytorch_chapter).
    That said, it is quite convenient to be able to build and train a standard MLP
    in just a few lines of code using Scikit-Learn: it lets you tackle many complex
    tasks very quickly.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn不支持GPU加速，其神经网络功能相当有限。这就是为什么我们将在第10章开始使用PyTorch。尽管如此，使用Scikit-Learn仅用几行代码就能构建和训练一个标准的MLP是非常方便的：它让你能够快速处理许多复杂任务。
- en: 'In general, the mean squared error is the right loss to use for a regression
    tasks, but if you have a lot of outliers in the training set, you may sometimes
    prefer to use the mean absolute error instead, or preferably the *Huber loss*,
    which is a combination of both: it is quadratic when the error is smaller than
    a threshold *δ* (typically 1), but linear when the error is larger than *δ*. The
    linear part makes it less sensitive to outliers than the mean squared error, and
    the quadratic part allows it to converge faster and be more precise than the mean
    absolute error. Unfortunately, `MLPRegressor` only supports the MSE loss.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，均方误差是回归任务中正确的损失函数，但如果训练集中有很多异常值，你有时可能更愿意使用平均绝对误差，或者最好是使用*Huber损失*，它是两者的结合：当误差小于阈值*δ*（通常为1）时，它是二次的，但当误差大于*δ*时，它是线性的。线性部分使其对异常值不如均方误差敏感，而二次部分允许它更快地收敛并比平均绝对误差更精确。不幸的是，`MLPRegressor`只支持均方误差损失。
- en: '[Table 9-1](#regression_mlp_architecture) summarizes the typical architecture
    of a regression MLP.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[表9-1](#regression_mlp_architecture)总结了回归MLP的典型架构。'
- en: Table 9-1\. Typical regression MLP architecture
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1\. 典型回归MLP架构
- en: '| Hyperparameter | Typical value |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 典型值 |'
- en: '| --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| # hidden layers | Depends on the problem, but typically 1 to 5 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏层数量 | 取决于问题，但通常是1到5层 |'
- en: '| # neurons per hidden layer | Depends on the problem, but typically 10 to
    100 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 每个隐藏层的神经元数量 | 取决于问题，但通常是10到100个 |'
- en: '| # output neurons | 1 per target dimension |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 输出神经元数量 | 每个目标维度1个 |'
- en: '| Hidden activation | ReLU |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏激活 | ReLU |'
- en: '| Output activation | None, or ReLU/softplus (if positive outputs) or sigmoid/tanh
    (if bounded outputs) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 输出激活 | 无，或ReLU/softplus（如果输出为正）或sigmoid/tanh（如果输出有界） |'
- en: '| Loss function | MSE, or Huber if outliers |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 损失函数 | 均方误差（MSE），或Huber（如果有异常值） |'
- en: All right, MLPs can tackle regression tasks. What else can they do?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，MLP可以处理回归任务。它们还能做什么呢？
- en: Classification MLPs
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类MLP
- en: 'MLPs can also be used for classification tasks. For a binary classification
    problem, you just need a single output neuron using the sigmoid activation function:
    the output will be a number between 0 and 1, which you can interpret as the estimated
    probability of the positive class. The estimated probability of the negative class
    is equal to one minus that number.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: MLP也可以用于分类任务。对于二元分类问题，你只需要一个输出神经元并使用sigmoid激活函数：输出将是一个介于0和1之间的数字，你可以将其解释为正类的估计概率。负类的估计概率等于1减去那个数字。
- en: 'MLPs can also easily handle multilabel binary classification tasks (see [Chapter 3](ch03.html#classification_chapter)).
    For example, you could have an email classification system that predicts whether
    each incoming email is ham or spam, and simultaneously predicts whether it is
    an urgent or nonurgent email. In this case, you would need two output neurons,
    both using the sigmoid activation function: the first would output the probability
    that the email is spam, and the second would output the probability that it is
    urgent. More generally, you would dedicate one output neuron for each positive
    class. Note that the output probabilities do not necessarily add up to 1\. This
    lets the model output any combination of labels: you can have nonurgent ham, urgent
    ham, nonurgent spam, and perhaps even urgent spam (although that would probably
    be an error).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: MLP也可以轻松处理多标签二元分类任务（见[第3章](ch03.html#classification_chapter)）。例如，你可能有一个电子邮件分类系统，它预测每封收到的电子邮件是垃圾邮件还是正常邮件，并同时预测它是紧急邮件还是非紧急邮件。在这种情况下，你需要两个输出神经元，都使用sigmoid激活函数：第一个会输出邮件是垃圾邮件的概率，第二个会输出邮件是紧急邮件的概率。更一般地说，你会为每个正类分配一个输出神经元。请注意，输出概率不一定总和为1。这允许模型输出任何标签组合：你可以有非紧急的正常邮件、紧急的正常邮件、非紧急的垃圾邮件，甚至可能是紧急的垃圾邮件（尽管这可能是错误）。
- en: If each instance can belong only to a single class, out of three or more possible
    classes (e.g., classes 0 through 9 for digit image classification), then you need
    to have one output neuron per class, and you should use the softmax activation
    function for the whole output layer (see [Figure 9-10](#fnn_for_classification_diagram)).
    The softmax function (introduced in [Chapter 4](ch04.html#linear_models_chapter))
    will ensure that all the estimated probabilities are between 0 and 1, and that
    they add up to 1, since the classes are exclusive. As we saw in [Chapter 3](ch03.html#classification_chapter),
    this is called multiclass classification.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个实例只能属于单个类别，从三个或更多可能的类别（例如，数字图像分类中的类别0到9）中，那么你需要为每个类别有一个输出神经元，并且应该在整个输出层使用softmax激活函数（见[图9-10](#fnn_for_classification_diagram)）。softmax函数（在第4章中介绍）将确保所有估计的概率都在0到1之间，并且它们的总和为1，因为类别是互斥的。正如我们在[第3章](ch03.html#classification_chapter)中看到的，这被称为多类分类。
- en: Regarding the loss function, since we are predicting probability distributions,
    the cross-entropy loss (or *x-entropy* or log loss for short, see [Chapter 4](ch04.html#linear_models_chapter))
    is generally a good choice.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关于损失函数，由于我们正在预测概率分布，交叉熵损失（或简称为*x-entropy*或log loss，见[第4章](ch04.html#linear_models_chapter)）通常是一个不错的选择。
- en: '![Diagram illustrating a modern Multi-Layer Perceptron (MLP) architecture for
    classification, featuring input, hidden, and output layers with ReLU and softmax
    functions.](assets/hmls_0910.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图9-10说明现代多层感知器（MLP）的分类架构，包括具有ReLU和softmax函数的输入、隐藏和输出层。](assets/hmls_0910.png)'
- en: Figure 9-10\. A modern MLP (including ReLU and softmax) for classification
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10。用于分类的现代MLP（包括ReLU和softmax）
- en: '[Table 9-2](#classification_mlp_architecture) summarizes the typical architecture
    of a classification MLP.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[表9-2](#classification_mlp_architecture)总结了分类MLP的典型架构。'
- en: Table 9-2\. Typical classification MLP architecture
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-2。典型的分类MLP架构
- en: '| Hyperparameter | Binary classification | Multilabel binary classification
    | Multiclass classification |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 二进制分类 | 多标签二进制分类 | 多类分类 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| # hidden layers | Typically 1 to 5 layers, depending on the task |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| # 隐藏层 | 通常1到5层，具体取决于任务 |'
- en: '| # output neurons | 1 | 1 per binary label | 1 per class |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| # 输出神经元 | 1 | 每个二进制标签1 | 每个类别1 |'
- en: '| Output layer activation | Sigmoid | Sigmoid | Softmax |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 输出层激活 | Sigmoid | Sigmoid | Softmax |'
- en: '| Loss function | X-entropy | X-entropy | X-entropy |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 损失函数 | X-entropy | X-entropy | X-entropy |'
- en: As you might expect, Scikit-Learn offers an `MLPClassifier` class in the `sklearn.neural_network`
    package, which you can use for binary or multiclass classification. It is almost
    identical to the `MLPRegressor` class, except that its output layer uses the softmax
    activation function, and it minimizes the cross-entropy loss rather than the MSE.
    Moreover, the `score()` method returns the model’s accuracy rather than the R²
    score. Let’s try it out.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所预期，Scikit-Learn在`sklearn.neural_network`包中提供了一个`MLPClassifier`类，你可以用它来进行二进制或多类分类。它与`MLPRegressor`类几乎相同，只是它的输出层使用softmax激活函数，并且它最小化交叉熵损失而不是MSE。此外，`score()`方法返回模型的准确率而不是R²分数。让我们试试看。
- en: 'We could tackle the iris dataset, but that task is too simple for a neural
    net: a linear model would do just as well and wouldn’t risk overfitting. So let’s
    instead tackle a more complex task: Fashion MNIST. This is a drop-in replacement
    of MNIST (introduced in [Chapter 3](ch03.html#classification_chapter)). It has
    the exact same format as MNIST (70,000 grayscale images of 28 × 28 pixels each,
    with 10 classes), but the images represent fashion items rather than handwritten
    digits, so each class is much more diverse, and the problem turns out to be significantly
    more challenging than MNIST. For example, a simple linear model reaches about
    92% accuracy on MNIST, but only about 83% on Fashion MNIST. Let’s see if we can
    do better with an MLP.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以处理鸢尾花数据集，但对于神经网络来说这个任务太简单了：线性模型也能做得一样好，而且不会存在过拟合的风险。所以让我们来处理一个更复杂的任务：Fashion
    MNIST。这是MNIST（在第3章中介绍）的替代品。它具有与MNIST完全相同的格式（70,000个28 × 28像素的灰度图像，每个图像有10个类别），但图像代表的是时尚商品而不是手写数字，因此每个类别的多样性更大，这个问题最终证明比MNIST更具挑战性。例如，简单的线性模型在MNIST上的准确率约为92%，但在Fashion
    MNIST上只有约83%。让我们看看我们是否能用MLP做得更好。
- en: 'First, let’s load the dataset using the `fetch_openml()` function, very much
    like we did for MNIST in [Chapter 3](ch03.html#classification_chapter). Note that
    the targets are represented as strings `''0''`, `''1''`, …​, `''9''`, so we convert
    them to integers:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用`fetch_openml()`函数加载数据集，这与我们在[第3章](ch03.html#classification_chapter)中为MNIST所做的一样。注意，目标表示为字符串`'0'`、`'1'`、…、`'9'`，因此我们将它们转换为整数：
- en: '[PRE7]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The data is already shuffled, so we just take the first 60,000 images for training,
    and the last 10,000 for testing:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已经打乱了，所以我们只取前60,000张图像用于训练，最后10,000张用于测试：
- en: '[PRE8]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each image is represented as a 1D integer array containing 784 pixel intensities
    ranging from 0 to 255\. You can use the `plt.imshow()` function to plot an image,
    but first you need to reshape it to `[28, 28]`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像都表示为一个包含784个像素强度的1D整数数组，范围从0到255。你可以使用`plt.imshow()`函数来绘制图像，但首先你需要将其重塑为`[28,
    28]`：
- en: '[PRE9]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you run this code, you should see the ankle boot represented in the top-right
    corner of [Figure 9-11](#fashion_mnist_plot).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这段代码，你应该能在[图9-11](#fashion_mnist_plot)的右上角看到踝靴的表示。
- en: '![Grid of the first four samples from each class in the Fashion MNIST dataset,
    showing various clothing and footwear items labeled by category.](assets/hmls_0911.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![Fashion MNIST数据集中每个类别的四个样本的网格，显示各种服装和鞋类，按类别标记。](assets/hmls_0911.png)'
- en: Figure 9-11\. First four samples from each class in Fashion MNIST
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-11。Fashion MNIST中每个类别的四个样本
- en: 'With MNIST, when the label is equal to 5, it means that the image represents
    the handwritten digit 5\. Easy. For Fashion MNIST, however, we need the list of
    class names to know what we are dealing with. Scikit-Learn does not provide it,
    so let’s create it:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST中，当标签等于5时，意味着图像代表的是手写数字5。很简单。然而，对于Fashion MNIST，我们需要知道我们正在处理哪个类别的名称列表。Scikit-Learn没有提供它，所以让我们创建一个：
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can now confirm that the first image in the training set represents an ankle
    boot:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以确认训练集中第一张图像代表的是一双踝靴：
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We’re ready to build the classification MLP:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建分类MLP了：
- en: '[PRE12]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This code is very similar to the regression code we used earlier, but there
    are a few differences:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与我们之前使用的回归代码非常相似，但有一些不同之处：
- en: Of course, it’s a classification task so we use an `MLPClassifier` rather than
    an `MLPRegressor`.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当然，这是一个分类任务，所以我们使用`MLPClassifier`而不是`MLPRegressor`。
- en: We use just two hidden layers with 300 and 100 neurons, respectively. You can
    try a different number of hidden layers, and change the number of neurons as well
    if you want.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只使用了两个隐藏层，分别有300个和100个神经元。你可以尝试不同的隐藏层数量，如果你愿意，也可以更改神经元的数量。
- en: 'We also use a `MinMaxScaler` instead of a `StandardScaler`. We need it to shrink
    the pixel intensities down to the 0–1 range rather than 0–255: having features
    in this range usually works better with the default hyperparameters used by `MLPClassifier`,
    such as its default learning rate and weight initialization scale. You might wonder
    why we didn’t use a `StandardScaler`? Well some pixels don’t vary much across
    images; for example, the pixels around the edges are almost always white. If we
    used the `StandardScaler`, these pixels would get scaled up to have the same variance
    as every other pixel: as a result, we would give more importance to these pixels
    than they probably deserve. Using the `MinMaxScaler` often works better than the
    `StandardScaler` for images (but your mileage may vary).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还使用了`MinMaxScaler`而不是`StandardScaler`。我们需要它将像素强度缩小到0-1的范围，而不是0-255：在这个范围内的特征通常与`MLPClassifier`使用的默认超参数（如默认学习率和权重初始化比例）配合得更好。你可能想知道为什么我们没有使用`StandardScaler`？好吧，一些像素在图像中变化不大；例如，边缘周围的像素几乎总是白色。如果我们使用`StandardScaler`，这些像素会被缩放到与其他像素相同的方差：结果，我们会给予这些像素比它们应得的更多的重要性。对于图像来说，`MinMaxScaler`通常比`StandardScaler`表现得更好（但效果可能因人而异）。
- en: Lastly, the `score()` function returns the model’s accuracy.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`score()`函数返回模型的准确率。
- en: If you run this code, you will find that the model reaches about 89.7% accuracy
    on the validation set during training (the exact value is given by `mlp_clf.best_validation_score_`),
    but it starts overfitting a bit toward the end, so it ends up at just 89.2% accuracy.
    When we evaluate the model on the test set, we get 87.1%, which is not bad for
    this task, although we can do better with other neural net architectures such
    as convolutional neural networks ([Chapter 12](ch12.html#cnn_chapter)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这段代码，你会在训练过程中发现模型在验证集上的准确率大约为89.7%（确切值由`mlp_clf.best_validation_score_`给出），但到了最后，它开始有点过拟合，所以最终准确率只有89.2%。当我们对测试集上的模型进行评估时，得到87.1%，对于这个任务来说还不错，尽管我们可以通过其他神经网络架构（如卷积神经网络[第12章](ch12.html#cnn_chapter)）做得更好。
- en: You probably noticed that training was quite slow. That’s because the hidden
    layers have a *lot* of parameters, so there are many computations to run at each
    iteration. For example, the first hidden layer has 784 × 300 connection weights,
    plus 300 bias terms, which adds up to 235,500 parameters! All these parameters
    give the model quite a lot of flexibility to fit the training data, but it also
    means that there’s a high risk of overfitting, especially when you do not have
    a lot of training data. In this case, you may want to use regularization techniques
    such as early stopping and ℓ[2] regularization.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到训练相当慢。这是因为隐藏层有很多参数，所以每次迭代都要运行很多计算。例如，第一隐藏层有784 × 300个连接权重，加上300个偏置项，总共235,500个参数！所有这些参数使模型有很大的灵活性来拟合训练数据，但也意味着存在很高的过拟合风险，尤其是在你没有很多训练数据的情况下。在这种情况下，你可能想使用正则化技术，如提前停止和ℓ[2]正则化。
- en: 'Once the model is trained, you can use it to classify new images:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，你就可以用它来对新图像进行分类：
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'All these predictions are correct, except for the one at index 12, which should
    be a 7 (sneaker) instead of a 8 (bag). You might want to know how confident the
    model was about these predictions, especially the bad one. For this, you can use
    `model.predict_proba()` instead of `model.predict()`, like we did in [Chapter 3](ch03.html#classification_chapter):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些预测都是正确的，除了索引12的那个，它应该是7（运动鞋）而不是8（包）。你可能想知道模型对这些预测有多自信，特别是对错误的预测。为此，你可以使用`model.predict_proba()`而不是`model.predict()`，就像我们在[第3章](ch03.html#classification_chapter)中做的那样：
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Hmm, that’s not great: the model is telling us that it’s 100% confident that
    the image represents a bag (index 8). So not only is the model wrong, it’s 100%
    confident that it’s right. In fact, across all 10,000 images in the test set,
    there are only 16 images that the model is less than 99.9% confident about, despite
    the fact that its accuracy is about 90%. That’s why you should always treat estimated
    probabilities with a grain of salt: neural nets have a strong tendency to be overconfident,
    especially if they are trained for a bit too long.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这不太好：模型告诉我们它100%确信图像代表一个包（索引8）。所以不仅模型是错误的，它还100%确信它是正确的。实际上，在测试集的10,000张图像中，只有16张图像的模型对其信心低于99.9%，尽管其准确率约为90%。这就是为什么你应该总是带着怀疑的态度对待估计概率：神经网络有很强的过度自信倾向，尤其是如果训练时间过长的话。
- en: Tip
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The targets for classification tasks can be class indices (e.g., 3) or class
    probabilities, typically one-hot vectors (e.g., [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]).
    But if your model tends to be overconfident, you can try the *label smoothing*
    technique:⁠^([14](ch09.html#id2192)) reduce the target class’s probability slightly
    (e.g., from 1 down to 0.9) and distribute the rest evenly across the other classes
    (e.g., [0.1/9, 0.1/9, 0.1/9, 0.9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9]).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务的目标可以是类别索引（例如，3）或类别概率，通常是单热向量（例如，[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]）。但如果你的模型倾向于过度自信，你可以尝试**标签平滑**技术：⁠^([14](ch09.html#id2192))
    稍微降低目标类别的概率（例如，从1降到0.9），并将剩余的均匀分配到其他类别（例如，[0.1/9, 0.1/9, 0.1/9, 0.9, 0.1/9, 0.1/9,
    0.1/9, 0.1/9, 0.1/9, 0.1/9])。
- en: Still, getting 90% accuracy on Fashion MNIST is pretty good. You could get even
    better performance by fine-tuning the hyperparameters, for example using `RandomizedSearchCV`,
    as we did in [Chapter 2](ch02.html#project_chapter). However, the search space
    is quite large, so it helps to know roughly where to look.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，在Fashion MNIST上达到90%的准确率已经相当不错了。你可以通过微调超参数来获得更好的性能，例如使用`RandomizedSearchCV`，就像我们在[第2章](ch02.html#project_chapter)中做的那样。然而，搜索空间相当大，所以知道大致的搜索范围是有帮助的。
- en: Hyperparameter Tuning Guidelines
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整指南
- en: 'The flexibility of neural networks is also one of their main drawbacks: there
    are many hyperparameters to tweak. Not only can you use any imaginable network
    architecture, but even in a basic MLP you can change the number of layers, the
    number of neurons and the type of activation function to use in each layer, the
    weight initialization logic, the type of optimizer to use, its learning rate,
    the batch size, and more. What are some good values for these hyperparameters?'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的灵活性也是它们的主要缺点之一：有许多超参数需要调整。你不仅可以使用任何可以想象到的网络架构，即使在基本的MLP中，你也可以改变层数、每层的神经元数量以及要使用的激活函数类型、权重初始化逻辑、要使用的优化器类型、其学习率、批量大小等等。这些超参数有哪些好的值？
- en: Number of Hidden Layers
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: 'For many problems, you can begin with a single hidden layer and get reasonable
    results. An MLP with just one hidden layer can theoretically model even the most
    complex functions, provided it has enough neurons. But deep networks have a much
    higher *parameter efficiency* than shallow ones: they can model complex functions
    using exponentially fewer neurons than shallow nets, allowing them to reach much
    better performance with the same amount of training data. This is because their
    layered structure enables them to reuse and compose features across multiple levels:
    for example, the first layer in a face classifier may learn to recognize low-level
    features such as dots, arcs, or straight lines; while the second layer may learn
    to combine these low-level features into higher-level features such as squares
    or circles; and the third layer may learn to combine these higher-level features
    into a mouth, an eye, or a nose; and the top layer would then be able to use these
    top-level features to classify faces.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多问题，你可以从一个单一的隐藏层开始，并得到合理的结果。一个只有一个隐藏层的MLP在理论上可以模拟甚至是最复杂的函数，前提是它有足够的神经元。但深度网络比浅层网络具有更高的*参数效率*：它们可以使用比浅层网络指数级更少的神经元来模拟复杂函数，这使得它们能够在相同数量的训练数据下达到更好的性能。这是因为它们的分层结构使它们能够在多个级别上重用和组合特征：例如，在人脸分类器中的第一层可能会学习识别低级特征，如点、弧或直线；而第二层可能会学习将这些低级特征组合成更高级的特征，如正方形或圆形；第三层可能会学习将这些高级特征组合成嘴巴、眼睛或鼻子；顶层然后将能够使用这些顶级特征来分类人脸。
- en: Not only does this hierarchical architecture help DNNs converge faster to a
    good solution, but it also improves their ability to generalize to new datasets.
    For example, if you have already trained a model to recognize faces in pictures
    and you now want to train a new neural network to recognize hairstyles, you can
    kickstart the training by reusing the lower layers of the first network. Instead
    of randomly initializing the weights and biases of the first few layers of the
    new neural network, you can initialize them to the values of the weights and biases
    of the lower layers of the first network. This way the network will not have to
    learn from scratch all the low-level structures that occur in most pictures; it
    will only have to learn the higher-level structures (e.g., hairstyles). This is
    called *transfer learning*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层架构不仅有助于深度神经网络更快地收敛到好的解决方案，而且还能提高它们泛化到新数据集的能力。例如，如果你已经训练了一个模型来识别图片中的面孔，现在你想训练一个新的神经网络来识别发型，你可以通过重用第一个网络的底层来启动训练。而不是随机初始化新神经网络的底层权重和偏差，你可以将它们初始化为第一个网络底层权重和偏差的值。这样，网络就不必从头开始学习大多数图片中出现的所有低级结构；它只需要学习高级结构（例如，发型）。这被称为*迁移学习*。
- en: 'In summary, for many problems you can start with just one or two hidden layers,
    and the neural network will work pretty well. For instance, you can easily reach
    above 97% accuracy on the MNIST dataset using just one hidden layer with a few
    hundred neurons, and above 98% accuracy using two hidden layers with the same
    total number of neurons, in roughly the same amount of training time. For more
    complex problems, you can ramp up the number of hidden layers until you start
    overfitting the training set. Very complex tasks, such as large image classification
    or speech recognition, typically require networks with dozens of layers (or even
    hundreds, but not fully connected ones, as you will see in [Chapter 12](ch12.html#cnn_chapter)),
    and they need a huge amount of training data. You will rarely have to train such
    networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art
    network that performs a similar task. Training will then be a lot faster and require
    much less data.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对于许多问题，你可以从只有一个或两个隐藏层开始，神经网络的工作效果相当不错。例如，你只需使用一个包含几百个神经元的隐藏层，就可以轻松地在MNIST数据集上达到97%以上的准确率，使用两个具有相同总神经元数的隐藏层，在几乎相同的学习时间内，可以达到98%以上的准确率。对于更复杂的问题，你可以增加隐藏层的数量，直到开始过度拟合训练集。对于非常复杂的问题，如大型图像分类或语音识别，通常需要具有数十个（甚至数百个，但不是完全连接的，如你将在第12章中看到）层的网络，并且需要大量的训练数据。你很少需要从头开始训练这样的网络：更常见的是重用执行类似任务的预训练的顶尖网络的某些部分。这样训练将会更快，并且需要的数据更少。
- en: Number of Neurons per Hidden Layer
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每个隐藏层的神经元数量
- en: The number of neurons in the input and output layers is determined by the type
    of input and output your task requires. For example, the MNIST task requires 28
    × 28 = 784 inputs and 10 output neurons.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层中的神经元数量取决于你的任务所需的输入和输出类型。例如，MNIST任务需要28 × 28 = 784个输入和10个输出神经元。
- en: As for the hidden layers, it used to be common to size them to form a pyramid,
    with fewer and fewer neurons at each layer—the rationale being that many low-level
    features can coalesce into far fewer high-level features. A typical neural network
    for MNIST might have 3 hidden layers, the first with 300 neurons, the second with
    200, and the third with 100\. However, this practice has been largely abandoned
    because it seems that using the same number of neurons in all hidden layers performs
    just as well in most cases, or even better; plus, there is only one hyperparameter
    to tune, instead of one per layer. That said, depending on the dataset, it can
    sometimes help to make the first hidden layer a bit larger than the others.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层来说，过去通常的做法是将它们的大小设计成金字塔形，每一层的神经元数量越来越少——其理论依据是许多低级特征可以合并成更少的高级特征。对于MNIST任务，一个典型的神经网络可能包含3个隐藏层，第一层有300个神经元，第二层有200个，第三层有100个。然而，这种做法已经被大量放弃，因为似乎在大多数情况下，所有隐藏层使用相同数量的神经元表现同样好，甚至更好；此外，只需要调整一个超参数，而不是每一层都需要调整。尽管如此，根据数据集的不同，有时将第一隐藏层做得比其他层稍大一些可能会有所帮助。
- en: 'Just like the number of layers, you can try increasing the number of neurons
    gradually until the network starts overfitting. Alternatively, you can try building
    a model with slightly more layers and neurons than you actually need, then use
    early stopping and other regularization techniques to prevent it from overfitting
    too much. Vincent Vanhoucke, a Waymo researcher and former Googler, has dubbed
    this the “stretch pants” approach: instead of wasting time looking for pants that
    perfectly match your size, just use large stretch pants that will shrink down
    to the right size. With this approach, you avoid bottleneck layers that could
    ruin your model. Indeed, if a layer has too few neurons, it will lack the computational
    capacity to model complex relationships, and it may not even have enough representational
    power to preserve all the useful information from the inputs. For example, if
    you apply PCA (introduced in [Chapter 7](ch07.html#dimensionality_chapter)) to
    the Fashion MNIST training set, you will find that you need 187 dimensions to
    preserve 95% of the variance in the data. So if you set the number of neurons
    in the first hidden layer to some greater number, say 200, you can be confident
    that this layer will not be a bottleneck. However, you don’t want to add too many
    neurons, or else the model will have too many parameters to optimize, and it will
    take more time and data to train.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 就像层数的数量一样，你可以逐渐增加神经元的数量，直到网络开始过拟合。或者，你可以尝试构建一个比实际需要的层数和神经元数量稍多的模型，然后使用提前停止和其他正则化技术来防止它过度拟合。Waymo的研究员和前谷歌员工Vincent
    Vanhoucke将这种方法称为“紧身裤”方法：与其浪费时间寻找完美匹配你尺寸的裤子，不如使用可以缩到合适尺寸的大号紧身裤。采用这种方法，你可以避免可能导致模型损坏的瓶颈层。确实，如果一个层有太少的神经元，它将缺乏建模复杂关系的计算能力，甚至可能没有足够的表达能力来保留所有有用的输入信息。例如，如果你对Fashion
    MNIST训练集应用PCA（在第7章中介绍），你会发现你需要187个维度来保留数据中95%的方差。所以，如果你将第一隐藏层的神经元数量设置为某个更大的数字，比如200，你可以确信这个层不会成为瓶颈。然而，你不想添加太多的神经元，否则模型将具有太多的参数需要优化，这将需要更多的时间和数据来训练。
- en: Tip
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: In general, you will get more bang for your buck by increasing the number of
    layers rather than the number of neurons per layer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，通过增加层数而不是每层的神经元数量，你将获得更多的回报。
- en: That said, bottleneck layers are not always a bad thing. For example, limiting
    the dimensionality of the first hidden layers forces the neural net to keep only
    the most important dimensions, which can eliminate some of the noise in the data
    (but don’t go too far!). Also, having a bottleneck layer near the output layer
    can force the neural net to learn good representations of the data in the previous
    layers (i.e., packing more useful information in less space), which can help the
    neural net generalize, and can also be useful in and of itself for *representation
    learning*. We will get back to that in [Chapter 18](ch18.html#autoencoders_chapter).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，瓶颈层并不总是坏事。例如，限制第一隐藏层的维度迫使神经网络只保留最重要的维度，这可以消除数据中的一些噪声（但不要走得太远！）。此外，在输出层附近有一个瓶颈层可以迫使神经网络学习前一层中数据的良好表示（即，在更少的空间中打包更多有用的信息），这有助于神经网络泛化，并且对于**表示学习**本身也可能很有用。我们将在第18章（第18章中介绍）回到这一点。
- en: Learning Rate
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率
- en: 'The learning rate is a hugely important hyperparameter. In general, the optimal
    learning rate is about half of the maximum learning rate (i.e., the learning rate
    above which the training algorithm diverges, as we saw in [Chapter 4](ch04.html#linear_models_chapter)).
    One way to find a good learning rate is to train the model for a few hundred iterations,
    starting with a very low learning rate (e.g., 10^(–5)) and gradually increasing
    it up to a very large value (e.g., 10). This is done by multiplying the learning
    rate by a constant factor at each iteration (e.g., by (10 / 10^(-5))^(1 / 500)
    to go from 10^(–5) to 10 in 500 iterations). If you plot the loss as a function
    of the learning rate (using a log scale for the learning rate), you should see
    it dropping at first. But after a while, the learning rate will be too large,
    so the loss will shoot back up: the optimal learning rate is often a bit lower
    than the point at which the loss starts to climb (typically about 10 times lower
    than the turning point). You can then reinitialize your model and train it normally
    using this good learning rate.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是一个极其重要的超参数。一般来说，最佳学习率大约是最大学习率的一半（即训练算法发散的学习率，正如我们在[第4章](ch04.html#linear_models_chapter)中看到的）。找到良好学习率的一种方法是在几个迭代中训练模型，起始学习率非常低（例如，10的负5次方）并逐渐增加到非常大的值（例如，10）。这是通过在每次迭代中将学习率乘以一个常数因子来实现的（例如，通过(10
    / 10^(-5))^(1 / 500)在500次迭代内从10的负5次方增加到10）。如果你将损失作为学习率的函数绘制出来（使用对数刻度表示学习率），你应该会看到损失最初下降。但过了一段时间后，学习率会变得太大，因此损失会急剧上升：最佳学习率通常略低于损失开始上升的点（通常比转折点低10倍）。然后你可以重新初始化你的模型，并使用这个良好的学习率正常训练它。
- en: Tip
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: To change the learning rate during training when using Scikit-Learn, you must
    set the MLP’s `warm_start` hyperparameter to `True`, and fit the model one batch
    at a time using `partial_fit()`, much like we did with the `SGDRegressor` in [Chapter 4](ch04.html#linear_models_chapter).
    Simply update the learning rate at each iteration.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Scikit-Learn在训练期间更改学习率时，你必须将MLP的`warm_start`超参数设置为`True`，并使用`partial_fit()`一次拟合一个批次的模型，就像我们在[第4章](ch04.html#linear_models_chapter)中处理`SGDRegressor`时做的那样。只需在每次迭代中更新学习率即可。
- en: Batch Size
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理大小
- en: 'The batch size can have a significant impact on your model’s performance and
    training time. The main benefit of using large batch sizes is that hardware accelerators
    like GPUs can process them efficiently (as we will see in [Chapter 10](ch10.html#pytorch_chapter)),
    so the training algorithm will see more instances per second. Therefore, many
    researchers and practitioners recommend using the largest batch size that can
    fit in *VRAM* (video RAM, i.e., the GPU’s memory). There’s a catch, though: large
    batch sizes can sometimes lead to training instabilities, especially with smaller
    models and at the beginning of training, and the resulting model may not generalize
    as well as a model trained with a small batch size. Yann LeCun once tweeted “Friends
    don’t let friends use mini-batches larger than 32”, citing a [2018 paper](https://homl.info/smallbatch)⁠^([15](ch09.html#id2213))
    by Dominic Masters and Carlo Luschi which concluded that using small batches (from
    2 to 32) was preferable because small batches led to better models in less training
    time.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理大小会对你的模型性能和训练时间产生重大影响。使用大批处理的主要好处是硬件加速器，如GPU，可以高效地处理它们（正如我们在[第10章](ch10.html#pytorch_chapter)中将会看到的），因此训练算法每秒将看到更多的实例。因此，许多研究人员和从业者推荐使用可以适应*VRAM*（视频内存，即GPU的内存）的最大批处理大小。然而，有一个问题：大批处理有时会导致训练不稳定，尤其是在较小的模型和训练初期，并且生成的模型可能不如小批处理训练的模型泛化得好。Yann
    LeCun 曾经发推文说“朋友们不要让朋友们使用大于32的迷你批处理”，引用了Dominic Masters和Carlo Luschi在2018年发表的一篇[论文](https://homl.info/smallbatch)⁠^([15](ch09.html#id2213))，该论文得出结论，使用小批处理（从2到32）更可取，因为小批处理在更短的训练时间内导致了更好的模型。
- en: However, other research points in the opposite direction. For example, in 2017,
    papers by [Elad Hoffer et al.](https://homl.info/largebatch)⁠^([16](ch09.html#id2214))
    and [Priya Goyal et al.](https://homl.info/largebatch2)⁠^([17](ch09.html#id2215))
    showed that it is possible to use very large batch sizes (up to 8,192), along
    with various techniques such as warming up the learning rate (i.e., starting training
    with a small learning rate, then ramping it up), to obtain very short training
    times, without any generalization gap.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其他研究指出相反的方向。例如，2017年，Elad Hoffer等人撰写的论文[Elad Hoffer et al.](https://homl.info/largebatch)⁠^([16](ch09.html#id2214))和Priya
    Goyal等人撰写的论文[Priya Goyal et al.](https://homl.info/largebatch2)⁠^([17](ch09.html#id2215))表明，可以使用非常大的批量大小（高达8,192），结合各种技术，如预热学习率（即以较小的学习率开始训练，然后逐渐增加），以获得非常短的训练时间，而没有任何泛化差距。
- en: So one strategy is to use a large batch size, possibly with learning rate warmup,
    and if training is unstable or the final performance is disappointing, then try
    using a smaller batch size instead.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一种策略是使用较大的批量大小，可能还有学习率预热，如果训练不稳定或最终性能令人失望，那么尝试使用较小的批量大小。
- en: Other Hyperparameters
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他超参数
- en: 'Here are two more hyperparameters you can tune if you have the computation
    budget and the time:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有计算预算和时间，这里还有两个您可以调整的超参数：
- en: Optimizer
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器
- en: Choosing a better optimizer than plain old mini-batch gradient descent (and
    tuning its hyperparameters) can help speed up training and sometimes reach better
    performance.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 选择比传统的批量梯度下降更好的优化器（并调整其超参数）可以帮助加速训练，有时还能达到更好的性能。
- en: Activation function
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'We discussed how to choose the activation function earlier in this chapter:
    in general, the ReLU activation function is a good default for all hidden layers.
    In some cases, replacing ReLU with another function can help.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章前面讨论了如何选择激活函数：一般来说，ReLU激活函数是所有隐藏层的良好默认选项。在某些情况下，用另一个函数替换ReLU可能会有所帮助。
- en: Tip
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The optimal learning rate depends on the other hyperparameters—especially the
    batch size—so if you modify any hyperparameter, make sure to tune the learning
    rate again.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳学习率取决于其他超参数——特别是批量大小——因此，如果您修改了任何超参数，请确保再次调整学习率。
- en: For more best practices regarding tuning neural network hyperparameters, check
    out the excellent [2018 paper](https://homl.info/1cycle)⁠^([18](ch09.html#id2220))
    by Leslie Smith. The [Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)
    by Google researchers is also well worth reading. The free e-book [*Machine Learning
    Yearning* by Andrew Ng](https://homl.info/ngbook) also contains a wealth of practical
    advice.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 关于调整神经网络超参数的更多最佳实践，请参阅Leslie Smith撰写的优秀的2018年论文[2018 paper](https://homl.info/1cycle)⁠^([18](ch09.html#id2220))。谷歌研究人员撰写的[Deep
    Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)也值得一读。Andrew
    Ng的免费电子书[*Machine Learning Yearning*](https://homl.info/ngbook)也包含大量实用建议。
- en: Lastly, I highly recommend you go through exercise 1 at the end of this chapter.
    You will use a nice web interface to play with various neural network architectures
    and visualize their outputs. This will be very useful to better understand MLPs
    and grow a good intuition for the effects of each hyperparameter (number of layers
    and neurons, activation functions, and more).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我强烈建议您阅读本章末尾的练习1。您将使用一个不错的网络界面来尝试各种神经网络架构并可视化它们的输出。这将非常有用，以更好地理解MLPs并培养对每个超参数（层数和神经元数量、激活函数等）影响的良好直觉。
- en: This concludes our introduction to artificial neural networks and their implementation
    with Scikit-Learn. In the next chapter, we will switch to PyTorch, the leading
    open source library for neural networks, and we will use it to train and run MLPs
    much faster by exploiting the power of graphical processing units (GPUs). We will
    also start building more complex models, with multiple inputs and outputs.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对人工神经网络及其使用Scikit-Learn实现的介绍。在下一章中，我们将转向PyTorch，这是领先的神经网络开源库，我们将利用图形处理单元（GPU）的强大功能，用它来更快地训练和运行MLPs。我们还将开始构建更复杂的模型，具有多个输入和输出。
- en: Exercises
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'This [neural network playground](https://playground.tensorflow.org) is a great
    tool to build your intuitions without writing any code (it was built by the TensorFlow
    team, but there’s nothing TensorFlow-specific about it; in fact, it doesn’t even
    use TensorFlow). In this exercise, you will train several binary classifiers in
    just a few clicks, and tweak the model’s architecture and its hyperparameters
    to gain some intuition on how neural networks work and what their hyperparameters
    do. Take some time to explore the following:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个[神经网络游乐场](https://playground.tensorflow.org)是一个伟大的工具，可以在不编写任何代码的情况下建立你的直觉（它是由
    TensorFlow 团队构建的，但其中没有 TensorFlow 特有的东西；实际上，它甚至没有使用 TensorFlow）。在这个练习中，你只需点击几次就可以训练几个二元分类器，并调整模型的架构及其超参数，以获得一些关于神经网络如何工作以及它们的超参数做什么的直觉。花些时间探索以下内容：
- en: The patterns learned by a neural net. Try training the default neural network
    by clicking the Run button (top left). Notice how it quickly finds a good solution
    for the classification task. The neurons in the first hidden layer have learned
    simple patterns, while the neurons in the second hidden layer have learned to
    combine the simple patterns of the first hidden layer into more complex patterns.
    In general, the more layers there are, the more complex the patterns can be.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络学习到的模式。尝试通过点击运行按钮（左上角）来训练默认神经网络。注意，它如何快速找到一个好的分类任务解决方案。第一隐藏层的神经元已经学会了简单的模式，而第二隐藏层的神经元已经学会了将第一隐藏层的简单模式组合成更复杂的模式。一般来说，层数越多，模式可以越复杂。
- en: Activation functions. Try replacing the tanh activation function with a ReLU
    activation function, and train the network again. Notice that it finds a solution
    even faster, but this time the boundaries are linear. This is due to the shape
    of the ReLU function.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数。尝试将 tanh 激活函数替换为 ReLU 激活函数，并再次训练网络。注意，它甚至更快地找到了解决方案，但这次边界是线性的。这是由于 ReLU
    函数的形状造成的。
- en: The risk of local minima. Modify the network architecture to have just one hidden
    layer with three neurons. Train it multiple times (to reset the network weights,
    click the Reset button next to the Play button). Notice that the training time
    varies a lot, and sometimes it even gets stuck in a local minimum.
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 局部最小值的风险。修改网络架构，使其只有一个包含三个神经元的隐藏层。多次训练它（要重置网络权重，请点击播放按钮旁边的重置按钮）。注意，训练时间变化很大，有时它甚至会陷入局部最小值。
- en: What happens when neural nets are too small. Remove one neuron to keep just
    two. Notice that the neural network is now incapable of finding a good solution,
    even if you try multiple times. The model has too few parameters and systematically
    underfits the training set.
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当神经网络太小时会发生什么。移除一个神经元，只保留两个。注意，现在神经网络无法找到好的解决方案，即使你尝试多次。模型参数太少，系统性地欠拟合训练集。
- en: 'What happens when neural nets are large enough. Set the number of neurons to
    eight, and train the network several times. Notice that it is now consistently
    fast and never gets stuck. This highlights an important finding in neural network
    theory: large neural networks rarely get stuck in local minima, and even when
    they do, these local optima are often almost as good as the global optimum. However,
    they can still get stuck on long plateaus for a long time.'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当神经网络足够大时会发生什么。将神经元数量设置为八，并多次训练网络。注意，现在它始终运行得很快，并且从未陷入困境。这突显了神经网络理论中的一个重要发现：大型神经网络很少陷入局部最小值，即使它们确实如此，这些局部最优解通常几乎与全局最优解一样好。然而，它们仍然可能在长时间内卡在长平顶上。
- en: The risk of vanishing gradients in deep networks. Select the spiral dataset
    (the bottom-right dataset under “DATA”), and change the network architecture to
    have four hidden layers with eight neurons each. Notice that training takes much
    longer and often gets stuck on plateaus for long periods of time. Also notice
    that the neurons in the highest layers (on the right) tend to evolve faster than
    the neurons in the lowest layers (on the left). This problem, called the *vanishing
    gradients* problem, can be alleviated with better weight initialization and other
    techniques, better optimizers (such as AdaGrad or Adam), or batch normalization
    (discussed in [Chapter 11](ch11.html#deep_chapter)).
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度网络中梯度消失的风险。选择螺旋数据集（“数据”下的右下角数据集），并将网络结构更改为具有四个隐藏层，每个层有八个神经元。注意，训练需要更长的时间，并且经常长时间陷入平台期。同时注意，最高层（右侧）的神经元比最低层（左侧）的神经元进化得更快。这个问题，称为*梯度消失*问题，可以通过更好的权重初始化和其他技术、更好的优化器（如AdaGrad或Adam）或批量归一化（在第11章中讨论）来缓解。
- en: Go further. Take an hour or so to play around with other parameters and get
    a feel for what they do to build an intuitive understanding about neural networks.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步探索。花上一小时左右的时间，尝试调整其他参数，以了解它们对神经网络的影响，从而建立对神经网络的直观理解。
- en: 'Draw an ANN using the original artificial neurons (like the ones in [Figure 9-3](#nn_propositional_logic_diagram))
    that computes *A* ⊕ *B* (where ⊕ represents the XOR operation). Hint: *A* ⊕ *B*
    = (*A* ∧ ¬ *B*) ∨ (¬ *A* ∧ *B*).'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用原始的人工神经元（如[图9-3](#nn_propositional_logic_diagram)中的那些）绘制一个计算 *A* ⊕ *B*（其中
    ⊕ 表示异或操作）的ANN。提示：*A* ⊕ *B* = (*A* ∧ ¬ *B*) ∨ (¬ *A* ∧ *B*)。
- en: Why is it generally preferable to use a logistic regression classifier rather
    than a classic perceptron (i.e., a single layer of threshold logic units trained
    using the perceptron training algorithm)? How can you tweak a perceptron to make
    it equivalent to a logistic regression classifier?
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么通常更倾向于使用逻辑回归分类器而不是经典的感知器（即使用感知器训练算法训练的单层阈值逻辑单元）？你如何调整感知器使其等同于逻辑回归分类器？
- en: Why was the sigmoid activation function a key ingredient in training the first
    MLPs?
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么sigmoid激活函数是训练第一个MLP的关键成分？
- en: Name three popular activation functions. Can you draw them?
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出三种流行的激活函数。你能画出它们吗？
- en: Suppose you have an MLP composed of one input layer with 10 passthrough neurons,
    followed by one hidden layer with 50 artificial neurons, and finally one output
    layer with 3 artificial neurons. All artificial neurons use the ReLU activation
    function.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你有一个由一个输入层（10个passthrough神经元）、一个隐藏层（50个人工神经元）和一个输出层（3个人工神经元）组成的MLP。所有人工神经元都使用ReLU激活函数。
- en: What is the shape of the input matrix **X**?
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入矩阵 **X** 的形状是什么？
- en: What are the shapes of the hidden layer’s weight matrix **W**[*h*] and bias
    vector **b**[*h*]?
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏层的权重矩阵 **W**[*h*] 和偏置向量 **b**[*h*] 的形状是什么？
- en: What are the shapes of the output layer’s weight matrix **W**[*o*] and bias
    vector **b**[*o*]?
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层的权重矩阵 **W**[*o*] 和偏置向量 **b**[*o*] 的形状是什么？
- en: What is the shape of the network’s output matrix **Y**?
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络输出矩阵 **Y** 的形状是什么？
- en: Write the equation that computes the network’s output matrix **Y** as a function
    of **X**, **W**[*h*], **b**[*h*], **W**[*o*], and **b**[*o*].
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将计算网络输出矩阵 **Y** 作为 **X**、**W**[*h*]、**b**[*h*]、**W**[*o*] 和 **b**[*o*] 函数的方程式写出来。
- en: How many neurons do you need in the output layer if you want to classify email
    into spam or ham? What activation function should you use in the output layer?
    If instead you want to tackle MNIST, how many neurons do you need in the output
    layer, and which activation function should you use? What about for getting your
    network to predict housing prices, as in [Chapter 2](ch02.html#project_chapter)?
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想将电子邮件分类为垃圾邮件或正常邮件，输出层需要多少个神经元？在输出层应该使用哪种激活函数？如果你要处理MNIST，输出层需要多少个神经元，应该使用哪种激活函数？对于像[第2章](ch02.html#project_chapter)中提到的预测房价，应该如何调整网络结构？
- en: What is backpropagation and how does it work? What is the difference between
    backpropagation and reverse-mode autodiff?
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播是什么？它是如何工作的？反向传播和反向模式自动微分有什么区别？
- en: Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP
    overfits the training data, how could you tweak these hyperparameters to try to
    solve the problem?
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能列出在基本MLP中可以调整的所有超参数吗？如果MLP过度拟合训练数据，你应该如何调整这些超参数来尝试解决这个问题？
- en: Train a deep MLP on the CoverType dataset. You can load it using `sklearn.datasets.fetch_covtype()`.
    See if you can get over 93% accuracy on the test set by fine-tuning the hyperparameters
    manually and/or using `RandomizedSearchCV`.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 CoverType 数据集上训练一个深度 MLP。你可以使用 `sklearn.datasets.fetch_covtype()` 加载它。尝试通过手动调整超参数和使用
    `RandomizedSearchCV` 来获得超过 93% 的测试集准确率。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可以在本章笔记本的末尾找到，在 [*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: ^([1](ch09.html#id2093-marker)) You can get the best of both worlds by being
    open to biological inspirations without being afraid to create biologically unrealistic
    models, as long as they work well.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.html#id2093-marker)) 通过开放于生物启发而不害怕创建生物上不现实的模型，只要它们有效，你可以同时获得两者的最佳之处。
- en: '^([2](ch09.html#id2094-marker)) Warren S. McCulloch and Walter Pitts, “A Logical
    Calculus of the Ideas Immanent in Nervous Activity”, *The Bulletin of Mathematical
    Biology* 5, no. 4 (1943): 115–113.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.html#id2094-marker)) 沃伦·S·麦克洛奇和沃尔特·皮茨，《神经活动内在思想的逻辑演算》，*《数学生物学通报》*
    第 5 卷第 4 期（1943年）：115–113。
- en: ^([3](ch09.html#id2106-marker)) They are not actually attached, just so close
    that they can very quickly exchange chemical signals.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.html#id2106-marker)) 它们实际上并没有连接，只是非常接近，以至于可以非常快速地交换化学信号。
- en: ^([4](ch09.html#id2110-marker)) Image by Bruce Blaus ([Creative Commons 3.0](https://oreil.ly/pMbrK)).
    Reproduced from [*https://en.wikipedia.org/wiki/Neuron*](https://en.wikipedia.org/wiki/Neuron).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch09.html#id2110-marker)) 图片由布鲁斯·布劳斯提供 ([Creative Commons 3.0](https://oreil.ly/pMbrK))。从
    [*https://en.wikipedia.org/wiki/Neuron*](https://en.wikipedia.org/wiki/Neuron)
    复制。
- en: ^([5](ch09.html#id2111-marker)) In the context of machine learning, the phrase
    “neural networks” generally refers to ANNs, not BNNs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.html#id2111-marker)) 在机器学习的背景下，短语“神经网络”通常指的是人工神经网络（ANNs），而不是生物神经网络（BNNs）。
- en: ^([6](ch09.html#id2115-marker)) Drawing of a cortical lamination by S. Ramon
    y Cajal (public domain). Reproduced from [*https://en.wikipedia.org/wiki/Cerebral_cortex*](https://en.wikipedia.org/wiki/Cerebral_cortex).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch09.html#id2115-marker)) S. Ramón y Cajal 绘制的皮质层图（公有领域）。从 [*https://en.wikipedia.org/wiki/Cerebral_cortex*](https://en.wikipedia.org/wiki/Cerebral_cortex)
    复制。
- en: ^([7](ch09.html#id2122-marker)) Logistic regression and the logistic function
    were introduced in [Chapter 4](ch04.html#linear_models_chapter), along with several
    other concepts that we will heavily rely on in this chapter, including softmax,
    cross-entropy, gradient descent, early stopping, and more, so please make sure
    to read it first.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch09.html#id2122-marker)) 逻辑回归和对数几率函数在第 4 章（ch04.html#linear_models_chapter）中介绍，以及我们将在此章中大量依赖的几个其他概念，包括
    softmax、交叉熵、梯度下降、早停等，所以请务必先阅读它。
- en: ^([8](ch09.html#id2128-marker)) In some libraries, such as PyTorch, the weight
    matrix is transposed, so there’s one row per neuron, and one column per input
    feature.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch09.html#id2128-marker)) 在一些库中，例如 PyTorch，权重矩阵是转置的，因此每个神经元对应一行，每个输入特征对应一列。
- en: '^([9](ch09.html#id2132-marker)) Note that this solution is not unique: when
    data points are linearly separable, there is an infinity of hyperplanes that can
    separate them.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch09.html#id2132-marker)) 注意，这个解决方案不是唯一的：当数据点是线性可分时，存在无限多个超平面可以分离它们。
- en: ^([10](ch09.html#id2139-marker)) For example, when the inputs are (0, 1) the
    lower-left neuron computes 0 × 1 + 1 × 1 – 3 / 2 = –1 / 2, which is negative,
    so it outputs 0\. The lower-right neuron computes 0 × 1 + 1 × 1 – 1 / 2 = 1 /
    2, which is positive, so it outputs 1\. The output neuron receives the outputs
    of the first two neurons as its inputs, so it computes 0 × (–1) + 1 × 1 - 1 /
    2 = 1 / 2. This is positive, so it outputs 1.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch09.html#id2139-marker)) 例如，当输入为（0，1）时，左下角的神经元计算 0 × 1 + 1 × 1 – 3 /
    2 = –1 / 2，这是负数，因此它输出 0。右下角的神经元计算 0 × 1 + 1 × 1 – 1 / 2 = 1 / 2，这是正数，因此它输出 1。输出神经元接收前两个神经元的输出作为其输入，因此它计算
    0 × (–1) + 1 × 1 - 1 / 2 = 1 / 2。这是正数，因此它输出 1。
- en: ^([11](ch09.html#id2146-marker)) In the 1990s, an ANN with more than two hidden
    layers was considered deep. Nowadays, it is common to see ANNs with dozens of
    layers, or even hundreds, so the definition of “deep” is quite fuzzy.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch09.html#id2146-marker)) 在 20 世纪 90 年代，具有两个以上隐藏层的 ANN 被认为是深度网络。如今，我们看到具有数十个甚至数百个层的
    ANN 已很常见，因此“深度”的定义相当模糊。
- en: ^([12](ch09.html#id2152-marker)) David Rumelhart et al., “Learning Internal
    Representations by Error Propagation” (Defense Technical Information Center technical
    report, September 1985).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch09.html#id2152-marker)) 大卫·鲁梅尔哈特等人，《通过误差传播学习内部表示》（国防技术信息中心技术报告，1985年9月）。
- en: ^([13](ch09.html#id2162-marker)) Biological neurons seem to implement a roughly
    sigmoid (*S*-shaped) activation function, so researchers stuck to sigmoid functions
    for a very long time. But it turns out that ReLU generally works better in ANNs.
    This is one of the cases where the biological analogy was perhaps misleading.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch09.html#id2162-marker)) 生物神经元似乎实现了大约 sigmoid (*S*-形) 激活函数，因此研究人员长期坚持使用
    sigmoid 函数。但结果发现 ReLU 在人工神经网络中通常表现更好。这是生物类比可能误导的一个例子。
- en: '^([14](ch09.html#id2192-marker)) C. Szegedy et al., “Rethinking the Inception
    Architecture for Computer Vision”, CVPR 2016: 2818–2826.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '^([14](ch09.html#id2192-marker)) C. Szegedy 等人, “重新思考计算机视觉中的 Inception 架构”,
    CVPR 2016: 2818–2826.'
- en: ^([15](ch09.html#id2213-marker)) Dominic Masters and Carlo Luschi, “Revisiting
    Small Batch Training for Deep Neural Networks”, arXiv preprint arXiv:1804.07612
    (2018).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch09.html#id2213-marker)) Dominic Masters 和 Carlo Luschi, “重新审视深度神经网络的小批量训练”,
    arXiv 预印本 arXiv:1804.07612 (2018).
- en: '^([16](ch09.html#id2214-marker)) Elad Hoffer et al., “Train Longer, Generalize
    Better: Closing the Generalization Gap in Large Batch Training of Neural Networks”,
    *Proceedings of the 31st International Conference on Neural Information Processing
    Systems* (2017): 1729–1739.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '^([16](ch09.html#id2214-marker)) Elad Hoffer 等人, “训练更长时间，泛化更好：缩小神经网络大批量训练中的泛化差距”,
    *第 31 届国际神经网络信息处理系统会议论文集* (2017): 1729–1739.'
- en: '^([17](ch09.html#id2215-marker)) Priya Goyal et al., “Accurate, Large Minibatch
    SGD: Training ImageNet in 1 Hour”, arXiv preprint arXiv:1706.02677 (2017).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch09.html#id2215-marker)) Priya Goyal 等人, “准确、大批量 SGD：1 小时训练 ImageNet”,
    arXiv 预印本 arXiv:1706.02677 (2017).
- en: '^([18](ch09.html#id2220-marker)) Leslie N. Smith, “A Disciplined Approach to
    Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch Size, Momentum, and
    Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch09.html#id2220-marker)) Leslie N. Smith, “神经网络的超参数 disciplined 方法：第一部分——学习率、批量大小、动量和权重衰减”,
    arXiv 预印本 arXiv:1803.09820 (2018).
