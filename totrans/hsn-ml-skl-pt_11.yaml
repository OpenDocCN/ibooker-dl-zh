- en: Chapter 9\. Introduction to Artificial Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Birds inspired us to fly, burdock plants inspired Velcro, and nature has inspired
    countless more inventions. It seems only logical, then, to look at the brain’s
    architecture for inspiration on how to build an intelligent machine. This is the
    logic that sparked *artificial neural networks* (ANNs), machine learning models
    inspired by the networks of biological neurons found in our brains. However, although
    planes were inspired by birds, they don’t have to flap their wings to fly. Similarly,
    ANNs have gradually become quite different from their biological cousins. Some
    researchers even argue that we should drop the biological analogy altogether (e.g.,
    by saying “units” rather than “neurons”), lest we restrict our creativity to biologically
    plausible systems.⁠^([1](ch09.html#id2093))
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are at the very core of deep learning. They are versatile, powerful, and
    scalable, making them ideal to tackle large and highly complex machine learning
    tasks such as classifying billions of images (e.g., Google Images), powering speech
    recognition services (e.g., Apple’s Siri or Google Assistant) and chatbots (e.g.,
    ChatGPT or Claude), recommending the best videos to watch to hundreds of millions
    of users every day (e.g., YouTube), or learning how proteins fold (DeepMind’s
    AlphaFold).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces artificial neural networks, starting with a quick tour
    of the very first ANN architectures and leading up to multilayer perceptrons (MLPs),
    which are heavily used today (many other architectures will be explored in the
    following chapters). In this chapter, we will implement simple MLPs using Scikit-Learn
    to get our feet wet, and in the next chapter we will switch to PyTorch, as it
    is a much more flexible and efficient library for neural nets.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s go back in time to the origins of artificial neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: From Biological to Artificial Neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Surprisingly, ANNs have been around for quite a while: they were first introduced
    back in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter
    Pitts. In their [landmark paper](https://homl.info/43),⁠^([2](ch09.html#id2094))
    “A Logical Calculus of Ideas Immanent in Nervous Activity”, McCulloch and Pitts
    presented a simplified computational model of how biological neurons might work
    together in animal brains to perform complex computations using *propositional
    logic*. This was the first artificial neural network architecture. Since then
    many other architectures have been invented, as you will see.'
  prefs: []
  type: TYPE_NORMAL
- en: The early successes of ANNs led to the widespread belief that we would soon
    be conversing with truly intelligent machines. When it became clear in the 1960s
    that this promise would go unfulfilled (at least for quite a while), funding flew
    elsewhere, and ANNs entered a long winter. In the early 1980s, new architectures
    were invented and better training techniques were developed, sparking a revival
    of interest in *connectionism*, the study of neural networks. But progress was
    slow, and by the 1990s other powerful machine learning techniques had been invented,
    such as support vector machines. These techniques seemed to offer better results
    and stronger theoretical foundations than ANNs, so once again the study of neural
    networks was put on hold.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now witnessing yet another wave of interest in ANNs. Will this wave
    die out like the previous ones did? Well, here are a few good reasons to believe
    that this time is different and that the renewed interest in ANNs will have a
    much more profound impact on our lives:'
  prefs: []
  type: TYPE_NORMAL
- en: There is now a huge quantity of data available to train neural networks, and
    ANNs frequently outperform other ML techniques on very large and complex problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tremendous increase in computing power since the 1990s now makes it possible
    to train large neural networks in a reasonable amount of time. This is in part
    due to Moore’s law (the number of components in integrated circuits has doubled
    about every 2 years over the last 50 years), but also thanks to the gaming industry,
    which has stimulated the production of powerful *graphical processing units* (GPUs)
    by the millions: GPU cards were initially designed to accelerate graphics, but
    it turns out that neural networks perform similar computations (such as large
    matrix multiplications), so they can also be accelerated using GPUs. Moreover,
    cloud platforms have made this power accessible to everyone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training algorithms have been improved. To be fair they are only slightly
    different from the ones used in the 1990s, but these relatively small tweaks have
    had a huge positive impact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some theoretical limitations of ANNs have turned out to be benign in practice.
    For example, many people thought that ANN training algorithms were doomed because
    they were likely to get stuck in local optima, but it turns out that this is not
    a big problem in practice, especially for larger neural networks: the local optima
    often perform almost as well as the global optimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The invention of the Transformer architecture in 2017 (see [Chapter 15](ch15.html#transformer_chapter))
    has been a game changer: it can process and generate all sorts of data (e.g.,
    text, images, audio) unlike earlier, more specialized, architectures, and it performs
    great across a wide variety of tasks from robotics to protein folding. Moreover,
    it scales rather well, which has made it possible to train very large *foundation
    models* that can be reused across many different tasks, possibly with a bit of
    fine-tuning (that’s transfer learning), or just by prompting the model in the
    right way (that’s *in-context learning*, or ICL). For instance, you can give it
    a few examples of the task at hand (that’s *few-shot learning*, or FSL), or ask
    it to reason step-by-step (that’s *chain-of-thought* prompting, or CoT). It’s
    a new world!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANNs seem to have entered a virtuous circle of funding and progress. Amazing
    products based on ANNs regularly make the headline news, which pulls more and
    more attention and funding toward them, resulting in more and more progress and
    even more amazing products. AI is no longer just powering products in the shadows:
    since chatbots such as ChatGPT were released, the general public is now directly
    interacting daily with AI assistants, and the big tech companies are competing
    fiercely to grab this gigantic market: the pace of innovation is wild.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biological Neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we discuss artificial neurons, let’s take a quick look at a biological
    neuron (represented in [Figure 9-1](#biological_neuron_wikipedia)). It is an unusual-looking
    cell mostly found in animal brains. It’s composed of a *cell body* containing
    the nucleus and most of the cell’s complex components, many branching extensions
    called *dendrites*, plus one very long extension called the *axon*. The axon’s
    length may be just a few times longer than the cell body, or up to tens of thousands
    of times longer. Near its extremity the axon splits off into many branches called
    *telodendria*, and at the tip of these branches are minuscule structures called
    *synaptic terminals* (or simply *synapses*), which are connected to the dendrites
    or cell bodies of other neurons.⁠^([3](ch09.html#id2106)) Biological neurons produce
    short electrical impulses called *action potentials* (APs, or just *signals*),
    which travel along the axons and make the synapses release chemical signals called
    *neurotransmitters*. When a neuron receives a sufficient amount of these neurotransmitters
    within a few milliseconds, it fires its own electrical impulses (actually, it
    depends on the neurotransmitters, as some of them inhibit the neuron from firing).
  prefs: []
  type: TYPE_NORMAL
- en: '![Illustration of a biological neuron highlighting key components such as the
    cell body, dendrites, axon, telodendria, and synaptic terminals, demonstrating
    the neuron''s structure and connection points within neural networks.](assets/hmls_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A biological neuron⁠^([4](ch09.html#id2110))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thus, individual biological neurons seem to behave in a simple way, but they’re
    organized in a vast network of billions, with each neuron typically connected
    to thousands of other neurons. Highly complex computations can be performed by
    a network of fairly simple neurons, much like a complex anthill can emerge from
    the combined efforts of simple ants. The architecture of biological neural networks
    (BNNs)⁠^([5](ch09.html#id2111)) is the subject of active research, but some parts
    of the brain have been mapped. These efforts show that neurons are often organized
    in consecutive layers, especially in the cerebral cortex (the outer layer of the
    brain), as shown in [Figure 9-2](#biological_neural_network_wikipedia).
  prefs: []
  type: TYPE_NORMAL
- en: '![Illustration of layered neuron networks in the human cerebral cortex, emphasizing
    the complex organization of biological neural networks.](assets/hmls_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Multiple layers in a biological neural network (human cortex)⁠^([6](ch09.html#id2115))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Logical Computations with Neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'McCulloch and Pitts proposed a very simple model of the biological neuron,
    which later became known as an *artificial neuron*: it has one or more binary
    (on/off) inputs and one binary output. The artificial neuron activates its output
    when more than a certain number of its inputs are active. In their paper, McCulloch
    and Pitts showed that even with such a simplified model it is possible to build
    a network of artificial neurons that can compute any logical proposition you want.
    To see how such a network works, let’s build a few ANNs that perform various logical
    computations (see [Figure 9-3](#nn_propositional_logic_diagram)), assuming that
    a neuron is activated when at least two of its input connections are active.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing artificial neural networks performing logical computations,
    including AND, OR, and NOT operations with neurons labeled A, B, and C.](assets/hmls_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. ANNs performing simple logical computations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s see what these networks do:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first network on the left is the identity function: if neuron A is activated,
    then neuron C gets activated as well (since it receives two input signals from
    neuron A); but if neuron A is off, then neuron C is off as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second network performs a logical AND: neuron C is activated only when
    both neurons A and B are activated (a single input signal is not enough to activate
    neuron C).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third network performs a logical OR: neuron C gets activated if either
    neuron A or neuron B is activated (or both).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, if we suppose that an input connection can inhibit the neuron’s activity
    (which is the case with biological neurons), then the fourth network computes
    a slightly more complex logical proposition: neuron C is activated only if neuron
    A is active and neuron B is off. If neuron A is active all the time, then you
    get a logical NOT: neuron C is active when neuron B is off, and vice versa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can imagine how these networks can be combined to compute complex logical
    expressions (see the exercises at the end of the chapter for an example).
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *perceptron* is one of the simplest ANN architectures, invented in 1957
    by Frank Rosenblatt. It is based on a slightly different artificial neuron (see
    [Figure 9-4](#artificial_neuron_diagram)) called a *threshold logic unit* (TLU),
    or sometimes a *linear threshold unit* (LTU). The inputs and output are numbers
    (instead of binary on/off values), and each input connection is associated with
    a weight. The TLU first computes a linear function of its inputs: *z* = *w*[1]
    *x*[1] + *w*[2] *x*[2] + ⋯ + *w*[*n*] *x*[*n*] + *b* = **w**^⊺ **x** + *b*. Then
    it applies a *step function* to the result: *h*[**w**](**x**) = step(*z*). So
    it’s almost like logistic regression, except it uses a step function instead of
    the logistic function.⁠^([7](ch09.html#id2122)) Just like in logistic regression,
    the model parameters are the input weights **w** and the bias term *b*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of a Threshold Logic Unit (TLU) showing how inputs multiplied by
    weights are summed with a bias, and a step function is applied to determine the
    output.](assets/hmls_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-4\. TLU: an artificial neuron that computes a weighted sum of its
    inputs **w**^⊺ **x**, plus a bias term *b*, then applies a step function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The most common step function used in perceptrons is the *Heaviside step function*
    (see [Equation 9-1](#step_functions_equation)). Sometimes the sign function is
    used instead.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 9-1\. Common step functions used in perceptrons (assuming threshold
    = 0)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo form="prefix">heaviside</mo>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd>
    <mtd columnalign="left"><mrow><mo form="prefix">sgn</mo> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>+</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable>
  prefs: []
  type: TYPE_NORMAL
- en: A single TLU can be used for simple linear binary classification. It computes
    a linear function of its inputs, and if the result exceeds a threshold, it outputs
    the positive class. Otherwise, it outputs the negative class. This may remind
    you of logistic regression ([Chapter 4](ch04.html#linear_models_chapter)) or linear
    SVM classification (see the online chapter on SVMs at [*https://homl.info*](https://homl.info)).
    You could, for example, use a single TLU to classify iris flowers based on petal
    length and width. Training such a TLU would require finding the right values for
    *w*[1], *w*[2], and *b* (the training algorithm is discussed shortly).
  prefs: []
  type: TYPE_NORMAL
- en: A perceptron is composed of one or more TLUs organized in a single layer, where
    every TLU is connected to every input. Such a layer is called a *fully connected
    layer*, or a *dense layer*. The inputs constitute the *input layer*. And since
    the layer of TLUs produces the final outputs, it is called the *output layer*.
    For example, a perceptron with two inputs and three outputs is represented in
    [Figure 9-5](#perceptron_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of a perceptron architecture with two input neurons connected to
    three output neurons in a fully connected layer, illustrating TLUs in the output
    layer.](assets/hmls_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Architecture of a perceptron with two inputs and three output neurons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This perceptron can classify instances simultaneously into three different binary
    classes, which makes it a multilabel classifier. It may also be used for multiclass
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the magic of linear algebra, [Equation 9-2](#neural_network_layer_equation)
    can be used to efficiently compute the outputs of a layer of artificial neurons
    for several instances at once.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 9-2\. Computing the outputs of a fully connected layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $ModifyingAbove bold upper Y With caret equals phi left-parenthesis bold upper
    X bold upper W plus bold b right-parenthesis$
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: $ModifyingAbove bold upper Y With caret$ is the output matrix. It has one row
    per instance and one column per neuron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**X** is the input matrix. It has one row per instance and one column per input
    feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weight matrix **W** contains all the connection weights. It has one row
    per input feature and one column per neuron.⁠^([8](ch09.html#id2128))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bias vector **b** contains all the bias terms: one per neuron.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function *ϕ* is called the *activation function*: when the artificial neurons
    are TLUs, it is a step function (we will discuss other activation functions shortly).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In mathematics, the sum of a matrix and a vector is undefined. However, in
    data science, we allow “broadcasting”: adding a vector to a matrix means adding
    it to every row in the matrix. So, **XW** + **b** first multiplies **X** by **W**—which
    results in a matrix with one row per instance and one column per output—then adds
    the vector **b** to every row of that matrix, which adds each bias term to the
    corresponding output, for every instance. Moreover, *ϕ* is then applied itemwise
    to each item in the resulting matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how is a perceptron trained? The perceptron training algorithm proposed
    by Rosenblatt was largely inspired by *Hebb’s rule*. In his 1949 book, *The Organization
    of Behavior* (Wiley), Donald Hebb suggested that when a biological neuron triggers
    another neuron often, the connection between these two neurons grows stronger.
    Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire
    together, wire together”; that is, the connection weight between two neurons tends
    to increase when they fire simultaneously. This rule later became known as Hebb’s
    rule (or *Hebbian learning*). Perceptrons are trained using a variant of this
    rule that takes into account the error made by the network when it makes a prediction;
    the perceptron learning rule reinforces connections that help reduce the error.
    More specifically, the perceptron is fed one training instance at a time, and
    for each instance it makes its predictions. For every output neuron that produced
    a wrong prediction, it reinforces the connection weights from the inputs that
    would have contributed to the correct prediction. The rule is shown in [Equation
    9-3](#perceptron_update_rule).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 9-3\. Perceptron learning rule (weight update)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $w Subscript i comma j Baseline Superscript left-parenthesis next step right-parenthesis
    Baseline equals w Subscript i comma j Baseline plus eta left-parenthesis y Subscript
    j Baseline minus ModifyingAbove y With caret Subscript j Baseline right-parenthesis
    x Subscript i$
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w*[*i*,] [*j*] is the connection weight between the *i*^(th) input and the
    *j*^(th) neuron.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[*i*] is the *i*^(th) input value of the current training instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $ModifyingAbove y With caret Subscript j$ is the output of the *j*^(th) output
    neuron for the current training instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[*j*] is the target output of the *j*^(th) output neuron for the current
    training instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*η* is the learning rate (see [Chapter 4](ch04.html#linear_models_chapter)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision boundary of each output neuron is linear, so perceptrons are incapable
    of learning complex patterns (just like logistic regression classifiers). However,
    if the training instances are linearly separable, Rosenblatt demonstrated that
    this algorithm will converge to a solution.⁠^([9](ch09.html#id2132)) This is called
    the *perceptron convergence theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides a `Perceptron` class that can be used pretty much as
    you would expect—for example, on the iris dataset (introduced in [Chapter 4](ch04.html#linear_models_chapter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You may have noticed that the perceptron learning algorithm strongly resembles
    stochastic gradient descent (introduced in [Chapter 4](ch04.html#linear_models_chapter)).
    In fact, Scikit-Learn’s `Perceptron` class is equivalent to using an `SGDClassifier`
    with the following hyperparameters: `loss="perceptron"`, `learning_rate="constant"`,
    `eta0=1` (the learning rate), and `penalty=None` (no regularization).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Contrary to logistic regression classifiers, perceptrons do not output a class
    probability. This is one reason to prefer logistic regression over perceptrons.
    Moreover, perceptrons do not use any regularization by default, and training stops
    as soon as there are no more prediction errors on the training set, so the model
    typically does not generalize as well as logistic regression or a linear SVM classifier.
    However, perceptrons may train a bit faster.
  prefs: []
  type: TYPE_NORMAL
- en: In their 1969 monograph, *Perceptrons*, Marvin Minsky and Seymour Papert highlighted
    a number of serious weaknesses of perceptrons—in particular, the fact that they
    are incapable of solving some trivial problems (e.g., the *exclusive OR* (XOR)
    classification problem; see the left side of [Figure 9-6](#xor_diagram)). This
    is true of any other linear classification model (such as logistic regression
    classifiers), but researchers had expected much more from perceptrons, and some
    were so disappointed that they dropped neural networks altogether in favor of
    more formal approaches such as logic, problem solving, and search. The lack of
    practical applications also didn’t help.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that some of the limitations of perceptrons can be eliminated by
    stacking multiple perceptrons. The resulting ANN is called a *multilayer perceptron*
    (MLP).
  prefs: []
  type: TYPE_NORMAL
- en: The Multilayer Perceptron and Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An MLP can solve the XOR problem, as you can verify by computing the output
    of the MLP represented on the righthand side of [Figure 9-6](#xor_diagram): with
    inputs (0, 0) or (1, 1), the network outputs 0, and with inputs (0, 1) or (1,
    0) it outputs 1\. Try verifying that this network indeed solves the XOR problem!^([10](ch09.html#id2139))'
  prefs: []
  type: TYPE_NORMAL
- en: An MLP is composed of one input layer, one or more layers of artificial neurons
    (originally TLUs) called *hidden layers*, and one final layer of artificial neurons
    called the *output layer* (see [Figure 9-7](#mlp_diagram)). The layers close to
    the input layer are usually called the *lower layers*, and the ones close to the
    outputs are usually called the *upper layers*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the XOR classification problem and an MLP using threshold
    logic units to solve it.](assets/hmls_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. XOR classification problem and an MLP that solves it
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Diagram of a multilayer perceptron showing a feedforward neural network with
    two input neurons, a hidden layer of four neurons, and three output neurons.](assets/hmls_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Architecture of a multilayer perceptron with two inputs, one hidden
    layer of four neurons, and three output neurons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The signal flows only in one direction (from the inputs to the outputs), so
    this architecture is an example of a *feedforward neural network* (FNN).
  prefs: []
  type: TYPE_NORMAL
- en: When an ANN contains a deep stack of hidden layers,⁠^([11](ch09.html#id2146))
    it is called a *deep neural network* (DNN). The field of deep learning studies
    DNNs, and more generally it is interested in models containing deep stacks of
    computations. Even so, many people talk about deep learning whenever neural networks
    are involved (even shallow ones).
  prefs: []
  type: TYPE_NORMAL
- en: For many years researchers struggled to find a way to train MLPs, without success.
    In the early 1960s several researchers discussed the possibility of using gradient
    descent to train neural networks, but as we saw in [Chapter 4](ch04.html#linear_models_chapter),
    this requires computing the gradients of the model’s error with regard to the
    model parameters; it wasn’t clear at the time how to do this efficiently with
    such a complex model containing so many parameters, especially with the computers
    they had back then.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in 1970, a researcher named Seppo Linnainmaa introduced in his master’s
    thesis a technique to compute all the gradients automatically and efficiently.
    This algorithm is now called *reverse-mode automatic differentiation* (or *reverse-mode
    autodiff* for short). In just two passes through the network (one forward, one
    backward), it is able to compute the gradients of the neural network’s error with
    regard to every single model parameter. In other words, it can find out how each
    connection weight and each bias should be tweaked in order to reduce the neural
    network’s error. These gradients can then be used to perform a gradient descent
    step. If you repeat this process of computing the gradients automatically and
    taking a gradient descent step, the neural network’s error will gradually drop
    until it eventually reaches a minimum. This combination of reverse-mode autodiff
    and gradient descent is now called *backpropagation* (or *backprop* for short).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an analogy: imagine you are learning to shoot a basketball into the
    hoop. You throw the ball (that’s the forward pass), and you observe that it went
    far off to the right side (that’s the error computation), then you consider how
    you can change your body position to throw the ball a bit less to the right next
    time (that’s the backward pass): you realize that your arm will need to rotate
    a bit counterclockwise, and probably your whole upper body as well, which in turn
    means that your feet should turn too (notice how we’re going down the “layers”).
    Once you’ve thought it through, you actually move your body: that’s the gradient
    descent step. The smaller the errors, the smaller the adjustments. As you repeat
    the whole process many times, the error gradually gets smaller, and after a few
    hours of practice, you manage to get the ball through the hoop every time. Good
    job!'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are various autodiff techniques, with different pros and cons. Reverse-mode
    autodiff is well suited when the function to differentiate has many variables
    (e.g., connection weights and biases) and few outputs (e.g., one loss). If you
    want to learn more about autodiff, check out [Appendix A](app01.html#autodiff_appendix).
  prefs: []
  type: TYPE_NORMAL
- en: 'Backpropagation can actually be applied to all sorts of computational graphs,
    not just neural networks: indeed, Linnainmaa’s master’s thesis was not about neural
    nets at all, it was more general. It was several more years before backprop started
    to be used to train neural networks, but it still wasn’t mainstream. Then, in
    1985, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a [paper](https://homl.info/44)⁠^([12](ch09.html#id2152))
    analyzing how backpropagation allows neural networks to learn useful internal
    representations. Their results were so impressive that backpropagation was quickly
    popularized in the field. Over 40 years later, it is still by far the most popular
    training technique for neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run through how backpropagation works again in a bit more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: It handles one mini-batch at a time, and goes through the full training set
    multiple times. If each mini-batch contains 32 instances, and each instance has
    100 features, then the mini-batch will be represented as a matrix with 32 rows
    and 100 columns. Each pass through the training set is called an *epoch*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each mini-batch, the algorithm computes the output of all the neurons in
    the first hidden layer using [Equation 9-2](#neural_network_layer_equation). If
    the layer has 50 neurons, then its output is a matrix with one row per sample
    in the mini-batch (e.g., 32), and 50 columns (i.e., one per neuron). This matrix
    is then passed on to the next layer, its output is computed and passed to the
    next layer, and so on until we get the output of the last layer, the output layer.
    This is the *forward pass*: it is exactly like making predictions, except all
    intermediate results are preserved since they are needed for the backward pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, the algorithm measures the network’s output error (i.e., it uses a loss
    function that compares the desired output and the actual output of the network,
    and returns some measure of the error).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then it computes how much each output layer parameter contributed to the error.
    This is done analytically by applying the *chain rule* (one of the most fundamental
    rules in calculus), which makes this step fast and precise. The result is one
    gradient per parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm then measures how much of these error contributions came from
    each connection in the layer below, again using the chain rule, working backward
    until it reaches the input layer. As explained earlier, this reverse pass efficiently
    measures the error gradient across all the connection weights and biases in the
    network by propagating the error gradient backward through the network (hence
    the name of the algorithm).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the algorithm performs a gradient descent step to tweak all the connection
    weights and bias terms in the network, using the error gradients it just computed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is important to initialize all the hidden layers’ connection weights randomly,
    or else training will fail. For example, if you initialize all weights and biases
    to zero, then all neurons in a given layer will be perfectly identical, and thus
    backpropagation will affect them in exactly the same way, so they will remain
    identical. In other words, despite having hundreds of neurons per layer, your
    model will act as if it had only one neuron per layer: it won’t be too smart.
    If instead you randomly initialize the weights, you *break the symmetry* and allow
    backpropagation to train a diverse team of neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, backpropagation makes predictions for a mini-batch (forward pass),
    measures the error, then goes through each layer in reverse to measure the error
    contribution from each parameter (reverse pass), and finally tweaks the connection
    weights and biases to reduce the error (gradient descent step).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for backprop to work properly, Rumelhart and his colleagues made a
    key change to the MLP’s architecture: they replaced the step function with the
    logistic function, *σ*(*z*) = 1 / (1 + exp(–*z*)), also called the *sigmoid* function.
    This was essential because the step function contains only flat segments, so there
    is no gradient to work with (gradient descent cannot move on a flat surface),
    while the sigmoid function has a well-defined nonzero derivative everywhere, allowing
    gradient descent to make some progress at every step. In fact, the backpropagation
    algorithm works well with many other activation functions, not just the sigmoid
    function. Here are two other popular choices:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *hyperbolic tangent* function: tanh(*z*) = 2*σ*(2*z*) – 1'
  prefs: []
  type: TYPE_NORMAL
- en: Just like the sigmoid function, this activation function is *S*-shaped, continuous,
    and differentiable, but its output value ranges from –1 to 1 (instead of 0 to
    1 in the case of the sigmoid function). That range tends to make each layer’s
    output more or less centered around 0 at the beginning of training, which often
    helps speed up convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rectified linear unit function: ReLU(*z*) = max(0, *z*)'
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU function is continuous but unfortunately not differentiable at *z*
    = 0 (the slope changes abruptly, which can make gradient descent bounce around),
    and its derivative is 0 for *z* < 0\. In practice, however, it works very well
    and has the advantage of being fast to compute, so it has become the default for
    most architectures (except the Transformer architecture, as we will see in [Chapter 15](ch15.html#transformer_chapter)).⁠^([13](ch09.html#id2162))
    Importantly, the fact that it does not have a maximum output value helps reduce
    some issues during gradient descent (we will come back to this in [Chapter 11](ch11.html#deep_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: 'These popular activation functions and their derivatives are represented in
    [Figure 9-8](#activation_functions_plot). But wait! Why do we need activation
    functions in the first place? Well, if you chain several linear transformations,
    all you get is a linear transformation. For example, if f(*x*) = 2*x* + 3 and
    g(*x*) = 5*x* – 1, then chaining these two linear functions gives you another
    linear function: f(g(*x*)) = 2(5*x* – 1) + 3 = 10*x* + 1\. So if you don’t have
    some nonlinearity between layers, then even a deep stack of layers is equivalent
    to a single layer, and you can’t solve very complex problems with that. Conversely,
    a large enough DNN with nonlinear activations can theoretically approximate any
    continuous function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating four activation functions—Heaviside, ReLU, Sigmoid,
    and Tanh—alongside their respective derivatives, highlighting nonlinearity essential
    for deep neural networks.](assets/hmls_0908.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-8\. Activation functions (left) and their derivatives (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OK! You know where neural nets came from, what the MLP architecture looks like,
    and how it computes its outputs. You’ve also learned about the backpropagation
    algorithm. It’s time to see MLPs in action!
  prefs: []
  type: TYPE_NORMAL
- en: Building and Training MLPs with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLPs can tackle a wide range of tasks, but the most common are regression and
    classification. Scikit-Learn can help with both of these. Let’s start with regression.
  prefs: []
  type: TYPE_NORMAL
- en: Regression MLPs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How would you build an MLP for a regression task? Well, if you want to predict
    a single value (e.g., the price of a house, given many of its features), then
    you just need a single output neuron: its output is the predicted value. For multivariate
    regression (i.e., to predict multiple values at once), you need one output neuron
    per output dimension. For example, to locate the center of an object in an image,
    you need to predict 2D coordinates, so you need two output neurons. If you also
    want to place a bounding box around the object, then you need two more numbers:
    the width and the height of the object. So, you end up with four output neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn includes an `MLPRegressor` class, so let’s use it to build an
    MLP with three hidden layers composed of 50 neurons each, and train it on the
    California housing dataset. For simplicity, we will use Scikit-Learn’s `fetch_california_housing()`
    function to load the data. This dataset is simpler than the one we used in [Chapter 2](ch02.html#project_chapter),
    since it contains only numerical features (there is no `ocean_proximity` feature),
    and there are no missing values. The targets are also scaled down: each unit represents
    $100,000\. Let’s start by importing everything we will need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s fetch the California housing dataset and split it into a training
    set and a test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s create an `MLPRegressor` model with 3 hidden layers composed of 50
    neurons each. The first hidden layer’s input size (i.e., the number of rows in
    its weights matrix) and the output layer’s output size (i.e., the number of columns
    in its weights matrix) will adjust automatically to the dimensionality of the
    inputs and targets, respectively, when training starts. The model uses the ReLU
    activation function in all hidden layers, and no activation function at all on
    the output layer. We also set `verbose=True` to get details on the model’s progress
    during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Since neural nets can have a *lot* of parameters, they have a tendency to overfit
    the training set. To reduce this risk, one option is to use early stopping (introduced
    in [Chapter 4](ch04.html#linear_models_chapter)): when we set `early_stopping=True`,
    the `MLPRegressor` class automatically sets aside 10% of the training data and
    uses it to evaluate the model at each epoch (you can adjust the validation set’s
    size by setting `validation_fraction`). If the validation score stops improving
    for 10 epochs, training automatically stops (you can tweak this number of epochs
    by setting `n_iter_no_change`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s create a pipeline to standardize the input features before sending
    them to the `MLPRegressor`. This is very important because gradient descent does
    not converge very well when the features have very different scales, as we saw
    in [Chapter 4](ch04.html#linear_models_chapter). We can then train the model!
    The `MLPRegressor` class uses a variant of gradient descent called *Adam* (see
    [Chapter 11](ch11.html#deep_chapter)) to minimize the mean squared error. It also
    uses a tiny bit of ℓ[2] regularization (you can control its strength via the `alpha`
    hyperparameter, which defaults to 0.0001):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]` [PRE6] And there you go, you just trained your very first MLP! It required
    45 epochs, and as you can see, the training loss went down at each epoch. This
    loss corresponds to [Equation 4-9](ch04.html#ridge_cost_function) divided by 2,
    so you must multiply it by 2 to get the MSE (although not exactly because the
    loss includes the ℓ[2] regularization term). The validation score generally went
    up at each epoch. Like every regressor in Scikit-Learn, `MLPRegressor` uses the
    R² score by default for evaluation—that’s what the `score()` method returns. As
    we saw in [Chapter 2](ch02.html#project_chapter), the R² score measures the ratio
    of the variance that is explained by the model. In this case, it reaches close
    to 80% on the validation set, which is fairly good for this task:    [PRE7]py   [PRE8]`py
    [PRE9]py[PRE10][PRE11][PRE12] from sklearn.datasets import fetch_openml  fashion_mnist
    = fetch_openml(name="Fashion-MNIST", as_frame=False) targets = fashion_mnist.target.astype(int)
    [PRE13] X_train, y_train = fashion_mnist.data[:60_000], targets[:60_000] X_test,
    y_test = fashion_mnist.data[60_000:], targets[60_000:] [PRE14] import matplotlib.pyplot
    as plt  X_sample = X_train[0].reshape(28, 28)  # first image in the training set
    plt.imshow(X_sample, cmap="binary") plt.show() [PRE15] class_names = ["T-shirt/top",
    "Trouser", "Pullover", "Dress", "Coat",                "Sandal", "Shirt", "Sneaker",
    "Bag", "Ankle boot"] [PRE16] >>> class_names[y_train[0]] `''Ankle boot''` [PRE17][PRE18]
    from sklearn.neural_network import MLPClassifier from sklearn.preprocessing import
    MinMaxScaler  mlp_clf = MLPClassifier(hidden_layer_sizes=[300, 100], verbose=True,                         early_stopping=True,
    random_state=42) pipeline = make_pipeline(MinMaxScaler(), mlp_clf) pipeline.fit(X_train,
    y_train) accuracy = pipeline.score(X_test, y_test) [PRE19] >>> X_new = X_test[:15]  #
    let''s pretend these are 15 new images `>>>` `mlp_clf``.``predict``(``X_new``)`
    `` `array([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 8, 3, 4])` `` [PRE20]`` [PRE21]
    >>> y_proba = mlp_clf.predict_proba(X_new) `>>>` `y_proba``[``12``]` `` `array([0.,
    0., 0., 0., 0., 0., 0., 0., 1., 0.])` `` [PRE22] ``Hmm, that’s not great: the
    model is telling us that it’s 100% confident that the image represents a bag (index
    8). So not only is the model wrong, it’s 100% confident that it’s right. In fact,
    across all 10,000 images in the test set, there are only 16 images that the model
    is less than 99.9% confident about, despite the fact that its accuracy is about
    90%. That’s why you should always treat estimated probabilities with a grain of
    salt: neural nets have a strong tendency to be overconfident, especially if they
    are trained for a bit too long.    ###### Tip    The targets for classification
    tasks can be class indices (e.g., 3) or class probabilities, typically one-hot
    vectors (e.g., [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]). But if your model tends to be
    overconfident, you can try the *label smoothing* technique:⁠^([14](ch09.html#id2192))
    reduce the target class’s probability slightly (e.g., from 1 down to 0.9) and
    distribute the rest evenly across the other classes (e.g., [0.1/9, 0.1/9, 0.1/9,
    0.9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9]).    Still, getting 90% accuracy
    on Fashion MNIST is pretty good. You could get even better performance by fine-tuning
    the hyperparameters, for example using `RandomizedSearchCV`, as we did in [Chapter 2](ch02.html#project_chapter).
    However, the search space is quite large, so it helps to know roughly where to
    look.`` [PRE23]` [PRE24][PRE25][PRE26][PRE27][PRE28]  [PRE29]'
  prefs: []
  type: TYPE_NORMAL
