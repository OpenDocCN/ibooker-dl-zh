- en: Chapter 9\. Introduction to Artificial Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 人工神经网络简介
- en: Birds inspired us to fly, burdock plants inspired Velcro, and nature has inspired
    countless more inventions. It seems only logical, then, to look at the brain’s
    architecture for inspiration on how to build an intelligent machine. This is the
    logic that sparked *artificial neural networks* (ANNs), machine learning models
    inspired by the networks of biological neurons found in our brains. However, although
    planes were inspired by birds, they don’t have to flap their wings to fly. Similarly,
    ANNs have gradually become quite different from their biological cousins. Some
    researchers even argue that we should drop the biological analogy altogether (e.g.,
    by saying “units” rather than “neurons”), lest we restrict our creativity to biologically
    plausible systems.⁠^([1](ch09.html#id2093))
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 鸟类启发了我们飞翔，蒺藜植物启发了维可牢，而自然也启发了无数其他发明。因此，从大脑的架构中寻找灵感，以了解如何构建智能机器，这似乎是合乎逻辑的。这就是激发**人工神经网络**（ANNs）的逻辑，这些机器学习模型受到了我们大脑中生物神经元网络的启发。然而，尽管飞机是受鸟类启发的，但它们并不需要拍打翅膀才能飞翔。同样，ANNs逐渐与它们的生物表亲大相径庭。一些研究人员甚至认为，我们应该完全放弃生物类比（例如，用“单元”而不是“神经元”来表示），以免将我们的创造力限制在生物可能存在的系统中。⁠^([1](ch09.html#id2093))
- en: ANNs are at the very core of deep learning. They are versatile, powerful, and
    scalable, making them ideal to tackle large and highly complex machine learning
    tasks such as classifying billions of images (e.g., Google Images), powering speech
    recognition services (e.g., Apple’s Siri or Google Assistant) and chatbots (e.g.,
    ChatGPT or Claude), recommending the best videos to watch to hundreds of millions
    of users every day (e.g., YouTube), or learning how proteins fold (DeepMind’s
    AlphaFold).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs是深度学习的核心。它们功能多样、强大且可扩展，非常适合处理大型且高度复杂的机器学习任务，例如对数十亿张图片进行分类（例如，谷歌图片）、提供语音识别服务（例如，苹果的Siri或谷歌助手）和聊天机器人（例如，ChatGPT或Claude）、每天为上亿用户推荐最佳视频观看（例如，YouTube）或学习蛋白质如何折叠（DeepMind的AlphaFold）。
- en: This chapter introduces artificial neural networks, starting with a quick tour
    of the very first ANN architectures and leading up to multilayer perceptrons (MLPs),
    which are heavily used today (many other architectures will be explored in the
    following chapters). In this chapter, we will implement simple MLPs using Scikit-Learn
    to get our feet wet, and in the next chapter we will switch to PyTorch, as it
    is a much more flexible and efficient library for neural nets.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了人工神经网络，从对最早的ANN架构的快速浏览开始，一直介绍到今天广泛使用的多层感知器（MLPs）。在本章中，我们将使用Scikit-Learn实现简单的MLPs，以熟悉相关概念，在下一章中，我们将转向PyTorch，因为它是一个更灵活、更高效的神经网络库。
- en: Now let’s go back in time to the origins of artificial neural networks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到人工神经网络起源的时代。
- en: From Biological to Artificial Neurons
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从生物神经元到人工神经元
- en: 'Surprisingly, ANNs have been around for quite a while: they were first introduced
    back in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter
    Pitts. In their [landmark paper](https://homl.info/43),⁠^([2](ch09.html#id2094))
    “A Logical Calculus of Ideas Immanent in Nervous Activity”, McCulloch and Pitts
    presented a simplified computational model of how biological neurons might work
    together in animal brains to perform complex computations using *propositional
    logic*. This was the first artificial neural network architecture. Since then
    many other architectures have been invented, as you will see.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 惊讶的是，ANNs已经存在很长时间了：它们最早是在1943年由神经生理学家沃伦·麦克洛奇和数学家沃尔特·皮茨提出的。在他们的[里程碑式论文](https://homl.info/43)，⁠^([2](ch09.html#id2094))“内在于神经活动中的逻辑演算”中，麦克洛奇和皮茨提出了一种简化的计算模型，描述了生物神经元如何在动物大脑中协同工作，使用**命题逻辑**进行复杂计算。这是第一个人工神经网络架构。从那时起，还发明了许多其他架构，你将在下面看到。
- en: The early successes of ANNs led to the widespread belief that we would soon
    be conversing with truly intelligent machines. When it became clear in the 1960s
    that this promise would go unfulfilled (at least for quite a while), funding flew
    elsewhere, and ANNs entered a long winter. In the early 1980s, new architectures
    were invented and better training techniques were developed, sparking a revival
    of interest in *connectionism*, the study of neural networks. But progress was
    slow, and by the 1990s other powerful machine learning techniques had been invented,
    such as support vector machines. These techniques seemed to offer better results
    and stronger theoretical foundations than ANNs, so once again the study of neural
    networks was put on hold.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs的早期成功导致了一种广泛的认识，即我们很快就能与真正智能的机器进行对话。当在20世纪60年代变得明显这一承诺将无法实现（至少在相当长一段时间内）时，资金流向了其他地方，ANNs进入了一个漫长的冬天。在20世纪80年代初，发明了新的架构，并开发了更好的训练技术，这激发了人们对*连接主义*（神经网络的研究）的兴趣。但进展缓慢，到20世纪90年代，已经发明了其他强大的机器学习技术，如支持向量机。这些技术似乎比ANNs提供了更好的结果和更强的理论基础，因此，再次将神经网络的研究搁置起来。
- en: 'We are now witnessing yet another wave of interest in ANNs. Will this wave
    die out like the previous ones did? Well, here are a few good reasons to believe
    that this time is different and that the renewed interest in ANNs will have a
    much more profound impact on our lives:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正在见证对ANNs的又一波兴趣。这波浪潮会像之前的那样消亡吗？好吧，这里有几个很好的理由相信这次情况不同，并且对ANNs的新一轮兴趣将对我们的生活产生更深远的影响：
- en: There is now a huge quantity of data available to train neural networks, and
    ANNs frequently outperform other ML techniques on very large and complex problems.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在有大量的数据可用于训练神经网络，并且ANNs在非常大型和复杂的问题上经常优于其他ML技术。
- en: 'The tremendous increase in computing power since the 1990s now makes it possible
    to train large neural networks in a reasonable amount of time. This is in part
    due to Moore’s law (the number of components in integrated circuits has doubled
    about every 2 years over the last 50 years), but also thanks to the gaming industry,
    which has stimulated the production of powerful *graphical processing units* (GPUs)
    by the millions: GPU cards were initially designed to accelerate graphics, but
    it turns out that neural networks perform similar computations (such as large
    matrix multiplications), so they can also be accelerated using GPUs. Moreover,
    cloud platforms have made this power accessible to everyone.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自1990年代以来计算能力的巨大提升，现在使得在合理的时间内训练大型神经网络成为可能。这部分得益于摩尔定律（在过去50年中，集成电路中的组件数量大约每两年翻一番），同时也归功于游戏行业，它通过数百万台强大的*图形处理单元*（GPUs）刺激了生产：GPU卡最初是为了加速图形而设计的，但结果证明神经网络执行类似的计算（如大型矩阵乘法），因此它们也可以通过GPU加速。此外，云平台使得这种能力对每个人都可以访问。
- en: The training algorithms have been improved. To be fair they are only slightly
    different from the ones used in the 1990s, but these relatively small tweaks have
    had a huge positive impact.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练算法已经得到了改进。公平地说，它们与20世纪90年代使用的算法只有细微的差别，但这些相对小的调整产生了巨大的积极影响。
- en: 'Some theoretical limitations of ANNs have turned out to be benign in practice.
    For example, many people thought that ANN training algorithms were doomed because
    they were likely to get stuck in local optima, but it turns out that this is not
    a big problem in practice, especially for larger neural networks: the local optima
    often perform almost as well as the global optimum.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANNs的一些理论局限性在实践中证明是良性的。例如，许多人认为ANN训练算法注定要失败，因为它们很可能会陷入局部最优，但实际情况并非如此，尤其是在较大的神经网络中：局部最优解通常表现几乎与全局最优解相当。
- en: 'The invention of the Transformer architecture in 2017 (see [Chapter 15](ch15.html#transformer_chapter))
    has been a game changer: it can process and generate all sorts of data (e.g.,
    text, images, audio) unlike earlier, more specialized, architectures, and it performs
    great across a wide variety of tasks from robotics to protein folding. Moreover,
    it scales rather well, which has made it possible to train very large *foundation
    models* that can be reused across many different tasks, possibly with a bit of
    fine-tuning (that’s transfer learning), or just by prompting the model in the
    right way (that’s *in-context learning*, or ICL). For instance, you can give it
    a few examples of the task at hand (that’s *few-shot learning*, or FSL), or ask
    it to reason step-by-step (that’s *chain-of-thought* prompting, or CoT). It’s
    a new world!'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2017年Transformer架构的发明（见[第15章](ch15.html#transformer_chapter)）是一个转折点：它可以处理和生成各种数据（例如，文本、图像、音频），这与早期更专业化的架构不同，并且它在从机器人学到蛋白质折叠的广泛任务上表现出色。此外，它的扩展性相当好，这使得训练非常大的*基础模型*成为可能，这些模型可以在许多不同的任务中重复使用，可能只需要一点微调（这就是迁移学习），或者只需以正确的方式提示模型（这就是*情境学习*，或ICL）。例如，你可以给它一些当前任务的示例（这就是*少样本学习*，或FSL），或者要求它逐步推理（这就是*思维链*提示，或CoT）。这是一个全新的世界！
- en: 'ANNs seem to have entered a virtuous circle of funding and progress. Amazing
    products based on ANNs regularly make the headline news, which pulls more and
    more attention and funding toward them, resulting in more and more progress and
    even more amazing products. AI is no longer just powering products in the shadows:
    since chatbots such as ChatGPT were released, the general public is now directly
    interacting daily with AI assistants, and the big tech companies are competing
    fiercely to grab this gigantic market: the pace of innovation is wild.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络似乎进入了一个良性的资金和进步循环。基于人工神经网络惊人的产品经常成为头条新闻，这吸引了越来越多的关注和资金，从而带来了更多的进步和更加惊人的产品。人工智能不再是仅仅在阴影中为产品提供动力：自从ChatGPT等聊天机器人发布以来，公众现在每天都在直接与人工智能助手互动，大型科技公司正在激烈竞争以抢占这个巨大的市场：创新的速度是疯狂的。
- en: Biological Neurons
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生物神经元
- en: Before we discuss artificial neurons, let’s take a quick look at a biological
    neuron (represented in [Figure 9-1](#biological_neuron_wikipedia)). It is an unusual-looking
    cell mostly found in animal brains. It’s composed of a *cell body* containing
    the nucleus and most of the cell’s complex components, many branching extensions
    called *dendrites*, plus one very long extension called the *axon*. The axon’s
    length may be just a few times longer than the cell body, or up to tens of thousands
    of times longer. Near its extremity the axon splits off into many branches called
    *telodendria*, and at the tip of these branches are minuscule structures called
    *synaptic terminals* (or simply *synapses*), which are connected to the dendrites
    or cell bodies of other neurons.⁠^([3](ch09.html#id2106)) Biological neurons produce
    short electrical impulses called *action potentials* (APs, or just *signals*),
    which travel along the axons and make the synapses release chemical signals called
    *neurotransmitters*. When a neuron receives a sufficient amount of these neurotransmitters
    within a few milliseconds, it fires its own electrical impulses (actually, it
    depends on the neurotransmitters, as some of them inhibit the neuron from firing).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论人工神经元之前，让我们快速看一下生物神经元（如图[9-1](#biological_neuron_wikipedia)所示）。它是一种外形不寻常的细胞，主要存在于动物大脑中。它由一个包含细胞核和细胞大部分复杂成分的*细胞体*组成，许多分支称为*树突*，以及一个非常长的分支称为*轴突*。轴突的长度可能只是细胞体的几倍长，或者长达数万倍。在其末端，轴突分裂成许多称为*突触末梢*的分支，这些分支的尖端是微小的结构，称为*突触终端*（或简称*突触*），它们连接到其他神经元的树突或细胞体。⁠^([3](ch09.html#id2106))
    生物神经元产生短暂的电脉冲，称为*动作电位*（APs，或简称*信号*），这些信号沿着轴突传播，并使突触释放称为*神经递质*的化学信号。当一个神经元在几毫秒内接收到足够的这些神经递质时，它会发出自己的电脉冲（实际上，这取决于神经递质，因为其中一些会抑制神经元放电）。
- en: '![Illustration of a biological neuron highlighting key components such as the
    cell body, dendrites, axon, telodendria, and synaptic terminals, demonstrating
    the neuron''s structure and connection points within neural networks.](assets/hmls_0901.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![生物神经元的插图，突出显示细胞体、树突、轴突、突触末梢等关键组件，展示神经元在神经网络中的结构和连接点。](assets/hmls_0901.png)'
- en: Figure 9-1\. A biological neuron⁠^([4](ch09.html#id2110))
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1. 一个生物神经元⁠^([4](ch09.html#id2110))
- en: Thus, individual biological neurons seem to behave in a simple way, but they’re
    organized in a vast network of billions, with each neuron typically connected
    to thousands of other neurons. Highly complex computations can be performed by
    a network of fairly simple neurons, much like a complex anthill can emerge from
    the combined efforts of simple ants. The architecture of biological neural networks
    (BNNs)⁠^([5](ch09.html#id2111)) is the subject of active research, but some parts
    of the brain have been mapped. These efforts show that neurons are often organized
    in consecutive layers, especially in the cerebral cortex (the outer layer of the
    brain), as shown in [Figure 9-2](#biological_neural_network_wikipedia).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单个生物神经元似乎以简单的方式行事，但它们被组织在一个包含数十亿个神经元的庞大网络中，每个神经元通常连接到成千上万的其它神经元。一个由相对简单的神经元组成的网络可以执行高度复杂的计算，就像一个复杂的蚁群可以从简单蚂蚁的共同努力中产生一样。生物神经网络（BNNs）的架构是活跃研究的话题，但大脑的一些部分已经被绘制出来。这些努力表明，神经元通常以连续的层次组织，特别是在大脑皮层（大脑的外层），如图[图9-2](#biological_neural_network_wikipedia)所示。
- en: '![Illustration of layered neuron networks in the human cerebral cortex, emphasizing
    the complex organization of biological neural networks.](assets/hmls_0902.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![展示人类大脑皮层中分层神经元网络，强调生物神经网络的复杂组织。](assets/hmls_0902.png)'
- en: Figure 9-2\. Multiple layers in a biological neural network (human cortex)⁠^([6](ch09.html#id2115))
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. 生物神经网络（人类皮层）中的多层⁠^([6](ch09.html#id2115))
- en: Logical Computations with Neurons
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元的逻辑计算
- en: 'McCulloch and Pitts proposed a very simple model of the biological neuron,
    which later became known as an *artificial neuron*: it has one or more binary
    (on/off) inputs and one binary output. The artificial neuron activates its output
    when more than a certain number of its inputs are active. In their paper, McCulloch
    and Pitts showed that even with such a simplified model it is possible to build
    a network of artificial neurons that can compute any logical proposition you want.
    To see how such a network works, let’s build a few ANNs that perform various logical
    computations (see [Figure 9-3](#nn_propositional_logic_diagram)), assuming that
    a neuron is activated when at least two of its input connections are active.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 麦克洛奇和皮茨提出了一种非常简单的生物神经元模型，后来被称为**人工神经元**：它有一个或多个二进制（开/关）输入和一个二进制输出。当其输入中超过一定数量的输入活跃时，人工神经元会激活其输出。在他们的论文中，麦克洛奇和皮茨表明，即使使用这样一个简化的模型，也可以构建一个由人工神经元组成的网络，可以计算任何你想要的逻辑命题。为了了解这样一个网络是如何工作的，让我们构建几个执行各种逻辑计算的ANNs（人工神经网络），假设一个神经元在其至少两个输入连接活跃时被激活。
- en: '![Diagram showing artificial neural networks performing logical computations,
    including AND, OR, and NOT operations with neurons labeled A, B, and C.](assets/hmls_0903.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![展示人工神经网络执行逻辑计算，包括与、或和非操作的图，神经元标记为A、B和C。](assets/hmls_0903.png)'
- en: Figure 9-3\. ANNs performing simple logical computations
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. 执行简单逻辑计算的ANNs
- en: 'Let’s see what these networks do:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些网络是如何工作的：
- en: 'The first network on the left is the identity function: if neuron A is activated,
    then neuron C gets activated as well (since it receives two input signals from
    neuron A); but if neuron A is off, then neuron C is off as well.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧的第一个网络是恒等函数：如果神经元A被激活，那么神经元C也会被激活（因为它从神经元A那里接收了两个输入信号）；但如果神经元A关闭，那么神经元C也会关闭。
- en: 'The second network performs a logical AND: neuron C is activated only when
    both neurons A and B are activated (a single input signal is not enough to activate
    neuron C).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个网络执行逻辑与操作：只有当神经元A和神经元B都活跃时，神经元C才会被激活（单个输入信号不足以激活神经元C）。
- en: 'The third network performs a logical OR: neuron C gets activated if either
    neuron A or neuron B is activated (or both).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个网络执行逻辑或操作：如果神经元A或神经元B（或两者）被激活，则神经元C会被激活。
- en: 'Finally, if we suppose that an input connection can inhibit the neuron’s activity
    (which is the case with biological neurons), then the fourth network computes
    a slightly more complex logical proposition: neuron C is activated only if neuron
    A is active and neuron B is off. If neuron A is active all the time, then you
    get a logical NOT: neuron C is active when neuron B is off, and vice versa.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，如果我们假设一个输入连接可以抑制神经元的活性（这在生物神经元中是这种情况），那么第四个网络计算了一个稍微复杂一些的逻辑命题：只有当神经元A活跃且神经元B关闭时，神经元C才会被激活。如果神经元A始终活跃，那么你得到一个逻辑非：当神经元B关闭时，神经元C活跃，反之亦然。
- en: You can imagine how these networks can be combined to compute complex logical
    expressions (see the exercises at the end of the chapter for an example).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象这些网络如何组合起来计算复杂的逻辑表达式（参见本章末尾的练习以获取示例）。
- en: The Perceptron
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器
- en: 'The *perceptron* is one of the simplest ANN architectures, invented in 1957
    by Frank Rosenblatt. It is based on a slightly different artificial neuron (see
    [Figure 9-4](#artificial_neuron_diagram)) called a *threshold logic unit* (TLU),
    or sometimes a *linear threshold unit* (LTU). The inputs and output are numbers
    (instead of binary on/off values), and each input connection is associated with
    a weight. The TLU first computes a linear function of its inputs: *z* = *w*[1]
    *x*[1] + *w*[2] *x*[2] + ⋯ + *w*[*n*] *x*[*n*] + *b* = **w**^⊺ **x** + *b*. Then
    it applies a *step function* to the result: *h*[**w**](**x**) = step(*z*). So
    it’s almost like logistic regression, except it uses a step function instead of
    the logistic function.⁠^([7](ch09.html#id2122)) Just like in logistic regression,
    the model parameters are the input weights **w** and the bias term *b*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*感知器*是ANN架构中最简单的一种，由Frank Rosenblatt于1957年发明。它基于一种略微不同的称为*阈值逻辑单元*（TLU）或有时称为*线性阈值单元*（LTU）的人工神经元（参见[图9-4](#artificial_neuron_diagram)）。输入和输出都是数字（而不是二进制的开/关值），每个输入连接都与一个权重相关联。TLU首先计算其输入的线性函数：*z*
    = *w*[1] *x*[1] + *w*[2] *x*[2] + ⋯ + *w*[*n*] *x*[*n*] + *b* = **w**^⊺ **x**
    + *b*。然后它将阶跃函数应用于结果：*h*[**w**](**x**) = step(*z*)。所以它几乎就像逻辑回归，只是它使用的是阶跃函数而不是逻辑函数。⁠^([7](ch09.html#id2122))
    就像在逻辑回归中一样，模型参数是输入权重**w**和偏置项*b*。'
- en: '![Diagram of a Threshold Logic Unit (TLU) showing how inputs multiplied by
    weights are summed with a bias, and a step function is applied to determine the
    output.](assets/hmls_0904.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![阈值逻辑单元（TLU）的图示，展示了输入乘以权重后与偏置相加，然后应用阶跃函数以确定输出。](assets/hmls_0904.png)'
- en: 'Figure 9-4\. TLU: an artificial neuron that computes a weighted sum of its
    inputs **w**^⊺ **x**, plus a bias term *b*, then applies a step function'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4\. TLU：一种计算其输入**w**^⊺ **x**加权和加上偏置项*b*的人工神经元，然后应用阶跃函数
- en: The most common step function used in perceptrons is the *Heaviside step function*
    (see [Equation 9-1](#step_functions_equation)). Sometimes the sign function is
    used instead.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知器中最常用的阶跃函数是*海维塞德阶跃函数*（参见[方程式9-1](#step_functions_equation)）。有时会使用符号函数代替。
- en: Equation 9-1\. Common step functions used in perceptrons (assuming threshold
    = 0)
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式9-1\. 感知器中常用的阶跃函数（假设阈值为0）
- en: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo form="prefix">heaviside</mo>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd>
    <mtd columnalign="left"><mrow><mo form="prefix">sgn</mo> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>+</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo form="prefix">heaviside</mo>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd>
    <mtd columnalign="left"><mrow><mo form="prefix">sgn</mo> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>+</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable>
- en: A single TLU can be used for simple linear binary classification. It computes
    a linear function of its inputs, and if the result exceeds a threshold, it outputs
    the positive class. Otherwise, it outputs the negative class. This may remind
    you of logistic regression ([Chapter 4](ch04.html#linear_models_chapter)) or linear
    SVM classification (see the online chapter on SVMs at [*https://homl.info*](https://homl.info)).
    You could, for example, use a single TLU to classify iris flowers based on petal
    length and width. Training such a TLU would require finding the right values for
    *w*[1], *w*[2], and *b* (the training algorithm is discussed shortly).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 单个TLU可用于简单的线性二进制分类。它计算其输入的线性函数，如果结果超过阈值，则输出正类。否则，输出负类。这可能会让你想起逻辑回归 ([第4章](ch04.html#linear_models_chapter))
    或线性SVM分类（请参阅关于SVM的在线章节[*https://homl.info*](https://homl.info)）。例如，你可以使用单个TLU根据花瓣长度和宽度对鸢尾花进行分类。训练这样的TLU需要找到
    *w*[1]、*w*[2] 和 *b*（训练算法将在稍后讨论）的正确值。
- en: A perceptron is composed of one or more TLUs organized in a single layer, where
    every TLU is connected to every input. Such a layer is called a *fully connected
    layer*, or a *dense layer*. The inputs constitute the *input layer*. And since
    the layer of TLUs produces the final outputs, it is called the *output layer*.
    For example, a perceptron with two inputs and three outputs is represented in
    [Figure 9-5](#perceptron_diagram).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个感知器由一个或多个组织在单层中的TLU组成，其中每个TLU都与每个输入相连。这样的层被称为 *全连接层* 或 *密集层*。输入构成 *输入层*。由于TLU层产生最终输出，因此它被称为
    *输出层*。例如，具有两个输入和三个输出的感知器在 [图9-5](#perceptron_diagram) 中表示。
- en: '![Diagram of a perceptron architecture with two input neurons connected to
    three output neurons in a fully connected layer, illustrating TLUs in the output
    layer.](assets/hmls_0905.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![具有两个输入神经元连接到全连接层中的三个输出神经元的感知器架构图，展示了输出层中的TLU。](assets/hmls_0905.png)'
- en: Figure 9-5\. Architecture of a perceptron with two inputs and three output neurons
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. 具有两个输入和三个输出神经元的感知器架构
- en: This perceptron can classify instances simultaneously into three different binary
    classes, which makes it a multilabel classifier. It may also be used for multiclass
    classification.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个感知器可以将实例同时分类到三个不同的二进制类别，这使得它成为一个多标签分类器。它也可以用于多类分类。
- en: Thanks to the magic of linear algebra, [Equation 9-2](#neural_network_layer_equation)
    can be used to efficiently compute the outputs of a layer of artificial neurons
    for several instances at once.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了线性代数的魔力，[方程9-2](#neural_network_layer_equation) 可以用来有效地同时计算多个实例的人工神经元层的输出。
- en: Equation 9-2\. Computing the outputs of a fully connected layer
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 9-2\. 计算全连接层的输出
- en: $ModifyingAbove bold upper Y With caret equals phi left-parenthesis bold upper
    X bold upper W plus bold b right-parenthesis$
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $ModifyingAbove 粗体上标 Y With 叉等于 phi 左括号 粗体上标 X 粗体上标 W 加 粗体 b 右括号$
- en: 'In this equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: $ModifyingAbove bold upper Y With caret$ is the output matrix. It has one row
    per instance and one column per neuron.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $ModifyingAbove 粗体上标 Y With 叉$ 是输出矩阵。它每行对应一个实例，每列对应一个神经元。
- en: '**X** is the input matrix. It has one row per instance and one column per input
    feature.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X** 是输入矩阵。它每行对应一个实例，每列对应一个输入特征。'
- en: The weight matrix **W** contains all the connection weights. It has one row
    per input feature and one column per neuron.⁠^([8](ch09.html#id2128))
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵 **W** 包含所有连接权重。它每行对应一个输入特征，每列对应一个神经元。⁠^([8](ch09.html#id2128))
- en: 'The bias vector **b** contains all the bias terms: one per neuron.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置向量 **b** 包含所有偏置项：每个神经元一个。
- en: 'The function *ϕ* is called the *activation function*: when the artificial neurons
    are TLUs, it is a step function (we will discuss other activation functions shortly).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 *ϕ* 被称为 *激活函数*：当人工神经元是TLU时，它是一个阶跃函数（我们将在稍后讨论其他激活函数）。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In mathematics, the sum of a matrix and a vector is undefined. However, in
    data science, we allow “broadcasting”: adding a vector to a matrix means adding
    it to every row in the matrix. So, **XW** + **b** first multiplies **X** by **W**—which
    results in a matrix with one row per instance and one column per output—then adds
    the vector **b** to every row of that matrix, which adds each bias term to the
    corresponding output, for every instance. Moreover, *ϕ* is then applied itemwise
    to each item in the resulting matrix.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，矩阵和向量的和是未定义的。然而，在数据科学中，我们允许“广播”：将一个向量加到一个矩阵上意味着将它加到矩阵的每一行。因此，**XW** + **b**
    首先将 **X** 乘以 **W**——这会得到一个矩阵，其中每一行代表一个实例，每一列代表一个输出——然后将向量 **b** 加到该矩阵的每一行，这样就将每个偏置项加到了每个实例的对应输出上。此外，*ϕ*
    然后逐项应用于结果矩阵中的每个元素。
- en: So, how is a perceptron trained? The perceptron training algorithm proposed
    by Rosenblatt was largely inspired by *Hebb’s rule*. In his 1949 book, *The Organization
    of Behavior* (Wiley), Donald Hebb suggested that when a biological neuron triggers
    another neuron often, the connection between these two neurons grows stronger.
    Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire
    together, wire together”; that is, the connection weight between two neurons tends
    to increase when they fire simultaneously. This rule later became known as Hebb’s
    rule (or *Hebbian learning*). Perceptrons are trained using a variant of this
    rule that takes into account the error made by the network when it makes a prediction;
    the perceptron learning rule reinforces connections that help reduce the error.
    More specifically, the perceptron is fed one training instance at a time, and
    for each instance it makes its predictions. For every output neuron that produced
    a wrong prediction, it reinforces the connection weights from the inputs that
    would have contributed to the correct prediction. The rule is shown in [Equation
    9-3](#perceptron_update_rule).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，感知器是如何训练的呢？Rosenblatt 提出的感知器训练算法在很大程度上受到了 *Hebb 的规则* 的启发。在他的 1949 年著作《行为组织》（Wiley）中，Donald
    Hebb 建议，当一个生物神经元经常触发另一个神经元时，这两个神经元之间的连接会变得更强。Siegrid Löwel 后来用吸引人的短语总结了 Hebb 的想法，“Cells
    that fire together, wire together”；也就是说，当两个神经元同时触发时，它们之间的连接权重往往会增加。这个规则后来被称为 Hebb
    的规则（或 *Hebbian learning*）。感知器使用这个规则的变体进行训练，该变体考虑了网络在做出预测时犯的错误；感知器学习规则加强了有助于减少错误的连接。更具体地说，感知器一次被喂给一个训练实例，并为每个实例做出预测。对于每个产生错误预测的输出神经元，它加强了那些本应有助于正确预测的输入的连接权重。该规则在
    [方程 9-3](#perceptron_update_rule) 中显示。
- en: Equation 9-3\. Perceptron learning rule (weight update)
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 9-3\. 感知器学习规则（权重更新）
- en: $w Subscript i comma j Baseline Superscript left-parenthesis next step right-parenthesis
    Baseline equals w Subscript i comma j Baseline plus eta left-parenthesis y Subscript
    j Baseline minus ModifyingAbove y With caret Subscript j Baseline right-parenthesis
    x Subscript i$
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: $w Subscript i comma j Baseline Superscript left-parenthesis next step right-parenthesis
    Baseline equals w Subscript i comma j Baseline plus eta left-parenthesis y Subscript
    j Baseline minus ModifyingAbove y With caret Subscript j Baseline right-parenthesis
    x Subscript i$
- en: 'In this equation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*w*[*i*,] [*j*] is the connection weight between the *i*^(th) input and the
    *j*^(th) neuron.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[*i*,] [*j*] 是第 *i* 个输入和第 *j* 个神经元之间的连接权重。'
- en: '*x*[*i*] is the *i*^(th) input value of the current training instance.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[*i*] 是当前训练实例的第 *i* 个输入值。'
- en: $ModifyingAbove y With caret Subscript j$ is the output of the *j*^(th) output
    neuron for the current training instance.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $通过上标 caret 标记的 y Subscript j Baseline 对 y 进行修改$ 是当前训练实例的第 *j* 个输出神经元的输出。
- en: '*y*[*j*] is the target output of the *j*^(th) output neuron for the current
    training instance.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[*j*] 是当前训练实例的第 *j* 个输出神经元的期望输出。'
- en: '*η* is the learning rate (see [Chapter 4](ch04.html#linear_models_chapter)).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*η* 是学习率（参见[第4章](ch04.html#linear_models_chapter)）。'
- en: The decision boundary of each output neuron is linear, so perceptrons are incapable
    of learning complex patterns (just like logistic regression classifiers). However,
    if the training instances are linearly separable, Rosenblatt demonstrated that
    this algorithm will converge to a solution.⁠^([9](ch09.html#id2132)) This is called
    the *perceptron convergence theorem*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出神经元的决策边界是线性的，因此感知器无法学习复杂的模式（就像逻辑回归分类器一样）。然而，如果训练实例是线性可分的，Rosenblatt 证明了该算法将收敛到解决方案。⁠^([9](ch09.html#id2132))
    这被称为 *感知器收敛定理*。
- en: 'Scikit-Learn provides a `Perceptron` class that can be used pretty much as
    you would expect—for example, on the iris dataset (introduced in [Chapter 4](ch04.html#linear_models_chapter)):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一个`Perceptron`类，可以像预期的那样使用——例如，在鸢尾花数据集（在第4章中介绍）上：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You may have noticed that the perceptron learning algorithm strongly resembles
    stochastic gradient descent (introduced in [Chapter 4](ch04.html#linear_models_chapter)).
    In fact, Scikit-Learn’s `Perceptron` class is equivalent to using an `SGDClassifier`
    with the following hyperparameters: `loss="perceptron"`, `learning_rate="constant"`,
    `eta0=1` (the learning rate), and `penalty=None` (no regularization).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，感知器学习算法与随机梯度下降（在第4章中介绍）非常相似。事实上，Scikit-Learn的`Perceptron`类相当于使用具有以下超参数的`SGDClassifier`：`loss="perceptron"`，`learning_rate="constant"`，`eta0=1`（学习率），和`penalty=None`（无正则化）。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Contrary to logistic regression classifiers, perceptrons do not output a class
    probability. This is one reason to prefer logistic regression over perceptrons.
    Moreover, perceptrons do not use any regularization by default, and training stops
    as soon as there are no more prediction errors on the training set, so the model
    typically does not generalize as well as logistic regression or a linear SVM classifier.
    However, perceptrons may train a bit faster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归分类器不同，感知器不会输出一个类概率。这是选择逻辑回归而不是感知器的一个原因。此外，感知器默认不使用任何正则化，并且训练会在训练集上没有更多预测错误时停止，因此该模型通常不如逻辑回归或线性SVM分类器泛化得好。然而，感知器的训练可能更快一些。
- en: In their 1969 monograph, *Perceptrons*, Marvin Minsky and Seymour Papert highlighted
    a number of serious weaknesses of perceptrons—in particular, the fact that they
    are incapable of solving some trivial problems (e.g., the *exclusive OR* (XOR)
    classification problem; see the left side of [Figure 9-6](#xor_diagram)). This
    is true of any other linear classification model (such as logistic regression
    classifiers), but researchers had expected much more from perceptrons, and some
    were so disappointed that they dropped neural networks altogether in favor of
    more formal approaches such as logic, problem solving, and search. The lack of
    practical applications also didn’t help.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的1969年专著《感知器》中，Marvin Minsky 和 Seymour Papert 强调了感知器的一些严重弱点——特别是它们无法解决一些简单问题的事实（例如，*异或*（XOR）分类问题；参见[图9-6](#xor_diagram)的左侧）。这同样适用于任何其他线性分类模型（如逻辑回归分类器），但研究人员原本对感知器寄予厚望，有些人因此感到非常失望，以至于完全放弃了神经网络，转而采用更正式的方法，如逻辑、问题解决和搜索。缺乏实际应用也没有帮助。
- en: It turns out that some of the limitations of perceptrons can be eliminated by
    stacking multiple perceptrons. The resulting ANN is called a *multilayer perceptron*
    (MLP).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，通过堆叠多个感知器可以消除感知器的一些局限性。由此产生的ANN被称为*多层感知器*（MLP）。
- en: The Multilayer Perceptron and Backpropagation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层感知器和反向传播
- en: 'An MLP can solve the XOR problem, as you can verify by computing the output
    of the MLP represented on the righthand side of [Figure 9-6](#xor_diagram): with
    inputs (0, 0) or (1, 1), the network outputs 0, and with inputs (0, 1) or (1,
    0) it outputs 1\. Try verifying that this network indeed solves the XOR problem!^([10](ch09.html#id2139))'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个MLP可以解决XOR问题，你可以通过计算[图9-6](#xor_diagram)右侧表示的MLP的输出来验证这一点：当输入为(0, 0)或(1, 1)时，网络输出0，而当输入为(0,
    1)或(1, 0)时，它输出1。尝试验证这个网络确实解决了XOR问题！^([10](ch09.html#id2139))
- en: An MLP is composed of one input layer, one or more layers of artificial neurons
    (originally TLUs) called *hidden layers*, and one final layer of artificial neurons
    called the *output layer* (see [Figure 9-7](#mlp_diagram)). The layers close to
    the input layer are usually called the *lower layers*, and the ones close to the
    outputs are usually called the *upper layers*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: MLP由一个输入层、一个或多个称为*隐藏层*的人工神经元层（最初是TLU）和一个称为*输出层*的最终人工神经元层组成（参见[图9-7](#mlp_diagram)）。靠近输入层的层通常被称为*下层*，而靠近输出的层通常被称为*上层*。
- en: '![Diagram illustrating the XOR classification problem and an MLP using threshold
    logic units to solve it.](assets/hmls_0906.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![展示XOR分类问题和使用阈值逻辑单元解决它的MLP的图](assets/hmls_0906.png)'
- en: Figure 9-6\. XOR classification problem and an MLP that solves it
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6. XOR分类问题和解决它的MLP
- en: '![Diagram of a multilayer perceptron showing a feedforward neural network with
    two input neurons, a hidden layer of four neurons, and three output neurons.](assets/hmls_0907.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器图，展示了具有两个输入神经元、一个包含四个神经元的隐藏层和三个输出神经元的前馈神经网络。](assets/hmls_0907.png)'
- en: Figure 9-7\. Architecture of a multilayer perceptron with two inputs, one hidden
    layer of four neurons, and three output neurons
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7. 具有两个输入、一个包含四个神经元的隐藏层和三个输出神经元的多层感知器架构
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The signal flows only in one direction (from the inputs to the outputs), so
    this architecture is an example of a *feedforward neural network* (FNN).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 信号只沿一个方向流动（从输入到输出），因此这种架构是**前馈神经网络**（FNN）的一个例子。
- en: When an ANN contains a deep stack of hidden layers,⁠^([11](ch09.html#id2146))
    it is called a *deep neural network* (DNN). The field of deep learning studies
    DNNs, and more generally it is interested in models containing deep stacks of
    computations. Even so, many people talk about deep learning whenever neural networks
    are involved (even shallow ones).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个人工神经网络包含深层堆叠的隐藏层时⁠^([11](ch09.html#id2146))，它被称为**深度神经网络**（DNN）。深度学习领域研究DNN，更广泛地说，它对包含深层计算堆叠的模型感兴趣。即便如此，每当涉及到神经网络（即使是浅层神经网络）时，许多人都会谈论深度学习。
- en: For many years researchers struggled to find a way to train MLPs, without success.
    In the early 1960s several researchers discussed the possibility of using gradient
    descent to train neural networks, but as we saw in [Chapter 4](ch04.html#linear_models_chapter),
    this requires computing the gradients of the model’s error with regard to the
    model parameters; it wasn’t clear at the time how to do this efficiently with
    such a complex model containing so many parameters, especially with the computers
    they had back then.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，研究人员一直在努力寻找一种训练MLP的方法，但都没有成功。在20世纪60年代初，几位研究人员讨论了使用梯度下降来训练神经网络的可行性，但正如我们在[第4章](ch04.html#linear_models_chapter)中看到的，这需要计算模型误差相对于模型参数的梯度；在当时，如何以有效的方式计算这样一个包含许多参数的复杂模型的梯度并不清楚，尤其是在他们那时的计算机条件下。
- en: Then, in 1970, a researcher named Seppo Linnainmaa introduced in his master’s
    thesis a technique to compute all the gradients automatically and efficiently.
    This algorithm is now called *reverse-mode automatic differentiation* (or *reverse-mode
    autodiff* for short). In just two passes through the network (one forward, one
    backward), it is able to compute the gradients of the neural network’s error with
    regard to every single model parameter. In other words, it can find out how each
    connection weight and each bias should be tweaked in order to reduce the neural
    network’s error. These gradients can then be used to perform a gradient descent
    step. If you repeat this process of computing the gradients automatically and
    taking a gradient descent step, the neural network’s error will gradually drop
    until it eventually reaches a minimum. This combination of reverse-mode autodiff
    and gradient descent is now called *backpropagation* (or *backprop* for short).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在1970年，一位名叫Seppo Linnainmaa的研究者在他的硕士论文中介绍了一种自动且高效地计算所有梯度的技术。这个算法现在被称为**反向模式自动微分**（或简称**反向自动微分**）。通过网络中的两次遍历（一次正向，一次反向），它能够计算神经网络误差相对于每个模型参数的梯度。换句话说，它可以找出每个连接权重和每个偏置应该如何调整，以减少神经网络的误差。这些梯度可以用来执行梯度下降步骤。如果你重复这个过程，自动计算梯度并执行梯度下降步骤，神经网络的误差将逐渐降低，直到最终达到最小值。这种反向自动微分和梯度下降的组合现在被称为**反向传播**（或简称**反向传播**）。
- en: 'Here’s an analogy: imagine you are learning to shoot a basketball into the
    hoop. You throw the ball (that’s the forward pass), and you observe that it went
    far off to the right side (that’s the error computation), then you consider how
    you can change your body position to throw the ball a bit less to the right next
    time (that’s the backward pass): you realize that your arm will need to rotate
    a bit counterclockwise, and probably your whole upper body as well, which in turn
    means that your feet should turn too (notice how we’re going down the “layers”).
    Once you’ve thought it through, you actually move your body: that’s the gradient
    descent step. The smaller the errors, the smaller the adjustments. As you repeat
    the whole process many times, the error gradually gets smaller, and after a few
    hours of practice, you manage to get the ball through the hoop every time. Good
    job!'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个类比：想象你正在学习投篮进篮筐。你投掷篮球（这就是前向传递），然后观察到篮球偏离了右侧很远（这就是错误计算），然后你考虑如何改变你的身体位置，以便下次投掷时篮球向右偏移得少一些（这就是反向传递）：你意识到你的手臂需要稍微逆时针旋转，可能整个上半身也需要旋转，这反过来又意味着你的脚也需要转动（注意我们是如何沿着“层”向下走的）。一旦你想通了，你实际上就会移动你的身体：这就是梯度下降步骤。错误越小，调整越小。随着你重复整个过程多次，错误逐渐减小，经过几小时的练习，你每次都能成功将篮球投进篮筐。做得好！
- en: Note
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are various autodiff techniques, with different pros and cons. Reverse-mode
    autodiff is well suited when the function to differentiate has many variables
    (e.g., connection weights and biases) and few outputs (e.g., one loss). If you
    want to learn more about autodiff, check out [Appendix A](app01.html#autodiff_appendix).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种自动微分技术，各有优缺点。反向模式自动微分非常适合当要微分的函数有多个变量（例如，连接权重和偏差）而输出较少（例如，一个损失）时。如果你想了解更多关于自动微分的信息，请参阅[附录A](app01.html#autodiff_appendix)。
- en: 'Backpropagation can actually be applied to all sorts of computational graphs,
    not just neural networks: indeed, Linnainmaa’s master’s thesis was not about neural
    nets at all, it was more general. It was several more years before backprop started
    to be used to train neural networks, but it still wasn’t mainstream. Then, in
    1985, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a [paper](https://homl.info/44)⁠^([12](ch09.html#id2152))
    analyzing how backpropagation allows neural networks to learn useful internal
    representations. Their results were so impressive that backpropagation was quickly
    popularized in the field. Over 40 years later, it is still by far the most popular
    training technique for neural networks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播实际上可以应用于各种计算图，而不仅仅是神经网络：确实，林纳伊马亚的硕士论文根本不是关于神经网络的，它更加通用。在几年之后，反向传播才开始被用来训练神经网络，但那时它还不是主流。然后，在1985年，大卫·鲁梅尔哈特、杰弗里·辛顿和罗纳德·威廉姆斯发表了一篇[论文](https://homl.info/44)⁠^([12](ch09.html#id2152))，分析了反向传播如何使神经网络学习有用的内部表示。他们的结果非常令人印象深刻，反向传播很快就在该领域得到了普及。40多年后，它仍然是神经网络最受欢迎的训练技术。
- en: 'Let’s run through how backpropagation works again in a bit more detail:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次更详细地回顾一下反向传播的工作原理：
- en: It handles one mini-batch at a time, and goes through the full training set
    multiple times. If each mini-batch contains 32 instances, and each instance has
    100 features, then the mini-batch will be represented as a matrix with 32 rows
    and 100 columns. Each pass through the training set is called an *epoch*.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它一次处理一个迷你批次，并多次遍历整个训练集。如果每个迷你批次包含32个实例，每个实例有100个特征，那么迷你批次将表示为一个有32行和100列的矩阵。每次遍历训练集被称为一个*epoch*。
- en: 'For each mini-batch, the algorithm computes the output of all the neurons in
    the first hidden layer using [Equation 9-2](#neural_network_layer_equation). If
    the layer has 50 neurons, then its output is a matrix with one row per sample
    in the mini-batch (e.g., 32), and 50 columns (i.e., one per neuron). This matrix
    is then passed on to the next layer, its output is computed and passed to the
    next layer, and so on until we get the output of the last layer, the output layer.
    This is the *forward pass*: it is exactly like making predictions, except all
    intermediate results are preserved since they are needed for the backward pass.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个迷你批次，算法使用[方程式9-2](#neural_network_layer_equation)计算第一隐藏层中所有神经元的输出。如果该层有50个神经元，那么它的输出是一个矩阵，每行对应迷你批次中的每个样本（例如，32行），每列对应一个神经元（50列）。然后这个矩阵被传递到下一层，其输出被计算并传递到下一层，依此类推，直到我们得到最后一层的输出，即输出层。这是*前向传递*：它就像做出预测一样，只不过所有中间结果都被保留下来，因为它们对于反向传递是必需的。
- en: Next, the algorithm measures the network’s output error (i.e., it uses a loss
    function that compares the desired output and the actual output of the network,
    and returns some measure of the error).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，算法测量网络的输出错误（即，它使用一个损失函数来比较期望的输出和网络的实际输出，并返回一些错误度量）。
- en: Then it computes how much each output layer parameter contributed to the error.
    This is done analytically by applying the *chain rule* (one of the most fundamental
    rules in calculus), which makes this step fast and precise. The result is one
    gradient per parameter.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它计算每个输出层参数对错误的贡献有多大。这是通过应用*链式法则*（微积分中最基本的规则之一）来分析的，这使得这一步骤既快又准确。结果是每个参数一个梯度。
- en: The algorithm then measures how much of these error contributions came from
    each connection in the layer below, again using the chain rule, working backward
    until it reaches the input layer. As explained earlier, this reverse pass efficiently
    measures the error gradient across all the connection weights and biases in the
    network by propagating the error gradient backward through the network (hence
    the name of the algorithm).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法随后测量这些错误贡献中有多少来自下一层的每个连接，再次使用链式法则，反向工作直到达到输入层。如前所述，这种反向传递通过在网络中传播错误梯度来有效地测量网络中所有连接权重和偏差的错误梯度，因此得名该算法。
- en: Finally, the algorithm performs a gradient descent step to tweak all the connection
    weights and bias terms in the network, using the error gradients it just computed.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，算法执行梯度下降步骤，使用它刚刚计算的错误梯度来调整网络中的所有连接权重和偏差项。
- en: Warning
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'It is important to initialize all the hidden layers’ connection weights randomly,
    or else training will fail. For example, if you initialize all weights and biases
    to zero, then all neurons in a given layer will be perfectly identical, and thus
    backpropagation will affect them in exactly the same way, so they will remain
    identical. In other words, despite having hundreds of neurons per layer, your
    model will act as if it had only one neuron per layer: it won’t be too smart.
    If instead you randomly initialize the weights, you *break the symmetry* and allow
    backpropagation to train a diverse team of neurons.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有隐藏层的连接权重是随机的重要，否则训练将失败。例如，如果你将所有权重和偏差初始化为零，那么给定层的所有神经元将完全相同，因此反向传播将以完全相同的方式影响它们，因此它们将保持相同。换句话说，尽管每个层有数百个神经元，但你的模型将表现得好像每个层只有一个神经元：它不会太聪明。如果你随机初始化权重，你将*打破对称性*，并允许反向传播训练一个多样化的神经元团队。
- en: In short, backpropagation makes predictions for a mini-batch (forward pass),
    measures the error, then goes through each layer in reverse to measure the error
    contribution from each parameter (reverse pass), and finally tweaks the connection
    weights and biases to reduce the error (gradient descent step).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，反向传播为迷你批次进行预测（正向传递），测量错误，然后反向通过每一层来测量每个参数的错误贡献（反向传递），最后调整连接权重和偏差以减少错误（梯度下降步骤）。
- en: 'In order for backprop to work properly, Rumelhart and his colleagues made a
    key change to the MLP’s architecture: they replaced the step function with the
    logistic function, *σ*(*z*) = 1 / (1 + exp(–*z*)), also called the *sigmoid* function.
    This was essential because the step function contains only flat segments, so there
    is no gradient to work with (gradient descent cannot move on a flat surface),
    while the sigmoid function has a well-defined nonzero derivative everywhere, allowing
    gradient descent to make some progress at every step. In fact, the backpropagation
    algorithm works well with many other activation functions, not just the sigmoid
    function. Here are two other popular choices:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使反向传播正常工作，Rumelhart和他的同事们对MLP的架构进行了关键性的改变：他们将步函数替换为对数函数，*σ*(*z*) = 1 / (1
    + exp(–*z*))，也称为*sigmoid*函数。这是至关重要的，因为步函数只包含平坦的段，因此没有梯度可以工作（梯度下降无法在平坦表面上移动），而sigmoid函数在所有地方都有一个定义良好的非零导数，允许梯度下降在每一步都取得一些进展。事实上，反向传播算法与许多其他激活函数一起工作得很好，而不仅仅是sigmoid函数。这里还有两种其他流行的选择：
- en: 'The *hyperbolic tangent* function: tanh(*z*) = 2*σ*(2*z*) – 1'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数：tanh(*z*) = 2*σ*(2*z*) – 1
- en: Just like the sigmoid function, this activation function is *S*-shaped, continuous,
    and differentiable, but its output value ranges from –1 to 1 (instead of 0 to
    1 in the case of the sigmoid function). That range tends to make each layer’s
    output more or less centered around 0 at the beginning of training, which often
    helps speed up convergence.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Sigmoid函数一样，这个激活函数是*S*-形的，连续且可导，但其输出值范围从-1到1（与Sigmoid函数的0到1不同）。这个范围往往使得每个层的输出在训练开始时或多或少地围绕0中心，这通常有助于加快收敛速度。
- en: 'The rectified linear unit function: ReLU(*z*) = max(0, *z*)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 矩形线性单元函数：ReLU(*z*) = max(0, *z*)
- en: The ReLU function is continuous but unfortunately not differentiable at *z*
    = 0 (the slope changes abruptly, which can make gradient descent bounce around),
    and its derivative is 0 for *z* < 0\. In practice, however, it works very well
    and has the advantage of being fast to compute, so it has become the default for
    most architectures (except the Transformer architecture, as we will see in [Chapter 15](ch15.html#transformer_chapter)).⁠^([13](ch09.html#id2162))
    Importantly, the fact that it does not have a maximum output value helps reduce
    some issues during gradient descent (we will come back to this in [Chapter 11](ch11.html#deep_chapter)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数在*z* = 0处连续但不幸的是不可导（斜率突然改变，这可能导致梯度下降在周围弹跳），对于*z* < 0时其导数为0。然而，在实践中，它工作得非常好，并且具有计算速度快的优势，因此已成为大多数架构的默认选择（除了我们将要在第15章中看到的Transformer架构）。^([13](ch09.html#id2162))
    重要的是，它没有最大输出值的事实有助于在梯度下降过程中减少一些问题（我们将在第11章中回到这个问题）。
- en: 'These popular activation functions and their derivatives are represented in
    [Figure 9-8](#activation_functions_plot). But wait! Why do we need activation
    functions in the first place? Well, if you chain several linear transformations,
    all you get is a linear transformation. For example, if f(*x*) = 2*x* + 3 and
    g(*x*) = 5*x* – 1, then chaining these two linear functions gives you another
    linear function: f(g(*x*)) = 2(5*x* – 1) + 3 = 10*x* + 1\. So if you don’t have
    some nonlinearity between layers, then even a deep stack of layers is equivalent
    to a single layer, and you can’t solve very complex problems with that. Conversely,
    a large enough DNN with nonlinear activations can theoretically approximate any
    continuous function.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些流行的激活函数及其导数在[图9-8](#activation_functions_plot)中表示。但是等等！我们为什么需要激活函数呢？好吧，如果你链式连接几个线性变换，你得到的就是一个线性变换。例如，如果f(*x*)
    = 2*x* + 3和g(*x*) = 5*x* – 1，那么链式这两个线性函数给你另一个线性函数：f(g(*x*)) = 2(5*x* – 1) + 3
    = 10*x* + 1。所以如果你在层之间没有非线性，那么即使是一堆深的层也相当于一层，你无法用这种方法解决非常复杂的问题。相反，具有足够大的非线性激活的DNN可以从理论上逼近任何连续函数。
- en: '![Diagram illustrating four activation functions—Heaviside, ReLU, Sigmoid,
    and Tanh—alongside their respective derivatives, highlighting nonlinearity essential
    for deep neural networks.](assets/hmls_0908.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![说明四个激活函数——Heaviside、ReLU、Sigmoid和Tanh——及其各自导数的图，突出显示深度神经网络中必需的非线性。](assets/hmls_0908.png)'
- en: Figure 9-8\. Activation functions (left) and their derivatives (right)
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 激活函数（左）及其导数（右）
- en: OK! You know where neural nets came from, what the MLP architecture looks like,
    and how it computes its outputs. You’ve also learned about the backpropagation
    algorithm. It’s time to see MLPs in action!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧！你已经知道了神经网络是从哪里来的，MLP架构看起来像什么，以及它是如何计算其输出的。你还学习了反向传播算法。现在是时候看看MLP的实际应用了！
- en: Building and Training MLPs with Scikit-Learn
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn构建和训练MLP
- en: MLPs can tackle a wide range of tasks, but the most common are regression and
    classification. Scikit-Learn can help with both of these. Let’s start with regression.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: MLP可以处理各种任务，但最常见的是回归和分类。Scikit-Learn可以帮助处理这两者。让我们从回归开始。
- en: Regression MLPs
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归MLP
- en: 'How would you build an MLP for a regression task? Well, if you want to predict
    a single value (e.g., the price of a house, given many of its features), then
    you just need a single output neuron: its output is the predicted value. For multivariate
    regression (i.e., to predict multiple values at once), you need one output neuron
    per output dimension. For example, to locate the center of an object in an image,
    you need to predict 2D coordinates, so you need two output neurons. If you also
    want to place a bounding box around the object, then you need two more numbers:
    the width and the height of the object. So, you end up with four output neurons.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何构建一个用于回归任务的MLP？好吧，如果你想预测一个单一值（例如，给定许多特征的房子价格），那么你只需要一个输出神经元：其输出是预测值。对于多元回归（即一次性预测多个值），你需要每个输出维度一个输出神经元。例如，为了定位图像中物体的中心，你需要预测2D坐标，因此你需要两个输出神经元。如果你还想在物体周围放置一个边界框，那么你需要两个额外的数字：物体的宽度和高度。所以，你最终会有四个输出神经元。
- en: 'Scikit-Learn includes an `MLPRegressor` class, so let’s use it to build an
    MLP with three hidden layers composed of 50 neurons each, and train it on the
    California housing dataset. For simplicity, we will use Scikit-Learn’s `fetch_california_housing()`
    function to load the data. This dataset is simpler than the one we used in [Chapter 2](ch02.html#project_chapter),
    since it contains only numerical features (there is no `ocean_proximity` feature),
    and there are no missing values. The targets are also scaled down: each unit represents
    $100,000\. Let’s start by importing everything we will need:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn包含一个`MLPRegressor`类，所以让我们使用它来构建一个由50个神经元组成的三个隐藏层的MLP，并在加利福尼亚住房数据集上对其进行训练。为了简单起见，我们将使用Scikit-Learn的`fetch_california_housing()`函数来加载数据。这个数据集比我们在[第2章](ch02.html#project_chapter)中使用的数据集简单，因为它只包含数值特征（没有`ocean_proximity`特征），并且没有缺失值。目标值也被缩小了：每个单位代表$100,000。让我们首先导入我们将需要的一切：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let’s fetch the California housing dataset and split it into a training
    set and a test set:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们获取加利福尼亚住房数据集并将其分为训练集和测试集：
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let’s create an `MLPRegressor` model with 3 hidden layers composed of 50
    neurons each. The first hidden layer’s input size (i.e., the number of rows in
    its weights matrix) and the output layer’s output size (i.e., the number of columns
    in its weights matrix) will adjust automatically to the dimensionality of the
    inputs and targets, respectively, when training starts. The model uses the ReLU
    activation function in all hidden layers, and no activation function at all on
    the output layer. We also set `verbose=True` to get details on the model’s progress
    during training:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个由50个神经元组成的3个隐藏层的`MLPRegressor`模型。当训练开始时，第一个隐藏层的输入大小（即其权重矩阵中的行数）和输出层的大小（即其权重矩阵中的列数）将自动调整到输入和目标的维度。该模型在所有隐藏层中使用ReLU激活函数，在输出层上则没有任何激活函数。我们还设置了`verbose=True`以获取训练过程中模型进度的详细信息：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since neural nets can have a *lot* of parameters, they have a tendency to overfit
    the training set. To reduce this risk, one option is to use early stopping (introduced
    in [Chapter 4](ch04.html#linear_models_chapter)): when we set `early_stopping=True`,
    the `MLPRegressor` class automatically sets aside 10% of the training data and
    uses it to evaluate the model at each epoch (you can adjust the validation set’s
    size by setting `validation_fraction`). If the validation score stops improving
    for 10 epochs, training automatically stops (you can tweak this number of epochs
    by setting `n_iter_no_change`).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络可以有大量的参数，它们有过度拟合训练集的倾向。为了减少这种风险，一个选项是使用提前停止（在第4章中介绍）：当我们设置`early_stopping=True`时，`MLPRegressor`类会自动保留10%的训练数据，并在每个epoch评估模型（你可以通过设置`validation_fraction`调整验证集的大小）。如果验证分数在10个epoch内不再提高，训练将自动停止（你可以通过设置`n_iter_no_change`调整这个epoch数）。
- en: 'Now let’s create a pipeline to standardize the input features before sending
    them to the `MLPRegressor`. This is very important because gradient descent does
    not converge very well when the features have very different scales, as we saw
    in [Chapter 4](ch04.html#linear_models_chapter). We can then train the model!
    The `MLPRegressor` class uses a variant of gradient descent called *Adam* (see
    [Chapter 11](ch11.html#deep_chapter)) to minimize the mean squared error. It also
    uses a tiny bit of ℓ[2] regularization (you can control its strength via the `alpha`
    hyperparameter, which defaults to 0.0001):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个管道来标准化输入特征，在将它们发送到`MLPRegressor`之前。这非常重要，因为当特征具有非常不同的尺度时，梯度下降法收敛得不是很好，正如我们在[第4章](ch04.html#linear_models_chapter)中看到的。然后我们可以训练模型！`MLPRegressor`类使用一种称为*Adam*的梯度下降变体（见[第11章](ch11.html#deep_chapter)）来最小化均方误差。它还使用一点ℓ[2]正则化（你可以通过`alpha`超参数来控制其强度，默认值为0.0001）：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]` [PRE6] And there you go, you just trained your very first MLP! It required
    45 epochs, and as you can see, the training loss went down at each epoch. This
    loss corresponds to [Equation 4-9](ch04.html#ridge_cost_function) divided by 2,
    so you must multiply it by 2 to get the MSE (although not exactly because the
    loss includes the ℓ[2] regularization term). The validation score generally went
    up at each epoch. Like every regressor in Scikit-Learn, `MLPRegressor` uses the
    R² score by default for evaluation—that’s what the `score()` method returns. As
    we saw in [Chapter 2](ch02.html#project_chapter), the R² score measures the ratio
    of the variance that is explained by the model. In this case, it reaches close
    to 80% on the validation set, which is fairly good for this task:    [PRE7]py   [PRE8]`py
    [PRE9]py[PRE10][PRE11][PRE12] from sklearn.datasets import fetch_openml  fashion_mnist
    = fetch_openml(name="Fashion-MNIST", as_frame=False) targets = fashion_mnist.target.astype(int)
    [PRE13] X_train, y_train = fashion_mnist.data[:60_000], targets[:60_000] X_test,
    y_test = fashion_mnist.data[60_000:], targets[60_000:] [PRE14] import matplotlib.pyplot
    as plt  X_sample = X_train[0].reshape(28, 28)  # first image in the training set
    plt.imshow(X_sample, cmap="binary") plt.show() [PRE15] class_names = ["T-shirt/top",
    "Trouser", "Pullover", "Dress", "Coat",                "Sandal", "Shirt", "Sneaker",
    "Bag", "Ankle boot"] [PRE16] >>> class_names[y_train[0]] `''Ankle boot''` [PRE17][PRE18]
    from sklearn.neural_network import MLPClassifier from sklearn.preprocessing import
    MinMaxScaler  mlp_clf = MLPClassifier(hidden_layer_sizes=[300, 100], verbose=True,                         early_stopping=True,
    random_state=42) pipeline = make_pipeline(MinMaxScaler(), mlp_clf) pipeline.fit(X_train,
    y_train) accuracy = pipeline.score(X_test, y_test) [PRE19] >>> X_new = X_test[:15]  #
    let''s pretend these are 15 new images `>>>` `mlp_clf``.``predict``(``X_new``)`
    `` `array([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 8, 3, 4])` `` [PRE20]`` [PRE21]
    >>> y_proba = mlp_clf.predict_proba(X_new) `>>>` `y_proba``[``12``]` `` `array([0.,
    0., 0., 0., 0., 0., 0., 0., 1., 0.])` `` [PRE22] ``Hmm, that’s not great: the
    model is telling us that it’s 100% confident that the image represents a bag (index
    8). So not only is the model wrong, it’s 100% confident that it’s right. In fact,
    across all 10,000 images in the test set, there are only 16 images that the model
    is less than 99.9% confident about, despite the fact that its accuracy is about
    90%. That’s why you should always treat estimated probabilities with a grain of
    salt: neural nets have a strong tendency to be overconfident, especially if they
    are trained for a bit too long.    ###### Tip    The targets for classification
    tasks can be class indices (e.g., 3) or class probabilities, typically one-hot
    vectors (e.g., [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]). But if your model tends to be
    overconfident, you can try the *label smoothing* technique:⁠^([14](ch09.html#id2192))
    reduce the target class’s probability slightly (e.g., from 1 down to 0.9) and
    distribute the rest evenly across the other classes (e.g., [0.1/9, 0.1/9, 0.1/9,
    0.9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9]).    Still, getting 90% accuracy
    on Fashion MNIST is pretty good. You could get even better performance by fine-tuning
    the hyperparameters, for example using `RandomizedSearchCV`, as we did in [Chapter 2](ch02.html#project_chapter).
    However, the search space is quite large, so it helps to know roughly where to
    look.`` [PRE23]` [PRE24][PRE25][PRE26][PRE27][PRE28]  [PRE29]'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE5]` [PRE6]就这样，你已经训练出了你的第一个MLP！这需要45个epoch，正如你所见，每个epoch的训练损失都在下降。这个损失对应于[方程式4-9](ch04.html#ridge_cost_function)除以2，所以你必须乘以2来得到MSE（尽管并不完全准确，因为损失包括ℓ[2]正则化项）。验证分数通常在每个epoch都会上升。像Scikit-Learn中的每个回归器一样，`MLPRegressor`默认使用R²分数进行评估——这就是`score()`方法返回的内容。正如我们在[第二章](ch02.html#project_chapter)中看到的，R²分数衡量的是模型解释的方差比率。在这种情况下，它在验证集上接近80%，对于这个任务来说相当不错：    [PRE7]py   [PRE8]`py
    [PRE9]py[PRE10][PRE11][PRE12] 从sklearn.datasets导入fetch_openml  fashion_mnist =
    fetch_openml(name="Fashion-MNIST", as_frame=False) targets = fashion_mnist.target.astype(int)
    [PRE13] X_train, y_train = fashion_mnist.data[:60_000], targets[:60_000] X_test,
    y_test = fashion_mnist.data[60_000:], targets[60_000:] [PRE14] 导入matplotlib.pyplot
    as plt  X_sample = X_train[0].reshape(28, 28)  # 训练集中的第一张图片 plt.imshow(X_sample,
    cmap="binary") plt.show() [PRE15] class_names = ["T-shirt/top", "Trouser", "Pullover",
    "Dress", "Coat",                "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
    [PRE16] >>> class_names[y_train[0]] `''Ankle boot''` [PRE17][PRE18] 从sklearn.neural_network导入MLPClassifier
    从sklearn.preprocessing导入MinMaxScaler  mlp_clf = MLPClassifier(hidden_layer_sizes=[300,
    100], verbose=True,                         early_stopping=True, random_state=42)
    pipeline = make_pipeline(MinMaxScaler(), mlp_clf) pipeline.fit(X_train, y_train)
    accuracy = pipeline.score(X_test, y_test) [PRE19] >>> X_new = X_test[:15]  # 假设这些是15张新图像
    `>>>` `mlp_clf``.``predict``(``X_new``)` `` `array([9, 2, 1, 1, 6, 1, 4, 6, 5,
    7, 4, 5, 8, 3, 4])` `` [PRE20]`` [PRE21] >>> y_proba = mlp_clf.predict_proba(X_new)
    `>>>` `y_proba``[``12``]` `` `array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])`
    `` [PRE22] ``嗯，这不太好：模型告诉我们它100%确信图像代表一个包（索引8）。所以不仅模型是错误的，它还100%确信它是正确的。实际上，在测试集的10,000张图像中，只有16张图像模型对其信心不足于99.9%，尽管其准确率约为90%。这就是为什么你应该总是带着怀疑的态度对待估计概率：神经网络有很强的过度自信倾向，尤其是如果训练时间过长的话。    ######
    小贴士    分类任务的标签可以是类别索引（例如，3）或类别概率，通常是one-hot向量（例如，[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]）。但如果你的模型倾向于过度自信，你可以尝试*标签平滑*技术：⁠^([14](ch09.html#id2192))稍微降低目标类别的概率（例如，从1降到0.9），并将剩余的均匀分配到其他类别（例如，[0.1/9,
    0.1/9, 0.1/9, 0.9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9, 0.1/9]）。    然而，在Fashion
    MNIST上获得90%的准确率已经相当不错了。你可以通过微调超参数来获得更好的性能，例如使用`RandomizedSearchCV`，就像我们在[第二章](ch02.html#project_chapter)中所做的那样。然而，搜索空间相当大，所以知道大致的查找位置是有帮助的。``
    [PRE23]` [PRE24][PRE25][PRE26][PRE27][PRE28]  [PRE29]'
