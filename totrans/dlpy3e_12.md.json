["```py\nimport keras\nimport keras_hub\n\nimages_path = keras.utils.get_file(\n    \"coco\",\n    \"http://images.cocodataset.org/zips/train2017.zip\",\n    extract=True,\n)\nannotations_path = keras.utils.get_file(\n    \"annotations\",\n    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n    extract=True,\n) \n```", "```py\nimport json\n\nwith open(f\"{annotations_path}/annotations/instances_train2017.json\", \"r\") as f:\n    annotations = json.load(f)\n\n# Sorts image metadata by ID\nimages = {image[\"id\"]: image for image in annotations[\"images\"]}\n\n# Converts bounding box to coordinates on a unit square\ndef scale_box(box, width, height):\n    scale = 1.0 / max(width, height)\n    x, y, w, h = [v * scale for v in box]\n    x += (height - width) * scale / 2 if height > width else 0\n    y += (width - height) * scale / 2 if width > height else 0\n    return [x, y, w, h]\n\n# Aggregates all bounding box annotations by image ID\nmetadata = {}\nfor annotation in annotations[\"annotations\"]:\n    id = annotation[\"image_id\"]\n    if id not in metadata:\n        metadata[id] = {\"boxes\": [], \"labels\": []}\n    image = images[id]\n    box = scale_box(annotation[\"bbox\"], image[\"width\"], image[\"height\"])\n    metadata[id][\"boxes\"].append(box)\n    metadata[id][\"labels\"].append(annotation[\"category_id\"])\n    metadata[id][\"path\"] = images_path + \"/train2017/\" + image[\"file_name\"]\nmetadata = list(metadata.values()) \n```", "```py\n>>> len(metadata)\n117266\n>>> min([len(x[\"boxes\"]) for x in metadata])\n1\n>>> max([len(x[\"boxes\"]) for x in metadata])\n63\n>>> max(max(x[\"labels\"]) for x in metadata) + 1\n91\n>>> metadata[435]\n{\"boxes\": [[0.12, 0.27, 0.57, 0.33],\n  [0.0, 0.15, 0.79, 0.69],\n  [0.0, 0.12, 1.0, 0.75]],\n \"labels\": [17, 15, 2],\n \"path\": \"/root/.keras/datasets/coco/train2017/000000171809.jpg\"}\n>>> [keras_hub.utils.coco_id_to_name(x) for x in metadata[435][\"labels\"]]\n[\"cat\", \"bench\", \"bicycle\"]\n```", "```py\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import hsv_to_rgb\nfrom matplotlib.patches import Rectangle\n\ncolor_map = {0: \"gray\"}\n\ndef label_to_color(label):\n    # Uses the golden ratio to generate new hues of a bright color with\n    # the HSV colorspace\n    if label not in color_map:\n        h, s, v = (len(color_map) * 0.618) % 1, 0.5, 0.9\n        color_map[label] = hsv_to_rgb((h, s, v))\n    return color_map[label]\n\ndef draw_box(ax, box, text, color):\n    x, y, w, h = box\n    ax.add_patch(Rectangle((x, y), w, h, lw=2, ec=color, fc=\"none\"))\n    textbox = dict(fc=color, pad=1, ec=\"none\")\n    ax.text(x, y, text, c=\"white\", size=10, va=\"bottom\", bbox=textbox)\n\ndef draw_image(ax, image):\n    # Draws the image on a unit cube with (0, 0) at the top left\n    ax.set(xlim=(0, 1), ylim=(1, 0), xticks=[], yticks=[], aspect=\"equal\")\n    image = plt.imread(image)\n    height, width = image.shape[:2]\n    # Pads the image so it fits inside the unit cube\n    hpad = (1 - height / width) / 2 if width > height else 0\n    wpad = (1 - width / height) / 2 if height > width else 0\n    extent = [wpad, 1 - wpad, 1 - hpad, hpad]\n    ax.imshow(image, extent=extent) \n```", "```py\nsample = metadata[435]\nig, ax = plt.subplots(dpi=300)\ndraw_image(ax, sample[\"path\"])\nfor box, label in zip(sample[\"boxes\"], sample[\"labels\"]):\n    label_name = keras_hub.utils.coco_id_to_name(label)\n    draw_box(ax, box, label_name, label_to_color(label))\nplt.show() \n```", "```py\nimport random\n\nmetadata = list(filter(lambda x: len(x[\"boxes\"]) <= 4, metadata))\nrandom.shuffle(metadata) \n```", "```py\nimage_size = 448\n\nbackbone = keras_hub.models.Backbone.from_preset(\n    \"resnet_50_imagenet\",\n)\npreprocessor = keras_hub.layers.ImageConverter.from_preset(\n    \"resnet_50_imagenet\",\n    image_size=(image_size, image_size),\n) \n```", "```py\nfrom keras import layers\n\ngrid_size = 6\nnum_labels = 91\n\ninputs = keras.Input(shape=(image_size, image_size, 3))\nx = backbone(inputs)\n# Makes our backbone outputs smaller and then flattens the output\n# features\nx = layers.Conv2D(512, (3, 3), strides=(2, 2))(x)\nx = keras.layers.Flatten()(x)\n# Passes our flattened feature maps through two densely connected\n# layers\nx = layers.Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(grid_size * grid_size * (num_labels + 5))(x)\n# Reshapes outputs to a 6 × 6 grid\nx = layers.Reshape((grid_size, grid_size, num_labels + 5))(x)\n# Split box and class predictions\nbox_predictions = x[..., :5]\nclass_predictions = layers.Activation(\"softmax\")(x[..., 5:])\noutputs = {\"box\": box_predictions, \"class\": class_predictions}\nmodel = keras.Model(inputs, outputs) \n```", "```py\n>>> model.summary()\nModel: \"functional_3\"\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_7         │ (None, 448, 448,  │           0 │ -                  │\n│ (InputLayer)          │ 3)                │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ res_net_backbone_12   │ (None, 14, 14,    │  23,580,512 │ input_layer_7[0][… │\n│ (ResNetBackbone)      │ 2048)             │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ conv2d_3 (Conv2D)     │ (None, 6, 6, 512) │   9,437,696 │ res_net_backbone_… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ flatten_3 (Flatten)   │ (None, 18432)     │           0 │ conv2d_3[0][0]     │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_6 (Dense)       │ (None, 2048)      │  37,750,784 │ flatten_3[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_3 (Dropout)   │ (None, 2048)      │           0 │ dense_6[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_7 (Dense)       │ (None, 3456)      │   7,081,344 │ dropout_3[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ reshape_3 (Reshape)   │ (None, 6, 6, 96)  │           0 │ dense_7[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ get_item_7 (GetItem)  │ (None, 6, 6, 91)  │           0 │ reshape_3[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ get_item_6 (GetItem)  │ (None, 6, 6, 5)   │           0 │ reshape_3[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ activation_33         │ (None, 6, 6, 91)  │           0 │ get_item_7[0][0]   │\n│ (Activation)          │                   │             │                    │\n└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n Total params: 77,850,336 (296.98 MB)\n Trainable params: 77,797,088 (296.77 MB)\n Non-trainable params: 53,248 (208.00 KB)\n```", "```py\ndef to_grid(box):\n    x, y, w, h = box\n    cx, cy = (x + w / 2) * grid_size, (y + h / 2) * grid_size\n    ix, iy = int(cx), int(cy)\n    return (ix, iy), (cx - ix, cy - iy, w, h)\n\ndef from_grid(loc, box):\n    (xi, yi), (x, y, w, h) = loc, box\n    x = (xi + x) / grid_size - w / 2\n    y = (yi + y) / grid_size - h / 2\n    return (x, y, w, h) \n```", "```py\nimport numpy as np\nimport math\n\nclass_array = np.zeros((len(metadata), grid_size, grid_size))\nbox_array = np.zeros((len(metadata), grid_size, grid_size, 5))\n\nfor index, sample in enumerate(metadata):\n    boxes, labels = sample[\"boxes\"], sample[\"labels\"]\n    for box, label in zip(boxes, labels):\n        (x, y, w, h) = box\n        # Finds all grid cells whose center falls inside the box\n        left, right = math.floor(x * grid_size), math.ceil((x + w) * grid_size)\n        bottom, top = math.floor(y * grid_size), math.ceil((y + h) * grid_size)\n        class_array[index, bottom:top, left:right] = label\n\nfor index, sample in enumerate(metadata):\n    boxes, labels = sample[\"boxes\"], sample[\"labels\"]\n    for box, label in zip(boxes, labels):\n        # Transforms the box to the grid coordinate system\n        (xi, yi), (grid_box) = to_grid(box)\n        box_array[index, yi, xi] = [*grid_box, 1.0]\n        # Makes sure the class label for the box's center location\n        # matches the box\n        class_array[index, yi, xi] = label \n```", "```py\ndef draw_prediction(image, boxes, classes, cutoff=None):\n    fig, ax = plt.subplots(dpi=300)\n    draw_image(ax, image)\n    # Draws the YOLO output grid and class probability map\n    for yi, row in enumerate(classes):\n        for xi, label in enumerate(row):\n            color = label_to_color(label) if label else \"none\"\n            x, y, w, h = (v / grid_size for v in (xi, yi, 1.0, 1.0))\n            r = Rectangle((x, y), w, h, lw=2, ec=\"black\", fc=color, alpha=0.5)\n            ax.add_patch(r)\n    # Draws all boxes at each grid location above our cutoff\n    for yi, row in enumerate(boxes):\n        for xi, box in enumerate(row):\n            box, confidence = box[:4], box[4]\n            if not cutoff or confidence >= cutoff:\n                box = from_grid((xi, yi), box)\n                label = classes[yi, xi]\n                color = label_to_color(label)\n                name = keras_hub.utils.coco_id_to_name(label)\n                draw_box(ax, box, f\"{name} {max(confidence, 0):.2f}\", color)\n    plt.show()\n\ndraw_prediction(metadata[0][\"path\"], box_array[0], class_array[0], cutoff=1.0) \n```", "```py\nimport tensorflow as tf\n\n# Loads and resizes the model with tf.data\ndef load_image(path):\n    x = tf.io.read_file(path)\n    x = tf.image.decode_jpeg(x, channels=3)\n    return preprocessor(x)\n\nimages = tf.data.Dataset.from_tensor_slices([x[\"path\"] for x in metadata])\nimages = images.map(load_image, num_parallel_calls=8)\nlabels = {\"box\": box_array, \"class\": class_array}\nlabels = tf.data.Dataset.from_tensor_slices(labels)\n\n# Creates a merged dataset and batches it\ndataset = tf.data.Dataset.zip(images, labels).batch(16).prefetch(2)\n# Splits off some validation data\nval_dataset, train_dataset = dataset.take(500), dataset.skip(500) \n```", "```py\nfrom keras import ops\n\n# Unpacks a tensor of boxes\ndef unpack(box):\n    return box[..., 0], box[..., 1], box[..., 2], box[..., 3]\n\n# Computes the intersection area between two box tensors\ndef intersection(box1, box2):\n    cx1, cy1, w1, h1 = unpack(box1)\n    cx2, cy2, w2, h2 = unpack(box2)\n    left = ops.maximum(cx1 - w1 / 2, cx2 - w2 / 2)\n    bottom = ops.maximum(cy1 - h1 / 2, cy2 - h2 / 2)\n    right = ops.minimum(cx1 + w1 / 2, cx2 + w2 / 2)\n    top = ops.minimum(cy1 + h1 / 2, cy2 + h2 / 2)\n    return ops.maximum(0.0, right - left) * ops.maximum(0.0, top - bottom)\n\n# Computes the IoU between two box tensors\ndef intersection_over_union(box1, box2):\n    cx1, cy1, w1, h1 = unpack(box1)\n    cx2, cy2, w2, h2 = unpack(box2)\n    intersection_area = intersection(box1, box2)\n    a1 = ops.maximum(w1, 0.0) * ops.maximum(h1, 0.0)\n    a2 = ops.maximum(w2, 0.0) * ops.maximum(h2, 0.0)\n    union_area = a1 + a2 - intersection_area\n    return ops.divide_no_nan(intersection_area, union_area) \n```", "```py\ndef signed_sqrt(x):\n    return ops.sign(x) * ops.sqrt(ops.absolute(x) + keras.config.epsilon())\n\ndef box_loss(true, pred):\n    # Unpacks values\n    xy_true, wh_true, conf_true = true[..., :2], true[..., 2:4], true[..., 4:]\n    xy_pred, wh_pred, conf_pred = pred[..., :2], pred[..., 2:4], pred[..., 4:]\n    # If confidence_true is 0.0, there is no object in this grid cell.\n    no_object = conf_true == 0.0\n    # Computes box placement errors\n    xy_error = ops.square(xy_true - xy_pred)\n    wh_error = ops.square(signed_sqrt(wh_true) - signed_sqrt(wh_pred))\n    # Computes confidence error\n    iou = intersection_over_union(true, pred)\n    conf_target = ops.where(no_object, 0.0, ops.expand_dims(iou, -1))\n    conf_error = ops.square(conf_target - conf_pred)\n    # Concatenates the errors weith scaling hacks\n    error = ops.concatenate(\n        (\n            ops.where(no_object, 0.0, xy_error * 5.0),\n            ops.where(no_object, 0.0, wh_error * 5.0),\n            ops.where(no_object, conf_error * 0.5, conf_error),\n        ),\n        axis=-1,\n    )\n    # Returns one loss value per sample; Keras will sum over the batch.\n    return ops.sum(error, axis=(1, 2, 3)) \n```", "```py\nmodel.compile(\n    optimizer=keras.optimizers.Adam(2e-4),\n    loss={\"box\": box_loss, \"class\": \"sparse_categorical_crossentropy\"},\n)\nmodel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=4,\n) \n```", "```py\n# Rebatches our dataset to get a single sample instead of 16\nx, y = next(iter(val_dataset.rebatch(1)))\npreds = model.predict(x)\nboxes = preds[\"box\"][0]\n# Uses argmax to find the most likely label at each grid location\nclasses = np.argmax(preds[\"class\"][0], axis=-1)\n# Loads the image from disk to view it a full size\npath = metadata[0][\"path\"]\ndraw_prediction(path, boxes, classes, cutoff=0.1) \n```", "```py\ndraw_prediction(path, boxes, classes, cutoff=None) \n```", "```py\nurl = \"https://s3.us-east-1.amazonaws.com/book.keras.io/3e/seurat.jpg\"\npath = keras.utils.get_file(origin=url)\nimage = np.array([keras.utils.load_img(path)]) \n```", "```py\ndetector = keras_hub.models.ObjectDetector.from_preset(\n    \"retinanet_resnet50_fpn_v2_coco\",\n    bounding_box_format=\"rel_xywh\",\n)\npredictions = detector.predict(image) \n```", "```py\n>>> [(k, v.shape) for k, v in predictions.items()]\n[(\"boxes\", (1, 100, 4)),\n (\"confidence\", (1, 100)),\n (\"labels\", (1, 100)),\n (\"num_detections\", (1,))]\n>>> predictions[\"boxes\"][0][0]\narray([0.53, 0.00, 0.81, 0.29], dtype=float32)\n```", "```py\nfig, ax = plt.subplots(dpi=300)\ndraw_image(ax, path)\nnum_detections = predictions[\"num_detections\"][0]\nfor i in range(num_detections):\n    box = predictions[\"boxes\"][0][i]\n    label = predictions[\"labels\"][0][i]\n    label_name = keras_hub.utils.coco_id_to_name(label)\n    draw_box(ax, box, label_name, label_to_color(label))\nplt.show() \n```"]