- en: Chapter 3\. Deep Learning from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may not realize it, but you now have all the mathematical and conceptual
    foundations to answer the key questions about deep learning models that I posed
    at the beginning of the book: you understand *how* neural networks work—the computations
    involved with the matrix multiplications, the loss, and the partial derivatives
    with respect to that loss—as well as *why* those computations work (namely, the
    chain rule from calculus). We achieved this understanding by building neural networks
    from first principles, representing them as a series of “building blocks” where
    each building block was a single mathematical function. In this chapter, you’ll
    learn to represent these building blocks themselves as abstract Python classes
    and then use these classes to build deep learning models; by the end of this chapter,
    you will indeed have done “deep learning from scratch”!'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also map the descriptions of neural networks in terms of these building
    blocks to more conventional descriptions of deep learning models that you may
    have heard before. For example, by the end of this chapter, you’ll know what it
    means for a deep learning model to have “multiple hidden layers.” This is really
    the essence of understanding a concept: being able to translate between high-level
    descriptions and low-level details of what is actually going on. Let’s begin building
    toward this translation. So far, we’ve described models just in terms of the operations
    that happen at a low level. In the first part of this chapter, we’ll map this
    description of models to common higher-level concepts such as “layers” that will
    ultimately allow us to more easily describe more complex models.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning Definition: A First Pass'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What *is* a “deep learning” model? In the previous chapter, we defined a model
    as a mathematical function represented by a computational graph. The purpose of
    such a model was to try to map inputs, each drawn from some dataset with common
    characteristics (such as separate inputs representing different features of houses)
    to outputs drawn from a related distribution (such as the prices of those houses).
    We found that if we defined the model as a function that included *parameters*
    as inputs to some of its operations, we could “fit” it to optimally describe the
    data using the following procedure:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Repeatedly feed observations through the model, keeping track of the quantities
    computed along the way during this “forward pass.”
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate a *loss* representing how far off our model’s predictions were from
    the desired outputs or *target*.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the quantities computed on the forward pass and the chain rule math worked
    out in [Chapter 1](ch01.html#foundations), compute how much each of the input
    *parameters* ultimately affects this loss.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the values of the parameters so that the loss will hopefully be reduced
    when the next set of observations is passed through the model.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We started out with a model containing just a series of linear operations transforming
    our features into the target (which turned out to be equivalent to a traditional
    linear regression model). This had the expected limitation that, even when fit
    “optimally,” the model could nevertheless represent only linear relationships
    between our features and our target.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: We then defined a function structure that applied these linear operations first,
    then a *non*linear operation (the `sigmoid` function), and then a final set of
    linear operations. We showed that with this modification, our model *could* learn
    something closer to the true, nonlinear relationship between input and output,
    while having the additional benefit that it could learn relationships between
    *combinations* of our input features and the target.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the connection between models like these and deep learning models?
    We’ll start with a somewhat clumsy attempt at a definition: deep learning models
    are represented by series of operations that have *at least two, nonconsecutive*
    nonlinear functions involved.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: I’ll show where this definition comes from shortly, but first note that since
    deep learning models are just a series of operations, the process of training
    them is in fact *identical* to the process we’ve been using for the simpler models
    we’ve already seen. After all, what allows this training process to work is the
    differentiability of the model with respect to its inputs; and as mentioned in
    [Chapter 1](ch01.html#foundations), the composition of differentiable functions
    is differentiable, so as long as the individual operations making up the function
    are differentiable, the whole function will be differentiable, and we’ll be able
    to train it using the same four-step training procedure just described.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: However, so far our approach to actually training these models has been to compute
    these derivatives by manually coding the forward and backward passes and then
    multiplying the appropriate quantities together to get the derivatives. For the
    simple neural network model in [Chapter 2](ch02.html#fundamentals), this required
    17 steps. Because we’re describing the model at such a low level, it isn’t immediately
    clear how we could add more complexity to this model (or what exactly what that
    would mean) or even make a simple change such as swapping out a different nonlinear
    function for the sigmoid function. To transition to being able to build arbitrarily
    “deep” and otherwise “complex” deep learning models, we’ll have to think about
    where in these 17 steps we can create reusable components, at a higher level than
    individual operations, that we can swap in and out to build different models.
    To guide us in the right direction as far as which abstractions to create, we’ll
    try to map the operations we’ve been using to traditional descriptions of neural
    networks as being made up of “layers,” “neurons,” and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: As our first step, we’ll have to create an abstraction to represent the individual
    operations we’ve been working with so far, instead of continuing to code the same
    matrix multiplication and bias addition over and over again.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'The Building Blocks of Neural Networks: Operations'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Operation` class will represent one of the constituent functions in our
    neural networks. We know that at a high level, based on the way we’ve used such
    functions in our models, it should have `forward` and `backward` methods, each
    of which receives an `ndarray` as an input and outputs an `ndarray`. Some operations,
    such as matrix multiplication, seem to have *another* special kind of input, also
    an `ndarray`: the parameters. In our `Operation` class—or perhaps in another class
    that inherits from it—we should allow for `params` as another instance variable.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Another insight is that there seem to be two types of `Operation`s: some, such
    as the matrix multiplication, return an `ndarray` as output that is a different
    shape than the `ndarray` they received as input; by contrast, some `Operation`s,
    such as the `sigmoid` function, simply apply some function to each element of
    the input `ndarray`. What, then, is the “general rule” about the shapes of the
    `ndarray`s that get passed between our operations? Let’s consider the `ndarray`s
    passed through our `Operation`s: each `Operation` will send outputs forward on
    the forward pass and will receive an “output gradient” on the backward pass, which
    will represent the partial derivative of the loss with respect to every element
    of the `Operation`’s output (computed by the other `Operation`s that make up the
    network). Also on the backward pass, each `Operation` will send an “input gradient”
    backward, representing the partial derivative of the loss with respect to each
    element of the input.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'These facts place a few important restrictions on the workings of our `Operation`s
    that will help us ensure we’re computing the gradients correctly:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The shape of the *output gradient* `ndarray` must match the shape of the *output*.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape of the *input gradient* that the `Operation` sends backward during
    the backward pass must match the shape of the `Operation`’s *input*.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will all be clearer once you see it in a diagram; let’s look at that next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Diagram
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is all summarized in [Figure 3-1](#fig_03-01), for an operation `O` that
    is receiving inputs from an operation `N` and passing outputs on to another operation
    `P`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0301.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. An Operation, with input and output
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 3-2](#fig_03-02) covers the case of an `Operation` with parameters.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0302.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. A ParamOperation, with input and output and parameters
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Code
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With all this, we can write the fundamental building block for our neural network,
    an `Operation`, as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For any individual `Operation` that we define, we’ll have to implement the `_output`
    and `_input_grad` functions, so named because of the quantities they compute.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’re defining base classes like this primarily for pedagogical reasons: it
    is important to have the mental model that *all* `Operation`s you’ll encounter
    throughout deep learning fit this blueprint of sending inputs forward and gradients
    backward, with the shapes of what they receive on the forward pass matching the
    shapes of what they send backward on the backward pass, and vice versa.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll define the specific `Operation`s we’ve used thus far—matrix multiplication
    and so on—later in this chapter. First we’ll define another class that inherits
    from `Operation` that we’ll use specifically for `Operation`s that involve parameters:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Similar to the base `Operation`, an individual `ParamOperation` would have to
    define the `_param_grad` function in addition to the `_output` and `_input_grad`
    functions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now formalized the neural network building blocks we’ve been using
    in our models so far. We could skip ahead and define neural networks directly
    in terms of these `Operation`s, but there is an intermediate class we’ve been
    dancing around for a chapter and a half that we’ll define first: the `Layer`.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'The Building Blocks of Neural Networks: Layers'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In terms of `Operation`s, layers are a series of linear operations followed
    by a nonlinear operation. For example, our neural network from the last chapter
    could be said to have had five total operations: two linear operations—a weight
    multiplication and the addition of a bias term—followed the `sigmoid` function
    and then two more linear operations. In this case, we would say that the first
    three operations, up to and including the nonlinear one, would constitute the
    first layer, and the last two operations would constitute the second layer. In
    addition, we say that the input itself represents a special kind of layer called
    the *input* layer (in terms of numbering the layers, this layer doesn’t count,
    so that we can think of it as the “zeroth” layer). The last layer, similarly,
    is called the *output* layer. The middle layer—the “first one,” according to our
    numbering—also has an important name: it is called a *hidden* layer, since it
    is the only layer whose values we don’t typically see explicitly during the course
    of training.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The output layer is an important exception to this definition of layers, in
    that it does not *have* to have a nonlinear operation applied to it; this is simply
    because we often want the values that come out of this layer to have values between
    negative infinity and infinity (or at least between 0 and infinity), whereas nonlinear
    functions typically “squash down” their input to some subset of that range relevant
    to the particular problem we’re trying to solve (for example, the `sigmoid` function
    squashes down its input to between 0 and 1).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Diagrams
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make the connection explicit, [Figure 3-3](#fig_03-03) shows the diagram
    of the neural network from the prior chapter with the individual operations grouped
    into layers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0303.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. The neural network from the prior chapter with the operations grouped
    into layers
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see that the input represents an “input” layer, the next three operations
    (ending with the `sigmoid` function) represent the next layer, and the last two
    operations represent the last layer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'This is, of course, rather cumbersome. And that’s the point: representing neural
    networks as a series of individual operations, while showing clearly how neural
    networks work and how to train them, is too “low level” for anything more complicated
    than a two-layer neural network. That’s why the more common way to represent neural
    networks is in terms of layers, as shown in [Figure 3-4](#fig_03-04).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0304.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. The neural network from the prior chapter in terms of layers
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Connection to the brain
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, let’s make one last connection between what we’ve seen so far and
    a notion you’ve likely heard before: each layer can be said to have a certain
    number of *neurons* equal to *the dimensionality of the vector that represents
    each observation in the layer’s output*. The neural network from the prior example
    can thus be thought of as having 13 neurons in the input layer, then 13 neurons
    (again) in the hidden layer, and one neuron in the output layer.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Neurons in the brain have the property that they can receive inputs from many
    other neurons and will “fire” and send a signal forward only if the signals they
    receive cumulatively reach a certain “activation energy.” Neurons in the context
    of neural networks have a loosely analogous property: they do indeed send signals
    forward based on their inputs, but the inputs are transformed into outputs simply
    via a nonlinear function. Thus, this nonlinear function is called the *activation
    function*, and the values that come out of it are called the *activations* for
    that layer.^([1](ch03.html#idm45732624417528))'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve defined layers, we can state the more conventional definition
    of deep learning: *deep learning models are neural networks with more than one
    hidden layer.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: We can see that this is equivalent to the earlier definition that was purely
    in terms of `Operation`s, since a layer is just a series of `Operation`s with
    a nonlinear operation at the end.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve defined a base class for our `Operation`s, let’s show how it
    can serve as the fundamental building block of the models we saw in the prior
    chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Building Blocks on Building Blocks
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What specific `Operation`s do we need to implement for the models in the prior
    chapter to work? Based on our experience of implementing that neural network step
    by step, we know there are three kinds:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The matrix multiplication of the input with the matrix of parameters
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The addition of a bias term
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sigmoid` activation function
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start with the `WeightMultiply` `Operation`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here we simply code up the matrix multiplication on the forward pass, as well
    as the rules for “sending gradients backward” to both the inputs and the parameters
    on the backward pass (using the rules for doing so that we reasoned through at
    the end of [Chapter 1](ch01.html#foundations)). As you’ll see shortly, we can
    now use this as a *building block* that we can simply plug into our `Layer`s.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the addition operation, which we’ll call `BiasAdd`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, let’s do `sigmoid`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This simply implements the math described in the previous chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For both `sigmoid` and the `ParamOperation`, the step during the backward pass
    where we compute:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'is the step where we are applying the chain rule, and the corresponding rule
    for `WeightMultiply`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: is, as I argued in [Chapter 1](ch01.html#foundations), the analogue of the chain
    rule when the function in question is a matrix multiplication.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve defined these `Operation`s precisely, we can use *them* as building
    blocks to define a `Layer`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The Layer Blueprint
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because of the way we’ve written the `Operation`s, writing the `Layer` class
    is easy:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward` and `backward` methods simply involve sending the input successively
    forward through a series of `Operation`s—exactly as we’ve been doing in the diagrams
    all along! This is the most important fact about the working of `Layer`s; the
    rest of the code is a wrapper around this and mostly involves bookkeeping:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the correct series of `Operation`s in the `_setup_layer` function and
    initializing and storing the parameters in these `Operation`s (which will also
    take place in the `_setup_layer` function)
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing the correct values in `self.input_` and `self.output` on the `forward`
    method
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing the correct assertion checking in the `backward` method
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the `_params` and `_param_grads` functions simply extract the parameters
    and their gradients (with respect to the loss) from the `ParamOperation`s within
    the layer.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s what all that looks like:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Just as we moved from an abstract definition of an `Operation` to the implementation
    of specific `Operation`s needed for the neural network from [Chapter 2](ch02.html#fundamentals),
    let’s now implement the `Layer` from that network as well.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The Dense Layer
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We called the `Operation`s we’ve been dealing with `WeightMultiply`, `BiasAdd`,
    and so on. What should we call the layer we’ve been using so far? A `LinearNonLinear`
    layer?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'A defining characteristic of this layer is that *each output neuron is a function
    of all of the input neurons*. That is what the matrix multiplication is really
    doing: if the matrix is <math><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
    rows by <math><msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math>
    columns, the multiplication itself is computing <math><msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math>
    new features, each of which is a weighted linear combination of *all* of the <math><msub><mi>n</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow></msub></math> input features.^([2](ch03.html#idm45732623512888))
    Thus these layers are often called *fully connected* layers; recently, in the
    popular `Keras` library, they are also often called `Dense` layers, a more concise
    term that gets across the same idea.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what to call it and why, let’s define the `Dense` layer in
    terms of the operations we’ve already defined—as you’ll see, because of how we
    defined our `Layer` base class, all we need to do is to put the `Operation`s defined
    in the previous section in as a list in the `_setup_layer` function.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that we’ll make the default activation a `Linear` activation, which really
    means we apply no activation, and simply apply the identity function to the output
    of the layer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: What building blocks should we now add on top of `Operation` and `Layer`? To
    train our model, we know we’ll need a `NeuralNetwork` class to wrap around `Layer`s,
    just as `Layer`s wrapped around `Operation`s. It isn’t obvious what other classes
    will be needed, so we’ll just dive in and build `NeuralNetwork` and figure out
    the other classes we’ll need as we go.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The NeuralNetwork Class, and Maybe Others
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What should our `NeuralNetwork` class be able to do? At a high level, it should
    be able to *learn from data*: more precisely, it should be able to take in batches
    of data representing “observations” (`X`) and “correct answers” (`y`) and learn
    the relationship between `X` and `y`, which means learning a function that can
    transform `X` into predictions `p` that are very close to `y`.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'How exactly will this learning take place, given the `Layer` and `Operation`
    classes just defined? Recalling how the model from the last chapter worked, we’ll
    implement the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The neural network should take `X` and pass it successively forward through
    each `Layer` (which is really a convenient wrapper around feeding it through many
    `Operation`s), at which point the result will represent the `prediction`.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络应该接受`X`并将其逐步通过每个`Layer`（实际上是一个方便的包装器，用于通过许多`Operation`进行馈送），此时结果将代表`prediction`。
- en: Next, `prediction` should be compared with the value `y` to calculate the loss
    and generate the “loss gradient,” which is the partial derivative of the loss
    with respect to each element in the last layer in the network (namely, the one
    that generated the `prediction`).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，应该将`prediction`与值`y`进行比较，计算损失并生成“损失梯度”，这是与网络中最后一个层（即生成`prediction`的层）中的每个元素相关的损失的偏导数。
- en: Finally, we’ll send this loss gradient successively backward through each layer,
    along the way computing the “parameter gradients”—the partial derivative of the
    loss with respect to each of the parameters—and storing them in the corresponding
    `Operation`s.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将通过每个层将这个损失梯度逐步向后发送，同时计算“参数梯度”——损失对每个参数的偏导数，并将它们存储在相应的`Operation`中。
- en: Diagram
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图
- en: '[Figure 3-5](#backpropagation_now_in_terms) captures this description of a
    neural network in terms of `Layer`s.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-5](#backpropagation_now_in_terms)以`Layer`的术语捕捉了神经网络的描述。'
- en: '![Neural net diagram](assets/dlfs_0305.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](assets/dlfs_0305.png)'
- en: Figure 3-5\. Backpropagation, now in terms of Layers instead of Operations
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5。反向传播，现在以Layer而不是Operation的术语
- en: Code
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'How should we implement this? First, we’ll want our neural network to ultimately
    deal with `Layer`s the same way our `Layer`s dealt with `Operation`s. For example,
    we want the `forward` method to receive `X` as input and simply do something like:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何实现这一点？首先，我们希望我们的神经网络最终处理`Layer`的方式与我们的`Layer`处理`Operation`的方式相同。例如，我们希望`forward`方法接收`X`作为输入，然后简单地执行类似以下的操作：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, we’ll want our `backward` method to take in an argument—let’s initially
    call it `grad`—and do something like:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们希望我们的`backward`方法接收一个参数——我们最初称之为`grad`——然后执行类似以下的操作：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Where will `grad` come from? It has to come from the *loss*, a special function
    that takes in the `prediction` along with `y` and:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`grad`将从哪里来？它必须来自*损失*，一个特殊的函数，它接收`prediction`以及`y`，然后：'
- en: Computes a single number representing the “penalty” for the network making that
    `prediction`.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算代表网络进行该`prediction`的“惩罚”的单个数字。
- en: Sends backward a gradient for every element of the `prediction` with respect
    to the loss. This gradient is what the last `Layer` in the network will receive
    as the input to its `backward` function.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对每个`prediction`中的元素，发送一个梯度与损失相关的反向梯度。这个梯度是网络中最后一个`Layer`将作为其`backward`函数输入接收的内容。
- en: In the example from the prior chapter, the loss function was the squared difference
    between the `prediction` and the target, and the gradient of the `prediction`
    with respect to the loss was computed accordingly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章的示例中，损失函数是`prediction`和目标之间的平方差，相应地计算了`prediction`相对于损失的梯度。
- en: How should we implement this? It seems like this concept is important enough
    to deserve its own class. Furthermore, this class can be implemented similarly
    to the `Layer` class, except the `forward` method will produce an actual number
    (a `float`) as the loss, instead of an `ndarray` to be sent forward to the next
    `Layer`. Let’s formalize this.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何实现这一点？这个概念似乎很重要，值得拥有自己的类。此外，这个类可以类似于`Layer`类实现，只是`forward`方法将产生一个实际数字（一个`float`）作为损失，而不是一个`ndarray`被发送到下一个`Layer`。让我们正式化这一点。
- en: Loss Class
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失类
- en: 'The `Loss` base class will be similar to `Layer`—the `forward` and `backward`
    methods will check that the shapes of the appropriate `ndarray`s are identical
    and define two methods, `_output` and `_input_grad`, that any subclass of `Loss`
    will have to define:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`Loss`基类将类似于`Layer`——`forward`和`backward`方法将检查适当的`ndarray`的形状是否相同，并定义两个方法，`_output`和`_input_grad`，任何`Loss`子类都必须定义：'
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As in the `Operation` class, we check that the gradient that the loss sends
    backward is the same shape as the `prediction` received as input from the last
    layer of the network:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与`Operation`类一样，我们检查损失向后发送的梯度与从网络的最后一层接收的`prediction`的形状是否相同：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we simply code the forward and backward rules of the mean squared error
    loss formula.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简单地编写均方误差损失公式的前向和反向规则。
- en: This is the last key building block we need to build deep learning from scratch.
    Let’s review how these pieces fit together and then proceed with building a model!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要从头开始构建深度学习的最后一个关键构建块。让我们回顾一下这些部分如何组合在一起，然后继续构建模型！
- en: Deep Learning from Scratch
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始的深度学习
- en: 'We ultimately want to build a `NeuralNetwork` class, using [Figure 3-5](#backpropagation_now_in_terms)
    as a guide, that we can use to define and train deep learning models. Before we
    dive in and start coding, let’s describe precisely what such a class would be
    and how it would interact with the `Operation`, `Layer`, and `Loss` classes we
    just defined:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终希望构建一个`NeuralNetwork`类，使用[图3-5](#backpropagation_now_in_terms)作为指南，我们可以用来定义和训练深度学习模型。在我们深入编码之前，让我们准确描述一下这样一个类会是什么样的，以及它将如何与我们刚刚定义的`Operation`、`Layer`和`Loss`类进行交互：
- en: A `NeuralNetwork` will have a list of `Layer`s as an attribute. The `Layer`s
    would be as defined previously, with `forward` and `backward` methods. These methods
    take in `ndarray` objects and return `ndarray` objects.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`NeuralNetwork`将具有`Layer`列表作为属性。`Layer`将如先前定义的那样，具有`forward`和`backward`方法。这些方法接受`ndarray`对象并返回`ndarray`对象。'
- en: Each `Layer` will have a list of `Operation`s saved in the `operations` attribute
    of the layer during the `_setup_layer` function.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个`Layer`在`_setup_layer`函数期间的`operations`属性中保存了一个`Operation`列表。
- en: These `Operation`s, just like the `Layer` itself, have `forward` and `backward`
    methods that take in `ndarray` objects as arguments and return `ndarray` objects
    as outputs.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些`Operation`，就像`Layer`本身一样，有`forward`和`backward`方法，接受`ndarray`对象作为参数并返回`ndarray`对象作为输出。
- en: In each operation, the shape of the `output_grad` received in the `backward`
    method must be the same as the shape of the `output` attribute of the `Layer`.
    The same is true for the shapes of the `input_grad` passed backward during the
    `backward` method and the `input_` attribute.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some operations have parameters (stored in the `param` attribute); these operations
    inherit from the `ParamOperation` class. The same constraints on input and output
    shapes apply to `Layer`s and their `forward` and `backward` methods as well—they
    take in `ndarray` objects and output `ndarray` objects, and the shapes of the
    `input` and `output` attributes and their corresponding gradients must match.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `NeuralNetwork` will also have a `Loss`. This class will take the output of
    the last operation from the `NeuralNetwork` and the target, check that their shapes
    are the same, and calculate both a loss value (a number) and an `ndarray` `loss_grad`
    that will be fed into the output layer, starting backpropagation.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing Batch Training
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve covered several times the high-level steps for training a model one batch
    at a time. They are important and worth repeating:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Feed input through the model function (the “forward pass”) to get a prediction.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the number representing the loss.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the gradient of the loss with respect to the parameters, using the
    chain rule and the quantities computed during the forward pass.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameters using these gradients.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We would then feed a new batch of data through and repeat these steps.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Translating these steps into the `NeuralNetwork` framework just described is
    straightforward:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Receive `X` and `y` as inputs, both `ndarray`s.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed `X` successively forward through each `Layer`.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `Loss` to produce loss value and the loss gradient to be sent backward.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the loss gradient as input to the `backward` method for the network, which
    will calculate the `param_grads` for each layer in the network.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the `update_params` function on each layer, which will use the overall
    learning rate for the `NeuralNetwork` as well as the newly calculated `param_grads`.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We finally have our full definition of a neural network that can accommodate
    batch training. Now let’s code it up.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'NeuralNetwork: Code'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Coding all of this up is pretty straightforward:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With this `NeuralNetwork` class, we can implement the models from the prior
    chapter in a more modular, flexible way and define other models to represent complex
    nonlinear relationships between input and output. For example, here’s how to easily
    instantiate the two models we covered in the last chapter—the linear regression
    and the neural network:^([3](ch03.html#idm45732622822120))
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We’re basically done; now we just feed data repeatedly through the network in
    order for it to learn. To make this process cleaner and easier to extend to the
    more complicated deep learning scenarios we’ll see in the following chapter, however,
    it will help us to define another class that carries out the training, as well
    as an additional class that carries out the “learning,” or the actual updating
    of the `NeuralNetwork` parameters given the gradients computed on the backward
    pass. Let’s quickly define these two classes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Trainer and Optimizer
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let’s note the similarities between these classes and the code we used
    to train the network in [Chapter 2](ch02.html#fundamentals). There, we used the
    following code to implement the four steps described earlier for training the
    model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This code was within a `for` loop that repeatedly fed data through the function
    defining and updated our network.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'With the classes we have now, we’ll ultimately do this inside a `fit` function
    within the `Trainer` class that will mostly be a wrapper around the `train` function
    used in the prior chapter. (The full code for it is in this chapter’s [Jupyter
    Notebook](https://oreil.ly/2MV0aZI) on the book’s GitHub page.) The main difference
    is that inside this new function, the first two lines from the preceding code
    block will be replaced with this line:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Updating the parameters, which happens in the following two lines, will take
    place in a separate `Optimizer` class. And finally, the `for` loop that previously
    wrapped around all of this will take place in the `Trainer` class that wraps around
    the `NeuralNetwork` and the `Optimizer`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss why we need an `Optimizer` class and what it should look
    like.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the model we described in the last chapter, each `Layer` contains a simple
    rule for updating the weights based on the parameters and their gradients. As
    we’ll touch on in the next chapter, there are many other update rules we can use,
    such as ones involving the *history* of gradient updates rather than just the
    gradient updates from the specific batch that was fed in at that iteration. Creating
    a separate `Optimizer` class will give us the flexibility to swap in one update
    rule for another, something that we’ll explore in more detail in the next chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Description and code
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The base `Optimizer` class will take in a `NeuralNetwork` and, every time the
    `step` function is called, will update the parameters of the network based on
    their current values, their gradients, and any other information stored in the
    `Optimizer`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And here’s how this looks with the straightforward update rule we’ve seen so
    far, known as *stochastic gradient descent*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that while our `NeuralNetwork` class does not have an `_update_params`
    method, we do rely on the `params()` and `param_grads()` methods to extract the
    correct `ndarray`s for optimization.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: That’s the basic `Optimizer` class; let’s cover the `Trainer` class next.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Trainer
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to training the model as described previously, the `Trainer` class
    also links together the `NeuralNetwork` with the `Optimizer`, ensuring the latter
    trains the former properly. You may have noticed in the previous section that
    we didn’t pass in a `NeuralNetwork` when initializing our `Optimizer`; instead,
    we’ll assign the `NeuralNetwork` to be an attribute of the `Optimizer` when we
    initialize the `Trainer` class shortly, with this line:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the following subsection, I show a simplified but working version of the
    `Trainer` class that for now contains just the `fit` method. This method trains
    our model for a number of *epochs* and prints out the loss value after each set
    number of epochs. In each epoch, we:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle the data at the beginning of the epoch
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the data through the network in batches, updating the parameters after
    each batch has been fed through
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The epoch ends when we have fed the entire training set through the `Trainer`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Trainer code
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following is the code for a simple version of the `Trainer` class, we
    hide two self-explanatory helper methods used during the `fit` function: `generate_batches`,
    which generates batches of data from `X_train` and `y_train` for training, and
    `permute_data`, which shuffles `X_train` and `y_train` at the beginning of each
    epoch. We also include a `restart` argument in the `train` function: if `True`
    (default), it will reinitialize the model’s parameters to random values upon calling
    the `train` function:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the full version of this function in the book’s [GitHub repository](https://oreil.ly/2MV0aZI),
    we also implement *early stopping*, which does the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: It saves the loss value every `eval_every` epochs.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It checks whether the validation loss is lower than the last time it was calculated.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the validation loss is *not* lower, it uses the model from `eval_every` epochs
    ago.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we have everything we need to train these models!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Putting Everything Together
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the full code to train our network using all the `Trainer` and `Optimizer`
    classes and the two models defined before—`linear_regression` and `neural_network`.
    We’ll set the learning rate to `0.01` and the maximum number of epochs to `50`
    and evaluate our models every `10` epochs:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Using the same model-scoring functions from [Chapter 2](ch02.html#fundamentals),
    and wrapping them inside an `eval_regression_model` function, gives us these results:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: These are similar to the results of the linear regression we ran in the last
    chapter, confirming that our framework is working.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the same code with the `neural_network` model with a hidden layer with
    13 neurons, we get the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Again, these results are similar to what we saw in the prior chapter, and they’re
    significantly better than our straightforward linear regression.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Our First Deep Learning Model (from Scratch)
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that all of that setup is out of the way, defining our first deep learning
    model is trivial:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We won’t even try to be clever with this (yet). We’ll just add a hidden layer
    with the same dimensionality as the first layer, so that our network now has two
    hidden layers, each with 13 neurons.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Training this using the same learning rate and evaluation schedule as the prior
    models yields the following result:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We finally worked up to doing deep learning from scratch—and indeed, on this
    real-world problem, without the use of any tricks (just a bit of learning rate
    tuning), our deep learning model does perform slightly better than a neural network
    with just one hidden layer.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, we did so by building a framework that is easily extensible.
    We could easily implement other kinds of `Operation`s, wrap them in new `Layer`s,
    and drop them right in, assuming that they have defined `_output` and `_input_grad`
    methods and that the dimensions of their inputs, outputs, and parameters match
    those of their respective gradients. Similarly, we could easily drop different
    activation functions into our existing layers and see if it decreases our error
    metrics; I encourage you to clone the book’s [GitHub repo](https://oreil.ly/deep-learning-github)
    and try this!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and Next Steps
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next chapter, I’ll cover several tricks that will be essential to getting
    our models to train properly once we get to more challenging problems than this
    simple one^([4](ch03.html#idm45732621371848))—in particular, defining other `Loss`es
    and `Optimizer`s. I’ll also cover additional tricks for tuning our learning rates
    and modifying them throughout training, and I’ll show how to incorporate this
    into the `Optimizer` and `Trainer` classes. Finally, we’ll see Dropout, a new
    kind of `Operation` that has proven essential for increasing the training stability
    of deep learning models. Onward!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm45732624417528-marker)) Among all activation functions, the
    `sigmoid` function, which maps inputs to between 0 and 1, most closely mimics
    the actual activation of neurons in the brain, but in general activation functions
    can be any monotonic, nonlinear function.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch03.html#idm45732623512888-marker)) As we’ll see in [Chapter 5](ch05.html#convolution),
    this is not true of all layers: in *convolutional* layers, for example, each output
    feature is a combination of *only a small subset* of the input features.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#idm45732622822120-marker)) The learning rate of 0.01 isn’t special;
    we simply found it to be optimal in the course of experimenting while writing
    the prior chapter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#idm45732621371848-marker)) Even on this simple problem, changing
    the hyperparameters slightly can cause the deep learning model to fail to beat
    the two-layer neural network. Clone the [GitHub repo](https://oreil.ly/deep-learning-github)
    and try it yourself!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
