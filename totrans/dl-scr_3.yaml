- en: Chapter 3\. Deep Learning from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may not realize it, but you now have all the mathematical and conceptual
    foundations to answer the key questions about deep learning models that I posed
    at the beginning of the book: you understand *how* neural networks work—the computations
    involved with the matrix multiplications, the loss, and the partial derivatives
    with respect to that loss—as well as *why* those computations work (namely, the
    chain rule from calculus). We achieved this understanding by building neural networks
    from first principles, representing them as a series of “building blocks” where
    each building block was a single mathematical function. In this chapter, you’ll
    learn to represent these building blocks themselves as abstract Python classes
    and then use these classes to build deep learning models; by the end of this chapter,
    you will indeed have done “deep learning from scratch”!'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also map the descriptions of neural networks in terms of these building
    blocks to more conventional descriptions of deep learning models that you may
    have heard before. For example, by the end of this chapter, you’ll know what it
    means for a deep learning model to have “multiple hidden layers.” This is really
    the essence of understanding a concept: being able to translate between high-level
    descriptions and low-level details of what is actually going on. Let’s begin building
    toward this translation. So far, we’ve described models just in terms of the operations
    that happen at a low level. In the first part of this chapter, we’ll map this
    description of models to common higher-level concepts such as “layers” that will
    ultimately allow us to more easily describe more complex models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning Definition: A First Pass'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What *is* a “deep learning” model? In the previous chapter, we defined a model
    as a mathematical function represented by a computational graph. The purpose of
    such a model was to try to map inputs, each drawn from some dataset with common
    characteristics (such as separate inputs representing different features of houses)
    to outputs drawn from a related distribution (such as the prices of those houses).
    We found that if we defined the model as a function that included *parameters*
    as inputs to some of its operations, we could “fit” it to optimally describe the
    data using the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Repeatedly feed observations through the model, keeping track of the quantities
    computed along the way during this “forward pass.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate a *loss* representing how far off our model’s predictions were from
    the desired outputs or *target*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the quantities computed on the forward pass and the chain rule math worked
    out in [Chapter 1](ch01.html#foundations), compute how much each of the input
    *parameters* ultimately affects this loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the values of the parameters so that the loss will hopefully be reduced
    when the next set of observations is passed through the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We started out with a model containing just a series of linear operations transforming
    our features into the target (which turned out to be equivalent to a traditional
    linear regression model). This had the expected limitation that, even when fit
    “optimally,” the model could nevertheless represent only linear relationships
    between our features and our target.
  prefs: []
  type: TYPE_NORMAL
- en: We then defined a function structure that applied these linear operations first,
    then a *non*linear operation (the `sigmoid` function), and then a final set of
    linear operations. We showed that with this modification, our model *could* learn
    something closer to the true, nonlinear relationship between input and output,
    while having the additional benefit that it could learn relationships between
    *combinations* of our input features and the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the connection between models like these and deep learning models?
    We’ll start with a somewhat clumsy attempt at a definition: deep learning models
    are represented by series of operations that have *at least two, nonconsecutive*
    nonlinear functions involved.'
  prefs: []
  type: TYPE_NORMAL
- en: I’ll show where this definition comes from shortly, but first note that since
    deep learning models are just a series of operations, the process of training
    them is in fact *identical* to the process we’ve been using for the simpler models
    we’ve already seen. After all, what allows this training process to work is the
    differentiability of the model with respect to its inputs; and as mentioned in
    [Chapter 1](ch01.html#foundations), the composition of differentiable functions
    is differentiable, so as long as the individual operations making up the function
    are differentiable, the whole function will be differentiable, and we’ll be able
    to train it using the same four-step training procedure just described.
  prefs: []
  type: TYPE_NORMAL
- en: However, so far our approach to actually training these models has been to compute
    these derivatives by manually coding the forward and backward passes and then
    multiplying the appropriate quantities together to get the derivatives. For the
    simple neural network model in [Chapter 2](ch02.html#fundamentals), this required
    17 steps. Because we’re describing the model at such a low level, it isn’t immediately
    clear how we could add more complexity to this model (or what exactly what that
    would mean) or even make a simple change such as swapping out a different nonlinear
    function for the sigmoid function. To transition to being able to build arbitrarily
    “deep” and otherwise “complex” deep learning models, we’ll have to think about
    where in these 17 steps we can create reusable components, at a higher level than
    individual operations, that we can swap in and out to build different models.
    To guide us in the right direction as far as which abstractions to create, we’ll
    try to map the operations we’ve been using to traditional descriptions of neural
    networks as being made up of “layers,” “neurons,” and so on.
  prefs: []
  type: TYPE_NORMAL
- en: As our first step, we’ll have to create an abstraction to represent the individual
    operations we’ve been working with so far, instead of continuing to code the same
    matrix multiplication and bias addition over and over again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Building Blocks of Neural Networks: Operations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Operation` class will represent one of the constituent functions in our
    neural networks. We know that at a high level, based on the way we’ve used such
    functions in our models, it should have `forward` and `backward` methods, each
    of which receives an `ndarray` as an input and outputs an `ndarray`. Some operations,
    such as matrix multiplication, seem to have *another* special kind of input, also
    an `ndarray`: the parameters. In our `Operation` class—or perhaps in another class
    that inherits from it—we should allow for `params` as another instance variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another insight is that there seem to be two types of `Operation`s: some, such
    as the matrix multiplication, return an `ndarray` as output that is a different
    shape than the `ndarray` they received as input; by contrast, some `Operation`s,
    such as the `sigmoid` function, simply apply some function to each element of
    the input `ndarray`. What, then, is the “general rule” about the shapes of the
    `ndarray`s that get passed between our operations? Let’s consider the `ndarray`s
    passed through our `Operation`s: each `Operation` will send outputs forward on
    the forward pass and will receive an “output gradient” on the backward pass, which
    will represent the partial derivative of the loss with respect to every element
    of the `Operation`’s output (computed by the other `Operation`s that make up the
    network). Also on the backward pass, each `Operation` will send an “input gradient”
    backward, representing the partial derivative of the loss with respect to each
    element of the input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These facts place a few important restrictions on the workings of our `Operation`s
    that will help us ensure we’re computing the gradients correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: The shape of the *output gradient* `ndarray` must match the shape of the *output*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape of the *input gradient* that the `Operation` sends backward during
    the backward pass must match the shape of the `Operation`’s *input*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will all be clearer once you see it in a diagram; let’s look at that next.
  prefs: []
  type: TYPE_NORMAL
- en: Diagram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is all summarized in [Figure 3-1](#fig_03-01), for an operation `O` that
    is receiving inputs from an operation `N` and passing outputs on to another operation
    `P`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. An Operation, with input and output
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 3-2](#fig_03-02) covers the case of an `Operation` with parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. A ParamOperation, with input and output and parameters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With all this, we can write the fundamental building block for our neural network,
    an `Operation`, as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For any individual `Operation` that we define, we’ll have to implement the `_output`
    and `_input_grad` functions, so named because of the quantities they compute.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’re defining base classes like this primarily for pedagogical reasons: it
    is important to have the mental model that *all* `Operation`s you’ll encounter
    throughout deep learning fit this blueprint of sending inputs forward and gradients
    backward, with the shapes of what they receive on the forward pass matching the
    shapes of what they send backward on the backward pass, and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll define the specific `Operation`s we’ve used thus far—matrix multiplication
    and so on—later in this chapter. First we’ll define another class that inherits
    from `Operation` that we’ll use specifically for `Operation`s that involve parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the base `Operation`, an individual `ParamOperation` would have to
    define the `_param_grad` function in addition to the `_output` and `_input_grad`
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now formalized the neural network building blocks we’ve been using
    in our models so far. We could skip ahead and define neural networks directly
    in terms of these `Operation`s, but there is an intermediate class we’ve been
    dancing around for a chapter and a half that we’ll define first: the `Layer`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Building Blocks of Neural Networks: Layers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In terms of `Operation`s, layers are a series of linear operations followed
    by a nonlinear operation. For example, our neural network from the last chapter
    could be said to have had five total operations: two linear operations—a weight
    multiplication and the addition of a bias term—followed the `sigmoid` function
    and then two more linear operations. In this case, we would say that the first
    three operations, up to and including the nonlinear one, would constitute the
    first layer, and the last two operations would constitute the second layer. In
    addition, we say that the input itself represents a special kind of layer called
    the *input* layer (in terms of numbering the layers, this layer doesn’t count,
    so that we can think of it as the “zeroth” layer). The last layer, similarly,
    is called the *output* layer. The middle layer—the “first one,” according to our
    numbering—also has an important name: it is called a *hidden* layer, since it
    is the only layer whose values we don’t typically see explicitly during the course
    of training.'
  prefs: []
  type: TYPE_NORMAL
- en: The output layer is an important exception to this definition of layers, in
    that it does not *have* to have a nonlinear operation applied to it; this is simply
    because we often want the values that come out of this layer to have values between
    negative infinity and infinity (or at least between 0 and infinity), whereas nonlinear
    functions typically “squash down” their input to some subset of that range relevant
    to the particular problem we’re trying to solve (for example, the `sigmoid` function
    squashes down its input to between 0 and 1).
  prefs: []
  type: TYPE_NORMAL
- en: Diagrams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make the connection explicit, [Figure 3-3](#fig_03-03) shows the diagram
    of the neural network from the prior chapter with the individual operations grouped
    into layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. The neural network from the prior chapter with the operations grouped
    into layers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see that the input represents an “input” layer, the next three operations
    (ending with the `sigmoid` function) represent the next layer, and the last two
    operations represent the last layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is, of course, rather cumbersome. And that’s the point: representing neural
    networks as a series of individual operations, while showing clearly how neural
    networks work and how to train them, is too “low level” for anything more complicated
    than a two-layer neural network. That’s why the more common way to represent neural
    networks is in terms of layers, as shown in [Figure 3-4](#fig_03-04).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. The neural network from the prior chapter in terms of layers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Connection to the brain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, let’s make one last connection between what we’ve seen so far and
    a notion you’ve likely heard before: each layer can be said to have a certain
    number of *neurons* equal to *the dimensionality of the vector that represents
    each observation in the layer’s output*. The neural network from the prior example
    can thus be thought of as having 13 neurons in the input layer, then 13 neurons
    (again) in the hidden layer, and one neuron in the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neurons in the brain have the property that they can receive inputs from many
    other neurons and will “fire” and send a signal forward only if the signals they
    receive cumulatively reach a certain “activation energy.” Neurons in the context
    of neural networks have a loosely analogous property: they do indeed send signals
    forward based on their inputs, but the inputs are transformed into outputs simply
    via a nonlinear function. Thus, this nonlinear function is called the *activation
    function*, and the values that come out of it are called the *activations* for
    that layer.^([1](ch03.html#idm45732624417528))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve defined layers, we can state the more conventional definition
    of deep learning: *deep learning models are neural networks with more than one
    hidden layer.*'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that this is equivalent to the earlier definition that was purely
    in terms of `Operation`s, since a layer is just a series of `Operation`s with
    a nonlinear operation at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve defined a base class for our `Operation`s, let’s show how it
    can serve as the fundamental building block of the models we saw in the prior
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Building Blocks on Building Blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What specific `Operation`s do we need to implement for the models in the prior
    chapter to work? Based on our experience of implementing that neural network step
    by step, we know there are three kinds:'
  prefs: []
  type: TYPE_NORMAL
- en: The matrix multiplication of the input with the matrix of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The addition of a bias term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sigmoid` activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start with the `WeightMultiply` `Operation`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here we simply code up the matrix multiplication on the forward pass, as well
    as the rules for “sending gradients backward” to both the inputs and the parameters
    on the backward pass (using the rules for doing so that we reasoned through at
    the end of [Chapter 1](ch01.html#foundations)). As you’ll see shortly, we can
    now use this as a *building block* that we can simply plug into our `Layer`s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the addition operation, which we’ll call `BiasAdd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s do `sigmoid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This simply implements the math described in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For both `sigmoid` and the `ParamOperation`, the step during the backward pass
    where we compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'is the step where we are applying the chain rule, and the corresponding rule
    for `WeightMultiply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: is, as I argued in [Chapter 1](ch01.html#foundations), the analogue of the chain
    rule when the function in question is a matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve defined these `Operation`s precisely, we can use *them* as building
    blocks to define a `Layer`.
  prefs: []
  type: TYPE_NORMAL
- en: The Layer Blueprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because of the way we’ve written the `Operation`s, writing the `Layer` class
    is easy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward` and `backward` methods simply involve sending the input successively
    forward through a series of `Operation`s—exactly as we’ve been doing in the diagrams
    all along! This is the most important fact about the working of `Layer`s; the
    rest of the code is a wrapper around this and mostly involves bookkeeping:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the correct series of `Operation`s in the `_setup_layer` function and
    initializing and storing the parameters in these `Operation`s (which will also
    take place in the `_setup_layer` function)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing the correct values in `self.input_` and `self.output` on the `forward`
    method
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing the correct assertion checking in the `backward` method
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the `_params` and `_param_grads` functions simply extract the parameters
    and their gradients (with respect to the loss) from the `ParamOperation`s within
    the layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s what all that looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Just as we moved from an abstract definition of an `Operation` to the implementation
    of specific `Operation`s needed for the neural network from [Chapter 2](ch02.html#fundamentals),
    let’s now implement the `Layer` from that network as well.
  prefs: []
  type: TYPE_NORMAL
- en: The Dense Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We called the `Operation`s we’ve been dealing with `WeightMultiply`, `BiasAdd`,
    and so on. What should we call the layer we’ve been using so far? A `LinearNonLinear`
    layer?
  prefs: []
  type: TYPE_NORMAL
- en: 'A defining characteristic of this layer is that *each output neuron is a function
    of all of the input neurons*. That is what the matrix multiplication is really
    doing: if the matrix is <math><msub><mi>n</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
    rows by <math><msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math>
    columns, the multiplication itself is computing <math><msub><mi>n</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></math>
    new features, each of which is a weighted linear combination of *all* of the <math><msub><mi>n</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow></msub></math> input features.^([2](ch03.html#idm45732623512888))
    Thus these layers are often called *fully connected* layers; recently, in the
    popular `Keras` library, they are also often called `Dense` layers, a more concise
    term that gets across the same idea.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what to call it and why, let’s define the `Dense` layer in
    terms of the operations we’ve already defined—as you’ll see, because of how we
    defined our `Layer` base class, all we need to do is to put the `Operation`s defined
    in the previous section in as a list in the `_setup_layer` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that we’ll make the default activation a `Linear` activation, which really
    means we apply no activation, and simply apply the identity function to the output
    of the layer.
  prefs: []
  type: TYPE_NORMAL
- en: What building blocks should we now add on top of `Operation` and `Layer`? To
    train our model, we know we’ll need a `NeuralNetwork` class to wrap around `Layer`s,
    just as `Layer`s wrapped around `Operation`s. It isn’t obvious what other classes
    will be needed, so we’ll just dive in and build `NeuralNetwork` and figure out
    the other classes we’ll need as we go.
  prefs: []
  type: TYPE_NORMAL
- en: The NeuralNetwork Class, and Maybe Others
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What should our `NeuralNetwork` class be able to do? At a high level, it should
    be able to *learn from data*: more precisely, it should be able to take in batches
    of data representing “observations” (`X`) and “correct answers” (`y`) and learn
    the relationship between `X` and `y`, which means learning a function that can
    transform `X` into predictions `p` that are very close to `y`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How exactly will this learning take place, given the `Layer` and `Operation`
    classes just defined? Recalling how the model from the last chapter worked, we’ll
    implement the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The neural network should take `X` and pass it successively forward through
    each `Layer` (which is really a convenient wrapper around feeding it through many
    `Operation`s), at which point the result will represent the `prediction`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, `prediction` should be compared with the value `y` to calculate the loss
    and generate the “loss gradient,” which is the partial derivative of the loss
    with respect to each element in the last layer in the network (namely, the one
    that generated the `prediction`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we’ll send this loss gradient successively backward through each layer,
    along the way computing the “parameter gradients”—the partial derivative of the
    loss with respect to each of the parameters—and storing them in the corresponding
    `Operation`s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diagram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 3-5](#backpropagation_now_in_terms) captures this description of a
    neural network in terms of `Layer`s.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Backpropagation, now in terms of Layers instead of Operations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How should we implement this? First, we’ll want our neural network to ultimately
    deal with `Layer`s the same way our `Layer`s dealt with `Operation`s. For example,
    we want the `forward` method to receive `X` as input and simply do something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we’ll want our `backward` method to take in an argument—let’s initially
    call it `grad`—and do something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Where will `grad` come from? It has to come from the *loss*, a special function
    that takes in the `prediction` along with `y` and:'
  prefs: []
  type: TYPE_NORMAL
- en: Computes a single number representing the “penalty” for the network making that
    `prediction`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sends backward a gradient for every element of the `prediction` with respect
    to the loss. This gradient is what the last `Layer` in the network will receive
    as the input to its `backward` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the example from the prior chapter, the loss function was the squared difference
    between the `prediction` and the target, and the gradient of the `prediction`
    with respect to the loss was computed accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: How should we implement this? It seems like this concept is important enough
    to deserve its own class. Furthermore, this class can be implemented similarly
    to the `Layer` class, except the `forward` method will produce an actual number
    (a `float`) as the loss, instead of an `ndarray` to be sent forward to the next
    `Layer`. Let’s formalize this.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Loss` base class will be similar to `Layer`—the `forward` and `backward`
    methods will check that the shapes of the appropriate `ndarray`s are identical
    and define two methods, `_output` and `_input_grad`, that any subclass of `Loss`
    will have to define:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the `Operation` class, we check that the gradient that the loss sends
    backward is the same shape as the `prediction` received as input from the last
    layer of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we simply code the forward and backward rules of the mean squared error
    loss formula.
  prefs: []
  type: TYPE_NORMAL
- en: This is the last key building block we need to build deep learning from scratch.
    Let’s review how these pieces fit together and then proceed with building a model!
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We ultimately want to build a `NeuralNetwork` class, using [Figure 3-5](#backpropagation_now_in_terms)
    as a guide, that we can use to define and train deep learning models. Before we
    dive in and start coding, let’s describe precisely what such a class would be
    and how it would interact with the `Operation`, `Layer`, and `Loss` classes we
    just defined:'
  prefs: []
  type: TYPE_NORMAL
- en: A `NeuralNetwork` will have a list of `Layer`s as an attribute. The `Layer`s
    would be as defined previously, with `forward` and `backward` methods. These methods
    take in `ndarray` objects and return `ndarray` objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each `Layer` will have a list of `Operation`s saved in the `operations` attribute
    of the layer during the `_setup_layer` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These `Operation`s, just like the `Layer` itself, have `forward` and `backward`
    methods that take in `ndarray` objects as arguments and return `ndarray` objects
    as outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each operation, the shape of the `output_grad` received in the `backward`
    method must be the same as the shape of the `output` attribute of the `Layer`.
    The same is true for the shapes of the `input_grad` passed backward during the
    `backward` method and the `input_` attribute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some operations have parameters (stored in the `param` attribute); these operations
    inherit from the `ParamOperation` class. The same constraints on input and output
    shapes apply to `Layer`s and their `forward` and `backward` methods as well—they
    take in `ndarray` objects and output `ndarray` objects, and the shapes of the
    `input` and `output` attributes and their corresponding gradients must match.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `NeuralNetwork` will also have a `Loss`. This class will take the output of
    the last operation from the `NeuralNetwork` and the target, check that their shapes
    are the same, and calculate both a loss value (a number) and an `ndarray` `loss_grad`
    that will be fed into the output layer, starting backpropagation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing Batch Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve covered several times the high-level steps for training a model one batch
    at a time. They are important and worth repeating:'
  prefs: []
  type: TYPE_NORMAL
- en: Feed input through the model function (the “forward pass”) to get a prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the number representing the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the gradient of the loss with respect to the parameters, using the
    chain rule and the quantities computed during the forward pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameters using these gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We would then feed a new batch of data through and repeat these steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Translating these steps into the `NeuralNetwork` framework just described is
    straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Receive `X` and `y` as inputs, both `ndarray`s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed `X` successively forward through each `Layer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `Loss` to produce loss value and the loss gradient to be sent backward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the loss gradient as input to the `backward` method for the network, which
    will calculate the `param_grads` for each layer in the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the `update_params` function on each layer, which will use the overall
    learning rate for the `NeuralNetwork` as well as the newly calculated `param_grads`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We finally have our full definition of a neural network that can accommodate
    batch training. Now let’s code it up.
  prefs: []
  type: TYPE_NORMAL
- en: 'NeuralNetwork: Code'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Coding all of this up is pretty straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With this `NeuralNetwork` class, we can implement the models from the prior
    chapter in a more modular, flexible way and define other models to represent complex
    nonlinear relationships between input and output. For example, here’s how to easily
    instantiate the two models we covered in the last chapter—the linear regression
    and the neural network:^([3](ch03.html#idm45732622822120))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We’re basically done; now we just feed data repeatedly through the network in
    order for it to learn. To make this process cleaner and easier to extend to the
    more complicated deep learning scenarios we’ll see in the following chapter, however,
    it will help us to define another class that carries out the training, as well
    as an additional class that carries out the “learning,” or the actual updating
    of the `NeuralNetwork` parameters given the gradients computed on the backward
    pass. Let’s quickly define these two classes.
  prefs: []
  type: TYPE_NORMAL
- en: Trainer and Optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let’s note the similarities between these classes and the code we used
    to train the network in [Chapter 2](ch02.html#fundamentals). There, we used the
    following code to implement the four steps described earlier for training the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This code was within a `for` loop that repeatedly fed data through the function
    defining and updated our network.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the classes we have now, we’ll ultimately do this inside a `fit` function
    within the `Trainer` class that will mostly be a wrapper around the `train` function
    used in the prior chapter. (The full code for it is in this chapter’s [Jupyter
    Notebook](https://oreil.ly/2MV0aZI) on the book’s GitHub page.) The main difference
    is that inside this new function, the first two lines from the preceding code
    block will be replaced with this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Updating the parameters, which happens in the following two lines, will take
    place in a separate `Optimizer` class. And finally, the `for` loop that previously
    wrapped around all of this will take place in the `Trainer` class that wraps around
    the `NeuralNetwork` and the `Optimizer`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss why we need an `Optimizer` class and what it should look
    like.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the model we described in the last chapter, each `Layer` contains a simple
    rule for updating the weights based on the parameters and their gradients. As
    we’ll touch on in the next chapter, there are many other update rules we can use,
    such as ones involving the *history* of gradient updates rather than just the
    gradient updates from the specific batch that was fed in at that iteration. Creating
    a separate `Optimizer` class will give us the flexibility to swap in one update
    rule for another, something that we’ll explore in more detail in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Description and code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The base `Optimizer` class will take in a `NeuralNetwork` and, every time the
    `step` function is called, will update the parameters of the network based on
    their current values, their gradients, and any other information stored in the
    `Optimizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s how this looks with the straightforward update rule we’ve seen so
    far, known as *stochastic gradient descent*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that while our `NeuralNetwork` class does not have an `_update_params`
    method, we do rely on the `params()` and `param_grads()` methods to extract the
    correct `ndarray`s for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the basic `Optimizer` class; let’s cover the `Trainer` class next.
  prefs: []
  type: TYPE_NORMAL
- en: Trainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to training the model as described previously, the `Trainer` class
    also links together the `NeuralNetwork` with the `Optimizer`, ensuring the latter
    trains the former properly. You may have noticed in the previous section that
    we didn’t pass in a `NeuralNetwork` when initializing our `Optimizer`; instead,
    we’ll assign the `NeuralNetwork` to be an attribute of the `Optimizer` when we
    initialize the `Trainer` class shortly, with this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following subsection, I show a simplified but working version of the
    `Trainer` class that for now contains just the `fit` method. This method trains
    our model for a number of *epochs* and prints out the loss value after each set
    number of epochs. In each epoch, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle the data at the beginning of the epoch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the data through the network in batches, updating the parameters after
    each batch has been fed through
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The epoch ends when we have fed the entire training set through the `Trainer`.
  prefs: []
  type: TYPE_NORMAL
- en: Trainer code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following is the code for a simple version of the `Trainer` class, we
    hide two self-explanatory helper methods used during the `fit` function: `generate_batches`,
    which generates batches of data from `X_train` and `y_train` for training, and
    `permute_data`, which shuffles `X_train` and `y_train` at the beginning of each
    epoch. We also include a `restart` argument in the `train` function: if `True`
    (default), it will reinitialize the model’s parameters to random values upon calling
    the `train` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the full version of this function in the book’s [GitHub repository](https://oreil.ly/2MV0aZI),
    we also implement *early stopping*, which does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It saves the loss value every `eval_every` epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It checks whether the validation loss is lower than the last time it was calculated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the validation loss is *not* lower, it uses the model from `eval_every` epochs
    ago.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we have everything we need to train these models!
  prefs: []
  type: TYPE_NORMAL
- en: Putting Everything Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the full code to train our network using all the `Trainer` and `Optimizer`
    classes and the two models defined before—`linear_regression` and `neural_network`.
    We’ll set the learning rate to `0.01` and the maximum number of epochs to `50`
    and evaluate our models every `10` epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the same model-scoring functions from [Chapter 2](ch02.html#fundamentals),
    and wrapping them inside an `eval_regression_model` function, gives us these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: These are similar to the results of the linear regression we ran in the last
    chapter, confirming that our framework is working.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the same code with the `neural_network` model with a hidden layer with
    13 neurons, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Again, these results are similar to what we saw in the prior chapter, and they’re
    significantly better than our straightforward linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Our First Deep Learning Model (from Scratch)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that all of that setup is out of the way, defining our first deep learning
    model is trivial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We won’t even try to be clever with this (yet). We’ll just add a hidden layer
    with the same dimensionality as the first layer, so that our network now has two
    hidden layers, each with 13 neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training this using the same learning rate and evaluation schedule as the prior
    models yields the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We finally worked up to doing deep learning from scratch—and indeed, on this
    real-world problem, without the use of any tricks (just a bit of learning rate
    tuning), our deep learning model does perform slightly better than a neural network
    with just one hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, we did so by building a framework that is easily extensible.
    We could easily implement other kinds of `Operation`s, wrap them in new `Layer`s,
    and drop them right in, assuming that they have defined `_output` and `_input_grad`
    methods and that the dimensions of their inputs, outputs, and parameters match
    those of their respective gradients. Similarly, we could easily drop different
    activation functions into our existing layers and see if it decreases our error
    metrics; I encourage you to clone the book’s [GitHub repo](https://oreil.ly/deep-learning-github)
    and try this!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next chapter, I’ll cover several tricks that will be essential to getting
    our models to train properly once we get to more challenging problems than this
    simple one^([4](ch03.html#idm45732621371848))—in particular, defining other `Loss`es
    and `Optimizer`s. I’ll also cover additional tricks for tuning our learning rates
    and modifying them throughout training, and I’ll show how to incorporate this
    into the `Optimizer` and `Trainer` classes. Finally, we’ll see Dropout, a new
    kind of `Operation` that has proven essential for increasing the training stability
    of deep learning models. Onward!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm45732624417528-marker)) Among all activation functions, the
    `sigmoid` function, which maps inputs to between 0 and 1, most closely mimics
    the actual activation of neurons in the brain, but in general activation functions
    can be any monotonic, nonlinear function.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch03.html#idm45732623512888-marker)) As we’ll see in [Chapter 5](ch05.html#convolution),
    this is not true of all layers: in *convolutional* layers, for example, each output
    feature is a combination of *only a small subset* of the input features.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#idm45732622822120-marker)) The learning rate of 0.01 isn’t special;
    we simply found it to be optimal in the course of experimenting while writing
    the prior chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#idm45732621371848-marker)) Even on this simple problem, changing
    the hyperparameters slightly can cause the deep learning model to fail to beat
    the two-layer neural network. Clone the [GitHub repo](https://oreil.ly/deep-learning-github)
    and try it yourself!
  prefs: []
  type: TYPE_NORMAL
