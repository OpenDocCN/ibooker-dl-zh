["```py\nimport graphviz     #1\nimport networkx as nx   #1\nfrom networkx.drawing.nx_agraph import write_dot     #1\ndef plot_graph(G):    #1\n    dot_format = nx.nx_pydot.to_pydot(G).to_string()    #1 \n    return graphviz.Source(dot_format)   #1\n\nimport requests     #2\ndef download_code(url):     #2\n    response = requests.get(url)     #2\n    if response.status_code == 200:   #2\n        code_content = response.text   #2\n        print(\"Code fetched successfully.\")    #2\n        return code_content    #2\n    else:    #2\n        print(\"Failed to fetch code.\")   #2\n        return None     #2\n\nurl_do = (      #3\n    \"https://raw.githubusercontent.com/altdeep/\"      #3\n    \"causalML/master/book/pgmpy_do.py\"    #3\n)     #3\ncode_do = download_code(url_do)      #3\n\nurl_clone = (     #4\n    \"https://raw.githubusercontent.com/altdeep/\"      #4\n    \"causalML/master/book/chapter%209/hyp_function.py\"      #4\n)      #4\ncode_clone = download_code(url_clone)     #4\n\nprint(code_do)     #5\nprint(code_clone)      #5\n#exec(code_do)      #5\n#exec(code_clone)     #5\n```", "```py\nfrom pgmpy.factors.discrete.CPD import TabularCPD\n\np_door_with_car = TabularCPD(     #1\n    variable='Car Door Die Roll',     #1\n    variable_card=3,     #1\n    values=[[1/3], [1/3], [1/3]],     #1\n    state_names={'Car Door Die Roll': ['1st', '2nd', '3rd']}     #1\n)     #1\n\np_player_first_choice = TabularCPD(    #2\n    variable='1st Choice Die Roll',     #2\n    variable_card=3,     #2\n    values=[[1/3], [1/3], [1/3]],    #2\n    state_names={'1st Choice Die Roll': ['1st', '2nd', '3rd']}    #2\n)     #2\n\np_coin_flip = TabularCPD(        #3\n    variable='Coin Flip',     #3\n    variable_card=2,     #3\n    values=[[.5], [.5]],     #3\n    state_names={'Coin Flip': ['tails', 'heads']}     #3\n)     #3\n```", "```py\nf_strategy = TabularCPD(    \n    variable='Strategy',    \n    variable_card=2,    \n    values=[[1, 0], [0, 1]],    \n    evidence=['Coin Flip'],    \n    evidence_card=[2],    \n    state_names={    \n        'Strategy': ['stay', 'switch'],    \n        'Coin Flip': ['tails', 'heads']}    \n)\n```", "```py\nf_host_door_selection = TabularCPD(    \n    variable='Host Door Selection',    \n    variable_card=3,    \n    values=[    \n        [0,0,0,0,1,1,0,1,1,0,0,0,0,0,1,0,1,0],    \n        [1,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,1],    \n        [0,1,0,1,0,0,0,0,0,1,1,0,1,1,0,0,0,0]    \n    ],    \n    evidence=['Coin Flip',    \n              'Car Door Die Roll',    \n              '1st Choice Die Roll'],    \n    evidence_card=[2, 3, 3],    \n    state_names={    \n        'Host Door Selection':['1st', '2nd', '3rd'],    \n        'Coin Flip': ['tails', 'heads'],    \n        'Car Door Die Roll': ['1st', '2nd', '3rd'],    \n        '1st Choice Die Roll': ['1st', '2nd', '3rd']    \n    }    \n)\n```", "```py\nf_second_choice = TabularCPD(    \n    variable='2nd Choice',    \n    variable_card=3,    \n    values=[    \n        [1,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0],    \n        [0,1,0,0,1,0,0,1,0,1,0,1,0,1,0,1,0,1],    \n        [0,0,1,0,0,1,0,0,1,0,1,0,1,0,0,0,0,0]    \n    ],    \n    evidence=['Strategy', 'Host Door Selection',    \n              '1st Choice Die Roll'],    \n    evidence_card=[2, 3, 3],    \n    state_names={    \n        '2nd Choice': ['1st', '2nd', '3rd'],    \n        'Strategy': ['stay', 'switch'],    \n        'Host Door Selection': ['1st', '2nd', '3rd'],    \n        '1st Choice Die Roll': ['1st', '2nd', '3rd']    \n    }    \n)\n```", "```py\nf_win_or_lose = TabularCPD(    \n    variable='Win or Lose',    \n    variable_card=2,    \n    values=[    \n        [1,0,0,0,1,0,0,0,1],    \n        [0,1,1,1,0,1,1,1,0],    \n    ],  \n    evidence=['2nd Choice', 'Car Door Die Roll'],    \n    evidence_card=[3, 3],    \n    state_names={  \n        'Win or Lose': ['win', 'lose'],    \n        '2nd Choice': ['1st', '2nd', '3rd'],    \n        'Car Door Die Roll': ['1st', '2nd', '3rd']  \n    }    \n)\n```", "```py\nexogenous_vars = [\"Car Door Die Roll\",    #1\n                  \"Coin Flip\",    #1\n                  \"1st Choice Die Roll\"]   #1\nendogenous_vars = [\"Host Door Selection\",    #1\n                   \"Strategy\",    #1\n                   \"2nd Choice\", \"Win or Lose\"]   #1\n\nactual_world_edges = [     #2\n    ('Coin Flip', 'Host Door Selection'),     #2\n    ('Coin Flip', 'Strategy'),    #2\n    ('Car Door Die Roll', 'Host Door Selection'),   #2\n    ('1st Choice Die Roll', 'Host Door Selection'),     #2\n    ('1st Choice Die Roll', '2nd Choice'),     #2\n    ('Host Door Selection', '2nd Choice'),     #2\n    ('Strategy', '2nd Choice'),     #2\n    ('2nd Choice', 'Win or Lose'),     #2\n    ('Car Door Die Roll', 'Win or Lose')     #2\n]     #2\n\npossible_world_edges = [     #3\n    (a + \" Hyp\" if a in endogenous_vars else a,     #3\n     b + \" Hyp\" if b in endogenous_vars else b)     #3\n    for a, b in actual_world_edges     #3\n]     #3\n```", "```py\nfrom pgmpy.models import BayesianNetwork\n\ntwin_world_graph = BayesianNetwork(     #1\n    actual_world_edges +     #1\n    possible_world_edges     #1\n)   #1\n\ntwin_world_graph.add_cpds(    #2\n    p_door_with_car,    #3\n    p_player_first_choice,     #3\n    p_coin_flip,     #3\n    f_strategy,     #4\n    f_host_door_selection,    #4\n    f_second_choice,   #4\n    f_win_or_lose, #4\n    clone(f_strategy),   #5\n    clone(f_host_door_selection),     #5\n    clone(f_second_choice),    #5\n    clone(f_win_or_lose),     #5\n) \n\nplot_graph(twin_world_graph)\n```", "```py\nfrom pgmpy.inference import VariableElimination    #1\ninfer = VariableElimination(twin_world_graph)   #1\nstrategy_outcome = infer.query(    #2\n    ['Win or Lose'],    #2\n    evidence={\"Strategy\": \"switch\"}    #2\n)   #2\nprint(strategy_outcome)\n```", "```py\n+-------------------+--------------------+\n| Win or Lose       |   phi(Win or Lose) |\n+===================+====================+\n| Win or Lose(win)  |             0.6667 |\n+-------------------+--------------------+\n| Win or Lose(lose) |             0.3333 |\n+-------------------+--------------------+\n```", "```py\ncf_model = do(twin_world_graph, {'Strategy Hyp': 'switch'})     #1\ninfer = VariableElimination(cf_model)    #2\n\ncf_dist1 = infer.query(     #3\n    ['Win or Lose Hyp'],    #3\n    evidence={'Strategy': 'stay', 'Win or Lose': 'lose'}    #3\n)   \nprint(cf_dist1)\n\ncf_dist2 = infer.query(    #4\n    ['Win or Lose Hyp'],    #4\n    evidence={'Win or Lose': 'lose'}    #4\n)   \nprint(cf_dist2)\n```", "```py\n+-----------------------+------------------------+\n| Win or Lose Hyp       |   phi(Win or Lose Hyp) |\n+=======================+========================+\n| Win or Lose Hyp(win)  |                 1.0000 |\n+-----------------------+------------------------+\n| Win or Lose Hyp(lose) |                 0.0000 |\n+-----------------------+------------------------+\n```", "```py\n+-----------------------+------------------------+\n| Win or Lose Hyp       |   phi(Win or Lose Hyp) |\n+=======================+========================+\n| Win or Lose Hyp(win)  |                 0.6667 |\n+-----------------------+------------------------+\n| Win or Lose Hyp(lose) |                 0.3333 |\n+-----------------------+------------------------+\n```", "```py\nfrom torch import tensor\nfrom pyro.distributions import Bernoulli, Normal\nfrom pyro import sample\n\nfrom functools import partial    #1\nPseudoDelta = partial(Normal, scale=.01)   #1\n\ndef f_sex(N_sex):    #2\n    return sample(\"sex\", Bernoulli(N_sex))    #2\n\ndef f_femur(sex, N_femur):     #3\n    if sex == tensor(1.0):     #3\n        μ = 43.7 + 2.3 * N_femur     #3\n    else:     #3\n        μ = 40.238 + 1.9 * N_femur    #3\n    return sample(\"femur\", PseudoDelta(μ))     #3\n\ndef f_height(femur, sex, N_height):     #4\n    if sex == tensor(1.0):     #4\n        μ = 61.41 + 2.21 * femur + 7.62 * N_height     #4\n    else:  #4\n        μ = 54.1 + 2.47 * femur + 7 * N_height    #4\n    return sample(\"height\", PseudoDelta(μ))    #4\n\ndef model(exogenous):\n    N_sex = sample(\"N_sex\", exogenous['N_sex'])    #5\n    N_femur = sample(\"N_femur\", exogenous['N_femur'])    #5\n    N_height = sample(\"N_height\", exogenous['N_height'])     #5\n    sex = f_sex(N_sex)    #6\n    femur = f_femur(sex, N_femur)   #6\n    height = f_height(femur, sex, N_height)    #6\n    return sex, femur, height\n\nexogenous = {    #7\n    'N_sex': Bernoulli(.5),    #7\n    'N_femur': Normal(0., 1.),     #7\n    'N_height': Normal(0., 1.),    #7\n}   #7\n```", "```py\nimport matplotlib.pyplot as plt\nimport pyro\n\nint_model = pyro.do(model, data={\"femur\": tensor(46.0)})    #1\nint_samples = []     #2\nfor _ in range(10000):   #2\n    _, _, int_height = int_model(exogenous)     #2\n    int_samples.append(float(int_height))    #2\n\nplt.hist(     #3\n    int_samples,     #3\n    bins=20,    #3\n    alpha=0.5,   #3\n    label=\"Intervention Samples\",    #3\n    density=True     #3\n)    #3\nplt.ylim(0., .35)    #3\nplt.legend()  #3\nplt.xlabel(\"Height\")    #3\nplt.show()    #3\n```", "```py\nimport torch.distributions.constraints as constraints\nfrom pyro.primitives import param\nfrom pyro.distributions import Delta\n\ndef guide(exogenous):   #1\n    p = param(\"p\", tensor(.5),     #2\n              constraint=constraints.unit_interval)   #2\n    n_sex = sample(\"N_sex\", Bernoulli(p))   #2\n    sex = sample(\"sex\", Bernoulli(n_sex))    #3\n    n_femur_loc = param(\"n_femur_loc\", tensor(0.0))     #4\n    n_femur_scale = param(     #4\n        \"n_femur_scale\",    #4\n        tensor(1.0),     #4\n        constraint=constraints.positive    #4\n    )     #4\n    femur_dist = Normal(n_femur_loc, n_femur_scale)    #5\n    n_femur = sample(\"N_femur\", femur_dist)     #5\n    n_height_loc = param(\"n_height_loc\", tensor(0.0))    #5\n    n_height_scale = param(    #5\n        \"n_height_scale\",   #5\n        tensor(1.0),     #5\n        constraint=constraints.positive    #5\n    )     #5\n    height_dist =  Normal(n_height_loc, n_height_scale) #5\n    n_height = sample(\"N_height\", height_dist)     #5\n    femur = sample(\"femur\", Delta(n_femur))     #6\n    height = sample(\"height\", Delta(n_height))     #6\n```", "```py\nconditioned_model = pyro.condition(    \n    model,    \n    data={\"femur\": tensor(44.0), \"height\": tensor(165.0)}    \n)\n```", "```py\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\n\npyro.util.set_rng_seed(123)    #1\npyro.clear_param_store()     #2\nsvi = SVI(     #3\n          model=conditioned_model,\n          guide=guide,\n          optim=Adam({\"lr\": 0.003}),    #4\n          loss=Trace_ELBO()     #5\n)\n\nlosses = []    #6\nnum_steps = 5000     #7\nfor t in range(num_steps):    #7\n    losses.append(svi.step(exogenous))    #7\n\nplt.plot(losses)     #8\nplt.title(\"Loss During Training\")    #8\nplt.xlabel(\"step\")    #8\nplt.ylabel(\"loss\")     #8\n```", "```py\nn_sex_p = param(\"p\").item()     #1\nn_femur_loc = param(\"n_femur_loc\").item()   #1\nn_femur_scale = param(\"n_femur_scale\").item()   #1\nn_height_loc = param(\"n_height_loc\").item()   #1\nn_height_scale = param(\"n_height_scale\").item()  #1\n\nexogenous_posterior = {    #2\n    'N_sex': Bernoulli(n_sex_p),   #2\n    'N_femur': Normal(n_femur_loc, n_femur_scale),   #2\n    'N_height': Normal(n_height_loc, n_height_scale),     #2\n}     #2\n```", "```py\nint_model = pyro.do(model, data={\"femur\": tensor(46.0)})\n```", "```py\ncf_samples = []    \nfor _ in range(10000):    \n    _, _, cf_height = int_model(exogenous_posterior)    \n    cf_samples.append(float(cf_height))\n```", "```py\nplt.hist(    \n    int_samples,    \n    bins=20,    \n    alpha=0.5,    \n    label=\"Intervention Samples\",    \n    density=True    \n)\nplt.hist(    \n    cf_samples,   \n    bins=20,    \n    alpha=0.5,    \n    label=\"Counterfactual Samples\", \n    density=True    \n)  \nplt.ylim(0., .35)    \nplt.legend()    \nplt.xlabel(\"Height\")    \nplt.show()\n```", "```py\nimport torch\nfrom matplotlib import pyplot as plt\n\nimport io   #1\nimport urllib.request    #1\nimport numpy as np     #1\nurl = ('https://github.com/altdeep/causalML/blob/master/'    #1\n       'book/chapter%209/sprites_example.npz?raw=true')    #1\nwith urllib.request.urlopen(url) as response:    #1\n    data = response.read()    #1\nfile = io.BytesIO(data)    #1\nnpzfile = np.load(file)    #1\nimg_dict = dict(npzfile)     #1\nimg = torch.tensor(img_dict['image'].astype(np.float32) )    #2\nplt.imshow(img, cmap='Greys_r', interpolation='nearest')     #2\nplt.axis('off')  #2\nplt.title('original')     #2\nplt.show()    #2\ncausal_factor = torch.from_numpy(img_dict['label']).unsqueeze(0)     #3\nprint(causal_factor)     #3\n```", "```py\nimport requests\nimport torch.nn as nn\n\nCARDINALITY = [1, 3, 6, 40, 32, 32]    #1\n\nclass EncoderCausalFactors(nn.Module):     #2\n    def __init__(self, image_dim, factor_dim):\n        super(EncoderCausalFactors, self).__init__()\n        self.image_dim = image_dim\n        self.factor_dim = factor_dim\n        hidden_dim = 1000     #3\n        self.fc1 = nn.Linear(image_dim, hidden_dim)    #4\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)   #4\n        self.fc3 = nn.Linear(hidden_dim, factor_dim)    #4\n        self.softplus = nn.Softplus()  #4\n        self.sigmoid = nn.Sigmoid()     #5\n\n    def forward(self, img):\n        img = img.reshape(-1, self.image_dim)    #6\n        hidden1 = self.softplus(self.fc1(img))     #7\n        hidden2 = self.softplus(self.fc2(hidden1))    #7\n        p_loc = self.sigmoid(self.fc3(hidden2))     #8\n        return p_loc    #8\n\nencoder_n_causal_factors = EncoderCausalFactors( #9\n    image_dim=64*64,   #9\n    factor_dim=sum(CARDINALITY)   #9\n)  #9\n```", "```py\nurl = ('https://github.com/altdeep/causalML/raw/master/'    \n       'book/chapter%209/sprites-model-encoder-causal-factors.pt')    \nresponse = requests.get(url)    \nresponse.raise_for_status()    \nwith open('temp_weights.pt', 'wb') as f:    \n    f.write(response.content)    \nstate_dict = torch.load(    \n    'temp_weights.pt',    \n    map_location=torch.device('cpu')    \n)    \nencoder_n_causal_factors.load_state_dict(state_dict)\n```", "```py\nfrom pyro import distributions as dist\n\ndef decode_one_hot(factor_encoded, cardinality=CARDINALITY):  \n    split = [   \n        torch.split(element, cardinality)     #1\n        for element in factor_encoded   #1\n    ]   #1\n    labels = [[int(torch.argmax(vec)) for vec in item]    #1\n              for item in split]    #1\n    return torch.tensor(labels)   #1\n\ndef sample_one_hot(p_encoded, cardinality=CARDINALITY):     #2\n    split = [torch.split(element, cardinality)    #2\n             for element in p_encoded]  #2\n    sample_list = [   #2\n        [     #2\n            dist.OneHotCategorical(p_vec).sample()    #2\n            for p_vec in item    #2\n        ] for item in split  #2\n    ]     #2\n    sample = torch.stack([    #2\n        torch.cat(samples, -1)     #2\n        for samples in sample_list    #2\n    ])    #2\n    return sample     #2\n\ninferred_cause_p = encoder_n_causal_factors.forward(img)     #3\nsampled_factors = sample_one_hot(     #3\n    inferred_cause_p   #3\n)     #3\nprint(decode_one_hot(sampled_factors))     #3\n```", "```py\nclass EncoderNImage(nn.Module):     #1\n    def __init__(self, image_dim, factor_dim, n_image_dim):\n        super(EncoderNImage, self).__init__()\n        self.image_dim = image_dim\n        self.factor_dim = factor_dim\n        self.n_image_dim = n_image_dim\n        hidden_dim = 1000\n        self.fc1 = nn.Linear(\n            self.image_dim + self.factor_dim, hidden_dim   #2\n        )    #2\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)     #2\n        self.fc31 = nn.Linear(hidden_dim, n_image_dim)     #2\n        self.fc32 = nn.Linear(hidden_dim, n_image_dim)     #2\n        self.softplus = nn.Softplus()    #2\n\n    def forward(self, img, factor):\n        img = img.reshape(-1, self.image_dim)    #3\n        inputs = torch.cat((img, factor), -1)    #4\n        hidden1 = self.softplus(self.fc1(inputs))    #5\n        hidden2 = self.softplus(self.fc2(hidden1))     #5\n        n_image_loc = self.fc31(hidden2)     #6\n        n_image_scale = torch.exp(self.fc32(hidden2))    #6\n        return n_image_loc, n_image_scale    #6\n\nencoder_n_image = EncoderNImage(     #7\n    image_dim=64*64,    #7\n    factor_dim=sum(CARDINALITY),     #7\n    n_image_dim=50     #7\n)     #7\n```", "```py\ndef encode_one_hot(factor, cardinality=CARDINALITY):    \n    new_factor = []    \n    for i, factor_length in enumerate(cardinality):    \n        new_factor.append(    \n            torch.nn.functional.one_hot(    \n                factor[:,i].to(torch.int64), int(factor_length)    \n            )    \n        )    \n    new_factor = torch.cat(new_factor, -1)    \n    return new_factor.to(torch.float32)\n```", "```py\nweight_url = (\"https://github.com/altdeep/causalML/raw/master/\"    #1\n              \"book/chapter%209/sprites-model-encoder-n-image.pt\")   #1\nresponse = requests.get(weight_url)   #1\nresponse.raise_for_status()   #1\nwith open('temp_weights.pt', 'wb') as f:   #1\n    f.write(response.content)   #1\nstate_dict = torch.load(  #1\n    'temp_weights.pt',   #1\n    map_location=torch.device('cpu')   #1\n)    #1\nencoder_n_image.load_state_dict(state_dict)    #1\nn_image_loc, n_image_scale = encoder_n_image.forward(     #2\n    img,    #2\n    encode_one_hot(causal_factor)     #2\n)    #2\nn_image = torch.normal(n_image_loc, n_image_scale)    #3\n```", "```py\nclass Decoder(nn.Module):     #1\n    def __init__(self, image_dim, factor_dim, n_image_dim):\n        super(Decoder, self).__init__()\n        hidden_dim = 1000\n        self.fc1 = nn.Linear(n_image_dim + factor_dim, hidden_dim)     #2\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)   #2\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)   #2\n        self.fc4 = nn.Linear(hidden_dim, image_dim)    #2\n        self.softplus = nn.Softplus()    #2\n        self.sigmoid = nn.Sigmoid()   #2\n\n    def forward(self, n_image, factor):\n        inputs = torch.cat((n_image, factor), -1)     #3\n        hidden1 = self.softplus(self.fc1(inputs))     #4\n        hidden2 = self.softplus(self.fc2(hidden1))     #4\n        hidden3 = self.softplus(self.fc3(hidden2))     #4\n        p_img = self.sigmoid(self.fc4(hidden3))     #5\n        return p_img     #5\n\ndecoder = Decoder(     #6\n    image_dim=64*64,    #6\n    factor_dim=sum(CARDINALITY),   #6\n    n_image_dim=50    #6\n)     #6\n```", "```py\ndcdr_url = (\"https://github.com/altdeep/causalML/raw/master/\"    \n       \"book/chapter%209/sprites-model-decoder.pt\")    \nresponse = requests.get(dcdr_url)    \nresponse.raise_for_status()    \nwith open('temp_weights.pt', 'wb') as f:    \n    f.write(response.content)    \nstate_dict = torch.load(    \n    'temp_weights.pt',    \n    map_location=torch.device('cpu')    \n)    \ndecoder.load_state_dict(state_dict)\n```", "```py\ndef compare_reconstruction(original, generated):    \n    fig = plt.figure()    \n    ax0 = fig.add_subplot(121)    \n    plt.imshow(    \n        original.cpu().reshape(64, 64),    \n        cmap='Greys_r',    \n        interpolation='nearest'    \n    )    \n    plt.axis('off')    \n    plt.title('actual')    \n    ax1 = fig.add_subplot(122)    \n    plt.imshow(    \n        generated.reshape(64, 64),    \n        cmap='Greys_r', interpolation='nearest')    \n    plt.axis('off')    \n    plt.title('counterfactual')    \n    plt.show()\n```", "```py\ndef p_n_image(n_image_params):    #1\n    n_image_loc, n_image_scale, n_unif_upper = n_image_params     #2\n    n_image_norm = dist.Normal(     #3\n        n_image_loc, n_image_scale     #3\n    ).to_event(1).sample()     #3\n    n_image_unif = dist.Uniform(0, n_unif_upper).expand(     #4\n        torch.Size([1, 64*64])    #4\n    ).sample()    #4\n    n_image = n_image_norm, n_image_unif    #5\n    return n_image\n\ndef f_image(factor, n_image):    #6\n    n_image_norm, n_image_unif = n_image     #7\n    p_output = decoder.forward(     #8\n        n_image_norm,     #8\n        encode_one_hot(factor)     #8\n    )\n    sim_img = (n_image_unif <= p_output).int()     #9\n    return sim_img\n```", "```py\ndef abduct(img, factor, smoother=1e-3):     #1\n    n_image_loc, n_image_scale = encoder_n_image.forward(     #2\n        img, encode_one_hot(factor)    #2\n    )   #2\n    n_unif_upper = decoder.forward(     #3\n        n_image_loc,     #3\n        encode_one_hot(factor)     #3\n    )     #3\n    n_unif_upper = n_unif_upper * (1 - 2 * smoother) + smoother     #3\n    p_image_params = n_image_loc, n_image_scale, n_unif_upper     #4\n    return p_image_params\n\ndef do_action(factor, element=1, val=2):     #5\n    intervened_factor = factor.clone()   #5\n    intervened_factor[0][element] = val    #5\n    return intervened_factor     #5\n\ndef predict(intervened_factor, n_image_params):     #6\n    n_image = p_n_image(n_image_params)    #6\n    sim_img = f_image(intervened_factor, n_image)     #6\n    return sim_img    #6\n\ndef counterfactual(img, factor):    #7\n    p_image_params = abduct(img, factor)    #7\n    intervened_factor = do_action(factor)   #7\n    pred_recon = predict(intervened_factor, p_image_params)    #7\n    compare_reconstruction(img, pred_recon)    #7\n\ncounterfactual(img, causal_factor)    #8\n```"]