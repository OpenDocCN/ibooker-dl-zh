# 第三章。快速了解机器学习

在技术领域中，很少有像机器学习和人工智能（AI）周围那样神秘的领域。即使您是另一个领域的经验丰富的工程师，机器学习也可能看起来是一个需要大量先验知识的复杂主题。许多开发人员在开始阅读有关机器学习的内容时会感到沮丧，因为这些解释涉及学术论文、晦涩的 Python 库和高级数学。甚至知道从哪里开始都可能感到令人生畏。

实际上，机器学习很容易理解，任何人都可以通过文本编辑器访问。学习了一些关键思想后，您可以轻松地在自己的项目中使用它。在所有神秘感之下，是一套解决各种问题的有用工具。有时候可能会*感觉*像魔术，但其实只是代码，您不需要博士学位来使用它。

这本书是关于如何在微型设备上使用机器学习的。在本章的其余部分，您将学习所有开始所需的机器学习知识。我们将涵盖基本概念，探索一些工具，并训练一个简单的机器学习模型。我们的重点是微型硬件，因此我们不会花太多时间讨论深度学习背后的理论，或者使其运作的数学。后面的章节将更深入地探讨工具和如何优化嵌入式设备的模型。但是在本章结束时，您将熟悉关键术语，了解一般工作流程，并知道去哪里学习更多。

在本章中，我们涵盖以下内容：

+   机器学习实际上是什么

+   它可以解决的问题类型

+   关键术语和思想

+   使用深度学习解决问题的工作流程，这是机器学习中最流行的方法之一

###### 提示

有许多书籍和课程解释深度学习背后的科学，所以我们不会在这里做这个。尽管如此，这是一个迷人的主题，我们鼓励您去探索！我们在“学习机器学习”中列出了一些我们喜欢的资源。但请记住，您不需要所有的理论来开始构建有用的东西。

# 机器学习实际上是什么

想象您拥有一台制造小部件的机器。有时它会出故障，修复起来很昂贵。也许如果您在机器运行期间收集数据，您可能能够预测何时会出现故障，并在损坏发生之前停止运行。例如，您可以记录其生产速率、温度和振动情况。也许这些因素的某种组合表明即将出现问题。但是您如何找出呢？

这是机器学习旨在解决的问题类型的示例。从根本上讲，机器学习是一种利用计算机根据过去观察来预测事物的技术。我们收集有关工厂机器性能的数据，然后创建一个计算机程序来分析这些数据，并用它来预测未来状态。

创建机器学习程序与编写代码的传统过程不同。在传统软件中，程序员设计一个算法，该算法接受输入，应用各种规则，并返回输出。程序员计划算法的内部操作，并通过代码行明确实现。要预测工厂机器的故障，程序员需要了解数据中哪些测量值表示问题，并编写代码来有意识地检查它们。

这种方法对许多问题都有效。例如，我们知道水在海平面上沸腾的温度是 100°C，因此可以轻松编写一个程序，根据当前温度和海拔高度来预测水是否正在沸腾。但在许多情况下，很难知道哪些因素的确切组合预测了给定状态。继续以我们的工厂机器示例为例，可能有各种不同的生产速率、温度和振动水平的组合可能表明问题，但从数据中看不出来。

为了创建一个机器学习程序，程序员将数据输入到一种特殊类型的算法中，让算法发现规则。这意味着作为程序员，我们可以创建基于复杂数据的预测程序，而不必完全理解所有复杂性。机器学习算法基于我们提供的数据构建系统的*模型*，通过我们称之为*训练*的过程。模型是一种计算机程序。我们通过这个模型运行数据来进行预测，这个过程称为*推理*。

机器学习有许多不同的方法。其中最流行的之一是*深度学习*，它基于人类大脑可能如何工作的简化想法。在深度学习中，一组模拟神经元（由数字数组表示）被训练来模拟各种输入和输出之间的关系。不同的*架构*或模拟神经元的排列对不同的任务很有用。例如，一些架构擅长从图像数据中提取含义，而其他架构最适合预测序列中的下一个值。

本书中的示例侧重于深度学习，因为它是解决适合微控制器的问题类型的灵活且强大的工具。也许令人惊讶的是，深度学习甚至可以在内存和处理能力有限的设备上运行。事实上，在本书的过程中，您将学习如何创建一些非常惊人的深度学习模型，但这些模型仍然符合微型设备的限制。

下一节解释了创建和使用深度学习模型的基本工作流程。

# 深度学习工作流程

在前一节中，我们概述了使用深度学习来预测工厂机器何时可能会发生故障的场景。在本节中，我们介绍了使这一情况发生所需的工作。

这个过程将涉及以下任务：

1.  确定目标

1.  收集数据集

1.  设计模型架构

1.  训练模型

1.  转换模型

1.  运行推理

1.  评估和故障排除

让我们逐一走过它们。

## 确定目标

当您设计任何类型的算法时，重要的是首先明确您希望它做什么。机器学习也不例外。您需要决定您想要预测什么，以便确定要收集哪些数据以及使用哪种模型架构。

在我们的例子中，我们想要预测我们的工厂机器是否即将发生故障。我们可以将这表示为一个*分类*问题。分类是一个机器学习任务，它接受一组输入数据，并返回这些数据符合一组已知*类别*的概率。在我们的例子中，我们可能有两个类别：“正常”，表示我们的机器正常运行，没有问题，“异常”，表示我们的机器显示出可能很快会发生故障的迹象。

这意味着我们的目标是创建一个将我们的输入数据分类为“正常”或“异常”的模型。

## 收集数据集

我们的工厂可能有大量可用数据，从机器的运行温度到某一天食堂提供的食物类型。鉴于我们刚刚建立的目标，我们可以开始确定我们需要的数据。

### 选择数据

深度学习模型可以学会忽略嘈杂或无关的数据。也就是说，最好只使用与解决问题相关的信息来训练模型。由于今天的食堂食物不太可能影响我们机器的运行，我们可能可以将其从数据集中排除。否则，模型将需要学会否定那些无关的输入，并且可能容易学习到虚假的关联——也许我们的机器总是在提供比萨的日子出故障。

在决定是否包含数据时，您应该始终尝试将领域专业知识与实验相结合。您还可以使用统计技术来尝试识别哪些数据是重要的。如果您仍然不确定是否包含某个数据源，您可以始终训练两个模型，看哪个效果最好！

假设我们已经确定了最有前途的数据为*生产速率*、*温度*和*振动*。我们的下一步是收集一些数据，以便我们可以训练一个模型。

###### 提示

选择的数据在您想要进行预测时也是可用的非常重要。例如，由于我们决定用温度读数训练我们的模型，当我们进行推理时，我们将需要提供来自完全相同物理位置的温度读数。这是因为模型学习了如何理解其输入如何预测其输出。如果我们最初在机器内部的温度数据上训练模型，那么在当前室温上运行模型可能不起作用。

### 收集数据

要训练有效的模型需要多少数据是很难确定的。这取决于许多因素，例如变量之间的关系复杂性、噪音量以及类别之间的区分程度。然而，有一个经验法则始终成立：数据越多，越好！

你应该努力收集代表系统中可能发生的所有条件和事件的数据。如果我们的机器可能以几种不同的方式出现故障，我们应该确保捕获每种类型故障周围的数据。如果一个变量随着时间自然变化，收集代表整个范围的数据是很重要的。例如，如果机器在温暖的日子里温度升高，你应该确保包括冬天和夏天的数据。这种多样性将帮助你的模型代表每种可能的情况，而不仅仅是一些特定的情况。

我们收集关于工厂的数据可能会被记录为一组*时间序列*，意味着定期收集的一系列读数。例如，我们可能每分钟记录一次温度，每小时记录一次生产速率，每秒记录一次振动水平。在收集数据后，我们需要将这些时间序列转换为适合我们模型的形式。

### 标记数据

除了收集数据，我们还需要确定哪些数据代表“正常”和“异常”操作。我们将在训练过程中提供这些信息，以便我们的模型学习如何对输入进行分类。将数据与类别相关联的过程称为*标记*，而“正常”和“异常”类别是我们的*标签*。

###### 注意

这种训练方式，即在训练期间指导算法数据的含义，被称为*监督学习*。生成的分类模型将能够处理传入的数据并预测其可能属于哪个类别。

为了标记我们收集到的时间序列数据，我们需要记录机器工作和故障的时间段。我们可能会假设机器故障前的时间段通常代表异常操作。然而，由于我们不能从数据的表面看出异常操作，正确地获取这些信息可能需要一些实验！

在我们决定如何标记数据之后，我们可以生成一个包含标签的时间序列，并将其添加到我们的数据集中。

### 我们的最终数据集

表 3-1 列出了我们在工作流程中此刻已经收集的数据源。

表 3-1. 数据源

| 数据源 | 间隔 | 样本读数 |
| --- | --- | --- |
| 生产速率 | 每 2 分钟一次 | 100 个单位 |
| 温度 | 每分钟一次 | 30°C |
| 振动（典型值的百分比） | 每 10 秒一次 | 23% |
| 标签（“正常”或“异常”） | 每 10 秒一次 | 正常 |

表格显示了每个数据源的时间间隔。例如，温度每分钟记录一次。我们还生成了一个包含数据标签的时间序列。我们的标签间隔是每 10 秒 1 次，与其他时间序列的最小间隔相同。这意味着我们可以轻松确定数据中每个数据点的标签。

现在我们已经收集了数据，是时候用它来设计和训练模型了。

## 设计模型架构

有许多类型的深度学习模型架构，旨在解决各种问题。在训练模型时，您可以选择设计自己的架构或基于研究人员开发的现有架构。对于许多常见问题，您可以在网上找到免费的预训练模型。

在本书的过程中，我们将向您介绍几种不同的模型架构，但除了这里介绍的内容外，还有大量可能性。设计模型既是一门艺术也是一门科学，模型架构是一个重要的研究领域。每天都会有新的架构被发明。

在决定架构时，您需要考虑您试图解决的问题类型、您可以访问的数据类型以及在将数据馈送到模型之前可以对数据进行的转换方式（我们将很快讨论数据转换）。事实上，由于最有效的架构取决于您正在处理的数据类型，您的数据和模型架构是紧密相连的。尽管我们在这里分开标题介绍它们，但它们总是会被一起考虑。

您还需要考虑将在其上运行模型的设备的约束，因为微控制器通常具有有限的内存和较慢的处理器，较大的模型需要更多内存并需要更长时间运行 - 模型的大小取决于它包含的神经元数量以及这些神经元的连接方式。此外，一些设备配备了硬件加速功能，可以加快某些类型的模型架构的执行速度，因此您可能希望根据您考虑的设备的优势来定制您的模型。

在我们的情况下，我们可能首先通过几层神经元训练一个简单模型，然后通过迭代过程优化架构，直到获得有用的结果。您将在本书的后面看到如何做到这一点。

深度学习模型接受输入并生成*张量*形式的输出。对于本书的目的，¹ 张量本质上是一个可以包含数字或其他张量的列表；您可以将其视为类似于数组。我们的假设简单模型将以张量作为输入。以下小节描述了我们如何将数据转换为这种形式。

### 从数据生成特征

我们已经确定我们的模型将接受某种张量作为输入。但正如我们之前讨论的，我们的数据以时间序列的形式呈现。我们如何将时间序列数据转换为可以传递到模型中的张量呢？

我们现在的任务是决定如何从我们的数据中生成特征。在机器学习中，术语*特征*指的是模型训练的特定类型信息。不同类型的模型训练在不同类型的特征上。例如，一个模型可能接受一个单一标量值作为其唯一输入特征。

但是输入可能比这更复杂：一个设计用于处理图像的模型可能接受一个多维张量的图像数据作为输入，而一个设计用于基于多个特征进行预测的模型可能接受一个包含多个标量值的向量，每个特征对应一个值。

回想一下，我们决定我们的模型应该使用生产速率、温度和振动来进行预测。以它们的原始形式，作为具有不同间隔的时间序列，这些数据将不适合传递给模型。下一节将解释原因。

#### 窗口化

在下图中，我们的时间序列中的每个数据都用一个星号表示。当前标签包含在数据中，因为标签是训练所必需的。我们的目标是训练一个模型，可以根据当前条件在任何给定时刻预测机器是正常运行还是异常运行：

```py
Production:    *                       *            (every 2 minutes)
Temperature:   *           *           *            (every minute)
Vibration:     * * * * * * * * * * * * * * * * *    (every 10 seconds)
Label:         * * * * * * * * * * * * * * * * *    (every 10 seconds)
```

然而，由于我们的时间序列具有不同的间隔（比如每分钟一次，或者每 10 秒一次），如果我们只传入给定时刻可用的数据，可能不包括我们可用的所有数据类型。例如，在下图中突出显示的时刻，只有振动数据可用。这意味着我们的模型在尝试进行预测时只有振动信息：

```py
                                              ┌─┐
Production:    *                       *      │ │
Temperature:   *           *           *      │ │
Vibration:     * * * * * * * * * * * * * * * *│*│
Label:         * * * * * * * * * * * * * * * *│*│
                                              └─┘
```

解决这个问题的一个方法可能是选择一个时间窗口，并将该窗口内的所有数据合并为一组数值。例如，我们可以决定使用一个一分钟的时间窗口，并查看其中包含的所有数值：

```py
                                    ┌───────────┐
Production:    *                    │  *        │
Temperature:   *           *        │  *        │
Vibration:     * * * * * * * * * * *│* * * * * *│
Label:         * * * * * * * * * * *│* * * * * *│
                                    └───────────┘
```

如果我们对每个时间序列的窗口中的所有值求平均，并对当前窗口中缺少数据点的任何值取最近的值，我们最终得到一组单一值。我们可以根据窗口中是否存在任何“异常”标签来决定如何标记这个快照。如果窗口中有任何“异常”存在，窗口应该被标记为“异常”。如果没有，应该被标记为“正常”：

```py
                                    ┌───────────┐
Production:    *                    │  *        │  Average: 102
Temperature:   *           *        │  *        │  Average: 34°C
Vibration:     * * * * * * * * * * *│* * * * * *│  Average: 18%
Label:         * * * * * * * * * * *│* * * * * *│  Label:   "normal"
                                    └───────────┘
```

这三个非标签数值是我们的特征！我们可以将它们作为一个向量传递给我们的模型，每个时间序列有一个元素：

```py
[102 34 .18]
```

在训练过程中，我们可以为每 10 秒的数据计算一个新的窗口，并将其传递给我们的模型，使用标签来通知训练算法我们期望的输出。在推断过程中，每当我们想要使用模型来预测异常行为时，我们只需查看我们的数据，计算最近的窗口，将其通过模型运行，并接收一个预测。

这是一个简单的方法，实际上可能并不总是有效，但这是一个很好的起点。您很快会发现，机器学习就是试错的过程！

在我们继续训练之前，让我们再谈一下关于输入数值的最后一点。

#### 归一化

通常，您向神经网络提供的数据将以填充有*浮点*值或*浮点数*的张量形式呈现。浮点数是一种用于表示具有小数点的数字的数据类型。为了让训练算法有效地工作，这些浮点数值需要在大小上相似。事实上，如果所有数值都表示为 0 到 1 范围内的数字，那将是理想的。

让我们再次看一下上一节中的输入张量：

```py
[102 34 .18]
```

这些数值在非常不同的尺度上：温度超过 100，而振动表示为 1 的分数。为了将这些值传递给我们的网络，我们需要对它们进行*归一化*，使它们都在一个类似的范围内。

一种方法是计算数据集中每个特征的平均值，并从值中减去。这样做的效果是将数字压缩到接近零。这里有一个例子：

```py
Temperature series:
[108 104 102 103 102]

Mean:
103.8

Normalized values, calculated by subtracting 103.8 from each temperature:
[ 4.2 0.2 -1.8 -0.8 -1.8 ]
```

您经常会遇到归一化的情况之一是当图像被输入神经网络时，以不同的方式实现。计算机通常将图像存储为 8 位整数的矩阵，其值范围从 0 到 255。为了使这些值归一化，使它们都在 0 到 1 之间，每个 8 位值都乘以`1/255`。这里有一个示例，其中包含一个 3×3 像素的灰度图像，其中每个像素的值表示其亮度：

```py
Original 8-bit values:
[[255 175 30]
 [0   45  24]
 [130 192 87]]

Normalized values:
[[1\.         0.68627451 0.11764706]
 [0\.         0.17647059 0.09411765]
 [0.50980392 0.75294118 0.34117647]]
```

### 用机器学习思考

到目前为止，我们已经学会了如何开始用机器学习解决问题。在我们的工厂场景中，我们已经决定了一个合适的目标，收集和标记了适当的数据，设计了要传递到模型中的特征，并选择了一个模型架构。无论我们试图解决什么问题，我们都会使用相同的方法。重要的是要注意，这是一个迭代过程，我们经常在 ML 工作流程的各个阶段之间来回，直到我们找到一个有效的模型，或者决定任务太困难。

例如，想象一下我们正在构建一个预测天气的模型。我们需要决定我们的目标（例如，预测明天是否会下雨），收集和标记数据集（例如过去几年的天气报告），设计我们将传递给模型的特征（也许是过去两天的平均条件），并选择适合这种数据类型和我们要运行的设备的模型架构。我们会想出一些初始想法，测试它们，并调整我们的方法，直到获得良好的结果。

我们工作流程中的下一步是训练，我们将在以下部分中探讨。

## 训练模型

训练是模型学习为给定的输入集合产生正确输出的过程。它涉及将训练数据输入模型，并对其进行小的调整，直到它能够做出最准确的预测。

正如我们之前讨论的，模型是由排列成层的数字数组表示的模拟神经元网络。这些数字被称为*权重*和*偏置*，或者统称为网络的*参数*。

当数据被输入网络时，它会通过每一层中的权重和偏置进行连续的数学运算进行转换。模型的输出是通过这些操作运行输入的结果。图 3-1 显示了一个具有两层的简单网络。

模型的权重从随机值开始，偏置通常从值 0 开始。在训练过程中，将数据的*批次*输入模型，并将模型的输出与期望输出（在我们的情况下是正确的标签“正常”或“异常”）进行比较。一种称为*反向传播*的算法逐渐调整权重和偏置，以使随着时间的推移，模型的输出越来越接近期望值。训练以*周期*（意味着迭代）来衡量，直到我们决定停止为止。

![一个简单的深度学习网络](img/timl_0301.png)

###### 图 3-1。一个具有两层的简单深度学习网络

通常情况下，当一个模型的性能停止改善时，我们会停止训练。当它开始做出准确的预测时，就说它已经*收敛*。为了确定一个模型是否已经收敛，我们可以分析其在训练过程中的性能图表。两个常见的性能指标是*损失*和*准确性*。损失指标给出了一个数值估计，表明模型离产生预期答案有多远，而*准确性*指标告诉我们它选择正确预测的百分比。一个完美的模型将具有 0.0 的损失和 100%的准确性，但真实模型很少是完美的。

图 3-2 显示了深度学习网络在训练过程中的损失和准确性。您可以看到随着训练的进行，准确性增加，损失减少，直到达到一个模型不再改善的点。

为了尝试改善模型的性能，我们可以改变模型的架构，调整用于设置模型和调节训练过程的各种值。这些值被统称为*超参数*，它们包括诸如要运行的训练周期数和每个层中的神经元数等变量。每次我们进行更改时，我们可以重新训练模型，查看指标，并决定是否进一步优化。希望，时间和迭代将产生一个具有可接受准确性的模型！

![显示训练过程中模型收敛的图表](img/timl_0302.png)

###### 图 3-2。显示训练过程中模型收敛的图表

###### 注意

重要的是要记住，并没有保证你能够达到足够好的准确性来解决你正在尝试解决的问题。数据集中并不总是包含足够的信息来进行准确的预测，有些问题甚至无法解决，即使使用最先进的深度学习技术也不行。也就是说，即使模型不是 100%准确，它也可能是有用的。在我们的工厂示例中，即使只能部分时间预测异常操作也可能会大有帮助。

### 欠拟合和过拟合

模型无法收敛的两个最常见原因是*欠拟合*和*过拟合*。

神经网络学习*拟合*其在数据中识别的模式。如果一个模型被正确拟合，它将为给定的输入产生正确的输出。当一个模型*欠拟合*时，它还没有能够学习到足够强的这些模式的表示形式，以便能够做出良好的预测。这可能由于各种原因导致，最常见的是架构太小，无法捕捉应该建模的系统的复杂性，或者没有足够的数据进行训练。

当一个模型*过拟合*时，它已经对其训练数据学习得太好了。模型能够准确预测其训练数据的细微之处，但它无法将其学习推广到以前没有见过的数据。通常情况下，这是因为模型已经完全记住了训练数据，或者它已经学会依赖于训练数据中存在但现实世界中不存在的一种捷径。

例如，想象一下，你正在训练一个模型来将照片分类为包含狗或猫。如果你的训练数据中所有的狗照片都是在室外拍摄的，而所有的猫照片都是在室内拍摄的，你的模型可能会学会作弊，并利用每张照片中天空的存在来预测是哪种动物。这意味着如果未来的狗自拍照片恰好是在室内拍摄的话，它可能会错误分类。

有许多方法来对抗过拟合。一种可能性是减小模型的大小，使其没有足够的容量来学习其训练集的精确表示。一组称为*正则化*的技术可以在训练过程中应用，以减少过拟合的程度。为了充分利用有限的数据，可以使用一种称为*数据增强*的技术，通过切片和切块现有数据来生成新的人工数据点。但是，打败过拟合的最佳方法，如果可能的话，是获得一个更大更多样化的数据集。更多的数据总是有帮助的！

### 训练、验证和测试

要评估模型的性能，我们可以看看它在训练数据上的表现如何。然而，这只告诉我们故事的一部分。在训练过程中，模型学会尽可能紧密地拟合其训练数据。正如我们之前看到的，在某些情况下，模型将开始过拟合训练数据，这意味着它在训练数据上表现良好，但在现实生活中却不行。

要了解何时发生这种情况，我们需要使用新数据*验证*模型，这些数据在训练中没有使用。将数据集分成三部分——*训练*、*验证*和*测试*是常见的。典型的分割是 60%的训练数据，20%的验证数据和 20%的测试数据。这种分割必须这样做，以便每个部分包含相同的信息分布，并以保持数据结构的方式进行。例如，由于我们的数据是时间序列，我们可以将其潜在地分成三个连续的时间段。如果我们的数据不是时间序列，我们可以随机抽样数据点。

在训练过程中，*训练*数据集用于训练模型。定期，来自*验证*数据集的数据被馈送到模型中，并计算损失。因为模型以前没有见过这些数据，所以它的损失分数是模型表现的更可靠指标。通过比较训练和验证损失（以及准确性，或其他可用的指标），您可以看到模型是否过拟合。

图 3-3 显示了一个过拟合的模型。您可以看到随着训练损失的降低，验证损失却上升了。这意味着模型在预测训练数据方面变得更好，但失去了对新数据的泛化能力。

![显示模型在训练过程中过拟合的图表](img/timl_0303.png)

###### 图 3-3. 显示模型在训练过程中过拟合的图表

当我们调整我们的模型和训练过程以提高性能并避免过拟合时，我们希望看到我们的验证指标得到改善。

然而，这个过程有一个不幸的副作用。通过优化以改善验证指标，我们可能只是在推动模型朝着过拟合训练数据*和*验证数据的方向！我们所做的每一个调整都会使模型稍微更好地适应验证数据，最终，我们可能会遇到与之前相同的过拟合问题。

为了验证这种情况没有发生，我们在训练模型的最后一步是在我们的*测试*数据上运行它，并确认它的表现与验证期间一样好。如果没有，我们已经优化了我们的模型以过拟合我们的训练和验证数据。在这种情况下，我们可能需要回到起点，提出一个新的模型架构，因为如果我们继续调整以提高在测试数据上的表现，我们也会过拟合到那里。

当我们有一个在训练、验证和测试数据上表现良好的模型后，这个过程的训练部分就结束了。接下来，我们准备好在设备上运行我们的模型！

## 转换模型

在整本书中，我们使用 TensorFlow 来构建和训练模型。一个 TensorFlow 模型本质上是一组指令，告诉一个*解释器*如何转换数据以产生输出。当我们想要使用我们的模型时，我们只需将其加载到内存中，并使用 TensorFlow 解释器执行它。

然而，TensorFlow 的解释器是设计用于在强大的台式计算机和服务器上运行模型的。由于我们将在微型微控制器上运行我们的模型，我们需要一个专为我们的用例设计的不同解释器。幸运的是，TensorFlow 提供了一个解释器和相关工具，用于在小型、低功耗设备上运行模型。这套工具称为 TensorFlow Lite。

在 TensorFlow Lite 可以运行模型之前，首先必须将其转换为 TensorFlow Lite 格式，然后保存到磁盘上作为文件。我们使用一个名为*TensorFlow Lite Converter*的工具来完成这个过程。转换器还可以应用特殊优化，旨在减小模型的大小并帮助其运行更快，通常不会牺牲性能。

在第十三章中，我们深入探讨了 TensorFlow Lite 的细节以及它如何帮助我们在微小设备上运行模型。目前，你只需要知道你需要转换你的模型，并且转换过程快速简单。

## 运行推断

模型转换后，就可以部署了！我们现在将使用 TensorFlow Lite for Microcontrollers C++ 库来加载模型并进行预测。

由于这是我们的模型与应用代码相遇的部分，我们需要编写一些代码，从传感器获取原始输入数据并将其转换为模型训练的相同形式。然后，我们将这些转换后的数据传递给我们的模型并运行推断。

这将导致包含预测的输出数据。在我们的分类器模型的情况下，输出将是每个类别“正常”和“异常”的得分。对于分类数据的模型，通常所有类别的得分将总和为 1，得分最高的类别将是预测。得分之间的差异越大，对预测的置信度就越高。表 3-2 列出了一些示例输出。

表 3-2\. 示例输出

| 正常得分 | 异常得分 | 解释 |
| --- | --- | --- |
| 0.1 | 0.9 | 在异常状态下有高置信度 |
| 0.9 | 0.1 | 在正常状态下有高置信度 |
| 0.7 | 0.3 | 对正常状态有轻微置信度 |
| 0.49 | 0.51 | 结果不确定，因为两种状态都没有明显领先 |

在我们的工厂机器示例中，每个单独的推断仅考虑数据的一个快照——它告诉我们在过去 10 秒内出现异常状态的概率，基于各种传感器读数。由于现实世界的数据通常混乱，机器学习模型并不完美，因此可能会出现临时故障导致错误分类的情况。例如，由于临时传感器故障，我们可能会看到温度值的突然上升。这种瞬态、不可靠的输入可能导致输出分类短暂地不符合现实。

为了防止这些瞬时故障导致问题，我们可能会在一段时间内对模型的所有输出取平均值。例如，我们可以每 10 秒在当前数据窗口上运行我们的模型，并取最后 6 个输出的平均值，以给出每个类别的平滑得分。这意味着瞬时问题被忽略，我们只对一致的行为采取行动。我们使用这种技术来帮助唤醒词检测在第七章。

在为每个类别得分后，由我们的应用代码决定如何处理。也许如果连续检测到异常状态一分钟，我们的代码将发送信号关闭机器并通知维护团队。

## 评估和故障排除

在我们部署了模型并在设备上运行后，我们将开始看到其真实世界的性能是否达到我们的期望。即使我们已经证明我们的模型在测试数据上做出了准确的预测，但在实际问题上的表现可能会有所不同。

出现这种情况可能有很多原因。例如，训练中使用的数据可能并不完全代表实际操作中可用的数据。也许由于当地气候，我们机器的温度通常比我们收集数据集的那台机器要凉爽。这可能会影响我们模型的预测，使其不再像预期的那样准确。

另一个可能性是我们的模型可能已经过度拟合我们的数据集，而我们没有意识到。在“训练模型”中，我们学到了当数据集恰好包含模型可以学习识别的额外信号时，这种情况可能会发生。

如果我们的模型在生产中不起作用，我们需要进行一些故障排除。首先，我们排除可能影响到达我们模型的数据的任何硬件问题（如故障传感器或意外噪音）。其次，我们从部署模型的设备中捕获一些数据，并将其与我们的原始数据集进行比较，以确保它在同一范围内。如果不是，也许环境条件或传感器特性存在差异，而我们没有预料到。如果数据检查通过，可能过拟合是问题所在。

在排除硬件问题后，解决过拟合问题的最佳方法通常是使用更多数据进行训练。我们可以从部署的硬件中捕获额外的数据，将其与原始数据集结合起来，然后重新训练我们的模型。在这个过程中，我们可以应用正则化和数据增强技术，以帮助充分利用我们拥有的数据。

要达到良好的实际性能，有时可能需要对模型、硬件和相关软件进行一些迭代。如果遇到问题，要像处理其他技术问题一样对待。采用科学方法进行故障排除，排除可能的因素，并分析数据以找出问题所在。

# 总结

现在您已经熟悉了机器学习从业者使用的基本工作流程，我们准备在 TinyML 冒险中迈出下一步。

在第四章中，我们将构建我们的第一个模型并将其部署到一些微型硬件上！

这个对于“张量”一词的定义与数学和物理学对该词的定义不同，但在数据科学中已成为常态。
