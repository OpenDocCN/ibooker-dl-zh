<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="ch__audio"> <span class="chapter-title-numbering"><span class="num-string">6</span></span> <span class="title-text"> Analyzing images and videos</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li id="p2">Analyzing images</li> 
    <li id="p3">Comparing images</li> 
    <li id="p4">Analyzing videos</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>In the previous chapters, we have seen how to analyze text and structured data. Does that cover everything? Not even close! By far, the largest portion of data out there comes in the form of images and videos. For instance, videos alone account for an impressive two-thirds of the total data volume exchanged over the internet! In this chapter, we will see how language models can also help us extract useful insights from such data types.</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>The following sections introduce a couple of small projects that process images and video data. GPT-4o is a natively multimodal model; we can use it for all these tasks. First, we will see how to use GPT-4o to answer free-form questions (in natural language) about images. Second, we will use GPT-4o to build an automated picture-tagging application, automatically tagging our holiday pictures with the people who appear in them.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Finally, we will use GPT-4o to automatically generate titles for video files. The goal of these mini-projects is to illustrate features for visual data processing offered by the latest generation of large language models. After working through those projects, you should be able to build your own applications for image and video data processing in various scenarios.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h2 class=" readable-text-h2" id="sec__videosetup"><span class="num-string browsable-reference-id">6.1</span> Setup</h2> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>You will need to install one more Python package to run the example code. Specifically, you need OpenCV, a library for image processing. In the terminal, run the following command to install OpenCV:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p10"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install opencv-python==4.8.1.78</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>We will use this library to, for example, read images from disk and split videos into frames.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Next, you need to install one more library, enabling you to send requests directly to OpenAI’s web services (which you will use to send pictures stored locally to OpenAI):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p13"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install requests==2.31.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>Well done! If you didn’t encounter any error messages running these commands, your system is now configured for image and video data analysis using GPT-4o. Let’s start with our first project in the next section.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h2 class=" readable-text-h2" id="answering-questions-about-images"><span class="num-string browsable-reference-id">6.2</span> Answering questions about images</h2> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Neural networks for detecting objects (such as cars) in images have been around for many years. So what’s the big deal about processing images with GPT-4o?</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>The primary limitation of classical models for image processing is that they need to be trained for specific analysis tasks. For example, let’s say you have a neural network that is really good at detecting pictures of cats. You can use that to filter out cat pictures from your personal collection. However, maybe you’re not into cats in general but are specifically interested in golden Persian cats. Unless your model is trained to detect that specific type of cat, you’re out of luck and need to label enough example pictures yourself. Doing that is tedious, and you may end up not using the model and instead going through the pictures by hand. The big deal about image processing with GPT-4o (and similar models) is that it solves a wide range of tasks with images based on just a description of the task (in natural language).</p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>We will use that to build a generic question-answering system for images. As a user, you will formulate arbitrary questions in natural language and point to a picture, and the system will generate a text answer. For example, asking the system to detect “golden Persian cats” in a picture should work out of the box without needing task-specific training data.</p> 
  </div> 
  <div class="readable-text" id="p19"> 
   <h3 class=" readable-text-h3" id="specifying-multimodal-input"><span class="num-string browsable-reference-id">6.2.1</span> Specifying multimodal input</h3> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>In this section, we will create a system that takes two inputs:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p21">A URL leading to an image on the web</li> 
   <li class="readable-text" id="p22">A natural language question about the image</li> 
  </ul> 
  <div class="readable-text" id="p23"> 
   <p>The output is an answer to the question (as text). Internally, the system uses GPT-4o to process the question on the input image. It generates multimodal prompts, combining multiple types of data (here, text and images). Figure <a href="#fig__vqaprompt">6.1</a> shows an example prompt: it contains one image (of an apple) and a question about the image (whether the image shows a banana). The correct answer is “No” in this case.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p24">  
   <img alt="figure" src="../Images/CH06_F01_Trummer.png" width="750" height="623"/> 
   <h5 class=" figure-container-h5" id="fig__vqaprompt"><span class="num-string">Figure <span class="browsable-reference-id">6.1</span></span> Multimodal prompt containing an image and text. The prompt instructs the language model to decide whether the picture shows a banana. In this case, the expected output is “No” (otherwise “Yes”).</h5>
  </div> 
  <div class="readable-text" id="p25"> 
   <p>How can we create such prompts for GPT-4o? We can reuse the chat completions endpoint for that. As a reminder, this endpoint takes as input a list of prior messages exchanged between the user and (potentially) the system. For our visual question-answering system, we only need a single message (that originates from the user).</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>Unlike the prior code, messages can now contain multimodal content. In this specific case, this content consists of one text snippet (the question asked by the user) and one image (specified as a URL for the moment). This is the message we will use in the following code (<code>question</code> is a variable containing the question text, and <code>image_url</code> is the URL to the image):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p27"> 
   <div class="code-area-container"> 
    <pre class="code-area">{'role':'user', 'content':[
    {'type':'text', 'text':question},  #1
    {'type':'image_url', 'image_url':{
        'url':image_url}}          #2
    ]
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 Question text
     <br/>#2 Image URL
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>First, note the <code>role</code> attribute identifying the message as generated by the user. Second, the message content is specified as a list of Python dictionaries. Each of those dictionaries describes one element of the message. As we are now considering multimodal data—that is, images and text—we need to clarify the type (or <em>modality</em>) of each input element. This is accomplished by setting the <code>type</code> attribute to either <code>text</code> or <code>image_url</code>. The actual content is specified using either the <code>text</code> attribute (<strong class="cueball">1</strong>) or (in the case of an image) the <code>image_url</code> attribute (<strong class="cueball">2</strong>). GPT-4o is flexible enough to understand that the question refers to the image and to process both appropriately.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p29"> 
   <p> <span class="print-book-callout-head">Tip</span> Whereas the input contains a single picture, the content of a message may contain multiple elements of the same type: for example, multiple images. We will exploit that capability for the project in the next section. </p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <h3 class=" readable-text-h3" id="code-discussion"><span class="num-string browsable-reference-id">6.2.2</span> Code discussion</h3> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>The following listing shows the complete code for our visual question-answering system. Taking the image URL and a question as input (<strong class="cueball">3</strong>), the actual magic (i.e., visual question-answering) happens in the <code>analyze_image</code> function (<strong class="cueball">1</strong>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p32"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__visualqa"><span class="num-string">Listing <span class="browsable-reference-id">6.1</span></span> Answering questions about images via language models</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import openai
import time

client = openai.OpenAI()

def analyze_image(image_url, question):       #1
    """ Use language model to answer questions about image.
    
    Args:
        image_url: URL leading to image.
        question: question about image.
    
    Returns:
        Answer generated by the language model.
    """
    for nr_retries in range(1, 4):
        try:
            response = client.chat.completions.create(
                model='gpt-4o',
                messages=[                  #2
                    {'role':'user', 'content':[
                        {'type':'text', 'text':question}, 
                        {'type':'image_url', 'image_url':{
                            'url':image_url
                            }
                        }]
                    }]
                )
            return response.choices[0].message.content
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')

if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()                   #3
    parser.add_argument('imageurl', type=str, help='URL to image')
    parser.add_argument('question', type=str, help='Question about image')
    args = parser.parse_args()
    
    answer = analyze_image(args.imageurl, args.question)
    print(answer)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Answers question about an image
     <br/>#2 Multimodal content
     <br/>#3 Input parameters
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>As you see, all it takes is a few lines of Python code to answer questions about images! The function <code>analyze_image</code> (<strong class="cueball">1</strong>) contains but a single call to GPT-4o, using the message described in the previous subsection (<strong class="cueball">2</strong>). The fact that we now provide multimodal input does not change the format of the answer. Again, we get an object containing a message generated by the language model. Although the input may now be multimodal, the output is text. As we instructed the language model to generate an answer to the input question (<strong class="cueball">3</strong>) (i.e., exactly what the user is looking for), the output is directly printed out for the user.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <h3 class=" readable-text-h3" id="trying-it-out"><span class="num-string browsable-reference-id">6.2.3</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>Time to test our visual question-answering system! You’re back at Banana (a producer of various consumer electronics, including laptops and smartphones, introduced in chapter 2) and looking for a new company logo. You want to base your logo on a picture of a banana. Searching the web, you find large repositories with images of fruit. But which of them are bananas? Instead of going through images by hand, you would much rather delegate that task to a language model. Luckily, you can directly use the code from the previous section by specifying the URL of each fruit picture, together with the question “Is this a banana (“Yes”,“No”)?” This means you’re using the visual question-answering system essentially as a classification method (which is only one of many possible use cases). You can then write a simple script, iterating over all relevant URLs and retaining the ones where the answer is “Yes.”</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>On the book’s companion website, you will find the code from listing <a href="#code__visualqa">6.1</a> as well as pictures of fruit (look for the links labeled Fruit 1 to Fruit 5). Download the code, change to the containing repository in the terminal, and run the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p37"> 
   <div class="code-area-container"> 
    <pre class="code-area">python [URL] 'Is this a banana ("Yes","No")?'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>In this command, replace <code>[URL]</code> with the URL of the image (you can obtain a suitable URL by, for example, copying the Fruit 1 link on the book’s website).</p> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Object classification is relatively easy, particularly for objects as common as bananas. So you should see accurate results for most examples. Try a few different fruits and possibly other images of your choice. The range of questions you can ask is virtually unlimited (putting aside the rather generous input length limit of 128,000 tokens, about 300 pages of text).</p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>A word of caution may be in order when it comes to processing costs. Processing images via GPT-4o can be expensive! The precise cost depends on the image size and the degree of detail used for image processing. You can control the degree of precision using the <code>detail</code> parameter. For instance, choose a low degree of precision using the following specification of image URLs (in the model input):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p41"> 
   <div class="code-area-container"> 
    <pre class="code-area">{'type':'image_url', 'image_url':{'url':image_url, 'detail':'low'}}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Set the <code>detail</code> attribute to <code>low</code> to pay a cost equivalent to 85 tokens per image (i.e., the cost equivalent of processing a text with 85 tokens using GPT-4o). If you set the degree of detail to <code>high</code> (the default), the cost consists of a fixed amount of 85 tokens and a variable amount that depends on the image size. To calculate the variable cost component, we first scale the image to a size (in pixels) of 2,048 × 2,048 (while maintaining the aspect ratio). This scaling step only applies to pictures with a size beyond 2,048 × 2,048 pixels. The second scaling step is performed in any case. It scales the shorter side of the image to a size of 768 pixels. Now consider the minimum number of 512 × 512 pixel squares needed to cover the image after the second scaling step. The variable cost component is proportional to the number of squares multiplied by a factor of 170 tokens (the cost per square, set by OpenAI).</p> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>For instance, let’s say we want to process an image of size 1,024 × 1,024 pixels with high precision. In that case, we can skip the first scaling step, as the image still fits within a 2,048 × 2,048 pixel square. The second scaling step, however, is performed in any case. It scales the image to a size of 768 × 768 pixels. To cover a square with a length of 768 pixels on both sides, we require four squares of size 512 × 512 pixels. That means that to process our image, we pay the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p44">85 tokens (the fixed cost component)</li> 
   <li class="readable-text" id="p45">4 × 170 tokens = 680 tokens (the variable cost component)</li> 
  </ul> 
  <div class="readable-text" id="p46"> 
   <p>In total, we therefore pay 85 + 4 × 170 = 765 tokens. Given current prices, this corresponds to $0.003825 (i.e., less than one cent). Although that may seem acceptable, always keep costs in mind when processing large repositories of images via language models.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p47"> 
   <p> <span class="print-book-callout-head">Tip</span> To find the price for processing images with a specific resolution, you can also use the OpenAI price calculator: <a href="https://openai.com/pricing">https://openai.com/pricing</a>. </p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <h2 class=" readable-text-h2" id="tagging-people-in-images"><span class="num-string browsable-reference-id">6.3</span> Tagging people in images</h2> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Imagine the following situation: you just came back from a (well-deserved) holiday with friends, and, of course, you have taken a large number of pictures. You want to send your friends the pictures in which they appear. But how to do that efficiently? You could go through the pictures by hand and tag each friend individually. But having just come back from vacation, your email inbox is overflowing, and you don’t have time to go through holiday pictures. What can you do?</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h3 class=" readable-text-h3" id="overview"><span class="num-string browsable-reference-id">6.3.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>In this section, we will create a small application that automatically tags people in images. Users provide three inputs:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p52">The path of a directory containing the pictures to tag</li> 
   <li class="readable-text" id="p53">The path of a directory containing pictures of the people to look for</li> 
   <li class="readable-text" id="p54">The path to an output directory into which tagged pictures are written</li> 
  </ul> 
  <div class="readable-text" id="p55"> 
   <p>To keep things simple, we will use filenames to represent tags. We assume that pictures showing people to look for are named after the person they show. For example, let’s say we have images named Joe.png and Jane.png in our directory containing the people to look for. Given a picture to tag, we will simply change the filename by prefixing it with the names of people that appear in it.</p> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>For instance, assume we have an image called beach.png in which both Joe and Jane appear. Then, in the output directory, we will create two files called Joebeach.png and Janebeach.png, showing that both appear in the beach picture. If we want to send out all pictures showing the same person, such as Joe, we can search for all files whose name satisfies the regular expression <code>Joe*.png</code> (with <code>*</code> representing arbitrary strings).</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>Internally, as a first step, we need to load pictures representing people to look for, as well as the pictures to tag. We will consider each pair of a person to look for and a picture to tag. For example, if we are looking for five people and have 10 pictures to tag, that makes 50 pairs to consider. For each of these pairs, we use GPT-4o to decide whether the corresponding person appears in the picture to tag.</p> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>To make that happen, we will need multimodal prompts containing text and two pictures. The first picture shows the person to look for, and the second picture shows the picture to tag. Via text, we can instruct the language model to compare the pictures to decide whether the same person appears. Whenever we find a match—that is, a combination of a person and a picture in which that person appears—we will copy the corresponding picture to the output folder, prefixing its name with the name of the person.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>Figure <a href="#fig__tagprompt">6.2</a> shows an example prompt. On the left, we have a picture of Jane, one of the people we are looking for. On the right side, we have a picture to tag. The text instructions ask the language model to compare the two pictures, producing the answer “Yes” if they show the same person (and “No” otherwise). In this case, the pictures do not show the same person, and the correct answer should be “No.”</p> 
  </div> 
  <div class="browsable-container figure-container" id="p60">  
   <img alt="figure" src="../Images/CH06_F02_Trummer.png" width="1050" height="552"/> 
   <h5 class=" figure-container-h5" id="fig__tagprompt"><span class="num-string">Figure <span class="browsable-reference-id">6.2</span></span> Multimodal prompt containing two images and text: the prompt instructs the language model to check whether the two pictures show the same person (expected answer: “Yes”) or not (expected answer: “No”).</h5>
  </div> 
  <div class="readable-text" id="p61"> 
   <h3 class=" readable-text-h3" id="encoding-locally-stored-images"><span class="num-string browsable-reference-id">6.3.2</span> Encoding locally stored images</h3> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>In the previous section, we used GPT-4o to analyze images on the web. Now we are talking about our private holiday pictures. We may not want to make all of them publicly accessible on the web. So how can we share them with GPT-4o alone?</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>We may have to convert images into a format suitable for GPT-4o. GPT-4o supports a wide range of image formats, including PNG, JPEG, WEBP, and GIF. For any format, the image file size is currently limited to 20 MB. To upload pictures of the supported types to GPT-4o, we first need to encode them using a base64 encoding.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p64"> 
    <h5 class=" callout-container-h5 readable-text-h5">What is base64 encoding?</h5> 
   </div> 
   <div class="readable-text" id="p65"> 
    <p> The base64 encoding is a way to encode binary data as a printable string. As the name base64 suggests, the alphabet we use for the string is based on 64 characters. This means we can represent each character using six bits (because six bits allow representing 2<sup>6</sup> = 64 possible characters). As computers store data at the granularity of bytes (i.e., 8 bits), it is convenient to encode groups of three bytes (i.e., 24 bits) together. Using base64 encoding, three bytes can be used to represent four characters (as 24/6 = 4).</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>In Python, we can use the <code>base64</code> library to encode binary data in the base64 format. The following code opens an image file stored at <code>image_path</code> and encodes it using base64 format:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p67"> 
   <div class="code-area-container"> 
    <pre class="code-area">with open(image_path, 'rb') as image_file:
    encoded = base64.b64encode(image_file.read())</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>We have transformed the binary image data into a string in base64 format. Before sending such images to GPT-4o, we still need to make one final transformation: we must represent the string using the UTF-8 encoding.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p69"> 
    <h5 class=" callout-container-h5 readable-text-h5">What is UTF-8 encoding?</h5> 
   </div> 
   <div class="readable-text" id="p70"> 
    <p> UTF-8 is a way to represent string data. It is extremely popular and used by about 98% of sites on the web. UTF-8 can represent over a million characters, covering a variety of languages. We can represent those characters using a fixed number of bytes: four bytes to represent each character. However, this is inefficient because it does not exploit the fact that certain characters are much more common than others. If we encode common characters with fewer bytes while reserving many-byte representations for the less common ones, we can represent the same text with fewer bytes. This is what UTF-8 does, and because different characters may need a different number of bytes for representation, it is also called a <em>variable-length standard</em>. At the same time, UTF-8 is designed to be backward-compatible with the older ASCII standard, using the same encoding as ASCII for the first 128 characters.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>To transform our base64 string encoding of the image into UTF-8, we can use Python’s <code>decode</code> function. Assuming that the image is still encoded in the <code>encoded</code> string variable, we can do so using the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p72"> 
   <div class="code-area-container"> 
    <pre class="code-area">image = encoded.decode('utf-8')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>The resulting image, encoded as UTF-8 text string, is suitable as input for GPT-4o. Next, we will see how we can upload images in this format to the OpenAI platform. After uploading them, we can include references to those pictures in our prompts. Images are generally specified as components of the prompt:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p74"> 
   <div class="code-area-container"> 
    <pre class="code-area">{'type':'image_url', 'image_url':{'url':image_url}}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>Here, <code>image_url</code> represents the URL that leads to the image to analyze. Previously, we used publicly accessible URLs for that. Now we are analyzing private images that we will send to OpenAI to be used only to process specific requests. Assuming that <code>image</code> still represents the image encoded as a string, we can set the image URL as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <div class="code-area-container"> 
    <pre class="code-area">image_url = {'url':f'data:image/png;base64,{image}'}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>This code assumes that the image is of type PNG (if not, replace the string <code>png</code> with the appropriate format identifier such as <code>jpeg</code>). The URL combines metadata about the image (such as the image type and encoding) with a string suffix representing the picture itself.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3" id="sending-locally-stored-images-to-openai"><span class="num-string browsable-reference-id">6.3.3</span> Sending locally stored images to OpenAI</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>We will use this project as an opportunity to demonstrate an alternative way to interact with GPT models. Doing so will give us insights into how OpenAI’s Python library works internally. So far, we have been using Python wrappers that send requests to OpenAI’s platform in the background. To send our local images to GPT-4o, we will create those requests ourselves.</p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>We use Python’s <code>requests</code> library to create HTTP requests, sending our prompts (with text and images) to GPT-4o and collecting the answer. More precisely, we will be sending HTTP Post requests. This is the type of request accepted by the OpenAI platform. Such requests can be sent via the <code>requests.post</code> method.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Our requests will contain all relevant information needed by GPT-4o to solve the task we are interested in (in this case, verifying whether two images show the same person). First, we need to include headers in the request. We will use the following headers:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p82"> 
   <div class="code-area-container"> 
    <pre class="code-area">headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer ...'
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>You see that we’re specifying headers as a Python dictionary. For our use case, we only need to store two properties: the type of our payload (we plan to send JSON content) and our access credentials (the three dots represent our OpenAI access key).</p> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Next, we need to specify the payload—that is, the content that we primarily want to send via the request:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <div class="code-area-container"> 
    <pre class="code-area">payload = {
    'model': 'gpt-4o',  #1
    'messages': [
        {'role': 'user', 'content': ...}  #2
        ],
    'max_tokens':1  #3
    }</pre> 
    <div class="code-annotations-overlay-container">
     #1 Model specification
     <br/>#2 First message
     <br/>#3 Output length
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>You may notice that the payload contains exactly the fields we would typically specify in our invocations to the <code>completions.create</code> method. That is not a coincidence, as the latter method creates requests with a similar payload internally. First, the payload specifies the model (<strong class="cueball">1</strong>): <code>gpt-4o</code> (to be able to process multimodal input prompts). We specify a list of messages with a single entry (<strong class="cueball">2</strong>). This message is marked as originating from the user (<code>role:user</code>), and its content, abbreviated by three dots, will contain text instructions and images. Finally, we limit the answer length to a single token (<code>max_tokens:1</code>) (<strong class="cueball">3</strong>). That makes sense because we are searching for a binary result: either the same person appears in multiple input images (expected answer: “Yes”) or not (expected answer: “No”).</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>Having generated headers and a payload, we can invoke GPT-4o using the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p88"> 
   <div class="code-area-container"> 
    <pre class="code-area">response = requests.post(
        'https://api.openai.com/v1/chat/completions', 
        headers=headers, json=payload)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>As the first parameter, the invocation of <code>requests.post</code> specifies the URL to send the request to. In this case, <code>https://api.openai.com/v1/chat/completions</code> indicates that we want to perform a task of type <code>Completion</code>, using one of OpenAI’s chat models (which applies to GPT-4o). We use the headers and payload created previously.</p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>The response contains the GPT-4o result object. We can access the answer (indicating whether two images show the same person) via the following code snippet:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p91"> 
   <div class="code-area-container"> 
    <pre class="code-area">response.json()['choices'][0]['message']['content']</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p92"> 
   <h3 class=" readable-text-h3" id="the-end-to-end-implementation"><span class="num-string browsable-reference-id">6.3.4</span> The end-to-end implementation</h3> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>We are now ready to discuss the end-to-end implementation! Listing <a href="#code__taggingpictures">6.2</a> contains code for tagging people in pictures. Have a look at the main function (<strong class="cueball">4</strong>) first. As discussed previously, users specify three directories as command-line parameters (<strong class="cueball">5</strong>): a directory containing pictures to tag, a directory containing people to use for tagging, and an output directory.</p> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>As a first step, we load all images to tag as well as all images of the people to search for. We use the <code>load_images</code> function (<strong class="cueball">1</strong>) for that. This function retrieves a list of all files in the input directory and then considers those ending with the suffix .png (i.e., we consider all PNG images). As discussed previously, we need to encode images as strings (via base64 encoding) that are ultimately represented via the UTF-8 encoding. The result of <code>load_images</code> is a Python dictionary mapping filenames to the associated, encoded images. This dictionary is returned as the result of the function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p95"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__taggingpictures"><span class="num-string">Listing <span class="browsable-reference-id">6.2</span></span> Tagging people in pictures stored locally</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import base64
import os
import requests
import shutil

def load_images(in_dir):            #1
    """ Loads images from a directory.
    
    Args:
        in_dir: path of input directory.
    
    Returns:
        directory mapping file names to PNG images.
    """
    name_to_image = {}
    file_names = os.listdir(in_dir)
    for file_name in file_names:
        if file_name.endswith('.png'):
            image_path = os.path.join(in_dir, file_name)
            with open(image_path, 'rb') as image_file:
                encoded = base64.b64encode(image_file.read())
                image = encoded.decode('utf-8')
                name_to_image[file_name] = image
    
    return name_to_image

def create_prompt(             #2
    person_image, image_to_label): 
    """ Create prompt to compare images.
    
    Args:
        person_image: image showing a person.
        image_to_label: image to assign to a label.
    
    Returns:
        prompt to verify if the same person appears in both images.
    """
    task = {'type':'text', 
            'text':'Do the images show the same person ("Yes"/"No")?'}
    prompt = [task]
    for image in [person_image, image_to_label]:
        image_url = {'url':f'data:image/png;base64,{image}'}
        image_msg = {'type':'image_url', 'image_url':image_url}
        prompt += [image_msg]
    
    return prompt

def call_llm(ai_key, prompt):              #3
    """ Call language model to process prompt with local images.
    
    Args:
        ai_key: key to access OpenAI.
        prompt: a prompt merging text and local images.
    
    Returns:
        answer by the language model.
    """
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ai_key}'
    }
    payload = {
        'model': 'gpt-4o',
        'messages': [
            {'role': 'user', 'content': prompt}
            ],
        'max_tokens':1
        }
    response = requests.post(
        'https://api.openai.com/v1/chat/completions', 
        headers=headers, json=payload)
    return response.json()['choices'][0]['message']['content']

if __name__ == '__main__':  #4
    
    parser = argparse.ArgumentParser()             #5
    parser.add_argument('peopledir', type=str, help='Images of people')
    parser.add_argument('picsdir', type=str, help='Images to tag')
    parser.add_argument('outdir', type=str, help='Output directory')
    args = parser.parse_args()
    
    people_images = load_images(args.peopledir)
    unlabeled_images = load_images(args.picsdir)

    for person_name, person_image in people_images.items():  #6
        for un_name, un_image in unlabeled_images.items():   #7
            prompt = create_prompt(person_image, un_image)
            ai_key = os.getenv('OPENAI_API_KEY')
            response = call_llm(ai_key, prompt)
            description = f'{un_name} versus {person_name}?'
            print(f'{description} -&gt; {response}')
            
            if response == 'Yes':              #8
                labeled_name = f'{person_name[:-4]}{un_name}'
                source_path = os.path.join(args.picsdir, un_name)
                target_path = os.path.join(args.outdir, labeled_name)
                shutil.copy(source_path, target_path)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Loads images from disk
     <br/>#2 Creates a multimodal prompt
     <br/>#3 Generates an answer for the prompt
     <br/>#4 Tags images with people
     <br/>#5 Command-line parameters
     <br/>#6 Over people
     <br/>#7 Over images
     <br/>#8 Copies image in case of a match
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>After applying the <code>load_images</code> function to each of the two input directories, we end up with two Python dictionaries. One maps filenames of images showing people (which, by convention, are the names of those people) to the corresponding encoded image. The other maps the filenames of images to be tagged to the encoded images.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>Our goal is to match each picture to be tagged to all people that appear in it. As we use prompts comparing only two pictures at once, we need to look at each combination of a person and of an image to tag. That is why we use a double-nested <code>for</code> loop: one iterates over people (<strong class="cueball">6</strong>) and the other over images to tag (<strong class="cueball">7</strong>).</p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>For each combination of an image to tag and a person, we create a multimodal prompt using <code>create_prompt</code>. This function (<strong class="cueball">2</strong>) assembles both encoded pictures, together with text instructions, into a prompt. The text instructions (“Do the images show the same person (“Yes”/“No”)?”) define the task as well as the expected output format (“Yes” or “No”). Each prompt is sent to GPT-4o via <code>call_llm</code>. As discussed previously, this function (<strong class="cueball">3</strong>) uses the requests API to send locally stored images, together with text instructions, to GPT-4o. If GPT-4o answers “Yes,” the currently considered person appears in the currently considered image to tag.</p> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>If the person appears in the image (<strong class="cueball">8</strong>), we tag the image as follows. We use the name of the person (the name of the associated picture file without the .png suffix) and prepend it to the name of the file to tag. Next, we copy the file to tag to the output directory using the new filename (which indicates the tagging result).</p> 
  </div> 
  <div class="readable-text" id="p100"> 
   <h3 class=" readable-text-h3" id="trying-it-out-1"><span class="num-string browsable-reference-id">6.3.5</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>Let’s try it! If you have real vacation pictures to tag, you can use them. Otherwise, you will find suitable test data on the book’s companion website. Look for the Tagging link to access a zipped file; download this file and unzip its contents. After decompression, you should see three subdirectories in the resulting folder:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p102"><em>people</em>—A folder containing pictures of people (in this case, actors from the <em>Avengers</em> series). Filenames contain the names of the corresponding actors.</li> 
   <li class="readable-text" id="p103"><em>pics</em>—Another set of pictures (in this case, more pictures of the same actors as in the people folder) to tag with the names of actors.</li> 
   <li class="readable-text" id="p104"><em>processed</em>—An empty folder that can be used as the output directory.</li> 
  </ul> 
  <div class="readable-text" id="p105"> 
   <p>We’ll assume that the decompressed folder is stored under the path /tagging (e.g., the path /tagging/people then leads to the subfolder with pictures of people to search for). Execute the code by running the following command from the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p106"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing2.py /tagging/people  /tagging/pics /tagging/processed</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p107"> 
   <p> <span class="print-book-callout-head">Tip</span> If you are invoking the code on a Windows platform, you will have to adapt these paths. In particular, you will have to replace / with \. </p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>During processing, the implementation prints updates on whether specific people appear in specific pictures. The sample data contains two people to look for and four images to tag. This means processing should not take more than a few minutes (typically less than two).</p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>After processing finishes, look in the output folder. You should see pictures to tag, prefixed with the names of people appearing in them. Not bad for a few lines of Python code!</p> 
  </div> 
  <div class="readable-text" id="p110"> 
   <h2 class=" readable-text-h2" id="generating-titles-for-videos"><span class="num-string browsable-reference-id">6.4</span> Generating titles for videos</h2> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>Besides many pictures (which we can now automatically tag, thanks to the code outlined in the previous section!), you also took quite a few videos on vacation. Automatically assigned filenames are not very informative. Which video is the one showing you swimming in the ocean? It would be great to assign meaningful captions to those videos and help you find the ones you’re looking for faster. But who has time to manually label videos? Again, we can use language models to do that task automatically.</p> 
  </div> 
  <div class="readable-text" id="p112"> 
   <h3 class=" readable-text-h3" id="overview-1"><span class="num-string browsable-reference-id">6.4.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>We will develop a system that automatically assigns suitable titles to videos. This system uses GPT-4o in the background. To assign titles to videos, we will submit multimodal prompts containing video frames (i.e., images) together with text instructing the language model to come up with a title. Figure <a href="#fig__videoprompt">6.3</a> shows an example prompt.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p114">  
   <img alt="figure" src="../Images/CH06_F03_Trummer.png" width="1100" height="450"/> 
   <h5 class=" figure-container-h5" id="fig__videoprompt"><span class="num-string">Figure <span class="browsable-reference-id">6.3</span></span> Multimodal prompt for video processing: based on a selection of video frames, the language model is instructed to generate a suitable title.</h5>
  </div> 
  <div class="readable-text" id="p115"> 
   <p>It consists of multiple video frames (we only see the first and the last frame in figure <a href="#fig__videoprompt">6.3</a>; the three dots represent the ones in between) and the text instructions “Generate a concise title for the video.” Note that we have to pay for each video frame we’re submitting to GPT-4o. That means video data processing via GPT-4o quickly becomes expensive!</p> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>As an answer, GPT-4o should send back a reasonable title. In the example shown in figure <a href="#fig__videoprompt">6.3</a>, this can be a reference to cars and, potentially, even a reference to the location (shown as white text in the frames).</p> 
  </div> 
  <div class="readable-text" id="p117"> 
   <h3 class=" readable-text-h3" id="encoding-video-frames"><span class="num-string browsable-reference-id">6.4.2</span> Encoding video frames</h3> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>First we need to discuss video formats. In the last section, we saw how to encode images stored locally. Now we will expand that to videos. Ultimately, our goal is to extract a sequence of frames. However, videos are typically not stored as a sequence of frames but using more efficient encodings. For us, that means we first have to extract images from a video.</p> 
  </div> 
  <div class="readable-text" id="p119"> 
   <p>We will use the OpenCV library for that. OpenCV is the Open Source Computer Vision Library. It provides various functionalities for computer vision as well as for image and video processing in general. Of course, we will use GPT-4o to do the computer vision part. Nevertheless, OpenCV will be useful for extracting frames from videos. If you haven’t done so already, now would be a good time to set up OpenCV by following the instructions in section <a href="#sec__videosetup">6.1</a>.</p> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>Let’s assume that the installation has worked and you can access OpenCV from Python. The corresponding Python library is called <code>cv2</code> (a name you will often see as a prefix in the following code snippets).</p> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>To work with a video stored locally, we first need to open the corresponding file. Run this code to open a video stored under the path <code>video_path</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p122"> 
   <div class="code-area-container"> 
    <pre class="code-area">video = cv2.VideoCapture(video_path)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>Using the variable <code>video</code>, we can now read the video’s content via the <code>read</code> method:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p124"> 
   <div class="code-area-container"> 
    <pre class="code-area">success, frame = video.read()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>The result consists of tuples with two components: a <code>success</code> flag and a video frame. The <code>success</code> flag indicates whether we were able to read another frame. That’s no longer the case once we reach the end of the video. In that case, we do not obtain a valid frame, and the <code>success</code> flag is set to <code>False</code>.</p> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>Let’s assume that we are able to read another frame. In that case, we will turn the frame into an image we can send to GPT-4o. OpenCV has us covered and provides the corresponding functionality:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p127"> 
   <div class="code-area-container"> 
    <pre class="code-area">_, buffer = cv2.imencode('.jpg', frame)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>The <code>imencode</code> function turns a video frame into an image of the corresponding type. Here, we transform the frame into a JPEG picture. From the resulting tuple, the second component (<code>buffer</code>) is interesting for our purposes. It contains a binary representation of the corresponding picture.</p> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>That’s a situation we know from the previous section: we have a binary representation of an image and want to turn it into a suitable format for GPT-4o. Again, we first encode the image as a string via base64 encoding and then represent that string via UTF-8:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p130"> 
   <div class="code-area-container"> 
    <pre class="code-area">encoded = base64.b64encode(buffer)
frame = encoded.decode('utf-8')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>The resulting <code>frame</code> is encoded properly to be included as part of a GPT-4o prompt. Once you’re done processing the video, close the corresponding video capture object using the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p132"> 
   <div class="code-area-container"> 
    <pre class="code-area">video.release()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>Next, we will put everything together to generate video titles for arbitrary videos.</p> 
  </div> 
  <div class="readable-text" id="p134"> 
   <h3 class=" readable-text-h3" id="the-end-to-end-implementation-1"><span class="num-string browsable-reference-id">6.4.3</span> The end-to-end implementation</h3> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>Listing <a href="#code__videocaption">6.3</a> generates titles for videos stored locally. The only input parameter is the path to the video. Given that, the implementation extracts some of the video frames (<strong class="cueball">4</strong>) and then generates a prompt instructing GPT-4o to generate a video title based on a sample of frames. After sending this prompt to the language model, the answer contains a proposed video title.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p136"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__videocaption"><span class="num-string">Listing <span class="browsable-reference-id">6.3</span></span> Generating a video title via language models</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import cv2
import base64
import openai
import time

client = openai.OpenAI()

def extract_frames(video_path):    #1
    """ Extracts frames from a video.
    
    Args:
        video_path: path to video file.
    
    Returns:
        list of first ten video frames.
    """
    video = cv2.VideoCapture(video_path)
    frames = []
    while video.isOpened() and len(frames) &lt;= 10:
        success, frame = video.read()
        if not success:
            break
        
        _, buffer = cv2.imencode('.jpg', frame)
        encoded = base64.b64encode(buffer)
        frame = encoded.decode('utf-8')
        frames += [frame]
    
    video.release()
    return frames

def create_prompt(frames):                     #2
    """ Create prompt to generate title for video.
    
    Args:
        frames: frames of video.
    
    Returns:
        prompt containing multimodal data (as list).
    """
    prompt = ['Generate a concise title for the video.']
    for frame in frames[:10]:
        element = {'image':frame, 'resize':768}
        prompt += [element]
    return prompt

def call_llm(prompt):                             #3
    """ Query large language model and return answer.
    
    Args:
        prompt: input prompt for language model.
    
    Returns:
        Answer by the language model.
    """
    for nr_retries in range(1, 4):
        try:
            response = client.chat.completions.create(
                model='gpt-4o',
                messages=[
                    {'role':'user', 'content':prompt}
                    ]
                )
            return response.choices[0].message.content
        except:
            time.sleep(nr_retries * 2)
    raise Exception('Cannot query OpenAI model!')

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('videopath', type=str, help='Path of video file')
    args = parser.parse_args()

    frames = extract_frames(args.videopath)  #4
    prompt = create_prompt(frames)
    title = call_llm(prompt)
    print(title)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Extracts video frames
     <br/>#2 Creates multimodal prompt
     <br/>#3 Queries the language model
     <br/>#4 Titles videos
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>The code extracts video frames using <code>extract_frames</code> (<strong class="cueball">1</strong>). As discussed previously, this function uses the OpenCV library to open the video for frame extraction and proceeds to read each frame consecutively. We will only use up to 10 frames to generate a video title. That’s why extraction ends after at most 10 frames (or fewer if the video is very short). Each extracted frame is encoded according to GPT-4o’s requirements (i.e., JPEG images encoded as strings). The result of the function is a list of encoded frames.</p> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>During prompt generation (<strong class="cueball">2</strong>), we combine relevant text instructions (“Generate a concise title for the video.”) with the first 10 frames from the video. To send those images, along with instructions, to GPT-4o, we use a Python wrapper again (<strong class="cueball">3</strong>). Alternatively, we can create requests ourselves (as in the previous project). The response of the language model should contain a suitable title for our video.</p> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>Of course, we are only sending the first few frames of the video. If the video content changes drastically after those few frames, the title may not be optimal. The reason we only send 10 frames is computation fees. Keep in mind that you’re paying for each picture submitted in the prompt! Sending all frames of larger videos is typically prohibitively expensive. That’s why we content ourselves with sending only a small subset of video frames.</p> 
  </div> 
  <div class="readable-text" id="p140"> 
   <h3 class=" readable-text-h3" id="trying-it-out-2"><span class="num-string browsable-reference-id">6.4.4</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>Let’s try our video title generator! On the book’s companion website, this chapter’s section includes a Cars link that will guide you to a short video from a traffic camera showing traffic on a busy road. Download the video to your local machine.</p> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>Open the terminal, and change to the directory containing the code for this chapter. We’ll assume that the video was downloaded into the same directory (if not, replace the name of the video, cars.mp4, with the full path leading to it).</p> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>Run the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p144"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing3.py cars.mp4</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>After a few seconds of computation time, you should see a proposal for a video title: for example, “Traffic Conditions on I-5 at SR 516 and 188th Street” (the precise title may vary across different runs due to randomization).</p> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>Note that the title integrates information—the name of the location—that is only available in the form of text in the video. Using GPT-4o to extract text from images may be useful in various scenarios: for example, to extract data from forms.</p> 
  </div> 
  <div class="readable-text" id="p147"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p148">GPT-4o processes images as well as text.</li> 
   <li class="readable-text" id="p149">Prompts can integrate text snippets and images.</li> 
   <li class="readable-text" id="p150">GPT-4o supports multiple image formats.</li> 
   <li class="readable-text" id="p151">Images can be specified via a public image URL.</li> 
   <li class="readable-text" id="p152">Locally stored images can be uploaded to OpenAI.</li> 
   <li class="readable-text" id="p153">GPT-4o processes images in string encoding.</li> 
   <li class="readable-text" id="p154">Processing images is costly compared to processing text. The cost of image processing may depend on the image size. Processing images with a low degree of detail reduces costs.</li> 
   <li class="readable-text" id="p155">The <code>base64</code> library can encode images as strings.</li> 
   <li class="readable-text" id="p156">Decompose videos into their frames to send them to GPT-4o.</li> 
   <li class="readable-text" id="p157">The OpenCV library can be used to extract frames from videos.</li> 
  </ul> 
 </div></div></body></html>