- en: 4 Implementing a GPT model from Scratch To Generate Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 从头实现 GPT 模型以生成文本
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Coding a GPT-like large language model (LLM) that can be trained to generate
    human-like text
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码一个类似 GPT 的大型语言模型（LLM），可以训练生成类人文本
- en: Normalizing layer activations to stabilize neural network training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化层激活以稳定神经网络训练
- en: Adding shortcut connections in deep neural networks to train models more effectively
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度神经网络中添加快捷连接以更有效地训练模型
- en: Implementing transformer blocks to create GPT models of various sizes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现变压器模块以创建各种规模的 GPT 模型
- en: Computing the number of parameters and storage requirements of GPT models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算 GPT 模型的参数数量和存储需求
- en: In the previous chapter, you learned and coded the *multi-head attention* mechanism,
    one of the core components of LLMs. In this chapter, we will now code the other
    building blocks of an LLM and assemble them into a GPT-like model that we will
    train in the next chapter to generate human-like text, as illustrated in Figure
    4.1.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，你学习并编码了*多头注意力*机制，这是 LLM 的核心组件之一。在本章中，我们将编码 LLM 的其他构建块，并将它们组装成一个类似 GPT
    的模型，我们将在下一章中训练以生成类人文本，如图 4.1 所示。
- en: Figure 4.1 A mental model of the three main stages of coding an LLM, pretraining
    the LLM on a general text dataset, and finetuning it on a labeled dataset. This
    chapter focuses on implementing the LLM architecture, which we will train in the
    next chapter.
  id: totrans-8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.1 编码 LLM 的三个主要阶段的心理模型，包括在通用文本数据集上进行预训练和在标记数据集上进行微调。本章重点实现 LLM 架构，我们将在下一章对其进行训练。
- en: '![](images/04__image001.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image001.png)'
- en: The LLM architecture, referenced in Figure 4.1, consists of several building
    blocks that we will implement throughout this chapter. We will begin with a top-down
    view of the model architecture in the next section before covering the individual
    components in more detail.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 所提及的 LLM 架构由多个构建块组成，我们将在本章中实现这些构建块。我们将首先在下一节中从整体视角了解模型架构，然后更详细地讨论各个组件。
- en: 4.1 Coding an LLM architecture
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 编码 LLM 架构
- en: LLMs, such as GPT (which stands for *Generative Pretrained Transformer*), are
    large deep neural network architectures designed to generate new text one word
    (or token) at a time. However, despite their size, the model architecture is less
    complicated than you might think, since many of its components are repeated, as
    we will see later. Figure 4.2 provides a top-down view of a GPT-like LLM, with
    its main components highlighted.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM，如 GPT（代表*生成预训练变压器*），是大型深度神经网络架构，旨在一次生成一个词（或标记）来生成新文本。然而，尽管其规模庞大，模型架构却没有你想象的那么复杂，因为它的许多组件是重复的，正如我们稍后将看到的。图
    4.2 提供了类似 GPT 的 LLM 的顶视图，并突出显示其主要组件。
- en: Figure 4.2 A mental model of a GPT model. Next to the embedding layers, it consists
    of one or more transformer blocks containing the masked multi-head attention module
    we implemented in the previous chapter.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.2 GPT 模型的心理模型。在嵌入层旁边，它由一个或多个变压器模块组成，其中包含我们在前一章中实现的掩蔽多头注意力模块。
- en: '![](images/04__image003.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image003.png)'
- en: As you can see in Figure 4.2, we have already covered several aspects, such
    as input tokenization and embedding, as well as the masked multi-head attention
    module. The focus of this chapter will be on implementing the core structure of
    the GPT model, including its *transformer blocks*, which we will then train in
    the next chapter to generate human-like text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.2 所示，我们已经涵盖了多个方面，例如输入标记化和嵌入，以及我们在前一章实现的掩蔽多头注意力模块。本章将重点实现 GPT 模型的核心结构，包括其*变压器模块*，我们将在下一章中对其进行训练以生成类人文本。
- en: In the previous chapters, we used smaller embedding dimensions for simplicity,
    ensuring that the concepts and examples could comfortably fit on a single page.
    Now, in this chapter, we are scaling up to the size of a small GPT-2 model, specifically
    the smallest version with 124 million parameters, as described in Radford *et
    al.*'s paper, "Language Models are Unsupervised Multitask Learners." Note that
    while the original report mentions 117 million parameters, this was later corrected.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，为了简化，我们使用了较小的嵌入维度，以确保概念和示例能够舒适地适应在一页上。现在，在本章中，我们将规模扩大到小型 GPT-2 模型的大小，具体是具有
    1.24 亿参数的最小版本，正如 Radford *等人* 的论文《语言模型是无监督多任务学习者》中所描述的。请注意，尽管原报告提到有 1.17 亿参数，但这一点后来得到了更正。
- en: Chapter 6 will focus on loading pretrained weights into our implementation and
    adapting it for larger GPT-2 models with 345, 762, and 1,542 million parameters.
    In the context of deep learning and LLMs like GPT, the term "parameters" refers
    to the trainable weights of the model. These weights are essentially the internal
    variables of the model that are adjusted and optimized during the training process
    to minimize a specific loss function. This optimization allows the model to learn
    from the training data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章将重点介绍如何将预训练权重加载到我们的实现中，并将其调整为更大的GPT-2模型，参数量分别为345、762和1,542百万。在深度学习和像GPT这样的LLM的背景下，“参数”一词指的是模型的可训练权重。这些权重本质上是模型的内部变量，在训练过程中进行调整和优化，以最小化特定的损失函数。这种优化使模型能够从训练数据中学习。
- en: For example, in a neural network layer that is represented by a 2,048x2,048-dimensional
    matrix (or tensor) of weights, each element of this matrix is a parameter. Since
    there are 2,048 rows and 2,048 columns, the total number of parameters in this
    layer is 2,048 multiplied by 2,048, which equals 4,194,304 parameters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个由2,048x2,048维矩阵（或张量）表示的神经网络层中，每个元素都是一个参数。由于有2,048行和2,048列，因此该层的参数总数为2,048乘以2,048，等于4,194,304个参数。
- en: GPT-2 versus GPT-3
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-2与GPT-3
- en: Note that we are focusing on GPT-2 because OpenAI has made the weights of the
    pretrained model publicly available, which we will load into our implementation
    in chapter 6\. GPT-3 is fundamentally the same in terms of model architecture,
    except that it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion
    parameters in GPT-3, and it is trained on more data. As of this writing, the weights
    for GPT-3 are not publicly available. GPT-2 is also a better choice for learning
    how to implement LLMs, as it can be run on a single laptop computer, whereas GPT-3
    requires a GPU cluster for training and inference. According to Lambda Labs, it
    would take 355 years to train GPT-3 on a single V100 datacenter GPU, and 665 years
    on a consumer RTX 8000 GPU.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们专注于GPT-2，因为OpenAI已公开提供预训练模型的权重，我们将在第6章将其加载到我们的实现中。GPT-3在模型架构上基本相同，只是将GPT-2的参数量从15亿扩展到1750亿，并且在更多数据上进行了训练。到目前为止，GPT-3的权重尚未公开可用。GPT-2也是学习如何实现LLM的更好选择，因为它可以在单台笔记本电脑上运行，而GPT-3则需要一个GPU集群进行训练和推断。根据Lambda
    Labs的数据，使用单个V100数据中心GPU训练GPT-3需要355年，而使用消费者RTX 8000 GPU则需要665年。
- en: 'We specify the configuration of the small GPT-2 model via the following Python
    dictionary, which we will use in the code examples later:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下Python字典指定小型GPT-2模型的配置，这将在后面的代码示例中使用：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the `GPT_CONFIG_124M` dictionary, we use concise variable names for clarity
    and to prevent long lines of code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在`GPT_CONFIG_124M`字典中，我们使用简洁的变量名以提高清晰度，并防止代码行过长：
- en: '`"vocab_size"` refers to a vocabulary of 50,257 words, as used by the BPE tokenizer
    from chapter 2.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"vocab_size"`指的是一个包含50,257个单词的词汇表，使用的是第2章中的BPE分词器。'
- en: '`"context_length"` denotes the maximum number of input tokens the model can
    handle, via the positional embeddings discussed in chapter 2.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"context_length"`表示模型可以处理的最大输入令牌数量，使用第2章讨论的位置信息嵌入。'
- en: '`"emb_dim"` represents the embedding size, transforming each token into a 768-dimensional
    vector.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"emb_dim"`表示嵌入大小，将每个令牌转换为768维向量。'
- en: '`"n_heads"` indicates the count of attention heads in the multi-head attention
    mechanism, as implemented in chapter 3.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"n_heads"`指的是多头注意力机制中的注意力头数量，如第3章中实现的那样。'
- en: '`"n_layers"` specifies the number of transformer blocks in the model, which
    will be elaborated on in upcoming sections.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"n_layers"`指定模型中变换器块的数量，将在后续部分详细说明。'
- en: '`"drop_rate"` indicates the intensity of the dropout mechanism (0.1 implies
    a 10% drop of hidden units) to prevent overfitting, as covered in chapter 3.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"drop_rate"`表示丢弃机制的强度（0.1表示隐藏单元的10%丢弃），以防止过拟合，如第3章所述。'
- en: '`"qkv_bias"` determines whether to include a bias vector in the `Linear` layers
    of the multi-head attention for query, key, and value computations. We will initially
    disable this, following the norms of modern LLMs, but will revisit it in chapter
    6 when we load pretrained GPT-2 weights from OpenAI into our model.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"qkv_bias"`决定是否在多头注意力的`Linear`层中包含一个偏置向量，用于查询、键和值的计算。我们最初会禁用此选项，遵循现代LLM的规范，但在第6章加载OpenAI的预训练GPT-2权重时会重新考虑。'
- en: Using the configuration above, we will start this chapter by implementing a
    GPT placeholder architecture (`DummyGPTModel`) in this section, as shown in Figure
    4.3\. This will provide us with a big-picture view of how everything fits together
    and what other components we need to code in the upcoming sections to assemble
    the full GPT model architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述配置，我们将在本节中实现一个GPT占位符架构（`DummyGPTModel`），如图4.3所示。这将为我们提供一个整体视图，了解所有内容是如何组合在一起的，以及我们在接下来的章节中需要编码的其他组件，以组装完整的GPT模型架构。
- en: Figure 4.3 A mental model outlining the order in which we code the GPT architecture.
    In this chapter, we will start with the GPT backbone, a placeholder architecture,
    before we get to the individual core pieces and eventually assemble them in a
    transformer block for the final GPT architecture.
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3是一个心理模型，概述了我们编码GPT架构的顺序。在本章中，我们将从GPT骨架（一个占位符架构）开始，然后逐步到达各个核心部分，最终将它们组装成变换块，形成最终的GPT架构。
- en: '![](images/04__image005.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image005.png)'
- en: 'The numbered boxes shown in Figure 4.3 illustrate the order in which we tackle
    the individual concepts required to code the final GPT architecture. We will start
    with step 1, a placeholder GPT backbone we call `DummyGPTModel`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3中显示的编号框展示了我们解决编码最终GPT架构所需的各个概念的顺序。我们将从步骤1开始，一个我们称之为`DummyGPTModel`的占位符GPT骨架：
- en: Listing 4.1 A placeholder GPT model architecture class
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1 一个占位符GPT模型架构类
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `DummyGPTModel` class in this code defines a simplified version of a GPT-like
    model using PyTorch's neural network module (`nn.Module`). The model architecture
    in the `DummyGPTModel` class consists of token and positional embeddings, dropout,
    a series of transformer blocks (`DummyTransformerBlock`), a final layer normalization
    (`DummyLayerNorm`), and a linear output layer (`out_head`). The configuration
    is passed in via a Python dictionary, for instance, the `GPT_CONFIG_124M` dictionary
    we created earlier.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中的`DummyGPTModel`类定义了一个使用PyTorch神经网络模块（`nn.Module`）的简化版本的类GPT模型。`DummyGPTModel`类中的模型架构包括标记和位置嵌入、丢弃层、一系列变换块（`DummyTransformerBlock`）、最后的层归一化（`DummyLayerNorm`）和线性输出层（`out_head`）。配置通过Python字典传递，例如，我们之前创建的`GPT_CONFIG_124M`字典。
- en: 'The `forward` method describes the data flow through the model: it computes
    token and positional embeddings for the input indices, applies dropout, processes
    the data through the transformer blocks, applies normalization, and finally produces
    logits with the linear output layer.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`方法描述了数据在模型中的流动：它计算输入索引的标记和位置嵌入，应用丢弃层，通过变换块处理数据，应用归一化，最后通过线性输出层生成logits。'
- en: The code above is already functional, as we will see later in this section after
    we prepare the input data. However, for now, note in the code above that we have
    used placeholders (`DummyLayerNorm` and `DummyTransformerBlock`) for the transformer
    block and layer normalization, which we will develop in later sections.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码已经可以正常运行，正如我们在本节后面将看到的那样，待我们准备输入数据后。不过，目前请注意，上面的代码中我们使用了占位符（`DummyLayerNorm`和`DummyTransformerBlock`）作为变换块和层归一化的代表，我们将在后面的章节中进行开发。
- en: Next, we will prepare the input data and initialize a new GPT model to illustrate
    its usage. Building on the figures we have seen in chapter 2, where we coded the
    tokenizer, Figure 4.4 provides a high-level overview of how data flows in and
    out of a GPT model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将准备输入数据并初始化一个新的GPT模型，以说明其用法。基于我们在第2章中看到的图示，我们对标记器进行了编码，图4.4提供了数据如何在GPT模型中流入和流出的高层概述。
- en: Figure 4.4 A big-picture overview showing how the input data is tokenized, embedded,
    and fed to the GPT model. Note that in our `DummyGPTClass` coded earlier, the
    token embedding is handled inside the GPT model. In LLMs, the embedded input token
    dimension typically matches the output dimension. The output embeddings here represent
    the context vectors we discussed in chapter 3.
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4展示了一个全景概述，显示输入数据是如何被标记、嵌入并输入到GPT模型中的。请注意，在我们之前编写的`DummyGPTClass`中，标记嵌入是在GPT模型内部处理的。在大型语言模型（LLMs）中，嵌入输入的标记维度通常与输出维度相匹配。这里的输出嵌入表示我们在第3章讨论的上下文向量。
- en: '![](images/04__image007.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image007.png)'
- en: 'To implement the steps shown in Figure 4.4, we tokenize a batch consisting
    of two text inputs for the GPT model using the tiktoken tokenizer introduced in
    chapter 2:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现图4.4中所示的步骤，我们使用第2章中介绍的tiktoken标记器，对包含两个文本输入的批次进行标记，以供GPT模型使用：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting token IDs for the two texts are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 两个文本的结果标记ID如下：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we initialize a new 124 million parameter `DummyGPTModel` instance and
    feed it the tokenized `batch`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化一个新的124百万参数的`DummyGPTModel`实例，并将其喂入标记化的`batch`：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The model outputs, which are commonly referred to as logits, are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出通常称为logits，具体如下：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output tensor has two rows corresponding to the two text samples. Each text
    sample consists of 4 tokens; each token is a 50,257-dimensional vector, which
    matches the size of the tokenizer's vocabulary.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输出张量有两行，分别对应于两个文本样本。每个文本样本由4个标记组成；每个标记是一个50,257维的向量，大小与分词器的词汇表相匹配。
- en: The embedding has 50,257 dimensions because each of these dimensions refers
    to a unique token in the vocabulary. At the end of this chapter, when we implement
    the postprocessing code, we will convert these 50,257-dimensional vectors back
    into token IDs, which we can then decode into words.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入有50,257个维度，因为这些维度对应于词汇表中的唯一标记。在本章末尾，当我们实现后处理代码时，我们将把这些50,257维的向量转换回标记ID，然后可以解码成单词。
- en: Now that we have taken a top-down look at the GPT architecture and its in- and
    outputs, we will code the individual placeholders in the upcoming sections, starting
    with the real layer normalization class that will replace the `DummyLayerNorm`
    in the previous code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对GPT架构及其输入输出进行了自上而下的了解，我们将在接下来的部分中编码各个占位符，从真正的层归一化类开始，该类将替换前面代码中的`DummyLayerNorm`。
- en: 4.2 Normalizing activations with layer normalization
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 使用层归一化归一化激活
- en: 'Training deep neural networks with many layers can sometimes prove challenging
    due to issues like vanishing or exploding gradients. These issues lead to unstable
    training dynamics and make it difficult for the network to effectively adjust
    its weights, which means the learning process struggles to find a set of parameters
    (weights) for the neural network that minimizes the loss function. In other words,
    the network has difficulty learning the underlying patterns in the data to a degree
    that would allow it to make accurate predictions or decisions. (If you are new
    to neural network training and the concepts of gradients, a brief introduction
    to these concepts can be found in *Section A.4, Automatic Differentiation Made
    Easy* in *Appendix A: Introduction to PyTorch*. However, a deep mathematical understanding
    of gradients is not required to follow the contents of this book.)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 训练具有多个层的深度神经网络有时会面临挑战，例如梯度消失或爆炸等问题。这些问题导致训练动态不稳定，使网络难以有效调整其权重，这意味着学习过程难以找到一组参数（权重），以最小化损失函数。换句话说，网络在学习数据中的潜在模式方面存在困难，无法达到能够做出准确预测或决策的程度。（如果你对神经网络训练和梯度的概念不熟悉，可以在*附录A：PyTorch简介*中的*第A.4节，自动微分简单易懂*找到这些概念的简要介绍。但是，要理解本书内容并不需要深厚的数学基础。）
- en: In this section, we will implement *layer normalization* to improve the stability
    and efficiency of neural network training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现*层归一化*以提高神经网络训练的稳定性和效率。
- en: The main idea behind layer normalization is to adjust the activations (outputs)
    of a neural network layer to have a mean of 0 and a variance of 1, also known
    as unit variance. This adjustment speeds up the convergence to effective weights
    and ensures consistent, reliable training. As we have seen in the previous section,
    based on the `DummyLayerNorm` placeholder, in GPT-2 and modern transformer architectures,
    layer normalization is typically applied before and after the multi-head attention
    module and before the final output layer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化的主要思想是调整神经网络层的激活（输出），使其均值为0，方差为1，也称为单位方差。这种调整加快了有效权重的收敛，并确保了训练的一致性和可靠性。正如我们在前一节中看到的，基于`DummyLayerNorm`占位符，在GPT-2和现代变换器架构中，层归一化通常在多头注意模块之前和之后，以及在最终输出层之前应用。
- en: Before we implement layer normalization in code, Figure 4.5 provides a visual
    overview of how layer normalization functions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现代码中的层归一化之前，图4.5提供了层归一化功能的视觉概述。
- en: Figure 4.5 An illustration of layer normalization where the 5 layer outputs,
    also called activations, are normalized such that they have a zero mean and variance
    of 1.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5层归一化的示意图，其中5层输出（也称为激活）被归一化，使其具有零均值和方差为1。
- en: '![](images/04__image009.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image009.png)'
- en: 'We can recreate the example shown in Figure 4.5 via the following code, where
    we implement a neural network layer with 5 inputs and 6 outputs that we apply
    to two input examples:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码重现图 4.5 中的示例，在这里我们实现一个具有 5 个输入和 6 个输出的神经网络层，应用于两个输入示例：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This prints the following tensor, where the first row lists the layer outputs
    for the first input and the second row lists the layer outputs for the second
    row:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下张量，其中第一行列出了第一个输入的层输出，第二行列出了第二个输入的层输出：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The neural network layer we have coded consists of a `Linear` layer followed
    by a non-linear activation function, `ReLU` (short for Rectified Linear Unit),
    which is a standard activation function in neural networks. If you are unfamiliar
    with `ReLU`, it simply thresholds negative inputs to 0, ensuring that a layer
    outputs only positive values, which explains why the resulting layer output does
    not contain any negative values. (Note that we will use another, more sophisticated
    activation function in GPT, which we will introduce in the next section).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写的神经网络层由一个 `Linear` 层和一个非线性激活函数 `ReLU`（即修正线性单元）组成，后者是神经网络中的标准激活函数。如果你不熟悉
    `ReLU`，它会将负输入阈值为 0，确保层只输出正值，这也解释了为什么结果层输出不包含任何负值。（请注意，我们将在 GPT 中使用另一种更复杂的激活函数，在下一节中介绍）。
- en: 'Before we apply layer normalization to these outputs, let''s examine the mean
    and variance:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对这些输出应用层归一化之前，让我们先检查均值和方差：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first row in the mean tensor above contains the mean value for the first
    input row, and the second output row contains the mean for the second input row.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述均值张量中的第一行包含第一个输入行的均值，而第二输出行包含第二个输入行的均值。
- en: Using `keepdim=True` in operations like mean or variance calculation ensures
    that the output tensor retains the same shape as the input tensor, even though
    the operation reduces the tensor along the dimension specified via `dim`. For
    instance, without `keepdim=True`, the returned mean tensor would be a 2-dimensional
    vector `[0.1324, 0.2170]` instead of a 2×1-dimensional matrix `[[0.1324], [0.2170]]`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在均值或方差计算等操作中使用 `keepdim=True` 可以确保输出张量与输入张量保持相同的形状，即使该操作沿着通过 `dim` 指定的维度减少了张量。例如，如果不使用
    `keepdim=True`，返回的均值张量将是一个二维向量 `[0.1324, 0.2170]`，而不是一个 2×1 的矩阵 `[[0.1324], [0.2170]]`。
- en: The `dim` parameter specifies the dimension along which the calculation of the
    statistic (here, mean or variance) should be performed in a tensor, as shown in
    Figure 4.6.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim` 参数指定了在张量中进行统计量（这里是均值或方差）计算的维度，如图 4.6 所示。'
- en: Figure 4.6 An illustration of the dim parameter when calculating the mean of
    a tensor. For instance, if we have a 2D tensor (matrix) with dimensions `[rows,
    columns]`, using `dim=0` will perform the operation across rows (vertically, as
    shown at the bottom), resulting in an output that aggregates the data for each
    column. Using `dim=1` or `dim=-1` will perform the operation across columns (horizontally,
    as shown at the top), resulting in an output aggregating the data for each row.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.6 说明了在计算张量均值时的 `dim` 参数示例。例如，如果我们有一个维度为 `[rows, columns]` 的二维张量（矩阵），使用 `dim=0`
    将会在行上（如底部所示）进行操作，输出结果将汇总每列的数据。使用 `dim=1` 或 `dim=-1` 将会在列上（如顶部所示）进行操作，输出结果将汇总每行的数据。
- en: '![](images/04__image011.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image011.png)'
- en: As Figure 4.6 explains, for a 2D tensor (like a matrix), using `dim=-1` for
    operations such as mean or variance calculation is the same as using `dim=1`.
    This is because -1 refers to the tensor's last dimension, which corresponds to
    the columns in a 2D tensor. Later, when adding layer normalization to the GPT
    model, which produces 3D tensors with shape `[batch_size, num_tokens, embedding_size]`,
    we can still use `dim=-1` for normalization across the last dimension, avoiding
    a change from `dim=1` to `dim=2`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图 4.6 所解释的，对于二维张量（如矩阵），在均值或方差计算等操作中使用 `dim=-1` 与使用 `dim=1` 是相同的。这是因为 -1 指的是张量的最后一个维度，在二维张量中对应于列。稍后，在将层归一化添加到生成对抗网络（GPT）模型时，该模型生成形状为
    `[batch_size, num_tokens, embedding_size]` 的三维张量，我们仍然可以使用 `dim=-1` 进行最后维度的归一化，从而避免将
    `dim=1` 改为 `dim=2`。
- en: 'Next, let us apply layer normalization to the layer outputs we obtained earlier.
    The operation consists of subtracting the mean and dividing by the square root
    of the variance (also known as standard deviation):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们对之前获得的层输出应用层归一化。该操作包括减去均值并除以方差的平方根（也称为标准差）：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can see based on the results, the normalized layer outputs, which now
    also contain negative values, have zero mean and a variance of 1:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，我们可以看到，归一化层的输出现在也包含负值，均值为零，方差为 1：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that the value 2.9802e-08 in the output tensor is the scientific notation
    for 2.9802 × 10-8, which is 0.0000000298 in decimal form. This value is very close
    to 0, but it is not exactly 0 due to small numerical errors that can accumulate
    because of the finite precision with which computers represent numbers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出张量中的值 2.9802e-08 是 2.9802 × 10^-8 的科学计数法，十进制形式为 0.0000000298。这个值非常接近 0，但由于计算机表示数字的有限精度，可能会由于小的数值误差而不完全等于
    0。
- en: 'To improve readability, we can also turn off the scientific notation when printing
    tensor values by setting `sci_mode` to False:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可读性，我们还可以通过将 `sci_mode` 设置为 False 来关闭打印张量值时的科学计数法：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'So far, in this section, we have coded and applied layer normalization in a
    step-by-step process. Let''s now encapsulate this process in a PyTorch module
    that we can use in the GPT model later:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这一节中，我们已经逐步编码并应用了层归一化。现在让我们将这个过程封装到一个 PyTorch 模块中，以便稍后在 GPT 模型中使用：
- en: Listing 4.2 A layer normalization class
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.2 一个层归一化类
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This specific implementation of layer Normalization operates on the last dimension
    of the input tensor x, which represents the embedding dimension (`emb_dim`). The
    variable `eps` is a small constant (epsilon) added to the variance to prevent
    division by zero during normalization. The `scale` and `shift` are two trainable
    parameters (of the same dimension as the input) that the LLM automatically adjusts
    during training if it is determined that doing so would improve the model's performance
    on its training task. This allows the model to learn appropriate scaling and shifting
    that best suit the data it is processing.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该层归一化的具体实现作用于输入张量 x 的最后一个维度，代表嵌入维度（`emb_dim`）。变量 `eps` 是一个小常数（epsilon），在归一化过程中加到方差上以防止除以零。`scale`
    和 `shift` 是两个可训练参数（与输入维度相同），如果发现这样做可以提高模型在训练任务上的表现，LLM 会在训练过程中自动调整它们。这使得模型能够学习最佳适合其处理数据的适当缩放和偏移。
- en: Biased variance
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏差方差
- en: In our variance calculation method, we have opted for an implementation detail
    by setting `unbiased=False`. For those curious about what this means, in the variance
    calculation, we divide by the number of inputs *n* in the variance formula. This
    approach does not apply Bessel's correction, which typically uses *n-1* instead
    of *n* in the denominator to adjust for bias in sample variance estimation. This
    decision results in a so-called biased estimate of the variance. For large-scale
    language models (LLMs), where the embedding dimension n is significantly large,
    the difference between using n and n-1 is practically negligible. We chose this
    approach to ensure compatibility with the GPT-2 model's normalization layers and
    because it reflects TensorFlow's default behavior, which was used to implement
    the original GPT-2 model. Using a similar setting ensures our method is compatible
    with the pretrained weights we will load in chapter 6.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方差计算方法中，我们选择了通过将 `unbiased=False` 来实现细节。对于那些好奇这意味着什么的人，在方差计算中，我们在方差公式中用输入的数量
    *n* 来除。这种方法不适用贝塞尔修正，贝塞尔修正通常在分母中使用 *n-1* 以调整样本方差估计的偏差。这一决定导致方差的所谓偏差估计。对于大型语言模型（LLMs），在嵌入维度
    n 非常大的情况下，使用 n 和 n-1 之间的差异在实践中几乎可以忽略不计。我们选择这种方法以确保与 GPT-2 模型的归一化层兼容，并且因为它反映了用于实现原始
    GPT-2 模型的 TensorFlow 的默认行为。使用类似的设置确保我们的方法与我们将在第六章中加载的预训练权重兼容。
- en: 'Let''s now try the `LayerNorm` module in practice and apply it to the batch
    input:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在实践中尝试 `LayerNorm` 模块，并将其应用于批量输入：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As we can see based on the results, the layer normalization code works as expected
    and normalizes the values of each of the two inputs such that they have a mean
    of 0 and a variance of 1:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，我们可以看到，层归一化代码按预期工作，并对两个输入的值进行归一化，使它们的均值为 0，方差为 1：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this section, we covered one of the building blocks we will need to implement
    the GPT architecture, as shown in the mental model in Figure 4.7.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了实现 GPT 架构所需的一个构建模块，如图 4.7 中的心理模型所示。
- en: Figure 4.7 A mental model listing the different building blocks we implement
    in this chapter to assemble the GPT architecture.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.7 一个心理模型列出了我们在本章中实现的不同构建模块，以组装 GPT 架构。
- en: '![](images/04__image013.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image013.png)'
- en: In the next section, we will look at the GELU activation function, which is
    one of the activation functions used in LLMs, instead of the traditional ReLU
    function we used in this section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨 GELU 激活函数，这是 LLM 中使用的激活函数之一，而不是我们在本节中使用的传统 ReLU 函数。
- en: Layer normalization versus batch normalization
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 层归一化与批归一化
- en: If you are familiar with batch normalization, a common and traditional normalization
    method for neural networks, you may wonder how it compares to layer normalization.
    Unlike batch normalization, which normalizes across the batch dimension, layer
    normalization normalizes across the feature dimension. LLMs often require significant
    computational resources, and the available hardware or the specific use case can
    dictate the batch size during training or inference. Since layer normalization
    normalizes each input independently of the batch size, it offers more flexibility
    and stability in these scenarios. This is particularly beneficial for distributed
    training or when deploying models in environments where resources are constrained.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉批归一化，这是神经网络中一种常见的传统归一化方法，你可能会想知道它与层归一化的比较。与跨批次维度进行归一化的批归一化不同，层归一化是跨特征维度进行归一化。LLM
    通常需要大量计算资源，可用的硬件或具体的使用案例可能会决定训练或推理期间的批量大小。由于层归一化对每个输入的归一化与批量大小无关，因此在这些场景中提供了更大的灵活性和稳定性。这对分布式训练或在资源受限的环境中部署模型特别有利。
- en: 4.3 Implementing a feed forward network with GELU activations
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 实现具有 GELU 激活的前馈网络
- en: In this section, we implement a small neural network submodule that is used
    as part of the transformer block in LLMs. We begin with implementing the *GELU*
    activation function, which plays a crucial role in this neural network submodule.
    (For additional information on implementing neural networks in PyTorch, please
    see section A.5 Implementing multilayer neural networks in Appendix A.)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现一个小型神经网络子模块，该模块作为 LLM 中变压器块的一部分。我们首先实现 *GELU* 激活函数，它在这个神经网络子模块中起着至关重要的作用。（有关在
    PyTorch 中实现神经网络的更多信息，请参见附录 A 中的 A.5 实现多层神经网络部分。）
- en: Historically, the ReLU activation function has been commonly used in deep learning
    due to its simplicity and effectiveness across various neural network architectures.
    However, in LLMs, several other activation functions are employed beyond the traditional
    ReLU. Two notable examples are GELU (*Gaussian Error Linear Unit*) and SwiGLU
    (*Sigmoid-Weighted Linear Unit*).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，ReLU 激活函数由于其简单性和在各种神经网络架构中的有效性而被广泛使用。然而，在 LLM 中，除了传统的 ReLU 之外，还采用了其他几种激活函数。两个显著的例子是
    GELU（*高斯误差线性单元*）和 SwiGLU（*Sigmoid 加权线性单元*）。
- en: GELU and SwiGLU are more complex and smooth activation functions incorporating
    Gaussian and sigmoid-gated linear units, respectively. They offer improved performance
    for deep learning models, unlike the simpler ReLU.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: GELU 和 SwiGLU 是更复杂且光滑的激活函数，分别融合了高斯和 sigmoid 门控线性单元。与更简单的 ReLU 相比，它们为深度学习模型提供了更好的性能。
- en: 'The GELU activation function can be implemented in several ways; the exact
    version is defined as GELU(x)=x Φ(x), where Φ(x) is the cumulative distribution
    function of the standard Gaussian distribution. In practice, however, it''s common
    to implement a computationally cheaper approximation (the original GPT-2 model
    was also trained with this approximation):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: GELU 激活函数可以以多种方式实现；确切的版本定义为 GELU(x)=x Φ(x)，其中 Φ(x) 是标准高斯分布的累积分布函数。然而在实际中，通常实现一个计算上更便宜的近似值（原始的
    GPT-2 模型也是使用这个近似值进行训练的）：
- en: GELU(x) ≈ 0.5 ⋅ x ⋅ (1 + tanh[√((2/π)) ⋅ (x + 0.044715 ⋅ x^3])
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: GELU(x) ≈ 0.5 ⋅ x ⋅ (1 + tanh[√((2/π)) ⋅ (x + 0.044715 ⋅ x^3])
- en: 'In code, we can implement this function as PyTorch module as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以将此函数作为 PyTorch 模块实现如下：
- en: Listing 4.3 An implementation of the GELU activation function
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3 GELU 激活函数的实现
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, to get an idea of what this GELU function looks like and how it compares
    to the ReLU function, let''s plot these functions side by side:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了了解这个 GELU 函数的样子以及它与 ReLU 函数的比较，我们将这些函数并排绘制：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As we can see in the resulting plot in Figure 4.8, ReLU is a piecewise linear
    function that outputs the input directly if it is positive; otherwise, it outputs
    zero. GELU is a smooth, non-linear function that approximates ReLU but with a
    non-zero gradient for negative values.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 4.8 中的结果图中可以看出，ReLU 是一个分段线性函数，当输入为正时直接输出输入；否则输出零。GELU 是一个平滑的非线性函数，它近似于 ReLU，但对负值具有非零梯度。
- en: Figure 4.8 The output of the GELU and ReLU plots using matplotlib. The x-axis
    shows the function inputs and the y-axis shows the function outputs.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.8使用matplotlib绘制的GELU和ReLU图的输出。x轴显示函数输入，y轴显示函数输出。
- en: '![](images/04__image015.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image015.png)'
- en: The smoothness of GELU, as shown in Figure 4.8, can lead to better optimization
    properties during training, as it allows for more nuanced adjustments to the model's
    parameters. In contrast, ReLU has a sharp corner at zero, which can sometimes
    make optimization harder, especially in networks that are very deep or have complex
    architectures. Moreover, unlike RELU, which outputs zero for any negative input,
    GELU allows for a small, non-zero output for negative values. This characteristic
    means that during the training process, neurons that receive negative input can
    still contribute to the learning process, albeit to a lesser extent than positive
    inputs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如图4.8所示，GELU的平滑性在训练过程中可以带来更好的优化特性，因为它允许对模型参数进行更细致的调整。相比之下，ReLU在零处有一个尖锐的拐角，这有时会使优化变得更加困难，特别是在非常深或复杂架构的网络中。此外，与ReLU在任何负输入时输出零不同，GELU允许负值有一个小的非零输出。这一特性意味着，在训练过程中，接收到负输入的神经元仍然可以为学习过程作出贡献，尽管贡献程度低于正输入。
- en: 'Next, let''s use the GELU function to implement the small neural network module,
    `FeedForward`, that we will be using in the LLM''s transformer block later:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用GELU函数来实现我们将在LLM的变换器模块中使用的小型神经网络模块`FeedForward`：
- en: Listing 4.4 A feed forward neural network module
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.4 一个前馈神经网络模块
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As we can see in the preceding code, the `FeedForward` module is a small neural
    network consisting of two `Linear` layers and a `GELU` activation function. In
    the 124 million parameter GPT model, it receives the input batches with tokens
    that have an embedding size of 768 each via the `GPT_CONFIG_124M` dictionary where
    `GPT_CONFIG_124M["emb_dim"] = 768`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的代码所示，`FeedForward`模块是一个由两个`Linear`层和一个`GELU`激活函数组成的小型神经网络。在124百万参数的GPT模型中，它通过`GPT_CONFIG_124M`字典接收嵌入大小为768的令牌输入批次，其中`GPT_CONFIG_124M["emb_dim"]
    = 768`。
- en: Figure 4.9 shows how the embedding size is manipulated inside this small feed
    forward neural network when we pass it some inputs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9展示了在传入一些输入时，嵌入大小是如何在这个小型前馈神经网络中被操作的。
- en: Figure 4.9 provides a visual overview of the connections between the layers
    of the feed forward neural network. It is important to note that this neural network
    can accommodate variable batch sizes and numbers of tokens in the input. However,
    the embedding size for each token is determined and fixed when initializing the
    weights.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.9提供了前馈神经网络层之间连接的视觉概览。值得注意的是，这个神经网络可以容纳可变的批次大小和输入中的令牌数量。然而，每个令牌的嵌入大小在初始化权重时是确定并固定的。
- en: '![](images/04__image017.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image017.png)'
- en: 'Following the example in Figure 4.9, let''s initialize a new `FeedForward`
    module with a token embedding size of 768 and feed it a batch input with 2 samples
    and 3 tokens each:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图4.9中的示例，让我们初始化一个新的`FeedForward`模块，令令牌嵌入大小为768，并输入一个包含2个样本和每个样本3个令牌的批次：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we can see, the shape of the output tensor is the same as that of the input
    tensor:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，输出张量的形状与输入张量的形状相同：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `FeedForward` module we implemented in this section plays a crucial role
    in enhancing the model's ability to learn from and generalize the data. Although
    the input and output dimensions of this module are the same, it internally expands
    the embedding dimension into a higher-dimensional space through the first linear
    layer as illustrated in Figure 4.10\. This expansion is followed by a non-linear
    GELU activation, and then a contraction back to the original dimension with the
    second linear transformation. Such a design allows for the exploration of a richer
    representation space.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一部分实现的`FeedForward`模块在增强模型从数据中学习和概括能力方面发挥了关键作用。尽管该模块的输入和输出维度相同，但它通过第一层线性层内部将嵌入维度扩展到更高维空间，如图4.10所示。这一扩展之后是非线性GELU激活，然后通过第二次线性变换收缩回原始维度。这种设计允许探索更丰富的表示空间。
- en: Figure 4.10 An illustration of the expansion and contraction of the layer outputs
    in the feed forward neural network. First, the inputs expand by a factor of 4
    from 768 to 3072 values. Then, the second layer compresses the 3072 values back
    into a 768-dimensional representation.
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.10 是前馈神经网络中层输出的扩展和收缩示意图。首先，输入从 768 扩展到 3072 值，然后第二层将 3072 值压缩回 768 维表示。
- en: '![](images/04__image019.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image019.png)'
- en: Moreover, the uniformity in input and output dimensions simplifies the architecture
    by enabling the stacking of multiple layers, as we will do later, without the
    need to adjust dimensions between them, thus making the model more scalable.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输入和输出维度的一致性简化了架构，使我们能够堆叠多个层，而无需在它们之间调整维度，从而使模型更具可扩展性。
- en: As illustrated in Figure 4.11, we have now implemented most of the LLM's building
    blocks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.11 所示，我们现在已实现了大部分 LLM 的构建块。
- en: Figure 4.11 A mental model showing the topics we cover in this chapter, with
    the black checkmarks indicating those that we have already covered.
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.11 显示了我们在本章中涵盖的主题的心理模型，黑色勾选标记表示我们已经覆盖的内容。
- en: '![](images/04__image021.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image021.png)'
- en: In the next section, we will go over the concept of shortcut connections that
    we insert between different layers of a neural network, which are important for
    improving the training performance in deep neural network architectures.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨我们在神经网络的不同层之间插入的**快捷连接**概念，这对提高深度神经网络架构的训练性能至关重要。
- en: 4.4 Adding shortcut connections
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 添加快捷连接
- en: Next, let's discuss the concept behind *shortcut connections*, also known as
    skip or residual connections. Originally, shortcut connections were proposed for
    deep networks in computer vision (specifically, in residual networks) to mitigate
    the challenge of vanishing gradients. The vanishing gradient problem refers to
    the issue where gradients (which guide weight updates during training) become
    progressively smaller as they propagate backward through the layers, making it
    difficult to effectively train earlier layers, as illustrated in Figure 4.12.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论*快捷连接*的概念，也称为跳过连接或残差连接。最初，快捷连接是为计算机视觉中的深度网络（特别是残差网络）提出的，以缓解消失梯度的问题。消失梯度问题是指梯度（指导训练期间权重更新的量）在反向传播时逐渐变小，导致较早层的有效训练变得困难，如图
    4.12 所示。
- en: Figure 4.12 A comparison between a deep neural network consisting of 5 layers
    without (on the left) and with shortcut connections (on the right). Shortcut connections
    involve adding the inputs of a layer to its outputs, effectively creating an alternate
    path that bypasses certain layers. The gradient illustrated in Figure 1.1 denotes
    the mean absolute gradient at each layer, which we will compute in the code example
    that follows.
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.12 显示了一个包含 5 层的深度神经网络的比较，左侧是没有（左边）快捷连接，右侧是有快捷连接。快捷连接涉及将层的输入添加到其输出中，实际上创建了一条绕过某些层的替代路径。图
    1.1 中所示的梯度表示每层的平均绝对梯度，我们将在随后的代码示例中计算。
- en: '![](images/04__image023.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image023.png)'
- en: As illustrated in Figure 4.12, a shortcut connection creates an alternative,
    shorter path for the gradient to flow through the network by skipping one or more
    layers, which is achieved by adding the output of one layer to the output of a
    later layer. This is why these connections are also known as skip connections.
    They play a crucial role in preserving the flow of gradients during the backward
    pass in training.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.12 所示，快捷连接通过跳过一个或多个层为梯度提供了一个替代的、更短的流动路径，这通过将一层的输出添加到后续层的输出来实现。这就是这些连接也被称为跳过连接的原因。它们在训练中的反向传播中保持梯度流动起着至关重要的作用。
- en: 'In the code example below, we implement the neural network shown in Figure
    4.12 to see how we can add shortcut connections in the `forward` method:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们实现了图 4.12 中显示的神经网络，以查看如何在`forward`方法中添加快捷连接：
- en: Listing 4.5 A neural network to illustrate shortcut connections
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 4.5 一个神经网络以说明快捷连接
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The code implements a deep neural network with 5 layers, each consisting of
    a `Linear` layer and a `GELU` activation function. In the forward pass, we iteratively
    pass the input through the layers and optionally add the shortcut connections
    depicted in Figure 4.12 if the `self.use_shortcut` attribute is set to `True`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码实现了一个具有5层的深度神经网络，每层由一个`Linear`层和一个`GELU`激活函数组成。在前向传递中，我们迭代地将输入通过各层传递，如果`self.use_shortcut`属性设置为`True`，则可选择添加图4.12所示的跳跃连接。
- en: 'Let''s use this code to first initialize a neural network without shortcut
    connections. Here, each layer will be initialized such that it accepts an example
    with 3 input values and returns 3 output values. The last layer returns a single
    output value:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这段代码首先初始化一个没有跳跃连接的神经网络。在这里，每一层将被初始化为接受具有3个输入值的示例，并返回3个输出值。最后一层返回一个单一的输出值：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we implement a function that computes the gradients in the the model''s
    backward pass:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个函数来计算模型的反向传播中的梯度：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding code, we specify a loss function that computes how close the
    model output and a user-specified target (here, for simplicity, the value 0) are.
    Then, when calling `loss.backward()`, PyTorch computes the loss gradient for each
    layer in the model. We can iterate through the weight parameters via `model.named_parameters()`.
    Suppose we have a 3×3 weight parameter matrix for a given layer. In that case,
    this layer will have 3×3 gradient values, and we print the mean absolute gradient
    of these 3×3 gradient values to obtain a single gradient value per layer to compare
    the gradients between layers more easily.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们指定了一个损失函数，用于计算模型输出与用户指定目标（这里为了简单起见，目标值为0）之间的接近程度。然后，当调用`loss.backward()`时，PyTorch会计算模型中每一层的损失梯度。我们可以通过`model.named_parameters()`遍历权重参数。假设我们有一个3×3的权重参数矩阵，对于给定的层，这一层将具有3×3的梯度值，我们打印这3×3梯度值的平均绝对梯度，以便为每一层获得一个单一的梯度值，从而更容易比较各层之间的梯度。
- en: In short, the `.backward()` method is a convenient method in PyTorch that computes
    loss gradients, which are required during model training, without implementing
    the math for the gradient calculation ourselves, thereby making working with deep
    neural networks much more accessible. If you are unfamiliar with the concept of
    gradients and neural network training, I recommend reading sections *A.4, Automatic
    differentiation made easy* and *A.7 A typical training loop* in *appendix A*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，`.backward()`方法是PyTorch中的一个方便方法，用于计算在模型训练过程中所需的损失梯度，而不需要我们自己实现梯度计算的数学公式，从而使得处理深度神经网络变得更加容易。如果你对梯度和神经网络训练的概念不熟悉，建议阅读*附录A*中的*A.4，自动微分简化*和*A.7，典型训练循环*部分。
- en: 'Let''s now use the `print_gradients` function and apply it to the model without
    skip connections:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`print_gradients`函数并将其应用于没有跳跃连接的模型：
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we can see based on the output of the `print_gradients` function, the gradients
    become smaller as we progress from the last layer (`layers.4`) to the first layer
    (`layers.0`), which is a phenomenon called the vanishing gradient problem.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从`print_gradients`函数的输出中看到的，随着我们从最后一层（`layers.4`）向第一层（`layers.0`）推进，梯度变得越来越小，这种现象被称为消失梯度问题。
- en: 'Let''s now instantiate a model with skip connections and see how it compares:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实例化一个带有跳跃连接的模型，看看它的表现如何：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As we can see, based on the output, the last layer `(layers.4`) still has a
    larger gradient than the other layers. However, the gradient value stabilizes
    as we progress towards the first layer (`layers.0`) and doesn't shrink to a vanishingly
    small value.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，根据输出，最后一层（`layers.4`）的梯度仍然大于其他层。然而，随着我们向第一层（`layers.0`）推进，梯度值趋于稳定，并不会缩小到消失的极小值。
- en: In conclusion, shortcut connections are important for overcoming the limitations
    posed by the vanishing gradient problem in deep neural networks. Shortcut connections
    are a core building block of very large models such as LLMs, and they will help
    facilitate more effective training by ensuring consistent gradient flow across
    layers when we train the GPT model in the next chapter.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，跳跃连接对于克服深度神经网络中的消失梯度问题的限制非常重要。跳跃连接是非常大型模型（例如LLMs）的核心构建块，它们将有助于在下一个章节中训练GPT模型时确保各层之间的梯度流动一致，从而促进更有效的训练。
- en: After introducing shortcut connections, we will now connect all of the previously
    covered concepts (layer normalization, GELU activations, feed forward module,
    and shortcut connections) in a transformer block in the next section, which is
    the final building block we need to code the GPT architecture.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入快捷连接后，我们将在下一节中将之前讨论的所有概念（层归一化、GELU 激活、前馈模块和快捷连接）连接起来，这也是我们编写 GPT 架构所需的最终构建块。
- en: 4.5 Connecting attention and linear layers in a transformer block
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 在 transformer block 中连接注意力层和线性层
- en: 'In this section, we are implementing the *transformer block*, a fundamental
    building block of GPT and other LLM architectures. This block, which is repeated
    a dozen times in the 124 million parameter GPT-2 architecture, combines several
    concepts we have previously covered: multi-head attention, layer normalization,
    dropout, feed forward layers, and GELU activations, as illustrated in Figure 4.13\.
    In the next section, we will then connect this transformer block to the remaining
    parts of the GPT architecture.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将实现 *transformer block*，这是 GPT 和其他 LLM 架构的基本构建块。这个块在参数为 1.24 亿的 GPT-2
    架构中重复了十几次，结合了我们之前讨论的几个概念：多头注意力、层归一化、丢弃法、前馈层和 GELU 激活，如图 4.13 所示。在下一节中，我们将把这个 transformer
    block 连接到 GPT 架构的其余部分。
- en: Figure 4.13 An illustration of a transformer block. The bottom of the diagram
    shows input tokens that have been embedded into 768-dimensional vectors. Each
    row corresponds to one token's vector representation. The outputs of the transformer
    block are vectors of the same dimension as the input, which can then be fed into
    subsequent layers in an LLM.
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.13 transformer block 的示意图。图的底部显示已嵌入到 768 维向量中的输入标记。每一行对应一个标记的向量表示。transformer
    block 的输出是与输入相同维度的向量，这些向量可以进一步输入到 LLM 的后续层中。
- en: '![](images/04__image025.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image025.png)'
- en: As shown in Figure 4.13, the transformer block combines several components,
    including the masked multi-head attention module from chapter 3 and the `FeedForward`
    module we implemented in Section 4.3.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.13 所示，transformer block 结合了多个组件，包括第 3 章中的遮蔽多头注意力模块和我们在第 4.3 节实现的 `FeedForward`
    模块。
- en: When a transformer block processes an input sequence, each element in the sequence
    (for example, a word or subword token) is represented by a fixed-size vector (in
    the case of Figure 4.13, 768 dimensions). The operations within the transformer
    block, including multi-head attention and feed forward layers, are designed to
    transform these vectors in a way that preserves their dimensionality.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当 transformer block 处理输入序列时，序列中的每个元素（例如，一个单词或子词标记）都由一个固定大小的向量表示（在图 4.13 中为 768
    维）。transformer block 内的操作，包括多头注意力和前馈层，旨在以保留其维度的方式转换这些向量。
- en: The idea is that the self-attention mechanism in the multi-head attention block
    identifies and analyzes relationships between elements in the input sequence.
    In contrast, the feed forward network modifies the data individually at each position.
    This combination not only enables a more nuanced understanding and processing
    of the input but also enhances the model's overall capacity for handling complex
    data patterns.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 其思想是，多头注意力模块中的自注意力机制识别并分析输入序列中元素之间的关系。相比之下，前馈网络在每个位置上单独修改数据。这种组合不仅使输入的理解和处理更加细致，也增强了模型处理复杂数据模式的整体能力。
- en: 'In code, we can create the `TransformerBlock` as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以如下创建 `TransformerBlock`：
- en: Listing 4.6 The transformer block component of GPT
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.6 GPT 的 transformer block 组件
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The given code defines a `TransformerBlock` class in PyTorch that includes a
    multi-head attention mechanism (`MultiHeadAttention`) and a feed forward network
    (`FeedForward`), both configured based on a provided configuration dictionary
    (`cfg`), such as `GPT_CONFIG_124M`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的代码在 PyTorch 中定义了一个 `TransformerBlock` 类，其中包含一个多头注意力机制（`MultiHeadAttention`）和一个前馈网络（`FeedForward`），两者均基于提供的配置字典（`cfg`）进行配置，例如
    `GPT_CONFIG_124M`。
- en: Layer normalization (`LayerNorm`) is applied before each of these two components,
    and dropout is applied after them to regularize the model and prevent overfitting.
    This is also known as *Pre-LayerNorm*. Older architectures, such as the original
    transformer model, applied layer normalization after the self-attention and feed-forward
    networks instead, known as *Post-LayerNorm*, which often leads to worse training
    dynamics.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个组件之前应用层归一化（`LayerNorm`），并在它们之后应用 dropout，以对模型进行正则化并防止过拟合。这也被称为 *Pre-LayerNorm*。较旧的架构，如原始的变换器模型，通常在自注意力和前馈网络之后应用层归一化，称为
    *Post-LayerNorm*，这往往导致更糟糕的训练动态。
- en: The class also implements the forward pass, where each component is followed
    by a shortcut connection that adds the input of the block to its output. This
    critical feature helps gradients flow through the network during training and
    improves the learning of deep models as explained in section 4.4.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 该类还实现了前向传播，其中每个组件后面都有一个快捷连接，将模块的输入添加到其输出中。这个关键特性帮助梯度在训练过程中通过网络流动，并改善深度模型的学习，如第
    4.4 节所述。
- en: 'Using the `GPT_CONFIG_124M` dictionary we defined earlier, let''s instantiate
    a transformer block and feed it some sample data:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前定义的 `GPT_CONFIG_124M` 字典，让我们实例化一个变换器模块并输入一些样本数据：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As we can see from the code output, the transformer block maintains the input
    dimensions in its output, indicating that the transformer architecture processes
    sequences of data without altering their shape throughout the network.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码输出中可以看出，变换器模块在其输出中保持输入维度，表明变换器架构处理数据序列时不会改变其形状。
- en: The preservation of shape throughout the transformer block architecture is not
    incidental but a crucial aspect of its design. This design enables its effective
    application across a wide range of sequence-to-sequence tasks, where each output
    vector directly corresponds to an input vector, maintaining a one-to-one relationship.
    However, the output is a context vector that encapsulates information from the
    entire input sequence, as we learned in chapter 3\. This means that while the
    physical dimensions of the sequence (length and feature size) remain unchanged
    as it passes through the transformer block, the content of each output vector
    is re-encoded to integrate contextual information from across the entire input
    sequence.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器模块架构中保持形状不变并不是偶然，而是其设计的一个重要方面。这个设计使其能够有效地应用于各种序列到序列任务，其中每个输出向量直接对应于一个输入向量，保持一一对应的关系。然而，输出是一个上下文向量，封装了整个输入序列的信息，正如我们在第
    3 章中学到的。这意味着，虽然序列的物理维度（长度和特征大小）在通过变换器模块时保持不变，但每个输出向量的内容被重新编码，以整合来自整个输入序列的上下文信息。
- en: With the transformer block implemented in this section, we now have all the
    building blocks, as shown in Figure 4.14, needed to implement the GPT architecture
    in the next section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节实现的变换器模块中，我们现在拥有了实现 GPT 架构所需的所有构建块，如图 4.14 所示。
- en: Figure 4.14 A mental model of the different concepts we have implemented in
    this chapter so far.
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.14 是我们迄今在本章中实现的不同概念的心理模型。
- en: '![](images/04__image027.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image027.png)'
- en: As illustrated in Figure 4.14, the transformer block combines layer normalization,
    the feed forward network, including GELU activations, and shortcut connections,
    which we already covered earlier in this chapter. As we will see in the upcoming
    chapter, this transformer block will make up the main component of the GPT architecture
    we will implement
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.14 所示，变换器模块结合了层归一化、前馈网络（包括 GELU 激活函数）以及之前在本章中介绍的快捷连接。正如我们将在接下来的章节中看到的，这个变换器模块将构成我们将要实现的
    GPT 架构的主要组件。
- en: 4.6 Coding the GPT model
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 编码 GPT 模型
- en: We started this chapter with a big-picture overview of a GPT architecture that
    we called `DummyGPTModel`. In this `DummyGPTModel` code implementation, we showed
    the input and outputs to the GPT model, but its building blocks remained a black
    box using a `DummyTransformerBlock` and `DummyLayerNorm` class as placeholders.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们对一个称为 `DummyGPTModel` 的 GPT 架构进行了概述。在这个 `DummyGPTModel` 代码实现中，我们展示了输入和输出到
    GPT 模型，但其构建块仍然是一个黑箱，使用 `DummyTransformerBlock` 和 `DummyLayerNorm` 类作为占位符。
- en: In this section, we are now replacing the `DummyTransformerBlock` and `DummyLayerNorm`
    placeholders with the real `TransformerBlock` and `LayerNorm` classes we coded
    later in this chapter to assemble a fully working version of the original 124
    million parameter version of GPT-2\. In chapter 5, we will pretrain a GPT-2 model,
    and in chapter 6, we will load in the pretrained weights from OpenAI.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们现在用本章稍后编码的真实 `TransformerBlock` 和 `LayerNorm` 类替换 `DummyTransformerBlock`
    和 `DummyLayerNorm` 占位符，以组装出原始 1.24 亿参数版本的 GPT-2 的完整工作版本。在第 5 章中，我们将对 GPT-2 模型进行预训练，而在第
    6 章中，我们将加载来自 OpenAI 的预训练权重。
- en: Before we assemble the GPT-2 model in code, let's look at its overall structure
    in Figure 4.15, which combines all the concepts we covered so far in this chapter.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们用代码组装 GPT-2 模型之前，让我们看看图 4.15 中的整体结构，它结合了本章迄今为止涵盖的所有概念。
- en: Figure 4.15 An overview of the GPT model architecture. This figure illustrates
    the flow of data through the GPT model. Starting from the bottom, tokenized text
    is first converted into token embeddings, which are then augmented with positional
    embeddings. This combined information forms a tensor that is passed through a
    series of transformer blocks shown in the center (each containing multi-head attention
    and feed forward neural network layers with dropout and layer normalization),
    which are stacked on top of each other and repeated 12 times.
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.15 GPT 模型架构的概述。该图说明了数据在 GPT 模型中的流动。从底部开始，标记化文本首先转换为标记嵌入，然后与位置嵌入结合。这些结合的信息形成一个张量，通过中心显示的一系列变换器块（每个块包含多头注意力和具有
    dropout 和层归一化的前馈神经网络层）传递，这些块相互堆叠并重复 12 次。
- en: '![](images/04__image029.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image029.png)'
- en: As shown in Figure 4.15, the transformer block we coded in Section 4.5 is repeated
    many times throughout a GPT model architecture. In the case of the 124 million
    parameter GPT-2 model, it's repeated 12 times, which we specify via the `"n_layers"`
    entry in the `GPT_CONFIG_124M` dictionary. In the case of the largest GPT-2 model
    with 1,542 million parameters, this transformer block is repeated 36 times.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.15 所示，我们在第 4.5 节中编码的变换器块在 GPT 模型架构中重复多次。在 1.24 亿参数的 GPT-2 模型中，它重复 12 次，我们通过
    `GPT_CONFIG_124M` 字典中的 `"n_layers"` 条目指定。在参数数量达到 1,542 亿的最大 GPT-2 模型中，这个变换器块重复了
    36 次。
- en: As shown in Figure 4.15, the output from the final transformer block then goes
    through a final layer normalization step before reaching the linear output layer.
    This layer maps the transformer's output to a high-dimensional space (in this
    case, 50,257 dimensions, corresponding to the model's vocabulary size) to predict
    the next token in the sequence.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.15 所示，最终变换器块的输出经过最后一个层归一化步骤后，才到达线性输出层。该层将变换器的输出映射到高维空间（在本例中为 50,257 维，对应于模型的词汇大小），以预测序列中的下一个标记。
- en: 'Let''s now implement the architecture we see in Figure 4.15 in code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在代码中实现图 4.15 中看到的架构：
- en: Listing 4.7 The GPT model architecture implementation
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.7 GPT 模型架构实现
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Thanks to the `TransformerBlock` class we implemented in Section 4.5, the `GPTModel`
    class is relatively small and compact.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了我们在第 4.5 节中实现的 `TransformerBlock` 类，`GPTModel` 类相对较小且紧凑。
- en: The `__init__` constructor of this `GPTModel` class initializes the token and
    positional embedding layers using the configurations passed in via a Python dictionary,
    `cfg`. These embedding layers are responsible for converting input token indices
    into dense vectors and adding positional information, as discussed in chapter
    2.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此 `GPTModel` 类的 `__init__` 构造函数使用通过 Python 字典 `cfg` 传入的配置初始化标记和位置嵌入层。这些嵌入层负责将输入标记索引转换为稠密向量，并添加位置信息，如第
    2 章所讨论的。
- en: Next, the `__init__` method creates a sequential stack of `TransformerBlock`
    modules equal to the number of layers specified in `cfg`. Following the transformer
    blocks, a `LayerNorm` layer is applied, standardizing the outputs from the transformer
    blocks to stabilize the learning process. Finally, a linear output head without
    bias is defined, which projects the transformer's output into the vocabulary space
    of the tokenizer to generate logits for each token in the vocabulary.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`__init__` 方法创建一个与 `cfg` 中指定的层数相等的 `TransformerBlock` 模块的顺序堆栈。在变换器块之后，应用
    `LayerNorm` 层，对变换器块的输出进行标准化，以稳定学习过程。最后，定义一个没有偏差的线性输出头，将变换器的输出投影到标记器的词汇空间，以为词汇中的每个标记生成
    logits。
- en: The forward method takes a batch of input token indices, computes their embeddings,
    applies the positional embeddings, passes the sequence through the transformer
    blocks, normalizes the final output, and then computes the logits, representing
    the next token's unnormalized probabilities. We will convert these logits into
    tokens and text outputs in the next section.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 前向方法接收一批输入令牌索引，计算它们的嵌入，应用位置嵌入，将序列通过变换器块传递，标准化最终输出，然后计算对数值，表示下一个令牌的未归一化概率。我们将在下一部分将这些对数值转换为令牌和文本输出。
- en: 'Let''s now initialize the 124 million parameter GPT model using the `GPT_CONFIG_124M`
    dictionary we pass into the cfg parameter and feed it with the batch text input
    we created at the beginning of this chapter:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们传入cfg参数的`GPT_CONFIG_124M`字典初始化1.24亿参数的GPT模型，并用本章开头创建的批量文本输入进行填充：
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code prints the contents of the input batch followed by the output
    tensor:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码打印了输入批次的内容，后面是输出张量：
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As we can see, the output tensor has the shape `[2, 4, 50257]`, since we passed
    in 2 input texts with 4 tokens each. The last dimension, 50,257, corresponds to
    the vocabulary size of the tokenizer. In the next section, we will see how to
    convert each of these 50,257-dimensional output vectors back into tokens.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，输出张量的形状为`[2, 4, 50257]`，因为我们传入了2个输入文本，每个文本包含4个令牌。最后一个维度50,257对应于标记器的词汇大小。在下一部分中，我们将看到如何将这50,257维的输出向量转换回令牌。
- en: Before we move on to the next section and code the function that converts the
    model outputs into text, let's spend a bit more time with the model architecture
    itself and analyze its size.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一部分并编写将模型输出转换为文本的函数之前，让我们花点时间来分析模型架构本身及其大小。
- en: 'Using the `numel()` method, short for "number of elements," we can collect
    the total number of parameters in the model''s parameter tensors:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`numel()`方法，即“元素数量”的缩写，我们可以收集模型参数张量中的总参数数量：
- en: '[PRE34]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The result is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE35]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now, a curious reader might notice a discrepancy. Earlier, we spoke of initializing
    a 124 million parameter GPT model, so why is the actual number of parameters 163
    million, as shown in the preceding code output?
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，好奇的读者可能会注意到一个差异。我们之前提到初始化一个包含1.24亿参数的GPT模型，那么为什么实际的参数数量是1.63亿，如前面的代码输出所示？
- en: 'The reason is a concept called weight tying that is used in the original GPT-2
    architecture, which means that the original GPT-2 architecture is reusing the
    weights from the token embedding layer inits output layer. To understand what
    this means, let''s take a look at the shapes of the token embedding layer and
    linear output layer that we initialized on the `model` via the `GPTModel` earlier:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在原始GPT-2架构中使用了一个叫做权重绑定的概念，这意味着原始GPT-2架构在其输出层中重用了令牌嵌入层的权重。为了理解这意味着什么，让我们看看之前通过`GPTModel`在`model`上初始化的令牌嵌入层和线性输出层的形状：
- en: '[PRE36]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As we can see based on the print outputs, the weight tensors for both these
    layers have the same shape:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们根据打印输出所看到的，这两层的权重张量具有相同的形状：
- en: '[PRE37]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The token embedding and output layers are very large due to the number of rows
    for the 50,257 in the tokenizer''s vocabulary. Let''s remove the output layer
    parameter count from the total GPT-2 model count according to the weight tying:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标记器词汇中50,257行的数量，令牌嵌入和输出层非常大。让我们根据权重绑定从总的GPT-2模型计数中去除输出层参数计数：
- en: '[PRE38]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE39]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As we can see, the model is now only 124 million parameters large, matching
    the original size of the GPT-2 model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，该模型现在只有1.24亿个参数，匹配原始GPT-2模型的大小。
- en: Weight tying reduces the overall memory footprint and computational complexity
    of the model. However, in my experience, using separate token embedding and output
    layers results in better training and model performance; hence, we are using separate
    layers in our `GPTModel` implementation. The same is true for modern LLMs. However,
    we will revisit and implement the weight tying concept later in chapter 6 when
    we load the pretrained weights from OpenAI.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 权重绑定减少了模型的整体内存占用和计算复杂度。然而，根据我的经验，使用单独的令牌嵌入层和输出层可以获得更好的训练和模型性能；因此，我们在`GPTModel`实现中使用单独的层。现代大型语言模型也是如此。然而，我们将在第6章中重新审视并实现权重绑定的概念，当时我们将从OpenAI加载预训练的权重。
- en: Exercise 4.1 Number of parameters in feed forward and attention modules
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习4.1 前馈和注意模块中的参数数量
- en: Calculate and compare the number of parameters that are contained in the feed
    forward module and those that are contained in the multi-head attention module.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 计算并比较前馈模块和多头注意力模块中包含的参数数量。
- en: 'Lastly, let us compute the memory requirements of the 163 million parameters
    in our `GPTModel` object:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们计算我们`GPTModel`对象中1.63亿个参数的内存需求：
- en: '[PRE40]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The result is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE41]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In conclusion, by calculating the memory requirements for the 163 million parameters
    in our `GPTModel` object and assuming each parameter is a 32-bit float taking
    up 4 bytes, we find that the total size of the model amounts to 621.83 MB, illustrating
    the relatively large storage capacity required to accommodate even relatively
    small LLMs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，通过计算我们`GPTModel`对象中1.63亿个参数的内存需求，并假设每个参数为占用4字节的32位浮点数，我们发现模型的总大小达到了621.83
    MB，这说明即使是相对较小的LLM也需要相对较大的存储容量。
- en: In this section, we implemented the GPTModel architecture and saw that it outputs
    numeric tensors of shape `[batch_size, num_tokens, vocab_size]`. In the next section,
    we will write the code to convert these output tensors into text.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们实现了GPTModel架构，并看到它输出形状为`[batch_size, num_tokens, vocab_size]`的数字张量。在下一节中，我们将编写代码将这些输出张量转换为文本。
- en: Exercise 4.2 Initializing larger GPT models
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习4.2 初始化更大的GPT模型
- en: In this chapter, we initialized a 124 million parameter GPT model, which is
    known as "GPT-2 small." Without making any code modifications besides updating
    the configuration file, use the GPTModel class to implement GPT-2 medium (using
    1024-dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads),
    GPT-2 large (1280-dimensional embeddings, 36 transformer blocks, 20 multi-head
    attention heads), and GPT-2 XL (1600-dimensional embeddings, 48 transformer blocks,
    25 multi-head attention heads). As a bonus, calculate the total number of parameters
    in each GPT model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们初始化了一个124百万参数的GPT模型，称为“GPT-2 small”。在不进行任何代码修改的情况下，仅更新配置文件，使用GPTModel类实现GPT-2
    medium（使用1024维嵌入、24个变换器块、16个多头注意力头）、GPT-2 large（1280维嵌入、36个变换器块、20个多头注意力头）和GPT-2
    XL（1600维嵌入、48个变换器块、25个多头注意力头）。作为附加内容，计算每个GPT模型中的总参数数量。
- en: 4.7 Generating text
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 生成文本
- en: In this final section of this chapter, we will implement the code that converts
    the tensor outputs of the GPT model back into text. Before we get started, let's
    briefly review how a generative model like an LLM generates text one word (or
    token) at a time, as shown in Figure 4.16.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节中，我们将实现将GPT模型的张量输出转换回文本的代码。在开始之前，让我们简要回顾一下生成模型（如LLM）如何逐字（或逐标记）生成文本，如图4.16所示。
- en: Figure 4.16 This diagram illustrates the step-by-step process by which an LLM
    generates text, one token at a time. Starting with an initial input context ("Hello,
    I am"), the model predicts a subsequent token during each iteration, appending
    it to the input context for the next round of prediction. As shown, the first
    iteration adds "a", the second "model", and the third "ready", progressively building
    the sentence.
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.16此图展示了LLM逐步生成文本的过程，每次生成一个标记。从初始输入上下文（“你好，我是”）开始，模型在每次迭代中预测下一个标记，并将其附加到输入上下文中进行下一轮预测。如图所示，第一次迭代添加了“一个”，第二次添加了“模型”，第三次添加了“准备”，逐步构建完整句子。
- en: '![](images/04__image031.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image031.png)'
- en: 'Figure 4.16 illustrates the step-by-step process by which a GPT model generates
    text given an input context, such as "Hello, I am," on a big-picture level. With
    each iteration, the input context grows, allowing the model to generate coherent
    and contextually appropriate text. By the 6th iteration, the model has constructed
    a complete sentence: "Hello, I am a model ready to help."'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16展示了GPT模型在给定输入上下文（如“你好，我是”）时生成文本的逐步过程。从大局来看，随着每次迭代，输入上下文不断增长，使模型能够生成连贯且符合上下文的文本。在第6次迭代时，模型构建了完整的句子：“你好，我是一个准备帮助的模型。”
- en: In the previous section, we saw that our current `GPTModel` implementation outputs
    tensors with shape `[batch_size, num_token, vocab_size]`. Now, the question is,
    how does a GPT model go from these output tensors to the generated text shown
    in Figure 4.16?
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到当前的`GPTModel`实现输出形状为`[batch_size, num_token, vocab_size]`的张量。现在的问题是，GPT模型如何将这些输出张量转换为图4.16所示的生成文本？
- en: The process by which a GPT model goes from output tensors to generated text
    involves several steps, as illustrated in Figure 4.17\. These steps include decoding
    the output tensors, selecting tokens based on a probability distribution, and
    converting these tokens into human-readable text.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型从输出张量到生成文本的过程涉及多个步骤，如图4.17所示。这些步骤包括解码输出张量、根据概率分布选择令牌，并将这些令牌转换为人类可读的文本。
- en: Figure 4.17 details the mechanics of text generation in a GPT model by showing
    a single iteration in the token generation process. The process begins by encoding
    the input text into token IDs, which are then fed into the GPT model. The outputs
    of the model are then converted back into text and appended to the original input
    text.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.17详细说明了GPT模型中文本生成的机制，通过展示令牌生成过程中的单次迭代。该过程首先将输入文本编码为令牌ID，然后将其输入到GPT模型中。模型的输出随后被转换回文本并附加到原始输入文本中。
- en: '![](images/04__image033.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image033.png)'
- en: The next-token generation process detailed in Figure 4.17 illustrates a single
    step where the GPT model generates the next token given its input.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17详细说明的下一个令牌生成过程展示了GPT模型在给定输入的情况下生成下一个令牌的单个步骤。
- en: In each step, the model outputs a matrix with vectors representing potential
    next tokens. The vector corresponding to the next token is extracted and converted
    into a probability distribution via the softmax function. Within the vector containing
    the resulting probability scores, the index of the highest value is located, which
    translates to the token ID. This token ID is then decoded back into text, producing
    the next token in the sequence. Finally, this token is appended to the previous
    inputs, forming a new input sequence for the subsequent iteration. This step-by-step
    process enables the model to generate text sequentially, building coherent phrases
    and sentences from the initial input context.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中，模型输出一个矩阵，其中向量代表潜在的下一个令牌。对应于下一个令牌的向量被提取，并通过softmax函数转换为概率分布。在包含结果概率分数的向量中，定位最高值的索引，该索引对应于令牌ID。然后将此令牌ID解码回文本，从而生成序列中的下一个令牌。最后，该令牌被附加到先前的输入中，形成下一次迭代的新输入序列。这个逐步的过程使得模型能够顺序生成文本，从初始输入上下文中构建连贯的短语和句子。
- en: In practice, we repeat this process over many iterations, such as shown in Figure
    4.16 earlier, until we reach a user-specified number of generated tokens.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在多个迭代中重复此过程，如图4.16所示，直到达到用户指定的生成令牌数量。
- en: 'In code, we can implement the token-generation process as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以按如下方式实现令牌生成过程：
- en: Listing 4.8 A function for the GPT model to generate text
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单4.8 为GPT模型生成文本的函数
- en: '[PRE42]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The code snippet provided demonstrates a simple implementation of a generative
    loop for a language model using PyTorch. It iterates for a specified number of
    new tokens to be generated, crops the current context to fit the model's maximum
    context size, computes predictions and then selects the next token based on the
    highest probability prediction.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的代码片段演示了使用PyTorch为语言模型实现生成循环的简单实现。它循环指定数量的新令牌生成，裁剪当前上下文以适应模型的最大上下文大小，计算预测，然后根据最高概率预测选择下一个令牌。
- en: In the preceeding code, the `generate_text_simple` function, we use a softmax
    function to convert the logits into a probability distribution from which we identify
    the position with the highest value via `torch.argmax`. The softmax function is
    monotonic, meaning it preserves the order of its inputs when transformed into
    outputs. So, in practice, the softmax step is redundant since the position with
    the highest score in the softmax output tensor is the same position in the logit
    tensor. In other words, we could apply the `torch.argmax` function to the logits
    tensor directly and get identical results. However, we coded the conversion to
    illustrate the full process of transforming logits to probabilities, which can
    add additional intuition, such as that the model generates the most likely next
    token, which is known as *greedy decoding*.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`generate_text_simple`函数中，我们使用softmax函数将logits转换为概率分布，从中通过`torch.argmax`识别最高值的位置。softmax函数是单调的，这意味着它在转换为输出时保持输入的顺序。因此，实际上，softmax步骤是多余的，因为softmax输出张量中最高分数的位置与logit张量中的相同位置。换句话说，我们可以直接将`torch.argmax`函数应用于logits张量，并获得相同的结果。然而，我们进行了转换编码，以展示从logits到概率的完整过程，这可以增加额外的直觉，例如模型生成最可能的下一个令牌，这被称为*贪婪解码*。
- en: In the next chapter, when we will implement the GPT training code, we will also
    introduce additional sampling techniques where we modify the softmax outputs such
    that the model doesn't always select the most likely token, which introduces variability
    and creativity in the generated text.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，当我们实现 GPT 训练代码时，我们还将介绍其他采样技术，通过修改 softmax 输出，使模型不会总是选择最可能的令牌，从而在生成的文本中引入变异性和创造性。
- en: This process of generating one token ID at a time and appending it to the context
    using the `generate_text_simple` function is further illustrated in Figure 4.18\.
    (The token ID generation process for each iteration is detailed in Figure 4.17.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 逐个生成令牌 ID 并将其附加到上下文中的这个过程，使用 `generate_text_simple` 函数进一步在图 4.18 中进行了说明。（每次迭代的令牌
    ID 生成过程详见图 4.17。）
- en: Figure 4.18 An illustration showing six iterations of a token prediction cycle,
    where the model takes a sequence of initial token IDs as input, predicts the next
    token, and appends this token to the input sequence for the next iteration. (The
    token IDs are also translated into their corresponding text for better understanding.)
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.18 显示了令牌预测周期的六次迭代的示例，其中模型将初始令牌 ID 的序列作为输入，预测下一个令牌，并将该令牌附加到输入序列中以进行下一次迭代。（令牌
    ID 也被翻译为相应的文本以便于理解。）
- en: '![](images/04__image035.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](images/04__image035.png)'
- en: As shown in Figure 4.18, we generate the token IDs in an iterative fashion.
    For instance, in iteration 1, the model is provided with the tokens corresponding
    to "Hello , I am", predicts the next token (with ID 257, which is "a"), and appends
    it to the input. This process is repeated until the model produces the complete
    sentence "Hello, I am a model ready to help." after six iterations.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.18 所示，我们以迭代的方式生成令牌 ID。例如，在第 1 次迭代中，模型提供了与“Hello, I am”对应的令牌，预测下一个令牌（ID
    为 257，即“a”），并将其附加到输入中。这个过程会重复，直到模型在六次迭代后生成完整的句子“Hello, I am a model ready to help.”。
- en: Let's now try out the `generate_text_simple` function with the `"Hello, I am"`
    context as model input, as shown in Figure 4.18, in practice.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在实践中尝试使用图 4.18 中的“Hello, I am”上下文作为模型输入的 `generate_text_simple` 函数。
- en: 'First, we encode the input context into token IDs:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将输入上下文编码为令牌 ID：
- en: '[PRE43]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The encoded IDs are as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 编码的 ID 如下：
- en: '[PRE44]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, we put the model into `.eval()` mode, which disables random components
    like dropout, which are only used during training, and use the `generate_text_simple`
    function on the encoded input tensor:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将模型置于 `.eval()` 模式，禁用像 dropout 这样的随机组件，这些组件仅在训练期间使用，并在编码输入张量上使用 `generate_text_simple`
    函数：
- en: '[PRE45]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The resulting output token IDs are as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出令牌 ID 如下：
- en: '[PRE46]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Using the `.decode` method of the tokenizer, we can convert the IDs back into
    text:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 tokenizer 的 `.decode` 方法，我们可以将 ID 转换回文本：
- en: '[PRE47]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The model output in text format is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的文本格式输出如下：
- en: '[PRE48]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As we can see, based on the preceding output, the model generated gibberish,
    which is not at all like the coherent text shown in Figure 4.18\. What happened?
    The reason why the model is unable to produce coherent text is that we haven't
    trained it yet. So far, we just implemented the GPT architecture and initialized
    a GPT model instance with initial random weights.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，基于前面的输出，模型生成了杂乱无章的文本，完全不像图 4.18 中显示的连贯文本。这是怎么回事？模型无法生成连贯文本的原因是我们尚未训练它。到目前为止，我们只是实现了
    GPT 架构，并用初始随机权重初始化了一个 GPT 模型实例。
- en: Model training is a large topic in itself, and we will tackle it in the next
    chapter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练本身是一个庞大的主题，我们将在下一章进行探讨。
- en: Exercise 4.3 Using separate dropout parameters
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 4.3 使用单独的 dropout 参数
- en: 'At the beginning of this chapter, we defined a global `"drop_rate"` setting
    in the `GPT_CONFIG_124M` dictionary to set the dropout rate in various places
    throughout the GPTModel architecture. Change the code to specify a separate dropout
    value for the various dropout layers throughout the model architecture. (Hint:
    there are three distinct places where we used dropout layers: the embedding layer,
    shortcut layer, and multi-head attention module.)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们在 `GPT_CONFIG_124M` 字典中定义了一个全局的 `"drop_rate"` 设置，以便在 GPTModel 架构的各个地方设置
    dropout 率。请更改代码以为模型架构中的各种 dropout 层指定单独的 dropout 值。（提示：我们使用了三个不同的地方来应用 dropout
    层：嵌入层、快捷层和多头注意模块。）
- en: 4.8 Summary
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8 总结
- en: Layer normalization stabilizes training by ensuring that each layer's outputs
    have a consistent mean and variance.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层归一化通过确保每层的输出具有一致的均值和方差来稳定训练。
- en: Shortcut connections are connections that skip one or more layers by feeding
    the output of one layer directly to a deeper layer, which helps mitigate the vanishing
    gradient problem when training deep neural networks, such as LLMs.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快捷连接是通过将一个层的输出直接馈送到更深层来跳过一个或多个层的连接，这有助于缓解训练深度神经网络（例如LLMs）时的梯度消失问题。
- en: Transformer blocks are a core structural component of GPT models, combining
    masked multi-head attention modules with fully connected feed-forward networks
    that use the GELU activation function.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer模块是GPT模型的核心结构组件，结合了带掩码的多头注意力模块和使用GELU激活函数的全连接前馈网络。
- en: GPT models are LLMs with many repeated transformer blocks that have millions
    to billions of parameters.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT模型是具有许多重复的transformer模块的LLMs，参数数量从百万到十亿不等。
- en: GPT models come in various sizes, for example, 124, 345, 762, and 1542 million
    parameters, which we can implement with the same `GPTModel` Python class.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT模型有多种大小，例如，124、345、762和1542百万参数，我们可以使用相同的`GPTModel` Python类来实现。
- en: The text generation capability of a GPT-like LLM involves decoding output tensors
    into human-readable text by sequentially predicting one token at a time based
    on a given input context.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT类LLM的文本生成能力涉及通过基于给定输入上下文顺序预测一个token，将输出张量解码为人类可读的文本。
- en: Without training, a GPT model generates incoherent text, which underscores the
    importance of model training for coherent text generation, which is the topic
    of subsequent chapters.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有训练，GPT模型会生成不连贯的文本，这凸显了模型训练在生成连贯文本中的重要性，这是后续章节的主题。
