- en: 4 Implementing a GPT model from Scratch To Generate Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coding a GPT-like large language model (LLM) that can be trained to generate
    human-like text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing layer activations to stabilize neural network training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding shortcut connections in deep neural networks to train models more effectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing transformer blocks to create GPT models of various sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the number of parameters and storage requirements of GPT models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, you learned and coded the *multi-head attention* mechanism,
    one of the core components of LLMs. In this chapter, we will now code the other
    building blocks of an LLM and assemble them into a GPT-like model that we will
    train in the next chapter to generate human-like text, as illustrated in Figure
    4.1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 A mental model of the three main stages of coding an LLM, pretraining
    the LLM on a general text dataset, and finetuning it on a labeled dataset. This
    chapter focuses on implementing the LLM architecture, which we will train in the
    next chapter.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: The LLM architecture, referenced in Figure 4.1, consists of several building
    blocks that we will implement throughout this chapter. We will begin with a top-down
    view of the model architecture in the next section before covering the individual
    components in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Coding an LLM architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs, such as GPT (which stands for *Generative Pretrained Transformer*), are
    large deep neural network architectures designed to generate new text one word
    (or token) at a time. However, despite their size, the model architecture is less
    complicated than you might think, since many of its components are repeated, as
    we will see later. Figure 4.2 provides a top-down view of a GPT-like LLM, with
    its main components highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 A mental model of a GPT model. Next to the embedding layers, it consists
    of one or more transformer blocks containing the masked multi-head attention module
    we implemented in the previous chapter.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in Figure 4.2, we have already covered several aspects, such
    as input tokenization and embedding, as well as the masked multi-head attention
    module. The focus of this chapter will be on implementing the core structure of
    the GPT model, including its *transformer blocks*, which we will then train in
    the next chapter to generate human-like text.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we used smaller embedding dimensions for simplicity,
    ensuring that the concepts and examples could comfortably fit on a single page.
    Now, in this chapter, we are scaling up to the size of a small GPT-2 model, specifically
    the smallest version with 124 million parameters, as described in Radford *et
    al.*'s paper, "Language Models are Unsupervised Multitask Learners." Note that
    while the original report mentions 117 million parameters, this was later corrected.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6 will focus on loading pretrained weights into our implementation and
    adapting it for larger GPT-2 models with 345, 762, and 1,542 million parameters.
    In the context of deep learning and LLMs like GPT, the term "parameters" refers
    to the trainable weights of the model. These weights are essentially the internal
    variables of the model that are adjusted and optimized during the training process
    to minimize a specific loss function. This optimization allows the model to learn
    from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a neural network layer that is represented by a 2,048x2,048-dimensional
    matrix (or tensor) of weights, each element of this matrix is a parameter. Since
    there are 2,048 rows and 2,048 columns, the total number of parameters in this
    layer is 2,048 multiplied by 2,048, which equals 4,194,304 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 versus GPT-3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that we are focusing on GPT-2 because OpenAI has made the weights of the
    pretrained model publicly available, which we will load into our implementation
    in chapter 6\. GPT-3 is fundamentally the same in terms of model architecture,
    except that it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion
    parameters in GPT-3, and it is trained on more data. As of this writing, the weights
    for GPT-3 are not publicly available. GPT-2 is also a better choice for learning
    how to implement LLMs, as it can be run on a single laptop computer, whereas GPT-3
    requires a GPU cluster for training and inference. According to Lambda Labs, it
    would take 355 years to train GPT-3 on a single V100 datacenter GPU, and 665 years
    on a consumer RTX 8000 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the configuration of the small GPT-2 model via the following Python
    dictionary, which we will use in the code examples later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `GPT_CONFIG_124M` dictionary, we use concise variable names for clarity
    and to prevent long lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"vocab_size"` refers to a vocabulary of 50,257 words, as used by the BPE tokenizer
    from chapter 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"context_length"` denotes the maximum number of input tokens the model can
    handle, via the positional embeddings discussed in chapter 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"emb_dim"` represents the embedding size, transforming each token into a 768-dimensional
    vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"n_heads"` indicates the count of attention heads in the multi-head attention
    mechanism, as implemented in chapter 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"n_layers"` specifies the number of transformer blocks in the model, which
    will be elaborated on in upcoming sections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"drop_rate"` indicates the intensity of the dropout mechanism (0.1 implies
    a 10% drop of hidden units) to prevent overfitting, as covered in chapter 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"qkv_bias"` determines whether to include a bias vector in the `Linear` layers
    of the multi-head attention for query, key, and value computations. We will initially
    disable this, following the norms of modern LLMs, but will revisit it in chapter
    6 when we load pretrained GPT-2 weights from OpenAI into our model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the configuration above, we will start this chapter by implementing a
    GPT placeholder architecture (`DummyGPTModel`) in this section, as shown in Figure
    4.3\. This will provide us with a big-picture view of how everything fits together
    and what other components we need to code in the upcoming sections to assemble
    the full GPT model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 A mental model outlining the order in which we code the GPT architecture.
    In this chapter, we will start with the GPT backbone, a placeholder architecture,
    before we get to the individual core pieces and eventually assemble them in a
    transformer block for the final GPT architecture.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The numbered boxes shown in Figure 4.3 illustrate the order in which we tackle
    the individual concepts required to code the final GPT architecture. We will start
    with step 1, a placeholder GPT backbone we call `DummyGPTModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 A placeholder GPT model architecture class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `DummyGPTModel` class in this code defines a simplified version of a GPT-like
    model using PyTorch's neural network module (`nn.Module`). The model architecture
    in the `DummyGPTModel` class consists of token and positional embeddings, dropout,
    a series of transformer blocks (`DummyTransformerBlock`), a final layer normalization
    (`DummyLayerNorm`), and a linear output layer (`out_head`). The configuration
    is passed in via a Python dictionary, for instance, the `GPT_CONFIG_124M` dictionary
    we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward` method describes the data flow through the model: it computes
    token and positional embeddings for the input indices, applies dropout, processes
    the data through the transformer blocks, applies normalization, and finally produces
    logits with the linear output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The code above is already functional, as we will see later in this section after
    we prepare the input data. However, for now, note in the code above that we have
    used placeholders (`DummyLayerNorm` and `DummyTransformerBlock`) for the transformer
    block and layer normalization, which we will develop in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will prepare the input data and initialize a new GPT model to illustrate
    its usage. Building on the figures we have seen in chapter 2, where we coded the
    tokenizer, Figure 4.4 provides a high-level overview of how data flows in and
    out of a GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 A big-picture overview showing how the input data is tokenized, embedded,
    and fed to the GPT model. Note that in our `DummyGPTClass` coded earlier, the
    token embedding is handled inside the GPT model. In LLMs, the embedded input token
    dimension typically matches the output dimension. The output embeddings here represent
    the context vectors we discussed in chapter 3.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To implement the steps shown in Figure 4.4, we tokenize a batch consisting
    of two text inputs for the GPT model using the tiktoken tokenizer introduced in
    chapter 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting token IDs for the two texts are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we initialize a new 124 million parameter `DummyGPTModel` instance and
    feed it the tokenized `batch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The model outputs, which are commonly referred to as logits, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output tensor has two rows corresponding to the two text samples. Each text
    sample consists of 4 tokens; each token is a 50,257-dimensional vector, which
    matches the size of the tokenizer's vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding has 50,257 dimensions because each of these dimensions refers
    to a unique token in the vocabulary. At the end of this chapter, when we implement
    the postprocessing code, we will convert these 50,257-dimensional vectors back
    into token IDs, which we can then decode into words.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have taken a top-down look at the GPT architecture and its in- and
    outputs, we will code the individual placeholders in the upcoming sections, starting
    with the real layer normalization class that will replace the `DummyLayerNorm`
    in the previous code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Normalizing activations with layer normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training deep neural networks with many layers can sometimes prove challenging
    due to issues like vanishing or exploding gradients. These issues lead to unstable
    training dynamics and make it difficult for the network to effectively adjust
    its weights, which means the learning process struggles to find a set of parameters
    (weights) for the neural network that minimizes the loss function. In other words,
    the network has difficulty learning the underlying patterns in the data to a degree
    that would allow it to make accurate predictions or decisions. (If you are new
    to neural network training and the concepts of gradients, a brief introduction
    to these concepts can be found in *Section A.4, Automatic Differentiation Made
    Easy* in *Appendix A: Introduction to PyTorch*. However, a deep mathematical understanding
    of gradients is not required to follow the contents of this book.)'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will implement *layer normalization* to improve the stability
    and efficiency of neural network training.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind layer normalization is to adjust the activations (outputs)
    of a neural network layer to have a mean of 0 and a variance of 1, also known
    as unit variance. This adjustment speeds up the convergence to effective weights
    and ensures consistent, reliable training. As we have seen in the previous section,
    based on the `DummyLayerNorm` placeholder, in GPT-2 and modern transformer architectures,
    layer normalization is typically applied before and after the multi-head attention
    module and before the final output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Before we implement layer normalization in code, Figure 4.5 provides a visual
    overview of how layer normalization functions.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 An illustration of layer normalization where the 5 layer outputs,
    also called activations, are normalized such that they have a zero mean and variance
    of 1.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can recreate the example shown in Figure 4.5 via the following code, where
    we implement a neural network layer with 5 inputs and 6 outputs that we apply
    to two input examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following tensor, where the first row lists the layer outputs
    for the first input and the second row lists the layer outputs for the second
    row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The neural network layer we have coded consists of a `Linear` layer followed
    by a non-linear activation function, `ReLU` (short for Rectified Linear Unit),
    which is a standard activation function in neural networks. If you are unfamiliar
    with `ReLU`, it simply thresholds negative inputs to 0, ensuring that a layer
    outputs only positive values, which explains why the resulting layer output does
    not contain any negative values. (Note that we will use another, more sophisticated
    activation function in GPT, which we will introduce in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we apply layer normalization to these outputs, let''s examine the mean
    and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first row in the mean tensor above contains the mean value for the first
    input row, and the second output row contains the mean for the second input row.
  prefs: []
  type: TYPE_NORMAL
- en: Using `keepdim=True` in operations like mean or variance calculation ensures
    that the output tensor retains the same shape as the input tensor, even though
    the operation reduces the tensor along the dimension specified via `dim`. For
    instance, without `keepdim=True`, the returned mean tensor would be a 2-dimensional
    vector `[0.1324, 0.2170]` instead of a 2×1-dimensional matrix `[[0.1324], [0.2170]]`.
  prefs: []
  type: TYPE_NORMAL
- en: The `dim` parameter specifies the dimension along which the calculation of the
    statistic (here, mean or variance) should be performed in a tensor, as shown in
    Figure 4.6.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 An illustration of the dim parameter when calculating the mean of
    a tensor. For instance, if we have a 2D tensor (matrix) with dimensions `[rows,
    columns]`, using `dim=0` will perform the operation across rows (vertically, as
    shown at the bottom), resulting in an output that aggregates the data for each
    column. Using `dim=1` or `dim=-1` will perform the operation across columns (horizontally,
    as shown at the top), resulting in an output aggregating the data for each row.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image011.png)'
  prefs: []
  type: TYPE_IMG
- en: As Figure 4.6 explains, for a 2D tensor (like a matrix), using `dim=-1` for
    operations such as mean or variance calculation is the same as using `dim=1`.
    This is because -1 refers to the tensor's last dimension, which corresponds to
    the columns in a 2D tensor. Later, when adding layer normalization to the GPT
    model, which produces 3D tensors with shape `[batch_size, num_tokens, embedding_size]`,
    we can still use `dim=-1` for normalization across the last dimension, avoiding
    a change from `dim=1` to `dim=2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us apply layer normalization to the layer outputs we obtained earlier.
    The operation consists of subtracting the mean and dividing by the square root
    of the variance (also known as standard deviation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see based on the results, the normalized layer outputs, which now
    also contain negative values, have zero mean and a variance of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that the value 2.9802e-08 in the output tensor is the scientific notation
    for 2.9802 × 10-8, which is 0.0000000298 in decimal form. This value is very close
    to 0, but it is not exactly 0 due to small numerical errors that can accumulate
    because of the finite precision with which computers represent numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve readability, we can also turn off the scientific notation when printing
    tensor values by setting `sci_mode` to False:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, in this section, we have coded and applied layer normalization in a
    step-by-step process. Let''s now encapsulate this process in a PyTorch module
    that we can use in the GPT model later:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 A layer normalization class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This specific implementation of layer Normalization operates on the last dimension
    of the input tensor x, which represents the embedding dimension (`emb_dim`). The
    variable `eps` is a small constant (epsilon) added to the variance to prevent
    division by zero during normalization. The `scale` and `shift` are two trainable
    parameters (of the same dimension as the input) that the LLM automatically adjusts
    during training if it is determined that doing so would improve the model's performance
    on its training task. This allows the model to learn appropriate scaling and shifting
    that best suit the data it is processing.
  prefs: []
  type: TYPE_NORMAL
- en: Biased variance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our variance calculation method, we have opted for an implementation detail
    by setting `unbiased=False`. For those curious about what this means, in the variance
    calculation, we divide by the number of inputs *n* in the variance formula. This
    approach does not apply Bessel's correction, which typically uses *n-1* instead
    of *n* in the denominator to adjust for bias in sample variance estimation. This
    decision results in a so-called biased estimate of the variance. For large-scale
    language models (LLMs), where the embedding dimension n is significantly large,
    the difference between using n and n-1 is practically negligible. We chose this
    approach to ensure compatibility with the GPT-2 model's normalization layers and
    because it reflects TensorFlow's default behavior, which was used to implement
    the original GPT-2 model. Using a similar setting ensures our method is compatible
    with the pretrained weights we will load in chapter 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now try the `LayerNorm` module in practice and apply it to the batch
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see based on the results, the layer normalization code works as expected
    and normalizes the values of each of the two inputs such that they have a mean
    of 0 and a variance of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we covered one of the building blocks we will need to implement
    the GPT architecture, as shown in the mental model in Figure 4.7.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 A mental model listing the different building blocks we implement
    in this chapter to assemble the GPT architecture.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image013.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will look at the GELU activation function, which is
    one of the activation functions used in LLMs, instead of the traditional ReLU
    function we used in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Layer normalization versus batch normalization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you are familiar with batch normalization, a common and traditional normalization
    method for neural networks, you may wonder how it compares to layer normalization.
    Unlike batch normalization, which normalizes across the batch dimension, layer
    normalization normalizes across the feature dimension. LLMs often require significant
    computational resources, and the available hardware or the specific use case can
    dictate the batch size during training or inference. Since layer normalization
    normalizes each input independently of the batch size, it offers more flexibility
    and stability in these scenarios. This is particularly beneficial for distributed
    training or when deploying models in environments where resources are constrained.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Implementing a feed forward network with GELU activations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we implement a small neural network submodule that is used
    as part of the transformer block in LLMs. We begin with implementing the *GELU*
    activation function, which plays a crucial role in this neural network submodule.
    (For additional information on implementing neural networks in PyTorch, please
    see section A.5 Implementing multilayer neural networks in Appendix A.)
  prefs: []
  type: TYPE_NORMAL
- en: Historically, the ReLU activation function has been commonly used in deep learning
    due to its simplicity and effectiveness across various neural network architectures.
    However, in LLMs, several other activation functions are employed beyond the traditional
    ReLU. Two notable examples are GELU (*Gaussian Error Linear Unit*) and SwiGLU
    (*Sigmoid-Weighted Linear Unit*).
  prefs: []
  type: TYPE_NORMAL
- en: GELU and SwiGLU are more complex and smooth activation functions incorporating
    Gaussian and sigmoid-gated linear units, respectively. They offer improved performance
    for deep learning models, unlike the simpler ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GELU activation function can be implemented in several ways; the exact
    version is defined as GELU(x)=x Φ(x), where Φ(x) is the cumulative distribution
    function of the standard Gaussian distribution. In practice, however, it''s common
    to implement a computationally cheaper approximation (the original GPT-2 model
    was also trained with this approximation):'
  prefs: []
  type: TYPE_NORMAL
- en: GELU(x) ≈ 0.5 ⋅ x ⋅ (1 + tanh[√((2/π)) ⋅ (x + 0.044715 ⋅ x^3])
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we can implement this function as PyTorch module as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 An implementation of the GELU activation function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to get an idea of what this GELU function looks like and how it compares
    to the ReLU function, let''s plot these functions side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the resulting plot in Figure 4.8, ReLU is a piecewise linear
    function that outputs the input directly if it is positive; otherwise, it outputs
    zero. GELU is a smooth, non-linear function that approximates ReLU but with a
    non-zero gradient for negative values.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 The output of the GELU and ReLU plots using matplotlib. The x-axis
    shows the function inputs and the y-axis shows the function outputs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image015.png)'
  prefs: []
  type: TYPE_IMG
- en: The smoothness of GELU, as shown in Figure 4.8, can lead to better optimization
    properties during training, as it allows for more nuanced adjustments to the model's
    parameters. In contrast, ReLU has a sharp corner at zero, which can sometimes
    make optimization harder, especially in networks that are very deep or have complex
    architectures. Moreover, unlike RELU, which outputs zero for any negative input,
    GELU allows for a small, non-zero output for negative values. This characteristic
    means that during the training process, neurons that receive negative input can
    still contribute to the learning process, albeit to a lesser extent than positive
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s use the GELU function to implement the small neural network module,
    `FeedForward`, that we will be using in the LLM''s transformer block later:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 A feed forward neural network module
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the preceding code, the `FeedForward` module is a small neural
    network consisting of two `Linear` layers and a `GELU` activation function. In
    the 124 million parameter GPT model, it receives the input batches with tokens
    that have an embedding size of 768 each via the `GPT_CONFIG_124M` dictionary where
    `GPT_CONFIG_124M["emb_dim"] = 768`.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 shows how the embedding size is manipulated inside this small feed
    forward neural network when we pass it some inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 provides a visual overview of the connections between the layers
    of the feed forward neural network. It is important to note that this neural network
    can accommodate variable batch sizes and numbers of tokens in the input. However,
    the embedding size for each token is determined and fixed when initializing the
    weights.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following the example in Figure 4.9, let''s initialize a new `FeedForward`
    module with a token embedding size of 768 and feed it a batch input with 2 samples
    and 3 tokens each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the shape of the output tensor is the same as that of the input
    tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `FeedForward` module we implemented in this section plays a crucial role
    in enhancing the model's ability to learn from and generalize the data. Although
    the input and output dimensions of this module are the same, it internally expands
    the embedding dimension into a higher-dimensional space through the first linear
    layer as illustrated in Figure 4.10\. This expansion is followed by a non-linear
    GELU activation, and then a contraction back to the original dimension with the
    second linear transformation. Such a design allows for the exploration of a richer
    representation space.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 An illustration of the expansion and contraction of the layer outputs
    in the feed forward neural network. First, the inputs expand by a factor of 4
    from 768 to 3072 values. Then, the second layer compresses the 3072 values back
    into a 768-dimensional representation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image019.png)'
  prefs: []
  type: TYPE_IMG
- en: Moreover, the uniformity in input and output dimensions simplifies the architecture
    by enabling the stacking of multiple layers, as we will do later, without the
    need to adjust dimensions between them, thus making the model more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in Figure 4.11, we have now implemented most of the LLM's building
    blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 A mental model showing the topics we cover in this chapter, with
    the black checkmarks indicating those that we have already covered.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image021.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will go over the concept of shortcut connections that
    we insert between different layers of a neural network, which are important for
    improving the training performance in deep neural network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Adding shortcut connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, let's discuss the concept behind *shortcut connections*, also known as
    skip or residual connections. Originally, shortcut connections were proposed for
    deep networks in computer vision (specifically, in residual networks) to mitigate
    the challenge of vanishing gradients. The vanishing gradient problem refers to
    the issue where gradients (which guide weight updates during training) become
    progressively smaller as they propagate backward through the layers, making it
    difficult to effectively train earlier layers, as illustrated in Figure 4.12.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 A comparison between a deep neural network consisting of 5 layers
    without (on the left) and with shortcut connections (on the right). Shortcut connections
    involve adding the inputs of a layer to its outputs, effectively creating an alternate
    path that bypasses certain layers. The gradient illustrated in Figure 1.1 denotes
    the mean absolute gradient at each layer, which we will compute in the code example
    that follows.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image023.png)'
  prefs: []
  type: TYPE_IMG
- en: As illustrated in Figure 4.12, a shortcut connection creates an alternative,
    shorter path for the gradient to flow through the network by skipping one or more
    layers, which is achieved by adding the output of one layer to the output of a
    later layer. This is why these connections are also known as skip connections.
    They play a crucial role in preserving the flow of gradients during the backward
    pass in training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code example below, we implement the neural network shown in Figure
    4.12 to see how we can add shortcut connections in the `forward` method:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 A neural network to illustrate shortcut connections
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The code implements a deep neural network with 5 layers, each consisting of
    a `Linear` layer and a `GELU` activation function. In the forward pass, we iteratively
    pass the input through the layers and optionally add the shortcut connections
    depicted in Figure 4.12 if the `self.use_shortcut` attribute is set to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use this code to first initialize a neural network without shortcut
    connections. Here, each layer will be initialized such that it accepts an example
    with 3 input values and returns 3 output values. The last layer returns a single
    output value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we implement a function that computes the gradients in the the model''s
    backward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we specify a loss function that computes how close the
    model output and a user-specified target (here, for simplicity, the value 0) are.
    Then, when calling `loss.backward()`, PyTorch computes the loss gradient for each
    layer in the model. We can iterate through the weight parameters via `model.named_parameters()`.
    Suppose we have a 3×3 weight parameter matrix for a given layer. In that case,
    this layer will have 3×3 gradient values, and we print the mean absolute gradient
    of these 3×3 gradient values to obtain a single gradient value per layer to compare
    the gradients between layers more easily.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the `.backward()` method is a convenient method in PyTorch that computes
    loss gradients, which are required during model training, without implementing
    the math for the gradient calculation ourselves, thereby making working with deep
    neural networks much more accessible. If you are unfamiliar with the concept of
    gradients and neural network training, I recommend reading sections *A.4, Automatic
    differentiation made easy* and *A.7 A typical training loop* in *appendix A*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now use the `print_gradients` function and apply it to the model without
    skip connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As we can see based on the output of the `print_gradients` function, the gradients
    become smaller as we progress from the last layer (`layers.4`) to the first layer
    (`layers.0`), which is a phenomenon called the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now instantiate a model with skip connections and see how it compares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, based on the output, the last layer `(layers.4`) still has a
    larger gradient than the other layers. However, the gradient value stabilizes
    as we progress towards the first layer (`layers.0`) and doesn't shrink to a vanishingly
    small value.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, shortcut connections are important for overcoming the limitations
    posed by the vanishing gradient problem in deep neural networks. Shortcut connections
    are a core building block of very large models such as LLMs, and they will help
    facilitate more effective training by ensuring consistent gradient flow across
    layers when we train the GPT model in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: After introducing shortcut connections, we will now connect all of the previously
    covered concepts (layer normalization, GELU activations, feed forward module,
    and shortcut connections) in a transformer block in the next section, which is
    the final building block we need to code the GPT architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Connecting attention and linear layers in a transformer block
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we are implementing the *transformer block*, a fundamental
    building block of GPT and other LLM architectures. This block, which is repeated
    a dozen times in the 124 million parameter GPT-2 architecture, combines several
    concepts we have previously covered: multi-head attention, layer normalization,
    dropout, feed forward layers, and GELU activations, as illustrated in Figure 4.13\.
    In the next section, we will then connect this transformer block to the remaining
    parts of the GPT architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 An illustration of a transformer block. The bottom of the diagram
    shows input tokens that have been embedded into 768-dimensional vectors. Each
    row corresponds to one token's vector representation. The outputs of the transformer
    block are vectors of the same dimension as the input, which can then be fed into
    subsequent layers in an LLM.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image025.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in Figure 4.13, the transformer block combines several components,
    including the masked multi-head attention module from chapter 3 and the `FeedForward`
    module we implemented in Section 4.3.
  prefs: []
  type: TYPE_NORMAL
- en: When a transformer block processes an input sequence, each element in the sequence
    (for example, a word or subword token) is represented by a fixed-size vector (in
    the case of Figure 4.13, 768 dimensions). The operations within the transformer
    block, including multi-head attention and feed forward layers, are designed to
    transform these vectors in a way that preserves their dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that the self-attention mechanism in the multi-head attention block
    identifies and analyzes relationships between elements in the input sequence.
    In contrast, the feed forward network modifies the data individually at each position.
    This combination not only enables a more nuanced understanding and processing
    of the input but also enhances the model's overall capacity for handling complex
    data patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we can create the `TransformerBlock` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 The transformer block component of GPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The given code defines a `TransformerBlock` class in PyTorch that includes a
    multi-head attention mechanism (`MultiHeadAttention`) and a feed forward network
    (`FeedForward`), both configured based on a provided configuration dictionary
    (`cfg`), such as `GPT_CONFIG_124M`.
  prefs: []
  type: TYPE_NORMAL
- en: Layer normalization (`LayerNorm`) is applied before each of these two components,
    and dropout is applied after them to regularize the model and prevent overfitting.
    This is also known as *Pre-LayerNorm*. Older architectures, such as the original
    transformer model, applied layer normalization after the self-attention and feed-forward
    networks instead, known as *Post-LayerNorm*, which often leads to worse training
    dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: The class also implements the forward pass, where each component is followed
    by a shortcut connection that adds the input of the block to its output. This
    critical feature helps gradients flow through the network during training and
    improves the learning of deep models as explained in section 4.4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `GPT_CONFIG_124M` dictionary we defined earlier, let''s instantiate
    a transformer block and feed it some sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the code output, the transformer block maintains the input
    dimensions in its output, indicating that the transformer architecture processes
    sequences of data without altering their shape throughout the network.
  prefs: []
  type: TYPE_NORMAL
- en: The preservation of shape throughout the transformer block architecture is not
    incidental but a crucial aspect of its design. This design enables its effective
    application across a wide range of sequence-to-sequence tasks, where each output
    vector directly corresponds to an input vector, maintaining a one-to-one relationship.
    However, the output is a context vector that encapsulates information from the
    entire input sequence, as we learned in chapter 3\. This means that while the
    physical dimensions of the sequence (length and feature size) remain unchanged
    as it passes through the transformer block, the content of each output vector
    is re-encoded to integrate contextual information from across the entire input
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: With the transformer block implemented in this section, we now have all the
    building blocks, as shown in Figure 4.14, needed to implement the GPT architecture
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.14 A mental model of the different concepts we have implemented in
    this chapter so far.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image027.png)'
  prefs: []
  type: TYPE_IMG
- en: As illustrated in Figure 4.14, the transformer block combines layer normalization,
    the feed forward network, including GELU activations, and shortcut connections,
    which we already covered earlier in this chapter. As we will see in the upcoming
    chapter, this transformer block will make up the main component of the GPT architecture
    we will implement
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Coding the GPT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started this chapter with a big-picture overview of a GPT architecture that
    we called `DummyGPTModel`. In this `DummyGPTModel` code implementation, we showed
    the input and outputs to the GPT model, but its building blocks remained a black
    box using a `DummyTransformerBlock` and `DummyLayerNorm` class as placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are now replacing the `DummyTransformerBlock` and `DummyLayerNorm`
    placeholders with the real `TransformerBlock` and `LayerNorm` classes we coded
    later in this chapter to assemble a fully working version of the original 124
    million parameter version of GPT-2\. In chapter 5, we will pretrain a GPT-2 model,
    and in chapter 6, we will load in the pretrained weights from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Before we assemble the GPT-2 model in code, let's look at its overall structure
    in Figure 4.15, which combines all the concepts we covered so far in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.15 An overview of the GPT model architecture. This figure illustrates
    the flow of data through the GPT model. Starting from the bottom, tokenized text
    is first converted into token embeddings, which are then augmented with positional
    embeddings. This combined information forms a tensor that is passed through a
    series of transformer blocks shown in the center (each containing multi-head attention
    and feed forward neural network layers with dropout and layer normalization),
    which are stacked on top of each other and repeated 12 times.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image029.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in Figure 4.15, the transformer block we coded in Section 4.5 is repeated
    many times throughout a GPT model architecture. In the case of the 124 million
    parameter GPT-2 model, it's repeated 12 times, which we specify via the `"n_layers"`
    entry in the `GPT_CONFIG_124M` dictionary. In the case of the largest GPT-2 model
    with 1,542 million parameters, this transformer block is repeated 36 times.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Figure 4.15, the output from the final transformer block then goes
    through a final layer normalization step before reaching the linear output layer.
    This layer maps the transformer's output to a high-dimensional space (in this
    case, 50,257 dimensions, corresponding to the model's vocabulary size) to predict
    the next token in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now implement the architecture we see in Figure 4.15 in code:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 The GPT model architecture implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to the `TransformerBlock` class we implemented in Section 4.5, the `GPTModel`
    class is relatively small and compact.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__` constructor of this `GPTModel` class initializes the token and
    positional embedding layers using the configurations passed in via a Python dictionary,
    `cfg`. These embedding layers are responsible for converting input token indices
    into dense vectors and adding positional information, as discussed in chapter
    2.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the `__init__` method creates a sequential stack of `TransformerBlock`
    modules equal to the number of layers specified in `cfg`. Following the transformer
    blocks, a `LayerNorm` layer is applied, standardizing the outputs from the transformer
    blocks to stabilize the learning process. Finally, a linear output head without
    bias is defined, which projects the transformer's output into the vocabulary space
    of the tokenizer to generate logits for each token in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: The forward method takes a batch of input token indices, computes their embeddings,
    applies the positional embeddings, passes the sequence through the transformer
    blocks, normalizes the final output, and then computes the logits, representing
    the next token's unnormalized probabilities. We will convert these logits into
    tokens and text outputs in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now initialize the 124 million parameter GPT model using the `GPT_CONFIG_124M`
    dictionary we pass into the cfg parameter and feed it with the batch text input
    we created at the beginning of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code prints the contents of the input batch followed by the output
    tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the output tensor has the shape `[2, 4, 50257]`, since we passed
    in 2 input texts with 4 tokens each. The last dimension, 50,257, corresponds to
    the vocabulary size of the tokenizer. In the next section, we will see how to
    convert each of these 50,257-dimensional output vectors back into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to the next section and code the function that converts the
    model outputs into text, let's spend a bit more time with the model architecture
    itself and analyze its size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `numel()` method, short for "number of elements," we can collect
    the total number of parameters in the model''s parameter tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now, a curious reader might notice a discrepancy. Earlier, we spoke of initializing
    a 124 million parameter GPT model, so why is the actual number of parameters 163
    million, as shown in the preceding code output?
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason is a concept called weight tying that is used in the original GPT-2
    architecture, which means that the original GPT-2 architecture is reusing the
    weights from the token embedding layer inits output layer. To understand what
    this means, let''s take a look at the shapes of the token embedding layer and
    linear output layer that we initialized on the `model` via the `GPTModel` earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see based on the print outputs, the weight tensors for both these
    layers have the same shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The token embedding and output layers are very large due to the number of rows
    for the 50,257 in the tokenizer''s vocabulary. Let''s remove the output layer
    parameter count from the total GPT-2 model count according to the weight tying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the model is now only 124 million parameters large, matching
    the original size of the GPT-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: Weight tying reduces the overall memory footprint and computational complexity
    of the model. However, in my experience, using separate token embedding and output
    layers results in better training and model performance; hence, we are using separate
    layers in our `GPTModel` implementation. The same is true for modern LLMs. However,
    we will revisit and implement the weight tying concept later in chapter 6 when
    we load the pretrained weights from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.1 Number of parameters in feed forward and attention modules
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Calculate and compare the number of parameters that are contained in the feed
    forward module and those that are contained in the multi-head attention module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let us compute the memory requirements of the 163 million parameters
    in our `GPTModel` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, by calculating the memory requirements for the 163 million parameters
    in our `GPTModel` object and assuming each parameter is a 32-bit float taking
    up 4 bytes, we find that the total size of the model amounts to 621.83 MB, illustrating
    the relatively large storage capacity required to accommodate even relatively
    small LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we implemented the GPTModel architecture and saw that it outputs
    numeric tensors of shape `[batch_size, num_tokens, vocab_size]`. In the next section,
    we will write the code to convert these output tensors into text.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.2 Initializing larger GPT models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this chapter, we initialized a 124 million parameter GPT model, which is
    known as "GPT-2 small." Without making any code modifications besides updating
    the configuration file, use the GPTModel class to implement GPT-2 medium (using
    1024-dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads),
    GPT-2 large (1280-dimensional embeddings, 36 transformer blocks, 20 multi-head
    attention heads), and GPT-2 XL (1600-dimensional embeddings, 48 transformer blocks,
    25 multi-head attention heads). As a bonus, calculate the total number of parameters
    in each GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Generating text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this final section of this chapter, we will implement the code that converts
    the tensor outputs of the GPT model back into text. Before we get started, let's
    briefly review how a generative model like an LLM generates text one word (or
    token) at a time, as shown in Figure 4.16.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.16 This diagram illustrates the step-by-step process by which an LLM
    generates text, one token at a time. Starting with an initial input context ("Hello,
    I am"), the model predicts a subsequent token during each iteration, appending
    it to the input context for the next round of prediction. As shown, the first
    iteration adds "a", the second "model", and the third "ready", progressively building
    the sentence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16 illustrates the step-by-step process by which a GPT model generates
    text given an input context, such as "Hello, I am," on a big-picture level. With
    each iteration, the input context grows, allowing the model to generate coherent
    and contextually appropriate text. By the 6th iteration, the model has constructed
    a complete sentence: "Hello, I am a model ready to help."'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we saw that our current `GPTModel` implementation outputs
    tensors with shape `[batch_size, num_token, vocab_size]`. Now, the question is,
    how does a GPT model go from these output tensors to the generated text shown
    in Figure 4.16?
  prefs: []
  type: TYPE_NORMAL
- en: The process by which a GPT model goes from output tensors to generated text
    involves several steps, as illustrated in Figure 4.17\. These steps include decoding
    the output tensors, selecting tokens based on a probability distribution, and
    converting these tokens into human-readable text.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.17 details the mechanics of text generation in a GPT model by showing
    a single iteration in the token generation process. The process begins by encoding
    the input text into token IDs, which are then fed into the GPT model. The outputs
    of the model are then converted back into text and appended to the original input
    text.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image033.png)'
  prefs: []
  type: TYPE_IMG
- en: The next-token generation process detailed in Figure 4.17 illustrates a single
    step where the GPT model generates the next token given its input.
  prefs: []
  type: TYPE_NORMAL
- en: In each step, the model outputs a matrix with vectors representing potential
    next tokens. The vector corresponding to the next token is extracted and converted
    into a probability distribution via the softmax function. Within the vector containing
    the resulting probability scores, the index of the highest value is located, which
    translates to the token ID. This token ID is then decoded back into text, producing
    the next token in the sequence. Finally, this token is appended to the previous
    inputs, forming a new input sequence for the subsequent iteration. This step-by-step
    process enables the model to generate text sequentially, building coherent phrases
    and sentences from the initial input context.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we repeat this process over many iterations, such as shown in Figure
    4.16 earlier, until we reach a user-specified number of generated tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we can implement the token-generation process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 A function for the GPT model to generate text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The code snippet provided demonstrates a simple implementation of a generative
    loop for a language model using PyTorch. It iterates for a specified number of
    new tokens to be generated, crops the current context to fit the model's maximum
    context size, computes predictions and then selects the next token based on the
    highest probability prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceeding code, the `generate_text_simple` function, we use a softmax
    function to convert the logits into a probability distribution from which we identify
    the position with the highest value via `torch.argmax`. The softmax function is
    monotonic, meaning it preserves the order of its inputs when transformed into
    outputs. So, in practice, the softmax step is redundant since the position with
    the highest score in the softmax output tensor is the same position in the logit
    tensor. In other words, we could apply the `torch.argmax` function to the logits
    tensor directly and get identical results. However, we coded the conversion to
    illustrate the full process of transforming logits to probabilities, which can
    add additional intuition, such as that the model generates the most likely next
    token, which is known as *greedy decoding*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, when we will implement the GPT training code, we will also
    introduce additional sampling techniques where we modify the softmax outputs such
    that the model doesn't always select the most likely token, which introduces variability
    and creativity in the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: This process of generating one token ID at a time and appending it to the context
    using the `generate_text_simple` function is further illustrated in Figure 4.18\.
    (The token ID generation process for each iteration is detailed in Figure 4.17.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.18 An illustration showing six iterations of a token prediction cycle,
    where the model takes a sequence of initial token IDs as input, predicts the next
    token, and appends this token to the input sequence for the next iteration. (The
    token IDs are also translated into their corresponding text for better understanding.)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/04__image035.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in Figure 4.18, we generate the token IDs in an iterative fashion.
    For instance, in iteration 1, the model is provided with the tokens corresponding
    to "Hello , I am", predicts the next token (with ID 257, which is "a"), and appends
    it to the input. This process is repeated until the model produces the complete
    sentence "Hello, I am a model ready to help." after six iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now try out the `generate_text_simple` function with the `"Hello, I am"`
    context as model input, as shown in Figure 4.18, in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we encode the input context into token IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The encoded IDs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we put the model into `.eval()` mode, which disables random components
    like dropout, which are only used during training, and use the `generate_text_simple`
    function on the encoded input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output token IDs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `.decode` method of the tokenizer, we can convert the IDs back into
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The model output in text format is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, based on the preceding output, the model generated gibberish,
    which is not at all like the coherent text shown in Figure 4.18\. What happened?
    The reason why the model is unable to produce coherent text is that we haven't
    trained it yet. So far, we just implemented the GPT architecture and initialized
    a GPT model instance with initial random weights.
  prefs: []
  type: TYPE_NORMAL
- en: Model training is a large topic in itself, and we will tackle it in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.3 Using separate dropout parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'At the beginning of this chapter, we defined a global `"drop_rate"` setting
    in the `GPT_CONFIG_124M` dictionary to set the dropout rate in various places
    throughout the GPTModel architecture. Change the code to specify a separate dropout
    value for the various dropout layers throughout the model architecture. (Hint:
    there are three distinct places where we used dropout layers: the embedding layer,
    shortcut layer, and multi-head attention module.)'
  prefs: []
  type: TYPE_NORMAL
- en: 4.8 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Layer normalization stabilizes training by ensuring that each layer's outputs
    have a consistent mean and variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shortcut connections are connections that skip one or more layers by feeding
    the output of one layer directly to a deeper layer, which helps mitigate the vanishing
    gradient problem when training deep neural networks, such as LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer blocks are a core structural component of GPT models, combining
    masked multi-head attention modules with fully connected feed-forward networks
    that use the GELU activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT models are LLMs with many repeated transformer blocks that have millions
    to billions of parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT models come in various sizes, for example, 124, 345, 762, and 1542 million
    parameters, which we can implement with the same `GPTModel` Python class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text generation capability of a GPT-like LLM involves decoding output tensors
    into human-readable text by sequentially predicting one token at a time based
    on a given input context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without training, a GPT model generates incoherent text, which underscores the
    importance of model training for coherent text generation, which is the topic
    of subsequent chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
