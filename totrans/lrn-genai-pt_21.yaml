- en: Appendix B. Minimally qualified readers and deep learning basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B. 最基本合格的读者和深度学习基础知识
- en: This book is intended for machine learning enthusiasts and data scientists across
    various business fields who possess intermediate Python programming skills and
    are interested in learning about generative AI. Through this book, readers will
    learn to create novel and innovative content—such as images, text, numbers, shapes,
    and audio—that can benefit their employers’ businesses and advance their own careers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在面向具有中级Python编程技能并希望了解生成式AI的机器学习爱好者以及各商业领域的数据科学家。通过本书，读者将学会创建新颖和创新的内容——如图像、文本、数字、形状和音频——这些内容可以造福他们的雇主业务并推进他们自己的职业生涯。
- en: This book is designed for those who have a solid grasp of Python. You should
    be familiar with variable types like integers, floats, strings, and Booleans.
    You should also be comfortable creating *for* and *while* loops and understand
    conditional execution and branching (e.g., using *if*, *elif*, and *else* statements).
    The book involves frequent use of Python functions and classes, and you should
    know how to install and import third-party Python libraries and packages. If you
    need to brush up on these skills, the free online Python tutorial provided by
    W3Schools is a great resource ([https://www.w3schools.com/python/](https://www.w3schools.com/python/)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是为那些对Python有扎实掌握的人设计的。你应该熟悉整数、浮点数、字符串和布尔值等变量类型。你还应该熟悉创建*for*和*while*循环，并理解条件执行和分支（例如，使用*if*、*elif*和*else*语句）。本书经常使用Python函数和类，你应该知道如何安装和导入第三方Python库和包。如果你需要复习这些技能，W3Schools提供的免费在线Python教程是一个很好的资源（[https://www.w3schools.com/python/](https://www.w3schools.com/python/))）。
- en: Additionally, you should have a basic understanding of machine learning, particularly
    neural networks and deep learning. In this appendix, we will review key concepts
    such as loss functions, activation functions, and optimizers, which are essential
    for developing and training deep neural networks. However, this appendix is not
    meant to be a comprehensive tutorial on these topics. If you find gaps in your
    understanding, it is strongly recommended that you address them before proceeding
    with the projects in this book. A good book for this purpose is *Deep Learning
    with PyTorch* by Stevens, Antiga, and Viehmann (2020).^([1](#footnote-000))
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你应该对机器学习有一个基本的了解，特别是神经网络和深度学习。在本附录中，我们将回顾诸如损失函数、激活函数和优化器等关键概念，这些对于开发和管理深度神经网络至关重要。然而，本附录并非旨在成为这些主题的全面教程。如果你发现你的理解有不足之处，强烈建议你在继续进行本书中的项目之前解决这些问题。为此，一本很好的书是Stevens、Antiga和Viehmann（2020年）所著的《Deep
    Learning with PyTorch》。（[1](#footnote-000)）
- en: No prior experience with PyTorch or generative AI is required. In chapter 2,
    you will learn the basics of PyTorch, starting with its basic data types. You
    will also implement an end-to-end deep learning project in PyTorch to get hands-on
    experience. The goal of chapter 2 is to prepare you to use PyTorch for building
    and training various generative models in the book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要具备PyTorch或生成式AI的先验经验。在第2章中，你将学习PyTorch的基础知识，从其基本数据类型开始。你还将使用PyTorch实现一个端到端的深度学习项目，以获得实践经验。第2章的目标是为你使用PyTorch构建和训练书中各种生成模型做好准备。
- en: B.1 Deep learning and deep neural networks
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 深度学习和深度神经网络
- en: Machine learning (ML) represents a new paradigm in AI. Unlike traditional rule-based
    AI, which involves programming explicit rules into a computer, ML involves feeding
    the computer various examples and allowing it to learn the rules on its own. Deep
    learning is a subset of ML that employs deep neural networks for this learning
    process.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）代表了人工智能领域的一个新范式。与将显式规则编程到计算机中的传统基于规则的AI不同，ML涉及向计算机提供各种示例，并允许它自己学习规则。深度学习是ML的一个子集，它使用深度神经网络来完成这一学习过程。
- en: In this section, you’ll learn about neural networks and why some are considered
    deep neural networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解神经网络以及为什么一些被认为是深度神经网络。
- en: B.1.1 Anatomy of a neural network
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.1 神经网络的解剖结构
- en: A neural network aims to mimic the functioning of the human brain. It consists
    of an input layer, an output layer, and zero, one, or more hidden layers in between.
    The term “deep neural networks” refers to networks with many hidden layers, which
    tend to be more powerful.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络旨在模仿人脑的功能。它由一个输入层、一个输出层以及介于两者之间零个、一个或多个隐藏层组成。术语“深度神经网络”指的是具有许多隐藏层的网络，它们通常更强大。
- en: We’ll start with a simpler example featuring two hidden layers, as shown in
    figure B.1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从包含两个隐藏层的简单示例开始，如图B.1所示。
- en: '![](../../OEBPS/Images/APPB_F01_Liu.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/APPB_F01_Liu.png)'
- en: Figure B.1 The structure of a neural network. A neural network is composed of
    an input layer; zero, one, or more hidden layers; and an output layer. Each layer
    contains one or more neurons. Neurons in each layer are connected to those in
    the preceding and subsequent layers, with the strength of these connections represented
    by weights. In this figure, the neural network features an input layer with three
    neurons, two hidden layers with six and four neurons, respectively, and an output
    layer with two neurons.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.1 神经网络的结构。神经网络由输入层；零个、一个或多个隐藏层；和输出层组成。每一层包含一个或多个神经元。每一层的神经元与前一层和后一层的神经元相连，这些连接的强度由权重表示。在此图中，神经网络具有一个包含三个神经元的输入层，两个分别包含六个和四个神经元的隐藏层，以及一个包含两个神经元的输出层。
- en: A neural network consists of an input layer, a variable number of hidden layers,
    and an output layer. Each layer is made up of one or more neurons. Neurons in
    one layer connect to neurons in the previous and next layers, with the connection
    strengths measured by weights. In the example illustrated in figure B.1, the neural
    network features an input layer with three neurons, two hidden layers containing
    six and four neurons, respectively, and an output layer with two neurons.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由输入层、可变数量的隐藏层和输出层组成。每一层由一个或多个神经元组成。同一层的神经元与前一层和后一层的神经元相连，连接强度由权重衡量。如图B.1所示，该神经网络具有一个包含三个神经元的输入层，两个包含六个和四个神经元的隐藏层，以及一个包含两个神经元的输出层。
- en: B.1.2 Different types of layers in neural networks
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.2 神经网络中的不同类型层
- en: Within a neural network, various types of layers serve distinct purposes. The
    most common is the dense layer, where each neuron is connected to every neuron
    in the next layer. Because of this full connectivity, a dense layer is also referred
    to as a fully connected layer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，各种类型的层具有不同的作用。最常见的是密集层，其中每个神经元都与下一层的每个神经元相连。由于这种完全连接性，密集层也被称为全连接层。
- en: Another frequently used type of neural layer, especially in this book, is the
    convolutional layer. Convolutional layers treat input as multidimensional data
    and are adept at extracting patterns from it. In our book, convolutional layers
    are often employed to extract spatial features from images.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在本书中经常使用的高级神经网络层是卷积层。卷积层将输入视为多维数据，并擅长从中提取模式。在我们的书中，卷积层通常用于从图像中提取空间特征。
- en: Convolutional layers differ from fully connected (dense) layers in several key
    ways. First, each neuron in a convolutional layer connects only to a small region
    of the input. This design is based on the understanding that in image data, local
    groups of pixels are more likely to be related. This local connectivity significantly
    reduces the number of parameters, making convolutional neural networks (CNNs)
    more efficient. Second, CNNs utilize shared weights—the same weights are applied
    across different regions of the input. This mechanism is similar to sliding a
    filter across the entire input space. This filter detects specific features (e.g.,
    edges or textures) regardless of their position in the input, which leads to the
    property of translation invariance. Due to their structure, CNNs are more efficient
    for image processing, requiring fewer parameters than fully connected networks
    of similar size. This results in faster training times and lower computational
    costs. Additionally, CNNs are generally more effective at capturing spatial hierarchies
    in image data. We discuss CNNs in detail in chapter 4.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层与全连接（密集）层在几个关键方面有所不同。首先，卷积层中的每个神经元仅连接到输入的小区域。这种设计基于这样的理解：在图像数据中，局部像素组更有可能相关。这种局部连接显著减少了参数数量，使得卷积神经网络（CNNs）更加高效。其次，CNNs使用共享权重——相同的权重应用于输入的不同区域。这种机制类似于在整个输入空间上滑动一个过滤器。这个过滤器检测特定的特征（例如，边缘或纹理），而不管它们在输入中的位置如何，这导致了平移不变性的特性。由于它们的结构，CNNs在图像处理方面更加高效，所需参数少于类似大小的全连接网络。这导致了更快的训练时间和更低的计算成本。此外，CNNs通常在捕获图像数据中的空间层次结构方面更加有效。我们将在第4章中详细讨论CNNs。
- en: The third type of neural network is the recurrent neural network (RNN). Fully
    connected networks treat each input independently, processing each input separately
    without considering any relationship or order between different inputs. In contrast,
    RNNs are specifically designed to handle sequential data. In an RNN, the output
    at a given time step depends not only on the current input but also on previous
    inputs. This allows RNNs to maintain a form of memory, capturing information from
    previous time steps to influence the processing of the current input. See chapter
    8 for details on RNNs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种神经网络是循环神经网络（RNN）。全连接网络独立处理每个输入，分别处理每个输入，不考虑不同输入之间的任何关系或顺序。相比之下，RNN专门设计来处理序列数据。在RNN中，给定时间步的输出不仅取决于当前输入，还取决于之前的输入。这允许RNN保持一种记忆形式，从之前的时间步捕获信息以影响当前输入的处理。详见第8章关于RNN的详细信息。
- en: B.1.3 Activation Functions
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.3 激活函数
- en: Activation functions are a crucial component of neural networks, functioning
    as the mechanisms that transform inputs into outputs and determine when a neuron
    should activate. Some functions are akin to on-off switches, playing a pivotal
    role in enhancing the power of neural networks. Without activation functions,
    neural networks would be limited to learning only linear relationships in data.
    By introducing nonlinearity, activation functions enable the creation of complex,
    nonlinear relationships between inputs and outputs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是神经网络的关键组成部分，作为将输入转换为输出的机制，并决定何时激活神经元。一些函数类似于开关，在增强神经网络能力方面发挥着关键作用。没有激活函数，神经网络将仅限于学习数据中的线性关系。通过引入非线性，激活函数能够创建输入和输出之间的复杂非线性关系。
- en: The most commonly-used activation function is the rectified linear unit (ReLU).
    A ReLU activates the neuron when the input is positive, effectively allowing information
    to pass through. When the input is negative, the neuron is deactivated. This straightforward
    on-off behavior facilitates the modeling of nonlinear relationships.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的激活函数是线性整流单元（ReLU）。当输入为正时，ReLU会激活神经元，有效地允许信息通过。当输入为负时，神经元被关闭。这种简单明了的开/关行为有助于建模非线性关系。
- en: Another commonly used activation function is the sigmoid function, which is
    particularly suited for binary classification problems. The sigmoid function compresses
    inputs into a range between 0 and 1, effectively representing the probabilities
    of a binary outcome.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用的激活函数是sigmoid函数，它特别适合二分类问题。sigmoid函数将输入压缩到0和1之间，有效地表示二元结果的概率。
- en: For multicategory classification tasks, the softmax function is employed. The
    softmax function transforms a vector of values into a probability distribution,
    where the values sum to 1\. This is ideal for modeling the probabilities of multiple
    outcomes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类别分类任务，使用softmax函数。softmax函数将值向量转换为概率分布，其中值之和为1。这对于建模多个结果的概率是理想的。
- en: Lastly, the tanh activation function is noteworthy. Similar to the sigmoid function,
    tanh produces values between –1 and 1\. This characteristic is especially useful
    when working with images, as image data often contains values within this range.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，tanh激活函数值得关注。与sigmoid函数类似，tanh产生介于-1和1之间的值。这一特性在处理图像时特别有用，因为图像数据通常包含这个范围内的值。
- en: B.2 Training a deep neural network
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 训练深度神经网络
- en: This section provides an overview of the steps involved in training a neural
    network. A key aspect of this process is dividing your training dataset into a
    train set, a validation set, and a test set, which is crucial for developing a
    robust deep neural network. We will also discuss various loss functions and optimizers
    used in training neural networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了训练神经网络所涉及的步骤。这个过程的关键方面是将您的训练数据集分为训练集、验证集和测试集，这对于开发健壮的深度神经网络至关重要。我们还将讨论在训练神经网络中使用的各种损失函数和优化器。
- en: B.2.1 The training process
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.1 训练过程
- en: Once a neural network is built, the next step is to gather a training dataset
    to train the model. Figure B.2 illustrates the steps in the training process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 构建神经网络后，下一步是收集训练数据集以训练模型。图B.2展示了训练过程中的步骤。
- en: 'On the left side of figure B.2, we see the initial division of the training
    dataset into three subsets: the train set, the validation set, and the test set.
    This division is critical for building a robust deep neural network. The training
    set is the subset of data used to train the model, where the model learns patterns,
    weights, and biases. The validation set is used to evaluate the model’s performance
    during training and to decide when to stop training. The test set is used to assess
    the final performance of the model after training is complete, providing an unbiased
    evaluation of the model’s ability to generalize to new, unseen data.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在图B.2的左侧，我们可以看到训练数据集被分为三个子集：训练集、验证集和测试集。这种划分对于构建一个鲁棒的深度神经网络至关重要。训练集是用于训练模型的数据子集，其中模型学习模式、权重和偏差。验证集用于在训练过程中评估模型的表现，并决定何时停止训练。测试集用于在训练完成后评估模型的最终性能，提供对模型泛化到新、未见数据能力的无偏评估。
- en: During the training phase, the model is trained on data in the train set. It
    iteratively adjusts its parameters to minimize the loss function (see the next
    subsection on different loss functions). After each epoch, the model’s performance
    is evaluated using the validation set. If the performance on the validation set
    continues to improve, training proceeds. If the performance ceases to improve,
    training is stopped to prevent overfitting.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，模型在训练集中的数据上进行训练。它迭代地调整其参数以最小化损失函数（参见下一节关于不同损失函数的说明）。在每个周期之后，使用验证集评估模型的表现。如果验证集上的性能继续改进，则继续训练。如果性能停止改进，则停止训练以防止过拟合。
- en: '![](../../OEBPS/Images/APPB_F02_Liu.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/APPB_F02_Liu.png)'
- en: 'Figure B.2 Training a neural network. The training dataset is divided into
    three subsets: the train set, the validation set, and the test set. The process
    for training a neural network involves the following steps. In the training phase,
    the train set is used to train the neural network and adjust its parameters to
    minimize the loss function. During each iteration of training, the model updates
    its parameters based on data in the train set. In the validation phase of each
    iteration, the model is evaluated using the validation set. The performance on
    the validation set helps determine if the model is still improving. If the model’s
    performance on the validation set continues to improve, the next iteration of
    training proceeds using the train set. If the model’s performance on the validation
    set stops improving, the training process is stopped to prevent overfitting. Once
    training is complete, the trained model is evaluated on the test set. This evaluation
    provides the final testing results, giving an estimate of the model’s performance
    on unseen data.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.2 训练神经网络。训练数据集被分为三个子集：训练集、验证集和测试集。训练神经网络的步骤如下。在训练阶段，使用训练集来训练神经网络并调整其参数以最小化损失函数。在每次训练迭代的每个周期中，模型根据训练集中的数据更新其参数。在每个迭代的验证阶段，使用验证集评估模型。验证集上的性能有助于确定模型是否仍在改进。如果模型在验证集上的性能继续改进，则使用训练集进行下一次训练迭代。如果模型在验证集上的性能停止改进，则停止训练过程以防止过拟合。一旦训练完成，在测试集上评估训练好的模型。这次评估提供了最终的测试结果，给出了模型在未见数据上的性能估计。
- en: Once training is complete, the testing phase begins. The model is applied to
    the test set (unseen data) to assess its final performance and report results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，测试阶段开始。将模型应用于测试集（未见数据）以评估其最终性能并报告结果。
- en: Dividing the dataset into three different sets is essential for several reasons.
    The train subset allows the model to learn patterns and features from the data
    and to adjust its parameters. The validation subset serves as a check against
    overfitting by enabling performance monitoring during training. The test subset
    provides an unbiased evaluation of the model’s generalization ability, estimating
    its real-world performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集划分为三个不同的集合对于几个原因至关重要。训练子集允许模型从数据中学习模式和特征，并调整其参数。验证子集通过在训练期间进行性能监控来作为防止过拟合的检查。测试子集提供了对模型泛化能力的无偏评估，估计其在现实世界中的性能。
- en: By appropriately splitting the data and utilizing each set for its intended
    purpose, we ensure that the model is well trained and unbiasedly evaluated.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当地分割数据并利用每个集合的预期用途，我们确保模型得到了良好的训练和公正的评估。
- en: B.2.2 Loss functions
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.2 损失函数
- en: Loss functions are essential for measuring the accuracy of our predictions and
    guiding the optimization process when training deep neural networks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数对于衡量我们预测的准确性以及在训练深度神经网络时指导优化过程至关重要。
- en: A commonly used loss function is the mean squared error (MSE or L2 loss). MSE
    calculates the average squared difference between the model’s predictions and
    the actual values. A closely related loss function is the mean absolute error
    (MAE or L1 loss). MAE calculates the average absolute difference between predictions
    and actual values. MAE is often used if the data are noisy and have many outliers
    since it punishes extreme values less than the L2 loss.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的损失函数之一是均方误差（MSE或L2损失）。MSE计算模型预测值与实际值之间的平均平方差异。与之密切相关的一个损失函数是平均绝对误差（MAE或L1损失）。MAE计算预测值与实际值之间的平均绝对差异。当数据有噪声且有许多异常值时，MAE经常被使用，因为它对极端值的惩罚小于L2损失。
- en: For binary classification tasks, where predictions are binary (0 or 1), the
    preferred loss function is binary cross-entropy. This function measures the average
    difference between predicted probabilities and actual binary labels.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类任务，其中预测是二元的（0或1），首选的损失函数是二元交叉熵。此函数衡量预测概率与实际二元标签之间的平均差异。
- en: In multicategory classification tasks, where predictions can take multiple discrete
    values, the categorical cross-entropy loss function is employed. This function
    measures the average difference between predicted probability distributions and
    actual distributions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类别分类任务中，预测可以取多个离散值，此时使用的是类别交叉熵损失函数。此函数衡量预测概率分布与实际分布之间的平均差异。
- en: During the training of ML models such as deep neural networks, we adjust the
    model parameters to minimize the loss function. The adjustment magnitude is proportional
    to the first derivative of the loss function with respect to the model parameters.
    The learning rate controls the speed of these adjustments. If the learning rate
    is too high, the model parameters may oscillate around the optimal values and
    never converge. Conversely, if the learning rate is too low, the learning process
    becomes slow, and it takes a long time for the parameters to converge.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练如深度神经网络等机器学习模型的过程中，我们调整模型参数以最小化损失函数。调整幅度与损失函数相对于模型参数的一阶导数成正比。学习率控制这些调整的速度。如果学习率过高，模型参数可能会在最优值周围振荡，永远不会收敛。相反，如果学习率过低，学习过程会变得缓慢，参数收敛需要很长时间。
- en: B.2.3 Optimizers
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.3 优化器
- en: Optimizers are algorithms used in training deep neural networks to adjust the
    model’s weights to minimize the loss function. They guide the learning process
    by determining how the model’s parameters should be updated at each step, thus
    enhancing performance over time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是在训练深度神经网络时用于调整模型权重以最小化损失函数的算法。它们通过确定模型参数在每一步应该如何更新来指导学习过程，从而随着时间的推移提高性能。
- en: One example of an optimizer is a stochastic gradient descent (SGD). A SGD adjusts
    weights by moving them in the direction of the negative gradient of the loss function.
    It updates weights using a subset of the data (mini-batch) at each iteration,
    which helps speed up the training process and improve generalization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的一个例子是随机梯度下降（SGD）。SGD通过将权重移动到损失函数负梯度的方向来调整权重。它使用数据的一个子集（迷你批）在每个迭代中更新权重，这有助于加快训练过程并提高泛化能力。
- en: 'In this book, the most commonly used optimizer is Adam (Adaptive Moment Estimation).
    Adam combines the benefits of two other extensions of SGD: AdaGrad and RMSProp.
    It computes adaptive learning rates for each parameter based on estimates of the
    first and second moments of the gradients. This adaptability makes Adam particularly
    suitable for problems involving large datasets and/or numerous parameters.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，最常用的优化器是Adam（自适应矩估计）。Adam结合了SGD（随机梯度下降）的两种其他扩展的优势：AdaGrad和RMSProp。它根据梯度的第一和二阶矩的估计来为每个参数计算自适应学习率。这种适应性使得Adam特别适合涉及大数据集和/或大量参数的问题。
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([1](#footnote-000-backlink))  Eli Stevens, Luca Antiga, and Thomas Viehmann,
    2020, *Deep Learning with PyTorch*, Manning Publications.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-000-backlink))  埃利·史蒂文斯，卢卡·安蒂加，托马斯·维曼，2020年，*使用PyTorch进行深度学习*，Manning出版社。
