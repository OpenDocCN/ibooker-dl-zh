- en: Chapter 14\. Natural Language Processing with RNNs and Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章\. 使用RNN和注意力进行自然语言处理
- en: 'When Alan Turing imagined his famous [Turing test](https://homl.info/turingtest)⁠^([1](ch14.html#id3189))
    in 1950, he proposed a way to evaluate a machine’s ability to match human intelligence.
    He could have tested for many things, such as the ability to recognize cats in
    pictures, play chess, compose music, or escape a maze, but, interestingly, he
    chose a linguistic task. More specifically, he devised a *chatbot* capable of
    fooling its interlocutor into thinking it was human.⁠^([2](ch14.html#id3191))
    This test does have its weaknesses: a set of hardcoded rules can fool unsuspecting
    or naive humans (e.g., the machine could give vague predefined answers in response
    to some keywords, it could pretend that it is joking or drunk to get a pass on
    its weirdest answers, or it could escape difficult questions by answering them
    with its own questions), and many aspects of human intelligence are utterly ignored
    (e.g., the ability to interpret nonverbal communication such as facial expressions,
    or to learn a manual task). But the test does highlight the fact that mastering
    language is arguably *Homo sapiens*’s greatest cognitive ability.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当艾伦·图灵在1950年构想他著名的[Turing测试](https://homl.info/turingtest)⁠^([1](ch14.html#id3189))时，他提出了一种评估机器匹配人类智能能力的方法。他本可以测试许多事情，例如识别图片中的猫、下棋、作曲或逃离迷宫的能力，但有趣的是，他选择了一个语言任务。更具体地说，他设计了一个能够欺骗其对话者认为它是人类的*聊天机器人*。⁠^([2](ch14.html#id3191))
    这个测试确实有其弱点：一组硬编码的规则可以欺骗毫无戒备或天真的人类（例如，机器可以对某些关键词给出模糊的预定义答案，它可以假装在开玩笑或醉酒以通过其最奇怪的答案，或者它可以通过用自己提出的问题来回答困难的问题来逃避问题），并且忽略了人类智能的许多方面（例如，解读非言语交流的能力，如面部表情，或学习手动任务的能力）。但这个测试确实突出了掌握语言可能是*智人*最大的认知能力。
- en: Until recently, state-of-the-art natural language processing (NLP) models were
    pretty much all based on recurrent neural networks (introduced in [Chapter 13](ch13.html#rnn_chapter)).
    However, in recent years, RNNs have been replaced with transformers, which we
    will explore in [Chapter 15](ch15.html#transformer_chapter). That said, it’s still
    important to learn how RNNs can be used for NLP tasks, if only because it helps
    better understand transformers. Moreover, most of the techniques we will discuss
    in this chapter are also useful with Transformer architectures (e.g., tokenization,
    beam search, attention mechanisms, and more). Plus, RNNs have recently made a
    surprise comeback in the form of state space models (SSMs) (see “State-Space Models
    (SSMs)” at [*https://homl.info*](https://homl.info)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，最先进的自然语言处理（NLP）模型几乎都是基于循环神经网络（在第13章中介绍）。然而，近年来，RNN已被transformers所取代，我们将在第15章（[15](ch15.html#transformer_chapter)）中探讨。尽管如此，了解如何使用RNN进行NLP任务仍然很重要，这不仅因为这样有助于更好地理解transformers。此外，本章中我们将讨论的大部分技术也适用于Transformer架构（例如，标记化、束搜索、注意力机制等）。此外，RNN最近以状态空间模型（SSMs）的形式意外回归（参见“状态空间模型（SSMs）”在[*https://homl.info*](https://homl.info)）。
- en: This chapter is organized in three sections. In the first section, we will start
    by building a *character RNN*, or *char-RNN*, trained to predict the next character
    in a sentence. On the way, we will learn about trainable embeddings. Our char-RNN
    will be our first tiny *language model*, capable of generating original text.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为三个部分。在第一部分，我们将从构建一个*字符RNN*，或*char-RNN*开始，该RNN被训练来预测句子中的下一个字符。在这个过程中，我们将了解可训练嵌入。我们的char-RNN将成为我们的第一个微型的*语言模型*，能够生成原始文本。
- en: 'In the second section, we will turn to text classification, and more specifically
    sentiment analysis, which aims to predict how positive or negative some text is.
    Our model will read movie reviews and estimate the rater’s feeling about the movie.
    This time, instead of splitting the text into individual characters, we will split
    it into *tokens*: a token is a small piece of text from a fixed-sized vocabulary,
    such as the top 10,000 most common words in the English language, or the most
    common subwords (e.g., “smartest” = “smart” + “est”), or even individual characters
    or bytes. To split the text into tokens, we will use a *tokenizer*. This section
    will also introduce popular Hugging Face libraries: the *Datasets* library to
    download datasets, the *Tokenizers* library for tokenizers, and the *Transformers*
    library for popular models, downloaded automatically from the *Hugging Face Hub*.
    Hugging Face is a hugely influential company and open source community, and it
    plays a central role in the open source AI space, especially in NLP.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将转向文本分类，更具体地说，是情感分析，其目的是预测某些文本的积极或消极程度。我们的模型将阅读电影评论并估计评论者对电影的感受。这次，我们不会将文本拆分为单个字符，而是将其拆分为*tokens*：一个token是从固定大小的词汇中提取的小块文本，例如英语中最常见的10,000个单词，或者最常见的子词（例如，“smartest”=“smart”+“est”），甚至单个字符或字节。为了将文本拆分为tokens，我们将使用*tokensizer*。本节还将介绍流行的Hugging
    Face库：*Datasets*库用于下载数据集，*Tokenizers*库用于tokenizers，以及*Transformers*库用于流行的模型，这些模型将自动从*Hugging
    Face Hub*下载。Hugging Face是一家极具影响力的公司，也是开源社区，它在开源AI领域，尤其是在NLP领域扮演着核心角色。
- en: 'The final boss of this chapter will be neural machine translation (NMT), the
    topic of the third and last section: we will build an encoder-decoder model capable
    of translating English to Spanish. This will lead us to *attention mechanisms*,
    which we will apply to our encoder-decoder model to improve its capacity to handle
    long input texts. As their name suggests, attention mechanisms are neural network
    components that learn to select the part of the inputs that the model should focus
    on at each time step. They directly led to the transformers revolution, as we
    will see in the next chapter.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最终目标将是神经机器翻译（NMT），这是第三部分和最后一部分的主题：我们将构建一个能够将英语翻译成西班牙语的编码器-解码器模型。这将引导我们到*注意力机制*，我们将将其应用于我们的编码器-解码器模型，以提高其处理长输入文本的能力。正如其名称所暗示的，注意力机制是神经网络组件，它们学会在每个时间步选择模型应该关注的输入部分。它们直接导致了下一章中我们将看到的转换器革命。
- en: Let’s start with a simple and fun char-RNN model that can write like Shakespeare
    (sort of).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常简单且有趣的char-RNN模型开始，它可以写出类似莎士比亚的作品（某种程度上）。
- en: Generating Shakespearean Text Using a Character RNN
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用字符RNN生成莎士比亚风格的文本
- en: 'In a famous [2015 blog post](https://homl.info/charrnn) titled “The Unreasonable
    Effectiveness of Recurrent Neural Networks”, Andrej Karpathy showed how to train
    an RNN to predict the next character in a sentence. This *char-RNN* can then be
    used to generate novel text, one character at a time. Here is a small sample of
    the text generated by a char-RNN model after it was trained on all of Shakespeare’s
    works:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇著名的[2015年博客文章](https://homl.info/charrnn)中，标题为“循环神经网络的不合理有效性”，安德烈·卡帕西展示了如何训练一个RNN来预测句子中的下一个字符。这个*char-RNN*可以用来逐个字符生成新的文本。以下是char-RNN模型在训练了所有莎士比亚作品后生成的一小段文本样本：
- en: 'PANDARUS:'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PANDARUS：
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alas, I think he shall be come approached and the day
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 唉，我想他将被接近，那一天
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When little srain would be attain’d into being never fed,
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当微小的压力得以形成却从未被满足时，
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And who is but a chain and subjects of his death,
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谁是但一条链和死亡的主宰，
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I should not sleep.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不应该睡觉。
- en: Not exactly a masterpiece, but it is still impressive that the model was able
    to learn words, grammar, proper punctuation, and more, just by learning to predict
    the next character in a sentence. This is our first example of a *language model*.
    In the remainder of this section we’ll build a char-RNN step by step, starting
    with the creation of the dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个杰作，但令人印象深刻的是，模型仅通过学习预测句子中的下一个字符就能学会单词、语法、正确的标点符号等等。这是我们第一个*语言模型*的例子。在本节的剩余部分，我们将逐步构建char-RNN，从创建数据集开始。
- en: Creating the Training Dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练数据集
- en: 'First, let’s download a subset of Shakespeare’s works (about 25%). The data
    is loaded from Andrej Karpathy’s [char-rnn project](https://github.com/karpathy/char-rnn):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们下载莎士比亚作品的一个子集（大约25%）。数据是从安德烈·卡帕西的[char-rnn项目](https://github.com/karpathy/char-rnn)中加载的：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s print the first few lines:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先打印出前几行：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2][PRE3]``py[PRE4]``py Note that we call `lower()` to ignore case and thereby
    reduce the vocabulary size. We must now assign a token ID to each character. For
    this, we can just use its index in the vocabulary. To decode the output of our
    model, we will also need a way to go from a token ID to a character:    [PRE5]py`
    `>>>` `char_to_id``[``"a"``]` [PRE6]py [PRE7]``py[PRE8][PRE9][PRE10] import torch  def
    encode_text(text):     return torch.tensor([char_to_id[char] for char in text.lower()])  def
    decode_text(char_ids):     return "".join([id_to_char[char_id.item()] for char_id
    in char_ids]) [PRE11] >>> encoded = encode_text("Hello, world!") `>>>` `encoded`
    [PRE12] [PRE13][PRE14]`` [PRE15] from torch.utils.data import Dataset, DataLoader  class
    CharDataset(Dataset):     def __init__(self, text, window_length):         self.encoded_text
    = encode_text(text)         self.window_length = window_length      def __len__(self):         return
    len(self.encoded_text) - self.window_length      def __getitem__(self, idx):         if
    idx >= len(self):             raise IndexError("dataset index out of range")         end
    = idx + self.window_length         window = self.encoded_text[idx : end]         target
    = self.encoded_text[idx + 1 : end + 1]         return window, target [PRE16] window_length
    = 50 batch_size = 512  # reduce if your GPU cannot handle such a large batch size
    train_set = CharDataset(shakespeare_text[:1_000_000], window_length) valid_set
    = CharDataset(shakespeare_text[1_000_000:1_060_000], window_length) test_set =
    CharDataset(shakespeare_text[1_060_000:], window_length) train_loader = DataLoader(train_set,
    batch_size=batch_size, shuffle=True) valid_loader = DataLoader(valid_set, batch_size=batch_size)
    test_loader = DataLoader(test_set, batch_size=batch_size) [PRE17]` [PRE18][PRE19][PRE20][PRE21][PRE22]
    [PRE23]`py` [PRE24]  [PRE25][PRE26]``py[PRE27]`py` ## Embeddings    An embedding
    is a dense representation of some higher-dimensional data, typically a categorical
    feature. If there are 50,000 possible categories, then one-hot encoding produces
    a 50,000-dimensional sparse vector (i.e., containing mostly zeros). In contrast,
    an embedding is a comparatively small dense vector; for example, with just 300
    dimensions.    ###### Tip    The embedding size is a hyperparameter you can tune.
    As a rule of thumb, a good embedding size is often close to the square root of
    the number of categories.    In deep learning, embeddings are usually initialized
    randomly, and they are then trained by gradient descent, along with the other
    model parameters. For example, if we wanted to train a neural network on the California
    housing dataset (see [Chapter 2](ch02.html#project_chapter)), we could represent
    the `ocean_proximity` categorical feature using embeddings. The `"NEAR BAY"` category
    could be represented initially by a random vector such as `[0.831, 0.696]`, while
    the `"NEAR OCEAN"` category might be represented by another random vector such
    as `[0.127, 0.868]` (in this example we are using 2D embeddings).    Since these
    embeddings are trainable, they will gradually improve during training; and as
    they represent fairly similar categories in this example, gradient descent will
    certainly end up pushing them closer together, while it will tend to move them
    away from the `"INLAND"` category’s embedding (see [Figure 14-2](#embedding_diagram)).
    Indeed, the better the representation, the easier it will be for the neural network
    to make accurate predictions, so training tends to make embeddings useful representations
    of the categories. This is called *representation learning* (you will see other
    types of representation learning in [Chapter 18](ch18.html#autoencoders_chapter)).  ![Diagram
    illustrating the improvement of embeddings during training, showing categories
    such as "Near ocean," "Near bay," and "Inland" in an embedding space.](assets/hmls_1402.png)  ######
    Figure 14-2\. Embeddings will gradually improve during training    Not only will
    embeddings generally be useful representations for the task at hand, but quite
    often these same embeddings can be reused successfully for other tasks. The most
    common example of this is *word embeddings* (i.e., embeddings of individual words):
    when you are working on a natural language processing task, you are often better
    off reusing pretrained word embeddings than training your own, as we will see
    later in this chapter.    The idea of using vectors to represent words dates back
    to the 1960s, and many sophisticated techniques have been used to generate useful
    vectors, including using neural networks. But things really took off in 2013,
    when Tomáš Mikolov and other Google researchers published a [paper](https://homl.info/word2vec)⁠^([3](ch14.html#id3210))
    describing an efficient technique to learn word embeddings using neural networks,
    significantly outperforming previous attempts. This allowed them to learn embeddings
    on a very large corpus of text: they trained a neural network to predict the words
    near any given word and obtained astounding word embeddings. For example, synonyms
    had very close embeddings, and semantically related words such as *France*, *Spain*,
    and *Italy* were clustered together.    It’s not just about proximity, though:
    word embeddings are also organized along meaningful axes in the embedding space.
    Here is a famous example: if you compute *King – Man + Woman* (adding and subtracting
    the embedding vectors of these words), then the result will be very close to the
    embedding of the word *Queen* (see [Figure 14-3](#word_embedding_diagram)). In
    other words, the word embeddings encode the concept of gender! Similarly, you
    can compute *Madrid – Spain + France*, and the result is close to *Paris*, which
    seems to show that the notion of capital city is also encoded in the embeddings.  ![Diagram
    illustrating how word embeddings calculate "King - Man + Woman" to approximate
    the position of "Queen," demonstrating the encoding of the gender concept in the
    embedding space.](assets/hmls_1403.png)  ###### Figure 14-3\. Word embeddings
    of similar words tend to be close, and some axes seem to encode meaningful concepts    ######
    Warning    Word embeddings can have some meaningful structure, as the “King –
    Man + Woman” shows. However, they are also noisy and often hard to interpret.
    I’ve added some code at the end of the notebook so you can judge for yourself.    Unfortunately,
    word embeddings sometimes capture our worst biases. For example, although they
    correctly learn that *Man is to King as Woman is to Queen*, they also seem to
    learn that *Man is to Doctor as Woman is to Nurse*: quite a sexist bias! To be
    fair, this particular example is probably exaggerated, as was pointed out in a
    [2019 paper](https://homl.info/fairembeds)⁠^([4](ch14.html#id3211)) by Malvina
    Nissim et al. Nevertheless, ensuring fairness in deep learning algorithms is an
    important and active research topic.    PyTorch provides an `nn.Embedding` module,
    which wraps an *embedding matrix*: this matrix has one row per possible category
    (e.g., one row for each token in the vocabulary) and one column per embedding
    dimension. The embedding dimensionality is a hyperparameter you can tune. By default,
    the embedding matrix is initialized randomly.    To convert a category ID to an
    embedding, the `nn.Embedding` layer just looks up and returns the corresponding
    row. That’s all there is to it! For example, let’s initialize an `nn.Embedding`
    layer with five categories and 3D embeddings, and use it to encode some categories:    [PRE28]py`
    `>>>` `embed` `=` `nn``.``Embedding``(``5``,` `3``)`  `# 5 categories × 3D embeddings`
    [PRE29]py [PRE30] [PRE31] [PRE32][PRE33]` [PRE34][PRE35]py[PRE36] To have more
    control over the diversity of the generated text, we can divide the logits by
    a number called the *temperature*, which we can tweak as we wish. A temperature
    close to zero favors high-probability characters, while a high temperature gives
    all characters an equal probability. Lower temperatures are typically preferred
    when generating fairly rigid and precise text, such as mathematical equations,
    while higher temperatures are preferred when generating more diverse and creative
    text. Let’s write a `next_char()` helper function that will use this approach
    to pick the next character to add to the input text:    [PRE37]    Next, we can
    write another small helper function that will repeatedly call `next_​char()` to
    get the next character and append it to the given text:    [PRE38]    We are now
    ready to generate some text! Let’s try low, medium, and high temperatures:    [PRE39]
    `To be or not to be the better from the cause` `that thou think you may be so
    be gone.`  `romeo:` `that` `>>>` `print``(``extend_text``(``model``,` `"To be
    or not to b"``,` `temperature``=``100``))` `` `To be or not to b-c3;m-rkn&:x:uyve:b&hi
    n;n-h;wt3k` `&cixxh:a!kq$c$ 3 ncq$ ;;wq cp:!xq;yh` `!3` `d!nhi.` `` [PRE40]   [PRE41]
    ``Notice the repetitions when the temperature is low: “the state and the” appears
    twice. The intermediate temperature led to more convincing results, although Romeo
    wasn’t very talkative today. But in the last example the temperature was way too
    high—we fried Shakespeare. To generate more convincing text, a common technique
    is to sample only from the top *k* characters, or only from the smallest set of
    top characters whose total probability exceeds some threshold: this is called
    *top-p sampling*, or *nucleus sampling*. Alternatively, you could try using *beam
    search*, which we will discuss later in this chapter, or using more `nn.GRU` layers
    and more neurons per layer, training for longer, and adding more regularization
    if needed.    ###### Note    The model is currently incapable of learning patterns
    longer than `window_length`, which is just 50 characters. You could try making
    this window larger, but it would also make training harder, and even LSTM and
    GRU cells cannot handle very long sequences.⁠^([6](ch14.html#id3232))    Interestingly,
    although a char-RNN model is just trained to predict the next character, this
    seemingly simple task actually requires it to learn some higher-level tasks as
    well. For example, to find the next character after “Great movie, I really “,
    it’s helpful to understand that the sentence is positive, so what follows is more
    likely to be the letter “l” (for “loved”) rather than “h” (for “hated”). In fact,
    a [2017 paper](https://homl.info/sentimentneuron)⁠^([7](ch14.html#id3233)) by
    Alec Radford and other OpenAI researchers describes how the authors trained a
    big char-RNN-like model on a large dataset, and found that one of the neurons
    acted as an excellent sentiment analysis classifier. Although the model was trained
    without any labels, the *sentiment neuron*—as they called it—reached state-of-the-art
    performance on sentiment analysis benchmarks (this foreshadowed and motivated
    unsupervised pretraining in NLP).    Speaking of which, let’s say farewell to
    Shakespeare and turn to the second part of this chapter: sentiment analysis.``
    [PRE42]` [PRE43][PRE44][PRE45][PRE46][PRE47] [PRE48]`py` [PRE49] [PRE50][PRE51]``py[PRE52]py[PRE53]py[PRE54]py[PRE55]`py[PRE56]py[PRE57]``py[PRE58]py[PRE59][PRE60][PRE61][PRE62]
    [PRE63]`py[PRE64]py[PRE65]``py[PRE66][PRE67] Notice that frequent words like “what”
    and “movie” have been identified by the BPE model and are represented by a single
    token, while less frequent words like “awesome” are split into multiple tokens.
    Also note that the smiley was not part of the training data, so it gets replaced
    with the unknown token `"<unk>"`.    The tokenizer provides a `get_vocab()` method
    which returns a dictionary mapping each token to its ID. You can also use the
    `token_to_id()` method to map a single token, or conversely use the `id_to_token()`
    method to go from ID to token. However, you will more often use the `decode()`
    method to convert a list of token IDs into a string:    [PRE68]py   [PRE69] The
    tokenizer keeps track of each token’s start and end offset in the original string,
    which can come in handy, especially for debugging:    [PRE70]   [PRE71][PRE72]``py[PRE73]``
    [PRE74]`` [PRE75]` Notice how the first and second review were padded with 0s,
    which is our padding token ID. Each `Encoding` object also includes an `attention_mask`
    attribute containing a 1 for each nonpadding token, and a 0 for each padding token.
    This can be used in your models to easily ignore the padded time steps: just multiply
    a tensor with the attention mask. In some cases you will prefer to have the list
    of sequence lengths (ignoring padding). Here’s how to get both the attention mask
    tensor and the sequence lengths:    [PRE76][PRE77]`` `>>>` `attention_mask` [PRE78]
    `>>>` `lengths` `` `tensor([281, 114, 285])` `` [PRE79]` [PRE80][PRE81]   [PRE82]
    ``You may have noted that spaces were not handled very well by our tokenizer.
    In particular, the word “awesome” came back as “aw es ome”, and “movie!” came
    back as “movie !”. This is because the `Whitespace` pre-tokenizer dropped all
    spaces, therefore the BPE tokenizer doesn’t know where spaces should go and it
    just adds spaces between all tokens. To fix this, we can replace the `Whitespace`
    pre-tokenizer with the `ByteLevel` pre-tokenizer: it replaces all spaces with
    a special character Ġ so the BPE model doesn’t lose track of them. For example,
    if you use this pre-tokenizer and you encode and decode the text “what an awesome
    movie! ![](assets/smiling-face-with-smiling-eyes_1f60a_(1).png)“, you will get:
    “Ġwhat Ġan Ġaw es ome Ġmovie !”. After removing the spaces, then replacing every
    Ġ with a space, you get " what an awesome movie!”. This is almost perfect, except
    for the extra space at the start—which is easily removed—and the lost emoji, which
    was replaced with an unknown token because it’s not in the vocabulary, and dropped
    by the `decode()` method.    As its name suggests, the `ByteLevel` pre-tokenizer
    allows the BPE model to work at the byte level, rather than the character level:
    unsurprisingly, this is called Byte-level BPE (BBPE). For example, the ![](assets/smiling-face-with-smiling-eyes_1f60a_(1).png)
    emoji will be converted to four bytes, using Unicode’s UTF-8 encoding. This means
    that BBPE will never output an unknown token if its vocabulary contains all 256
    possible bytes, since any text can be broken down into its individual bytes whenever
    longer tokens are not found in the vocabulary. This makes BBPE well suited when
    the corpus contains rare characters such as emojis.    Another important variant
    of BPE is [WordPiece](https://homl.info/wordpiece),⁠^([9](ch14.html#id3256)) proposed
    by Google in 2016\. This tokenization algorithm is very similar to BPE, but instead
    of adding the most frequent adjacent pair of tokens to the vocabulary at each
    iteration, it adds the pair with the highest score. This score is computed using
    [Equation 14-1](#wordpiece_equation): the frequency(AB) term is just like in BPE—it
    boosts pairs that are frequent in the corpus. However, the denominator reduces
    the score of a pair when the individual tokens are themselves frequent. This normalization
    tends to favor more useful and meaningful tokens than BPE, and the algorithm often
    produces shorter encoded sequences than BPE or BBPE.    ##### Equation 14-1\.
    WordPiece score for a pair AB composed of tokens A and B  $score left-parenthesis
    AB right-parenthesis equals StartFraction frequency left-parenthesis AB right-parenthesis
    Over freq left-parenthesis upper A right-parenthesis dot freq left-parenthesis
    upper B right-parenthesis EndFraction dot len left-parenthesis vocab right-parenthesis$  To
    train a WordPiece tokenizer using the Tokenizers library, you can use the same
    code as for BPE, but replace the `BPE` model with `WordPiece`, and the `BpeTrainer`
    with `WordPieceTrainer`. If you encode and decode the same review as earlier,
    you will get “what an aw esome movie !”. Notice that WordPiece adds a prefix to
    tokens that are inside a word, which makes it easy to reconstruct the original
    string: just remove " #“# (as well as spaces before punctuations). Note that the
    smiley emoji once again disappeared because it was not in the vocabulary.    One
    last popular tokenization algorithm we will discuss is Unigram LM (Language Model),
    introduced in a [2018 paper](https://homl.info/subword)⁠^([10](ch14.html#id3258))
    by Taku Kudo at Google. This technique is a bit different than the previous ones:
    it starts out with a very large vocabulary containing every frequent word, subword,
    and character in the training corpus, then it gradually removes the least useful
    tokens until it reaches the desired vocabulary size. To determine how useful a
    token is, this method makes one big simplifying assumption: it assumes that the
    corpus was sampled randomly from the vocabulary, one token at a time (hence the
    name Unigram LM), and that every token was sampled independently from the others.
    Therefore, this tokenizer model assumes that the probability of sampling the pair
    AB is equal to the probability of sampling A times the probability of sampling
    B. Given this assumption, it can estimate the probability of sampling the whole
    training corpus. At each iteration, the training algorithm attempts to remove
    tokens without reducing this overall probability too much.    For example, suppose
    that the vocabulary contains the tokens “them”, “the”, and “m”, respectively,
    with 1%, 5%, and 2% probability. This means that the word “them” has a 1% chance
    of being sampled as the single token “them”, or a 5% × 2% = 0.1% chance of being
    sampled as the pair “the” + “m”. Overall, the word “them” has a 1% + 0.1% = 1.1%
    chance of being sampled. If we remove the token “them” from the vocabulary, then
    the probability of sampling the word “them” drops down to 0.1%. If instead we
    remove either “m” or “the”, then the probability only drops down to 1% since we
    can still sample the single token “them”. So if the training corpus only contained
    the word “them”, then the algorithm would prefer to drop either “the” or “m”.
    Of course, in reality the corpus contains many other words that contain these
    two tokens, so the algorithm will likely find other less useful tokens to drop.    ######
    Tip    Unigram LM is great for languages that don’t use spaces to separate words,
    like English does. For example, Chinese text does not use spaces between words,
    Vietnamese uses spaces even within words, and German often attaches multiple words
    together, without spaces.    The same paper also proposed a novel regularization
    technique called *subword regularization*, which improves generalization and robustness
    by introducing some randomness in tokenization while training the NLP model (not
    the tokenizer model). For example, assuming the vocabulary contains the tokens
    “them”, “the”, and “m”, and you choose to use subword regularization, then the
    word “them” will sometimes be tokenized as “the” + “m”, and sometimes as “them”.
    This technique works best with *morphologically rich languages*, meaning languages
    where words carry a lot of grammatical information through affixes, inflections,
    and internal modifications (such as Arabic, Finnish, German, Hungarian, Polish,
    or Turkish), as opposed to languages that rely on word order or additional helper
    words (such as English, Chinese, Thai, or Vietnamese).    Unfortunately, the Tokenizers
    library does not natively support subword regularization, so you either have to
    implement it yourself, or you can use Google’s [*SentencePiece*](https://github.com/google/sentencepiece)
    library (`pip install sentencepiece`) which provides an open source implementation.
    This project is described in a [2018 paper](https://homl.info/sentencepiece)⁠^([11](ch14.html#id3263))
    by Taku Kudo and John Richardson.    [Table 14-1](#tokenizer_summary_table) summarizes
    the three main tokenizers used today.      Table 14-1\. Overview of the three
    main tokenizers   | Feature | BBPE | WordPiece | Unigram LM | | --- | --- | ---
    | --- | | **How** | Merge most frequent pairs | Merge pairs that maximize data
    likelihood | Remove least likely tokens | | **Pros** | Fast, simple, great for
    multilingual | Good balance of efficiency and token quality | Most meaningful,
    shortest sequences | | **Cons** | Can produce awkward splits | Less robust than
    BBPE for multilingual | Slower to train and tokenize | | **Used By** | GPT, Llama,
    RoBERTa, BLOOM | BERT, DistilBERT, ELECTRA | T5, ALBERT, mBART |    Training your
    own tokenizer is useful in many situations; for example, if you are dealing with
    domain-specific text, such as medical, legal, or engineering documents full of
    jargon, or if the text is written in a low-resource language or dialect, or if
    it’s code written in a new programming language, and so on. However, in most cases
    you will want to simply reuse a pretrained tokenizer.`` [PRE83]` [PRE84]` [PRE85]``
    [PRE86][PRE87]py[PRE88]py`` [PRE89]`py [PRE90]`py` [PRE91]`py``  [PRE92]`py[PRE93]py[PRE94]py
    [PRE95]`py[PRE96]py[PRE97]py[PRE98]py[PRE99]py[PRE100]`py[PRE101]py[PRE102]` [PRE103]`
    As you can see, this BERT model is implemented using PyTorch, and it contains
    a regular `nn.Embedding` layer. We could just replace our model’s `nn.Embedding`
    layer with this one (and retrain our model), but we can keep models cleanly separated
    by initializing our own `nn.Embedding` layer with a copy of the pretrained embedding
    matrix. This can be done using the `Embedding.from_pretrained()` function:    [PRE104]    Note
    that we set `freeze=True` when creating the `nn.Embedding` layer: this makes it
    nontrainable and ensures that the pretrained embeddings won’t be damaged by large
    gradients at the beginning of training. You can train the model for a few epochs
    like this, then make the embedding layer trainable and continue training, letting
    the model fine-tune the embeddings for our task.    Pretrained word embeddings
    have been popular for quite a while, starting with Google’s [Word2vec embeddings](https://homl.info/word2vec)
    (2013), Stanford’s [GloVe embeddings](https://homl.info/glove) (2014), Facebook’s
    [FastText embeddings](https://fasttext.cc) (2016), and more. However, this approach
    has its limits. In particular, a word has a single representation, no matter the
    context. For example, the word “right” is encoded the same way in “left and right”
    and “right and wrong”, even though it means two very different things. To address
    this limitation, a [2018 paper](https://homl.info/elmo)⁠^([13](ch14.html#id3298))
    by Matthew Peters introduced *Embeddings from Language Models* (ELMo): these are
    contextualized word embeddings learned from the internal states of a deep bidirectional
    RNN language model. In other words, instead of just using pretrained word embeddings
    in your model, you can reuse several layers of a pretrained language model.    At
    roughly the same time, the [Universal Language Model Fine-Tuning (ULMFiT) paper](https://homl.info/ulmfit)⁠^([14](ch14.html#id3303))
    by Jeremy Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised
    pretraining for NLP tasks. The authors trained an LSTM language model on a huge
    text corpus using self-supervised learning (i.e., generating the labels automatically
    from the data), then they fine-tuned it on various tasks. Their model outperformed
    the state of the art on six text classification tasks by a large margin (reducing
    the error rate by 18%–24% in most cases). Moreover, the authors showed that a
    pretrained model fine-tuned on just 100 labeled examples could achieve the same
    performance as one trained from scratch on 10,000 examples. Before the ULMFiT
    paper, using pretrained models was only the norm in computer vision; in the context
    of NLP, pretraining was limited to word embeddings. This paper marked the beginning
    of a new era in NLP: today, reusing pretrained language models is the norm.    For
    example, why not reuse the entire pretrained BERT model for our sentiment analysis
    model? To use the BERT model, the Transformers library lets us call it like a
    function, passing it the tokenized reviews:    [PRE105][PRE106]`` `...` [PRE107]
    `>>>` `bert_output``.``last_hidden_state``.``shape` `` `torch.Size([3, 200, 768])`
    `` [PRE108]` [PRE109][PRE110]   [PRE111] [PRE112]`py [PRE113]py``  [PRE114]`py[PRE115][PRE116][PRE117]``py[PRE118]py
    [PRE119]py`` We first tokenize the review, then we call the model, it returns
    a `ModelOutput` object containing the logits, and we convert these logits to estimated
    probabilities using the `torch.softmax()` function. Ouch! The model classified
    this review as negative with 65.53% confidence! Indeed, the BERT model inside
    `BertForSequenceClassification` is pretrained, but not the classification head,
    so we’re going to get terrible performance until we actually train this model
    on the IMDb dataset.    If you pass labels when calling this model (or any other
    model from the Transformers library), then it also computes the loss and returns
    it in the `ModelOutput` object. For example:    [PRE120]py[PRE121]` `... `    `labels``=``torch``.``tensor``([``1``],`
    `device``=``device``))` [PRE122] `>>>` `output``.``loss` `` `tensor(0.4226, device=''cuda:0'',
    dtype=torch.float16)` `` [PRE123]` [PRE124][PRE125][PRE126]   [PRE127] ``Since
    `num_labels` is greater than 1, the model computes the `nn.CrossEntropyLoss` (which
    is implemented as `nn.LogSoftmax` followed by `nn.NLLLoss`—that’s why we see `grad_fn=<NllLossBackward0>`).
    If we had used `num_labels=1`, then the model would have used the `nn.MSELoss`
    instead; this can be useful for regression tasks.    We could now train this model
    using our own training code, as we did so far, but the Transformers library provides
    a convenient *Trainer API*, so let’s check it out.`` [PRE128]` [PRE129]`` [PRE130][PRE131]py[PRE132]py
    [PRE133] [PRE134][PRE135]` `... `                           `truncation``=``True``,`
    `max_length``=``512``)` [PRE136] `>>>` `classifier_imdb``(``train_reviews``[:``10``])`
    `` `[{''label'': ''POSITIVE'', ''score'': 0.9996108412742615},`  `{''label'':
    ''POSITIVE'', ''score'': 0.9998623132705688},`  `[...]`  `{''label'': ''POSITIVE'',
    ''score'': 0.9978922009468079},`  `{''label'': ''NEGATIVE'', ''score'': 0.9997020363807678}]`
    `` [PRE137]` [PRE138][PRE139]   [PRE140][PRE141]``py[PRE142][PRE143] >>> model_name
    = "huggingface/distilbert-base-uncased-finetuned-mnli" `>>>` `classifier_mnli`
    `=` `pipeline``(``"text-classification"``,` `model``=``model_name``)` [PRE144]`
    `>>>` `classifier_mnli``([` [PRE145] `... `    `"She loves me. [SEP] She loves
    me not. [SEP]"``,` [PRE146]`py [PRE147] [PRE148][PRE149]``py[PRE150]``py`` [PRE151]`py
    [PRE152]`py` [PRE153]`py`` [PRE154]`py[PRE155][PRE156][PRE157] [PRE158][PRE159][PRE160][PRE161][PRE162]``
    [PRE163][PRE164][PRE165] [PRE166]`py[PRE167]py[PRE168]py[PRE169]py[PRE170]py[PRE171]py`
    [PRE172]`py[PRE173]py[PRE174]py[PRE175][PRE176][PRE177][PRE178][PRE179]``py[PRE180]`py`
    [PRE181]py [PRE182]py``  [PRE183]py`` [PRE184]py [PRE185]`py [PRE186]py`` [PRE187]py[PRE188][PRE189][PRE190][PRE191]py[PRE192]py[PRE193]py[PRE194]py[PRE195]`py[PRE196]py[PRE197]'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2][PRE3]``py[PRE4]``py 注意，我们调用 `lower()` 来忽略大小写，从而减少词汇量。现在我们必须为每个字符分配一个标记ID。为此，我们可以直接使用它在词汇表中的索引。为了解码我们模型的输出，我们还需要一种从标记ID到字符的转换方法：    [PRE5]py`
    `>>>` `char_to_id``[``"a"``]` [PRE6]py [PRE7]``py[PRE8][PRE9][PRE10] 导入 torch  def
    encode_text(text):     return torch.tensor([char_to_id[char] for char in text.lower()])  def
    decode_text(char_ids):     return "".join([id_to_char[char_id.item()] for char_id
    in char_ids]) [PRE11] >>> encoded = encode_text("Hello, world!") `>>>` `encoded`
    [PRE12] [PRE13][PRE14]`` [PRE15] 从 torch.utils.data 导入 Dataset, DataLoader  class
    CharDataset(Dataset):     def __init__(self, text, window_length):         self.encoded_text
    = encode_text(text)         self.window_length = window_length      def __len__(self):         return
    len(self.encoded_text) - self.window_length      def __getitem__(self, idx):         if
    idx >= len(self):             raise IndexError("dataset index out of range")         end
    = idx + self.window_length         window = self.encoded_text[idx : end]         target
    = self.encoded_text[idx + 1 : end + 1]         return window, target [PRE16] window_length
    = 50 batch_size = 512  # 如果你的GPU无法处理如此大的批量大小，请减少此值 train_set = CharDataset(shakespeare_text[:1_000_000],
    window_length) valid_set = CharDataset(shakespeare_text[1_000_000:1_060_000],
    window_length) test_set = CharDataset(shakespeare_text[1_060_000:], window_length)
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True) valid_loader
    = DataLoader(valid_set, batch_size=batch_size) test_loader = DataLoader(test_set,
    batch_size=batch_size) [PRE17]` [PRE18][PRE19][PRE20][PRE21][PRE22] [PRE23]`py`
    [PRE24]  [PRE25][PRE26]``py[PRE27]`py` ## 嵌入    嵌入是某些高维数据的密集表示，通常是分类特征。如果有 50,000
    个可能的类别，那么 one-hot 编码会产生一个 50,000 维的稀疏向量（即，大部分是零）。相比之下，嵌入是一个相对较小的密集向量；例如，只有 300
    维。    ###### 小贴士    嵌入大小是一个可以调整的超参数。一般来说，一个好的嵌入大小通常接近类别数量的平方根。    在深度学习中，嵌入通常随机初始化，然后与模型的其他参数一起通过梯度下降进行训练。例如，如果我们想在加利福尼亚住房数据集（见[第
    2 章](ch02.html#project_chapter)）上训练神经网络，我们可以使用嵌入来表示 `ocean_proximity` 分类特征。初始时，“NEAR
    BAY”类别可以表示为一个随机向量，例如 `[0.831, 0.696]`，而“NEAR OCEAN”类别可能表示为另一个随机向量，例如 `[0.127,
    0.868]`（在这个例子中我们使用 2D 嵌入）。    由于这些嵌入是可训练的，它们将在训练过程中逐渐改进；并且由于在这个例子中它们代表相当相似的类型，梯度下降将最终将它们推向彼此，同时它倾向于将它们从“INLAND”类别的嵌入中移开（见[图
    14-2](#embedding_diagram)）。确实，表示越好，神经网络做出准确预测就越容易，因此训练往往使嵌入成为类别的有用表示。这被称为 *表示学习*（你将在[第
    18 章](ch18.html#autoencoders_chapter)中看到其他类型的表示学习）。  ![说明训练过程中嵌入改进的图，显示“近海”、“近湾”和“内陆”等类别在嵌入空间中的位置。](assets/hmls_1402.png)  ######
    图 14-2\. 嵌入将在训练过程中逐渐改进    不仅嵌入通常是有用的任务表示，而且这些相同的嵌入通常可以成功地用于其他任务。最常见的例子是 *词嵌入*（即，单个单词的嵌入）：当你从事自然语言处理任务时，通常最好重用预训练的词嵌入，而不是自己训练，正如我们将在本章后面看到的那样。    使用向量来表示单词的想法可以追溯到
    20 世纪 60 年代，已经使用了许多复杂的技术来生成有用的向量，包括使用神经网络。但直到 2013 年，当 Tomáš Mikolov 和其他谷歌研究人员发表了一篇
    [论文](https://homl.info/word2vec)⁠^([3](ch14.html#'
