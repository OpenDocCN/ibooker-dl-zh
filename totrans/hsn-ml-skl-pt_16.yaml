- en: Chapter 14\. Natural Language Processing with RNNs and Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 自然语言处理与RNN和注意力
- en: 'When Alan Turing imagined his famous [Turing test](https://homl.info/turingtest)⁠^([1](ch14.html#id3189))
    in 1950, he proposed a way to evaluate a machine’s ability to match human intelligence.
    He could have tested for many things, such as the ability to recognize cats in
    pictures, play chess, compose music, or escape a maze, but, interestingly, he
    chose a linguistic task. More specifically, he devised a *chatbot* capable of
    fooling its interlocutor into thinking it was human.⁠^([2](ch14.html#id3191))
    This test does have its weaknesses: a set of hardcoded rules can fool unsuspecting
    or naive humans (e.g., the machine could give vague predefined answers in response
    to some keywords, it could pretend that it is joking or drunk to get a pass on
    its weirdest answers, or it could escape difficult questions by answering them
    with its own questions), and many aspects of human intelligence are utterly ignored
    (e.g., the ability to interpret nonverbal communication such as facial expressions,
    or to learn a manual task). But the test does highlight the fact that mastering
    language is arguably *Homo sapiens*’s greatest cognitive ability.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当艾伦·图灵在1950年想象他著名的[Turing测试](https://homl.info/turingtest)⁠^([1](ch14.html#id3189))时，他提出了一种评估机器匹配人类智能能力的方法。他本可以测试许多事情，比如识别图片中的猫、下棋、作曲或逃离迷宫的能力，但有趣的是，他选择了一个语言任务。更具体地说，他设计了一个能够欺骗其对话者认为它是人类的*聊天机器人*。⁠^([2](ch14.html#id3191))
    这个测试确实有其弱点：一组硬编码的规则可以欺骗没有防备或天真的人类（例如，机器可以对某些关键词给出模糊的预定义答案，它可以假装在开玩笑或喝醉来通过其最奇怪的答案，或者它可以用自己的问题来逃避困难的问题），并且忽略了人类智能的许多方面（例如，解释非言语交流的能力，如面部表情，或者学习手动任务的能力）。但这个测试确实突出了掌握语言可能是*智人*最大的认知能力。
- en: Until recently, state-of-the-art natural language processing (NLP) models were
    pretty much all based on recurrent neural networks (introduced in [Chapter 13](ch13.html#rnn_chapter)).
    However, in recent years, RNNs have been replaced with transformers, which we
    will explore in [Chapter 15](ch15.html#transformer_chapter). That said, it’s still
    important to learn how RNNs can be used for NLP tasks, if only because it helps
    better understand transformers. Moreover, most of the techniques we will discuss
    in this chapter are also useful with Transformer architectures (e.g., tokenization,
    beam search, attention mechanisms, and more). Plus, RNNs have recently made a
    surprise comeback in the form of state space models (SSMs) (see “State-Space Models
    (SSMs)” at [*https://homl.info*](https://homl.info)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，最先进的自然语言处理（NLP）模型基本上都是基于循环神经网络（在第13章中介绍），即RNN。然而，近年来，RNN已被transformers所取代，我们将在第15章中探讨这一点。尽管如此，了解RNN如何用于NLP任务仍然很重要，这不仅因为这样有助于更好地理解transformers。此外，本章中我们将讨论的大部分技术也适用于Transformer架构（例如，分词、束搜索、注意力机制等）。此外，RNN最近以状态空间模型（SSMs）的形式意外回归（参见“状态空间模型（SSMs）”[*https://homl.info*](https://homl.info)）。
- en: This chapter is organized in three sections. In the first section, we will start
    by building a *character RNN*, or *char-RNN*, trained to predict the next character
    in a sentence. On the way, we will learn about trainable embeddings. Our char-RNN
    will be our first tiny *language model*, capable of generating original text.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为三个部分。在第一部分，我们将从构建一个*字符RNN*，或*char-RNN*开始，该RNN被训练来预测句子中的下一个字符。在这个过程中，我们将了解可训练嵌入。我们的char-RNN将成为我们的第一个微型的*语言模型*，能够生成原始文本。
- en: 'In the second section, we will turn to text classification, and more specifically
    sentiment analysis, which aims to predict how positive or negative some text is.
    Our model will read movie reviews and estimate the rater’s feeling about the movie.
    This time, instead of splitting the text into individual characters, we will split
    it into *tokens*: a token is a small piece of text from a fixed-sized vocabulary,
    such as the top 10,000 most common words in the English language, or the most
    common subwords (e.g., “smartest” = “smart” + “est”), or even individual characters
    or bytes. To split the text into tokens, we will use a *tokenizer*. This section
    will also introduce popular Hugging Face libraries: the *Datasets* library to
    download datasets, the *Tokenizers* library for tokenizers, and the *Transformers*
    library for popular models, downloaded automatically from the *Hugging Face Hub*.
    Hugging Face is a hugely influential company and open source community, and it
    plays a central role in the open source AI space, especially in NLP.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将转向文本分类，更具体地说，是情感分析，其目的是预测某些文本是积极还是消极。我们的模型将阅读电影评论并估计评论者对电影的感受。这次，我们不会将文本拆分为单个字符，而是将其拆分为*tokens*：一个token是从固定大小的词汇中提取的小块文本，例如英语中最常见的10,000个单词，或者最常见的子词（例如，“smartest”
    = “smart” + “est”），甚至单个字符或字节。为了将文本拆分为tokens，我们将使用*tokensizer*。本节还将介绍流行的Hugging
    Face库：*Datasets*库用于下载数据集，*Tokenizers*库用于tokenizers，以及*Transformers*库用于流行的模型，这些模型将自动从*Hugging
    Face Hub*下载。Hugging Face是一家极具影响力的公司，也是开源社区，它在开源AI领域，尤其是在NLP领域扮演着核心角色。
- en: 'The final boss of this chapter will be neural machine translation (NMT), the
    topic of the third and last section: we will build an encoder-decoder model capable
    of translating English to Spanish. This will lead us to *attention mechanisms*,
    which we will apply to our encoder-decoder model to improve its capacity to handle
    long input texts. As their name suggests, attention mechanisms are neural network
    components that learn to select the part of the inputs that the model should focus
    on at each time step. They directly led to the transformers revolution, as we
    will see in the next chapter.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最终目标将是神经机器翻译（NMT），这是第三部分和最后一部分的主题：我们将构建一个能够将英语翻译成西班牙语的编码器-解码器模型。这将引导我们到*注意力机制*，我们将将其应用于编码器-解码器模型，以提高其处理长输入文本的能力。正如其名所示，注意力机制是神经网络组件，它们学会在每个时间步选择模型应该关注的输入部分。它们直接导致了下一章中我们将看到的transformers革命。
- en: Let’s start with a simple and fun char-RNN model that can write like Shakespeare
    (sort of).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常简单且有趣的char-RNN模型开始，它可以模仿莎士比亚的风格写作（或者说类似）。
- en: Generating Shakespearean Text Using a Character RNN
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用字符RNN生成莎士比亚风格的文本
- en: 'In a famous [2015 blog post](https://homl.info/charrnn) titled “The Unreasonable
    Effectiveness of Recurrent Neural Networks”, Andrej Karpathy showed how to train
    an RNN to predict the next character in a sentence. This *char-RNN* can then be
    used to generate novel text, one character at a time. Here is a small sample of
    the text generated by a char-RNN model after it was trained on all of Shakespeare’s
    works:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇著名的[2015博客文章](https://homl.info/charrnn)《循环神经网络的不合理有效性》中，安德烈·卡帕西展示了如何训练一个RNN来预测句子中的下一个字符。这个*char-RNN*可以用来逐个字符生成新的文本。以下是char-RNN模型在训练了莎士比亚的所有作品后生成的一段文本的样本：
- en: 'PANDARUS:'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 潘达罗斯：
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alas, I think he shall be come approached and the day
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 唉，我想他将被接近，那一天
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When little srain would be attain’d into being never fed,
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当微小的努力得以实现却从未得到滋养，
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And who is but a chain and subjects of his death,
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谁又能逃脱死亡的锁链，
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I should not sleep.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不应该睡觉。
- en: Not exactly a masterpiece, but it is still impressive that the model was able
    to learn words, grammar, proper punctuation, and more, just by learning to predict
    the next character in a sentence. This is our first example of a *language model*.
    In the remainder of this section we’ll build a char-RNN step by step, starting
    with the creation of the dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是一部杰作，但令人印象深刻的是，模型仅通过学习预测句子中的下一个字符就能学会单词、语法、正确的标点符号等等。这是我们第一个*语言模型*的例子。在本节的剩余部分，我们将逐步构建char-RNN，从创建数据集开始。
- en: Creating the Training Dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练数据集
- en: 'First, let’s download a subset of Shakespeare’s works (about 25%). The data
    is loaded from Andrej Karpathy’s [char-rnn project](https://github.com/karpathy/char-rnn):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们下载莎士比亚作品的子集（大约25%）。数据是从安德烈·卡帕西的[char-rnn项目](https://github.com/karpathy/char-rnn)加载的：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s print the first few lines:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出前几行：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looks like Shakespeare, all right!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来像是莎士比亚的作品，没错！
- en: 'Neural networks work with numbers, not text, so we need a way to encode text
    into numbers. In general, this is done by splitting the text into *tokens*, such
    as words or characters, and assigning an integer ID to each possible token. For
    example, let’s split our text into characters, and assign an ID to each possible
    character. We first need to find the list of characters used in the text. This
    will constitute our token *vocabulary*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络处理的是数字而不是文本，因此我们需要一种方法将文本编码成数字。通常，这是通过将文本分割成*标记*，如单词或字符，并为每个可能的标记分配一个整数ID来完成的。例如，让我们将我们的文本分割成字符，并为每个可能的字符分配一个ID。我们首先需要找到文本中使用的字符列表。这将构成我们的标记*词汇表*：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that we call `lower()` to ignore case and thereby reduce the vocabulary
    size. We must now assign a token ID to each character. For this, we can just use
    its index in the vocabulary. To decode the output of our model, we will also need
    a way to go from a token ID to a character:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们调用`lower()`来忽略大小写，从而减少词汇表的大小。我们必须现在为每个字符分配一个标记ID。为此，我们可以简单地使用它在词汇表中的索引。为了解码我们模型的输出，我们还需要一种方法从标记ID到字符的转换：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, let’s create two helper functions to encode text to tensors of token
    IDs, and to decode them back to text:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建两个辅助函数，一个用于将文本编码为标记ID的张量，另一个用于将它们解码回文本：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s try them out:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, let’s prepare the dataset. Right now, we have a single, extremely long
    sequence of characters containing Shakespeare’s works. Just like we did in [Chapter 13](ch13.html#rnn_chapter),
    we can turn this long sequence into a dataset of windows that we can then use
    to train a sequence-to-sequence RNN. The targets will be similar to the inputs,
    but shifted by one time step into the “future”. For example, one sample in the
    dataset may be a sequence of character IDs representing the text “to be or not
    to b” (without the final “e”), and the corresponding target—a sequence of character
    IDs representing the text “o be or not to be” (with the final “e”, but without
    the leading “t”). Let’s create our dataset class:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们准备数据集。目前，我们有一个包含莎士比亚作品的极长字符序列。就像我们在第13章中做的那样[Chapter 13](ch13.html#rnn_chapter)，我们可以将这个长序列转换成一系列窗口，然后我们可以使用这些窗口来训练一个序列到序列的RNN。目标将与输入相似，但会向前移动一个时间步。例如，数据集中一个样本可能是一个字符ID序列，代表文本“to
    be or not to b”（没有最后的“e”），相应的目标是一个字符ID序列，代表文本“o be or not to be”（带有最后的“e”，但没有开头的“t”）。让我们创建我们的数据集类：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And now let’s create the data loaders, as usual. Since the text is quite large,
    we can afford to use roughly 90% for training (i.e., one million characters),
    and just 5% for validation, and 5% for testing (60,000 characters each):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们像往常一样创建数据加载器。由于文本相当大，我们可以使用大约90%用于训练（即一百万个字符），5%用于验证，5%用于测试（每个60,000个字符）：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Each batch will be composed of 512 50-character windows, where each character
    is represented by its token ID, and where each window comes with its 50-character
    target window (offset by one character). Note that the training batches are shuffled
    at each epoch (see [Figure 14-1](#window_dataset_diagram)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每个批次将由512个50个字符的窗口组成，其中每个字符由其标记ID表示，每个窗口都附带一个50个字符的目标窗口（偏移一个字符）。请注意，在每个epoch时，训练批次都会被随机打乱（见[图14-1](#window_dataset_diagram)）。
- en: '![Diagram illustrating a batch of input and target windows with a window length
    of 10, showing the offset relationship between inputs and targets.](assets/hmls_1401.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![说明具有10个窗口长度的输入和目标窗口的批次图，显示输入和目标之间的偏移关系。](assets/hmls_1401.png)'
- en: Figure 14-1\. Each training batch is composed of shuffled windows, along with
    their shifted targets. In this figure, the window length is 10 instead of 50.
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1。每个训练批次由随机打乱的窗口及其偏移的目标组成。在这个图中，窗口长度是10而不是50。
- en: Tip
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: We set the window length to 50, but you can try tuning it. It’s easier and faster
    to train RNNs on shorter input sequences, but the RNN will not be able to learn
    any pattern longer than the window length, so don’t make it too small.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将窗口长度设置为50，但你可以尝试调整它。在较短的输入序列上训练RNN更容易更快，但RNN将无法学习比窗口长度更长的任何模式，所以不要让它太小。
- en: While we could technically feed the token IDs directly to a neural network without
    any further preprocessing, it wouldn’t work very well. Indeed, as we saw in [Chapter 2](ch02.html#project_chapter),
    most ML models—including neural networks—assume that similar inputs represent
    similar things; unfortunately, similar IDs may represent totally unrelated tokens,
    and conversely, distant IDs may represent similar tokens. The neural net would
    be biased in a weird way, and it would have great difficulty overcoming this bias
    during training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然技术上我们可以直接将标记ID输入到神经网络中而不进行任何进一步的前处理，但这不会很好用。确实，正如我们在[第2章](ch02.html#project_chapter)中看到的，大多数机器学习模型——包括神经网络——都假设相似的输入代表相似的事物；不幸的是，相似的ID可能代表完全无关的标记，反之亦然，遥远的ID可能代表相似的标记。神经网络会以一种奇怪的方式产生偏差，并且在训练中很难克服这种偏差。
- en: 'One solution is to use one-hot encoding, since all one-hot vectors are equally
    distant from one another. However, when the vocabulary is large, one-hot vectors
    are equally large. In our case, the vocabulary contains just 39 characters, so
    each character would be represented by a 39-dimensional one-hot vector. That’s
    still manageable, but if we were dealing with words instead of characters, the
    vocabulary size could be in the tens of thousands, so one-hot encoding would be
    out of the question. Luckily, since we are dealing with neural networks, we have
    a better option: embeddings.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是使用独热编码，因为所有独热向量彼此之间距离相等。然而，当词汇量很大时，独热向量也很大。在我们的例子中，词汇量只包含39个字符，所以每个字符都由一个39维的独热向量表示。这仍然是可以管理的，但如果我们处理的是单词而不是字符，词汇量可能达到数千，那么独热编码就不可行了。幸运的是，由于我们处理的是神经网络，我们有一个更好的选择：嵌入。
- en: Embeddings
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: An embedding is a dense representation of some higher-dimensional data, typically
    a categorical feature. If there are 50,000 possible categories, then one-hot encoding
    produces a 50,000-dimensional sparse vector (i.e., containing mostly zeros). In
    contrast, an embedding is a comparatively small dense vector; for example, with
    just 300 dimensions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一种对某些高维数据的密集表示，通常是一个分类特征。如果有50,000个可能的类别，那么独热编码会产生一个50,000维的稀疏向量（即大部分是零）。相比之下，嵌入是一个相对较小的密集向量；例如，只有300个维度。
- en: Tip
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The embedding size is a hyperparameter you can tune. As a rule of thumb, a good
    embedding size is often close to the square root of the number of categories.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入大小是一个你可以调整的超参数。作为一个经验法则，一个好的嵌入大小通常接近类别数量的平方根。
- en: In deep learning, embeddings are usually initialized randomly, and they are
    then trained by gradient descent, along with the other model parameters. For example,
    if we wanted to train a neural network on the California housing dataset (see
    [Chapter 2](ch02.html#project_chapter)), we could represent the `ocean_proximity`
    categorical feature using embeddings. The `"NEAR BAY"` category could be represented
    initially by a random vector such as `[0.831, 0.696]`, while the `"NEAR OCEAN"`
    category might be represented by another random vector such as `[0.127, 0.868]`
    (in this example we are using 2D embeddings).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，嵌入通常随机初始化，然后与模型的其他参数一起通过梯度下降进行训练。例如，如果我们想在加利福尼亚住房数据集上训练一个神经网络（见[第2章](ch02.html#project_chapter)），我们可以使用嵌入来表示`ocean_proximity`分类特征。`"NEAR
    BAY"`类别最初可能由一个随机向量，如`[0.831, 0.696]`表示，而`"NEAR OCEAN"`类别可能由另一个随机向量，如`[0.127, 0.868]`表示（在这个例子中我们使用2D嵌入）。
- en: Since these embeddings are trainable, they will gradually improve during training;
    and as they represent fairly similar categories in this example, gradient descent
    will certainly end up pushing them closer together, while it will tend to move
    them away from the `"INLAND"` category’s embedding (see [Figure 14-2](#embedding_diagram)).
    Indeed, the better the representation, the easier it will be for the neural network
    to make accurate predictions, so training tends to make embeddings useful representations
    of the categories. This is called *representation learning* (you will see other
    types of representation learning in [Chapter 18](ch18.html#autoencoders_chapter)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些嵌入是可训练的，它们在训练过程中会逐渐改进；并且在这个例子中，它们代表相当相似的分类，因此梯度下降肯定会将它们推向彼此更近的位置，同时它倾向于将它们从“内陆”类别的嵌入（见[图14-2](#embedding_diagram)）移开。确实，表示越好，神经网络做出准确预测就越容易，因此训练往往使嵌入成为类别有用的表示。这被称为*表示学习*（你将在[第18章](ch18.html#autoencoders_chapter)中看到其他类型的表示学习）。
- en: '![Diagram illustrating the improvement of embeddings during training, showing
    categories such as "Near ocean," "Near bay," and "Inland" in an embedding space.](assets/hmls_1402.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![说明在训练过程中嵌入的改进，显示“靠近海洋”、“靠近海湾”和“内陆”等类别在嵌入空间中的情况。](assets/hmls_1402.png)'
- en: Figure 14-2\. Embeddings will gradually improve during training
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-2\. 嵌入将在训练过程中逐渐改进
- en: 'Not only will embeddings generally be useful representations for the task at
    hand, but quite often these same embeddings can be reused successfully for other
    tasks. The most common example of this is *word embeddings* (i.e., embeddings
    of individual words): when you are working on a natural language processing task,
    you are often better off reusing pretrained word embeddings than training your
    own, as we will see later in this chapter.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅嵌入通常是对手头任务的有用表示，而且这些相同的嵌入经常可以成功地用于其他任务。最常见的例子是*词嵌入*（即单个单词的嵌入）：当你从事自然语言处理任务时，你通常最好重用预训练的词嵌入，而不是自己训练，正如我们将在本章后面看到的那样。
- en: 'The idea of using vectors to represent words dates back to the 1960s, and many
    sophisticated techniques have been used to generate useful vectors, including
    using neural networks. But things really took off in 2013, when Tomáš Mikolov
    and other Google researchers published a [paper](https://homl.info/word2vec)⁠^([3](ch14.html#id3210))
    describing an efficient technique to learn word embeddings using neural networks,
    significantly outperforming previous attempts. This allowed them to learn embeddings
    on a very large corpus of text: they trained a neural network to predict the words
    near any given word and obtained astounding word embeddings. For example, synonyms
    had very close embeddings, and semantically related words such as *France*, *Spain*,
    and *Italy* were clustered together.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量来表示单词的想法可以追溯到20世纪60年代，许多复杂的技巧已经被用来生成有用的向量，包括使用神经网络。但真正的发展是在2013年，当时Tomáš
    Mikolov和其他谷歌研究人员发表了一篇[论文](https://homl.info/word2vec)⁠^([3](ch14.html#id3210))，描述了一种使用神经网络高效学习词嵌入的有效技术，显著优于之前的尝试。这使得他们能够在非常大的文本语料库上学习嵌入：他们训练了一个神经网络来预测任何给定单词附近的单词，并获得了惊人的词嵌入。例如，同义词有非常接近的嵌入，而像*法国*、*西班牙*和*意大利*这样的语义相关单词则聚集在一起。
- en: 'It’s not just about proximity, though: word embeddings are also organized along
    meaningful axes in the embedding space. Here is a famous example: if you compute
    *King – Man + Woman* (adding and subtracting the embedding vectors of these words),
    then the result will be very close to the embedding of the word *Queen* (see [Figure 14-3](#word_embedding_diagram)).
    In other words, the word embeddings encode the concept of gender! Similarly, you
    can compute *Madrid – Spain + France*, and the result is close to *Paris*, which
    seems to show that the notion of capital city is also encoded in the embeddings.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不仅仅是关于邻近性：词嵌入在嵌入空间中也是沿着有意义的轴组织起来的。这里有一个著名的例子：如果你计算*国王 – 男人 + 女人*（添加和减去这些单词的嵌入向量），那么结果将非常接近单词*王后*的嵌入（参见[图14-3](#word_embedding_diagram)）。换句话说，词嵌入编码了性别概念！同样，你可以计算*马德里
    – 西班牙 + 法国*，结果接近*巴黎*，这似乎表明首都的概念也被编码在嵌入中。
- en: '![Diagram illustrating how word embeddings calculate "King - Man + Woman" to
    approximate the position of "Queen," demonstrating the encoding of the gender
    concept in the embedding space.](assets/hmls_1403.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![说明词嵌入如何计算“国王 - 男人 + 女人”以近似“王后”的位置，从而演示在嵌入空间中编码性别概念。](assets/hmls_1403.png)'
- en: Figure 14-3\. Word embeddings of similar words tend to be close, and some axes
    seem to encode meaningful concepts
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-3\. 相似单词的词嵌入往往很接近，某些轴似乎编码了有意义的概念
- en: Warning
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Word embeddings can have some meaningful structure, as the “King – Man + Woman”
    shows. However, they are also noisy and often hard to interpret. I’ve added some
    code at the end of the notebook so you can judge for yourself.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入可以有一些有意义的结构，正如“国王 – 男人 + 女人”所示。然而，它们也是嘈杂的，通常很难解释。我在笔记本的末尾添加了一些代码，以便您可以自己判断。
- en: 'Unfortunately, word embeddings sometimes capture our worst biases. For example,
    although they correctly learn that *Man is to King as Woman is to Queen*, they
    also seem to learn that *Man is to Doctor as Woman is to Nurse*: quite a sexist
    bias! To be fair, this particular example is probably exaggerated, as was pointed
    out in a [2019 paper](https://homl.info/fairembeds)⁠^([4](ch14.html#id3211)) by
    Malvina Nissim et al. Nevertheless, ensuring fairness in deep learning algorithms
    is an important and active research topic.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，词嵌入有时会捕捉到我们最糟糕的偏见。例如，尽管它们正确地学习到*男人是国王，女人是王后*，但它们似乎也学习到*男人是医生，女人是护士*：这是一种相当严重的性别偏见！为了公平起见，这个特定的例子可能被夸大了，正如Malvina
    Nissim等人在2019年的一篇论文[2019 paper](https://homl.info/fairembeds)⁠^([4](ch14.html#id3211))中指出的。尽管如此，确保深度学习算法的公平性是一个重要且活跃的研究课题。
- en: 'PyTorch provides an `nn.Embedding` module, which wraps an *embedding matrix*:
    this matrix has one row per possible category (e.g., one row for each token in
    the vocabulary) and one column per embedding dimension. The embedding dimensionality
    is a hyperparameter you can tune. By default, the embedding matrix is initialized
    randomly.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一个`nn.Embedding`模块，它封装了一个*嵌入矩阵*：这个矩阵有每一可能的类别（例如，词汇表中的每个标记一行）和每个嵌入维度一列。嵌入维度性是一个你可以调整的超参数。默认情况下，嵌入矩阵是随机初始化的。
- en: 'To convert a category ID to an embedding, the `nn.Embedding` layer just looks
    up and returns the corresponding row. That’s all there is to it! For example,
    let’s initialize an `nn.Embedding` layer with five categories and 3D embeddings,
    and use it to encode some categories:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要将类别ID转换为嵌入，`nn.Embedding`层只需查找并返回相应的行。就是这样！例如，让我们初始化一个具有五个类别和3D嵌入的`nn.Embedding`层，并使用它来编码一些类别：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, category 3 gets encoded as the 3D vector `[0.2674, 0.5349, 0.8094]`,
    category 2 gets encoded (twice) as the 3D vector `[2.2082, -0.6380, 0.4617]`,
    and category 0 gets encoded as the 3D vector `[0.3367, 0.1288, 0.2345]` (categories
    1 and 4 were not used in this example). Since the layer is not trained yet, these
    encodings are just random.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，类别3被编码为3D向量`[0.2674, 0.5349, 0.8094]`，类别2被编码（两次）为3D向量`[2.2082, -0.6380,
    0.4617]`，而类别0被编码为3D向量`[0.3367, 0.1288, 0.2345]`（在这个例子中没有使用类别1和4）。由于该层尚未经过训练，这些编码只是随机的。
- en: 'Note that an embedding layer is mathematically equivalent to one-hot encoding
    followed by a linear layer (with no bias parameter). For example, if you create
    a linear layer with `nn.Linear(5, 3, bias=False)` and pass it the one-hot vector
    `torch.tensor([[0., 0., 0., 1., 0.]])`, you get a vector equal to row #3 of the
    linear layer’s transposed weight matrix (which acts as an embedding matrix). That’s
    because all rows in the transposed weight matrix get multiplied by zero, except
    for row #3 which gets multiplied by 1, so the result is just row #3\. However,
    it’s much more efficient to use `nn.Embedding(5, 3)` and pass it `torch.tensor([3])`:
    this looks up row #3 in the embedding matrix without the need for one-hot encoding,
    and without all the pointless multiplications by zero.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，嵌入层在数学上等同于one-hot编码后跟一个线性层（没有偏置参数）。例如，如果你创建一个带有`nn.Linear(5, 3, bias=False)`的线性层，并传递one-hot向量`torch.tensor([[0.,
    0., 0., 1., 0.]])`，你得到一个等于线性层转置权重矩阵的第3行的向量（该矩阵充当嵌入矩阵）。这是因为转置权重矩阵的所有行都乘以零，除了第3行乘以1，所以结果是第3行。然而，使用`nn.Embedding(5,
    3)`并传递`torch.tensor([3])`会更高效：这不需要one-hot编码，也不需要所有无意义的零乘法。
- en: OK, now that you know about embeddings, you are ready to build the Shakespeare
    model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在你已经了解了嵌入，你就可以开始构建莎士比亚模型了。
- en: Building and Training the Char-RNN Model
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和训练Char-RNN模型
- en: 'Since our dataset is reasonably large, and modeling language is quite a difficult
    task, we need more than a simple RNN with a few recurrent neurons. Let’s build
    and train a model with a two-layer `nn.GRU` module (introduced in [Chapter 13](ch13.html#rnn_chapter)),
    with 128 units per layer, and a bit of dropout. You can try tweaking the number
    of layers and units later, if needed:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集相当大，并且建模语言是一项相当困难的任务，我们需要比简单的具有几个循环神经元的RNN更复杂的模型。让我们构建并训练一个具有两层`nn.GRU`模块（在第13章[Chapter 13](ch13.html#rnn_chapter)中介绍）的模型，每层有128个单元，并加入一些dropout。如果需要，你可以稍后尝试调整层数和单元数：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s go over this code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这段代码：
- en: We use an `nn.Embedding` layer as the first layer, to encode the character IDs.
    As we just saw, the `nn.Embedding` layer’s number of input dimensions is the number
    of categories, so in our case it’s the number of distinct character IDs. The embedding
    size is a hyperparameter you can tune—we’ll set it to 10 for now. Whereas the
    inputs of the `nn.Embedding` layer will be integer tensors of shape [*batch size*,
    *window length*], the outputs of the `nn.Embedding` layer will be float tensors
    of shape [*batch size*, *window length*, *embedding size*].
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个`nn.Embedding`层作为第一层，来编码字符ID。正如我们刚才看到的，`nn.Embedding`层的输入维度数是类别数，所以在我们这个例子中是不同字符ID的数量。嵌入大小是一个可以调整的超参数，我们暂时将其设置为10。而`nn.Embedding`层的输入将是形状为[*批次大小*，*窗口长度*]的整数张量，而`nn.Embedding`层的输出将是形状为[*批次大小*，*窗口长度*，*嵌入大小*]的浮点张量。
- en: The `nn.GRU` layer has 10 inputs (i.e., the embedding size), 128 outputs (i.e.,
    the hidden size), two layers, and as usual we must specify `batch_first=True`
    because otherwise the layer assumes that the batch dimension comes after the time
    dimension.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.GRU`层有10个输入（即嵌入大小），128个输出（即隐藏大小），两层，并且通常我们必须指定`batch_first=True`，因为否则该层假设批次维度在时间维度之后。'
- en: 'We use an `nn.Linear` layer for the output layer: it must have 39 units because
    there are 39 distinct characters in the text, and we want to output a logit for
    each possible character (at each time step).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`nn.Linear`层作为输出层：它必须有39个单元，因为文本中有39个不同的字符，我们希望为每个可能的字符（在每个时间步）输出一个logit。
- en: In the `forward()` method, we just call these layers one by one. Note that the
    `nn.GRU` layer’s output shape is [*batch size*, *window length*, *hidden size*],
    and the `nn.Linear` layer’s output shape is [*batch size*, *window length*, *vocabulary
    size*], but as we saw in [Chapter 13](ch13.html#rnn_chapter), the `nn.CrossEntropyLoss`
    and `Accuracy` modules that we will use for training both expect the class dimension
    (i.e., `vocab_size`) to be the second dimension, not the last one. This is why
    we must permute the last two dimensions of the `nn.Linear` layer’s output. Note
    that the `nn.GRU` layer also returns the final hidden states, but we ignore them.⁠^([5](ch14.html#id3218))
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`forward()`方法中，我们只是依次调用这些层。请注意，`nn.GRU`层的输出形状是[*批次大小*，*窗口长度*，*隐藏大小*]，而`nn.Linear`层的输出形状是[*批次大小*，*窗口长度*，*词汇表大小*]，但正如我们在[第13章](ch13.html#rnn_chapter)中看到的，我们将用于训练的`nn.CrossEntropyLoss`和`Accuracy`模块都期望类维度（即`vocab_size`）是第二个维度，而不是最后一个维度。这就是为什么我们必须对`nn.Linear`层的输出最后两个维度进行置换。请注意，`nn.GRU`层还返回最终的隐藏状态，但我们忽略了它们。⁠^([5](ch14.html#id3218))
- en: Now you can now train and evaluate the model as usual, using the `nn.CrossEntropyLoss`
    and the `Accuracy` metric.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以像往常一样训练和评估模型，使用`nn.CrossEntropyLoss`和`Accuracy`指标。
- en: 'And now let’s use our model to predict the next character in a sentence:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用我们的模型来预测句子中的下一个字符：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We first encode the text, add a batch dimension of size 1, and move the tensor
    to the GPU. Then we call our model and get logits for each time step. We’re only
    interested in logits for the final time step (hence the –1), and we want to know
    which token ID has the highest logit, so we use `argmax()`. We then use `item()`
    to extract the token ID from the tensor. Lastly, we convert the token ID to a
    character, and that’s our prediction.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对文本进行编码，添加一个大小为1的批次维度，并将张量移动到GPU上。然后我们调用我们的模型，并为每个时间步获取logits。我们只对最后一个时间步的logits感兴趣（因此是-1），我们想知道哪个token
    ID的logit最高，所以我们使用`argmax()`。然后我们使用`item()`从张量中提取token ID。最后，我们将token ID转换为字符，这就是我们的预测。
- en: The model correctly predicts “e”, great! Now let’s use this model to pretend
    we’re Shakespeare!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型正确预测了“e”，太好了！现在让我们用这个模型假装我们是莎士比亚！
- en: Warning
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you are running this code on Colab with a GPU activated, then training will
    take a few hours. You can reduce the number of epochs if you don’t want to wait
    that long, but of course the model’s accuracy will probably be lower. If the Colab
    session times out, make sure to reconnect quickly, or else the Colab runtime will
    be destroyed.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个激活了GPU的Colab上运行此代码，那么训练可能需要几个小时。如果你不想等那么久，你可以减少epoch的数量，但当然，模型的准确率可能会更低。如果Colab会话超时，请确保快速重新连接，否则Colab运行时将被销毁。
- en: Generating Fake Shakespearean Text
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成假莎士比亚文本
- en: 'To generate new text using the char-RNN model, we could feed it some text,
    make the model predict the most likely next letter, add it to the end of the text,
    then give the extended text to the model to guess the next letter, and so on.
    This is called *greedy decoding*. But in practice this often leads to the same
    words being repeated over and over again. Instead, we can sample the next character
    randomly, using the model’s estimated probability distribution: if the model estimates
    a probability *p* for a given token, then this token will be sampled with probability
    *p*. This process will generate more diverse and interesting text since the most
    likely token won’t always be sampled. To sample the next token, we can use the
    `torch.multinomial()` function, which samples random class indices, given a list
    of class probabilities. For example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 char-RNN 模型生成新的文本，我们可以给它一些文本，让模型预测最可能的下一个字母，将其添加到文本的末尾，然后将扩展后的文本给模型以猜测下一个字母，依此类推。这被称为
    *贪婪解码*。但在实践中，这通常会导致相同的单词反复出现。相反，我们可以随机采样下一个字符，使用模型估计的概率分布：如果模型估计一个给定标记的概率为 *p*，那么这个标记将以概率
    *p* 被采样。这个过程将生成更多样化和有趣的文本，因为最可能的标记不总是会采样。为了采样下一个标记，我们可以使用 `torch.multinomial()`
    函数，该函数根据给定的类概率列表采样随机类索引。例如：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To have more control over the diversity of the generated text, we can divide
    the logits by a number called the *temperature*, which we can tweak as we wish.
    A temperature close to zero favors high-probability characters, while a high temperature
    gives all characters an equal probability. Lower temperatures are typically preferred
    when generating fairly rigid and precise text, such as mathematical equations,
    while higher temperatures are preferred when generating more diverse and creative
    text. Let’s write a `next_char()` helper function that will use this approach
    to pick the next character to add to the input text:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地控制生成文本的多样性，我们可以将 logits 除以一个称为 *温度* 的数字，我们可以随意调整它。接近零的温度有利于高概率字符，而高温度则使所有字符具有相等的概率。在生成较为严格和精确的文本时，如数学方程式，通常更喜欢较低的温度，而在生成更多样化和富有创意的文本时，则更喜欢较高的温度。让我们编写一个
    `next_char()` 辅助函数，该函数将使用这种方法来选择要添加到输入文本中的下一个字符：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we can write another small helper function that will repeatedly call
    `next_​char()` to get the next character and append it to the given text:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以编写另一个小的辅助函数，该函数将反复调用 `next_​char()` 来获取下一个字符并将其附加到给定的文本中：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are now ready to generate some text! Let’s try low, medium, and high temperatures:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以生成一些文本了！让我们尝试低、中、高温度：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Notice the repetitions when the temperature is low: “the state and the” appears
    twice. The intermediate temperature led to more convincing results, although Romeo
    wasn’t very talkative today. But in the last example the temperature was way too
    high—we fried Shakespeare. To generate more convincing text, a common technique
    is to sample only from the top *k* characters, or only from the smallest set of
    top characters whose total probability exceeds some threshold: this is called
    *top-p sampling*, or *nucleus sampling*. Alternatively, you could try using *beam
    search*, which we will discuss later in this chapter, or using more `nn.GRU` layers
    and more neurons per layer, training for longer, and adding more regularization
    if needed.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当温度较低时，请注意重复： “the state and the” 出现了两次。中等温度导致了更令人信服的结果，尽管罗密欧今天并不太健谈。但在最后一个例子中，温度过高——我们烧毁了莎士比亚。为了生成更令人信服的文本，一种常见的技术是只从前
    *k* 个字符中采样，或者只从总概率超过某个阈值的最小的一组前 *k* 个字符中采样：这被称为 *top-p 采样* 或 *核采样*。或者，你也可以尝试使用
    *beam search*，我们将在本章后面讨论，或者使用更多的 `nn.GRU` 层和每层的更多神经元，如果需要的话，进行更长时间的训练，并添加更多的正则化。
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The model is currently incapable of learning patterns longer than `window_length`,
    which is just 50 characters. You could try making this window larger, but it would
    also make training harder, and even LSTM and GRU cells cannot handle very long
    sequences.⁠^([6](ch14.html#id3232))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当前模型无法学习超过 `window_length` 的模式，而 `window_length` 只有 50 个字符。你可以尝试将这个窗口做得更大，但这也会使训练更加困难，甚至
    LSTM 和 GRU 单元也无法处理非常长的序列。⁠^([6](ch14.html#id3232))
- en: Interestingly, although a char-RNN model is just trained to predict the next
    character, this seemingly simple task actually requires it to learn some higher-level
    tasks as well. For example, to find the next character after “Great movie, I really
    “, it’s helpful to understand that the sentence is positive, so what follows is
    more likely to be the letter “l” (for “loved”) rather than “h” (for “hated”).
    In fact, a [2017 paper](https://homl.info/sentimentneuron)⁠^([7](ch14.html#id3233))
    by Alec Radford and other OpenAI researchers describes how the authors trained
    a big char-RNN-like model on a large dataset, and found that one of the neurons
    acted as an excellent sentiment analysis classifier. Although the model was trained
    without any labels, the *sentiment neuron*—as they called it—reached state-of-the-art
    performance on sentiment analysis benchmarks (this foreshadowed and motivated
    unsupervised pretraining in NLP).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管char-RNN模型只是训练来预测下一个字符，但这个看似简单的任务实际上还需要它学习一些更高级的任务。例如，为了找到“Great movie,
    I really ”之后的下一个字符，了解这个句子是积极的很有帮助，因此跟随的更有可能是字母“l”（代表“loved”），而不是“h”（代表“hated”）。实际上，Alec
    Radford和其他OpenAI研究人员在2017年发表的一篇[论文](https://homl.info/sentimentneuron)⁠^([7](ch14.html#id3233))中描述了作者如何在大型数据集上训练了一个类似char-RNN的大模型，并发现其中一个神经元充当了一个出色的情感分析分类器。尽管模型是在没有任何标签的情况下训练的，但被称为“sentiment
    neuron”——正如他们所称呼的——在情感分析基准测试中达到了最先进的性能（这预示并激励了NLP中的无监督预训练）。
- en: 'Speaking of which, let’s say farewell to Shakespeare and turn to the second
    part of this chapter: sentiment analysis.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这里，让我们告别莎士比亚，转向本章的第二部分：情感分析。
- en: Sentiment Analysis Using Hugging Face Libraries
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hugging Face库进行情感分析
- en: 'One of the most common applications of NLP is text classification—especially
    sentiment analysis. If image classification on the MNIST dataset is the “Hello,
    world!” of computer vision, then sentiment analysis on the IMDb reviews dataset
    is the “Hello, world!” of natural language processing. The IMDb dataset consists
    of 50,000 movie reviews in English (25,000 for training, 25,000 for testing) extracted
    from the famous [Internet Movie Database](https://imdb.com), along with a simple
    binary target for each review indicating whether it is negative (0) or positive
    (1). Just like MNIST, the IMDb reviews dataset is popular for good reasons: it
    is simple enough to be tackled on a laptop in a reasonable amount of time, but
    challenging enough to be fun and rewarding.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中最常见的应用之一是文本分类——特别是情感分析。如果MNIST数据集上的图像分类是计算机视觉中的“Hello, world!”，那么IMDb评论数据集上的情感分析就是自然语言处理中的“Hello,
    world!”。IMDb数据集包含50,000篇英文电影评论（25,000用于训练，25,000用于测试），这些评论是从著名的[互联网电影数据库](https://imdb.com)提取的，每个评论都有一个简单的二元目标，表示它是负面（0）还是正面（1）。就像MNIST一样，IMDb评论数据集因其简单性而广受欢迎：它足够简单，可以在合理的时间内用笔记本电脑处理，但挑战性足够，既有趣又有成就感。
- en: 'To download the IMDb dataset, we will use the Hugging Face *Datasets* library,
    which gives easy access to hundreds of thousands of datasets hosted on the Hugging
    Face Hub. It is preinstalled on Colab; otherwise it can be installed using `pip
    install datasets`. We’ll use 80% of the original training set for training, and
    the remaining 20% for validation, using the `train_test_split()` method to split
    the set:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载IMDb数据集，我们将使用Hugging Face *Datasets* 库，它提供了对托管在Hugging Face Hub上的数十万个数据集的便捷访问。它在Colab上预先安装；否则可以使用`pip
    install datasets`进行安装。我们将使用原始训练集的80%进行训练，剩余的20%用于验证，使用`train_test_split()`方法来分割数据集：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s inspect a couple of reviews:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查几篇评论：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first review immediately says that it’s a wonderful movie; no need to read
    any further: it’s clearly positive (label = 1). The second review is much harder
    to classify: it contains a detailed description of the movie, sprinkled with both
    positive and negative comments. Luckily, the conclusion is quite clearly negative,
    making the task much easier (label = 0). Still, it’s not a trivial task.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇评论立即表明这是一部精彩的电影；没有必要再读下去了：它显然是正面的（标签 = 1）。第二篇评论的分类要困难得多：它包含了对电影的详细描述，其中穿插着正面和负面的评论。幸运的是，结论非常明显是负面的，这使得任务变得容易得多（标签
    = 0）。尽管如此，这并不是一个简单任务。
- en: A simple char-RNN model would struggle; we need a more powerful tokenization
    technique. So let’s focus on tokenization before we return to sentiment analysis.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的char-RNN模型会感到吃力；我们需要一个更强大的分词技术。因此，在我们回到情感分析之前，让我们先关注分词。
- en: Tokenization Using the Hugging Face Tokenizers Library
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Hugging Face Tokenizers库进行分词
- en: In a [2016 paper](https://homl.info/rarewords),⁠^([8](ch14.html#id3241)) Rico
    Sennrich et al. from the University of Edinburgh explored several methods to tokenize
    and detokenize text at the subword level. This way, even if your model encounters
    a rare word it has never seen before, it can still reasonably guess what it means.
    For example, even if the model never saw the word “smartest” during training,
    if it learned the word “smart” and it also learned that the suffix “est” means
    “the most”, it can infer the meaning of “smartest”. One of the techniques the
    authors evaluated is *byte pair encoding* (BPE), introduced by Philip Gage in
    1994 (initially for data compression). BPE works by splitting the whole training
    set into individual characters, then at each iteration it finds the most frequent
    pair of adjacent tokens and adds it to the vocabulary. It repeats this process
    until the vocabulary reaches the desired size.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇 [2016 年的论文](https://homl.info/rarewords)，⁠^([8](ch14.html#id3241)) 中，爱丁堡大学的
    Rico Sennrich 等人探索了几种在子词级别分词和去分词文本的方法。这样，即使你的模型遇到了它以前从未见过的罕见词，它仍然可以合理地猜测它的含义。例如，即使模型在训练期间从未见过单词“smartest”，如果它学会了单词“smart”，并且它还学会了后缀“est”意味着“the
    most”，它就可以推断出“smartest”的含义。作者评估的技术之一是 *字节对编码*（BPE），由 Philip Gage 在 1994 年引入（最初用于数据压缩）。BPE
    通过将整个训练集拆分为单个字符，然后在每次迭代中找到最频繁的相邻标记对并将其添加到词汇表中工作。它重复这个过程，直到词汇表达到所需的大小。
- en: 'The [Hugging Face Tokenizers library](https://homl.info/tokenizers) includes
    highly efficient implementations of several popular tokenization algorithms, including
    BPE. It is preinstalled on Colab (or you can install it with `pip install tokenizers`).
    Here’s how to train a BPE model on the IMDb dataset:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hugging Face Tokenizers 库](https://homl.info/tokenizers)包含了几个流行的分词算法的高效实现，包括
    BPE。它在 Colab 上预先安装（或者你也可以使用 `pip install tokenizers` 来安装）。以下是使用 IMDb 数据集训练 BPE
    模型的步骤：'
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s walk through this code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步来看这段代码：
- en: 'We import the Tokenizers library, and we create a BPE model, specifying an
    unknown token `"<unk>"` which will be used later if we try to tokenize some text
    containing tokens that the model never saw during training: the unknown tokens
    will be replaced with the `"<unk>"` token.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们导入 Tokenizers 库，并创建一个 BPE 模型，指定一个未知标记 `"<unk>"`，如果我们在尝试分词包含模型在训练期间从未见过的标记的文本时使用它：未知标记将被替换为
    `"<unk>"` 标记。
- en: We then create a `Tokenizer` based on the BPE model.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们基于 BPE 模型创建一个 `Tokenizer`。
- en: The Tokenizers library lets you specify optional preprocessing and post-processing
    steps, and it also provides common preprocessors and postprocessors. In this example,
    we use the `Whitespace` preprocessor which splits the text at spaces (and drops
    the spaces), and also separates groups of letters and groups of nonletters. For
    example “Hello, world!!!” will be split into ["Hello”, “,”, “world”, “!!!"]. The
    BPE algorithm will then run on these individual chunks, which dramatically speeds
    up training and improves token quality (at least when the text is in English)
    by providing reasonable word boundaries.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tokenizers 库允许你指定可选的预处理和后处理步骤，并且它还提供了常见的预处理器和后处理程序。在这个例子中，我们使用 `Whitespace`
    预处理程序，它在空格处分割文本（并丢弃空格），并且还分离字母组和非字母组。例如，“Hello, world!!!”将被分割成 ["Hello”, “,”,
    “world”, “!!!"]。然后 BPE 算法将运行在这些单独的块上，这极大地加快了训练速度并提高了分词质量（至少当文本是英文时）通过提供合理的单词边界。
- en: 'We then define a list of special tokens: a padding token `"<pad>"` that will
    come in handy when we create batches of texts of different lengths, and the unknown
    token we have already discussed.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们定义一个特殊标记列表：一个填充标记 `"<pad>"`，当我们在不同长度的文本创建批次时将很有用，以及我们之前讨论过的未知标记。
- en: We create a `BpeTrainer`, specifying the maximum vocabulary size and the list
    of special tokens. The trainer will add the special tokens at the beginning of
    the vocabulary, so `"<pad>"` will be token 0, and `"<unk>"` will be token 1.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建一个 `BpeTrainer`，指定最大词汇表大小和特殊标记列表。训练器将在词汇表的开头添加特殊标记，所以 `"<pad>"` 将是标记 0，而
    `"<unk>"` 将是标记 1。
- en: Next we create a list of all the text in the IMBd training set.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含 IMDb 训练集中所有文本的列表。
- en: Lastly, we train the tokenizer on this list, using the `BpeTrainer`. A few seconds
    later, the BPE tokenizer is ready to be used!
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `BpeTrainer` 在这个列表上训练分词器。几秒钟后，BPE 分词器就准备好使用了！
- en: 'Now let’s use our BPE tokenizer to tokenize some text:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们的 BPE 分词器来分词一些文本：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `encode()` method returns an `Encoding` object that contains eight tokens.
    Let’s look at these tokens and their IDs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode()` 方法返回一个包含八个标记的 `Encoding` 对象。让我们来看看这些标记及其 ID：'
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that frequent words like “what” and “movie” have been identified by the
    BPE model and are represented by a single token, while less frequent words like
    “awesome” are split into multiple tokens. Also note that the smiley was not part
    of the training data, so it gets replaced with the unknown token `"<unk>"`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，像“what”和“movie”这样的常用词已被 BPE 模型识别，并由单个标记表示，而像“awesome”这样的不常用词则被拆分为多个标记。此外，需要注意的是，笑脸不是训练数据的一部分，因此它被替换为未知标记
    `"<unk>"`。
- en: 'The tokenizer provides a `get_vocab()` method which returns a dictionary mapping
    each token to its ID. You can also use the `token_to_id()` method to map a single
    token, or conversely use the `id_to_token()` method to go from ID to token. However,
    you will more often use the `decode()` method to convert a list of token IDs into
    a string:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器提供了一个 `get_vocab()` 方法，它返回一个将每个标记映射到其 ID 的字典。您还可以使用 `token_to_id()` 方法将单个标记映射，或者相反使用
    `id_to_token()` 方法从 ID 到标记。然而，您更经常使用 `decode()` 方法将标记 ID 列表转换为字符串：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The tokenizer keeps track of each token’s start and end offset in the original
    string, which can come in handy, especially for debugging:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器会跟踪每个标记在原始字符串中的起始和结束偏移量，这在调试时非常有用：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It’s also possible to encode a whole batch of strings at once. For example,
    let’s encode the first three reviews of the training set:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以一次编码整个字符串批次。例如，让我们编码训练集的前三个评论：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If we want to create a single integer tensor containing the token IDs of all
    three reviews, we must first ensure that they all have the same number of tokens,
    which is not the case right now. For this, we can ask the tokenizer to pad the
    shorter reviews with the padding token ID until they are as long as the longest
    review in the batch. We can also ask the tokenizer to truncate any sequence longer
    than some maximum length, since RNNs don’t handle very long sequences very well
    anyway:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想创建一个包含所有三个评论标记 ID 的单个整数张量，我们必须首先确保它们都具有相同数量的标记，但目前并非如此。为此，我们可以要求分词器用填充标记
    ID 填充较短的评论，直到它们的长度与批次中最长的评论一样长。我们还可以要求分词器截断任何超过某些最大长度的序列，因为 RNN 本身就不擅长处理非常长的序列：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now let’s encode the batch again. This time all sequences will have the same
    number of tokens, so we can create a tensor containing all the token IDs:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次编码批次。这次所有序列都将具有相同数量的标记，因此我们可以创建一个包含所有标记 ID 的张量：
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Notice how the first and second review were padded with 0s, which is our padding
    token ID. Each `Encoding` object also includes an `attention_mask` attribute containing
    a 1 for each nonpadding token, and a 0 for each padding token. This can be used
    in your models to easily ignore the padded time steps: just multiply a tensor
    with the attention mask. In some cases you will prefer to have the list of sequence
    lengths (ignoring padding). Here’s how to get both the attention mask tensor and
    the sequence lengths:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一和第二个评论是用 0 填充的，这是我们的填充标记 ID。每个 `Encoding` 对象还包括一个 `attention_mask` 属性，其中包含每个非填充标记的
    1，以及每个填充标记的 0。这可以在您的模型中用来轻松忽略填充的时间步：只需将张量与注意力掩码相乘。在某些情况下，您可能更喜欢有序列长度列表（忽略填充）。以下是获取注意力掩码张量和序列长度的方法：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You may have noted that spaces were not handled very well by our tokenizer.
    In particular, the word “awesome” came back as “aw es ome”, and “movie!” came
    back as “movie !”. This is because the `Whitespace` pre-tokenizer dropped all
    spaces, therefore the BPE tokenizer doesn’t know where spaces should go and it
    just adds spaces between all tokens. To fix this, we can replace the `Whitespace`
    pre-tokenizer with the `ByteLevel` pre-tokenizer: it replaces all spaces with
    a special character Ġ so the BPE model doesn’t lose track of them. For example,
    if you use this pre-tokenizer and you encode and decode the text “what an awesome
    movie! ![](assets/smiling-face-with-smiling-eyes_1f60a_(1).png)“, you will get:
    “Ġwhat Ġan Ġaw es ome Ġmovie !”. After removing the spaces, then replacing every
    Ġ with a space, you get " what an awesome movie!”. This is almost perfect, except
    for the extra space at the start—which is easily removed—and the lost emoji, which
    was replaced with an unknown token because it’s not in the vocabulary, and dropped
    by the `decode()` method.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们的分词器处理空格并不很好。特别是，单词“awesome”返回为“aw es ome”，而“movie!”返回为“movie !”。这是因为`Whitespace`预分词器丢弃了所有空格，因此BPE分词器不知道空格应该放在哪里，它只是在所有标记之间添加空格。为了解决这个问题，我们可以用`ByteLevel`预分词器替换`Whitespace`预分词器：它将所有空格替换为特殊字符
    Ġ，这样BPE模型就不会丢失它们的跟踪。例如，如果你使用这个预分词器，并且对文本“what an awesome movie! ![](assets/smiling-face-with-smiling-eyes_1f60a_(1).png)”进行编码和解码，你会得到：“Ġwhat
    Ġan Ġaw es ome Ġmovie !”。在移除空格后，然后将每个 Ġ 替换为空格，你得到 " what an awesome movie!"。这几乎是完美的，除了开头多余的空格——这很容易移除——以及丢失的emoji，因为它不在词汇表中，被`decode()`方法丢弃。
- en: 'As its name suggests, the `ByteLevel` pre-tokenizer allows the BPE model to
    work at the byte level, rather than the character level: unsurprisingly, this
    is called Byte-level BPE (BBPE). For example, the ![](assets/smiling-face-with-smiling-eyes_1f60a_(1).png)
    emoji will be converted to four bytes, using Unicode’s UTF-8 encoding. This means
    that BBPE will never output an unknown token if its vocabulary contains all 256
    possible bytes, since any text can be broken down into its individual bytes whenever
    longer tokens are not found in the vocabulary. This makes BBPE well suited when
    the corpus contains rare characters such as emojis.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，`ByteLevel`预分词器允许BPE模型在字节级别而不是字符级别工作：不出所料，这被称为字节级BPE（BBPE）。例如，![](assets/smiling-face-with-smiling-eyes_1f60a_(1).png)
    emoji将被转换为四个字节，使用Unicode的UTF-8编码。这意味着如果其词汇表包含所有256个可能的字节，BBPE永远不会输出未知标记，因为任何文本都可以在其词汇表中找不到更长标记时分解为其单个字节。这使得BBPE非常适合包含诸如emoji之类的罕见字符的语料库。
- en: 'Another important variant of BPE is [WordPiece](https://homl.info/wordpiece),⁠^([9](ch14.html#id3256))
    proposed by Google in 2016\. This tokenization algorithm is very similar to BPE,
    but instead of adding the most frequent adjacent pair of tokens to the vocabulary
    at each iteration, it adds the pair with the highest score. This score is computed
    using [Equation 14-1](#wordpiece_equation): the frequency(AB) term is just like
    in BPE—it boosts pairs that are frequent in the corpus. However, the denominator
    reduces the score of a pair when the individual tokens are themselves frequent.
    This normalization tends to favor more useful and meaningful tokens than BPE,
    and the algorithm often produces shorter encoded sequences than BPE or BBPE.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: BPE的另一个重要变体是Google在2016年提出的[WordPiece](https://homl.info/wordpiece)，⁠^([9](ch14.html#id3256))。这个分词算法与BPE非常相似，但在每次迭代中，它不是将最频繁的相邻标记对添加到词汇表中，而是添加得分最高的标记对。这个得分是使用[方程式14-1](#wordpiece_equation)计算的：频率(AB)项与BPE中的类似——它提高了在语料库中频繁出现的标记对。然而，分母在单个标记本身频繁时降低了标记对的得分。这种归一化往往比BPE更有用和有意义，并且该算法通常产生的编码序列比BPE或BBPE更短。
- en: Equation 14-1\. WordPiece score for a pair AB composed of tokens A and B
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式14-1。由标记A和B组成的AB对的WordPiece得分
- en: $score left-parenthesis AB right-parenthesis equals StartFraction frequency
    left-parenthesis AB right-parenthesis Over freq left-parenthesis upper A right-parenthesis
    dot freq left-parenthesis upper B right-parenthesis EndFraction dot len left-parenthesis
    vocab right-parenthesis$
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $score left-parenthesis AB right-parenthesis equals StartFraction frequency
    left-parenthesis AB right-parenthesis Over freq left-parenthesis upper A right-parenthesis
    dot freq left-parenthesis upper B right-parenthesis EndFraction dot len left-parenthesis
    vocab right-parenthesis$
- en: 'To train a WordPiece tokenizer using the Tokenizers library, you can use the
    same code as for BPE, but replace the `BPE` model with `WordPiece`, and the `BpeTrainer`
    with `WordPieceTrainer`. If you encode and decode the same review as earlier,
    you will get “what an aw esome movie !”. Notice that WordPiece adds a prefix to
    tokens that are inside a word, which makes it easy to reconstruct the original
    string: just remove " #“# (as well as spaces before punctuations). Note that the
    smiley emoji once again disappeared because it was not in the vocabulary.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '要使用Tokenizers库训练WordPiece标记器，你可以使用与BPE相同的代码，但将`BPE`模型替换为`WordPiece`，将`BpeTrainer`替换为`WordPieceTrainer`。如果你对之前相同的评论进行编码和解码，你会得到“what
    an aw esome movie !”。注意，WordPiece会给单词内部的标记添加一个前缀，这使得重建原始字符串变得容易：只需移除" #“#（以及标点符号前的空格）。注意，笑脸表情符号再次消失了，因为它不在词汇表中。'
- en: 'One last popular tokenization algorithm we will discuss is Unigram LM (Language
    Model), introduced in a [2018 paper](https://homl.info/subword)⁠^([10](ch14.html#id3258))
    by Taku Kudo at Google. This technique is a bit different than the previous ones:
    it starts out with a very large vocabulary containing every frequent word, subword,
    and character in the training corpus, then it gradually removes the least useful
    tokens until it reaches the desired vocabulary size. To determine how useful a
    token is, this method makes one big simplifying assumption: it assumes that the
    corpus was sampled randomly from the vocabulary, one token at a time (hence the
    name Unigram LM), and that every token was sampled independently from the others.
    Therefore, this tokenizer model assumes that the probability of sampling the pair
    AB is equal to the probability of sampling A times the probability of sampling
    B. Given this assumption, it can estimate the probability of sampling the whole
    training corpus. At each iteration, the training algorithm attempts to remove
    tokens without reducing this overall probability too much.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的最后一种流行的标记化算法是Unigram LM（语言模型），它是由Google的Taku Kudo在2018年的一篇[论文](https://homl.info/subword)⁠^([10](ch14.html#id3258))中提出的。这种技术与之前的不同：它从一个包含训练语料库中每个频繁单词、子单词和字符的非常大的词汇表开始，然后逐渐移除最不有用的标记，直到达到所需的词汇表大小。为了确定一个标记的有用性，这种方法做出一个大的简化假设：它假设语料库是从词汇表中随机采样的，一次一个标记（因此得名Unigram
    LM），并且每个标记都是独立于其他标记采样的。因此，这个标记器模型假设采样对AB的概率等于采样A的概率乘以采样B的概率。基于这个假设，它可以估计采样整个训练语料库的概率。在每次迭代中，训练算法尝试移除标记，同时尽量不减少这个整体概率。
- en: For example, suppose that the vocabulary contains the tokens “them”, “the”,
    and “m”, respectively, with 1%, 5%, and 2% probability. This means that the word
    “them” has a 1% chance of being sampled as the single token “them”, or a 5% ×
    2% = 0.1% chance of being sampled as the pair “the” + “m”. Overall, the word “them”
    has a 1% + 0.1% = 1.1% chance of being sampled. If we remove the token “them”
    from the vocabulary, then the probability of sampling the word “them” drops down
    to 0.1%. If instead we remove either “m” or “the”, then the probability only drops
    down to 1% since we can still sample the single token “them”. So if the training
    corpus only contained the word “them”, then the algorithm would prefer to drop
    either “the” or “m”. Of course, in reality the corpus contains many other words
    that contain these two tokens, so the algorithm will likely find other less useful
    tokens to drop.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设词汇表包含“them”、“the”和“m”这三个标记，分别具有1%、5%和2%的概率。这意味着单词“them”有1%的概率被采样为单个标记“them”，或者有5%
    × 2% = 0.1%的概率被采样为“the” + “m”这对。总的来说，单词“them”有1% + 0.1% = 1.1%的概率被采样。如果我们从词汇表中移除标记“them”，那么采样单词“them”的概率将下降到0.1%。如果我们移除“m”或“the”中的任何一个，那么概率只会下降到1%，因为我们仍然可以采样单个标记“them”。所以如果训练语料库只包含单词“them”，那么算法会更倾向于删除“the”或“m”。当然，在现实中语料库包含许多其他包含这两个标记的单词，因此算法可能会找到其他不那么有用的标记来删除。
- en: Tip
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Unigram LM is great for languages that don’t use spaces to separate words, like
    English does. For example, Chinese text does not use spaces between words, Vietnamese
    uses spaces even within words, and German often attaches multiple words together,
    without spaces.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 单词一元语言模型对于像英语这样的不使用空格分隔单词的语言来说非常好。例如，中文文本之间没有空格，越南语甚至在单词内部使用空格，而德语经常将多个单词连在一起，不使用空格。
- en: The same paper also proposed a novel regularization technique called *subword
    regularization*, which improves generalization and robustness by introducing some
    randomness in tokenization while training the NLP model (not the tokenizer model).
    For example, assuming the vocabulary contains the tokens “them”, “the”, and “m”,
    and you choose to use subword regularization, then the word “them” will sometimes
    be tokenized as “the” + “m”, and sometimes as “them”. This technique works best
    with *morphologically rich languages*, meaning languages where words carry a lot
    of grammatical information through affixes, inflections, and internal modifications
    (such as Arabic, Finnish, German, Hungarian, Polish, or Turkish), as opposed to
    languages that rely on word order or additional helper words (such as English,
    Chinese, Thai, or Vietnamese).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，该论文还提出了一种新颖的正则化技术，称为 *子词正则化*，通过在训练 NLP 模型（而非分词器模型）时引入一些随机性来提高泛化能力和鲁棒性。例如，假设词汇表包含标记“them”、“the”和“m”，并且您选择使用子词正则化，那么单词“them”有时会被分词为“the”
    + “m”，有时作为“them”。这种技术在 *形态丰富的语言* 中效果最好，这意味着在这些语言中，单词通过词缀、屈折和内部修改（如阿拉伯语、芬兰语、德语、匈牙利语、波兰语或土耳其语）携带大量语法信息，而与那些依赖词序或额外辅助词（如英语、中文、泰语或越南语）的语言形成对比。
- en: Unfortunately, the Tokenizers library does not natively support subword regularization,
    so you either have to implement it yourself, or you can use Google’s [*SentencePiece*](https://github.com/google/sentencepiece)
    library (`pip install sentencepiece`) which provides an open source implementation.
    This project is described in a [2018 paper](https://homl.info/sentencepiece)⁠^([11](ch14.html#id3263))
    by Taku Kudo and John Richardson.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Tokenizers 库本身不支持子词正则化，因此您必须自己实现它，或者可以使用 Google 的 [*SentencePiece*](https://github.com/google/sentencepiece)
    库 (`pip install sentencepiece`)，该库提供了一个开源实现。该项目在 Taku Kudo 和 John Richardson 的
    2018 年论文 [2018 paper](https://homl.info/sentencepiece)⁠^([11](ch14.html#id3263))
    中进行了描述。
- en: '[Table 14-1](#tokenizer_summary_table) summarizes the three main tokenizers
    used today.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 14-1](#tokenizer_summary_table) 总结了今天使用的三种主要分词器。'
- en: Table 14-1\. Overview of the three main tokenizers
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14-1\. 三种主要分词器的概述
- en: '| Feature | BBPE | WordPiece | Unigram LM |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Feature | BBPE | WordPiece | Unigram LM |'
- en: '| --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **How** | Merge most frequent pairs | Merge pairs that maximize data likelihood
    | Remove least likely tokens |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **How** | 合并最频繁的成对 | 合并最大化数据可能性的成对 | 移除最不可能的标记 |'
- en: '| **Pros** | Fast, simple, great for multilingual | Good balance of efficiency
    and token quality | Most meaningful, shortest sequences |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **Pros** | 快速、简单，非常适合多语言 | 效率和分词质量之间的良好平衡 | 最有意义的，最短的序列 |'
- en: '| **Cons** | Can produce awkward splits | Less robust than BBPE for multilingual
    | Slower to train and tokenize |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **Cons** | 可能产生尴尬的分割 | 对于多语言不如 BBPE 坚固 | 训练和分词速度较慢 |'
- en: '| **Used By** | GPT, Llama, RoBERTa, BLOOM | BERT, DistilBERT, ELECTRA | T5,
    ALBERT, mBART |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **Used By** | GPT, Llama, RoBERTa, BLOOM | BERT, DistilBERT, ELECTRA | T5,
    ALBERT, mBART |'
- en: Training your own tokenizer is useful in many situations; for example, if you
    are dealing with domain-specific text, such as medical, legal, or engineering
    documents full of jargon, or if the text is written in a low-resource language
    or dialect, or if it’s code written in a new programming language, and so on.
    However, in most cases you will want to simply reuse a pretrained tokenizer.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，自己训练分词器是有用的；例如，如果您正在处理特定领域的文本，如医学、法律或工程文档，这些文档充满了术语，或者文本是用低资源语言或方言写的，或者它是用新编程语言编写的代码，等等。然而，在大多数情况下，您可能只想简单地重用预训练的分词器。
- en: Reusing Pretrained Tokenizers
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重用预训练分词器
- en: 'To download a pretrained tokenizer, we will use the [Hugging Face *Transformers
    library*](https://homl.info/transformerslib). This library provides many popular
    models for NLP, computer vision, audio processing, and more. Pretrained weights
    are available for almost all of these models, and the library can automatically
    download them from the Hugging Face Hub. The models were originally all based
    on the *Transformer architecture* (which we will discuss in detail in [Chapter 15](ch15.html#transformer_chapter)),
    hence the name of the library, but other kinds of models are now available as
    well, such as CNNs. Lastly, each model comes with all the tools it needs, including
    tokenizers for NLP models: in a single line of code, you can have a fully functional,
    high-performance model for a given task, as we will see later in this chapter.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载预训练的分词器，我们将使用 [Hugging Face *Transformers 库*](https://homl.info/transformerslib)。这个库提供了许多流行的模型，用于
    NLP、计算机视觉、音频处理等。几乎所有这些模型都提供了预训练的权重，并且库可以自动从 Hugging Face Hub 下载它们。这些模型最初都是基于 *Transformer
    架构*（我们将在第 15 章[第 15 章](ch15.html#transformer_chapter)中详细讨论），因此库的名称，但现在也有其他类型的模型可用，例如
    CNN。最后，每个模型都附带所有需要的工具，包括 NLP 模型的分词器：在一行代码中，你可以拥有一个针对给定任务的完全功能、高性能模型，正如我们将在本章后面看到的那样。
- en: 'For now, let’s just grab the pretrained tokenizer from some NLP model. For
    example, the following code downloads the pretrained BBPE tokenizer used by the
    GPT-2 model (a text generation model), and it uses this tokenizer to encode the
    first 3 IMDb reviews, truncating the encoded sequences if they exceed 500 tokens:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们只需从某个 NLP 模型中获取预训练的分词器。例如，以下代码下载了 GPT-2 模型（一个文本生成模型）使用的预训练 BBPE 分词器，并使用这个分词器来编码前
    3 个 IMDb 评论，如果编码的序列超过 500 个标记，则截断编码序列：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Notice that we use the tokenizer object like a function. The result is a dictionary-like
    object of type `BatchEncoding`. You can get the token IDs using the `"input_ids"`
    key. It returns a Python list of lists of token IDs. For example, let’s look at
    the first 10 token IDs of the first encoded review, and use the tokenizer to decode
    them, using its `decode()` method:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们像使用函数一样使用分词器对象。结果是类型为 `BatchEncoding` 的类似字典的对象。你可以使用 `"input_ids"` 键来获取标记
    ID。它返回一个标记 ID 的 Python 列表列表。例如，让我们看看第一个编码评论的前 10 个标记 ID，并使用分词器来解码它们，使用其 `decode()`
    方法：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If you would prefer to use a pretrained WordPiece tokenizer, you can reuse
    the tokenizer of any LLM that was pretrained using WordPiece, such as BERT (another
    popular NLP model, which stands for Bidirectional Encoder Representations from
    Transformers). This tokenizer has a padding token (unlike the previous tokenizer,
    since GPT-2 didn’t need it), so we can specify `padding=True` when encoding a
    batch of reviews: as usual, the shortest texts will be padded to the length of
    the longest one using the padding token. This allows us to also specify `return_tensors="pt"`
    to get a PyTorch tensor instead of a Python list of lists of token IDs: very convenient!
    So let’s encode the first three IMDb reviews:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更愿意使用预训练的 WordPiece 分词器，你可以重用任何使用 WordPiece 预训练的 LLM 的分词器，例如 BERT（另一个流行的
    NLP 模型，代表 Transformer 的双向编码表示）。这个分词器有一个填充标记（与之前的分词器不同，因为 GPT-2 不需要它），所以当编码一批评论时，我们可以指定
    `padding=True`：像往常一样，最短的文本将使用填充标记填充到最长的文本长度。这允许我们还可以指定 `return_tensors="pt"` 来获取
    PyTorch 张量而不是 Python 列表列表的标记 ID：非常方便！所以让我们编码前三个 IMDb 评论：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'The name `"bert-base-uncased"` refers to a *model checkpoint*: this particular
    checkpoint is a case-insensitive BERT model, pretrained on English text. Other
    checkpoints are available, such as `"bert-large-cased"` if you want a larger and
    case-sensitive BERT model, or `"bert-base-multilingual-uncased"` if you want an
    uncased model pretrained on over 100 languages. For now we are just using the
    model’s tokenizer.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 `"bert-base-uncased"` 指的是一个 *模型检查点*：这个特定的检查点是大小写不敏感的 BERT 模型，在英语文本上预训练。其他检查点也可用，例如
    `"bert-large-cased"`，如果你需要一个更大且大小写敏感的 BERT 模型，或者 `"bert-base-multilingual-uncased"`，如果你需要一个在超过
    100 种语言上预训练的无大小写模型。目前我们只是使用模型的分词器。
- en: 'The resulting token IDs and attention masks are nicely padded tensors:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的标记ID和注意力掩码是很好的填充张量：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Notice that each token ID sequence starts with token 101 ([CLS]), and ends with
    token 102 ([SEP]) (ignoring padding tokens). These tokens are needed by the BERT
    model (as we will see in [Chapter 15](ch15.html#transformer_chapter)), but unless
    your model needs them too, you can drop them by setting `add_special_tokens=False`
    when calling the tokenizer.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个标记ID序列都以标记101 ([CLS]) 开头，并以标记102 ([SEP]) 结尾（忽略填充标记）。这些标记是BERT模型所需的（正如我们将在[第15章](ch15.html#transformer_chapter)中看到的那样），但除非您的模型也需要它们，否则您可以通过在调用分词器时设置`add_special_tokens=False`来删除它们。
- en: 'What about a pretrained Unigram LM tokenizer? Well, many models were trained
    using Unigram LM, such as ALBERT, T5, or XML-R models, just to name a few. For
    example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，预训练的单词LM分词器怎么样呢？嗯，许多模型都是使用Unigram LM训练的，例如ALBERT、T5或XML-R模型，仅举几个例子。例如：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The Transformers library also provides an object that can wrap your own tokenizer
    (from the Tokenizers library) and give it the same API as the pretrained tokenizers
    (from the Transformers library). For example, let’s wrap the BPE tokenizer we
    trained earlier:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers库还提供了一个对象，可以将您的自己的分词器（来自Tokenizers库）包装起来，并给它与预训练的分词器（来自Transformers库）相同的API。例如，让我们包装我们之前训练过的BPE分词器：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With that, we have all the tokenization tools we need, so let’s go back to sentiment
    analysis.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就拥有了所有需要的分词工具，所以让我们回到情感分析。
- en: Building and Training a Sentiment Analysis Model
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和训练情感分析模型
- en: 'Our sentiment analysis model must be trained using batches of tokenized reviews.
    However, the datasets we created did not take care of tokenization. One option
    would be to update them (e.g., using the `map()` method), but it’s just as simple
    to handle tokenization in the data loaders. To do this, we can pass a function
    to the `DataLoader` constructor using its `collate_fn` argument: the data loader
    will call this function for every batch, passing it a list of dataset samples.
    Our function will take this batch, tokenize the reviews, truncate and pad them
    if needed, and return a `BatchEncoding` object containing PyTorch tensors for
    the token IDs and attention masks, along with another tensor containing the labels.
    For tokenization, we will simply use the pretrained WordPiece tokenizer we just
    loaded:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 sentiment analysis 模型必须使用分词评论的批次进行训练。然而，我们创建的数据集并没有处理分词。一个选择是更新它们（例如，使用`map()`方法），但处理数据加载器中的分词同样简单。为此，我们可以通过`collate_fn`参数将一个函数传递给`DataLoader`构造函数：数据加载器将为每个批次调用此函数，传递一个数据集样本列表。我们的函数将接受这个批次，对评论进行分词，如果需要则截断和填充它们，并返回一个包含PyTorch张量（标记ID和注意力掩码）的`BatchEncoding`对象，以及另一个包含标签的张量。对于分词，我们将简单地使用我们刚刚加载的预训练WordPiece分词器：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we’re ready to create our sentiment analysis model:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好创建我们的情感分析模型：
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As you can see, this model is very similar to our Shakespeare model, but with
    a few important differences:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个模型与我们之前的莎士比亚模型非常相似，但有一些重要的区别：
- en: When creating the `nn.Embedding` layer, we set its `padding_idx` argument to
    our padding ID. This ensures that the padding ID gets embedded as a nontrainable
    zero vector to reduce the impact of padding tokens on the loss.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建`nn.Embedding`层时，我们将`padding_idx`参数设置为我们的填充ID。这确保填充ID被嵌入为不可训练的零向量，以减少填充标记对损失的影响。
- en: Since this is a sequence-to-vector model, not a sequence-to-sequence model,
    we only need the last output of the top GRU layer to make our final prediction
    (through the output `nn.Linear` layer). We could have used `outputs[:, -1]` instead
    of `hidden_states[-1]`, as they are equal.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这是一个序列到向量模型，而不是序列到序列模型，我们只需要顶层GRU层的最后一个输出来进行最终预测（通过输出`nn.Linear`层）。我们可以使用`outputs[:,
    -1]`而不是`hidden_states[-1]`，因为它们是相等的。
- en: The output `nn.Linear` layer has a single output dimension because it’s a binary
    classification model. The final output will be a 2D tensor with a single column
    containing one logit per review, positive for positive reviews, and negative for
    negative reviews.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出的`nn.Linear`层只有一个输出维度，因为它是一个二分类模型。最终输出将是一个二维张量，包含一个列，每条评论有一个logit，正面评论为正，负面评论为负。
- en: The `forward()` method takes a `BatchEncoding` object as input, containing the
    token IDs (possibly padded and truncated).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()`方法接受一个`BatchEncoding`对象作为输入，其中包含标记ID（可能已填充和截断）。'
- en: We can then train this model using the `nn.BCEWithLogitsLoss` since this is
    a binary classification task. It reaches close to 85% accuracy on the validation
    set, which is reasonably good, although the best models reach human level, slightly
    above 90% accuracy. It’s probably not possible to go much higher than that; because
    many reviews are ambiguous, classifying them feels like flipping a coin.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `nn.BCEWithLogitsLoss` 训练这个模型，因为这是一个二元分类任务。它在验证集上的准确率接近 85%，这相当不错，尽管最好的模型达到了人类水平，略高于
    90% 的准确率。可能无法进一步提高；因为许多评论是模糊的，分类它们感觉就像抛硬币一样。
- en: 'One problem with our model is the fact that we are not fully ignoring the padding
    tokens. Indeed, if a review ends with many padding tokens, the `nn.GRU` module
    will have to process them, and by the time it gets through all of them, it might
    have forgotten what the review was all about. To avoid this, we can use a *packed
    sequence* instead of a regular tensor. A packed sequence is a special data structure
    designed to efficiently represent a batch of sequences of variable lengths.⁠^([12](ch14.html#id3281))
    You can use the `pack_padded_sequence()` function to convert a tensor containing
    padded sequences to a packed sequence object, and conversely you can use the `pad_packed_sequence()`
    function whenever you want to convert a packed sequence object to a padded tensor:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的一个问题是我们没有完全忽略填充标记。确实，如果一条评论以许多填充标记结束，`nn.GRU` 模块将不得不处理它们，等到它处理完所有这些标记时，它可能已经忘记了评论的内容。为了避免这种情况，我们可以使用一个
    *打包序列* 而不是常规张量。打包序列是一种特殊的数据结构，用于高效地表示一批长度可变的序列。你可以使用 `pack_padded_sequence()`
    函数将包含填充序列的张量转换为打包序列对象，相反，你可以在需要将打包序列对象转换为填充张量时使用 `pad_packed_sequence()` 函数：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: By default, the `pack_padded_sequence()` function assumes that the sequences
    in the batch are ordered from the longest to the shortest. If this is not the
    case, you must set `enforce_sorted=False`. Moreover, the function also assumes
    that the time dimension comes before the batch dimension. If the batch dimension
    is first, you must set `batch_first=True`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`pack_padded_sequence()` 函数假定批次中的序列按从长到短的顺序排列。如果不是这种情况，你必须设置 `enforce_sorted=False`。此外，该函数还假定时间维度在批次维度之前。如果批次维度是第一个，你必须设置
    `batch_first=True`。
- en: 'PyTorch’s recurrent layers support packed sequences: they efficiently process
    the sequences, stopping at the end of each sequence. So let’s update our sentiment
    analysis model to use packed sequences. In the `forward()` method, just replace
    the `self.gru(embeddings)` line with the following code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的循环层支持打包序列：它们高效地处理序列，在每个序列的末尾停止。所以让我们更新我们的情感分析模型以使用打包序列。在 `forward()`
    方法中，只需将 `self.gru(embeddings)` 行替换为以下代码：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This code starts by computing the length of each sequence in the batch, just
    like we did earlier, then it packs the embeddings tensor and passes the packed
    sequence to the `nn.GRU` module. With that, the model will properly handle sequences
    without being bothered by any padding tokens. You don’t actually need to set `padding_idx`
    anymore when creating the `nn.Embedding` layer, but it doesn’t hurt, and it makes
    debugging a bit easier, so I prefer to keep it.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先计算批次中每个序列的长度，就像我们之前做的那样，然后打包嵌入张量，并将打包序列传递给 `nn.GRU` 模块。有了这个，模型将正确处理序列，而不会被任何填充标记所打扰。实际上，在创建
    `nn.Embedding` 层时，你不再需要设置 `padding_idx`，但这并不妨碍，并且使调试变得更容易，所以我更喜欢保留它。
- en: 'Another way to improve our model is to let it look at the review in both directions:
    left to right, and right to left. Let’s see how this works.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 提高我们模型的另一种方法是让它从两个方向查看评论：从左到右，然后从右到左。让我们看看这是如何工作的。
- en: Note
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you pass a packed sequence to an `nn.GRU` module, its outputs will also be
    a packed sequence, and you will need to convert it back to a padded tensor before
    you can pass it to the next layers. Luckily, we don’t need these outputs for our
    sentiment analysis model, only the hidden states.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将一个打包序列传递给 `nn.GRU` 模块，它的输出也将是一个打包序列，在你将其传递给下一层之前，你需要将其转换回填充张量。幸运的是，我们不需要这些输出用于我们的情感分析模型，只需要隐藏状态。
- en: Bidirectional RNNs
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向 RNN
- en: At each time step, a regular recurrent layer only looks at past and present
    inputs before generating its output. In other words, it is *causal*, meaning it
    cannot look into the future. This type of RNN makes sense when forecasting time
    series, or in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks
    like text classification, or in the encoder of a seq2seq model, it is often preferable
    to look ahead at the next words before encoding a given word.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，一个常规循环层只查看过去和当前的输入，然后生成其输出。换句话说，它是*因果的*，这意味着它不能看到未来。这种类型的RNN在预测时间序列或在序列到序列（seq2seq）模型的解码器中是有意义的。但对于文本分类或seq2seq模型的编码器中的任务，通常更倾向于在编码给定单词之前查看下一个单词。
- en: 'For example, consider the phrases “the right arm”, “the right person”, and
    “the right to speak”: to properly encode the word “right”, you need to look ahead.
    One solution is to run two recurrent layers on the same inputs, one reading the
    words from left to right and the other reading them from right to left, then combine
    their outputs at each time step, typically by concatenating them. This is what
    a *bidirectional recurrent layer* does (see [Figure 14-4](#bidirectional_rnn_diagram)).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑短语“the right arm”，“the right person”，和“the right to speak”：为了正确编码单词“right”，你需要查看前方。一种解决方案是在相同的输入上运行两个循环层，一个从左到右读取单词，另一个从右到左读取，然后在每个时间步将它们的输出合并，通常是通过连接它们。这就是*双向循环层*所做的事情（参见[图14-4](#bidirectional_rnn_diagram)）。
- en: '![Diagram illustrating a bidirectional recurrent layer, showing inputs processed
    in both directions and outputs combined.](assets/hmls_1404.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图示双向循环层，显示双向处理输入并合并输出。](assets/hmls_1404.png)'
- en: Figure 14-4\. A bidirectional recurrent layer
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-4\. 双向循环层
- en: To make our sentiment analysis model bidirectional, we can just set `bidirectional=True`
    when creating the `nn.GRU` layer (this also works with the `nn.RNN` and `nn.LSTM`
    modules).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 要使我们的情感分析模型双向，我们只需在创建`nn.GRU`层时设置`bidirectional=True`（这也适用于`nn.RNN`和`nn.LSTM`模块）。
- en: 'However, once we do that, we must adjust our model a bit. In particular, we
    must double the input dimension of the output `nn.Linear` layer, since the hidden
    states will double in size:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦我们这样做，我们必须对我们的模型进行一些调整。特别是，我们必须将输出`nn.Linear`层的输入维度加倍，因为隐藏状态的大小将加倍：
- en: '[PRE36]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We must also concatenate the forward and backward hidden states of the GRU’s
    top layer before passing the result to the output layer. For this, we can replace
    the last line of the `forward()` method (i.e., `return self.output(hidden_states[-1])`)
    with the following code:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须在将结果传递到输出层之前，连接GRU顶层的前向和后向隐藏状态。为此，我们可以将`forward()`方法的最后一行（即`return self.output(hidden_states[-1])`）替换为以下代码：
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s see how the middle line works:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看中间行是如何工作的：
- en: 'Until now, the shape of the hidden states returned by the `nn.GRU` module was
    [*number of layers*, *batch size*, *hidden size*], so [2, 256, 64] in our case.
    But when we set `bidirectional=True`, we doubled the first dimension size, so
    we now have a shape of [4, 256, 64]: the tensor contains the hidden states for
    layer 1 forward, layer 1 backward, layer 2 forward, and layer 2 backward. Since
    we only want the top layer’s hidden states, both forward and backward, we must
    get `hidden_states[-2:]`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，`nn.GRU`模块返回的隐藏状态的形状是[*层数*，*批量大小*，*隐藏大小*]，在我们的例子中是[2, 256, 64]。但是当我们设置`bidirectional=True`时，我们将第一个维度的尺寸加倍，所以我们现在有一个形状为[4,
    256, 64]的形状：张量包含第1层前向、第1层后向、第2层前向和第2层后向的隐藏状态。由于我们只想获取顶层的前向和后向隐藏状态，我们必须获取`hidden_states[-2:]`。
- en: 'We also need to concatenate the forward and backward states. One way to do
    this is to permute the first two dimensions of the top hidden states using `permute(1,
    0, 2)` to get the shape [256, 2, 64], then reshape the result using `reshape(-1,
    n_dims)` (where `n_dims` equals 128) to get the desired shape: [256, 2 * 64].'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还需要连接前向和后向状态。一种方法是通过使用`permute(1, 0, 2)`对顶层隐藏状态的前两个维度进行置换，以获得形状[256, 2, 64]，然后使用`reshape(-1,
    n_dims)`（其中`n_dims`等于128）重塑结果，以获得所需的形状：[256, 2 * 64]。
- en: Note
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this model we only use the last hidden states, ignoring the outputs at each
    time step. If you ever want to use the outputs of a bidirectional module, be aware
    that its last dimension’s size will be doubled.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们只使用最后一个隐藏状态，忽略每个时间步的输出。如果你想要使用双向模块的输出，请注意，其最后一个维度的尺寸将是加倍。
- en: 'You can try training this model, but you will not see any improvement in this
    case, because the first model actually overfit the training set, and this new
    version makes it even worse: it reaches over 99% accuracy on the training set,
    but just 84% on the validation set. To fix this, you could try to regularize the
    model a bit more, reduce the size of the model, or increase the size of the training
    set.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试训练这个模型，但在这个情况下你不会看到任何改进，因为第一个模型实际上过度拟合了训练集，而这个新版本甚至更糟：它在训练集上达到了超过99%的准确率，但在验证集上只有84%。为了解决这个问题，你可以尝试对模型进行更多的正则化，减小模型的大小，或者增加训练集的大小。
- en: 'But let’s instead try something different: using pretrained embeddings.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们尝试一些不同的事情：使用预训练的嵌入。
- en: Reusing Pretrained Embeddings and Language Models
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新使用预训练的嵌入和语言模型
- en: 'Our model was able to learn useful embeddings for thousands of tokens, based
    on just 25,000 movie reviews: that’s quite impressive! Imagine how good the embeddings
    would be if we had billions of reviews to train on. The good news is that we can
    reuse word embeddings even when they were trained on some other (very) large text
    corpus, even if it was not composed of movie reviews, and even if they were not
    trained for sentiment analysis. After all, the word “amazing” generally has the
    same meaning whether you use it to talk about movies or anything else.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型能够根据仅有25,000条电影评论，为数千个标记学习到有用的嵌入：这相当令人印象深刻！想象一下，如果我们有数十亿条评论来训练，这些嵌入会多么好。好消息是，即使这些词嵌入是在某些其他（非常）大的文本语料库上训练的，即使它不是由电影评论组成的，即使它们不是为了情感分析而训练的，我们也可以重用词嵌入。毕竟，“amazing”这个词无论你用它来谈论电影还是其他任何东西，通常都有相同的意思。
- en: 'Since we used pretrained tokens for the BERT model, we might as well try using
    its embedding layer. First, we need to download the pretrained model using the
    `AutoModel.from_pretrained()` function from the Transformers library, then we
    can directly access its embeddings layer:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了BERT模型的预训练标记，我们不妨尝试使用它的嵌入层。首先，我们需要使用Transformers库中的`AutoModel.from_pretrained()`函数下载预训练模型，然后我们可以直接访问其嵌入层：
- en: '[PRE38]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As you can see, this BERT model is implemented using PyTorch, and it contains
    a regular `nn.Embedding` layer. We could just replace our model’s `nn.Embedding`
    layer with this one (and retrain our model), but we can keep models cleanly separated
    by initializing our own `nn.Embedding` layer with a copy of the pretrained embedding
    matrix. This can be done using the `Embedding.from_pretrained()` function:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个BERT模型是使用PyTorch实现的，它包含一个常规的`nn.Embedding`层。我们可以直接用这个替换我们模型的`nn.Embedding`层（并重新训练我们的模型），但我们可以通过用预训练嵌入矩阵的副本初始化我们自己的`nn.Embedding`层来保持模型干净地分离。这可以通过使用`Embedding.from_pretrained()`函数来完成：
- en: '[PRE39]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that we set `freeze=True` when creating the `nn.Embedding` layer: this
    makes it nontrainable and ensures that the pretrained embeddings won’t be damaged
    by large gradients at the beginning of training. You can train the model for a
    few epochs like this, then make the embedding layer trainable and continue training,
    letting the model fine-tune the embeddings for our task.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在创建`nn.Embedding`层时设置了`freeze=True`：这使得它不可训练，并确保预训练的嵌入在训练初期不会被大的梯度损坏。你可以这样训练模型几个周期，然后使嵌入层可训练，并继续训练，让模型微调我们的任务嵌入。
- en: 'Pretrained word embeddings have been popular for quite a while, starting with
    Google’s [Word2vec embeddings](https://homl.info/word2vec) (2013), Stanford’s
    [GloVe embeddings](https://homl.info/glove) (2014), Facebook’s [FastText embeddings](https://fasttext.cc)
    (2016), and more. However, this approach has its limits. In particular, a word
    has a single representation, no matter the context. For example, the word “right”
    is encoded the same way in “left and right” and “right and wrong”, even though
    it means two very different things. To address this limitation, a [2018 paper](https://homl.info/elmo)⁠^([13](ch14.html#id3298))
    by Matthew Peters introduced *Embeddings from Language Models* (ELMo): these are
    contextualized word embeddings learned from the internal states of a deep bidirectional
    RNN language model. In other words, instead of just using pretrained word embeddings
    in your model, you can reuse several layers of a pretrained language model.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练词嵌入已经流行了一段时间，从Google的[Word2vec嵌入](https://homl.info/word2vec)（2013年），Stanford的[GloVe嵌入](https://homl.info/glove)（2014年），Facebook的[FastText嵌入](https://fasttext.cc)（2016年）等开始。然而，这种方法有其局限性。特别是，无论在什么语境下，一个词只有一个表示。例如，“right”这个词在“left
    and right”和“right and wrong”中的编码方式相同，尽管它的意思非常不同。为了解决这个局限性，Matthew Peters在2018年发表的一篇[论文](https://homl.info/elmo)⁠^([13](ch14.html#id3298))中引入了*从语言模型中提取嵌入*（ELMo）：这些是从深度双向RNN语言模型的内部状态中学习到的上下文化词嵌入。换句话说，你不仅可以在你的模型中使用预训练的词嵌入，还可以重用预训练语言模型的几个层。
- en: 'At roughly the same time, the [Universal Language Model Fine-Tuning (ULMFiT)
    paper](https://homl.info/ulmfit)⁠^([14](ch14.html#id3303)) by Jeremy Howard and
    Sebastian Ruder demonstrated the effectiveness of unsupervised pretraining for
    NLP tasks. The authors trained an LSTM language model on a huge text corpus using
    self-supervised learning (i.e., generating the labels automatically from the data),
    then they fine-tuned it on various tasks. Their model outperformed the state of
    the art on six text classification tasks by a large margin (reducing the error
    rate by 18%–24% in most cases). Moreover, the authors showed that a pretrained
    model fine-tuned on just 100 labeled examples could achieve the same performance
    as one trained from scratch on 10,000 examples. Before the ULMFiT paper, using
    pretrained models was only the norm in computer vision; in the context of NLP,
    pretraining was limited to word embeddings. This paper marked the beginning of
    a new era in NLP: today, reusing pretrained language models is the norm.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在同一时间，Jeremy Howard和Sebastian Ruder的[通用语言模型微调（ULMFiT）论文](https://homl.info/ulmfit)⁠^([14](ch14.html#id3303))展示了无监督预训练在NLP任务中的有效性。作者在一个巨大的文本语料库上使用自监督学习（即自动从数据中生成标签）训练了一个LSTM语言模型，然后他们在各种任务上微调它。他们的模型在六个文本分类任务上大幅超越了当时的最佳水平（在大多数情况下将错误率降低了18%–24%）。此外，作者还表明，仅在100个标记示例上微调的预训练模型可以达到从头开始训练10,000个示例时的相同性能。在ULMFiT论文之前，使用预训练模型在计算机视觉中是常态；在NLP的背景下，预训练仅限于词嵌入。这篇论文标志着NLP新时代的开始：今天，重用预训练语言模型是常态。
- en: 'For example, why not reuse the entire pretrained BERT model for our sentiment
    analysis model? To use the BERT model, the Transformers library lets us call it
    like a function, passing it the tokenized reviews:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为什么不为我们情感分析模型重用整个预训练的BERT模型呢？要使用BERT模型，Transformers库允许我们像调用函数一样调用它，传递给它分词后的评论：
- en: '[PRE40]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'BERT’s output includes an attribute named `last_hidden_state`, which contains
    contextualized embeddings for each token. The word “last” in this case refers
    to the last layer, not the last time step (BERT is a transformer, not an RNN).
    This `last_hidden_state` tensor has a shape of [*batch size*, *max sequence length*,
    *hidden size*]. Let’s use these contextualized embeddings in a sentiment analysis
    model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的输出包括一个名为`last_hidden_state`的属性，它包含每个标记的上下文化嵌入。这里的“last”指的是最后一层，而不是最后一个时间步（BERT是一个转换器，而不是RNN）。这个`last_hidden_state`张量的形状为[*批大小*，*最大序列长度*，*隐藏大小*]。让我们在情感分析模型中使用这些上下文化嵌入：
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that we don’t need to make the `nn.GRU` module bidirectional since the
    contextualized embeddings already looked ahead.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不需要将`nn.GRU`模块设置为双向，因为上下文化嵌入已经向前看了。
- en: If you freeze the BERT model (e.g., using `model.bert.requires_grad_(False)`)
    and train the rest of the model, you will notice a significant performance boost,
    reaching over 88% accuracy. Wonderful!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你冻结BERT模型（例如，使用`model.bert.requires_grad_(False)`）并训练模型的其他部分，你将注意到显著的性能提升，达到88%以上的准确率。太棒了！
- en: 'Another option is to use only the contextualized embedding for the very first
    token, which is the *class token* [CLS]. Indeed, during pretraining, the BERT
    model had to perform a text classification task based solely on this token’s contextualized
    embedding (we will discuss BERT pretraining in more detail in [Chapter 15](ch15.html#transformer_chapter)).
    As a result, it learned to summarize the most important features of the text into
    this embedding. This simplifies our model quite a bit, since we can get rid of
    the `nn.GRU` module altogether, and the `forward()` method becomes much shorter:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是只对第一个标记（即 *类别标记* [CLS]）使用上下文化的嵌入。实际上，在预训练过程中，BERT 模型必须仅基于这个标记的上下文化嵌入执行文本分类任务（我们将在
    [第 15 章](ch15.html#transformer_chapter) 中更详细地讨论 BERT 预训练）。因此，它学会了将文本的最重要特征总结到这个嵌入中。这简化了我们的模型，因为我们完全可以去掉
    `nn.GRU` 模块，并且 `forward()` 方法变得更短：
- en: '[PRE42]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In fact, the BERT model contains an extra hidden layer on top of the class embedding,
    composed of an `nn.Linear` module and an `nn.Tanh` module. This hidden layer is
    called the *pooler*. To use it, just replace `bert_output.last_hidden_state[:,
    0]` with `bert_output.pooler_output`. You may also want to unfreeze the pooler
    after a few epochs to fine-tune it for the IMDb task.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，BERT 模型在类别嵌入之上包含一个额外的隐藏层，由一个 `nn.Linear` 模块和一个 `nn.Tanh` 模块组成。这个隐藏层被称为 *池化器*。要使用它，只需将
    `bert_output.last_hidden_state[:, 0]` 替换为 `bert_output.pooler_output`。您可能还希望在几个epoch之后解冻池化器，以便针对
    IMDb 任务进行微调。
- en: So we started by reusing only the pretrained tokenizer, then we reused the pretrained
    embeddings, then most of the pretrained BERT model, and finally the full model,
    adding only an `nn.Linear` layer on top of the pooler. We can actually go one
    step further and just use an off-the-shelf class for sentence classification.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先只重用了预训练的标记器，然后重用了预训练的嵌入，接着是大部分预训练的 BERT 模型，最后是完整的模型，只在池化器之上添加了一个 `nn.Linear`
    层。我们实际上可以更进一步，直接使用现成的句子分类类。
- en: Task-Specific Classes
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务特定类
- en: 'To tackle our binary classification task using BERT, we can use the `BertForSequenceClassification`
    class provided by the Transformers library. It’s just a BERT model plus a classification
    head on top. All you need to do to create this model is specify the pretrained
    BERT checkpoint you want to use, the number of output units for your classification
    task, and optionally the data type (we’ll use 16-bit floats to fit on small GPUs):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 BERT 解决我们的二分类任务，我们可以使用 Transformers 库提供的 `BertForSequenceClassification`
    类。它只是一个带有分类头的 BERT 模型。要创建此模型，您只需指定要使用的预训练 BERT 检查点、您分类任务的输出单元数，以及可选的数据类型（我们将使用
    16 位浮点数以适应小型 GPU）：
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Tip
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The Transformers library contains many task-specific classes based on various
    pretrained models, such as `BertForQuestionAnswering` or `RobertaForSequenceClassification`
    (see [Chapter 15](ch15.html#transformer_chapter)). You can also use `AutoModelForSequenceClassification`
    to let the library pick the right class for you, based on the requested model
    checkpoint (e.g., if you ask for `"bert-base-uncased"`, you will get an instance
    of `BertForSequenceClassification`). Similar `AutoModelFor[...]` classes are available
    for other tasks.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 库包含许多基于各种预训练模型的任务特定类，例如 `BertForQuestionAnswering` 或 `RobertaForSequenceClassification`（见
    [第 15 章](ch15.html#transformer_chapter)）。您还可以使用 `AutoModelForSequenceClassification`
    让库为您选择正确的类，基于请求的模型检查点（例如，如果您请求 `"bert-base-uncased"`，您将获得一个 `BertForSequenceClassification`
    的实例）。类似的 `AutoModelFor[...]` 类也适用于其他任务。
- en: Until now we have always used a single output for binary classification, so
    why did we set `num_labels=2`? Well, for simplicity Hugging Face prefers to treat
    binary classification exactly like multiclass classification, so this model will
    output two logits instead of one, and it must be trained using the `nn.CrossEntropyLoss`
    instead of `nn.BCELoss` or `nn.BCEWithLogitsLoss`. If you want to convert the
    logits to estimated probabilities, you must use `torch.softmax()` rather than
    `torch.sigmoid()`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直使用单个输出进行二分类，那么为什么我们设置了 `num_labels=2` 呢？嗯，为了简单起见，Hugging Face 更愿意将二分类完全视为多类分类，因此此模型将输出两个
    logits 而不是一个，并且必须使用 `nn.CrossEntropyLoss` 而不是 `nn.BCELoss` 或 `nn.BCEWithLogitsLoss`
    进行训练。如果您想将 logits 转换为估计的概率，必须使用 `torch.softmax()` 而不是 `torch.sigmoid()`。
- en: 'Let’s call this model on a very positive review:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用这个模型对一个非常积极的评论进行测试：
- en: '[PRE44]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We first tokenize the review, then we call the model, it returns a `ModelOutput`
    object containing the logits, and we convert these logits to estimated probabilities
    using the `torch.softmax()` function. Ouch! The model classified this review as
    negative with 65.53% confidence! Indeed, the BERT model inside `BertForSequenceClassification`
    is pretrained, but not the classification head, so we’re going to get terrible
    performance until we actually train this model on the IMDb dataset.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对评论进行分词，然后调用模型，它返回一个包含 logits 的 `ModelOutput` 对象，我们使用 `torch.softmax()`
    函数将这些 logits 转换为估计的概率。哎呀！模型以 65.53% 的置信度将这篇评论分类为负面！实际上，`BertForSequenceClassification`
    中的 BERT 模型是预训练的，但分类头不是，所以我们直到在 IMDb 数据集上实际训练这个模型之前都会得到很差的表现。
- en: 'If you pass labels when calling this model (or any other model from the Transformers
    library), then it also computes the loss and returns it in the `ModelOutput` object.
    For example:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你调用此模型（或从 Transformers 库中的任何其他模型）时传递标签，那么它也会计算损失，并在 `ModelOutput` 对象中返回它。例如：
- en: '[PRE45]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Since `num_labels` is greater than 1, the model computes the `nn.CrossEntropyLoss`
    (which is implemented as `nn.LogSoftmax` followed by `nn.NLLLoss`—that’s why we
    see `grad_fn=<NllLossBackward0>`). If we had used `num_labels=1`, then the model
    would have used the `nn.MSELoss` instead; this can be useful for regression tasks.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `num_labels` 大于 1，模型会计算 `nn.CrossEntropyLoss`（它被实现为 `nn.LogSoftmax` 后跟 `nn.NLLLoss`——这就是为什么我们看到
    `grad_fn=<NllLossBackward0>`）。如果我们使用了 `num_labels=1`，那么模型将使用 `nn.MSELoss` 代替；这对于回归任务可能很有用。
- en: We could now train this model using our own training code, as we did so far,
    but the Transformers library provides a convenient *Trainer API*, so let’s check
    it out.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用自己的训练代码来训练这个模型，就像我们之前做的那样，但 Transformers 库提供了一个方便的 *Trainer API*，所以让我们来看看它。
- en: The Trainer API
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Trainer API
- en: The Trainer API lets you fine-tune a model on your own dataset with very little
    boilerplate code. It can save model checkpoints during training, apply early stopping,
    distribute the computations across GPUs, log metrics, take care of padding, batching,
    shuffling, and more. Let’s use the Trainer API to train our IMDb model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer API 允许你用很少的样板代码在自己的数据集上微调模型。它可以在训练期间保存模型检查点，应用早期停止，跨 GPU 分布计算，记录指标，处理填充、批处理、洗牌等等。让我们使用
    Trainer API 来训练我们的 IMDb 模型。
- en: 'The Trainer API works directly with dataset objects, not data loaders, but
    it expects the datasets to contain tokenized text, not strings, so we must take
    care of tokenization. We can do this quite simply using the dataset’s `map()`
    method (this method is implemented by the Datasets library; it’s not available
    on pure PyTorch datasets):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer API 直接与数据集对象一起工作，而不是数据加载器，但它期望数据集包含分词后的文本，而不是字符串，因此我们必须注意分词。我们可以通过使用数据集的
    `map()` 方法（此方法由 Datasets 库实现；它不适用于纯 PyTorch 数据集）来非常简单地做到这一点：
- en: '[PRE46]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Since we set `batched=True`, the `map()` method passes batches of reviews to
    the `tokenize_batch()` method: this is optional, but it significantly speeds up
    this preprocessing step. The `tokenize_batch()` method tokenizes the given batch
    of reviews, and the resulting fields are added to each instance by the `map()`
    method. This includes fields such as `token_ids` and `attention_mask`, which the
    model expects.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们设置了 `batched=True`，`map()` 方法将评论的批次传递给 `tokenize_batch()` 方法：这是可选的，但它显著加快了这一预处理步骤。`tokenize_batch()`
    方法对给定的评论批次进行分词，`map()` 方法将结果字段添加到每个实例中。这包括模型期望的字段，如 `token_ids` 和 `attention_mask`。
- en: 'To evaluate our model, we can write a simple function that takes an object
    with two attributes: `label_ids` and `predictions`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，我们可以编写一个简单的函数，该函数接受一个具有两个属性的对象：`label_ids` 和 `predictions`：
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Tip
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Alternatively, you can use metrics provided by the Hugging Face *Evaluate library*:
    they are designed to work nicely with the Transformers library. Alternatively,
    although the Trainer API does not support the streaming metrics from the TorchMetrics
    library, you can still use them if you wrap them inside a function.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用 Hugging Face *Evaluate library* 提供的指标：它们被设计得与 Transformers 库配合得很好。或者，尽管
    Trainer API 不支持来自 TorchMetrics 库的流式指标，但你仍然可以在一个函数中包装它们来使用。
- en: 'Next, we must specify our training configuration in a `TrainingArguments` object:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须在 `TrainingArguments` 对象中指定我们的训练配置：
- en: '[PRE48]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We specify that the logs and model checkpoints must be saved in the `my_imdb_model`
    directory; training should run for 2 epochs (you can increase this if you want);
    the batch size is 128 for both training and evaluation (you can tweak this depending
    on the amount of VRAM you have); we want evaluation, logging, and saving to take
    place at the end of each epoch; and the best model should be loaded at the end
    of training based on the validation accuracy. Lastly, the `report_to` argument
    lets you specify one or more tools that the training code will report logs to,
    such as TensorBoard or [Weights & Biases (W&B)](https://wandb.ai). This can be
    useful to visualize the learning curves. For simplicity, I set `report_to="none"`
    to turn reporting off.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定日志和模型检查点必须保存在`my_imdb_model`目录中；训练应该运行2个epoch（如果你想增加这个数字）；训练和评估的批量大小都是128（你可以根据你的VRAM量进行调整）；我们希望在每个epoch结束时进行评估、记录和保存；并且根据验证准确率，在训练结束时加载最佳模型。最后，`report_to`参数允许你指定一个或多个训练代码将日志报告给的工具，例如TensorBoard或[Weights
    & Biases (W&B)](https://wandb.ai)。这可以用来可视化学习曲线。为了简单起见，我将`report_to="none"`设置为关闭报告。
- en: 'Lastly, we create a `Trainer` object and pass it the model, along with the
    training arguments, the training and validation sets, the evaluation function,
    plus a data collator which will take care of padding. Finally, we call the trainer’s
    `train()` method, and we’re done! The model reaches about 90% accuracy on the
    validation set after just two epochs:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个`Trainer`对象，并将模型、训练参数、训练集和验证集、评估函数以及一个负责填充的数据合并器传递给它。最后，我们调用训练器的`train()`方法，这样就完成了！模型在仅经过两个epoch后，在验证集上的准确率达到了大约90%：
- en: '[PRE49]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Great, you now know how to download a pretrained model like BERT and fine-tune
    it on your own dataset! But what if you don’t have a dataset at all, and you just
    want to use a pretrained model that was already fine-tuned for sentiment analysis?
    For this, you can use the *pipelines API*.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，你现在知道如何下载像BERT这样的预训练模型并在自己的数据集上进行微调了！但如果你根本没有任何数据集，只想使用已经针对情感分析微调过的预训练模型怎么办？为此，你可以使用*pipelines
    API*。
- en: Hugging Face Pipelines
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face Pipelines
- en: 'The Transformers library provides a very convenient API to download and use
    pretrained pipelines for various tasks. Each pipeline contains a pretrained model
    along with its corresponding preprocessing and post-processing modules. For example
    let’s create a sentiment analysis pipeline and run it on the first 10 IMDb reviews
    in the training set:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers库提供了一个非常方便的API，用于下载和使用各种任务的预训练管道。每个管道都包含一个预训练模型以及相应的预处理和后处理模块。例如，让我们创建一个情感分析管道，并在训练集中的前10条IMDb评论上运行它：
- en: '[PRE50]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Well, it could hardly be any easier, could it? Just create a pipeline by specifying
    the task and the model to use, and a couple of other parameters, depending on
    the task, and off you go! In this example, each review gets a `"POSITIVE"` or
    `"NEGATIVE"` label, along with a score equal to the model’s estimated probability
    for that label. This particular model actually reaches 88.2% accuracy on the validation
    set, which is reasonably good. Here a few points to note:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这几乎不可能更简单了，对吧？只需通过指定任务和要使用的模型以及一些其他参数来创建一个管道，然后就可以开始了！在这个例子中，每条评论都会得到一个`"POSITIVE"`或`"NEGATIVE"`标签，以及一个等于模型对该标签估计概率的分数。这个特定的模型在验证集上的准确率实际上达到了88.2%，这是一个相当不错的成绩。以下是一些需要注意的点：
- en: 'If you don’t specify a model, the `pipeline()` function will use the default
    model for the chosen task. For sentiment analysis, at the time of writing, it’s
    the model we chose: it’s a DistilBERT model—a scaled down version of BERT—with
    an uncased tokenizer, trained on the English Wikipedia and a corpus of English
    books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你没有指定模型，`pipeline()`函数将使用所选任务的默认模型。对于情感分析，在撰写本文时，它是我们选择的模型：这是一个DistilBERT模型——BERT的缩小版——带有非大小写分词器，在英语维基百科和一组英语书籍上训练，并在斯坦福情感树库v2
    (SST 2)任务上进行微调。
- en: The pipeline automatically uses the GPU if you have one. If you have several
    GPUs, you can specify which one to use by setting the pipeline’s `device` argument
    to the GPU index.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有GPU，管道会自动使用它。如果你有多个GPU，你可以通过将管道的`device`参数设置为GPU索引来指定使用哪一个。
- en: The models from the Transformers library are always in evaluation mode by default
    (no need to call `eval()`).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers库中的模型默认总是处于评估模式（无需调用`eval()`）。
- en: The score is for the chosen label, not for the positive class. In particular,
    since this is a binary classification task, the score cannot be lower than 0.5
    (or else the model would have picked the other label).
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数是针对所选标签的，而不是针对正类。特别是，由于这是一个二元分类任务，分数不能低于0.5（否则模型会选择另一个标签）。
- en: The model we chose is well-suited for general-purpose sentiment analysis, such
    as movie reviews, but other models are better suited for specific use cases, such
    as social media posts (e.g., trained on a large dataset of tweets, then fine-tuned
    on a sentiment analysis dataset). To find the best model for your use case, you
    can search the list of available models on the [*Hugging Face Hub*](https://huggingface.co/models).
    However, there are over 80,000 models available in the “text-classification” category
    alone, so you will need to use the filters to narrow down the options. In particular,
    start by filtering on the task, and sort by trending or most-liked models. You
    can also continue to filter by language and dataset, if necessary. Prefer models
    from reputable sources (e.g., models from users huggingface, facebook, google,
    cardiffnlp, and so on), and if the model includes executable code, make absolutely
    sure you trust the user (and if you do, set `trust_remote_code=True` when calling
    the `pipeline()` function).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的是适用于通用情感分析的模型，例如电影评论，但其他模型更适合特定的用例，例如社交媒体帖子（例如，在大型推文数据集上训练，然后在情感分析数据集上进行微调）。为了找到最适合您用例的最佳模型，您可以在[*Hugging
    Face Hub*](https://huggingface.co/models)上搜索可用的模型列表。然而，仅“文本分类”类别中就有超过80,000个模型可供选择，因此您需要使用过滤器来缩小选项范围。特别是，从任务开始过滤，并按趋势或最受欢迎的模型排序。如果需要，您还可以继续按语言和数据集进行过滤。优先选择来自信誉良好的来源的模型（例如，来自huggingface、facebook、google、cardiffnlp等用户的模型），并且如果模型包含可执行代码，请确保您信任该用户（如果您信任，请在调用`pipeline()`函数时设置`trust_remote_code=True`）。
- en: 'There are many text classification tasks other than sentiment analysis. For
    example, a model fine-tuned on the multi-genre natural language inference (MultiNLI)
    dataset can classify a pair of texts (each ending with a separation token [SEP])
    into three classes: contradiction (if the texts contradict each other), entailment
    (if the first text entails the second), or neutral otherwise. For example:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了情感分析之外，还有许多其他文本分类任务。例如，在多体裁自然语言推理（MultiNLI）数据集上微调的模型可以将一对文本（每个文本以分隔标记[SEP]结束）分类为三个类别：矛盾（如果文本相互矛盾）、蕴涵（如果第一个文本蕴涵第二个文本），或者中性。例如：
- en: '[PRE51]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Many other NLP tasks are also available via the pipeline API, such as question
    answering, summarization, sentence similarity, text generation, token classification,
    translation, and more. And it doesn’t stop there! There are also many computer
    vision tasks, such as image classification, image segmentation, object detection,
    image-to-text, text-to-image, depth estimation, and even audio tasks, such as
    audio classification, speech-to-text, text-to-speech, and so on. Make sure to
    check out the full list at [*https://huggingface.co/tasks*](https://huggingface.co/tasks).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 通过管道API还可以通过许多其他NLP任务，例如问答、摘要、句子相似度、文本生成、标记分类、翻译等。而且这还远不止于此！还有许多计算机视觉任务，例如图像分类、图像分割、目标检测、图像到文本、文本到图像、深度估计，甚至音频任务，例如音频分类、语音到文本、文本到语音等。请确保查看[*https://huggingface.co/tasks*](https://huggingface.co/tasks)上的完整列表。
- en: Warning
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Before you download a model, make sure you trust the hosting platform (e.g.,
    the Hugging Face Hub) and the model’s author: the model may contain executable
    code, which could be malicious. It could also produce biased outputs, or it may
    have been trained with copyrighted or sensitive private data which might be leaked
    to your users, or it might even have *poisoned weights* which could make it produce
    harmful content (e.g., propaganda) only for some types of inputs, otherwise behaving
    normally.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载模型之前，请确保您信任托管平台（例如，Hugging Face Hub）和模型的作者：模型可能包含可执行代码，这可能是有害的。它也可能产生有偏见的输出，或者它可能使用受版权保护或敏感的私人数据进行训练，这些数据可能泄露给您的用户，或者它甚至可能包含*有毒权重*，这可能导致它仅对某些类型的输入产生有害内容（例如，宣传），而其他情况下则表现正常。
- en: 'Time to step back. So far we have looked at text generation using a char-RNN,
    and sentiment analysis using various subword tokenization methods, pretrained
    embeddings, and even entire pretrained models. Along the way, we discussed embeddings,
    tokenizers, and the Hugging Face libraries. In the next section, we will explore
    another important NLP task: *neural machine translation* (NMT). Specifically,
    we will build an encoder-decoder model capable of translating English to Spanish,
    and we will see how to boost its performance using beam search and attention mechanisms.
    ¡Vamos!'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候回顾一下了。到目前为止，我们已经探讨了使用char-RNN进行文本生成，以及使用各种子词标记化方法、预训练嵌入甚至整个预训练模型进行情感分析。在这个过程中，我们讨论了嵌入、标记化器和Hugging
    Face库。在下一节中，我们将探索另一个重要的NLP任务：*神经机器翻译*（NMT）。具体来说，我们将构建一个能够将英语翻译成西班牙语的编码器-解码器模型，并了解如何使用束搜索和注意力机制来提高其性能。¡Vamos!
- en: An Encoder-Decoder Network for Neural Machine Translation
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于神经机器翻译的编码器-解码器网络
- en: Let’s begin with a relatively simple [sequence-to-sequence NMT model](https://homl.info/nmtmodel)⁠^([15](ch14.html#id3354))
    that will translate English text to Spanish (see [Figure 14-5](#machine_translation_diagram)).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从相对简单的[序列到序列NMT模型](https://homl.info/nmtmodel)⁠^([15](ch14.html#id3354))开始，该模型将英语文本翻译成西班牙语（见图[图14-5](#machine_translation_diagram)）。
- en: '![Diagram of an encoder-decoder network illustrating a sequence-to-sequence
    model for translating "I like soccer" from English to Spanish as "Me gusta el
    fútbol."](assets/hmls_1405.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![编码器-解码器网络图，展示了将“我喜欢足球”从英语翻译成西班牙语为“我喜欢足球”的序列到序列模型](assets/hmls_1405.png)'
- en: Figure 14-5\. A simple machine translation model
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-5.一个简单的机器翻译模型
- en: 'In short, the architecture is as follows: English texts are fed as inputs to
    the encoder, and the decoder outputs the Spanish translations. Note that the Spanish
    translations are also used as inputs to the decoder during training, but shifted
    back by one step. In other words, during training the decoder is given as input
    the token that it *should* have output at the previous step, regardless of what
    it actually output. This is called *teacher forcing*—a technique that significantly
    speeds up training and improves the model’s performance. For the very first token,
    the decoder is given the start-of-sequence (SoS, a.k.a. beginning-of-sequence,
    BoS) token (`"<s>"`), and the decoder is expected to end the text with an end-of-sequence
    (EoS) token (`"</s>"`).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，架构如下：英语文本作为输入传递给编码器，解码器输出西班牙语翻译。请注意，在训练期间，西班牙语翻译也被用作解码器的输入，但向后移动了一步。换句话说，在训练期间，解码器被提供的输入是它在前一步应该输出的标记，而不考虑它实际上输出了什么。这被称为*教师强制*——一种显著加快训练速度并提高模型性能的技术。对于第一个标记，解码器被提供起始序列（SoS，也称为序列开始，BoS）标记（`"<s>"`），并且解码器被期望以结束序列（EoS）标记（`"</s>"`）结束文本。
- en: Each token is initially represented by its ID (e.g., `4553` for the token “soccer”).
    Next, an `nn.Embedding` layer returns the token embedding. These token embeddings
    are then fed to the encoder and the decoder.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记最初由其ID表示（例如，标记“足球”的ID为`4553`）。接下来，一个`nn.Embedding`层返回标记嵌入。这些标记嵌入随后被输入到编码器和解码器中。
- en: At each step, the decoder’s dense output layer (i.e., an `nn.Linear` layer)
    outputs a logit score for each token in the output vocabulary (i.e., Spanish).
    If you pass these logits through the softmax function, you get an estimated probability
    for each possible token. For example, at the first step the word “Me” may have
    a probability of 7%, “Yo” may have a probability of 1%, and so on. This is very
    much like a regular classification task, and indeed we will train the model using
    the `nn.CrossEntropyLoss`, much like we did in the char-RNN model.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步，解码器的密集输出层（即一个`nn.Linear`层）为输出词汇表中的每个标记（即西班牙语）输出一个logit分数。如果你将这些logit通过softmax函数传递，你会得到每个可能标记的估计概率。例如，在第一步，单词“我”可能有一个7%的概率，“你”可能有一个1%的概率，依此类推。这非常像一项常规的分类任务，实际上，我们将使用`nn.CrossEntropyLoss`来训练模型，就像我们在char-RNN模型中所做的那样。
- en: Note that at inference time (after training), you will not have the target text
    to feed to the decoder. Instead, you need to feed it the word that it has just
    output at the previous step, as shown in [Figure 14-6](#inference_decoder_diagram)
    (this will require an embedding lookup that is not shown in the diagram).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在推理时间（训练后），你将没有目标文本可以输入到解码器中。相反，你需要输入它在前一步刚刚输出的单词，如图[图14-6](#inference_decoder_diagram)所示（这需要嵌入查找，图中未显示）。
- en: '![Diagram showing a sequence of recurrent neural network cells where each cell
    outputs a token that is fed as input to the next cell during inference.](assets/hmls_1406.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![展示序列循环神经网络细胞图的示意图，其中每个细胞在推理期间输出一个标记，该标记作为下一个细胞的输入。](assets/hmls_1406.png)'
- en: Figure 14-6\. At inference time, the decoder is fed as input the word it just
    output at the previous time step
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-6。在推理时间，解码器将前一个时间步输出的单词作为输入
- en: Tip
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: In a [2015 paper](https://homl.info/scheduledsampling),⁠^([16](ch14.html#id3359))
    Samy Bengio et al. proposed gradually switching from feeding the decoder the previous
    *target* token to feeding it the previous *output* token during training.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇[2015年的论文](https://homl.info/scheduledsampling)⁠^([16](ch14.html#id3359))中，Samy
    Bengio等人提出，在训练过程中逐渐从向解码器提供前一个*目标*标记切换到提供前一个*输出*标记。
- en: 'Let’s build and train this model! First, we need to download a dataset of English/Spanish
    text pairs. For this, we will use the Datasets library to download English/Spanish
    pairs from the *Tatoeba Challenge* dataset. The [Tatoeba project](https://tatoeba.org)
    is a language-learning initiative started in 2006 by Trang Ho, where contributors
    have created a huge collection of text pairs from many languages. The Tatoeba
    Challenge dataset was created by researchers from the University of Helsinki to
    benchmark machine translation systems, using data extracted from the Tatoeba project.
    The training set is quite large so we will use the validation set as our training
    set, setting aside 20% for validation. We will also download the test set:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建和训练这个模型！首先，我们需要下载一个英语/西班牙语文本对的语料库。为此，我们将使用Dataset库从*Tatoeba Challenge*语料库下载英语/西班牙语文本对。The
    [Tatoeba项目](https://tatoeba.org)是由Trang Ho于2006年启动的一个语言学习倡议，贡献者创建了一个包含许多语言文本对的巨大集合。Tatoeba
    Challenge语料库是由赫尔辛基大学的研究人员创建的，用于评估机器翻译系统，使用从Tatoeba项目提取的数据。训练集相当大，因此我们将使用验证集作为我们的训练集，留出20%用于验证。我们还将下载测试集：
- en: '[PRE52]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Each sample in the dataset is a dictionary containing an English text along
    with its Spanish translation. For example:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每个样本都是一个包含英语文本及其西班牙语翻译的字典。例如：
- en: '[PRE53]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We will need to tokenize this text. We could use a different tokenizer for
    English and Spanish, but these two languages have many words in common (e.g.,
    animal, color, hotel, hospital, idea, radio, motor), and many similar subwords
    (e.g., pre, auto, inter, uni), so it makes sense to use a common tokenizer. Let’s
    train a BPE tokenizer on all the training text, both English and Spanish:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对这个文本进行分词。我们可以为英语和西班牙语使用不同的分词器，但这两种语言有很多共同词汇（例如，animal, color, hotel, hospital,
    idea, radio, motor），以及许多类似的子词（例如，pre, auto, inter, uni），因此使用一个共同的分词器是有意义的。让我们在所有训练文本上训练一个BPE分词器，包括英语和西班牙语：
- en: '[PRE54]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let’s test this tokenizer:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试这个分词器：
- en: '[PRE55]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Perfect! Now let’s create a small utility class that will hold tokenized English
    texts (i.e., the *source* token ID sequences), along with the corresponding tokenized
    Spanish targets (i.e., the *target* token ID sequences), plus the corresponding
    attention masks. For this, we can create a `namedtuple` base class (i.e., a tuple
    with named fields), and extend it to add a `to()` method, which will make it easy
    to move all these tensors to the GPU:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！现在让我们创建一个小的实用类，它将包含分词后的英语文本（即*源*标记ID序列），以及相应的分词后的西班牙语目标（即*目标*标记ID序列），再加上相应的注意力掩码。为此，我们可以创建一个`namedtuple`基类（即具有命名字段的元组），并扩展它以添加一个`to()`方法，这将使将所有这些张量移动到GPU变得容易：
- en: '[PRE56]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, let’s create the data loaders:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建数据加载器：
- en: '[PRE57]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The `nmt_collate_fn()` function starts by extracting all the English and Spanish
    texts from the given batch. In the process, it also adds an SoS token at the start
    of each Spanish text, as well as an EoS token at the end. It then tokenizes both
    the English and Spanish texts using our BPE tokenizer. Next, the input sequences
    and the attention masks are converted to tensors and wrapped in an `NmtPair`.
    Importantly, the function drops the EoS token from the decoder inputs, and drops
    the SoS token from the decoder targets. For example, the inputs may contain the
    token IDs for “<s> Me gusta el fútbol”, while the targets may contain the token
    IDs for “Me gusta el fútbol </s>”. Lastly, the function returns the inputs (i.e.,
    the `NmtPair`) along with the targets. Then we just create the data loaders as
    usual.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`nmt_collate_fn()`函数首先从给定的批次中提取所有英语和西班牙语文本。在这个过程中，它还在每个西班牙语文本的开头添加一个SoS标记，以及在结尾添加一个EoS标记。然后，它使用我们的BPE标记器对英语和西班牙语文本进行标记化。接下来，将输入序列和注意力掩码转换为张量，并包装在`NmtPair`中。重要的是，该函数从解码器输入中删除EoS标记，并从解码器目标中删除SoS标记。例如，输入可能包含“<s>
    Me gusta el fútbol”的标记ID，而目标可能包含“Me gusta el fútbol </s>'
- en: 'And now we are ready to build our translation model. It’s just like [Figure 14-5](#machine_translation_diagram),
    except the encoder and decoder share the same `nn.Embedding` layer, and the encoder
    and decoder `nn.GRU` modules contain two layers each:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好构建我们的翻译模型。它就像[图14-5](#machine_translation_diagram)一样，只是编码器和解码器共享相同的`nn.Embedding`层，编码器和解码器的`nn.GRU`模块各包含两层：
- en: '[PRE58]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Almost everything in this model should look familiar: it’s very similar to
    our previous models. We create the modules in the constructor, then the `forward()`
    method embeds the input sequences (both English and Spanish), it packs the English
    embeddings and passes them through the encoder, then it passes the Spanish embeddings
    to the decoder, along with the encoder’s last hidden states (across all `nn.GRU`
    layers). Lastly, the decoder’s outputs are passed through the output `nn.Linear`
    layer, and the final outputs are permuted to ensure that the class dimension (containing
    the token logits) is the second dimension, since this is expected by the `nn.CrossEntropyLoss`
    and the `Accuracy` metric, as we saw earlier.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型中的几乎所有内容都应该看起来很熟悉：它与我们的先前模型非常相似。我们在构造函数中创建模块，然后`forward()`方法将输入序列（英语和西班牙语）嵌入，打包英语嵌入并通过编码器传递，然后它将西班牙语嵌入传递给解码器，同时传递编码器的最后一个隐藏状态（所有`nn.GRU`层）。最后，解码器的输出通过输出`nn.Linear`层传递，最终输出通过重新排列以确保类别维度（包含标记对数）是第二个维度，因为`nn.CrossEntropyLoss`和`Accuracy`度量标准期望如此，正如我们之前所看到的。
- en: Note
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The most common metric used in NMT is the *bilingual evaluation understudy*
    (BLEU) score, which compares each translation produced by the model with several
    good translations produced by humans. It counts the number of *n*-grams (sequences
    of *n* words) that appear in any of the target translations and adjusts the score
    to take into account the frequency of the produced *n*-grams in the target translations.
    It is implemented by TorchMetric’s `BLEUScore` class.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在NMT中最常用的度量标准是*bilingual evaluation understudy*（BLEU）分数，它将模型生成的每个翻译与人类生成的几个良好翻译进行比较。它计算出现在任何目标翻译中的*n*-gram（*n*个单词的序列）的数量，并调整分数以考虑生成的*n*-gram在目标翻译中的频率。它通过TorchMetric的`BLEUScore`类实现。
- en: 'We could have packed the Spanish embeddings, but then the decoder’s outputs
    would have been packed sequences, which we would have had to pad before we passed
    them to the output layer. We avoided this complexity because we can just configure
    the loss to ignore the output tokens when the targets are padding tokens, like
    this:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以打包西班牙语嵌入，但那样的话，解码器的输出将是打包的序列，我们得在将它们传递到输出层之前填充它们。我们避免了这种复杂性，因为我们只需配置损失函数，使其在目标为填充标记时忽略输出标记，如下所示：
- en: '[PRE59]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Now you can train this model (e.g., for 10 epochs using a `Nadam` optimizer
    with `lr = 0.001`), and it will take quite a while. It’s actually not that long
    when you consider the fact that the model is learning two languages at once!
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以训练这个模型（例如，使用`Nadam`优化器，`lr = 0.001`进行10个epoch），这将花费相当长的时间。实际上，当你考虑到模型一次学习两种语言的事实时，它并不长！
- en: 'While it’s training, let’s write a little helper function to translate some
    English text to Spanish using our model. It will start by calling the model with
    the English text for the encoder, and a single SoS token for the decoder. The
    decoder will just output logits for the first token in the translation. Our function
    will then pick the most likely token (i.e., with the highest logit) and add it
    to the decoder inputs, then it will call the model again to get the next token.
    It will repeat this process, adding one token at a time, until the model outputs
    an EoS token:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，让我们编写一个小的辅助函数，使用我们的模型将一些英语文本翻译成西班牙语。它将首先使用英语文本调用编码器，并为解码器提供一个单个SoS标记。解码器将只为翻译中的第一个标记输出logits。然后我们的函数将选择最可能的标记（即具有最高logit的标记），并将其添加到解码器输入中，然后再次调用模型以获取下一个标记。它将重复此过程，每次添加一个标记，直到模型输出一个EoS标记：
- en: '[PRE60]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This implementation works but it’s not optimized at all. We could run the encoder
    just once on the English text, and we could also run the decoder just once per
    time step, instead of running it over the whole growing text at each iteration.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现方法是可行的，但它根本就没有优化。我们可以在英语文本上只运行一次编码器，也可以在每个时间步只运行一次解码器，而不是在每次迭代中在整个增长文本上运行它。
- en: Let’s try translating some text!
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试翻译一些文本！
- en: '[PRE61]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Hurray, it works! We just built a model from scratch that can translate English
    to Spanish.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 哈哈，成功了！我们从头开始构建了一个模型，可以将英语翻译成西班牙语。
- en: 'If you play around with our translation model, you will find that it often
    works reasonably well on short text, but it really struggles with longer sentences.
    For example:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你玩我们的翻译模型，你会发现它在处理短文本时通常表现良好，但处理长句时却真的很困难。例如：
- en: '[PRE62]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The translation says “I like to play with my friends”. Oops, there’s no mention
    of soccer. So how can we improve this model? One way is to increase the training
    set size and add more `nn.GRU` layers in both the encoder and the decoder. You
    could also make the encoder bidirectional (but not the decoder, or else it would
    no longer be causal and it would see the full translation at each time step, instead
    of just the previous tokens). Another popular technique that can greatly improve
    the performance of a translation model at inference time is *beam search*.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译说“我喜欢和我的朋友们一起玩”。哎呀，没有提到足球。那么我们如何改进这个模型呢？一种方法是通过增加训练集的大小，并在编码器和解码器中添加更多的`nn.GRU`层。你还可以使编码器双向（但不是解码器，否则它就不再是因果的，它会在每个时间步看到完整的翻译，而不是仅仅看到之前的标记）。另一种可以极大地提高翻译模型在推理时性能的技术是*带宽搜索*。
- en: Beam Search
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带宽搜索
- en: To translate an English text to Spanish, we call our model several times, producing
    one word at a time. Unfortunately, this means that when the model makes one mistake,
    it is stuck with it for the rest of the translation, which can cause more errors,
    making the translation worse and worse. For example, suppose we want to translate
    “I like soccer”, and the model correctly starts with “Me”, but then predicts “gustan”
    (plural) instead of “gusta” (singular). This mistake is understandable, since
    “Me gustan” is the correct way to start translating “I like” in many cases. Once
    the model has made this mistake, it is stuck with “gustan”. It then reasonably
    adds “los”, which is the plural for “the”. But since the model never saw “los
    fútbol” in the training data (soccer is singular, not plural), the model tries
    to find something reasonable to add, and given the context it adds “jugadores”,
    which means “the players”. So “I like soccer” gets translated to “I like the players”.
    One error caused a chain of errors.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 要将英语文本翻译成西班牙语，我们需要多次调用我们的模型，每次生成一个单词。不幸的是，这意味着当模型犯了一个错误时，它会一直带着这个错误完成整个翻译，这可能会导致更多的错误，从而使翻译越来越差。例如，假设我们想要翻译“我喜欢足球”，模型正确地以“我”开始，但随后预测的是“gustan”（复数）而不是“gusta”（单数）。这个错误是可以理解的，因为“Me
    gustan”在很多情况下是翻译“I like”的正确方式。一旦模型犯了这个错误，它就会一直使用“gustan”。然后它合理地添加了“los”，这是“the”的复数形式。但由于模型从未在训练数据中看到“los
    fútbol”（足球是单数，不是复数），模型试图找到一些合理的东西来添加，并且根据上下文，它添加了“jugadores”，这意味着“球员”。所以“我喜欢足球”被翻译成了“I
    like the players”。一个错误引发了一系列错误。
- en: 'How can we give the model a chance to go back and fix mistakes it made earlier?
    One of the most common solutions is *beam search*: it keeps track of a short list
    of the *k* most promising output sequences (say, the top three), and at each decoder
    step it tries to extend each of them by one word, keeping only the *k* most likely
    sequences. The parameter *k* is called the *beam width*.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何给模型一个机会去纠正它之前犯的错误？最常用的解决方案之一是**光束搜索**：它跟踪一个包含k个最有希望的输出序列的短列表（比如，前三个），在每次解码步骤中，它尝试通过一个单词扩展每个序列，并只保留k个最可能的序列。参数k被称为**光束宽度**。
- en: For example, suppose you use the model to translate the sentence “I like soccer”
    using beam search with a beam width of three (see [Figure 14-7](#beam_search_diagram)).
    At the first decoder step, the model will output an estimated probability for
    each possible first word in the translated sentence. Suppose the top three words
    are “Me” (75% estimated probability), “a” (3%), and “como” (1%). That’s our short
    list so far. Next, we use the model to find the next word for each sentence. For
    the first sentence (“Me”), perhaps the model outputs a probability of 36% for
    the word “gustan”, 32% for the word “gusta”, 16% for the word “encanta”, and so
    on. Note that these are actually *conditional* probabilities, given that the sentence
    starts with “Me”. For the second sentence (“a”), the model might output a conditional
    probability of 50% for the word “mi”, and so on. Assuming the vocabulary has 10,000
    tokens, we will end up with 10,000 probabilities per sentence.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你使用该模型，通过具有三个宽度光束的光束搜索来翻译句子“我喜欢足球”（参见图14-7）。在第一个解码步骤中，模型将为翻译句子中每个可能的首个单词输出一个估计概率。假设前三个单词是“Me”（估计概率为75%），“a”（3%）和“como”（1%）。这就是我们到目前为止的短列表。接下来，我们使用模型为每个句子找到下一个单词。对于第一个句子（“Me”），模型可能为单词“gustan”输出36%的概率，为单词“gusta”输出32%的概率，为单词“encanta”输出16%，等等。请注意，这些实际上是**条件**概率，给定句子以“Me”开头。对于第二个句子（“a”），模型可能为单词“mi”输出50%的条件概率，等等。假设词汇表有10,000个标记，我们将为每个句子得到10,000个概率。
- en: 'Next, we compute the probabilities of each of the 30,000 two-token sentences
    we considered (3 × 10,000). We do this by multiplying the estimated conditional
    probability of each word by the estimated probability of the sentence it completes.
    For example, the estimated probability of the sentence “Me” was 75%, while the
    estimated conditional probability of the word “gustan” (given that the first word
    is “Me”) was 36%, so the estimated probability of the sentence “Me gustan” is
    75% × 36% = 27%. After computing the probabilities of all 30,000 two-word sentences,
    we keep only the top 3\. In this example they all start with the word “Me”: “Me
    gustan” (27%), “Me gusta” (24%), and “Me encanta” (12%). Right now, the sentence
    “Me gustan” is winning, but “Me gusta” has not been eliminated.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算我们考虑的30,000个双词句子的概率（3 × 10,000）。我们通过将每个单词的估计条件概率乘以完成该句子的估计概率来完成这项工作。例如，句子“Me”的估计概率为75%，而单词“gustan”（在第一个单词是“Me”的情况下）的估计条件概率为36%，因此句子“Me
    gustan”的估计概率为75% × 36% = 27%。在计算所有30,000个双词句子的概率后，我们只保留前三个。在这个例子中，它们都以单词“Me”开头：“Me
    gustan”（27%），“Me gusta”（24%）和“Me encanta”（12%）。目前，“Me gustan”领先，但“Me gusta”尚未被淘汰。
- en: '![Diagram illustrating a beam search process with a beam width of three, showing
    probabilities for Spanish sentence constructions starting with "Me" and progressing
    through different stages.](assets/hmls_1407.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![展示具有三个宽度光束的搜索过程图，显示以“Me”开头并经过不同阶段的西班牙语句子构造的概率](assets/hmls_1407.png)'
- en: Figure 14-7\. Beam search, with a beam width of three
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-7. 具有三个宽度光束的搜索
- en: 'Then we repeat the same process: we use the model to predict the next word
    in each of these three sentences, and we compute the probabilities of all 30,000
    three-word sentences we considered. Perhaps the top 3 are now “Me gustan los”
    (10%), “Me gusta el” (8%), and “Me gusta mucho” (2%). At the next step we may
    get “Me gusta el fútbol” (6%), “Me gusta mucho el” (1%), and “Me gusta el deporte”
    (0.2%). Notice that “Me gustan” was eliminated, and the correct translation is
    now ahead. We boosted our encoder-decoder model’s performance without any extra
    training, simply by using it more wisely.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重复同样的过程：我们使用模型预测这三个句子中的每一个句子中的下一个单词，并计算我们考虑的30,000个三词句子的概率。也许现在排名前三的是“Me
    gustan los”（10%）、“Me gusta el”（8%）和“Me gusta mucho”（2%）。在下一步中，我们可能会得到“Me gusta
    el fútbol”（6%）、“Me gusta mucho el”（1%）和“Me gusta el deporte”（0.2%）。请注意，“Me gustan”已被排除，正确的翻译现在就在眼前。我们通过更明智地使用模型，而没有进行任何额外的训练，就提高了编码器-解码器模型的表现。
- en: 'The notebook for this chapter contains a very simple `beam_search()` function,
    if you’re interested, but in general you will probably want to use the implementation
    provided by the `GenerationMixin` class in the Transformers library. This is where
    the text generation models from the Transformers library get their `generate()`
    method: it accepts a `num_beams` argument which you can set to the desired beam
    width if you want to use beam search. It also provides a `do_sample` argument
    that will randomly sample the next token using the probability distribution output
    by the model, just like we did earlier with our char-RNN model. Other generation
    strategies are also supported and can be combined (see [*https://homl.info/hfgen*](https://homl.info/hfgen)
    for more details).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的笔记本包含一个非常简单的`beam_search()`函数，如果你感兴趣的话，但通常你可能会想使用Transformers库中`GenerationMixin`类提供的实现。这就是Transformers库中的文本生成模型获得`generate()`方法的地方：它接受一个`num_beams`参数，你可以将其设置为所需的beam宽度，如果你想使用beam搜索。它还提供了一个`do_sample`参数，它将使用模型输出的概率分布随机采样下一个标记，就像我们之前在char-RNN模型中做的那样。还支持其他生成策略，并且可以组合使用（更多详情请见[*https://homl.info/hfgen*](https://homl.info/hfgen)）。
- en: 'With all this, you can get reasonably good translations for fairly short sentences.
    For example, the following translation is correct:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，你可以为相当短的句子获得相当好的翻译。例如，以下翻译是正确的：
- en: '[PRE63]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Unfortunately, this model will still be pretty bad at translating long sentences:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个模型在翻译长句子方面仍然会很糟糕：
- en: '[PRE64]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: This translates to “I like to play with play with the players of the beach”.
    That’s not quite right. Once again, the problem comes from the limited short-term
    memory of RNNs. *Attention mechanisms* are the game-changing innovation that addressed
    this problem.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这翻译成“我喜欢和海滩上的玩家一起玩”。这并不完全正确。再次强调，问题源于RNNs有限的短期记忆。*注意力机制*是解决这一问题的颠覆性创新。
- en: Attention Mechanisms
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制
- en: 'Consider the path from the word “soccer” to its translation “fútbol” back in
    [Figure 14-5](#machine_translation_diagram): it is quite long! This means that
    a representation of this word (along with all the other words) needs to be carried
    over many steps before it is actually used. Can’t we make this path shorter?'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑从单词“soccer”到其翻译“fútbol”的路径，如[图14-5](#machine_translation_diagram)所示：它相当长！这意味着这个单词（以及所有其他单词）的代表需要在实际使用之前跨越许多步骤。我们能不能使这条路径更短？
- en: This was the core idea in a landmark [2014 paper](https://homl.info/attention)⁠^([20](ch14.html#id3394))
    by Dzmitry Bahdanau et al., where the authors introduced a technique that allowed
    the decoder to focus on the appropriate words (as encoded by the encoder) at each
    time step. For example, at the time step where the decoder needs to output the
    word “fútbol”, it will focus its attention on the word “soccer”. This means that
    the path from an input word to its translation is now much shorter, so the short-term
    memory limitations of RNNs have much less impact. Attention mechanisms revolutionized
    neural machine translation (and deep learning in general), allowing a significant
    improvement in the state of the art, especially for long sentences (e.g., over
    30 words).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Dzmitry Bahdanau等人于2014年发表的一篇里程碑式[论文](https://homl.info/attention)⁠^([20](ch14.html#id3394))的核心思想，其中作者介绍了一种技术，允许解码器在每个时间步集中关注由编码器编码的适当单词。例如，在解码器需要输出单词“fútbol”的时间步，它将集中关注单词“soccer”。这意味着从输入单词到其翻译的路径现在要短得多，因此RNNs的短期记忆限制的影响要小得多。注意力机制彻底改变了神经机器翻译（以及深度学习），特别是在长句子（例如，超过30个单词）方面，显著提高了当前的最佳水平。
- en: '[Figure 14-8](#attention_diagram) shows our encoder-decoder model with an added
    attention mechanism:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-8](#attention_diagram)显示了我们的编码器-解码器模型，并添加了注意力机制：'
- en: On the left, you have the encoder and the decoder (I’ve made the encoder bidirectional
    in this figure, as it’s generally a good idea).
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在左侧，你有编码器和解码器（我在这个图中将编码器设置为双向的，因为这通常是一个好主意）。
- en: Instead of sending the encoder’s final hidden state to the decoder, as well
    as the previous target word at each time step (which is still done, but it is
    not shown in the figure), we now send all of the encoder’s outputs to the decoder
    as well.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在将所有编码器的输出发送到解码器，而不是像之前那样将编码器的最终隐藏状态以及每个时间步的先前目标单词发送到解码器（这仍然会做，但在图中没有显示）。
- en: 'Since the decoder cannot deal with all these encoder outputs at once, they
    need to be aggregated: at each time step, the decoder’s memory cell computes a
    weighted sum of all the encoder outputs. This determines which words the decoder
    will focus on at this step.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于解码器不能一次处理所有这些编码器输出，因此它们需要被聚合：在每个时间步，解码器的记忆单元计算所有编码器输出的加权总和。这决定了解码器在这个步骤将关注哪些单词。
- en: 'The weight *α*[(*t*,*i*)] is the weight of the *i*^(th) encoder output at the
    *t*^(th) decoder time step. For example, if the weight *α*[(3,2)] is much larger
    than the weights *α*[(3,0)] and *α*[(3,1)], then the decoder will pay much more
    attention to the encoder’s output for word #2 (“soccer”) than to the other two
    outputs, at least at this time step.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '权重 *α*[(*t*,*i*)] 是第 *i* 个编码器输出在第 *t* 个解码器时间步的权重。例如，如果权重 *α*[(3,2)] 远大于权重 *α*[(3,0)]
    和 *α*[(3,1)]，那么解码器将更多地关注编码器的输出对于单词 #2（“soccer”）而不是其他两个输出，至少在这个时间步。'
- en: 'The rest of the decoder works just like earlier: at each time step the memory
    cell receives the inputs we just discussed, plus the hidden state from the previous
    time step, and finally (although it is not represented in the diagram) it receives
    the target word from the previous time step (or at inference time, the output
    from the previous time step).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的其余部分工作方式与之前相同：在每个时间步，记忆单元接收我们刚才讨论的输入，加上前一个时间步的隐藏状态，最后（尽管在图中没有表示）它接收前一个时间步的目标单词（或在推理时间，前一个时间步的输出）。
- en: '![Diagram illustrating a neural machine translation model using an encoder-decoder
    network with attention, showing how encoder outputs are weighted and aggregated
    by the alignment model.](assets/hmls_1408.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用注意力机制的编码器-解码器网络进行神经机器翻译的图解，显示编码器的输出是如何通过对齐模型加权并聚合的](assets/hmls_1408.png)'
- en: Figure 14-8\. Neural machine translation using an encoder-decoder network with
    an attention model
  id: totrans-337
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-8\. 使用注意力模型的编码器-解码器网络进行神经机器翻译
- en: 'But where do these *α*[(*t*,*i*)] weights come from? Well, they are generated
    by a small neural network called an *alignment model* (or an *attention layer*),
    which is trained jointly with the rest of the encoder-decoder model. This alignment
    model is illustrated on the righthand side of [Figure 14-8](#attention_diagram):'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些 *α*[(*t*,*i*)] 权重从何而来？嗯，它们是由一个小型神经网络生成的，称为 *对齐模型*（或 *注意力层*），该模型与编码器-解码器模型的其余部分一起训练。这个对齐模型在[图14-8](#attention_diagram)的右侧进行了说明：
- en: It starts with a dense layer (i.e., `nn.Linear`) that takes as input each of
    the encoder’s outputs, along with the decoder’s previous hidden state (e.g., **h**[(2)]),
    and outputs a score (or energy) for each encoder output (e.g., *e*[(3,] [2)]).
    This score measures how well each encoder output is aligned with the decoder’s
    previous hidden state.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从一个密集层（即`nn.Linear`）开始，该层接受编码器的每个输出，以及解码器的上一个隐藏状态（例如，**h**[(2)]），并输出每个编码器输出的分数（或能量）（例如，*e*[(3,]
    [2)]）。这个分数衡量每个编码器输出与解码器的上一个隐藏状态对齐的程度。
- en: For example, in [Figure 14-8](#attention_diagram), the model has already output
    “me gusta el” (meaning “I like”), so it’s now expecting a noun. The word “soccer”
    is the one that best aligns with the current state, so it gets a high score.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，在[图14-8](#attention_diagram)中，模型已经输出了“me gusta el”（意味着“我喜欢”），所以它现在期待一个名词。单词“soccer”是与当前状态最佳对齐的单词，因此它得到了高分。
- en: Finally, all the scores go through a softmax layer to get a final weight for
    each encoder output (e.g., *α*[(3,2)]). All the weights for a given decoder time
    step add up to 1.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，所有分数都通过softmax层得到每个编码器输出的最终权重（例如，*α*[(3,2)]）。给定解码器时间步的所有权重加起来等于1。
- en: This particular attention mechanism is called *Bahdanau attention* (named after
    the 2014 paper’s first author). Since it concatenates the encoder output with
    the decoder’s previous hidden state, it is sometimes called *concatenative attention*
    (or *additive attention*).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特定的注意力机制被称为*Bahdanau注意力*（以2014年论文的第一作者命名）。由于它将编码器的输出与解码器的先前隐藏状态连接起来，有时也称为*连接注意力*（或*加性注意力*）。
- en: 'In short, the attention mechanism provides a way to focus the attention of
    the model on part of the inputs. That said, there’s another way to think of this
    whole process: it acts as a differentiable memory retrieval mechanism. For example,
    let’s suppose the encoder analyzed the input sentence “I like soccer”, and it
    managed to understand that the word “I” is the subject, the word “like” is the
    verb, and the word “soccer” is the noun, so it encoded this information in its
    outputs for these words. Now suppose the decoder has already translated “I like”,
    and it thinks that it should translate the noun next. For this, it needs to fetch
    the noun from the input sentence. This is analogous to a dictionary lookup: it’s
    as if the encoder had created a dictionary {"subject”: “I”, “verb”: “like”, “noun”:
    “soccer"} and the decoder wanted to look up the value that corresponds to the
    key “noun”.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '简而言之，注意力机制提供了一种让模型关注输入部分的方法。尽管如此，还有另一种思考整个过程的方式：它充当一个可微的记忆检索机制。例如，假设编码器分析了输入句子“我喜欢足球”，并且它设法理解“我”是主语，“喜欢”是动词，“足球”是名词，因此它将这些信息编码在输出中。现在假设解码器已经翻译了“我喜欢”，并且它认为接下来应该翻译名词。为此，它需要从输入句子中检索名词。这类似于字典查找：就好像编码器创建了一个字典`{"subject":
    "I", "verb": "like", "noun": "soccer"}`，而解码器想要查找与键“noun”对应的值。'
- en: 'However, the model does not have discrete tokens to represent the keys (like
    “subject”, “verb”, or “noun”); instead, it has vectorized representations of these
    concepts that it learned during training, so the query it will use for the lookup
    will not perfectly match any key in the dictionary. One solution is to compute
    a similarity measure between the query and each key in the dictionary, and then
    use the softmax function to convert these similarity scores to weights that add
    up to 1\. As we just saw, that’s exactly what the attention layer does. If the
    key that represents the noun is by far the most similar to the query, then that
    key’s weight will be close to 1\. Next, the attention layer computes a weighted
    sum of the corresponding values: if the weight of the “noun” key is close to 1,
    then the weighted sum will be very close to the representation of the word “soccer”.
    In short, the decoder queried for a noun and the attention mechanism retrieved
    it.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该模型没有离散的标记来表示键（如“主语”、“动词”或“名词”）；相反，它具有在训练期间学习到的这些概念的向量表示，因此它将用于查找的查询将不会与字典中的任何键完美匹配。一种解决方案是计算查询与字典中每个键之间的相似度度量，然后使用softmax函数将这些相似度分数转换为加起来为1的权重。正如我们刚才看到的，这正是注意力层所做的事情。如果表示名词的键与查询最为相似，那么该键的权重将接近1。接下来，注意力层计算相应的值的加权总和：如果“名词”键的权重接近1，那么加权总和将非常接近单词“足球”的表示。简而言之，解码器查询了一个名词，而注意力机制检索到了它。
- en: In most modern implementations of attention mechanisms, the arguments are named
    `query`, `key`, and `value`. In our example, the query is the decoder’s hidden
    states, the key is the encoder’s outputs (this is used to compute the weights),
    and the value is also the encoder’s outputs (this is used to compute the final
    weighted sum).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数现代注意力机制的实现中，参数被命名为`query`、`key`和`value`。在我们的例子中，查询是解码器的隐藏状态，键是编码器的输出（用于计算权重），值也是编码器的输出（用于计算最终的加权总和）。
- en: Note
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If the input sentence is *n* words long, and assuming the output sentence is
    about as long, then the attention mechanism will need to compute about *n*² weights.
    This quadratic computational complexity becomes untractable when the sentences
    are too long.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入句子有*n*个单词长，并且假设输出句子长度大致相同，那么注意力机制将需要计算大约*n*²个权重。当句子过长时，这种二次计算复杂度变得无法处理。
- en: Another common attention mechanism, known as *Luong attention* or *multiplicative
    attention*, was proposed shortly after, in [2015](https://homl.info/luongattention),⁠^([21](ch14.html#id3403))
    by Minh-Thang Luong et al. Because the goal of the alignment model is to measure
    the similarity between one of the encoder’s outputs and the decoder’s previous
    hidden state, the authors proposed to simply compute the dot product (see [Chapter 4](ch04.html#linear_models_chapter))
    of these two vectors, as this is often a fairly good similarity measure, and modern
    hardware can compute it very efficiently. For this to be possible, both vectors
    must have the same dimensionality. The dot product gives a score, and all the
    scores (at a given decoder time step) go through a softmax layer to give the final
    weights, just like in Bahdanau attention.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的注意力机制，被称为 *Luong 注意力* 或 *乘性注意力*，在 [2015](https://homl.info/luongattention)⁠^([21](ch14.html#id3403))
    年由 Minh-Thang Luong 等人提出。由于对齐模型的目标是测量编码器输出之一与解码器前一个隐藏状态之间的相似度，因此作者们建议简单地计算这两个向量的点积（参见[第
    4 章](ch04.html#linear_models_chapter)），因为这通常是一个相当好的相似度度量，而现代硬件可以非常高效地计算它。为了实现这一点，这两个向量必须具有相同的维度。点积给出一个分数，所有分数（在给定的解码器时间步长）都通过
    softmax 层来给出最终的权重，就像在 Bahdanau 注意力中一样。
- en: Luong et al. also proposed to use the decoder’s hidden state at the current
    time step rather than at the previous time step (i.e., **h**[(*t*)] rather than
    **h**[(*t*–1)]) to compute the attention vector (denoted <msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>).
    This attention vector is then concatenated with the decoder’s hidden state to
    form an attentional hidden state, which is then used to predict the next token.
    This simplifies and speeds up the process by allowing the encoder and decoder
    to operate independently before attention is applied, rather than interweaving
    attention into the decoder’s recurrence.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Luong 等人还建议使用当前时间步的解码器隐藏状态而不是前一个时间步的隐藏状态（即，**h**[(*t*)] 而不是 **h**[(*t*–1)])
    来计算注意力向量（表示为 <msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>）。然后，这个注意力向量与解码器的隐藏状态连接起来形成一个注意力隐藏状态，然后用于预测下一个标记。这通过允许在应用注意力之前编码器和解码器独立操作，而不是将注意力交织到解码器的递归中，从而简化并加快了过程。
- en: The researchers also proposed a variant of the dot product mechanism where the
    encoder outputs first go through a fully connected layer (without a bias term)
    before the dot products are computed. This is called the “general” dot product
    approach. The researchers compared both dot product approaches with the concatenative
    attention mechanism (adding a rescaling parameter vector **v**), and they observed
    that the dot product variants performed better than concatenative attention. For
    this reason, concatenative attention is much less used now. The equations for
    these three attention mechanisms are summarized in [Equation 14-2](#attention_mechanisms_equation).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还提出了一种点积机制的变体，其中编码器的输出首先通过一个全连接层（没有偏置项）然后再计算点积。这被称为“通用”点积方法。研究人员将这两种点积方法与串联注意力机制（添加一个缩放参数向量
    **v**）进行了比较，并观察到点积变体比串联注意力表现更好。因此，现在串联注意力使用得很少。这三个注意力机制的方程式总结在[方程式 14-2](#attention_mechanisms_equation)中。
- en: Equation 14-2\. Attention mechanisms
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 14-2\. 注意力机制
- en: <msub><mover><mi mathvariant="bold">h</mi><mo>~</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>)</mo></mrow></msub><msub><mover><mi
    mathvariant="bold">y</mi><mo stretchy="false" style="math-style:normal;math-depth:0;">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mtext> with </mtext><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>)</mo></mrow></msub></mfenced></mrow><mrow><mstyle
    displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>'</mo></mrow></munder></mstyle><mi>exp</mi><mfenced><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>'</mo><mo>)</mo></mrow></msub></mfenced></mrow></mfrac><mtext> and </mtext><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>=</mo><mfenced
    open="{" close=""><mtable columnalign="left"><mtr><mtd><msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><msub><mover><mi
    mathvariant="bold">y</mi><mo stretchy="false" style="math-style:normal;math-depth:0;">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>d</mi><mi>o</mi><mi>t</mi></mtd></mtr><mtr><mtd><msup><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊺</mo></msup><mi
    mathvariant="bold">W</mi><msub><mover><mi mathvariant="bold">y</mi><mo stretchy="false"
    style="math-style:normal;math-depth:0;">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd><mtd><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi></mtd></mtr><mtr><mtd><msup><mi
    mathvariant="bold">v</mi><mo>⊺</mo></msup><mi>tanh</mi><mo>(</mo><mi mathvariant="bold">W</mi><mo>[</mo><msub><mi
    mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>;</mo><msub><mover><mi
    mathvariant="bold">y</mi><mo stretchy="false" style="math-style:normal;math-depth:0;">^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>]</mo><mo>)</mo></mtd><mtd><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi></mtd></mtr></mtable></mfenced>
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add Luong attention to our encoder-decoder model. Since PyTorch does
    not include a Luong attention function, we need to write our own. Luckily, it’s
    pretty short:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Just like in [Equation 14-2](#attention_mechanisms_equation), we first compute
    the attention scores, then we convert them to attention weights using the softmax
    function, and lastly we compute the attention output by multiplying the attention
    weights with the value (i.e., the encoder outputs). This implementation efficiently
    runs all these computations for the whole batch at once. The `query` argument
    corresponds to **h**[(*t*)] in [Equation 14-2](#attention_mechanisms_equation)
    (i.e., the decoder’s hidden states), and the `key` argument corresponds to $ModifyingAbove
    bold y With caret Subscript left-parenthesis i right-parenthesis$ (i.e., the encoder’s
    outputs), but only for the computation of the attention scores. The `value` argument
    also corresponds to $ModifyingAbove bold y With caret Subscript left-parenthesis
    i right-parenthesis$ , but only for the final computation of the weighted sum.
    The `key` and `value` arguments are generally identical, but there are a few scenarios
    where they can differ (e.g., some models use compressed keys to save memory and
    speed up the score computation). The shapes are shown in the comments: `B` is
    the batch size; `Lq` is the length of the longest query in the batch; `Lk` is
    the length of the longest key in the batch (note that each value must have the
    same length as its corresponding key); `dq` is the query’s embedding size, which
    must be the same as the key’s embedding size `dk`; and `dv` is the value’s embedding
    size.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在[方程14-2](#attention_mechanisms_equation)中一样，我们首先计算注意力分数，然后使用softmax函数将它们转换为注意力权重，最后通过将注意力权重与值（即编码器输出）相乘来计算注意力输出。这种实现可以有效地一次性为整个批次运行所有这些计算。`query`参数对应于[方程14-2](#attention_mechanisms_equation)中的**h**[(*t*)]（即解码器的隐藏状态），而`key`参数对应于$ModifyingAbove
    bold y With caret Subscript left-parenthesis i right-parenthesis$（即编码器的输出），但仅用于计算注意力分数。`value`参数也对应于$ModifyingAbove
    bold y With caret Subscript left-parenthesis i right-parenthesis$，但仅用于最终计算加权求和。`key`和`value`参数通常相同，但在一些场景中它们可以不同（例如，一些模型使用压缩键来节省内存并加快分数计算）。形状在注释中显示：`B`是批次大小；`Lq`是批次中最长查询的长度；`Lk`是批次中最长键的长度（注意，每个值必须与其对应的键具有相同的长度）；`dq`是查询的嵌入大小，它必须与键的嵌入大小`dk`相同；`dv`是值的嵌入大小。
- en: Tip
  id: totrans-356
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Since all arguments are 3D tensors, we could replace the `@` matrix multiplication
    operator with the *batch matrix multiplication* function: `torch.bmm()`. This
    function only works with batches of matrices (i.e., 3D tensors), but it’s optimized
    for this use case so it runs faster. The result is the same: each matrix in the
    first tensor gets multiplied by the corresponding matrix in the second tensor.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有参数都是3D张量，我们可以用*批量矩阵乘法*函数：`torch.bmm()`来替换`@`矩阵乘法运算符。这个函数只适用于矩阵批（即3D张量），但它针对这个用例进行了优化，所以运行得更快。结果是相同的：第一个张量中的每个矩阵都会与第二个张量中相应的矩阵相乘。
- en: 'Now let’s update our NMT model. The constructor needs just one modification—the
    output layer’s input size must be doubled, since we will concatenate the attention
    vectors to the decoder outputs:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来更新我们的NMT模型。构造函数只需要一个修改——输出层的输入大小必须加倍，因为我们将会将注意力向量连接到解码器输出：
- en: '[PRE66]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, let’s add attention to the `forward()` method:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将注意力添加到`forward()`方法中：
- en: '[PRE67]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let’s go through this code:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析这段代码：
- en: We compute the English and Spanish embeddings, the English sequence lengths,
    and we pack the English embeddings, just like earlier.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们计算英语和西班牙语的嵌入，英语序列长度，并将英语嵌入打包，就像之前一样。
- en: We then run the encoder like earlier, but we no longer ignore its outputs since
    we will need them for the attention function.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们像之前一样运行编码器，但我们不再忽略其输出，因为我们将会需要它们用于注意力函数。
- en: Next, we run the decoder, just like earlier.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们像之前一样运行解码器。
- en: Since the encoder’s inputs are represented as a packed sequence, its outputs
    are also represented as a packed sequence. Not many operations support packed
    sequences, so we must convert the encoder’s outputs to a padded tensor using the
    `pad_packed_sequence()` function.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于编码器的输入表示为一个打包序列，其输出也以打包序列的形式表示。支持打包序列的操作不多，因此我们必须使用`pad_packed_sequence()`函数将编码器的输出转换为填充张量。
- en: And now we can call our `attention()` function. Note that we pass the decoder
    outputs instead of the hidden states because the decoder only returns the last
    hidden states. That’s OK because the `nn.GRU` layer’s outputs are equal to its
    top-layer hidden states.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们可以调用我们的`attention()`函数。注意，我们传递的是解码器输出而不是隐藏状态，因为解码器只返回最后一个隐藏状态。这是可以的，因为`nn.GRU`层的输出等于其顶层隐藏状态。
- en: Lastly, we concatenate the attention output and the decoder outputs along the
    last dimension, and we pass the result through the output layer. As earlier, we
    also permute the last two dimensions of the result.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将注意力输出和解码器输出在最后一个维度上连接起来，并将结果通过输出层。和之前一样，我们也会对结果的最后两个维度进行置换。
- en: Warning
  id: totrans-369
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Our attention mechanism doesn’t ignore padding tokens. The model learns to ignore
    them during training, but it’s preferable to mask them entirely. We will see how
    in [Chapter 15](ch15.html#transformer_chapter).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关注机制不会忽略填充标记。模型在训练过程中学会忽略它们，但最好完全屏蔽它们。我们将在[第15章](ch15.html#transformer_chapter)中看到如何做到这一点。
- en: 'And that’s it! If you train this model, you will find that it now handles much
    longer sentences. For example:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！如果你训练这个模型，你会发现它现在可以处理更长的句子。例如：
- en: '[PRE68]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Perfect! We didn’t even have to use beam search. In fact, attention mechanisms
    turned out to be so powerful that some Google researchers tried getting rid of
    recurrent layers altogether, only using feedforward layers and attention. Surprisingly,
    it worked like a charm. This led the researchers to name their paper “Attention
    is all you need”, introducing the Transformer architecture to the world. This
    was the start of a huge revolution in NLP and beyond. In the next chapter, we
    will explore the Transformer architecture and see how it revolutionized deep learning.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们甚至不需要使用束搜索。事实上，注意力机制证明非常强大，以至于一些谷歌研究人员试图完全去掉循环层，只使用前馈层和注意力。令人惊讶的是，它表现得像魔法一样。这导致研究人员将他们的论文命名为“Attention
    is all you need”，向世界介绍了Transformer架构。这是NLP以及更广泛的领域内一场巨大革命的开始。在下一章中，我们将探索Transformer架构，看看它是如何改变深度学习的。
- en: Exercises
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What are the pros and cons of using a stateful RNN versus a stateless RNN?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用有状态RNN与无状态RNN相比，有哪些优缺点？
- en: Why do people use encoder-decoder RNNs rather than plain sequence-to-sequence
    RNNs for automatic translation?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么人们使用编码器-解码器RNN而不是普通的序列到序列RNN进行自动翻译？
- en: How can you deal with variable-length input sequences? What about variable-length
    output sequences?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何处理变长输入序列？变长输出序列又该如何处理？
- en: What is beam search, and why would you use it? What tool can you use to implement
    it?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是束搜索，你为什么需要使用它？你可以使用什么工具来实现它？
- en: What is an attention mechanism? How does it help?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是注意力机制？它是如何帮助的？
- en: When would you need to use sampled softmax?
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你在什么时候需要使用采样软最大化？
- en: '*Embedded Reber grammars* were used by Hochreiter and Schmidhuber in [their
    paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce
    strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s [nice introduction](https://homl.info/108)
    to this topic, then choose a particular embedded Reber grammar (such as the one
    represented on Orr’s page), and train an RNN to identify whether a string respects
    that grammar or not. You will first need to write a function capable of generating
    a training batch containing about 50% strings that respect the grammar, and 50%
    that don’t.'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hochreiter和Schmidhuber在他们的关于LSTMs的论文[他们的论文](https://homl.info/93)中使用了*嵌入Reber语法*。它们是产生如“BPBTSXXVPSEPE”等字符串的人工语法。查看Jenny
    Orr关于这个主题的[良好介绍](https://homl.info/108)，然后选择一个特定的嵌入Reber语法（例如Orr页面上表示的那个），并训练一个RNN来识别字符串是否遵守该语法。你首先需要编写一个函数，能够生成包含大约50%遵守语法的字符串和50%不遵守语法的训练批次。
- en: Train an encoder-decoder model that can convert a date string from one format
    to another (e.g., from “April 22, 2019” to “2019-04-22”).
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个编码器-解码器模型，可以将日期字符串从一种格式转换为另一种格式（例如，从“2019年4月22日”到“2019-04-22”）。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可以在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch14.html#id3189-marker)) Alan Turing, “Computing Machinery and Intelligence”,
    *Mind* 49 (1950): 433–460.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch14.html#id3189-marker)) 阿兰·图灵，“计算机与智能”，*心灵* 49 (1950): 433–460。'
- en: '^([2](ch14.html#id3191-marker)) Of course, the word *chatbot* came much later.
    Turing called his test the *imitation game*: machine A and human B chat with human
    interrogator C via text messages; the interrogator asks questions to figure out
    which one is the machine (A or B). The machine passes the test if it can fool
    the interrogator, while the human B must try to help the interrogator.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch14.html#id3191-marker)) 当然，单词*聊天机器人*出现得要晚得多。图灵称他的测试为*模仿游戏*：机器A和人类B通过文本消息与人类审问者C聊天；审问者提问以确定哪一个是机器（A或B）。如果机器能够欺骗审问者，则通过测试，而人类B必须帮助审问者。
- en: '^([3](ch14.html#id3210-marker)) Tomáš Mikolov et al., “Distributed Representations
    of Words and Phrases and Their Compositionality”, *Proceedings of the 26th International
    Conference on Neural Information Processing Systems* 2 (2013): 3111–3119.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch14.html#id3210-marker)) Tomáš Mikolov 等人，“词和短语的分布式表示及其组合性”，*《第26届国际神经信息处理系统会议论文集》*
    2 (2013): 3111–3119.'
- en: '^([4](ch14.html#id3211-marker)) Malvina Nissim et al., “Fair Is Better Than
    Sensational: Man Is to Doctor as Woman Is to Doctor”, arXiv preprint arXiv:1905.09866
    (2019).'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch14.html#id3211-marker)) Malvina Nissim 等人，“公平比轰动性更好：人等于医生，女性等于医生”，arXiv预印本
    arXiv:1905.09866 (2019).
- en: ^([5](ch14.html#id3218-marker)) It’s a convention in Python to name unused variables
    with an underscore prefix.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch14.html#id3218-marker)) 在Python中，命名未使用的变量时使用下划线前缀是一种惯例。
- en: ^([6](ch14.html#id3232-marker)) Another technique to capture longer patterns
    is to use a stateful RNN. It’s a bit more complex and not used as much, but if
    you’re interested I’ve included a section in this chapter’s notebook.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch14.html#id3232-marker)) 捕获更长的模式的一种技术是使用有状态的RNN。这稍微复杂一些，并且使用得不太多，但如果您感兴趣，我在本章的笔记本中包含了一个部分。
- en: ^([7](ch14.html#id3233-marker)) Alec Radford et al., “Learning to Generate Reviews
    and Discovering Sentiment”, arXiv preprint arXiv:1704.01444 (2017).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch14.html#id3233-marker)) Alec Radford 等人，“学习生成评论和发现情感”，arXiv预印本 arXiv:1704.01444
    (2017).
- en: '^([8](ch14.html#id3241-marker)) Rico Sennrich et al., “Neural Machine Translation
    of Rare Words with Subword Units”, *Proceedings of the 54th Annual Meeting of
    the Association for Computational Linguistics* 1 (2016): 1715–1725.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch14.html#id3241-marker)) Rico Sennrich 等人，“使用子词单元进行罕见词的神经机器翻译”，*《第54届计算语言学协会年会论文集》*
    1 (2016): 1715–1725.'
- en: '^([9](ch14.html#id3256-marker)) Yonghui Wu et al., “Google’s Neural Machine
    Translation System: Bridging the Gap Between Human and Machine Translation”, arXiv
    preprint arXiv:1609.08144 (2016).'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch14.html#id3256-marker)) Yonghui Wu 等人，“Google的神经机器翻译系统：弥合人类翻译与机器翻译之间的差距”，arXiv预印本
    arXiv:1609.08144 (2016).
- en: '^([10](ch14.html#id3258-marker)) Taku Kudo, “Subword Regularization: Improving
    Neural Network Translation Models with Multiple Subword Candidates”, arXiv preprint
    arXiv:1804.10959 (2018).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch14.html#id3258-marker)) Taku Kudo，“Subword Regularization：使用多个子词候选者改进神经网络翻译模型”，arXiv预印本
    arXiv:1804.10959 (2018).
- en: '^([11](ch14.html#id3263-marker)) Taku Kudo and John Richardson, “SentencePiece:
    A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural
    Text Processing”, arXiv preprint arXiv:1808.06226 (2018).'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch14.html#id3263-marker)) Taku Kudo 和 John Richardson，“SentencePiece：用于神经网络文本处理的简单且语言无关的子词标记器和去标记器”，arXiv预印本
    arXiv:1808.06226 (2018).
- en: ^([12](ch14.html#id3281-marker)) *Nested tensors* serve a similar purpose and
    are more convenient to use, but they are still in prototype stage at the time
    of writing. See [*https://pytorch.org/docs/stable/nested.html*](https://pytorch.org/docs/stable/nested.html)
    for more details.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch14.html#id3281-marker)) *嵌套张量* 具有类似的作用，并且更方便使用，但截至写作时它们仍处于原型阶段。有关更多详细信息，请参阅[*https://pytorch.org/docs/stable/nested.html*](https://pytorch.org/docs/stable/nested.html)。
- en: '^([13](ch14.html#id3298-marker)) Matthew Peters et al., “Deep Contextualized
    Word Representations”, *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*
    1 (2018): 2227–2237.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '^([13](ch14.html#id3298-marker)) Matthew Peters 等人，“Deep Contextualized Word
    Representations”，*《北美计算语言学协会第2018年会议：人机语言技术论文集》* 1 (2018): 2227–2237.'
- en: '^([14](ch14.html#id3303-marker)) Jeremy Howard and Sebastian Ruder, “Universal
    Language Model Fine-Tuning for Text Classification”, *Proceedings of the 56th
    Annual Meeting of the Association for Computational Linguistics* 1 (2018): 328–339.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '^([14](ch14.html#id3303-marker)) Jeremy Howard 和 Sebastian Ruder，“用于文本分类的通用语言模型微调”，*《第56届计算语言学协会年会论文集》*
    1 (2018): 328–339.'
- en: ^([15](ch14.html#id3354-marker)) Ilya Sutskever et al., “Sequence to Sequence
    Learning with Neural Networks”, arXiv preprint, arXiv:1409.3215 (2014).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch14.html#id3354-marker)) Ilya Sutskever 等人，“使用神经网络的序列到序列学习”，arXiv预印本，arXiv:1409.3215
    (2014).
- en: ^([16](ch14.html#id3359-marker)) Samy Bengio et al., “Scheduled Sampling for
    Sequence Prediction with Recurrent Neural Networks”, arXiv preprint arXiv:1506.03099
    (2015).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch14.html#id3359-marker)) Samy Bengio 等人，“使用循环神经网络进行序列预测的定时采样”，arXiv预印本
    arXiv:1506.03099 (2015).
- en: '^([17](ch14.html#id3376-marker)) Sébastien Jean et al., “On Using Very Large
    Target Vocabulary for Neural Machine Translation”, *Proceedings of the 53rd Annual
    Meeting of the Association for Computational Linguistics and the 7th International
    Joint Conference on Natural Language Processing of the Asian Federation of Natural
    Language Processing* 1 (2015): 1–10.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '^([17](ch14.html#id3376-marker)) Sébastien Jean 等人, “在神经机器翻译中使用非常大的目标词汇表”,
    *第53届计算语言学年会和第7届亚洲自然语言处理联合会国际自然语言处理联合会议论文集* 1 (2015): 1–10.'
- en: ^([18](ch14.html#id3378-marker)) Edouard Grave et al., “Efficient softmax approximation
    for GPUs”, arXiv preprint arXiv:1609.04309 (2016).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch14.html#id3378-marker)) Edouard Grave 等人, “针对GPU的高效softmax近似”, arXiv
    preprint arXiv:1609.04309 (2016).
- en: ^([19](ch14.html#id3383-marker)) Ofir Press, Lior Wolf, “Using the Output Embedding
    to Improve Language Models”, arXiv preprint arXiv:1608.05859 (2016).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch14.html#id3383-marker)) Ofir Press, Lior Wolf, “Using the Output Embedding
    to Improve Language Models”, arXiv preprint arXiv:1608.05859 (2016).
- en: ^([20](ch14.html#id3394-marker)) Dzmitry Bahdanau et al., “Neural Machine Translation
    by Jointly Learning to Align and Translate”, arXiv preprint arXiv:1409.0473 (2014).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch14.html#id3394-marker)) Dzmitry Bahdanau 等人, “通过联合学习对齐和翻译进行神经机器翻译”,
    arXiv preprint arXiv:1409.0473 (2014).
- en: '^([21](ch14.html#id3403-marker)) Minh-Thang Luong et al., “Effective Approaches
    to Attention-Based Neural Machine Translation”, *Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing* (2015): 1412–1421.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '^([21](ch14.html#id3403-marker)) Minh-Thang Luong 等人, “基于注意力的神经机器翻译的有效方法”,
    *2015年实证自然语言处理会议论文集* (2015): 1412–1421.'
