- en: Chapter 14\. Natural Language Processing with RNNs and Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When Alan Turing imagined his famous [Turing test](https://homl.info/turingtest)⁠^([1](ch14.html#id3189))
    in 1950, he proposed a way to evaluate a machine’s ability to match human intelligence.
    He could have tested for many things, such as the ability to recognize cats in
    pictures, play chess, compose music, or escape a maze, but, interestingly, he
    chose a linguistic task. More specifically, he devised a *chatbot* capable of
    fooling its interlocutor into thinking it was human.⁠^([2](ch14.html#id3191))
    This test does have its weaknesses: a set of hardcoded rules can fool unsuspecting
    or naive humans (e.g., the machine could give vague predefined answers in response
    to some keywords, it could pretend that it is joking or drunk to get a pass on
    its weirdest answers, or it could escape difficult questions by answering them
    with its own questions), and many aspects of human intelligence are utterly ignored
    (e.g., the ability to interpret nonverbal communication such as facial expressions,
    or to learn a manual task). But the test does highlight the fact that mastering
    language is arguably *Homo sapiens*’s greatest cognitive ability.'
  prefs: []
  type: TYPE_NORMAL
- en: Until recently, state-of-the-art natural language processing (NLP) models were
    pretty much all based on recurrent neural networks (introduced in [Chapter 13](ch13.html#rnn_chapter)).
    However, in recent years, RNNs have been replaced with transformers, which we
    will explore in [Chapter 15](ch15.html#transformer_chapter). That said, it’s still
    important to learn how RNNs can be used for NLP tasks, if only because it helps
    better understand transformers. Moreover, most of the techniques we will discuss
    in this chapter are also useful with Transformer architectures (e.g., tokenization,
    beam search, attention mechanisms, and more). Plus, RNNs have recently made a
    surprise comeback in the form of state space models (SSMs) (see “State-Space Models
    (SSMs)” at [*https://homl.info*](https://homl.info)).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is organized in three sections. In the first section, we will start
    by building a *character RNN*, or *char-RNN*, trained to predict the next character
    in a sentence. On the way, we will learn about trainable embeddings. Our char-RNN
    will be our first tiny *language model*, capable of generating original text.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second section, we will turn to text classification, and more specifically
    sentiment analysis, which aims to predict how positive or negative some text is.
    Our model will read movie reviews and estimate the rater’s feeling about the movie.
    This time, instead of splitting the text into individual characters, we will split
    it into *tokens*: a token is a small piece of text from a fixed-sized vocabulary,
    such as the top 10,000 most common words in the English language, or the most
    common subwords (e.g., “smartest” = “smart” + “est”), or even individual characters
    or bytes. To split the text into tokens, we will use a *tokenizer*. This section
    will also introduce popular Hugging Face libraries: the *Datasets* library to
    download datasets, the *Tokenizers* library for tokenizers, and the *Transformers*
    library for popular models, downloaded automatically from the *Hugging Face Hub*.
    Hugging Face is a hugely influential company and open source community, and it
    plays a central role in the open source AI space, especially in NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final boss of this chapter will be neural machine translation (NMT), the
    topic of the third and last section: we will build an encoder-decoder model capable
    of translating English to Spanish. This will lead us to *attention mechanisms*,
    which we will apply to our encoder-decoder model to improve its capacity to handle
    long input texts. As their name suggests, attention mechanisms are neural network
    components that learn to select the part of the inputs that the model should focus
    on at each time step. They directly led to the transformers revolution, as we
    will see in the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a simple and fun char-RNN model that can write like Shakespeare
    (sort of).
  prefs: []
  type: TYPE_NORMAL
- en: Generating Shakespearean Text Using a Character RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a famous [2015 blog post](https://homl.info/charrnn) titled “The Unreasonable
    Effectiveness of Recurrent Neural Networks”, Andrej Karpathy showed how to train
    an RNN to predict the next character in a sentence. This *char-RNN* can then be
    used to generate novel text, one character at a time. Here is a small sample of
    the text generated by a char-RNN model after it was trained on all of Shakespeare’s
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PANDARUS:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alas, I think he shall be come approached and the day
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When little srain would be attain’d into being never fed,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And who is but a chain and subjects of his death,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I should not sleep.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not exactly a masterpiece, but it is still impressive that the model was able
    to learn words, grammar, proper punctuation, and more, just by learning to predict
    the next character in a sentence. This is our first example of a *language model*.
    In the remainder of this section we’ll build a char-RNN step by step, starting
    with the creation of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Training Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s download a subset of Shakespeare’s works (about 25%). The data
    is loaded from Andrej Karpathy’s [char-rnn project](https://github.com/karpathy/char-rnn):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s print the first few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2][PRE3]``py[PRE4]``py Note that we call `lower()` to ignore case and thereby
    reduce the vocabulary size. We must now assign a token ID to each character. For
    this, we can just use its index in the vocabulary. To decode the output of our
    model, we will also need a way to go from a token ID to a character:    [PRE5]py`
    `>>>` `char_to_id``[``"a"``]` [PRE6]py [PRE7]``py[PRE8][PRE9][PRE10] import torch  def
    encode_text(text):     return torch.tensor([char_to_id[char] for char in text.lower()])  def
    decode_text(char_ids):     return "".join([id_to_char[char_id.item()] for char_id
    in char_ids]) [PRE11] >>> encoded = encode_text("Hello, world!") `>>>` `encoded`
    [PRE12] [PRE13][PRE14]`` [PRE15] from torch.utils.data import Dataset, DataLoader  class
    CharDataset(Dataset):     def __init__(self, text, window_length):         self.encoded_text
    = encode_text(text)         self.window_length = window_length      def __len__(self):         return
    len(self.encoded_text) - self.window_length      def __getitem__(self, idx):         if
    idx >= len(self):             raise IndexError("dataset index out of range")         end
    = idx + self.window_length         window = self.encoded_text[idx : end]         target
    = self.encoded_text[idx + 1 : end + 1]         return window, target [PRE16] window_length
    = 50 batch_size = 512  # reduce if your GPU cannot handle such a large batch size
    train_set = CharDataset(shakespeare_text[:1_000_000], window_length) valid_set
    = CharDataset(shakespeare_text[1_000_000:1_060_000], window_length) test_set =
    CharDataset(shakespeare_text[1_060_000:], window_length) train_loader = DataLoader(train_set,
    batch_size=batch_size, shuffle=True) valid_loader = DataLoader(valid_set, batch_size=batch_size)
    test_loader = DataLoader(test_set, batch_size=batch_size) [PRE17]` [PRE18][PRE19][PRE20][PRE21][PRE22]
    [PRE23]`py` [PRE24]  [PRE25][PRE26]``py[PRE27]`py` ## Embeddings    An embedding
    is a dense representation of some higher-dimensional data, typically a categorical
    feature. If there are 50,000 possible categories, then one-hot encoding produces
    a 50,000-dimensional sparse vector (i.e., containing mostly zeros). In contrast,
    an embedding is a comparatively small dense vector; for example, with just 300
    dimensions.    ###### Tip    The embedding size is a hyperparameter you can tune.
    As a rule of thumb, a good embedding size is often close to the square root of
    the number of categories.    In deep learning, embeddings are usually initialized
    randomly, and they are then trained by gradient descent, along with the other
    model parameters. For example, if we wanted to train a neural network on the California
    housing dataset (see [Chapter 2](ch02.html#project_chapter)), we could represent
    the `ocean_proximity` categorical feature using embeddings. The `"NEAR BAY"` category
    could be represented initially by a random vector such as `[0.831, 0.696]`, while
    the `"NEAR OCEAN"` category might be represented by another random vector such
    as `[0.127, 0.868]` (in this example we are using 2D embeddings).    Since these
    embeddings are trainable, they will gradually improve during training; and as
    they represent fairly similar categories in this example, gradient descent will
    certainly end up pushing them closer together, while it will tend to move them
    away from the `"INLAND"` category’s embedding (see [Figure 14-2](#embedding_diagram)).
    Indeed, the better the representation, the easier it will be for the neural network
    to make accurate predictions, so training tends to make embeddings useful representations
    of the categories. This is called *representation learning* (you will see other
    types of representation learning in [Chapter 18](ch18.html#autoencoders_chapter)).  ![Diagram
    illustrating the improvement of embeddings during training, showing categories
    such as "Near ocean," "Near bay," and "Inland" in an embedding space.](assets/hmls_1402.png)  ######
    Figure 14-2\. Embeddings will gradually improve during training    Not only will
    embeddings generally be useful representations for the task at hand, but quite
    often these same embeddings can be reused successfully for other tasks. The most
    common example of this is *word embeddings* (i.e., embeddings of individual words):
    when you are working on a natural language processing task, you are often better
    off reusing pretrained word embeddings than training your own, as we will see
    later in this chapter.    The idea of using vectors to represent words dates back
    to the 1960s, and many sophisticated techniques have been used to generate useful
    vectors, including using neural networks. But things really took off in 2013,
    when Tomáš Mikolov and other Google researchers published a [paper](https://homl.info/word2vec)⁠^([3](ch14.html#id3210))
    describing an efficient technique to learn word embeddings using neural networks,
    significantly outperforming previous attempts. This allowed them to learn embeddings
    on a very large corpus of text: they trained a neural network to predict the words
    near any given word and obtained astounding word embeddings. For example, synonyms
    had very close embeddings, and semantically related words such as *France*, *Spain*,
    and *Italy* were clustered together.    It’s not just about proximity, though:
    word embeddings are also organized along meaningful axes in the embedding space.
    Here is a famous example: if you compute *King – Man + Woman* (adding and subtracting
    the embedding vectors of these words), then the result will be very close to the
    embedding of the word *Queen* (see [Figure 14-3](#word_embedding_diagram)). In
    other words, the word embeddings encode the concept of gender! Similarly, you
    can compute *Madrid – Spain + France*, and the result is close to *Paris*, which
    seems to show that the notion of capital city is also encoded in the embeddings.  ![Diagram
    illustrating how word embeddings calculate "King - Man + Woman" to approximate
    the position of "Queen," demonstrating the encoding of the gender concept in the
    embedding space.](assets/hmls_1403.png)  ###### Figure 14-3\. Word embeddings
    of similar words tend to be close, and some axes seem to encode meaningful concepts    ######
    Warning    Word embeddings can have some meaningful structure, as the “King –
    Man + Woman” shows. However, they are also noisy and often hard to interpret.
    I’ve added some code at the end of the notebook so you can judge for yourself.    Unfortunately,
    word embeddings sometimes capture our worst biases. For example, although they
    correctly learn that *Man is to King as Woman is to Queen*, they also seem to
    learn that *Man is to Doctor as Woman is to Nurse*: quite a sexist bias! To be
    fair, this particular example is probably exaggerated, as was pointed out in a
    [2019 paper](https://homl.info/fairembeds)⁠^([4](ch14.html#id3211)) by Malvina
    Nissim et al. Nevertheless, ensuring fairness in deep learning algorithms is an
    important and active research topic.    PyTorch provides an `nn.Embedding` module,
    which wraps an *embedding matrix*: this matrix has one row per possible category
    (e.g., one row for each token in the vocabulary) and one column per embedding
    dimension. The embedding dimensionality is a hyperparameter you can tune. By default,
    the embedding matrix is initialized randomly.    To convert a category ID to an
    embedding, the `nn.Embedding` layer just looks up and returns the corresponding
    row. That’s all there is to it! For example, let’s initialize an `nn.Embedding`
    layer with five categories and 3D embeddings, and use it to encode some categories:    [PRE28]py`
    `>>>` `embed` `=` `nn``.``Embedding``(``5``,` `3``)`  `# 5 categories × 3D embeddings`
    [PRE29]py [PRE30] [PRE31] [PRE32][PRE33]` [PRE34][PRE35]py[PRE36] To have more
    control over the diversity of the generated text, we can divide the logits by
    a number called the *temperature*, which we can tweak as we wish. A temperature
    close to zero favors high-probability characters, while a high temperature gives
    all characters an equal probability. Lower temperatures are typically preferred
    when generating fairly rigid and precise text, such as mathematical equations,
    while higher temperatures are preferred when generating more diverse and creative
    text. Let’s write a `next_char()` helper function that will use this approach
    to pick the next character to add to the input text:    [PRE37]    Next, we can
    write another small helper function that will repeatedly call `next_​char()` to
    get the next character and append it to the given text:    [PRE38]    We are now
    ready to generate some text! Let’s try low, medium, and high temperatures:    [PRE39]
    `To be or not to be the better from the cause` `that thou think you may be so
    be gone.`  `romeo:` `that` `>>>` `print``(``extend_text``(``model``,` `"To be
    or not to b"``,` `temperature``=``100``))` `` `To be or not to b-c3;m-rkn&:x:uyve:b&hi
    n;n-h;wt3k` `&cixxh:a!kq$c$ 3 ncq$ ;;wq cp:!xq;yh` `!3` `d!nhi.` `` [PRE40]   [PRE41]
    ``Notice the repetitions when the temperature is low: “the state and the” appears
    twice. The intermediate temperature led to more convincing results, although Romeo
    wasn’t very talkative today. But in the last example the temperature was way too
    high—we fried Shakespeare. To generate more convincing text, a common technique
    is to sample only from the top *k* characters, or only from the smallest set of
    top characters whose total probability exceeds some threshold: this is called
    *top-p sampling*, or *nucleus sampling*. Alternatively, you could try using *beam
    search*, which we will discuss later in this chapter, or using more `nn.GRU` layers
    and more neurons per layer, training for longer, and adding more regularization
    if needed.    ###### Note    The model is currently incapable of learning patterns
    longer than `window_length`, which is just 50 characters. You could try making
    this window larger, but it would also make training harder, and even LSTM and
    GRU cells cannot handle very long sequences.⁠^([6](ch14.html#id3232))    Interestingly,
    although a char-RNN model is just trained to predict the next character, this
    seemingly simple task actually requires it to learn some higher-level tasks as
    well. For example, to find the next character after “Great movie, I really “,
    it’s helpful to understand that the sentence is positive, so what follows is more
    likely to be the letter “l” (for “loved”) rather than “h” (for “hated”). In fact,
    a [2017 paper](https://homl.info/sentimentneuron)⁠^([7](ch14.html#id3233)) by
    Alec Radford and other OpenAI researchers describes how the authors trained a
    big char-RNN-like model on a large dataset, and found that one of the neurons
    acted as an excellent sentiment analysis classifier. Although the model was trained
    without any labels, the *sentiment neuron*—as they called it—reached state-of-the-art
    performance on sentiment analysis benchmarks (this foreshadowed and motivated
    unsupervised pretraining in NLP).    Speaking of which, let’s say farewell to
    Shakespeare and turn to the second part of this chapter: sentiment analysis.``
    [PRE42]` [PRE43][PRE44][PRE45][PRE46][PRE47] [PRE48]`py` [PRE49] [PRE50][PRE51]``py[PRE52]py[PRE53]py[PRE54]py[PRE55]`py[PRE56]py[PRE57]``py[PRE58]py[PRE59][PRE60][PRE61][PRE62]
    [PRE63]`py[PRE64]py[PRE65]``py[PRE66][PRE67] Notice that frequent words like “what”
    and “movie” have been identified by the BPE model and are represented by a single
    token, while less frequent words like “awesome” are split into multiple tokens.
    Also note that the smiley was not part of the training data, so it gets replaced
    with the unknown token `"<unk>"`.    The tokenizer provides a `get_vocab()` method
    which returns a dictionary mapping each token to its ID. You can also use the
    `token_to_id()` method to map a single token, or conversely use the `id_to_token()`
    method to go from ID to token. However, you will more often use the `decode()`
    method to convert a list of token IDs into a string:    [PRE68]py   [PRE69] The
    tokenizer keeps track of each token’s start and end offset in the original string,
    which can come in handy, especially for debugging:    [PRE70]   [PRE71][PRE72]``py[PRE73]``
    [PRE74]`` [PRE75]` Notice how the first and second review were padded with 0s,
    which is our padding token ID. Each `Encoding` object also includes an `attention_mask`
    attribute containing a 1 for each nonpadding token, and a 0 for each padding token.
    This can be used in your models to easily ignore the padded time steps: just multiply
    a tensor with the attention mask. In some cases you will prefer to have the list
    of sequence lengths (ignoring padding). Here’s how to get both the attention mask
    tensor and the sequence lengths:    [PRE76][PRE77]`` `>>>` `attention_mask` [PRE78]
    `>>>` `lengths` `` `tensor([281, 114, 285])` `` [PRE79]` [PRE80][PRE81]   [PRE82]
    ``You may have noted that spaces were not handled very well by our tokenizer.
    In particular, the word “awesome” came back as “aw es ome”, and “movie!” came
    back as “movie !”. This is because the `Whitespace` pre-tokenizer dropped all
    spaces, therefore the BPE tokenizer doesn’t know where spaces should go and it
    just adds spaces between all tokens. To fix this, we can replace the `Whitespace`
    pre-tokenizer with the `ByteLevel` pre-tokenizer: it replaces all spaces with
    a special character Ġ so the BPE model doesn’t lose track of them. For example,
    if you use this pre-tokenizer and you encode and decode the text “what an awesome
    movie! ![](assets/smiling-face-with-smiling-eyes_1f60a_(1).png)“, you will get:
    “Ġwhat Ġan Ġaw es ome Ġmovie !”. After removing the spaces, then replacing every
    Ġ with a space, you get " what an awesome movie!”. This is almost perfect, except
    for the extra space at the start—which is easily removed—and the lost emoji, which
    was replaced with an unknown token because it’s not in the vocabulary, and dropped
    by the `decode()` method.    As its name suggests, the `ByteLevel` pre-tokenizer
    allows the BPE model to work at the byte level, rather than the character level:
    unsurprisingly, this is called Byte-level BPE (BBPE). For example, the ![](assets/smiling-face-with-smiling-eyes_1f60a_(1).png)
    emoji will be converted to four bytes, using Unicode’s UTF-8 encoding. This means
    that BBPE will never output an unknown token if its vocabulary contains all 256
    possible bytes, since any text can be broken down into its individual bytes whenever
    longer tokens are not found in the vocabulary. This makes BBPE well suited when
    the corpus contains rare characters such as emojis.    Another important variant
    of BPE is [WordPiece](https://homl.info/wordpiece),⁠^([9](ch14.html#id3256)) proposed
    by Google in 2016\. This tokenization algorithm is very similar to BPE, but instead
    of adding the most frequent adjacent pair of tokens to the vocabulary at each
    iteration, it adds the pair with the highest score. This score is computed using
    [Equation 14-1](#wordpiece_equation): the frequency(AB) term is just like in BPE—it
    boosts pairs that are frequent in the corpus. However, the denominator reduces
    the score of a pair when the individual tokens are themselves frequent. This normalization
    tends to favor more useful and meaningful tokens than BPE, and the algorithm often
    produces shorter encoded sequences than BPE or BBPE.    ##### Equation 14-1\.
    WordPiece score for a pair AB composed of tokens A and B  $score left-parenthesis
    AB right-parenthesis equals StartFraction frequency left-parenthesis AB right-parenthesis
    Over freq left-parenthesis upper A right-parenthesis dot freq left-parenthesis
    upper B right-parenthesis EndFraction dot len left-parenthesis vocab right-parenthesis$  To
    train a WordPiece tokenizer using the Tokenizers library, you can use the same
    code as for BPE, but replace the `BPE` model with `WordPiece`, and the `BpeTrainer`
    with `WordPieceTrainer`. If you encode and decode the same review as earlier,
    you will get “what an aw esome movie !”. Notice that WordPiece adds a prefix to
    tokens that are inside a word, which makes it easy to reconstruct the original
    string: just remove " #“# (as well as spaces before punctuations). Note that the
    smiley emoji once again disappeared because it was not in the vocabulary.    One
    last popular tokenization algorithm we will discuss is Unigram LM (Language Model),
    introduced in a [2018 paper](https://homl.info/subword)⁠^([10](ch14.html#id3258))
    by Taku Kudo at Google. This technique is a bit different than the previous ones:
    it starts out with a very large vocabulary containing every frequent word, subword,
    and character in the training corpus, then it gradually removes the least useful
    tokens until it reaches the desired vocabulary size. To determine how useful a
    token is, this method makes one big simplifying assumption: it assumes that the
    corpus was sampled randomly from the vocabulary, one token at a time (hence the
    name Unigram LM), and that every token was sampled independently from the others.
    Therefore, this tokenizer model assumes that the probability of sampling the pair
    AB is equal to the probability of sampling A times the probability of sampling
    B. Given this assumption, it can estimate the probability of sampling the whole
    training corpus. At each iteration, the training algorithm attempts to remove
    tokens without reducing this overall probability too much.    For example, suppose
    that the vocabulary contains the tokens “them”, “the”, and “m”, respectively,
    with 1%, 5%, and 2% probability. This means that the word “them” has a 1% chance
    of being sampled as the single token “them”, or a 5% × 2% = 0.1% chance of being
    sampled as the pair “the” + “m”. Overall, the word “them” has a 1% + 0.1% = 1.1%
    chance of being sampled. If we remove the token “them” from the vocabulary, then
    the probability of sampling the word “them” drops down to 0.1%. If instead we
    remove either “m” or “the”, then the probability only drops down to 1% since we
    can still sample the single token “them”. So if the training corpus only contained
    the word “them”, then the algorithm would prefer to drop either “the” or “m”.
    Of course, in reality the corpus contains many other words that contain these
    two tokens, so the algorithm will likely find other less useful tokens to drop.    ######
    Tip    Unigram LM is great for languages that don’t use spaces to separate words,
    like English does. For example, Chinese text does not use spaces between words,
    Vietnamese uses spaces even within words, and German often attaches multiple words
    together, without spaces.    The same paper also proposed a novel regularization
    technique called *subword regularization*, which improves generalization and robustness
    by introducing some randomness in tokenization while training the NLP model (not
    the tokenizer model). For example, assuming the vocabulary contains the tokens
    “them”, “the”, and “m”, and you choose to use subword regularization, then the
    word “them” will sometimes be tokenized as “the” + “m”, and sometimes as “them”.
    This technique works best with *morphologically rich languages*, meaning languages
    where words carry a lot of grammatical information through affixes, inflections,
    and internal modifications (such as Arabic, Finnish, German, Hungarian, Polish,
    or Turkish), as opposed to languages that rely on word order or additional helper
    words (such as English, Chinese, Thai, or Vietnamese).    Unfortunately, the Tokenizers
    library does not natively support subword regularization, so you either have to
    implement it yourself, or you can use Google’s [*SentencePiece*](https://github.com/google/sentencepiece)
    library (`pip install sentencepiece`) which provides an open source implementation.
    This project is described in a [2018 paper](https://homl.info/sentencepiece)⁠^([11](ch14.html#id3263))
    by Taku Kudo and John Richardson.    [Table 14-1](#tokenizer_summary_table) summarizes
    the three main tokenizers used today.      Table 14-1\. Overview of the three
    main tokenizers   | Feature | BBPE | WordPiece | Unigram LM | | --- | --- | ---
    | --- | | **How** | Merge most frequent pairs | Merge pairs that maximize data
    likelihood | Remove least likely tokens | | **Pros** | Fast, simple, great for
    multilingual | Good balance of efficiency and token quality | Most meaningful,
    shortest sequences | | **Cons** | Can produce awkward splits | Less robust than
    BBPE for multilingual | Slower to train and tokenize | | **Used By** | GPT, Llama,
    RoBERTa, BLOOM | BERT, DistilBERT, ELECTRA | T5, ALBERT, mBART |    Training your
    own tokenizer is useful in many situations; for example, if you are dealing with
    domain-specific text, such as medical, legal, or engineering documents full of
    jargon, or if the text is written in a low-resource language or dialect, or if
    it’s code written in a new programming language, and so on. However, in most cases
    you will want to simply reuse a pretrained tokenizer.`` [PRE83]` [PRE84]` [PRE85]``
    [PRE86][PRE87]py[PRE88]py`` [PRE89]`py [PRE90]`py` [PRE91]`py``  [PRE92]`py[PRE93]py[PRE94]py
    [PRE95]`py[PRE96]py[PRE97]py[PRE98]py[PRE99]py[PRE100]`py[PRE101]py[PRE102]` [PRE103]`
    As you can see, this BERT model is implemented using PyTorch, and it contains
    a regular `nn.Embedding` layer. We could just replace our model’s `nn.Embedding`
    layer with this one (and retrain our model), but we can keep models cleanly separated
    by initializing our own `nn.Embedding` layer with a copy of the pretrained embedding
    matrix. This can be done using the `Embedding.from_pretrained()` function:    [PRE104]    Note
    that we set `freeze=True` when creating the `nn.Embedding` layer: this makes it
    nontrainable and ensures that the pretrained embeddings won’t be damaged by large
    gradients at the beginning of training. You can train the model for a few epochs
    like this, then make the embedding layer trainable and continue training, letting
    the model fine-tune the embeddings for our task.    Pretrained word embeddings
    have been popular for quite a while, starting with Google’s [Word2vec embeddings](https://homl.info/word2vec)
    (2013), Stanford’s [GloVe embeddings](https://homl.info/glove) (2014), Facebook’s
    [FastText embeddings](https://fasttext.cc) (2016), and more. However, this approach
    has its limits. In particular, a word has a single representation, no matter the
    context. For example, the word “right” is encoded the same way in “left and right”
    and “right and wrong”, even though it means two very different things. To address
    this limitation, a [2018 paper](https://homl.info/elmo)⁠^([13](ch14.html#id3298))
    by Matthew Peters introduced *Embeddings from Language Models* (ELMo): these are
    contextualized word embeddings learned from the internal states of a deep bidirectional
    RNN language model. In other words, instead of just using pretrained word embeddings
    in your model, you can reuse several layers of a pretrained language model.    At
    roughly the same time, the [Universal Language Model Fine-Tuning (ULMFiT) paper](https://homl.info/ulmfit)⁠^([14](ch14.html#id3303))
    by Jeremy Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised
    pretraining for NLP tasks. The authors trained an LSTM language model on a huge
    text corpus using self-supervised learning (i.e., generating the labels automatically
    from the data), then they fine-tuned it on various tasks. Their model outperformed
    the state of the art on six text classification tasks by a large margin (reducing
    the error rate by 18%–24% in most cases). Moreover, the authors showed that a
    pretrained model fine-tuned on just 100 labeled examples could achieve the same
    performance as one trained from scratch on 10,000 examples. Before the ULMFiT
    paper, using pretrained models was only the norm in computer vision; in the context
    of NLP, pretraining was limited to word embeddings. This paper marked the beginning
    of a new era in NLP: today, reusing pretrained language models is the norm.    For
    example, why not reuse the entire pretrained BERT model for our sentiment analysis
    model? To use the BERT model, the Transformers library lets us call it like a
    function, passing it the tokenized reviews:    [PRE105][PRE106]`` `...` [PRE107]
    `>>>` `bert_output``.``last_hidden_state``.``shape` `` `torch.Size([3, 200, 768])`
    `` [PRE108]` [PRE109][PRE110]   [PRE111] [PRE112]`py [PRE113]py``  [PRE114]`py[PRE115][PRE116][PRE117]``py[PRE118]py
    [PRE119]py`` We first tokenize the review, then we call the model, it returns
    a `ModelOutput` object containing the logits, and we convert these logits to estimated
    probabilities using the `torch.softmax()` function. Ouch! The model classified
    this review as negative with 65.53% confidence! Indeed, the BERT model inside
    `BertForSequenceClassification` is pretrained, but not the classification head,
    so we’re going to get terrible performance until we actually train this model
    on the IMDb dataset.    If you pass labels when calling this model (or any other
    model from the Transformers library), then it also computes the loss and returns
    it in the `ModelOutput` object. For example:    [PRE120]py[PRE121]` `... `    `labels``=``torch``.``tensor``([``1``],`
    `device``=``device``))` [PRE122] `>>>` `output``.``loss` `` `tensor(0.4226, device=''cuda:0'',
    dtype=torch.float16)` `` [PRE123]` [PRE124][PRE125][PRE126]   [PRE127] ``Since
    `num_labels` is greater than 1, the model computes the `nn.CrossEntropyLoss` (which
    is implemented as `nn.LogSoftmax` followed by `nn.NLLLoss`—that’s why we see `grad_fn=<NllLossBackward0>`).
    If we had used `num_labels=1`, then the model would have used the `nn.MSELoss`
    instead; this can be useful for regression tasks.    We could now train this model
    using our own training code, as we did so far, but the Transformers library provides
    a convenient *Trainer API*, so let’s check it out.`` [PRE128]` [PRE129]`` [PRE130][PRE131]py[PRE132]py
    [PRE133] [PRE134][PRE135]` `... `                           `truncation``=``True``,`
    `max_length``=``512``)` [PRE136] `>>>` `classifier_imdb``(``train_reviews``[:``10``])`
    `` `[{''label'': ''POSITIVE'', ''score'': 0.9996108412742615},`  `{''label'':
    ''POSITIVE'', ''score'': 0.9998623132705688},`  `[...]`  `{''label'': ''POSITIVE'',
    ''score'': 0.9978922009468079},`  `{''label'': ''NEGATIVE'', ''score'': 0.9997020363807678}]`
    `` [PRE137]` [PRE138][PRE139]   [PRE140][PRE141]``py[PRE142][PRE143] >>> model_name
    = "huggingface/distilbert-base-uncased-finetuned-mnli" `>>>` `classifier_mnli`
    `=` `pipeline``(``"text-classification"``,` `model``=``model_name``)` [PRE144]`
    `>>>` `classifier_mnli``([` [PRE145] `... `    `"She loves me. [SEP] She loves
    me not. [SEP]"``,` [PRE146]`py [PRE147] [PRE148][PRE149]``py[PRE150]``py`` [PRE151]`py
    [PRE152]`py` [PRE153]`py`` [PRE154]`py[PRE155][PRE156][PRE157] [PRE158][PRE159][PRE160][PRE161][PRE162]``
    [PRE163][PRE164][PRE165] [PRE166]`py[PRE167]py[PRE168]py[PRE169]py[PRE170]py[PRE171]py`
    [PRE172]`py[PRE173]py[PRE174]py[PRE175][PRE176][PRE177][PRE178][PRE179]``py[PRE180]`py`
    [PRE181]py [PRE182]py``  [PRE183]py`` [PRE184]py [PRE185]`py [PRE186]py`` [PRE187]py[PRE188][PRE189][PRE190][PRE191]py[PRE192]py[PRE193]py[PRE194]py[PRE195]`py[PRE196]py[PRE197]'
  prefs: []
  type: TYPE_NORMAL
