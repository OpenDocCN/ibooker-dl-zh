- en: appendix E Parameter-efficient fine-tuning with LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Low-rank adaptation* (*LoRA*) is one of the most widely used techniques for
    *parameter-efficient fine-tuning*. The following discussion is based on the spam
    classification fine-tuning example given in chapter 6\. However, LoRA fine-tuning
    is also applicable to the supervised *instruction fine-tuning* discussed in chapter
    7.'
  prefs: []
  type: TYPE_NORMAL
- en: E.1 Introduction to LoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LoRA is a technique that adapts a pretrained model to better suit a specific,
    often smaller dataset by adjusting only a small subset of the model’s weight parameters.
    The “low-rank” aspect refers to the mathematical concept of limiting model adjustments
    to a smaller dimensional subspace of the total weight parameter space. This effectively
    captures the most influential directions of the weight parameter changes during
    training. The LoRA method is useful and popular because it enables efficient fine-tuning
    of large models on task-specific data, significantly cutting down on the computational
    costs and resources usually required for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a large weight matrix *W* is associated with a specific layer. LoRA
    can be applied to all linear layers in an LLM. However, we focus on a single layer
    for illustration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: When training deep neural networks, during backpropagation, we learn a D*W*
    matrix, which contains information on how much we want to update the original
    weight parameters to minimize the loss function during training. Hereafter, I
    use the term “weight” as shorthand for the model’s weight parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In regular training and fine-tuning, the weight update is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-E-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The LoRA method, proposed by Hu et al. ([https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)),
    offers a more efficient alternative to computing the weight updates D*W* by learning
    an approximation of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-E-2.png)'
  prefs: []
  type: TYPE_IMG
- en: where *A* and *B* are two matrices much smaller than *W*, and *AB* represents
    the matrix multiplication product between *A* and *B*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using LoRA, we can then reformulate the weight update we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-E-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure E.1 illustrates the weight update formulas for full fine-tuning and LoRA
    side by side.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/E-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure E.1 A comparison between weight update methods: regular fine-tuning
    and LoRA. Regular fine-tuning involves updating the pretrained weight matrix W
    directly with DW (left). LoRA uses two smaller matrices, A and B, to approximate
    DW, where the product AB is added to W, and r denotes the inner dimension, a tunable
    hyperparameter (right).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you paid close attention, you might have noticed that the visual representations
    of full fine-tuning and LoRA in figure E.1 differ slightly from the earlier presented
    formulas. This variation is attributed to the distributive law of matrix multiplication,
    which allows us to separate the original and updated weights rather than combine
    them. For example, in the case of regular fine-tuning with *x* as the input data,
    we can express the computation as
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-E-4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can write the following for LoRA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-E-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Besides reducing the number of weights to update during training, the ability
    to keep the LoRA weight matrices separate from the original model weights makes
    LoRA even more useful in practice. Practically, this allows for the pretrained
    model weights to remain unchanged, with the LoRA matrices being applied dynamically
    after training when using the model.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the LoRA weights separate is very useful in practice because it enables
    model customization without needing to store multiple complete versions of an
    LLM. This reduces storage requirements and improves scalability, as only the smaller
    LoRA matrices need to be adjusted and saved when we customize LLMs for each specific
    customer or application.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see how LoRA can be used to fine-tune an LLM for spam classification,
    similar to the fine-tuning example in chapter 6.
  prefs: []
  type: TYPE_NORMAL
- en: E.2 Preparing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before applying LoRA to the spam classification example, we must load the dataset
    and pretrained model we will work with. The code here repeats the data preparation
    from chapter 6\. (Instead of repeating the code, we could open and run the chapter
    6 notebook and insert the LoRA code from section E.4 there.)
  prefs: []
  type: TYPE_NORMAL
- en: First, we download the dataset and save it as CSV files.
  prefs: []
  type: TYPE_NORMAL
- en: Listing E.1 Downloading and preparing the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we create the `SpamDataset` instances.
  prefs: []
  type: TYPE_NORMAL
- en: Listing E.2 Instantiating PyTorch datasets
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After creating the PyTorch dataset objects, we instantiate the data loaders.
  prefs: []
  type: TYPE_NORMAL
- en: Listing E.3 Creating PyTorch data loaders
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As a verification step, we iterate through the data loaders and check that
    the batches contain eight training examples each, where each training example
    consists of 120 tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we print the total number of batches in each dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have the following number of batches per dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: E.3 Initializing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We repeat the code from chapter 6 to load and prepare the pretrained GPT model.
    We begin by downloading the model weights and loading them into the `GPTModel`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing E.4 Loading a pretrained GPT model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Vocabulary size'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Context length'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Dropout rate'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Query-key-value bias'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that the model was loaded corrected, let’s double-check that it generates
    coherent text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that the model generates coherent text, which is
    an indicator that the model weights are loaded correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we prepare the model for classification fine-tuning, similar to chapter
    6, where we replace the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we calculate the initial classification accuracy of the not-fine-tuned
    model (we expect this to be around 50%, which means that the model is not able
    to distinguish between spam and nonspam messages yet reliably):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The initial prediction accuracies are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: E.4 Parameter-efficient fine-tuning with LoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we modify and fine-tune the LLM using LoRA. We begin by initializing a
    LoRALayer that creates the matrices *A* and *B*, along with the `alpha` scaling
    factor and the `rank` (*r*) setting. This layer can accept an input and compute
    the corresponding output, as illustrated in figure E.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/E-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure E.2 The LoRA matrices A and B are applied to the layer inputs and are
    involved in computing the model outputs. The inner dimension r of these matrices
    serves as a setting that adjusts the number of trainable parameters by varying
    the sizes of A and B.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In code, this LoRA layer can be implemented as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Listing E.5 Implementing a LoRA layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The same initialization used for Linear layers in PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: The `rank` governs the inner dimension of matrices *A* and *B*. Essentially,
    this setting determines the number of extra parameters introduced by LoRA, which
    creates balance between the adaptability of the model and its efficiency via the
    number of parameters used.
  prefs: []
  type: TYPE_NORMAL
- en: The other important setting, `alpha`, functions as a scaling factor for the
    output from the low-rank adaptation. It primarily dictates the degree to which
    the output from the adapted layer can affect the original layer’s output. This
    can be seen as a way to regulate the effect of the low-rank adaptation on the
    layer’s output. The `LoRALayer` class we have implemented so far enables us to
    transform the inputs of a layer.
  prefs: []
  type: TYPE_NORMAL
- en: In LoRA, the typical goal is to substitute existing `Linear` layers, allowing
    weight updates to be applied directly to the pre-existing pretrained weights,
    as illustrated in figure E.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/E-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure E.3 The integration of LoRA into a model layer. The original pretrained
    weights (W) of a layer are combined with the outputs from LoRA matrices (A and
    B), which approximate the weight update matrix (DW). The final output is calculated
    by adding the output of the adapted layer (using LoRA weights) to the original
    output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To integrate the original `Linear` layer weights, we now create a `LinearWithLoRA`
    layer. This layer utilizes the previously implemented `LoRALayer` and is designed
    to replace existing `Linear` layers within a neural network, such as the self-attention
    modules or feed-forward modules in the `GPTModel`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing E.6 Replacing `Linear` layers with `LinearWithLora` layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This code combines a standard `Linear` layer with the `LoRALayer`. The `forward`
    method computes the output by adding the results from the original linear layer
    and the LoRA layer.
  prefs: []
  type: TYPE_NORMAL
- en: Since the weight matrix *B* (`self.B` in `LoRALayer`) is initialized with zero
    values, the product of matrices *A* and *B* results in a zero matrix. This ensures
    that the multiplication does not alter the original weights, as adding zero does
    not change them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply LoRA to the earlier defined `GPTModel`, we introduce a `replace_linear_
    with_lora` function. This function will swap all existing `Linear` layers in the
    model with the newly created `LinearWithLoRA` layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Replaces the Linear layer with LinearWithLoRA'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Recursively applies the same function to child modules'
  prefs: []
  type: TYPE_NORMAL
- en: We have now implemented all the necessary code to replace the `Linear` layers
    in the `GPTModel` with the newly developed `LinearWithLoRA` layers for parameter-efficient
    fine-tuning. Next, we will apply the `LinearWithLoRA` upgrade to all `Linear`
    layers found in the multihead attention, feed-forward modules, and the output
    layer of the `GPTModel`, as shown in figure E.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/E-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure E.4 The architecture of the GPT model. It highlights the parts of the
    model where `Linear` layers are upgraded to `LinearWithLoRA` layers for parameter-efficient
    fine-tuning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Before we apply the `LinearWithLoRA` layer upgrades, we first freeze the original
    model parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can see that none of the 124 million model parameters are trainable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `replace_linear_with_lora` to replace the `Linear` layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After adding the LoRA layers, the number of trainable parameters is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we reduced the number of trainable parameters by almost 50× when
    using LoRA. A `rank` and `alpha` of 16 are good default choices, but it is also
    common to increase the rank parameter, which in turn increases the number of trainable
    parameters. Alpha is usually chosen to be half, double, or equal to the rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s verify that the layers have been modified as intended by printing the
    model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The model now includes the new `LinearWithLoRA` layers, which themselves consist
    of the original `Linear` layers, set to nontrainable, and the new LoRA layers,
    which we will fine-tune.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin fine-tuning the model, let’s calculate the initial classification
    accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The resulting accuracy values are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: These accuracy values are identical to the values from chapter 6\. This result
    occurs because we initialized the LoRA matrix *B* with zeros. Consequently, the
    product of matrices *AB* results in a zero matrix. This ensures that the multiplication
    does not alter the original weights since adding zero does not change them.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s move on to the exciting part—fine-tuning the model using the training
    function from chapter 6\. The training takes about 15 minutes on an M3 MacBook
    Air laptop and less than half a minute on a V100 or A100 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Listing E.7 Fine-tuning a model with LoRA layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output we see during the training is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Training the model with LoRA took longer than training it without LoRA (see
    chapter 6) because the LoRA layers introduce an additional computation during
    the forward pass. However, for larger models, where backpropagation becomes more
    costly, models typically train faster with LoRA than without it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the model received perfect training and very high validation
    accuracy. Let’s also visualize the loss curves to better see whether the training
    has converged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Figure E.5 plots the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/E-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure E.5 The training and validation loss curves over six epochs for a machine
    learning model. Initially, both training and validation loss decrease sharply
    and then they level off, indicating the model is converging, which means that
    it is not expected to improve noticeably with further training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In addition to evaluating the model based on the loss curves, let’s also calculate
    the accuracies on the full training, validation, and test set (during the training,
    we approximated the training and validation set accuracies from five batches via
    the `eval_iter=5` setting):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The resulting accuracy values are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: These results show that the model performs well across training, validation,
    and test datasets. With a training accuracy of 100%, the model has perfectly learned
    the training data. However, the slightly lower validation and test accuracies
    (96.64% and 97.33%, respectively) suggest a small degree of overfitting, as the
    model does not generalize quite as well on unseen data compared to the training
    set. Overall, the results are very impressive, considering we fine-tuned only
    a relatively small number of model weights (2.7 million LoRA weights instead of
    the original 124 million model weights).
  prefs: []
  type: TYPE_NORMAL
- en: index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SYMBOLS
  prefs: []
  type: TYPE_NORMAL
- en: '[124M parameter](../Text/chapter-5.html#p263)'
  prefs: []
  type: TYPE_NORMAL
- en: '[\[EOS] (end of sequence) token](../Text/chapter-2.html#p126)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.reshape method](../Text/appendix-a.html#p93), [2nd](../Text/appendix-a.html#p101)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.to() method](../Text/appendix-a.html#p78), [2nd](../Text/appendix-a.html#p293)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.weight attribute](../Text/chapter-5.html#p254)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.eval() mode](../Text/chapter-4.html#p221)'
  prefs: []
  type: TYPE_NORMAL
- en: '[__getitem__ method](../Text/appendix-a.html#p201)'
  prefs: []
  type: TYPE_NORMAL
- en: '[\[PAD] (padding) token](../Text/chapter-2.html#p127)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.T method](../Text/appendix-a.html#p102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.backward() method](../Text/chapter-4.html#p130), [2nd](../Text/appendix-d.html#p45)'
  prefs: []
  type: TYPE_NORMAL
- en: '[%timeit command](../Text/appendix-a.html#p317)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.matmul method](../Text/appendix-a.html#p106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[04_preference-tuning-with-dpo folder](../Text/chapter-7.html#p264)'
  prefs: []
  type: TYPE_NORMAL
- en: '[355M parameter](../Text/chapter-7.html#p131)'
  prefs: []
  type: TYPE_NORMAL
- en: '[\[BOS] (beginning of sequence) token](../Text/chapter-2.html#p125)'
  prefs: []
  type: TYPE_NORMAL
- en: '[\<|unk|> tokens](../Text/chapter-2.html#p96), [2nd](../Text/chapter-2.html#p98),
    [3rd](../Text/chapter-2.html#p100), [4th](../Text/chapter-2.html#p109), [5th](../Text/chapter-2.html#p147)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.view method](../Text/chapter-3.html#p293), [2nd](../Text/chapter-3.html#p294)'
  prefs: []
  type: TYPE_NORMAL
- en: '[__init__ constructor](../Text/chapter-3.html#p172), [2nd](../Text/chapter-4.html#p170),
    [3rd](../Text/appendix-a.html#p144)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.shape attribute](../Text/appendix-a.html#p89)'
  prefs: []
  type: TYPE_NORMAL
- en: '[@ operator](../Text/appendix-a.html#p110)'
  prefs: []
  type: TYPE_NORMAL
- en: '[__len__ method](../Text/appendix-a.html#p202)'
  prefs: []
  type: TYPE_NORMAL
- en: '[\<|endoftext|> token](../Text/chapter-2.html#p146)'
  prefs: []
  type: TYPE_NORMAL
- en: '[.pth extension](../Text/chapter-5.html#p242)'
  prefs: []
  type: TYPE_NORMAL
- en: <i>Dolma\
  prefs: []
  type: TYPE_NORMAL
- en: '[An Open Corpus of Three Trillion Tokens for LLM Pretraining Research</> (Soldaini
    et al.)](../Text/chapter-1.html#p64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[== comparison operator](../Text/appendix-a.html#p253)'
  prefs: []
  type: TYPE_NORMAL
- en: A
  prefs: []
  type: TYPE_NORMAL
- en: '[arXiv](../Text/chapter-7.html#p267)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Alpaca dataset](../Text/chapter-7.html#p172), [2nd](../Text/appendix-b.html#p88)'
  prefs: []
  type: TYPE_NORMAL
- en: '[argmax function](../Text/chapter-5.html#p43), [2nd](../Text/chapter-5.html#p45),
    [3rd](../Text/chapter-5.html#p189), [4th](../Text/chapter-5.html#p195), [5th](../Text/chapter-6.html#p146),
    [6th](../Text/appendix-a.html#p245), [7th](../Text/appendix-a.html#p249)'
  prefs: []
  type: TYPE_NORMAL
- en: attention mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: '[coding](../Text/chapter-3.html#p1), [2nd](../Text/chapter-3.html#p23)'
  prefs: []
  type: TYPE_NORMAL
- en: '[problem with modeling long sequences](../Text/chapter-3.html#p13)'
  prefs: []
  type: TYPE_NORMAL
- en: '[attention scores](../Text/chapter-3.html#p46)'
  prefs: []
  type: TYPE_NORMAL
- en: '[AI (artificial intelligence)](../Text/appendix-a.html#p14)'
  prefs: []
  type: TYPE_NORMAL
- en: '[autograd engine](../Text/appendix-a.html#p129)'
  prefs: []
  type: TYPE_NORMAL
- en: '[alpha scaling factor](../Text/appendix-e.html#p54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[autoregressive model](../Text/chapter-1.html#p72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[attention weights, computing step by step](../Text/chapter-3.html#p116), [2nd](../Text/chapter-3.html#p163)'
  prefs: []
  type: TYPE_NORMAL
- en: '[attn_scores](../Text/chapter-3.html#p173)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Axolotl](../Text/chapter-7.html#p270)'
  prefs: []
  type: TYPE_NORMAL
- en: '[allowed_max_length](../Text/appendix-c.html#p114)'
  prefs: []
  type: TYPE_NORMAL
- en: '[AdamW optimizer](../Text/chapter-5.html#p156), [2nd](../Text/appendix-b.html#p59)'
  prefs: []
  type: TYPE_NORMAL
- en: B
  prefs: []
  type: TYPE_NORMAL
- en: '[Bahdanau attention mechanism](../Text/chapter-3.html#p24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[backpropagation](../Text/chapter-5.html#p63)'
  prefs: []
  type: TYPE_NORMAL
- en: '[BERT (bidirectional encoder representations from transformers)](../Text/chapter-1.html#p48)'
  prefs: []
  type: TYPE_NORMAL
- en: '[BPE (byte pair encoding)](../Text/chapter-2.html#p129)'
  prefs: []
  type: TYPE_NORMAL
- en: '[batch_size](../Text/chapter-7.html#p173)'
  prefs: []
  type: TYPE_NORMAL
- en: C
  prefs: []
  type: TYPE_NORMAL
- en: '[compute_accuracy function](../Text/appendix-a.html#p262), [2nd](../Text/appendix-a.html#p264)'
  prefs: []
  type: TYPE_NORMAL
- en: '[causal attention mask](../Text/chapter-6.html#p137)'
  prefs: []
  type: TYPE_NORMAL
- en: '[clip_grad_norm_ function](../Text/appendix-d.html#p34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[calc_loss_loader function](../Text/chapter-5.html#p130)'
  prefs: []
  type: TYPE_NORMAL
- en: '[cross_entropy function](../Text/chapter-5.html#p77), [2nd](../Text/chapter-5.html#p80),
    [3rd](../Text/chapter-5.html#p91)'
  prefs: []
  type: TYPE_NORMAL
- en: '[conversational performance](../Text/chapter-7.html#p187)'
  prefs: []
  type: TYPE_NORMAL
- en: '[custom_collate_draft_1](../Text/chapter-7.html#p67)'
  prefs: []
  type: TYPE_NORMAL
- en: '[custom_collate_draft_2](../Text/chapter-7.html#p78)'
  prefs: []
  type: TYPE_NORMAL
- en: '[calc_accuracy_loader function](../Text/chapter-6.html#p157)'
  prefs: []
  type: TYPE_NORMAL
- en: '[calc_loss_batch function](../Text/chapter-5.html#p132), [2nd](../Text/chapter-6.html#p165),
    [3rd](../Text/chapter-6.html#p167)'
  prefs: []
  type: TYPE_NORMAL
- en: classification
  prefs: []
  type: TYPE_NORMAL
- en: '[tasks](../Text/chapter-1.html#p41)'
  prefs: []
  type: TYPE_NORMAL
- en: '[custom_collate_fn function](../Text/chapter-7.html#p112), [2nd](../Text/appendix-c.html#p104)'
  prefs: []
  type: TYPE_NORMAL
- en: '[classify_review function](../Text/chapter-6.html#p202)'
  prefs: []
  type: TYPE_NORMAL
- en: '[context_length](../Text/chapter-4.html#p23)'
  prefs: []
  type: TYPE_NORMAL
- en: '[cfg dictionary](../Text/chapter-4.html#p150)'
  prefs: []
  type: TYPE_NORMAL
- en: '[computing gradients](../Text/appendix-a.html#p62)'
  prefs: []
  type: TYPE_NORMAL
- en: '[context vectors](../Text/chapter-3.html#p41), [2nd](../Text/chapter-3.html#p106),
    [3rd](../Text/chapter-3.html#p280)'
  prefs: []
  type: TYPE_NORMAL
- en: '[CausalAttention class](../Text/chapter-3.html#p250), [2nd](../Text/chapter-3.html#p258)'
  prefs: []
  type: TYPE_NORMAL
- en: D
  prefs: []
  type: TYPE_NORMAL
- en: '[DistributedSampler](../Text/appendix-a.html#p325)'
  prefs: []
  type: TYPE_NORMAL
- en: '[dim parameter](../Text/chapter-4.html#p65), [2nd](../Text/chapter-4.html#p65)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset class](../Text/chapter-2.html#p181), [2nd](../Text/chapter-6.html#p59),
    [3rd](../Text/appendix-a.html#p193), [4th](../Text/appendix-a.html#p194), [5th](../Text/appendix-a.html#p200),
    [6th](../Text/appendix-a.html#p206), [7th](../Text/appendix-a.html#p223)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DataLoader class](../Text/chapter-7.html#p53), [2nd](../Text/chapter-7.html#p118)'
  prefs: []
  type: TYPE_NORMAL
- en: datasets
  prefs: []
  type: TYPE_NORMAL
- en: '[downloading](../Text/chapter-7.html#p19)'
  prefs: []
  type: TYPE_NORMAL
- en: '[download_and_load_gpt2 function](../Text/chapter-5.html#p261), [2nd](../Text/chapter-5.html#p274),
    [3rd](../Text/chapter-6.html#p88)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DummyGPTClass](../Text/chapter-4.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DistributedDataParallel class](../Text/appendix-a.html#p333)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DummyLayerNorm](../Text/chapter-4.html#p35), [2nd](../Text/chapter-4.html#p48)'
  prefs: []
  type: TYPE_NORMAL
- en: '[placeholder](../Text/chapter-4.html#p52)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DummyGPTModel](../Text/chapter-4.html#p29), [2nd](../Text/chapter-4.html#p31),
    [3rd](../Text/chapter-4.html#p33), [4th](../Text/chapter-4.html#p42)'
  prefs: []
  type: TYPE_NORMAL
- en: '[deep learning](../Text/appendix-a.html#p18)'
  prefs: []
  type: TYPE_NORMAL
- en: '[dot products](../Text/chapter-3.html#p53)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DDP (DistributedDataParallel) strategy](../Text/appendix-a.html#p322)'
  prefs: []
  type: TYPE_NORMAL
- en: '[device variable](../Text/chapter-7.html#p115)'
  prefs: []
  type: TYPE_NORMAL
- en: '[decode method](../Text/chapter-2.html#p142), [2nd](../Text/chapter-2.html#p152)'
  prefs: []
  type: TYPE_NORMAL
- en: '[data loaders](../Text/chapter-6.html#p49), [2nd](../Text/chapter-6.html#p82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[code for](../Text/appendix-c.html#p14)'
  prefs: []
  type: TYPE_NORMAL
- en: dropout
  prefs: []
  type: TYPE_NORMAL
- en: '[defined](../Text/chapter-3.html#p234)'
  prefs: []
  type: TYPE_NORMAL
- en: '[drop_rate](../Text/chapter-4.html#p27)'
  prefs: []
  type: TYPE_NORMAL
- en: '[drop_last parameter](../Text/appendix-a.html#p213)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DummyTransformerBlock](../Text/chapter-4.html#p163)'
  prefs: []
  type: TYPE_NORMAL
- en: '[data list](../Text/chapter-7.html#p22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ddp_setup function](../Text/appendix-a.html#p340)'
  prefs: []
  type: TYPE_NORMAL
- en: '[d_out argument](../Text/chapter-3.html#p312), [2nd](../Text/appendix-c.html#p27)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DataFrame](../Text/chapter-6.html#p26)'
  prefs: []
  type: TYPE_NORMAL
- en: E
  prefs: []
  type: TYPE_NORMAL
- en: '[eps variable](../Text/chapter-4.html#p78)'
  prefs: []
  type: TYPE_NORMAL
- en: '[evaluate_model function](../Text/chapter-5.html#p150), [2nd](../Text/chapter-5.html#p150),
    [3rd](../Text/chapter-5.html#p154), [4th](../Text/chapter-6.html#p179)'
  prefs: []
  type: TYPE_NORMAL
- en: '[embedding size](../Text/chapter-2.html#p239)'
  prefs: []
  type: TYPE_NORMAL
- en: '[emergent behavior](../Text/chapter-1.html#p77)'
  prefs: []
  type: TYPE_NORMAL
- en: '[encoder](../Text/chapter-3.html#p15)'
  prefs: []
  type: TYPE_NORMAL
- en: '[encode method](../Text/chapter-2.html#p76), [2nd](../Text/chapter-2.html#p138),
    [3rd](../Text/chapter-2.html#p175)'
  prefs: []
  type: TYPE_NORMAL
- en: '[emb_dim](../Text/chapter-4.html#p24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[eval_iter value](../Text/chapter-6.html#p196)'
  prefs: []
  type: TYPE_NORMAL
- en: F
  prefs: []
  type: TYPE_NORMAL
- en: '[find_highest_gradient function](../Text/appendix-d.html#p46)'
  prefs: []
  type: TYPE_NORMAL
- en: '[first_batch variable](../Text/chapter-2.html#p188)'
  prefs: []
  type: TYPE_NORMAL
- en: '[FeedForward module](../Text/chapter-4.html#p106), [2nd](../Text/chapter-4.html#p108),
    [3rd](../Text/chapter-4.html#p112), [4th](../Text/chapter-4.html#p146)'
  prefs: []
  type: TYPE_NORMAL
- en: '[format_input function](../Text/chapter-7.html#p35), [2nd](../Text/chapter-7.html#p37),
    [3rd](../Text/chapter-7.html#p41), [4th](../Text/chapter-7.html#p230), [5th](../Text/appendix-c.html#p94),
    [6th](../Text/appendix-c.html#p96)'
  prefs: []
  type: TYPE_NORMAL
- en: fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: '[LLMs, to follow instructions](../Text/chapter-7.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[categories of](../Text/chapter-6.html#p11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[for classification](../Text/chapter-6.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[forward method](../Text/chapter-4.html#p34), [2nd](../Text/chapter-4.html#p121)'
  prefs: []
  type: TYPE_NORMAL
- en: G
  prefs: []
  type: TYPE_NORMAL
- en: '[generate_and_print_sample function](../Text/chapter-5.html#p152)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GELU (Gaussian error linear unit)](../Text/appendix-b.html#p41)'
  prefs: []
  type: TYPE_NORMAL
- en: '[activation function](../Text/chapter-4.html#p85), [2nd](../Text/chapter-4.html#p123)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPTModel](../Text/chapter-4.html#p169), [2nd](../Text/chapter-4.html#p192),
    [3rd](../Text/chapter-4.html#p195), [4th](../Text/chapter-4.html#p199), [5th](../Text/chapter-5.html#p31),
    [6th](../Text/appendix-e.html#p66)'
  prefs: []
  type: TYPE_NORMAL
- en: '[class](../Text/chapter-4.html#p202), [2nd](../Text/chapter-6.html#p90)'
  prefs: []
  type: TYPE_NORMAL
- en: '[code](../Text/chapter-5.html#p114)'
  prefs: []
  type: TYPE_NORMAL
- en: '[instance](../Text/chapter-5.html#p18), [2nd](../Text/chapter-5.html#p238),
    [3rd](../Text/chapter-5.html#p286), [4th](../Text/chapter-5.html#p300)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT (Generative Pre-trained Transformer)](../Text/chapter-1.html#p48)'
  prefs: []
  type: TYPE_NORMAL
- en: '[architecture](../Text/chapter-1.html#p69)'
  prefs: []
  type: TYPE_NORMAL
- en: '[coding](../Text/chapter-4.html#p162), [2nd](../Text/chapter-4.html#p200)'
  prefs: []
  type: TYPE_NORMAL
- en: '[implementing from scratch to generate text](../Text/chapter-4.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[grad_fn value](../Text/appendix-a.html#p180)'
  prefs: []
  type: TYPE_NORMAL
- en: '[gpt_download.py Python module](../Text/chapter-5.html#p258)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT_CONFIG_124M dictionary](../Text/chapter-4.html#p21), [2nd](../Text/chapter-4.html#p153),
    [3rd](../Text/chapter-4.html#p164), [4th](../Text/chapter-4.html#p173), [5th](../Text/chapter-4.html#p231),
    [6th](../Text/chapter-5.html#p16)'
  prefs: []
  type: TYPE_NORMAL
- en: '[generative text models, evaluating](../Text/chapter-5.html#p11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GenAI (generative AI)](../Text/chapter-1.html#p16)'
  prefs: []
  type: TYPE_NORMAL
- en: '[gpt2-medium355M-sft.pth file](../Text/chapter-7.html#p200)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPTDatasetV1 class](../Text/chapter-2.html#p180), [2nd](../Text/chapter-2.html#p182),
    [3rd](../Text/chapter-2.html#p184)'
  prefs: []
  type: TYPE_NORMAL
- en: '[generate_text_simple function](../Text/chapter-4.html#p213), [2nd](../Text/chapter-4.html#p215),
    [3rd](../Text/chapter-4.html#p217), [4th](../Text/chapter-5.html#p42), [5th](../Text/chapter-5.html#p181),
    [6th](../Text/chapter-5.html#p186)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-4](../Text/chapter-7.html#p209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-2](../Text/chapter-4.html#p18)'
  prefs: []
  type: TYPE_NORMAL
- en: '[model](../Text/chapter-7.html#p158)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tokenizer](../Text/chapter-6.html#p54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-3](../Text/chapter-1.html#p62)'
  prefs: []
  type: TYPE_NORMAL
- en: '[generate_model_scores function](../Text/chapter-7.html#p244), [2nd](../Text/chapter-7.html#p246)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Google Colab](../Text/appendix-a.html#p48)'
  prefs: []
  type: TYPE_NORMAL
- en: '[generate function](../Text/chapter-5.html#p235), [2nd](../Text/chapter-5.html#p294),
    [3rd](../Text/chapter-7.html#p142), [4th](../Text/chapter-7.html#p177), [5th](../Text/chapter-7.html#p179),
    [6th](../Text/chapter-7.html#p192), [7th](../Text/appendix-c.html#p60)'
  prefs: []
  type: TYPE_NORMAL
- en: I
  prefs: []
  type: TYPE_NORMAL
- en: '[init_process_group function](../Text/appendix-a.html#p335)'
  prefs: []
  type: TYPE_NORMAL
- en: '[instruction dataset](../Text/chapter-7.html#p10)'
  prefs: []
  type: TYPE_NORMAL
- en: '[information leakage](../Text/chapter-3.html#p216)'
  prefs: []
  type: TYPE_NORMAL
- en: '[input_embeddings](../Text/chapter-2.html#p261)'
  prefs: []
  type: TYPE_NORMAL
- en: '[InstructionDataset class](../Text/chapter-7.html#p55), [2nd](../Text/appendix-c.html#p102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[instruction fine-tuning](../Text/chapter-1.html#p41)'
  prefs: []
  type: TYPE_NORMAL
- en: '[instruction following, creating data loaders for instruction dataset](../Text/chapter-7.html#p110),
    [2nd](../Text/chapter-7.html#p126)'
  prefs: []
  type: TYPE_NORMAL
- en: '[overview](../Text/chapter-7.html#p12)'
  prefs: []
  type: TYPE_NORMAL
- en: '[’instruction’ object](../Text/chapter-7.html#p26)'
  prefs: []
  type: TYPE_NORMAL
- en: K
  prefs: []
  type: TYPE_NORMAL
- en: '[keepdim parameter](../Text/chapter-4.html#p64)'
  prefs: []
  type: TYPE_NORMAL
- en: L
  prefs: []
  type: TYPE_NORMAL
- en: '[logits tensor](../Text/chapter-5.html#p85)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LinearWithLoRA layer](../Text/appendix-e.html#p70), [2nd](../Text/appendix-e.html#p83)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LoRALayer class](../Text/appendix-e.html#p59), [2nd](../Text/appendix-e.html#p64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[loss metric](../Text/chapter-5.html#p26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LLMs (large language models)](../Text/chapter-2.html#p7), [2nd](../Text/chapter-2.html#p10)'
  prefs: []
  type: TYPE_NORMAL
- en: '[applications of](../Text/chapter-1.html#p24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[building and using](../Text/chapter-1.html#p31), [2nd](../Text/chapter-1.html#p42),
    [3rd](../Text/chapter-1.html#p79)'
  prefs: []
  type: TYPE_NORMAL
- en: '[coding architecture](../Text/chapter-4.html#p11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[coding attention mechanisms, causal attention mechanism](../Text/chapter-3.html#p194),
    [2nd](../Text/chapter-3.html#p262)'
  prefs: []
  type: TYPE_NORMAL
- en: '[fine-tuning](../Text/chapter-7.html#p149), [2nd](../Text/chapter-7.html#p204),
    [3rd](../Text/appendix-b.html#p72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[fine-tuning for classification](../Text/chapter-6.html#p100), [2nd](../Text/chapter-6.html#p140),
    [3rd](../Text/chapter-6.html#p144), [4th](../Text/chapter-6.html#p173), [5th](../Text/chapter-6.html#p202)'
  prefs: []
  type: TYPE_NORMAL
- en: '[instruction fine-tuning, loading pretrained LLMs](../Text/chapter-7.html#p128),
    [2nd](../Text/chapter-7.html#p147)'
  prefs: []
  type: TYPE_NORMAL
- en: '[overview of](../Text/chapter-1.html#p1), [2nd](../Text/chapter-1.html#p13),
    [3rd](../Text/chapter-1.html#p22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[utilizing large datasets](../Text/chapter-1.html#p57)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linear layers](../Text/appendix-e.html#p60), [2nd](../Text/appendix-e.html#p68)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LayerNorm](../Text/chapter-4.html#p81), [2nd](../Text/chapter-4.html#p151),
    [3rd](../Text/chapter-4.html#p171)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LIMA dataset](../Text/appendix-b.html#p90)'
  prefs: []
  type: TYPE_NORMAL
- en: '[layer normalization](../Text/chapter-4.html#p51), [2nd](../Text/chapter-4.html#p88)'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_state_dict method](../Text/chapter-5.html#p248)'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_weights_into_gpt function](../Text/chapter-5.html#p288), [2nd](../Text/chapter-5.html#p290),
    [3rd](../Text/chapter-5.html#p291), [4th](../Text/chapter-5.html#p292)'
  prefs: []
  type: TYPE_NORMAL
- en: '[loss.backward() function](../Text/chapter-4.html#p128)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linear layer weights](../Text/appendix-e.html#p62)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Llama 3 model](../Text/chapter-7.html#p206)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LLama 2 model](../Text/chapter-5.html#p107)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LoRA (low-rank adaptation)](../Text/chapter-7.html#p259), [2nd](../Text/appendix-e.html#p2),
    [3rd](../Text/appendix-e.html#p4)'
  prefs: []
  type: TYPE_NORMAL
- en: '[parameter-efficient fine-tuning](../Text/appendix-e.html#p25), [2nd](../Text/appendix-e.html#p41)'
  prefs: []
  type: TYPE_NORMAL
- en: M
  prefs: []
  type: TYPE_NORMAL
- en: '[main function](../Text/appendix-a.html#p338)'
  prefs: []
  type: TYPE_NORMAL
- en: '[max_length](../Text/chapter-5.html#p115), [2nd](../Text/chapter-6.html#p62),
    [3rd](../Text/appendix-c.html#p82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[model.eval() function](../Text/chapter-5.html#p245)'
  prefs: []
  type: TYPE_NORMAL
- en: '[MultiHeadAttention class](../Text/chapter-3.html#p290), [2nd](../Text/chapter-3.html#p309),
    [3rd](../Text/chapter-3.html#p310), [4th](../Text/chapter-3.html#p314), [5th](../Text/chapter-3.html#p317),
    [6th](../Text/appendix-b.html#p32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[model.train() setting](../Text/appendix-a.html#p233)'
  prefs: []
  type: TYPE_NORMAL
- en: '[MultiHeadAttentionWrapper class](../Text/chapter-3.html#p272), [2nd](../Text/chapter-3.html#p274),
    [3rd](../Text/chapter-3.html#p282), [4th](../Text/chapter-3.html#p283), [5th](../Text/chapter-3.html#p286),
    [6th](../Text/chapter-3.html#p287), [7th](../Text/chapter-3.html#p291)'
  prefs: []
  type: TYPE_NORMAL
- en: '[machine learning](../Text/appendix-a.html#p15)'
  prefs: []
  type: TYPE_NORMAL
- en: '[multi-head attention](../Text/chapter-3.html#p249), [2nd](../Text/chapter-3.html#p265)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Module base class](../Text/appendix-a.html#p143)'
  prefs: []
  type: TYPE_NORMAL
- en: '[multiprocessing submodule](../Text/appendix-a.html#p334)'
  prefs: []
  type: TYPE_NORMAL
- en: '[masked attention](../Text/chapter-3.html#p194)'
  prefs: []
  type: TYPE_NORMAL
- en: '[multinomial function](../Text/chapter-5.html#p191), [2nd](../Text/chapter-5.html#p202),
    [3rd](../Text/chapter-5.html#p203)'
  prefs: []
  type: TYPE_NORMAL
- en: '[macOS](../Text/appendix-a.html#p312)'
  prefs: []
  type: TYPE_NORMAL
- en: '[model_response](../Text/chapter-7.html#p198)'
  prefs: []
  type: TYPE_NORMAL
- en: '[minbpe repository](../Text/appendix-b.html#p21)'
  prefs: []
  type: TYPE_NORMAL
- en: '[model_configs table](../Text/chapter-5.html#p278)'
  prefs: []
  type: TYPE_NORMAL
- en: '[mps device](../Text/chapter-7.html#p113)'
  prefs: []
  type: TYPE_NORMAL
- en: N
  prefs: []
  type: TYPE_NORMAL
- en: '[NEW_CONFIG dictionary](../Text/chapter-5.html#p284)'
  prefs: []
  type: TYPE_NORMAL
- en: neural networks
  prefs: []
  type: TYPE_NORMAL
- en: '[implementing feed forward network with GELU activations](../Text/chapter-4.html#p92),
    [2nd](../Text/chapter-4.html#p115)'
  prefs: []
  type: TYPE_NORMAL
- en: '[nn.Linear layers](../Text/chapter-3.html#p181)'
  prefs: []
  type: TYPE_NORMAL
- en: '[n_heads](../Text/chapter-4.html#p25)'
  prefs: []
  type: TYPE_NORMAL
- en: '[numel() method](../Text/chapter-4.html#p178)'
  prefs: []
  type: TYPE_NORMAL
- en: '[num_heads dimension](../Text/chapter-3.html#p295)'
  prefs: []
  type: TYPE_NORMAL
- en: O
  prefs: []
  type: TYPE_NORMAL
- en: '[output layer nodes](../Text/chapter-6.html#p103)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ollama run llama3 command](../Text/chapter-7.html#p216), [2nd](../Text/chapter-7.html#p220),
    [3rd](../Text/chapter-7.html#p225)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ollama serve command](../Text/chapter-7.html#p213), [2nd](../Text/chapter-7.html#p214),
    [3rd](../Text/chapter-7.html#p215), [4th](../Text/chapter-7.html#p228)'
  prefs: []
  type: TYPE_NORMAL
- en: '[optimizer.zero_grad() method](../Text/appendix-a.html#p235)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ollama application](../Text/chapter-7.html#p207), [2nd](../Text/chapter-7.html#p219)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ollama Llama 3 method](../Text/appendix-c.html#p106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ollama run command](../Text/chapter-7.html#p232)'
  prefs: []
  type: TYPE_NORMAL
- en: P
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: '[and Torch](../Text/appendix-a.html#p41)'
  prefs: []
  type: TYPE_NORMAL
- en: '[automatic differentiation](../Text/appendix-a.html#p124), [2nd](../Text/appendix-a.html#p139)'
  prefs: []
  type: TYPE_NORMAL
- en: '[computation graphs](../Text/appendix-a.html#p116)'
  prefs: []
  type: TYPE_NORMAL
- en: '[data loaders](../Text/chapter-7.html#p45)'
  prefs: []
  type: TYPE_NORMAL
- en: '[dataset objects](../Text/appendix-e.html#p30)'
  prefs: []
  type: TYPE_NORMAL
- en: '[efficient data loaders](../Text/appendix-a.html#p192)'
  prefs: []
  type: TYPE_NORMAL
- en: '[implementing multilayer neural networks](../Text/appendix-a.html#p141), [2nd](../Text/appendix-a.html#p190)'
  prefs: []
  type: TYPE_NORMAL
- en: '[installing](../Text/appendix-a.html#p24), [2nd](../Text/appendix-a.html#p51)'
  prefs: []
  type: TYPE_NORMAL
- en: '[loading and saving model weights in](../Text/chapter-5.html#p237)'
  prefs: []
  type: TYPE_NORMAL
- en: '[optimizing training performance with GPUs](../Text/appendix-a.html#p282)'
  prefs: []
  type: TYPE_NORMAL
- en: '[overview](../Text/appendix-a.html#p2), [2nd](../Text/appendix-a.html#p6)'
  prefs: []
  type: TYPE_NORMAL
- en: '[with a NumPy-like API](../Text/appendix-a.html#p59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[pip installer](../Text/chapter-2.html#p132)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Phi-3 model](../Text/appendix-b.html#p93)'
  prefs: []
  type: TYPE_NORMAL
- en: '[print_gradients function](../Text/chapter-4.html#p131), [2nd](../Text/chapter-4.html#p135)'
  prefs: []
  type: TYPE_NORMAL
- en: '[plot_values function](../Text/chapter-6.html#p192)'
  prefs: []
  type: TYPE_NORMAL
- en: parameters
  prefs: []
  type: TYPE_NORMAL
- en: '[calculating](../Text/appendix-c.html#p34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[perplexity](../Text/chapter-5.html#p96)'
  prefs: []
  type: TYPE_NORMAL
- en: '[partial derivatives](../Text/appendix-a.html#p127)'
  prefs: []
  type: TYPE_NORMAL
- en: '[print statement](../Text/chapter-2.html#p59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[plot_losses function](../Text/chapter-7.html#p166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Python version](../Text/appendix-a.html#p26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Prometheus model](../Text/appendix-b.html#p96)'
  prefs: []
  type: TYPE_NORMAL
- en: '[prompt styles](../Text/chapter-7.html#p30)'
  prefs: []
  type: TYPE_NORMAL
- en: '[pretraining](../Text/chapter-1.html#p39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[calculating training and validation set losses](../Text/chapter-5.html#p102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[on unlabeled data](../Text/chapter-5.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[training LLMs](../Text/chapter-5.html#p143), [2nd](../Text/chapter-5.html#p170)'
  prefs: []
  type: TYPE_NORMAL
- en: '[print_sampled_tokens function](../Text/chapter-5.html#p205), [2nd](../Text/appendix-c.html#p51)'
  prefs: []
  type: TYPE_NORMAL
- en: '[pos_embeddings](../Text/chapter-2.html#p254), [2nd](../Text/chapter-2.html#p257)'
  prefs: []
  type: TYPE_NORMAL
- en: '[preference fine-tuning](../Text/appendix-b.html#p101)'
  prefs: []
  type: TYPE_NORMAL
- en: Q
  prefs: []
  type: TYPE_NORMAL
- en: '[qkv_bias](../Text/chapter-4.html#p28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[query_llama function](../Text/chapter-7.html#p235)'
  prefs: []
  type: TYPE_NORMAL
- en: '[query_model function](../Text/chapter-7.html#p239)'
  prefs: []
  type: TYPE_NORMAL
- en: R
  prefs: []
  type: TYPE_NORMAL
- en: '[responses, extracting and saving](../Text/chapter-7.html#p175), [2nd](../Text/chapter-7.html#p202)'
  prefs: []
  type: TYPE_NORMAL
- en: '[re library](../Text/chapter-2.html#p36)'
  prefs: []
  type: TYPE_NORMAL
- en: '[RMSNorm](../Text/appendix-b.html#p40)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ReLU (rectified linear unit)](../Text/chapter-4.html#p58), [2nd](../Text/chapter-4.html#p93)'
  prefs: []
  type: TYPE_NORMAL
- en: '[re.split command](../Text/chapter-2.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[replace_linear_with_lora function](../Text/appendix-e.html#p74)'
  prefs: []
  type: TYPE_NORMAL
- en: '[raw text](../Text/chapter-1.html#p37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[retrieval-augmented generation](../Text/chapter-2.html#p17)'
  prefs: []
  type: TYPE_NORMAL
- en: '[RNNs (recurrent neural networks)](../Text/chapter-3.html#p16)'
  prefs: []
  type: TYPE_NORMAL
- en: '[random_split function](../Text/chapter-6.html#p43)'
  prefs: []
  type: TYPE_NORMAL
- en: S
  prefs: []
  type: TYPE_NORMAL
- en: '[shortcut connections](../Text/chapter-4.html#p118), [2nd](../Text/chapter-4.html#p142)'
  prefs: []
  type: TYPE_NORMAL
- en: '[saving and loading models](../Text/appendix-a.html#p274)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SimpleTokenizerV1 class](../Text/chapter-2.html#p78), [2nd](../Text/chapter-2.html#p80)'
  prefs: []
  type: TYPE_NORMAL
- en: '[spawn function](../Text/appendix-a.html#p337)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sequential class](../Text/appendix-a.html#p152)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SelfAttention_v2 class](../Text/chapter-3.html#p183), [2nd](../Text/chapter-3.html#p189),
    [3rd](../Text/chapter-3.html#p190)'
  prefs: []
  type: TYPE_NORMAL
- en: '[softmax_naive function](../Text/chapter-3.html#p68), [2nd](../Text/chapter-3.html#p70)'
  prefs: []
  type: TYPE_NORMAL
- en: '[sci_mode parameter](../Text/chapter-4.html#p72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[set_printoptions method](../Text/appendix-a.html#p244)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SGD (stochastic gradient descent)](../Text/appendix-a.html#p229)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SelfAttention_v1 class](../Text/chapter-3.html#p171), [2nd](../Text/chapter-3.html#p187)'
  prefs: []
  type: TYPE_NORMAL
- en: '[softmax function](../Text/appendix-a.html#p186), [2nd](../Text/appendix-a.html#p234),
    [3rd](../Text/appendix-a.html#p240)'
  prefs: []
  type: TYPE_NORMAL
- en: '[self.register_buffer() call](../Text/chapter-3.html#p257)'
  prefs: []
  type: TYPE_NORMAL
- en: '[state_dict](../Text/chapter-5.html#p246), [2nd](../Text/appendix-a.html#p276)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SpamDataset class](../Text/chapter-6.html#p58), [2nd](../Text/chapter-6.html#p60),
    [3rd](../Text/chapter-6.html#p65)'
  prefs: []
  type: TYPE_NORMAL
- en: '[special context tokens](../Text/chapter-2.html#p118)'
  prefs: []
  type: TYPE_NORMAL
- en: '[stride setting](../Text/chapter-2.html#p193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[self.out_proj layer](../Text/chapter-3.html#p308)'
  prefs: []
  type: TYPE_NORMAL
- en: '[supervised learning](../Text/appendix-a.html#p19)'
  prefs: []
  type: TYPE_NORMAL
- en: '[supervised data, fine-tuning model on](../Text/chapter-6.html#p175)'
  prefs: []
  type: TYPE_NORMAL
- en: '[strip() function](../Text/chapter-7.html#p145)'
  prefs: []
  type: TYPE_NORMAL
- en: supervised instruction fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: '[preparing dataset for](../Text/chapter-7.html#p17), [2nd](../Text/chapter-7.html#p49)'
  prefs: []
  type: TYPE_NORMAL
- en: '[settings dictionary](../Text/chapter-5.html#p270), [2nd](../Text/chapter-5.html#p276)'
  prefs: []
  type: TYPE_NORMAL
- en: '[self-attention mechanism](../Text/chapter-3.html#p31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[computing attention weights for all input tokens](../Text/chapter-3.html#p79),
    [2nd](../Text/chapter-3.html#p108)'
  prefs: []
  type: TYPE_NORMAL
- en: '[implementing with trainable weights](../Text/chapter-3.html#p110), [2nd](../Text/chapter-3.html#p192)'
  prefs: []
  type: TYPE_NORMAL
- en: '[without trainable weights](../Text/chapter-3.html#p37), [2nd](../Text/chapter-3.html#p77)'
  prefs: []
  type: TYPE_NORMAL
- en: '[single-head attention, stacking multiple layers](../Text/chapter-3.html#p269)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SimpleTokenizerV2 class](../Text/chapter-2.html#p96), [2nd](../Text/chapter-2.html#p109)'
  prefs: []
  type: TYPE_NORMAL
- en: T
  prefs: []
  type: TYPE_NORMAL
- en: '[text generation function, modifying](../Text/chapter-5.html#p225)'
  prefs: []
  type: TYPE_NORMAL
- en: '[train_ratio](../Text/chapter-5.html#p118)'
  prefs: []
  type: TYPE_NORMAL
- en: '[text data](../Text/chapter-2.html#p1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[adding special context tokens](../Text/chapter-2.html#p96), [2nd](../Text/chapter-2.html#p128)'
  prefs: []
  type: TYPE_NORMAL
- en: '[creating token embeddings](../Text/chapter-2.html#p208)'
  prefs: []
  type: TYPE_NORMAL
- en: '[sliding window](../Text/chapter-2.html#p155), [2nd](../Text/chapter-2.html#p202)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tokenization, byte pair encoding](../Text/chapter-2.html#p131), [2nd](../Text/chapter-2.html#p153)'
  prefs: []
  type: TYPE_NORMAL
- en: '[torch.save function](../Text/chapter-5.html#p240)'
  prefs: []
  type: TYPE_NORMAL
- en: '[token IDs](../Text/chapter-2.html#p64), [2nd](../Text/chapter-2.html#p94)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tensor library](../Text/appendix-a.html#p11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[TransformerBlock class](../Text/chapter-4.html#p149)'
  prefs: []
  type: TYPE_NORMAL
- en: '[token_embedding_layer](../Text/chapter-2.html#p241), [2nd](../Text/chapter-2.html#p252)'
  prefs: []
  type: TYPE_NORMAL
- en: '[token embeddings](../Text/chapter-2.html#p204), [2nd](../Text/chapter-2.html#p229)'
  prefs: []
  type: TYPE_NORMAL
- en: '[train_simple_function](../Text/appendix-c.html#p66)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ToyDataset class](../Text/appendix-a.html#p197), [2nd](../Text/appendix-a.html#p199)'
  prefs: []
  type: TYPE_NORMAL
- en: training function
  prefs: []
  type: TYPE_NORMAL
- en: '[enhancing](../Text/appendix-d.html#p2)'
  prefs: []
  type: TYPE_NORMAL
- en: '[modified](../Text/appendix-d.html#p55), [2nd](../Text/appendix-d.html#p63)'
  prefs: []
  type: TYPE_NORMAL
- en: '[train_data subset](../Text/chapter-5.html#p120)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tril function](../Text/chapter-3.html#p204)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tokenizing text](../Text/chapter-2.html#p25), [2nd](../Text/chapter-2.html#p61)'
  prefs: []
  type: TYPE_NORMAL
- en: training, optimizing performance with GPUs
  prefs: []
  type: TYPE_NORMAL
- en: '[PyTorch computations on GPU devices](../Text/appendix-a.html#p284)'
  prefs: []
  type: TYPE_NORMAL
- en: '[selecting available GPUs on multi-GPU machine](../Text/appendix-a.html#p342),
    [2nd](../Text/appendix-a.html#p357)'
  prefs: []
  type: TYPE_NORMAL
- en: '[single-GPU training](../Text/appendix-a.html#p304)'
  prefs: []
  type: TYPE_NORMAL
- en: '[training with multiple GPUs](../Text/appendix-a.html#p320)'
  prefs: []
  type: TYPE_NORMAL
- en: '[test_loader](../Text/appendix-a.html#p208)'
  prefs: []
  type: TYPE_NORMAL
- en: '[train_loader](../Text/appendix-a.html#p212)'
  prefs: []
  type: TYPE_NORMAL
- en: '[torch.sum method](../Text/appendix-a.html#p257)'
  prefs: []
  type: TYPE_NORMAL
- en: '[training loops](../Text/appendix-a.html#p225), [2nd](../Text/appendix-a.html#p271)'
  prefs: []
  type: TYPE_NORMAL
- en: '[cosine decay](../Text/appendix-d.html#p24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[gradient clipping](../Text/appendix-d.html#p33)'
  prefs: []
  type: TYPE_NORMAL
- en: '[learning rate warmup](../Text/appendix-d.html#p10)'
  prefs: []
  type: TYPE_NORMAL
- en: '[train_classifier_simple function](../Text/chapter-6.html#p181), [2nd](../Text/chapter-6.html#p194)'
  prefs: []
  type: TYPE_NORMAL
- en: '[training batches, organizing data into](../Text/chapter-7.html#p51), [2nd](../Text/chapter-7.html#p108)'
  prefs: []
  type: TYPE_NORMAL
- en: '[text generation](../Text/chapter-4.html#p204)'
  prefs: []
  type: TYPE_NORMAL
- en: '[using GPT to generate text](../Text/chapter-5.html#p14)'
  prefs: []
  type: TYPE_NORMAL
- en: '[top-k sampling](../Text/chapter-5.html#p207), [2nd](../Text/chapter-5.html#p208)'
  prefs: []
  type: TYPE_NORMAL
- en: '[text_data](../Text/appendix-d.html#p7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[transformer architecture](../Text/chapter-1.html#p15), [2nd](../Text/chapter-1.html#p44),
    [3rd](../Text/chapter-1.html#p55), [4th](../Text/chapter-3.html#p26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[temperature scaling](../Text/chapter-5.html#p181), [2nd](../Text/chapter-5.html#p196)'
  prefs: []
  type: TYPE_NORMAL
- en: '[train_model_simple function](../Text/chapter-5.html#p147), [2nd](../Text/chapter-5.html#p149),
    [3rd](../Text/chapter-5.html#p157), [4th](../Text/chapter-5.html#p245), [5th](../Text/chapter-5.html#p251),
    [6th](../Text/chapter-6.html#p177)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tensor2d](../Text/appendix-a.html#p67)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tensor3d](../Text/appendix-a.html#p67)'
  prefs: []
  type: TYPE_NORMAL
- en: '[torch.no_grad() context manager](../Text/appendix-a.html#p182)'
  prefs: []
  type: TYPE_NORMAL
- en: '[test_set dictionary](../Text/chapter-7.html#p191), [2nd](../Text/chapter-7.html#p196)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tensors](../Text/appendix-a.html#p59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[common tensor operations](../Text/appendix-a.html#p84)'
  prefs: []
  type: TYPE_NORMAL
- en: '[tensor data types](../Text/appendix-a.html#p69)'
  prefs: []
  type: TYPE_NORMAL
- en: '[torch.nn.Linear layers](../Text/appendix-a.html#p158)'
  prefs: []
  type: TYPE_NORMAL
- en: '[transformer blocks](../Text/chapter-4.html#p13), [2nd](../Text/chapter-6.html#p106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[connecting attention and linear layers in](../Text/chapter-4.html#p144), [2nd](../Text/chapter-4.html#p159)'
  prefs: []
  type: TYPE_NORMAL
- en: '[text generation loss](../Text/chapter-5.html#p29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[torchvision library](../Text/appendix-a.html#p31)'
  prefs: []
  type: TYPE_NORMAL
- en: U
  prefs: []
  type: TYPE_NORMAL
- en: '[unbiased parameter](../Text/chapter-4.html#p80)'
  prefs: []
  type: TYPE_NORMAL
- en: '[unlabeled data, decoding strategies to control randomness](../Text/chapter-5.html#p172)'
  prefs: []
  type: TYPE_NORMAL
- en: V
  prefs: []
  type: TYPE_NORMAL
- en: '[variable-length inputs](../Text/chapter-5.html#p117)'
  prefs: []
  type: TYPE_NORMAL
- en: '[vocab_size](../Text/chapter-4.html#p22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[v vector](../Text/appendix-d.html#p35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[vectors](../Text/appendix-a.html#p66), [2nd](../Text/appendix-a.html#p114)'
  prefs: []
  type: TYPE_NORMAL
- en: W
  prefs: []
  type: TYPE_NORMAL
- en: '[W<.Subscript>q</> matrix](../Text/chapter-3.html#p292)'
  prefs: []
  type: TYPE_NORMAL
- en: '[weight parameters](../Text/chapter-3.html#p131), [2nd](../Text/chapter-5.html#p9)'
  prefs: []
  type: TYPE_NORMAL
- en: '[word embeddings](../Text/chapter-2.html#p13), [2nd](../Text/chapter-2.html#p23)'
  prefs: []
  type: TYPE_NORMAL
- en: '[weight_decay parameter](../Text/chapter-6.html#p200)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Word2Vec](../Text/chapter-2.html#p18)'
  prefs: []
  type: TYPE_NORMAL
- en: weights
  prefs: []
  type: TYPE_NORMAL
- en: '[initializing model with pretrained weights](../Text/chapter-6.html#p84)'
  prefs: []
  type: TYPE_NORMAL
- en: '[loading pretrained weights from OpenAI](../Text/chapter-5.html#p253), [2nd](../Text/chapter-5.html#p302)'
  prefs: []
  type: TYPE_NORMAL
- en: '[word positions, encoding](../Text/chapter-2.html#p231)'
  prefs: []
  type: TYPE_NORMAL
- en: '[weight splits](../Text/chapter-3.html#p285)'
  prefs: []
  type: TYPE_NORMAL
- en: X
  prefs: []
  type: TYPE_NORMAL
- en: '[X training example](../Text/appendix-a.html#p178)'
  prefs: []
  type: TYPE_NORMAL
