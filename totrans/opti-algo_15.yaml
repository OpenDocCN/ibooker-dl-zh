- en: 11 Supervised and unsupervised learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 监督学习和无监督学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Reviewing the basics of artificial intelligence, machine learning, and deep
    learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾人工智能、机器学习和深度学习的基础知识
- en: Understanding graph machine learning, graph embedding, and graph convolutional
    networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解图机器学习、图嵌入和图卷积网络
- en: Understanding attention mechanisms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解注意力机制
- en: Understanding self-organizing maps
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自组织映射
- en: Solving optimization problems using supervised and unsupervised machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用监督学习和无监督机器学习解决优化问题
- en: Artificial intelligence (AI) is one of the fastest growing fields of technology,
    driven by advancements in computing power, access to vast amounts of data, breakthroughs
    in algorithms, and increased investment from both public and private sectors.
    AI aims to create intelligent systems or machines that can exhibit intelligent
    behavior, often by mimicking or drawing inspiration from biological intelligence.
    These systems can be designed to function autonomously or with some human guidance,
    and ideally, they can adapt to environments with diverse structures, observability
    levels, and dynamics. AI augments our intelligence by empowering us to analyze
    vast amounts of multidimensional, multimodal data and identify hidden patterns
    that would be difficult for humans to recognize. AI also supports our learning
    and decision-making by providing relevant insights and potential courses of action.
    AI encompasses various subfields, such as situation awareness (comprising perception,
    comprehension, and projection), knowledge representation, cognitive reasoning,
    machine learning, data analytics (covering descriptive, diagnostic, predictive,
    and prescriptive analytics), problem solving (involving constraint satisfaction
    and problem-solving using search and optimization), as well as digital and physical
    automation (such as conversational AI and robotics).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）是技术领域增长最快的领域之一，由计算能力的提升、大量数据的获取、算法的突破以及公共和私营部门的增加投资所驱动。AI旨在创建能够表现出智能行为或机器，通常是通过模仿或从生物智能中汲取灵感。这些系统可以设计为自主运行或在一定程度上接受人类指导，理想情况下，它们可以适应具有不同结构、可观察性和动态的环境。AI通过赋予我们分析大量多维、多模态数据的能力并识别人类难以识别的隐藏模式来增强我们的智能。AI还通过提供相关见解和潜在的行动方案来支持我们的学习和决策。AI包括各种子领域，如态势感知（包括感知、理解和预测）、知识表示、认知推理、机器学习、数据分析（包括描述性、诊断性、预测性和规范性分析）、问题解决（涉及约束满足和搜索与优化）、以及数字和物理自动化（如对话式AI和机器人）。
- en: 'In this last part of the book, we will explore the convergence of two branches
    of AI: machine learning and optimization. Our focus will be on showcasing the
    practical applications of machine learning in tackling optimization problems.
    This chapter provides an overview of machine learning fundamentals as essential
    background knowledge, and then it delves into applications of supervised and unsupervised
    machine learning in handling optimization problems. Reinforcement learning will
    be covered in the next chapter.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一部分，我们将探讨人工智能两个分支的融合：机器学习和优化。我们的重点是展示机器学习在解决优化问题中的实际应用。本章提供了机器学习基础知识的概述，作为必要的背景知识，然后深入探讨了监督学习和无监督机器学习在处理优化问题中的应用。强化学习将在下一章中介绍。
- en: 11.1 A day in the life of AI-empowered daily routines
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 智能赋能日常生活的每一天
- en: AI, and machine learning in particular, forms the foundation of many successful
    disruptive industries and has successfully delivered many commercial products
    that touch everybody’s life every day. Starting at home, voice assistants eagerly
    await your commands, effortlessly controlling smart appliances and adjusting the
    smart thermostat to ensure comfort and convenience. Smart meters intelligently
    manage energy consumption, optimizing efficiency and reducing costs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AI，尤其是机器学习，构成了许多成功颠覆性产业的基础，并成功推出了许多触及每个人日常生活的商业产品。从家庭开始，语音助手热切地等待您的命令，轻松控制智能设备并调整智能恒温器以确保舒适和便利。智能电表智能管理能源消耗，优化效率并降低成本。
- en: On the route to school or work, navigation apps with location intelligence guide
    the way, considering real-time traffic updates to provide the fastest and most
    efficient route. Shared mobility services offer flexible transportation options
    on demand, while advanced driver assistance systems enhance safety and convenience
    if you decide to drive. In the not-too-distant future, we will enjoy safe and
    entertaining self-driving vehicles as a third living space, after our homes and
    workplaces, with consumer-centric products and services.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上学或上班的路上，具有位置智能的导航应用指引路线，考虑实时交通更新以提供最快和最有效的路线。共享出行服务提供按需的灵活交通选项，而先进的驾驶辅助系统在您决定驾驶时增强安全性和便利性。在不远的将来，我们将享受安全有趣的自动驾驶汽车作为第三个生活空间，在家庭和工作场所之后，拥有以消费者为中心的产品和服务。
- en: Once at school or at the workplace, AI becomes an invaluable tool for personalization
    and to boost productivity. Personalized learning platforms cater to individual
    needs, adapting teaching methods and content to maximize understanding and retention.
    Summarization and grammar-checking algorithms aid in crafting flawless documents,
    while translation tools bridge language barriers effortlessly. Excel AI formula
    generators streamline complex calculations, saving time and effort. Human-like
    text generation enables natural and coherent writing, while audio, image, and
    video generation from text unlock creative possibilities. Optimization algorithms
    ensure optimal resource allocation and scheduling, maximizing efficiency in various
    scenarios, and handle different design, planning, and control problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在学校或工作场所，人工智能成为个性化提升生产力的无价工具。个性化的学习平台满足个人需求，调整教学方法和内容以最大化理解和记忆。摘要和语法检查算法有助于制作无瑕疵的文档，而翻译工具则轻松跨越语言障碍。Excel人工智能公式生成器简化了复杂的计算，节省了时间和精力。类似人类的文本生成使写作自然流畅，而从文本生成音频、图像和视频则开启了创意的可能性。优化算法确保资源分配和调度最优化，在各种场景中最大化效率，并处理不同的设计、规划和控制问题。
- en: During shopping, AI enhances the experience in numerous ways. Voice search enables
    hands-free exploration, while searching by images allows for effortless discovery
    of desired items. Semantic search understands context and intent, providing more
    accurate results. Recommendation engines offer personalized suggestions based
    on individual preferences and online shopping behavior, while last-mile or door-to-door
    delivery services ensure timely, transparent, and convenient package arrival.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在购物过程中，人工智能以多种方式提升了购物体验。语音搜索实现了免提探索，而通过图像搜索则可以轻松发现所需商品。语义搜索理解上下文和意图，提供更准确的结果。推荐引擎根据个人偏好和在线购物行为提供个性化建议，而最后一英里或门到门的配送服务确保及时、透明和便利的包裹送达。
- en: In the realm of health, AI revolutionizes personalized healthcare, assisting
    with diagnosis, treatment planning, and rehabilitation. Lab automation speeds
    up testing processes, improving accuracy and efficiency. AI-driven drug discovery
    and delivery enable the development of innovative treatments and targeted therapies,
    transforming lives.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在健康领域，人工智能革命性地改变了个性化医疗保健，协助诊断、治疗规划和康复。实验室自动化加速了测试流程，提高了准确性和效率。人工智能驱动的药物发现和递送使创新治疗和靶向疗法的开发成为可能，改变人们的生活。
- en: During leisure time, AI contributes to physical and mental well-being. Fitness
    planning apps tailor workout routines to individual goals and capabilities, providing
    personalized guidance and motivation. Trip planning tools recommend exciting destinations
    and itineraries, ensuring memorable experiences. AI-powered meditation apps offer
    customized relaxation experiences, soothing the mind and promoting mindfulness.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在休闲时间，人工智能有助于身心健康。健身计划应用根据个人目标和能力定制锻炼计划，提供个性化的指导和动力。行程规划工具推荐令人兴奋的目的地和行程，确保难忘的体验。人工智能驱动的冥想应用提供定制的放松体验，舒缓心灵并促进正念。
- en: Machine learning, a prominent subfield of artificial intelligence, has played
    a pivotal role in bringing AI from the confines of high-tech research labs to
    the convenience of our daily lives.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，作为人工智能的一个突出子领域，在将人工智能从高科技研究实验室的局限带到我们日常生活的便利中发挥了关键作用。
- en: 11.2 Demystifying machine learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 揭秘机器学习
- en: 'The goal of learning is to create an internal model or abstraction of the external
    world. More comprehensively, Stanislas Dehaene, in *How We Learn* [1], introduced
    seven key definitions of learning that lie at the heart of present-day machine
    learning algorithms:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的目的是创建外部世界的内部模型或抽象。更全面地说，Stanislas Dehaene 在 *《我们如何学习》* [1] 中介绍了学习的关键定义，这些定义是当今机器学习算法的核心：
- en: Learning is adjusting the parameters of a mental model.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是调整心理模型的参数。
- en: Learning is exploring a combinatorial explosion.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是探索组合爆炸。
- en: Learning is minimizing errors.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是最小化错误。
- en: Learning is exploring the space of possibilities.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是探索可能性空间。
- en: Learning is optimizing a reward function.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是优化奖励函数。
- en: Learning is restricting search space.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是限制搜索空间。
- en: Learning is projecting a priori hypotheses.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是投射先验假设。
- en: 'Machine learning (ML) is a subfield of AI that endows an artificial system
    or process with the ability to learn from experience and observation without being
    explicitly programmed. Thomas Mitchell, in *Machine Learning*, defines ML as follows:
    “A computer program is said to learn from experience *E* with respect to some
    class of tasks *T* and performance measure *P*, if its performance at tasks in
    *T*, as measured by *P*, improves with experience *E*” [2]. In his book *The Master
    Algorithm*, Pedro Domingos summarizes the ML schools of thought into five main
    schools [3], illustrated in figure 11.1:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是人工智能的一个子领域，它赋予人工系统或过程从经验和观察中学习的能力，而不需要明确编程。Thomas Mitchell 在 *《机器学习》*
    中将 ML 定义如下：“如果一个计算机程序在任务 *T* 中，根据性能度量 *P*，从经验 *E* 中学习，那么它的性能随着经验 *E* 的提高而提高” [2]。在他的书
    *《大师算法》* 中，Pedro Domingos 将机器学习学派总结为五个主要学派 [3]，如图11.1所示：
- en: Bayesians with probabilistic inference as the master algorithm
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以概率推理作为主算法的贝叶斯主义者
- en: Symbolists with rules and trees as the main core algorithm within this paradigm
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以规则和树作为该范式主要核心算法的符号主义者
- en: Connectionists who use neural networks with backpropagation as a master algorithm
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反向传播作为主算法的神经网络连接主义者
- en: Evolutionaries who rely on the evolutionary computing paradigm
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于进化计算范式的进化论者
- en: Analogizers who use mathematical techniques like support vector machines with
    different kernels
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同核的支持向量机等数学技术的类比主义者
- en: '![](../Images/CH11_F01_Khamis.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F01_Khamis.png)'
- en: Figure 11.1 Different ML schools of thought according to Domingos’ *The Master
    Algorithm*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 根据 Domingos 的 *《大师算法》* 列出的不同机器学习学派
- en: Nowadays, connectionist learning approaches have attracted most of the attention,
    thanks to their perception and learning capabilities in several challenging domains.
    These statistical ML algorithms follow a bottom-up inductive reasoning paradigm
    (i.e., inferring general rules from a set of examples) to discover patterns from
    vast amounts of data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，连接主义学习方法因其几个具有挑战性的领域的感知和学习能力而吸引了大部分关注。这些统计机器学习算法遵循自下而上的归纳推理范式（即从一组示例中推断一般规则）来从大量数据中发现模式。
- en: The unreasonable effectiveness of data
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的不合理有效性
- en: Simple models and a lot of data trump more elaborate models based on less data
    [4]. This means that having a large amount of data to train simple models is often
    more effective than using complex models with only a small amount of data. For
    example, in self-driving vehicles, a simple model that has been trained on millions
    of hours of driving data can often be more effective in recognizing and reacting
    to diverse road situations than a more complex model trained on a smaller dataset.
    This is because the massive amount of data helps the simple model learn a wide
    range of patterns and scenarios, including adversarial and edge cases it might
    encounter, making it more adaptable and reliable in real-world driving conditions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简单模型和大量数据胜过基于较少数据的更复杂模型 [4]。这意味着拥有大量数据来训练简单模型通常比使用只有少量数据的复杂模型更有效。例如，在自动驾驶汽车中，一个在数百万小时驾驶数据上训练过的简单模型，在识别和反应各种道路情况时，通常比在更小数据集上训练的更复杂模型更有效。这是因为大量数据帮助简单模型学习广泛的各种模式和场景，包括它可能遇到的对抗性和边缘情况，使其在现实世界的驾驶条件下更具适应性和可靠性。
- en: These connectionist learning or statistical ML approaches are based on the experimental
    findings that even very complex problems in artificial intelligence may be solved
    by simple statistical models trained on massive datasets [4]. Statistical ML is
    currently the most famous form of AI. The rapid advancement of this form of ML
    can be attributed primarily to the widespread availability of big data and open
    source tools, enhanced computational power such as AI accelerators, and substantial
    research and development funding from both public and private sectors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基于连接主义学习或统计机器学习的方法基于实验发现，即使是非常复杂的人工智能问题也可能通过在大量数据集上训练的简单统计模型来解决[4]。统计机器学习是目前最著名的AI形式。这种形式机器学习的快速进步主要归因于大数据的广泛应用、开源工具的普及、增强的计算能力，如AI加速器，以及公共和私营部门的大量研发资金。
- en: Generally speaking, ML algorithms can be categorized into supervised, unsupervised,
    hybrid learning, and reinforcement learning algorithms, as illustrated in figure
    11.2.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，机器学习算法可以分为监督学习、无监督学习、混合学习和强化学习算法，如图11.2所示。
- en: '![](../Images/CH11_F02_Khamis.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F02_Khamis.png)'
- en: Figure 11.2 ML taxonomy as a subfield of AI
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 机器学习作为人工智能子领域的分类
- en: '*Supervised learning*—This approach uses inductive inference to approximate
    mapping functions between data and known labels or classes. This mapping is learned
    using already labeled training data. *Classification* (predicting discrete or
    categorical values) and *regression* (predicting continuous values) are common
    tasks in supervised learning. For example, classification seeks a scoring function
    *f*:*Χ*×*C*⟶*R*, where *X* represents the training data space and *C* represents
    the label or class space. This mapping can be learned using *N* training examples
    of the form {(*x*[11], *x*[21], …, *x[m]*[1], *c*[1]), (*x*[12], *x*[22], …, *x[m]*[2],
    *c*[2]), …, (*x*[1]*[N], x*[2]*[N]*, …, *x[mN], c[N]*)}, where *x[i]* is the feature
    vector of the *i*-th example, *m* is number of features, and *c[i]* is the corresponding
    class. The predicted class is the class that gives the highest score of *f*, i.e.,
    *c*(*x*) = argmax*[c]f*(*x*,*c*). In the context of self-driving vehicles, supervised
    learning might be used to train a model to recognize traffic signs. The input
    data would be images of various traffic signs, and the correct output (the labels)
    would be the type of each sign. The trained model could then identify traffic
    signs correctly when driving. Feedforward neural networks (FNNs) or multilayer
    perceptrons (MLPs), convolutional neural networks (CNNs), recurrent neural networks
    (RNNs), long short-term memory (LSTM) networks, and sequence-to-sequence (Seq2Seq)
    models are examples of common neural network architectures that are typically
    trained using supervised learning. Examples of solving combinatorial problems
    using supervised ML are provided in sections 11.6, 11.7, and 11.9.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监督学习*——这种方法使用归纳推理来近似数据与已知标签或类别之间的映射函数。这种映射是通过使用已经标记的训练数据来学习的。在监督学习中，*分类*（预测离散或分类值）和*回归*（预测连续值）是常见的任务。例如，分类寻求一个评分函数
    *f*:*Χ*×*C*⟶*R*，其中 *X* 代表训练数据空间，*C* 代表标签或类别空间。这种映射可以使用 *N* 个训练示例来学习，形式为 {(*x*[11],
    *x*[21], …, *x[m]*[1], *c*[1]), (*x*[12], *x*[22], …, *x[m]*[2], *c*[2]), …, (*x*[1]*[N],
    x*[2]*[N]*, …, *x[mN], c[N]*)}，其中 *x[i]* 是第 *i* 个示例的特征向量，*m* 是特征数量，*c[i]* 是相应的类别。预测的类别是给出最高评分的类别，即
    *c*(*x*) = argmax*[c]f*(*x*,*c*)。在自动驾驶汽车的情况下，监督学习可能被用来训练一个模型来识别交通标志。输入数据将是各种交通标志的图像，正确的输出（标签）将是每个标志的类型。训练好的模型随后可以在驾驶时正确识别交通标志。前馈神经网络（FNNs）或多层感知器（MLPs）、卷积神经网络（CNNs）、循环神经网络（RNNs）、长短期记忆（LSTM）网络和序列到序列（Seq2Seq）模型是通常使用监督学习训练的常见神经网络架构的例子。使用监督机器学习解决组合问题的例子在11.6、11.7和11.9节中提供。'
- en: '*Unsupervised learning*—This approach deals with unlabeled data through techniques
    like *clustering* and *dimensionality reduction*. In clustering, for example,
    *n* objects (each could be a vector of *d* features) are given, and the task is
    to group them based on certain similarity measures into *c* groups (clusters)
    in such a way that all objects in a single group have a “natural” relation to
    one another, and objects not in the same group are somehow different. For instance,
    unsupervised learning might be used in self-driving vehicles to cluster similar
    driving scenarios or environments. Using unsupervised learning, the car might
    learn to identify different types of intersections or roundabouts, even if no
    one has explicitly labeled the data with these categories. Autoencoders, k-means,
    density-based spatial clustering (DBSCAN), principal component analysis (PCA),
    and self- organizing maps (SOMs) are examples of unsupervised learning methods.
    SOM is explained in section 11.4\. An example of a combinatorial problem using
    SOM is provided in section 11.8\.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无监督学习*—这种方法通过诸如*聚类*和*降维*等技术处理未标记数据。例如，在聚类中，给出*n*个对象（每个对象可能是一个*d*特征的向量），任务是根据某些相似性度量将它们分组到*c*个组（簇）中，使得单个组中的所有对象彼此之间有“自然”的关系，而不同组中的对象在某种程度上是不同的。例如，无监督学习可能用于自动驾驶车辆中聚类相似的驾驶场景或环境。使用无监督学习，汽车可能学会识别不同类型的交叉口或环岛，即使没有人明确地将这些类别标记为数据。自编码器、k-means、基于密度的空间聚类（DBSCAN）、主成分分析（PCA）和自组织映射（SOMs）是未监督学习方法的例子。SOM在11.4节中解释。11.8节提供了一个使用SOM的组合问题的例子。'
- en: '*Hybrid learning*—This approach includes *semi-supervised learning and self-supervised
    learning* techniques. Semi-supervised learning is a mix of supervised and unsupervised
    learning where only a fraction of the input data is labeled with corresponding
    outputs. In this case, the training process uses the small amount of labeled data
    available and pseudo-labels the rest of the dataset—for example, training a self-driving
    vehicle''s perception system with a limited set of labeled driving scenarios,
    then using a vast collection of unlabeled driving data to improve its ability
    to recognize and respond to various road conditions and obstacles. Self-supervised
    learning is an ML process where a model learns meaningful representations of the
    input data by using the inherent structure or relationships within the data itself.
    This is achieved by creating supervised learning tasks from the unlabeled data.
    For instance, a self-supervised model might be trained to predict the next word
    in a sentence based on the previous words or to reconstruct an image from a scrambled
    version. These learned representations can then be used for various downstream
    tasks, such as image classification or object detection. In the context of self-driving
    vehicles, a perception system can be trained to identify essential features in
    unlabeled driving scenes, such as lane markings, pedestrians, and other vehicles.
    Then, the learned features are utilized as pseudo-labels to classify new driving
    scenes in a supervised manner, enabling the vehicle to make decisions based on
    its understanding of the road environment.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合学习*—这种方法包括*半监督学习和自监督学习*技术。半监督学习是监督学习和无监督学习相结合的方法，其中只有一小部分输入数据被标记为相应的输出。在这种情况下，训练过程使用可用的少量标记数据，并对数据集的其余部分进行伪标记——例如，使用有限的标记驾驶场景集训练自动驾驶车辆的感知系统，然后使用大量的未标记驾驶数据来提高其识别和响应各种道路状况和障碍物的能力。自监督学习是一种机器学习过程，模型通过使用数据本身固有的结构或关系来学习输入数据的有效表示。这是通过从未标记数据中创建监督学习任务来实现的。例如，一个自监督模型可能被训练来根据前面的单词预测句子中的下一个单词，或者从打乱版本的图像中重建图像。这些学习到的表示可以用于各种下游任务，例如图像分类或目标检测。在自动驾驶车辆的情况下，感知系统可以被训练来识别未标记驾驶场景中的关键特征，例如车道标记、行人和其他车辆。然后，学习到的特征被用作伪标签，以监督方式对新的驾驶场景进行分类，使车辆能够根据其对道路环境的理解做出决策。'
- en: '*Reinforcement learning (RL)*—This approach learns from interactions through
    a feedback loop or by trial and error. A learning agent learns to make decisions
    by taking actions in an environment to maximize some notion of cumulative reward.
    For self-driving vehicles, reinforcement learning could be used in the decision-making
    process. For instance, the car might learn over time the best way to merge into
    traffic on a busy highway. It would receive positive rewards for successful merges
    and negative rewards for dangerous maneuvers or failed attempts. Over time, through
    trial and error and the desire to maximize the reward, the car would learn an
    optimal policy for merging into traffic. More details about RL are provided in
    the next chapter.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强化学习（RL）*——这种方法通过反馈循环或通过试错来学习。学习代理通过在环境中采取行动以最大化某种累积奖励的概念来学习做出决策。对于自动驾驶汽车，强化学习可以在决策过程中使用。例如，汽车可能随着时间的推移学会在繁忙的高速公路上并入交通的最佳方式。它将因成功的并入而获得正面奖励，因危险的操作或失败的尝试而获得负面奖励。随着时间的推移，通过试错和最大化奖励的愿望，汽车将学会并入交通的最佳策略。关于强化学习的更多细节将在下一章提供。'
- en: Deep learning (DL) is a subfield of ML concerned with learning underlying features
    in data using neural networks with many layers (hence “deep”) enabling artificial
    systems to build complex concepts out of simpler concepts. DL enables learning
    discriminative features or representations and learning at different levels of
    abstraction. To achieve this, the network uses hierarchical feature learning and
    employs a handful of convolutional layers. DL revolutionizes the field of ML by
    reducing the need for extensive data preprocessing. DL models can automatically
    extract highly discriminative features from raw data, eliminating the need for
    hand-crafted feature engineering. This end-to-end learning process significantly
    reduces the reliance on human experts, as the model learns to extract meaningful
    representations and patterns directly from the input data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习（ML）的一个子领域，它使用具有多层（因此称为“深度”）的神经网络来学习数据中的底层特征，使人工系统能够从更简单的概念中构建复杂的概念。深度学习能够学习具有判别性的特征或表示，并在不同抽象级别上进行学习。为了实现这一点，网络使用层次特征学习和采用少量卷积层。深度学习通过减少对大量数据预处理的需求，革新了机器学习领域。深度学习模型可以从原始数据中自动提取高度判别性的特征，从而消除了手动特征工程的需求。这种端到端的学习过程显著减少了对人专家的依赖，因为模型能够直接从输入数据中提取有意义的表示和模式。
- en: Unlike traditional ML algorithms, DL models have the ability to directly consume
    and process various forms of structured and unstructured data, such as text, audio,
    images, video, and even graphs. Graph-structured data is particularly important
    in the field of combinatorial optimization due to its ability to capture and represent
    the relationships and constraints between elements in optimization problems. Geometric
    DL is a subfield of ML that combines graph theory with DL.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习算法不同，深度学习模型能够直接消费和处理各种形式的结构化和非结构化数据，如文本、音频、图像、视频，甚至图。图结构数据在组合优化领域尤为重要，因为它能够捕捉和表示优化问题中元素之间的关系和约束。几何深度学习是机器学习的一个子领域，它将图论与深度学习相结合。
- en: The following two sections address graph machine learning and self-organizing
    maps in more detail. They are essential background knowledge to the use cases
    described later in this chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两节更详细地介绍了图机器学习和自组织映射。它们是本章后面描述的使用案例的必要背景知识。
- en: 11.3 Machine learning with graphs
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 使用图的机器学习
- en: As explained in section 3.1, a graph is a nonlinear data structure composed
    of entities known as *vertices* (or nodes) and the relationships between them,
    known as *edges* (or *arcs* or *links*). Data coming from different domains can
    be nicely captured using a graph. Social media networks, for instance, employ
    graphs to depict connections between users and to analyze social interactions,
    which in turn drive content propagation and recommendations. Navigation applications
    use graphs to represent physical locations and the paths between them, enabling
    route calculations, real-time traffic updates, and estimated time of arrival (ETA)
    predictions. Recommender systems rely on graphs to model user–item interactions
    and preferences, thereby offering personalized recommendations. Search engines
    use web graphs, where web pages are nodes and hyperlinks are edges, to crawl and
    index the internet and facilitate efficient information retrieval. Knowledge graphs
    offer a structured representation of factual information, relationships, and entities,
    and they’re used in diverse fields from digital assistants to enterprise data
    integration. Question-answering engines use graphs to understand and decompose
    complex questions and search for relevant answers in structured datasets. In the
    realm of chemistry, molecular structures can be viewed as graphs, where atoms
    are nodes and bonds are edges, supporting tasks like discovering compounds and
    predicting properties.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如第3.1节所述，图是一种非线性数据结构，由称为*顶点*（或节点）的实体及其之间的关系组成，这些关系称为*边*（或*弧*或*链接*）。来自不同领域的数据可以使用图很好地捕捉。例如，社交媒体网络使用图来描绘用户之间的连接并分析社会互动，这反过来又推动内容传播和推荐。导航应用程序使用图来表示物理位置及其之间的路径，从而实现路线计算、实时交通更新和预计到达时间（ETA）预测。推荐系统依赖于图来模拟用户-项目交互和偏好，从而提供个性化推荐。搜索引擎使用网页图，其中网页是节点，超链接是边，以爬取和索引互联网并促进高效的信息检索。知识图谱提供了事实信息、关系和实体的结构化表示，并在从数字助手到企业数据集成等众多领域得到应用。问答引擎使用图来理解和分解复杂问题，并在结构化数据集中搜索相关答案。在化学领域，分子结构可以被视为图，其中原子是节点，键是边，支持发现化合物和预测性质等任务。
- en: 'Graph-structured data is vital due to its power to model complex relationships
    and dependencies between entities in an intuitive, self-descriptive, intrinsically
    explainable, and natural way. Unlike traditional tabular data, graphs allow for
    the representation of networked relationships and complex interconnectedness between
    entities of interest, making them an excellent tool for modeling numerous real-world
    systems. Tabular data can be converted into graph-structured data—the specific
    definitions of nodes and edges would depend on what relationships you’re interested
    in examining within the data. For example, in the context of a FIFA dataset, we
    can define nodes and edges based on the information available in this dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图结构化数据至关重要，因为它能够以直观、自描述、本质上可解释和自然的方式对实体之间的复杂关系和依赖进行建模。与传统表格数据不同，图允许表示感兴趣实体之间的网络关系和复杂相互关联，使它们成为建模众多现实世界系统的优秀工具。可以将表格数据转换为图结构化数据——节点和边的具体定义将取决于你想要在数据中检查哪些关系。例如，在FIFA数据集的背景下，我们可以根据该数据集中可用的信息定义节点和边：
- en: '*Nodes*—Nodes represent entities of interest and could be the players, the
    clubs they play for, or their nationalities. Each of these entities could be a
    separate node in the graph. For example, Lionel Messi, Inter Miami, and Argentina
    could all be individual nodes in the graph.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点*—节点代表感兴趣的实体，可以是球员、他们所效力的俱乐部或他们的国籍。这些实体中的每一个都可以是图中的一个单独节点。例如，莱昂内尔·梅西、国际米兰和阿根廷都可以是图中的单独节点。'
- en: '*Edges*—Edges represent the relationships between the nodes. For instance,
    an edge could connect a player to the club they play for, indicating that the
    player is part of that club. Another edge could connect a player to their nationality,
    showing that the player belongs to that country. So, for example, Lionel Messi
    could be connected to Inter Miami with an edge indicating that Messi plays for
    Inter Miami, and another edge could connect Lionel Messi to Argentina, indicating
    his nationality.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边缘*—边缘代表节点之间的关系。例如，一条边可以连接一个球员和他们所效力的俱乐部，表示该球员是该俱乐部的一员。另一条边可以连接一个球员和他们的国籍，表明该球员属于那个国家。因此，例如，莱昂内尔·梅西可以通过一条边与国际米兰连接，表示梅西为国际米兰效力，另一条边可以将莱昂内尔·梅西与阿根廷连接，表示他的国籍。'
- en: The next listing shows how to convert tabular data for 10 selected soccer players
    into a graph using NetworkX.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了如何使用NetworkX将10位选定足球运动员的表格数据转换为图。
- en: Listing 11.1 Converting tabular data to a graph
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.1 将表格数据转换为图
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As a continuation of listing 11.1, we can create a NetworkX graph whose nodes
    represent the player name, club, and nationality and whose edges represent the
    semantic relationships between these nodes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为11.1列表的延续，我们可以创建一个NetworkX图，其节点代表球员姓名、俱乐部和国籍，而边则代表这些节点之间的语义关系。
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Create a new graph.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个新的图。
- en: ② Add nodes and edges for clubs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为俱乐部添加节点和边。
- en: ③ Add nodes and edges for nationalities.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 为国籍添加节点和边。
- en: ④ Create the layout
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建布局
- en: ⑤ Set the size of the figure.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置图形的大小。
- en: ⑥ Get lists of player, club, and nationality nodes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 获取球员、俱乐部和国籍节点的列表。
- en: ⑦ Draw nodes in different colors.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 用不同颜色绘制节点。
- en: ⑧ Draw edges.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 绘制边。
- en: ⑨ Draw edge labels.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 绘制边标签。
- en: ⑩ Draw node labels.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 绘制节点标签。
- en: Figure 11.3 shows the data for the 10 selected soccer players in a graph. This
    graph shows the entities of interest (player, club, and nationality) and their
    relationships. For example, L. Messi is a player who plays for Inter Miami and
    is from Argentina.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3显示了10位选定足球运动员的图数据。此图显示了感兴趣的实体（球员、俱乐部和国籍）及其关系。例如，L.梅西是一名为Inter Miami效力的球员，来自阿根廷。
- en: '![](../Images/CH11_F03_Khamis.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F03_Khamis.png)'
- en: Figure 11.3 Graph-structured data for 10 selected soccer players
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 10位选定足球运动员的图结构数据
- en: Graph data fundamentally differs from Euclidean data, as the concept of distance
    is not simply a matter of straight-line (Euclidean) distance between two points.
    In the case of a graph, what matters is the structure of the nodes and edges—whether
    two nodes are connected by an edge and how they are connected to other nodes in
    the graph. Table 11.1 summarizes the differences between Euclidean and non-Euclidean
    graph data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图形数据与欧几里得数据根本不同，因为距离的概念不仅仅是两点之间的直线（欧几里得）距离。在图的情况下，重要的是节点和边的结构——两个节点是否通过边连接，以及它们如何连接到图中的其他节点。表11.1总结了欧几里得数据和非欧几里得图数据之间的差异。
- en: Table 11.1 Euclidean data versus non-Euclidean graph data
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.1 欧几里得数据与非欧几里得图数据
- en: '| Aspects | Euclidean data | Non-Euclidean graph data |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 欧几里得数据 | 非欧几里得图数据 |'
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Common data types | Numerical, text, audio, images, videos | Road networks,
    social networks, web pages, and molecular structures |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 常见数据类型 | 数值、文本、音频、图像、视频 | 道路网络、社交网络、网页和分子结构 |'
- en: '| Dimensionality | Can be 1D (e.g., numbers, text), 2D (e.g., images, heatmaps),
    or higher-dimensional (e.g., RGB-D images or depth maps, 3D point cloud data)
    | Large dimensionality (e.g., a Pinterest graph has 3 billion nodes and 18 billion
    edges) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 可以为1D（例如，数字、文本），2D（例如，图像、热图），或更高维（例如，RGB-D图像或深度图，3D点云数据） | 大维度（例如，Pinterest图有30亿个节点和180亿条边）
    |'
- en: '| Structure | Fixed structure (e.g., in the case of an image, the structure
    is embedded via pixel proximity) | Arbitrary structure (every node can have a
    different neural structure because the network neighborhood around it is different,
    as the model adapts to the data) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 固定结构（例如，在图像的情况下，结构通过像素邻近性嵌入） | 任意结构（每个节点都可以有不同的神经网络结构，因为其网络邻域不同，模型适应数据）
    |'
- en: '| Spatial locality | Yes (i.e., data points that are close together in the
    input space are also likely to be close together in the output space). | No, “closeness”
    is determined by the graph structure, not spatial arrangement (i.e., two nodes
    that are “close” to each other might not necessarily have similar properties or
    features, such as in the case of a traffic light node and a crosswalk node). |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 空间局部性 | 是（即在输入空间中彼此靠近的数据点也可能会在输出空间中彼此靠近）。 | 否，“接近”由图结构决定，而不是空间排列（即彼此“接近”的两个节点可能不具有相似的性质或特征，例如在交通灯节点和人行横道节点的情况下）。
    |'
- en: '| Shift-invariance | Yes (i.e., data-inherent meaning is preserved when shifted;
    for instance, the concept of a cat in a picture does not change if the cat is
    in the top left corner or the bottom right corner of the image). | No (in a graph,
    there’s no inherent meaning to the “position” of a node that can be “shifted”).
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 平移不变性 | 是（即在平移时保留数据固有的意义；例如，图片中猫的概念不会因为猫在图像的右上角或左下角而改变）。 | 否（在图中，节点的“位置”没有固有的意义，不能“平移”）。
    |'
- en: '| Ordinality or hierarchy | Yes | No, graph data has “permutation invariance”—the
    specific ordering or labeling of nodes doesn’t usually affect the underlying relationships
    and properties of the graph. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 序数或层次 | 是 | 否，图数据具有“排列不变性”——节点的特定顺序或标签通常不会影响图的基本关系和属性。|'
- en: '| Shortest path between two points | A straight line | Is not necessarily a
    straight line |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 两个点之间的最短路径 | 直线 | 不一定是直线 |'
- en: '| Examples of ML models | Convolutional neural networks (CNNs), long short-term
    memory (LSTM), and recurrent neural networks (RNNs) | Graph neural networks (GNNs),
    graph convolutional networks (GCNs), temporal graph networks (TGNs), spatial-temporal
    graph neural networks (STGNNs) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习模型示例 | 卷积神经网络（CNNs）、长短期记忆（LSTM）和循环神经网络（RNNs） | 图神经网络（GNNs）、图卷积网络（GCNs）、时序图网络（TGNs）、时空图神经网络（STGNNs）|'
- en: '*Geometric deep learning* (GDL) is an umbrella term for emerging techniques
    seeking to extend (structured) deep neural models to handle non-Euclidean data
    with underlying geometric structures, such as graphs (networks of connected entities),
    point clouds (collections of 3D data points), molecules (chemical structures),
    and manifolds (curved, high-dimensional surfaces). Graph machine learning (GML)
    is a subfield of ML that focuses on developing algorithms and models capable of
    learning from graph-structured data. Graph embedding or representation learning
    is the first step in performing ML tasks such as node classification (predicting
    a category for each node), link prediction (forecasting connections between nodes),
    and community detection (identifying groups of interconnected nodes). The next
    subsection describes different graph embedding techniques.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*几何深度学习*（GDL）是一个总称，用于描述旨在将（结构化）深度神经网络扩展到处理具有潜在几何结构的非欧几里得数据的新兴技术，例如图（连接实体的网络）、点云（3D数据点的集合）、分子（化学结构）和流形（弯曲、高维表面）。图机器学习（GML）是机器学习的一个子领域，专注于开发能够从图结构化数据中学习的算法和模型。图嵌入或表示学习是执行机器学习任务（如节点分类（预测每个节点的类别）、链接预测（预测节点之间的连接）和社区检测（识别相互连接的节点组））的第一步。下一小节将描述不同的图嵌入技术。'
- en: 11.3.1 Graph embedding
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 图嵌入
- en: Graph embedding is a task that aims to learn a mapping from a discrete high-
    dimensional graph domain to a low-dimensional continuous domain. Through the process
    of graph embedding, graph nodes, edges, and their features are transformed into
    continuous vectors while preserving the structural information of the graph. For
    example, as shown in figure 11.4, an encoder, *ENC*(*v*), maps node *v* from the
    input graph space *G* to a low-dimensional vector *h[v]* in the embedding or latent
    space *H* based on the node’s position in the graph, its local neighborhood structure,
    or its features, or some combination of the three.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入是一个旨在将离散的高维图域映射到低维连续域的任务。通过图嵌入的过程，图节点、边及其特征被转换为连续向量，同时保留图的结构信息。例如，如图11.4所示，编码器
    *ENC*(*v*) 根据节点 *v* 在图中的位置、其局部邻域结构或其特征，或这三个的组合，将节点 *v* 从输入图空间 *G* 映射到嵌入或潜在空间 *H*
    中的低维向量 *h[v]*。
- en: '![](../Images/CH11_F04_Khamis.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH11_F04_Khamis.png)'
- en: Figure 11.4 Graph embedding
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 图嵌入
- en: This encoder needs to be optimized to minimize the difference between the similarity
    of a pair of nodes in the graph and their similarity in the embedding space. Nodes
    that are connected or nearby in the graph should be close in the embedded space.
    Conversely, nodes that are not connected or are far apart in the graph should
    be far apart in the embedded space. In a more generalized encoder/decoder architecture,
    a decoder is added to extract user-specified information from the low-dimensional
    embedding [5]. By jointly optimizing the encoder and decoder, the system learns
    to compress information about the graph structure into the low-dimensional embedding
    space.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此编码器需要优化以最小化图中一对节点相似性与它们在嵌入空间中相似性之间的差异。图中连接或靠近的节点在嵌入空间中应该靠近。相反，图中未连接或相距较远的节点在嵌入空间中应该相距较远。在更通用的编码器/解码器架构中，添加了解码器以从低维嵌入中提取用户指定的信息[5]。通过联合优化编码器和解码器，系统学习将关于图结构的信息压缩到低维嵌入空间中。
- en: 'There are various methods for graph embedding which can be broadly classified
    into transductive (shallow) embedding and inductive embedding:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入有多种方法，可以大致分为归纳（浅层）嵌入和归纳嵌入：
- en: '*Transductive embedding*—In the transductive learning paradigm, the model learns
    embeddings only for the nodes present in the graph during the training phase.
    The learned embeddings are specific to these nodes, and the model cannot generate
    embeddings for new nodes that weren’t present during training. These methods are
    difficult to scale and are suitable for static graphs. Examples of transductive
    methods for graph embedding include random walk (e.g., node2vec and DeepWalk)
    and matrix factorization (e.g., graph factorization and HOPE).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*归纳嵌入*——在归纳学习范式下，模型仅在训练阶段学习图中存在的节点的嵌入。这些学习到的嵌入仅针对这些节点，模型不能为训练期间未出现的节点生成嵌入。这些方法难以扩展，适用于静态图。图嵌入的归纳方法示例包括随机游走（例如，node2vec和DeepWalk）和矩阵分解（例如，图分解和HOPE）。'
- en: '*Inductive embedding*—Inductive learning methods can generalize to unseen nodes
    or entire graphs that were not present during training. They do this by learning
    a function that generates the embedding of a node based on its features and the
    structure of its local neighborhood, which can be applied to any node, regardless
    of whether it was present during training or not. These methods are suitable for
    evolving graphs. Examples of inductive methods for graph embedding are graph neural
    networks (GNN) and graph convolutional networks (GCNs).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*归纳嵌入*——归纳学习方法可以推广到训练期间未出现的节点或整个图。它们通过学习一个函数来实现，该函数根据节点的特征和其局部邻域的结构生成节点的嵌入，这可以应用于任何节点，无论它是否在训练期间出现。这些方法适用于动态图。图嵌入的归纳方法示例包括图神经网络（GNN）和图卷积网络（GCNs）。'
- en: Appendix A contains examples of some of these methods. For more information,
    see Broadwater and Stillman’s *Graph Neural Networks in Action* [6]. We’ll focus
    on GCN, as it is the most relevant approach to the combinatorial optimization
    application presented in this chapter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 附录A包含了一些这些方法的示例。更多信息，请参阅Broadwater和Stillman的《图神经网络实战》[6]。我们将重点关注GCN，因为它是最相关的组合优化应用方法，本章将介绍。
- en: Transductive versus inductive learning
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳学习与归纳学习比较
- en: '*Transductive learning* aims to learn from a specific set of data to a specific
    set of predictions without generalizing to new data. *Inductive learning* aims
    to learn general rules from observed training cases. These general rules can then
    be applied to new, unseen data.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*归纳学习*旨在从特定的数据集中学习到特定的预测结果，而不对新的数据进行泛化。*归纳学习*旨在从观察到的训练案例中学习一般规则。这些一般规则可以应用于新的、未见过的数据。'
- en: The *convolution operation* forms the basis of representation learning in many
    structured data scenarios, enabling the automatic learning of meaningful features
    from raw data, thereby obviating the need for manual feature engineering. Convolution
    is a mathematical operation that takes two functions (input data and a kernel,
    filter, or feature detector) and measures their overlap or merges the two sets
    of information to produce a feature map. One critical aspect of convolution is
    its ability to respect and utilize the known structural relationships among data
    points, such as the positional associations among pixels, the temporal order of
    time points, or the edges linking nodes in a network. In traditional ML, convolutional
    neural networks (CNNs) employ the convolution operator as a key tool for identifying
    spatial patterns within images. This is made possible by the inherent grid-like
    structure of image data, which allows the model to slide filters over the image,
    exploit the spatial regularities, and extract features in a manner akin to pattern
    recognition.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积操作*是许多结构化数据场景中表征学习的基础，它能够从原始数据中自动学习有意义的特征，从而避免了手动特征工程的需要。卷积是一种数学运算，它接受两个函数（输入数据和核、滤波器或特征检测器）并测量它们的重叠或合并这两组信息以生成特征图。卷积的一个关键方面是其能够尊重并利用数据点之间已知的结构关系，例如像素之间的位置关联、时间点的时序或网络中节点之间的边缘。在传统的机器学习中，卷积神经网络（CNNs）使用卷积算子作为识别图像中空间模式的关键工具。这是由图像数据的固有网格状结构所实现的，它允许模型在图像上滑动滤波器，利用空间规律性，并以类似于模式识别的方式提取特征。'
- en: However, in the realm of graph machine learning (GML), the situation changes
    considerably. The data in this context is non-Euclidean, as explained previously
    in table 11.1, meaning that it isn’t arranged on a regular grid like pixels are
    in an image or points are on a 3D surface. Instead, it’s represented in the form
    of a network or graph, which can capture complex relationships. Moreover, this
    data exhibits order invariance, implying that the output does not change with
    the rearrangement of nodes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在图机器学习（GML）领域，情况发生了相当大的变化。在此背景下，数据是非欧几里得的，如前所述在表11.1中解释，这意味着它不是像图像中的像素或3D表面上的点那样排列在常规网格上。相反，它以网络或图的形式表示，可以捕捉复杂的关系。此外，这种数据表现出顺序不变性，这意味着输出不会随着节点的重新排列而改变。
- en: Unlike CNNs, which operate on a regular grid, GCNs are designed to work with
    data that’s structured as a graph, which can represent a wide variety of irregular
    and complex structures. Each node is connected to its neighbors without any predefined
    pattern, and the convolution operation is applied to a node and its direct neighbors
    in the graph.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与在常规网格上运行的卷积神经网络（CNNs）不同，图卷积网络（GCNs）被设计用来处理以图结构组织的数据，这种结构可以表示各种不规则和复杂的结构。每个节点都与其邻居通过没有任何预定义模式的连接相连，卷积操作应用于图中的节点及其直接邻居。
- en: How does Google DeepMind predict the estimated time of arrival?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌DeepMind是如何预测到达时间的？
- en: Have you ever wondered how Google Maps predicts the estimated time of arrival
    (ETA) when you’re planning your trip? Google DeepMind uses a GML approach to do
    so. The traditional ML approach would be to break the route down into a number
    of road segments, predict the time to traverse each road segment using a feedforward
    neural network, and sum them up to get the ETA. However, the underlying assumption
    of feedforward NN is that the road segments are independent of each other. In
    reality, road segment traffic easily influences the ETA of neighboring road segments,
    so the samples are not independent.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经想过，当你计划旅行时，谷歌地图是如何预测到达时间（ETA）的？谷歌DeepMind使用GML方法来做到这一点。传统的机器学习方法是将路线分解成多个路段，使用前馈神经网络预测穿越每个路段所需的时间，并将它们加起来得到ETA。然而，前馈神经网络的基本假设是路段之间是相互独立的。在现实中，路段交通很容易影响相邻路段的ETA，因此样本不是独立的。
- en: For instance, consider the situation where congestion on a minor road influences
    the traffic flow on a main road. When the model encompasses multiple junctions,
    it naturally develops the capacity to predict slowdowns at intersections, delays
    due to converging traffic, and the total time taken in stop-and-go traffic conditions.
    A better approach is to use GML to take the influence of the neighboring road
    segments into consideration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这种情况：一条次要道路上的拥堵会影响主路的交通流量。当模型包含多个交叉口时，它自然会发展出预测交叉口减速、因交通汇聚导致的延误以及停车和行驶交通条件下所需总时间的预测能力。一个更好的方法是使用GML来考虑相邻路段的影响。
- en: In this case, the road network will first be converted into a graph where each
    road segment is represented as a node. If two road segments are connected to each
    other, their corresponding nodes will be connected by an edge in the graph. Graph
    embedding is then generated by GNN to map the node features and graph structures
    from a high- dimensional discrete graph space to a low-dimensional continuous
    latent space. Information is propagated and aggregated across the graph through
    a technique called *message passing*, where, at the end, the embedding vector
    for each node contains and encodes its own information as well as the network
    information from all its neighboring nodes, according to the degree of neighborhood.
    Adjacent nodes pass messages to each other. In the first pass, each node knows
    about its neighbor. In the second pass, every node knows about its neighbor’s
    neighbors, and this information is encoded into the embedding, and so on. This
    allows us to represent the influence of the traffic in each of the neighboring
    road segments.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，道路网络首先将被转换为图，其中每个路段被表示为一个节点。如果两个路段相互连接，它们对应的节点将在图中通过边连接。然后，通过GNN生成图嵌入，将节点特征和图结构从高维离散图空间映射到低维连续潜在空间。通过称为“消息传递”的技术在图中传播和聚合信息，在最后，每个节点的嵌入向量包含并编码了其自身的信息以及所有相邻节点的网络信息，根据邻域的程度。相邻节点相互传递消息。在第一次传递中，每个节点了解其邻居。在第二次传递中，每个节点了解其邻居的邻居，并将这些信息编码到嵌入中，依此类推。这使得我们能够表示相邻路段中交通的影响。
- en: The accuracy of real time ETAs was improved by up to 50% in places like Berlin,
    Jakarta, São Paulo, Sydney, Tokyo, and Washington DC using this approach [7].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，在柏林、雅加达、圣保罗、悉尼、东京和华盛顿特区等地，实时ETAs的准确性提高了高达50% [7]。
- en: As illustrated in figure 11.5, given an input graph, which includes node features
    *x[v]* and an adjacency matrix *A*, a GCN transforms the features of each node
    into a latent or embedding space *H*, while preserving the graph structure denoted
    by the adjacency matrix *A*. These latent vectors provide a rich representation
    of each node, making it possible to perform node classification independently.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如图11.5所示，给定一个输入图，该图包括节点特征*x[v]*和邻接矩阵*A*，GCN将每个节点的特征转换为一个潜在或嵌入空间*H*，同时保留由邻接矩阵*A*表示的图结构。这些潜在向量提供了每个节点的丰富表示，使得独立进行节点分类成为可能。
- en: '![](../Images/CH11_F05_Khamis.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5](../Images/CH11_F05_Khamis.png)'
- en: Figure 11.5 Graph embedding and node, link, and graph classification
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 图嵌入和节点、链接和图分类
- en: Moreover, GCNs are also capable of predicting characteristics related to edges,
    such as whether a link exists between two nodes. Once node embeddings are generated,
    the likelihood of an edge between two nodes *v* and *u* can be predicted based
    on their embeddings *h[v]*, *h[u]*. A common approach is to compute a similarity
    measure (e.g., a dot product) between the embeddings of two nodes. This similarity
    can then be passed through a sigmoid function to predict the probability of an
    edge. The errors (loss) on predictions will be backpropagated and update the weights
    in neural networks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GCN还能够预测与边相关的特征，例如两个节点之间是否存在链接。一旦生成了节点嵌入，就可以根据节点*v*和*u*的嵌入*h[v]*，*h[u]*预测它们之间边的可能性。一种常见的方法是计算两个节点嵌入之间的相似度度量（例如，点积）。然后，可以将这种相似度通过sigmoid函数传递，以预测边的概率。预测上的错误（损失）将反向传播并更新神经网络中的权重。
- en: Finally, GCNs enable classification at the level of the entire graph. This can
    be achieved by aggregating all the latent or embedding vectors (*H*) for all the
    nodes. The aggregation function used must be permutation invariant, meaning the
    output should remain the same regardless of the order of the nodes. Common examples
    of such functions are summation or averaging or maximizing. Once you’ve aggregated
    the latent vectors into a single representation, you can feed this representation
    into a module (e.g., a neural network layer) to predict an output for the whole
    graph. In essence, GCNs allow node-level, edge-level, and graph-level predictions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，GCN能够在整个图的层面上进行分类。这可以通过聚合所有节点的所有潜在或嵌入向量(*H*)来实现。所使用的聚合函数必须是排列不变的，这意味着输出应该与节点的顺序无关。此类函数的常见例子是求和、平均或最大化。一旦将潜在向量聚合为单一表示，就可以将此表示输入到一个模块（例如，一个神经网络层）中，以预测整个图的输出。本质上，GCN允许进行节点级、边级和图级预测。
- en: To better understand how GCN works, let’s consider a graph with five nodes,
    as shown in figure 11.6\. For each node in the graph, the first step is to find
    the neighboring nodes. Let’s assume we want to examine how the embedding for node
    5 is generated. As you can see in the original graph (upper-left corner of figure
    11.6), nodes 2 and 4 are neighbors of node 5\. The second step is message-passing,
    which is the process of nodes sending, receiving, and aggregating messages from
    their neighbors to iteratively update their features. This allows GCNs to learn
    a representation for each node that captures both its own features and its context
    within the graph. The learned representations can then be used for downstream
    tasks like node classification, link prediction, or graph classification.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解GCN的工作原理，让我们考虑一个包含五个节点的图，如图11.6所示。对于图中的每个节点，第一步是找到其相邻节点。假设我们想检查节点5的嵌入是如何生成的。如图11.6左上角的原图所示，节点2和4是节点5的相邻节点。第二步是消息传递，这是节点向其相邻节点发送、接收和聚合消息的过程，以迭代更新其特征。这使得GCN能够学习每个节点的表示，该表示既捕获了其自身的特征，也捕获了其在图中的上下文。学习到的表示可以用于下游任务，如节点分类、链接预测或图分类。
- en: '![](../Images/CH11_F06_Khamis.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F06_Khamis.png)'
- en: Figure 11.6 Message passing and updating in GCN
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 GCN中的消息传递和更新
- en: The embedding of node *v* after *t* layers of neighborhood aggregation considering
    *N*(*v*) neighboring nodes is based on the formula shown in figure 11.7\. The
    initial 0^(th) layer embeddings *h[v]*⁰ are equal to node features x[v].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到节点v的N(v)个相邻节点，经过t层邻域聚合后的节点v的嵌入基于图11.7所示的公式。初始的0层嵌入h[v]⁰等于节点特征x[v]。
- en: '![](../Images/CH11_F07_Khamis.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F07_Khamis.png)'
- en: Figure 11.7 Embedding function in GCN
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 GCN中的嵌入函数
- en: This formula is applied recursively to get another, better vector *h* at each
    time step, where *h* is the vector representation of the nodes in the latent space.
    The weight matrix is learned through training on given data. At the beginning,
    each node in the graph is aware only of its own initial features. In the first
    layer of the GCN, each node communicates with its immediate neighbors, aggregating
    its own features and receiving features from those neighbors. As we move to the
    second layer, each node again communicates with its neighbors. However, because
    the neighbors have already incorporated information from their own neighbors in
    the first layer, the original node now indirectly accesses information from two
    hops away in the graph—its neighbors’ neighbors. As this process repeats through
    more layers in the GCN, information is propagated and aggregated across the graph.
    At the end, the embedding vector for each node contains and encodes its own information
    as well as the network information from all its neighboring nodes according to
    the degree of neighborhood, or its *k*-hop neighborhood, to create context embedding.
    The *k-hop neighborhood*, or neighborhood of radius *k*, of a node is a set of
    neighboring nodes at a distance less than or equal to *k*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式递归应用于每个时间步得到另一个更好的向量h，其中h是节点在潜在空间中的向量表示。权重矩阵通过在给定数据上的训练学习得到。一开始，图中的每个节点只知道其自身的初始特征。在GCN的第一层中，每个节点与其直接相邻的节点进行通信，聚合其自身的特征并接收来自这些邻居的特征。当我们移动到第二层时，每个节点再次与其邻居进行通信。然而，因为邻居已经在第一层中从其自身的邻居那里获取了信息，所以原始节点现在间接地访问了来自两个跳远的图中的信息——其邻居的邻居。随着这个过程在GCN的更多层中重复，信息在图中传播和聚合。最后，每个节点的嵌入向量包含并编码了其自身的信息以及所有相邻节点的网络信息，根据邻域的程度，或其*k*-跳邻域，以创建上下文嵌入。节点的*k*-跳邻域，或半径为*k*的邻域，是一组距离小于或等于*k*的相邻节点。
- en: Listing 11.2 shows how to generate node embedding for the Cora dataset using
    GCN. The Cora dataset consists of 2,708 scientific publications classified into
    one of seven classes. The citation network consists of 5,429 links. Each publication
    in the dataset is described by a 0/1-valued word vector indicating the absence/presence
    of the corresponding word in the dictionary. The dictionary consists of 1,433
    unique words.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.2展示了如何使用GCN生成Cora数据集的节点嵌入。Cora数据集包含2,708篇科学出版物，分为七个类别之一。引用网络由5,429个链接组成。数据集中的每篇出版物都由一个0/1值的词向量描述，表示字典中相应单词的存在/不存在。该字典包含1,433个独特的单词。
- en: 'PyG (PyTorch Geometric) is used and can be installed as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyG（PyTorch Geometric）并可以按照以下方式安装：
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: More information about PyG CUDA installation is available in the PyG documentation
    ([https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 PyG CUDA 安装的详细信息可在 PyG 文档中找到（[https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html))。
- en: We’ll start by importing the libraries we’ll use.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入我们将使用的库。
- en: Listing 11.2 Node embedding using GCN
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2 使用 GCN 进行节点嵌入
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'PyG provides several datasets that can be loaded directly, such as KarateClub,
    Cora, Amazon, Reddit, etc. The Cora dataset is part of the Planetoid dataset and
    can be loaded as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: PyG 提供了多个可以直接加载的数据集，例如 KarateClub、Cora、Amazon、Reddit 等。Cora 数据集是 Planetoid 数据集的一部分，可以按以下方式加载：
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see in the following code, the GCN model is defined with two `GCNConv`
    layers (`GCNConv`) and a `torch.nn.Dropout` layer. `GCNConv` is a graph convolution
    layer, and `torch.nn.Dropout` is a dropout layer, which randomly zeroes some of
    the elements of the input tensor with probability 0.5 during training as a simple
    way to prevent overfitting.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下代码所示，GCN 模型使用两个 `GCNConv` 层（`GCNConv`）和一个 `torch.nn.Dropout` 层定义。`GCNConv`
    是一个图卷积层，`torch.nn.Dropout` 是一个 dropout 层，在训练期间以概率 0.5 随机将输入张量的一些元素置零，作为一种简单的防止过拟合的方法。
- en: 'The `forward` function defines the forward pass of the model. It takes a data
    object as input, representing the graph, and the features of the nodes and the
    adjacency list of the graph are extracted from the input data. The node features
    (`x`) are passed through the first GCN layer `conv1`, a `relu` activation function,
    a dropout layer, and finally the second GCN layer `conv2`. The adjacency list,
    `edge_index`, is required for the convolution operation in the GCN layers. The
    output of the network is then returned:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 函数定义了模型的正向传递。它接受一个数据对象作为输入，表示图，并从输入数据中提取节点的特征和图的邻接列表。节点特征（`x`）通过第一个
    GCN 层 `conv1`、一个 `relu` 激活函数、一个 dropout 层，最后通过第二个 GCN 层 `conv2`。邻接列表 `edge_index`
    是 GCN 层中卷积操作所必需的。然后返回网络的输出：'
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As a continuation of listing 11.2, the following code snippet trains the GCN
    model on a single graph and extracts the node embedding from the trained model.
    The `model` is trained for 200 epochs. Its gradients are first zeroed, then the
    forward pass is computed, and the negative log-likelihood loss is calculated on
    the training nodes (those marked by `data.train_mask`). The backward pass is then
    computed to get the gradients, and the optimizer performs a step to update the
    model parameters. The model is set to evaluation mode and is run on the graph
    again to obtain the final node embeddings:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 11.2 列表的延续，以下代码片段在单个图上训练 GCN 模型并从训练模型中提取节点嵌入。`model` 在 200 个时期内进行训练。其梯度首先被置零，然后计算前向传递，并在训练节点（由
    `data.train_mask` 标记的节点）上计算负对数似然损失。然后计算反向传递以获取梯度，并优化器执行一步以更新模型参数。模型被设置为评估模式，并在图上再次运行以获得最终的节点嵌入：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① If CUDA is available, the code uses the GPU; otherwise, it will use the CPU.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ① 如果 CUDA 可用，代码将使用 GPU；否则，它将使用 CPU。
- en: ② Create an instance of the GCN model, and move it to the chosen device.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建 GCN 模型的一个实例，并将其移动到选定的设备上。
- en: ③ Load the first graph in the dataset, and move it to the device.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 加载数据集中的第一个图，并将其移动到设备上。
- en: ④ Use the Adam optimizer with a learning rate of 0.01 and weight decay (a form
    of regularization) of 0.0005.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用学习率为 0.01 且权重衰减（一种正则化形式）为 0.0005 的 Adam 优化器。
- en: ⑤ Train the model for 200 epochs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 对模型进行 200 个时期的训练。
- en: ⑥ Set evaluation mode.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 设置评估模式。
- en: ⑦ Obtain the final node embeddings.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 获取最终的节点嵌入。
- en: The `.detach()` function is used to detach the output from the computational
    graph and returns a new tensor that doesn’t require a gradient. The embeddings
    are then moved from the GPU (if they were on the GPU) to the CPU. This is done
    to make the data accessible for further processing, such as converting it to a
    NumPy array. The generated embedding has a size of (2708, 7), where the number
    of nodes is 2,708 and the number of classes or subjects is 7\. Dimensionality
    reduction using principle component analysis (PCA) is applied to visualize the
    embedding in 2D as shown in figure 11.8\.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `.detach()` 函数将输出从计算图中分离出来，并返回一个新的不需要梯度的张量。然后，将嵌入从 GPU（如果它们在 GPU 上）移动到 CPU。这样做是为了使数据可用于进一步处理，例如将其转换为
    NumPy 数组。生成的嵌入大小为 (2708, 7)，其中节点数为 2,708，类别或主题数为 7。使用主成分分析（PCA）进行降维，以便如图 11.8
    所示在 2D 中可视化嵌入。
- en: '![](../Images/CH11_F08_Khamis.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F08_Khamis.png)'
- en: Figure 11.8 Node embedding using GCN in PyG
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 使用PyG中的GCN进行节点嵌入
- en: As you can see, the node embedding makes the nodes belonging to the same classes
    cluster together. This means increased discrimination power of the features, which
    results in more accurate predictions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，节点嵌入使得属于同一类的节点聚集在一起。这意味着特征的可区分性增强，从而提高了预测的准确性。
- en: The complete version of listing 11.2 available in the book’s GitHub repo also
    shows how to generate node embedding using the GCN available in StellarGraph.
    StellarGraph is a Python library for ML on graphs and networks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 书中GitHub仓库中可用的列表11.2的完整版本还展示了如何使用StellarGraph中的GCN生成节点嵌入。StellarGraph是一个用于图和网络上的机器学习的Python库。
- en: 11.3.2 Attention mechanisms
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 注意力机制
- en: As you saw in figure 11.7, the embedding function in GCN consists of message
    passing, aggregation, and update functions. The message passing function mainly
    integrates messages from the node’s neighbors based on a learnable weight matrix
    *W^t*. This weight matrix does not reflect the degree of importance of neighboring
    nodes. The convolution operation applies the same learned weights to all neighbors
    of a node as a linear transformation, without explicitly accounting for their
    importance or relevance. This might not be ideal because some segments may need
    more attention than others.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图11.7中看到的，GCN中的嵌入函数包括消息传递、聚合和更新函数。消息传递函数主要根据可学习的权重矩阵 *W^t* 整合节点邻居的消息。这个权重矩阵并不反映相邻节点的重要性程度。卷积操作将相同的可学习权重作为线性变换应用于节点的所有邻居，而没有明确考虑它们的重要性或相关性。这可能不是理想的，因为某些部分可能需要比其他部分更多的注意力。
- en: The concept of “attention” in DL essentially permits the model to selectively
    concentrate on specific segments of the input data as it produces the output sequence.
    This mechanism ensures that context is maintained and propagated from the initial
    stages to the end. It also allows the model to dynamically allocate its resources
    by focusing on the most important parts of the input at each time step. In a broad
    sense, attention in DL can be visualized as a vector consisting of importance
    or relevance scores. These scores help quantify the relationship or association
    between a node in a graph and all other nodes in the graph.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）中“注意力”的概念本质上允许模型在生成输出序列时，有选择性地集中关注输入数据的特定部分。这种机制确保了上下文从初始阶段到末尾的维持和传播。它还允许模型通过在每个时间步集中关注输入的最重要部分来动态分配其资源。从广义上讲，深度学习中的注意力可以可视化为一个由重要性或相关性分数组成的向量。这些分数有助于量化图中节点与图中所有其他节点之间的关系或关联。
- en: Attention is all you need
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力即一切
- en: The groundbreaking paper “Attention Is All You Need” [8] proposes a new Transformer
    model for processing sequential data like text. In the world of language processing
    and translation, models usually read an entire sentence or document word by word,
    in order (like we do when we read a book), and then make predictions based on
    that. These models have some difficulties understanding long sentences and recalling
    information from far away in the text. In the case of long sequences, there is
    a high probability that the initial context will be lost by the end of the sequence.
    This is called the *forgetting problem*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 具有里程碑意义的论文“Attention Is All You Need” [8] 提出了一种新的Transformer模型，用于处理如文本之类的序列数据。在语言处理和翻译的世界里，模型通常逐字逐句地读取整个句子或文档，按照顺序（就像我们读书时一样），然后基于此做出预测。这些模型在理解长句子和从文本中回忆信息方面存在一些困难。在长序列的情况下，到序列末尾时初始上下文可能会丢失。这被称为*遗忘问题*。
- en: The authors of the paper propose a different way of handling this task. Instead
    of reading everything in order, their model focuses on different parts of the
    input at different times, almost like it’s jumping around the text. This is what
    they refer to as “attention.” The attention mechanism allows the model to dynamically
    prioritize which parts of the input are most relevant for each word it’s trying
    to predict, making it more effective at understanding context and reducing confusion
    arising from long sentences or complex phrases. For more details, see “The Annotated
    Transformer” [9].
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者们提出了一种处理这个任务的不同方法。他们的模型不是按顺序读取所有内容，而是在不同时间关注输入的不同部分，几乎就像在文本中跳跃一样。这就是他们所说的“注意力”。注意力机制允许模型动态地优先考虑对于它试图预测的每个单词来说，输入的哪些部分是最相关的，使其在理解上下文和减少长句子或复杂短语引起的混淆方面更加有效。更多细节，请参阅“Annotated
    Transformer” [9]。
- en: 'Figure 11.9b shows a graph attention network (GAT), where a weighting factor
    or attention coefficient *α* is added to the embedding equation to reflect the
    importance of the neighboring nodes. GAT uses a weighted adjacency matrix instead
    of nonweighted adjacency matrix used in case of GCN (figure 11.9a). An attentional
    mechanism *a* is used to compute unnormalized coefficients *e[vu]* across pairs
    of nodes *v* and *u* based on their features:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9b展示了图注意力网络（GAT），其中添加了一个权重因子或注意力系数*α*到嵌入方程中，以反映邻近节点的重要性。GAT使用加权邻接矩阵而不是GCN中使用的非加权邻接矩阵（图11.9a）。使用一个注意力机制*a*来计算节点对*v*和*u*之间的未归一化系数*e[vu]*，基于它们的特征：
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH11_F08_Khamis-EQ01.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F08_Khamis-EQ01.png)'
- en: '| 11.1 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 11.1 |'
- en: An example of this attentional mechanism can be dot-product attention that measures
    the similarity or alignment between the features of the two nodes, providing a
    quantitative indication of how much attention node *v* should give to node *u*.
    Other mechanisms may involve learned attention weights, nonlinear transformations,
    or more complex interactions between node features. Following the graph structure,
    node *v* can attend over nodes in its neighborhood only *i* ∈ *N[v]*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种注意力机制的例子可以是点积注意力，它衡量两个节点特征的相似性或对齐度，提供了节点*v*应该给予节点*u*多少注意力的定量指示。其他机制可能涉及学习到的注意力权重、非线性变换或节点特征之间更复杂的交互。遵循图结构，节点*v*只能在其邻域内的节点*i*
    ∈ *N[v]*上关注。
- en: Attention coefficients are typically normalized using the softmax function so
    that they are comparable, irrespective of the scale or distribution of raw scores
    in different neighborhoods or contexts. Note that in figure 11.9b, for simplicity,
    the attention coefficients *α[vu]* are denoted as *α[u]*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力系数通常使用softmax函数进行归一化，以便它们是可比较的，无论原始分数在不同邻域或上下文中的规模或分布如何。请注意，在图11.9b中，为了简化，注意力系数*α[vu]*被表示为*α[u]*。
- en: '|'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH11_F08_Khamis-EQ02.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F08_Khamis-EQ02.png)'
- en: '| 11.2 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 11.2 |'
- en: '![](../Images/CH11_F09_Khamis.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F09_Khamis.png)'
- en: Figure 11.9 Graph convolutional network (GCN) vs. graph attention network (GAT)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9 图卷积网络（GCN）与图注意力网络（GAT）的比较
- en: '*Multi-head attention* is a key component in GATs and also in the Transformer
    model discussed in the “Attention Is All You Need” paper. In a multi-head attention
    mechanism, the model has multiple sets of attention weights. Each set (or “head”)
    can learn to pay attention to different parts of the input. Instead of having
    just one focus of attention, the model can have multiple focuses, allowing it
    to capture different types of relationships and patterns in the data. In the context
    of GATs, a multi-head attention mechanism allows each node in the graph to focus
    on different neighboring nodes in different ways, as shown in figure 11.10.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*多头注意力*是GAT和“注意力即一切”论文中讨论的Transformer模型的关键组成部分。在多头注意力机制中，模型拥有多组注意力权重。每一组（或称为“头”）可以学习关注输入的不同部分。模型不再只有一个关注点，而是可以有多个关注点，使其能够捕捉数据中的不同类型的关系和模式。在GAT的上下文中，多头注意力机制允许图中的每个节点以不同的方式关注不同的邻近节点，如图11.10所示。'
- en: '![](../Images/CH11_F10_Khamis.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F10_Khamis.png)'
- en: Figure 11.10 Multi-head attention with *H* = 3 heads by node 5\. *α*[52], *α*[54],
    and *α*[55] are the attention coefficients between the nodes. The aggregated features
    from each head are averaged to obtain the final embedding of the node.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10展示了具有3个头的多头注意力机制，节点5的*α*[52]，*α*[54]，和*α*[55]是节点之间的注意力系数。每个头的聚合特征被平均以获得节点的最终嵌入。
- en: Once the multiple heads have performed their respective attention operations,
    their results are typically averaged. This process condenses the diverse perspectives
    captured by the multiple attention heads into a single output. After the results
    of the multi-head attention operation are combined, a final nonlinearity is then
    applied. This step typically involves the use of a softmax function or logistic
    sigmoid function, especially in classification problems. These functions serve
    to translate the model’s final outputs into probabilities, making the output easier
    to interpret and more useful for prediction tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个头完成它们各自的关注操作后，通常会对它们的结果进行平均。这个过程将多个注意力头捕获的多样化视角压缩成一个单一的输出。在多头注意力操作的结果结合之后，随后应用一个最终的非线性变换。这一步通常涉及使用softmax函数或逻辑sigmoid函数，尤其是在分类问题中。这些函数的作用是将模型的最终输出转换为概率，使得输出更容易解释，并更适用于预测任务。
- en: 11.3.3 Pointer networks
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.3 指针网络
- en: Sequential ML involves dealing with data where the order of observations matters,
    such as time series data, sentences, or permutations. Sequential ML tasks can
    be classified based on the number of inputs and outputs, as shown in table 11.2\.
    A *sequence-to- sequence* (seq2seq) model takes a sequence of items and outputs
    another sequence of items. Recurrent neural networks (RNN) and long short-term
    memory (LSTM) have been established as state-of-the-art approaches in seq2seq
    modeling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 序列机器学习涉及处理观察顺序重要的数据，如时间序列数据、句子或排列。根据输入和输出的数量，序列机器学习任务可以分类，如表11.2所示。*序列到序列*（seq2seq）模型接收一系列项目并输出另一个序列的项目。循环神经网络（RNN）和长短期记忆（LSTM）已被确立为seq2seq建模中的最先进方法。
- en: Table 11.2 Sequential ML
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.2 序列机器学习
- en: '| Task | Example |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 示例 |'
- en: '| --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| One-to-one | Image classification. We provide a single image as input, and
    the model outputs the classification or category, like “dog” or “cat,” as a single
    output. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 一对一 | 图像分类。我们提供一个单独的图像作为输入，模型输出分类或类别，如“狗”或“猫”，作为单个输出。 |'
- en: '| One-to-many | Image captioning. We input a single image into the model, and
    it generates a sequence of words describing that image. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 一对多 | 图像描述。我们将单个图像输入模型，它生成描述该图像的一系列单词。 |'
- en: '| Many-to-one | Sentiment analysis. We input a sequence of words (like a sentence
    or a tweet), and the model outputs a single sentiment score (like “positive,"
    “negative,” or “neutral”). |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 多对一 | 情感分析。我们输入一系列单词（如句子或推文），模型输出一个单一的情感评分（如“积极”、“消极”或“中性”）。 |'
- en: '| Many-to-many (type 1) | Sequence input and sequence output, like in the case
    of named entity recognition (NER). We input a sentence (a sequence of words),
    and the model outputs the recognized entity, such as a person, organization, location,
    etc. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 多对多（类型1） | 序列输入和序列输出，如命名实体识别（NER）的情况。我们输入一个句子（单词序列），模型输出识别的实体，如人、组织、地点等。
    |'
- en: '| Many-to-many (type 2), known as a synchronized sequence model | Synced sequence
    input and output. The model takes a sequence of inputs but doesn’t output anything
    until the entire sequence has been read. Then it outputs a sequence. An example
    of this is video classification, where the model takes a sequence of video frames
    as input and then outputs a sequence of labels for those frames. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 多对多（类型2），称为同步序列模型 | 同步序列输入和输出。该模型接收一系列输入，但在读取整个序列之前不输出任何内容。然后它输出一个序列。此类的一个例子是视频分类，其中模型接收一系列视频帧作为输入，然后输出这些帧的标签序列。
    |'
- en: In discrete combinatorial optimization problems like the travelling salesman
    problem, sorting tasks, or the convex hull problem, both the input and output
    data are sequential. However, traditional seq2seq models struggle to solve these
    problems effectively. This is primarily because the discrete categories of output
    elements are not predetermined. Instead, they are contingent on the variable size
    of the input (for instance, the output dictionary is dependent on the input length).
    The *pointer network* (Ptr-Net) model [10] addresses this problem by utilizing
    attention as a mechanism to point to or select a member of the input sequence
    for the output. This model not only enhances performance over the conventional
    seq2seq model equipped with input attention, but it also enables us to generalize
    to output dictionaries of variable sizes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散组合优化问题，如旅行商问题、排序任务或凸包问题中，输入和输出数据都是序列化的。然而，传统的seq2seq模型在有效解决这些问题上存在困难。这主要是因为输出元素的离散类别不是预先确定的。相反，它们取决于输入的变量大小（例如，输出字典依赖于输入长度）。*指针网络*（Ptr-Net）模型[10]通过利用注意力机制来指向或选择输入序列中的一个成员作为输出，来解决此问题。该模型不仅提高了配备输入注意力的传统seq2seq模型的性能，而且还使我们能够推广到可变大小的输出字典。
- en: 'While traditional attention mechanisms distribute attention over the input
    sequence to generate an output element, Ptr-Net instead uses attention as a pointer.
    This pointer is used to select an element from the input sequence to be included
    in the output sequence. Let’s consider the convex hull problem as an example of
    a discrete combinatorial optimization problem. A convex hull is a geometric shape,
    specifically a polygon, that fully encompasses a given set of points. It achieves
    this by optimizing two distinct parameters: it maximizes the area that the shape
    covers, while simultaneously minimizing the boundary or circumference of the shape,
    as illustrated in figure 11.11\. To understand this concept, it can be useful
    to imagine stretching a rubber band around the extreme points or vertices of the
    set. When you release the rubber band, it automatically encompasses the entire
    set in the smallest perimeter possible, and this is essentially what a convex
    hull does.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当传统的注意力机制在输入序列上分配注意力以生成输出元素时，Ptr-Net则将注意力用作指针。这个指针用于从输入序列中选择一个元素，并将其包含在输出序列中。让我们以凸包问题作为一个离散组合优化问题的例子。凸包是一种几何形状，具体来说是一个多边形，它完全包围了一个给定的点集。它是通过优化两个不同的参数来实现的：它最大化了形状覆盖的面积，同时最小化形状的边界或周长，如图11.11所示。为了理解这个概念，可以想象将橡皮筋拉伸到集合的极点或顶点周围。当你释放橡皮筋时，它会自动以可能的最小周长包围整个集合，这正是凸包所做的事情。
- en: '![](../Images/CH11_F11_Khamis.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 凸包问题](../Images/CH11_F11_Khamis.png)'
- en: Figure 11.11 The convex hull problem. a) A valid convex hull that encloses all
    points while maximizing the area and minimizing the circumference. Note that the
    number of points included in the output sequence of the polygon may be smaller
    than the number of given points. b) An invalid convex hull, as the circumference
    is not minimized. c) An invalid convex hull, as not all the points are enclosed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 凸包问题。a) 一个有效的凸包，包围了所有点，同时最大化面积和最小化周长。注意，多边形的输出序列中包含的点数可能少于给定的点数。b) 一个无效的凸包，因为周长没有最小化。c)
    一个无效的凸包，因为并非所有点都被包围。
- en: Convex hulls have a multitude of applications across a variety of disciplines.
    For example, in the field of image recognition, convex hulls can help determine
    the shape and boundary of objects within an image. Similarly, in robotics, they
    can assist in obstacle detection and navigation by defining the “reachable” space
    around a robot.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 凸包在各个学科中有着多种应用。例如，在图像识别领域，凸包可以帮助确定图像中物体的形状和边界。同样，在机器人领域，它们可以通过定义机器人周围的“可达”空间来协助障碍物检测和导航。
- en: The problem of finding or computing a convex hull, given a set of points, has
    been addressed through various algorithms. For example, the Graham scan algorithm
    sorts the points according to their angle with the point at the bottom of the
    hull and then processes them to find the convex hull [11]. The Jarvis march (or
    the gift wrapping algorithm) starts with the leftmost point and wraps the remaining
    points like wrapping a gift [12]. The quickhull algorithm finds the convex hull
    of a point set by recursively dividing the set into subsets, selecting the point
    farthest from the line between two extreme points, and eliminating points within
    the formed triangles until the hull’s vertices are identified [13].
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 给定点集，寻找或计算凸包的问题已经通过各种算法得到解决。例如，Graham扫描算法根据点与凸包底部点的角度对点进行排序，然后处理它们以找到凸包[11]。Jarvis行进（或礼物包装算法）从最左边的点开始，将剩余的点像包装礼物一样包裹起来[12]。Quickhull算法通过递归地将集合划分为子集，选择离两个极点之间的线最远的点，并消除形成三角形内的点，直到识别出凸包的顶点[13]。
- en: As shown in figure 11.12, Ptr-Net takes as input a planar set of points *P*
    = {*P*[1], *P*[2], …, *P[n]*} with *n* elements each, where *P[j]* = (*x[j], y[j]*)
    are the Cartesian coordinates of the points. The outputs *C[P]* = {*C*[1], *C*[2],…,
    *C[m]*[(]*[P]*[)]} are sequences representing the solution associated with the
    point set *P*. In this figure, Ptr-Net estimates the output sequence [1 4 2] from
    the input data points [1 2 3 4]. This output sequence represents the convex hull
    that includes all the input points with maximum area and minimum circumference.
    As can be seen, the convex hull is formed by connecting *P*[1], *P*[2], and *P*[4].
    The third point *P*[3] is inside this convex hull.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 11.12 所示，Ptr-Net 以平面点集 *P* = {*P*[1], *P*[2], …, *P[n]*} 作为输入，其中每个元素有 *n*
    个，*P[j]* = (*x[j], y[j]*) 是点的笛卡尔坐标。输出 *C[P]* = {*C*[1], *C*[2],…, *C[m]*[(]*[P]*[)]}
    是代表与点集 *P* 相关的解决方案的序列。在此图中，Ptr-Net 从输入数据点 [1 2 3 4] 估计输出序列 [1 4 2]。此输出序列表示包含所有输入点、面积最大和周长最小的凸包。如图所示，凸包是通过连接
    *P*[1]，*P*[2]，和 *P*[4] 形成的。第三个点 *P*[3] 在此凸包内部。
- en: '![](../Images/CH11_F12_Khamis.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F12_Khamis.png)'
- en: Figure 11.12 Pointer network (Prt-Net) estimating the output sequence [1 4 2]
    from the input data points [1 2 3 4]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 Pointer network (Prt-Net) 从输入数据点 [1 2 3 4] 估计输出序列 [1 4 2]
- en: 'Ptr-Net consists of three main components:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Ptr-Net 由三个主要组件组成：
- en: '*Encoder*—The encoder is a recurrent neural network (RNN), often implemented
    with long short-term memory (LSTM) units or gated recurrent units (GRUs). The
    encoder’s purpose is to process the input sequence, converting each input element
    into a corresponding hidden state. These hidden states (*e*[1],…, *e*[n]) encapsulate
    the context-dependent representation of the elements in the input sequence.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码器*—编码器是一个循环神经网络（RNN），通常使用长短期记忆（LSTM）单元或门控循环单元（GRUs）实现。编码器的目的是处理输入序列，将每个输入元素转换为相应的隐藏状态。这些隐藏状态
    (*e*[1],…, *e*[n]) 封装了输入序列中元素的上下文相关表示。'
- en: '*Decoder*—Like the encoder, the decoder is also an RNN. It’s responsible for
    generating the output sequence (*d*[1],…, *d*[m]). For each output step, it takes
    the previous output and its own hidden state as inputs.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解码器*—与编码器一样，解码器也是一个 RNN。它负责生成输出序列 (*d*[1],…, *d*[m])。对于每个输出步骤，它将前一个输出及其自身的隐藏状态作为输入。'
- en: '*Attention mechanism (pointer)*—The attention mechanism in a Ptr-Net operates
    as a pointer. It computes a distribution over the hidden states output by the
    encoder, indicating where to “point” in the input sequence for each output step.
    Essentially, it decides which of the inputs should be the next output. The attention
    mechanism is a softmax function over the learned attention scores, which gives
    a probability distribution over the input sequence, signifying the likeliness
    of each element being pointed at.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意力机制（指针）*—Ptr-Net 中的注意力机制作为一个指针操作。它计算编码器输出的隐藏状态上的分布，指示每个输出步骤在输入序列中的“指向”位置。本质上，它决定哪些输入应该是下一个输出。注意力机制是学习到的注意力分数上的
    softmax 函数，它给出了输入序列上的概率分布，表示每个元素被指向的可能性。'
- en: 'The attention vector at each output time *i* is computed using the following
    equations:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个输出时间 *i* 计算注意力向量使用以下方程：
- en: '|'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH11_F12_Khamis-EQ03.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F12_Khamis-EQ03.png)'
- en: '| 11.3 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 11.3 |'
- en: '|'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH11_F12_Khamis-EQ04.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F12_Khamis-EQ04.png)'
- en: '| 11.4 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 11.4 |'
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH11_F12_Khamis-EQ05.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F12_Khamis-EQ05.png)'
- en: '| 11.5 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 11.5 |'
- en: where
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*u[j]* is the attention vector or alignment score that represents the similarity
    between the decoder and encoder hidden states. *v, W*[1], and *W*[2] are learnable
    parameters of the model. If the same hidden dimensionality is used for the encoder
    and decoder (typically 512), *v* is a vector, and *W*[1] and *W*[2] are square
    matrices.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u[j]* 是表示解码器和编码器隐藏状态之间相似性的注意力向量或对齐分数。*v*，*W*[1]，和 *W*[2] 是模型的可学习参数。如果编码器和解码器使用相同的隐藏维度（通常为
    512），则 *v* 是一个向量，而 *W*[1] 和 *W*[2] 是方阵。'
- en: '*a[j]* is the attention mask over the input or weights computed by applying
    the softmax operation to the alignment scores.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a[j]* 是对输入或通过应用 softmax 操作到对齐分数计算出的权重上的注意力掩码。'
- en: '*d[i]*^’ is the context vector that is fed into the decoder at each time step.
    In other words, *d[i]* and *d[i]*^’ are concatenated and used as the hidden states
    from which the predictions are made. This weighted sum of all the encoder hidden
    states allows the decoder to flexibly focus the attention on the most relevant
    parts of the input sequence.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d[i]*''是每个时间步输入到解码器中的上下文向量。换句话说，*d[i]*和*d[i]*''被连接起来，用作预测的隐藏状态。所有编码器隐藏状态的加权总和允许解码器灵活地将注意力集中在输入序列的最相关部分。'
- en: Ptr-Net can process variable-length sequences and solve complex combinatorial
    problems, especially those involving sorting or ordering tasks, where the output
    is a permutation of the input, as you will see in section 11.9.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Ptr-Net可以处理可变长度的序列并解决复杂的组合问题，特别是那些涉及排序或排序任务的问题，其中输出是输入的排列，正如你将在第11.9节中看到的。
- en: 11.4 Self-organizing maps
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 自组织映射
- en: The *self-organizing map* (SOM), also known as a *self-organizing feature map*
    (SOFM) or *Kohonen map*, is a type of artificial neural network (ANN) that is
    trained with unsupervised learning to produce a low-dimensional (typically two-dimensional),
    discretized representation of the input space of the training samples, called
    a *map*. SOMs are distinguished from traditional ANNs by the nature of their learning
    process, known as *competitive learning*. In such algorithms, processing elements
    or neurons compete for the right to respond to a subset of the input data. The
    degree to which an output neuron is activated is amplified as the similarity between
    the neuron’s weight vector and the input grows. The similarity between the weight
    vector and the input, leading to neuron activation, is commonly gauged through
    the calculation of Euclidean distance. The output unit that demonstrates the highest
    level of activation, or equivalently the shortest distance, in response to a specific
    input is deemed the best matching unit (BMU) or the “winning” neuron, as illustrated
    in figure 11.13\. This winner is then drawn incrementally closer to the input
    data point by adjusting its weight.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**自组织映射**（SOM），也称为**自组织特征映射**（SOFM）或**Kohonen映射**，是一种人工神经网络（ANN），通过无监督学习进行训练，以产生训练样本输入空间的低维（通常是二维）离散表示，称为**映射**。SOMs与传统ANN的区别在于其学习过程的性质，称为**竞争学习**。在这些算法中，处理元素或神经元为了响应输入数据的一个子集而竞争。输出神经元激活的程度随着神经元权重向量和输入之间的相似性增加而增强。权重向量和输入之间的相似性，导致神经元激活，通常通过计算欧几里得距离来衡量。在响应特定输入时，表现出最高激活水平或等效的最短距离的输出单元被认为是最佳匹配单元（BMU）或“获胜”神经元，如图11.13所示。然后，通过调整其权重，将这个获胜者逐渐调整到输入数据点更近的位置。'
- en: '![](../Images/CH11_F13_Khamis.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F13_Khamis.png)'
- en: Figure 11.13 Self-organizing map (SOM) with a Gaussian neighborhood function
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 带高斯邻域函数的自组织映射（SOM）
- en: 'A key characteristic of SOM is the concept of a *neighborhood function*, which
    ensures that not only the winning neuron but also its neighbors learn from each
    new input, creating clusters of similar data. This allows the network to preserve
    the topological properties of the input space. Equation 11.6 shows an example
    of a neighborhood function:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: SOM的一个关键特性是**邻域函数**的概念，它确保不仅获胜神经元，而且其邻居也会从每个新的输入中学习，从而创建相似数据的簇。这使得网络能够保留输入空间的整体拓扑属性。方程11.6展示了邻域函数的一个例子：
- en: '|'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH11_F13_Khamis-EQ06.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F13_Khamis-EQ06.png)'
- en: '| 11.6 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 11.6 |'
- en: where *v* is the index of the node in the map, *u* is the index of the winning
    neuron, *LDist*(*u,v*) represents the lattice distance between *u* and *v*, and
    *σ* is the bandwidth of the Gaussian kernel. In SOMs, *σ* represents the radius
    or width of the neighborhood and determines how far the influence of the winning
    neuron extends to its neighbors during the weight update phase. A large *σ* means
    a broader neighborhood is affected. On the other hand, a small *σ* means that
    fewer neighboring neurons are influenced. When *σ* is set to an extremely small
    value, the neighborhood effectively shrinks to include only the winning neuron
    itself. This means that only the winning neuron’s weights are significantly updated
    in response to the input, while the weights of the other neurons are barely or
    not at all affected. This behavior, where only the winning neuron is updated,
    is referred to as “winner take all” learning.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *v* 是地图中节点的索引，*u* 是获胜神经元的索引，*LDist*(u,v*) 代表 *u* 和 *v* 之间的晶格距离，而 *σ* 是高斯核的带宽。在SOMs中，*σ*
    代表邻域的半径或宽度，并决定了获胜神经元的权重更新阶段对其邻居的影响范围。大的 *σ* 意味着影响范围更广的邻域。另一方面，小的 *σ* 意味着受影响的邻近神经元更少。当
    *σ* 设置为极小值时，邻域实际上缩小到只包括获胜神经元本身。这意味着只有获胜神经元的权重会因输入而显著更新，而其他神经元的权重几乎或根本不受影响。这种只更新获胜神经元的行为被称为“胜者全得”学习。
- en: Algorithm 11.1 shows the steps of SOM, assuming that *D[t]* is a target input
    data vector, *W[v]* is the current weight vector of node *v, θ*(*u*,*v*,*s*) is
    the neighborhood function that represents the restraint due to the distance from
    the winning neuron, and *α* is a learning rate where *α* ∈ (0,1).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 算法11.1展示了SOM的步骤，假设 *D[t]* 是目标输入数据向量，*W[v]* 是节点 *v* 的当前权重向量，θ(u,v,s*) 是表示获胜神经元距离引起的约束的邻域函数，而
    *α* 是学习率，其中 *α* ∈ (0,1)。
- en: Algorithm 11.1 Self-organizing map (SOM)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 算法11.1 自组织映射（SOM）
- en: '[PRE7]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: SOMs were initially used as a dimensionality reduction method for data visualization
    and clustering tasks. For example, the neural phonetic typewriter was one of the
    early applications of Kohonen’s SOM algorithm. It was a system where spoken phonemes
    (the smallest unit of speech that can distinguish one word from another) were
    recognized and converted into symbols. When someone spoke into the system, the
    SOM would classify the input phoneme and type the corresponding symbol. SOMs can
    be applied to different problems such as feature extraction, adaptive control,
    and travelling salesman problems (see section 11.8).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs最初被用作数据可视化和聚类任务的数据降维方法。例如，Kohonen的SOM算法的早期应用之一是神经语音打字机。这是一个系统，其中可以识别和将语音音素（可以区分一个词与另一个词的最小语音单位）转换为符号。当有人向系统说话时，SOM会分类输入的音素并输入相应的符号。SOMs可以应用于不同的问题，如特征提取、自适应控制和旅行商问题（见第11.8节）。
- en: SOMs offer a significant advantage in that they preserve the relative distances
    between points as calculated within the input space. Points that are close in
    the input space are mapped onto neighboring units within the SOM, making SOMs
    effective tools for analyzing clusters within high-dimensional data. When using
    techniques like principal component analysis (PCA) to handle high-dimensional
    data, data loss may occur when reducing the dimensions to two. If the data contains
    numerous dimensions and if each dimension carries valuable information, then SOMs
    can be superior to PCA for dimensionality reduction purposes. Beyond this, SOMs
    also possess the ability to generalize. Through this process, the network can
    identify or categorize input data that it has not previously encountered. This
    new input is associated with a specific unit on the map and is thus mapped accordingly.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: SOMs的一个显著优势在于它们保留了输入空间内计算出的点之间的相对距离。在输入空间中靠近的点被映射到SOM中的邻近单元，这使得SOM成为分析高维数据中簇的有效工具。当使用主成分分析（PCA）等技术处理高维数据时，在将维度降低到二维时可能会发生数据丢失。如果数据包含许多维度，并且每个维度都携带有价值的信息，那么SOMs在降维方面可能优于PCA。除此之外，SOMs还具有泛化的能力。通过这个过程，网络可以识别或分类它以前未遇到的输入数据。这种新的输入与地图上的特定单元相关联，因此相应地映射。
- en: The previous sections have offered a fundamental foundation in ML, equipping
    you with essential background knowledge. The upcoming sections will delve deeply
    into the practical applications of supervised and unsupervised ML in tackling
    optimization problems.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 前几节提供了机器学习的基本基础，为你提供了必要的背景知识。接下来的几节将深入探讨监督和无监督机器学习在解决优化问题中的实际应用。
- en: 11.5 Machine learning for optimization problems
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 优化问题的机器学习
- en: 'The utilization of ML techniques to tackle combinatorial optimization problems
    represents an emergent and exciting field of study. *Neural combinatorial optimization*
    refers to the application of ML and neural network models, specifically seq2seq
    supervised models, unsupervised models, and reinforcement learning, to solve combinatorial
    optimization problems. Within this context, the application of ML to combinatorial
    optimization has been comprehensively described by Yoshua Bengio and his co-authors
    [14]. The authors depict three distinctive methods for harnessing ML for combinatorial
    optimization (see figure 11.14):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 利用机器学习技术解决组合优化问题代表了一个新兴且令人兴奋的研究领域。"神经组合优化"指的是将机器学习和神经网络模型，特别是seq2seq监督模型、无监督模型和强化学习，应用于解决组合优化问题。在此背景下，Yoshua
    Bengio及其合作者[14]全面描述了机器学习在组合优化中的应用。作者描述了三种利用机器学习解决组合优化问题的独特方法（见图11.14）：
- en: '![](../Images/CH11_F14_Khamis.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F14_Khamis.png)'
- en: Figure 11.14 Machine learning (ML) for combinatorial optimization (CO) problems
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 机器学习（ML）在组合优化（CO）问题中的应用
- en: '*End-to-end learning*—To use ML to address optimization problems, we need to
    instruct the ML model to formulate solutions directly from the input instance.
    An example of this approach is Ptr-Net, which is trained on *m* points and validated
    on *n* points for a Euclidean planar symmetric TSP [10]. Examples of solving combinatorial
    optimization problems using end-to-end learning are provided in sections 11.6,
    11.7, and 11.9.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*端到端学习*—为了使用机器学习解决优化问题，我们需要指导机器学习模型直接从输入实例中制定解决方案。Ptr-Net是这种方法的例子，它在*m*个点上训练，在*n*个点上验证欧几里得平面对称TSP
    [10]。使用端到端学习解决组合优化问题的例子在11.6、11.7和11.9节中提供。'
- en: '*Learning to configure algorithms*—The second method involves applying an ML
    model to enhance a combinatorial optimization algorithm with pertinent information.
    In this regard, ML can offer a parameterization of the algorithm. Examples of
    such parameters comprise, but are not restricted to, learning rate or step size
    in gradient descent methodologies; initial temperature or cooling schedule in
    simulated annealing; standard deviation of Gaussian mutation or selective crossover
    in genetic algorithms; inertia weight or cognitive and social acceleration coefficients
    in particle swarm optimization (PSO); or rate of evaporation, influence of pheromone
    deposition, or influence of the desirability of state transition in ant colony
    optimization (ACO).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习配置算法*—第二种方法涉及将机器学习模型应用于组合优化算法，并利用相关信息进行增强。在这方面，机器学习可以提供算法的参数化。此类参数的例子包括但不限于梯度下降方法中的学习率或步长；模拟退火中的初始温度或冷却计划；遗传算法中高斯变异的标准差或选择性交叉；粒子群优化（PSO）中的惯性权重或认知和社会加速系数；或者蚁群优化（ACO）中蒸发率、信息素沉积的影响或状态转换的期望影响。'
- en: '*ML in conjunction with optimization algorithms*—The third method calls for
    a combinatorial optimization algorithm to repetitively consult the same ML model
    for decision-making purposes. The ML model accepts as input the current state
    of the algorithm, which could encompass the problem definition. The fundamental
    distinction between this approach and the other two lies in the repeated utilization
    of the same ML model by the combinatorial optimization algorithm to make identical
    kinds of decisions, approximately as many times as the total number of iterations
    of the algorithm. An example of this approach is DL-assisted heuristic tree search
    (DLTS), which consists of a heuristic tree search in which decisions about which
    branches to explore and how to bound nodes are made by deep neural networks (DNNs)
    [15].'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与优化算法结合的机器学习*——第三种方法要求使用组合优化算法反复咨询相同的机器学习模型进行决策。机器学习模型接受算法当前状态作为输入，这可能包括问题定义。与另外两种方法的基本区别在于，组合优化算法重复使用相同的机器学习模型来做出相同类型的决策，大约与算法的总迭代次数一样多。这种方法的例子是深度学习辅助启发式树搜索（DLTS），其中决策树搜索中关于探索哪些分支以及如何界定节点由深度神经网络（DNNs）[15]做出。'
- en: 'Another intriguing research paper by Vesselinova et al. delves into some pertinent
    questions concerning the intersection of ML and combinatorial optimization [16].
    Specifically, the paper investigates the following questions:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Vesselinova等人的一篇引人入胜的研究论文深入探讨了机器学习和组合优化交叉的一些相关问题[16]。具体来说，该论文研究了以下问题：
- en: Can ML techniques be utilized to automate the process of learning heuristics
    for combinatorial optimization tasks and, as a result, solve these problems more
    efficiently?
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能否利用机器学习技术自动化组合优化任务的学习启发式过程，从而更有效地解决这些问题？
- en: What essential ML methods have been employed to tackle these real-world problems?
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这些现实世界问题都使用了哪些基本的机器学习方法？
- en: How applicable are these methods to practical domains?
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法在实用领域中的适用性如何？
- en: 'This paper offers a thorough survey of various applications of supervised and
    reinforcement learning strategies in tackling optimization problems. The authors
    analyze these learning approaches by examining their application to a range of
    optimization problems:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对监督学习和强化学习策略在解决优化问题中的应用进行了全面综述。作者通过考察这些学习方法在一系列优化问题中的应用来分析这些学习途径：
- en: The knapsack problem (KP), where the goal is to maximize the total value of
    items chosen without exceeding the capacity of the knapsack
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背包问题（KP），目标是最大化所选物品的总价值，同时不超过背包的容量
- en: The maximal clique (MC) and maximal independent set (MIS) problems, which both
    involve identifying subsets of a graph with specific properties
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大团（MC）和最大独立集（MIS）问题，这两个问题都涉及识别具有特定属性的图子集
- en: The maximum coverage problem (MCP), which requires selecting a subset of items
    to maximize coverage
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大覆盖问题（MCP），需要选择一个子集以最大化覆盖范围
- en: The maximum cut (MaxCut) and minimum vertex cover (MVC) problems, which involve
    partitioning a graph in particular ways
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大切割（MaxCut）和最小顶点覆盖（MVC）问题，这些问题涉及以特定方式划分图
- en: In addition, the paper discusses the application of ML approaches to the satisfiability
    problem (SAT), which is a decision problem involving Boolean logic; the classic
    TSP, which requires finding the shortest possible route that visits a given set
    of cities and returns to the origin city; and the vehicular routing problem (VRP),
    which is a generalized version of TSP where multiple “salesmen” (vehicles) are
    allowed. More information about benchmark optimization problems is provided in
    appendix B.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该论文还讨论了机器学习方法在可满足性问题（SAT）中的应用，这是一个涉及布尔逻辑的决策问题；经典的旅行商问题（TSP），需要找到访问给定城市集并返回起点城市的最短路径；以及车辆路径问题（VRP），这是TSP的推广版本，允许有多个“销售人员”（车辆）。关于基准优化问题的更多信息请参见附录B。
- en: Optimization by prompting (OPRO) is described in Chengrun et al.’s “Large Language
    Models as Optimizers” article as a simple and effective approach to using large
    language models (LLMs) as optimizers, where the optimization task is described
    in natural language [17]. Additional examples showcasing the use of ML in addressing
    optimization problems can be accessed through the AI for Smart Mobility publication
    hub ([https://medium.com/ai4sm](https://medium.com/ai4sm)). To stimulate further
    exploration and draw more researchers into this emerging domain, a competition
    named Machine Learning for Combinatorial Optimization (ML4CO) was organized as
    part of the Neural Information Processing Systems (NeurIPS) conference. The competition
    posed a unique proposition for participants, requiring them to devise ML models
    or algorithms targeted at resolving three separate challenges. Each of these challenges
    mirrors a specific control task that commonly emerges in conventional optimization
    solvers. This competition provides a platform where researchers can explore and
    test novel ML strategies, contributing to the advancement of the field of combinatorial
    optimization.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Chengrun 等人在“大型语言模型作为优化器”文章中描述了通过提示（OPRO）进行优化，这是一种简单而有效的方法，用于使用大型语言模型（LLMs）作为优化器，其中优化任务以自然语言描述
    [17]。更多展示机器学习在解决优化问题中应用的示例可以通过 AI for Smart Mobility 发布中心获取（[https://medium.com/ai4sm](https://medium.com/ai4sm)）。为了进一步激发探索并吸引更多研究人员进入这个新兴领域，作为神经信息处理系统（NeurIPS）会议的一部分，组织了一场名为机器学习组合优化（ML4CO）的比赛。比赛为参与者提出了一个独特的命题，要求他们设计针对解决三个不同挑战的机器学习模型或算法。这些挑战中的每一个都反映了在传统优化求解器中常见的一个特定控制任务。这场比赛提供了一个平台，研究人员可以在其中探索和测试新的机器学习策略，从而推动组合优化领域的发展。
- en: 11.6 Solving function optimization using supervised machine learning
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 使用监督机器学习解决函数优化
- en: '*Amortized optimization*, or *learning to optimize*, is an approach where ML
    models are used to rapidly predict the solutions to an optimization problem. Amortized
    optimization methods try to learn the mapping between the decision variable space
    and the optimal or near-optimal solution space. The learned model can be used
    to predict the optimal value of an objective function, enabling fast solvers.
    The computation cost of the optimization process is spread out between learning
    and inferencing. This is the reason for the name “amortized optimization,” as
    the word “amortization” generally refers to spreading out costs.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*摊销优化*，或 *学习优化*，是一种使用机器学习模型快速预测优化问题解的方法。摊销优化方法试图学习决策变量空间与最优或近似最优解空间之间的映射。学习到的模型可以用来预测目标函数的最优值，从而实现快速求解器。优化过程的计算成本在学习和推理之间分散。这就是“摊销优化”这个名称的由来，因为“摊销”一词通常指的是分散成本。'
- en: B. Amos shows several examples of how to use amortized optimization to solve
    optimization problems in his tutorial [18]. For example, a supervised ML approach
    can learn to solve optimization problems over spheres. Here the objective is to
    find the extreme values of a function defined on the earth or other space that
    can be approximated with a sphere of the form
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: B. Amos 在他的教程 [18] 中展示了如何使用摊销优化来解决优化问题的几个示例。例如，一个监督机器学习方法可以学习解决球面上的优化问题。在这里，目标是找到定义在地球或其他空间上、可以用球面形式近似的函数的极值
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH11_F14_Khamis-EQ07.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH11_F14_Khamis-EQ07.png)'
- en: '| 11.7 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 11.7 |'
- en: 'where *S*² is the surface of the unit 2-sphere embedded in real-number space
    *R*³ as *S*²:= {*y* ∈ *R*³ | ||*y*||[2] =1}, and *x* is some parameterization
    of the function *f* : *S*² × *X* → *R*. ||*y*||[2] refers to the Euclidean norm
    (also known as the *L2 norm* or *2-norm*) of a vector *y*. More details about
    the amortization objective function are available in Amos’s “Tutorial on amortized
    optimization for learning to optimize over continuous domains” [18].'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 *S*² 是嵌入在实数空间 *R*³ 中的单位 2-球面，表示为 *S*²:= {*y* ∈ *R*³ | ||*y*||[2] =1}，而 *x*
    是函数 *f* : *S*² × *X* → *R* 的某种参数化。||*y*||[2] 指的是向量 *y* 的欧几里得范数（也称为 *L2 范数* 或 *2-范数*）。关于摊销目标函数的更多细节可以在
    Amos 的“关于在连续域上学习优化的摊销优化教程”中找到 [18]。'
- en: Listing 11.3 shows the steps for applying amortized optimization based on supervised
    learning to solve the problem of finding the extreme values of a function defined
    on the earth or other spaces. We’ll start by defining two conversion functions,
    `celestial_to_euclidean()` and `euclidean_to_celestial()`, that convert between
    celestial coordinates (right ascension, `ra`, and declination, `dec`) and Euclidean
    coordinates (`x, y, z`).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.3展示了使用基于监督学习的摊销优化来解决在地球或其他空间上定义的函数的极值问题的步骤。我们将首先定义两个转换函数，`celestial_to_euclidean()`
    和 `euclidean_to_celestial()`，它们将天球坐标（赤经 `ra` 和赤纬 `dec`）与欧几里得坐标（`x, y, z`）之间进行转换。
- en: The celestial coordinate system
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 天球坐标系
- en: The *astronomical* or *celestial coordinate system* is a reference system used
    to specify the positions of objects in the sky, such as satellites, stars, planets,
    galaxies, and other celestial bodies. There are several celestial coordinate systems,
    with the most common being the equatorial system. In the equatorial system, right
    ascension (RA) and declination (Dec) are the two numbers used to fix the location
    of an object in the sky. These coordinates are analogous to the latitude and longitude
    used in earth’s geographic coordinate system.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 天文或天球坐标系是用于指定天空中物体位置（如卫星、恒星、行星、星系和其他天体）的参考系。有几个天球坐标系，其中最常见的是赤道系统。在赤道系统中，赤经（RA）和赤纬（Dec）是用于确定天空中物体位置的两个数字。这些坐标与地球地理坐标系中使用的纬度和经度相似。
- en: As shown in the following figure, RA is measured in hours, minutes, and seconds
    (h:m:s), and it is analogous to longitude in earth’s coordinate system. RA is
    the angular distance of an object measured eastward along the celestial equator
    from the vernal equinox (the point where the sun crosses the celestial equator
    during the March equinox). The celestial equator is an imaginary great circle
    on the celestial sphere, lying in the same plane as earth’s equator. Dec is measured
    in degrees and represents the angular distance of an object north or south of
    the celestial equator. It is analogous to latitude in earth’s coordinate system.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图中所示，赤经（RA）是以小时、分钟和秒（h:m:s）来测量的，并且与地球坐标系中的经度相似。赤经是从春分点（太阳在3月春分时穿过天赤道的点）沿天赤道向东测量的一个物体的角距离。天赤道是位于天球上的一个想象的大圆，位于与地球赤道相同的平面上。赤纬（Dec）是以度来测量的，表示一个物体在天赤道北或南的角距离。它与地球坐标系中的纬度相似。
- en: '![](../Images/CH11_F14_UN01_Khamis.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F14_UN01_Khamis.png)'
- en: Celestial coordinate system with an example point with a right ascension of
    10 hours and a declination of 30 degrees
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 具有赤经10小时和赤纬30度的示例点的天球坐标系
- en: Positive declination is used for objects above the celestial equator, and negative
    declination is used for objects below the celestial equator.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 正赤纬用于天赤道以上的物体，负赤纬用于天赤道以下的物体。
- en: The `sphere_dist(x, y)` function calculates the Riemannian distance (the great-
    circle distance) between two points on the sphere in the Euclidean space. This
    distance represents the shortest (geodesic) path between two points on the surface
    of a sphere, measured along the surface rather than through the interior of the
    sphere. The function asserts that the input vectors are two-dimensional. Then
    it calculates the dot product of *x* and *y* and returns the arccosine of the
    result, which corresponds to the angle between *x* and *y*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`sphere_dist(x, y)` 函数计算欧几里得空间中球面上两点之间的黎曼距离（大圆距离）。这个距离代表两点之间最短（测地线）路径，沿着球面测量而不是穿过球体内部。该函数断言输入向量是二维的。然后它计算
    *x* 和 *y* 的点积，并返回结果的反余弦值，这对应于 *x* 和 *y* 之间的角度。'
- en: Listing 11.3 Solving a function optimization problem using supervised learning
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.3 使用监督学习解决函数优化问题
- en: '[PRE8]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Convert from celestial coordinates to Euclidean coordinates.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将天球坐标转换为欧几里得坐标。
- en: ② Convert from Euclidean coordinates to celestial coordinates.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将欧几里得坐标转换为天球坐标。
- en: ③ Calculate the Riemannian distance between two points on the sphere.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算球面上两点之间的黎曼距离。
- en: 'We then define a `c-convex` class as a subclass of `nn.Module`, which makes
    it a trainable model in PyTorch. Cohen and his co-authors defined *c-convex* in
    their “Riemannian convex potential maps” article as a synthetic class of optimization
    problems defined on the sphere [19]. The `c-convex` class models a c-convex function
    on the sphere with `n_components` components that we can sample data from for
    training. The `gamma` parameter controls the aggregation of the components of
    the function, and `seed` is used to initialize the random number generator for
    reproducibility. It also generates random parameters `ys` (which are unit vectors
    in the 3D space) and `alphas` (which are scalars between 0 and 0.7) for each component
    of the c-convex function. The parameters are concatenated into a single `params`
    vector. The `forward(xyz)` method calculates the value of the c-convex function
    at the point `xyz`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着定义一个`c-convex`类作为`nn.Module`的子类，这使得它成为PyTorch中的一个可训练模型。Cohen及其合作者在他们的“黎曼凸势映射”文章中将*c-凸*定义为在球面上定义的优化问题的合成类
    [19]。`c-convex`类在球面上模拟一个具有`n_components`个分量的c-凸函数，我们可以从中采样数据用于训练。`gamma`参数控制函数分量的聚合，`seed`用于初始化随机数生成器以实现可重复性。它还为c-凸函数的每个分量生成随机参数`ys`（它们是3D空间中的单位向量）和`alphas`（它们是介于0和0.7之间的标量）。参数被连接成一个单一的`params`向量。`forward(xyz)`方法计算在点`xyz`处的c-凸函数的值：
- en: '[PRE9]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Define a c-convex function.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义一个c-凸函数。
- en: ② Sample random parameters.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ② 样本随机参数。
- en: ③ Computes the output of the c-convex function given input coordinates xyz on
    the sphere.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算在球面上给定输入坐标xyz的c-凸函数的输出。
- en: 'As a continuation of the preceding code, we define an amortized model, which
    takes a parameter vector as input and outputs a 3D vector representing a point
    on the sphere. The amortized model uses a neural network to learn a mapping from
    the parameter space to the 3D space of points on the sphere. The code also initializes
    a list of `c_convex` objects with different seeds and sets the number of parameters
    for the amortized model:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 作为前面代码的延续，我们定义了一个摊销模型，它接受一个参数向量作为输入，并输出表示球面上一点的3D向量。摊销模型使用神经网络来学习从参数空间到球面上点的3D空间的映射。代码还初始化了一个具有不同种子的`c_convex`对象列表，并设置了摊销模型的参数数量：
- en: '[PRE10]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Create a list of integers representing different seeds.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个表示不同种子的整数列表。
- en: ② Create an fs list that contains different instances of the c_convex class.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个包含c_convex类不同实例的fs列表。
- en: ③ Set the number of parameters in the first c_convex object (fs[0]).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置第一个c_convex对象（fs[0]）的参数数量。
- en: 'The amortized model is represented as `nn.Module` in the following code. The
    neural network is defined as a feedforward neural network or a multilayer perceptron
    that consists of three fully connected (linear) layers with ReLU activation functions:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，摊销模型表示为`nn.Module`。神经网络被定义为包含三个具有ReLU激活函数的全连接（线性）层的前馈神经网络或多层感知器：
- en: '[PRE11]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Number of parameters in the c-convex function that will be used as input to
    the neural network
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将用作神经网络输入的c-凸函数中的参数数量
- en: ② Define the layers of the neural network in sequence.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ② 按顺序定义神经网络的层。
- en: ③ Define the forward pass of the amortized model, which maps the input p (parameter
    vector) to a point on the sphere.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定义摊销模型的前向传递，它将输入p（参数向量）映射到球面上的一个点。
- en: 'We can now train the amortized model to learn a mapping from parameter vectors
    to points on the sphere. It uses a list of c_convex functions (fs) with different
    random seeds to generate training data. The amortized model is trained using an
    Adam optimizer, and its progress is visualized using a tqdm progress bar. The
    resulting output points on the sphere are stored in a tensor xs:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以训练一个摊销模型来学习从参数向量到球面上点的映射。它使用具有不同随机种子的c_convex函数（fs）列表来生成训练数据。摊销模型使用Adam优化器进行训练，其进度使用tqdm进度条进行可视化。球面上的输出点存储在张量xs中：
- en: '[PRE12]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Set the number of hidden units for the AmortizedModel neural network.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置AmortizedModel神经网络的隐藏单元数量。
- en: ② Set the random seed to ensure the reproducibility of the training process.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置随机种子以确保训练过程的可重复性。
- en: ③ Create an instance of the AmortizedModel.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建AmortizedModel的一个实例。
- en: ④ Create an Adam optimizer to update the parameters with a learning rate of
    0.0005.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建一个学习率为0.0005的Adam优化器来更新参数。
- en: ⑤ Store the output points on the sphere for each iteration of training.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 存储每次训练迭代的球面上的输出点。
- en: ⑥ Training loop
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 训练循环
- en: ⑦ Store the losses for each c_convex function and the corresponding output points
    on the sphere (xis).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 存储每个c_convex函数及其对应的球面输出点（xis）的损失。
- en: ⑧ Iterate over each c_convex function (f) in the list fs.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 遍历列表fs中的每个c_convex函数（f）。
- en: 'After training is complete, all the predicted output points on the sphere are
    stacked along a new dimension, resulting in a tensor `xs` with the following shape:
    number of iterations, number of `c_convex` functions, 3\. Each element in this
    tensor represents a point on the sphere predicted by the amortized model at different
    stages of training. It generates a visual representation of the training progress
    for the amortized model and `c_convex` functions, as shown in figure 11.15.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，所有预测的输出点沿新维度堆叠在球面上，从而得到一个形状为“迭代次数，c_convex函数数量，3”的张量xs。这个张量中的每个元素代表在训练的不同阶段由摊销模型预测的球面上的一个点。它生成了摊销模型和c_convex函数的训练进度可视化表示，如图11.15所示。
- en: '![](../Images/CH11_F15_Khamis.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F15_Khamis.png)'
- en: Figure 11.15 Examples of output from the trained amortized model
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 训练的摊销模型输出的示例
- en: The complete version of listing 11.3 is available in the book’s GitHub repo.
    It creates a grid of celestial coordinates, evaluates the `c_convex` functions
    and the amortized model on this grid, and then plots contour maps of the functions,
    the predicted paths, and the optimal points on the sphere. The optimal points
    are the points that give minimum loss, given that supervised learning is used
    to train the amortized model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.3的完整版本可在本书的GitHub仓库中找到。它创建了一个天体坐标网格，评估网格上的c_convex函数和摊销模型，然后绘制函数、预测路径和球面上的最优点的等高线图。最优点是给出最小损失的点，前提是使用监督学习来训练摊销模型。
- en: 11.7 Solving TSP using supervised graph machine learning
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.7 使用监督图机器学习解决TSP
- en: Joshi, Laurent, and Bresson, in their “Graph Neural Networks for the Travelling
    Salesman Problem” article [20], proposed a generic end-to-end pipeline to tackle
    combinatorial optimization problems such as the traveling salesman problem (TSP),
    vehicle routing problem (VRP), satisfiability problem (SAT), maximum cut (MaxCut),
    and maximal independent set (MIS). Figure 11.16 shows the steps of solving TSP
    using ML.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Joshi、Laurent和Bresson在他们的“图神经网络用于旅行商问题”文章[20]中提出了一种通用的端到端流程来解决组合优化问题，如旅行商问题（TSP）、车辆路径问题（VRP）、可满足性问题（SAT）、最大切割（MaxCut）和最大独立集（MIS）。图11.16显示了使用机器学习解决TSP的步骤。
- en: '![](../Images/CH11_F16_Khamis.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F16_Khamis.png)'
- en: Figure 11.16 End-to-end pipeline for combinatorial optimization problems
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16 组合优化问题的端到端流程
- en: Following this approach, we start by defining the graph problem in the form
    of node features and an adjacency matrix between the nodes. A low-dimensional
    graph embedding is then generated using GNN or GCN, based on the message-passing
    approach. The probability of nodes or edges belonging to the solution is predicted
    using multilayer perceptrons (MLPs). A graph search, such as beam search (see
    chapter 4), is then applied to search the graph with the probability distribution
    over the edge to find a feasible candidate solution. Learning by imitation (supervised
    learning) and learning by exploration (reinforcement learning) are applied. Supervised
    learning minimizes the loss between optimal solutions (obtained by a well-known
    solver such as Concorde in the case of TSP) and the model’s prediction. The reinforcement
    learning approach uses a policy gradient to minimize the length of the tour predicted
    by the model at the end of decoding. Reinforcement learning is discussed in the
    next chapter.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这种方法，我们首先以节点特征和节点之间的邻接矩阵的形式定义图问题。然后，基于消息传递方法，使用GNN或GCN生成低维图嵌入。使用多层感知器（MLPs）预测节点或边属于解决方案的概率。然后，应用图搜索，如束搜索（见第4章），通过边的概率分布搜索图，以找到可行的候选解决方案。应用模仿学习（监督学习）和探索学习（强化学习）。监督学习最小化最优解（在TSP的情况下，由Concorde等知名求解器获得）与模型预测之间的损失。强化学习方法使用策略梯度来最小化解码结束时模型预测的旅行长度。强化学习将在下一章讨论。
- en: Training an ML model from scratch and applying it to solve TSP requires a substantial
    amount of code and data preprocessing. Listing 11.4 shows how you can use the
    pretrained models to solve different instances of TSP. We start by importing the
    libraries and modules we’ll use. These libraries provide functionality for handling
    data, performing computations, visualization, and optimization. The Gurobi library
    is used to eliminate subtours during optimization and to calculate the reduce
    costs for a set of points (see appendix A). We set the `CUDA_DEVICE_ORDER` and
    `CUDA_VISIBLE_DEVICES` environment variables to control the GPU device visibility.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练一个机器学习模型并将其应用于解决TSP需要大量的代码和数据预处理。列表11.4展示了如何使用预训练模型来解决TSP的不同实例。我们首先导入我们将使用的库和模块。这些库提供了处理数据、执行计算、可视化和优化的功能。Gurobi库用于优化过程中的子回路消除以及计算一组点的降低成本（见附录A）。我们设置`CUDA_DEVICE_ORDER`和`CUDA_VISIBLE_DEVICES`环境变量以控制GPU设备的可见性。
- en: Listing 11.4 Solving TSP using supervised ML
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.4 使用监督机器学习解决TSP
- en: '[PRE13]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As a continuation, the following `opts` class contains several class-level
    attributes that define the following options and configurations:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 作为延续，以下`opts`类包含几个类级别属性，定义了以下选项和配置：
- en: '`dataset path`—The TSP dataset available in the book’s GitHub repo.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset path`—书中GitHub仓库中可用的TSP数据集。'
- en: '`batch size`—This determines the number of TSP instances (problems) processed
    simultaneously during training or evaluation. It specifies how many TSP instances
    are grouped together and processed in parallel.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch size`—这决定了在训练或评估期间同时处理的TSP实例（问题）数量。它指定了将多少个TSP实例分组在一起并行处理。'
- en: '`number of samples`—This is the number of samples per TSP size.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number of samples`—这是每个TSP大小下的样本数量。'
- en: '`neighbors`—This is used in the TSP data processing pipeline to specify the
    proportion (percentage) of nearest neighbors to consider for graph sparsification.
    It controls the connectivity of the TSP graph by selecting a subset of the nearest
    neighbors for each node.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighbors`—在TSP数据处理管道中使用，用于指定用于图稀疏化的最近邻的比例（百分比）。它通过为每个节点选择最近邻的子集来控制TSP图的连通性。'
- en: '`knn strategy`—This is the strategy used to determine the number of nearest
    neighbors when performing graph sparsification. In the code, the `''percentage''`
    value indicates that the number of nearest neighbors is determined by the `neighbors`
    parameter, which specifies the percentage of neighbors to consider.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`knn strategy`—这是在执行图稀疏化时确定最近邻数量的策略。在代码中，`''percentage''`值表示最近邻的数量由`neighbors`参数确定，该参数指定了要考虑的邻居百分比。'
- en: '`model`—This is the path for the pretrained ML model. The model used is a pretrained
    GNN model available in the book’s GitHub repo.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`—这是预训练机器学习模型的路径。所使用的模型是书中GitHub仓库中可用的预训练GNN模型。'
- en: '`use_cuda`—This checks if CUDA is available on the system. CUDA is a parallel
    computing platform and programming model that allows for efficient execution of
    computations on NVIDIA GPUs. `torch.cuda.is_available()` returns a Boolean value
    (true or false) indicating whether CUDA is available or not. If CUDA is available,
    that means a compatible NVIDIA GPU is present on the system and can be utilized
    for accelerated computations.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cuda`—这检查系统上是否有CUDA可用。CUDA是一个并行计算平台和编程模型，允许在NVIDIA GPU上高效执行计算。`torch.cuda.is_available()`返回一个布尔值（true或false），指示CUDA是否可用。如果CUDA可用，则表示系统上存在兼容的NVIDIA
    GPU，并且可以用于加速计算。'
- en: '`device`—This is the device to be used for computations:'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`—这是用于计算的设备：'
- en: '[PRE14]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next step is to create a dataset object using the TSP class with the following
    parameters:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用TSP类和以下参数创建一个数据集对象：
- en: '`filename`—The path or filename of the dataset to be used, specified by `opts
    .dataset_path`'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filename`—要使用的数据集的路径或文件名，由`opts .dataset_path`指定'
- en: '`batch_size`—The number of samples to include in each batch, specified by `opts.batch_size`'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`—每个批次中包含的样本数量，由`opts.batch_size`指定'
- en: '`num_samples`—The total number of samples to include in the dataset, specified
    by `opts.num_samples`'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_samples`—要在数据集中包含的总样本数量，由`opts.num_samples`指定'
- en: '`neighbors`—The value representing the number of nearest neighbors for graph
    sparsification, specified by `opts.neighbors`'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighbors`—表示用于图稀疏化的最近邻数量的值，由`opts.neighbors`指定'
- en: '`knn_strat`—The strategy for selecting nearest neighbors (`''percentage''`
    or `None`), specified by `opts.knn_strat`'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`knn_strat`—选择最近邻的策略（`''percentage''`或`None`），由`opts.knn_strat`指定'
- en: '`supervised`—A Boolean value indicating whether the dataset is used for supervised
    learning, set to `True`'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`supervised`—一个布尔值，指示数据集是否用于监督学习，设置为`True`'
- en: 'The `make_dataset` method creates an instance of the TSP dataset class and
    initializes it with the provided arguments, returning the `dataset` object:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_dataset`方法创建TSP数据集类的实例，并用提供的参数初始化它，返回`dataset`对象：'
- en: '[PRE15]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following line creates a data loader object that enables convenient iteration
    over the dataset in batches, which is useful for processing the data during evaluation.
    The `dataset` object created in the previous line will be used as the source of
    the data. You can provide other optional arguments to customize the behavior of
    the data loader, such as `shuffle` (to shuffle the data) and `num_workers` (to
    specify the number of worker processes for data loading):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行创建了一个数据加载器对象，它允许以批处理方式方便地迭代数据集，这在评估期间处理数据时很有用。上一行创建的`dataset`对象将用作数据源。您还可以提供其他可选参数来自定义数据加载器的行为，例如`shuffle`（用于打乱数据）和`num_workers`（用于指定数据加载的工作进程数）：
- en: '[PRE16]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now load the trained model and assign it to the `model` variable. If
    the model is wrapped in `torch.nn.DataParallel`, it extracts the underlying module
    by accessing `model.module`. `DataParallel` is a PyTorch wrapper that allows for
    parallel execution of models on multiple GPUs. If the model is indeed an instance
    of `DataParallel`, it extracts the underlying model module by accessing the `module`
    attribute. This step is necessary to ensure consistent behavior when accessing
    model attributes and methods. The decode type of the model is then set to `"greedy"`.
    This means that during inference or evaluation, the model should use a greedy
    decoding strategy to generate output predictions:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以加载训练好的模型并将其分配给`model`变量。如果模型被`torch.nn.DataParallel`包装，它将通过访问`model.module`提取底层模块。`DataParallel`是PyTorch的一个包装器，它允许在多个GPU上并行执行模型。如果模型确实是`DataParallel`的实例，它将通过访问`module`属性提取底层模型模块。这一步是必要的，以确保在访问模型属性和方法时保持一致的行为。然后设置模型的解码类型为`"greedy"`。这意味着在推理或评估期间，模型应使用贪婪解码策略来生成输出预测：
- en: '[PRE17]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Load a pretrained model.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载预训练模型。
- en: ② Extract the underlying module.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ② 提取底层模块。
- en: ③ Set the decoding type of the model to "greedy".
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将模型的解码类型设置为"greedy"。
- en: ④ Set the model’s mode to evaluation
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将模型的模式设置为评估模式
- en: The complete version of listing 11.4, including the visualization code, is available
    in the book’s GitHub repo. Figure 11.17 shows the output produced by the pretrained
    ML model for the TSP50 instance.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.4的完整版本，包括可视化代码，可在本书的GitHub仓库中找到。图11.17显示了预训练ML模型为TSP50实例生成的输出。
- en: '![](../Images/CH11_F17_Khamis.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F17_Khamis.png)'
- en: Figure 11.17 The TSP50 solution using a pretrained ML model
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17 使用预训练ML模型的TSP50解决方案
- en: 'The figure shows the following seven plots related to the TSP instance and
    the model’s predictions:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了与TSP实例和模型预测相关的以下七个图表：
- en: '*Concorde*—The plot in the upper-left corner shows the ground truth solution
    generated by the Concorde solver, which is an efficient implementation of the
    branch-and-cut algorithm for solving TSP instances to optimality. It shows the
    nodes of the TSP problem as circles connected by edges, representing the optimal
    tour calculated by Concorde. The title of the plot indicates the length (cost)
    of the tour obtained from Concorde.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Concorde*—左上角的图表显示了Concorde求解器生成的真实解决方案，Concorde是解决TSP实例以最优性为目的的分支和切割算法的高效实现。它显示TSP问题的节点作为由边连接的圆圈，代表Concorde计算出的最优回路。图表的标题表明了从Concorde获得的路程（成本）长度。'
- en: '*1 - (Reduce Costs)*—The second plot contains the shortest subtour and shows
    the reduced costs for the points in these subtours using the Gurobi optimization
    library. It displays the edges of the TSP as red lines, with the edge color indicating
    the reduced cost value.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1 - (降低成本)*—第二个图表包含最短子回路，并使用Gurobi优化库显示了这些子回路中点的降低成本。它以红色线条显示TSP的边，边颜色表示降低的成本值。'
- en: '*Prediction Heatmap*—The third plot presents a heatmap visualization of the
    model’s predictions for the TSP problem. It uses a color scale to represent the
    prediction probabilities of edges, with higher probabilities shown in darker shades.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测热图*—第三个图表展示了模型对TSP问题的预测的热图可视化。它使用颜色尺度来表示边的预测概率，较高的概率以较深的阴影显示。'
- en: '*Greedy Solution*—The fourth plot illustrates the solution generated by the
    ML model using a greedy decoding strategy. It displays the nodes of the TSP problem
    connected by edges, representing the tour obtained from the model. The title of
    the plot shows the length (cost) of the tour calculated by the model.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*贪婪解*—第四幅图展示了ML模型使用贪婪解码策略生成的解决方案。它显示了通过边连接的TSP问题的节点，代表模型获得的路线。图的标题显示了模型计算出的路线长度（成本）。'
- en: '*Euclidean Distance (norm by max)*—The lower-left plot is a heatmap visualization
    of the Euclidean distances between nodes in the TSP problem. It uses a color scale
    to represent the distances, with lighter shades indicating smaller distances.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欧几里得距离（最大值归一化）*—左下角的图是TSP问题中节点之间欧几里得距离的热力图可视化。它使用颜色尺度来表示距离，较浅的色调表示较小的距离。'
- en: '*Reduce Costs*—The lower-middle plot is a heatmap representation of the reduced
    costs of edges in the TSP problem. It shows the reduced costs as a color scale,
    with lower values displayed in lighter shades.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降低成本*—中间下方的图是TSP问题中边降低成本的热力图表示。它以颜色尺度显示降低的成本，较低值以较浅的色调显示。'
- en: '*1 - (Model Predictions)*—The lower-right plot presents a heatmap visualization
    of the model’s predictions for the TSP problem, similar to the third plot. However,
    in this case, the heatmap displays “1 - (Model Predictions)” by subtracting the
    model’s prediction probabilities from 1\. Darker shades represent lower probabilities,
    indicating stronger confidence in the edge selection.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1 - (模型预测)*—右下角的图展示了模型对TSP问题预测的热力图可视化，类似于第三幅图。然而，在这种情况下，热力图通过从1减去模型的预测概率来显示“1
    - (模型预测)”。较深的色调代表较低的概率，表明对边选择的信心更强。'
- en: This example demonstrated how we can employ a pretrained GNN model for solving
    TSP. Figure 11.17 displays the model’s solution alongside the Concorde TSP solver’s
    results for a TSP instance comprising 50 points of interest. More information
    and complete code, including model training steps, are available in “Learning
    the Travelling Salesperson Problem Requires Rethinking Generalization” GitHub
    repo [21].
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 本例演示了如何使用预训练的GNN模型来解决TSP。图11.17显示了模型解决方案与Concorde TSP求解器对包含50个兴趣点的TSP实例结果的并排显示。更多信息及完整代码，包括模型训练步骤，可在“Learning
    the Travelling Salesperson Problem Requires Rethinking Generalization” GitHub仓库[21]中找到。
- en: 11.8 Solving TSP using unsupervised machine learning
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.8 使用无监督机器学习解决TSP
- en: As an example of an unsupervised ML approach, listing 11.5 shows how we can
    solve TSP using self-organizing maps (SOMs). We start by importing the libraries
    we’ll use. Some helper functions are imported from the som-tsp implementation
    described in Vicente’s blog post [22] to read the TSP instance, get the neighborhood,
    get the route, select the closest candidate, and calculate the route distance
    and plot the route. We read the TSP instance from the provided URL and obtain
    the cities and normalize their coordinates to a range of [0, 1].
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 作为无监督ML方法的一个例子，列表11.5展示了我们如何使用自组织映射（SOMs）来解决TSP。我们首先导入我们将使用的库。一些辅助函数是从Vicente的博客文章[22]中描述的som-tsp实现导入的，用于读取TSP实例、获取邻域、获取路线、选择最近的候选者以及计算路线距离和绘制路线。我们从提供的URL读取TSP实例，并获取城市并将它们的坐标归一化到[0,
    1]范围内。
- en: Listing 11.5 Solving TSP using unsupervised learning
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.5 使用无监督学习解决TSP
- en: '[PRE18]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Define the URL where the TSP instances are located.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义TSP实例所在的URL。
- en: ② TSP instance
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ② TSP实例
- en: ③ Download the file if it does not exist.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果不存在，则下载文件。
- en: ④ Read the TSP problem.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 读取TSP问题。
- en: ⑤ Obtain the normalized set of cities (with coordinates in [0,1]).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 获取归一化的城市集合（坐标在[0,1]范围内）。
- en: 'We can now set up various parameters and initialize a network of neurons for
    the SOM:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以设置各种参数并初始化SOM的神经元网络：
- en: '[PRE19]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① The population size is 8 times the number of cities.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ① 种群大小是城市数量的8倍。
- en: ② Set the number of iterations.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置迭代次数。
- en: ③ Set the learning rate.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置学习率。
- en: ④ Generate an adequate network of neurons.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 生成足够的神经元网络。
- en: 'As a continuation, the following code snippet implements the training loop
    for SOM. This loop iterates over the specified number of training iterations using
    `tqdm` to show a progress bar:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 作为延续，以下代码片段实现了SOM的训练循环。此循环使用`tqdm`显示进度条，遍历指定的训练迭代次数：
- en: '[PRE20]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Store the lengths of the TSP routes during the SOM training iterations.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在SOM训练迭代过程中存储TSP路线的长度。
- en: ② Store the x and y coordinates of the neurons in the network during the training
    iterations.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在训练迭代过程中存储网络中神经元的x和y坐标。
- en: ③ Training loop
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 训练循环
- en: ④ Print only if the current iteration index is a multiple of 100.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 只有当当前迭代索引是 100 的倍数时才打印。
- en: ⑤ Choose a random city.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 选择一个随机城市。
- en: ⑥ Find the index of the neuron (winner) in the SOM network that is closest to
    the randomly chosen city.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 在 SOM 网络中找到最接近随机选择城市的神经元的索引（获胜者）。
- en: ⑦ Generate a filter that applies changes to the winner’s gaussian.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 生成一个应用于获胜者高斯滤波器的过滤器。
- en: ⑧ Update the network’s weights.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 更新网络的权重。
- en: ⑨ Append the current coordinates to the paths.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 将当前坐标附加到路径上。
- en: ⑩ Decay the learning rate and the neighborhood radius n at each iteration to
    gradually reduce the influence of the Gaussian filter over time.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 在每次迭代中衰减学习率和邻域半径 n，以逐渐减少高斯滤波器随时间对高斯滤波器的影响。
- en: ⑪ Check for the plotting interval.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 检查绘图间隔。
- en: ⑫ Check if any parameter has completely decayed.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 检查是否有任何参数已经完全衰减。
- en: ⑬ Calculate distance, and store it in the route_lengths list.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 计算距离，并将其存储在 route_lengths 列表中。
- en: ⑭ Indicate that the specified number of training iterations has been completed.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 指示已完成的指定数量的训练迭代次数。
- en: The following code snippet plots the route length in each iteration.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段绘制了每次迭代的路线长度。
- en: '[PRE21]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Figure 11.18 shows the route length per iteration. The final route length is
    9,816, and the optimal length for the Qatar TSP instance used, `qa194.tsp`, is
    9,352\.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18 展示了每次迭代的路线长度。最终路线长度为 9,816，卡塔尔 TSP 实例使用的最优长度为 `qa194.tsp`，为 9,352。
- en: '![](../Images/CH11_F18_Khamis.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F18_Khamis.png)'
- en: Figure 11.18 Route length per iteration of SOM for the Qatar TSP. The final
    route length is 9,816, and the optimal solution is 9,352.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18 SOM 对卡塔尔 TSP 的迭代路线长度。最终路线长度为 9,816，最优解为 9,352。
- en: The complete version of listing 11.5 is available in the book’s GitHub repo,
    and it contains an implementation based on MiniSom. MiniSom is a minimalistic
    and Numpy-based implementation of SOM. You can install this library using `!pip
    install minisom`. However, the route obtained by MiniSom is 11,844.47, which is
    far from the optimal length of 9,352 for this TSP instance. To improve the result,
    you can experiment with the provided code and try to tune SOM parameters such
    as the number of neurons, the sigma, the learning rate, and the number of iterations.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.5 的完整版本可在本书的 GitHub 仓库中找到，它包含基于 MiniSom 的实现。MiniSom 是 SOM 的最小化、基于 Numpy
    的实现。您可以使用 `!pip install minisom` 安装此库。然而，MiniSom 获得的路线为 11,844.47，这远远低于此 TSP 实例的最优长度
    9,352。为了提高结果，您可以尝试提供的代码并尝试调整 SOM 参数，如神经元数量、sigma、学习率和迭代次数。
- en: 11.9 Finding a convex hull
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.9 寻找凸包
- en: 'Ptr-Net can be used to tackle the convex hull problem using a supervised learning
    approach, as described by Vinyals and his co-authors in their “Pointer networks”
    article [10]. Ptr-Net has two key components: an encoder and a decoder, as illustrated
    in figure 11.19\.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: Ptr-Net 可以使用监督学习方法来解决凸包问题，如 Vinyals 及其合著者在他们的“指针网络”文章 [10] 中所述。Ptr-Net 有两个关键组件：一个编码器和一个解码器，如图
    11.19 所示。
- en: '![](../Images/CH11_F19_Khamis.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F19_Khamis.png)'
- en: Figure 11.19 Solving the convex hull problem using Ptr-Net. The output in each
    step is a pointer to the input that maximizes the probability distribution.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.19 使用 Ptr-Net 解决凸包问题。每一步的输出是指向最大化概率分布的输入的指针。
- en: The encoder, a recurrent neural network (RNN), converts the raw input sequence.
    In this case, it coordinates delineating the points for which we want to determine
    the convex hull into a more manageable representation.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器，一个循环神经网络（RNN），将原始输入序列转换为。在这种情况下，它将我们想要确定凸包的点协调到更易于管理的表示。
- en: This encoded vector is then passed on to the decoder. The vector acts as the
    modulator for a content-based attention mechanism, which is applied over the inputs.
    The content-based attention mechanism can be likened to a spotlight that highlights
    different segments of the input data at varying times, focusing on the most pertinent
    parts of the task at hand.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 此编码向量随后传递给解码器。该向量作为基于内容注意力机制的内容调节器，该机制应用于输入。基于内容的注意力机制可以比作一个聚光灯，在不同时间突出显示输入数据的各个部分，专注于手头任务的最相关部分。
- en: The output of this attention mechanism is a softmax distribution with a dictionary
    size equal to the length of the input. This softmax distribution gives probabilities
    to every point in the input sequence. This setup allows Ptr-Net to probabilistically
    decide at each step which point should be added next to the convex hull. This
    is determined based on the current state of the input and the network’s internal
    state. The training process is repeated until the network has made a decision
    for every point, yielding a complete resolution to the convex hull problem.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 shows the steps for solving convex hull problem using pointer networks.
    We start by importing several necessary libraries and modules, such as torch,
    numpy, and matplotlib. The three helper classes `Data`, `ptr_net`, and `Disp`
    are imported based on the implementations provided in McGough’s “Pointer Networks
    with Transformers” article [23]. They contain functions for generating training
    and validation data, defining the pointer network architecture, and visualizing
    the results. This code generates two datasets for training and validation respectively.
    These datasets consist of random 2D points, where the number of points in each
    sample (the convex hull problem’s input) varies between `min_samples` and `max_samples`.
    `Scatter2DDataset` is a custom dataset class used to generate these random 2D
    point datasets.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 Solving a convex hull problem using pointer networks
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Running this code generates 100,000 training points and 1,000 validation points.
    We can then set the parameters of the pointer network. These parameters include
    a `TOKENS` dictionary containing the following tokens:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '`<eos>`—End-of-sequence token with the index 0'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c_inputs`—Number of input features for the model'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c_embed`—Number of embedding dimensions'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c_hidden`—Number of hidden units in the model'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_heads`—Number of attention heads in the multi-head self-attention mechanism'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_layers`—Number of layers in the model'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout`—Dropout probability, which is used for regularization'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cuda`—A Boolean flag indicating whether to use CUDA (GPU) if available
    or CPU'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_workers`—Number of worker threads for data loading in DataLoader'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The training parameters include `n_epochs` (number of training epochs), `batch_size`
    (batch size used during training), `lr` (learning rate for the optimizer), and
    `log_interval` (interval for logging training progress). The code checks if CUDA
    (GPU) is available and sets the `device` variable accordingly:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As a continuation, we load the training and validation data with the specified
    `batch_size` and `num_workers`:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `ConvexNet` model is a Ptr-Net model that is implemented as a transformer
    architecture with an encoder and decoder that use `nn.TransformerEncoderLayer`
    and apply multi-head self-attention. The complete code is available in the `ptr_net.py`
    class, available in the book’s GitHub repo. This model is initialized with the
    predefined hyperparameters. The `AverageMeter` class is used for keeping track
    of the average loss and accuracy during training and validation:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① Create a ConvexNet model.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: ② Use the Adam optimizer for training the model.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: ③ Use negative log-likelihood loss as the loss function.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: ④ Keep track of the average loss and accuracy during training and validation.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now perform the training and evaluation loop for a model (`ConvexNet`)
    using PyTorch. The model is being trained on the `train_loader` dataset with known
    labels and evaluated on the `val_loader` dataset:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ① Train the model.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterate over batches of training data.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: ③ Set the model’s parameters’ gradients to zero to avoid accumulation from previous
    batches.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: ④ Calculate the loss
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ A safeguard check to ensure that the loss value during the training process
    is not a NaN.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Perform a backward pass and optimization step.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Update training loss and accuracy.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Print the training progress.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation, the trained model (model) is evaluated on a validation dataset
    (val_dataset) to calculate the validation loss, accuracy, and overlap between
    the convex hull of the input data and the predicted pointer sequences. We start
    by setting the model to evaluation mode, where the model’s parameters are frozen
    and the batch normalization or dropout layers behave differently than during training.
    The code then iterates through the validation dataset using the val_loader, which
    provides batches of data (batch_data), ground truth labels (batch_labels), and
    the lengths of each sequence (batch_lengths):'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ① Set the model to evaluation mode.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: ② Initialize an empty list to store the overlap values between the convex hull
    of the input data and the predicted pointer sequences.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: ③ Iterate through the validation dataset.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: ④ Produce pointer scores and argmax predictions.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Calculate the validation loss.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Update the validation loss.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Ignore the loss contribution from positions where the <eos> token is present
    in batch_labels.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Calculate the masked accuracy.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Update the validation accuracy.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Iterate through each batch’s data, lengths, and pointer argmax predictions.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: ⑪ Calculate the overlap between the convex hull of the input data and the predicted
    pointer sequences.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: ⑫ Print the epoch-wise validation loss, accuracy, and mean overlap.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: ⑬ Reset the metrics.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'You can display the results of training and validation losses and accuracies
    using the `Disp_results` helper function:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding line of code will generate output like the following:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After model training and validation, we can test the model. The following test
    function will evaluate a trained model (`model`) on a test dataset. The function
    evaluates the model’s accuracy and overlap with the convex hull for different
    test sample sizes. This test function takes as inputs the model, the number of
    test samples, and the number of points per sample. The code performs the test
    for different numbers of points per sample (`i`) by iterating from 5 to 45 in
    steps of 5\. The `AverageMeter` class is used to keep track of average loss and
    accuracy during testing:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ① Set the number of test samples to be generated for each test.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: ② Test function
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: ③ Generate the test dataset.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: ④ Iterate through the batches of test data.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Track the loss and accuracy.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Update the overlap between the convex hull and predicted pointer sequences.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Print the accuracy and overlap.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Iterate and print results for different sample sizes.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'This code will produce output like the following:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s now test the trained model and see how well this model generalizes to
    new unseen data. We’ll use a dataset with 50 points to test the trained and validated
    model and calculate the convex hull overlap between the predicted hull and the
    ground truth hull obtained by SciPy. We pass the batch of input data and its lengths
    through the model to obtain the predicted scores (`log_pointer_scores`) and the
    argmax indices (`pointer_argmaxs`) of the pointer network. The ground truth is
    the convex hull obtained using the `ConvexHull` function from `scipy.spatial`:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① Set the number of points in each sample.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: ② Create a test dataset.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: ③ Load the first batch of data from the test dataset.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: ④ Obtain the predicted scores and the argmax indices of the pointer network.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Extract the predicted argmax indices for the selected sample from the batch.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Filter out the special tokens (e.g., <eos>) and adjust the indices for indexing
    the points correctly.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Extract and print the 2D points for the selected sample from the batch.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Ground truth convex hull
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Calculate the convex hull overlap.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Print the list of predicted convex hull indices, convex hull indices, and
    overlap percentage.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code will produce output like the following. You can run the preceding
    code snippets multiple times to get a high percentage of hull overlap:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following code snippet can be used to visualize the convex hull generated
    by the pointer network (`ConvexNet`) in comparison with the convex hull generated
    by `scipy.spatial` as a ground truth:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Set the default figure size, and create the first subplot.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: ② Compute the convex hull of a set of points (points) using the ConvexHull function
    from scipy.spatial.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: ③ Display the points and their convex hull in the first subplot.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: ④ Create a second subplot.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Display the points and the convex hull generated by ConvexNet.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.20 shows the convex hulls generated by SciPy and ConvexNet. These
    convex hulls are identical in some instances (i.e., hull overlap = 100.00%), yet
    achieving this consistency requires proper training and careful tuning of the
    ConvexNet parameters.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20显示了SciPy和ConvexNet生成的凸包。在某些情况下，这些凸包是相同的（即，凸包重叠=100.00%），但要实现这种一致性需要适当的训练和仔细调整ConvexNet参数。
- en: '![](../Images/CH11_F20_Khamis.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH11_F20_Khamis.png)'
- en: Figure 11.20 Convex hulls generated by SciPy and Ptr-Net for 50 points
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20 SciPy和Ptr-Net为50个点生成的凸包
- en: This chapter has offered a fundamental foundation in ML and discussed the applications
    of supervised and unsupervised ML in handling optimization problems. The next
    chapter will focus on reinforcement learning and will delve deeply into its practical
    applications in tackling optimization problems.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 本章在机器学习方面提供了一个基本的基础，并讨论了监督和无监督机器学习在处理优化问题中的应用。下一章将重点介绍强化学习，并深入探讨其在解决优化问题中的实际应用。
- en: Summary
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Machine learning (ML), a branch of artificial intelligence (AI), grants an artificial
    system or process the capacity to learn from experiences and observations, rather
    than through explicit programming.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习（ML）是人工智能（AI）的一个分支，它赋予人工系统或过程从经验和观察中学习的能力，而不是通过明确的编程。
- en: Deep learning (DL) is a subset of ML that is focused on the detection of inherent
    features within data by employing deep neural networks. This allows artificial
    systems to form intricate concepts from simpler ones.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习的一个子集，它通过使用深度神经网络来检测数据中的固有特征。这使得人工系统能够从更简单的概念中形成复杂的概念。
- en: Geometric deep learning (GDL) extends (structured) deep neural models to handle
    non-Euclidean data with underlying geometric structures, such as graphs, point
    clouds, and manifolds.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几何深度学习（GDL）扩展（结构化）深度神经网络以处理具有潜在几何结构的非欧几里得数据，例如图、点云和流形。
- en: Graph machine learning (GML) is a subfield of ML that focuses on developing
    algorithms and models capable of learning from graph-structured data.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图机器学习（GML）是机器学习的一个子领域，专注于开发能够从图结构数据中学习的算法和模型。
- en: Graph embedding represents the process of creating a conversion from the discrete,
    high-dimensional graph domain to a lower-dimensional continuous domain.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图嵌入表示将离散、高维图域转换为低维连续域的过程。
- en: The attention mechanism allows a model to selectively focus on certain portions
    of the input data while it is in the process of generating the output sequence.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制允许模型在生成输出序列的过程中有选择性地关注输入数据的一定部分。
- en: The pointer network (Ptr-Net) is a variation of the sequence-to-sequence model
    with attention designed to deal with variable-sized input data sequences.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指针网络（Ptr-Net）是一种具有注意力机制的序列到序列模型变体，旨在处理可变大小的输入数据序列。
- en: A self-organizing map (SOM), also known as a Kohonen map, is a type of artificial
    neural network (ANN) used for unsupervised learning. SOMs differ from other types
    of ANNs, as they apply competitive learning rather than error-correction learning
    (such as backpropagation with gradient descent).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自组织图（SOM），也称为Kohonen图，是一种用于无监督学习的人工神经网络（ANN）。SOM与其他类型的ANN不同，因为它们应用竞争学习而不是错误纠正学习（如梯度下降的反向传播）。
- en: Neural combinatorial optimization refers to the application of ML to solve combinatorial
    optimization problems.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经组合优化是指将机器学习应用于解决组合优化问题。
- en: 'Harnessing ML for combinatorial optimization can be achieved through three
    main methods: end-to-end learning where the model directly formulates solutions,
    using ML to configure and improve optimization algorithms, and integrating ML
    with optimization algorithms where the model continuously guides the optimization
    algorithm based on its current state.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用机器学习（ML）进行组合优化可以通过三种主要方法实现：端到端学习，其中模型直接制定解决方案；使用机器学习来配置和改进优化算法；以及将机器学习与优化算法集成，其中模型根据其当前状态持续引导优化算法。
