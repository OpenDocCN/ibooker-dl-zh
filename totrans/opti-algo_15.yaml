- en: 11 Supervised and unsupervised learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 监督学习和无监督学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Reviewing the basics of artificial intelligence, machine learning, and deep
    learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾人工智能、机器学习和深度学习的基础知识
- en: Understanding graph machine learning, graph embedding, and graph convolutional
    networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解图机器学习、图嵌入和图卷积网络
- en: Understanding attention mechanisms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解注意力机制
- en: Understanding self-organizing maps
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自组织映射
- en: Solving optimization problems using supervised and unsupervised machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用监督学习和无监督机器学习解决优化问题
- en: Artificial intelligence (AI) is one of the fastest growing fields of technology,
    driven by advancements in computing power, access to vast amounts of data, breakthroughs
    in algorithms, and increased investment from both public and private sectors.
    AI aims to create intelligent systems or machines that can exhibit intelligent
    behavior, often by mimicking or drawing inspiration from biological intelligence.
    These systems can be designed to function autonomously or with some human guidance,
    and ideally, they can adapt to environments with diverse structures, observability
    levels, and dynamics. AI augments our intelligence by empowering us to analyze
    vast amounts of multidimensional, multimodal data and identify hidden patterns
    that would be difficult for humans to recognize. AI also supports our learning
    and decision-making by providing relevant insights and potential courses of action.
    AI encompasses various subfields, such as situation awareness (comprising perception,
    comprehension, and projection), knowledge representation, cognitive reasoning,
    machine learning, data analytics (covering descriptive, diagnostic, predictive,
    and prescriptive analytics), problem solving (involving constraint satisfaction
    and problem-solving using search and optimization), as well as digital and physical
    automation (such as conversational AI and robotics).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）是技术领域增长最快的领域之一，由计算能力的提升、大量数据的获取、算法的突破以及公共和私营部门的增加投资所驱动。AI旨在创建能够表现出智能行为或机器，通常是通过模仿或从生物智能中汲取灵感。这些系统可以设计为自主运行或在一定程度上接受人类指导，理想情况下，它们可以适应具有不同结构、可观察性和动态的环境。AI通过赋予我们分析大量多维、多模态数据的能力并识别人类难以识别的隐藏模式来增强我们的智能。AI还通过提供相关见解和潜在的行动方案来支持我们的学习和决策。AI包括各种子领域，如态势感知（包括感知、理解和预测）、知识表示、认知推理、机器学习、数据分析（包括描述性、诊断性、预测性和规范性分析）、问题解决（涉及约束满足和搜索与优化）、以及数字和物理自动化（如对话式AI和机器人）。
- en: 'In this last part of the book, we will explore the convergence of two branches
    of AI: machine learning and optimization. Our focus will be on showcasing the
    practical applications of machine learning in tackling optimization problems.
    This chapter provides an overview of machine learning fundamentals as essential
    background knowledge, and then it delves into applications of supervised and unsupervised
    machine learning in handling optimization problems. Reinforcement learning will
    be covered in the next chapter.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一部分，我们将探讨人工智能两个分支的融合：机器学习和优化。我们的重点是展示机器学习在解决优化问题中的实际应用。本章提供了机器学习基础知识的概述，作为必要的背景知识，然后深入探讨了监督学习和无监督机器学习在处理优化问题中的应用。强化学习将在下一章中介绍。
- en: 11.1 A day in the life of AI-empowered daily routines
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 智能赋能日常生活的每一天
- en: AI, and machine learning in particular, forms the foundation of many successful
    disruptive industries and has successfully delivered many commercial products
    that touch everybody’s life every day. Starting at home, voice assistants eagerly
    await your commands, effortlessly controlling smart appliances and adjusting the
    smart thermostat to ensure comfort and convenience. Smart meters intelligently
    manage energy consumption, optimizing efficiency and reducing costs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AI，尤其是机器学习，构成了许多成功颠覆性产业的基础，并成功推出了许多触及每个人日常生活的商业产品。从家庭开始，语音助手热切地等待您的命令，轻松控制智能设备并调整智能恒温器以确保舒适和便利。智能电表智能管理能源消耗，优化效率并降低成本。
- en: On the route to school or work, navigation apps with location intelligence guide
    the way, considering real-time traffic updates to provide the fastest and most
    efficient route. Shared mobility services offer flexible transportation options
    on demand, while advanced driver assistance systems enhance safety and convenience
    if you decide to drive. In the not-too-distant future, we will enjoy safe and
    entertaining self-driving vehicles as a third living space, after our homes and
    workplaces, with consumer-centric products and services.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上学或上班的路上，具有位置智能的导航应用指引路线，考虑实时交通更新以提供最快和最有效的路线。共享出行服务提供按需的灵活交通选项，而先进的驾驶辅助系统在您决定驾驶时增强安全性和便利性。在不远的将来，我们将享受安全有趣的自动驾驶汽车作为第三个生活空间，在家庭和工作场所之后，拥有以消费者为中心的产品和服务。
- en: Once at school or at the workplace, AI becomes an invaluable tool for personalization
    and to boost productivity. Personalized learning platforms cater to individual
    needs, adapting teaching methods and content to maximize understanding and retention.
    Summarization and grammar-checking algorithms aid in crafting flawless documents,
    while translation tools bridge language barriers effortlessly. Excel AI formula
    generators streamline complex calculations, saving time and effort. Human-like
    text generation enables natural and coherent writing, while audio, image, and
    video generation from text unlock creative possibilities. Optimization algorithms
    ensure optimal resource allocation and scheduling, maximizing efficiency in various
    scenarios, and handle different design, planning, and control problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在学校或工作场所，人工智能成为个性化提升生产力的无价工具。个性化的学习平台满足个人需求，调整教学方法和内容以最大化理解和记忆。摘要和语法检查算法有助于制作无瑕疵的文档，而翻译工具则轻松跨越语言障碍。Excel人工智能公式生成器简化了复杂的计算，节省了时间和精力。类似人类的文本生成使写作自然流畅，而从文本生成音频、图像和视频则开启了创意的可能性。优化算法确保资源分配和调度最优化，在各种场景中最大化效率，并处理不同的设计、规划和控制问题。
- en: During shopping, AI enhances the experience in numerous ways. Voice search enables
    hands-free exploration, while searching by images allows for effortless discovery
    of desired items. Semantic search understands context and intent, providing more
    accurate results. Recommendation engines offer personalized suggestions based
    on individual preferences and online shopping behavior, while last-mile or door-to-door
    delivery services ensure timely, transparent, and convenient package arrival.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在购物过程中，人工智能以多种方式提升了购物体验。语音搜索实现了免提探索，而通过图像搜索则可以轻松发现所需商品。语义搜索理解上下文和意图，提供更准确的结果。推荐引擎根据个人偏好和在线购物行为提供个性化建议，而最后一英里或门到门的配送服务确保及时、透明和便利的包裹送达。
- en: In the realm of health, AI revolutionizes personalized healthcare, assisting
    with diagnosis, treatment planning, and rehabilitation. Lab automation speeds
    up testing processes, improving accuracy and efficiency. AI-driven drug discovery
    and delivery enable the development of innovative treatments and targeted therapies,
    transforming lives.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在健康领域，人工智能革命性地改变了个性化医疗保健，协助诊断、治疗规划和康复。实验室自动化加速了测试流程，提高了准确性和效率。人工智能驱动的药物发现和递送使创新治疗和靶向疗法的开发成为可能，改变人们的生活。
- en: During leisure time, AI contributes to physical and mental well-being. Fitness
    planning apps tailor workout routines to individual goals and capabilities, providing
    personalized guidance and motivation. Trip planning tools recommend exciting destinations
    and itineraries, ensuring memorable experiences. AI-powered meditation apps offer
    customized relaxation experiences, soothing the mind and promoting mindfulness.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在休闲时间，人工智能有助于身心健康。健身计划应用根据个人目标和能力定制锻炼计划，提供个性化的指导和动力。行程规划工具推荐令人兴奋的目的地和行程，确保难忘的体验。人工智能驱动的冥想应用提供定制的放松体验，舒缓心灵并促进正念。
- en: Machine learning, a prominent subfield of artificial intelligence, has played
    a pivotal role in bringing AI from the confines of high-tech research labs to
    the convenience of our daily lives.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，作为人工智能的一个突出子领域，在将人工智能从高科技研究实验室的局限带到我们日常生活的便利中发挥了关键作用。
- en: 11.2 Demystifying machine learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 揭秘机器学习
- en: 'The goal of learning is to create an internal model or abstraction of the external
    world. More comprehensively, Stanislas Dehaene, in *How We Learn* [1], introduced
    seven key definitions of learning that lie at the heart of present-day machine
    learning algorithms:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的目的是创建外部世界的内部模型或抽象。更全面地说，Stanislas Dehaene 在 *《我们如何学习》* [1] 中介绍了学习的关键定义，这些定义是当今机器学习算法的核心：
- en: Learning is adjusting the parameters of a mental model.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是调整心理模型的参数。
- en: Learning is exploring a combinatorial explosion.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是探索组合爆炸。
- en: Learning is minimizing errors.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是最小化错误。
- en: Learning is exploring the space of possibilities.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是探索可能性空间。
- en: Learning is optimizing a reward function.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是优化奖励函数。
- en: Learning is restricting search space.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是限制搜索空间。
- en: Learning is projecting a priori hypotheses.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是投射先验假设。
- en: 'Machine learning (ML) is a subfield of AI that endows an artificial system
    or process with the ability to learn from experience and observation without being
    explicitly programmed. Thomas Mitchell, in *Machine Learning*, defines ML as follows:
    “A computer program is said to learn from experience *E* with respect to some
    class of tasks *T* and performance measure *P*, if its performance at tasks in
    *T*, as measured by *P*, improves with experience *E*” [2]. In his book *The Master
    Algorithm*, Pedro Domingos summarizes the ML schools of thought into five main
    schools [3], illustrated in figure 11.1:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是人工智能的一个子领域，它赋予人工系统或过程从经验和观察中学习的能力，而不需要明确编程。Thomas Mitchell 在 *《机器学习》*
    中将 ML 定义如下：“如果一个计算机程序在任务 *T* 中，根据性能度量 *P*，从经验 *E* 中学习，那么它的性能随着经验 *E* 的提高而提高” [2]。在他的书
    *《大师算法》* 中，Pedro Domingos 将机器学习学派总结为五个主要学派 [3]，如图11.1所示：
- en: Bayesians with probabilistic inference as the master algorithm
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以概率推理作为主算法的贝叶斯主义者
- en: Symbolists with rules and trees as the main core algorithm within this paradigm
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以规则和树作为该范式主要核心算法的符号主义者
- en: Connectionists who use neural networks with backpropagation as a master algorithm
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反向传播作为主算法的神经网络连接主义者
- en: Evolutionaries who rely on the evolutionary computing paradigm
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于进化计算范式的进化论者
- en: Analogizers who use mathematical techniques like support vector machines with
    different kernels
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同核的支持向量机等数学技术的类比主义者
- en: '![](../Images/CH11_F01_Khamis.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F01_Khamis.png)'
- en: Figure 11.1 Different ML schools of thought according to Domingos’ *The Master
    Algorithm*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 根据 Domingos 的 *《大师算法》* 列出的不同机器学习学派
- en: Nowadays, connectionist learning approaches have attracted most of the attention,
    thanks to their perception and learning capabilities in several challenging domains.
    These statistical ML algorithms follow a bottom-up inductive reasoning paradigm
    (i.e., inferring general rules from a set of examples) to discover patterns from
    vast amounts of data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，连接主义学习方法因其几个具有挑战性的领域的感知和学习能力而吸引了大部分关注。这些统计机器学习算法遵循自下而上的归纳推理范式（即从一组示例中推断一般规则）来从大量数据中发现模式。
- en: The unreasonable effectiveness of data
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的不合理有效性
- en: Simple models and a lot of data trump more elaborate models based on less data
    [4]. This means that having a large amount of data to train simple models is often
    more effective than using complex models with only a small amount of data. For
    example, in self-driving vehicles, a simple model that has been trained on millions
    of hours of driving data can often be more effective in recognizing and reacting
    to diverse road situations than a more complex model trained on a smaller dataset.
    This is because the massive amount of data helps the simple model learn a wide
    range of patterns and scenarios, including adversarial and edge cases it might
    encounter, making it more adaptable and reliable in real-world driving conditions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简单模型和大量数据胜过基于较少数据的更复杂模型 [4]。这意味着拥有大量数据来训练简单模型通常比使用只有少量数据的复杂模型更有效。例如，在自动驾驶汽车中，一个在数百万小时驾驶数据上训练过的简单模型，在识别和反应各种道路情况时，通常比在更小数据集上训练的更复杂模型更有效。这是因为大量数据帮助简单模型学习广泛的各种模式和场景，包括它可能遇到的对抗性和边缘情况，使其在现实世界的驾驶条件下更具适应性和可靠性。
- en: These connectionist learning or statistical ML approaches are based on the experimental
    findings that even very complex problems in artificial intelligence may be solved
    by simple statistical models trained on massive datasets [4]. Statistical ML is
    currently the most famous form of AI. The rapid advancement of this form of ML
    can be attributed primarily to the widespread availability of big data and open
    source tools, enhanced computational power such as AI accelerators, and substantial
    research and development funding from both public and private sectors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基于连接主义学习或统计机器学习的方法基于实验发现，即使是非常复杂的人工智能问题也可能通过在大量数据集上训练的简单统计模型来解决[4]。统计机器学习是目前最著名的AI形式。这种形式机器学习的快速进步主要归因于大数据的广泛应用、开源工具的普及、增强的计算能力，如AI加速器，以及公共和私营部门的大量研发资金。
- en: Generally speaking, ML algorithms can be categorized into supervised, unsupervised,
    hybrid learning, and reinforcement learning algorithms, as illustrated in figure
    11.2.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，机器学习算法可以分为监督学习、无监督学习、混合学习和强化学习算法，如图11.2所示。
- en: '![](../Images/CH11_F02_Khamis.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F02_Khamis.png)'
- en: Figure 11.2 ML taxonomy as a subfield of AI
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 机器学习作为人工智能子领域的分类
- en: '*Supervised learning*—This approach uses inductive inference to approximate
    mapping functions between data and known labels or classes. This mapping is learned
    using already labeled training data. *Classification* (predicting discrete or
    categorical values) and *regression* (predicting continuous values) are common
    tasks in supervised learning. For example, classification seeks a scoring function
    *f*:*Χ*×*C*⟶*R*, where *X* represents the training data space and *C* represents
    the label or class space. This mapping can be learned using *N* training examples
    of the form {(*x*[11], *x*[21], …, *x[m]*[1], *c*[1]), (*x*[12], *x*[22], …, *x[m]*[2],
    *c*[2]), …, (*x*[1]*[N], x*[2]*[N]*, …, *x[mN], c[N]*)}, where *x[i]* is the feature
    vector of the *i*-th example, *m* is number of features, and *c[i]* is the corresponding
    class. The predicted class is the class that gives the highest score of *f*, i.e.,
    *c*(*x*) = argmax*[c]f*(*x*,*c*). In the context of self-driving vehicles, supervised
    learning might be used to train a model to recognize traffic signs. The input
    data would be images of various traffic signs, and the correct output (the labels)
    would be the type of each sign. The trained model could then identify traffic
    signs correctly when driving. Feedforward neural networks (FNNs) or multilayer
    perceptrons (MLPs), convolutional neural networks (CNNs), recurrent neural networks
    (RNNs), long short-term memory (LSTM) networks, and sequence-to-sequence (Seq2Seq)
    models are examples of common neural network architectures that are typically
    trained using supervised learning. Examples of solving combinatorial problems
    using supervised ML are provided in sections 11.6, 11.7, and 11.9.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监督学习*——这种方法使用归纳推理来近似数据与已知标签或类别之间的映射函数。这种映射是通过使用已经标记的训练数据来学习的。在监督学习中，*分类*（预测离散或分类值）和*回归*（预测连续值）是常见的任务。例如，分类寻求一个评分函数
    *f*:*Χ*×*C*⟶*R*，其中 *X* 代表训练数据空间，*C* 代表标签或类别空间。这种映射可以使用 *N* 个训练示例来学习，形式为 {(*x*[11],
    *x*[21], …, *x[m]*[1], *c*[1]), (*x*[12], *x*[22], …, *x[m]*[2], *c*[2]), …, (*x*[1]*[N],
    x*[2]*[N]*, …, *x[mN], c[N]*)}，其中 *x[i]* 是第 *i* 个示例的特征向量，*m* 是特征数量，*c[i]* 是相应的类别。预测的类别是给出最高评分的类别，即
    *c*(*x*) = argmax*[c]f*(*x*,*c*)。在自动驾驶汽车的情况下，监督学习可能被用来训练一个模型来识别交通标志。输入数据将是各种交通标志的图像，正确的输出（标签）将是每个标志的类型。训练好的模型随后可以在驾驶时正确识别交通标志。前馈神经网络（FNNs）或多层感知器（MLPs）、卷积神经网络（CNNs）、循环神经网络（RNNs）、长短期记忆（LSTM）网络和序列到序列（Seq2Seq）模型是通常使用监督学习训练的常见神经网络架构的例子。使用监督机器学习解决组合问题的例子在11.6、11.7和11.9节中提供。'
- en: '*Unsupervised learning*—This approach deals with unlabeled data through techniques
    like *clustering* and *dimensionality reduction*. In clustering, for example,
    *n* objects (each could be a vector of *d* features) are given, and the task is
    to group them based on certain similarity measures into *c* groups (clusters)
    in such a way that all objects in a single group have a “natural” relation to
    one another, and objects not in the same group are somehow different. For instance,
    unsupervised learning might be used in self-driving vehicles to cluster similar
    driving scenarios or environments. Using unsupervised learning, the car might
    learn to identify different types of intersections or roundabouts, even if no
    one has explicitly labeled the data with these categories. Autoencoders, k-means,
    density-based spatial clustering (DBSCAN), principal component analysis (PCA),
    and self- organizing maps (SOMs) are examples of unsupervised learning methods.
    SOM is explained in section 11.4\. An example of a combinatorial problem using
    SOM is provided in section 11.8\.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无监督学习*—这种方法通过诸如*聚类*和*降维*等技术处理未标记数据。例如，在聚类中，给出*n*个对象（每个对象可能是一个*d*特征的向量），任务是根据某些相似性度量将它们分组到*c*个组（簇）中，使得单个组中的所有对象彼此之间有“自然”的关系，而不同组中的对象在某种程度上是不同的。例如，无监督学习可能用于自动驾驶车辆中聚类相似的驾驶场景或环境。使用无监督学习，汽车可能学会识别不同类型的交叉口或环岛，即使没有人明确地将这些类别标记为数据。自编码器、k-means、基于密度的空间聚类（DBSCAN）、主成分分析（PCA）和自组织映射（SOMs）是未监督学习方法的例子。SOM在11.4节中解释。11.8节提供了一个使用SOM的组合问题的例子。'
- en: '*Hybrid learning*—This approach includes *semi-supervised learning and self-supervised
    learning* techniques. Semi-supervised learning is a mix of supervised and unsupervised
    learning where only a fraction of the input data is labeled with corresponding
    outputs. In this case, the training process uses the small amount of labeled data
    available and pseudo-labels the rest of the dataset—for example, training a self-driving
    vehicle''s perception system with a limited set of labeled driving scenarios,
    then using a vast collection of unlabeled driving data to improve its ability
    to recognize and respond to various road conditions and obstacles. Self-supervised
    learning is an ML process where a model learns meaningful representations of the
    input data by using the inherent structure or relationships within the data itself.
    This is achieved by creating supervised learning tasks from the unlabeled data.
    For instance, a self-supervised model might be trained to predict the next word
    in a sentence based on the previous words or to reconstruct an image from a scrambled
    version. These learned representations can then be used for various downstream
    tasks, such as image classification or object detection. In the context of self-driving
    vehicles, a perception system can be trained to identify essential features in
    unlabeled driving scenes, such as lane markings, pedestrians, and other vehicles.
    Then, the learned features are utilized as pseudo-labels to classify new driving
    scenes in a supervised manner, enabling the vehicle to make decisions based on
    its understanding of the road environment.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合学习*—这种方法包括*半监督学习和自监督学习*技术。半监督学习是监督学习和无监督学习相结合的方法，其中只有一小部分输入数据被标记为相应的输出。在这种情况下，训练过程使用可用的少量标记数据，并对数据集的其余部分进行伪标记——例如，使用有限的标记驾驶场景集训练自动驾驶车辆的感知系统，然后使用大量的未标记驾驶数据来提高其识别和响应各种道路状况和障碍物的能力。自监督学习是一种机器学习过程，模型通过使用数据本身固有的结构或关系来学习输入数据的有效表示。这是通过从未标记数据中创建监督学习任务来实现的。例如，一个自监督模型可能被训练来根据前面的单词预测句子中的下一个单词，或者从打乱版本的图像中重建图像。这些学习到的表示可以用于各种下游任务，例如图像分类或目标检测。在自动驾驶车辆的情况下，感知系统可以被训练来识别未标记驾驶场景中的关键特征，例如车道标记、行人和其他车辆。然后，学习到的特征被用作伪标签，以监督方式对新的驾驶场景进行分类，使车辆能够根据其对道路环境的理解做出决策。'
- en: '*Reinforcement learning (RL)*—This approach learns from interactions through
    a feedback loop or by trial and error. A learning agent learns to make decisions
    by taking actions in an environment to maximize some notion of cumulative reward.
    For self-driving vehicles, reinforcement learning could be used in the decision-making
    process. For instance, the car might learn over time the best way to merge into
    traffic on a busy highway. It would receive positive rewards for successful merges
    and negative rewards for dangerous maneuvers or failed attempts. Over time, through
    trial and error and the desire to maximize the reward, the car would learn an
    optimal policy for merging into traffic. More details about RL are provided in
    the next chapter.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强化学习（RL）*——这种方法通过反馈循环或通过试错来学习。学习代理通过在环境中采取行动以最大化某种累积奖励的概念来学习做出决策。对于自动驾驶汽车，强化学习可以在决策过程中使用。例如，汽车可能随着时间的推移学会在繁忙的高速公路上并入交通的最佳方式。它将因成功的并入而获得正面奖励，因危险的操作或失败的尝试而获得负面奖励。随着时间的推移，通过试错和最大化奖励的愿望，汽车将学会并入交通的最佳策略。关于强化学习的更多细节将在下一章提供。'
- en: Deep learning (DL) is a subfield of ML concerned with learning underlying features
    in data using neural networks with many layers (hence “deep”) enabling artificial
    systems to build complex concepts out of simpler concepts. DL enables learning
    discriminative features or representations and learning at different levels of
    abstraction. To achieve this, the network uses hierarchical feature learning and
    employs a handful of convolutional layers. DL revolutionizes the field of ML by
    reducing the need for extensive data preprocessing. DL models can automatically
    extract highly discriminative features from raw data, eliminating the need for
    hand-crafted feature engineering. This end-to-end learning process significantly
    reduces the reliance on human experts, as the model learns to extract meaningful
    representations and patterns directly from the input data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习（ML）的一个子领域，它使用具有多层（因此称为“深度”）的神经网络来学习数据中的底层特征，使人工系统能够从更简单的概念中构建复杂的概念。深度学习能够学习具有判别性的特征或表示，并在不同抽象级别上进行学习。为了实现这一点，网络使用层次特征学习和采用少量卷积层。深度学习通过减少对大量数据预处理的需求，革新了机器学习领域。深度学习模型可以从原始数据中自动提取高度判别性的特征，从而消除了手动特征工程的需求。这种端到端的学习过程显著减少了对人专家的依赖，因为模型能够直接从输入数据中提取有意义的表示和模式。
- en: Unlike traditional ML algorithms, DL models have the ability to directly consume
    and process various forms of structured and unstructured data, such as text, audio,
    images, video, and even graphs. Graph-structured data is particularly important
    in the field of combinatorial optimization due to its ability to capture and represent
    the relationships and constraints between elements in optimization problems. Geometric
    DL is a subfield of ML that combines graph theory with DL.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习算法不同，深度学习模型能够直接消费和处理各种形式的结构化和非结构化数据，如文本、音频、图像、视频，甚至图。图结构数据在组合优化领域尤为重要，因为它能够捕捉和表示优化问题中元素之间的关系和约束。几何深度学习是机器学习的一个子领域，它将图论与深度学习相结合。
- en: The following two sections address graph machine learning and self-organizing
    maps in more detail. They are essential background knowledge to the use cases
    described later in this chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两节更详细地介绍了图机器学习和自组织映射。它们是本章后面描述的使用案例的必要背景知识。
- en: 11.3 Machine learning with graphs
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 使用图的机器学习
- en: As explained in section 3.1, a graph is a nonlinear data structure composed
    of entities known as *vertices* (or nodes) and the relationships between them,
    known as *edges* (or *arcs* or *links*). Data coming from different domains can
    be nicely captured using a graph. Social media networks, for instance, employ
    graphs to depict connections between users and to analyze social interactions,
    which in turn drive content propagation and recommendations. Navigation applications
    use graphs to represent physical locations and the paths between them, enabling
    route calculations, real-time traffic updates, and estimated time of arrival (ETA)
    predictions. Recommender systems rely on graphs to model user–item interactions
    and preferences, thereby offering personalized recommendations. Search engines
    use web graphs, where web pages are nodes and hyperlinks are edges, to crawl and
    index the internet and facilitate efficient information retrieval. Knowledge graphs
    offer a structured representation of factual information, relationships, and entities,
    and they’re used in diverse fields from digital assistants to enterprise data
    integration. Question-answering engines use graphs to understand and decompose
    complex questions and search for relevant answers in structured datasets. In the
    realm of chemistry, molecular structures can be viewed as graphs, where atoms
    are nodes and bonds are edges, supporting tasks like discovering compounds and
    predicting properties.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如第3.1节所述，图是一种非线性数据结构，由称为*顶点*（或节点）的实体及其之间的关系组成，这些关系称为*边*（或*弧*或*链接*）。来自不同领域的数据可以使用图很好地捕捉。例如，社交媒体网络使用图来描绘用户之间的连接并分析社会互动，这反过来又推动内容传播和推荐。导航应用程序使用图来表示物理位置及其之间的路径，从而实现路线计算、实时交通更新和预计到达时间（ETA）预测。推荐系统依赖于图来模拟用户-项目交互和偏好，从而提供个性化推荐。搜索引擎使用网页图，其中网页是节点，超链接是边，以爬取和索引互联网并促进高效的信息检索。知识图谱提供了事实信息、关系和实体的结构化表示，并在从数字助手到企业数据集成等众多领域得到应用。问答引擎使用图来理解和分解复杂问题，并在结构化数据集中搜索相关答案。在化学领域，分子结构可以被视为图，其中原子是节点，键是边，支持发现化合物和预测性质等任务。
- en: 'Graph-structured data is vital due to its power to model complex relationships
    and dependencies between entities in an intuitive, self-descriptive, intrinsically
    explainable, and natural way. Unlike traditional tabular data, graphs allow for
    the representation of networked relationships and complex interconnectedness between
    entities of interest, making them an excellent tool for modeling numerous real-world
    systems. Tabular data can be converted into graph-structured data—the specific
    definitions of nodes and edges would depend on what relationships you’re interested
    in examining within the data. For example, in the context of a FIFA dataset, we
    can define nodes and edges based on the information available in this dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图结构化数据至关重要，因为它能够以直观、自描述、本质上可解释和自然的方式对实体之间的复杂关系和依赖进行建模。与传统表格数据不同，图允许表示感兴趣实体之间的网络关系和复杂相互关联，使它们成为建模众多现实世界系统的优秀工具。可以将表格数据转换为图结构化数据——节点和边的具体定义将取决于你想要在数据中检查哪些关系。例如，在FIFA数据集的背景下，我们可以根据该数据集中可用的信息定义节点和边：
- en: '*Nodes*—Nodes represent entities of interest and could be the players, the
    clubs they play for, or their nationalities. Each of these entities could be a
    separate node in the graph. For example, Lionel Messi, Inter Miami, and Argentina
    could all be individual nodes in the graph.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点*—节点代表感兴趣的实体，可以是球员、他们所效力的俱乐部或他们的国籍。这些实体中的每一个都可以是图中的一个单独节点。例如，莱昂内尔·梅西、国际米兰和阿根廷都可以是图中的单独节点。'
- en: '*Edges*—Edges represent the relationships between the nodes. For instance,
    an edge could connect a player to the club they play for, indicating that the
    player is part of that club. Another edge could connect a player to their nationality,
    showing that the player belongs to that country. So, for example, Lionel Messi
    could be connected to Inter Miami with an edge indicating that Messi plays for
    Inter Miami, and another edge could connect Lionel Messi to Argentina, indicating
    his nationality.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边缘*—边缘代表节点之间的关系。例如，一条边可以连接一个球员和他们所效力的俱乐部，表示该球员是该俱乐部的一员。另一条边可以连接一个球员和他们的国籍，表明该球员属于那个国家。因此，例如，莱昂内尔·梅西可以通过一条边与国际米兰连接，表示梅西为国际米兰效力，另一条边可以将莱昂内尔·梅西与阿根廷连接，表示他的国籍。'
- en: The next listing shows how to convert tabular data for 10 selected soccer players
    into a graph using NetworkX.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了如何使用NetworkX将10位选定足球运动员的表格数据转换为图。
- en: Listing 11.1 Converting tabular data to a graph
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.1 将表格数据转换为图
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As a continuation of listing 11.1, we can create a NetworkX graph whose nodes
    represent the player name, club, and nationality and whose edges represent the
    semantic relationships between these nodes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为11.1列表的延续，我们可以创建一个NetworkX图，其节点代表球员姓名、俱乐部和国籍，而边则代表这些节点之间的语义关系。
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Create a new graph.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个新的图。
- en: ② Add nodes and edges for clubs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为俱乐部添加节点和边。
- en: ③ Add nodes and edges for nationalities.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 为国籍添加节点和边。
- en: ④ Create the layout
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建布局
- en: ⑤ Set the size of the figure.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置图形的大小。
- en: ⑥ Get lists of player, club, and nationality nodes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 获取球员、俱乐部和国籍节点的列表。
- en: ⑦ Draw nodes in different colors.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 用不同颜色绘制节点。
- en: ⑧ Draw edges.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 绘制边。
- en: ⑨ Draw edge labels.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 绘制边标签。
- en: ⑩ Draw node labels.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 绘制节点标签。
- en: Figure 11.3 shows the data for the 10 selected soccer players in a graph. This
    graph shows the entities of interest (player, club, and nationality) and their
    relationships. For example, L. Messi is a player who plays for Inter Miami and
    is from Argentina.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3显示了10位选定足球运动员的图数据。此图显示了感兴趣的实体（球员、俱乐部和国籍）及其关系。例如，L.梅西是一名为Inter Miami效力的球员，来自阿根廷。
- en: '![](../Images/CH11_F03_Khamis.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F03_Khamis.png)'
- en: Figure 11.3 Graph-structured data for 10 selected soccer players
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 10位选定足球运动员的图结构数据
- en: Graph data fundamentally differs from Euclidean data, as the concept of distance
    is not simply a matter of straight-line (Euclidean) distance between two points.
    In the case of a graph, what matters is the structure of the nodes and edges—whether
    two nodes are connected by an edge and how they are connected to other nodes in
    the graph. Table 11.1 summarizes the differences between Euclidean and non-Euclidean
    graph data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图形数据与欧几里得数据根本不同，因为距离的概念不仅仅是两点之间的直线（欧几里得）距离。在图的情况下，重要的是节点和边的结构——两个节点是否通过边连接，以及它们如何连接到图中的其他节点。表11.1总结了欧几里得数据和非欧几里得图数据之间的差异。
- en: Table 11.1 Euclidean data versus non-Euclidean graph data
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.1 欧几里得数据与非欧几里得图数据
- en: '| Aspects | Euclidean data | Non-Euclidean graph data |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 欧几里得数据 | 非欧几里得图数据 |'
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Common data types | Numerical, text, audio, images, videos | Road networks,
    social networks, web pages, and molecular structures |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 常见数据类型 | 数值、文本、音频、图像、视频 | 道路网络、社交网络、网页和分子结构 |'
- en: '| Dimensionality | Can be 1D (e.g., numbers, text), 2D (e.g., images, heatmaps),
    or higher-dimensional (e.g., RGB-D images or depth maps, 3D point cloud data)
    | Large dimensionality (e.g., a Pinterest graph has 3 billion nodes and 18 billion
    edges) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 可以为1D（例如，数字、文本），2D（例如，图像、热图），或更高维（例如，RGB-D图像或深度图，3D点云数据） | 大维度（例如，Pinterest图有30亿个节点和180亿条边）
    |'
- en: '| Structure | Fixed structure (e.g., in the case of an image, the structure
    is embedded via pixel proximity) | Arbitrary structure (every node can have a
    different neural structure because the network neighborhood around it is different,
    as the model adapts to the data) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 固定结构（例如，在图像的情况下，结构通过像素邻近性嵌入） | 任意结构（每个节点都可以有不同的神经网络结构，因为其网络邻域不同，模型适应数据）
    |'
- en: '| Spatial locality | Yes (i.e., data points that are close together in the
    input space are also likely to be close together in the output space). | No, “closeness”
    is determined by the graph structure, not spatial arrangement (i.e., two nodes
    that are “close” to each other might not necessarily have similar properties or
    features, such as in the case of a traffic light node and a crosswalk node). |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 空间局部性 | 是（即在输入空间中彼此靠近的数据点也可能会在输出空间中彼此靠近）。 | 否，“接近”由图结构决定，而不是空间排列（即彼此“接近”的两个节点可能不具有相似的性质或特征，例如在交通灯节点和人行横道节点的情况下）。
    |'
- en: '| Shift-invariance | Yes (i.e., data-inherent meaning is preserved when shifted;
    for instance, the concept of a cat in a picture does not change if the cat is
    in the top left corner or the bottom right corner of the image). | No (in a graph,
    there’s no inherent meaning to the “position” of a node that can be “shifted”).
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 平移不变性 | 是（即在平移时保留数据固有的意义；例如，图片中猫的概念不会因为猫在图像的右上角或左下角而改变）。 | 否（在图中，节点的“位置”没有固有的意义，不能“平移”）。
    |'
- en: '| Ordinality or hierarchy | Yes | No, graph data has “permutation invariance”—the
    specific ordering or labeling of nodes doesn’t usually affect the underlying relationships
    and properties of the graph. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| Shortest path between two points | A straight line | Is not necessarily a
    straight line |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| Examples of ML models | Convolutional neural networks (CNNs), long short-term
    memory (LSTM), and recurrent neural networks (RNNs) | Graph neural networks (GNNs),
    graph convolutional networks (GCNs), temporal graph networks (TGNs), spatial-temporal
    graph neural networks (STGNNs) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '*Geometric deep learning* (GDL) is an umbrella term for emerging techniques
    seeking to extend (structured) deep neural models to handle non-Euclidean data
    with underlying geometric structures, such as graphs (networks of connected entities),
    point clouds (collections of 3D data points), molecules (chemical structures),
    and manifolds (curved, high-dimensional surfaces). Graph machine learning (GML)
    is a subfield of ML that focuses on developing algorithms and models capable of
    learning from graph-structured data. Graph embedding or representation learning
    is the first step in performing ML tasks such as node classification (predicting
    a category for each node), link prediction (forecasting connections between nodes),
    and community detection (identifying groups of interconnected nodes). The next
    subsection describes different graph embedding techniques.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Graph embedding
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph embedding is a task that aims to learn a mapping from a discrete high-
    dimensional graph domain to a low-dimensional continuous domain. Through the process
    of graph embedding, graph nodes, edges, and their features are transformed into
    continuous vectors while preserving the structural information of the graph. For
    example, as shown in figure 11.4, an encoder, *ENC*(*v*), maps node *v* from the
    input graph space *G* to a low-dimensional vector *h[v]* in the embedding or latent
    space *H* based on the node’s position in the graph, its local neighborhood structure,
    or its features, or some combination of the three.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F04_Khamis.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 Graph embedding
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: This encoder needs to be optimized to minimize the difference between the similarity
    of a pair of nodes in the graph and their similarity in the embedding space. Nodes
    that are connected or nearby in the graph should be close in the embedded space.
    Conversely, nodes that are not connected or are far apart in the graph should
    be far apart in the embedded space. In a more generalized encoder/decoder architecture,
    a decoder is added to extract user-specified information from the low-dimensional
    embedding [5]. By jointly optimizing the encoder and decoder, the system learns
    to compress information about the graph structure into the low-dimensional embedding
    space.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various methods for graph embedding which can be broadly classified
    into transductive (shallow) embedding and inductive embedding:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '*Transductive embedding*—In the transductive learning paradigm, the model learns
    embeddings only for the nodes present in the graph during the training phase.
    The learned embeddings are specific to these nodes, and the model cannot generate
    embeddings for new nodes that weren’t present during training. These methods are
    difficult to scale and are suitable for static graphs. Examples of transductive
    methods for graph embedding include random walk (e.g., node2vec and DeepWalk)
    and matrix factorization (e.g., graph factorization and HOPE).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inductive embedding*—Inductive learning methods can generalize to unseen nodes
    or entire graphs that were not present during training. They do this by learning
    a function that generates the embedding of a node based on its features and the
    structure of its local neighborhood, which can be applied to any node, regardless
    of whether it was present during training or not. These methods are suitable for
    evolving graphs. Examples of inductive methods for graph embedding are graph neural
    networks (GNN) and graph convolutional networks (GCNs).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A contains examples of some of these methods. For more information,
    see Broadwater and Stillman’s *Graph Neural Networks in Action* [6]. We’ll focus
    on GCN, as it is the most relevant approach to the combinatorial optimization
    application presented in this chapter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Transductive versus inductive learning
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '*Transductive learning* aims to learn from a specific set of data to a specific
    set of predictions without generalizing to new data. *Inductive learning* aims
    to learn general rules from observed training cases. These general rules can then
    be applied to new, unseen data.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The *convolution operation* forms the basis of representation learning in many
    structured data scenarios, enabling the automatic learning of meaningful features
    from raw data, thereby obviating the need for manual feature engineering. Convolution
    is a mathematical operation that takes two functions (input data and a kernel,
    filter, or feature detector) and measures their overlap or merges the two sets
    of information to produce a feature map. One critical aspect of convolution is
    its ability to respect and utilize the known structural relationships among data
    points, such as the positional associations among pixels, the temporal order of
    time points, or the edges linking nodes in a network. In traditional ML, convolutional
    neural networks (CNNs) employ the convolution operator as a key tool for identifying
    spatial patterns within images. This is made possible by the inherent grid-like
    structure of image data, which allows the model to slide filters over the image,
    exploit the spatial regularities, and extract features in a manner akin to pattern
    recognition.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: However, in the realm of graph machine learning (GML), the situation changes
    considerably. The data in this context is non-Euclidean, as explained previously
    in table 11.1, meaning that it isn’t arranged on a regular grid like pixels are
    in an image or points are on a 3D surface. Instead, it’s represented in the form
    of a network or graph, which can capture complex relationships. Moreover, this
    data exhibits order invariance, implying that the output does not change with
    the rearrangement of nodes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Unlike CNNs, which operate on a regular grid, GCNs are designed to work with
    data that’s structured as a graph, which can represent a wide variety of irregular
    and complex structures. Each node is connected to its neighbors without any predefined
    pattern, and the convolution operation is applied to a node and its direct neighbors
    in the graph.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: How does Google DeepMind predict the estimated time of arrival?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever wondered how Google Maps predicts the estimated time of arrival
    (ETA) when you’re planning your trip? Google DeepMind uses a GML approach to do
    so. The traditional ML approach would be to break the route down into a number
    of road segments, predict the time to traverse each road segment using a feedforward
    neural network, and sum them up to get the ETA. However, the underlying assumption
    of feedforward NN is that the road segments are independent of each other. In
    reality, road segment traffic easily influences the ETA of neighboring road segments,
    so the samples are not independent.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider the situation where congestion on a minor road influences
    the traffic flow on a main road. When the model encompasses multiple junctions,
    it naturally develops the capacity to predict slowdowns at intersections, delays
    due to converging traffic, and the total time taken in stop-and-go traffic conditions.
    A better approach is to use GML to take the influence of the neighboring road
    segments into consideration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the road network will first be converted into a graph where each
    road segment is represented as a node. If two road segments are connected to each
    other, their corresponding nodes will be connected by an edge in the graph. Graph
    embedding is then generated by GNN to map the node features and graph structures
    from a high- dimensional discrete graph space to a low-dimensional continuous
    latent space. Information is propagated and aggregated across the graph through
    a technique called *message passing*, where, at the end, the embedding vector
    for each node contains and encodes its own information as well as the network
    information from all its neighboring nodes, according to the degree of neighborhood.
    Adjacent nodes pass messages to each other. In the first pass, each node knows
    about its neighbor. In the second pass, every node knows about its neighbor’s
    neighbors, and this information is encoded into the embedding, and so on. This
    allows us to represent the influence of the traffic in each of the neighboring
    road segments.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of real time ETAs was improved by up to 50% in places like Berlin,
    Jakarta, São Paulo, Sydney, Tokyo, and Washington DC using this approach [7].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in figure 11.5, given an input graph, which includes node features
    *x[v]* and an adjacency matrix *A*, a GCN transforms the features of each node
    into a latent or embedding space *H*, while preserving the graph structure denoted
    by the adjacency matrix *A*. These latent vectors provide a rich representation
    of each node, making it possible to perform node classification independently.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F05_Khamis.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 Graph embedding and node, link, and graph classification
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, GCNs are also capable of predicting characteristics related to edges,
    such as whether a link exists between two nodes. Once node embeddings are generated,
    the likelihood of an edge between two nodes *v* and *u* can be predicted based
    on their embeddings *h[v]*, *h[u]*. A common approach is to compute a similarity
    measure (e.g., a dot product) between the embeddings of two nodes. This similarity
    can then be passed through a sigmoid function to predict the probability of an
    edge. The errors (loss) on predictions will be backpropagated and update the weights
    in neural networks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Finally, GCNs enable classification at the level of the entire graph. This can
    be achieved by aggregating all the latent or embedding vectors (*H*) for all the
    nodes. The aggregation function used must be permutation invariant, meaning the
    output should remain the same regardless of the order of the nodes. Common examples
    of such functions are summation or averaging or maximizing. Once you’ve aggregated
    the latent vectors into a single representation, you can feed this representation
    into a module (e.g., a neural network layer) to predict an output for the whole
    graph. In essence, GCNs allow node-level, edge-level, and graph-level predictions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how GCN works, let’s consider a graph with five nodes,
    as shown in figure 11.6\. For each node in the graph, the first step is to find
    the neighboring nodes. Let’s assume we want to examine how the embedding for node
    5 is generated. As you can see in the original graph (upper-left corner of figure
    11.6), nodes 2 and 4 are neighbors of node 5\. The second step is message-passing,
    which is the process of nodes sending, receiving, and aggregating messages from
    their neighbors to iteratively update their features. This allows GCNs to learn
    a representation for each node that captures both its own features and its context
    within the graph. The learned representations can then be used for downstream
    tasks like node classification, link prediction, or graph classification.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F06_Khamis.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 Message passing and updating in GCN
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The embedding of node *v* after *t* layers of neighborhood aggregation considering
    *N*(*v*) neighboring nodes is based on the formula shown in figure 11.7\. The
    initial 0^(th) layer embeddings *h[v]*⁰ are equal to node features x[v].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F07_Khamis.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 Embedding function in GCN
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: This formula is applied recursively to get another, better vector *h* at each
    time step, where *h* is the vector representation of the nodes in the latent space.
    The weight matrix is learned through training on given data. At the beginning,
    each node in the graph is aware only of its own initial features. In the first
    layer of the GCN, each node communicates with its immediate neighbors, aggregating
    its own features and receiving features from those neighbors. As we move to the
    second layer, each node again communicates with its neighbors. However, because
    the neighbors have already incorporated information from their own neighbors in
    the first layer, the original node now indirectly accesses information from two
    hops away in the graph—its neighbors’ neighbors. As this process repeats through
    more layers in the GCN, information is propagated and aggregated across the graph.
    At the end, the embedding vector for each node contains and encodes its own information
    as well as the network information from all its neighboring nodes according to
    the degree of neighborhood, or its *k*-hop neighborhood, to create context embedding.
    The *k-hop neighborhood*, or neighborhood of radius *k*, of a node is a set of
    neighboring nodes at a distance less than or equal to *k*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 shows how to generate node embedding for the Cora dataset using
    GCN. The Cora dataset consists of 2,708 scientific publications classified into
    one of seven classes. The citation network consists of 5,429 links. Each publication
    in the dataset is described by a 0/1-valued word vector indicating the absence/presence
    of the corresponding word in the dictionary. The dictionary consists of 1,433
    unique words.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'PyG (PyTorch Geometric) is used and can be installed as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: More information about PyG CUDA installation is available in the PyG documentation
    ([https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by importing the libraries we’ll use.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 Node embedding using GCN
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'PyG provides several datasets that can be loaded directly, such as KarateClub,
    Cora, Amazon, Reddit, etc. The Cora dataset is part of the Planetoid dataset and
    can be loaded as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see in the following code, the GCN model is defined with two `GCNConv`
    layers (`GCNConv`) and a `torch.nn.Dropout` layer. `GCNConv` is a graph convolution
    layer, and `torch.nn.Dropout` is a dropout layer, which randomly zeroes some of
    the elements of the input tensor with probability 0.5 during training as a simple
    way to prevent overfitting.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward` function defines the forward pass of the model. It takes a data
    object as input, representing the graph, and the features of the nodes and the
    adjacency list of the graph are extracted from the input data. The node features
    (`x`) are passed through the first GCN layer `conv1`, a `relu` activation function,
    a dropout layer, and finally the second GCN layer `conv2`. The adjacency list,
    `edge_index`, is required for the convolution operation in the GCN layers. The
    output of the network is then returned:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As a continuation of listing 11.2, the following code snippet trains the GCN
    model on a single graph and extracts the node embedding from the trained model.
    The `model` is trained for 200 epochs. Its gradients are first zeroed, then the
    forward pass is computed, and the negative log-likelihood loss is calculated on
    the training nodes (those marked by `data.train_mask`). The backward pass is then
    computed to get the gradients, and the optimizer performs a step to update the
    model parameters. The model is set to evaluation mode and is run on the graph
    again to obtain the final node embeddings:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① If CUDA is available, the code uses the GPU; otherwise, it will use the CPU.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: ② Create an instance of the GCN model, and move it to the chosen device.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: ③ Load the first graph in the dataset, and move it to the device.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: ④ Use the Adam optimizer with a learning rate of 0.01 and weight decay (a form
    of regularization) of 0.0005.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Train the model for 200 epochs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Set evaluation mode.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Obtain the final node embeddings.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: The `.detach()` function is used to detach the output from the computational
    graph and returns a new tensor that doesn’t require a gradient. The embeddings
    are then moved from the GPU (if they were on the GPU) to the CPU. This is done
    to make the data accessible for further processing, such as converting it to a
    NumPy array. The generated embedding has a size of (2708, 7), where the number
    of nodes is 2,708 and the number of classes or subjects is 7\. Dimensionality
    reduction using principle component analysis (PCA) is applied to visualize the
    embedding in 2D as shown in figure 11.8\.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F08_Khamis.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 Node embedding using GCN in PyG
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the node embedding makes the nodes belonging to the same classes
    cluster together. This means increased discrimination power of the features, which
    results in more accurate predictions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The complete version of listing 11.2 available in the book’s GitHub repo also
    shows how to generate node embedding using the GCN available in StellarGraph.
    StellarGraph is a Python library for ML on graphs and networks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.2 Attention mechanisms
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you saw in figure 11.7, the embedding function in GCN consists of message
    passing, aggregation, and update functions. The message passing function mainly
    integrates messages from the node’s neighbors based on a learnable weight matrix
    *W^t*. This weight matrix does not reflect the degree of importance of neighboring
    nodes. The convolution operation applies the same learned weights to all neighbors
    of a node as a linear transformation, without explicitly accounting for their
    importance or relevance. This might not be ideal because some segments may need
    more attention than others.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The concept of “attention” in DL essentially permits the model to selectively
    concentrate on specific segments of the input data as it produces the output sequence.
    This mechanism ensures that context is maintained and propagated from the initial
    stages to the end. It also allows the model to dynamically allocate its resources
    by focusing on the most important parts of the input at each time step. In a broad
    sense, attention in DL can be visualized as a vector consisting of importance
    or relevance scores. These scores help quantify the relationship or association
    between a node in a graph and all other nodes in the graph.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Attention is all you need
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The groundbreaking paper “Attention Is All You Need” [8] proposes a new Transformer
    model for processing sequential data like text. In the world of language processing
    and translation, models usually read an entire sentence or document word by word,
    in order (like we do when we read a book), and then make predictions based on
    that. These models have some difficulties understanding long sentences and recalling
    information from far away in the text. In the case of long sequences, there is
    a high probability that the initial context will be lost by the end of the sequence.
    This is called the *forgetting problem*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the paper propose a different way of handling this task. Instead
    of reading everything in order, their model focuses on different parts of the
    input at different times, almost like it’s jumping around the text. This is what
    they refer to as “attention.” The attention mechanism allows the model to dynamically
    prioritize which parts of the input are most relevant for each word it’s trying
    to predict, making it more effective at understanding context and reducing confusion
    arising from long sentences or complex phrases. For more details, see “The Annotated
    Transformer” [9].
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.9b shows a graph attention network (GAT), where a weighting factor
    or attention coefficient *α* is added to the embedding equation to reflect the
    importance of the neighboring nodes. GAT uses a weighted adjacency matrix instead
    of nonweighted adjacency matrix used in case of GCN (figure 11.9a). An attentional
    mechanism *a* is used to compute unnormalized coefficients *e[vu]* across pairs
    of nodes *v* and *u* based on their features:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F08_Khamis-EQ01.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: '| 11.1 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: An example of this attentional mechanism can be dot-product attention that measures
    the similarity or alignment between the features of the two nodes, providing a
    quantitative indication of how much attention node *v* should give to node *u*.
    Other mechanisms may involve learned attention weights, nonlinear transformations,
    or more complex interactions between node features. Following the graph structure,
    node *v* can attend over nodes in its neighborhood only *i* ∈ *N[v]*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Attention coefficients are typically normalized using the softmax function so
    that they are comparable, irrespective of the scale or distribution of raw scores
    in different neighborhoods or contexts. Note that in figure 11.9b, for simplicity,
    the attention coefficients *α[vu]* are denoted as *α[u]*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F08_Khamis-EQ02.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: '| 11.2 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '![](../Images/CH11_F09_Khamis.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 Graph convolutional network (GCN) vs. graph attention network (GAT)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '*Multi-head attention* is a key component in GATs and also in the Transformer
    model discussed in the “Attention Is All You Need” paper. In a multi-head attention
    mechanism, the model has multiple sets of attention weights. Each set (or “head”)
    can learn to pay attention to different parts of the input. Instead of having
    just one focus of attention, the model can have multiple focuses, allowing it
    to capture different types of relationships and patterns in the data. In the context
    of GATs, a multi-head attention mechanism allows each node in the graph to focus
    on different neighboring nodes in different ways, as shown in figure 11.10.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F10_Khamis.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 Multi-head attention with *H* = 3 heads by node 5\. *α*[52], *α*[54],
    and *α*[55] are the attention coefficients between the nodes. The aggregated features
    from each head are averaged to obtain the final embedding of the node.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Once the multiple heads have performed their respective attention operations,
    their results are typically averaged. This process condenses the diverse perspectives
    captured by the multiple attention heads into a single output. After the results
    of the multi-head attention operation are combined, a final nonlinearity is then
    applied. This step typically involves the use of a softmax function or logistic
    sigmoid function, especially in classification problems. These functions serve
    to translate the model’s final outputs into probabilities, making the output easier
    to interpret and more useful for prediction tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.3 Pointer networks
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sequential ML involves dealing with data where the order of observations matters,
    such as time series data, sentences, or permutations. Sequential ML tasks can
    be classified based on the number of inputs and outputs, as shown in table 11.2\.
    A *sequence-to- sequence* (seq2seq) model takes a sequence of items and outputs
    another sequence of items. Recurrent neural networks (RNN) and long short-term
    memory (LSTM) have been established as state-of-the-art approaches in seq2seq
    modeling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.2 Sequential ML
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Example |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| One-to-one | Image classification. We provide a single image as input, and
    the model outputs the classification or category, like “dog” or “cat,” as a single
    output. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| One-to-many | Image captioning. We input a single image into the model, and
    it generates a sequence of words describing that image. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| Many-to-one | Sentiment analysis. We input a sequence of words (like a sentence
    or a tweet), and the model outputs a single sentiment score (like “positive,"
    “negative,” or “neutral”). |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| Many-to-many (type 1) | Sequence input and sequence output, like in the case
    of named entity recognition (NER). We input a sentence (a sequence of words),
    and the model outputs the recognized entity, such as a person, organization, location,
    etc. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| Many-to-many (type 2), known as a synchronized sequence model | Synced sequence
    input and output. The model takes a sequence of inputs but doesn’t output anything
    until the entire sequence has been read. Then it outputs a sequence. An example
    of this is video classification, where the model takes a sequence of video frames
    as input and then outputs a sequence of labels for those frames. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: In discrete combinatorial optimization problems like the travelling salesman
    problem, sorting tasks, or the convex hull problem, both the input and output
    data are sequential. However, traditional seq2seq models struggle to solve these
    problems effectively. This is primarily because the discrete categories of output
    elements are not predetermined. Instead, they are contingent on the variable size
    of the input (for instance, the output dictionary is dependent on the input length).
    The *pointer network* (Ptr-Net) model [10] addresses this problem by utilizing
    attention as a mechanism to point to or select a member of the input sequence
    for the output. This model not only enhances performance over the conventional
    seq2seq model equipped with input attention, but it also enables us to generalize
    to output dictionaries of variable sizes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'While traditional attention mechanisms distribute attention over the input
    sequence to generate an output element, Ptr-Net instead uses attention as a pointer.
    This pointer is used to select an element from the input sequence to be included
    in the output sequence. Let’s consider the convex hull problem as an example of
    a discrete combinatorial optimization problem. A convex hull is a geometric shape,
    specifically a polygon, that fully encompasses a given set of points. It achieves
    this by optimizing two distinct parameters: it maximizes the area that the shape
    covers, while simultaneously minimizing the boundary or circumference of the shape,
    as illustrated in figure 11.11\. To understand this concept, it can be useful
    to imagine stretching a rubber band around the extreme points or vertices of the
    set. When you release the rubber band, it automatically encompasses the entire
    set in the smallest perimeter possible, and this is essentially what a convex
    hull does.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F11_Khamis.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 The convex hull problem. a) A valid convex hull that encloses all
    points while maximizing the area and minimizing the circumference. Note that the
    number of points included in the output sequence of the polygon may be smaller
    than the number of given points. b) An invalid convex hull, as the circumference
    is not minimized. c) An invalid convex hull, as not all the points are enclosed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Convex hulls have a multitude of applications across a variety of disciplines.
    For example, in the field of image recognition, convex hulls can help determine
    the shape and boundary of objects within an image. Similarly, in robotics, they
    can assist in obstacle detection and navigation by defining the “reachable” space
    around a robot.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The problem of finding or computing a convex hull, given a set of points, has
    been addressed through various algorithms. For example, the Graham scan algorithm
    sorts the points according to their angle with the point at the bottom of the
    hull and then processes them to find the convex hull [11]. The Jarvis march (or
    the gift wrapping algorithm) starts with the leftmost point and wraps the remaining
    points like wrapping a gift [12]. The quickhull algorithm finds the convex hull
    of a point set by recursively dividing the set into subsets, selecting the point
    farthest from the line between two extreme points, and eliminating points within
    the formed triangles until the hull’s vertices are identified [13].
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 11.12, Ptr-Net takes as input a planar set of points *P*
    = {*P*[1], *P*[2], …, *P[n]*} with *n* elements each, where *P[j]* = (*x[j], y[j]*)
    are the Cartesian coordinates of the points. The outputs *C[P]* = {*C*[1], *C*[2],…,
    *C[m]*[(]*[P]*[)]} are sequences representing the solution associated with the
    point set *P*. In this figure, Ptr-Net estimates the output sequence [1 4 2] from
    the input data points [1 2 3 4]. This output sequence represents the convex hull
    that includes all the input points with maximum area and minimum circumference.
    As can be seen, the convex hull is formed by connecting *P*[1], *P*[2], and *P*[4].
    The third point *P*[3] is inside this convex hull.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F12_Khamis.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 Pointer network (Prt-Net) estimating the output sequence [1 4 2]
    from the input data points [1 2 3 4]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Ptr-Net consists of three main components:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '*Encoder*—The encoder is a recurrent neural network (RNN), often implemented
    with long short-term memory (LSTM) units or gated recurrent units (GRUs). The
    encoder’s purpose is to process the input sequence, converting each input element
    into a corresponding hidden state. These hidden states (*e*[1],…, *e*[n]) encapsulate
    the context-dependent representation of the elements in the input sequence.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Decoder*—Like the encoder, the decoder is also an RNN. It’s responsible for
    generating the output sequence (*d*[1],…, *d*[m]). For each output step, it takes
    the previous output and its own hidden state as inputs.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention mechanism (pointer)*—The attention mechanism in a Ptr-Net operates
    as a pointer. It computes a distribution over the hidden states output by the
    encoder, indicating where to “point” in the input sequence for each output step.
    Essentially, it decides which of the inputs should be the next output. The attention
    mechanism is a softmax function over the learned attention scores, which gives
    a probability distribution over the input sequence, signifying the likeliness
    of each element being pointed at.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The attention vector at each output time *i* is computed using the following
    equations:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F12_Khamis-EQ03.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: '| 11.3 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F12_Khamis-EQ04.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: '| 11.4 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F12_Khamis-EQ05.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: '| 11.5 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: where
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '*u[j]* is the attention vector or alignment score that represents the similarity
    between the decoder and encoder hidden states. *v, W*[1], and *W*[2] are learnable
    parameters of the model. If the same hidden dimensionality is used for the encoder
    and decoder (typically 512), *v* is a vector, and *W*[1] and *W*[2] are square
    matrices.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a[j]* is the attention mask over the input or weights computed by applying
    the softmax operation to the alignment scores.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*d[i]*^’ is the context vector that is fed into the decoder at each time step.
    In other words, *d[i]* and *d[i]*^’ are concatenated and used as the hidden states
    from which the predictions are made. This weighted sum of all the encoder hidden
    states allows the decoder to flexibly focus the attention on the most relevant
    parts of the input sequence.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ptr-Net can process variable-length sequences and solve complex combinatorial
    problems, especially those involving sorting or ordering tasks, where the output
    is a permutation of the input, as you will see in section 11.9.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Self-organizing maps
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *self-organizing map* (SOM), also known as a *self-organizing feature map*
    (SOFM) or *Kohonen map*, is a type of artificial neural network (ANN) that is
    trained with unsupervised learning to produce a low-dimensional (typically two-dimensional),
    discretized representation of the input space of the training samples, called
    a *map*. SOMs are distinguished from traditional ANNs by the nature of their learning
    process, known as *competitive learning*. In such algorithms, processing elements
    or neurons compete for the right to respond to a subset of the input data. The
    degree to which an output neuron is activated is amplified as the similarity between
    the neuron’s weight vector and the input grows. The similarity between the weight
    vector and the input, leading to neuron activation, is commonly gauged through
    the calculation of Euclidean distance. The output unit that demonstrates the highest
    level of activation, or equivalently the shortest distance, in response to a specific
    input is deemed the best matching unit (BMU) or the “winning” neuron, as illustrated
    in figure 11.13\. This winner is then drawn incrementally closer to the input
    data point by adjusting its weight.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F13_Khamis.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 Self-organizing map (SOM) with a Gaussian neighborhood function
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'A key characteristic of SOM is the concept of a *neighborhood function*, which
    ensures that not only the winning neuron but also its neighbors learn from each
    new input, creating clusters of similar data. This allows the network to preserve
    the topological properties of the input space. Equation 11.6 shows an example
    of a neighborhood function:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F13_Khamis-EQ06.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: '| 11.6 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: where *v* is the index of the node in the map, *u* is the index of the winning
    neuron, *LDist*(*u,v*) represents the lattice distance between *u* and *v*, and
    *σ* is the bandwidth of the Gaussian kernel. In SOMs, *σ* represents the radius
    or width of the neighborhood and determines how far the influence of the winning
    neuron extends to its neighbors during the weight update phase. A large *σ* means
    a broader neighborhood is affected. On the other hand, a small *σ* means that
    fewer neighboring neurons are influenced. When *σ* is set to an extremely small
    value, the neighborhood effectively shrinks to include only the winning neuron
    itself. This means that only the winning neuron’s weights are significantly updated
    in response to the input, while the weights of the other neurons are barely or
    not at all affected. This behavior, where only the winning neuron is updated,
    is referred to as “winner take all” learning.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 11.1 shows the steps of SOM, assuming that *D[t]* is a target input
    data vector, *W[v]* is the current weight vector of node *v, θ*(*u*,*v*,*s*) is
    the neighborhood function that represents the restraint due to the distance from
    the winning neuron, and *α* is a learning rate where *α* ∈ (0,1).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 11.1 Self-organizing map (SOM)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: SOMs were initially used as a dimensionality reduction method for data visualization
    and clustering tasks. For example, the neural phonetic typewriter was one of the
    early applications of Kohonen’s SOM algorithm. It was a system where spoken phonemes
    (the smallest unit of speech that can distinguish one word from another) were
    recognized and converted into symbols. When someone spoke into the system, the
    SOM would classify the input phoneme and type the corresponding symbol. SOMs can
    be applied to different problems such as feature extraction, adaptive control,
    and travelling salesman problems (see section 11.8).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: SOMs offer a significant advantage in that they preserve the relative distances
    between points as calculated within the input space. Points that are close in
    the input space are mapped onto neighboring units within the SOM, making SOMs
    effective tools for analyzing clusters within high-dimensional data. When using
    techniques like principal component analysis (PCA) to handle high-dimensional
    data, data loss may occur when reducing the dimensions to two. If the data contains
    numerous dimensions and if each dimension carries valuable information, then SOMs
    can be superior to PCA for dimensionality reduction purposes. Beyond this, SOMs
    also possess the ability to generalize. Through this process, the network can
    identify or categorize input data that it has not previously encountered. This
    new input is associated with a specific unit on the map and is thus mapped accordingly.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: The previous sections have offered a fundamental foundation in ML, equipping
    you with essential background knowledge. The upcoming sections will delve deeply
    into the practical applications of supervised and unsupervised ML in tackling
    optimization problems.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 Machine learning for optimization problems
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The utilization of ML techniques to tackle combinatorial optimization problems
    represents an emergent and exciting field of study. *Neural combinatorial optimization*
    refers to the application of ML and neural network models, specifically seq2seq
    supervised models, unsupervised models, and reinforcement learning, to solve combinatorial
    optimization problems. Within this context, the application of ML to combinatorial
    optimization has been comprehensively described by Yoshua Bengio and his co-authors
    [14]. The authors depict three distinctive methods for harnessing ML for combinatorial
    optimization (see figure 11.14):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F14_Khamis.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 Machine learning (ML) for combinatorial optimization (CO) problems
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '*End-to-end learning*—To use ML to address optimization problems, we need to
    instruct the ML model to formulate solutions directly from the input instance.
    An example of this approach is Ptr-Net, which is trained on *m* points and validated
    on *n* points for a Euclidean planar symmetric TSP [10]. Examples of solving combinatorial
    optimization problems using end-to-end learning are provided in sections 11.6,
    11.7, and 11.9.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning to configure algorithms*—The second method involves applying an ML
    model to enhance a combinatorial optimization algorithm with pertinent information.
    In this regard, ML can offer a parameterization of the algorithm. Examples of
    such parameters comprise, but are not restricted to, learning rate or step size
    in gradient descent methodologies; initial temperature or cooling schedule in
    simulated annealing; standard deviation of Gaussian mutation or selective crossover
    in genetic algorithms; inertia weight or cognitive and social acceleration coefficients
    in particle swarm optimization (PSO); or rate of evaporation, influence of pheromone
    deposition, or influence of the desirability of state transition in ant colony
    optimization (ACO).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ML in conjunction with optimization algorithms*—The third method calls for
    a combinatorial optimization algorithm to repetitively consult the same ML model
    for decision-making purposes. The ML model accepts as input the current state
    of the algorithm, which could encompass the problem definition. The fundamental
    distinction between this approach and the other two lies in the repeated utilization
    of the same ML model by the combinatorial optimization algorithm to make identical
    kinds of decisions, approximately as many times as the total number of iterations
    of the algorithm. An example of this approach is DL-assisted heuristic tree search
    (DLTS), which consists of a heuristic tree search in which decisions about which
    branches to explore and how to bound nodes are made by deep neural networks (DNNs)
    [15].'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another intriguing research paper by Vesselinova et al. delves into some pertinent
    questions concerning the intersection of ML and combinatorial optimization [16].
    Specifically, the paper investigates the following questions:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Can ML techniques be utilized to automate the process of learning heuristics
    for combinatorial optimization tasks and, as a result, solve these problems more
    efficiently?
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What essential ML methods have been employed to tackle these real-world problems?
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How applicable are these methods to practical domains?
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This paper offers a thorough survey of various applications of supervised and
    reinforcement learning strategies in tackling optimization problems. The authors
    analyze these learning approaches by examining their application to a range of
    optimization problems:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: The knapsack problem (KP), where the goal is to maximize the total value of
    items chosen without exceeding the capacity of the knapsack
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximal clique (MC) and maximal independent set (MIS) problems, which both
    involve identifying subsets of a graph with specific properties
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum coverage problem (MCP), which requires selecting a subset of items
    to maximize coverage
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum cut (MaxCut) and minimum vertex cover (MVC) problems, which involve
    partitioning a graph in particular ways
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the paper discusses the application of ML approaches to the satisfiability
    problem (SAT), which is a decision problem involving Boolean logic; the classic
    TSP, which requires finding the shortest possible route that visits a given set
    of cities and returns to the origin city; and the vehicular routing problem (VRP),
    which is a generalized version of TSP where multiple “salesmen” (vehicles) are
    allowed. More information about benchmark optimization problems is provided in
    appendix B.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Optimization by prompting (OPRO) is described in Chengrun et al.’s “Large Language
    Models as Optimizers” article as a simple and effective approach to using large
    language models (LLMs) as optimizers, where the optimization task is described
    in natural language [17]. Additional examples showcasing the use of ML in addressing
    optimization problems can be accessed through the AI for Smart Mobility publication
    hub ([https://medium.com/ai4sm](https://medium.com/ai4sm)). To stimulate further
    exploration and draw more researchers into this emerging domain, a competition
    named Machine Learning for Combinatorial Optimization (ML4CO) was organized as
    part of the Neural Information Processing Systems (NeurIPS) conference. The competition
    posed a unique proposition for participants, requiring them to devise ML models
    or algorithms targeted at resolving three separate challenges. Each of these challenges
    mirrors a specific control task that commonly emerges in conventional optimization
    solvers. This competition provides a platform where researchers can explore and
    test novel ML strategies, contributing to the advancement of the field of combinatorial
    optimization.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 Solving function optimization using supervised machine learning
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Amortized optimization*, or *learning to optimize*, is an approach where ML
    models are used to rapidly predict the solutions to an optimization problem. Amortized
    optimization methods try to learn the mapping between the decision variable space
    and the optimal or near-optimal solution space. The learned model can be used
    to predict the optimal value of an objective function, enabling fast solvers.
    The computation cost of the optimization process is spread out between learning
    and inferencing. This is the reason for the name “amortized optimization,” as
    the word “amortization” generally refers to spreading out costs.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: B. Amos shows several examples of how to use amortized optimization to solve
    optimization problems in his tutorial [18]. For example, a supervised ML approach
    can learn to solve optimization problems over spheres. Here the objective is to
    find the extreme values of a function defined on the earth or other space that
    can be approximated with a sphere of the form
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F14_Khamis-EQ07.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: '| 11.7 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: 'where *S*² is the surface of the unit 2-sphere embedded in real-number space
    *R*³ as *S*²:= {*y* ∈ *R*³ | ||*y*||[2] =1}, and *x* is some parameterization
    of the function *f* : *S*² × *X* → *R*. ||*y*||[2] refers to the Euclidean norm
    (also known as the *L2 norm* or *2-norm*) of a vector *y*. More details about
    the amortization objective function are available in Amos’s “Tutorial on amortized
    optimization for learning to optimize over continuous domains” [18].'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 shows the steps for applying amortized optimization based on supervised
    learning to solve the problem of finding the extreme values of a function defined
    on the earth or other spaces. We’ll start by defining two conversion functions,
    `celestial_to_euclidean()` and `euclidean_to_celestial()`, that convert between
    celestial coordinates (right ascension, `ra`, and declination, `dec`) and Euclidean
    coordinates (`x, y, z`).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The celestial coordinate system
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The *astronomical* or *celestial coordinate system* is a reference system used
    to specify the positions of objects in the sky, such as satellites, stars, planets,
    galaxies, and other celestial bodies. There are several celestial coordinate systems,
    with the most common being the equatorial system. In the equatorial system, right
    ascension (RA) and declination (Dec) are the two numbers used to fix the location
    of an object in the sky. These coordinates are analogous to the latitude and longitude
    used in earth’s geographic coordinate system.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following figure, RA is measured in hours, minutes, and seconds
    (h:m:s), and it is analogous to longitude in earth’s coordinate system. RA is
    the angular distance of an object measured eastward along the celestial equator
    from the vernal equinox (the point where the sun crosses the celestial equator
    during the March equinox). The celestial equator is an imaginary great circle
    on the celestial sphere, lying in the same plane as earth’s equator. Dec is measured
    in degrees and represents the angular distance of an object north or south of
    the celestial equator. It is analogous to latitude in earth’s coordinate system.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F14_UN01_Khamis.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Celestial coordinate system with an example point with a right ascension of
    10 hours and a declination of 30 degrees
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Positive declination is used for objects above the celestial equator, and negative
    declination is used for objects below the celestial equator.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The `sphere_dist(x, y)` function calculates the Riemannian distance (the great-
    circle distance) between two points on the sphere in the Euclidean space. This
    distance represents the shortest (geodesic) path between two points on the surface
    of a sphere, measured along the surface rather than through the interior of the
    sphere. The function asserts that the input vectors are two-dimensional. Then
    it calculates the dot product of *x* and *y* and returns the arccosine of the
    result, which corresponds to the angle between *x* and *y*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 Solving a function optimization problem using supervised learning
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Convert from celestial coordinates to Euclidean coordinates.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: ② Convert from Euclidean coordinates to celestial coordinates.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculate the Riemannian distance between two points on the sphere.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define a `c-convex` class as a subclass of `nn.Module`, which makes
    it a trainable model in PyTorch. Cohen and his co-authors defined *c-convex* in
    their “Riemannian convex potential maps” article as a synthetic class of optimization
    problems defined on the sphere [19]. The `c-convex` class models a c-convex function
    on the sphere with `n_components` components that we can sample data from for
    training. The `gamma` parameter controls the aggregation of the components of
    the function, and `seed` is used to initialize the random number generator for
    reproducibility. It also generates random parameters `ys` (which are unit vectors
    in the 3D space) and `alphas` (which are scalars between 0 and 0.7) for each component
    of the c-convex function. The parameters are concatenated into a single `params`
    vector. The `forward(xyz)` method calculates the value of the c-convex function
    at the point `xyz`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Define a c-convex function.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ② Sample random parameters.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ③ Computes the output of the c-convex function given input coordinates xyz on
    the sphere.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the preceding code, we define an amortized model, which
    takes a parameter vector as input and outputs a 3D vector representing a point
    on the sphere. The amortized model uses a neural network to learn a mapping from
    the parameter space to the 3D space of points on the sphere. The code also initializes
    a list of `c_convex` objects with different seeds and sets the number of parameters
    for the amortized model:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Create a list of integers representing different seeds.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: ② Create an fs list that contains different instances of the c_convex class.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: ③ Set the number of parameters in the first c_convex object (fs[0]).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'The amortized model is represented as `nn.Module` in the following code. The
    neural network is defined as a feedforward neural network or a multilayer perceptron
    that consists of three fully connected (linear) layers with ReLU activation functions:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Number of parameters in the c-convex function that will be used as input to
    the neural network
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: ② Define the layers of the neural network in sequence.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: ③ Define the forward pass of the amortized model, which maps the input p (parameter
    vector) to a point on the sphere.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now train the amortized model to learn a mapping from parameter vectors
    to points on the sphere. It uses a list of c_convex functions (fs) with different
    random seeds to generate training data. The amortized model is trained using an
    Adam optimizer, and its progress is visualized using a tqdm progress bar. The
    resulting output points on the sphere are stored in a tensor xs:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Set the number of hidden units for the AmortizedModel neural network.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: ② Set the random seed to ensure the reproducibility of the training process.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: ③ Create an instance of the AmortizedModel.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: ④ Create an Adam optimizer to update the parameters with a learning rate of
    0.0005.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Store the output points on the sphere for each iteration of training.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Training loop
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Store the losses for each c_convex function and the corresponding output points
    on the sphere (xis).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Iterate over each c_convex function (f) in the list fs.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'After training is complete, all the predicted output points on the sphere are
    stacked along a new dimension, resulting in a tensor `xs` with the following shape:
    number of iterations, number of `c_convex` functions, 3\. Each element in this
    tensor represents a point on the sphere predicted by the amortized model at different
    stages of training. It generates a visual representation of the training progress
    for the amortized model and `c_convex` functions, as shown in figure 11.15.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F15_Khamis.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 Examples of output from the trained amortized model
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: The complete version of listing 11.3 is available in the book’s GitHub repo.
    It creates a grid of celestial coordinates, evaluates the `c_convex` functions
    and the amortized model on this grid, and then plots contour maps of the functions,
    the predicted paths, and the optimal points on the sphere. The optimal points
    are the points that give minimum loss, given that supervised learning is used
    to train the amortized model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 Solving TSP using supervised graph machine learning
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Joshi, Laurent, and Bresson, in their “Graph Neural Networks for the Travelling
    Salesman Problem” article [20], proposed a generic end-to-end pipeline to tackle
    combinatorial optimization problems such as the traveling salesman problem (TSP),
    vehicle routing problem (VRP), satisfiability problem (SAT), maximum cut (MaxCut),
    and maximal independent set (MIS). Figure 11.16 shows the steps of solving TSP
    using ML.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F16_Khamis.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 End-to-end pipeline for combinatorial optimization problems
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Following this approach, we start by defining the graph problem in the form
    of node features and an adjacency matrix between the nodes. A low-dimensional
    graph embedding is then generated using GNN or GCN, based on the message-passing
    approach. The probability of nodes or edges belonging to the solution is predicted
    using multilayer perceptrons (MLPs). A graph search, such as beam search (see
    chapter 4), is then applied to search the graph with the probability distribution
    over the edge to find a feasible candidate solution. Learning by imitation (supervised
    learning) and learning by exploration (reinforcement learning) are applied. Supervised
    learning minimizes the loss between optimal solutions (obtained by a well-known
    solver such as Concorde in the case of TSP) and the model’s prediction. The reinforcement
    learning approach uses a policy gradient to minimize the length of the tour predicted
    by the model at the end of decoding. Reinforcement learning is discussed in the
    next chapter.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Training an ML model from scratch and applying it to solve TSP requires a substantial
    amount of code and data preprocessing. Listing 11.4 shows how you can use the
    pretrained models to solve different instances of TSP. We start by importing the
    libraries and modules we’ll use. These libraries provide functionality for handling
    data, performing computations, visualization, and optimization. The Gurobi library
    is used to eliminate subtours during optimization and to calculate the reduce
    costs for a set of points (see appendix A). We set the `CUDA_DEVICE_ORDER` and
    `CUDA_VISIBLE_DEVICES` environment variables to control the GPU device visibility.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4 Solving TSP using supervised ML
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As a continuation, the following `opts` class contains several class-level
    attributes that define the following options and configurations:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset path`—The TSP dataset available in the book’s GitHub repo.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch size`—This determines the number of TSP instances (problems) processed
    simultaneously during training or evaluation. It specifies how many TSP instances
    are grouped together and processed in parallel.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number of samples`—This is the number of samples per TSP size.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neighbors`—This is used in the TSP data processing pipeline to specify the
    proportion (percentage) of nearest neighbors to consider for graph sparsification.
    It controls the connectivity of the TSP graph by selecting a subset of the nearest
    neighbors for each node.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`knn strategy`—This is the strategy used to determine the number of nearest
    neighbors when performing graph sparsification. In the code, the `''percentage''`
    value indicates that the number of nearest neighbors is determined by the `neighbors`
    parameter, which specifies the percentage of neighbors to consider.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`—This is the path for the pretrained ML model. The model used is a pretrained
    GNN model available in the book’s GitHub repo.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cuda`—This checks if CUDA is available on the system. CUDA is a parallel
    computing platform and programming model that allows for efficient execution of
    computations on NVIDIA GPUs. `torch.cuda.is_available()` returns a Boolean value
    (true or false) indicating whether CUDA is available or not. If CUDA is available,
    that means a compatible NVIDIA GPU is present on the system and can be utilized
    for accelerated computations.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device`—This is the device to be used for computations:'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next step is to create a dataset object using the TSP class with the following
    parameters:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '`filename`—The path or filename of the dataset to be used, specified by `opts
    .dataset_path`'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`—The number of samples to include in each batch, specified by `opts.batch_size`'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_samples`—The total number of samples to include in the dataset, specified
    by `opts.num_samples`'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neighbors`—The value representing the number of nearest neighbors for graph
    sparsification, specified by `opts.neighbors`'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`knn_strat`—The strategy for selecting nearest neighbors (`''percentage''`
    or `None`), specified by `opts.knn_strat`'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`supervised`—A Boolean value indicating whether the dataset is used for supervised
    learning, set to `True`'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `make_dataset` method creates an instance of the TSP dataset class and
    initializes it with the provided arguments, returning the `dataset` object:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following line creates a data loader object that enables convenient iteration
    over the dataset in batches, which is useful for processing the data during evaluation.
    The `dataset` object created in the previous line will be used as the source of
    the data. You can provide other optional arguments to customize the behavior of
    the data loader, such as `shuffle` (to shuffle the data) and `num_workers` (to
    specify the number of worker processes for data loading):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now load the trained model and assign it to the `model` variable. If
    the model is wrapped in `torch.nn.DataParallel`, it extracts the underlying module
    by accessing `model.module`. `DataParallel` is a PyTorch wrapper that allows for
    parallel execution of models on multiple GPUs. If the model is indeed an instance
    of `DataParallel`, it extracts the underlying model module by accessing the `module`
    attribute. This step is necessary to ensure consistent behavior when accessing
    model attributes and methods. The decode type of the model is then set to `"greedy"`.
    This means that during inference or evaluation, the model should use a greedy
    decoding strategy to generate output predictions:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Load a pretrained model.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: ② Extract the underlying module.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: ③ Set the decoding type of the model to "greedy".
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: ④ Set the model’s mode to evaluation
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: The complete version of listing 11.4, including the visualization code, is available
    in the book’s GitHub repo. Figure 11.17 shows the output produced by the pretrained
    ML model for the TSP50 instance.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F17_Khamis.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 The TSP50 solution using a pretrained ML model
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'The figure shows the following seven plots related to the TSP instance and
    the model’s predictions:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '*Concorde*—The plot in the upper-left corner shows the ground truth solution
    generated by the Concorde solver, which is an efficient implementation of the
    branch-and-cut algorithm for solving TSP instances to optimality. It shows the
    nodes of the TSP problem as circles connected by edges, representing the optimal
    tour calculated by Concorde. The title of the plot indicates the length (cost)
    of the tour obtained from Concorde.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1 - (Reduce Costs)*—The second plot contains the shortest subtour and shows
    the reduced costs for the points in these subtours using the Gurobi optimization
    library. It displays the edges of the TSP as red lines, with the edge color indicating
    the reduced cost value.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prediction Heatmap*—The third plot presents a heatmap visualization of the
    model’s predictions for the TSP problem. It uses a color scale to represent the
    prediction probabilities of edges, with higher probabilities shown in darker shades.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Greedy Solution*—The fourth plot illustrates the solution generated by the
    ML model using a greedy decoding strategy. It displays the nodes of the TSP problem
    connected by edges, representing the tour obtained from the model. The title of
    the plot shows the length (cost) of the tour calculated by the model.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Euclidean Distance (norm by max)*—The lower-left plot is a heatmap visualization
    of the Euclidean distances between nodes in the TSP problem. It uses a color scale
    to represent the distances, with lighter shades indicating smaller distances.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reduce Costs*—The lower-middle plot is a heatmap representation of the reduced
    costs of edges in the TSP problem. It shows the reduced costs as a color scale,
    with lower values displayed in lighter shades.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1 - (Model Predictions)*—The lower-right plot presents a heatmap visualization
    of the model’s predictions for the TSP problem, similar to the third plot. However,
    in this case, the heatmap displays “1 - (Model Predictions)” by subtracting the
    model’s prediction probabilities from 1\. Darker shades represent lower probabilities,
    indicating stronger confidence in the edge selection.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This example demonstrated how we can employ a pretrained GNN model for solving
    TSP. Figure 11.17 displays the model’s solution alongside the Concorde TSP solver’s
    results for a TSP instance comprising 50 points of interest. More information
    and complete code, including model training steps, are available in “Learning
    the Travelling Salesperson Problem Requires Rethinking Generalization” GitHub
    repo [21].
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 11.8 Solving TSP using unsupervised machine learning
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an example of an unsupervised ML approach, listing 11.5 shows how we can
    solve TSP using self-organizing maps (SOMs). We start by importing the libraries
    we’ll use. Some helper functions are imported from the som-tsp implementation
    described in Vicente’s blog post [22] to read the TSP instance, get the neighborhood,
    get the route, select the closest candidate, and calculate the route distance
    and plot the route. We read the TSP instance from the provided URL and obtain
    the cities and normalize their coordinates to a range of [0, 1].
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.5 Solving TSP using unsupervised learning
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Define the URL where the TSP instances are located.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: ② TSP instance
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ③ Download the file if it does not exist.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: ④ Read the TSP problem.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Obtain the normalized set of cities (with coordinates in [0,1]).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now set up various parameters and initialize a network of neurons for
    the SOM:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① The population size is 8 times the number of cities.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ② Set the number of iterations.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ③ Set the learning rate.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ④ Generate an adequate network of neurons.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation, the following code snippet implements the training loop
    for SOM. This loop iterates over the specified number of training iterations using
    `tqdm` to show a progress bar:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Store the lengths of the TSP routes during the SOM training iterations.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: ② Store the x and y coordinates of the neurons in the network during the training
    iterations.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: ③ Training loop
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: ④ Print only if the current iteration index is a multiple of 100.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Choose a random city.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Find the index of the neuron (winner) in the SOM network that is closest to
    the randomly chosen city.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Generate a filter that applies changes to the winner’s gaussian.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Update the network’s weights.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Append the current coordinates to the paths.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Decay the learning rate and the neighborhood radius n at each iteration to
    gradually reduce the influence of the Gaussian filter over time.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: ⑪ Check for the plotting interval.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: ⑫ Check if any parameter has completely decayed.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: ⑬ Calculate distance, and store it in the route_lengths list.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: ⑭ Indicate that the specified number of training iterations has been completed.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet plots the route length in each iteration.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Figure 11.18 shows the route length per iteration. The final route length is
    9,816, and the optimal length for the Qatar TSP instance used, `qa194.tsp`, is
    9,352\.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F18_Khamis.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 Route length per iteration of SOM for the Qatar TSP. The final
    route length is 9,816, and the optimal solution is 9,352.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: The complete version of listing 11.5 is available in the book’s GitHub repo,
    and it contains an implementation based on MiniSom. MiniSom is a minimalistic
    and Numpy-based implementation of SOM. You can install this library using `!pip
    install minisom`. However, the route obtained by MiniSom is 11,844.47, which is
    far from the optimal length of 9,352 for this TSP instance. To improve the result,
    you can experiment with the provided code and try to tune SOM parameters such
    as the number of neurons, the sigma, the learning rate, and the number of iterations.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 11.9 Finding a convex hull
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ptr-Net can be used to tackle the convex hull problem using a supervised learning
    approach, as described by Vinyals and his co-authors in their “Pointer networks”
    article [10]. Ptr-Net has two key components: an encoder and a decoder, as illustrated
    in figure 11.19\.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F19_Khamis.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 Solving the convex hull problem using Ptr-Net. The output in each
    step is a pointer to the input that maximizes the probability distribution.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: The encoder, a recurrent neural network (RNN), converts the raw input sequence.
    In this case, it coordinates delineating the points for which we want to determine
    the convex hull into a more manageable representation.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: This encoded vector is then passed on to the decoder. The vector acts as the
    modulator for a content-based attention mechanism, which is applied over the inputs.
    The content-based attention mechanism can be likened to a spotlight that highlights
    different segments of the input data at varying times, focusing on the most pertinent
    parts of the task at hand.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: The output of this attention mechanism is a softmax distribution with a dictionary
    size equal to the length of the input. This softmax distribution gives probabilities
    to every point in the input sequence. This setup allows Ptr-Net to probabilistically
    decide at each step which point should be added next to the convex hull. This
    is determined based on the current state of the input and the network’s internal
    state. The training process is repeated until the network has made a decision
    for every point, yielding a complete resolution to the convex hull problem.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 shows the steps for solving convex hull problem using pointer networks.
    We start by importing several necessary libraries and modules, such as torch,
    numpy, and matplotlib. The three helper classes `Data`, `ptr_net`, and `Disp`
    are imported based on the implementations provided in McGough’s “Pointer Networks
    with Transformers” article [23]. They contain functions for generating training
    and validation data, defining the pointer network architecture, and visualizing
    the results. This code generates two datasets for training and validation respectively.
    These datasets consist of random 2D points, where the number of points in each
    sample (the convex hull problem’s input) varies between `min_samples` and `max_samples`.
    `Scatter2DDataset` is a custom dataset class used to generate these random 2D
    point datasets.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 Solving a convex hull problem using pointer networks
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Running this code generates 100,000 training points and 1,000 validation points.
    We can then set the parameters of the pointer network. These parameters include
    a `TOKENS` dictionary containing the following tokens:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '`<eos>`—End-of-sequence token with the index 0'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c_inputs`—Number of input features for the model'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c_embed`—Number of embedding dimensions'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c_hidden`—Number of hidden units in the model'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_heads`—Number of attention heads in the multi-head self-attention mechanism'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_layers`—Number of layers in the model'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout`—Dropout probability, which is used for regularization'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cuda`—A Boolean flag indicating whether to use CUDA (GPU) if available
    or CPU'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_workers`—Number of worker threads for data loading in DataLoader'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The training parameters include `n_epochs` (number of training epochs), `batch_size`
    (batch size used during training), `lr` (learning rate for the optimizer), and
    `log_interval` (interval for logging training progress). The code checks if CUDA
    (GPU) is available and sets the `device` variable accordingly:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As a continuation, we load the training and validation data with the specified
    `batch_size` and `num_workers`:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `ConvexNet` model is a Ptr-Net model that is implemented as a transformer
    architecture with an encoder and decoder that use `nn.TransformerEncoderLayer`
    and apply multi-head self-attention. The complete code is available in the `ptr_net.py`
    class, available in the book’s GitHub repo. This model is initialized with the
    predefined hyperparameters. The `AverageMeter` class is used for keeping track
    of the average loss and accuracy during training and validation:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① Create a ConvexNet model.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: ② Use the Adam optimizer for training the model.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: ③ Use negative log-likelihood loss as the loss function.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: ④ Keep track of the average loss and accuracy during training and validation.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now perform the training and evaluation loop for a model (`ConvexNet`)
    using PyTorch. The model is being trained on the `train_loader` dataset with known
    labels and evaluated on the `val_loader` dataset:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ① Train the model.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterate over batches of training data.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: ③ Set the model’s parameters’ gradients to zero to avoid accumulation from previous
    batches.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: ④ Calculate the loss
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ A safeguard check to ensure that the loss value during the training process
    is not a NaN.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Perform a backward pass and optimization step.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Update training loss and accuracy.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Print the training progress.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation, the trained model (model) is evaluated on a validation dataset
    (val_dataset) to calculate the validation loss, accuracy, and overlap between
    the convex hull of the input data and the predicted pointer sequences. We start
    by setting the model to evaluation mode, where the model’s parameters are frozen
    and the batch normalization or dropout layers behave differently than during training.
    The code then iterates through the validation dataset using the val_loader, which
    provides batches of data (batch_data), ground truth labels (batch_labels), and
    the lengths of each sequence (batch_lengths):'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ① Set the model to evaluation mode.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: ② Initialize an empty list to store the overlap values between the convex hull
    of the input data and the predicted pointer sequences.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: ③ Iterate through the validation dataset.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: ④ Produce pointer scores and argmax predictions.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Calculate the validation loss.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Update the validation loss.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Ignore the loss contribution from positions where the <eos> token is present
    in batch_labels.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Calculate the masked accuracy.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Update the validation accuracy.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Iterate through each batch’s data, lengths, and pointer argmax predictions.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: ⑪ Calculate the overlap between the convex hull of the input data and the predicted
    pointer sequences.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: ⑫ Print the epoch-wise validation loss, accuracy, and mean overlap.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: ⑬ Reset the metrics.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'You can display the results of training and validation losses and accuracies
    using the `Disp_results` helper function:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding line of code will generate output like the following:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After model training and validation, we can test the model. The following test
    function will evaluate a trained model (`model`) on a test dataset. The function
    evaluates the model’s accuracy and overlap with the convex hull for different
    test sample sizes. This test function takes as inputs the model, the number of
    test samples, and the number of points per sample. The code performs the test
    for different numbers of points per sample (`i`) by iterating from 5 to 45 in
    steps of 5\. The `AverageMeter` class is used to keep track of average loss and
    accuracy during testing:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ① Set the number of test samples to be generated for each test.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: ② Test function
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: ③ Generate the test dataset.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: ④ Iterate through the batches of test data.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Track the loss and accuracy.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Update the overlap between the convex hull and predicted pointer sequences.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Print the accuracy and overlap.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Iterate and print results for different sample sizes.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'This code will produce output like the following:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s now test the trained model and see how well this model generalizes to
    new unseen data. We’ll use a dataset with 50 points to test the trained and validated
    model and calculate the convex hull overlap between the predicted hull and the
    ground truth hull obtained by SciPy. We pass the batch of input data and its lengths
    through the model to obtain the predicted scores (`log_pointer_scores`) and the
    argmax indices (`pointer_argmaxs`) of the pointer network. The ground truth is
    the convex hull obtained using the `ConvexHull` function from `scipy.spatial`:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① Set the number of points in each sample.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: ② Create a test dataset.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: ③ Load the first batch of data from the test dataset.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: ④ Obtain the predicted scores and the argmax indices of the pointer network.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Extract the predicted argmax indices for the selected sample from the batch.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Filter out the special tokens (e.g., <eos>) and adjust the indices for indexing
    the points correctly.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Extract and print the 2D points for the selected sample from the batch.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Ground truth convex hull
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Calculate the convex hull overlap.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Print the list of predicted convex hull indices, convex hull indices, and
    overlap percentage.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code will produce output like the following. You can run the preceding
    code snippets multiple times to get a high percentage of hull overlap:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following code snippet can be used to visualize the convex hull generated
    by the pointer network (`ConvexNet`) in comparison with the convex hull generated
    by `scipy.spatial` as a ground truth:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Set the default figure size, and create the first subplot.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: ② Compute the convex hull of a set of points (points) using the ConvexHull function
    from scipy.spatial.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: ③ Display the points and their convex hull in the first subplot.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: ④ Create a second subplot.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Display the points and the convex hull generated by ConvexNet.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.20 shows the convex hulls generated by SciPy and ConvexNet. These
    convex hulls are identical in some instances (i.e., hull overlap = 100.00%), yet
    achieving this consistency requires proper training and careful tuning of the
    ConvexNet parameters.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH11_F20_Khamis.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 Convex hulls generated by SciPy and Ptr-Net for 50 points
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has offered a fundamental foundation in ML and discussed the applications
    of supervised and unsupervised ML in handling optimization problems. The next
    chapter will focus on reinforcement learning and will delve deeply into its practical
    applications in tackling optimization problems.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning (ML), a branch of artificial intelligence (AI), grants an artificial
    system or process the capacity to learn from experiences and observations, rather
    than through explicit programming.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning (DL) is a subset of ML that is focused on the detection of inherent
    features within data by employing deep neural networks. This allows artificial
    systems to form intricate concepts from simpler ones.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric deep learning (GDL) extends (structured) deep neural models to handle
    non-Euclidean data with underlying geometric structures, such as graphs, point
    clouds, and manifolds.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph machine learning (GML) is a subfield of ML that focuses on developing
    algorithms and models capable of learning from graph-structured data.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph embedding represents the process of creating a conversion from the discrete,
    high-dimensional graph domain to a lower-dimensional continuous domain.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention mechanism allows a model to selectively focus on certain portions
    of the input data while it is in the process of generating the output sequence.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pointer network (Ptr-Net) is a variation of the sequence-to-sequence model
    with attention designed to deal with variable-sized input data sequences.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A self-organizing map (SOM), also known as a Kohonen map, is a type of artificial
    neural network (ANN) used for unsupervised learning. SOMs differ from other types
    of ANNs, as they apply competitive learning rather than error-correction learning
    (such as backpropagation with gradient descent).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural combinatorial optimization refers to the application of ML to solve combinatorial
    optimization problems.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Harnessing ML for combinatorial optimization can be achieved through three
    main methods: end-to-end learning where the model directly formulates solutions,
    using ML to configure and improve optimization algorithms, and integrating ML
    with optimization algorithms where the model continuously guides the optimization
    algorithm based on its current state.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
