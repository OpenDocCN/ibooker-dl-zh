<html><head></head><body><section data-pdf-bookmark="Chapter 3. Going Beyond the Basics: Detecting Features in Images" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">
      <h1><span class="label">Chapter 3. </span>Going Beyond the Basics: Detecting Features in Images</h1>
      <p>In <a data-type="xref" href="ch02.html#ch02_introduction_to_computer_vision_1748548889076080">Chapter 2</a>, you learned how<a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="about" data-type="indexterm" id="id739"/><a contenteditable="false" data-primary="feature detection" data-secondary="about" data-type="indexterm" id="id740"/> to get started with computer vision by creating a simple neural network that matched the input pixels of the Fashion MNIST dataset to 10 labels, each of which represented a type (or class) of clothing. And while you created a network that was pretty good at detecting clothing types, there was a clear drawback. Your neural network was trained on small monochrome images, each of which contained only a single item of clothing, and each item was centered within the image.</p>
      <p>To take the model to the next level, you need it to be able to detect <em>features</em> in images. So, for example, instead of looking merely at the raw pixels in the image, what if we could filter the images down to constituent elements? Matching those elements, instead of raw pixels, would help the model detect the contents of images more effectively. For example, consider the Fashion MNIST dataset that we used in the last chapter. When detecting a shoe, the neural network may have been activated by lots of dark pixels clustered at the bottom of the image, which it would see as the sole of the shoe. But if the shoe were not centered and filling the frame, this logic wouldn’t hold.</p>
      <p>One method of detecting features comes from photography and image processing methodologies that you may already be familiar with. If you’ve ever used a tool like Photoshop or GIMP to sharpen an image, you’ve used a mathematical filter that works on the pixels of the image. <a contenteditable="false" data-primary="convolutions" data-secondary="about" data-type="indexterm" id="id741"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="about" data-type="indexterm" id="id742"/><a contenteditable="false" data-primary="feature detection" data-secondary="convolutional neural network implemented" data-tertiary="about CNNs" data-type="indexterm" id="id743"/><a contenteditable="false" data-primary="feature detection" data-secondary="convolutions" data-tertiary="about convolutions" data-type="indexterm" id="id744"/><a contenteditable="false" data-primary="neural networks" data-secondary="convolutional neural network implemented" data-tertiary="about CNNs" data-type="indexterm" id="id745"/>Another word for what these filters do is <em>convolution</em>, and by using such filters in a neural network, you’ll create a <em>convolutional neural network</em> (CNN).</p>
      <p>In this chapter, you’ll start by learning about how to use convolutions to detect <span class="keep-together">features</span> in an image. Then, you’ll dig deeper into classifying images based on the <span class="keep-together">features</span> within. We’ll also explore augmentation of images to get more features and transfer learning to take preexisting features that were learned by others, and then we’ll look briefly into optimizing your models by using dropouts.</p>
      <section data-pdf-bookmark="Convolutions" data-type="sect1"><div class="sect1" id="ch03_convolutions_1748570891075168">
        <h1>Convolutions</h1>
        <p>A <em>convolution</em> is simply<a contenteditable="false" data-primary="convolutions" data-type="indexterm" id="id746"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="convolutions" data-type="indexterm" id="id747"/><a contenteditable="false" data-primary="weights" data-secondary="convolutions" data-type="indexterm" id="id748"/><a contenteditable="false" data-primary="feature detection" data-secondary="convolutions" data-type="indexterm" id="id749"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="convolutions" data-type="indexterm" id="id750"/> a filter of weights that are used to multiply a pixel by its neighbors to get a new value for the pixel. For example, consider the ankle boot image from Fashion MNIST and the pixel values for it (see <a data-type="xref" href="#ch03_figure_1_1748570891059985">Figure 3-1</a>).</p>
        <figure><div class="figure" id="ch03_figure_1_1748570891059985">
          <img alt="" src="assets/aiml_0301.png"/>
          <h6><span class="label">Figure 3-1. </span>Ankle boot with convolution</h6>
        </div></figure>
        <p>If we look at the pixel in the middle of the selection, we can see that it has the value 192. (Recall that Fashion MNIST uses monochrome images with pixel values from 0 to 255.) The pixel above and to the left has the value 0, the one immediately above has the value 64, etc.</p>
        <p>If we then define a filter in the same 3 × 3 grid, as shown below the original values, we can transform that pixel by calculating a new value for it. We do this by multiplying the current value of each pixel in the grid by the value in the same position in the filter grid and then summing up the total amount. This total will be the new value for the current pixel, and we then repeat this calculation for all pixels in the image.</p>
        <p>So, in this case, while the current value of the pixel in the center of the selection is 192, we calculate the new value after applying the filter as follows:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="n">new_val</code> <code class="o">=</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code> <code class="o">*</code> <code class="mi">0</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="mi">0</code> <code class="o">*</code> <code class="mi">64</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="err">–</code><code class="mi">2</code> <code class="o">*</code> <code class="mi">128</code><code class="p">)</code> <code class="o">+</code> 
     <code class="p">(</code><code class="mf">.5</code> <code class="o">*</code> <code class="mi">48</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="mf">4.5</code> <code class="o">*</code> <code class="mi">192</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="err">–</code><code class="mf">1.5</code> <code class="o">*</code> <code class="mi">144</code><code class="p">)</code> <code class="o">+</code> 
     <code class="p">(</code><code class="mf">1.5</code> <code class="o">*</code> <code class="mi">142</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="mi">2</code> <code class="o">*</code> <code class="mi">226</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="err">–</code><code class="mi">3</code> <code class="o">*</code> <code class="mi">168</code><code class="p">)</code></pre>
        <p>The result equals 577, which will be the new value for the pixel. Repeating this process for every pixel in the image will give us a filtered image.</p>
        <p>Now, let’s consider the impact of applying<a contenteditable="false" data-primary="SciPy" data-secondary="ascent image for convolutions" data-type="indexterm" id="id751"/> a filter on a more complicated image: specifically, the <a href="https://oreil.ly/wP8TE">ascent image</a> that’s built into SciPy for easy testing. This is a 512 × 512 grayscale image that shows two people climbing a <span class="keep-together">staircase.</span></p>
        <p>Using a filter with negative values on the left, positive values on the right, and zeros in the middle will end up removing most of the information from the image except for vertical lines (see <a data-type="xref" href="#ch03_figure_2_1748570891060023">Figure 3-2</a>).</p>
        <figure><div class="figure" id="ch03_figure_2_1748570891060023">
          <img alt="" src="assets/aiml_0302.png"/>
          <h6><span class="label">Figure 3-2. </span>Using a filter to derive vertical lines</h6>
        </div></figure>
        <p>Similarly, a small change to the filter can emphasize the horizontal lines (see <a data-type="xref" href="#ch03_figure_3_1748570891060045">Figure 3-3</a>).</p>
        <figure><div class="figure" id="ch03_figure_3_1748570891060045">
          <img alt="" src="assets/aiml_0303.png"/>
          <h6><span class="label">Figure 3-3. </span>Using a filter to derive horizontal lines</h6>
        </div></figure>
        <p>These examples also show that the amount of information in the image is reduced. Therefore, we can potentially <em>learn</em> a set of filters that reduce the image to features, and those features can be matched to labels as before. Previously, we learned parameters that were used in neurons to match inputs to outputs, and similarly, we can learn the best filters to match inputs to outputs over time.</p>
        <p>When we combine convolution<a contenteditable="false" data-primary="convolutions" data-secondary="pooling with" data-type="indexterm" id="id752"/><a contenteditable="false" data-primary="pooling" data-secondary="convolutions with" data-type="indexterm" id="id753"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="convolutions with pooling" data-type="indexterm" id="id754"/><a contenteditable="false" data-primary="feature detection" data-secondary="convolutions with pooling" data-type="indexterm" id="id755"/> with pooling, we can reduce the amount of information in the image while maintaining the features. We’ll explore that next.</p>
      </div></section>
      <section data-pdf-bookmark="Pooling" data-type="sect1"><div class="sect1" id="ch03_pooling_1748570891075225">
        <h1>Pooling</h1>
        <p><em>Pooling</em> is the process of <a contenteditable="false" data-primary="pooling" data-type="indexterm" id="id756"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="pooling" data-type="indexterm" id="id757"/><a contenteditable="false" data-primary="feature detection" data-secondary="pooling" data-type="indexterm" id="id758"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="pooling" data-type="indexterm" id="id759"/>eliminating pixels in your image while maintaining the semantics of the content within the image. It’s best explained visually. <a data-type="xref" href="#ch03_figure_4_1748570891060063">Figure 3-4</a> depicts<a contenteditable="false" data-primary="pooling" data-secondary="max pooling" data-type="indexterm" id="id760"/><a contenteditable="false" data-primary="max pooling" data-type="indexterm" id="id761"/> the concept of <em>max pooling</em>.</p>
        <figure><div class="figure" id="ch03_figure_4_1748570891060063">
          <img alt="" src="assets/aiml_0304.png"/>
          <h6><span class="label">Figure 3-4. </span>An example of max pooling</h6>
        </div></figure>
        <p>In this case, consider the box on the left to be the pixels in a monochrome image. We group them into 2 × 2 arrays, so in this case, the 16 pixels are grouped into four 2 × 2 arrays. <a contenteditable="false" data-primary="pooling" data-secondary="pools" data-type="indexterm" id="id762"/>These arrays are called <em>pools</em>.</p>
        <p>Then, we select the <em>maximum</em> value in each of the groups and reassemble them into a new image. Thus, the pixels on the left are reduced by 75% (from 16 to 4), with the maximum value from each pool making up the new image. <a data-type="xref" href="#ch03_figure_5_1748570891060080">Figure 3-5</a> shows the version<a contenteditable="false" data-primary="SciPy" data-secondary="ascent image with max pooling" data-type="indexterm" id="id763"/> of ascent from <a data-type="xref" href="#ch03_figure_2_1748570891060023">Figure 3-2</a>, with the vertical lines enhanced, after max pooling has been applied.</p>
                        <figure><div class="figure" id="ch03_figure_5_1748570891060080">
          <img alt="" src="assets/aiml_0305.png"/>
          <h6><span class="label">Figure 3-5. </span>Ascent after applying vertical filter and max pooling</h6>
        </div></figure>
        <p>Note how the filtered features have not just been maintained but have been further enhanced. Also, the image size has changed from 512 × 512 to 256 × 256—making it a quarter of the original size.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>There are other approaches to pooling.<a contenteditable="false" data-primary="pooling" data-secondary="min pooling" data-type="indexterm" id="id764"/><a contenteditable="false" data-primary="min pooling" data-type="indexterm" id="id765"/><a contenteditable="false" data-primary="pooling" data-secondary="average pooling" data-type="indexterm" id="id766"/><a contenteditable="false" data-primary="average pooling" data-type="indexterm" id="id767"/> These include <em>min pooling</em>, which takes the smallest pixel value from the pool, and <em>average pooling</em>, which takes the overall average value from the pool.</p>
        </div>
      </div></section>
      <section data-pdf-bookmark="Implementing Convolutional Neural Networks" data-type="sect1"><div class="sect1" id="ch03_implementing_convolutional_neural_networks_1748570891075280">
        <h1>Implementing Convolutional Neural Networks</h1>
        <p>In <a data-type="xref" href="ch02.html#ch02_introduction_to_computer_vision_1748548889076080">Chapter 2</a> you created a<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="implementing" data-type="indexterm" id="ch3cnn"/><a contenteditable="false" data-primary="neural networks" data-secondary="convolutional neural network implemented" data-type="indexterm" id="ch3cnn2"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="CNN implemented" data-type="indexterm" id="ch3cnn3"/><a contenteditable="false" data-primary="feature detection" data-secondary="convolutional neural network implemented" data-type="indexterm" id="ch3cnn4"/><a contenteditable="false" data-primary="pooling" data-secondary="convolutional neural network implemented" data-type="indexterm" id="ch3cnn5"/><a contenteditable="false" data-primary="convolutions" data-secondary="convolutional neural network implemented" data-type="indexterm" id="ch3cnn6"/> neural network that recognized fashion images. For convenience, here’s the code to define the model:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define the model</code>
<code class="k">class</code> <code class="nc">FashionMNISTModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">FashionMNISTModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">flatten</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear_relu_stack</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">10</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">LogSoftmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">flatten</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">logits</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">linear_relu_stack</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">logits</code>
 
<code class="n">model</code> <code class="o">=</code> <code class="n">FashionMNISTModel</code><code class="p">()</code>
 
<code class="c1"># Define the loss function and optimizer</code>
<code class="n">loss_function</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">NLLLoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>
 
<code class="c1"># Train the model</code>
<code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="n">dataloader</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">):</code>
    <code class="n">size</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">dataloader</code><code class="o">.</code><code class="n">dataset</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">batch</code><code class="p">,</code> <code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">dataloader</code><code class="p">):</code>
        <code class="c1"># Compute prediction and loss</code>
        <code class="n">pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
 
        <code class="c1"># Backpropagation</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
 
        <code class="k">if</code> <code class="n">batch</code> <code class="o">%</code> <code class="mi">100</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="n">loss</code><code class="p">,</code> <code class="n">current</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code> <code class="n">batch</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"loss: </code><code class="si">{</code><code class="n">loss</code><code class="si">:</code><code class="s2">&gt;7f</code><code class="si">}</code>  <code class="w"/>
                    <code class="p">[{</code><code class="n">current</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">5</code><code class="n">d</code><code class="p">}</code><code class="o">/</code><code class="p">{</code><code class="n">size</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">5</code><code class="n">d</code><code class="p">}]</code><code class="s2">")</code><code class="w"/>
 
<code class="c1"># Training process</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">5</code>
<code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Epoch </code><code class="si">{</code><code class="n">t</code><code class="o">+</code><code class="mi">1</code><code class="si">}</code><code class="se">\n</code><code class="s2">-------------------------------"</code><code class="p">)</code>
    <code class="n">train</code><code class="p">(</code><code class="n">train_loader</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_function</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Done!"</code><code class="p">)</code></pre>
        <p>To convert this to a CNN, you simply use convolutional layers in our model definition on top of the current linear ones. You’ll also add pooling layers.</p>
        <p>To implement a convolutional layer, you’ll use the <code>nn.Conv2D</code> type. It accepts as parameters the number of convolutions to use in the layer, the size of the convolutions, the activation function, etc.</p>
        <p>For example, here’s a convolutional layer that uses this type:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
        <p>In this case, we want the layer to learn <code>64</code> convolutions. It will randomly initialize them, and over time, it will learn the filter values that work best to match the input values to their labels. The <code>kernel_size = 3</code> indicates the size of the filter. Earlier, we showed you 3 × 3 filters, and that’s what we’re specifying here. The 3 × 3 filter is the most common size of filter. You can change it as you see fit, but you’ll typically see an odd number of axes like 5 × 5 or 7 × 7 because of how filters remove pixels from the borders of the image, as you’ll see later.</p>
        <p>Here’s how to use a pooling layer in the neural network. You’ll typically do this immediately after the convolutional layer:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code></pre>
        <p>In the example back in <a data-type="xref" href="#ch03_figure_4_1748570891060063">Figure 3-4</a>, we split the image into 2 × 2 pools and picked the maximum value in each. However, we could have used the parameters that you see here to define the pool size. The <code>kernel_size=2</code> parameter indicates that our pools are 2 × 2, and the <code>stride=2</code> parameter means that the filter will jump over two pixels to get the next pool.</p>
        <p>Now, let’s explore the full code to define a model for Fashion MNIST with a CNN:<a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="convolutional neural network" data-type="indexterm" id="id768"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="convolutional neural network" data-type="indexterm" id="id769"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define the CNN model</code>
<code class="k">class</code> <code class="nc">FashionCNN</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">FashionCNN</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">layer1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">2</code><code class="p">))</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">layer2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="mi">2</code><code class="p">))</code>  <code class="c1"># Output: 64 x 6 x 6</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">64</code> <code class="o">*</code> <code class="mi">6</code> <code class="o">*</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">128</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>  <code class="c1"># 10 classes</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">layer1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">layer2</code><code class="p">(</code><code class="n">out</code><code class="p">)</code>
        <code class="n">out</code> <code class="o">=</code> <code class="n">out</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="n">out</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code> <code class="err">–</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># Flatten the output</code>
        <code class="n">out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">out</code><code class="p">)</code>
        <code class="n">out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">out</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">out</code></pre>
        <p>Here, we see that the class has two functions, one for initialization and one that will be called during the forward pass in each epoch during training.</p>
        <p>The <code>init</code> simply defines what each of the layers in our neural network will look like. The first layer (<code>self.layer1</code>) will take in the one-dimensional input, have <code>64</code> convolutions, a <code>kernel_size</code> of <code>3</code>, and <code>padding</code> of <code>1</code>. It will then ReLU the output before max pooling it.</p>
        <p>The next layer (<code>self.layer2</code>) will take the 64 convolutions of output from the previous layer and then output <code>64</code> of its own before ReLUing them and max pooling them. Its output will now be <code>64 × 6 × 6</code> because the <code>MaxPool</code> halves the size of the image.</p>
        <p>The data is then fed to the next layer (<code>self.fc1</code>, where <code>fc</code> stands for <em>fully connected</em>), with the input being the shape of the output of the previous layer. The output is 128, which is the same number of neurons we used in <a data-type="xref" href="ch02.html#ch02_introduction_to_computer_vision_1748548889076080">Chapter 2</a> for the deep neural network (DNN). </p>
        <p>Finally, these 128 are fed into the final layer (<code>self.fc1</code>) with 10 outputs—that represent the 10 classes.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>In the DNN, we ran the input through a <code>Flatten</code> layer prior to feeding it into the first <code>Dense</code> layer. We’ve lost that in the input layer here—instead, we’ve just specified the 1-D input shape. Note that prior to the first <code>Linear</code> layer, after convolutions and pooling, the data will be flattened.</p>
        </div>
        <p>Then, we stack these layers in the <code>forward</code> function. We can see that we get the data <code>x</code> and pass it through <code>layer1</code> to get <code>out</code>, which is passed to <code>layer2</code> to get a new <code>out</code>. <span class="keep-together">At this point,</span> we have the convolutions that we’ve learned, but we need to flatten them before loading them into the <code>Linear</code> layers <code>fc1</code> and <code>fc2</code>. The <code>out = out.view(out.size(0), -1)</code> achieves this.</p>
        <p>If we train this network on the same data for the same 50 epochs as we used when training the network shown in <a data-type="xref" href="ch02.html#ch02_introduction_to_computer_vision_1748548889076080">Chapter 2</a>, we will see that it works nicely. We can get to 91% accuracy on the test set quite easily:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Train</code> <code class="n">Epoch</code><code class="p">:</code> <code class="mi">44</code> <code class="o">--</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.091689</code>
<code class="n">Train</code> <code class="n">Epoch</code><code class="p">:</code> <code class="mi">45</code> <code class="o">--</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.066864</code>
<code class="n">Train</code> <code class="n">Epoch</code><code class="p">:</code> <code class="mi">46</code> <code class="o">--</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.061322</code>
<code class="n">Train</code> <code class="n">Epoch</code><code class="p">:</code> <code class="mi">47</code> <code class="o">--</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.056557</code>
<code class="n">Train</code> <code class="n">Epoch</code><code class="p">:</code> <code class="mi">48</code> <code class="o">--</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.039695</code>
<code class="n">Train</code> <code class="n">Epoch</code><code class="p">:</code> <code class="mi">49</code> <code class="o">--</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.056213</code>
<code class="n">Accuracy</code> <code class="n">of</code> <code class="n">the</code> <code class="n">network</code> <code class="n">on</code> <code class="n">the</code> <code class="mi">10000</code> <code class="n">test</code> <code class="n">images</code><code class="p">:</code> <code class="mf">91.31</code><code class="o">%</code></pre>
        <p>So, we can see that adding convolutions to the neural network definitely increases its ability to classify images. Next, let’s take a look at the journey an image takes through the network so we can get a little bit more of an understanding of why this process works.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>If you are using the accompanying<a contenteditable="false" data-primary="model.to(device) for accelerator" data-type="indexterm" id="id770"/><a contenteditable="false" data-primary="PyTorch" data-secondary="model.to(device) for accelerator" data-type="indexterm" id="id771"/><a contenteditable="false" data-primary=".to(device) for accelerator" data-primary-sortas="to(device)" data-type="indexterm" id="id772"/><a contenteditable="false" data-primary="accelerators" data-secondary="model.to(device) for accelerator" data-type="indexterm" id="id773"/> code from my GitHub, you’ll notice that I’m using model.to(device) a lot. In PyTorch, if an accelerator is available, you can request that the model and/or its data use the accelerator with this command.<a contenteditable="false" data-primary="" data-startref="ch3cnn" data-type="indexterm" id="id774"/><a contenteditable="false" data-primary="" data-startref="ch3cnn2" data-type="indexterm" id="id775"/><a contenteditable="false" data-primary="" data-startref="ch3cnn3" data-type="indexterm" id="id776"/><a contenteditable="false" data-primary="" data-startref="ch3cnn4" data-type="indexterm" id="id777"/><a contenteditable="false" data-primary="" data-startref="ch3cnn5" data-type="indexterm" id="id778"/><a contenteditable="false" data-primary="" data-startref="ch3cnn6" data-type="indexterm" id="id779"/></p>
        </div>
      </div></section>
      <section data-pdf-bookmark="Exploring the Convolutional Network" data-type="sect1"><div class="sect1" id="ch03_exploring_the_convolutional_network_1748570891075332">
        <h1>Exploring the Convolutional Network</h1>
        <p>With the torchsummary library, you<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="exploring via torchsummary library" data-type="indexterm" id="ch3exp"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="CNN explored via torchsummary library" data-type="indexterm" id="ch3exp2"/><a contenteditable="false" data-primary="feature detection" data-secondary="convolutional neural network implemented" data-tertiary="CNN explored via torchsummary library" data-type="indexterm" id="ch3exp3"/><a contenteditable="false" data-primary="neural networks" data-secondary="convolutional neural network implemented" data-tertiary="exploring via torchsummary library" data-type="indexterm" id="ch3exp4"/><a contenteditable="false" data-primary="torchsummary library for exploring CNN" data-type="indexterm" id="ch3exp5"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="convolutional neural network" data-tertiary="exploring via torchsummary library" data-type="indexterm" id="ch3exp6"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="convolutional neural network" data-tertiary="exploring via torchsummary library" data-type="indexterm" id="ch3exp7"/> can inspect your model. When you run it on the Fashion MNIST convolutional network we’ve been working on, you’ll see something like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">torchsummary</code> <code class="kn">import</code> <code class="n">summary</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">FashionCNN</code><code class="p">()</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code> 
<code class="n">summary</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">input_size</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">))</code>  <code class="c1"># (Channels, Height, Width)</code>
<code class="o">----------------------------------------------------------------</code>
        <code class="n">Layer</code> <code class="p">(</code><code class="nb">type</code><code class="p">)</code>               <code class="n">Output</code> <code class="n">Shape</code>         <code class="n">Param</code> <code class="c1">#</code>
<code class="o">================================================================</code>
            <code class="n">Conv2d</code><code class="o">-</code><code class="mi">1</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]</code>             <code class="mi">640</code>
              <code class="n">ReLU</code><code class="o">-</code><code class="mi">2</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]</code>               <code class="mi">0</code>
         <code class="n">MaxPool2d</code><code class="o">-</code><code class="mi">3</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="mi">14</code><code class="p">,</code> <code class="mi">14</code><code class="p">]</code>               <code class="mi">0</code>
            <code class="n">Conv2d</code><code class="o">-</code><code class="mi">4</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="mi">12</code><code class="p">,</code> <code class="mi">12</code><code class="p">]</code>          <code class="mi">36</code><code class="p">,</code><code class="mi">928</code>
              <code class="n">ReLU</code><code class="o">-</code><code class="mi">5</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="mi">12</code><code class="p">,</code> <code class="mi">12</code><code class="p">]</code>               <code class="mi">0</code>
         <code class="n">MaxPool2d</code><code class="o">-</code><code class="mi">6</code>             <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">]</code>               <code class="mi">0</code>
            <code class="n">Linear</code><code class="o">-</code><code class="mi">7</code>                  <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">128</code><code class="p">]</code>         <code class="mi">295</code><code class="p">,</code><code class="mi">040</code>
            <code class="n">Linear</code><code class="o">-</code><code class="mi">8</code>                   <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">10</code><code class="p">]</code>           <code class="mi">1</code><code class="p">,</code><code class="mi">290</code>
<code class="o">================================================================</code>
<code class="n">Total</code> <code class="n">params</code><code class="p">:</code> <code class="mi">333</code><code class="p">,</code><code class="mi">898</code>
<code class="n">Trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">333</code><code class="p">,</code><code class="mi">898</code>
<code class="n">Non</code><code class="o">-</code><code class="n">trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">0</code>
<code class="o">----------------------------------------------------------------</code>
<code class="n">Input</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.00</code>
<code class="n">Forward</code><code class="o">/</code><code class="n">backward</code> <code class="k">pass</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">1.02</code>
<code class="n">Params</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">1.27</code>
<code class="n">Estimated</code> <code class="n">Total</code> <code class="n">Size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">2.30</code></pre>
        <p>Let’s first take a look at the <code>Output Shape</code> column to get an understanding of what’s going on here. Our first layer will have 28 × 28 images and apply 64 filters to them. But because our filter is 3 × 3, a one-pixel border around the image would typically be lost, reducing our overall information to 26 × 26 pixels. However, because we used the <code>padding=1</code> parameter, the image was artificially inflated to 30 × 30, meaning that its output would be the correct 28 × 28 and no information would be lost.</p>
        <p>If you don’t pad the image, you’ll end up with a result like the one in <a data-type="xref" href="#ch03_figure_6_1748570891060097">Figure 3-6</a>. If we take each of the boxes as a pixel in the image, the first possible filter we can use starts in the second row and the second column. The same would happen on the right side and at the bottom of the diagram.</p>
        <figure><div class="figure" id="ch03_figure_6_1748570891060097">
          <img alt="" src="assets/aiml_0306.png"/>
          <h6><span class="label">Figure 3-6. </span>Losing pixels when running a filter</h6>
        </div></figure>
        <p>Thus, an image that is <em>a</em> × <em>b</em> pixels in shape when run through a 3 × 3 filter will become (<em>a</em> – 2) × (<em>b</em> – 2) pixels in shape. Similarly, a 5 × 5 filter would make it (<em>a</em> – 4) × (<em>b</em> – 4), and so on. As we’re using a 28 × 28 image and a 3 × 3 filter, our output would now be 26 × 26. But because we padded the image up to 30 × 30 (again, to prevent loss of information), the output is now 28 × 28.</p>
        <p>After that, the pooling layer will be 2 × 2, so the size of the image will halve on each axis, and it will then become 14 × 14. The next convolutional layer does <em>not</em> use padding, so it will reduce this further to 12 × 12, and the next pooling will output 6 x 6.</p>
        <p>So, by the time the image has gone through two convolutional layers, the result will be many 6 × 6 images. How many? We can see that in the <code>Param #</code> (number of parameters) column.</p>
        <p>Each convolution is a 3 × 3 filter, plus a bias. Remember earlier, with our dense layers, when each layer was <em>y</em> = <em>wx</em> + <em>b</em>, where <em>w</em> was our parameter (aka weight) and <em>b</em> was our bias? This case is very similar, except that because the filter is 3 × 3, there are 9 parameters to learn. Given that we have 64 convolutions defined, we’ll have 640 overall parameters. (Each convolution has 9 parameters plus a bias, for a total of 10, and there are 64 of them.)</p>
        <p>The <code>ReLU and MaxPooling</code> layers don’t learn anything; they just reduce the image, so there are no learned parameters there—hence, 0 are reported.</p>
        <p>The next convolutional layer has 64 filters, but each of them is multiplied across the <em>previous</em> 64 filters, each of which has 9 parameters. We have a bias on each of the new 64 filters, so our number of parameters should be (64 × (64 × 9)) + 64, which gives us 36,928 parameters the network needs to learn.</p>
        <p>If this is confusing, try changing the number of convolutions in the first layer to something else—for example, 10. You’ll see that the number of parameters in the second layer becomes 5,824, which is (64 × (10 × 9)) + 64).</p>
        <p>By the time we get through the second convolution, our images are 6 × 6, and we have 64 of them. If we multiply this out, we’ll have 1,600 values, which we’ll feed into a dense layer of 128 neurons. Each neuron has a weight and a bias, and we’ll have 128 of them, so the number of parameters the network will learn is ((6 × 6 × 64) × 128) + 128, giving us 295,040 parameters.</p>
        <p>Then, our final dense layer of 10 neurons will take in the output of the previous 128, so the number of parameters learned will be (128 × 10) + 10, which is 1,290.</p>
        <p>The total number of parameters will be the sum of all of these: 333,898.</p>
        <p>Training this network requires us to learn the best set of these 333,898 parameters to match the input images to their labels. It’s a slower process because there are more parameters, but as we can see from the results, it also builds a more accurate model!</p>
        <p>Of course, with this dataset, we still have the limitation that the images are 28 × 28, monochrome, and centered. So next we’ll take a look at using convolutions to explore a more complex dataset comprising color pictures of horses and humans, and we’ll try to make the model determine whether an image contains one or the other. In this case, the subject won’t always be centered in the image like with Fashion MNIST, so we’ll have to rely on convolutions to spot distinguishing features.<a contenteditable="false" data-primary="" data-startref="ch3exp" data-type="indexterm" id="id780"/><a contenteditable="false" data-primary="" data-startref="ch3exp3" data-type="indexterm" id="id781"/><a contenteditable="false" data-primary="" data-startref="ch3exp4" data-type="indexterm" id="id782"/><a contenteditable="false" data-primary="" data-startref="ch3exp5" data-type="indexterm" id="id783"/><a contenteditable="false" data-primary="" data-startref="ch3exp6" data-type="indexterm" id="id784"/><a contenteditable="false" data-primary="" data-startref="ch3exp7" data-type="indexterm" id="id785"/></p>
      </div></section>
      <section data-pdf-bookmark="Building a CNN to Distinguish Between Horses and Humans" data-type="sect1"><div class="sect1" id="ch03_building_a_cnn_to_distinguish_between_horses_and_h_1748570891075390">
        <h1>Building a CNN to Distinguish Between <span class="keep-together">Horses and Humans</span></h1>
        <p>In this section, we’ll explore a more complex scenario than the Fashion MNIST classifier.<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="horses versus humans" data-type="indexterm" id="ch3hvh"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="horses versus humans" data-type="indexterm" id="ch3hvh2"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-type="indexterm" id="ch3hvh3"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-type="indexterm" id="ch3hvh4"/><a contenteditable="false" data-primary="humans versus horses distinguished" data-type="indexterm" id="ch3hvh5"/><a contenteditable="false" data-primary="classifiers" data-secondary="horses versus humans" data-type="indexterm" id="ch3hvh6"/> We’ll extend what we’ve learned about convolutions and CNNs to try to classify the contents of images in which the location of a feature isn’t always in the same place. I’ve created the “Horses or Humans” dataset for this purpose.</p>
        <section data-pdf-bookmark="The “Horses or Humans” Dataset" data-type="sect2"><div class="sect2" id="ch03_the_horses_or_humans_dataset_1748570891075446">
          <h2>The “Horses or Humans” Dataset</h2>
          <p> <a href="https://oreil.ly/8VXwy">The dataset for this section</a> contains over<a contenteditable="false" data-primary="datasets" data-secondary="horses and humans" data-type="indexterm" id="ch3handh"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="dataset" data-type="indexterm" id="ch3handh2"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="dataset" data-type="indexterm" id="ch3handh3"/> a thousand 300 × 300–pixel images. Approximately half the images are of horses, and the other half are of humans—and all are rendered in different poses. You can see some examples in <a data-type="xref" href="#ch03_figure_7_1748570891060112">Figure 3-7</a>.</p>
          <figure class="width-70"><div class="figure" id="ch03_figure_7_1748570891060112">
            <img alt="" src="assets/aiml_0307.png"/>
            <h6><span class="label">Figure 3-7. </span>Horses and humans</h6>
          </div></figure>
          <p>As you can see, the subjects have different orientations and poses, and the image composition varies. Consider the two horses, for example—their heads are oriented differently, and one image is zoomed out (showing the complete animal), while the other is zoomed in (showing just the head and part of the body). Similarly, the humans are lit differently, have different skin tones, and are posed differently. The man has his hands on his hips, while the woman has hers outstretched. The images also contain backgrounds such as trees and beaches, so a classifier will have to determine which parts of the image are the important features that determine what makes a horse a horse and a human a human, without being affected by the background.</p>
          <p>While the previous examples of predicting <em>y</em> = 2<em>x</em> – 1 or classifying small monochrome images of clothing <em>might</em> have been possible with traditional coding, it’s clear that this example is far more difficult and that you are crossing the line into where ML is essential to solve a problem.</p>
          <p>An interesting side note<a contenteditable="false" data-primary="datasets" data-secondary="horses and humans" data-tertiary="computer-generated images" data-type="indexterm" id="id786"/><a contenteditable="false" data-primary="datasets" data-secondary="computer-generated images for training" data-type="indexterm" id="id787"/><a contenteditable="false" data-primary="training" data-secondary="computer-generated images for" data-type="indexterm" id="id788"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="dataset" data-tertiary="computer-generated images" data-type="indexterm" id="id789"/> is that these images are all computer generated. The theory is that features spotted in a CGI image of a horse should apply to a real image, and you’ll see how well this works later in this chapter.<a contenteditable="false" data-primary="" data-startref="ch3handh" data-type="indexterm" id="id790"/><a contenteditable="false" data-primary="" data-startref="ch3handh2" data-type="indexterm" id="id791"/><a contenteditable="false" data-primary="" data-startref="ch3handh3" data-type="indexterm" id="id792"/></p>
        </div></section>
        <section data-pdf-bookmark="Handling the Data" data-type="sect2"><div class="sect2" id="ch03_handling_the_data_1748570891075497">
          <h2>Handling the Data</h2>
          <p>The Fashion MNIST dataset that<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="horses versus humans" data-tertiary="data preparation" data-type="indexterm" id="ch3prep"/><a contenteditable="false" data-primary="datasets" data-secondary="horses and humans" data-tertiary="data preparation" data-type="indexterm" id="ch3prep2"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="data preparation" data-type="indexterm" id="ch3prep3"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="data preparation" data-type="indexterm" id="ch3prep4"/><a contenteditable="false" data-primary="datasets" data-secondary="Fashion MNIST database" data-tertiary="labeled" data-type="indexterm" id="id793"/><a contenteditable="false" data-primary="datasets" data-secondary="horses and humans" data-tertiary="subdirectories instead of labels" data-type="indexterm" id="id794"/> you’ve been using up to this point comes with labels, and every image file has an associated file with the label details. Many image-based datasets do not have this, and “Horses or Humans” is no exception. <a contenteditable="false" data-primary="DataLoader" data-secondary="subdirectories assigned as image labels" data-type="indexterm" id="id795"/>Instead of labels, the images are sorted into subdirectories of each type, and with the DataLoader in PyTorch, you can use this structure to <em>automatically</em> assign labels to images.</p>
          <p>First, you simply need to ensure that your directory structure has a set of named subdirectories, with each subdirectory being a label. For example, the “Horses or Humans” dataset is available as a set of ZIP files, one of which contains the training data (1,000+ images) and another of which contains the validation data (256 images). When you download and unpack them into a local directory for training and validation, you need to ensure that they are in a file structure like the one in <a data-type="xref" href="#ch03_figure_8_1748570891060128">Figure 3-8</a>.</p>
          <p>Here’s the code to get the training data and extract it into the appropriately named subdirectories, as shown in <a data-type="xref" href="#ch03_figure_8_1748570891060128">Figure 3-8</a>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">urllib.request</code>
<code class="kn">import</code> <code class="nn">zipfile</code>
 
<code class="n">url</code> <code class="o">=</code> <code class="s2">"https://storage.googleapis.com/learning-datasets/</code><code class="w"/>
                                            <code class="n">horse</code><code class="o">-</code><code class="ow">or</code><code class="o">-</code><code class="n">human</code><code class="o">.</code><code class="n">zip</code><code class="s2">"</code><code class="w"/>
<code class="n">file_name</code> <code class="o">=</code> <code class="s2">"horse-or-human.zip"</code>
<code class="n">training_dir</code> <code class="o">=</code> <code class="s1">'horse-or-human/training/'</code>
<code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlretrieve</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">file_name</code><code class="p">)</code>
 
<code class="n">zip_ref</code> <code class="o">=</code> <code class="n">zipfile</code><code class="o">.</code><code class="n">ZipFile</code><code class="p">(</code><code class="n">file_name</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">)</code>
<code class="n">zip_ref</code><code class="o">.</code><code class="n">extractall</code><code class="p">(</code><code class="n">training_dir</code><code class="p">)</code>
<code class="n">zip_ref</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre>
          <figure><div class="figure" id="ch03_figure_8_1748570891060128">
            <img alt="" src="assets/aiml_0308.png"/>
            <h6><span class="label">Figure 3-8. </span>Ensuring that images are in named subdirectories</h6>
          </div></figure>
          <p>This code simply downloads the ZIP of the training data and unzips it into a directory at <em>horse-or-human/training</em>. (We’ll deal with downloading the validation data shortly.) This is the parent directory that will contain subdirectories for the image types.</p>
          <p>Now, to use the <code>DataLoader</code>, we simply use the following code:<a contenteditable="false" data-primary="DataLoader" data-secondary="human or horse dataset" data-type="indexterm" id="id796"/></p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">datasets</code><code class="p">,</code> <code class="n">transforms</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code>
 
<code class="c1"># Define transformations</code>
<code class="n">transform</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Resize</code><code class="p">((</code><code class="mi">150</code><code class="p">,</code> <code class="mi">150</code><code class="p">)),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">])</code>
<code class="p">])</code>
 
<code class="c1"># Load the datasets</code>
<code class="n">train_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">ImageFolder</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="n">training_dir</code><code class="p">,</code> 
                                     <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>
<code class="n">val_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">ImageFolder</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="n">validation_dir</code><code class="p">,</code> 
                                   <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>
 
<code class="c1"># Data loaders</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">val_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">val_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>
          <p>First, we create an instance of a <code>transforms</code> object that we’ll call <code>transform</code>. This will determine the rules for how we modify the images. It resizes the image to 150 × 150 and then normalizes it into a tensor. Note that the raw images are actually 300 × 300, but to make training quicker for the purposes of learning, I’ve resized them to 150 × 150.</p>
          <p>Then, we specify the <code>dataset</code> objects to be <code>datasets.ImageFolder</code> types and point them to the required directory, and that will generate images for the training process by flowing them from that directory while applying the transform. The directory for training is <code>training_dir</code>, and the directory for validation is <code>validation_dir</code>, as specified earlier.<a contenteditable="false" data-primary="" data-startref="ch3prep" data-type="indexterm" id="id797"/><a contenteditable="false" data-primary="" data-startref="ch3prep2" data-type="indexterm" id="id798"/><a contenteditable="false" data-primary="" data-startref="ch3prep3" data-type="indexterm" id="id799"/><a contenteditable="false" data-primary="" data-startref="ch3prep4" data-type="indexterm" id="id800"/> </p>
        </div></section>
        <section data-pdf-bookmark="CNN Architecture for “Horses or Humans”" data-type="sect2"><div class="sect2" id="ch03_cnn_architecture_for_horses_or_humans_1748570891075547">
          <h2>CNN Architecture for “Horses or Humans”</h2>
          <p>There are several major differences<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="horses versus humans" data-tertiary="CNN architecture" data-type="indexterm" id="ch3arch"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="CNN architecture" data-type="indexterm" id="ch3arch2"/><a contenteditable="false" data-primary="neural networks" data-secondary="convolutional neural network implemented" data-tertiary="CNN architecture for horses versus humans" data-type="indexterm" id="ch3arch3"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="CNN architecture" data-type="indexterm" id="ch3arch4"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="Fashion MNIST data versus horses and humans" data-type="indexterm" id="id801"/><a contenteditable="false" data-primary="datasets" data-secondary="Fashion MNIST versus horses and humans" data-type="indexterm" id="id802"/> between this dataset and the Fashion MNIST one, and you have to take them into account when designing an architecture for classifying the images. First, the images are much larger—150 × 150 pixels—so more layers may be needed. Second, the images are in full color, not grayscale, so each image will have three channels instead of one. Third, there are only two image types, so we can actually classify them with only <em>one</em> output neuron. To do this, we’ll drive the value of that neuron toward 0 for one of the labels and toward 1 for the other. The <code>sigmoid</code> function is ideal for this process of driving the value to one of these extremes. You can see this at the bottom of the <code>forward</code> function:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">HorsesHumansCNN</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">HorsesHumansCNN</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">conv1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">16</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">conv2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">conv3</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">pool</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MaxPool2d</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">64</code> <code class="o">*</code> <code class="mi">18</code> <code class="o">*</code> <code class="mi">18</code><code class="p">,</code> <code class="mi">512</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">drop</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.25</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">512</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">pool</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">conv1</code><code class="p">(</code><code class="n">x</code><code class="p">)))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">pool</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">conv2</code><code class="p">(</code><code class="n">x</code><code class="p">)))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">pool</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">conv3</code><code class="p">(</code><code class="n">x</code><code class="p">)))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code> <code class="o">*</code> <code class="mi">18</code> <code class="o">*</code> <code class="mi">18</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>  <code class="c1"># Use sigmoid to output probabilities</code>
        <code class="k">return</code> <code class="n">x</code></pre>
          <p>There are a number of things to note here. First of all, take a look at the very first layer. We’re defining 16 filters, each of which has a <code>kernel_size</code> of 3, but the input shape is 3. Remember that this is because our input image is in color: there are three channels, instead of just one for the monochrome Fashion MNIST dataset we were using earlier.</p>
          <p>At the other end, notice that<a contenteditable="false" data-primary="binary classification via single neuron" data-type="indexterm" id="id803"/><a contenteditable="false" data-primary="neurons" data-secondary="binary classification via single neuron" data-type="indexterm" id="id804"/><a contenteditable="false" data-primary="sigmoid function" data-type="indexterm" id="id805"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="single output neuron so only binary" data-type="indexterm" id="id806"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="horses versus humans single output neuron" data-type="indexterm" id="id807"/> there’s only one neuron in the output layer. This is because we’re using a binary classifier, and we can get a binary classification with just a single neuron if we activate it with a sigmoid function. The purpose of the sigmoid function is to drive one set of values toward 0 and the other toward 1, which is perfect for binary classification.</p>
          <p>Next, notice how we stack several more convolutional layers. We do this because our image source is quite large and we want, over time, to have many smaller images, each with features highlighted. If we take a look at the results of a <code>summary</code>, we’ll see this in action:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="o">----------------------------------------------------------------</code>
        <code class="n">Layer</code> <code class="p">(</code><code class="nb">type</code><code class="p">)</code>               <code class="n">Output</code> <code class="n">Shape</code>         <code class="n">Param</code> <code class="c1">#</code>
<code class="o">================================================================</code>
            <code class="n">Conv2d</code><code class="o">-</code><code class="mi">1</code>         <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">16</code><code class="p">,</code> <code class="mi">150</code><code class="p">,</code> <code class="mi">150</code><code class="p">]</code>             <code class="mi">448</code>
         <code class="n">MaxPool2d</code><code class="o">-</code><code class="mi">2</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">16</code><code class="p">,</code> <code class="mi">75</code><code class="p">,</code> <code class="mi">75</code><code class="p">]</code>               <code class="mi">0</code>
            <code class="n">Conv2d</code><code class="o">-</code><code class="mi">3</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">75</code><code class="p">,</code> <code class="mi">75</code><code class="p">]</code>           <code class="mi">4</code><code class="p">,</code><code class="mi">640</code>
         <code class="n">MaxPool2d</code><code class="o">-</code><code class="mi">4</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">37</code><code class="p">,</code> <code class="mi">37</code><code class="p">]</code>               <code class="mi">0</code>
            <code class="n">Conv2d</code><code class="o">-</code><code class="mi">5</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="mi">37</code><code class="p">,</code> <code class="mi">37</code><code class="p">]</code>          <code class="mi">18</code><code class="p">,</code><code class="mi">496</code>
         <code class="n">MaxPool2d</code><code class="o">-</code><code class="mi">6</code>           <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="mi">18</code><code class="p">,</code> <code class="mi">18</code><code class="p">]</code>               <code class="mi">0</code>
            <code class="n">Linear</code><code class="o">-</code><code class="mi">7</code>                  <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">512</code><code class="p">]</code>      <code class="mi">10</code><code class="p">,</code><code class="mi">617</code><code class="p">,</code><code class="mi">344</code>
           <code class="n">Dropout</code><code class="o">-</code><code class="mi">8</code>                  <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">512</code><code class="p">]</code>               <code class="mi">0</code>
            <code class="n">Linear</code><code class="o">-</code><code class="mi">9</code>                    <code class="p">[</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>             <code class="mi">513</code>
<code class="o">================================================================</code>
<code class="n">Total</code> <code class="n">params</code><code class="p">:</code> <code class="mi">10</code><code class="p">,</code><code class="mi">641</code><code class="p">,</code><code class="mi">441</code>
<code class="n">Trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">10</code><code class="p">,</code><code class="mi">641</code><code class="p">,</code><code class="mi">441</code>
<code class="n">Non</code><code class="o">-</code><code class="n">trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">0</code>
<code class="o">----------------------------------------------------------------</code>
<code class="n">Input</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.26</code>
<code class="n">Forward</code><code class="o">/</code><code class="n">backward</code> <code class="k">pass</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">5.98</code>
<code class="n">Params</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">40.59</code>
<code class="n">Estimated</code> <code class="n">Total</code> <code class="n">Size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">46.83</code>
<code class="o">----------------------------------------------------------------</code></pre>
          <p>Note that by the time the data has gone through all the convolutional and pooling layers, it ends up as 18 × 18 items. The theory is that these will be activated feature maps that are relatively simple because they will contain just 324 pixels. We can then pass these feature maps to the dense neural network to match them to the appropriate labels.</p>
          <p>This, of course, leads this network to have many more parameters than the previous network, so it will be slower to train. With this architecture, we’re going to learn over 10 million parameters.</p>
          <div data-type="tip"><h6>Tip</h6>
            <p>The code in this section, as well as in many<a contenteditable="false" data-primary="Python" data-secondary="libraries in book repository" data-type="indexterm" id="id808"/><a contenteditable="false" data-primary="book code online" data-secondary="Python libraries" data-type="indexterm" id="id809"/><a contenteditable="false" data-primary="online resources" data-secondary="code for book" data-tertiary="Python libraries" data-type="indexterm" id="id810"/> other places in this book, may require you to import Python libraries. To find the correct imports, you can check out <a href="https://github.com/lmoroney/PyTorch-Book-FIles">the book’s repository</a>.</p>
          </div>
          <p>To train the network, we’ll have to compile it with a loss function and an optimizer.<a contenteditable="false" data-primary="loss functions" data-secondary="BCELoss" data-type="indexterm" id="id811"/><a contenteditable="false" data-primary="BCELoss (binary cross entropy) loss function" data-type="indexterm" id="id812"/><a contenteditable="false" data-primary="binary cross entropy (BCELoss) loss function" data-type="indexterm" id="id813"/> In this case, the loss function can be the <code>BCELoss,</code> where <code>BCE</code> stands for <em>binary cross entropy</em>. As the name suggests, because there are only two classes in this scenario, this is a loss function that is designed for it. <a contenteditable="false" data-primary="optimizers" data-secondary="Adam" data-type="indexterm" id="id814"/><a contenteditable="false" data-primary="Adam optimizer" data-type="indexterm" id="id815"/>For the <code>optimizer</code>, we can continue using the same <code>Adam</code> that we used earlier. Here’s the code:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">)</code></pre>
          <p>We then train in the usual way: </p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">train_model</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">):</code>
        <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
        <code class="n">running_loss</code> <code class="o">=</code> <code class="mf">0.0</code>
        <code class="k">for</code> <code class="n">images</code><code class="p">,</code> <code class="n">labels</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
            <code class="n">images</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">images</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="n">labels</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>  
            <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
            <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">images</code><code class="p">)</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">)</code>
            <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">outputs</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>
            <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
            <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
            <code class="n">running_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()</code></pre>
          <p>One thing to note is that the labels are converted to <code>floats</code> because of the binary cross entropy, where the value of the final output node will be a float value. </p>
          <p>Over just 15 epochs, this architecture gives us a very impressive 95%+ accuracy on the training set. Of course, this is just with the training data, and this performance isn’t an indication of the network’s potential performance on data that it hasn’t previously seen.</p>
          <p>Next, we’ll look at adding the validation set and measuring its performance to give us a good indication of how this model might perform in real life.<a contenteditable="false" data-primary="" data-startref="ch3arch" data-type="indexterm" id="id816"/><a contenteditable="false" data-primary="" data-startref="ch3arch2" data-type="indexterm" id="id817"/><a contenteditable="false" data-primary="" data-startref="ch3arch3" data-type="indexterm" id="id818"/><a contenteditable="false" data-primary="" data-startref="ch3arch4" data-type="indexterm" id="id819"/></p>
        </div></section>
        <section data-pdf-bookmark="Adding Validation to the “Horses or Humans” Dataset" data-type="sect2"><div class="sect2" id="ch03_adding_validation_to_the_horses_or_humans_datase_1748570891075599">
          <h2>Adding Validation to the “Horses or Humans” Dataset</h2>
          <p>To add validation, you’ll need<a contenteditable="false" data-primary="datasets" data-secondary="training, validation, and testing" data-type="indexterm" id="id820"/><a contenteditable="false" data-primary="datasets" data-secondary="horses and humans" data-tertiary="validation dataset" data-type="indexterm" id="id821"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="horses versus humans" data-tertiary="validation during training" data-type="indexterm" id="ch3val"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="validation during training" data-type="indexterm" id="ch3val2"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="validation during training" data-type="indexterm" id="ch3val3"/><a contenteditable="false" data-primary="validation" data-secondary="horses versus humans dataset" data-type="indexterm" id="ch3val4"/> a validation dataset that’s separate from the training one. In some cases, you’ll get a master dataset that you have to split yourself, but in the case of “Horses or Humans,” there’s a separate validation set that you can download. In the preceding code snippet, you’ve already downloaded the training and validation datasets, put them in directories, and set up data loaders for each of them. However, for training, you only used one of these datasets—the one that was set up to load the training data. So next, we’ll switch the model into evaluation mode and explore how well it did with the validation data.</p>
          <div data-type="note" epub:type="note"><h6>Note</h6>
            <p>You may be wondering why<a contenteditable="false" data-primary="testing" data-secondary="validation versus" data-type="indexterm" id="id822"/><a contenteditable="false" data-primary="validation" data-secondary="testing versus" data-type="indexterm" id="id823"/><a contenteditable="false" data-primary="training" data-secondary="validation during, testing after" data-type="indexterm" id="id824"/> we’re talking about a validation dataset here, rather than a test dataset, and whether the two are the same thing. For simple models like the ones developed in the previous chapters, it’s often sufficient to split the dataset into two parts: one for training and one for testing. But for more complex models like the one we’re building here, you’ll want to create separate validation and test sets. </p>
            <p>What’s the difference? <em>Training data</em> is the data that is used to teach the network how the data and labels fit together, while <em>validation data</em> is used to see how the network is doing with previously unseen data <em>while</em> you are training (i.e., it isn’t used to fit data to labels but to inspect how well the fitting is going). Also, <em>test data</em> is used after training to evaluate how the network does with data it has never previously seen. Some datasets come with a three-way split, and in other cases, you’ll want to separate the test set into two parts for validation and testing. Here, you’ll download some additional images for testing the model.</p>
          </div>
          <p>To download the validation set and unzip it into a different directory, you can use code that’s very similar to that used for the training images.</p>
          <p>Then, to perform the validation, you simply update your <code>train_model</code> method to perform a validation at the end of each training loop (or epoch) and report on the results. For example, you can do this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">train_model</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">):</code>
        <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
        <code class="n">running_loss</code> <code class="o">=</code> <code class="mf">0.0</code>
        <code class="k">for</code> <code class="n">images</code><code class="p">,</code> <code class="n">labels</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
            <code class="n">images</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">images</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="n">labels</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>  
            <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
            <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">images</code><code class="p">)</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">)</code>
            <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">outputs</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>
            <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
            <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
            <code class="n">running_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
 
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Epoch </code><code class="si">{</code><code class="n">epoch</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s1">, Loss: </code><code class="si">{</code><code class="n">running_loss</code> <code class="o">/</code> 
                <code class="nb">len</code><code class="p">(</code><code class="n">train_loader</code><code class="p">)</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
 
    <code class="c1"># Evaluate on training set</code>
        <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
        <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
            <code class="n">correct</code> <code class="o">=</code> <code class="mi">0</code>
            <code class="n">total</code> <code class="o">=</code> <code class="mi">0</code>
            <code class="k">for</code> <code class="n">images</code><code class="p">,</code> <code class="n">labels</code> <code class="ow">in</code> <code class="n">train_loader</code><code class="p">:</code>
                <code class="n">images</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">images</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> 
                                 <code class="n">labels</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
                <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">images</code><code class="p">)</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">)</code>
                <code class="n">predicted</code> <code class="o">=</code> <code class="n">outputs</code> <code class="o">&gt;</code> <code class="mf">0.5</code>  <code class="c1"># Threshold predictions</code>
                <code class="n">total</code> <code class="o">+=</code> <code class="n">labels</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
                <code class="n">correct</code> <code class="o">+=</code> <code class="p">(</code><code class="n">predicted</code> <code class="o">==</code> <code class="n">labels</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Test Set Accuracy: </code><code class="si">{</code><code class="mi">100</code> <code class="o">*</code> <code class="n">correct</code> <code class="o">/</code> <code class="n">total</code><code class="si">}</code><code class="s1">%'</code><code class="p">)</code>
 
        <code class="c1"># Evaluate on validation set</code>
        <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
        <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
            <code class="n">correct</code> <code class="o">=</code> <code class="mi">0</code>
            <code class="n">total</code> <code class="o">=</code> <code class="mi">0</code>
            <code class="k">for</code> <code class="n">images</code><code class="p">,</code> <code class="n">labels</code> <code class="ow">in</code> <code class="n">val_loader</code><code class="p">:</code>
                <code class="n">images</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">images</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> 
                                 <code class="n">labels</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
                <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">images</code><code class="p">)</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">)</code>
                <code class="n">predicted</code> <code class="o">=</code> <code class="n">outputs</code> <code class="o">&gt;</code> <code class="mf">0.5</code>  <code class="c1"># Threshold predictions</code>
                <code class="n">total</code> <code class="o">+=</code> <code class="n">labels</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
                <code class="n">correct</code> <code class="o">+=</code> <code class="p">(</code><code class="n">predicted</code> <code class="o">==</code> <code class="n">labels</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Validation Set Accuracy: </code><code class="si">{</code><code class="mi">100</code> <code class="o">*</code> <code class="n">correct</code> <code class="o">/</code> <code class="n">total</code><code class="si">}</code><code class="s1">%'</code><code class="p">)</code>    
 
<code class="n">train_model</code><code class="p">(</code><code class="mi">50</code><code class="p">)</code></pre>
          <p>I added code here to do <em>both</em> the training and the validation and report on accuracy. Note that this is really just for learning purposes, so you can compare. In a real-world scenario, checking the accuracy of training data is a waste of processing time!</p>
          <p>After training for 10 epochs, you should see that your model is 99%+ accurate on the training set but only about 88% on the validation set: </p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">Epoch</code> <code class="mi">7</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.0016404045829699512</code>
<code class="n">Training</code> <code class="n">Set</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">100.0</code><code class="o">%</code>
<code class="n">Validation</code> <code class="n">Set</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.28125</code><code class="o">%</code>
<code class="n">Epoch</code> <code class="mi">8</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.0010613293736610378</code>
<code class="n">Training</code> <code class="n">Set</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">100.0</code><code class="o">%</code>
<code class="n">Validation</code> <code class="n">Set</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">89.0625</code><code class="o">%</code>
<code class="n">Epoch</code> <code class="mi">9</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.0008372313717332979</code>
<code class="n">Training</code> <code class="n">Set</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">100.0</code><code class="o">%</code>
<code class="n">Validation</code> <code class="n">Set</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">86.328125</code><code class="o">%</code>
<code class="n">Epoch</code> <code class="mi">10</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.0006578459407812646</code>
<code class="n">Training</code> <code class="n">Set</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">100.0</code><code class="o">%</code>
<code class="n">Validation</code> <code class="n">Set</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">87.5</code><code class="o">%</code></pre>
          <p>This is an indication that the model is overfitting, which is something we also saw in the previous chapter.<em> </em>It’s easy to be lulled into a false sense of security by the 100% accuracy, but the other figure is more representative of how your model will behave in the real world.</p>
          <p>Still, the performance isn’t bad, considering how few images it was trained on and how diverse those images were. You’re beginning to hit a wall caused by lack of data, but there are some techniques that you can use to improve your model’s performance. We’ll explore them later in this chapter, but before that, let’s take a look at how to <em>use</em> this model.<a contenteditable="false" data-primary="" data-startref="ch3val" data-type="indexterm" id="id825"/><a contenteditable="false" data-primary="" data-startref="ch3val2" data-type="indexterm" id="id826"/><a contenteditable="false" data-primary="" data-startref="ch3val3" data-type="indexterm" id="id827"/><a contenteditable="false" data-primary="" data-startref="ch3val4" data-type="indexterm" id="id828"/></p>
        </div></section>
        <section data-pdf-bookmark="Testing “Horses or Humans” Images" data-type="sect2"><div class="sect2" id="ch03_testing_horses_or_humans_images_1748570891075649">
          <h2>Testing “Horses or Humans” Images</h2>
          <p>It’s all very well and good to be able<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="horses versus humans" data-tertiary="testing on images" data-type="indexterm" id="ch3tst"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="testing on images" data-type="indexterm" id="ch3tst2"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="testing on images" data-type="indexterm" id="ch3tst3"/><a contenteditable="false" data-primary="testing" data-secondary="horses versus humans model on images" data-type="indexterm" id="ch3tst4"/> to build a model, but of course, you want to try it out. A major frustration of mine when I was starting my AI journey was that I could find lots of code that showed me how to build models and charts of how those models were performing, but very rarely was there code to help me kick the tires of the model myself to try it out. I’ll try to help you avoid that problem in this book!</p>
          <p>Testing the model is perhaps <a contenteditable="false" data-primary="online resources" data-secondary="code for book" data-tertiary="testing horses versus humans Colab notebook" data-type="indexterm" id="id829"/><a contenteditable="false" data-primary="book code online" data-secondary="testing horses versus humans Colab notebook" data-type="indexterm" id="id830"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="testing on images" data-tertiary="Colab notebook on GitHub" data-type="indexterm" id="id831"/><a contenteditable="false" data-primary="testing" data-secondary="horses versus humans model on images" data-tertiary="Colab notebook on GitHub" data-type="indexterm" id="id832"/><a contenteditable="false" data-primary="Google Colab" data-secondary="testing horses versus humans notebook" data-type="indexterm" id="id833"/><a contenteditable="false" data-primary="GitHub" data-secondary="testing horses versus humans Colab notebook" data-type="indexterm" id="id834"/>easiest using Colab. I’ve provided a “Horses or Humans” notebook on GitHub that you can open directly in <a href="http://bit.ly/horsehuman">Colab</a>.</p>
          <p>Once you’ve trained the model, you’ll see a section called “Running the Model.” Before running it, you should <a contenteditable="false" data-primary="datasets" data-secondary="images on Pixabay.com" data-type="indexterm" id="id835"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="testing on images" data-tertiary="images on Pixabay.com" data-type="indexterm" id="id836"/><a contenteditable="false" data-primary="online resources" data-secondary="images on Pixabay.com" data-type="indexterm" id="id837"/><a contenteditable="false" data-primary="testing" data-secondary="horses versus humans model on images" data-tertiary="images on Pixabay.com" data-type="indexterm" id="id838"/>find a few pictures of horses or humans online and download them to your computer. I recommend you go to <a href="http://pixabay.com">Pixabay.com</a>, which is a really good site to check out for royalty-free images. It’s also a good idea to get your test images together first, because the node can time out while you’re searching.</p>
          <p><a data-type="xref" href="#ch03_figure_9_1748570891060144">Figure 3-9</a> shows a few pictures of horses and humans that I downloaded from Pixabay to test the model.</p>
          <figure><div class="figure" id="ch03_figure_9_1748570891060144">
            <img alt="" src="assets/aiml_0309.png"/>
            <h6><span class="label">Figure 3-9. </span>Test images</h6>
          </div></figure>
          <p>When they were uploaded, as you can see in <a data-type="xref" href="#ch03_figure_10_1748570891060159">Figure 3-10</a>, the model <a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="testing on images" data-tertiary="incorrect classification" data-type="indexterm" id="id839"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="incorrect classification" data-type="indexterm" id="id840"/>correctly classified one image as a human and another as a horse—but despite the fact that the third image was obviously of a human, the model incorrectly classified it as a horse!</p>
          <p>You can also upload multiple images simultaneously and have the model make predictions for all of them. You may also notice that it tends to overfit toward horses. If the human isn’t fully posed (i.e., if you can’t see their full body), the model can skew toward horses. That’s what happened in this case. The first human model is fully posed, and the image resembles many of the poses in the dataset, so the model was able to classify her correctly. On the other hand, the second human model is facing the camera, but only her upper half is in the image. There was no training data that looked like that, so the model couldn’t correctly identify her.</p>
                    <figure><div class="figure" id="ch03_figure_10_1748570891060159">
            <img src="assets/aiml_0310.png"/>
            <h6><span class="label">Figure 3-10. </span>Executing the model</h6>
          </div></figure>
          <p>Let’s now explore the code to see what it’s doing. Perhaps the most important part is this chunk:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">load_image</code><code class="p">(</code><code class="n">image_path</code><code class="p">,</code> <code class="n">transform</code><code class="p">):</code>
    <code class="c1"># Load image</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">image_path</code><code class="p">)</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s1">'RGB'</code><code class="p">)</code>  <code class="c1"># Convert to RGB</code>
    <code class="c1"># Apply transformations</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">transform</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
    <code class="c1"># Add batch dimension, as the model expects batches</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">image</code></pre>
          <p>Here, we are loading the image from the path that Colab wrote it to. Note that we specify a <code>transform</code> to apply to the image. <a contenteditable="false" data-primary="training" data-secondary="test images same size as training images" data-type="indexterm" id="id841"/><a contenteditable="false" data-primary="testing" data-secondary="training images same size as test images" data-type="indexterm" id="id842"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="test images same size as training images" data-type="indexterm" id="id843"/>The images being uploaded can be any shape, but if we are going to feed them into the model, they <em>must</em> be the same size that the model was trained on. So, if we use the same <code>transform</code> that we defined when performing the training, we’ll know it’s in the same dimensions.</p>
          <p>At the end is this strange command: <code>image = image.unsqueeze(0)</code>. </p>
          <p>When you look back at how the model was trained,<a contenteditable="false" data-primary="DataLoader" data-secondary="human or horse dataset" data-type="indexterm" id="id844"/> the DataLoader objects batched the images going into it. If you think of an image as a 2D array of pixels, then the batch is an array of 2D arrays, which of course is then a 3D array. </p>
          <p>But when we’re using this code with one image at a time, there’s no batch, so to make this a 3D array (which is technically a batch with one item in it), we can just unsqueeze the image along axis 0 to simulate this.</p>
          <p>With our image in the right format, it’s easy to do the classification:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">image</code><code class="p">)</code></pre>
          <p>The model then returns an array containing the classifications for the batch. Because there’s only one classification in this case, it’s effectively an array containing an array. You can see this back in <a data-type="xref" href="#ch03_figure_10_1748570891060159">Figure 3-10</a>, where for the first human model, the array looks like <code>tensor([[2.1368e-05]], device='cuda:0').</code></p>
          <p>So now, it’s simply a matter of inspecting the value of the first element in that array. If it’s greater than 0.5, we’re looking at a human:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">class_name</code> <code class="o">=</code> <code class="s2">"Human"</code> <code class="k">if</code> <code class="n">prediction</code><code class="o">.</code><code class="n">item</code><code class="p">()</code> <code class="o">==</code> <code class="mi">1</code> <code class="k">else</code> <code class="s2">"Horse"</code></pre>
          <p>There are a few important points to consider here. <a contenteditable="false" data-primary="datasets" data-secondary="horses and humans" data-tertiary="computer-generated images" data-type="indexterm" id="id845"/><a contenteditable="false" data-primary="datasets" data-secondary="computer-generated images for training" data-type="indexterm" id="id846"/><a contenteditable="false" data-primary="training" data-secondary="computer-generated images for" data-type="indexterm" id="id847"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="computer-generated images for training" data-type="indexterm" id="id848"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="dataset" data-tertiary="computer-generated images" data-type="indexterm" id="id849"/>First, even though the network was trained on synthetic, computer-generated imagery, it performs quite well at spotting horses and humans and differentiating them in real photographs. This is a potential boon in that you may not need thousands of photographs to train a model, and you can do it relatively cheaply with CGI.</p>
          <p>But this dataset also demonstrates a fundamental issue you will face. Your training set cannot hope to represent <em>every</em> possible scenario your model might face in the wild, and thus, the model will always have some level of overspecialization toward the training set. We saw a clear and simple example of this earlier in this section, when the model mischaracterized the human in the center of <a data-type="xref" href="#ch03_figure_9_1748570891060144">Figure 3-9</a>. The training set didn’t include a human in that pose, and thus, the model didn’t “learn” that a human could look like that. As a result, there was every chance it might see the figure as a horse, and in this case, it did.</p>
          <p>What’s the solution? The obvious one is to add more training data, with humans in that particular pose and others that weren’t initially represented. That isn’t always possible, though. Fortunately, there’s a neat trick in PyTorch that you can use to virtually extend your dataset—it’s called <em>image augmentation</em>, and we’ll explore that next.<a contenteditable="false" data-primary="" data-startref="ch3hvh" data-type="indexterm" id="id850"/><a contenteditable="false" data-primary="" data-startref="ch3hvh2" data-type="indexterm" id="id851"/><a contenteditable="false" data-primary="" data-startref="ch3hvh3" data-type="indexterm" id="id852"/><a contenteditable="false" data-primary="" data-startref="ch3hvh4" data-type="indexterm" id="id853"/><a contenteditable="false" data-primary="" data-startref="ch3hvh5" data-type="indexterm" id="id854"/><a contenteditable="false" data-primary="" data-startref="ch3hvh6" data-type="indexterm" id="id855"/><a contenteditable="false" data-primary="" data-startref="ch3tst" data-type="indexterm" id="id856"/><a contenteditable="false" data-primary="" data-startref="ch3tst2" data-type="indexterm" id="id857"/><a contenteditable="false" data-primary="" data-startref="ch3tst3" data-type="indexterm" id="id858"/><a contenteditable="false" data-primary="" data-startref="ch3tst4" data-type="indexterm" id="id859"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Image Augmentation" data-type="sect1"><div class="sect1" id="ch03_image_augmentation_1748570891075700">
        <h1>Image Augmentation</h1>
        <p>In the previous section, you built<a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="image augmentation" data-type="indexterm" id="ch3imaug"/><a contenteditable="false" data-primary="feature detection" data-secondary="image augmentation" data-type="indexterm" id="ch3imaug2"/><a contenteditable="false" data-primary="image augmentation" data-type="indexterm" id="ch3imaug3"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="testing on images" data-tertiary="image augmentation" data-type="indexterm" id="ch3imaug4"/> a horse-or-human classifier model that was trained on a relatively small dataset. As a result, you soon began to hit problems classifying some previously unseen images, such as the miscategorization of a woman as a horse because the training set didn’t include any images of people in that pose.</p>
        <p>One way to deal with such problems is with <em>image augmentation</em>. The idea behind this technique is that as PyTorch is loading your data, it can create additional new data by amending what it has using a number of transforms. For example, take a look at <a data-type="xref" href="#fig-3-11">Figure 3-11</a>. While there is nothing in the dataset that looks like the woman on the right, the image on the left is somewhat similar.</p>
        
<figure><div class="figure" id="fig-3-11">
<img alt="" src="assets/aiml_0311.png"/>
<h6><span class="label">Figure 3-11. </span>Dataset similarities</h6>
</div></figure>

        <p>So, if you could, for example, zoom into the image on the left as you are training, as shown in <a data-type="xref" href="#fig-3-12">Figure 3-12</a>, you would increase the chances of the model being able to correctly classify the image on the right as a person.</p>
        
<figure><div class="figure" id="fig-3-12">
<img alt="" src="assets/aiml_0312.png"/>
<h6><span class="label">Figure 3-12. </span>Zooming in on the training set data</h6>
</div></figure>

        <p>In a similar way, you can broaden the training set with a variety of other transformations, including the following:</p>
        <ul>
          <li>
            <p>Rotation (turning the image)</p>
          </li>
          <li>
            <p>Shifting horizontally (moving the pixels horizontally with wrapping)</p>
          </li>
          <li>
            <p>Shifting vertically (moving the pixels vertically with wrapping)</p>
          </li>
          <li>
            <p>Shearing (moving the pixels either horizontally or vertically but offsetting so that the image would look like parallelogram)</p>
          </li>
          <li>
            <p>Zooming (magnifying a particular region)</p>
          </li>
          <li>
            <p>Flipping (vertically or horizontally)</p>
          </li>
        </ul>
        <p>Because you’ve been using the <code>datasets.ImageFolder</code> and a <code>DataLoader</code> to load the images, you’ve seen the model do a transform already—when it normalized the images like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define transformations</code>
<code class="n">transform</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Resize</code><code class="p">((</code><code class="mi">150</code><code class="p">,</code> <code class="mi">150</code><code class="p">)),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">])</code>
<code class="p">])</code></pre>
        <p>Many other transforms are easily available within the torchvision.transforms library, so, for example, you could do something like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Transforms for the training data</code>
<code class="n">train_transforms</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">RandomHorizontalFlip</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">RandomRotation</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">RandomResizedCrop</code><code class="p">(</code><code class="mi">150</code><code class="p">),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">])</code>  
<code class="p">])</code>
 
<code class="c1"># Transforms for the validation data</code>
<code class="n">val_transforms</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Resize</code><code class="p">(</code><code class="mi">150</code><code class="p">),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">CenterCrop</code><code class="p">(</code><code class="mi">150</code><code class="p">),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">])</code>
<code class="p">])</code></pre>
        <p>Here, in addition to rescaling the image to normalize it, you’re doing the following:</p>
        <ul>
          <li>
            <p>Randomly flipping horizontally</p>
          </li>
          <li>
            <p>Randomly rotating up to 20 degrees left or right</p>
          </li>
          <li>
            <p>Randomly cropping a 150 × 150 window instead of resizing</p>
          </li>
        </ul>
        <p>In addition, the transforms.RandomAffine library gives you the facility to do all of these things, as well as adding stuff like scaling the image (zooming in or out), shearing the image, etc. Here’s an example:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">transforms</code><code class="o">.</code><code class="n">RandomAffine</code><code class="p">(</code>
    <code class="n">degrees</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>  <code class="c1"># No rotation</code>
    <code class="n">translate</code><code class="o">=</code><code class="p">(</code><code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">),</code>  <code class="c1"># Translate up to 20% vert and horizontally</code>
    <code class="n">scale</code><code class="o">=</code><code class="p">(</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">1.2</code><code class="p">),</code>  <code class="c1"># Zoom in or out by 20%</code>
    <code class="n">shear</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>  <code class="c1"># Shear by up to 20 degrees</code>
<code class="p">),</code></pre>
        <p>When you retrain with these parameters, one of the first things you’ll notice is that training takes longer because of all the image processing. Also, your model’s accuracy may not be as high as it was, because previously it was overfitting to a largely uniform set of data.</p>
        <p>In my case, when I was training with these augmentations, my accuracy went down from 99% to 94% after 15 epochs, with validation much lower at 64%. This likely indicates overfitting in the model, but it warrants investigation by training with more epochs! One other thing to note is that random cropping might also be an <span class="keep-together">issue—the</span> CGI images generally center the subject, so random cropping will give partial <span class="keep-together">subjects.</span></p>
        <p>But what about the image from <a data-type="xref" href="#ch03_figure_9_1748570891060144">Figure 3-9</a> that the model misclassified earlier? <a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="testing on images" data-tertiary="incorrect classification" data-type="indexterm" id="id860"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="incorrect classification" data-type="indexterm" id="id861"/>This time, the model gets it right. Thanks to the image augmentations, the training set now has sufficient coverage for the model to understand that this particular image is a human too (see <a data-type="xref" href="#ch03_figure_11_1748570891060175">Figure 3-13</a>). This is just a single data point, and it may not be representative of the results for real data, but it’s a small step in the right direction.</p>
        <figure><div class="figure" id="ch03_figure_11_1748570891060175">
          <img src="assets/aiml_0313.png"/>
          <h6><span class="label">Figure 3-13. </span>The woman is now correctly classified</h6>
        </div></figure>
        <p>As you can see, even with a relatively small dataset like “Horses or Humans,” you can start to build a pretty decent classifier. With larger datasets, you could take this further. Another way you can improve the model is by using features that the model has already learned elsewhere. <a contenteditable="false" data-primary="computer vision" data-secondary="transfer learning" data-tertiary="about" data-type="indexterm" id="id862"/><a contenteditable="false" data-primary="pretrained models" data-secondary="transfer learning" data-tertiary="about" data-type="indexterm" id="id863"/><a contenteditable="false" data-primary="transfer learning" data-secondary="about" data-type="indexterm" id="id864"/><a contenteditable="false" data-primary="feature detection" data-secondary="transfer learning" data-tertiary="about" data-type="indexterm" id="id865"/>Many researchers with massive resources (millions of images) and huge models that have been trained on thousands of classes have shared their models, and by using a concept called <em>transfer learning</em>, you can use the features those models learned and apply them to your data. We’ll explore that next!<a contenteditable="false" data-primary="" data-startref="ch3imaug" data-type="indexterm" id="id866"/><a contenteditable="false" data-primary="" data-startref="ch3imaug2" data-type="indexterm" id="id867"/><a contenteditable="false" data-primary="" data-startref="ch3imaug3" data-type="indexterm" id="id868"/><a contenteditable="false" data-primary="" data-startref="ch3imaug4" data-type="indexterm" id="id869"/></p>
      </div></section>
      <section data-pdf-bookmark="Transfer Learning" data-type="sect1"><div class="sect1" id="ch03_transfer_learning_1748570891075750">
        <h1>Transfer Learning</h1>
        <p>As we’ve already seen in this chapter,<a contenteditable="false" data-primary="transfer learning" data-type="indexterm" id="ch3tl"/><a contenteditable="false" data-primary="computer vision" data-secondary="transfer learning" data-type="indexterm" id="ch3tl2"/><a contenteditable="false" data-primary="feature detection" data-secondary="transfer learning" data-type="indexterm" id="ch3tl3"/><a contenteditable="false" data-primary="pretrained models" data-secondary="transfer learning" data-type="indexterm" id="ch3tl4"/><a contenteditable="false" data-primary="Google Inception" data-secondary="transfer learning" data-type="indexterm" id="ch3tl6"/><a contenteditable="false" data-primary="computer vision" data-secondary="transfer learning" data-tertiary="Google Inception trained on ImageNet" data-type="indexterm" id="ch3tl7"/><a contenteditable="false" data-primary="transfer learning" data-secondary="Inception V3 by Google" data-type="indexterm" id="ch3tl8"/><a contenteditable="false" data-primary="feature detection" data-secondary="transfer learning" data-tertiary="Google Inception trained on ImageNet" data-type="indexterm" id="ch3tl9"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="convolutions" data-type="indexterm" id="id870"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="convolutions" data-type="indexterm" id="id871"/><a contenteditable="false" data-primary="convolutions" data-secondary="about" data-type="indexterm" id="id872"/><a contenteditable="false" data-primary="feature detection" data-secondary="convolutions" data-tertiary="about convolutions" data-type="indexterm" id="id873"/> the use of convolutions to extract features can be a powerful tool for identifying the contents of an image. If we use this tool, we can then feed the resulting feature maps into the dense layers of a neural network to match them to the labels and give us a more accurate way of determining the contents of an image. Using this approach with a simple fast-to-train neural network and some image augmentation techniques, we built a model that was 80–90% accurate at distinguishing between a horse and a human when it was trained on a very small dataset.</p>
        <p>However, we can improve our model even further by using a method called <em>transfer learning</em>. The idea behind it is simple: instead of having our model learn a set of filters from scratch for our dataset, why not have it use a set of filters that were learned on a much larger dataset, with many more features than we can “afford” to build from scratch? We can place these filters in our network and then train a model with our data using the pre-learned filters. For example, while our “Horses or Humans” dataset has only two classes, we can use an existing model that has been pretrained for one thousand classes—but at some point, we’ll have to throw away some of the preexisting network and add the layers that will let us have a classifier for two classes.</p>
        <p><a data-type="xref" href="#ch03_figure_12_1748570891060190">Figure 3-14</a> shows what<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="horses versus humans" data-tertiary="CNN architecture" data-type="indexterm" id="id874"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="CNN architecture" data-type="indexterm" id="id875"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="CNN architecture" data-type="indexterm" id="id876"/><a contenteditable="false" data-primary="neural networks" data-secondary="convolutional neural network implemented" data-tertiary="CNN architecture for horses versus humans" data-type="indexterm" id="id877"/> a CNN architecture for a classification task like ours might look like. We have a series of convolutional layers that lead to a dense layer, which in turn leads to an output layer.</p>
        <figure><div class="figure" id="ch03_figure_12_1748570891060190">
          <img alt="" src="assets/aiml_0314.png"/>
          <h6><span class="label">Figure 3-14. </span>A CNN architecture</h6>
        </div></figure>
        <p>We’ve seen that we can build a pretty good classifier using this architecture. <a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="transfer learning architecture" data-type="indexterm" id="id878"/><a contenteditable="false" data-primary="neural networks" data-secondary="convolutional neural network implemented" data-tertiary="CNN architecture of transfer learning" data-type="indexterm" id="id879"/><a contenteditable="false" data-primary="transfer learning" data-secondary="CNN architecture of" data-type="indexterm" id="id880"/>But what if we could use transfer learning to take the pre-learned layers from another model, freeze or lock them so that they aren’t trainable, and then put them on top of our model, like in <a data-type="xref" href="#ch03_figure_13_1748570891060207">Figure 3-15</a>?</p>
        <figure><div class="figure" id="ch03_figure_13_1748570891060207">
          <img alt="" src="assets/aiml_0315.png"/>
          <h6><span class="label">Figure 3-15. </span>Taking and locking layers from another architecture via transfer learning</h6>
        </div></figure>
        <p>When we consider that once they’ve been trained, all these layers are just a set of numbers indicating the filter values, weights, and biases along with a known architecture (the number of filters per layer, the size of the filter, etc.), the idea of reusing them is pretty straightforward.</p>
        <p>Let’s look at how this would appear in code. <a contenteditable="false" data-primary="datasets" data-secondary="ImageNet database for Google Inception" data-type="indexterm" id="id881"/><a contenteditable="false" data-primary="ImageNet database for Google Inception" data-type="indexterm" id="id882"/><a contenteditable="false" data-primary="Google Inception" data-secondary="trained on ImageNet database" data-type="indexterm" id="id883"/><a contenteditable="false" data-primary="transfer learning" data-secondary="Inception V3 by Google" data-tertiary="trained on ImageNet database" data-type="indexterm" id="id884"/>There are several pretrained models already available from a variety of sources, so we’ll use version 3 of the popular Inception model from Google, which is trained on more than a million images from a database called ImageNet. Inception has dozens of layers, and it can classify images into one thousand categories. </p>
        <p>The torchvision.models library <a contenteditable="false" data-primary="torchvision.models library" data-secondary="Inception V3" data-type="indexterm" id="ch3inc"/><a contenteditable="false" data-primary="pretrained models" data-secondary="torchvision.models library" data-tertiary="Inception V3" data-type="indexterm" id="ch3inc2"/>contains a number of models, including Inception V3, so we can easily get access to the pretrained model:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">models</code><code class="p">,</code> <code class="n">transforms</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code>
<code class="kn">from</code> <code class="nn">torchvision.datasets</code> <code class="kn">import</code> <code class="n">ImageFolder</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
 
<code class="c1"># Load the pretrained Inception V3 model</code>
<code class="n">pre_trained_model</code> <code class="o">=</code> <code class="n">models</code><code class="o">.</code><code class="n">inception_v3</code><code class="p">(</code><code class="n">pretrained</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">aux_logits</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
        <p>Now, we have a full Inception model that’s pretrained. If you want to inspect its architecture, you can do so with this code:<a contenteditable="false" data-primary="models" data-secondary="inspecting architecture of" data-type="indexterm" id="id885"/><a contenteditable="false" data-primary="pretrained models" data-secondary="inspecting architecture of model" data-type="indexterm" id="id886"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">print_model_summary</code><code class="p">(</code><code class="n">model</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">module</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">named_modules</code><code class="p">():</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">name</code><code class="si">}</code><code class="s2"> : </code><code class="si">{</code><code class="n">module</code><code class="o">.</code><code class="vm">__class__</code><code class="o">.</code><code class="vm">__name__</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
 
<code class="c1"># Example of how to use the function with your pretrained model</code>
<code class="n">print_model_summary</code><code class="p">(</code><code class="n">pre_trained_model</code><code class="p">)</code></pre>
        <p>Be warned—this model is huge! Still, you should take a look through it to see the layers and their names. I like to use the one called <code>Mixed7_c</code> because its output is nice and small—it consists of 8 × 8 images—but you should feel free to experiment with others.</p>
        <p>Next, we’ll freeze the <a contenteditable="false" data-primary="pretrained models" data-secondary="cropping" data-type="indexterm" id="id887"/><a contenteditable="false" data-primary="cropping models" data-type="indexterm" id="id888"/><a contenteditable="false" data-primary="models" data-secondary="cropping" data-type="indexterm" id="id889"/>entire network from retraining and then set a variable to point to <code>mixed7</code>’s output as where we want to crop the network. We can do that with this code:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Freeze all layers up to and including the 'Mixed_7c'</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">parameter</code> <code class="ow">in</code> <code class="n">pre_trained_model</code><code class="o">.</code><code class="n">named_parameters</code><code class="p">():</code>
    <code class="n">parameter</code><code class="o">.</code><code class="n">requires_grad</code> <code class="o">=</code> <code class="kc">False</code>
    <code class="k">if</code> <code class="s1">'Mixed_7c'</code> <code class="ow">in</code> <code class="n">name</code><code class="p">:</code>
        <code class="k">break</code></pre>
        <p>You’ll notice that we’re printing the output shape of the last layer, and you’ll also see that we’re getting 8 × 8 images at this point. This indicates that by the time the images have been fed through to <code>Mixed_7c</code>, the output images from the filters are 8 × 8 in size, so they’re pretty easy to manage. Again, you don’t have to choose that specific layer; you’re welcome to experiment with others.</p>
        <p>Now, let’s see how<a contenteditable="false" data-primary="pretrained models" data-secondary="transfer learning" data-tertiary="modifying the model" data-type="indexterm" id="id890"/><a contenteditable="false" data-primary="transfer learning" data-secondary="Inception V3 by Google" data-tertiary="modifying the model" data-type="indexterm" id="id891"/><a contenteditable="false" data-primary="feature detection" data-secondary="transfer learning" data-tertiary="modifying Inception model" data-type="indexterm" id="id892"/><a contenteditable="false" data-primary="computer vision" data-secondary="transfer learning" data-tertiary="modifying the Inception model" data-type="indexterm" id="id893"/> to modify the model for transfer learning. It’s pretty straightforward—if you go back to the output from the custom <code>print_model_summary</code> from a moment ago, you’ll see that the <em>last</em> layer in the model is called <code>fc</code>. As you might expect, <em>fc</em> stands for <em>fully connected</em>, which is effectively a Linear layer with our densely connected neurons. </p>
        <p>So now, it becomes as simple as replacing that layer with a new layer called <code>fc</code>. <span class="keep-together">We don’t need to <em>know</em></span> the input shape for it ahead of time—we can inspect its <span class="keep-together"><code>in_features</code></span> property to find that. So now, to create a new layer of 1,024 neurons that outputs to another layer of two neurons and replace the <code>fc</code> from Inception, all we have to do is this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Modify the existing fully connected layer</code>
<code class="n">num_ftrs</code> <code class="o">=</code> <code class="n">pre_trained_model</code><code class="o">.</code><code class="n">fc</code><code class="o">.</code><code class="n">in_features</code>
<code class="n">pre_trained_model</code><code class="o">.</code><code class="n">fc</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">num_ftrs</code><code class="p">,</code> <code class="mi">1024</code><code class="p">),</code>  <code class="c1"># New fully connected layer </code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>                <code class="c1"># Activation layer</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1024</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>         <code class="c1"># Final layer for binary classification</code>
<code class="p">)</code></pre>
        <p>It’s as simple as creating a new set of Linear layers from the last output, because we’ll be feeding the results into a dense layer. So, we then add a Linear layer of 1,024 neurons and a dense layer with two neurons for our output. Also, you’ve probably noticed that in the previous model, we did it with one neuron and used sigmoid activation for the two classes—so you’re probably wondering why we’re going to two neurons in the output layer now. This was primarily a stylistic choice. <a contenteditable="false" data-primary="Google Inception" data-secondary="designed for n neurons to output n classes" data-type="indexterm" id="id894"/>Inception was designed for <em>n</em> neurons to output for <em>n</em> classes, and I wanted to keep that approach.</p>
        <p>Training the model on this architecture over only three epochs gave us an accuracy of 99%+, with a validation accuracy of 95%+. Clearly, that’s a vast improvement. <a contenteditable="false" data-primary="Google Inception" data-secondary="trained on ImageNet database" data-type="indexterm" id="id895"/>Also, remember that Inception learned a massive set of features that it could use to classify the many classes it was trained on. It turns out that that feature set is also incredibly useful for learning how to classify any other images—not least, those from “Horses or Humans.”</p>
        <p>The results we got from this model are much better than those we got from our previous model, but you can continue to tweak and improve it. You can also explore how the model will work with a much larger dataset,<a contenteditable="false" data-primary="Kaggle “Dogs vs. Cats” dataset" data-type="indexterm" id="ch3dnc"/><a contenteditable="false" data-primary="“Dogs vs. Cats” dataset (Kaggle)" data-primary-sortas="Dogs vs. Cats" data-type="indexterm" id="ch3dnc2"/><a contenteditable="false" data-primary="datasets" data-secondary="“Dogs vs. Cats” by Kaggle" data-secondary-sortas="Dogs vs. Cats" data-type="indexterm" id="ch3dnc3"/><a contenteditable="false" data-primary="computer vision" data-secondary="transfer learning" data-tertiary="“Dogs vs. Cats” dataset by Kaggle" data-type="indexterm" id="ch3dnc4"/><a contenteditable="false" data-primary="feature detection" data-secondary="transfer learning" data-tertiary="“Dogs vs. Cats” dataset by Kaggle" data-tertiary-sortas="Dogs vs. Cats" data-type="indexterm" id="ch3dnc5"/><a contenteditable="false" data-primary="classifiers" data-secondary="“Dogs vs. Cats” dataset by Kaggle" data-secondary-sortas="Dogs vs. Cats" data-type="indexterm" id="ch3dnc6"/> like the famous “<a href="https://oreil.ly/UhWMk">Dogs vs. Cats”</a> from Kaggle. It’s an extremely varied dataset consisting of 25,000 images of cats and dogs, often with the subjects somewhat obscured—for example, if they are held by a human.<a contenteditable="false" data-primary="" data-startref="ch3tl7" data-type="indexterm" id="id896"/><a contenteditable="false" data-primary="" data-startref="ch3tl8" data-type="indexterm" id="id897"/><a contenteditable="false" data-primary="" data-startref="ch3tl9" data-type="indexterm" id="id898"/></p>
        <p>Using the same algorithm and model design as before, you can train a “Dogs vs. Cats” classifier on Colab, using a GPU at about 3 minutes per epoch. </p>
        <p>When I tested with very complex pictures like those in <a data-type="xref" href="#ch03_figure_14_1748570891060222">Figure 3-16</a>, this classifier got them all correct. I chose one picture of a dog with catlike ears and one with its back turned. Both pictures of cats were nontypical.</p>
        <figure><div class="figure" id="ch03_figure_14_1748570891060222">
          <img alt="" src="assets/aiml_0316.png"/>
          <h6><span class="label">Figure 3-16. </span>Unusual dogs and cats that the model classified correctly</h6>
        </div></figure>
        <p>To parse the results, you can use code like this:</p>

        <pre data-code-language="python" data-type="programlisting">     <code class="k">def</code> <code class="nf">load_image</code><code class="p">(</code><code class="n">image_path</code><code class="p">,</code> <code class="n">transform</code><code class="p">):</code>
    <code class="c1"># Load image</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">image_path</code><code class="p">)</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s1">'RGB'</code><code class="p">)</code>  <code class="c1"># Convert to RGB </code>
    <code class="c1"># Apply transformations</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">transform</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
    <code class="c1"># Add batch dimension, as the model expects batches</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">image</code>

    <code class="c1"># Prediction function</code>
<code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">image_path</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">device</code><code class="p">,</code> <code class="n">transform</code><code class="p">):</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">load_image</code><code class="p">(</code><code class="n">image_path</code><code class="p">,</code> <code class="n">transform</code><code class="p">)</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">output</code><code class="p">)</code>
        <code class="n">prediction</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">prediction</code><code class="p">)</code></pre>
        <p>Note the lines where I’m printing the output of the image, calculating the prediction from that, and printing that.</p>
        <p>When you upload some images to Colab, you can see how they predict in <a data-type="xref" href="#ch03_figure_15_1748570891060236">Figure 3-17</a>.</p>
        <figure><div class="figure" id="ch03_figure_15_1748570891060236">
          <img src="assets/aiml_0317.png"/>
          <h6><span class="label">Figure 3-17. </span>Classifying the cat washing its paw</h6>
        </div></figure>
        <p>The first image uploaded was “labrador,” which, as its name suggests, is of a dog. The tensor returned from the model contained [–14.9642, 18.3943], meaning a very low number for the first label and a very high one for the second. Given that we used an image directory when training, the labels ended up being in alphabetical order, so it was low for cat and high for dog. Then, when we called <code>torch.max</code>, it gave us [1]. That indicates that neuron 1 is the one for this classification—thus, the image is a dog.</p>
        <p>The second image had [5.3486, –4.8260], with the first neuron being higher. Thus, it detected a cat. The size of these numbers indicates the strength of the prediction. For example, it was much surer that the first image is a dog than it was that the second image is a cat. </p>
        <p>You can find the complete code for the “Horses or Humans” and “Dogs vs. Cats” classifiers in the book’s <a href="https://github.com/lmoroney/tfbook">GitHub repository</a>.<a contenteditable="false" data-primary="“Dogs vs. Cats” dataset (Kaggle)" data-primary-sortas="Dogs vs. Cats" data-secondary="code online" data-type="indexterm" id="id899"/><a contenteditable="false" data-primary="classifiers" data-secondary="“Dogs vs. Cats” dataset by Kaggle" data-secondary-sortas="Dogs vs. Cats" data-tertiary="code online" data-type="indexterm" id="id900"/><a contenteditable="false" data-primary="Kaggle “Dogs vs. Cats” dataset" data-secondary="code online" data-type="indexterm" id="id901"/><a contenteditable="false" data-primary="computer vision" data-secondary="code online for horses vs. humans and dogs vs. cats" data-type="indexterm" id="id902"/><a contenteditable="false" data-primary="book code online" data-secondary="horses vs. humans and dogs vs. cats" data-type="indexterm" id="id903"/><a contenteditable="false" data-primary="online resources" data-secondary="code for book" data-tertiary="horses vs. humans and dogs vs. cats" data-type="indexterm" id="id904"/><a contenteditable="false" data-primary="GitHub" data-secondary="code for horses vs. humans and dogs vs. cats" data-type="indexterm" id="id905"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="code online" data-type="indexterm" id="id906"/><a contenteditable="false" data-primary="classifiers" data-secondary="horses versus humans" data-tertiary="code online" data-type="indexterm" id="id907"/><a contenteditable="false" data-primary="feature detection" data-secondary="horses versus humans" data-tertiary="code online" data-type="indexterm" id="id908"/><a contenteditable="false" data-primary="" data-startref="ch3tl" data-type="indexterm" id="id909"/><a contenteditable="false" data-primary="" data-startref="ch3tl2" data-type="indexterm" id="id910"/><a contenteditable="false" data-primary="" data-startref="ch3tl3" data-type="indexterm" id="id911"/><a contenteditable="false" data-primary="" data-startref="ch3tl4" data-type="indexterm" id="id912"/><a contenteditable="false" data-primary="" data-startref="ch3tl6" data-type="indexterm" id="id913"/><a contenteditable="false" data-primary="" data-startref="ch3inc" data-type="indexterm" id="id914"/><a contenteditable="false" data-primary="" data-startref="ch3inc2" data-type="indexterm" id="id915"/><a contenteditable="false" data-primary="" data-startref="ch3dnc" data-type="indexterm" id="id916"/><a contenteditable="false" data-primary="" data-startref="ch3dnc2" data-type="indexterm" id="id917"/><a contenteditable="false" data-primary="" data-startref="ch3dnc3" data-type="indexterm" id="id918"/><a contenteditable="false" data-primary="" data-startref="ch3dnc4" data-type="indexterm" id="id919"/><a contenteditable="false" data-primary="" data-startref="ch3dnc5" data-type="indexterm" id="id920"/><a contenteditable="false" data-primary="" data-startref="ch3dnc6" data-type="indexterm" id="id921"/></p>
      </div></section>
      <section data-pdf-bookmark="Multiclass Classification" data-type="sect1"><div class="sect1" id="ch03_multiclass_classification_1748570891075798">
        <h1>Multiclass Classification</h1>
        <p>In all of the examples so far,<a contenteditable="false" data-primary="computer vision" data-secondary="multiclass classification" data-type="indexterm" id="ch3mcc"/><a contenteditable="false" data-primary="multiclass classification" data-type="indexterm" id="ch3mcc2"/><a contenteditable="false" data-primary="classifiers" data-secondary="multiclass classification" data-type="indexterm" id="ch3mcc3"/><a contenteditable="false" data-primary="feature detection" data-secondary="multiclass classification" data-type="indexterm" id="ch3mcc4"/> you’ve been building <em>binary</em> classifiers—ones that choose between two options (horses or humans, cats or dogs). On the other hand, when you’re building <em>multiclass classifiers</em>, the models are almost the same but there are a few important differences. Instead of a single neuron that is sigmoid activated or two neurons that are binary activated, your output layer will now require <em>n</em> neurons, where <em>n</em> is the number of classes you want to classify. You’ll also have to change your loss function to an appropriate one for multiple categories. </p>
        <p>A neat feature of the <code>nn.CrossEntropyLoss</code> loss function<a contenteditable="false" data-primary="loss functions" data-secondary="nn.CrossEntropyLoss" data-type="indexterm" id="id922"/><a contenteditable="false" data-primary="nn.CrossEntropyLoss loss function" data-type="indexterm" id="id923"/><a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="single output neuron so only binary" data-type="indexterm" id="id924"/><a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="horses versus humans single output neuron" data-type="indexterm" id="id925"/> in PyTorch is that it can handle multiple categories, so the “Cats vs. Dogs” and “Horses or Humans” transfer learning classifiers you’ve built thus far in this chapter can use it without modification. But the “Horses or Humans” classifier that you built at the beginning with a <em>single</em> output neuron will not be able to because it can’t handle more than two classes. This is always something to look out for, and it’s a common bug when you start writing code for classification.</p>
        <p>To go beyond two-class classification,<a contenteditable="false" data-primary="datasets" data-secondary="Rock, Paper, Scissors" data-type="indexterm" id="ch3rps"/><a contenteditable="false" data-primary="Rock, Paper, Scissors dataset" data-type="indexterm" id="ch3rps2"/><a contenteditable="false" data-primary="multiclass classification" data-secondary="Rock, Paper, Scissors dataset" data-type="indexterm" id="ch3rps3"/><a contenteditable="false" data-primary="computer vision" data-secondary="multiclass classification" data-tertiary="Rock, Paper, Scissors dataset" data-type="indexterm" id="ch3rps4"/><a contenteditable="false" data-primary="classifiers" data-secondary="multiclass classification" data-tertiary="Rock, Paper, Scissors dataset" data-type="indexterm" id="ch3rps5"/><a contenteditable="false" data-primary="feature detection" data-secondary="multiclass classification" data-tertiary="Rock, Paper, Scissors dataset" data-type="indexterm" id="ch3rps6"/> consider, for example, the game Rock, Paper, Scissors. If you wanted to train a dataset to recognize the different hand gestures used in this game, you’d need to handle three categories. Fortunately, there’s a <a href="https://oreil.ly/VHhmS">simple dataset</a> you can use for this.</p>
        <p>There are two downloads: a training set of many diverse hands, with different sizes, shapes, colors, and details such as nail polish; and a testing set of equally diverse hands, none of which are in the training set. You can see some examples in <a data-type="xref" href="#ch03_figure_16_1748570891060251">Figure 3-18</a>.</p>
        <figure><div class="figure" id="ch03_figure_16_1748570891060251">
          <img alt="Examples of Rock/Paper/Scissors gestures" src="assets/aiml_0318.png"/>
          <h6><span class="label">Figure 3-18. </span>Examples of Rock, Paper, Scissors gestures</h6>
        </div></figure>
        <p>Using the dataset is simple. You can download and unzip it—the sorted subdirectories are already present in the ZIP file—and then use it to initialize an <code>ImageFolder</code>:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="err">!</code><code class="n">wget</code> <code class="o">--</code><code class="n">no</code><code class="o">-</code><code class="n">check</code><code class="o">-</code><code class="n">certificate</code> \
 <code class="n">https</code><code class="p">:</code><code class="o">//</code><code class="n">storage</code><code class="o">.</code><code class="n">googleapis</code><code class="o">.</code><code class="n">com</code><code class="o">/</code><code class="n">learning</code><code class="o">-</code><code class="n">datasets</code><code class="o">/</code><code class="n">rps</code><code class="o">.</code><code class="n">zip</code> \
 <code class="o">-</code><code class="n">O</code> <code class="o">/</code><code class="n">tmp</code><code class="o">/</code><code class="n">rps</code><code class="o">.</code><code class="n">zip</code>
<code class="n">local_zip</code> <code class="o">=</code> <code class="s1">'/tmp/rps.zip'</code>
<code class="n">zip_ref</code> <code class="o">=</code> <code class="n">zipfile</code><code class="o">.</code><code class="n">ZipFile</code><code class="p">(</code><code class="n">local_zip</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">)</code>
<code class="n">zip_ref</code><code class="o">.</code><code class="n">extractall</code><code class="p">(</code><code class="s1">'/tmp/'</code><code class="p">)</code>
<code class="n">zip_ref</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
<code class="n">training_dir</code> <code class="o">=</code> <code class="s2">"/tmp/rps/"</code>
 
<code class="n">train_dataset</code> <code class="o">=</code> <code class="n">ImageFolder</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="n">training_dir</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code></pre>
        <p>Be sure to use a <code>transform</code> that fits the input shape of your model. In the last few examples, we were using Inception, and it’s 299 × 299.</p>
        <p>You can use the ImageFolder for your DataLoader in the usual way:<a contenteditable="false" data-primary="DataLoader" data-secondary="ImageFolder as" data-type="indexterm" id="id926"/><a contenteditable="false" data-primary="ImageFolder" data-secondary="as DataLoader" data-secondary-sortas="DataLoader" data-type="indexterm" id="id927"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
        <p>Earlier, when we tweaked the Inception model for “Horses or Humans” or “Cats vs. Dogs,” there were only <em>two</em> classes and thus <em>two</em> output neurons. Given that this data has <em>three</em> classes, we need to be sure that we change the new fully connected layer at the bottom accordingly:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Modify the existing fully connected layer</code>
<code class="n">num_ftrs</code> <code class="o">=</code> <code class="n">pre_trained_model</code><code class="o">.</code><code class="n">fc</code><code class="o">.</code><code class="n">in_features</code>
<code class="n">pre_trained_model</code><code class="o">.</code><code class="n">fc</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">num_ftrs</code><code class="p">,</code> <code class="mi">1024</code><code class="p">),</code>  <code class="c1"># New fully connected layer </code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>                  <code class="c1"># Activation layer</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1024</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>         <code class="c1"># Final layer for binary classification</code>
<code class="p">)</code></pre>
        <p>Now, training the model works as before: you specify the loss function and optimizer, and you call the <code>train_model()</code> function. For good repetition, this function is the same as the one used in the “Horses or Humans” and “Cats vs. Dogs” examples:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Only optimize parameters that are set to be trainable</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="nb">filter</code><code class="p">(</code><code class="k">lambda</code> <code class="n">p</code><code class="p">:</code> <code class="n">p</code><code class="o">.</code><code class="n">requires_grad</code><code class="p">,</code> 
                    <code class="n">pre_trained_model</code><code class="o">.</code><code class="n">parameters</code><code class="p">()),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">)</code>
 
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">CrossEntropyLoss</code><code class="p">()</code>
 
<code class="c1"># Train the model</code>
<code class="n">train_model</code><code class="p">(</code><code class="n">pre_trained_model</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">,</code> <code class="n">num_epochs</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code></pre>
        <p>Your code for testing predictions<a contenteditable="false" data-primary="prediction" data-secondary="multiclass classification" data-type="indexterm" id="id928"/> will also need to change somewhat. There are now three output neurons, and they will output a high value for the predicted class and lower values for the other classes. </p>
        <p>Note also that when you’re using the <code>ImageFolder</code>, the classes are loaded in alphabetical order—so while you might expect the output neurons to be in the order of the name of the game, the order will in fact be Paper, Rock, Scissors.</p>
        <p>Code that you can use to try out predictions in a Colab notebook will look like the following. It’s very similar to what you saw earlier:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">load_image</code><code class="p">(</code><code class="n">image_path</code><code class="p">,</code> <code class="n">transform</code><code class="p">):</code>
    <code class="c1"># Load image</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">image_path</code><code class="p">)</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s1">'RGB'</code><code class="p">)</code>  <code class="c1"># Convert to RGB </code>
    <code class="c1"># Apply transformations</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">transform</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
    <code class="c1"># Add batch dimension, as the model expects batches</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">image</code>
 
    <code class="c1"># Prediction function</code>
<code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">image_path</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">device</code><code class="p">,</code> <code class="n">transform</code><code class="p">):</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">load_image</code><code class="p">(</code><code class="n">image_path</code><code class="p">,</code> <code class="n">transform</code><code class="p">)</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">output</code><code class="p">)</code>
        <code class="n">prediction</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">prediction</code><code class="p">)</code></pre>
        <p>Note that it doesn’t parse the output; it just prints the classes. <a data-type="xref" href="#ch03_figure_17_1748570891060264">Figure 3-19</a> shows what it looks like in actual use.</p>
        <figure><div class="figure" id="ch03_figure_17_1748570891060264">
          <img src="assets/aiml_0319.png"/>
          <h6><span class="label">Figure 3-19. </span>Testing the Rock, Paper, Scissors classifier</h6>
        </div></figure>
        <p>You can see from the filenames what the images were. </p>
        <p>If you explore this a little deeper, you can see that the file named <em>scissors4.png</em> had an output of –2.5582, –1.7362, 3.8465]. The largest number is the third one, and if you think alphabetically, you can see that the third neuron represents scissors, so it was classified correctly. Similar results were achieved for the other files.</p>
        <p>Some images that you can<a contenteditable="false" data-primary="multiclass classification" data-secondary="Rock, Paper, Scissors dataset" data-tertiary="images online for testing" data-type="indexterm" id="id929"/><a contenteditable="false" data-primary="Rock, Paper, Scissors dataset" data-secondary="images online for testing" data-type="indexterm" id="id930"/><a contenteditable="false" data-primary="computer vision" data-secondary="multiclass classification" data-tertiary="images online for testing" data-type="indexterm" id="id931"/><a contenteditable="false" data-primary="feature detection" data-secondary="multiclass classification" data-tertiary="images online for testing" data-type="indexterm" id="id932"/><a contenteditable="false" data-primary="classifiers" data-secondary="multiclass classification" data-tertiary="images online for testing" data-type="indexterm" id="id933"/><a contenteditable="false" data-primary="online resources" data-secondary="Rock, Paper, Scissors images for testing" data-type="indexterm" id="id934"/> use to test the dataset <a href="https://oreil.ly/dEUpx">are available to download</a>. Alternatively, of course, you can try your own. Note that the training images are all done against a plain white background, though, so there may be some confusion if there is a lot of detail in the background of the photos you take.<a contenteditable="false" data-primary="" data-startref="ch3mcc" data-type="indexterm" id="id935"/><a contenteditable="false" data-primary="" data-startref="ch3mcc2" data-type="indexterm" id="id936"/><a contenteditable="false" data-primary="" data-startref="ch3mcc3" data-type="indexterm" id="id937"/><a contenteditable="false" data-primary="" data-startref="ch3rps" data-type="indexterm" id="id938"/><a contenteditable="false" data-primary="" data-startref="ch3rps2" data-type="indexterm" id="id939"/><a contenteditable="false" data-primary="" data-startref="ch3rps3" data-type="indexterm" id="id940"/><a contenteditable="false" data-primary="" data-startref="ch3rps4" data-type="indexterm" id="id941"/><a contenteditable="false" data-primary="" data-startref="ch3rps5" data-type="indexterm" id="id942"/><a contenteditable="false" data-primary="" data-startref="ch3rps6" data-type="indexterm" id="id943"/><a contenteditable="false" data-primary="" data-startref="ch3mcc4" data-type="indexterm" id="id944"/></p>
      </div></section>
      <section data-pdf-bookmark="Dropout Regularization" data-type="sect1"><div class="sect1" id="ch03_dropout_regularization_1748570891075844">
        <h1>Dropout Regularization</h1>
        <p>Earlier in this chapter, we discussed<a contenteditable="false" data-primary="computer vision" data-secondary="detecting features" data-tertiary="dropout regularization" data-type="indexterm" id="ch3drop"/><a contenteditable="false" data-primary="feature detection" data-secondary="dropout regularization" data-type="indexterm" id="ch3drop2"/><a contenteditable="false" data-primary="dropout regularization" data-type="indexterm" id="ch3drop3"/><a contenteditable="false" data-primary="overfitting" data-secondary="dropout helping overcome" data-type="indexterm" id="ch3drop4"/><a contenteditable="false" data-primary="neural networks" data-secondary="dropout regularization helping overcome overfitting" data-type="indexterm" id="ch3drop5"/> overfitting, in which a network may become too specialized in a particular type of input data and thus fare poorly on others. One technique to help overcome this is use of <em>dropout regularization</em>.</p>
        <p>When a neural network is being trained,<a contenteditable="false" data-primary="neurons" data-secondary="overspecialization leading to overfitting" data-type="indexterm" id="id945"/> each individual neuron will have an effect on neurons in subsequent layers. Over time, particularly in larger networks, some neurons can become overspecialized—and that feeds downstream, potentially <span class="keep-together">causing</span> the network as a whole to become overspecialized and thus leading to overfitting. Additionally, neighboring neurons can end up with similar weights and biases, and if not monitored, this condition can lead the overall model to become overspecialized on the features activated by those neurons.</p>
        <p>For example, consider the neural network in <a data-type="xref" href="#ch03_figure_18_1748570891060278">Figure 3-20</a>, in which there are layers of 2, 6, 6, and 2 neurons. The neurons in the middle layers might end up with very similar weights and biases.</p>
        <figure class="width-60"><div class="figure" id="ch03_figure_18_1748570891060278">
          <img alt="" src="assets/aiml_0320.png"/>
          <h6><span class="label">Figure 3-20. </span>A simple neural network</h6>
        </div></figure>
        <p class="pagebreak-before less_space">While training, if you remove a random number of neurons and ignore them, <span class="keep-together">then their contribution</span> to the neurons in the next layer is temporarily blocked (see <a data-type="xref" href="#ch03_figure_19_1748570891060292">Figure 3-21</a>). They are effectively dropped out, leading to the term <em>dropout <span class="keep-together">regularization</span></em>.</p>
        <figure class="width-40"><div class="figure" id="ch03_figure_19_1748570891060292">
          <img alt="" src="assets/aiml_0321.png"/>
          <h6><span class="label">Figure 3-21. </span>A neural network with dropouts</h6>
        </div></figure>
        <p>This reduces the chances of the neurons becoming overspecialized. The network will still learn the same number of parameters, but it should be better at generalization—that is, it should be more resilient to different inputs.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>The concept of dropouts<a contenteditable="false" data-primary="Srivastav, Nitish" data-type="indexterm" id="id946"/><a contenteditable="false" data-primary="“Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (Srivastav)" data-primary-sortas="Dropout: A Simple Way" data-type="indexterm" id="id947"/> was proposed by Nitish Srivastava et al. in their 2014 paper “<a href="https://oreil.ly/673CJ">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>.”</p>
        </div>
        <p>To implement dropouts in PyTorch, you can just use a simple layer like this:<a contenteditable="false" data-primary="dropout regularization" data-secondary="implementing dropouts in PyTorch" data-type="indexterm" id="id948"/><a contenteditable="false" data-primary="feature detection" data-secondary="dropout regularization" data-tertiary="implementing dropouts in PyTorch" data-type="indexterm" id="id949"/><a contenteditable="false" data-primary="neural networks" data-secondary="dropout regularization helping overcome overfitting" data-tertiary="implementing dropouts in PyTorch" data-type="indexterm" id="id950"/><a contenteditable="false" data-primary="overfitting" data-secondary="dropout helping overcome" data-tertiary="implementing dropouts in PyTorch" data-type="indexterm" id="id951"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">)</code></pre>
        <p>This will drop out, at random, the specified percentage of neurons (here, 50%) in the specified layer. Note that it may take some experimentation to find the correct percentage for your network.</p>
        <p>For a simple example that demonstrates this, consider the new fully connected layers we added to the bottom of Inception with the transfer learning example in this <span class="keep-together">chapter.</span></p>
        <p>Here it is for Rock, Paper, Scissors with three output neurons:<a contenteditable="false" data-primary="Rock, Paper, Scissors dataset" data-secondary="dropout regularization implemented" data-type="indexterm" id="id952"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">num_ftrs</code> <code class="o">=</code> <code class="n">pre_trained_model</code><code class="o">.</code><code class="n">fc</code><code class="o">.</code><code class="n">in_features</code>
<code class="n">pre_trained_model</code><code class="o">.</code><code class="n">fc</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">num_ftrs</code><code class="p">,</code> <code class="mi">1024</code><code class="p">),</code>  <code class="c1"># New fully connected layer </code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>                <code class="c1"># Activation layer</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1024</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>         <code class="c1"># Final layer for RPS</code>
<code class="p">)</code></pre>
        <p>With dropout added, it would look like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">num_ftrs</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fc</code><code class="o">.</code><code class="n">in_features</code>
<code class="n">model</code><code class="o">.</code><code class="n">fc</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">),</code>  <code class="c1"># Adding dropout before the final FC layer</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">num_ftrs</code><code class="p">,</code> <code class="mi">1024</code><code class="p">),</code>  <code class="c1"># Reduce dimensionality to 1024</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">),</code>  <code class="c1"># Adding another dropout layer after ReLU activation</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1024</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>  <code class="c1"># Final layer for RPS</code>
<code class="p">)</code></pre>
        <p>The examples that we used in this chapter for transfer learning are already learning really well without the use of dropouts. However, I’d recommend that you always consider dropouts when building your models because they can greatly reduce waste in the ML process—letting your model learn just as well but much faster!</p>
        <p>Additionally, as you design your neural networks, keep in mind that getting great results on your training set is not always a good thing because it could be a sign of overfitting. Introducing dropouts can help you remove that problem so that you can optimize your network in other areas without that false sense of security.<a contenteditable="false" data-primary="" data-startref="ch3drop" data-type="indexterm" id="id953"/><a contenteditable="false" data-primary="" data-startref="ch3drop2" data-type="indexterm" id="id954"/><a contenteditable="false" data-primary="" data-startref="ch3drop3" data-type="indexterm" id="id955"/><a contenteditable="false" data-primary="" data-startref="ch3drop4" data-type="indexterm" id="id956"/><a contenteditable="false" data-primary="" data-startref="ch3drop5" data-type="indexterm" id="id957"/></p>
      </div></section>
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch03_summary_1748570891075895">
        <h1>Summary</h1>
        <p>This chapter introduced you to a more advanced way of achieving computer vision by using convolutional neural networks. You saw how to use convolutions to apply filters that can extract features from images, and you designed your first neural networks to deal with more complex vision scenarios than those you encountered with the MNIST and Fashion MNIST datasets. You also explored techniques to improve your network’s accuracy and avoid overfitting, such as the use of image augmentation and dropouts.</p>
        <p>Before we explore further scenarios, in <a data-type="xref" href="ch04.html#ch04_using_data_with_pytorch_1748548966496246">Chapter 4</a>, you’ll get an introduction to PyTorch data, which is a technology that makes it much easier for you to get access to data for training and testing your networks. In this chapter, you downloaded ZIP files and extracted images, but that’s not always going to be possible. With PyTorch datasets, you’ll be able to access lots of datasets with a standard API.</p>
      </div></section>
    </div></section></body></html>