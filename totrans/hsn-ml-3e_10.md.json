["```py\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs([...])  # make the blobs: y contains the cluster IDs, but we\n                          # will not use them; that's what we want to predict\nk = 5\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X)\n```", "```py\n>>> y_pred\narray([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n>>> y_pred is kmeans.labels_\nTrue\n```", "```py\n>>> kmeans.cluster_centers_\narray([[-2.80389616,  1.80117999],\n [ 0.20876306,  2.25551336],\n [-2.79290307,  2.79641063],\n [-1.46679593,  2.28585348],\n [-2.80037642,  1.30082566]])\n```", "```py\n>>> import numpy as np\n>>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n>>> kmeans.predict(X_new)\narray([1, 1, 2, 2], dtype=int32)\n```", "```py\n>>> kmeans.transform(X_new).round(2)\narray([[2.81, 0.33, 2.9 , 1.49, 2.89],\n [5.81, 2.8 , 5.85, 4.48, 5.84],\n [1.21, 3.29, 0.29, 1.69, 1.71],\n [0.73, 3.22, 0.36, 1.55, 1.22]])\n```", "```py\ngood_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\nkmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\nkmeans.fit(X)\n```", "```py\n>>> kmeans.inertia_\n211.59853725816836\n```", "```py\n>>> kmeans.score(X)\n-211.5985372581684\n```", "```py\nfrom sklearn.cluster import MiniBatchKMeans\n\nminibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)\nminibatch_kmeans.fit(X)\n```", "```py\n>>> from sklearn.metrics import silhouette_score\n>>> silhouette_score(X, kmeans.labels_)\n0.655517642572828\n```", "```py\n>>> import PIL\n>>> image = np.asarray(PIL.Image.open(filepath))\n>>> image.shape\n(533, 800, 3)\n```", "```py\nX = image.reshape(-1, 3)\nkmeans = KMeans(n_clusters=8, random_state=42).fit(X)\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\nsegmented_img = segmented_img.reshape(image.shape)\n```", "```py\nfrom sklearn.datasets import load_digits\n\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, y_train = X_digits[:1400], y_digits[:1400]\nX_test, y_test = X_digits[1400:], y_digits[1400:]\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\nn_labeled = 50\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\n```", "```py\n>>> log_reg.score(X_test, y_test)\n0.7481108312342569\n```", "```py\nk = 50\nkmeans = KMeans(n_clusters=k, random_state=42)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)\nX_representative_digits = X_train[representative_digit_idx]\n```", "```py\ny_representative_digits = np.array([1, 3, 6, 0, 7, 9, 2, ..., 5, 1, 9, 9, 3, 7])\n```", "```py\n>>> log_reg = LogisticRegression(max_iter=10_000)\n>>> log_reg.fit(X_representative_digits, y_representative_digits)\n>>> log_reg.score(X_test, y_test)\n0.8488664987405542\n```", "```py\ny_train_propagated = np.empty(len(X_train), dtype=np.int64)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]\n```", "```py\n>>> log_reg = LogisticRegression()\n>>> log_reg.fit(X_train, y_train_propagated)\n>>> log_reg.score(X_test, y_test)\n0.8942065491183879\n```", "```py\npercentile_closest = 99\n\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\nfor i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist > cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1\n\npartially_propagated = (X_cluster_dist != -1)\nX_train_partially_propagated = X_train[partially_propagated]\ny_train_partially_propagated = y_train_propagated[partially_propagated]\n```", "```py\n>>> log_reg = LogisticRegression(max_iter=10_000)\n>>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n>>> log_reg.score(X_test, y_test)\n0.9093198992443325\n```", "```py\n>>> (y_train_partially_propagated == y_train[partially_propagated]).mean()\n0.9755555555555555\n```", "```py\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=1000, noise=0.05)\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\n```", "```py\n>>> dbscan.labels_\narray([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5, [...], 3,  3,  4,  2,  6,  3])\n```", "```py\n>>> dbscan.core_sample_indices_\narray([  0,   4,   5,   6,   7,   8,  10,  11, [...], 993, 995, 997, 998, 999])\n>>> dbscan.components_\narray([[-0.02137124,  0.40618608],\n [-0.84192557,  0.53058695],\n [...],\n [ 0.79419406,  0.60777171]])\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=50)\nknn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n```", "```py\n>>> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n>>> knn.predict(X_new)\narray([1, 0, 1, 0])\n>>> knn.predict_proba(X_new)\narray([[0.18, 0.82],\n [1\\.  , 0\\.  ],\n [0.12, 0.88],\n [1\\.  , 0\\.  ]])\n```", "```py\n>>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n>>> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n>>> y_pred[y_dist > 0.2] = -1\n>>> y_pred.ravel()\narray([-1,  0,  1, -1])\n```", "```py\nfrom sklearn.mixture import GaussianMixture\n\ngm = GaussianMixture(n_components=3, n_init=10)\ngm.fit(X)\n```", "```py\n>>> gm.weights_\narray([0.39025715, 0.40007391, 0.20966893])\n>>> gm.means_\narray([[ 0.05131611,  0.07521837],\n [-1.40763156,  1.42708225],\n [ 3.39893794,  1.05928897]])\n>>> gm.covariances_\narray([[[ 0.68799922,  0.79606357],\n [ 0.79606357,  1.21236106]],\n\n [[ 0.63479409,  0.72970799],\n [ 0.72970799,  1.1610351 ]],\n\n [[ 1.14833585, -0.03256179],\n [-0.03256179,  0.95490931]]])\n```", "```py\n>>> gm.converged_\nTrue\n>>> gm.n_iter_\n4\n```", "```py\n>>> gm.predict(X)\narray([0, 0, 1, ..., 2, 2, 2])\n>>> gm.predict_proba(X).round(3)\narray([[0.977, 0\\.   , 0.023],\n [0.983, 0.001, 0.016],\n [0\\.   , 1\\.   , 0\\.   ],\n ...,\n [0\\.   , 0\\.   , 1\\.   ],\n [0\\.   , 0\\.   , 1\\.   ],\n [0\\.   , 0\\.   , 1\\.   ]])\n```", "```py\n>>> X_new, y_new = gm.sample(6)\n>>> X_new\narray([[-0.86944074, -0.32767626],\n [ 0.29836051,  0.28297011],\n [-2.8014927 , -0.09047309],\n [ 3.98203732,  1.49951491],\n [ 3.81677148,  0.53095244],\n [ 2.84104923, -0.73858639]])\n>>> y_new\narray([0, 0, 1, 2, 2, 2])\n```", "```py\n>>> gm.score_samples(X).round(2)\narray([-2.61, -3.57, -3.33, ..., -3.51, -4.4 , -3.81])\n```", "```py\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities < density_threshold]\n```", "```py\n>>> gm.bic(X)\n8189.747000497186\n>>> gm.aic(X)\n8102.521720382148\n```", "```py\n>>> from sklearn.mixture import BayesianGaussianMixture\n>>> bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\n>>> bgm.fit(X)\n>>> bgm.weights_.round(2)\narray([0.4 , 0.21, 0.4 , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  ])\n```"]