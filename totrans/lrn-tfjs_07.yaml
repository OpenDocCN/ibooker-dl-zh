- en: Chapter 6\. Advanced Models and UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “It always seems impossible until it’s done.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Nelson Mandela
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You have a baseline for understanding models. You’ve consumed and utilized models
    and even displayed the results in overlays. It might seem like the sky is the
    limit. However, you’ve already seen that models tend to return information in
    various and complex ways. For the Tic-Tac-Toe model, you wanted only one move,
    but it still returns all nine possible boxes, leaving some cleanup work for you
    before you could utilize the model’s output. As models get more complicated, this
    problem can compound. In this chapter, we will select a widespread and complex
    model type for object detection and work through the UI and concepts to give you
    a full sense of what kind of tasks might befall you.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review what your current workflow looks like. First, you select a model.
    Identify if it is a Layers or Graph model. Even if you didn’t have this information,
    you’d be able to figure it out by trying to load it one way or another.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you’ll need to identify the inputs and outputs for the model—not just
    the shape, but what the data actually represents. You batch your data, call `predict`
    on the model, and the output is good to go, right?
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there’s just a little more you should know. Some of the latest
    and greatest models have significant differences from what you’ve come to expect.
    In many ways, they are far superior, and in other ways, they are more cumbersome.
    Don’t fret, because you’ve built a strong foundation in tensors and canvas overlays
    from the previous chapter. With a little coaching, you can handle this new world
    of advanced models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will:'
  prefs: []
  type: TYPE_NORMAL
- en: Dive into how theory can challenge your tensor skills
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn about advanced model characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn lots of new image and machine learning terminology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the best way to draw multiple boxes for object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to draw labels for detections on a canvas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you finish this chapter, you’ll have a strong understanding of the theoretical
    demands of implementing advanced TensorFlow.js models. This chapter serves as
    a cognitive walk-through of one of the most powerful models you can use today,
    and with that comes a lot of learning. It won’t be hard, but get your learning
    cap on, and don’t shy away from the complexities. If you follow the logic explained
    in this chapter, you’ll have a deep understanding and command of theories and
    practices core to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet Again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you cruise around [TFHub.dev](https://tfhub.dev), you might have seen our
    old friend MobileNet mentioned in quite a few flavors and versions. One version
    has a simple name, `ssd_mobilenet_v2`, for image object detection (see the highlighted
    selection in [Figure 6-1](#tfhub_mobilenet_ssd)).
  prefs: []
  type: TYPE_NORMAL
- en: How exciting! It seems you can take your code from your previous TensorFlow
    Hub example and change the model to view a collection of bounding boxes and their
    associated classes, correct?
  prefs: []
  type: TYPE_NORMAL
- en: '![MobileNet SSD on TFHub](assets/ltjs_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. MobileNet for object detection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Upon doing so, you immediately get a failure that asks for you to use `model.executeAsync`
    instead of `model.predict` (see [Figure 6-2](#executeAsync_error)).
  prefs: []
  type: TYPE_NORMAL
- en: '![MobileNet Predict Error](assets/ltjs_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Predict won’t work
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So what went wrong? By now, you might have a flurry of questions.
  prefs: []
  type: TYPE_NORMAL
- en: What is this `executeAsync` that the model wants?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is this MobileNet model for object detection?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why does this model spec not care about input size?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the “SSD” part of the name in regard to machine learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Parcel, you might have gotten errors about a `regeneratorRuntime` not being
    defined. This is due to deprecation in a Babel polyfill. If you get this error,
    you can add the packages `core-js` and `regenerator-runtime` and import them in
    your main file. See the associated [GitHub code for this chapter](https://oreil.ly/LKc8v)
    if you have this issue.
  prefs: []
  type: TYPE_NORMAL
- en: This is a perfect example of an advanced model that needs a little more information,
    theory, and history to comprehend. It’s also a great time to learn some concepts
    we’ve kept buried for convenience. By the end of this chapter, you’ll be ready
    to handle some new terminology, best practices, and features of complex models.
  prefs: []
  type: TYPE_NORMAL
- en: SSD MobileNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The book has mentioned two models by name up to this point, but there’s been
    no elaboration. MobileNet and Inception are published model architectures created
    by the Google AI team. You’ll be architecting your own models in the next chapter,
    but it’s fair to say they won’t be as advanced as these two well-known models.
    Each model has a specific set of benefits and pitfalls. Accuracy isn’t always
    the only metric for a model.
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet is a specific architecture for low-latency, low-power models. That
    makes it excellent for devices and the web. While an Inception-based model will
    be more accurate, MobileNet’s speed and size make it a standard tool for classification
    and object detection on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the [performance and latency charts published by Google that compare
    model versions on devices](https://oreil.ly/dHEKZ). You can see that while Inception
    v2 is several times larger and requires significantly more calculations to make
    a single prediction, MobileNetV2 is much faster, and while it’s not as accurate,
    it’s still close. MobileNetV3 is even promising to be more accurate with only
    a small bump in size. The core research and advancements of these models make
    them excellent battle-tested resources with known trade-offs. It’s for these reasons
    you’ll see the same model architectures used over and over for novel problems.
  prefs: []
  type: TYPE_NORMAL
- en: Both of the aforementioned architectures have been trained by Google with millions
    of images. The classic 1,001 classes MobileNet and Inception can identify come
    from a well-known dataset called [ImageNet](https://image-net.org/about.php).
    So after being trained for a long time on many computers in the cloud, the models
    are tuned for immediate use. While these models are classification models, they
    can be repurposed to detect objects as well.
  prefs: []
  type: TYPE_NORMAL
- en: Just like a building, models can be modified slightly to handle different objectives.
    For example, a theater can be modified from its original purpose of hosting live
    performances to facilitate 3D feature films. Yes, some small changes will need
    to be made, but the overall architecture is significantly reusable. The same goes
    for models that are repurposed from classification to object detection.
  prefs: []
  type: TYPE_NORMAL
- en: There are several different ways to perform object detection. One way is called
    a *region-based convolutional neural network* (R-CNN). Don’t confuse R-CNNs with
    RNNs, which are wholly different and a real thing in machine learning. Region-based
    convolutional neural networks might sound like a spell from *Harry Potter*, but
    they’re just a popular way of detecting objects by looking at patches of an image
    with a sliding window (i.e., sampling a smaller part of the image repeatedly until
    you have covered the whole image). R-CNNs are often slow but extremely accurate.
    The slow aspect doesn’t work well with websites and mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: The second popular way of detecting objects is to use another buzzword, a “fully
    convolutional” approach (more about convolutions in [Chapter 10](ch10.html#the_chapter_10)).
    These approaches do not have a deep neural network, and that’s why they avoid
    requiring a specific input size. That’s right, you don’t need to resize images
    for a fully convolutional approach, and they’re fast, too.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the “SSD” in SSD MobileNet matters. It stands for *single-shot
    detector*. Yes, you and I were probably thinking of solid-state drives, but naming
    things can be hard, so we’ll give data science a pass. SSD model types are architected
    as fully convolutional models that get one shot to identify features of an image
    as a whole. This “single-shot” makes SSDs significantly faster than R-CNNs. Without
    getting too far into details, an SSD model has two major components, a *backbone
    model* that understands how to recognize objects, and an *SSD head* to localize
    the objects. The backbone, in this case, is the fast and friendly MobileNet.
  prefs: []
  type: TYPE_NORMAL
- en: Combining MobileNet and SSD requires a little magic called *control flow* that
    allows you to conditionally run operations in your model. That is what makes the
    `predict` method move from being straightforward to requiring the async call `executeAsync`.
    When a model implements control flow, the synchronous `predict` method will not
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional logic is normally handled in the native language, but that slows
    things down significantly. While most of TensorFlow.js can be optimized by utilizing
    GPU or web assembly (WASM) backends, a conditional statement in JavaScript would
    require unloading optimized tensors and reloading them. The SSD MobileNet model
    hides that headache for you at the low, low cost of utilizing control flow operations.
    While implementing control flow is outside the scope of this book, consuming models
    that utilize these advanced features is not.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the modern nature of this model, it’s not set up to handle batches
    of images. That means the only limitation of the input is not the image size but
    the batch size. It does, however, expect a batch of one, so a 1,024 × 768 RGB
    image would go into this model in the shape `[1, 768, 1024, 3]`, with `1` being
    the stack size for the batch, `768` being the image height, `1024` being the image
    width, and `3` being the RGB values for each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: It’s always important to dig into what kind of input and output you’ll be dealing
    with. It’s worth noting that the model’s output bounding boxes follow the classic
    height and then width architecture of the input, unlike the pet faces detector.
    That means the bounding boxes will be `[y1, x1, y2, x2]` instead of `[x1, y1,
    x2, y2]`. Small hiccups like these can be quite frustrating if they aren’t caught.
    Your bounding boxes would look completely broken. Whenever you implement a new
    model, it’s important that you verify the specification from all available documentation.
  prefs: []
  type: TYPE_NORMAL
- en: There’s one last caveat before digging into code. In my experience, object detection
    in production is rarely used to identify thousands of different classes, as you’ve
    seen in MobileNet and Inception. There are many good reasons for this, so object
    detection is usually tested and trained on a few classes. One common group of
    labeled data that people use for object detection training is [the Microsoft Common
    Objects in Context (COCO)](https://cocodataset.org/#home) dataset. This SSD MobileNet
    used that dataset to teach the model to see 80 different classes. While 80 is
    a significant drop from 1,001 possible classes, it’s still an impressive set.
  prefs: []
  type: TYPE_NORMAL
- en: Now you understand more about SSD MobileNet than most people who use it. You
    know it’s an object detection model that uses control flow to link the MobileNet
    speed to SSD results for 80 classes. This knowledge will help you later in interpreting
    the model’s results.
  prefs: []
  type: TYPE_NORMAL
- en: Bounding Outputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you understand the model, you can get the results. The value returned
    by `executeAsync` in this model is a normal JavaScript array of two tensor stacks.
    The first tensor stack is what was detected, and the second tensor stack is the
    bounding box stack for each detection—in other words, scores and their boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Model Outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can review the results of an image with a few lines of code. The following
    does just that and is also available in the [chapter’s source code](https://oreil.ly/JLo5C):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_advanced_models_and_ui_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the TFHub URL for the JavaScript model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_advanced_models_and_ui_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The input is expanded in rank to be a batch of one with the shape [1, height,
    width, 3].
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_advanced_models_and_ui_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting tensor is [1, 1917, 90], which has returned 1,917 detections with
    the 90 probability values in each row adding up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_advanced_models_and_ui_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The tensor is shaped as [1, 1917, 4], providing the bounding boxes for each
    of the 1,917 detections.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-3](#ssd_results) displays the output of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![SSD MobileNet output in console](assets/ltjs_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. The output from the previous code
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’re likely surprised to see 90 values instead of 80 for the possible classes.
    It’s still only 80 possible classes. Ten of the result indices in that model aren’t
    used.
  prefs: []
  type: TYPE_NORMAL
- en: While it looks like you’re done, there are a few red flags. As you might assume,
    drawing 1,917 boxes isn’t going to be useful or effective, but try it and see.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying All Outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s time to write the code to draw multiple bounding boxes. The gut reaction
    is that 1,917 detections is excessive. It’s time to write some code to verify
    this. Since the code is getting a bit promise-heavy, it’s a good time to switch
    over to async/await. This will stop the code from indenting further and will increase
    readability. If you’re unfamiliar with switching between promises and async/await,
    please review that aspect of JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: The full code to draw the model detections can be found in the [book source
    code file *too_many.html*](https://oreil.ly/bMPVa). This code is using the same
    techniques as described in the object localization section in the previous chapter,
    but with the parameter order adjusted to fit the expected output of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Drawing every single detection regardless of the model’s confidence wasn’t hard,
    but the resulting output is completely unusable, as shown in [Figure 6-4](#useless_boxes).
  prefs: []
  type: TYPE_NORMAL
- en: '![Too many detections](assets/ltjs_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. 1,917 bounding boxes, rendering the image useless
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The mess you’re seeing in [Figure 6-4](#useless_boxes) indicates that there
    are plenty of detections, but there’s no clarity. Can you guess what’s causing
    this noise? There are two factors in the noise you’re seeing.
  prefs: []
  type: TYPE_NORMAL
- en: Detection Cleanup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first criticism of the resulting boxes is that there’s no quality or quantity
    check. The code isn’t checking the probability of the detection values or filtering
    for the most confident. For all you know, the model might be 0.001% sure of a
    detection, and that infinitesimal detection is not worth drawing a box. The first
    step to cleaning this up is to set a minimum threshold for a detection score and
    a maximum number of boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Second, after close inspection, the boxes that are being drawn seem to be detecting
    the same objects over and over with slight variations. This will be verified in
    a bit. It would be best that their overlap was limited when they have identified
    the same class. If two overlapping boxes detect a person, just take the one with
    the highest detection score.
  prefs: []
  type: TYPE_NORMAL
- en: The model did (or did not do) a great job at finding things in the photo, so
    now it’s your job to apply the cleanup.
  prefs: []
  type: TYPE_NORMAL
- en: Quality Checking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You’ll want the highest-ranked predictions. You can do this by suppressing
    any bounding box below a given score. Identify the highest scores of the entire
    detection series with a single call to `topk` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Calling `topk` on all the detection results returns an array with only the best
    of the best, as `k` defaults to `1`. The index of each detection corresponds with
    the class, and the value is the confidence of the detection. The output would
    look like [Figure 6-5](#topk_ssd_results).
  prefs: []
  type: TYPE_NORMAL
- en: '![Topk detections logs on entire batch](assets/ltjs_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. The `topk` call works on the entire batch
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the prominent detection is lower than a given threshold, you can deny drawing
    the box. You could then limit the boxes you’re drawing to draw only the top N
    predictions. We’ll leave the code for this exercise to the Chapter Challenge,
    because it does nothing to solve the second issue. Quality checking alone causes
    clusters of boxes around your strongest predictions, rather than singular predictions.
    The resulting boxes look like your detection system drank too much coffee (see
    [Figure 6-6](#overlap)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Box overlaps](assets/ltjs_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Drawing the 20 top predictions creates fuzzy boxing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fortunately, there’s a built-in way to solve these fuzzy boxes, and it provides
    you with some new terminology for your dinner parties.
  prefs: []
  type: TYPE_NORMAL
- en: IoUs and NMS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, you might have thought IoUs were just an approved fiat currency backed
    by Lloyd Christmas, but in the world of object detection and training, they stand
    for *intersection over union*. Intersection over union is an evaluation metric
    for identifying accuracy and overlap of object detectors. The accuracy part is
    great for training, and the overlap is great for cleaning up overlapping output.
  prefs: []
  type: TYPE_NORMAL
- en: IoU is the formula for identifying how much area two boxes share in their overlap.
    If the boxes overlap perfectly, the IoU is one, and the less they fit, the closer
    the number is to zero. The title “IoU” comes from the formula for this calculation.
    The intersection area of the boxes is divided by the union area of the boxes,
    as illustrated in [Figure 6-7](#IoU).
  prefs: []
  type: TYPE_NORMAL
- en: '![IoU diagram](assets/ltjs_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Intersection over union
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now you have a quick formula to check the similarity of bounding boxes. Using
    the IoU formula, you can enact an algorithm called *nonmaximum suppression* (NMS)
    to remove duplicates. NMS automatically grabs the highest-scoring boxes and dismisses
    any similar boxes with IoU over a designated level. [Figure 6-8](#active_nms)
    shows a simple example with three scored boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-max Suppression diagram](assets/ltjs_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Only the max survives; the other lower-scoring boxes are removed
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you set the IoU for NMS to 0.5, then any box that shares 50% of its area
    with a higher-scoring box will get deleted. This works expertly for eliminating
    boxes that overlap the same object. However, it can be a problem for two objects
    that overlap each other and should have two bounding boxes. This is an issue for
    real objects that have an unfortunate angle that places them on top of one another
    because their bounding boxes will cancel each other out, and you’ll get only one
    detection for two actual objects. For this situation, you can enable an advanced
    version of NMS called [Soft-NMS](https://arxiv.org/pdf/1704.04503.pdf), which
    will decay scores of overlapping boxes rather than removing them. If their scores
    are still high enough after being decayed, the detections will survive and get
    their own bounding box, even if the IoU was extremely high. [Figure 6-9](#IoU_2)
    properly identifies two objects with extreme intersection with Soft-NMS.
  prefs: []
  type: TYPE_NORMAL
- en: '![Soft-NMS example](assets/ltjs_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Real-world overlapping objects can still be detected with Soft-NMS
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The best part about Soft-NMS is that it’s built into TensorFlow.js. I recommend
    that you utilize this TensorFlow.js function for all your object detection needs.
    For this exercise, you will use that incorporated method, named `tf.image.nonMaxSuppressionWithScoreAsync`.
    TensorFlow.js has quite a few NMS algorithms built in, but `tf.image.nonMaxSuppressionWithScoreAsync`
    has two qualities that make it excellent for use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`WithScore` provides Soft-NMS support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Async` stops the GPU from locking up the UI thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be careful when using nonasync advanced methods because they can lock the entire
    UI. If you would like to remove the Soft-NMS aspect for any reason, you can set
    the last parameter (the Soft-NMS Sigma) to zero, and then you’ve got a traditional
    NMS.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In just a few lines of code, you’ve clarified the SSD results to a few unclouded
    detections.
  prefs: []
  type: TYPE_NORMAL
- en: The result will be an object with two properties. The `selectedIndices` property
    will be a tensor of the indices of the boxes that made the cut, and the `selectedScores`
    would be their corresponding scores. You could loop over the chosen results and
    draw your bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_advanced_models_and_ui_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a normal JavaScript array from the indices of the resulting high-scoring
    boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_advanced_models_and_ui_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Get the highest-scoring index from a previous `topk` call.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_advanced_models_and_ui_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The classes are imported as an array to match the given result indices. This
    structure is just like the code from the Inception example in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_advanced_models_and_ui_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Log what is being boxed in the canvas so you can verify the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_advanced_models_and_ui_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Disallow negative numbers, so boxes at least start in frame. Otherwise, some
    boxes would be cut off from the top left.
  prefs: []
  type: TYPE_NORMAL
- en: The number of detections returned varies but is limited to the specifications
    set in the NMS. The example code resulted in five correct detections, as illustrated
    in [Figure 6-10](#non_max).
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-max Suppression result](assets/ltjs_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. The clean Soft-NMS detected results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The console logs from the loop print out that the five detections were three
    “person” detections, one “wine glass,” and one “dining table.” Compare the five
    logs in [Figure 6-11](#nm_results) with the five bound boxes in [Figure 6-10](#non_max).
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-max Suppression result logs](assets/ltjs_0611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-11\. The result log classes and confidence levels
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The UI has come so far. It only makes sense that the overlays should identify
    the detections and their percentage confidence. Average users don’t know to look
    at the console for logs.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Text Overlays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are all kinds of fancy ways you can add text to a canvas and have it identify
    the bounding box associated. For this demo, we’ll review the simplest method and
    leave the more aesthetic layouts as a task for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing text to a canvas can be done with the canvas 2D context’s `fillText`
    method. You can position the text at the top left of every box by reusing the
    `X, Y` coordinates you used to draw the boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two issues to note with drawing text:'
  prefs: []
  type: TYPE_NORMAL
- en: Text can easily have low contrast with the background.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text that is drawn at the same time as a box might be covered by boxes drawn
    afterward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortunately, both of these are easy to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Solving Low Contrast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The typical method for creating readable labels is to draw a background box
    and then place your text. As you know, `strokeRect` creates a box with no fill
    color, so it should be no surprise that `fillRect` draws a box with a fill color.
  prefs: []
  type: TYPE_NORMAL
- en: How big should the rectangle be? A simple answer would be to draw the rectangle
    to the width of the detection box, but there’s no guarantee that the box will
    be wide enough, and when the box is very wide, this creates large blocking bars
    in your results. The only effective solution is to measure the text and draw the
    box accordingly. Text height can be set by utilizing the context `font` property,
    and the width can be determined with `measureText`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you might consider that you have to subtract the font height from your
    drawing position so that it draws the text inside the box rather than on top of
    the box, but context already has a property you can set to keep life simple. The
    `context.textBaseline` property has all kinds of options. [Figure 6-12](#textBaseline)
    shows starting points for each of the possible property options.
  prefs: []
  type: TYPE_NORMAL
- en: '![text baseline options](assets/ltjs_0612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. Setting the `textBaseline` to `top` keeps the text inside the
    X and Y coordinates
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now you know how to draw a filled rectangle to the appropriate size and place
    your labels inside. You can combine these methods inside your `forEach` loop where
    you’re drawing detections and draw your results. The labels are drawn in the upper-left
    side of each detection, as shown in [Figure 6-13](#example_label).
  prefs: []
  type: TYPE_NORMAL
- en: '![Drawing results displayed](assets/ltjs_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. The labels are drawn with each box
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s important that the text is drawn *after* the background box; otherwise,
    the box will be painted on top of the text. For our purposes, the labels will
    be drawn with a slightly different color green than the bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_advanced_models_and_ui_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Set the font and size to use on the labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_advanced_models_and_ui_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Set `textBaseline` as mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_advanced_models_and_ui_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Add a little horizontal padding to be used in the `fillRect` render.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_advanced_models_and_ui_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Draw the rectangle using the same `startX` and `startY` that were used to draw
    the bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_advanced_models_and_ui_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Change the `fillStyle` to be black for the text render.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_advanced_models_and_ui_CO3-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, draw the text. This probably should have been padded a little, too.
  prefs: []
  type: TYPE_NORMAL
- en: Now each detection has a nearly readable label. However, depending on your image,
    you might have noticed a few issues that we will now solve.
  prefs: []
  type: TYPE_NORMAL
- en: Solving Draw Order
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though the labels are drawn on top of the boxes, the boxes are drawn at
    separate times and can easily overlap with some existing label text, rendering
    them difficult or even impossible to read. As you can see in [Figure 6-14](#ctx_overlap),
    the dining table percentage is tough to read due to an overlapping detection.
  prefs: []
  type: TYPE_NORMAL
- en: '![Context overlap issue](assets/ltjs_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-14\. Context draw order overlap issue
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One way to solve this problem is to iterate over the detections and draw boxes,
    and then do a second pass and draw the text. This would ensure the text was drawn
    last at the cost of iterating over the detections in two subsequent loops.
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative, you could handle this with code. You can set the context
    `globalCompositeOperation` to do all kinds of amazing things. One simple operation
    is to tell the context to render above or below existing content, effectively
    setting a z-order.
  prefs: []
  type: TYPE_NORMAL
- en: The `strokeRect` calls can be set with `globalCompositeOperation` to `destination-over`.
    This means any pixels that exist in the destination will win and be placed over
    the added content. This effectively draws under any existing content.
  prefs: []
  type: TYPE_NORMAL
- en: Then, when you’re drawing your labels, return `globalCompositionOperation` to
    its default behavior, which is `source-over`. This draws the new source pixels
    over any existing drawings. If you flip back and forth between these two operations,
    you can ensure your labels are top priority and handle everything inside the master
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Altogether, the singular loop to draw bounding boxes, label boxes, and labels
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_advanced_models_and_ui_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Draw under any existing content.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_advanced_models_and_ui_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Draw over any existing content.
  prefs: []
  type: TYPE_NORMAL
- en: The result is a dynamic human-readable result that you can share with your friends
    (see [Figure 6-15](#destination_over)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Completely working Object Detection](assets/ltjs_0615.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-15\. Using `destination-over` fixes overlap issues
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Connecting to a Webcam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What’s the benefit of all this speed? As mentioned earlier, there was a choice
    of SSD over R-CNN, MobileNet over Inception, and drawing the canvas in one pass
    instead of two. When you load the page, it looks pretty slow. It seems to take
    at least four seconds just to load and render.
  prefs: []
  type: TYPE_NORMAL
- en: Yes, getting everything in place takes a moment, but after the memory has been
    allocated and the model is downloaded, you can see some pretty significant speed.
    Yes, it’s enough to run live detection on your webcam.
  prefs: []
  type: TYPE_NORMAL
- en: The key to speeding up the process is to run your setup code once and then move
    on to running a loop of detections. This does mean you need to break up the monolithic
    codebase from this lesson; otherwise, you’ll get an unusable interface. For simplicity
    you can break up the project as shown in [Example 6-1](#overall_webcam_arch).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. Breaking up a codebase
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_advanced_models_and_ui_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The longest delay is when loading the model; this should happen first and only
    once.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_advanced_models_and_ui_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: For efficiency, you can capture the video element once and pass that reference
    into the places it’s needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_advanced_models_and_ui_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the webcam should happen only once.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_advanced_models_and_ui_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The `performDetections` method can loop forever when detecting the content in
    the webcam and drawing the boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_advanced_models_and_ui_CO5-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t let the errors get swallowed up with all these `awaits`.
  prefs: []
  type: TYPE_NORMAL
- en: Moving from Image to Video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s actually not complicated to move from a still image to video because the
    hard part of turning what you see into a tensor is handled by `tf.fromPixels`.
    The `tf.fromPixels` method can read a canvas, an image, and even a video element.
    So the complexity lies in changing out the `img` tag for a `video` tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'You start by switching out the tags. The original `img` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It’s worth noting that the video element is a bit more complicated with width/height
    properties because there’s the input video width/height and the actual client
    width/height. For this reason, all the calculations that were using `width` will
    need to use `clientWidth`, and similarly, `height` will need to be `clientHeight`.
    If you use the wrong property, the boxes will not align or might not even show
    up at all.
  prefs: []
  type: TYPE_NORMAL
- en: Activating a Webcam
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our purposes, we’ll only set up the default webcam. This corresponds to
    point four in [Example 6-1](#overall_webcam_arch). If you’re unfamiliar with `getUserMedia`,
    take a moment to analyze how the video element is connected to the webcam. This
    is also the time when you can move your canvas context setup to fit the video
    element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_advanced_models_and_ui_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: These are the webcam user media configuration constraints. There are [several
    options](https://oreil.ly/MkWml) you can apply here, but for simplicity, it’s
    kept quite simple.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_advanced_models_and_ui_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This conditional check is to support older browsers that do not support the
    new `srcObject` configuration. This can likely be deprecated depending on your
    support needs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_advanced_models_and_ui_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: You can’t access the video until it is loaded, so the event is wrapped in a
    promise so it can be awaited.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_advanced_models_and_ui_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the event you’ll need to wait for before you can pass the video element
    to `tf.fromPixels`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_advanced_models_and_ui_CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: While taking this opportunity to set up the canvas, notice the use of `clientWidth`
    instead of `width`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_advanced_models_and_ui_CO6-6)'
  prefs: []
  type: TYPE_NORMAL
- en: The promise resolves with information you’ll need to pass along to the detect
    and draw loop.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing Detections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lastly, you perform your detections and drawing just like you did for an image.
    At the beginning of each call you will need to remove all the detections from
    the previous call; otherwise, your canvas will slowly fill up with old detections.
    Clearing the canvas is simple; you can use `clearRect` to remove anything from
    the designated coordinates. Passing the entire canvas width and height will wipe
    the slate clean.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: At the end of each drawn detection, *do not* dispose the model in your cleanup,
    as you’ll need it in each detection. Everything else, however, can and should
    get disposed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `performDetections` function that was identified in [Example 6-1](#overall_webcam_arch)
    should call itself recursively in an infinite loop. The function can loop faster
    than a canvas can draw. To assure it’s not wasting cycles, use the browser’s `requestAnimationFrame`
    to throttle this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it. You’ve moved from a still image to video input at real-time speeds
    with a few logical adjustments. On my computer I was seeing around 16 frames per
    second. In the world of AI, that is more than fast enough to handle most use cases.
    I used it to facilitate proof I’m at least 97% a person, as shown in [Figure 6-16](#webcam_detection_hat).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of SSD MobileNet working in the browser](assets/ltjs_0616.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-16\. Fully functioning webcam with SSD MobileNet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Chapter Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on tackling one of the most useful yet complex models to exist
    on TensorFlow Hub. While it’s simple to hide the complexity of this model with
    JavaScript, you’re now familiar with some of the most impressive concepts in object
    detection and clarification. Machine learning is burdened with the concept of
    solving a problem quickly and then solving the follow-up code to attach the majestic
    properties of AI to a given domain. You can expect a good bit of research to accompany
    any significantly advanced model and field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter Challenge: Top Detective'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NMS simplifies both sorting and eliminating detections. Let’s pretend you wanted
    to solve the problem of identifying the top predictions and then sorting them
    from highest to lowest, so you could create a graphic like [Figure 6-6](#overlap).
    Rather than relying on NMS to find your most viable and highest values, you’ll
    need to solve the highest-value problem yourself. Take this small but similar
    grouping as the entire dataset of detection. Imagine this `[1, 6, 5]` tensor collection
    of detections is your `result[0]`, and you want only the top three detections
    with the highest confidence values for any class. How could you solve this?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Your resulting solution should print `[3, 4, 2]` because the tensor at index
    3 has the largest value (12.2) in its set of all the values, followed by index
    4 (which contains 5.3) and then index 2 (5.2).
  prefs: []
  type: TYPE_NORMAL
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  prefs: []
  type: TYPE_NORMAL
- en: Review Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What does SSD stand for in the world of object detection machine learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What method do you need to use to predict on a model that uses dynamic control
    flow operations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many classes and how many values does SSD MobileNet predict?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the method for deduplicating detections of the same object?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a drawback of using large synchronous TensorFlow.js calls?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What method should you use to identify the width of a label?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What `globalCompositeOperation` overwrites existing content on the canvas?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these questions are available in [Appendix A](app01.html#book_appendix).
  prefs: []
  type: TYPE_NORMAL
