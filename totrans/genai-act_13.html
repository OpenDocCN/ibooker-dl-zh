<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">11</span> </span> <span class="chapter-title-text">Scaling up: Best practices for production deployment</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Challenges and deployment options to consider for an application ready for production</li> 
    <li class="readable-text" id="p3">Production best practices covering scalability, latency, caching, and managed identities</li> 
    <li class="readable-text" id="p4">Observability of LLM applications, with some practical examples</li> 
    <li class="readable-text" id="p5">LLMOps and how it compliments MLOps</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>When organizations are ready to take their generative AI models from the realm of proof of concept (PoC) to the real world of production, they embark on a journey that requires careful consideration of key aspects. This chapter will discuss deployment and scaling options, sharing best practices for making generative AI solutions operational, reliable, performant, and secure.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>Deploying and scaling generative AI models in a production setting is a complex task that requires meticulous consideration of various factors. While building a PoC can be a thrilling way to test an idea’s feasibility, taking it to production introduces a whole new realm of operational, technical, and business considerations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>This chapter will focus on the key aspects developers must consider when deploying and scaling generative AI models in a production environment. We will discuss the operational criteria critical to monitoring the systems’ health, deployment options, and best practices for ensuring reliability, performance, and security.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>We will also delve into the concepts of large language model operations (LLMOps) and machine learning operations (MLOps), which are essential and empowering for managing the lifecycle of generative AI models in production. Additionally, the chapter will underscore the importance of cost management and budgeting for models deployed in production and provide some enlightening case studies of successful deployment and scaling of generative AI models in a production environment.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>By the end of this chapter, you will experience a transformative journey of understanding the key considerations and best practices for deploying generative AI models to production. Let’s dive into this exciting world of knowledge by exploring some of the challenges most enterprises face when deploying a GenAI application to production.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_190"><span class="num-string">11.1</span> Challenges for production deployments</h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Generative AI apps in an enterprise production environment face specific challenges that differ from those in conventional machine learning (ML). However, some of the challenges remain the same. For example, developers must deal with the complicated relationship of computational resource requirements, data quality standards, performance goals, the possibility of output variability, and the changing security situation around these powerful models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>One of the primary challenges in deploying generative AI models is their complexity. These models can be computationally intensive and require significant resources to train and deploy, even when factoring in today’s cloud-scale infrastructure and computing. Consequently, scaling the models to handle large volumes of requests or deploying them in resource-constrained environments can be difficult. Developers must carefully consider the hardware and software requirements of the models, as well as the infrastructure required to support them to ensure that they can be deployed and scaled effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>Another challenge in deploying generative AI models is ensuring the quality and availability of data. A key aspect of the quality of data is also knowing the source of the data and whether it is an authoritative or authentic source, which is important. These models rely heavily on data quality and availability, and any problems with the data can significantly affect the models’ performance and accuracy. Developers must implement robust data validation and quality control processes and monitor the data sources and pipelines used to train and deploy the models to ensure the data is accurate, relevant, and current. This can be done by measuring accuracy with predictive performance metrics, relevance through task-specific evaluations, and currency by tracking data freshness. Enterprises should implement robust monitoring systems and document data lineage to maintain high data integrity standards. Chapter 12 covers evaluations and benchmarks in more detail.</p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>Model performance and accuracy are also critical considerations when deploying generative AI models. Developers must carefully monitor the models’ performance and accuracy and implement regular testing and validation processes to ensure the models function as expected. In an ideal world, this requires a deep understanding of the models’ underlying algorithms and architectures and the ability to diagnose and resolve any problems that may arise. However, in a practical sense, most enterprises will have a cross-functional team of developers, data scientists, and business experts who will collectively help understand, guide, and model architecture and deployment considerations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>Reliability and availability are also key considerations in deploying generative AI models. These models must be reliable and available to meet the business’ needs, which requires careful consideration of factors such as redundancy, failover, and disaster recovery. Developers must implement robust monitoring and maintenance processes to ensure that the models function as expected and be prepared to respond quickly to any problems. Of course, most enterprises rely on the hyper-scaler they are using to provide much of this service. These services’ underlying reliability and availability are closely linked to those providers. With small language models (SLMs) also in the mix and being used with large language models (LLMs), the reliability and scale considerations are different, especially when considering edge deployments for SLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>Security and compliance are also critical considerations. These models can process sensitive data, which must be protected from unauthorized access, theft, or misuse. Enterprises must ensure that the models comply with relevant regulations and standards, such as GDPR, HIPAA, or PCI-DSS, and implement robust security controls to protect the data and the models themselves.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>Companies must first know each regulation’s requirements to comply with these data protection regulations. This involves managing consent, securing sensitive information, and handling data breaches. They should track and control personal data used by LLMs, apply strong security measures, and include privacy in the system design from the beginning. Frequent compliance audits, employee training, and vendor management are important for maintaining standards. A good incident response plan for data breaches and careful record-keeping will help with compliance. Furthermore, using built-in compliance features of cloud services can assist in meeting these requirements. By keeping up with compliance standards and taking these steps, enterprises can use LLMs to match legal and regulatory obligations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>Cost management is another important consideration. Models can be expensive to deploy and maintain, particularly when it comes to computing, storage, and networking resources. Developers must carefully manage the costs associated with deploying and scaling the models and be prepared to make tradeoffs between cost and performance as needed.</p> 
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>Integrating existing systems and workflows is also critical in deploying generative AI models. These models often must be integrated with existing systems and workflows, which can be complex and time-consuming. Developers must ensure that the models are compatible with existing systems and can be easily integrated into existing workflows. They must also be prepared to work closely with other teams and stakeholders to ensure a smooth deployment.</p> 
  </div> 
  <div class="readable-text intended-text" id="p21"> 
   <p>Human-in-the-loop considerations are another important factor. These models often require human intervention or oversight, particularly when they are used to make critical decisions or generate content that requires human review. Developers must ensure the models are designed with human-in-the-loop considerations and implement robust processes for managing and monitoring human intervention.</p> 
  </div> 
  <div class="readable-text intended-text" id="p22"> 
   <p>Ethical considerations are the final important factor in deploying generative AI models. These models can have significant ethical implications, particularly regarding bias, fairness, and transparency. Thus, developers must ensure that the models are designed and deployed ethically and must be prepared to address ethical concerns. Chapter 13 covers this topic in depth.</p> 
  </div> 
  <div class="readable-text intended-text" id="p23"> 
   <p>By understanding these challenges and considerations, developers can design and deploy generative AI models that are scalable, reliable, and secure and meet the business’ needs in a production environment. Several challenges and considerations must be addressed when deploying generative AI models in a production environment to ensure successful implementation. The following key points highlight these critical aspects:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p24"> <em>Complexity of generative AI models</em><em> </em>—High computational requirements and significant resources are required for training and deployment. Consider hardware, software, and infrastructure for effective scaling. </li> 
   <li class="readable-text" id="p25"> <em>Data quality and availability</em><em> </em>—These are essential for model performance and accuracy. Implement robust data validation and quality control processes, and monitor data sources. </li> 
   <li class="readable-text" id="p26"> <em>Model performance and accuracy</em><em> </em>—Regular testing and validation are required. Cross-functional teams can aid in understanding and resolving problems. </li> 
   <li class="readable-text" id="p27"> <em>Reliability and availability</em><em> </em>—Implement redundancy, failover, and disaster recovery. Use robust monitoring and maintenance processes. There is dependence on hyper-scalers for service reliability. </li> 
   <li class="readable-text" id="p28"> <em>Security and compliance</em><em> </em>—Protect sensitive data from unauthorized access. Ensure compliance with regulations such as GDPR, HIPAA, and PCI-DSS. Implement security controls, and manage data protection effectively. </li> 
   <li class="readable-text" id="p29"> <em>Cost management</em><em> </em>—This involves careful management of computing, storage, and networking costs, balancing cost and performance. </li> 
   <li class="readable-text" id="p30"> <em>Integration with existing systems</em><em> </em>—Ensure compatibility and smooth integration with current systems and workflows. Collaborate with other teams and stakeholders. </li> 
   <li class="readable-text" id="p31"> <em>Human-in-the-loop considerations</em><em> </em>—Design models with human oversight for critical decisions. Implement processes for managing human intervention. </li> 
   <li class="readable-text" id="p32"> <em>Ethical considerations</em><em> </em>—Address bias, fairness, and transparency problems. Ensure ethical design and deployment of models. </li> 
  </ul> 
  <div class="readable-text" id="p33"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_191"><span class="num-string">11.2</span> Deployment options</h2> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Several options are available when deploying generative AI apps, with the best choice depending on factors such as model size and complexity, desired scalability and availability, and available infrastructure and resources.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>Cloud deployment offers advantages such as scalability, diverse compute options, and managed services for easier deployment. However, consider potential ongoing costs, vendor lock-in, and data privacy concerns. On-premise deployment provides greater control, performance optimization, and data security, but it requires significant upfront investment and in-house expertise and may involve slower scaling. A hybrid approach combines both strengths, allowing sensitive data to remain on-premise, while using cloud scalability and introducing management complexity.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>Regardless of the chosen deployment path, several core technologies facilitate the process. Containerization ensures consistent model execution across environments, while serverless functions are ideal for dynamic workloads. API gateways provide structured access for other applications to utilize models, and specialized GenAI platforms can streamline the deployment and management of LLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>Cloud deployment is popular due to its scalability and flexibility, particularly with providers such as Microsoft Azure, Amazon Web Services (AWS), and Google Cloud Platform (GCP). Depending on their needs, developers can choose from virtual machines, containers, or serverless functions. However, it’s crucial to carefully assess the required infrastructure and resources, including GPUs, memory, storage, and network bandwidth. Implementing load balancing and redundancy strategies ensures scalability and availability, while robust monitoring and automated testing are essential for maintaining performance and health.</p> 
  </div> 
  <div class="readable-text intended-text" id="p38"> 
   <p>By carefully considering these factors, developers can ensure reliable, scalable, and cost-effective deployment of generative AI Apps, regardless of the chosen environment.</p> 
  </div> 
  <div class="readable-text" id="p39"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_192"><span class="num-string">11.3</span> Managed LLMs via API</h2> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>In addition to the deployment options previously discussed, it’s important to note that some LLMs are only available via an API hosted online in a managed manner. This is often the case with cutting-edge models developed by AI research organizations or large tech companies. As we know, GenAI models require significant computational resources, making them difficult to run on-premise or in a hybrid manner. Table 11.1 outlines some of the advantages of managed LLMs.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p41"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 11.1</span> Advantages of managed LLMs</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Advantages 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Ease of use <br/></td> 
      <td>  Managed LLMs via API are typically easy to use. Developers can send requests to the API and receive responses without worrying about the underlying infrastructure or model complexities. <br/></td> 
     </tr> 
     <tr> 
      <td>  Continuous updates <br/></td> 
      <td>  The providers of these managed LLMs often continuously update and improve their models. An API allows you to take advantage of these improvements without manually updating your models. <br/></td> 
     </tr> 
     <tr> 
      <td>  Scalability <br/></td> 
      <td>  Managed LLMs via API can handle high volumes of requests and scale automatically based on demand, similar to other cloud-based services. <br/></td> 
     </tr> 
     <tr> 
      <td>  Model complexity <br/></td> 
      <td>  LLMs are enormously complex ML models and can present several challenges for enterprises, particularly those without extensive experience in AI and ML. Managed services offload this complexity to the provider, exposing the inference via an API. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>There are also some constraints and challenges to consider when using managed LLMs via an API, as outlined in table 11.2.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p43"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 11.2</span> Considerations with managed LLMs</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Considerations 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Cost <br/></td> 
      <td>  The cost of using a managed LLM via API can vary significantly based on usage. While some providers offer free tiers, more extensive use can incur significant costs. <br/></td> 
     </tr> 
     <tr> 
      <td>  Dependency <br/></td> 
      <td>  Using a managed LLM via API, you depend on the provider for the model and the infrastructure. If the provider experiences downtime or discontinues the service, this could affect your application. <br/></td> 
     </tr> 
     <tr> 
      <td>  Data privacy <br/></td> 
      <td>  Data is sent to the provider’s servers for processing using a managed LLM via API, which can raise privacy concerns, especially regarding sensitive data. <br/></td> 
     </tr> 
     <tr> 
      <td>  Limited customization <br/></td> 
      <td>  While managed LLMs via API offers ease of use, they typically offer limited customization options. You’re limited to the capabilities and configurations provided by the API and can’t modify the underlying model. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>In summary, while managed LLMs via API offer several benefits, they also come with certain considerations. Whether they are the right option for your GenAI application depends on your needs and constraints. If you require a high level of customization, have strict data privacy requirements, or need to run your model offline, then an on-premise or hybrid deployment might be more suitable. However, a managed LLM via API could be a good choice if you value ease of use, continuous updates, and automatic scaling.</p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_193"><span class="num-string">11.4</span> Best practices for production deployment</h2> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>To use GenAI applications, a comprehensive approach is required that involves careful planning and execution to ensure scalability, reliability, and security. When using LLMs in your application, you need to think about aspects such as LLMOps, observability, and tooling to handle the lifecycle of your application effectively. In addition, you need to consider other aspects such as model serving and management, reliability and performance considerations, and security and compliance considerations. These areas are important to ensuring that the application does what it should and follows high reliability, security, and compliance standards.</p> 
  </div> 
  <div class="readable-text intended-text" id="p47"> 
   <p>In this section, you will learn about many of these aspects, such as metrics for LLM inference, how to measure and understand latency for LLMs, scalability, inference options for LLMs, quotas and rate limits, and observability. It will provide you with a complete guide to help you scale the GenAI application in production.</p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_194"><span class="num-string">11.4.1</span> Metrics for LLM inference</h3> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>One of the most important metrics from the production deployment perspective is related to LLM inference. This is the main area that we all work on and deal with when developing GenAI applications. As we have seen, LLMs produce text in two steps: the prompt, where the input tokens are processed at once, and decoding, where text is created one token at a time sequentially. Each created token is added to the input and used again by the model to create the next token. Generation ends when the LLM produces a special stop token or when a user-defined condition is satisfied (e.g., a maximum number of tokens has been produced).</p> 
  </div> 
  <div class="readable-text intended-text" id="p50"> 
   <p>Understanding and managing key operational metrics related to LLM inference becomes critical. Many of these metrics are new and still too early for most users to be comfortable with, but the following four metrics are particularly important: time to the first token, time per output token, latency, and throughput. Table 11.3 outlines the definition and importance of these operational criteria. Later in the chapter, you will see how to measure this on our LLM deployment.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p51"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 11.3</span> LLM inference metrics</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Metric 
       </div></th> 
      <th> 
       <div>
         Definition 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Time to first token (TTFT) <br/></td> 
      <td>  Measures the time it takes for the model to generate the first token after a user query. Lower TTFT means a more responsive user experience. TTFT is influenced by the time required to process the prompt and generate the first output token. <br/></td> 
     </tr> 
     <tr> 
      <td>  Time-per-output token (TPOT) <br/></td> 
      <td>  Calculates the time required for the model to generate one token for a specific query. Lower TPOT means faster text generation. The model size, the hardware configuration, and the decoding algorithm influence TPOT. <br/></td> 
     </tr> 
     <tr> 
      <td>  Latency <br/></td> 
      <td>  This metric measures the time it takes for data to move from its starting point to its destination. In the case of LLMs, it is the time for the model to generate a response to the user. The model and the tokens generated influence LLMs’ latency. Generally, most of the time is spent generating complete tokens, which are generated one at a time. The longer the generation, the higher the latency. <br/></td> 
     </tr> 
     <tr> 
      <td>  Throughput <br/></td> 
      <td>  Measures the amount of data that can be transferred in a unit of time. In this case, the number of output tokens per second on a deployment unit can be served across all requests. <br/></td> 
     </tr> 
     <tr> 
      <td>  Request per second (RPS) <br/></td> 
      <td>  RPS measures the throughput of LLMs in production and indicates the number of requests an LLM can handle every second. This metric is crucial for understanding the scalability and efficiency of LLMs when deployed in real-world applications. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text print-book-callout" id="p52"> 
   <p><span class="print-book-callout-head">Note</span>  RPS and throughput are often used interchangeably in the context of performance metrics, but they can have nuanced differences. In essence, while RPS is about the incoming load, throughput is about the server’s output or the successful handling of that load. A high throughput with a high RPS indicates a well-performing server, while a low throughput with a high RPS might suggest that the server is struggling to keep up with the demand.</p> 
  </div> 
  <div class="readable-text" id="p53"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_195"><span class="num-string">11.4.2</span> Latency</h3> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>Latency is a common metric used by almost everyone, but it is unclear and needs to be reexamined generative AI. The usual definition of latency does not fit well, as those APIs only gave back one result instead of multiple streaming responses. Because output generation depends greatly on input, GenAI has different latency points to consider. For instance, one latency is the first token latency; another is the full end-to-end latency after all the generation is done.</p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>We can’t rely on the second end-to-end latency alone, as we now know prompt size and output token count are the key influencing factors. The generation varies with the query (i.e., the prompt)—it is not a useful metric unless we compare similar tokens. For example, the following two require different amounts of computation and time, even when the input tokens are roughly the same:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p56"> <em>Example 1</em><em> </em>—Generate a three-verse poem on why dogs are amazing. </li> 
   <li class="readable-text" id="p57"> <em>Example 2</em><em> </em>—Generate a three-page poem on why dogs are amazing. </li> 
  </ul> 
  <div class="readable-text" id="p58"> 
   <p>The first example has 11 tokens, and the second one has 10 tokens when using the <code>cl100kbase</code> tokenizer (used by the newer GPT models). However, the generated tokens are very different. Also, as previously described, the time-per-output token (TPOT) does not consider the input prompt. The input prompt is also large for many tasks such as summarization because retrieval-augmented generation (RAG) is used for in-context information. Thus, using TPOT as a way of measuring latency is not precise.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>The model size also affects resource usage; a smaller model is usually more efficient and uses fewer resources, while a larger model is more capable and powerful but takes much more time. Let’s use an example to show how to measure this.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>The following listing shows a simple method for measuring the latency of the Azure OpenAI Chat API. Unlike the previous examples, which use software development kits (SDK), this one uses the REST APIs, and hence, we have to construct the payload and call the POST methods. We choose the number of requests to simulate and have a main function that employs a <code>ThreadPoolExecutor</code> to send several API requests simultaneously. It passes the <code>call_api_and_measure_latency()</code> function to the executor for each simulated request, gathers the latencies of all the requests, computes the average latency, and displays it.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p61"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.1</span> Measuring latency</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import os
...

AZURE_ENDPOINT = os.getenv("AOAI_ENDPOINT")      <span class="aframe-location"/> #1
AOAI_API_KEY = os.getenv("AOAI_KEY")             #1
MODEL = "gpt35"                                  #1
API_VERSION = "2024-02-15-preview"               #1

headers = {
    "api-key": AOAI_API_KEY,
    "Content-Type": "application/json"
}

def get_payload():                                     <span class="aframe-location"/> #2
    return {
        "model": MODEL,
        "max_tokens": 50,
        "messages": [{"role": "system", "content": 
                     <span class="">↪</span>"You are a helpful assistant."}, 
                     {"role": "user", "content": "Hello, world!"}],
        "temperature": 0.95,
        "stream": True                                 <span class="aframe-location"/> #3
    }

def call_api_and_measure_latency():                   <span class="aframe-location"/> #4
    payload = get_payload()
    start_time = time.time()        <span class="aframe-location"/> #5
    response = requests.post(AZURE_ENDPOINT,
                             headers=headers,
                             json=payload, timeout=20)
    latency = time.time() - start_time                  <span class="aframe-location"/> #6
    return latency, response.status_code

num_requests = 50                                  <span class="aframe-location"/> #7

def main():
    with ThreadPoolExecutor(max_workers=20) as executor:  <span class="aframe-location"/> #8
        futures = [executor.submit(call_api_and_measure_latency)
                   <span class="">↪</span>for _ in range(num_requests)]
        latencies = []
        for future in tqdm(as_completed(futures), total=num_requests):
            latency, status_code = future.result()
            print(f"Latency: {latency}s, Status Code: {status_code}")
            latencies.append(latency)

    average_latency = sum(latencies) / len(latencies)     <span class="aframe-location"/> #9
    print(f"Average Latency: {average_latency}s")

if __name__ == "__main__":
    main()</pre> 
    <div class="code-annotations-overlay-container">
     #1 Setting Azure OpenAI Chat API endpoint and API key
     <br/>#2 Defines the payload, including the model details to use
     <br/>#3 We stream the response so we can start getting the response faster.
     <br/>#4 Function to call the Azure OpenAI Chat API and measure latency
     <br/>#5 Start time used to calculate latency
     <br/>#6 End time used to calculate latency
     <br/>#7 Number of requests to simulate
     <br/>#8 Simulates concurrent API calls
     <br/>#9 Calculates and print latency metrics
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Figure 11.1 shows an example of the output executed with 50 iterations and an average latency of 11.35 seconds on a pay-as-you-go (PAYGO) instance. This is the round-trip call from the client to the service, not the latency of the service itself. This isn’t great, and for most production workloads, we need to look at the reserved capacity, which we will cover in the next section.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p63">  
   <img alt="figure" src="../Images/CH11_F01_Bahree.png" width="1100" height="515"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.1</span> Azure OpenAI latency example<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p64"> 
   <p>As shown in figure 11.2, in this example, we can use Azure’s out-of-the-box features to get service metrics such as latency. Using the default metric options, we see an average latency on this PAYGO instance of 95.37 milliseconds.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p65">  
   <img alt="figure" src="../Images/CH11_F02_Bahree.png" width="1100" height="475"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.2</span> Azure requests and latency average</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p66"> 
   <p><span class="print-book-callout-head">Note </span> The code we saw before is a basic example showing us how to measure latency and a view from a production perspective; it is not a good implementation for load testing latency, especially if one is not using PAYGO. A better approach is to use a script with OSS tools such as Apache JMeter (<a href="https://jmeter.apache.org">https://jmeter.apache.org</a>) or Locust (<a href="https://locust.io">https://locust.io</a>).</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_196"><span class="num-string">11.4.3</span> Scalability</h3> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>One of the main scaling options an enterprise should consider when deploying a production application that uses an LLM, such as Azure OpenAI, is provisioned throughput units (PTUs). PTUs for Azure OpenAI are units of model processing capacity that you can reserve and deploy for processing prompts and generating completions. They embody a normalized way of representing the throughput for your deployment, with each model–version pair requiring different amounts for deployment and throughput per PTU. The throughput per PTU can differ based on the model type and version, and it’s important to know this to scale your application well. </p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>A PTU is essentially the same as a reserved instance that other Azure services have, but it is only a feature of Azure’s OpenAI service. When an application needs to scale and uses multiple AI services, the reserved instance capacity must be considered across all of those services, as there isn’t a universal service that reserves capacity for a specific application or subscription.</p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>To deploy a model in Azure OpenAI using PTUs, we must select the “provisioned-managed” deployment type and indicate how many PTUs are required for the workload, as shown in figure 11.3. We also need to calculate the size of our specific workload shapes, which you can do with the Azure OpenAI Capacity calculator. This calculation helps determine the right number of PTUs for your deployment.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p71">  
   <img alt="figure" src="../Images/CH11_F03_Bahree.png" width="1100" height="643"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.3</span> PTU deployment options on Azure OpenAI</h5>
  </div> 
  <div class="readable-text" id="p72"> 
   <p>In addition to PTUs, enterprises can utilize a PAYGO model, which uses tokens per minute (TPM) consumed on demand. This model can be combined with PTUs to optimize utilization and cost. Furthermore, API Management (APIM) can be used with Azure OpenAI to manage and implement policies for queuing, rate throttling, error handling, and usage quotas.</p> 
  </div> 
  <div class="readable-text intended-text" id="p73"> 
   <p>By running the same latency tests performed for PAYGO on the PTU instance with slight modifications, we get the following results across both when using GPT-4 and the same model version. We randomly pick a prompt from a list to call and loop through 100 iterations in each case. An average of 2.9 seconds of end-to-end latency on PTUs is pretty decent compared to 6.3 seconds on PAYGO, which is not bad but not great:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p74"> 
   <div class="code-area-container"> 
    <pre class="code-area">Starting PTU test...
Median Latency: 1.582270622253418s
Average Latency: 2.947581880092621s
Min Latency: 0.7084167003631592s
Max Latency: 11.790298700332642s

Starting PAYGO test...
Median Latency: 2.391003727912903s
Average Latency: 6.372000885009766s
Min Latency: 0.4583735466003418s
Max Latency: 89.96037220954895s</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>The code in listing 11.2 shows the difference. This function iterates over the two OpenAI clients and their corresponding models. A <code>ThreadPoolExecutor</code> with 20 workers is created for each client–model pair, and tasks are submitted. Each task is a call to the <code>call_completion_api()</code> function (a wrapper around the Azure OpenAI completion API) with a randomly chosen input from the test inputs. It collects the latencies of all the tasks, calculates the median, average, minimum, and maximum latency, and prints these metrics.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.2</span> Measuring latency between PAYGO and PTU</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">test_inputs = ["Hello", "How are you?", 
     <span class="">↪</span>"What's the capital of Hawaii?", "Tell me a dad joke", 
     <span class="">↪</span>"Tell me a story", "What's your favorite movie?", 
     <span class="">↪</span>"What's the meaning of life?", "What's the capital of India?",
     <span class="">↪</span>"What's the square root of 1976?", "What's the largest mammal?",
     <span class="">↪</span>"Write a story about a Panda F1 driver in less 
        <span class="">↪</span>than {MAX_TOKENS} words"]

def main():
    for client, model, test_name in [(ptu_client, 
      <span class="">↪</span>PTU_MODEL, "PTU"), (paygo_client, PAYGO_MODEL, "PAYGO")]:
        print(f"Starting {test_name} test...")
        with ThreadPoolExecutor(max_workers=20) as executor:
            latencies = []
            futures = [executor.submit(call_completion_api, 
                       <span class="">↪</span>client, model, input) for input in 
                       <span class="">↪</span>random.choices(test_inputs, k=NUM_INTERATION)]
            for future in tqdm(as_completed(futures), 
            <span class="">↪</span>total=NUM_INTERATION):
                latency, token_count = future.result()
                if latency is not None and token_count is not None:
                    logging.info(f"Latency: {latency}s, 
                    <span class="">↪</span>Token Count: {token_count}")
                    latencies.append(latency)

        # Calculate and print metrics
        average_latency = sum(latencies) / len(latencies)
        <span class="">↪</span>if latencies else None
        min_latency = min(latencies) if latencies else None
        max_latency = max(latencies) if latencies else None
        median_latency = statistics.median(latencies) 
        <span class="">↪</span>if latencies else None

        print(f"Median Latency: {median_latency}s")
        print(f"Average Latency: {average_latency}s")
        print(f"Min Latency: {min_latency}s")
        print(f"Max Latency: {max_latency}s")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_197"><span class="num-string">11.4.4</span> PAYGO</h3> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>The PAYGO model with TPM is a flexible payment method that lets you pay only for the resources you use. The method is especially helpful for applications that have changing usage patterns and do not need constant processing capacity. It is the standard for most customers and applications across most providers. TPM is the measure of the model’s processing power. When you send a request to the model, it uses a certain number of tokens based on the prompt and the response’s complexity and length. We are billed for each token consumed, so as the usage increases, you pay more, and if it decreases, you pay less.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>Most cloud-based LLMs have a quota management feature that lets you assign rate limits to your deployments up to a global limit. Similarly, deployment and rate limits are associated with a model deployment. We can also assign a certain TPM to a specific deployment; when we do that, the available quota for that model will be reduced by that amount.</p> 
  </div> 
  <div class="readable-text intended-text" id="p80"> 
   <p>The PAYGO model is advantageous for scaling because it allows you to distribute TPM globally within a subscription and region, providing the flexibility to manage the allocation of rate limits across the deployments within your subscription. This model is ideal for applications with peak times of high usage followed by periods of low or no usage, as it ensures you only pay for what you use.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_198"><span class="num-string">11.4.5</span> Quotas and rate limits</h3> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>Quotas and rate limits are two mechanisms used in cloud services to manage and control resource usage. Quotas are the total amount of a resource that a user or service can consume over a specified period, such as a day or a month. They act as a cap on usage to prevent overconsumption of resources and ensure fair distribution among users.</p> 
  </div> 
  <div class="readable-text intended-text" id="p83"> 
   <p>In contrast, rate limits control the frequency of requests to a service. They are typically defined as the number of requests that can be made per second or minute. By limiting the rate at which users can make requests, rate limits help manage load and avoid overloading systems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>In essence, quotas refer to the quantity of resources you can use, while rate limits refer to the frequency of access to those resources. Understanding both is crucial for efficient API management and avoiding service disruptions for enterprises. By adhering to rate limits, enterprises can ensure their applications do not send more requests than a service can handle at a given time, which helps maintain performance and stability. Meanwhile, by staying within quotas, they can control their costs and prevent unexpected overages.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>Quotas for the OpenAI service, particularly for Azure OpenAI, are defined as limits on the resources or computational capacity a user or organization can consume. These quotas are typically measured in TPM and assigned on a per-region, per-model basis. The quotas ensure that the service can maintain consistent and predictable performance for all users.</p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>Enterprises should think about these quotas as a way to manage their usage and costs effectively. They must monitor their consumption to avoid exceeding these limits, which could lead to additional charges or service interruptions. It’s also important for enterprises to understand the rate limits associated with their deployments and plan accordingly.</p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>For example, if an enterprise has a quota of 240,000 TPM for a specific model in a region, it could create one deployment of 240K TPM, two of 120K TPM each, or multiple deployments adding up to less than 240K TPM in that region. For example, figure 11.4 outlines the quota setting for a specific Azure OpenAI endpoint and the various models deployed.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p88">  
   <img alt="figure" src="../Images/CH11_F04_Bahree.png" width="1012" height="441"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.4</span> Azure OpenAI model quota setting</h5>
  </div> 
  <div class="readable-text" id="p89"> 
   <p>OpenAI has its system of quotas, but they are structured differently. OpenAI’s quotas are typically related to usage limits that are set based on the billing information provided by the user. Once billing information is entered, users have an approved usage limit of a set amount per month (the default is $100), which can automatically increase as usage on the platform grows. Users move from one usage tier to another, as shown in figure 11.5. Users can review their current usage limit in the account settings under the limits page.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p90">  
   <img alt="figure" src="../Images/CH11_F05_Bahree.png" width="1061" height="669"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.5</span> OpenAI quota tiers</h5>
  </div> 
  <div class="readable-text" id="p91"> 
   <p>These quotas are designed to help manage and predict costs and prevent resource overuse. Enterprises should monitor their usage closely to ensure they stay within these limits and understand how these limits can scale with increased usage. </p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_199"><span class="num-string">11.4.6</span> Managing quota</h3> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>Managing quotas effectively is crucial for maintaining consistent and predictable application performance. Here are some best practices to consider:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p94"> <em>Understand your limits</em>. Familiarize yourself with the default quotas and limits that apply to the models, as each model and region can have different default quota limits. </li> 
   <li class="readable-text" id="p95"> <em>Monitor your usage</em>. Implement monitoring strategies to keep track of your usage against the assigned quotas. This will help you avoid unexpected throttling and ensure a good customer experience. </li> 
   <li class="readable-text" id="p96"> <em>Implement retry logic</em>. In your application, include retry logic to handle rate limit errors. This will allow your application to wait and retry the request after a brief pause rather than failing outright. A simple way to do this is to use the <code>Tenacity</code> library (an OSS library): </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p97"> 
   <div class="code-area-container"> 
    <pre class="code-area">from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
)
@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def completion_backoff(conversation):
    response = client.chat.completions.create(
        model=MODEL,
        messages=conversation,
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
    )
    return response</pre>  
   </div> 
  </div> 
  <ul> 
   <li class="readable-text" id="p98"> <em>Avoid sharp workload changes</em>. Gradually increase your workload to prevent sudden spikes that could lead to throttling. Test different load increase patterns to find the most efficient approach for your application. Note that throttling intentionally slows down or limits the requests an app or service can handle over a certain period. The server or service provider usually enforces this to prevent system overloads, ensure fair usage, and maintain quality of service. As we know, throttling is a common practice in API management and cloud-based services to manage resources efficiently and protect the system from potential abuse or denial of service (DoS) attacks. It’s also used to prevent a single user or service from consuming all available resources and affecting the performance of other users or services. </li> 
   <li class="readable-text" id="p99"> <em>Manage TPM allocation</em>. Use the quota management feature to increase TPM on deployments with high traffic and reduce TPM on deployments with limited needs. This helps balance the load and optimize resource utilization. </li> 
   <li class="readable-text" id="p100"> <em>Request quota increases</em>. If you consistently exceed your quota limits, consider requesting an increase through the Azure portal or by contacting Microsoft support or your cloud provider for those not on Azure. </li> 
   <li class="readable-text" id="p101"> <em>Distribute requests evenly</em>. To avoid hitting the requests-per-minute (RPM) rate limit, distribute your requests evenly over time. Many cloud providers, including Azure OpenAI, evaluate incoming requests’ rates over a short period and may throttle if the RPM limit is surpassed. </li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p102"> 
   <p><span class="print-book-callout-head">Note </span> With Azure OpenAI, you can combine PAYGO and PTUs to meet your workloads. This hybrid approach lets you use the flexibility of PAYGO for variable workloads, while having the reliability and consistency of PTUs for steady workloads. When you do this, PTUs are good for workloads with stable performance needs as they give you a fixed amount of throughput capacity that you reserve ahead of time, ensuring low latency variation. Furthermore, PAYGO is great for handling uncertain workloads where the usage can change. You’re charged based on the tokens used per minute, which means you pay more when your usage is high and less when it’s low.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>By actively managing their quotas and rate limits, enterprises can ensure they have the necessary capacity for their applications, while controlling costs and maintaining service availability.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_200"><span class="num-string">11.4.7</span> Observability</h3> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>Observability for LLM applications refers to monitoring, logging, and tracing to ensure the application works as intended and fixes problems when they occur. Let’s examine each one in a little more detail:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p106"> <em>Monitoring</em><em> </em>—Measure key performance indicators (KPIs) such as response times, throughput, error rates, and resource utilization. This data is essential for knowing the state of your application and making smart choices about scaling and optimization. </li> 
   <li class="readable-text" id="p107"> <em>Logging</em><em> </em>—Detailed logs should record requests and responses, including the input prompts and the model’s outputs. This information is priceless for debugging, understanding model behavior, and enhancing the user experience. </li> 
   <li class="readable-text" id="p108"> <em>Tracing</em><em> </em>—Use tracing to track the route of requests through your application. This is especially important for applications with complex architectures or multiple models and services. Tracing helps locate bottlenecks and areas for optimization. </li> 
  </ul> 
  <div class="readable-text" id="p109"> 
   <p>In the following sections, we use MLflow, Traceloop, and Prompt flow to show you how to implement this. Let’s start with MLflow.</p> 
  </div> 
  <div class="readable-text" id="p110"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">MLFlow</h4> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>MLflow is an open source platform that aims to manage the ML lifecycle, including experimentation, reproducibility, and deployment. It helps practitioners simplify their MLflow works with tools for tracking experiments, packaging code, and managing models. MLflow’s main components include tracking, model registry, and a server for deploying models, facilitating teamwork and innovation in ML projects.</p> 
  </div> 
  <div class="readable-text intended-text" id="p112"> 
   <p>MLflow enhances the observability of LLMs by providing tools that streamline the deployment and monitoring process. It offers a unified interface for interacting with different LLM providers, simplifying the integration and management of models. MLflow’s platform-agnostic nature also facilitates seamless integrations and deployments across various cloud platforms, further aiding in the observability and management of LLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>As shown in listing 11.3, we use MLflow to achieve this. This basic console chat application uses Azure OpenAI and randomly uses a few prompts in the list <code>text_inputs</code>. We can set how many times to repeat this using multiple threads. When we call the chat completion API, we log various features to demonstrate how MLflow can be applied. </p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>We require that MLflow and Prometheus (<a href="https://prometheus.io">https://prometheus.io</a>) be installed and running at an endpoint to run this. In our case, we run this locally in a Docker container exposed at port 5000. The docker-compose file is shown in the following listing. The book’s GitHub repository (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>) also has all the code. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p115"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.3</span> docker-cmpose file for MLflow</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    command: mlflow server --backend-store-uri /mlflow/mlruns 
    <span class="">↪</span>--default-artifact-root /mlflow/artifacts --host 0.0.0.0
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow/mlruns:/mlflow/mlruns
      - ./mlflow/artifacts:/mlflow/artifacts

  prometheus:
    image: prom/prometheus:latest
    command: --config.file=/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/data:/prometheus/data
    depends_on:
      - mlflow</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>We start by running the Docker container using the docker compose command, as shown: <code>docker compose up -d</code>. The <code>-d</code> parameter runs this as detached, which can be helpful and run in the background. As outlined in listing 11.4, we begin by specifying MLflow’s tracking URI (<code>http://localhost:5000</code>); this is the location where MLflow will store the data that we log and also assign a name for the experiment (<code>GenAI_ book</code>) so we can distinguish it from others. Of course, we are the sole users of this example since it runs locally. In addition, we need to install the following two dependencies for this to work: <code>– mlflow</code> and <code>colorama</code>. With conda, this can be installed using <code>conda install -c conda-forge mlflow colorama</code>, or with pip using <code>pip install mlflow colorama.</code></p> 
  </div> 
  <div class="readable-text intended-text" id="p117"> 
   <p>We measure features such as token count, prompts, conversation, and so forth. We also compute the time needed to receive a response and store it. We use the <code>mlflow.log_metrics()</code> function to store all these metrics. We also store the parameters used in the API request using the <code>mlflow.log_params()</code> function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p118"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.4</span> MLflow observability example</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import prometheus_client as prom
import mlflow
...

# Set OpenAI API key
API_KEY = os.getenv("OPENAI_API_BOOK_KEY")
MODEL = "gpt-3.5-turbo"
MLFLOW_URI = "http://localhost:5000"
...

# Initialize OpenAI client
client = OpenAI(api_key=API_KEY)

# Set MLflow tracking URI
mlflow.set_tracking_uri(MLFLOW_URI)
mlflow.set_experiment("GenAI_book")

def generate_text(conversation, max_tokens=100)-&gt;str:
    start_time = time.time()
    response = client.chat.completions.create(
        model=MODEL,
        messages=conversation,
    )
    latency = time.time() - start_time
    message_response = response.choices[0].message.content

    # Count tokens in the prompt, and the completion
    prompt_tokens = count_tokens(conversation[-1]['content'])
    conversation_tokens = count_tokens(str(conversation))
    completion_tokens = count_tokens(message_response)

    # Log metrics using MLflow
    with mlflow.start_run():
        mlflow.log_metrics({
            "request_count": 1,
            "request_latency": latency,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "conversation_tokens": conversation_tokens
            })
        mlflow.log_params({
            "model": MODEL,
            "temperature": TEMPERATURE,
            "top_p": TOP_P,
            "frequency_penalty": FREQUENCY_PENALTY,
            "presence_penalty": PRESENCE_PENALTY
            })        

    return message_response

if __name__ == "__main__":
    conversation = [{"role": "system", "content": 
                   <span class="">↪</span>"You are a helpful assistant."}]

    while True:
        user_input = input(f"You: ")
        conversation.append({"role": "user", "content": user_input})
        output = generate_text(conversation, 256)
        print_ai_output(output)
        conversation.append({"role": "assistant", "content": output})</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p119"> 
   <p>Logging this data allows you to compare different runs, examine your model’s performance, and see how parameter changes affect the output using the MLflow UI. Figure 11.6 shows an example of the information when we run multiple experiments and can contrast them.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p120">  
   <img alt="figure" src="../Images/CH11_F06_Bahree.png" width="1100" height="714"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.6</span> MLFlow experiments dashboard<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p121"> 
   <p>Figure 11.7 shows some metrics we have been monitoring: the <code>completion_tokens</code> and how they relate to the request latency when the <code>request_latency</code> is plotted.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p122">  
   <img alt="figure" src="../Images/CH11_F07_Bahree.png" width="1100" height="673"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.7</span> MLflow model metrics examples</h5>
  </div> 
  <div class="readable-text" id="p123"> 
   <p>Figure 11.8 illustrates how we can also log some of the prompt details and the generated response, which is very useful for observability. Of course, this should be done carefully, depending on the privacy and legal implications of who can access this telemetry.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p124">  
   <img alt="figure" src="../Images/CH11_F08_Bahree.png" width="1012" height="631"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.8</span> MLflow prompt and response details</h5>
  </div> 
  <div class="readable-text" id="p125"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Traceloop and OpenLLMetry</h4> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>Traceloop (<a href="https://www.traceloop.com/">https://www.traceloop.com/</a>) is an observability tool for monitoring LLM applications. It offers features such as real-time alerts and execution tracing to ensure quality deployment. OpenLLMetry, built on OpenTelemetry, is an open source extension maintained by Traceloop that enhances LLM observability. It integrates with Traceloop’s tools and adds LLM-specific monitoring capabilities, facilitating developers’ work with LLM observability, while aligning with OpenTelemetry standards.</p> 
  </div> 
  <div class="readable-text intended-text" id="p127"> 
   <p>OpenLLMetry extends OpenTelemetry’s functionality to cover generic operations such as database and API interactions and custom extensions for LLM-specific operations. This includes calls to LLM providers such as OpenAI or Anthropic and interactions with vector databases such as Chroma or Pinecone. In other words, OpenLLMetry offers a specialized toolkit for LLM applications, making it easier for developers to begin with observability in this domain, while still generating standard OpenTelemetry data that can be compatible with existing observability stacks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p128"> 
   <p>Integrating this with the existing application is quite simple. We need to install the Traceloop SDK (<code>pip</code> <code>install</code> <code>traceloop-sdk</code>). Next, we create a login and get an API key at <a href="https://app.traceloop.com/">https://app.traceloop.com/</a>. Initializing this is simple using <code>Traceloop.init()</code>, which instruments it automatically.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p129"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.5</span> Using Traceloop</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import os
from traceloop.sdk import Traceloop
...

LOAD_TEST_ITERATIONS = 50

# Set OpenAI
API_KEY = os.getenv("AOAI_PTU_KEY")
ENDPOINT = os.getenv("AOAI_PTU_ENDPOINT")
...

# Initialize Traceloop
TRACELOOP_API_KEY = os.getenv("TRACELOOP_API_KEY")
Traceloop.init(api_key=TRACELOOP_API_KEY)

client = AzureOpenAI(
    azure_endpoint = ENDPOINT,
    api_key=API_KEY,
    api_version="2024-02-15-preview"
)

# Define the conversation as a list of messages
conversation = [
    {"role": "system", "content": "You are a helpful assistant."},
]

# Define a list of test inputs
test_inputs = ["Hello", "How are you?", "What's the weather like?", 
     <span class="">↪</span>"Tell me a joke", "Tell me a story", "What's your favorite movie?",
     <span class="">↪</span>"What's the meaning of life?", "What's the capital of France?", 
     <span class="">↪</span>"What's the square root of 144?", "What's the largest mammal?"]

print("Starting load test...")
for _ in tqdm(range(LOAD_TEST_ITERATIONS)):
    # Generate a random user input
    user_input = random.choice(test_inputs)

    # Add user input to the conversation
    conversation.append({"role": "user", "content": user_input})

    # Make the API call
    response = client.chat.completions.create(
        model=MODEL,
        messages=conversation,
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
    )

print("Load test complete.")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>Traceloop also has multiple integration points into other systems and various LLM APIs. See <a href="https://mng.bz/gAJx">https://mng.bz/gAJx</a> for more details. For our purposes, we’ll use the default dashboard for our example, as shown in figure 11.9.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p131">  
   <img alt="figure" src="../Images/CH11_F09_Bahree.png" width="1017" height="672"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.9</span> Traceloop observability</h5>
  </div> 
  <div class="readable-text" id="p132"> 
   <p>Given that we can dig into various traces from an observability perspective, we get many details of the API calls (figure 11.10). In this example, we can see the system prompts, the user prompt, the completion, and other instrumentations, such as token usage. This can be a very powerful feature for many enterprise applications.</p> 
  </div> 
  <div class="readable-text" id="p133"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Prompt flow</h4> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>Prompt flow is an open source set of tools and features from Microsoft. It improves the creation process of AI applications, especially those that use LLMs. It helps with the design, evaluation, and implementation stages of AI applications, providing a simple interface for developers to work with LLMs.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p135">  
   <img alt="figure" src="../Images/CH11_F10_Bahree.png" width="1034" height="653"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.10</span> Traceloop observability example<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p136"> 
   <p>Prompt flow is a key feature for developers who want to use LLMs in enterprise applications, as it helps with both observability and LLMOps aspects. It lets developers build executable workflows that combine LLMs, prompts, and Python tools. This allows developers to find and fix errors and improve flows more easily, with the extra advantage of team collaboration features. Developers can create different prompt options, evaluate their effectiveness, and adjust the LLM’s performance as needed.</p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>Prompt flow consists of four stages, as illustrated in figure 11.11. The first stage, initialization, involves selecting a business use case, gathering a smaller dataset, and building a basic prompt and flow. Next, the experimentation stage requires testing and modifying the initial prompt until it reaches a good outcome. The third stage, evaluation and refinement, involves measuring the prompt’s quality and the flow’s performance on a larger dataset, with more adjustments and improvements made to achieve the desired output. Finally, the production stage involves launching the flow for production use, tracking usage, feedback, and any problems that may occur in a production setting.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p138">  
   <img alt="figure" src="../Images/CH11_F11_Bahree.png" width="870" height="515"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.11</span> Prompt flow lifecycle </h5>
  </div> 
  <div class="readable-text intended-text" id="p139"> 
   <p>Prompt flow offers many benefits when an application moves from development to production. It helps the application work well with existing CI/CD pipelines and gives powerful version control and collaborative tools for scaling LLM applications. This complete environment allows developers to deploy LLM-powered applications with more confidence, supported by the ability to track and understand the model’s behavior in a live setting. Therefore, the prompt flow is a key part of the deployment strategy, ensuring that applications using LLMs are strong, dependable, and prepared for the production needs of the enterprise level. More details, including easy-start samples, can be found in Prompt flow’s GitHub repository at <a href="https://github.com/microsoft/promptflow">https://github.com/microsoft/promptflow</a>.<span class="aframe-location"/></p> 
  </div> 
  <div class="readable-text print-book-callout" id="p140"> 
   <p><span class="print-book-callout-head">NOTE </span> Model serving involves deploying trained models to make predictions with new data. It’s a critical component for applications’ responsiveness and scalability. However, it demands significant investment in skills, computing resources across data centers, operational costs, and specialized hardware such as GPUs with InfiniBand connectivity. An open source software library such as vLLM could benefit organizations considering model serving. The efficient model hinges on scalable infrastructure, which can adjust resources for demand and ensure availability and cost-efficiency. Caching strategies and load balancing are key to reducing latency and evenly distributing requests. A solid update strategy, employing blue–green deployments, ensures smooth model transitions with minimal downtime. For more details on vLLM, see <a href="https://www.vllm.ai/">https://www.vllm.ai/</a>.</p> 
  </div> 
  <div class="readable-text" id="p141"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_201"><span class="num-string">11.4.8</span> Security and compliance considerations</h3> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>Security and compliance are critical, especially when dealing with user data and potentially sensitive information. Adhering to best practices helps protect your users and ensures your application complies with relevant laws and regulations.</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p143"> <em>Data encryption</em><em> </em>—Encrypt sensitive data at rest and in transit to protect against unauthorized access. Use secure protocols such as TLS for data in transit and utilize encryption features offered by your cloud provider for data at rest. </li> 
   <li class="readable-text" id="p144"> <em>Access control</em><em> </em>—Implement strict access controls to ensure only authorized personnel can access production data and infrastructure. Use role-based access control (RBAC) and the principle of least privilege (PoLP) to minimize the risk of data breaches. </li> 
   <li class="readable-text" id="p145"> <em>Compliance audits</em><em> </em>—Regularly audit your application and its infrastructure for compliance with relevant regulations and standards, such as GDPR, HIPAA, or CCPA, depending on your application’s domain and geographical scope. This may involve conducting security assessments, vulnerability scanning, and compliance checks. </li> 
   <li class="readable-text" id="p146"> <em>Anomaly detection</em><em> </em>—Deploy anomaly detection systems to monitor for unusual activity that could indicate a security breach or system misuse. This includes monitoring for abnormal usage patterns or unauthorized access attempts, allowing for rapid response to potential threats. </li> 
  </ul> 
  <div class="readable-text" id="p147"> 
   <p>Azure OpenAI Service offers many of these features as standard to meet enterprise readiness and compliance needs. As most enterprises demand, other cloud providers such as AWS and GCP have some versions of these controls.</p> 
  </div> 
  <div class="readable-text" id="p148"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_202"><span class="num-string">11.5</span> GenAI operational considerations</h2> 
  </div> 
  <div class="readable-text" id="p149"> 
   <p>Operational aspects of GenAI applications, particularly those utilizing LLMs such as GPT-4, are critical for ensuring smooth and efficient functioning of these systems. Understanding and managing key operational metrics such as tokens, latency, requests per second (RPS), and time to first byte (TTFB) are vital for optimizing performance, user experience, and cost. Let’s examine the definition, get a better understanding of the importance of these operational criteria, and explore how to measure and manage them effectively.</p> 
  </div> 
  <div class="readable-text" id="p150"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_203"><span class="num-string">11.5.1</span> Reliability and performance considerations</h3> 
  </div> 
  <div class="readable-text" id="p151"> 
   <p>Any production system, including the GenAI application, must be reliable and performant to meet the needs and expectations of your users. This means your system should be able to cope with different failures and scenarios. An API management or proxy system can assist you with many of these aspects, which we will discuss next:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p152"> <em>Monitoring tools</em><em> </em>—Utilize monitoring tools and services to measure these operational metrics continuously. Tools such as Prometheus (<a href="https://prometheus.io">https://prometheus.io</a>) for metric collection and Grafana (<a href="https://grafana.com">https://grafana.com</a>) for visualization can provide real-time insights into your application’s performance. Cloud providers also offer native monitoring solutions that can be employed. </li> 
   <li class="readable-text" id="p153"> <em>Performance testing</em><em> </em>—Regularly conduct performance testing to simulate various load conditions and measure how your application responds. Tools such as Apache JMeter (<a href="https://jmeter.apache.org">https://jmeter.apache.org</a>) or Locust (<a href="https://locust.io">https://locust.io</a>) can simulate multiple users interacting with your application to assess its throughput and latency under stress. </li> 
   <li class="readable-text" id="p154"> <em>Optimization techniques</em>—Implementing effective optimization techniques is crucial for overall application performance, resource utilization, and user experience: 
    <ul> 
     <li> <em>Token management</em><em> </em>—Optimize the use of tokens by refining input prompts and responses. This can involve trimming unnecessary text, using more efficient encoding techniques, or customizing the model to produce shorter, more concise outputs without compromising quality. </li> 
     <li> <em>Caching</em><em> </em>—Implement caching strategies for frequently requested information to reduce latency and lower the computational load on your system. This is especially effective for static or rarely changing data. </li> 
     <li> <em>Load balancing and auto-scaling</em><em> </em>—Use load balancers to distribute traffic evenly across your infrastructure, and implement auto-scaling to adjust resources dynamically based on demand. This helps maintain low latency and high RPS by ensuring your system can handle spikes in traffic without manual intervention. </li> 
    </ul></li> 
   <li class="readable-text" id="p155"> <em>Cost management</em><em> </em>—Monitor and manage costs related to operational metrics, especially token usage, as this directly affects the cost of using LLM APIs. Implement quota systems or rate limiting if necessary to prevent unexpected spikes in usage. </li> 
  </ul> 
  <div class="readable-text" id="p156"> 
   <p>By focusing on these operational aspects and continuously monitoring and optimizing based on real-world data, developers can ensure that their GenAI applications are functional but also efficient, scalable, and cost-effective. This holistic approach to operational management is crucial for the success of any application using the power of LLMs.</p> 
  </div> 
  <div class="readable-text" id="p157"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_204"><span class="num-string">11.5.2</span> Managed identities</h3> 
  </div> 
  <div class="readable-text" id="p158"> 
   <p>Azure OpenAI has a key advantage over OpenAI or other LLM providers in terms of using managed identities for authentication. This method follows the best practices for enterprise production deployments, improving security and making credential management easier. Managed identities avoid the need to handle keys directly, lowering the chance of key exposure and simplifying the process of changing credentials. They also offer an automated way to authenticate services running on Azure with other Azure resources, using Azure Active Directory (AAD) for identity management (also known as Entra ID).</p> 
  </div> 
  <div class="readable-text intended-text" id="p159"> 
   <p>When using managed identities with Azure OpenAI, enterprises have a couple of authentication methods available—RBAC and Entra ID. The former allows for more complex security scenarios and involves assigning roles (e.g., user or contributor) to enable API calls without key-based authentication. Conversely, the latter is used to authenticate our OpenAI resource using a bearer token obtained through the Azure CLI. It requires a custom subdomain name and is suitable for applications running on Azure services such as VMs, function apps, and VM scale sets.</p> 
  </div> 
  <div class="readable-text intended-text" id="p160"> 
   <p>Managed identities offer several benefits over traditional key-based authentication methods, especially regarding security and management. Some of the key advantages are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p161"> <em>No need to manage credentials</em><em> </em>—Managed identities eliminate the need for developers to manage the secrets, credentials, certificates, and keys used to secure communication between services. </li> 
   <li class="readable-text" id="p162"> <em>Automatic credential rotation</em><em> </em>—System-assigned managed identities are tied to the lifecycle of the Azure resource, and Azure automatically handles the lifecycle of the credentials, including their rotation. </li> 
   <li class="readable-text" id="p163"> <em>Enhanced security</em><em> </em>—Since credentials are not stored in the code, there’s a reduced risk of credential leaks. Managed identities also use AAD for authentication, which is more secure than storing and managing keys within your application. </li> 
   <li class="readable-text" id="p164"> <em>Simplified access management</em><em> </em>—Managed identities can be granted access to other Azure resources supporting Azure AD authentication, simplifying access management. Furthermore, user-assigned managed identities can be used by multiple resources, which can be particularly useful for complex environments and applications that need to scale. </li> 
  </ul> 
  <div class="readable-text" id="p165"> 
   <p>These benefits contribute to a more secure and efficient environment for managing access to Azure resources, making managed identities a preferred choice for many enterprise scenarios. The following listing shows a simple example of implementing a managed identity using Azure OpenAI. Note that this might require installing the Azure Identity package, which can be done via pip<code>:</code> <code>pip</code> <code>install azure-identity</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p166"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.6</span> Using managed identities with Azure OpenAI</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import os
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, 
<span class="">↪</span>get_bearer_token_provider

AZURE_ENDPOINT = os.getenv("AOAI_ENDPOINT")
API_VERSION = "2024-02-15-preview"

token_provider = get_bearer_token_provider(
    DefaultAzureCredential(),
    "https://cognitiveservices.azure.com/.default"
)

client = AzureOpenAI(
    api_version=API_VERSION,
    azure_endpoint=AZURE_ENDPOINT,
    azure_ad_token_provider=token_provider,
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p167"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_205"><span class="num-string">11.5.3</span> Caching</h3> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>Implementing caching when using OpenAI’s LLM in a production app is a strategic move to enhance performance and cost efficiency. Caching stores frequently requested data in a faster-access storage system, allowing for reduced latency, as repeated queries can be served swiftly. This improves user experience and minimizes operational costs by reducing the number of necessary API calls, often associated with fees.</p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>Moreover, services typically impose rate limits to prevent excessive use, and caching helps us adhere to these limits while maintaining a responsive service. Regarding the best practices for caching with Redis, it’s crucial to design cache keys uniquely representing each request and its context. An effective invalidation strategy, such as setting a time-to-live (TTL) for keys, ensures the cache doesn’t serve outdated information.</p> 
  </div> 
  <div class="readable-text intended-text" id="p170"> 
   <p>The cache-aside pattern is a recommended approach where the application checks the cache first and, upon a miss, retrieves data from the source, updates the cache, and then returns the response. Monitoring your cache’s hit rates and performance metrics is essential to gauge its effectiveness and make necessary optimizations. It’s important to handle cache misses gracefully and ensure the application can operate correctly even when temporarily unavailable.</p> 
  </div> 
  <div class="readable-text intended-text" id="p171"> 
   <p>We can illustrate how caching an LLM generation can benefit the application greatly in terms of cost and experience. However, we should not cache anything without a clear reason, hoping it will improve things, but consider it in the context of the use case and the related types of generations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p172"> 
   <p>For our caching example, we will use Redis and build on that from our RAG implementation earlier in chapter 8. Using the same Docker container, we will use the RedisVL library, a Python library designed for tasks like semantic search and real-time RAG pipelines. It provides an easy-to-use interface for vector-based searches and index management. RedisVL is built on the <code>redis-py</code> client and helps integrate Redis’ capabilities into AI-driven applications. We start by installing via pip: <code>pip</code> <code>install</code> <code>redisvl</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p173"> 
   <p>We continue by listing all the indexes in the Redis database, which only has one index, <code>posts</code>, from our RAG implementation earlier in chapter 8. </p> 
  </div> 
  <div class="readable-text intended-text" id="p174"> 
   <p>The <code>rvl</code> <code>index</code> <code>listall</code> command to see all the indexes is as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p175"> 
   <div class="code-area-container"> 
    <pre class="code-area">11:33:52 [RedisVL] INFO   Indices:
11:33:52 [RedisVL] INFO   1. Posts</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p176"> 
   <p>Next, we initialize the cache, which is created if the cache does not exist. The cache initialization requires some parameters—the name (case sensitive), the prefix for the hash entries, the connection string (local in our case, as we are running it in Docker locally), and the distance threshold. The distance threshold can vary depending on the embedding code and the use case and can be changed on the fly.</p> 
  </div> 
  <div class="readable-text intended-text" id="p177"> 
   <p>Our function, <code>answer_question()</code>, takes a question and uses the <code>check()</code> method on the <code>llmcache</code> instance to search the question in the cache. If the cache has results, it gives back the response. If the cache is empty, it calls the <code>generate_response</code> function to get a response from the OpenAI client, which is then stored in the cache. Note that some of the code is skipped for simplicity. The following listing shows the whole thing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p178"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.7</span> Using Redis cache for OpenAI response</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from openai import AzureOpenAI
from redisvl.extensions.llmcache import SemanticCache
import numpy as np
...
# Set your OpenAI API key
AOAI_API_KEY = os.getenv("AOAI_KEY")
...

def initialize_cache():
    # Initialize the semantic cache
    llmcache = SemanticCache(
        name="GenAIBookCache",            <span class="aframe-location"/> #1
        prefix="bookcache",                 <span class="aframe-location"/>  #2
       <span class="aframe-location"/> redis_url="redis://localhost:6379",   #3
        distance_threshold=0.1          <span class="aframe-location"/>  #4
    )
    return llmcache

# Define a list of questions
input_questions = ["What is the capital of UK?", ... 
                   "What is the capital of Japan?"]

def generate_response(conversation, max_tokens=25)-&gt;str:
    response = client.chat.completions.create(
        ...
    )
    return response.choices[0].message.content

def answer_question(question: str) -&gt; str:
    conversation = [{"role": "assistant", "content": question}]

    results = llmcache.check(prompt=question)
    if results:
        answer = results[0]["response"]
    else:
        answer = generate_response(conversation)
        llmcache.store(prompt=question, response=answer)
    return answer

if __name__ == "__main__":
    llmcache = initialize_cache()

    times_without_cache = []
    times_with_cache = []

    for question in input_questions:
        # Without caching
        start_time = time.time()
        answer = generate_response([{"role": "assistant",
                   <span class="">↪</span>"content": question}])
        end_time = time.time()
        times_without_cache.append(end_time-start_time)

        # With caching
        start_time = time.time()
        answer = answer_question(question)
        end_time = time.time()
        times_with_cache.append(end_time-start_time)

    avg_time_without_cache = np.mean(times_without_cache)
    avg_time_with_cache = np.mean(times_with_cache)

    print(f"Avg time taken without cache: {avg_time_without_cache}")
    print(f"Avg time taken with LLM cache enabled: {avg_time_with_cache}")
    print(f"Percentage of time saved: {round((avg_time_without_cache – 
            <span class="">↪</span>avg_time_with_cache) / avg_time_without_cache * 100, 2)}%")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Index name
     <br/>#2 Redis key prefix for hash entries
     <br/>#3 Redis connection url string
     <br/>#4 Semantic cache distance threshold
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p179"> 
   <p>When we run this, an example output is </p> 
  </div> 
  <div class="browsable-container listing-container" id="p180"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">11:16:17 redisvl.index.index INFO   Index already exists, not overwriting.
Cache hit for prompt: What is the capital of UK?, answer: London
...
Cache miss for prompt: What is the capital of India?, added to 
<span class="">↪</span>cache with response: The capital of India is New Delhi.
Avg time taken without cache: 0.7652951717376709
Avg time taken with LLM cache enabled: 0.23438820838928223
Percentage of time saved: 69.37%</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p181"> 
   <p>The TTL mechanism determines how long a piece of data should be stored in a cache before it’s considered stale and can be deleted. With Redis, once the TTL expires, the</p> 
  </div> 
  <div class="readable-text" id="p182"> 
   <p>cached data is automatically removed, ensuring that outdated information isn’t served to users. This helps maintain the freshness of the data being accessed by the application. This can be set as follows: <code>llmcache.set_ttl(5)</code> <code># 5</code> <code>seconds</code>.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p183">  
   <img alt="figure" src="../Images/CH11_F12_Bahree.png" width="736" height="744"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.12</span> Redis cache statistics for <code>GenAIBookCache</code></h5>
  </div> 
  <div class="readable-text intended-text" id="p184"> 
   <p>We can use the <code>rvl stats</code> command with the cache name as an argument to view the cache details. Figure 11.12 shows the output of this command: <code>rvl stats —i GenAIBookCache</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p185"> 
   <p>We have seen the components we must consider when making a GenAI application scalable and operational. There is one more topic to cover: LLMOps and MLOps. These are not just for getting AI applications to work; they’re for doing so in a maintainable, ethical, and scalable way. This is why they are regarded as vital for any enterprise that wants to use AI technology well. Let’s explore them more closely.</p> 
  </div> 
  <div class="readable-text" id="p186"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_206"><span class="num-string">11.6</span> LLMOps and MLOps</h2> 
  </div> 
  <div class="readable-text" id="p187"> 
   <p>Machine learning operations (MLOps) apply DevOps principles and best practices to develop, deploy, and manage ML models and applications. MLOps aims to streamline the ML lifecycle, from data preparation and experimentation to model training and serving, while ensuring quality, reliability, and scalability.</p> 
  </div> 
  <div class="readable-text intended-text" id="p188"> 
   <p>LLMOps is a specialized domain within MLOps that focuses on the operational aspects of LLMs. LLMs are deep learning models that can generate natural language text and perform various natural language processing (NLP) tasks based on the input provided. Examples of LLMs include GPT-4, BERT, and similar advanced AI systems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p189"> 
   <p>LLMOps introduces tools and best practices that help manage the lifecycle of LLMs and LLM-powered applications, such as prompt engineering, fine-tuning, deployment, monitoring, and governance. LLMOps also addresses the unique challenges and risks associated with LLMs, such as bias, hallucination, prompt injection, and ethical concerns.</p> 
  </div> 
  <div class="readable-text intended-text" id="p190"> 
   <p>Both LLMOps and MLOps share some common goals and challenges, such as automating and orchestrating the ML pipeline; ensuring reproducibility, traceability, and versioning of data, code, models, and experiments; monitoring and optimizing the performance, availability, and resource utilization of models and applications in production; implementing security, privacy, and compliance measures to protect data and models from unauthorized access and misuse; and incorporating feedback loops and continuous improvement cycles to update and refine models and applications based on changing requirements and user behavior.</p> 
  </div> 
  <div class="readable-text intended-text" id="p191"> 
   <p>However, LLMOps and MLOps also have some distinct differences, and switching from MLOps to LLMOps is a paradigm shift—specifically in data, model complexity (including size), and model output in the context of generation:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p192"> <em>Data</em><em> </em>—LLMs are pretrained on massive text datasets, such as the Common Crawl corpus, and can be adapted for specific use cases using prompt engineering and fine-tuning techniques. This reduces the need for extensive data collection and labeling and introduces the risk of data leakage and contamination from the pretraining data. </li> 
   <li class="readable-text" id="p193"> <em>Computational resources</em><em> </em>—GenAI models, such as LLMs, are very large and complex, often consisting of billions of parameters and requiring specialized hardware and infrastructure to train and run, such as high-end GPUs, memory, and so forth. This poses significant challenges for model storage, distribution, inference, cost, and energy efficiency. This challenge is further amplified when we want to scale up to many users to handle incoming requests without compromising performance. </li> 
   <li class="readable-text" id="p194"> <em>Model generation</em><em> </em>—LLMs are designed to generate coherent and contextually appropriate text rather than adhering to factual accuracy. This leads to various risks, such as bias amplification, hallucination, prompt injection, and ethical concerns. These risks require careful evaluation and mitigation strategies, such as responsible AI frameworks, human oversight, and explainability tools. </li> 
  </ul> 
  <div class="readable-text" id="p195"> 
   <p>Table 11.4 outlines key differences in the shift to LLMOps from MLOps. </p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p196"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 11.4</span> Differences between MLOps and LLMOps</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Area 
       </div></th> 
      <th> 
       <div>
         Traditional MLOps 
       </div></th> 
      <th> 
       <div>
         LLMOps 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Target audience <br/></td> 
      <td>  ML engineers, data scientists <br/></td> 
      <td>  Application developers, ML engineering, and data scientists <br/></td> 
     </tr> 
     <tr> 
      <td>  Components <br/></td> 
      <td>  Model, data, inference environments, features <br/></td> 
      <td>  LLMs, prompts, tokens, generations, APIs, embeddings, vector databases <br/></td> 
     </tr> 
     <tr> 
      <td>  Metrics <br/></td> 
      <td>  Accuracy (F1 score, precision, recall, etc.) <br/></td> 
      <td>  Quality (similarity), groundedness (accuracy), cost (tokens), latency, evaluations (Perplexity, BLEU, ROUGE, etc.) <br/></td> 
     </tr> 
     <tr> 
      <td>  Models <br/></td> 
      <td>  Typically built from scratch <br/></td> 
      <td>  Typically, prebuilt with inference via an API and multiple versions in production simultaneously <br/></td> 
     </tr> 
     <tr> 
      <td>  Ethical concerns <br/></td> 
      <td>  Bias in training data <br/></td> 
      <td>  Misuse and generation of harmful, fake, and biased output <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p197"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Why LLMOps and MLOps?</h4> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>LLMOps and MLOps are key to the responsible and efficient deployment of LLMs and ML models, ensuring ethical and performance standards. They address problems such as slow development, inconsistent model quality, and high costs, while providing advantages such as speed, consistency, and risk management. LLMOps covers tools and practices for managing LLMs, including prompt engineering, fine-tuning, and governance, resulting in faster development, better quality, cost reduction, and risk control.</p> 
  </div> 
  <div class="readable-text intended-text" id="p199"> 
   <p>Given their complexity, effective management is critical for generative AI models’ performance and cost efficiency. Important factors in LLMOps include model selection, deployment strategies, and version control. The right model size and configuration are essential, possibly customized to specific data. Options between cloud services and private infrastructure balance convenience and data security. Versioning and automated pipelines support smooth updates and rollbacks, enabling continuous integration and deployment. Adopting LLMOps ensures the successful, ethical use of generative AI, maximizing benefits and minimizing risks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p200"> 
   <p>LLMOps and MLOps are crucial for the production deployment of AI applications. They provide the necessary infrastructure to ensure that AI applications are operational, sustainable, responsible, and capable of scaling according to user demand. For developers and technical professionals, these frameworks offer a way to maintain quality assurance, follow compliance and ethical standards, and cost-effectively manage AI applications. In an enterprise environment where reliability and scalability are vital, LLMOps and MLOps are essential for successfully integrating AI technology.</p> 
  </div> 
  <div class="readable-text" id="p201"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Monitoring and telemetry systems</h4> 
  </div> 
  <div class="readable-text" id="p202"> 
   <p>While capable of delivering high-value business outcomes, powerful LLMs require careful monitoring and management to ensure optimal performance, accuracy, security, and user experience. Monitoring is an important part of LLMOps and MLOps, as it shows how well models and applications work in production. Continuous monitoring is vital for LLMOps, as for many production systems. It helps LLMOps teams solve problems quickly, ensuring the system is speedy and dependable. Monitoring covers performance metrics, such as response time, throughput, and resource utilization, enabling quick intervention if there are delays or performance declines. Telemetry tracking is crucial in this process, providing valuable insights into the model’s behavior and enabling continuous improvement.</p> 
  </div> 
  <div class="readable-text intended-text" id="p203"> 
   <p>Moreover, ethical AI deployment must check for bias or harmful outputs. Using fairness-aware monitoring methods, LLMOps teams ensure that LLMs work ethically, minimizing unwanted biases and increasing user trust. Frequent model updates and maintenance, supported by automated pipelines, ensure that the LLM stays current with the latest developments and data trends, ensuring continued effectiveness and adaptability.</p> 
  </div> 
  <div class="readable-text" id="p204"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_207"><span class="num-string">11.7</span> Checklist for production deployment</h2> 
  </div> 
  <div class="readable-text" id="p205"> 
   <p>We covered many topics in this chapter. Before we end it, let’s summarize some of the advice into a simple checklist that can be handy as a reference guide when deploying applications to production. The following categories are the same as those described earlier in the chapter. Of course, as with most of this advice, this is incomplete and should be used as part of the wider set of responsibilities:</p> 
  </div> 
  <ul> 
   <li class="readable-text buletless-item" id="p206"> Scaling and deployment 
    <ul> 
     <li> <em>Assess computational resources</em><em> </em>—Determine your generative AI models’ hardware and software requirements and ensure the infrastructure can support them effectively. </li> 
     <li> <em>Quality and availability of data</em><em> </em>—Implement robust data validation, quality control processes, and continuous monitoring to ensure data accuracy and relevance. </li> 
     <li> <em>Model performance and reliability</em><em> </em>—Set up regular testing and validation processes to monitor models’ performance. Plan for redundancy, failover, and disaster recovery to ensure high availability. </li> 
     <li> <em>Security and compliance</em><em> </em>—Apply encryption, access controls, and regular compliance audits. Ensure that your models adhere to regulations such as GDPR or HIPAA. </li> 
     <li> <em>Cost management</em><em> </em>—Closely monitor and manage the costs of deploying and maintaining your models. Be prepared to make tradeoffs between cost and performance. </li> 
     <li> <em>System integration</em><em> </em>—Ensure that the generative AI models can be easily integrated into existing systems and workflows. </li> 
     <li> <em>Human in the loop</em><em> </em>—Design the models to include human oversight and intervention where necessary. </li> 
     <li> <em>Ethical considerations</em><em> </em>—When deploying your models, address ethical implications, such as bias and fairness. </li> 
    </ul></li> 
   <li class="readable-text buletless-item" id="p207"> Best practices for production deployment 
    <ul> 
     <li> <em>Metrics for LLM inference</em><em> </em>—Focus on key metrics such as time to first token (TTFT), time per output token (TPOT), latency, and throughput. Use tools such as MLflow to track these metrics. </li> 
     <li> <em>Manage latency</em><em> </em>—Understand different latency points, and measure them accurately. Consider the influence of prompt size and model size on latency. </li> 
     <li> <em>Scalability</em><em> </em>—Utilize PTUs and PAYGO models to scale your application effectively. Use API management for queuing, rate throttling, and managing usage quotas. </li> 
     <li> <em>Quotas and rate limits</em><em> </em>—Implement strategies to manage quotas and rate limits effectively, including understanding your limits, monitoring usage, and implementing retry logic. </li> 
     <li> <em>Observability</em><em> </em>—Use tools such as MLflow, Traceloop, and Prompt flow to monitor, log, and trace your application for improved performance and user experience. </li> 
     <li> <em>Security and compliance</em><em> </em>—Encrypt data, control access, conduct compliance audits, and deploy anomaly detection systems. </li> 
    </ul></li> 
   <li class="readable-text buletless-item" id="p208"> LLMOps and MLOps 
    <ul> 
     <li> <em>Adopt LLMOps and MLOps frameworks</em><em> </em>—Ensure that your application follows best practices in LLMOps and MLOps for maintainable, ethical, and scalable AI solutions. </li> 
     <li> <em>Monitoring and telemetry systems</em><em> </em>—Use fairness-aware monitoring methods and telemetry tracking to ensure ethical AI deployment and continuous improvement of your models. </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p209"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_208">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p210"> Generative AI models are complex and resource intensive, requiring careful consideration of data quality, performance, security, cost, and ethical implications. </li> 
   <li class="readable-text" id="p211"> For any production deployments, we must follow several best practices: monitor key metrics, optimize latency, ensure scalability, implement observability tools, prioritize security and compliance, and employ managed identities and caching. </li> 
   <li class="readable-text" id="p212"> For observability, we implement monitoring, logging, and tracing tools such as MLflow, Traceloop, and Prompt flow to understand model behavior, diagnose problems, and improve user experience. </li> 
   <li class="readable-text" id="p213"> LLMOps is a specialized domain within MLOps that focuses on managing the unique challenges and risks of LLMs. Both share common goals such as automation, reproducibility, monitoring, and security but differ in data requirements, model complexity, and output characteristics. LLMOps addresses unique challenges such as bias, hallucination, and ethical concerns associated with LLMs. </li> 
  </ul>
 </div></div></body></html>