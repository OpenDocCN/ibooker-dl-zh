- en: Chapter 3\. Convolutional Neural Networks
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章. 卷积神经网络
- en: After experimenting with the fully connected neural networks in [Chapter 2](index_split_033.html#filepos97807),
    you probably noticed a few things. If you attempted to add more layers or vastly
    increase the number of parameters, you almost certainly ran out of memory on your
    GPU. In addition, it took a while to train to anything resembling somewhat decent
    accuracy, and even that wasn’t much to shout about, especially considering the
    hype surrounding deep learning. What’s going on?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](index_split_033.html#filepos97807)中尝试完全连接的神经网络后，您可能注意到了一些问题。如果尝试添加更多层或大幅增加参数数量，您几乎肯定会在GPU上耗尽内存。此外，训练时间很长，准确性也不尽如人意，尤其考虑到围绕深度学习的炒作。到底发生了什么？
- en: It’s true that a fully connected or (feed-forward) network can function as a
    universal approximator, but the theory doesn’t say how long it’ll take you to
    train it to become that approximation to the function you’re really after. But
    we can do better, especially with images. In this chapter, you’ll learn about
    convolutional neural networks (CNNs) and how they form the backbone of the most
    accurate image classifiers around today (we take a look at a couple of them in
    some detail along the way). We build up a new convolutional-based architecture
    for our fish versus cat application and show that it is quicker to train and more
    accurate than what we were doing in the previous chapter. Let’s get started!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接或（前馈）网络可以作为通用逼近器，但这个理论并没有说明训练它变成你真正想要的函数逼近需要多长时间。但我们可以做得更好，特别是在处理图像时。在本章中，您将了解卷积神经网络（CNNs）以及它们如何构成当今最准确的图像分类器的基础（我们将详细介绍其中的一些）。我们为鱼与猫的应用构建了一个基于卷积的新架构，并展示它比我们在上一章中所做的更快、更准确。让我们开始吧！
- en: Our First Convolutional Model
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个卷积模型
- en: This time around, I’m going to share the final model architecture first, and
    then discuss all the new pieces. And as I mentioned in [Chapter 2](index_split_033.html#filepos97807),
    the training method we created is independent of the model, so you can go ahead
    and test this model out first and then come back for the explanation!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我将首先分享最终的模型架构，然后讨论所有新的部分。正如我在[第2章](index_split_033.html#filepos97807)中提到的，我们创建的训练方法与模型无关，因此您可以先测试这个模型，然后再回来了解解释！
- en: '`class``CNNNet``(``nn``.``Module``):``def``__init__``(``self``,``num_classes``=``2``):``super``(``CNNNet``,``self``)``.``__init__``()``self``.``features``=``nn``.``Sequential``(``nn``.``Conv2d``(``3``,``64``,``kernel_size``=``11``,``stride``=``4``,``padding``=``2``),``nn``.``ReLU``(),``nn``.``MaxPool2d``(``kernel_size``=``3``,``stride``=``2``),``nn``.``Conv2d``(``64``,``192``,``kernel_size``=``5``,``padding``=``2``),``nn``.``ReLU``(),``nn``.``MaxPool2d``(``kernel_size``=``3``,``stride``=``2``),``nn``.``Conv2d``(``192``,``384``,``kernel_size``=``3``,``padding``=``1``),``nn``.``ReLU``(),``nn``.``Conv2d``(``384``,``256``,``kernel_size``=``3``,``padding``=``1``),``nn``.``ReLU``(),``nn``.``Conv2d``(``256``,``256``,``kernel_size``=``3``,``padding``=``1``),``nn``.``ReLU``(),``nn``.``MaxPool2d``(``kernel_size``=``3``,``stride``=``2``),``)``self``.``avgpool``=``nn``.``AdaptiveAvgPool2d``((``6``,``6``))``self``.``classifier``=``nn``.``Sequential``(``nn``.``Dropout``(),``nn``.``Linear``(``256``*``6``*``6``,``4096``),``nn``.``ReLU``(),``nn``.``Dropout``(),``nn``.``Linear``(``4096``,``4096``),``nn``.``ReLU``(),``nn``.``Linear``(``4096``,``num_classes``)``)``def``forward``(``self``,``x``):``x``=``self``.``features``(``x``)``x``=``self``.``avgpool``(``x``)``x``=``torch``.``flatten``(``x``,``1``)``x``=``self``.``classifier``(``x``)``return``x`'
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`class``CNNNet``(``nn``.``Module``):``def``__init__``(``self``,``num_classes``=``2``):``super``(``CNNNet``,``self``)``.``__init__``()``self``.``features``=``nn``.``Sequential``(``nn``.``Conv2d``(``3``,``64``,``kernel_size``=``11``,``stride``=``4``,``padding``=``2``),``nn``.``ReLU``(),``nn``.``MaxPool2d``(``kernel_size``=``3``,``stride``=``2``),``nn``.``Conv2d``(``64``,``192``,``kernel_size``=``5``,``padding``=``2``),``nn``.``ReLU``(),``nn``.``MaxPool2d``(``kernel_size``=``3``,``stride``=``2``),``nn``.``Conv2d``(``192``,``384``,``kernel_size``=``3``,``padding``=``1``),``nn``.``ReLU``(),``nn``.``Conv2d``(``384``,``256``,``kernel_size``=``3``,``padding``=``1``),``nn``.``ReLU``(),``nn``.``Conv2d``(``256``,``256``,``kernel_size``=``3``,``padding``=``1``),``nn``.``ReLU``(),``nn``.``MaxPool2d``(``kernel_size``=``3``,``stride``=``2``),``)``self``.``avgpool``=``nn``.``AdaptiveAvgPool2d``((``6``,``6``))``self``.``classifier``=``nn``.``Sequential``(``nn``.``Dropout``(),``nn``.``Linear``(``256``*``6``*``6``,``4096``),``nn``.``ReLU``(),``nn``.``Dropout``(),``nn``.``Linear``(``4096``,``4096``),``nn``.``ReLU``(),``nn``.``Linear``(``4096``,``num_classes``)``)``def``forward``(``self``,``x``):``x``=``self``.``features``(``x``)``x``=``self``.``avgpool``(``x``)``x``=``torch``.``flatten``(``x``,``1``)``x``=``self``.``classifier``(``x``)``return``x`'
- en: 'The first thing to notice is the use of `nn.Sequential()`. This allows us to
    create a chain of layers. When we use one of these chains in `forward()`, the
    input goes through each element of the array of layers in succession. You can
    use this to break your model into more logical arrangements. In this network,
    we have two chains: the `features` block and the `classifier`. Let’s take a look
    at the new layers we’re introducing, starting with `Conv2d`.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是使用`nn.Sequential()`。这使我们能够创建一系列层。当我们在`forward()`中使用其中一个链时，输入将依次通过每个层的数组元素。您可以使用这个将模型分解为更合理的安排。在这个网络中，我们有两个链：`features`块和`classifier`。让我们看看我们正在引入的新层，从`Conv2d`开始。
- en: Convolutions
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积
- en: 'The `Conv2d` layer is a 2D convolution. If we have a grayscale image, it consists
    of an array, x pixels wide and y pixels high, with each entry having a value that
    indicates whether it’s black or white or somewhere in between (we assume an 8-bit
    image, so each value can vary from 0 to 255). For this example we look at a small,
    square image that’s 4 pixels high and wide:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conv2d`层是一个2D卷积。如果我们有一幅灰度图像，它由一个数组组成，宽为x像素，高为y像素，每个条目都有一个值，指示它是黑色、白色还是介于两者之间（我们假设是8位图像，因此每个值的范围是0到255）。在这个例子中，我们看一个4像素高宽的小方形图像：'
- en: '![](images/00003.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00003.jpg)'
- en: 'Next we introduce something called a filter, or convolutional kernel. This
    is another matrix, most likely smaller, which we will drag across our image. Here’s
    our 2 × 2 filter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们介绍一个叫做滤波器或卷积核的东西。这是另一个矩阵，很可能更小，我们将它拖过我们的图像。这是我们的2×2滤波器：
- en: '![](images/00009.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00009.jpg)'
- en: 'To produce our output, we take the smaller filter and pass it over the original
    input, like a magnifying glass over a piece of paper. Starting from the top left,
    our first calculation is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了产生我们的输出，我们取较小的滤波器并将其传递到原始输入上，就像放大镜放在一张纸上一样。从左上角开始，我们的第一个计算如下：
- en: '![](images/00016.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00016.jpg)'
- en: 'And all we do is multiply each element in the matrix by its corresponding member
    in the other matrix and sum the result: (`10` × `1`) + (`11` × `0`) + (`2` × `1`)
    + (`123` × `0`) = `12`. Having done that, we move the filter across and begin
    again. But how much should we move the filter? In this case, we move the filter
    across by 2, meaning that our second calculation is:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的就是将矩阵中的每个元素与另一个矩阵中的对应成员相乘，然后将结果相加：(`10` × `1`) + (`11` × `0`) + (`2` ×
    `1`) + (`123` × `0`) = `12`。做完这个之后，我们移动滤波器并重新开始。但是我们应该移动滤波器多少呢？在这种情况下，我们将滤波器向右移动2个单位，这意味着我们的第二次计算是：
- en: '![](images/00021.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00021.jpg)'
- en: 'This gives us an output of 13\. We now move our filter down and back to the
    left and repeat the process, giving us this final result (or feature map):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个输出为13。现在我们将滤波器向下移动并向左移动，重复这个过程，给出这个最终结果（或特征图）：
- en: '![](images/00029.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00029.jpg)'
- en: In [Figure 3-1](#filepos227682), you can see how this works graphically, with
    a 3 × 3 kernel being dragged across a 4 × 4 tensor and producing a 2 × 2 output
    (though each segment is based on nine elements instead of the four in our first
    example).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3-1](#filepos227682)中，您可以看到这是如何以图形方式工作的，一个3×3的核被拖过一个4×4的张量，产生一个2×2的输出（尽管每个部分是基于九个元素而不是我们第一个示例中的四个）。
- en: '![](images/00035.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00035.jpg)'
- en: Figure 3-1\. How a 3 × 3 kernel operates across a 4 × 4 input
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-1。3×3核如何在4×4输入上操作
- en: 'A convolutional layer will have many of these filters, the values of which
    are filled in by the training of the network, and all the filters in the layer
    share the same bias values. Let’s go back to how we’re invoking the `Conv2d` layer
    and see some of the other options that we can set:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层将有许多这样的滤波器，这些滤波器的值是由网络的训练填充的，该层中的所有滤波器共享相同的偏置值。让我们回到如何调用`Conv2d`层并看看我们可以设置的其他选项：
- en: '`nn``.``Conv2d``(``in_channels``,``out_channels``,``kernel_size``,``stride``,``padding``)`'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`nn``.``Conv2d``(``in_channels``,``out_channels``,``kernel_size``,``stride``,``padding``)`'
- en: The `in_channels` is the number of input channels we’ll be receiving in the
    layer. At the beginning of the network, we’re taking in the RGB image as input,
    so the number of input channels is three. `out_channels` is, unsurprisingly, the
    number of output channels, which corresponds to the number of filters in our conv
    layer. Next is `kernel_size`, which describes the height and width of our filter.^([1](index_split_069.html#filepos356705))
    This can be a single scalar specifying a square (e.g., in the first conv layer,
    we’re setting up an 11 × 11 filter), or you can use a tuple (such as (3,5) for
    a 3 × 5 filter).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`in_channels`是我们在该层接收的输入通道数。在网络的开头，我们将RGB图像作为输入，因此输入通道数为三。`out_channels`是输出通道数，对应于卷积层中滤波器的数量。接下来是`kernel_size`，描述了我们滤波器的高度和宽度。这可以是一个单一的标量，指定一个正方形（例如，在第一个卷积层中，我们设置了一个11×11的滤波器），或者您可以使用一个元组（例如(3,5)表示一个3×5的滤波器）。'
- en: 'The next two parameters seem harmless enough, but they can have big effects
    on the downstream layers of your network, and even what that particular layer
    ends up looking at. `stride` indicates how many steps across the input we move
    when we adjust the filter to a new position. In our example, we end up with a
    stride of 2, which has the effect of making a feature map that is half the size
    of the input. But we could have also moved with a stride of 1, which would give
    us a feature map output of 4 × 4, the same size of the input. We can also pass
    in a tuple (a,b) that would allow us to move a across and b down on each step.
    Now, you might be wondering, what happens when it gets to the end? Let’s take
    a look. If we drag our filter along with a stride of 1, we eventually get to this
    point:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个参数似乎足够无害，但它们可能对网络的下游层产生重大影响，甚至影响该特定层最终查看的内容。`stride`指示我们在调整滤波器到新位置时在输入上移动多少步。在我们的示例中，我们最终得到一个步幅为2，这使得特征图的大小是输入大小的一半。但我们也可以使用步幅为1移动，这将给我们一个4×4的特征图输出，与输入的大小相同。我们还可以传入一个元组(a,b)，允许我们在每一步上移动a个单位并向下移动b个单位。现在，您可能想知道，当它到达末尾时会发生什么。让我们看看。如果我们使用步幅为1拖动我们的滤波器，最终会到达这一点：
- en: '![](images/00041.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00041.jpg)'
- en: 'We don’t have enough elements in our input to do a full convolution. So what
    happens? This is where the `padding` parameter comes in. If we give a `padding`
    value of 1, our input looks a bit like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入中没有足够的元素来进行完整的卷积。那么会发生什么？这就是`padding`参数发挥作用的地方。如果我们给定一个`padding`值为1，我们的输入看起来有点像这样：
- en: '![](images/00048.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00048.jpg)'
- en: 'Now when we get to the edge, our values covered by the filter are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当我们到达边缘时，我们的滤波器覆盖的值如下：
- en: '![](images/00056.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00056.jpg)'
- en: If you don’t set padding, any edge cases that PyTorch encounters in the last
    columns of the input are simply thrown away. It’s up to you to set padding appropriately.
    Just as with `stride` and `kernel_size`, you can also pass in a tuple for `height`
    × `weight` padding instead of a single number that pads the same in both directions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不设置填充，PyTorch在输入的最后一列遇到的任何边缘情况都会被简单地丢弃。您需要适当地设置填充。就像`stride`和`kernel_size`一样，您也可以传入一个元组来设置`height`×`weight`填充，而不是填充相同的单个数字。
- en: That’s what the `Conv2d` layers are doing in our model. But what about those
    `MaxPool2d` layers?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们模型中的`Conv2d`层在做的事情。那么`MaxPool2d`层呢？
- en: Pooling
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 池化
- en: In conjunction with the convolution layers, you will often see pooling layers.
    These layers reduce the resolution of the network from the previous input layer,
    which gives us fewer parameters in lower layers. This compression results in faster
    computation for a start, and it helps prevent overfitting in the network.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'In our model, we’re using `MaxPool2d` with a kernel size of 3 and a stride
    of 2\. Let’s have a look at how that works with an example. Here’s a 5 × 3 input:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/00066.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'Using the kernel size of 3 × 3 and a stride of 2, we get two 3 × 3 tensors
    from the pooling:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/00072.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: '![](images/00005.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: In `MaxPool` we take the maximum value from each of these tensors, giving us
    an output tensor of [6,9]. Just as in the convolutional layers, there’s a `padding`
    option to `MaxPool` that creates a border of zero values around the tensor in
    case the stride goes outside the tensor window.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, you can pool with other functions aside from taking the
    maximum value from a kernel. A popular alternative is to take the average of the
    tensor values, which allows all of the tensor data to contribute to the pool instead
    of just one value in the `max` case (and if you think about an image, you can
    imagine that you might want to consider the nearest neighbors of a pixel). Also,
    PyTorch provides `AdaptiveMaxPool` and `AdaptiveAvgPool` layers, which work independently
    of the incoming input tensor’s dimensions (we have an `AdaptiveAvgPool` in our
    model, for example). I recommend using these in model architectures that you construct
    over the standard `MaxPool` or `AvgPool` layers, because they allow you to create
    architectures that can work with different input dimensions; this is handy when
    working with disparate datasets.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: We have one more new component to talk about, one that is incredibly simple
    yet important for training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'One recurring issue with neural networks is their tendency to overfit to training
    data, and a large amount of ongoing work is done in the deep learning world to
    identify approaches that allow networks to learn and generalize to nontraining
    data without simply learning how to just respond to the training inputs. The `Dropout`
    layer is a devilishly simple way of doing this that has the benefit of being easy
    to understand and effective: what if we just don’t train a random bunch of nodes
    within the network during a training cycle? Because they won’t be updated, they
    won’t have the chance to overfit to the input data, and because it’s random, each
    training cycle will ignore a different selection of the input, which should help
    generalization even further.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the `Dropout` layers in our example CNN network are initialized
    with `0.5`, meaning that 50% of the input tensor is randomly zeroed out. If you
    want to change that to 20%, add the `p` parameter to the initialization call:
    `Dropout(p=0.2)`.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: NOTE
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Dropout` should take place only during training. If it was happening during
    inference time, you’d lose a chunk of your network’s reasoning power, which is
    not what we want! Thankfully, PyTorch’s implementation of `Dropout` works out
    which mode you’re running in and passes all the data through the `Dropout` layer
    at inference time.'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Having looked at our little CNN model and examined the layer types in depth,
    let’s take a look at other models that have been made in the past ten years.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: History of CNN Architectures
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Although CNN models have been around for decades (LeNet-5 was used for digit
    recognition on check in the late 1990s, for example), it wasn’t until GPUs became
    widely available that deep CNN networks became practical. Even then, it has been
    only seven years since deep learning networks started to overwhelm all other existing
    approaches in image classification. In this section, we take a little journey
    back through the last few years to talk about some milestones in CNN-based learning
    and investigate some new techniques along the way.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet was, in many ways, the architecture that changed everything. It was
    released in 2012 and destroyed all other entries in that year’s ImageNet competition
    with a top-5 error rate of 15.3% (the second place entry had a top-5 error of
    26.2%, just to give you an idea of how much better it was than other state-of-the-art
    methods). AlexNet was one of the first architectures to introduce the concepts
    of `MaxPool` and `Dropout`, and even popularize the then less-well-known `ReLU`
    activation function. It was one of the first architectures to demonstrate that
    many layers were possible and efficient to train on a GPU. Although it’s not state
    of the art anymore, it remains an important milestone in deep learning history.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet在许多方面改变了一切的架构。它于2012年发布，并在当年的ImageNet比赛中以15.3%的前5错误率摧毁了所有其他参赛作品（第二名的前5错误率为26.2%，这让你了解了它比其他最先进方法要好多少）。AlexNet是第一个引入`MaxPool`和`Dropout`概念的架构之一，甚至推广了当时不太知名的`ReLU`激活函数。它是第一个证明许多层次在GPU上训练是可能且有效的架构之一。虽然它现在不再是最先进的，但它仍然是深度学习历史上的一个重要里程碑。
- en: What does the AlexNet architecture look like? Aha, well, it’s time to let you
    in on a little secret. The network we’ve been using in this chapter so far? It’s
    AlexNet. Surprise! That’s why we used the standard `MaxPool2d` instead of `AdaptiveMaxPool2d`,
    to match the original AlexNet definition.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet架构是什么样的？啊哈，是时候让你知道一个小秘密了。我们在本章中一直在使用的网络？就是AlexNet。惊喜！这就是为什么我们使用标准的`MaxPool2d`而不是`AdaptiveMaxPool2d`，以匹配原始的AlexNet定义。
- en: Inception/GoogLeNet
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Inception/GoogLeNet
- en: Let’s skip ahead to the winner of the 2014 ImageNet competition. The GoogLeNet
    architecture introduced the Inception module that addressed some of the deficiencies
    of AlexNet. In that network, the kernels of the convolutional layers are fixed
    at a certain resolution. We might expect that an image will have details that
    are important at both the macro- and microscale. It may be easier to determine
    whether an object is a car with a large kernel, but to determine whether it’s
    an SUV or a hatchback may require a smaller kernel. And to determine the model,
    we might need an even smaller kernel to make out details such as logos and insignias.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跳到2014年ImageNet比赛的冠军。GoogLeNet架构引入了Inception模块，解决了AlexNet的一些不足之处。在该网络中，卷积层的卷积核分辨率是固定的。我们可能期望一幅图像在宏观和微观尺度上都有重要细节。使用较大的卷积核可能更容易确定一个对象是否是汽车，但要确定它是SUV还是掀背车可能需要一个较小的卷积核。而要确定车型，我们可能需要一个更小的卷积核来识别标志和徽标等细节。
- en: The Inception network instead runs a series of convolutions of different sizes
    all on the same input, and concatenates all of the filters together to pass on
    to the next layer. Before it does any of those, though, it does a 1 × 1 convolution
    as a bottleneck that compresses the input tensor, meaning that the 3 × 3 and 5
    × 5 kernels operate on a fewer number of filters than they would if the 1 × 1
    convolution wasn’t present. You can see an Inception module illustrated in [Figure 3-2](#filepos240566).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Inception网络在同一输入上运行一系列不同大小的卷积，并将所有滤波器连接在一起传递到下一层。不过，在执行任何操作之前，它会进行一个1×1的卷积作为瓶颈，压缩输入张量，这意味着3×3和5×5的卷积核操作的滤波器数量比没有1×1卷积存在时要少。你可以在[图3-2](#filepos240566)中看到一个Inception模块的示例。
- en: '![](images/00011.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00011.jpg)'
- en: Figure 3-2\. An Inception module
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-2\. 一个Inception模块
- en: The original GoogLeNet architecture uses nine of these modules stacked on top
    of each other, forming a deep network. Despite the depth, it uses fewer parameters
    overall than AlexNet while delivering a human-like performance of an 6.67% top-5
    error rate.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的GoogLeNet架构使用了九个这些模块堆叠在一起，形成一个深度网络。尽管深度较大，但它的参数总数比AlexNet少，同时提供了一个类似人类的性能，前5错误率为6.67%。
- en: VGG
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: VGG
- en: The second-place entry in 2014’s ImageNet was from the University of Oxford
    — the Visual Geometry Group (VGG) network. In contrast to GoogLeNet, VGG is a
    simpler stack of convolutional layers. Coming in various configurations of longer
    stacks of convolutional filters combined with two large hidden linear layers before
    the final classification layer, it shows off the power of simple deep architectures
    (scoring an 8.8% top-5 error in its VGG-16 configuration). [Figure 3-3](#filepos242581)
    shows the layers of the VGG-16 from end to end.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年ImageNet比赛的第二名是来自牛津大学的视觉几何组（VGG）网络。与GoogLeNet相比，VGG是一个更简单的卷积层堆叠。VGG以不同配置的更长的卷积滤波器堆叠以及最终分类层之前的两个大隐藏线性层组合而成，展示了简单深度架构的强大性能（在VGG-16配置中获得了8.8%的前5错误率）。[图3-3](#filepos242581)展示了VGG-16从头到尾的层。
- en: The downside of the VGG approach is that the final fully connected layers make
    the network balloon to a large size, weighing in at 138 million parameters in
    comparison with GoogLeNet’s 7 million. Having said that, the VGG network is still
    quite popular in the deep learning world despite its huge size, as it’s easy to
    reason about because of its simpler construction and the early availability of
    trained weights. You’ll often see it used in style transfer applications (e.g.,
    turning a photo into a Van Gogh painting) as its combination of convolutional
    filters do appear to capture that sort of information in a way that’s easier to
    observe than the more complex networks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: VGG方法的缺点是最终的全连接层使网络膨胀到一个庞大的规模，参数数量达到了1380万，而GoogLeNet只有700万。尽管如此，VGG网络在深度学习领域仍然非常受欢迎，因为它的构造更简单，训练权重早期可用。你经常会看到它在风格转移应用中使用（例如，将照片转换为梵高的画作），因为它的卷积滤波器组合似乎能够捕捉到这种信息，而且更容易观察比更复杂的网络。
- en: '![](images/00017.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00017.jpg)'
- en: Figure 3-3\. VGG-16
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-3\. VGG-16
- en: ResNet
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet
- en: A year later, Microsoft’s ResNet architecture won the ImageNet 2015 competition
    with a top-5 score of 4.49% in its ResNet-152 variant and 3.57% in an ensemble
    model (essentially beyond human ability at this point). The innovation that ResNet
    brought was an improvement on the Inception-style stacking bundle of layers approach,
    wherein each bundle performed the usual CNN operations but also added the incoming
    input to the output of the block, as shown in [Figure 3-4](#filepos243965).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一年后，微软的ResNet架构以其ResNet-152变体在ImageNet 2015竞赛中获得了4.49%的前5得分，并在一个集成模型中获得了3.57%的得分（在这一点上基本上超越了人类的能力）。ResNet带来的创新是改进了Inception风格的堆叠层次方法，其中每个捆绑执行了通常的CNN操作，但还将传入输入添加到块的输出中，如[图3-4](#filepos243965)所示。
- en: The advantage of this set up is that each block passes through the original
    input to the next layer, allowing the “signal” of the training data to traverse
    through deeper networks than possible in either VGG or Inception. (This loss of
    weight changes in deep networks is known as a vanishing gradient because of the
    gradient changes in backpropagation tending to zero during the training process.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置的优势在于每个块将原始输入传递到下一层，允许训练数据的“信号”在比VGG或Inception更深的网络中传播。 （在深度网络中的权重变化丢失被称为梯度消失，因为在训练过程中反向传播中的梯度变化趋于零。）
- en: '![](images/00023.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00023.jpg)'
- en: Figure 3-4\. A ResNet block
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-4。一个ResNet块
- en: Other Architectures Are Available!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他架构可用！
- en: Since 2015 or so, plenty of other architectures have incrementally improved
    the accuracy on ImageNet, such as DenseNet (an extension of the ResNet idea that
    allows for the construction of 1,000-layer monster architectures), but also a
    lot of work has gone into creating architectures such as SqueezeNet and MobileNet,
    which offer reasonable accuracy but are tiny compared to architectures such as
    VGG, ResNet, or Inception.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 自2015年以来，许多其他架构已经逐步提高了在ImageNet上的准确性，例如DenseNet（这是ResNet思想的延伸，允许构建1,000层的庞大架构），但也有很多工作致力于创建架构，如SqueezeNet和MobileNet，它们提供了合理的准确性，但与VGG、ResNet或Inception等架构相比要小得多。
- en: Another big area of research is getting neural networks to start designing neural
    networks themselves. The most successful attempt so far is, of course, from Google,
    whose AutoML system generated an architecture called NASNet that has a top-5 error
    rate of 3.8% on ImageNet, which is state of the art as I type this at the start
    of 2019 (along with another autogenerated architecture from Google called PNAS).
    In fact, the organizers of the ImageNet competition have decided to call a halt
    to further competitions in this space because the architectures have already gone
    beyond human levels of ability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的研究领域是让神经网络开始设计神经网络本身。到目前为止，最成功的尝试当然来自谷歌，其AutoML系统生成了一种名为NASNet的架构，在ImageNet上的前5错误率为3.8%，这是我在2019年初撰写本文时的最新技术水平（还有谷歌另一种自动生成的架构称为PNAS）。事实上，ImageNet竞赛的组织者已决定停止在这一领域进行进一步的竞赛，因为这些架构已经超越了人类的能力水平。
- en: That brings us to the state of the art as of the time this book goes to press,
    so let’s take a look at how we can use these models instead of defining our own.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到了本书付印时的最新技术水平，所以让我们看看如何使用这些模型而不是定义我们自己的。
- en: Using Pretrained Models in PyTorch
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中使用预训练模型
- en: 'Obviously, having to define a model each time you want to use one would be
    a chore, especially once you move away from AlexNet, so PyTorch provides many
    of the most popular models by default in the `torchvision` library. For AlexNet,
    all you need to do is this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，每次想使用模型都必须定义一个模型将是一项繁琐的任务，特别是一旦您远离AlexNet，因此PyTorch在`torchvision`库中默认提供了许多最受欢迎的模型。对于AlexNet，您只需要这样做：
- en: '`import``torchvision.models``as``models``alexnet``=``models``.``alexnet``(``num_classes``=``2``)`'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``torchvision.models``as``models``alexnet``=``models``.``alexnet``(``num_classes``=``2``)`'
- en: Definitions for VGG, ResNet, Inception, DenseNet, and SqueezeNet variants are
    also available. That gives you the model definition, but you can also go a step
    further and call `models.alexnet(pretrained=True)` to download a pretrained set
    of weights for AlexNet, allowing you to use it immediately for classification
    with no extra training. (But as you’ll see in the next chapter, you will likely
    want to do some additional training to improve the accuracy on your particular
    dataset.)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: VGG、ResNet、Inception、DenseNet和SqueezeNet变体的定义也是可用的。这为您提供了模型定义，但您也可以进一步调用`models.alexnet(pretrained=True)`来下载AlexNet的预训练权重集，从而可以立即将其用于分类而无需额外训练。（但正如您将在下一章中看到的，您可能希望进行一些额外的训练以提高您特定数据集的准确性。）
- en: Having said that, there is something to be said for building the models yourself
    at least once to get a feel for how they fit together. It’s a good way to get
    some practice building model architectures within PyTorch, and of course you can
    compare with the provided models to make sure that what you come up with matches
    the actual definition. But how do you find out what that structure is?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，至少有一次自己构建模型以了解它们如何组合是有意义的。这是一个很好的方法，可以通过PyTorch构建模型架构，并且您可以与提供的模型进行比较，以确保您提出的内容与实际定义匹配。但是，您如何找出这种结构呢？
- en: Examining a Model’s Structure
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型的结构
- en: 'If you’re curious about how one of these models is constructed, there’s an
    easy way to get PyTorch to help you out. As an example, here’s a look at the entire
    ResNet-18 architecture, which we get by simply calling the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解其中一个模型是如何构建的，有一个简单的方法可以让PyTorch帮助您。例如，这里是整个ResNet-18架构的外观，我们只需调用以下内容：
- en: '`print``(``model``)``ResNet``(``(``conv1``):``Conv2d``(``3``,``64``,``kernel_size``=``(``7``,``7``),``stride``=``(``2``,``2``),``padding``=``(``3``,``3``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``64``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``maxpool``):``MaxPool2d``(``kernel_size``=``3``,``stride``=``2``,``padding``=``1``,``dilation``=``1``,``ceil_mode``=``False``)``(``layer1``):``Sequential``(``(``0``):``BasicBlock``(``(``conv1``):``Conv2d``(``64``,``64``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``64``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``conv2``):``Conv2d``(``64``,``64``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn2``):``BatchNorm2d``(``64``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``)``(``1``):``BasicBlock``(``(``conv1``):``Conv2d``(``64``,``64``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``64``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``conv2``):``Conv2d``(``64``,``64``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn2``):``BatchNorm2d``(``64``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``)``)``(``layer2``):``Sequential``(``(``0``):``BasicBlock``(``(``conv1``):``Conv2d``(``64``,``128``,``kernel_size``=``(``3``,``3``),``stride``=``(``2``,``2``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``128``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``conv2``):``Conv2d``(``128``,``128``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn2``):``BatchNorm2d``(``128``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``downsample``):``Sequential``(``(``0``):``Conv2d``(``64``,``128``,``kernel_size``=``(``1``,``1``),``stride``=``(``2``,``2``),``bias``=``False``)``(``1``):``BatchNorm2d``(``128``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``)``)``(``1``):``BasicBlock``(``(``conv1``):``Conv2d``(``128``,``128``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``128``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``conv2``):``Conv2d``(``128``,``128``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn2``):``BatchNorm2d``(``128``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``)``)``(``layer3``):``Sequential``(``(``0``):``BasicBlock``(``(``conv1``):``Conv2d``(``128``,``256``,``kernel_size``=``(``3``,``3``),``stride``=``(``2``,``2``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``256``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``conv2``):``Conv2d``(``256``,``256``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn2``):``BatchNorm2d``(``256``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``downsample``):``Sequential``(``(``0``):``Conv2d``(``128``,``256``,``kernel_size``=``(``1``,``1``),``stride``=``(``2``,``2``),``bias``=``False``)``(``1``):``BatchNorm2d``(``256``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``)``)``(``1``):``BasicBlock``(``(``conv1``):``Conv2d``(``256``,``256``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``256``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``conv2``):``Conv2d``(``256``,``256``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn2``):``BatchNorm2d``(``256``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``)``)``(``layer4``):``Sequential``(``(``0``):``BasicBlock``(``(``conv1``):``Conv2d``(``256``,``512``,``kernel_size``=``(``3``,``3``),``stride``=``(``2``,``2``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``512``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``conv2``):``Conv2d``(``512``,``512``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn2``):``BatchNorm2d``(``512``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``downsample``):``Sequential``(``(``0``):``Conv2d``(``256``,``512``,``kernel_size``=``(``1``,``1``),``stride``=``(``2``,``2``),``bias``=``False``)``(``1``):``BatchNorm2d``(``512``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``)``)``(``1``):``BasicBlock``(``(``conv1``):``Conv2d``(``512``,``512``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn1``):``BatchNorm2d``(``512``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``(``relu``):``ReLU``(``inplace``)``(``conv2``):``Conv2d``(``512``,``512``,``kernel_size``=``(``3``,``3``),``stride``=``(``1``,``1``),``padding``=``(``1``,``1``),``bias``=``False``)``(``bn2``):``BatchNorm2d``(``512``,``eps``=``1e-05``,``momentum``=``0.1``,``affine``=``True``,``track_running_stats``=``True``)``)``)``(``avgpool``):``AdaptiveAvgPool2d``(``output_size``=``(``1``,``1``))``(``fc``):``Linear``(``in_features``=``512``,``out_features``=``1000``,``bias``=``True``)``)`'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There’s almost nothing here you haven’t already seen in this chapter, with the
    exception of `BatchNorm2d`. Let’s have a look at what that does in one of those
    layers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，除了`BatchNorm2d`之外，几乎没有什么是你之前没有看到的。让我们看看其中一个层中的`BatchNorm2d`是做什么的。
- en: BatchNorm
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: BatchNorm
- en: 'BatchNorm, short for batch normalization, is a simple layer that has one task
    in life: using two learned parameters (meaning that it will be trained along with
    the rest of the network) to try to ensure that each minibatch that goes through
    the network has a mean centered around zero with a variance of 1\. You might ask
    why we need to do this when we’ve already normalized our input by using the transform
    chain in [Chapter 2](index_split_033.html#filepos97807). For smaller networks,
    `BatchNorm` is indeed less useful, but as they get larger, the effect of any layer
    on another, say 20 layers down, can be vast because of repeated multiplication,
    and you may end up with either vanishing or exploding gradients, both of which
    are fatal to the training process. The `BatchNorm` layers make sure that even
    if you use a model such as ResNet-152, the multiplications inside your network
    don’t get out of hand.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: BatchNorm，即批量归一化，是一个简单的层，它的任务是使用两个学习参数（意味着它将与网络的其余部分一起训练）来尝试确保通过网络的每个小批量具有以零为中心的均值和方差为1。你可能会问为什么我们需要这样做，当我们已经通过使用[第2章](index_split_033.html#filepos97807)中的变换链对输入进行了归一化。对于较小的网络，`BatchNorm`确实不太有用，但随着网络变得更大，任何一层对另一层的影响，比如说20层之后，可能会很大，因为重复的乘法，你可能最终会得到消失或爆炸的梯度，这两者对训练过程都是致命的。`BatchNorm`层确保即使你使用ResNet-152这样的模型，你的网络内部的乘法也不会失控。
- en: 'You might be wondering: if we have `BatchNorm` in our network, why are we normalizing
    the input at all in the training loop’s transformation chain? After all, shouldn’t
    `BatchNorm` do the work for us? And the answer here is yes, you could do that!
    But it’ll take longer for the network to learn how to get the inputs under control,
    as they’ll have to discover the initial transform themselves, which will make
    training longer.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：如果我们的网络中有`BatchNorm`，为什么在训练循环的转换链中还要对输入进行归一化？毕竟，`BatchNorm`不应该为我们做这项工作吗？答案是是的，你可以这样做！但网络需要更长的时间来学习如何控制输入，因为它们必须自己发现初始变换，这将使训练时间更长。
- en: 'I recommend that you instantiate all of the architectures we’ve talked about
    so far and use `print(model)` to see which layers they use and in what order operations
    happen. After that, there’s another key question: which of these architectures
    should I use?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你实例化我们到目前为止讨论过的所有架构，并使用`print(model)`来查看它们使用的层以及操作发生的顺序。之后，还有另一个关键问题：我应该使用这些架构中的哪一个？
- en: Which Model Should You Use?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该使用哪个模型？
- en: The unhelpful answer is, whichever one works best for you, naturally! But let’s
    dig in a little. First, although I suggest that you try the NASNet and PNAS architectures
    at the moment, I wouldn’t wholeheartedly recommend them, despite their impressive
    results on ImageNet. They can be surprisingly memory-hungry in operation, and
    the transfer learning technique, which you learn about in [Chapter 4](index_split_070.html#filepos357125),
    is not quite as effective compared to the human-built architectures including
    ResNet.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 不太有用的答案是，自然是哪个对你最好的！但让我们深入一点。首先，尽管我建议你目前尝试NASNet和PNAS架构，但我不会全力推荐它们，尽管它们在ImageNet上取得了令人印象深刻的结果。它们在操作中可能会消耗惊人的内存，并且迁移学习技术，你将在[第4章](index_split_070.html#filepos357125)中了解到，与包括ResNet在内的人工构建的架构相比，效果不是很好。
- en: I suggest that you have a look around the image-based competitions on [Kaggle](https://www.kaggle.com),
    a website that runs hundreds of data science competitions, and see what the winning
    entries are using. More than likely you’ll end up seeing a bunch of ResNet-based
    ensembles. Personally, I like and use the ResNet architectures over and above
    any of the others listed here, first because they offer good accuracy, and second
    because it’s easy to start out experimenting with a ResNet-34 model for fast iteration
    and then move to larger ResNets (and more realistically, an ensemble of different
    ResNet architectures, just as Microsoft used in their ImageNet win in 2015) once
    I feel I have something promising.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你在[Kaggle](https://www.kaggle.com)上看看基于图像的比赛，这是一个举办数百个数据科学竞赛的网站，并看看获胜作品在使用什么。很可能你会看到一堆基于ResNet的集成。就我个人而言，我喜欢并使用ResNet架构，而不是这里列出的任何其他架构，首先是因为它们提供了良好的准确性，其次是因为可以轻松开始尝试使用ResNet-34模型进行快速迭代，然后转向更大的ResNet（更现实地说，是不同ResNet架构的集成，就像微软在2015年ImageNet比赛中使用的那样），一旦我觉得有一些有希望的东西。
- en: Before we end the chapter, I have some breaking news concerning downloading
    pretrained models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，我有一些关于下载预训练模型的突发新闻。
- en: 'One-Stop Shopping for Models: PyTorch Hub'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Hub：模型一站式购物
- en: 'A recent announcement in the PyTorch world provides an additional route to
    get models: PyTorch Hub. This is supposed to become a central location for obtaining
    any published model in the future, whether it’s for operating on images, text,
    audio, video, or any other type of data. To obtain a model in this fashion, you
    use the `torch.hub` module:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch世界最近的一个公告提供了另一种获取模型的途径：PyTorch Hub。这应该成为未来获取任何已发布模型的中心位置，无论是用于处理图像、文本、音频、视频还是任何其他类型的数据。要以这种方式获取模型，你可以使用`torch.hub`模块：
- en: '`model``=``torch``.``hub``.``load``(``''pytorch/vision''``,``''resnet50''``,``pretrained``=``True``)`'
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`model``=``torch``.``hub``.``load``(``''pytorch/vision''``,``''resnet50''``,``pretrained``=``True``)`'
- en: The first parameter points to a GitHub owner and repository (with an optional
    tag/branch identifier in the string as well); the second is the model requested
    (in this case, `resnet50`); and finally, the third indicates whether to download
    pretrained weights. You can also use `torch.hub.list('pytorch/vision')` to discover
    all the models inside that repository that are available to download.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数指向GitHub的所有者和仓库（字符串中还可以包含可选的标签/分支标识符）；第二个是请求的模型（在本例中为`resnet50`）；最后，第三个指示是否下载预训练权重。您还可以使用`torch.hub.list('pytorch/vision')`来发现该仓库中可供下载的所有模型。
- en: PyTorch Hub is brand new as of mid-2019, so there aren’t a huge number of models
    available as I write this, but I expect it to become a popular way to distribute
    and download models by the end of the year. All the models in this chapter can
    be loaded through the `pytorch/vision` repo in PytorchHub, so feel free to use
    this loading process instead of `torchvision.models`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Hub是2019年中新推出的，因此在我写这篇文章时可用的模型数量并不多，但我预计到年底它将成为一种流行的分发和下载模型的方式。本章中的所有模型都可以通过PytorchHub中的`pytorch/vision`仓库加载，所以请随意使用这种加载过程，而不是`torchvision.models`。
- en: Conclusion
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you’ve taken a quick walk-through of how CNN-based neural networks
    work, including features such as `Dropout`, `MaxPool`, and `BatchNorm`. You’ve
    also looked at the most popular architectures used in industry today. Before moving
    on to the next chapter, play with the architectures we’ve been talking about and
    see how they compare. (Don’t forget, you don’t need to train them! Just download
    the weights and test the model.)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经快速了解了基于CNN的神经网络是如何工作的，包括`Dropout`、`MaxPool`和`BatchNorm`等特性。您还看过了当今工业中最流行的架构。在继续下一章之前，尝试一下我们讨论过的架构，看看它们之间的比较。（不要忘记，您不需要对它们进行训练！只需下载权重并测试模型。）
- en: We’re going to close out our look at computer vision by using these pretrained
    models as a starting point for a custom solution for our cats versus fish problem
    that uses transfer learning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用这些预训练模型作为我们猫与鱼问题的自定义解决方案的起点来结束我们对计算机视觉的探讨，这将使用迁移学习。
- en: Further Reading
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[AlexNet: “ImageNet Classification with Deep Convolutional Neural Networks”](https://oreil.ly/CsoFv)
    by Alex Krizhevsky et al. (2012)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AlexNet：“使用深度卷积神经网络进行ImageNet分类”](https://oreil.ly/CsoFv) 作者：Alex Krizhevsky等（2012）'
- en: '[VGG: “Very Deep Convolutional Networks for Large-Scale Image Recognition”](https://arxiv.org/abs/1409.1556)
    by Karen Simonyan and Andrew Zisserman (2014)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VGG：“用于大规模图像识别的非常深的卷积网络”](https://arxiv.org/abs/1409.1556) 作者：Karen Simonyan和Andrew
    Zisserman（2014）'
- en: '[Inception: “Going Deeper with Convolutions”](https://arxiv.org/abs/1409.4842)
    by Christian Szegedy et al. (2014)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Inception：“通过卷积进行更深入的研究”](https://arxiv.org/abs/1409.4842) 作者：Christian Szegedy等（2014）'
- en: '[ResNet: “Deep Residual Learning for Image Recognition”](https://arxiv.org/abs/1512.03385)
    by Kaiming He et al. (2015)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ResNet：“深度残差学习用于图像识别”](https://arxiv.org/abs/1512.03385) 作者：何凯明等（2015）'
- en: '[NASNet: “Learning Transferable Architectures for Scalable Image Recognition”](https://arxiv.org/abs/1707.07012)
    by Barret Zoph et al. (2017)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NASNet：“学习可转移的架构用于可扩展的图像识别”](https://arxiv.org/abs/1707.07012) 作者：Barret Zoph等（2017）'
- en: '[1](index_split_054.html#filepos229817)'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1](index_split_054.html#filepos229817)'
- en: Kernel and filter tend to be used interchangeably in the literature. If you
    have experience in graphics processing, kernel is probably more familiar to you,
    but I prefer filter.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在文献中，核和滤波器往往可以互换使用。如果您有图形处理经验，核可能更熟悉，但我更喜欢滤波器。
