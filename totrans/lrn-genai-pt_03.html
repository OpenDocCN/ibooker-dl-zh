<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">3 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/>Generative adversarial networks: Shape and number generation</h1>
<p class="co-summary-head"><a id="marker-43"/>This chapter covers<a id="idIndexMarker000"/><a id="marker-42"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">Building generator and discriminator networks in generative adversarial networks from scratch</li>
<li class="co-summary-bullet">Using GANs to generate data points to form shapes (e.g., exponential growth curve)</li>
<li class="co-summary-bullet">Generating integer sequences that are all multiples of 5</li>
<li class="co-summary-bullet">Training, saving, loading, and using GANs</li>
<li class="co-summary-bullet">Evaluating GAN performance and determining training stop points</li>
</ul>
<p class="body">Close to half of the generative models in this book belong to a category called generative adversarial networks (GANs). The method was first proposed by Ian Goodfellow and his coauthors in 2014.<sup class="footnotenumber" id="footnote-001-backlink"><a class="url1" href="#footnote-001">1</a></sup> GANs, celebrated for their ease of implementation and versatility, empower individuals with even rudimentary knowledge of deep learning to construct their models from the ground up. The word “adversarial” in GAN refers to the fact that the two neural networks compete against each other in a zero-sum game framework. The generative network tries to create data instances indistinguishable from real samples. In contrast, the discriminative network tries to identify the generated samples from real ones. These versatile models can generate various content formats, from geometric shapes and sequences of numbers to high-resolution color images and even realistic-sounding musical compositions. <a id="idTextAnchor004"/></p>
<p class="body">In this chapter, we’ll briefly review the theory behind GANs. Then, I’ll show you how to implement that knowledge in PyTorch. You’ll learn to build your first GAN from scratch so that all the details are demystified. To make the example relatable, imagine you put $1 in a savings account that pays 8% a year. You want to find out the balance in your account based on the number of years you have invested. The true relation is an exponential growth curve. You’ll learn to use GANs to generate data sample<a id="idTextAnchor005"/>s—pairs of values <span class="times">(x, y)</span> that form such an exponential growth curve, with a mathematical relation <span class="times">y = 1.08<sup class="fm-superscript">x</sup></span>. Armed with this skill, you’ll be able to generate data to mimic any shape: sine, cosine, quadratic, and so on.</p>
<p class="body">In the second project in this chapter, you’ll learn how to use GANs to generate a sequence of numbers that are all multiples of 5. But you can change the pattern to multiples of 2, 3, 7, or other patterns. Along the way, you’ll learn how to create a generator network and a discriminator network from scratch. You’ll learn how to train, save, and use GANs. Further, you’ll also learn to assess the performance of GANs either by visualizing samples generated by the generator network or by measuring the divergence between the generated sample distribution and the real data distribution.</p>
<p class="body"><a id="marker-44"/>Imagine that you need data to train a machine learning (ML) model to predict the relation between pairs of values <span class="times">(x, y)</span>. However, the training dataset is costly and time-consuming for human beings to prepare by hand. GANs can be well-suited to generate data in such cases: while the generated values of <span class="times">x</span> and <span class="times">y</span> generally conform to a mathematical relation, there is also noise in the generated data. The noise can be useful for preventing overfitting when the generated data is used to train the ML model. <a id="idIndexMarker001"/></p>
<p class="body">The primary goal of this chapter is not necessarily to generate novel content with the most practical use. Instead, my objective is to teach you how to train and use GANs to create various formats of content from scratch. Along the way, you will gain a solid understanding of the inner workings of GANs. This foundation will allow us to concentrate on other, more advanced, aspects of GANs in later chapters when generating other content such as high-resolution images or realistic-sounding music (e.g., convolutional neural networks or how to represent a piece of music as a multidimensional object).</p>
<h2 class="fm-head" id="heading_id_3">3.1 Steps involved in training GANs</h2>
<p class="body">In chapter 1, you gained a high-level overview of the theories behind GANs. In this section, I’ll provide a summary of the steps involved in training GANs in general and in creating data points to form an exponential growth curve in particular.<a id="idIndexMarker002"/><a id="idIndexMarker003"/></p>
<p class="body">Let’s return to our previous example: you plan to invest in a savings account that pays 8% annual interest. You put $1 in the account today and want to know how much money you’ll have in the account in the future.</p>
<p class="body">The amount in your account in the future, y, depends on how long you invest in the savings account. Let’s denote the number of years you invest by <span class="times">x</span>, which can be a number, say, between 0 and 50. For example, if you invest for 1 year, the balance is <span class="times">$1.08</span>; if you invest for 2 years, the balance is <span class="times">1.08<sup class="fm-superscript">2</sup> = $1.17</span>. To generalize, the relationship between <span class="times">x</span> and <span class="times">y</span> is <span class="times">y = 1.08<sup class="fm-superscript">x</sup></span>. The function depicts an exponential growth curve. Note here that x can be a whole number such as 1 or 2, as well as a decimal number such as 1.14 or 2.35 and the formula still works.</p>
<p class="body">Training GANs to generate data points that conform to a specific mathematical relation, like the preceding example, is a multistep process. In your case, you want to generate data points <span class="times">(x, y)</span> such that <span class="times">y = 1.08<sup class="fm-superscript">x</sup></span>. Figure 3.1 provides a diagram of the architecture of GANs and the steps involved in generating an exponential growth curve. When you generate other content such as a sequence of integers, images, or music, you follow similar steps, as you’ll see in the second project in this chapter, as well as in other GAN models later in this book.<a id="marker-45"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="359" src="../../OEBPS/Images/CH03_F01_Liu.png" width="842"/></p>
<p class="figurecaption">Figure 3.1 The steps involved in training GANs to generate an exponential growth curve and the dual-network architecture in GANs. The generator obtains a random noise vector Z from the latent space (top left) to create a fake sample and presents it to the discriminator (middle). The discriminator classifies a sample as real (from the training set) or fake (created by the generator). The predictions are compared to the ground truth and both the discriminator and the generator learn from the predictions. After many iterations of training, the generator learns to create shapes that are indistinguishable from real samples.</p>
</div>
<p class="body">Before we start, we need to obtain a training dataset to train GANs. In our running example, we’ll generate a dataset of <span class="times">(x, y)</span> pairs using the mathematical relation <span class="times">y = 1.08<sup class="fm-superscript">x</sup></span>. We use the savings account example so that the numbers are relatable. The techniques you learn in this chapter can be applied to other shapes: sine, cosine, U-shape, and so on. You can choose a range of <span class="times">x</span> values (say, 0 to 50) and calculate the corresponding <span class="times">y</span> values. Since we usually train models in batches of data in deep learning, the number of observations in your training dataset is usually set to a multiple of the batch size. A real sample is located at the top of figure 3.1, which has an exponential growth curve shape.</p>
<p class="body">Once you have the training set ready, you need to create two networks in GANs: a generator and a discriminator. The generator, located at the bottom left of figure 3.1, takes a random noise vector <span class="times">Z</span> as the input and generates data points (step 1 of our training loop). The random noise vector <span class="times">Z</span> used by the generator is obtained from the latent space, which represents the range of possible outputs the GAN can produce and is central to the GAN’s ability to generate diverse data samples. In chapter 5, we’ll explore the latent space to select the attributes of the content created by the generator. The discriminator, located at the center of figure 3.1, evaluates whether a given data point <span class="times">(x, y)</span> is real (from the training dataset) or fake (created by the generator); this is step 2 of our training loop.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">The meaning of the latent space</p>
<p class="fm-sidebar-text">The latent space in a GAN is a conceptual space where each point can be transformed into a realistic data instance by the generator. This space represents the range of possible outputs the GAN can produce and is central to the GAN’s ability to generate varied and complex data. The latent space acquires its significance exclusively when it is employed in conjunction with the generative model. Within this context, one can interpolate between points in the latent space to affect the attributes of output, which we’ll discuss in chapter 5.</p>
</div>
<p class="body"><a id="marker-46"/>To know how to adjust model parameters, we must choose the right loss functions. We need to define the loss functions for both the generator and discriminator. The loss function encourages the generator to generate data points that resemble data points from the training dataset, making the discriminator classify them as real. The loss function encourages the discriminator to correctly classify real and generated data points.</p>
<p class="body">In each iteration of the training loop, we alternate between training the discriminator and the generator. During each training iteration, we sample a batch of real <span class="times">(x, y)</span> data points from the training dataset and a batch of fake data points generated by the generator. When training the discriminator, we compare the predictions by the discriminative model, which is a probability that the sample is from the training set, with the ground truth, which is 1 if the sample is real and 0 if the sample is fake (shown at the right of figure 3.1); this constitutes half of step 3 in the training loop. We adjust the weights in the discriminator network slightly so that in the next iteration, the predicted probability moves closer to the ground truth (half of step 4 in our training loop).</p>
<p class="body">When training the generator, we feed fake samples to the discriminative model and obtain a probability that the sample is real (the other half of step 3). We then adjust the weights in the generator network slightly so that in the next iteration, the predicted probability moves closer to 1 (since the generator wants to create samples to fool the discriminator into thinking they are real); this constitutes the other half of step 4. We repeat this process for many iterations, making the generator network create more realistic data points.</p>
<p class="body">A natural question is when to stop training the GANs. For that, you evaluate the GAN’s performance by generating a set of synthetic data points and comparing them to the real data points from the training dataset. In most cases, we use visualization techniques to assess how well the generated data conforms to the desired relation. However, in our running example, since we know the distribution of the training data, we can calculate the mean squared error (MSE) between the generated data and the true data distribution. We stop training GANs when the generated samples stop improving their qualities after a fixed number of rounds of training.</p>
<p class="body">At this point, the model is considered trained. We then discard the discriminator and keep the generator. To create an exponential growth curve, we feed a random noise vector Z to the trained generator and obtain pairs of <span class="times">(x, y)</span> to form the desired shape.<a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="marker-47"/></p>
<h2 class="fm-head" id="heading_id_4">3.2 Preparing training data</h2>
<p class="body">In this section, you’ll create the training dataset so that you can use it to train the GAN model later in this chapter. Specifically, you’ll create pairs of data points <span class="times">(x, y)</span> that conform to the exponential growth shape. You’ll place them in batches so that they are ready to be fed to deep neural networks.<a id="idIndexMarker006"/><a id="idIndexMarker007"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The code for this chapter, as well as other chapters in this book, is available at the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>.</p>
<h3 class="fm-head1" id="heading_id_5">3.2.1 A training dataset that forms an exponential growth curve</h3>
<p class="body">We’ll create a dataset that contains many observations of data pairs, <span class="times">(x, y)</span>, where <span class="times">x</span> is uniformly distributed in the interval <span class="times">[0, 50]</span> and <span class="times">y</span> is related to <span class="times">x</span> based on the formula <span class="times">y = 1.08<sup class="fm-superscript">x</sup></span>, as shown in the following listing. <a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="idIndexMarker010"/></p>
<p class="fm-code-listing-caption">Listing 3.1 Creating training data to form an exponential growth shape</p>
<pre class="programlisting">import torch
  
torch.manual_seed(0)                                <span class="fm-combinumeral">①</span>
  
observations = 2048
  
train_data = torch.zeros((observations, 2))         <span class="fm-combinumeral">②</span>
  
train_data[:,0]=50*torch.rand(observations)         <span class="fm-combinumeral">③</span>
  
train_data[:,1]=1.08**train_data[:,0]               <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Fixes the random state so results are reproducible</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a tensor with 2,048 rows and 2 columns</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Generates values of x between 0 and 50</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Generates values of y based on the relation y = 1.08<sup class="fm-superscript1">x</sup></p>
<p class="body">First, we create 2,048 values of <span class="times">x</span> between 0 and 50 using the <code class="fm-code-in-text">torch.rand()</code> method. We use the <code class="fm-code-in-text">manual_seed()</code> method in PyTorch to fix the random state so that all results are reproducible. We first create a PyTorch tensor, <code class="fm-code-in-text">train_data</code>, with 2,048 rows and 2 columns. The values of <span class="times">x</span> are placed in the first column in the tensor <code class="fm-code-in-text">train_data</code>. The <code class="fm-code-in-text">rand()</code> method in PyTorch generates random values between 0.0 and 1.0. By multiplying the value by 50, the resulting values of <span class="times">x</span> are between 0.0 and 50.0. We then fill the second column of <code class="fm-code-in-text">train_data</code> with values of <span class="times">y = 1.08<sup class="fm-superscript">x</sup></span>. <a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="marker-48"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 3.1</p>
<p class="fm-sidebar-text">Modify listing 3.1 so that the relation between <span class="times">x</span> and <span class="times">y</span> is <span class="times">y = sin(x)</span> by using the <code class="fm-code-in-text1">torch.sin()</code> function. Set the value<a id="idTextAnchor006"/> of <span class="times">x</span> between <span class="times">–5</span> and <span class="times">5</span> by using this line of code: <code class="fm-code-in-text1">train_data[:,0]=10*(torch.rand(observations)-0.5)</code>. <a id="idIndexMarker015"/></p>
</div>
<p class="body">We plot the relation between <span class="times">x</span> and <span class="times">y</span> by using the Matplotlib library.</p>
<p class="fm-code-listing-caption">Listing 3.2 Visualizing the relation between <span class="times">x</span> and <span class="times">y</span></p>
<pre class="programlisting">import matplotlib.pyplot as plt
  
fig=plt.figure(dpi=100,figsize=(8,6))
plt.plot(train_data[:,0],train_data[:,1],".",c="r")    <span class="fm-combinumeral">①</span>
plt.xlabel("values of x",fontsize=15)
plt.ylabel("values of $y=1.08^x$",fontsize=15)         <span class="fm-combinumeral">②</span>
plt.title("An exponential growth shape",fontsize=20)   <span class="fm-combinumeral">③</span>
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Plots the relation between <span class="times">x</span> and <span class="times">y</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Labels y-axis</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a title for the plot</p>
<p class="body">You will see an exponential growth curve shape after running listing 3.2, which is similar to the top graph in figure 3.1.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 3.2</p>
<p class="fm-sidebar-text">Modify listing 3.2 to plot the relation between <span class="times">x</span> and <span class="times">y = sin(x)</span> based on your changes in exercise 3.1. Make sure you change the y-axis label and the title in the plot to reflect the changes you made.</p>
</div>
<h3 class="fm-head1" id="heading_id_6">3.2.2 Preparing the training dataset</h3>
<p class="body">We’ll place the data samples you just created into batches so that we can feed them to the discriminator network. We use the <code class="fm-code-in-text">DataLoader()</code> class in PyTorch to wrap an iterable around the training dataset so that we can easily access the samples during training, like so:<a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>
<pre class="programlisting">from torch.utils.data import DataLoader
  
batch_size=128
train_loader=DataLoader(
    train_data,
    batch_size=batch_size,
    shuffle=True)</pre>
<p class="body">Make sure you select the total number of observations and the batch size so that all batches have the same number of samples in them. We chose 2,048 observations with a batch size of 128. As a result, we have <span class="times">2,048/128 = 16</span> batches. The <code class="fm-code-in-text">shuffle=True</code> argument in <code class="fm-code-in-text">DataLoader()</code> shuffles the observations randomly before dividing them into batches. <a id="idIndexMarker018"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Shuffling makes sure that the data samples are evenly distributed and observations within a batch are not correlated, which, in turn, stabilizes training. In this specific example, shuffling ensures that values of x fall randomly between 0 and 50, instead of clustering in a certain range, say, between 0 and 5.</p>
<p class="body">You can access a batch of data by using the <code class="fm-code-in-text">next()</code> and <code class="fm-code-in-text">iter()</code> methods, like so:<a id="idIndexMarker019"/><a id="idIndexMarker020"/></p>
<pre class="programlisting">batch0=next(iter(train_loader))
print(batch0)</pre>
<p class="body">You will see 128 pairs of numbers <span class="times">(x, y)</span>, where the value of <span class="times">x</span> falls randomly between 0 and 50. Further, the values of <span class="times">x</span> and <span class="times">y</span> in each pair conform to the relation <span class="times">y = 1.08<sup class="fm-superscript">x</sup></span>. <a id="idIndexMarker021"/><a id="idIndexMarker022"/><a id="marker-49"/></p>
<h2 class="fm-head" id="heading_id_7">3.3 Creating GANs</h2>
<p class="body">Now that the training dataset is ready, we’ll create a discriminator network and a generator network. The discriminator network is a binary classifier, which is very similar to the binary classifier for clothing items we have created and trained in chapter 2. Here, the discriminator’s job is to classify the samples into either real or fake. The generator network, on the other hand, tries to create data points <span class="times">(x, y)</span> that are indistinguishable from those in the training set so that the discriminator will classify them as real. <a id="idIndexMarker023"/></p>
<h3 class="fm-head1" id="heading_id_8">3.3.1 The discriminator network</h3>
<p class="body">We use PyTorch to create a discriminator neural network. We’ll use fully connected (dense) layers with <code class="fm-code-in-text">ReLU</code> activations. We’ll also use dropout layers to prevent overfitting. We create a sequential deep neural network in PyTorch to represent the discriminator, as shown in the following listing.<a id="idIndexMarker024"/><a id="idIndexMarker025"/></p>
<p class="fm-code-listing-caption">Listing 3.3 Creating a discriminator network</p>
<pre class="programlisting">import torch.nn as nn
  
device="cuda" if torch.cuda.is_available() else "cpu"    <span class="fm-combinumeral">①</span>
  
D=nn.Sequential(
    nn.Linear(2,256),                                    <span class="fm-combinumeral">②</span>
    nn.ReLU(),
    nn.Dropout(0.3),                                     <span class="fm-combinumeral">③</span>
    nn.Linear(256,128),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(128,64),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(64,1),                                     <span class="fm-combinumeral">④</span>
    nn.Sigmoid()).to(device)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Automatically checks if CUDA-enabled GPU is available.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The number of input features in the first layer is 2, matching the number of elements in each data instance, which has two values, <span class="times">x</span> and <span class="times">y</span> .</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The dropout layer prevents overfitting.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The number of output features in the last layer is 1 so that we can squeeze it into a value between 0 and 1.</p>
<p class="body">Make sure that in the first layer, the input shape is 2 because, in our sample, each data instance has two values in it: <span class="times">x</span> and <span class="times">y</span> . The number of inputs in the first layer should always match with the size of the input data. Also, make sure that the number of output features is 1 in the last layer: the output of the discriminator network is a single value. We use the sigmoid activation function to squeeze the output to the range <span class="times">[0, 1]</span> so that it can be interpreted as the probability, <span class="times">p</span>, that the sample is real. With the complementary probability, <span class="times">1 – p</span>, the sample is fake. This is very similar to what we have done in chapter 2 when a binary classifier attempts to identify a piece of clothing item as either an ankle boot or a t-shirt.</p>
<p class="body">The hidden layers have 256, 128, and 64 neurons in them, respectively. There is nothing magical about these numbers, and you can easily change them and have similar results as long as they are in a reasonable range. If the number of neurons in hidden layers is too large, it may lead to overfitting of the model; if the number is too small, it may lead to underfitting. The number of neurons can be optimized separately using a validation set through hyperparameter tuning.</p>
<p class="body"><a id="marker-50"/>Dropout layers randomly deactivate (or “drop out”) a certain percentage of neurons in the layer to which they are applied. This means that these neurons do not participate in forward or backward passes during training. Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and random fluctuations, leading to poor performance on unseen data. Dropout layers are an effective way to prevent overfitting.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">2</a></sup></p>
<h3 class="fm-head1" id="heading_id_9">3.3.2 The generator network</h3>
<p class="body">The generator’s job is to create a pair of numbers <span class="times">(x, y)</span> so that it can pass the screening of the discriminator. That is, the generator is trying to create a pair of numbers to maximize the probability that the discriminator thinks that the numbers are from the training dataset (i.e., they conform to the relation <span class="times">y = 1.08<sup class="fm-superscript">x</sup></span>). We create the neural network in the following listing to represent the generator.<a id="idIndexMarker026"/></p>
<p class="fm-code-listing-caption">Listing 3.4 Creating a generator network</p>
<pre class="programlisting">G=nn.Sequential(
    nn.Linear(2,16),               <span class="fm-combinumeral">①</span>
    nn.ReLU(),
    nn.Linear(16,32),
    nn.ReLU(),
    nn.Linear(32,2)).to(device)    <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The number of input features in the first layer is 2, the same as the dimension of the random noise vector from the latent space.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The number of output features in the last layer is 2, the same as the dimension of the data sample, which contains two values <span class="times">(x, y)</span>.</p>
<p class="body">We feed a random noise vector from a 2D latent space, (<span class="times">z<sub class="fm-subscript">1</sub>, z<sub class="fm-subscript">2</sub></span>), to the generator. The generator then generates a pair of values <span class="times">(x, y)</span>, based on the input from the latent space. Here we use a 2D latent space, but changing the dimension to other numbers such as 5 or 10 wouldn’t affect our results.</p>
<h3 class="fm-head1" id="heading_id_10">3.3.3 Loss functions, optimizers, and early stopping</h3>
<p class="body"><a id="marker-51"/>Since the discriminator network is essentially performing a binary classification task (identifying a data sample as real or fake), we use binary cross-entropy loss, the preferred loss function in binary classifications, for the discriminator network. The discriminator is trying to maximize the accuracy of the binary classification: identify a real sample as real and a fake sample as fake. The weights in the discriminator network are updated based on the gradient of the loss function with respect to the weights.<a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>
<p class="body">The generator is trying to minimize the probability that the fake sample is being identified as fake. Therefore, we’ll also use binary cross-entropy loss for the generator network: the generator updates its network weights so that the generated samples will be classified as real by the discriminator in a binary classification problem.</p>
<p class="body">As we have done in chapter 2, we use the Adam optimizer as the gradient descent algorithm. We set the learning rate to 0.0005. Let’s code those steps in by using PyTorch:</p>
<pre class="programlisting">loss_fn=nn.BCELoss()
lr=0.0005
optimD=torch.optim.Adam(D.parameters(),lr=lr)
optimG=torch.optim.Adam(G.parameters(),lr=lr)</pre>
<p class="body">One question remains before we get to the actual training: How many epochs should we train the GANs? How do we know the model is well trained so that the generator is ready to create samples that can mimic the exponential growth curve shape? If you recall, in chapter 2, we split the training set further into a train set and a validation set. We then used the loss in the validation set to determine whether the parameters had converged so that we could stop training. However, GANs are trained using a different approach compared to traditional supervised learning models (such as the classification models you have seen in chapter 2). Since the quality of the generated samples improves throughout training, the discriminator’s task becomes more and more difficult (in a way, the discriminator in GANs is making predictions on a moving target). The loss from the discriminator network is not a good indicator of the quality of the model.</p>
<p class="body">One common method to measure the performance of GANs is through visual inspection. Humans can assess the quality and realism of generated data instances by simply looking at them. This is a qualitative approach but can be very informative. But in our simple case, since we know the exact distribution of the training dataset, we’ll look at the MSE of the generated samples relative to samples in the training set and use it as a measure of the performance of the generator. Let’s code that in:</p>
<pre class="programlisting">mse=nn.MSELoss()                              <span class="fm-combinumeral">①</span>
  
def performance(fake_samples):
    real=1.08**fake_samples[:,0]              <span class="fm-combinumeral">②</span>
    mseloss=mse(fake_samples[:,1],real)       <span class="fm-combinumeral">③</span>
    return mseloss</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Uses MSE as the criterion to measure performance</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Finds out the true distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Compares the generated distribution with the true distribution and calculates MSE</p>
<p class="body">We’ll stop training the model if the performance of the generator doesn’t improve in, say, 1,000 epochs. Therefore, we define an early stopping class, as we did in chapter 2, to decide when to stop training the model.</p>
<p class="fm-code-listing-caption">Listing 3.5 An early stopping class to decide when to stop training</p>
<pre class="programlisting">class EarlyStop:
    def __init__(self, patience=1000):       <span class="fm-combinumeral">①</span>
        self.patience = patience
        self.steps = 0
        self.min_gdif = float('inf')
    def stop(self, gdif):                    <span class="fm-combinumeral">②</span>
        if gdif &lt; self.min_gdif:             <span class="fm-combinumeral">③</span>
            self.min_gdif = gdif
            self.steps = 0
        elif gdif &gt;= self.min_gdif:
            self.steps += 1
        if self.steps &gt;= self.patience:      <span class="fm-combinumeral">④</span>
            return True
        else:
            return False
  
stopper=EarlyStop()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sets the default value of patience to 1000</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines the stop() method</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> If a new minimum difference between the generated distribution and true distribution is reached, updates the value of min_gdif.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Stops training if the model stops improving for 1,000 epochs</p>
<p class="body"><a id="marker-52"/>With that, we have all the components we need to train our GANs, which we’ll do in the next section.<a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/></p>
<h2 class="fm-head" id="heading_id_11">3.4 Training and using GANs for shape generation</h2>
<p class="body">Now that we have the training data and two networks, we’ll train the model. After that, we’ll discard the discriminator and use the generator to generate data points to form an exponential growth curve shape.<a id="idIndexMarker036"/><a id="idIndexMarker037"/></p>
<h3 class="fm-head1" id="heading_id_12">3.4.1 The training of GANs</h3>
<p class="body">We first create labels for real samples and fake samples, respectively. Specifically, we’ll label all real samples as 1s and all fake samples as 0s. During the training process, the discriminator compares its own predictions with the labels to receive feedback so that it can adjust model parameters to make better predictions in the next iteration. <a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>
<p class="body">Here, we define two tensors, <code class="fm-code-in-text">real_labels</code> and <code class="fm-code-in-text">fake_labels</code>:<a id="idIndexMarker040"/><a id="idIndexMarker041"/></p>
<pre class="programlisting">real_labels=torch.ones((batch_size,1))
real_labels=real_labels.to(device)
  
fake_labels=torch.zeros((batch_size,1))
fake_labels=fake_labels.to(device)</pre>
<p class="body"><a id="marker-53"/>The tensor <code class="fm-code-in-text">real_labels</code> is 2D with a shape of <code class="fm-code-in-text">(batch_size, 1)</code>—that is, 128 rows and 1 column. We use 128 rows because we’ll feed a batch of 128 real samples to the discriminator network to obtain 128 predictions. Similarly, the tensor <code class="fm-code-in-text">fake_labels</code> is 2D with a shape of <code class="fm-code-in-text">(batch_size, 1)</code>. We’ll feed a batch of 128 fake samples to the discriminator network to obtain 128 predictions and compare them with the ground truth: 128 labels of 0s. We move the two tensors to the GPU for fast training if your computer has a CUDA-enabled GPU.<a id="idIndexMarker042"/><a id="idIndexMarker043"/></p>
<p class="body">To train the GANs, we define a few functions so that the training loop looks organized. The first function, <code class="fm-code-in-text">train_D_on_real()</code>, trains the discriminator network with a batch of real samples.<a id="idIndexMarker044"/></p>
<p class="fm-code-listing-caption">Listing 3.6 Defining a <code class="fm-code-in-text">train_D_on_real()</code> function</p>
<pre class="programlisting">def train_D_on_real(real_samples):
    real_samples=real_samples.to(device)
    optimD.zero_grad()
    out_D=D(real_samples)                    <span class="fm-combinumeral">①</span>
    loss_D=loss_fn(out_D,real_labels)        <span class="fm-combinumeral">②</span>
    loss_D.backward()
    optimD.step()                            <span class="fm-combinumeral">③</span>
    return loss_D</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Makes predictions on real samples</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Calculates loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Backpropagation (i.e., updates model weights in the discriminator network so predictions are more accurate in the next iteration)</p>
<p class="body">The function <code class="fm-code-in-text">train_D_on_real()</code> first moves the real samples to GPU if the computer has a CUDA-enabled GPU. The discriminator network, D, makes predictions on the batch of samples. The model then compares the discriminator’s predictions, <code class="fm-code-in-text">out_D</code>, with the ground truth, <code class="fm-code-in-text">real_labels</code>, and calculates the loss of the predictions accordingly. The <code class="fm-code-in-text">backward()</code> method calculates the gradients of the loss function with respect to model parameters. The <code class="fm-code-in-text">step()</code> method adjusts the model parameters (that is, backpropagation). The <code class="fm-code-in-text">zero_grad()</code> method means that we explicitly set the gradients to 0 before backpropagation. Otherwise, the accumulated gradients instead of the incremental gradients are used on every <code class="fm-code-in-text">backward()</code> call.<a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/><a id="idIndexMarker048"/></p>
<p class="fm-callout"><span class="fm-callout-head">TIP</span> We call the <code class="fm-code-in-text2">zero_grad()</code> method before updating model weights when training each batch of data. We explicitly set the gradients to 0 before backpropagation to use incremental gradients instead of the accumulated gradients on every <code class="fm-code-in-text2">backward()</code> call.<a id="idIndexMarker049"/></p>
<p class="body">The second function, <code class="fm-code-in-text">train_D_on_fake()</code>, trains the discriminator network with a batch of fake samples.<a id="idIndexMarker050"/><a id="marker-54"/></p>
<p class="fm-code-listing-caption">Listing 3.7 Defining the <code class="fm-code-in-text">train_D_on_fake()</code> function</p>
<pre class="programlisting">def train_D_on_fake():
    noise=torch.randn((batch_size,2))
    noise=noise.to(device)
    fake_samples=G(noise)                    <span class="fm-combinumeral">①</span>
    optimD.zero_grad()
    out_D=D(fake_samples)                    <span class="fm-combinumeral">②</span>
    loss_D=loss_fn(out_D,fake_labels)        <span class="fm-combinumeral">③</span>
    loss_D.backward()
    optimD.step()                            <span class="fm-combinumeral">④</span>
    return loss_D</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates a batch of fake samples</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Makes predictions on the fake samples</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Backpropagation</p>
<p class="body">The function <code class="fm-code-in-text">train_D_on_fake()</code> first feeds a batch of random noise vectors from the latent space to the generator to obtain a batch of fake samples. The function then presents the fake samples to the discriminator to obtain predictions. The function compares the discriminator’s predictions, <code class="fm-code-in-text">out_D</code>, with the ground truth, <code class="fm-code-in-text">fake_labels</code>, and calculates the loss of the predictions accordingly. Finally, it adjusts the model parameters based on the gradients of the loss function with respect to model weights.<a id="idIndexMarker051"/></p>
<p class="fm-callout"><span class="fm-callout-head">Note</span> We use the terms <i class="fm-italics">weights</i> and <i class="fm-italics">parameters</i> interchangeably. Strictly speaking, model parameters also include bias terms, but we use the term <i class="fm-italics">model weights</i> loosely to include model biases. Similarly, we use the terms <i class="fm-italics">adjusting weights</i>, <i class="fm-italics">adjusting parameters</i>, and <i class="fm-italics">backpropagation</i> interchangeably.<a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/></p>
<p class="body">The third function, <code class="fm-code-in-text">train_G()</code>, trains the generator network with a batch of fake samples.<a id="idIndexMarker058"/></p>
<p class="fm-code-listing-caption">Listing 3.8 Defining the <code class="fm-code-in-text">train_G()</code> function</p>
<pre class="programlisting">def train_G(): 
    noise=torch.randn((batch_size,2))
    noise=noise.to(device)
    optimG.zero_grad()
    fake_samples=G(noise)                     <span class="fm-combinumeral">①</span>
    out_G=D(fake_samples)                     <span class="fm-combinumeral">②</span>
    loss_G=loss_fn(out_G,real_labels)         <span class="fm-combinumeral">③</span>
    loss_G.backward()
    optimG.step()                             <span class="fm-combinumeral">④</span>
    return loss_G, fake_samples </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a batch of fake samples</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Presents the fake samples to the discriminator to obtain predictions</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates the loss based on whether G has succeeded</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Backpropagation (i.e., updates weights in the generator network so the generated samples are more realistic in the next iteration)</p>
<p class="body">To train the generator, we first feed a batch of random noise vectors from the latent space to the generator to obtain a batch of fake samples. We then present the fake samples to the discriminator network to obtain a batch of predictions. We compare the discriminator’s predictions with <code class="fm-code-in-text">real_labels</code>, a tensor of 1s, and calculate the loss. It’s important that we use a tensor of 1s, not a tensor of 0s, as the labels, because the objective of the generator is to fool the discriminator into thinking that fake samples are real. Finally, we adjust the model parameters based on the gradients of the loss function with respect to model weights so that in the next iteration, the generator can create more realistic samples.</p>
<p class="fm-callout"><span class="fm-callout-head">Note</span> We use the tensor <code class="fm-code-in-text2">real_labels</code> (a tensor of 1s) instead of <code class="fm-code-in-text2">fake_labels</code> (a tensor of 0s) when calculating loss and assessing the generator network because the generator wants the discriminator to predict fake samples as real.<a id="idIndexMarker059"/><a id="idIndexMarker060"/></p>
<p class="body"><a id="marker-55"/>Finally, we define a function, <code class="fm-code-in-text">test_epoch()</code>, which prints out the losses for the discriminator and the generator periodically. Further, it plots the data points generated by the generator and compares them to those in the training set. The function <code class="fm-code-in-text">test_epoch()</code> is shown in the following listing.<a id="idIndexMarker061"/></p>
<p class="fm-code-listing-caption">Listing 3.9 Defining the <code class="fm-code-in-text">test_epoch()</code> function</p>
<pre class="programlisting">import os
os.makedirs("files", exist_ok=True)                           <span class="fm-combinumeral">①</span>
  
def test_epoch(epoch,gloss,dloss,n,fake_samples):
    if epoch==0 or (epoch+1)%25==0:
        g=gloss.item()/n
        d=dloss.item()/n
        print(f"at epoch {epoch+1}, G loss: {g}, D loss {d}") <span class="fm-combinumeral">②</span>
        fake=fake_samples.detach().cpu().numpy()
        plt.figure(dpi=200)
        plt.plot(fake[:,0],fake[:,1],"*",c="g",
            label="generated samples")                        <span class="fm-combinumeral">③</span>
        plt.plot(train_data[:,0],train_data[:,1],".",c="r",
            alpha=0.1,label="real samples")                   <span class="fm-combinumeral">④</span>
        plt.title(f"epoch {epoch+1}")
        plt.xlim(0,50)
        plt.ylim(0,50)
        plt.legend()
        plt.savefig(f"files/p{epoch+1}.png")
        plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a folder to hold files</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Periodically prints out losses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Plots the generated points as asterisks (*)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Plots training data as dots (.)</p>
<p class="body">After every 25 epochs, the function prints out the average losses for the generator and the discriminator in the epoch. Further, it plots a batch of fake data points generated by the generator (in asterisks) and compares them to the data points in the training set (in dots). The plot is saved as an image in your local folder /files/.</p>
<p class="body">Now we are ready to train the model. We iterate through all batches in the training dataset. For each batch of data, we first train the discriminator using the real samples. After that, the generator creates a batch of fake samples, and we use them to train the discriminator again. Finally, we let the generator create a batch of fake samples again, but this time, we use them to train the generator instead. We train the model until the early stopping condition is satisfied, as shown in the following listing.<a id="idIndexMarker062"/><a id="idIndexMarker063"/><a id="marker-56"/></p>
<p class="fm-code-listing-caption">Listing 3.10 Training GANs to generate an exponential growth curve</p>
<pre class="programlisting">for epoch in range(10000):                                  <span class="fm-combinumeral">①</span>
    gloss=0
    dloss=0
    for n, real_samples in enumerate(train_loader):         <span class="fm-combinumeral">②</span>
        loss_D=train_D_on_real(real_samples)
        dloss+=loss_D
        loss_D=train_D_on_fake()
        dloss+=loss_D
        loss_G,fake_samples=train_G()
        gloss+=loss_G
    test_epoch(epoch,gloss,dloss,n,fake_samples)            <span class="fm-combinumeral">③</span>
    gdif=performance(fake_samples).item()
    if stopper.stop(gdif)==True:                            <span class="fm-combinumeral">④</span>
        break</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Starts training loops</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Iterates through all batches in the training dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Shows generated samples periodically</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Determines if training should stop</p>
<p class="body">The training takes a few minutes if you are using GPU training. Otherwise, it may take 20 to 30 minutes, depending on the hardware configuration on your computer. Alternatively, you can download the trained model from the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>.</p>
<p class="body">After 25 epochs of training, the generated data are scattered around the point (0,0) and don’t form any meaningful shape (an epoch is when all training data is used for training once). After 200 epochs of training, the data points start to form an exponential growth curve shape, even though many points are far away from the dotted curve, which is formed by points from the training set. After 1,025 epochs, the generated points fit closely with the exponential growth curve. Figure 3.2 provides subplots of the output from six different epochs. Our GANs work really well: the generator is able to generate data points to form the desired shape.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="922" src="../../OEBPS/Images/CH03_F02_Liu.png" width="842"/></p>
<p class="figurecaption">Figure 3.2 Subplots of the comparison of the generated shape (asterisks in the graph) with the true exponential growth curve shape (dots in the graph) at different stages of the training process. At epoch 25, the generated samples don’t form any meaningful shape. At epoch 200, the samples start to look like an exponential growth curve shape. At epoch 1025, the generated samples align closely with the exponential growth curve.<a id="marker-57"/></p>
</div>
<h3 class="fm-head1" id="heading_id_13">3.4.2 Saving and using the trained generator</h3>
<p class="body">Now that the GANs are trained, we’ll discard the discriminator network, as we always do in GANs, and save the trained generator network in the local folder, as follows:<a id="idIndexMarker064"/><a id="idIndexMarker065"/></p>
<pre class="programlisting">import os
os.makedirs("files", exist_ok=True)
scripted = torch.jit.script(G) 
scripted.save('files/exponential.pt') </pre>
<p class="body">The <code class="fm-code-in-text">torch.jit.script()</code> method scripts a function or a <code class="fm-code-in-text">nn.Module</code> class as TorchScript code using the TorchScript compiler. We use the method to script our trained generator network and save it as a file, <code class="fm-code-in-text">exponential.pt</code>, on your computer.<a id="idIndexMarker066"/><a id="idIndexMarker067"/><a id="marker-58"/></p>
<p class="body">To use the generator, we don’t even need to define the model. We simply load up the saved file and use it to generate data points as follows:</p>
<pre class="programlisting">new_G=torch.jit.load('files/exponential.pt',
                     map_location=device)
new_G.eval()</pre>
<p class="body">The trained generator is now loaded to your device, which is either <code class="fm-code-in-text">CPU</code> or <code class="fm-code-in-text">CUDA</code> depending on if you have a CUDA-enabled GPU on your computer. The <code class="fm-code-in-text">map_location=device</code> argument in <code class="fm-code-in-text">torch.jit.load()</code> specifies where to load the generator. We can now use the trained generator to generate a batch of data points:<a id="idIndexMarker068"/><a id="idIndexMarker069"/></p>
<pre class="programlisting">noise=torch.randn((batch_size,2)).to(device)
new_data=new_G(noise) </pre>
<p class="body">Here, we first obtain a batch of random noise vectors from the latent space. We then feed them to the generator to produce the fake data. We can plot the generated data:</p>
<pre class="programlisting">fig=plt.figure(dpi=100)
plt.plot(new_data.detach().cpu().numpy()[:,0],
  new_data.detach().cpu().numpy()[:,1],"*",c="g",
        label="generated samples")                    <span class="fm-combinumeral">①</span>
plt.plot(train_data[:,0],train_data[:,1],".",c="r",
         alpha=0.1,label="real samples")              <span class="fm-combinumeral">②</span>
plt.title("Inverted-U Shape Generated by GANs")
plt.xlim(0,50)
plt.ylim(0,50)
plt.legend()
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Plots the generated data samples as asterisks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Plots the training data as dots</p>
<p class="body">You should see a plot similar to the last subplot in figure 3.2: the generated data samples closely resemble an exponential growth curve.</p>
<p class="body">Congratulations! You have created and trained your very first GANs. Armed with this skill, you can easily change the code so that the generated data matches other shapes such as sine, cosine, U-shape, and so on. <a id="idIndexMarker070"/><a id="idIndexMarker071"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 3.3</p>
<p class="fm-sidebar-text">Modify the programs in the first project so that the generator generates data samples to form a sine shape between <span class="times">x = –5</span> and <span class="times">x = 5</span>. When you plot the data samples, set the value of <span class="times">y</span> between <span class="times">–1.2</span> and <span class="times">1.2</span>.</p>
</div>
<h2 class="fm-head" id="heading_id_14">3.5 Generating numbers with patterns</h2>
<p class="body">In this second project, you’ll build and train GANs to generate a sequence of 10 integers between 0 and 99, all of them multiples of 5. The main steps involved are similar to those to generate an exponential growth curve, with the exception that the training set is not data points with two values <span class="times">(x, y)</span>. Instead, the training dataset is a sequence of integers that are all multiples of 5 between 0 and 99.<a id="idIndexMarker072"/><a id="idIndexMarker073"/><a id="marker-59"/></p>
<p class="body">In this section, you’ll first learn to convert the training data into a format that neural networks understand: one-hot variables. Further, you’ll convert one-hot variables back to an integer between 0 and 99, so it’s easy for human beings to understand. Hence you are essentially translating data between human-readable and model-ready formats. After that, you’ll create a discriminator and a generator and train the GANs. You’ll also use early stopping to determine when the training is finished. You then discard the discriminator and use the trained generator to create a sequence of integers with the pattern you want.</p>
<h3 class="fm-head1" id="heading_id_15">3.5.1 What are one-hot variables?</h3>
<p class="body">One-hot encoding is a technique used in ML and data preprocessing to represent categorical data as binary vectors. Categorical data consists of categories or labels, such as colors, types of animals, or cities, which are not inherently numeric. ML algorithms typically work with numerical data, so converting categorical data into a numerical format is necessary.<a id="idIndexMarker074"/><a id="idIndexMarker075"/><a id="idIndexMarker076"/></p>
<p class="body">Imagine you are working with a categorical feature—for example, the color of a house that can take values “red,” “green,” and “blue.” With one-hot encoding, each category is represented as a binary vector. You’ll create three binary columns, one for each category. The color “red” is one-hot encoded as <span class="times">[1, 0, 0]</span>, “green” as <span class="times">[0, 1, 0]</span>, and “blue” as <span class="times">[0, 0, 1]</span>. Doing so preserves the categorical information without introducing any ordinal relationship between the categories. Each category is treated as independent.</p>
<p class="body">Here we define a <code class="fm-code-in-text">onehot_encoder()</code> function to convert an integer to a one-hot variable:<a id="idIndexMarker077"/></p>
<pre class="programlisting">import torch
def onehot_encoder(position,depth):
    onehot=torch.zeros((depth,))
    onehot[position]=1
    return onehot</pre>
<p class="body">The function takes two arguments: the first argument, <code class="fm-code-in-text">position</code>, is the index at which the value is turned on as 1, and the second argument, <code class="fm-code-in-text">depth</code>, is the length of the one-hot variable. For example, if we print out the value of <code class="fm-code-in-text">onehot_encoder(1,5)</code>, it will look like this:</p>
<pre class="programlisting">print(onehot_encoder(1,5))</pre>
<p class="body">The result is</p>
<pre class="programlisting">tensor([0., 1., 0., 0., 0.])</pre>
<p class="body">The result shows a five-value tensor with the second place (the index value of which is 1) turned on as 1 and the rest turned off as 0s.</p>
<p class="body">Now that you understand how one-hot encoding works, you can convert any integer between 0 and 99 to a one-hot variable:</p>
<pre class="programlisting">def int_to_onehot(number):
    onehot=onehot_encoder(number,100)
    return onehot</pre>
<p class="body">Let’s use the function to convert the number 75 to a 100-value tensor:</p>
<pre class="programlisting">onehot75=int_to_onehot(75)
print(onehot75)</pre>
<p class="body">The output is</p>
<pre class="programlisting">tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0.])</pre>
<p class="body">The result is a 100-value tensor with the 76<sup class="fm-superscript">th</sup> place (the index value of which is 75) turned on as 1 and all other positions turned off as 0s.</p>
<p class="body"><a id="marker-60"/>To function <code class="fm-code-in-text">int_to_onehot()</code> converts an integer into a one-hot variable. In a way, the function is translating human-readable language into model-ready language.<a id="idIndexMarker078"/></p>
<p class="body">Next, we want to translate model-ready language back to human-readable language. Suppose we have a one-hot variable: How can we convert it into an integer that humans understand? The following function <code class="fm-code-in-text">onehot_to_int()</code> accomplishes that goal:<a id="idIndexMarker079"/></p>
<pre class="programlisting">def onehot_to_int(onehot):
    num=torch.argmax(onehot)
    return num.item()</pre>
<p class="body">The function <code class="fm-code-in-text">onehot_to_int()</code> takes the argument <code class="fm-code-in-text">onehot</code> and converts it into an integer based on which position has the highest value.<a id="idIndexMarker080"/></p>
<p class="body">Let’s test the function to see what happens if we use the tensor <code class="fm-code-in-text">onehot75</code> we just created as the input:</p>
<pre class="programlisting">print(onehot_to_int(onehot75))</pre>
<p class="body">The output is</p>
<pre class="programlisting">75</pre>
<p class="body">The result shows that the function converts the one-hot variable to an integer 75, which is the right answer. So we know that the functions are defined properly.</p>
<p class="body">Next, we’ll build and train GANs to generate multiples of 5.<a id="idIndexMarker081"/><a id="idIndexMarker082"/><a id="idIndexMarker083"/></p>
<h3 class="fm-head1" id="heading_id_16">3.5.2 GANs to generate numbers with patterns</h3>
<p class="body">Our goal is to build and train a model so that the generator can generate a sequence of 10 integers, all multiples of 5. We first prepare the training data and then convert them to model-ready numbers in batches. Finally, we use the trained generator to generate the patterns we want.<a id="idIndexMarker084"/><a id="idIndexMarker085"/><a id="marker-61"/></p>
<p class="body">For simplicity, we’ll generate a sequence of 10 integers between 0 and 99. We’ll then convert the sequence into 10 model-ready numbers.</p>
<p class="body">The following function generates a sequence of 10 integers, all multiples of 5:</p>
<pre class="programlisting">def gen_sequence():
    indices = torch.randint(0, 20, (10,))
    values = indices*5
    return values   </pre>
<p class="body">We first use the <code class="fm-code-in-text">randint()</code> method in PyTorch to generate 10 numbers between 0 and 19. We then multiply them by 5 and convert them to PyTorch tensors. This creates 10 integers that are all multiples of 5.<a id="idIndexMarker086"/></p>
<p class="body">Let’s try to generate a sequence of training data:</p>
<pre class="programlisting">sequence=gen_sequence()
print(sequence)</pre>
<p class="body">The output is</p>
<pre class="programlisting">tensor([60, 95, 50, 55, 25, 40, 70,  5,  0, 55])</pre>
<p class="body">The values in the preceding output are all multiples of 5.</p>
<p class="body">Next, we convert each number to a one-hot variable so that we can feed them to the neural network later:</p>
<pre class="programlisting">import numpy as np
  
def gen_batch():
    sequence=gen_sequence()                            <span class="fm-combinumeral">①</span>
    batch=[int_to_onehot(i).numpy() for i in sequence] <span class="fm-combinumeral">②</span>
    batch=np.array(batch)
    return torch.tensor(batch)
batch=gen_batch()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a sequence of 10 numbers, all multiples of 5</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts each integer to a 100-value one-hot variable</p>
<p class="body">The preceding function <code class="fm-code-in-text">gen_batch()</code> creates a batch of data so that we can feed them to the neural network for training purposes.<a id="idIndexMarker087"/></p>
<p class="body">We also define a function <code class="fm-code-in-text">data_to_num()</code> to convert one-hot variables to a sequence of integers so that humans can understand the output:<a id="idIndexMarker088"/></p>
<pre class="programlisting">def data_to_num(data):
    num=torch.argmax(data,dim=-1)                      <span class="fm-combinumeral">①</span>
    return num
numbers=data_to_num(batch)                             <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Converts vectors to integers based on the largest values in a 100-value vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Applies the function on an example</p>
<p class="body">The <code class="fm-code-in-text">dim=-1</code> argument in the <code class="fm-code-in-text">torch.argmax()</code> function means we are trying to find the position (i.e., index) of the largest value in the last dimension: that is, among the 100-value one-hot vector, which position has the highest value.<a id="idIndexMarker089"/><a id="marker-62"/></p>
<p class="body">Next, we’ll create two neural networks: one for the discriminator D and one for the generator G. We’ll build GANs to generate the desired pattern of numbers. Similar to what we did earlier in this chapter, we create a discriminator network, which is a binary classifier that distinguishes fake samples from real samples. We also create a generator network to generate a sequence of 10 numbers. Here is the discriminator neural network:</p>
<pre class="programlisting">from torch import nn
D=nn.Sequential(
    nn.Linear(100,1),
    nn.Sigmoid()).to(device)</pre>
<p class="body">Since we’ll convert integers into 100-value one-hot variables, we use 100 as the input size in the first <code class="fm-code-in-text">Linear</code> layer in the model. The last <code class="fm-code-in-text">Linear</code> layer has just one output feature in it, and we use the sigmoid activation function to squeeze the output to the range [0, 1] so it can be interpreted as the probability, p, that the sample is real. With the complementary probability 1 – p, the sample is fake.<a id="idIndexMarker090"/></p>
<p class="body">The generator’s job is to create a sequence of numbers so that they can pass as real in front of the discriminator D. That is, G is trying to create a sequence of numbers to maximize the probability that D thinks that the numbers are from the training dataset.</p>
<p class="body">We create the following neural network to represent the generator G:</p>
<pre class="programlisting">G=nn.Sequential(
    nn.Linear(100,100),
    nn.ReLU()).to(device)</pre>
<p class="body">We’ll feed random noise vectors from a 100-dimensional latent space to the generator. The generator then creates a tensor of 100 values based on the input. Note here that we use the <code class="fm-code-in-text">ReLU</code> activation function at the last layer so that the output is nonnegative. Since we are trying to generate 100 values of 0 or 1, nonnegative values are appropriate here. <a id="idIndexMarker091"/></p>
<p class="body">As in the first project, we use the Adam optimizer for both the discriminator and the generator, with a learning rate of 0.0005:</p>
<pre class="programlisting">loss_fn=nn.BCELoss()
lr=0.0005
optimD=torch.optim.Adam(D.parameters(),lr=lr)
optimG=torch.optim.Adam(G.parameters(),lr=lr)</pre>
<p class="body">Now that we have the training data and two networks, we’ll train the model. After that, we’ll discard the discriminator and use the generator to generate a sequence of 10 integers.<a id="idIndexMarker092"/><a id="idIndexMarker093"/></p>
<h3 class="fm-head1" id="heading_id_17">3.5.3 Training the GANs to generate numbers with patterns</h3>
<p class="body"><a id="marker-63"/>The training process for this project is very similar to that in our first project in which you generated an exponential growth shape. <a id="idIndexMarker094"/><a id="idIndexMarker095"/></p>
<p class="body">We define a function <code class="fm-code-in-text">train_D_G()</code>, which is a combination of the three functions <code class="fm-code-in-text">train_D_on_real()</code>, <code class="fm-code-in-text">train_D_on_fake()</code>, and <code class="fm-code-in-text">train_G()</code> that we have defined for the first project. The function <code class="fm-code-in-text">train_D_G()</code> is in the Jupyter Notebook for this chapter in the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>. Take a look at the function <code class="fm-code-in-text">train_D_G()</code> so you can see what minor changes we have made compared to the three functions we defined for the first project. <a id="idIndexMarker096"/><a id="idIndexMarker097"/><a id="idIndexMarker098"/><a id="idIndexMarker099"/></p>
<p class="body">We use the same early stopping class that we defined for the first project so we know when to stop training. However, we have modified the <code class="fm-code-in-text">patience</code> argument to 800 when we instantiate the class, as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 3.11 Training GANs to generate multiples of 5</p>
<pre class="programlisting">stopper=EarlyStop(800)                                  <span class="fm-combinumeral">①</span>
  
mse=nn.MSELoss()
real_labels=torch.ones((10,1)).to(device)
fake_labels=torch.zeros((10,1)).to(device)
def distance(generated_data):                           <span class="fm-combinumeral">②</span>
    nums=data_to_num(generated_data)
    remainders=nums%5
    ten_zeros=torch.zeros((10,1)).to(device)
    mseloss=mse(remainders,ten_zeros)
    return mseloss
  
for i in range(10000):
    gloss=0
    dloss=0
    generated_data=train_D_G(D,G,loss_fn,optimD,optimG) <span class="fm-combinumeral">③</span>
    dis=distance(generated_data)
    if stopper.stop(dis)==True:
        break   
    if i % 50 == 0:
        print(data_to_num(generated_data))              <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates an instance of the early stopping class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines a distance() function to calculate the loss in the generated numbers</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Trains the GANs for one epoch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints out the generated sequence of integers after every 50 epochs</p>
<p class="body">We have also defined a <code class="fm-code-in-text">distance()</code> function to measure the difference between the training set and the generated data samples: it calculates the MSE of the remainder of each generated number when divided by 5. The measure is 0 when all generated numbers are multiples of 5. <a id="idIndexMarker100"/></p>
<p class="body">If you run the preceding code cell, you’ll see the following output:</p>
<pre class="programlisting">tensor([14, 34, 19, 89, 44,  5, 58,  6, 41, 87], device='cuda:0')
… 
tensor([ 0, 80, 65,  0,  0, 10, 80, 75, 75, 75], device='cuda:0')
tensor([25, 30,  0,  0, 65, 20, 80, 20, 80, 20], device='cuda:0')
tensor([65, 95, 10, 65, 75, 20, 20, 20, 65, 75], device='cuda:0')</pre>
<p class="body">In each iteration, we generate a batch of 10 numbers. We first train the discriminator D using real samples. After that, the generator creates a batch of fake samples, and we use them to train the discriminator D again. Finally, we let the generator create a batch of fake samples again, but we use them to train the generator G instead. We stop training if the generator network stops improving after 800 epochs since the last time the minimum loss was achieved. After every 50 epochs, we print out the sequence of 10 numbers created by the generator so you can tell if they are indeed all multiples of 5.</p>
<p class="body">The output during the training process is as shown previously. In the first few hundred epochs, the generator still produces numbers that are not multiples of 5. But after 900 epochs, all the numbers generated are multiples of 5. The training process takes just a minute or so with GPU training. It takes less than 10 minutes if you use CPU training. Alternatively, you can download the trained model from the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>.</p>
<h3 class="fm-head1" id="heading_id_18">3.5.4 Saving and using the trained model</h3>
<p class="body"><a id="marker-64"/>We’ll discard the discriminator and save the trained generator in the local folder:<a id="idIndexMarker101"/><a id="idIndexMarker102"/></p>
<pre class="programlisting">import os
os.makedirs("files", exist_ok=True)
scripted = torch.jit.script(G) 
scripted.save('files/num_gen.pt') </pre>
<p class="body">We have now saved the generator to the local folder. To use the generator, we simply load up the model and use it to generate a sequence of integers:</p>
<pre class="programlisting">new_G=torch.jit.load('files/num_gen.pt',
                     map_location=device)             <span class="fm-combinumeral">①</span>
new_G.eval()
noise=torch.randn((10,100)).to(device)                <span class="fm-combinumeral">②</span>
new_data=new_G(noise)                                 <span class="fm-combinumeral">③</span>
print(data_to_num(new_data))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads the saved generator</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Obtains random noise vectors</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Feeds the random noise vectors to the trained model to generate a sequence of integers</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting">tensor([40, 25, 65, 25, 20, 25, 95, 10, 10, 65], device='cuda:0')</pre>
<p class="body">The generated numbers are all multiples of 5.</p>
<p class="body">You can easily change the code to generate other patterns, such as odd numbers, even numbers, multiples of 3, and so on.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 3.4</p>
<p class="fm-sidebar-text">Modify the programs in the second project so that the generator generates a sequence of ten integers that are all multiples of 3.</p>
</div>
<p class="body">Now that you know how GANs work, you’ll be able to extend the idea behind GANs to other formats in later chapters, including high-resolution images and realistic-sounding music.<a id="idIndexMarker103"/><a id="idIndexMarker104"/><a id="marker-65"/></p>
<h2 class="fm-head" id="heading_id_19">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">GANs consist of two networks: a discriminator to distinguish fake samples from real samples and a generator to create samples that are indistinguishable from those in the training set.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The steps involved in GANs are preparing training data, creating a discriminator and a generator, training the model and deciding when to stop training, and finally, discarding the discriminator and using the trained generator to create new samples.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The content generated by GANs depends on the training data. When the training dataset contains data pairs <span class="times">(x, y)</span> that form an exponential growth curve, the generated samples are also data pairs that mimic such a shape. When the training dataset has sequences of numbers that are all multiples of 5, the generated samples are also sequences of numbers, with multiples of 5 in them.</p>
</li>
<li class="fm-list-bullet">
<p class="list">GANs are versatile and capable of generating many different formats of content.<a id="idIndexMarker105"/></p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-001"/><sup class="footnotenumber1"><a class="url1" href="#footnote-001-backlink">1</a></sup>  Goodfellow et al, 2014, “Generative Adversarial Nets.” <a class="url" href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a>.</p>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">2</a></sup>  Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, 2014, “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <i class="fm-italics">Journal of Machine Learning Research</i> 15 (56): 1929−1958.</p>
</div></body></html>