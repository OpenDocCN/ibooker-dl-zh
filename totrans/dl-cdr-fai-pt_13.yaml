- en: 'Chapter 10\. NLP Deep Dive: RNNs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。NLP深入探讨：RNNs
- en: 'In [Chapter 1](ch01.xhtml#chapter_intro), we saw that deep learning can be
    used to get great results with natural language datasets. Our example relied on
    using a pretrained language model and fine-tuning it to classify reviews. That
    example highlighted a difference between transfer learning in NLP and computer
    vision: in general, in NLP the pretrained model is trained on a different task.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.xhtml#chapter_intro)中，我们看到深度学习可以用于处理自然语言数据集并取得出色的结果。我们的示例依赖于使用预训练的语言模型，并对其进行微调以对评论进行分类。该示例突出了NLP和计算机视觉中迁移学习的区别：通常情况下，在NLP中，预训练模型是在不同任务上训练的。
- en: 'What we call a *language model* is a model that has been trained to guess the
    next word in a text (having read the ones before). This kind of task is called
    *self-supervised learning*: we do not need to give labels to our model, just feed
    it lots and lots of texts. It has a process to automatically get labels from the
    data, and this task isn’t trivial: to properly guess the next word in a sentence,
    the model will have to develop an understanding of the English (or other) language.
    Self-supervised learning can also be used in other domains; for instance, see
    [“Self-Supervised Learning and Computer Vision”](https://oreil.ly/ECjfJ) for an
    introduction to vision applications. Self-supervised learning is not usually used
    for the model that is trained directly, but instead is used for pretraining a
    model used for transfer learning.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所谓的*语言模型*是一个经过训练以猜测文本中下一个单词的模型（在读取之前的单词后）。这种任务称为*自监督学习*：我们不需要为我们的模型提供标签，只需向其提供大量文本。它有一个过程可以从数据中自动获取标签，这个任务并不是微不足道的：为了正确猜测句子中的下一个单词，模型将必须发展对英语（或其他语言）的理解。自监督学习也可以用于其他领域；例如，参见[“自监督学习和计算机视觉”](https://oreil.ly/ECjfJ)以了解视觉应用。自监督学习通常不用于直接训练的模型，而是用于预训练用于迁移学习的模型。
- en: 'Jargon: Self-Supervised Learning'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：自监督学习
- en: Training a model using labels that are embedded in the independent variable,
    rather than requiring external labels. For instance, training a model to predict
    the next word in a text.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入在自变量中的标签来训练模型，而不是需要外部标签。例如，训练一个模型来预测文本中的下一个单词。
- en: The language model we used in [Chapter 1](ch01.xhtml#chapter_intro) to classify
    IMDb reviews was pretrained on Wikipedia. We got great results by directly fine-tuning
    this language model to a movie review classifier, but with one extra step, we
    can do even better. The Wikipedia English is slightly different from the IMDb
    English, so instead of jumping directly to the classifier, we could fine-tune
    our pretrained language model to the IMDb corpus and then use *that* as the base
    for our classifier.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](ch01.xhtml#chapter_intro)中用于分类IMDb评论的语言模型是在维基百科上预训练的。通过直接微调这个语言模型到电影评论分类器，我们取得了出色的结果，但通过一个额外的步骤，我们甚至可以做得更好。维基百科的英语与IMDb的英语略有不同，因此，我们可以将我们的预训练语言模型微调到IMDb语料库，然后将*那个*作为我们分类器的基础。
- en: Even if our language model knows the basics of the language we are using in
    the task (e.g., our pretrained model is in English), it helps to get used to the
    style of the corpus we are targeting. It may be more informal language, or more
    technical, with new words to learn or different ways of composing sentences. In
    the case of the IMDb dataset, there will be lots of names of movie directors and
    actors, and often a less formal style of language than that seen in Wikipedia.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们的语言模型了解我们在任务中使用的语言的基础知识（例如，我们的预训练模型是英语），熟悉我们的目标语料库的风格也是有帮助的。它可能是更非正式的语言，或者更技术性的，有新词要学习或者不同的句子构成方式。在IMDb数据集的情况下，将会有很多电影导演和演员的名字，通常比维基百科中看到的语言更不正式。
- en: We already saw that with fastai, we can download a pretrained English language
    model and use it to get state-of-the-art results for NLP classification. (We expect
    pretrained models in many more languages to be available soon; they might well
    be available by the time you are reading this book, in fact.) So, why are we learning
    how to train a language model in detail?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，使用fastai，我们可以下载一个预训练的英语语言模型，并用它来获得NLP分类的最新结果。（我们预计很快将提供更多语言的预训练模型；实际上，当您阅读本书时，它们可能已经可用。）那么，为什么我们要详细学习如何训练语言模型呢？
- en: One reason, of course, is that it is helpful to understand the foundations of
    the models that you are using. But there is another very practical reason, which
    is that you get even better results if you fine-tune the (sequence-based) language
    model prior to fine-tuning the classification model. For instance, for the IMDb
    sentiment analysis task, the dataset includes 50,000 additional movie reviews
    that do not have any positive or negative labels attached. Since there are 25,000
    labeled reviews in the training set and 25,000 in the validation set, that makes
    100,000 movie reviews altogether. We can use all of these reviews to fine-tune
    the pretrained language model, which was trained only on Wikipedia articles; this
    will result in a language model that is particularly good at predicting the next
    word of a movie review.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一个原因是了解您正在使用的模型的基础知识是有帮助的。但还有另一个非常实际的原因，那就是如果在微调分类模型之前微调（基于序列的）语言模型，您将获得更好的结果。例如，对于IMDb情感分析任务，数据集包括额外的50,000条电影评论，这些评论没有任何积极或消极的标签。由于训练集中有25,000条带标签的评论，验证集中有25,000条，总共有100,000条电影评论。我们可以使用所有这些评论来微调仅在维基百科文章上训练的预训练语言模型，这将导致一个特别擅长预测电影评论下一个单词的语言模型。
- en: This is known as the Universal Language Model Fine-tuning (ULMFiT) approach.
    The [paper introducing it](https://oreil.ly/rET-C) showed that this extra stage
    of fine-tuning the language model, prior to transfer learning to a classification
    task, resulted in significantly better predictions. Using this approach, we have
    three stages for transfer learning in NLP, as summarized in [Figure 10-1](#ulmfit_process).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为通用语言模型微调（ULMFiT）方法。[介绍它的论文](https://oreil.ly/rET-C)表明，在将语言模型微调到传递学习到分类任务之前，这个额外的微调阶段会导致预测显著更好。使用这种方法，我们在NLP中有三个传递学习阶段，如[图10-1](#ulmfit_process)所总结。
- en: '![Diagram of the ULMFiT process](Images/dlcf_1001.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![ULMFiT过程的图表](Images/dlcf_1001.png)'
- en: Figure 10-1\. The ULMFiT process
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。ULMFiT过程
- en: We’ll now explore how to apply a neural network to this language modeling problem,
    using the concepts introduced in the preceding two chapters. But before reading
    further, pause and think about how *you* would approach this.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨如何将神经网络应用于这个语言建模问题，使用前两章介绍的概念。但在继续阅读之前，请暂停一下，思考一下*您*将如何处理这个问题。
- en: Text Preprocessing
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本预处理
- en: It’s not at all obvious how we’re going to use what we’ve learned so far to
    build a language model. Sentences can be different lengths, and documents can
    be long. So how can we predict the next word of a sentence using a neural network?
    Let’s find out!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们学到的如何构建语言模型并不明显。句子的长度可能不同，文档可能很长。那么我们如何使用神经网络来预测句子的下一个单词呢？让我们找出答案！
- en: 'We’ve already seen how categorical variables can be used as independent variables
    for a neural network. Here’s the approach we took for a single categorical variable:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到分类变量可以作为神经网络的独立变量使用。以下是我们为单个分类变量采取的方法：
- en: Make a list of all possible levels of that categorical variable (we’ll call
    this list the *vocab*).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 制作该分类变量的所有可能级别的列表（我们将称此列表为*词汇*）。
- en: Replace each level with its index in the vocab.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用词汇表中的索引替换每个级别。
- en: Create an embedding matrix for this containing a row for each level (i.e., for
    each item of the vocab).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此创建一个包含每个级别的行的嵌入矩阵（即，词汇表中的每个项目）。
- en: Use this embedding matrix as the first layer of a neural network. (A dedicated
    embedding matrix can take as inputs the raw vocab indexes created in step 2; this
    is equivalent to, but faster and more efficient than, a matrix that takes as input
    one-hot-encoded vectors representing the indexes.)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此嵌入矩阵用作神经网络的第一层。（专用嵌入矩阵可以将步骤2中创建的原始词汇索引作为输入；这相当于但比使用表示索引的独热编码向量作为输入更快速和更有效。）
- en: We can do nearly the same thing with text! What is new is the idea of a sequence.
    First we concatenate all of the documents in our dataset into one big long string
    and split it into words (or *tokens*), giving us a very long list of words. Our
    independent variable will be the sequence of words starting with the first word
    in our very long list and ending with the second to last, and our dependent variable
    will be the sequence of words starting with the second word and ending with the
    last word.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎可以用文本做同样的事情！新的是序列的概念。首先，我们将数据集中的所有文档连接成一个大字符串，然后将其拆分为单词（或*标记*），从而给我们一个非常长的单词列表。我们的独立变量将是从我们非常长的列表中的第一个单词开始并以倒数第二个单词结束的单词序列，我们的因变量将是从第二个单词开始并以最后一个单词结束的单词序列。
- en: 'Our vocab will consist of a mix of common words that are already in the vocabulary
    of our pretrained model and new words specific to our corpus (cinematographic
    terms or actor’s names, for instance). Our embedding matrix will be built accordingly:
    for words that are in the vocabulary of our pretrained model, we will take the
    corresponding row in the embedding matrix of the pretrained model; but for new
    words, we won’t have anything, so we will just initialize the corresponding row
    with a random vector.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的词汇将由一些常见词汇和我们语料库中特定的新词汇（例如电影术语或演员的名字）混合组成。我们的嵌入矩阵将相应构建：对于预训练模型词汇中的词，我们将使用预训练模型的嵌入矩阵中的相应行；但对于新词，我们将没有任何内容，因此我们将只是用随机向量初始化相应的行。
- en: 'Each of the steps necessary to create a language model has jargon associated
    with it from the world of natural language processing, and fastai and PyTorch
    classes available to help. The steps are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 创建语言模型所需的每个步骤都与自然语言处理领域的术语相关联，并且有fastai和PyTorch类可用于帮助。步骤如下：
- en: Tokenization
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化
- en: Convert the text into a list of words (or characters, or substrings, depending
    on the granularity of your model).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转换为单词列表（或字符，或子字符串，取决于您模型的粒度）。
- en: Numericalization
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数值化
- en: List all of the unique words that appear (the vocab), and convert each word
    into a number by looking up its index in the vocab.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列出所有出现的唯一单词（词汇表），并通过查找其在词汇表中的索引将每个单词转换为一个数字。
- en: Language model data loader creation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型数据加载器创建
- en: fastai provides an `LMDataLoader` class that automatically handles creating
    a dependent variable that is offset from the independent variable by one token.
    It also handles some important details, such as how to shuffle the training data
    in such a way that the dependent and independent variables maintain their structure
    as required.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: fastai提供了一个`LMDataLoader`类，它会自动处理创建一个依赖变量，该变量与独立变量相差一个标记。它还处理一些重要的细节，例如如何以保持所需结构的方式对训练数据进行洗牌。
- en: Language model creation
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型创建
- en: 'We need a special kind of model that does something we haven’t seen before:
    handles input lists that could be arbitrarily big or small. There are a number
    of ways to do this; in this chapter, we will be using a *recurrent neural network*
    (RNN). We will get to the details of RNNs in [Chapter 12](ch12.xhtml#chapter_nlp_dive),
    but for now, you can think of it as just another deep neural network.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种特殊类型的模型，可以处理我们以前没有见过的输入列表，这些列表可能非常大或非常小。有许多方法可以做到这一点；在本章中，我们将使用*循环神经网络*（RNN）。我们将在[第12章](ch12.xhtml#chapter_nlp_dive)中详细介绍RNN的细节，但现在，您可以将其视为另一个深度神经网络。
- en: Let’s take a look at how each step works in detail.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看每个步骤是如何工作的。
- en: Tokenization
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: When we said “convert the text into a list of words,” we left out a lot of details.
    For instance, what do we do with punctuation? How do we deal with a word like
    “don’t”? Is it one word or two? What about long medical or chemical words? Should
    they be split into their separate pieces of meaning? How about hyphenated words?
    What about languages like German and Polish, which can create really long words
    from many, many pieces? What about languages like Japanese and Chinese that don’t
    use bases at all, and don’t really have a well-defined idea of *word*?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说“将文本转换为单词列表”时，我们忽略了很多细节。例如，我们如何处理标点符号？我们如何处理像“don’t”这样的单词？它是一个单词还是两个？长的医学或化学术语怎么办？它们应该被分割成各自的含义部分吗？连字符词怎么处理？像德语和波兰语这样的语言如何处理，它们可以从许多部分组成一个非常长的单词？像日语和中文这样的语言如何处理，它们根本不使用基础，也没有一个明确定义的*单词*的概念？
- en: 'Because there is no one correct answer to these questions, there is no one
    approach to tokenization. There are three main approaches:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些问题没有一个正确答案，所以也没有一个分词的方法。有三种主要方法：
- en: Word-based
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单词的
- en: Split a sentence on spaces, as well as applying language-specific rules to try
    to separate parts of meaning even when there are no spaces (such as turning “don’t”
    into “do n’t”). Generally, punctuation marks are also split into separate tokens.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个句子按空格分割，同时应用特定于语言的规则，尝试在没有空格的情况下分隔含义部分（例如将“don’t”转换为“do n’t”）。通常，标点符号也会被分割成单独的标记。
- en: Subword based
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于子词的
- en: Split words into smaller parts, based on the most commonly occurring substrings.
    For instance, “occasion” might be tokenized as “o c ca sion”.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最常出现的子字符串将单词分割成较小的部分。例如，“occasion”可能被分词为“o c ca sion”。
- en: Character-based
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于字符的
- en: Split a sentence into its individual characters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个句子分割成其各个字符。
- en: We’ll look at word and subword tokenization here, and we’ll leave character-based
    tokenization for you to implement in the questionnaire at the end of this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里看一下单词和子词的分词，将字符为基础的分词留给你在本章末尾的问卷中实现。
- en: 'Jargon: Token'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行话：Token
- en: One element of a list created by the tokenization process. It could be a word,
    part of a word (a *subword*), or a single character.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由分词过程创建的列表的一个元素。它可以是一个单词，一个单词的一部分（一个*子词*），或一个单个字符。
- en: Word Tokenization with fastai
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用fastai进行单词分词
- en: Rather than providing its own tokenizers, fastai provides a consistent interface
    to a range of tokenizers in external libraries. Tokenization is an active field
    of research, and new and improved tokenizers are coming out all the time, so the
    defaults that fastai uses change too. However, the API and options shouldn’t change
    too much, since fastai tries to maintain a consistent API even as the underlying
    technology changes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: fastai并没有提供自己的分词器，而是提供了一个一致的接口来使用外部库中的一系列分词器。分词是一个活跃的研究领域，新的和改进的分词器不断涌现，因此fastai使用的默认值也会发生变化。然而，API和选项不应该发生太大变化，因为fastai试图在底层技术发生变化时保持一致的API。
- en: 'Let’s try it out with the IMDb dataset that we used in [Chapter 1](ch01.xhtml#chapter_intro):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一下我们在[第1章](ch01.xhtml#chapter_intro)中使用的IMDb数据集：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We’ll need to grab the text files in order to try out a tokenizer. Just as
    `get_image_files` (which we’ve used many times already), gets all the image files
    in a path, `get_text_files` gets all the text files in a path. We can also optionally
    pass `folders` to restrict the search to a particular list of subfolders:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要获取文本文件以尝试一个分词器。就像`get_image_files`（我们已经使用了很多次）获取路径中的所有图像文件一样，`get_text_files`获取路径中的所有文本文件。我们还可以选择性地传递`folders`来限制搜索到特定的子文件夹列表：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here’s a review that we’ll tokenize (we’ll print just the start of it here
    to save space):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个我们将要分词的评论（我们这里只打印开头部分以节省空间）：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we write this book, the default English word tokenizer for fastai uses a
    library called *spaCy*. It has a sophisticated rules engine with special rules
    for URLs, individual special English words, and much more. Rather than directly
    using `SpacyTokenizer`, however, we’ll use `WordTokenizer`, since that will always
    point to fastai’s current default word tokenizer (which may not necessarily be
    spaCy, depending when you’re reading this).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，fastai的默认英语单词分词器使用了一个名为*spaCy*的库。它有一个复杂的规则引擎，具有针对URL、特殊英语单词等的特殊规则，以及更多。然而，我们不会直接使用`SpacyTokenizer`，而是使用`WordTokenizer`，因为它将始终指向fastai当前默认的单词分词器（取决于你阅读本书的时间，可能不一定是spaCy）。
- en: 'Let’s try it out. We’ll use fastai’s `coll_repr(*collection*,*n*)` function
    to display the results. This displays the first *`n`* items of *`collection`*,
    along with the full size—it’s what `L` uses by default. Note that fastai’s tokenizers
    take a collection of documents to tokenize, so we have to wrap `txt` in a list:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试。我们将使用fastai的`coll_repr(*collection*,*n*)`函数来显示结果。这会显示*`collection`*的前*`n`*个项目，以及完整的大小——这是`L`默认使用的。请注意，fastai的分词器接受一个要分词的文档集合，因此我们必须将`txt`包装在一个列表中：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you see, spaCy has mainly just separated out the words and punctuation.
    But it does something else here too: it has split “it’s” into “it” and “’s”. That
    makes intuitive sense; these are separate words, really. Tokenization is a surprisingly
    subtle task, when you think about all the little details that have to be handled.
    Fortunately, spaCy handles these pretty well for us—for instance, here we see
    that “.” is separated when it terminates a sentence, but not in an acronym or
    number:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，spaCy主要只是将单词和标点符号分开。但它在这里也做了其他事情：它将“it's”分割成“it”和“’s”。这是直观的；这些实际上是分开的单词。分词是一个令人惊讶的微妙任务，当你考虑到所有必须处理的细节时。幸运的是，spaCy为我们处理得相当好——例如，在这里我们看到“.”在终止句子时被分开，但在首字母缩写或数字中不会被分开：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'fastai then adds some additional functionality to the tokenization process
    with the `Tokenizer` class:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后fastai通过`Tokenizer`类为分词过程添加了一些额外功能：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Notice that there are now some tokens that start with the characters “xx”, which
    is not a common word prefix in English. These are *special tokens*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: For example, the first item in the list, `xxbos`, is a special token that indicates
    the start of a new text (“BOS” is a standard NLP acronym that means “beginning
    of stream”). By recognizing this start token, the model will be able to learn
    it needs to “forget” what was said previously and focus on upcoming words.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: These special tokens don’t come from spaCy directly. They are there because
    fastai adds them by default, by applying a number of rules when processing text.
    These rules are designed to make it easier for a model to recognize the important
    parts of a sentence. In a sense, we are translating the original English language
    sequence into a simplified tokenized language—a language that is designed to be
    easy for a model to learn.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the rules will replace a sequence of four exclamation points with
    a single exclamation point, followed by a special *repeated character* token and
    then the number four. In this way, the model’s embedding matrix can encode information
    about general concepts such as repeated punctuation rather than requiring a separate
    token for every number of repetitions of every punctuation mark. Similarly, a
    capitalized word will be replaced with a special capitalization token, followed
    by the lowercase version of the word. This way, the embedding matrix needs only
    the lowercase versions of the words, saving compute and memory resources, but
    can still learn the concept of capitalization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the main special tokens you’ll see:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '`xxbos`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Indicates the beginning of a text (here, a review)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '`xxmaj`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Indicates the next word begins with a capital (since we lowercased everything)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '`xxunk`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Indicates the next word is unknown
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the rules that were used, you can check the default rules:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As always, you can look at the source code for each of them in a notebook by
    typing the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is a brief summary of what each does:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '`fix_html`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Replaces special HTML characters with a readable version (IMDb reviews have
    quite a few of these)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '`replace_rep`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Replaces any character repeated three times or more with a special token for
    repetition (`xxrep`), the number of times it’s repeated, then the character
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '`replace_wrep`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Replaces any word repeated three times or more with a special token for word
    repetition (`xxwrep`), the number of times it’s repeated, then the word
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '`spec_add_spaces`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Adds spaces around / and #'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '`rm_useless_spaces`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Removes all repetitions of the space character
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '`replace_all_caps`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Lowercases a word written in all caps and adds a special token for all caps
    (`xxcap`) in front of it
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '`replace_maj`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Lowercases a capitalized word and adds a special token for capitalized (`xxmaj`)
    in front of it
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '`lowercase`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Lowercases all text and adds a special token at the beginning (`xxbos`) and/or
    the end (`xxeos`)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at a few of them in action:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now let’s take a look at how subword tokenization would work.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Subword Tokenization
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the *word tokenization* approach seen in the preceding section,
    another popular tokenization method is *subword tokenization*. Word tokenization
    relies on an assumption that spaces provide a useful separation of components
    of meaning in a sentence. However, this assumption is not always appropriate.
    For instance, consider this sentence: 我的名字是郝杰瑞 (“My name is Jeremy Howard” in
    Chinese). That’s not going to work very well with a word tokenizer, because there
    are no spaces in it! Languages like Chinese and Japanese don’t use spaces, and
    in fact they don’t even have a well-defined concept of a “word.” Other languages,
    like Turkish and Hungarian, can add many subwords together without spaces, creating
    very long words that include a lot of separate pieces of information.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle these cases, it’s generally best to use subword tokenization. This
    proceeds in two steps:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Analyze a corpus of documents to find the most commonly occurring groups of
    letters. These become the vocab.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析一组文档以找到最常出现的字母组。这些将成为词汇表。
- en: Tokenize the corpus using this vocab of *subword units*.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个*子词单元*的词汇对语料库进行标记化。
- en: 'Let’s look at an example. For our corpus, we’ll use the first 2,000 movie reviews:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。对于我们的语料库，我们将使用前2,000条电影评论：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We instantiate our tokenizer, passing in the size of the vocab we want to create,
    and then we need to “train” it. That is, we need to have it read our documents
    and find the common sequences of characters to create the vocab. This is done
    with `setup`. As we’ll see shortly, `setup` is a special fastai method that is
    called automatically in our usual data processing pipelines. Since we’re doing
    everything manually at the moment, however, we have to call it ourselves. Here’s
    a function that does these steps for a given vocab size and shows an example output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化我们的标记器，传入我们想要创建的词汇表的大小，然后我们需要“训练”它。也就是说，我们需要让它阅读我们的文档并找到常见的字符序列以创建词汇表。这是通过`setup`完成的。正如我们将很快看到的，`setup`是一个特殊的fastai方法，在我们通常的数据处理流程中会自动调用。然而，由于目前我们正在手动执行所有操作，因此我们必须自己调用它。这是一个为给定词汇表大小执行这些步骤并显示示例输出的函数：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s try it out:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: When using fastai’s subword tokenizer, the special character `▁` represents
    a space character in the original text.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用fastai的子词标记器时，特殊字符`▁`代表原始文本中的空格字符。
- en: 'If we use a smaller vocab, each token will represent fewer characters, and
    it will take more tokens to represent a sentence:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用较小的词汇表，每个标记将代表更少的字符，并且需要更多的标记来表示一个句子：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'On the other hand, if we use a larger vocab, most common English words will
    end up in the vocab themselves, and we will not need as many to represent a sentence:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们使用较大的词汇表，大多数常见的英语单词将最终出现在词汇表中，我们将不需要那么多来表示一个句子：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Picking a subword vocab size represents a compromise: a larger vocab means
    fewer tokens per sentence, which means faster training, less memory, and less
    state for the model to remember; but on the downside, it means larger embedding
    matrices, which require more data to learn.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 选择子词词汇表大小代表一种折衷：较大的词汇表意味着每个句子的标记较少，这意味着训练速度更快，内存更少，并且模型需要记住的状态更少；但是，缺点是，这意味着更大的嵌入矩阵，这需要更多的数据来学习。
- en: Overall, subword tokenization provides a way to easily scale between character
    tokenization (i.e., using a small subword vocab) and word tokenization (i.e., using
    a large subword vocab), and handles every human language without needing language-specific
    algorithms to be developed. It can even handle other “languages” such as genomic
    sequences or MIDI music notation! For this reason, in the last year its popularity
    has soared, and it seems likely to become the most common tokenization approach
    (it may well already be, by the time you read this!).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，子词标记化提供了一种在字符标记化（即使用较小的子词词汇表）和单词标记化（即使用较大的子词词汇表）之间轻松切换的方法，并且处理每种人类语言而无需开发特定于语言的算法。它甚至可以处理其他“语言”，如基因组序列或MIDI音乐符号！因此，过去一年中，它的流行度飙升，似乎很可能成为最常见的标记化方法（当您阅读本文时，它可能已经是了！）。
- en: Once our texts have been split into tokens, we need to convert them to numbers.
    We’ll look at that next.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的文本被分割成标记，我们需要将它们转换为数字。我们将在下一步中看到这一点。
- en: Numericalization with fastai
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用fastai进行数字化
- en: '*Numericalization* is the process of mapping tokens to integers. The steps
    are basically identical to those necessary to create a `Category` variable, such
    as the dependent variable of digits in MNIST:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*数字化*是将标记映射到整数的过程。这些步骤基本上与创建`Category`变量所需的步骤相同，例如MNIST中数字的因变量：'
- en: Make a list of all possible levels of that categorical variable (the vocab).
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 制作该分类变量的所有可能级别的列表（词汇表）。
- en: Replace each level with its index in the vocab.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用词汇表中的索引替换每个级别。
- en: 'Let’s take a look at this in action on the word-tokenized text we saw earlier:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在之前看到的单词标记化文本上的实际操作：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Just as with `SubwordTokenizer`, we need to call `setup` on `Numericalize`;
    this is how we create the vocab. That means we’ll need our tokenized corpus first.
    Since tokenization takes a while, it’s done in parallel by fastai; but for this
    manual walk-through, we’ll use a small subset:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`SubwordTokenizer`一样，我们需要在`Numericalize`上调用`setup`；这是我们创建词汇表的方法。这意味着我们首先需要我们的标记化语料库。由于标记化需要一段时间，fastai会并行进行；但是对于这个手动演示，我们将使用一个小的子集：
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can pass this to `setup` to create our vocab:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个传递给`setup`来创建我们的词汇表：
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Our special rules tokens appear first, and then every word appears once, in
    frequency order. The defaults to `Numericalize` are `min_freq=3` and `max_vocab=60000`.
    `max_vocab=60000` results in fastai replacing all words other than the most common
    60,000 with a special *unknown word* token, `xxunk`. This is useful to avoid having
    an overly large embedding matrix, since that can slow down training and use up
    too much memory, and can also mean that there isn’t enough data to train useful
    representations for rare words. However, this last issue is better handled by
    setting `min_freq`; the default `min_freq=3` means that any word appearing fewer
    than three times is replaced with `xxunk`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特殊规则标记首先出现，然后每个单词按频率顺序出现一次。`Numericalize`的默认值为`min_freq=3`和`max_vocab=60000`。`max_vocab=60000`导致fastai用特殊的*未知单词*标记`xxunk`替换除最常见的60,000个单词之外的所有单词。这有助于避免过大的嵌入矩阵，因为这可能会减慢训练速度并占用太多内存，并且还可能意味着没有足够的数据来训练稀有单词的有用表示。然而，通过设置`min_freq`来处理最后一个问题更好；默认值`min_freq=3`意味着出现少于三次的任何单词都将被替换为`xxunk`。
- en: fastai can also numericalize your dataset using a vocab that you provide, by
    passing a list of words as the `vocab` parameter.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: fastai还可以使用您提供的词汇表对数据集进行数字化，方法是将单词列表作为`vocab`参数传递。
- en: 'Once we’ve created our `Numericalize` object, we can use it as if it were a
    function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了我们的`Numericalize`对象，我们可以像使用函数一样使用它：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This time, our tokens have been converted to a tensor of integers that our
    model can receive. We can check that they map back to the original text:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们的标记已经转换为模型可以接收的整数张量。我们可以检查它们是否映射回原始文本：
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now that we have numbers, we need to put them in batches for our model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Putting Our Texts into Batches for a Language Model
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with images, we needed to resize them all to the same height and
    width before grouping them together in a mini-batch so they could stack together
    efficiently in a single tensor. Here it’s going to be a little different, because
    one cannot simply resize text to a desired length. Also, we want our language
    model to read text in order, so that it can efficiently predict what the next
    word is. This means each new batch should begin precisely where the previous one
    left off.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have the following text:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go back over the example of classifying movie reviews
    we studied in chapter 1 and dig deeper under the surface. First we will look at
    the processing steps necessary to convert text into numbers and how to customize
    it. By doing this, we’ll have another example of the PreProcessor used in the
    data block API.
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Then we will study how we build a language model and train it for a while.
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The tokenization process will add special tokens and deal with punctuation
    to return this text:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: xxbos xxmaj in this chapter , we will go back over the example of classifying
    movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj
    first we will look at the processing steps necessary to convert text into numbers
    and how to customize it . xxmaj by doing this , we ‘ll have another example of
    the preprocessor used in the data block xxup api . \n xxmaj then we will study
    how we build a language model and train it for a while .
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We now have 90 tokens, separated by spaces. Let’s say we want a batch size
    of 6\. We need to break this text into 6 contiguous parts of length 15:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '| xxbos | xxmaj | in | this | chapter | , | we | will | go | back | over |
    the | example | of | classifying |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| movie | reviews | we | studied | in | chapter | 1 | and | dig | deeper |
    under | the | surface | . | xxmaj |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| first | we | will | look | at | the | processing | steps | necessary | to
    | convert | text | into | numbers | and |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| how | to | customize | it | . | xxmaj | by | doing | this | , | we | ‘ll
    | have | another | example |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| of | the | preprocessor | used | in | the | data | block | xxup | api | .
    | \n | xxmaj | then | we |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| will | study | how | we | build | a | language | model | and | train | it
    | for | a | while | . |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: In a perfect world, we could then give this one batch to our model. But that
    approach doesn’t scale, because outside this toy example, it’s unlikely that a
    single batch containing all the tokens would fit in our GPU memory (here we have
    90 tokens, but all the IMDb reviews together give several million).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: So, we need to divide this array more finely into subarrays of a fixed sequence
    length. It is important to maintain order within and across these subarrays, because
    we will use a model that maintains a state so that it remembers what it read previously
    when predicting what comes next.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our previous example with 6 batches of length 15, if we chose
    a sequence length of 5, that would mean we first feed the following array:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '| xxbos | xxmaj | in | this | chapter |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| movie | reviews | we | studied | in |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| first | we | will | look | at |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| how | to | customize | it | . |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| of | the | preprocessor | used | in |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| will | study | how | we | build |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: 'Then, this one:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '| , | we | will | go | back |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| chapter | 1 | and | dig | deeper |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| the | processing | steps | necessary | to |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| xxmaj | by | doing | this | , |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| the | data | block | xxup | api |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| a | language | model | and | train |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: 'And finally:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '| over | the | example | of | classifying |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| under | the | surface | . | xxmaj |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| convert | text | into | numbers | and |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| we | ‘ll | have | another | example |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| . | \n | xxmaj | then | we |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| it | for | a | while | . |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: Going back to our movie reviews dataset, the first step is to transform the
    individual texts into a stream by concatenating them together. As with images,
    it’s best to randomize the order of the inputs, so at the beginning of each epoch
    we will shuffle the entries to make a new stream (we shuffle the order of the
    documents, not the order of the words inside them, or the texts would not make
    sense anymore!).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的电影评论数据集，第一步是通过将各个文本串联在一起将其转换为流。与图像一样，最好随机化输入的顺序，因此在每个时期的开始，我们将对条目进行洗牌以生成新的流（我们对文档的顺序进行洗牌，而不是其中的单词顺序，否则文本将不再有意义！）。
- en: We then cut this stream into a certain number of batches (which is our *batch
    size*). For instance, if the stream has 50,000 tokens and we set a batch size
    of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is
    that we preserve the order of the tokens (so from 1 to 5,000 for the first mini-stream,
    then from 5,001 to 10,000…), because we want the model to read continuous rows
    of text (as in the preceding example). An `xxbos` token is added at the start
    of each text during preprocessing, so that the model knows when it reads the stream
    when a new entry is beginning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将此流切成一定数量的批次（这是我们的*批量大小*）。例如，如果流有50,000个标记，我们设置批量大小为10，这将给我们5,000个标记的10个小流。重要的是我们保留标记的顺序（因此从1到5,000为第一个小流，然后从5,001到10,000…），因为我们希望模型读取连续的文本行（如前面的示例）。在预处理期间，在每个文本的开头添加一个`xxbos`标记，以便模型知道当读取流时新条目何时开始。
- en: So to recap, at every epoch we shuffle our collection of documents and concatenate
    them into a stream of tokens. We then cut that stream into a batch of fixed-size
    consecutive mini-streams. Our model will then read the mini-streams in order,
    and thanks to an inner state, it will produce the same activation, whatever sequence
    length we picked.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，每个时期我们都会对文档集合进行洗牌，并将它们连接成一个标记流。然后将该流切成一批固定大小的连续小流。我们的模型将按顺序读取小流，并由于内部状态，无论我们选择的序列长度如何，它都将产生相同的激活。
- en: This is all done behind the scenes by the fastai library when we create an `LMDataLoader`.
    We do this by first applying our `Numericalize` object to the tokenized texts
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建`LMDataLoader`时，所有这些都是由fastai库在幕后完成的。我们首先将我们的`Numericalize`对象应用于标记化的文本
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'and then passing that to `LMDataLoader`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其传递给`LMDataLoader`：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Let’s confirm that this gives the expected results, by grabbing the first batch
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过获取第一批来确认这是否给出了预期的结果
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'and then looking at the first row of the independent variable, which should
    be the start of the first text:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后查看独立变量的第一行，这应该是第一个文本的开头：
- en: '[PRE37]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The dependent variable is the same thing offset by one token:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖变量是相同的，只是偏移了一个标记：
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This concludes all the preprocessing steps we need to apply to our data. We
    are now ready to train our text classifier.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们需要对数据应用的所有预处理步骤。我们现在准备训练我们的文本分类器。
- en: Training a Text Classifier
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练文本分类器
- en: 'As we saw at the beginning of this chapter, there are two steps to training
    a state-of-the-art text classifier using transfer learning: first we need to fine-tune
    our language model pretrained on Wikipedia to the corpus of IMDb reviews, and
    then we can use that model to train a classifier.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开头看到的那样，使用迁移学习训练最先进的文本分类器有两个步骤：首先，我们需要微调在Wikipedia上预训练的语言模型以适应IMDb评论的语料库，然后我们可以使用该模型来训练分类器。
- en: As usual, let’s start with assembling our data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，让我们从组装数据开始。
- en: Language Model Using DataBlock
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DataBlock的语言模型
- en: fastai handles tokenization and numericalization automatically when `TextBlock`
    is passed to `DataBlock`. All of the arguments that can be passed to `Tokenizer`
    and `Numericalize` can also be passed to `TextBlock`. In the next chapter, we’ll
    discuss the easiest ways to run each of these steps separately, to ease debugging,
    but you can always just debug by running them manually on a subset of your data
    as shown in the previous sections. And don’t forget about `DataBlock`’s handy
    `summary` method, which is very useful for debugging data issues.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当`TextBlock`传递给`DataBlock`时，fastai会自动处理标记化和数值化。所有可以传递给`Tokenizer`和`Numericalize`的参数也可以传递给`TextBlock`。在下一章中，我们将讨论分别运行每个步骤的最简单方法，以便进行调试，但您也可以通过在数据的子集上手动运行它们来进行调试，如前几节所示。不要忘记`DataBlock`的方便的`summary`方法，用于调试数据问题非常有用。
- en: 'Here’s how we use `TextBlock` to create a language model, using fastai’s defaults:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何使用`TextBlock`使用fastai的默认值创建语言模型的方式：
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: One thing that’s different from previous types we’ve used in `DataBlock` is
    that we’re not just using the class directly (i.e., `TextBlock(...)`, but instead
    are calling a *class method*. A class method is a Python method that, as the name
    suggests, belongs to a *class* rather than an *object*. (Be sure to search online
    for more information about class methods if you’re not familiar with them, since
    they’re commonly used in many Python libraries and applications; we’ve used them
    a few times previously in the book, but haven’t called attention to them.) The
    reason that `TextBlock` is special is that setting up the numericalizer’s vocab
    can take a long time (we have to read and tokenize every document to get the vocab).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在`DataBlock`中使用的以前类型不同的一件事是，我们不仅仅直接使用类（即`TextBlock（...）`，而是调用*类方法*。类方法是Python方法，如其名称所示，属于*类*而不是*对象*。（如果您对类方法不熟悉，请务必在网上搜索更多信息，因为它们在许多Python库和应用程序中常用；我们在本书中以前使用过几次，但没有特别提到。）`TextBlock`之所以特殊是因为设置数值化器的词汇表可能需要很长时间（我们必须读取和标记化每个文档以获取词汇表）。
- en: 'To be as efficient as possible, fastai performs a few optimizations:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能高效，fastai执行了一些优化：
- en: It saves the tokenized documents in a temporary folder, so it doesn’t have to
    tokenize them more than once.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将标记化的文档保存在临时文件夹中，因此不必多次对其进行标记化。
- en: It runs multiple tokenization processes in parallel, to take advantage of your
    computer’s CPUs.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它并行运行多个标记化过程，以利用计算机的CPU。
- en: We need to tell `TextBlock` how to access the texts, so that it can do this
    initial preprocessing—that’s what `from_folder` does.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要告诉`TextBlock`如何访问文本，以便它可以进行这种初始预处理——这就是`from_folder`的作用。
- en: '`show_batch` then works in the usual way:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`show_batch`然后以通常的方式工作：'
- en: '[PRE42]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '|  | text | text_ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | text | text_ |'
- en: '| --- | --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | xxbos xxmaj it ’s awesome ! xxmaj in xxmaj story xxmaj mode , your going
    from punk to pro . xxmaj you have to complete goals that involve skating , driving
    , and walking . xxmaj you create your own skater and give it a name , and you
    can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric
    throughout the game until he betrays you and gets you kicked off of the skateboard
    | xxmaj it ’s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk
    to pro . xxmaj you have to complete goals that involve skating , driving , and
    walking . xxmaj you create your own skater and give it a name , and you can make
    it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout
    the game until he betrays you and gets you kicked off of the skateboard xxunk
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 0 | xxbos xxmaj it ’s awesome ! xxmaj in xxmaj story xxmaj mode , your going
    from punk to pro . xxmaj you have to complete goals that involve skating , driving
    , and walking . xxmaj you create your own skater and give it a name , and you
    can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric
    throughout the game until he betrays you and gets you kicked off of the skateboard
    | xxmaj it ’s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk
    to pro . xxmaj you have to complete goals that involve skating , driving , and
    walking . xxmaj you create your own skater and give it a name , and you can make
    it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout
    the game until he betrays you and gets you kicked off of the skateboard xxunk
    |'
- en: '| 1 | what xxmaj i ‘ve read , xxmaj death xxmaj bed is based on an actual dream
    , xxmaj george xxmaj barry , the director , successfully transferred dream to
    film , only a genius could accomplish such a task . \n\n xxmaj old mansions make
    for good quality horror , as do portraits , not sure what to make of the killer
    bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also
    , this | xxmaj i ‘ve read , xxmaj death xxmaj bed is based on an actual dream
    , xxmaj george xxmaj barry , the director , successfully transferred dream to
    film , only a genius could accomplish such a task . \n\n xxmaj old mansions make
    for good quality horror , as do portraits , not sure what to make of the killer
    bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also
    , this is |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 1 | what xxmaj i ‘ve read , xxmaj death xxmaj bed is based on an actual dream
    , xxmaj george xxmaj barry , the director , successfully transferred dream to
    film , only a genius could accomplish such a task . \n\n xxmaj old mansions make
    for good quality horror , as do portraits , not sure what to make of the killer
    bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also
    , this | xxmaj i ‘ve read , xxmaj death xxmaj bed is based on an actual dream
    , xxmaj george xxmaj barry , the director , successfully transferred dream to
    film , only a genius could accomplish such a task . \n\n xxmaj old mansions make
    for good quality horror , as do portraits , not sure what to make of the killer
    bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also
    , this is |'
- en: Now that our data is ready, we can fine-tune the pretrained language model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据准备好了，我们可以对预训练语言模型进行微调。
- en: Fine-Tuning the Language Model
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调语言模型
- en: 'To convert the integer word indices into activations that we can use for our
    neural network, we will use embeddings, just as we did for collaborative filtering
    and tabular modeling. Then we’ll feed those embeddings into a *recurrent neural
    network* (RNN), using an architecture called *AWD-LSTM* (we will show you how
    to write such a model from scratch in [Chapter 12](ch12.xhtml#chapter_nlp_dive)).
    As we discussed earlier, the embeddings in the pretrained model are merged with
    random embeddings added for words that weren’t in the pretraining vocabulary.
    This is handled automatically inside `language_model_learner`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 将整数单词索引转换为我们可以用于神经网络的激活时，我们将使用嵌入，就像我们在协同过滤和表格建模中所做的那样。然后，我们将把这些嵌入馈送到*递归神经网络*（RNN）中，使用一种称为*AWD-LSTM*的架构（我们将在[第12章](ch12.xhtml#chapter_nlp_dive)中向您展示如何从头开始编写这样一个模型）。正如我们之前讨论的，预训练模型中的嵌入与为不在预训练词汇表中的单词添加的随机嵌入合并。这在`language_model_learner`内部自动处理：
- en: '[PRE43]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The loss function used by default is cross-entropy loss, since we essentially
    have a classification problem (the different categories being the words in our
    vocab). The *perplexity* metric used here is often used in NLP for language models:
    it is the exponential of the loss (i.e., `torch.exp(cross_entropy)`). We also
    include the accuracy metric to see how many times our model is right when trying
    to predict the next word, since cross entropy (as we’ve seen) is both hard to
    interpret and tells us more about the model’s confidence than its accuracy.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 默认使用的损失函数是交叉熵损失，因为我们基本上有一个分类问题（不同类别是我们词汇表中的单词）。这里使用的*困惑度*指标通常用于NLP的语言模型：它是损失的指数（即`torch.exp(cross_entropy)`）。我们还包括准确性指标，以查看我们的模型在尝试预测下一个单词时有多少次是正确的，因为交叉熵（正如我们所见）很难解释，并且更多地告诉我们有关模型信心而不是准确性。
- en: Let’s go back to the process diagram from the beginning of this chapter. The
    first arrow has been completed for us and made available as a pretrained model
    in fastai, and we’ve just built the `DataLoaders` and `Learner` for the second
    stage. Now we’re ready to fine-tune our language model!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本章开头的流程图。第一个箭头已经为我们完成，并作为fastai中的预训练模型提供，我们刚刚构建了第二阶段的`DataLoaders`和`Learner`。现在我们准备好对我们的语言模型进行微调！
- en: '![Diagram of the ULMFiT process](Images/dlcf_1001.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![ULMFiT过程的图表](Images/dlcf_1001.png)'
- en: 'It takes quite a while to train each epoch, so we’ll be saving the intermediate
    model results during the training process. Since `fine_tune` doesn’t do that for
    us, we’ll use `fit_one_cycle`. Just like `cnn_learner`, `language_model_learner`
    automatically calls `freeze` when using a pretrained model (which is the default),
    so this will train only the embeddings (the only part of the model that contains
    randomly initialized weights—i.e., embeddings for words that are in our IMDb vocab,
    but aren’t in the pretrained model vocab):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时代的训练需要相当长的时间，因此我们将在训练过程中保存中间模型结果。由于`fine_tune`不会为我们执行此操作，因此我们将使用`fit_one_cycle`。就像`cnn_learner`一样，当使用预训练模型（这是默认设置）时，`language_model_learner`在使用时会自动调用`freeze`，因此这将仅训练嵌入（模型中唯一包含随机初始化权重的部分——即我们IMDb词汇表中存在但不在预训练模型词汇表中的单词的嵌入）：
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '| epoch | train_loss | valid_loss | accuracy | perplexity | time |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| 0 | 4.120048 | 3.912788 | 0.299565 | 50.038246 | 11:39 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: This model takes a while to train, so it’s a good opportunity to talk about
    saving intermediary results.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Loading Models
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can easily save the state of your model like so:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This will create a file in *learn.path/models/* named *1epoch.pth*. If you
    want to load your model in another machine after creating your `Learner` the same
    way, or resume training later, you can load the content of this file as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Once the initial training has completed, we can continue fine-tuning the model
    after unfreezing:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '| epoch | train_loss | valid_loss | accuracy | perplexity | time |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| 0 | 3.893486 | 3.772820 | 0.317104 | 43.502548 | 12:37 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| 1 | 3.820479 | 3.717197 | 0.323790 | 41.148880 | 12:30 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3.735622 | 3.659760 | 0.330321 | 38.851997 | 12:09 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| 3 | 3.677086 | 3.624794 | 0.333960 | 37.516987 | 12:12 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3.636646 | 3.601300 | 0.337017 | 36.645859 | 12:05 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| 5 | 3.553636 | 3.584241 | 0.339355 | 36.026001 | 12:04 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| 6 | 3.507634 | 3.571892 | 0.341353 | 35.583862 | 12:08 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| 7 | 3.444101 | 3.565988 | 0.342194 | 35.374371 | 12:08 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| 8 | 3.398597 | 3.566283 | 0.342647 | 35.384815 | 12:11 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| 9 | 3.375563 | 3.568166 | 0.342528 | 35.451500 | 12:05 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: 'Once this is done, we save all of our model except the final layer that converts
    activations to probabilities of picking each token in our vocabulary. The model
    not including the final layer is called the *encoder*. We can save it with `save_encoder`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Jargon: Encoder'
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model not including the task-specific final layer(s). This term means much
    the same thing as “body” when applied to vision CNNs, but “encoder” tends to be
    more used for NLP and generative models.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'This completes the second stage of the text classification process: fine-tuning
    the language model. We can now use it to fine-tune a classifier using the IMDb
    sentiment labels. Before we move on to fine-tuning the classifier, however, let’s
    quickly try something different: using our model to generate random reviews.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because our model is trained to guess the next word of the sentence, we can
    use it to write new reviews:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'As you can see, we add some randomness (we pick a random word based on the
    probabilities returned by the model) so we don’t get exactly the same review twice.
    Our model doesn’t have any programmed knowledge of the structure of a sentence
    or grammar rules, yet it has clearly learned a lot about English sentences: we
    can see it capitalizes properly (*I* is transformed to *i* because our rules require
    two characters or more to consider a word as capitalized, so it’s normal to see
    it lowercased) and is using consistent tense. The general review makes sense at
    first glance, and it’s only if you read carefully that you can notice something
    is a bit off. Not bad for a model trained in a couple of hours!'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: But our end goal wasn’t to train a model to generate reviews, but to classify
    them…so let’s use this model to do just that.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Classifier DataLoaders
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re now moving from language model fine-tuning to classifier fine-tuning.
    To re-cap, a language model predicts the next word of a document, so it doesn’t
    need any external labels. A classifier, however, predicts an external label—in
    the case of IMDb, it’s the sentiment of a document.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the structure of our `DataBlock` for NLP classification will
    look very familiar. It’s nearly the same as we’ve seen for the many image classification
    datasets we’ve worked with:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Just as with image classification, `show_batch` shows the dependent variable
    (sentiment, in this case) with each independent variable (movie review text):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '|  | text | category |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| 0 | xxbos i rate this movie with 3 skulls , only coz the girls knew how to
    scream , this could ‘ve been a better movie , if actors were better , the twins
    were xxup ok , i believed they were evil , but the eldest and youngest brother
    , they sucked really bad , it seemed like they were reading the scripts instead
    of acting them … . spoiler : if they ‘re vampire ’s why do they freeze the blood
    ? vampires ca n’t drink frozen blood , the sister in the movie says let ’s drink
    her while she is alive … .but then when they ‘re moving to another house , they
    take on a cooler they ‘re frozen blood . end of spoiler \n\n it was a huge waste
    of time , and that made me mad coz i read all the reviews of how | neg |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 0 | xxbos 我给这部电影打了 3 颗头骨的评分，只是因为女孩们知道如何尖叫，这部电影本可以更好，如果演员更好的话，双胞胎还行，我相信他们是邪恶的，但是最大和最小的兄弟，他们表现得真的很糟糕，看起来他们在读剧本而不是表演……。剧透：如果他们是吸血鬼，为什么他们会冻结血液？吸血鬼不能喝冻结的血液，电影中的姐姐说让我们在她活着的时候喝她……。但是当他们搬到另一栋房子时，他们带了一个冷藏盒装着他们的冻结血液。剧透结束\n\n这是浪费时间，这让我很生气，因为我读了所有关于它的评论
    | neg |'
- en: '| 1 | xxbos i have read all of the xxmaj love xxmaj come xxmaj softly books
    . xxmaj knowing full well that movies can not use all aspects of the book , but
    generally they at least have the main point of the book . i was highly disappointed
    in this movie . xxmaj the only thing that they have in this movie that is in the
    book is that xxmaj missy ’s father comes to xxunk in the book both parents come
    ) . xxmaj that is all . xxmaj the story line was so twisted and far fetch and
    yes , sad , from the book , that i just could n’t enjoy it . xxmaj even if i did
    n’t read the book it was too sad . i do know that xxmaj pioneer life was rough
    , but the whole movie was a downer . xxmaj the rating | neg |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 1 | xxbos 我已经阅读了所有的《爱来的方式》系列书籍。我充分了解电影无法使用书中的所有方面，但通常它们至少会有书中的主要内容。我对这部电影感到非常失望。这部电影中唯一与书中相同的是，书中有
    xxmaj missy 的父亲来到 xxunk （在书中父母都来了）。就是这样。故事情节扭曲且牵强，是的，悲伤，与书中完全不同，我无法享受。即使我没有读过这本书，它也太悲伤了。我知道拓荒生活很艰难，但整部电影都是一个沮丧的故事。评分
    | neg |'
- en: '| 2 | xxbos xxmaj this , for lack of a better term , movie is lousy . xxmaj
    where do i start … … \n\n xxmaj cinemaphotography - xxmaj this was , perhaps ,
    the worst xxmaj i ‘ve seen this year . xxmaj it looked like the camera was being
    tossed from camera man to camera man . xxmaj maybe they only had one camera .
    xxmaj it gives you the sensation of being a volleyball . \n\n xxmaj there are
    a bunch of scenes , haphazardly , thrown in with no continuity at all . xxmaj
    when they did the '' split screen '' , it was absurd . xxmaj everything was squished
    flat , it looked ridiculous . \n\n xxmaj the color tones were way off . xxmaj
    these people need to learn how to balance a camera . xxmaj this '' movie '' is
    poorly made , and | neg |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 2 | xxbos 这部电影，用一个更好的词来说，很糟糕。我从哪里开始呢……\n\n电影摄影 - 这或许是我今年看过的最糟糕的。看起来就像摄影师之间在互相抛接相机。也许他们只有一台相机。这让你感觉像是一个排球。\n\n有一堆场景，零零散散地扔进去，完全没有连贯性。当他们做
    ''分屏'' 时，那是荒谬的。一切都被压扁了，看起来荒谬。颜色调整完全错了。这些人需要学会如何平衡相机。这部 ''电影'' 制作很差，| neg |'
- en: 'Looking at the `DataBlock` definition, every piece is familiar from previous
    data blocks we’ve built, with two important exceptions:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `DataBlock` 的定义来看，每个部分都与我们构建的先前数据块相似，但有两个重要的例外：
- en: '`TextBlock.from_folder` no longer has the `is_lm=True` parameter.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TextBlock.from_folder` 不再具有 `is_lm=True` 参数。'
- en: We pass the `vocab` we created for the language model fine-tuning.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们传递了为语言模型微调创建的 `vocab`。
- en: The reason that we pass the `vocab` of the language model is to make sure we
    use the same correspondence of token to index. Otherwise, the embeddings we learned
    in our fine-tuned language model won’t make any sense to this model, and the fine-tuning
    step won’t be of any use.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递语言模型的 `vocab` 是为了确保我们使用相同的标记到索引的对应关系。否则，我们在微调语言模型中学到的嵌入对这个模型没有任何意义，微调步骤也没有任何用处。
- en: 'By passing `is_lm=False` (or not passing `is_lm` at all, since it defaults
    to `False`), we tell `TextBlock` that we have regular labeled data, rather than
    using the next tokens as labels. There is one challenge we have to deal with,
    however, which has to do with collating multiple documents into a mini-batch.
    Let’s see with an example, by trying to create a mini-batch containing the first
    10 documents. First we’ll numericalize them:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递 `is_lm=False`（或者根本不传递 `is_lm`，因为它默认为 `False`），我们告诉 `TextBlock` 我们有常规标记的数据，而不是将下一个标记作为标签。然而，我们必须处理一个挑战，这与将多个文档合并成一个小批次有关。让我们通过一个示例来看，尝试创建一个包含前
    10 个文档的小批次。首先我们将它们数值化：
- en: '[PRE54]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let’s now look at how many tokens each of these 10 movie reviews has:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这 10 条电影评论中每条有多少个标记：
- en: '[PRE55]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Remember, PyTorch `DataLoader`s need to collate all the items in a batch into
    a single tensor, and a single tensor has a fixed shape (i.e., it has a particular
    length on every axis, and all items must be consistent). This should sound familiar:
    we had the same issue with images. In that case, we used cropping, padding, and/or
    squishing to make all the inputs the same size. Cropping might not be a good idea
    for documents, because it seems likely we’d remove some key information (having
    said that, the same issue is true for images, and we use cropping there; data
    augmentation hasn’t been well explored for NLP yet, so perhaps there are actually
    opportunities to use cropping in NLP too!). You can’t really “squish” a document.
    So that leaves padding!'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，PyTorch 的 `DataLoader` 需要将批次中的所有项目整合到一个张量中，而一个张量具有固定的形状（即，每个轴上都有特定的长度，并且所有项目必须一致）。这应该听起来很熟悉：我们在图像中也遇到了同样的问题。在那种情况下，我们使用裁剪、填充和/或压缩来使所有输入大小相同。对于文档来说，裁剪可能不是一个好主意，因为我们可能会删除一些关键信息（话虽如此，对于图像也是同样的问题，我们在那里使用裁剪；数据增强在自然语言处理领域尚未得到很好的探索，因此也许在自然语言处理中也有使用裁剪的机会！）。你不能真正“压缩”一个文档。所以只剩下填充了！
- en: We will expand the shortest texts to make them all the same size. To do this,
    we use a special padding token that will be ignored by our model. Additionally,
    to avoid memory issues and improve performance, we will batch together texts that
    are roughly the same lengths (with some shuffling for the training set). We do
    this by (approximately, for the training set) sorting the documents by length
    prior to each epoch. The result is that the documents collated into a single batch
    will tend of be of similar lengths. We won’t pad every batch to the same size,
    but will instead use the size of the largest document in each batch as the target
    size.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将扩展最短的文本以使它们都具有相同的大小。为此，我们使用一个特殊的填充标记，该标记将被我们的模型忽略。此外，为了避免内存问题并提高性能，我们将大致相同长度的文本批量处理在一起（对于训练集进行一些洗牌）。我们通过在每个时期之前（对于训练集）按长度对文档进行排序来实现这一点。结果是，整理成单个批次的文档往往具有相似的长度。我们不会将每个批次填充到相同的大小，而是使用每个批次中最大文档的大小作为目标大小。
- en: Dynamically Resize Images
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态调整图像大小
- en: It is possible to do something similar with images, which is especially useful
    for irregularly sized rectangular images, but at the time of writing no library
    provides good support for this yet, and there aren’t any papers covering it. It’s
    something we’re planning to add to fastai soon, however, so keep an eye on the
    book’s website; we’ll add information about this as soon as we have it working
    well.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 可以对图像执行类似的操作，这对于不规则大小的矩形图像特别有用，但在撰写本文时，尚无库提供良好的支持，也没有任何涵盖此内容的论文。然而，我们计划很快将其添加到fastai中，因此请关注本书的网站；一旦我们成功运行，我们将添加有关此内容的信息。
- en: The sorting and padding are automatically done by the data block API for us
    when using a `TextBlock` with `is_lm=False`. (We don’t have this same issue for
    language model data, since we concatenate all the documents together first and
    then split them into equally sized sections.)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`TextBlock`和`is_lm=False`时，数据块API会自动为我们进行排序和填充。（对于语言模型数据，我们不会遇到这个问题，因为我们首先将所有文档连接在一起，然后将它们分成相同大小的部分。）
- en: 'We can now create a model to classify our texts:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建一个用于分类文本的模型：
- en: '[PRE57]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The final step prior to training the classifier is to load the encoder from
    our fine-tuned language model. We use `load_encoder` instead of `load` because
    we have only pretrained weights available for the encoder; `load` by default raises
    an exception if an incomplete model is loaded:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练分类器之前的最后一步是从我们微调的语言模型中加载编码器。我们使用`load_encoder`而不是`load`，因为我们只有编码器的预训练权重可用；`load`默认情况下会在加载不完整的模型时引发异常：
- en: '[PRE58]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Fine-Tuning the Classifier
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调分类器
- en: 'The last step is to train with discriminative learning rates and *gradual unfreezing*.
    In computer vision, we often unfreeze the model all at once, but for NLP classifiers,
    we find that unfreezing a few layers at a time makes a real difference:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用有区分性的学习率和*逐步解冻*进行训练。在计算机视觉中，我们经常一次性解冻整个模型，但对于NLP分类器，我们发现逐层解冻会产生真正的差异：
- en: '[PRE59]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.347427 | 0.184480 | 0.929320 | 00:33 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.347427 | 0.184480 | 0.929320 | 00:33 |'
- en: 'In just one epoch, we get the same result as our training in [Chapter 1](ch01.xhtml#chapter_intro)—not
    too bad! We can pass `-2` to `freeze_to` to freeze all except the last two parameter
    groups:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅一个时期，我们就获得了与[第1章](ch01.xhtml#chapter_intro)中的训练相同的结果——还不错！我们可以将`freeze_to`设置为`-2`，以冻结除最后两个参数组之外的所有参数组：
- en: '[PRE60]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.247763 | 0.171683 | 0.934640 | 00:37 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.247763 | 0.171683 | 0.934640 | 00:37 |'
- en: 'Then we can unfreeze a bit more and continue training:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以解冻更多层并继续训练：
- en: '[PRE61]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.193377 | 0.156696 | 0.941200 | 00:45 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.193377 | 0.156696 | 0.941200 | 00:45 |'
- en: And finally, the whole model!
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，整个模型！
- en: '[PRE62]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.172888 | 0.153770 | 0.943120 | 01:01 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.172888 | 0.153770 | 0.943120 | 01:01 |'
- en: '| 1 | 0.161492 | 0.155567 | 0.942640 | 00:57 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.161492 | 0.155567 | 0.942640 | 00:57 |'
- en: We reached 94.3% accuracy, which was state-of-the-art performance just three
    years ago. By training another model on all the texts read backward and averaging
    the predictions of those two models, we can even get to 95.1% accuracy, which
    was the state of the art introduced by the ULMFiT paper. It was beaten only a
    few months ago, by fine-tuning a much bigger model and using expensive data augmentation
    techniques (translating sentences in another language and back, using another
    model for translation).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们达到了94.3%的准确率，这在仅仅三年前是最先进的性能。通过在所有文本上训练另一个模型，并对这两个模型的预测进行平均，我们甚至可以达到95.1%的准确率，这是由ULMFiT论文引入的最先进技术。仅仅几个月前，通过微调一个更大的模型并使用昂贵的数据增强技术（将句子翻译成另一种语言，然后再翻译回来，使用另一个模型进行翻译）来打破了这一记录。
- en: Using a pretrained model let us build a fine-tuned language model that is pretty
    powerful, to either generate fake reviews or help classify them. This is exciting
    stuff, but it’s good to remember that this technology can also be used for malign
    purposes.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型让我们构建了一个非常强大的微调语言模型，可以用来生成假评论或帮助对其进行分类。这是令人兴奋的事情，但要记住这项技术也可以被用于恶意目的。
- en: Disinformation and Language Models
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚假信息和语言模型
- en: Even simple algorithms based on rules, before the days of widely available deep
    learning language models, could be used to create fraudulent accounts and try
    to influence policymakers. Jeff Kao, now a computational journalist at ProPublica,
    analyzed the comments that were sent to the US Federal Communications Commission
    (FCC) regarding a 2017 proposal to repeal net neutrality. In his article [“More
    than a Million Pro-Repeal Net Neutrality Comments Were Likely Faked”](https://oreil.ly/ptq8B),
    he reports how he discovered a large cluster of comments opposing net neutrality
    that seemed to have been generated by some sort of Mad Libs–style mail merge.
    In [Figure 10-2](#disinformation), the fake comments have been helpfully color-coded
    by Kao to highlight their formulaic nature.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/dlcf_1002.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Comments received by the FCC during the net neutrality debate
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kao estimated that “less than 800,000 of the 22M+ comments…could be considered
    truly unique” and that “more than 99% of the truly unique comments were in favor
    of keeping net neutrality.”
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Given advances in language modeling that have occurred since 2017, such fraudulent
    campaigns could be nearly impossible to catch now. You now have all the necessary
    tools at your disposal to create a compelling language model—something that can
    generate context-appropriate, believable text. It won’t necessarily be perfectly
    accurate or correct, but it will be plausible. Think about what this technology
    would mean when put together with the kinds of disinformation campaigns we have
    learned about in recent years. Take a look at the Reddit dialogue shown in [Figure 10-3](#ethics_reddit),
    where a language model based on OpenAI’s GPT-2 algorithm is having a conversation
    with itself about whether the US government should cut defense spending.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![An algorithm talking to itself on Reddit](Images/dlcf_1003.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. An algorithm talking to itself on Reddit
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this case, it was explained that an algorithm was being used to generate
    the dialogue. But imagine what would happen if a bad actor decided to release
    such an algorithm across social networks—they could do it slowly and carefully,
    allowing the algorithm to gradually develop followers and trust over time. It
    would not take many resources to have literally millions of accounts doing this.
    In such a situation, we could easily imagine getting to a point where the vast
    majority of discourse online was from bots, and nobody would have any idea that
    it was happening.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: We are already starting to see examples of machine learning being used to generate
    identities. For example, [Figure 10-4](#katie_jones) shows a LinkedIn profile
    for Katie Jones.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/dlcf_1004.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Katie Jones’s LinkedIn profile
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Katie Jones was connected on LinkedIn to several members of mainstream Washington
    think tanks. But she didn’t exist. That image you see was autogenerated by a generative
    adversarial network, and somebody named Katie Jones has not, in fact, graduated
    from the Center for Strategic and International Studies.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Many people assume or hope that algorithms will come to our defense here—that
    we will develop classification algorithms that can automatically recognize autogenerated
    content. The problem, however, is that this will always be an arms race, in which
    better classification (or discriminator) algorithms can be used to create better
    generation algorithms.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored the last application covered out of the box by
    the fastai library: text. We saw two types of models: language models that can
    generate texts, and a classifier that determines whether a review is positive
    or negative. To build a state-of-the art classifier, we used a pretrained language
    model, fine-tuned it to the corpus of our task, then used its body (the encoder)
    with a new head to do the classification.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Before we end this part of the book, we’ll take a look at how the fastai library
    can help you assemble your data for your specific problems.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本书的这一部分之前，我们将看看 fastai 库如何帮助您为您的特定问题组装数据。
- en: Questionnaire
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷
- en: What is self-supervised learning?
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是自监督学习？
- en: What is a language model?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是语言模型？
- en: Why is a language model considered self-supervised?
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么语言模型被认为是自监督的？
- en: What are self-supervised models usually used for?
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自监督模型通常用于什么？
- en: Why do we fine-tune language models?
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们要微调语言模型？
- en: What are the three steps to create a state-of-the-art text classifier?
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一流文本分类器的三个步骤是什么？
- en: How do the 50,000 unlabeled movie reviews help create a better text classifier
    for the IMDb dataset?
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 50,000 个未标记的电影评论如何帮助为 IMDb 数据集创建更好的文本分类器？
- en: What are the three steps to prepare your data for a language model?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为语言模型准备数据的三个步骤是什么？
- en: What is tokenization? Why do we need it?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是标记化？为什么我们需要它？
- en: Name three approaches to tokenization.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出三种标记化方法。
- en: What is `xxbos`?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 `xxbos`？
- en: List four rules that fastai applies to text during tokenization.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出 fastai 在标记化期间应用的四条规则。
- en: Why are repeated characters replaced with a token showing the number of repetitions
    and the character that’s repeated?
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么重复字符被替换为一个显示重复次数和被重复的字符的标记？
- en: What is numericalization?
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是数值化？
- en: Why might there be words that are replaced with the “unknown word” token?
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么会有单词被替换为“未知单词”标记？
- en: With a batch size of 64, the first row of the tensor representing the first
    batch contains the first 64 tokens for the dataset. What does the second row of
    that tensor contain? What does the first row of the second batch contain? (Careful—students
    often get this one wrong! Be sure to check your answer on the book’s website.)
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用批量大小为 64，表示第一批次的张量的第一行包含数据集的前 64 个标记。那个张量的第二行包含什么？第二批次的第一行包含什么？（小心 - 学生经常答错这个问题！一定要在书的网站上检查你的答案。）
- en: Why do we need padding for text classification? Why don’t we need it for language
    modeling?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么文本分类需要填充？为什么语言建模不需要填充？
- en: What does an embedding matrix for NLP contain? What is its shape?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NLP 的嵌入矩阵包含什么？它的形状是什么？
- en: What is perplexity?
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是困惑度？
- en: Why do we have to pass the vocabulary of the language model to the classifier
    data block?
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们必须将语言模型的词汇传递给分类器数据块？
- en: What is gradual unfreezing?
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什是逐步解冻？
- en: Why is text generation always likely to be ahead of automatic identification
    of machine-generated texts?
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么文本生成总是可能领先于自动识别机器生成的文本？
- en: Further Research
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: See what you can learn about language models and disinformation. What are the
    best language models today? Take a look at some of their outputs. Do you find
    them convincing? How could a bad actor best use such a model to create conflict
    and uncertainty?
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看你能学到关于语言模型和虚假信息的什么。今天最好的语言模型是什么？看看它们的一些输出。你觉得它们令人信服吗？坏人如何最好地利用这样的模型来制造冲突和不确定性？
- en: Given the limitation that models are unlikely to be able to consistently recognize
    machine-generated texts, what other approaches may be needed to handle large-scale
    disinformation campaigns that leverage deep learning?
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑到模型不太可能能够一致地识别机器生成的文本，可能需要哪些其他方法来处理利用深度学习的大规模虚假信息活动？
