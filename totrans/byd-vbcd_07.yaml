- en: 'Chapter 5\. Understanding Generated Code: Review, Refine, Own'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You’ve learned how to prompt an AI to generate code, and by this point you’ve
    likely produced some code using these techniques. Now comes a critical phase:
    making sure that code is correct, safe, and maintainable.'
  prefs: []
  type: TYPE_NORMAL
- en: As a developer, you can’t just take the AI’s output and blithely ship it. You
    need to review it, test it, possibly improve it, and integrate it with the rest
    of your codebase. This chapter focuses on how to understand what the AI gave you,
    iteratively edit and debug it, and fully take ownership of the code as part of
    your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the AI’s code in terms of your original intent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The “majority solution” phenomenon, or why AI-generated code often looks like
    a common solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques to review code for clarity and potential issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging AI-written code when it doesn’t work as expected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactoring the code for style or efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing tests to validate the code’s behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By mastering these skills, you’ll be able to integrate AI contributions into
    your projects with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'From Intent to Implementation: Understanding the AI’s Interpretation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you get the AI’s code, your first step should be to compare it to your
    intent (the prompt you gave). Does the code fulfill the requirements you set out?
    Sometimes the AI might slightly misinterpret or only partially implement what
    you asked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read through the code carefully. Step through it in your mind or on paper:'
  prefs: []
  type: TYPE_NORMAL
- en: Trace what it does for a typical input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your prompt had multiple parts (“do X and Y”), verify that the AI has done
    them all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the AI didn’t add functionality you didn’t ask for—sometimes it
    will add an extra feature it “thinks” is useful, like adding logging or a parameter,
    which could be OK or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just as you would with a colleague’s code, if something is unclear, note it.
    If you look for a good reason for it to be there, you might find one. If you don’t,
    query it or consider removing it.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you ask for a prime-number checker and the AI code also prints
    something like “Checking 7…” for each number, that may be an artifact of how you
    prompted it or a pattern from its training data (some tutorial code prints its
    progress). If you don’t want that, plan to remove it or prompt the AI to remove
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Also make sure the edge cases are handled as you expect. If you intended it
    to handle empty input, does it? If the input could be `None` or negative, did
    the AI consider that?
  prefs: []
  type: TYPE_NORMAL
- en: If something about your prompt was ambiguous and the AI had to choose an interpretation,
    identify where that happened. Perhaps you didn’t specify an output format, and
    it chose to print results instead of returning them. Now you have to decide if
    you want to accept that or modify the code.
  prefs: []
  type: TYPE_NORMAL
- en: This understanding phase is crucial; don’t skip it. Even if you’re going to
    test the code, understanding it by reading is important because tests might not
    cover everything (and reading is faster for some obvious things).
  prefs: []
  type: TYPE_NORMAL
- en: Last, consider the AI’s assumptions. AI often goes for the “majority” or most
    common interpretation (which leads us to the next section).
  prefs: []
  type: TYPE_NORMAL
- en: 'The “Majority” Problem: Most Common Doesn’t Mean Most Appropriate'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI models trained on lots of code will often produce the solution that’s most
    represented in that training data (or the simplest solution that fits). I call
    this the *majority solution* effect. It’s correct in general cases, but it might
    not be the best for your specific situation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you ask for a search algorithm without further context, the
    AI might output a basic linear search, because that’s straightforward and common.
    Maybe you actually needed a binary search, but the AI didn’t know that efficiency
    was critical, because you didn’t say so. Linear search works for many moderate
    cases but not if performance is key.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the AI might use a global variable because many simple examples do,
    but perhaps in your project, that’s not acceptable practice.
  prefs: []
  type: TYPE_NORMAL
- en: Be mindful that the AI’s solution might optimize for a generic scenario. As
    a human developer, you have insight into context that the AI lacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify assumptions in the code. If it assumes a list is sorted or an input
    is valid, was that assumption OK? Did you specify it? If not, maybe it should
    have included a check.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider alternatives: If you know multiple ways to solve the problem (like
    different algorithms), did the AI pick one? Is it the one you want? If not, you
    can prompt for the alternative or just change it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the AI code works for the “usual” case but not for edge conditions that matter
    to you, that’s something to fix. For instance, maybe it didn’t consider integer
    overflow in some math. In many training examples, that might not have been addressed,
    but in your context, it could be important.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding that the AI tends toward generic solutions will make you better
    at reviewing its code. It’s not magic or tailor-made; it’s a very educated guess
    at a solution. The tailoring is your job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code Readability and Structure: Patterns and Potential Issues'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AI-generated code often has some telltale patterns. It might:'
  prefs: []
  type: TYPE_NORMAL
- en: Include more comments than usual or oddly phrased comments (since it learned
    from tutorial code, which tends to be heavily commented)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use certain variable names consistently (like i, j, k for loops)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lay out code in a somewhat verbose style (to cover general cases)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check for these and consider whether they match your project’s style. The code
    might be functionally fine but need a readability pass. In that pass, you may
    want to:'
  prefs: []
  type: TYPE_NORMAL
- en: Rename variables to be more descriptive or consistent with your codebase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove or refine comments. If it added a comment like `# check if number is
    prime` above a self-explanatory `if` statement, you could remove that. But if
    it has a comment explaining a complex bit of logic, that’s good—keep or improve
    it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure consistent formatting by running the code through a linter or formatter
    (like Black for Python or `gofmt` for Go) to match the spacing and bracket styles
    you want.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also look for any unusual structure. Did the AI define multiple classes or functions
    when you expected one? Sometimes it might break a problem into multiple functions
    because that’s how a training example did it. If that’s overkill, you can inline
    them (or vice versa). Is the code too clever or too naive? AI sometimes produces
    a very straightforward solution or, occasionally, a fancy one-liner. Does that
    align with your team’s preferences? If not, adjust accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other potential issues to watch out for include:'
  prefs: []
  type: TYPE_NORMAL
- en: Off-by-one errors
  prefs: []
  type: TYPE_NORMAL
- en: Yes, AI can make those, too. For example, loop boundaries can be tricky. If
    you have time, mentally test a simple case through the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Unhandled exceptions
  prefs: []
  type: TYPE_NORMAL
- en: Does the code assume that a file opens successfully or that all input is in
    the correct format? Add error handling if it’s needed.
  prefs: []
  type: TYPE_NORMAL
- en: Performance pitfalls
  prefs: []
  type: TYPE_NORMAL
- en: Maybe the AI is using an inner loop on a large dataset for membership checks,
    even though a better approach exists, like using a set. The AI solution might
    be correct but not optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Library usage
  prefs: []
  type: TYPE_NORMAL
- en: If the code uses a library, ensure it’s one you want to use (and that it’s available).
    Sometimes it might use, say, `numpy` for a simple sum (because it saw that in
    examples in its training data). If dragging in that dependency isn’t worth it,
    you can switch to pure Python or the library you intended.
  prefs: []
  type: TYPE_NORMAL
- en: Inconsistencies
  prefs: []
  type: TYPE_NORMAL
- en: Occasionally, the AI code might have minor inconsistencies, like a function
    docstring saying one thing but the code doing another (if it revised the logic
    but not the comment, for instance). Fix those.
  prefs: []
  type: TYPE_NORMAL
- en: Minor syntax issues
  prefs: []
  type: TYPE_NORMAL
- en: This is rare with well-tested models but not impossible in languages where it
    might confuse something.
  prefs: []
  type: TYPE_NORMAL
- en: Using outdated APIs
  prefs: []
  type: TYPE_NORMAL
- en: The AI might use an old version of a library’s function that has changed, for
    instance. If you see a function call you don’t recognize, quickly check the library
    docs to ensure it’s correct for the version you use.
  prefs: []
  type: TYPE_NORMAL
- en: Placeholders
  prefs: []
  type: TYPE_NORMAL
- en: If the AI output uses placeholders like “Your code here” (rare, but it can happen
    in a generic template), fill those in.
  prefs: []
  type: TYPE_NORMAL
- en: In short, treat the AI code as if an intern wrote it and left for the day. You
    need to review it for quality and integrate it properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Debugging Strategies: Finding and Fixing Errors'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s say you run the code (or write tests for it, which we’ll cover soon) and
    something’s not working. Debugging AI-generated code is no different than debugging
    your own or someone else’s code—except you didn’t write it, so you might be less
    familiar. But because you’ve carefully read it already, you’re in good shape (see
    [Figure 5-1](#ch05_figure_1_1752630043586545)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bevc_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-1\. The AI code debugging cycle: execute AI-generated code, capture
    errors, provide error context back to AI for analysis, implement suggested fixes,
    and iterate until resolution.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s a six-step approach to debugging:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reproduce the issue.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the function or code with inputs that fail. Observe the output or error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Locate the source of the issue.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use typical debugging techniques like print statements, or use a debugger to
    step through. If it’s a logical error (wrong output), trace the logic manually
    or with prints to see where it diverges from your expectations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Check the prompt against the code.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes the bug is simply that the code didn’t fully implement the requirement,
    like if you asked for something to be sorted but it isn’t sorting properly. That
    might mean the AI’s logic is flawed or that an edge case (like an empty list)
    isn’t handled.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Leverage the AI to debug!*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can actually feed the problematic code back into the AI and say, “This code
    is giving the wrong result for X. Can you help find the bug?” Often, it will analyze
    it (like a code review) and point out issues. For example, maybe it sees that
    a loop should go to `len(arr)` but goes to `len(arr)-1`. It might catch that quicker.
    (Be mindful to not fully trust it either—but it’s like asking a colleague to help
    debug.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Fix the code.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now you have a choice: fix it manually or prompt the AI for a corrected version.
    If the fix is obvious, just do it. If it’s not, you can try something like “The
    above function fails on input X (expected Y, got Z). Please correct it.” The AI
    might then adjust the code accordingly.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Test again.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure the bug is resolved and that no new issues have been introduced.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: I recommend using test-driven debugging. If possible, write a few tests for
    critical functions (more on that in the testing section later in this chapter).
    Any failing tests will directly show what’s wrong. This can be faster than manual
    checking, for anything but the simplest functions.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when debugging, be sure you ask *why*, not just *what*. Try to understand
    why the AI made the mistake. Was the prompt unclear on that point? This can inform
    how you prompt next time or whether you need to always double-check that aspect
    in AI outputs. For example, if you notice the AI often doesn’t handle empty inputs
    unless told, you’ll start always specifying that in prompts and reviewing for
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refactoring for Maintainability: Making AI Code Your Code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the code is functionally correct, consider refactoring it to align with
    your project’s standards and to make it easier to work with in the future. The
    AI’s job was to get you code quickly; your job is to polish it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another six-step process, this time for refactoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Align with style guidelines.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code through your formatter or linter. Fix any warnings like “Variable
    name should be lowercase” or “Line too long.” This instantly makes the code look
    like the rest of your codebase. Many AI tools do a decent job at style, but slight
    adjustments might be needed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Improve naming and structure.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the AI named functions `_helper1` and `_helper2` in a class, and you prefer
    meaningful names, rename them. If it created a bunch of small functions that are
    only used once, maybe inline them, unless they add clarity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Remove any unnecessary parts.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, perhaps the AI included a main block or test code in the output
    that you didn’t ask for. If you don’t need that, remove it. Conversely, maybe
    it wrote everything in one function but you want to split it into smaller pieces
    for clarity; if so, do that split now.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Add documentation.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If this code is intended to be part of a library or a module that others will
    use, add docstrings or comments where appropriate. The AI might have commented
    some, but ensure it meets your standards. For example, maybe your project requires
    a certain docstring format with parameters and returns documented.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Optimize if needed.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that the code works, is it efficient enough? If this code might be called
    in a tight loop or on large data, check its complexity. The AI might not have
    used the most optimal approach (again, the “majority solution” might be a simple
    loop, not a more optimized approach). If performance is a concern, refactor to
    a better algorithm. You can again involve the AI:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Optimize this code to run faster by using a set instead of a list for lookups.
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: But you, as a developer, often know what pattern you want, so you might just
    implement that change.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Simplify if needed.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes AI code can be overly verbose. For instance, it might use an if-else
    with returns where a single return with a condition would suffice. While explicit
    code is not necessarily bad, you might want to simplify it to fewer lines to improve
    readability without losing clarity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The goal of refactoring is that if another developer pulls up this code later,
    it shouldn’t be obvious that “an AI wrote this.” It should just look like good
    code. That often means giving it the small human touches that make code clean.
  prefs: []
  type: TYPE_NORMAL
- en: When you refactor, you need to verify you didn’t break anything. So let’s segue
    into testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Importance of Testing: Unit, Integration, and End to End'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Testing is always important, but it’s especially important for AI-generated
    code for two reasons.  First, since you didn’t write it from scratch, you want
    assurance that it will work in all cases. Second, if you prompt the AI for changes
    later or integrate more AI code, tests help you ensure that any new changes don’t
    break the existing functionality. Let’s look quickly at different kinds of tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests
  prefs: []
  type: TYPE_NORMAL
- en: Write tests for each function or module you got from the AI, particularly covering
    edge cases. For our prime example, you might test with a prime number, a nonprime,
    1 (an edge case), 0 or negative (maybe defining the expected behavior), a large
    prime, and so on. If the code passes all those tests, it’s likely correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can even ask the AI to generate these tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Write PyTest unit tests for the above function, covering edge cases.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It often does a decent job. Still, review them to ensure they’re valid and cover
    what you think is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests
  prefs: []
  type: TYPE_NORMAL
- en: If the AI code interacts with other parts of the codebase, like a function that
    uses a database, write a test that calls it in context. Does it actually store
    to the database what it should? If it produces output consumed by another function,
    chain them in a test.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end tests
  prefs: []
  type: TYPE_NORMAL
- en: If this code is part of a larger workflow, run a scenario from start to finish.
    For example, if the AI code was part of a web route, do a test request to that
    route in a test environment and see if the format, error handling, and everything
    else holds up.
  prefs: []
  type: TYPE_NORMAL
- en: The level of testing you need to do depends on how critical and complex the
    code is. But even a quick manual test run or simple assert statements in a script
    are better than nothing for verification. Remember, testing doesn’t just find
    bugs; it locks down behavior. If you change something later (or an AI does), testing
    helps you ensure the code’s functionality doesn’t regress.
  prefs: []
  type: TYPE_NORMAL
- en: Testing is also a good way to assert ownership. Once you’ve tested for and fixed
    any issues, you can be confident in the code. At this point, it’s fair to say
    the code is “yours,” just like any other code in the codebase. You understand
    it, you trust it, and you have tests to guard it.
  prefs: []
  type: TYPE_NORMAL
- en: A Note on AI and Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some AI coding tools are starting to integrate testing suggestions. For example,
    CodeWhisperer will sometimes suggest an assert after a piece of code. Use those
    suggestions as a starting point, but don’t assume they’re 100% comprehensive.
    Think of creative edge cases⁠—that’s one place where human intuition is still
    very valuable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve gone through generating, understanding, debugging, and refactoring the
    code. This loop might happen in a short span (within minutes, for a small function)
    or take longer (for a complex module, over hours or days, with intermittent AI
    assistance).
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important to acknowledge that *you, the developer, are responsible for
    the final code.* AI is a tool to accelerate creation, but it won’t take the blame
    if something fails. There’s also a licensing or copyright risk: [some AI providers](https://oreil.ly/kYyO_)
    say that outputs over a certain length might be statistically likely to contain
    copied material. It’s rare, and the providers mitigated the problem a lot, but
    just as you scan Stack Overflow answers for any obviously licensed text or attributions,
    do a quick check—especially if the output is big or too clean. For instance, if
    you prompt “implement quicksort” and the AI gives you 20 lines of pristine code,
    that’s probably fine and common knowledge. But if you ask for something obscure
    and get a large chunk of code, try searching a unique string from it online to
    see if it was pulled verbatim from somewhere. This issue has become more apparent
    recently, with [documented cases](https://oreil.ly/h_BzA) of AI systems reproducing
    text from journal articles and other copyrighted sources. As part of responsible
    code ownership, developers should verify the provenance of any AI-generated content
    that appears to go beyond generic patterns or seems unusually specific to particular
    sources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, integrate the code into your project: add it to your version control
    system, perhaps mentioning in your commit message that AI helped. There’s no requirement
    to do this, but some teams like to track it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Over time, you’ll likely modify this AI-generated code as requirements change.
    Treat it like any other code: don’t think, “Oh, that’s the AI’s code; I’ll ask
    the AI to change it.” You can, if you want, but you can also freely modify it
    by hand. Do whatever is most efficient and maintainable.'
  prefs: []
  type: TYPE_NORMAL
- en: Through careful review and testing, AI-generated code becomes just more code
    in your project. At that point, whether an AI wrote line 10 or you did is irrelevant—what
    matters is that it meets the project’s needs and standards.
  prefs: []
  type: TYPE_NORMAL
- en: By following these practices, you harness the speed of AI coding while ensuring
    quality. You avoid the pitfalls of unquestioningly trusting AI output and instead
    integrate it into a professional development workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Next, [Chapter 6](ch06.html#ch06_ai_driven_prototyping_tools_and_techniques_1752630043797954)
    examines how AI tools can fundamentally transform the prototyping phase of software
    development. I will explore practical techniques for leveraging AI assistants
    to accelerate the journey from initial concept to working prototype, often reducing
    development time from days to hours. The discussion covers specific AI-powered
    prototyping tools, including Vercel v0 and screenshot-to-code utilities, along
    with strategies for iterative refinement under AI guidance.
  prefs: []
  type: TYPE_NORMAL
- en: I will also address the critical transition process from AI-generated prototypes
    to production-ready code, examining both the opportunities and potential challenges
    that arise when AI becomes a central part of the development workflow. Through
    real-world case studies, I will demonstrate how developers are successfully using
    AI to test ideas rapidly while maintaining code quality—and avoiding common pitfalls
    that can emerge when moving too quickly from concept to implementation.
  prefs: []
  type: TYPE_NORMAL
