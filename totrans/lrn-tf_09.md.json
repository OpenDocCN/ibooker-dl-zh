["```py\n'python distribute.py --job_name=\"ps\" --task_index=0'\n\n```", "```py\n\tjob_name=\"ps\"\n\ttask_index=0\n\n```", "```py\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"name of job\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task\")\n\n```", "```py\nparameter_servers = [\"localhost:2222\"]\nworkers = [\"localhost:2223\",\n\"localhost:2224\",\n\"localhost:2225\"]\ncluster = tf.train.ClusterSpec({\"parameter_server\": parameter_servers,\n\"worker\": workers})\n```", "```py\nserver = tf.train.Server(cluster,\njob_name=\"worker\",\ntask_index=0)\n\n```", "```py\nwith tf.device(tf.train.replica_device_setter(\nworker_device=\"/job:worker/task:%d\" % 0,\ncluster=cluster)):\n\n# Build model...\n```", "```py\nserver.join()\n\n```", "```py\nsv = tf.train.Supervisor(is_chief=...,\nlogdir=...,\nglobal_step=...,\ninit_op=...)\n\n```", "```py\nwith sv.managed_session(server.target) as sess:\n\n# Train ... \n```", "```py\ntf.Session(config=tf.ConfigProto(log_device_placement=True))\n\n```", "```py\nwith tf.device('/gpu:0'):\n  op = ...  \n\n```", "```py\nwith tf.device(\"/job:worker/task:2\"): \n  op = ...\n\n```", "```py\nconfig = tf.ConfigProto(device_count={\"CPU\": 8},\ninter_op_parallelism_threads=8,\nintra_op_parallelism_threads=1)\nsess = tf.Session(config=config)\n\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nBATCH_SIZE = 50\nTRAINING_STEPS = 5000\nPRINT_EVERY = 100\nLOG_DIR = \"/tmp/log\"\n\nparameter_servers = [\"localhost:2222\"]\nworkers = [\"localhost:2223\",\n\"localhost:2224\",\n\"localhost:2225\"]\n\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"'ps' / 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task\")\nFLAGS = tf.app.flags.FLAGS\n\nserver = tf.train.Server(cluster,\njob_name=FLAGS.job_name,\ntask_index=FLAGS.task_index)\n\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\ndef net(x):\nx_image = tf.reshape(x, [-1, 28, 28, 1])\nnet = slim.layers.conv2d(x_image, 32, [5, 5], scope='conv1')\nnet = slim.layers.max_pool2d(net, [2, 2], scope='pool1')\nnet = slim.layers.conv2d(net, 64, [5, 5], scope='conv2')\nnet = slim.layers.max_pool2d(net, [2, 2], scope='pool2')\nnet = slim.layers.flatten(net, scope='flatten')\nnet = slim.layers.fully_connected(net, 500, scope='fully_connected')\nnet = slim.layers.fully_connected(net, 10, activation_fn=None,\n                                  scope='pred')\nreturn net\n\nif FLAGS.job_name == \"ps\":\nserver.join()\n\nelif FLAGS.job_name == \"worker\":\n\nwith tf.device(tf.train.replica_device_setter(\nworker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\ncluster=cluster)):\n\nglobal_step = tf.get_variable('global_step', [],\ninitializer=tf.constant_initializer(0),\ntrainable=False)\n\nx = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\ny_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\ny = net(x)\n\ncross_entropy = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(y, y_))\ntrain_step = tf.train.AdamOptimizer(1e-4)\\\n        .minimize(cross_entropy, global_step=global_step)\n\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ninit_op = tf.global_variables_initializer()\n\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\nlogdir=LOG_DIR,\nglobal_step=global_step,\ninit_op=init_op)\n\nwith sv.managed_session(server.target) as sess:\nstep = 0\n\nwhile not sv.should_stop() and step <= TRAINING_STEPS:\n\nbatch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n\n_, acc, step = sess.run([train_step, accuracy, global_step],\nfeed_dict={x: batch_x, y_: batch_y})\n\nif step % PRINT_EVERY == 0:\nprint \"Worker : {}, Step: {}, Accuracy (batch): {}\".\\\nformat(FLAGS.task_index, step, acc)\n\ntest_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, \n                                         y_: mnist.test.labels})\nprint \"Test-Accuracy: {}\".format(test_acc)\n\nsv.stop()\n\n```", "```py\npython distribute.py --job_name=\"ps\" --task_index=0\npython distribute.py --job_name=\"worker\" --task_index=0\npython distribute.py --job_name=\"worker\" --task_index=1\npython distribute.py --job_name=\"worker\" --task_index=2\n\n```", "```py\nimport subprocess\nsubprocess.Popen('python distribute.py --job_name=\"ps\" --task_index=0', \n                 shell=True)\nsubprocess.Popen('python distribute.py --job_name=\"worker\" --task_index=0', \n                 shell=True)\nsubprocess.Popen('python distribute.py --job_name=\"worker\" --task_index=1', \n                 shell=True)\nsubprocess.Popen('python distribute.py --job_name=\"worker\" --task_index=2', \n                 shell=True)\n\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nBATCH_SIZE = 50\nTRAINING_STEPS = 5000\nPRINT_EVERY = 100\nLOG_DIR = \"/tmp/log\"\n```", "```py\nparameter_servers = [\"localhost:2222\"]\nworkers = [\"localhost:2223\",\n           \"localhost:2224\",\n           \"localhost:2225\"]\n\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n```", "```py\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"'ps' / 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task\")\nFLAGS = tf.app.flags.FLAGS\n\n```", "```py\nserver = tf.train.Server(cluster,\n                         job_name=FLAGS.job_name,\n                         task_index=FLAGS.task_index)\n```", "```py\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\ndef net(x):\n    x_image = tf.reshape(x, [-1, 28, 28, 1])\n    net = slim.layers.conv2d(x_image, 32, [5, 5], scope='conv1')\n    net = slim.layers.max_pool2d(net, [2, 2], scope='pool1')\n    net = slim.layers.conv2d(net, 64, [5, 5], scope='conv2')\n    net = slim.layers.max_pool2d(net, [2, 2], scope='pool2')\n    net = slim.layers.flatten(net, scope='flatten')\n    net = slim.layers.fully_connected(net, 500, scope='fully_connected')\n    net = slim.layers.fully_connected(net, 10, activation_fn=None, scope='pred')\n    return net\n```", "```py\nif FLAGS.job_name == \"ps\":\nserver.join()\n\n```", "```py\nwith tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n\n    global_step = tf.get_variable('global_step', [],\n                                  initializer=tf.constant_initializer(0),\n                                  trainable=False)\n\n    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\n    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n    y = net(x)\n\n    cross_entropy = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(y, y_))\n    train_step = tf.train.AdamOptimizer(1e-4)\\\n            .minimize(cross_entropy, global_step=global_step)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    init_op = tf.global_variables_initializer()\n```", "```py\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\nlogdir=LOG_DIR,\nglobal_step=global_step,\ninit_op=init_op)\n\nwith sv.managed_session(server.target) as sess:\n```", "```py\nwhile not sv.should_stop() and step <= TRAINING_STEPS:\n\n    batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n\n    _, acc, step = sess.run([train_step, accuracy, global_step],\n                            feed_dict={x: batch_x, y_: batch_y})\n\n    if step % PRINT_EVERY == 0:\n        print \"Worker : {}, Step: {}, Accuracy (batch): {}\".\\\n            format(FLAGS.task_index, step, acc)\n\n```", "```py\nWorker : 1, Step: 0.0, Accuracy (batch): 0.140000000596\nWorker : 0, Step: 100.0, Accuracy (batch): 0.860000014305\n\n```", "```py\ntest_acc = sess.run(accuracy,\n                    feed_dict={x: mnist.test.images, y_: mnist.test.labels})\nprint \"Test-Accuracy: {}\".format(test_acc)\n```"]