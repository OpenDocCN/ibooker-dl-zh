["```py\nfrom datasets import get_dataset_config_names\n\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\nprint(f\"XTREME has {len(xtreme_subsets)} configurations\")\n```", "```py\nXTREME has 183 configurations\n```", "```py\npanx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\npanx_subsets[:3]\n```", "```py\n['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']\n```", "```py\nfrom datasets import load_dataset\n\nload_dataset(\"xtreme\", name=\"PAN-X.de\")\n```", "```py\nfrom collections import defaultdict\nfrom datasets import DatasetDict\n\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\nfracs = [0.629, 0.229, 0.084, 0.059]\n# Return a DatasetDict if a key doesn't exist\npanx_ch = defaultdict(DatasetDict)\n\nfor lang, frac in zip(langs, fracs):\n    # Load monolingual corpus\n    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    # Shuffle and downsample each split according to spoken proportion\n    for split in ds:\n        panx_ch[lang][split] = (\n            ds[split]\n            .shuffle(seed=0)\n            .select(range(int(frac * ds[split].num_rows))))\n```", "```py\nimport pandas as pd\n\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n             index=[\"Number of training examples\"])\n```", "```py\nelement = panx_ch[\"de\"][\"train\"][0]\nfor key, value in element.items():\n    print(f\"{key}: {value}\")\n```", "```py\nlangs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\nner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\ntokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der',\n'polnischen', 'Woiwodschaft', 'Pommern', '.']\n```", "```py\nfor key, value in panx_ch[\"de\"][\"train\"].features.items():\n    print(f\"{key}: {value}\")\n```", "```py\ntokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\nner_tags: Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER',\n'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None),\nlength=-1, id=None)\nlangs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n```", "```py\ntags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\nprint(tags)\n```", "```py\nClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n'B-LOC', 'I-LOC'], names_file=None, id=None)\n```", "```py\ndef create_tag_names(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_de = panx_ch[\"de\"].map(create_tag_names)\n```", "```py\nde_example = panx_de[\"train\"][0]\npd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n['Tokens', 'Tags'])\n```", "```py\nfrom collections import Counter\n\nsplit2freqs = defaultdict(Counter)\nfor split, dataset in panx_de.items():\n    for row in dataset[\"ner_tags_str\"]:\n        for tag in row:\n            if tag.startswith(\"B\"):\n                tag_type = tag.split(\"-\")[1]\n                split2freqs[split][tag_type] += 1\npd.DataFrame.from_dict(split2freqs, orient=\"index\")\n```", "```py\nfrom transformers import AutoTokenizer\n\nbert_model_name = \"bert-base-cased\"\nxlmr_model_name = \"xlm-roberta-base\"\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n```", "```py\ntext = \"Jack Sparrow loves New York!\"\nbert_tokens = bert_tokenizer(text).tokens()\nxlmr_tokens = xlmr_tokenizer(text).tokens()\n```", "```py\n\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")\n```", "```py\n'<s> Jack Sparrow loves New York!</s>'\n```", "```py\nimport torch.nn as nn\nfrom transformers import XLMRobertaConfig\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n    config_class = XLMRobertaConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        # Load model body\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        # Set up token classification head\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # Load and initialize weights\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                labels=None, **kwargs):\n        # Use model body to get encoder representations\n        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n                               token_type_ids=token_type_ids, **kwargs)\n        # Apply classifier to encoder representation\n        sequence_output = self.dropout(outputs[0])\n        logits = self.classifier(sequence_output)\n        # Calculate losses\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        # Return model output object\n        return TokenClassifierOutput(loss=loss, logits=logits,\n                                     hidden_states=outputs.hidden_states,\n                                     attentions=outputs.attentions)\n```", "```py\nindex2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n```", "```py\nfrom transformers import AutoConfig\n\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n                                         num_labels=tags.num_classes,\n                                         id2label=index2tag, label2id=tag2index)\n```", "```py\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlmr_model = (XLMRobertaForTokenClassification\n              .from_pretrained(xlmr_model_name, config=xlmr_config)\n              .to(device))\n```", "```py\ninput_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\npd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])\n```", "```py\noutputs = xlmr_model(input_ids.to(device)).logits\npredictions = torch.argmax(outputs, dim=-1)\nprint(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\nprint(f\"Shape of outputs: {outputs.shape}\")\n```", "```py\nNumber of tokens in sequence: 10\nShape of outputs: torch.Size([1, 10, 7])\n```", "```py\npreds = [tags.names[p] for p in predictions[0].cpu().numpy()]\npd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])\n```", "```py\ndef tag_text(text, tags, model, tokenizer):\n    # Get tokens with special characters\n    tokens = tokenizer(text).tokens()\n    # Encode the sequence into IDs\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    # Get predictions as distribution over 7 possible classes\n    outputs = model(inputs)[0]\n    # Take argmax to get most likely class per token\n    predictions = torch.argmax(outputs, dim=2)\n    # Convert to DataFrame\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n```", "```py\nfunction(examples: Dict[str, List]) -> Dict[str, List]\n```", "```py\nwords, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n```", "```py\ntokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\npd.DataFrame([tokens], index=[\"Tokens\"])\n```", "```py\nword_ids = tokenized_input.word_ids()\npd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])\n```", "```py\nprevious_word_idx = None\nlabel_ids = []\n\nfor word_idx in word_ids:\n    if word_idx is None or word_idx == previous_word_idx:\n        label_ids.append(-100)\n    elif word_idx != previous_word_idx:\n        label_ids.append(labels[word_idx])\n    previous_word_idx = word_idx\n\nlabels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\nindex = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)\n```", "```py\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n                                      is_split_into_words=True)\n    labels = []\n    for idx, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n```", "```py\ndef encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels, batched=True,\n                      remove_columns=['langs', 'ner_tags', 'tokens'])\n```", "```py\npanx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\n```", "```py\nfrom seqeval.metrics import classification_report\n\ny_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\ny_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\nprint(classification_report(y_true, y_pred))\n```", "```py\n              precision    recall  f1-score   support\n\n        MISC       0.00      0.00      0.00         1\n         PER       1.00      1.00      1.00         1\n\n   micro avg       0.50      0.50      0.50         2\n   macro avg       0.50      0.50      0.50         2\nweighted avg       0.50      0.50      0.50         2\n```", "```py\nimport numpy as np\n\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n\n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs = -100\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n\n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n\n    return preds_list, labels_list\n```", "```py\nfrom transformers import TrainingArguments\n\nnum_epochs = 3\nbatch_size = 24\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\nmodel_name = f\"{xlmr_model_name}-finetuned-panx-de\"\ntraining_args = TrainingArguments(\n    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\",\n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n    logging_steps=logging_steps, push_to_hub=True)\n```", "```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```", "```py\nfrom seqeval.metrics import f1_score\n\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions,\n                                       eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred)}\n```", "```py\nfrom transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n```", "```py\ndef model_init():\n    return (XLMRobertaForTokenClassification\n            .from_pretrained(xlmr_model_name, config=xlmr_config)\n            .to(device))\n```", "```py\nfrom transformers import Trainer\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n                  data_collator=data_collator, compute_metrics=compute_metrics,\n                  train_dataset=panx_de_encoded[\"train\"],\n                  eval_dataset=panx_de_encoded[\"validation\"],\n                  tokenizer=xlmr_tokenizer)\n```", "```py\ntrainer.train() trainer.push_to_hub(commit_message=\"Training completed!\")\n```", "```py\ntext_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\ntag_text(text_de, tags, trainer.model, xlmr_tokenizer)\n```", "```py\nfrom torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # Convert dict of lists to list of dicts suitable for data collator\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    # Pad inputs and labels and put all tensors on device\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    with torch.no_grad():\n        # Pass data through model\n        output = trainer.model(input_ids, attention_mask)\n        # logit.size: [batch_size, sequence_length, classes]\n        # Predict class with largest logit value on classes axis\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n    # Calculate loss per token after flattening batch dimension with view\n    loss = cross_entropy(output.logits.view(-1, 7),\n                         labels.view(-1), reduction=\"none\")\n    # Unflatten batch dimension and convert to numpy array\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\n\n    return {\"loss\":loss, \"predicted_label\": predicted_label}\n```", "```py\nvalid_set = panx_de_encoded[\"validation\"]\nvalid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\ndf = valid_set.to_pandas()\n```", "```py\nindex2tag[-100] = \"IGN\"\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(\n    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf[\"labels\"] = df[\"labels\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf['loss'] = df.apply(\n    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\ndf['predicted_label'] = df.apply(\n    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\ndf.head(1)\n```", "```py\ndf_tokens = df.apply(pd.Series.explode)\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\ndf_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\ndf_tokens.head(7)\n```", "```py\n(\n    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n    .sort_values(by=\"sum\", ascending=False)\n    .reset_index()\n    .round(2)\n    .head(10)\n    .T\n)\n```", "```py\n(\n    df_tokens.groupby(\"labels\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)\n    .sort_values(by=\"mean\", ascending=False)\n    .reset_index()\n    .round(2)\n    .T\n)\n```", "```py\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n```", "```py\nplot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n                      tags.names)\n```", "```py\ndef get_samples(df):\n    for _, row in df.iterrows():\n        labels, preds, tokens, losses = [], [], [], []\n        for i, mask in enumerate(row[\"attention_mask\"]):\n            if i not in {0, len(row[\"attention_mask\"])}:\n                labels.append(row[\"labels\"][i])\n                preds.append(row[\"predicted_label\"][i])\n                tokens.append(row[\"input_tokens\"][i])\n                losses.append(f\"{row['loss'][i]:.2f}\")\n        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels,\n                               \"preds\": preds, \"losses\": losses}).T\n        yield df_tmp\n\ndf[\"total_loss\"] = df[\"loss\"].apply(sum)\ndf_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n\nfor sample in get_samples(df_tmp):\n    display(sample)\n```", "```py\ndf_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\nfor sample in get_samples(df_tmp):\n    display(sample)\n```", "```py\ndef get_f1_score(trainer, dataset):\n    return trainer.predict(dataset).metrics[\"test_f1\"]\n```", "```py\nf1_scores = defaultdict(dict)\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")\n```", "```py\nF1-score of [de] model on [de] dataset: 0.868\n```", "```py\ntext_fr = \"Jeff Dean est informaticien chez Google en Californie\"\ntag_text(text_fr, tags, trainer.model, xlmr_tokenizer)\n```", "```py\ndef evaluate_lang_performance(lang, trainer):\n    panx_ds = encode_panx_dataset(panx_ch[lang])\n    return get_f1_score(trainer, panx_ds[\"test\"])\n```", "```py\nf1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")\n```", "```py\nF1-score of [de] model on [fr] dataset: 0.714\n```", "```py\nf1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")\n```", "```py\nF1-score of [de] model on [it] dataset: 0.692\n```", "```py\nf1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")\n```", "```py\nF1-score of [de] model on [en] dataset: 0.589\n```", "```py\ndef train_on_subset(dataset, num_samples):\n    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n    valid_ds = dataset[\"validation\"]\n    test_ds = dataset[\"test\"]\n    training_args.logging_steps = len(train_ds) // batch_size\n\n    trainer = Trainer(model_init=model_init, args=training_args,\n        data_collator=data_collator, compute_metrics=compute_metrics,\n        train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n    trainer.train()\n    if training_args.push_to_hub:\n        trainer.push_to_hub(commit_message=\"Training completed!\")\n\n    f1_score = get_f1_score(trainer, test_ds)\n    return pd.DataFrame.from_dict(\n        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})\n```", "```py\npanx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])\n```", "```py\ntraining_args.push_to_hub = False\nmetrics_df = train_on_subset(panx_fr_encoded, 250)\nmetrics_df\n```", "```py\nfor num_samples in [500, 1000, 2000, 4000]:\n    metrics_df = metrics_df.append(\n        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)\n```", "```py\nfig, ax = plt.subplots()\nax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\nmetrics_df.set_index(\"num_samples\").plot(ax=ax)\nplt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\nplt.ylim((0, 1))\nplt.xlabel(\"Number of Training Samples\")\nplt.ylabel(\"F1 Score\")\nplt.show()\n```", "```py\nfrom datasets import concatenate_datasets\n\ndef concatenate_splits(corpora):\n    multi_corpus = DatasetDict()\n    for split in corpora[0].keys():\n        multi_corpus[split] = concatenate_datasets(\n            [corpus[split] for corpus in corpora]).shuffle(seed=42)\n    return multi_corpus\n```", "```py\npanx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])\n```", "```py\ntraining_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\ntraining_args.push_to_hub = True\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n    eval_dataset=panx_de_fr_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")\n```", "```py\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")\n```", "```py\nF1-score of [de-fr] model on [de] dataset: 0.866\nF1-score of [de-fr] model on [fr] dataset: 0.868\nF1-score of [de-fr] model on [it] dataset: 0.815\nF1-score of [de-fr] model on [en] dataset: 0.677\n```", "```py\ncorpora = [panx_de_encoded]\n\n# Exclude German from iteration\nfor lang in langs[1:]:\n    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n    # Fine-tune on monolingual corpus\n    ds_encoded = encode_panx_dataset(panx_ch[lang])\n    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n    # Collect F1-scores in common dict\n    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n    # Add monolingual corpus to list of corpora to concatenate\n    corpora.append(ds_encoded)\n```", "```py\ncorpora_encoded = concatenate_splits(corpora)\n```", "```py\ntraining_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n    eval_dataset=corpora_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")\n```", "```py\nfor idx, lang in enumerate(langs):\n    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])\n```", "```py\nscores_data = {\"de\": f1_scores[\"de\"],\n               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n               \"all\": f1_scores[\"all\"]}\nf1_scores_df = pd.DataFrame(scores_data).T.round(4)\nf1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\",\n                         inplace=True)\nf1_scores_df\n```"]