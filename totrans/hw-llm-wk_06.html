<html><head></head><body>
  <div class="readable-text" id="p1"> &#13;
   <h1 class=" readable-text-h1" id="chp__man_vs_machine"> <span class="chapter-title-numbering"><span class="num-string">7</span></span> <span class="title-text"> Misconceptions, limits, and eminent abilities of LLMs</span> </h1> &#13;
  </div> &#13;
  <div class="introduction-summary"> &#13;
   <h3 class="introduction-header">This chapter covers</h3> &#13;
   <ul> &#13;
    <li class="readable-text" id="p2">How LLMs and humans differ in learning</li> &#13;
    <li class="readable-text" id="p3">Making LLMs better at latency and scale-sensitive applications</li> &#13;
    <li class="readable-text" id="p4">Producing intermediate outputs for better final results</li> &#13;
    <li class="readable-text" id="p5">How computational complexity limits what an LLM can do</li> &#13;
   </ul> &#13;
  </div> &#13;
  <div class="readable-text" id="p6"> &#13;
   <p>Thanks to ChatGPT, the world has become more broadly aware of LLMs and their capabilities. Despite this awareness, many misconceptions and misunderstandings about LLMs still exist. Many people believe that LLMs are continually learning and self-improving, are more intelligent than people, and will soon be able to solve every problem on earth. While these statements are hyperbolic, some earnestly fear that LLMs will seriously disrupt the world.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p7"> &#13;
   <p>We are not here to say there are no legitimate concerns about LLMs, and we will discuss these in more depth in the book’s last two chapters. Still, many thoughts and worries about LLMs that you may encounter are blown out of proportion compared to how LLMs and technology broadly evolve.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p8"> &#13;
   <p>This chapter will discuss a few critical aspects of how LLMs work and how these aspects relate to these misconceptions. Ultimately, these operational aspects of LLMs affect how you may want to use or avoid an LLM in practice.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p9"> &#13;
   <p>First, we will discuss the differences between how humans and LLMs learn. Humans are fast learners, but LLMs are static by default. Although LLMs can be incredibly effective at processing data, people are better equipped to be maximally productive when learning new things.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p10"> &#13;
   <p>Next, we will tackle why the word <em>thinking</em> is misleading when considering how an LLM works. We will highlight that it is better to think of an LLM’s operation as <em>computing</em> because LLMs have no distinction between formulating and emitting output. In contrast, people often “think before they speak.”</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p11"> &#13;
   <p>Finally, we will discuss the scope of what LLMs can compute and how computer science concepts help us understand some of the intrinsic limitations behind an LLM’s current and future capabilities. These three topics are interrelated, so you will see how they connect as we discuss each in more detail.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p12"> &#13;
   <h2 class=" readable-text-h2" id="human-rate-of-learning-vs.-llms"><span class="num-string browsable-reference-id">7.1</span> Human rate of learning vs. LLMs</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p13"> &#13;
   <p>While we have discussed it implicitly, it is helpful to be explicit about how an LLM’s training differs from a person’s learning. The fluid and often lucid text produced by generative AI and the analogies we use to relate the capabilities of LLMs to human capabilities may make it seem as if there were some relationship between the two. Many people online are touting the idea that such a connection between what an LLM can do and what a human can do is real. In reality, the two are very different and have important considerations for when, how, and why you might prefer a person over an AI and how humans and AI can work together.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p14"> &#13;
   <p>From the material we have covered so far, we know that LLMs learn by predicting the next word using hundreds of millions of documents as examples. In chapter <a href="../Text/chapter-4.html">4</a>, we presented the algorithmic process of “learning” in LLMs: the gradient descent algorithm, which alters the parameters of an LLM’s neural network by attempting to predict the next token in a sample input. Then, in chapter <a href="../Text/chapter-5.html">5</a>, we showed how fine-tuning algorithms, like RLHF, alter the parameters of the LLM again. These two components of learning in an LLM have minimal resemblance to human learning and impose some crucial limitations on what we can expect the LLM to do. One of the most critical aspects is the rate and efficacy of this learning approach as it relates to the volume of data provided to the training process.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p15"> &#13;
   <p>To explore this further, consider how an LLM learns relative to how people learn. Have you ever met anyone who never spoke to anyone else, never had a parent talk to them, and yet somehow understood language? Likely not. Indeed, conversation is a key part of linguistic acquisition [1]. At least initially, you acquire knowledge and language from interaction and communication with others and the environment. Consequentially, you can learn effectively with much less information than an LLM has in the data that it trains on.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p16"> &#13;
   <p>In the best-case scenarios of childhood language acquisition, studies have observed that children are exposed to around 15,000 total spoken words a month [2]. If we were to be generous and round this figure up to 20,000 words and consider this over 100 years, a person would encounter as many as 24 million spoken words throughout their entire life. This is clearly a vast overestimate. Couple this with the fact that most people can speak their native language fluently, with an implicit understanding of vocabulary and linguistic structure, by at least age 18. Now compare this with LLMs. GPT-3, for example, was trained on hundreds of billions of words. Based on word counts alone, this is a very inefficient way to learn language!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p17"> &#13;
   <p>Language acquisition also helps us recognize the stark differences in how words are acquired. Babies and toddlers start with simple words, such as <em>mama</em> and <em>dada</em>, and eventually learn basic concepts like colors, <em>no</em>, <em>food</em>, etc. More complex words are added over time, building on the prior words. Yet an LLM begins with seeing all words simultaneously based on their frequency of use. Indeed, it is accurate to imagine an LLM tokenizing this very book as part of its first “learning,” acquiring knowledge of all of its eventual vocabulary simultaneously instead of starting with simple concepts and building knowledge on top of those foundations. While this process contributes to the rate at which an LLM learns, it may detract from the LLM’s capabilities of drawing high-level relationships between concepts.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p18"> &#13;
   <p>An LLM’s key advantage over humans is the <em>scale</em> at which it operates and its ability to perform multiple tasks simultaneously. This advantage is a common theme throughout machine learning and deep learning. You cannot easily hire an army of people to comb through books, expense reports, internal documents, or whatever medium of information to perform knowledge work like writing a review, finding potential fraud, or answering an arcane policy question. However, you can quickly get an army of computers to attempt to automate these tasks. While an individual LLM can analyze multiple parts of a sentence simultaneously, you can employ multiple computers running the same LLM to work in parallel. Training the LLM presents a similar opportunity: LLMs are trained on more words than you will ever read or hear in your lifetime, and you can train a large LLM by renting or buying thousands of computers to do the work concurrently.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p19"> &#13;
   <p>Considering these facts in conjunction with the material we’ve covered in previous chapters, we can list several high-level pros and cons of using LLMs for tasks compared to humans. A summary of these factors is shown in figure <a href="#fig__llm_pro_con">7.1</a>, which describes how the advantages and disadvantages of LLMs will lead to natural benefits and drawbacks of their use and, thus, provide insights about where LLMs should and should not be used.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p20">  &#13;
   <img alt="figure" src="../Images/CH07_F01_Boozallen.png"/> &#13;
   <h5 class=" figure-container-h5" id="fig__llm_pro_con"><span class="num-string">Figure <span class="browsable-reference-id">7.1</span></span> A summary of the strengths and weaknesses of LLMs relative to humans performing the same task. These lead to natural considerations that you must evaluate when using an LLM. From these, we can draw broad recommendations for successful LLM use.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p21"> &#13;
   <p>Some of the benefits of LLMs are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p22">Well-trained LLMs have a broad collection of background information, so they perform well on many tasks that are not that different from what has been seen before, and little work is needed to make the model effective. While this is not necessarily correct or detailed information, the breadth of the topic areas that an LLM can receive and generate reasonable responses about is far beyond the areas that most individual people can cover.</li> &#13;
   <li class="readable-text" id="p23">For many tasks, there is no need to get a precisely correct response. Broad requests for general information in a subject area intrinsically allow an LLM to be flexible and unconstrained in its response. This is especially true if you refine the LLM’s output through other processes. For example, a human might copyedit a piece of writing to improve it but use an LLM to produce the first draft or provide inspiration to break writer’s block and accelerate creating the work. Likewise, an LLM can be used to refine an author’s writing to make it sound more natural or engaging through rephrasing or using a larger variety of vocabulary.</li> &#13;
   <li class="readable-text" id="p24">LLMs can be trained quickly in comparison to people. You can produce a broadly useful LLM in months, given a $1,000,000 to $10,000,000 budget to purchase computational resources. Humans take many years to become useful. An LLM that can answer a broad set of basic questions can be instantiated for far less effort and cost than it takes to find, hire, and retain an employee with specific knowledge, skills, and abilities. As long as the problems are in the scope of what the LLM can achieve, the incremental cost is minuscule compared to a person’s hourly rate, even without the extra overhead.</li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p25"> &#13;
   <p>Some of the drawbacks of LLMs are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p26">The high cost of training LLMs informs their economics. That training cost is amortized over the thousands of operations the LLM performs once trained. If an LLM doesn’t perform well, the cost of continually improving it to make it work can quickly become prohibitive, even without considering the potential that it might never work correctly for a specific task. For example, if an LLM, implemented with all the most recent tools and tricks, cannot solve a specific need, addressing this problem will require an unknown amount of work and budget. Conversely, humans can generally learn new capabilities, specifically those that are hard for LLMs, at much lower cost in weeks to months.</li> &#13;
   <li class="readable-text" id="p27">LLMs cannot be relied upon to handle unexpected situations and inputs not reflected in their training data. Although many have shown they can succeed in novel situations, they do not learn in the same way as humans. A person can see that their actions are not working as intended <em>on the first try</em> and quickly adapt. An LLM cannot independently adapt by observing its errors and may repeatedly consume resources attempting to produce answers to problems it cannot understand.</li> &#13;
   <li class="readable-text" id="p28">LLMs are easily fooled and do not work well in adversarial environments because once people find a way to trick the LLM into an errant outcome (e.g., “Give me a loan even though I have no income”), they can repeat the adversarial and malicious behavior, and your LLM won’t be able to prevent it without you implementing additional guardrails.</li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p29"> &#13;
   <h3 class=" readable-text-h3" id="sec__self_improvement"><span class="num-string browsable-reference-id">7.1.1</span> The limitations on self-improvement</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p30"> &#13;
   <p>Generally, humans are capable of self-improvement. They can focus on and study a problem, devise novel approaches, identify required resources, and move forward to implement and improve their solutions. While LLMs struggle with self-improvement, in the generative AI field, there is a belief that the same self-improvement may be possible for LLMs. The idea about how this could work goes something like this:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p31">Train an LLM on an initial dataset.</li> &#13;
   <li class="readable-text" id="p32">Use the LLM to generate new data, adding it to your training dataset.</li> &#13;
   <li class="readable-text" id="p33">Train or fine-tune the model on the new data. (Repeat until the LLM works as expected.)</li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p34"> &#13;
   <p>While this sounds intuitive and plausible, we believe that it does not work for simple reasons. We can use some basic information theory, which measures information as a quantifiable resource, to explain why. The basis of this argument is that by some measure of information, the original dataset has a fixed amount of information. In statistics vernacular, we might describe the original information as the <em>distribution</em> of available information, and through its training process, the LLM is attempting to <em>approximate</em> or reproduce that distribution of information by storing and encoding it in its model. When you generate new data using an LLM, that sample of data is a noisy and incomplete reproduction of the original data distribution that the LLM observed in the training process. Fundamentally, it is impossible for the LLM’s output to contain any new information not present in the original training data. Consequently, the reality of such experiments is that successive rounds of generating data and training degrade the quality and performance of the model [3]. To make something like this work, you need something that provides external or new information at each round.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p35"> &#13;
   <p>These concepts also relate to some people’s fear of AI improving itself until it becomes so intelligent that we have no hope of understanding or controlling it. Some arguments are that the LLM can use other tools, somehow acquiring outside information or more training data, to improve itself. Ultimately, this requires a belief that while there are limitations as to how far you can improve most technologies, LLMs will be immune to these limits, such as the law of diminishing returns. Figure <a href="#fig__self-training">7.2</a> describes the inherent limits to LLM self-improvement.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p36">  &#13;
   <img alt="figure" src="../Images/CH07_F02_Boozallen.png"/> &#13;
   <h5 class=" figure-container-h5" id="fig__self-training"><span class="num-string">Figure <span class="browsable-reference-id">7.2</span></span> Concerns that LLMs will self-improve require the belief that LLMs won’t follow the normal sigmoid or S-curve of diminishing returns that describes the development of almost all other technologies. For infinite self-improvement to happen, we must believe that constraints such as power, data, or computational capacity are always solvable and that somehow, humans would not otherwise solve them for areas outside of LLMs. Constraints such as these are why we can describe most tech-nology development using S-curves, where progress slows as more constraints take effect. In other words, we’ll eventually reach a state where we can’t just build a bigger computer.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p37"> &#13;
   <p>A great example of limitations on technical improvement is Moore’s law, which roughly states that the number of transistors on a chip would double every 18 to 24 months. Moore’s law has mostly accurately predicted the growth of transistors on a chip, but there are signs of the S-curve of diminishing returns in transistors. The rate of the number of transistors on a chip doubling is decreasing. More importantly, the total system performance has already entered this S-curve. The number of transistors correlates with total compute performance but does not directly indicate compute performance. Looking at the whole picture in figure <a href="#fig__moores_law">7.3</a>, you will see that other constraints prevent boundless improvements across the entire system. Moore’s law aside, the practical cost of high-performance GPUs and the infrastructure that hosts them is another barrier to boundless improvement.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p38"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">LLMs are not humans—do not judge them by human standards!</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p39"> &#13;
    <p> Many catchy headlines have proclaimed LLM performance on the MCAT exam for medical school, the bar exam for lawyers to practice law, and IQ tests to measure their intelligence. While these are always interesting and full of caveats such as “How many examples of the same kinds of questions are in the LLM’s training data?” these are not good ways to extrapolate about LLMs and their abilities relative to humans. Indeed, pinning down an exact definition of <em>intelligence</em> is complex and one of the reasons why multiple types of IQ tests exist [4]. Ultimately, these tests have been helpful in predicting people’s outcomes in various tasks. Still, these tests are not designed to evaluate AI algorithms, and we have no reason to believe they do so accurately or reasonably! The problem is correlation, notcausation. IQ tests all <em>correlate</em> with desirable outcomes, but they do not measure an underlying property that controls or causes outcomes in the same way, for example, that a blood sugar test does. In a blood sugar test, if your blood sugar is too low or too high, we know what will happen because it measures an importantunderlying property that <em>causes</em> the outcome of some process that we understand quite well. IQ tests are useful, but their usefulness comes from years of iteration and improvement. We now better understand which answers on these tests correlate to people’s performance, but they don’t measure the underlying causes of this performance.</p> &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p40">  &#13;
   <img alt="figure" src="../Images/CH07_F03_Boozallen.png"/> &#13;
   <h5 class=" figure-container-h5" id="fig__moores_law"><span class="num-string">Figure <span class="browsable-reference-id">7.3</span></span> Moores’s law is a common example of boundless growth, but it is misleading. Transistors keep doubling, but frequency, power, single-threaded performance, and total computing do not. So the total system performance has not continued to double approximately every two years. Other similar factors will constrain LLM performance and affect capability over time. Used under CC4.0 license from <a href="https://github.com/karlrupp/microprocessor-trend-data">https://github.com/karlrupp/microprocessor-trend-data</a>.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p41"> &#13;
   <p>There are many examples of outside information being used to improve generative AI. Some algorithms created for robotic hands use external information from a physics simulator. Apple uses 3D modeling software to generate data that improves iris recognition on their phones [5]. In the examples in chapter <a href="../Text/chapter-6.html">6</a>, you saw a potential path for improving an LLM using a compiler for code or the Lean language to verify mathematics. These examples demonstrate fully automatable processes that generate new information that can lead to self-improvement.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p42"> &#13;
   <p>Yet, there has never been an example of boundless self-improvement; the gains observed from using these external tools eventually reach a plateau and ultimately rely on humans to develop the side information by writing better physics simulators for the robots, better compilers for code, and better domain-knowledge systems like Lean. Improving these tools compounds a major expense of training LLMs, thus imposing a second economic limitation on the self-improvement of LLMs beyond what is practical.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p43"> &#13;
   <h3 class=" readable-text-h3" id="few-shot-learning"><span class="num-string browsable-reference-id">7.1.2</span> Few-shot learning</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p44"> &#13;
   <p>Few-shot learning is also called <em>in-context learning</em>. This technique involves providing examples of the type of output you want an LLM to produce as a part of the prompt you send it. Say you want an LLM to respond to a help-desk question with accurate information. You may give the LLM a prompt with a user’s question to the help desk, followed by an example of the appropriate kind of response. If you give only one example, it’s called <em>one-shot learning</em>. Providing two examples instead of a single example is known as <em>two-shot learning</em>, and so on, hence describing this approach as few-shot because the precise number of examples is generally not as important as the fact that only a few examples are provided. This method of incorporating examples in a prompt is a specific kind of prompt engineering, as demonstrated in figure <a href="#fig__few-shot-learning">7.4</a>.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p45">  &#13;
   <img alt="figure" src="../Images/CH07_F04_Boozallen.png"/> &#13;
   <h5 class=" figure-container-h5" id="fig__few-shot-learning"><span class="num-string">Figure <span class="browsable-reference-id">7.4</span></span> Prompts with examples of how you want the LLM to produce output are called few-shot prompts because the LLM has not seen any examples of this specific behavior in its training data. In your prompt, you can include examples of input and output similar to RLHF/supervised fine-tuning (SFT). This prompting style encourages the model to produce the desired output by providing examples of what the desired output should look like. Because LLMs train on such a large amount of unlabeled data, k-shot examples are an effective way to get better results with minimal effort.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p46"> &#13;
   <p>Including examples in your prompts is useful for improving an LLM’s performance at new tasks. You don’t need to use RLHF or SFT to alter the model, and it works better than zero-shot prompting, where we ask the LLM to do the task without examples. But is it efficient learning?</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p47"> &#13;
   <p>Few-shot prompting is not training because we are not altering the model in any way, as we would in the training or fine-tuning process. The “state” or weights of the LLM remain the same. However accurately the LLM performs the task on Monday, it will be exactly as accurate on Tuesday and Wednesday, no matter how many thousands or millions of few-shot prompts it deals with. There is no improvement to the model’s abilities unless you manually do something to include better examples in the prompt, provide more examples, or otherwise intervene somehow. In this sense, no true learning is happening, and nothing about the model changes. We just get improved output from the model by changing our prompt.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p48"> &#13;
   <p>Yet, in an abstract sense, the LLM is learning because the prompt changes the model’s behavior by providing additional context to describe the problem. The behavior exhibited via prompting correlates with behavior achieved through fine-tuning on similar examples [6]. What that means, in short, is that few-shot learning does not fundamentally reflect anything different from what gradient descent can already do.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p49"> &#13;
   <p> <span class="print-book-callout-head">Note</span> If you do not have a lot of data, few-shot prompting is probably the most effective way for you as a practitioner or user to get an LLM to work well on your data. Because we can think of this prompting as inefficient gradient descent or fine-tuning, you should expect diminishing returns as you add examples in a few-shot style. For example, if you include many examples of how you’d like an LLM to respond in your prompt and still do not get the needed performance, you should look at SFT, RLHF, and the other fine-tuning approaches we discussed in chapter <a href="../Text/chapter-5.html">5</a>. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p50"> &#13;
   <h2 class=" readable-text-h2" id="efficiency-of-work-a-10-watt-human-brain-vs.-a-2000-watt-computer"><span class="num-string browsable-reference-id">7.2</span> Efficiency of work: A 10-watt human brain vs. a 2000-watt computer</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p51"> &#13;
   <p>The human brain takes the equivalent of 10 watts to maintain consciousness, allowing you to read this book. A high-end workstation with a GPU for AI/ML work could easily use 2,000 watts. A high-end server for running the larger LLMs available today gets into the 10,000 to 15,000 watt range. Off the bat, it would seem like using an LLM could thus be 1,500<span><img alt="equation image" src="../Images/eq-chapter-7-51-1.png"/></span> more power inefficient than having a human do some task. We should be very proud of this aspect of our evolutionary success and efficiency, but it is also only one aspect of what we might mean by efficiency. We show that many different kinds of efficiency might benefit a person versus machines in figure <a href="#fig__efficency-of-work">7.5</a>.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p52">  &#13;
   <img alt="figure" src="../Images/CH07_F05_Boozallen.png"/> &#13;
   <h5 class=" figure-container-h5" id="fig__efficency-of-work"><span class="num-string">Figure <span class="browsable-reference-id">7.5</span></span> The expensive hardware that makes LLMs work leads to several trade-offs. For example, the startup cost of using LLMs is often high, and they do not adapt independently. This lack of independent adaptation leads to many natural weaknesses where a human would outperform an LLM. Some weak-nesses, such as the fact that a model doesn’t change without training, can be considered strengths. You don’t get repeatable processes that are easy to scale if each new LLM running behaves differently and unpredictably.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p53"> &#13;
   <h3 class=" readable-text-h3" id="power"><span class="num-string browsable-reference-id">7.2.1</span> Power</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p54"> &#13;
   <p>Power is one of the driving factors in determining the financial cost of creating and running an LLM, but the true need is not yet entirely clear. Yes, many providers will quote you a price for running an LLM, but we do not know the true costs each provider incurs or the margins each provider has established. For example, an LLM provider may be running a negative margin or loss-leader strategy, and the long-term cost of using an LLM could be higher than it appears based on today’s prices. We do know that LLMs generate significant demand for power, to such an extent that big tech companies are developing plans to build nuclear power plants to support the power needed by future data centers to run all the models they anticipate [7]. Based on this, it seems we can expect that new LLMs will be bigger and more power-hungry, yet their value will offset the cost of building dedicated power plants for their datacenters.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p55"> &#13;
   <p>Based on this factor, one needs to be careful when a successful LLM solution creates more demand; you may run into power capacity problems when satisfying that demand. You also may need to be careful about the elasticity of power costs. Not only could LLM providers change cost structures, but if you host an LLM yourself, power price fluctuations of <span><img alt="equation image" src="../Images/eq-chapter-7-55-1.png"/></span> do happen in the United States [8]. This may not be a problem if your intended customer base is only 20,000 users, but if you plan on building something that will serve millions of users or more, the cost of power could be a major operational and environmental hazard.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p56"> &#13;
   <h3 class=" readable-text-h3" id="latency-scalability-and-availability"><span class="num-string browsable-reference-id">7.2.2</span> Latency, scalability, and availability</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p57"> &#13;
   <p>Latency is the time it takes from querying an LLM to getting some output, scalability describes how quickly one can go from one to a thousand LLMs running, and availability describes the ability to have an LLM operational 24/7. These are all major advantages of LLM—and more broadly, computers in general—over people. LLMs and AI/ML can react to more situations faster, at any time, than humans. This reaction speed can be both good and bad. When you have a system that requires supervision and review of outputs, you do not get the full availability benefit of an LLM without developing a staffing plan to match.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p58"> &#13;
   <h3 class=" readable-text-h3" id="refinement"><span class="num-string browsable-reference-id">7.2.3</span> Refinement</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p59"> &#13;
   <p>As we discussed in section <a href="#sec__self_improvement">7.1.1</a>, LLMs cannot easily self-improve. However, people can and do improve, and it is a common goal to improve the efficiency of a process over time. You will need to keep people in the loop to engineer better prompts and create better training regimes to improve efficiency with LLMs; without them, LLM performance will not improve.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p60"> &#13;
   <p>Improving LLM efficiency does not just involve upgrading to newer LLMs or fine-tuning existing models but also includes building the infrastructure and recording inputs, outputs, and performance metrics to study what is working and what is not. You can use frameworks like DSPy that we discussed in section <a href="../Text/chapter-5.html#sec__llm_programming">5.5.2</a> to capture these items and to identify and handle the cases that do not work or start failing over time as world circumstances change. For example, you might develop an initial LLM that is working well. But those damn kids keep adding new emojis to the iDroids and appleBots [9]. Without additional training, your LLM will not understand these new emojis, but your customers will inevitably start using them, so the system will start performing poorly. You’ll never figure this out if you don’t record the input and output of the LLM in logs or solicit feedback from your users who can provide information about areas where the LLM is failing or succeeding. Capturing this information is essential for improving and refining the process, which LLMs cannot do without human intervention.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p61"> &#13;
   <p> <span class="print-book-callout-head">Note</span> The emoji problem is a great example of why eliminating coding and using only LLMs will probably never happen. The emojis will be new tokens that LLM will have never seen in training data, so it intrinsically will not be able to handle them. How would we handle this in practice? Our first attempt would be to write code that detects emojis and replaces them with a description of the emoji’s appearance, intent, and connotations. It might not work in every case, but that’s why you test and validate. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p62"> &#13;
   <p>In the ML field, considerable attention is given to the concept of data drift, where data in the real world constantly evolves beyond what is captured in a model’s training data. When dealing with natural language, emojis are just one concrete example of how real-world data will change over time as language use evolves. The emoji example can be extended to include the problems created by new terminology or new ways of using existing words in a language. By looking at the existing work in the field, we can identify additional techniques for measuring and mitigating data drift for LLMs, such as collecting additional training data and fine-tuning models or altering prompts to include supplementary definitions for previously unseen terminology.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p63"> &#13;
   <h2 class=" readable-text-h2" id="language-models-are-not-models-of-the-world"><span class="num-string browsable-reference-id">7.3</span> Language models are not models of the world</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p64"> &#13;
   <p>You can frequently elicit accurate information about the world from an LLM. As a result, it’s easy to assume that a language model knows things about the world. Indeed, as a reader of this book, you can reason about the world and what will happen without taking any particular action. Now, we are not discussing anything so sophisticated as predicting the stock market, but even simple actions and thoughts. For example, what would happen if you told someone their sweater was ugly?</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p65"> &#13;
   <p>You do not need to interact with the environment or find an ugly sweater to answer this question. You do not need to speak or interact with anyone or anything to answer this question. You can imagine the “world” of sweaters and the feelings someone else may have and infer the results. If I told you someone was wearing the sweater at a Christmas party (an ugly sweater contest, perhaps?), you could update your mental model of the world and infer outcomes without having lived them. An LLM cannot think before it speaks. Generating text is the closest an LLM gets to “thinking” (using the word loosely in this context). You can see a simple example of this in figure <a href="#fig__uglySweater">7.6</a>, where an LLM’s overly verbose reasoning ultimately leads it to reach a nice comment. Reasoning, whether done implicitly or explicitly by us humans, is distinct from us speaking about the thing we are reasoning about. For an LLM, there is no separation of processes; producing more output is required to “think more” about the answer. Therefore, LLMs are not capable of thought independent from generating output.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p66"> &#13;
   <p> <span class="print-book-callout-head">Warning</span> We loosely use the word “think” in the context of an LLM. To be pedantic, we mean that the calculations an LLM does to answer a question are not dynamic. Outputting 10 tokens takes the same amount of work regardless of the content of those tokens. Answering a complex problem that requires humans to think more will probably require an LLM to perform more computation, but that usually means the LLM must also produce longer output, even if the answer shouldn’t be any longer. Whenever anyone uses the term <em>thinking</em> in conjunction with an LLM, it is better to replace <em>thinking</em> with <em>calculating</em>. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p67">  &#13;
   <img alt="figure" src="../Images/CH07_F06_Boozallen.png"/> &#13;
   <h5 class=" figure-container-h5" id="fig__uglySweater"><span class="num-string">Figure <span class="browsable-reference-id">7.6</span></span> The context and reason why someone is wearing or doing something unusual may be in the realm of something that an LLM properly recognizes and for which it produces an appropriate response. However, it might not be possible for an LLM to reach that appropriate response without producing some intermediate text. For a math problem, this intermediate text could be useful, but the intermediate text may not always be appropriate or desirable for a user to see.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p68"> &#13;
   <p>This example demonstrates that an LLM cannot plan without generating text about the planning process. If the LLM is not producing text, it is as if it does not exist. There are methods for constructing prompts that will encourage LLMs to break down their outputs to simulate planning. This is often called <em>chain-of-thought</em> (CoT) prompting, where you include in the prompt a statement like “Let’s think step by step.” This step-by-step instruction often improves the model’s ability to perform tasks [10], but it is unclear why this improves performance. Once again, the ambiguity of what it means to “think” can cause unreasonable expectations of what LLMs can and cannot do.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p69"> &#13;
   <p>Even with CoT, LLMs will still make many mistakes, such as missing steps, missing calculations, and performing logically invalid reasoning [11]. Other factors may contribute to the performance gains observed when an LLM produces output broken into a series of steps. Consider:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p70">Back in chapter 3, we learned about transformers and the attention mechanism used in their implementations. We learned that the longer the input received and outputs produced by an LLM, the more calculations the transformer does. So does thinking step by step work better just because the LLM, via the transformer, gets to do more <em>computation</em>? If the LLM had a world model, it could do this computation about the output without generating the output.</li> &#13;
   <li class="readable-text" id="p71">LLMs reflect the nature of their training data. There may be content in that training data correlated with “think step by step” and other pedagogical materials with more verbose and usually correct content. Ultimately, we may manually align the LLM’s fuzzy recall with more relevant training documents rather than get the LLMs to perform a fundamentally different function.</li> &#13;
  </ul> &#13;
  <div class="readable-text print-book-callout" id="p72"> &#13;
   <p> <span class="print-book-callout-head">Warning</span> The precise definition of a “world model” is not yet well agreed upon and can have different connotations for different people. When discussing world models, it is a good idea to discuss the definition first so that folks are on the same page. A lot of LLM discourse talks past each other, something we will discuss further in the last two chapters of this book. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p73"> &#13;
   <p>These problems are challenging and involve open-ended research questions. Our stance is that the dramatic failures of LLMs highlight that these are more likely explanations than something deeper. Importantly, some niche research focuses on imbuing machine learning methods with world models. A technical but fairly accessible 2018 example of this from David Ha and Jürgen Schmidhuber is available online (<a class="uri" href="https://worldmodels.github.io/">https://worldmodels.github.io/</a>) and shows massive performance improvements compared with existing methods back then. Others are working on making world models for LLMs and using LLMs as world models [12]. Current methods do not have the same high degree of flexibility as humans; these examples are more limited in scope and work for one general class of problems.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p74"> &#13;
   <h2 class=" readable-text-h2" id="sec__chp7_computational_limits"><span class="num-string browsable-reference-id">7.4</span> Computational limits: Hard problems are still hard</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p75"> &#13;
   <p>Some people are worried about “runaway” AI, where an AI algorithm becomes so advanced and capable that it can solve problems we never could and that such an AI would not have objectives that align with human welfare. If such an AI existed, it could improve itself in ways we couldn’t improve ourselves, resulting in an even more powerful AI. Many folks have allowed this thought to run rampant, imagining that an LLM will become almost godlike in capability and ability to outreason humans. There is an ethics question here that we will discuss more in the last chapter of the book. For now, there is a simple technical reason why we are not so concerned about this idea, and it also helps us understand the realistic limitations of LLMs. Essentially, there are many ways to measure what we can call computational complexity or algorithmic complexity. By comparing the complexity of LLMs with other well-studied algorithms, we can be more specific about what LLMs can and cannot achieve. We will also discuss how approximate solutions to problems using LLMs can, where appropriate, avoid some of the complexity of precise solutions to the same problems.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p76"> &#13;
   <p>In computer science, we spend a lot of time learning about algorithmic complexity. For most students or practitioners, this means understanding how a change in the amount of input data changes how long it will take a process to produce results. One of the more ideal cases, which rarely happens in reality, is that if you double the inputs, the process will take twice as long. In other words, a process that could take 2 days for <span><img alt="equation image" src="../Images/eq-chapter-7-76-1.png"/></span> items (in the case of an LLM, an item might be a token) takes 4 days for <span><img alt="equation image" src="../Images/eq-chapter-7-76-2.png"/></span>. When discussing complexity in computer science, we often use mathematical notation, known as Big-O notation, to communicate different levels of complexity. When a process’s computation time grows at the same rate as the size of its input, it is called linear complexity and is denoted in Big-O notation as <span><img alt="equation image" src="../Images/eq-chapter-7-76-3.png"/></span>). If you draw a graph with data size on the x-axis and computation time on the y-axis, you would get a line because both data and computation time grow at the same rate. Other common real-world complexities include log-linear (<span><img alt="equation image" src="../Images/eq-chapter-7-76-4.png"/></span>), where <span><img alt="equation image" src="../Images/eq-chapter-7-76-5.png"/></span> might be closer to 4.4 days; quadratic (<span><img alt="equation image" src="../Images/eq-chapter-7-76-6.png"/></span>, where <span><img alt="equation image" src="../Images/eq-chapter-7-76-7.png"/></span> might be closer to 8 days; and exponential (<span><img alt="equation image" src="../Images/eq-chapter-7-76-8.png"/></span>), where computation time grows so quickly as the size of the input increases that there is a good chance the world will no longer exist before your algorithm finishes. In each of these cases, the graph of input size versus computation time becomes steeper as systems get more complex. In other words, for more complex algorithms, the processing time will grow faster as the amount of data processed increases.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p77"> &#13;
   <p>We’ve taken this short trip into computer science to help you understand the computational complexity of running an LLM. For an input of <span><img alt="equation image" src="../Images/eq-chapter-7-77-1.png"/></span> items, the LLM has a computational complexity of <span><img alt="equation image" src="../Images/eq-chapter-7-77-2.png"/></span> or quadratic complexity. If we can prove that an algorithm/task takes more than <span><img alt="equation image" src="../Images/eq-chapter-7-77-3.png"/></span> work, then we have essentially proven that an LLM cannot efficiently solve the problem because an LLM’s core algorithms aren’t able to execute algorithms with that level of complexity, precisely.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p78"> &#13;
   <p> <span class="print-book-callout-head">Warning</span> This isn’t a graduate class on formal methods or algorithms; we are providing a quick overview of the study of algorithmic complexity. The goal is to give you, the reader, a technical intuition for the problem, but we haven’t fully armed you with all the knowledge needed to discuss this subject in detail. To learn more about algorithms and complexity, see Aditya Y. Bhargava’s book <em>Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People</em> [13]. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p79"> &#13;
   <p>If it was possible to get an LLM to solve a problem that required, say, cubic complexity of <span><img alt="equation image" src="../Images/eq-chapter-7-79-1.png"/></span>, but the LLM itself had a faster (smaller) complexity of <span><img alt="equation image" src="../Images/eq-chapter-7-79-2.png"/></span>, then we would have a logical contradiction. In other words, an LLM can’t solve a complex problem faster than the complexity analysis states. Many real-world tasks and algorithms have worse than <span><img alt="equation image" src="../Images/eq-chapter-7-79-3.png"/></span> complexities. We describe a few examples in table <a href="#tab__algorithm_complexity">7.1</a>, and you’ll notice that the handful we’ve listed relate to logistics or resource allocation. For example, delivering packages and rescheduling flights are problems that have majorly painful algorithmic complexities.</p> &#13;
  </div> &#13;
  <div class="browsable-container browsable-table-container" id="p80"> &#13;
   <h5 class=" browsable-container-h5" id="tab__algorithm_complexity"><span class="num-string">Table <span class="browsable-reference-id">7.1</span></span> Some examples of important algorithms with different time complexities</h5> &#13;
  <img alt="figure" src="../Images/table_7_1.png"/> &#13;
  </div> &#13;
  <div class="readable-text" id="p81"> &#13;
   <p>A second important and related reason we care about algorithms is the <em>complexity class</em> of an algorithm. A complexity class defines the scope of possible algorithms that an algorithm can solve. The most famous complexity classes are <span><img alt="equation image" src="../Images/eq-chapter-7-81-1.png"/></span> (for polynomial) and <span><img alt="equation image" src="../Images/eq-chapter-7-81-2.png"/></span>, which are problems that take at least <span><img alt="equation image" src="../Images/eq-chapter-7-81-3.png"/></span> time to finish. These very broad classes contain basically all the problems you might ever care about.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p82"> &#13;
   <p> <span class="print-book-callout-head">Note</span> Many people think that <span><img alt="equation image" src="../Images/eq-chapter-7-82-1.png"/></span> stands for <em>not-polynomial</em>, but this is false! It actually means <em>nondeterministic polynomial</em>. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p83"> &#13;
   <p>What is interesting and informative is that William Merrill and Ashish Sabharwal [14] proved that an LLM’s ability to solve problems correlates to the number of tokens it generates in intermediate steps. For an LLM, generating a response falls into a complexity class called <span><img alt="equation image" src="../Images/eq-chapter-7-83-1.png"/></span> (we know, computer scientists are the worst at naming things). This complexity class is very restrictive, meaning an LLM can barely solve anything. As the intermediate steps <span><img alt="equation image" src="../Images/eq-chapter-7-83-2.png"/></span> become longer, you eventually reach the complexity class of <span><img alt="equation image" src="../Images/eq-chapter-7-83-3.png"/></span>. This means an LLM can never solve real-world problems that are NP or harder! We tie this all together in figure <a href="#fig__computationalComplexity">7.7</a>, which shows how these layers of complexity classes relate.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p84">  &#13;
   <img alt="figure" src="../Images/CH07_F07_Boozallen.png"/> &#13;
   <h5 class=" figure-container-h5" id="fig__computationalComplexity"><span class="num-string">Figure <span class="browsable-reference-id">7.7</span></span> A Venn diagram of computational complexities (assuming <span><img alt="equation image" src="../Images/eq-chapter-7-84-1.png"/></span>, a minor point for the nerds) relate to each other. The top arrows give examples of the kind of problem that a new complexity class lets you solve. The bottom arrows show where LLMs land in terms of their complexity.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p85"> &#13;
   <p>This finding is even more damaging because complexity classes describe the kinds of problems you can solve, not how efficiently you can solve them. For example, an LLM must generate on the order of <span><img alt="equation image" src="../Images/eq-chapter-7-85-1.png"/></span> tokens to solve an algorithm that involves <span><img alt="equation image" src="../Images/eq-chapter-7-85-2.png"/></span> complexity. Yet, an LLM also needs <span><img alt="equation image" src="../Images/eq-chapter-7-85-3.png"/></span> time to process <span><img alt="equation image" src="../Images/eq-chapter-7-85-4.png"/></span> tokens, so you end up with <span><img alt="equation image" src="../Images/eq-chapter-7-85-5.png"/></span> computational effort, a massive blow-up in complexity. Also, this complexity estimation does not account for LLM training data and the time required to develop prompts to get the LLM to perform the algorithm successfully without errors.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p86"> &#13;
   <h3 class=" readable-text-h3" id="sec__fuzzy_algos_fuzzy_problems"><span class="num-string browsable-reference-id">7.4.1</span> Using fuzzy algorithms for fuzzy problems</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p87"> &#13;
   <p>This discussion about algorithms and complexity may sound very damning for LLMs. In truth, it is only damning if you want to apply LLMs to problems that require correct outputs. If even the smallest error is unacceptable in your system, you should not use machine learning, let alone an LLM.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p88"> &#13;
   <p>Like machine learning at large, LLMs work best for fuzzy problems, where what makes something correct or incorrect is hard to describe. In fuzzy problems, it is often the case that it is OK if errors exist; other processes can remediate those errors, or the cost of errors is potentially small enough to ignore. That’s why text and natural language are a good fit for LLMs. The answers to problems like “What did Suzy mean in that email?” or “Did John mean to imply that in his text?” are intrinsically fuzzy. Human language is fraught with imprecision, clarification, and repetition that align well with the difficulty of getting LLMs to solve problems that require consistent and precise answers.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p89"> &#13;
   <h3 class=" readable-text-h3" id="when-close-enough-is-good-enough-for-hard-problems"><span class="num-string browsable-reference-id">7.4.2</span> When close enough is good enough for hard problems</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p90"> &#13;
   <p>To argue against ourselves for a moment, we should also point out that humans cannot solve NP-hard problems when we use <em>solve</em> to mean “arrive at the optimal solution for which no better solution exists.” We use approximations to solve complex problems because we know they are too hard to solve perfectly.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p91"> &#13;
   <p>For example, in table 7.1 and figure 7.7, we mentioned the traveling salesman problem, a famous and important problem for delivery route planning. The mail courier wants to deliver everyone’s mail in the minimum amount of time and distance traveled without repeating any routes. Computationally, finding the best route is NP-hard, so you can only apply it to a few hundred or maybe a thousand delivery destinations. However, there are much faster quadratic algorithms that approximate the problem, and we can prove they give us a path that is no worse than <span><img alt="equation image" src="../Images/eq-chapter-7-91-1.png"/></span> the travel distance of the minimum distance route. So in the real world, we use these and other techniques to get “close enough is good enough” solutions. So too can LLMs potentially get “close enough is good enough” solutions, but they are still constrained by the fact that they are inefficient for exact problems.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p92"> &#13;
   <p>Without an understanding of an LLM’s training data, we have difficulty estimating how well it might solve a difficult problem through approximation. Consider that the game of chess is technically harder than NP-hard. GPT-3.5 can play a decent game of chess that can defeat a real human [15], although not at the “dominating all humans” level that dedicated chess programs can achieve. Does this show that LLMs are good at approximately solving very hard problems?</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p93"> &#13;
   <p>Probably not. First, ChatGPT’s chess game dramatically improved after adding chess as an evaluation metric (<a class="uri" href="https://github.com/openai/evals/pull/45">https://github.com/openai/evals/pull/45</a>). It’s not unreasonable to suspect that the makers of ChatGPT performed fine-tuning that incorporated chess as an explicit goal. Second, the internet is full of games of chess for people to study and explore (<a class="uri" href="https://old.chesstempo.com/game-database.html">https://old.chesstempo.com/game-database.html</a>), so ChatGPT has likely been trained on full games of chess captured in its training data.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p94"> &#13;
   <p>Still, it is interesting that ChatGPT can use what is in its training data to play a reasonable game of chess, matching what it has seen before to slightly different situations in the future. When considering where an LLM-based solution will work best, we recommend this mental framework: apply LLMs to repetitive, mildly varying problems to maximize their utility. Applications such as text summarization, language translation, writing first drafts of documents, and checking existing writing all fit into this category.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p95"> &#13;
   <p>Similar lessons come from other areas of deep learning, where it is easier to reason about what is happening inside a model than for LLMs. For example, playing the game of Go has been one of the longest-standing challenges in AI research for decades. AI has only recently been able to beat champion-level players in the game. Like LLMs, Go-playing AIs train by observing many example games. Yet, if you built a Go-playing bot that performed unusual and/or nonsensical moves, it would defeat the “superhuman” AI but lose to human amateurs [16]. This example also highlights the risk of using LLMs in adversarial environments, where humans are far better at dealing with significant novelty in a situation than current AI/LLMs.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p96"> &#13;
   <h2 class=" readable-text-h2" id="summary">Summary</h2> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p97">The biggest advantage LLMs have over humans is the scale they achieve. LLMs can run at low cost, 24/7, and be resized to meet demand with far less effort than training up or reducing a human workforce.</li> &#13;
   <li class="readable-text" id="p98">Humans are better at handling highly novel situations, which is important if the people interacting with the LLM might be adversaries (e.g., trying to commit fraud).</li> &#13;
   <li class="readable-text" id="p99">We know LLMs work well for problems similar to what they have seen before in their training data, making them useful for repetitive work.</li> &#13;
   <li class="readable-text" id="p100">Prompt engineering is likely the most effective starting point to “teach” LLMs something new unless you can dedicate large amounts of effort and money to data collection and fine-tuning.</li> &#13;
   <li class="readable-text" id="p101">LLMs cannot self-improve and are inefficient at solving algorithmic problems requiring a specific correct answer. They work best on “fuzzy” problems where there is some range of satisfying outputs and some amount of error is acceptable.</li> &#13;
  </ul>&#13;
 </body></html>