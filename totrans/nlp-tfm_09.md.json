["```py\nfrom transformers import pipeline\n\nbert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\npipe = pipeline(\"text-classification\", model=bert_ckpt)\n```", "```py\nquery = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in\nParis and I need a 15 passenger van\"\"\"\npipe(query)\n```", "```py\n[{'label': 'car_rental', 'score': 0.549003541469574}]\n```", "```py\nclass PerformanceBenchmark:\n    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n        self.pipeline = pipeline\n        self.dataset = dataset\n        self.optim_type = optim_type\n\n    def compute_accuracy(self):\n        # We'll define this later\n        pass\n\n    def compute_size(self):\n        # We'll define this later\n        pass\n\n    def time_pipeline(self):\n        # We'll define this later\n        pass\n\n    def run_benchmark(self):\n        metrics = {}\n        metrics[self.optim_type] = self.compute_size()\n        metrics[self.optim_type].update(self.time_pipeline())\n        metrics[self.optim_type].update(self.compute_accuracy())\n        return metrics\n```", "```py\nfrom datasets import load_dataset\n\nclinc = load_dataset(\"clinc_oos\", \"plus\")\n```", "```py\nsample = clinc[\"test\"][42]\nsample\n```", "```py\n{'intent': 133, 'text': 'transfer $100 from my checking to saving account'}\n```", "```py\nintents = clinc[\"test\"].features[\"intent\"]\nintents.int2str(sample[\"intent\"])\n```", "```py\n'transfer'\n```", "```py\nfrom datasets import load_metric\n\naccuracy_score = load_metric(\"accuracy\")\n```", "```py\ndef compute_accuracy(self):\n    \"\"\"This overrides the PerformanceBenchmark.compute_accuracy() method\"\"\"\n    preds, labels = [], []\n    for example in self.dataset:\n        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n        label = example[\"intent\"]\n        preds.append(intents.str2int(pred))\n        labels.append(label)\n    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n    return accuracy\n\nPerformanceBenchmark.compute_accuracy = compute_accuracy\n```", "```py\nlist(pipe.model.state_dict().items())[42]\n```", "```py\n('bert.encoder.layer.2.attention.self.value.weight',\n tensor([[-1.0526e-02, -3.2215e-02,  2.2097e-02,  ..., -6.0953e-03,\n           4.6521e-03,  2.9844e-02],\n         [-1.4964e-02, -1.0915e-02,  5.2396e-04,  ...,  3.2047e-05,\n          -2.6890e-02, -2.1943e-02],\n         [-2.9640e-02, -3.7842e-03, -1.2582e-02,  ..., -1.0917e-02,\n           3.1152e-02, -9.7786e-03],\n         ...,\n         [-1.5116e-02, -3.3226e-02,  4.2063e-02,  ..., -5.2652e-03,\n           1.1093e-02,  2.9703e-03],\n         [-3.6809e-02,  5.6848e-02, -2.6544e-02,  ..., -4.0114e-02,\n           6.7487e-03,  1.0511e-03],\n         [-2.4961e-02,  1.4747e-03, -5.4271e-02,  ...,  2.0004e-02,\n           2.3981e-02, -4.2880e-02]]))\n```", "```py\ntorch.save(pipe.model.state_dict(), \"model.pt\")\n```", "```py\nimport torch\nfrom pathlib import Path\n\ndef compute_size(self):\n    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n    state_dict = self.pipeline.model.state_dict()\n    tmp_path = Path(\"model.pt\")\n    torch.save(state_dict, tmp_path)\n    # Calculate size in megabytes\n    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n    # Delete temporary file\n    tmp_path.unlink()\n    print(f\"Model size (MB) - {size_mb:.2f}\")\n    return {\"size_mb\": size_mb}\n\nPerformanceBenchmark.compute_size = compute_size\n```", "```py\nfrom time import perf_counter\n\nfor _ in range(3):\n    start_time = perf_counter()\n    _ = pipe(query)\n    latency = perf_counter() - start_time\n    print(f\"Latency (ms) - {1000 * latency:.3f}\")\n```", "```py\nLatency (ms) - 85.367\nLatency (ms) - 85.241\nLatency (ms) - 87.275\n```", "```py\nimport numpy as np\n\ndef time_pipeline(self, query=\"What is the pin number for my account?\"):\n    \"\"\"This overrides the PerformanceBenchmark.time_pipeline() method\"\"\"\n    latencies = []\n    # Warmup\n    for _ in range(10):\n        _ = self.pipeline(query)\n    # Timed run\n    for _ in range(100):\n        start_time = perf_counter()\n        _ = self.pipeline(query)\n        latency = perf_counter() - start_time\n        latencies.append(latency)\n    # Compute run statistics\n    time_avg_ms = 1000 * np.mean(latencies)\n    time_std_ms = 1000 * np.std(latencies)\n    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n\nPerformanceBenchmark.time_pipeline = time_pipeline\n```", "```py\npb = PerformanceBenchmark(pipe, clinc[\"test\"])\nperf_metrics = pb.run_benchmark()\n```", "```py\nModel size (MB) - 418.16\nAverage latency (ms) - 54.20 +\\- 1.91\nAccuracy on test set - 0.867\n```", "```py\nfrom transformers import TrainingArguments\n\nclass DistillationTrainingArguments(TrainingArguments):\n    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.alpha = alpha\n        self.temperature = temperature\n```", "```py\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import Trainer\n\nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs_stu = model(**inputs)\n        # Extract cross-entropy loss and logits from student\n        loss_ce = outputs_stu.loss\n        logits_stu = outputs_stu.logits\n        # Extract logits from teacher\n        with torch.no_grad():\n            outputs_tea = self.teacher_model(**inputs)\n            logits_tea = outputs_tea.logits\n        # Soften probabilities and compute distillation loss\n        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n        loss_kd = self.args.temperature ** 2 * loss_fct(\n            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n            F.softmax(logits_tea / self.args.temperature, dim=-1))\n        # Return weighted student loss\n        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n        return (loss, outputs_stu) if return_outputs else loss\n```", "```py\nfrom transformers import AutoTokenizer\n\nstudent_ckpt = \"distilbert-base-uncased\"\nstudent_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n\ndef tokenize_text(batch):\n    return student_tokenizer(batch[\"text\"], truncation=True)\n\nclinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\nclinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")\n```", "```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```", "```py\ndef compute_metrics(pred):\n    predictions, labels = pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy_score.compute(predictions=predictions, references=labels)\n```", "```py\nbatch_size = 48\n\nfinetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\nstudent_training_args = DistillationTrainingArguments(\n    output_dir=finetuned_ckpt, evaluation_strategy = \"epoch\",\n    num_train_epochs=5, learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01,\n    push_to_hub=True)\n```", "```py\nid2label = pipe.model.config.id2label\nlabel2id = pipe.model.config.label2id\n```", "```py\nfrom transformers import AutoConfig\n\nnum_labels = intents.num_classes\nstudent_config = (AutoConfig\n                  .from_pretrained(student_ckpt, num_labels=num_labels,\n                                   id2label=id2label, label2id=label2id))\n```", "```py\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef student_init():\n    return (AutoModelForSequenceClassification\n            .from_pretrained(student_ckpt, config=student_config).to(device))\n```", "```py\nteacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\nteacher_model = (AutoModelForSequenceClassification\n                 .from_pretrained(teacher_ckpt, num_labels=num_labels)\n                 .to(device))\n```", "```py\ndistilbert_trainer = DistillationTrainer(model_init=student_init,\n    teacher_model=teacher_model, args=student_training_args,\n    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n\ndistilbert_trainer.train()\n```", "```py\ndistilbert_trainer.push_to_hub(\"Training completed!\")\n```", "```py\nfinetuned_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\npipe = pipeline(\"text-classification\", model=finetuned_ckpt)\n```", "```py\noptim_type = \"DistilBERT\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())\n```", "```py\nModel size (MB) - 255.89\nAverage latency (ms) - 27.53 +\\- 0.60\nAccuracy on test set - 0.858\n```", "```py\nimport pandas as pd\n\ndef plot_metrics(perf_metrics, current_optim_type):\n    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n\n    for idx in df.index:\n        df_opt = df.loc[idx]\n        # Add a dashed circle around the current optimization type\n        if idx == current_optim_type:\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx,\n                        marker='$\\u25CC$')\n        else:\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n\n    legend = plt.legend(bbox_to_anchor=(1,1))\n    for handle in legend.legendHandles:\n        handle.set_sizes([20])\n\n    plt.ylim(80,90)\n    # Use the slowest model to define the x-axis range\n    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n    plt.xlim(1, xlim)\n    plt.ylabel(\"Accuracy (%)\")\n    plt.xlabel(\"Average latency (ms)\")\n    plt.show()\n\nplot_metrics(perf_metrics, optim_type)\n```", "```py\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -2, 2)\n    y = trial.suggest_float(\"y\", -2, 2)\n    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n```", "```py\nimport optuna\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=1000)\n```", "```py\nstudy.best_params\n```", "```py\n{'x': 1.003024865971437, 'y': 1.00315167589307}\n```", "```py\ndef hp_space(trial):\n    return {\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n        \"temperature\": trial.suggest_int(\"temperature\", 2, 20)}\n```", "```py\nbest_run = distilbert_trainer.hyperparameter_search(\n    n_trials=20, direction=\"maximize\", hp_space=hp_space)\n```", "```py\nprint(best_run)\n```", "```py\nBestRun(run_id='1', objective=0.927741935483871,\nhyperparameters={'num_train_epochs': 10, 'alpha': 0.12468168730193585,\n'temperature': 7})\n```", "```py\nfor k,v in best_run.hyperparameters.items():\n    setattr(student_training_args, k, v)\n\n# Define a new repository to store our distilled model\ndistilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\nstudent_training_args.output_dir = distilled_ckpt\n\n# Create a new Trainer with optimal parameters\ndistil_trainer = DistillationTrainer(model_init=student_init,\n    teacher_model=teacher_model, args=student_training_args,\n    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n\ndistil_trainer.train();\n```", "```py\ndistil_trainer.push_to_hub(\"Training complete\")\n```", "```py\ndistilled_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\npipe = pipeline(\"text-classification\", model=distilled_ckpt)\noptim_type = \"Distillation\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())\n```", "```py\nModel size (MB) - 255.89\nAverage latency (ms) - 25.96 +\\- 1.63\nAccuracy on test set - 0.868\n```", "```py\nplot_metrics(perf_metrics, optim_type)\n```", "```py\nimport matplotlib.pyplot as plt\n\nstate_dict = pipe.model.state_dict()\nweights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\nplt.hist(weights.flatten().numpy(), bins=250, range=(-0.3,0.3), edgecolor=\"C0\")\nplt.show()\n```", "```py\nzero_point = 0\nscale = (weights.max() - weights.min()) / (127 - (-128))\n```", "```py\n(weights / scale + zero_point).clamp(-128, 127).round().char()\n```", "```py\ntensor([[ -5,  -8,   0,  ...,  -6,  -4,   8],\n        [  8,   3,   1,  ...,  -4,   7,   0],\n        [ -9,  -6,   5,  ...,   1,   5,  -3],\n        ...,\n        [  6,   0,  12,  ...,   0,   6,  -1],\n        [  0,  -2, -12,  ...,  12,  -7, -13],\n        [-13,  -1, -10,  ...,   8,   2,  -2]], dtype=torch.int8)\n```", "```py\nfrom torch import quantize_per_tensor\n\ndtype = torch.qint8\nquantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\nquantized_weights.int_repr()\n```", "```py\ntensor([[ -5,  -8,   0,  ...,  -6,  -4,   8],\n        [  8,   3,   1,  ...,  -4,   7,   0],\n        [ -9,  -6,   5,  ...,   1,   5,  -3],\n        ...,\n        [  6,   0,  12,  ...,   0,   6,  -1],\n        [  0,  -2, -12,  ...,  12,  -7, -13],\n        [-13,  -1, -10,  ...,   8,   2,  -2]], dtype=torch.int8)\n```", "```py\n%%timeit\nweights @ weights\n```", "```py\n393 \u00b5s \u00b1 3.84 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```", "```py\nfrom torch.nn.quantized import QFunctional\n\nq_fn = QFunctional()\n```", "```py\n%%timeit\nq_fn.mul(quantized_weights, quantized_weights)\n```", "```py\n23.3 \u00b5s \u00b1 298 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```", "```py\nimport sys\n\nsys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())\n```", "```py\n3.999633833760527\n```", "```py\nfrom torch.quantization import quantize_dynamic\n\nmodel_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt).to(\"cpu\"))\n\nmodel_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n```", "```py\npipe = pipeline(\"text-classification\", model=model_quantized,\n                tokenizer=tokenizer)\noptim_type = \"Distillation + quantization\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())\n```", "```py\nModel size (MB) - 132.40\nAverage latency (ms) - 12.54 +\\- 0.73\nAccuracy on test set - 0.876\n```", "```py\nplot_metrics(perf_metrics, optim_type)\n```", "```py\nimport os\nfrom psutil import cpu_count\n\nos.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\nos.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\"\n```", "```py\nfrom transformers.convert_graph_to_onnx import convert\n\nmodel_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\nonnx_model_path = Path(\"onnx/model.onnx\")\nconvert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer,\n        output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")\n```", "```py\nfrom onnxruntime import (GraphOptimizationLevel, InferenceSession,\n                         SessionOptions)\n\ndef create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"):\n    options = SessionOptions()\n    options.intra_op_num_threads = 1\n    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n    session = InferenceSession(str(model_path), options, providers=[provider])\n    session.disable_fallback()\n    return session\n```", "```py\nonnx_model = create_model_for_provider(onnx_model_path)\n```", "```py\ninputs = clinc_enc[\"test\"][:1]\ndel inputs[\"labels\"]\nlogits_onnx = onnx_model.run(None, inputs)[0]\nlogits_onnx.shape\n```", "```py\n(1, 151)\n```", "```py\nnp.argmax(logits_onnx)\n```", "```py\n61\n```", "```py\nclinc_enc[\"test\"][0][\"labels\"]\n```", "```py\n61\n```", "```py\nfrom scipy.special import softmax\n\nclass OnnxPipeline:\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def __call__(self, query):\n        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n        inputs_onnx = {k: v.cpu().detach().numpy()\n                       for k, v in model_inputs.items()}\n        logits = self.model.run(None, inputs_onnx)[0][0, :]\n        probs = softmax(logits)\n        pred_idx = np.argmax(probs).item()\n        return [{\"label\": intents.int2str(pred_idx), \"score\": probs[pred_idx]}]\n```", "```py\npipe = OnnxPipeline(onnx_model, tokenizer)\npipe(query)\n```", "```py\n[{'label': 'car_rental', 'score': 0.7848334}]\n```", "```py\nclass OnnxPerformanceBenchmark(PerformanceBenchmark):\n    def __init__(self, *args, model_path, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_path = model_path\n\n    def compute_size(self):\n        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n        print(f\"Model size (MB) - {size_mb:.2f}\")\n        return {\"size_mb\": size_mb}\n```", "```py\noptim_type = \"Distillation + ORT\"\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n                              model_path=\"onnx/model.onnx\")\nperf_metrics.update(pb.run_benchmark())\n```", "```py\nModel size (MB) - 255.88\nAverage latency (ms) - 21.02 +\\- 0.55\nAccuracy on test set - 0.868\n```", "```py\nplot_metrics(perf_metrics, optim_type)\n```", "```py\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\n\nmodel_input = \"onnx/model.onnx\"\nmodel_output = \"onnx/model.quant.onnx\"\nquantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)\n```", "```py\nonnx_quantized_model = create_model_for_provider(model_output)\npipe = OnnxPipeline(onnx_quantized_model, tokenizer)\noptim_type = \"Distillation + ORT (quantized)\"\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n                              model_path=model_output)\nperf_metrics.update(pb.run_benchmark())\n```", "```py\nModel size (MB) - 64.20\nAverage latency (ms) - 9.24 +\\- 0.29\nAccuracy on test set - 0.877\n```", "```py\nplot_metrics(perf_metrics, optim_type)\n```"]