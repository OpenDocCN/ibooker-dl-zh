["```py\n$ pip install tensorflow\n\n```", "```py\n$ pip install virtualenv\n\n```", "```py\n$ cd ~\n$ mkdir envs\n$ virtualenv ~/envs/tensorflow\n\n```", "```py\n$ source ~/envs/tensorflow/bin/activate\n\n```", "```py\n(tensorflow)$\n```", "```py\n(tensorflow)$ pip install tensorflow\u200b\n```", "```py\n(tensorflow)$ deactivate\n\n```", "```py\n$\n\n```", "```py\nalias tensorflow=\"source ~/envs/tensorflow/bin/activate\"\n\n```", "```py\nimport tensorflow as tf\nprint(tf.__version__)\n\n```", "```py\nimport tensorflow as tf\n\nh = tf.constant(\"Hello\")\nw = tf.constant(\" World!\")\nhw = h + w\n\nwith tf.Session() as sess:\nans = sess.run(hw)\n\nprint (ans)\n```", "```py\nimport tensorflow as tf\n\n```", "```py\nimport tensorflow as tf\n\nh = tf.constant(\"Hello\")\nw = tf.constant(\" World!\")\nhw = h + w \n\n```", "```py\nph = \"Hello\"\npw = \" World!\"\nphw = h + w\n\n```", "```py\n>`print``phw`HelloWorld!\n```", "```py\n>`print``hw`Tensor(\"add:0\",shape=(),dtype=string)\n```", "```py\nhw = h + w \n\n```", "```py\nans = sess.run(hw)\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nDATA_DIR = '/tmp/data'\nNUM_STEPS = 1000\nMINIBATCH_SIZE = 100\n\ndata = input_data.read_data_sets(DATA_DIR, one_hot=True)\n\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\n\ny_true = tf.placeholder(tf.float32, [None, 10])\ny_pred = tf.matmul(x, W)\n\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\nlogits=y_pred, labels=y_true))\n\ngd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\ncorrect_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n\nwith tf.Session() as sess:\n\n# Train\nsess.run(tf.global_variables_initializer())\n\nfor _ in range(NUM_STEPS):\nbatch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\nsess.run(gd_step, feed_dict={x: batch_xs, y_true: batch_ys})\n\n# Test\nans = sess.run(accuracy, feed_dict={x: data.test.images, \n                                    y_true: data.test.labels})\n\nprint \"Accuracy: {:.4}%\".format(ans*100)\n\n```", "```py\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\nAccuracy: 91.83%\n\n```", "```py\nAccuracy: 91.86%\nAccuracy: 91.51%\nAccuracy: 91.62%\nAccuracy: 91.93%\nAccuracy: 91.88%\n\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n```", "```py\nDATA_DIR = '/tmp/data'\nNUM_STEPS = 1000\nMINIBATCH_SIZE = 100\n\n```", "```py\ndata = input_data.read_data_sets(DATA_DIR, one_hot=True)\n\n```", "```py\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\n\n```", "```py\ny_true = tf.placeholder(tf.float32, [None, 10])\ny_pred = tf.matmul(x, W)\n```", "```py\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\nlogits=y_pred, labels=y_true))\n\n```", "```py\ngd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\n```", "```py\ncorrect_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32)) \n\n```", "```py\nwith tf.Session() as sess:\n\n```", "```py\n    sess.run(tf.global_variables_initializer())\n\n```", "```py\n    for _ in range(NUM_STEPS):\n        batch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\n        sess.run(gd_step, feed_dict={x: batch_xs, y_true: batch_ys})\n\n```", "```py\n    ans = sess.run(accuracy, feed_dict={x: data.test.images,\n                                        y_true: data.test.labels})\n\n```", "```py\n    print \"Accuracy: {:.4}%\".format(ans*100)\n\n```", "```py\nfeed_dict={x: data.test.images, y_true: data.test.labels}\nans = sess.run(accuracy, feed_dict)\n\n```"]