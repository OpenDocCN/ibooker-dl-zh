- en: 7 Scaling and performance optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optimizing data analysis code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling code across multiple CPUs and GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code conversion between programming languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-based deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Dask framework for all of this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In today’s data-driven world, it’s more likely than not that the insights required
    by your business will be gained from vast amounts of data. As data volumes continue
    to grow and analytical complexity increases, and as algorithms and data processing
    pipelines reach the point where the sheer volume of data or the complexity of
    operations limits their ability to deliver timely and accurate results, you’ll
    encounter the need for code performance optimization. It will likely become essential
    for ensuring your analytics remain effective and efficient, even when the underlying
    data and computational demands grow.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ll assume that the analysis of the Brazilian e-commerce
    data we performed in previous chapters was so successful that, instead of encompassing
    a set of a hundred thousand orders, it now needs to be performed periodically
    on datasets of millions of orders. To make things even more interesting, let’s
    assume that the business grew and became multinational. Such growth could easily
    increase the amount of data by several orders of magnitude. At first glance, you
    might think throwing more CPUs at the problem or migrating to the latest GPU for
    its sheer processing power would solve the problem, but this would invariably
    involve either capital expenditure or an extension of the budget for cloud solutions.
    Just as an old IT adage states that any data will quickly fill all space allocated
    to it, you will quickly find that, when attempting any non-trivial data analysis,
    the calculations invariably consume all computing power available.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, you have numerous strategies for scaling and performance optimization
    at your disposal, including parallel and distributed computing, GPU acceleration,
    and specialized hardware or software solutions. By exploring various methods and
    tools, you should be able to find the most appropriate techniques for your specific
    data analytics needs.
  prefs: []
  type: TYPE_NORMAL
- en: There is a catch, though. There is no one-size-fits-all solution. Your choices
    will almost always be limited by the technology stack available to you and either
    by the budget or by what your business or institution has authorized for use.
    That is why we’ll provide an overview of different methods here rather than taking
    a deep dive into a single option.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see, generative AI, and particularly GPT models, can greatly support
    your optimization and scaling efforts. These advanced natural language processing
    models can help you identify relevant strategies, suggest improvements, review
    code, and even generate optimized code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will delve deeper into the importance of scaling and optimization
    in the context of data analytics. We will cover various techniques and approaches
    to achieve optimal performance and discuss the role of GPT models in supporting
    this process. Additionally, we will provide practical examples and case studies
    to demonstrate the real-world impact of effective scaling and optimization in
    data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are many tools that purport to perform optimization and scaling
    automatically, we believe that knowing these topics is vital in understanding
    proper code performance. Sadly, a lot of newbie data scientists, analysts, and
    engineers take code performance for granted or delegate it to said automated tools,
    potentially seriously underachieving in their efforts.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a solid understanding of the critical
    role that scaling and performance optimization play in data analytics, and you’ll
    have the knowledge necessary to identify, evaluate, and implement effective strategies
    to ensure your analytics pipeline remains efficient and effective in the face
    of ever-growing data volumes and computational demands.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Performance measurement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When setting out to optimize your analytics, the first crucial step is defining
    what good performance means for your goals. You need a clear yardstick to measure
    the success of your efforts. This means defining two basic concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: What characteristics of the system’s performance are most important? Should
    it prioritize quick response or accuracy? Should it allow multiple user access,
    or is it OK for users to have to queue? Can it block access while it is processing,
    or does it still need to allow users to view the data while they are waiting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the baselines for the preceding characteristics? Should we aim for
    the current expected load, or already plan for future expansion? Can we reliably
    define what a normal load will look like?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we could consider such metrics as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Latency**—*The time it takes for a data analysis system to begin displaying
    initial results after the data input is complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Throughput**—*The amount of data the system can process within a given time
    frame. It is typically measured in records per second or gigabytes per hour, reflecting
    the system’s efficiency in handling large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Parallelism or concurrency**—*The number of data analysis queries or processes
    the system can handle simultaneously without performance degradation. It demonstrates
    the system’s ability to manage multiple user requests or tasks at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bandwidth*—In the context of data analysis, this refers to the volume of data
    that can be imported into or exported out of the system per unit time. It is crucial
    for systems that need to handle large data transfers efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Response time*—The total time it takes for a system to complete a data analysis
    query or task from the moment it is initiated. This metric includes both the computation
    time and the time taken to retrieve and display the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Availability*—The likelihood that the data analysis system is operational
    and available for use when needed. It’s particularly important in systems requiring
    high uptime for continuous data analysis and decision-making processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalability*—The ability of the data analysis system to maintain or improve
    performance as the size of the data grows. This includes adding more data sources,
    handling more complex queries, or serving more users without a drop in performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reliability*—The consistency of the data analysis system in providing accurate
    and timely results under different conditions. This metric is often assessed by
    the frequency and severity of system failures or inaccuracies in the output data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at a real-world scenario: a company was moving its infrastructure
    to the cloud and, suddenly, solutions that worked well on virtual machines became
    unusable due to the sheer amount of data to be analyzed. Processing times shot
    up dramatically, sometimes exceeding a 5,400% increase. Management had to reassess
    their approach, targets, and priorities swiftly, and, due to limited capacities,
    it became critical to have a good understanding of what “good” means in this specific
    context.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing vastly different domains can illustrate the different priorities.
    If you consider a data analysis system for an astronomical observatory, it is
    likely that latency or response time will not be as critical as in the case of,
    for example, a patient-monitoring system in a hospital. Furthermore, it would
    probably make more sense to build a system for monitoring a single patient, and
    create separate instances for each patient, than invest in a single multi-patient
    system, given that each patient is likely to require separate care. In contrast,
    an automation system for a chemical plant would likely require many concurrent
    and linked processes to run and link together to provide a full picture to the
    operators.
  prefs: []
  type: TYPE_NORMAL
- en: Once you understand your requirements, you’ll be able to define whether your
    current system, whether automated or manual, is sufficient, and where it requires
    improvements. Then, once changes are made, you should be able to understand how
    your new system performs compared to the previous version, and whether the changes
    provided the desired impact.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll explore straightforward approaches and metrics that can
    be used to analyze the efficiency of your data analytics systems.
  prefs: []
  type: TYPE_NORMAL
- en: How to measure?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are various approaches to measuring the performance of data analytics
    systems, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Profiling*—Collecting detailed information about the execution of a program
    or algorithm, such as the time spent in each function or the amount of memory
    allocated. This data can help identify hot spots or areas of the code that consume
    the most resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stress testing*—Related to profiling, but under loads approaching or exceeding
    maximum expected amounts. This allows you to check if the system can still perform
    adequately if pushed to the limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Benchmarking*—Comparing the performance of a system against a reference implementation
    or industry-standard metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monitoring*—Collecting and analyzing performance metrics over time, allowing
    businesses to track the impact of optimization efforts and identify any emerging
    issues or trends in response to changing data volumes and computational demands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take an analysis example performed in chapter 4, where we used bootstrap
    methods to calculate confidence intervals for the average sales price and average
    review score per state. Bootstrapping is a computationally intensive technique,
    as it involves repeatedly drawing random samples with replacements from the original
    data, and recalculating the statistic of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several performance metrics can be used to evaluate the efficiency of a data
    analytics system. We’ll look at three of the most important areas in the rest
    of this section: execution time, throughput, and resource utilization. We will,
    however, focus mostly on the execution time. Depending on the properties and limitations
    of your computing environment, some of these areas might be of more importance
    than others.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 Execution time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Execution time is a direct indicator of your system’s overall speed and responsiveness.
    It’s defined as the time it takes to complete a specific task or process a given
    volume of data.
  prefs: []
  type: TYPE_NORMAL
- en: In CPU time, I’m young; in real time, my coffee’s cold again
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In today’s multitasking and multithreading systems, measuring execution time
    cannot be reliably performed with just a stopwatch. If your CPU is busy crunching
    numbers, and you decide to play the latest AAA game on the same machine to kill
    time, you could be disappointed with the performance of your program, even though
    it’s trying its absolute best.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, *CPU time* refers to the actual time your processor spent on
    the calculation, as opposed to handling all the other tasks, like updating your
    screen, running your antivirus in the background, and refreshing your browser
    while you’re reading the news. Invariably, this will be significantly less than
    *real time*, or elapsed time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the methods used in this chapter will rely on measuring elapsed time
    as a proxy, since it’s much easier to do and usually suffices unless you are running
    advanced analysis of execution times on very short pieces of code. However, bear
    these points in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s best to not run other intensive processes while measuring the performance
    of your calculations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When measuring performance, use relatively large dataset sizes, as this will
    make comparisons between different pieces of code more reliable by reducing the
    impact of overhead processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t be surprised if running the same code multiple times produces different
    execution times. It’s best to average the runtime of several runs to get a better
    approximation. Alternatively, take the minimum runtime from several attempts,
    as this will reflect the minimum impact of overhead on the processor.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if we can get our GPT advisor to provide us with the code required
    to measure the execution time of our bootstrap function.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I have a function `bootstrap_CI()`. `bootstrap_CI()`
    is being run from within another function, `calculate_ci(group)`, which, in turn,
    is run with the command: `ci_df = grouped_data.apply(calculate_ci)`. The `grouped_data`
    object is created as `grouped_data = filtered_df.groupby(''seller_state'')`'
  prefs: []
  type: TYPE_NORMAL
- en: Please provide code to measure the performance of the `bootstrap_CI()` function
    depending on the size of `filtered_df` DataFrame (randomly selecting appropriate
    subset), from 100 to 9944 elements, and graph it.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To measure the performance of the `bootstrap_CI()`
    function depending on the number of randomly selected entries from `filtered_df`
    DataFrame, you can create the function which uses the `sample` function from pandas,
    applies the `groupby` operation, and measures the time it takes to run the `apply`
    function with `calculate_ci` on the grouped data. Here’s the sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code measures the performance of the `calculate_ci()` function for different
    sizes of randomly-selected subsets of `filtered_df`. The execution times are plotted
    against the sizes of the subsets.
  prefs: []
  type: TYPE_NORMAL
- en: The result is presented in figure 7.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F01_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 Chart of time of executing bootstrapping function depending on the
    size of the processed DataFrame
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This metric is pretty straightforward; however, you need to remember that this
    is the narrowest metric, as its measurement is only valid for the specific input,
    system load, configurations, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, this simple example only takes into account a single measurement.
    In practice, this will not be very accurate, as modern computers are designed
    to perform many tasks concurrently, and that will have an impact on the one business-critical
    process you are trying to measure. Therefore, to isolate these variable effects,
    but still get a reliable reading, you should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform the profiling under expected loads. There is no point profiling a system
    on an isolated test server, and then expecting it to perform identically on a
    production server with 50 other processes running, competing for resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profile many runs and perform appropriate statistical analysis, calculating
    at the very least the minimum, maximum, and average times. This will give you
    a much better picture of expected performance than a single run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.1.2 Throughput
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughput is a measure of the rate at which a system can process data, typically
    measured in transactions per second or records processed per second. High throughput
    is essential for handling large data volumes and meeting real-time processing
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: As throughput is a simple division of the size of data by elapsed time, we slightly
    modified the code we used for the execution time measurement. You can find the
    full code in the Jupyter notebook in the book’s GitHub repository. The result,
    presented in figure 7.2, may be somewhat surprising.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F02_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 Dependence of the execution time (solid line) and throughput (dashed
    line) of the bootstrapping function on the size of the processed DataFrame
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Intuitively, if throughput is the number of processed records divided by the
    time of processing all records, each should be processed in the same way. It’s
    true that, if each record were processed independently and in a constant time,
    the throughput would remain constant. However, in practice, this is not always
    the case. Various factors can affect the processing time of each record and the
    overall throughput, among them record size and complexity (not our case), data
    dependencies (not our case), resource contention (maybe our case), algorithmic
    behavior (likely our case), or external factors like the network latency, disk
    speed, or other system processes.
  prefs: []
  type: TYPE_NORMAL
- en: Throughput can be a helpful tool when estimating the projected increase in resource
    requirements in relation to an increase in the amount of data to be processed.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3 Resource utilization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get a fuller view of how your code is performing, you should not just take
    into account the execution time of your code, but also observe system resources,
    like memory and storage, used during code execution. High resource utilization
    can tell you if the system is efficiently using the available resources, and it
    may also signal potential bottlenecks or resource contention.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring system resources is material for a separate book (and likely more
    than one), but some simple measurements and observations can tell you if you are
    hitting the limits, and measurements should be available for you irrespective
    of whether you use your Windows laptop (the Resource Monitor app), Linux (the
    top utility), or cloud solutions (provider-dependent).
  prefs: []
  type: TYPE_NORMAL
- en: 'Measurement of resource utilization is critical in distinguishing two primary
    limiting factors, or bottlenecks, of your program or task:'
  prefs: []
  type: TYPE_NORMAL
- en: '*CPU-bound tasks***—**These tasks are mainly limited by the processing power
    of the CPU on which your program runs. The performance of CPU-bound tasks can
    be improved by using a faster CPU or by parallelizing the task across multiple
    CPU cores. Examples of CPU-bound tasks include numerical simulations, complex
    mathematical calculations, and data processing tasks like sorting or searching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*I/O-bound tasks*—These tasks are primarily limited by input/output (I/O) operations,
    such as reading from or writing to a file (swapping data to a hard drive if you
    run out of memory), network communication, or user input. I/O-bound tasks spend
    most of their time waiting for I/O operations to complete rather than executing
    instructions on the CPU. The performance of I/O-bound tasks can be improved by
    using faster storage devices, optimizing I/O operations, or using asynchronous
    I/O to allow the program to perform other tasks while waiting for I/O operations
    to complete. Examples of I/O-bound tasks include reading large files, downloading
    data from the internet, or waiting for user input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see if we can get some quick solutions for resource measurement from our
    GPT advisor.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**How can I quickly check if my function is
    CPU-bound or IO-bound, and what is its memory usage?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To quickly determine if your function is
    CPU-bound or I/O-bound, and what is its memory usage, you can monitor the CPU
    usage, I/O activity and used memory while the function is running. Here’s a high-level
    approach to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Monitor CPU usage: Track the CPU usage during the execution of your function.
    If the CPU usage remains consistently high, it is a strong indication that your
    function is CPU-bound.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Monitor I/O activity: Track I/O operations, such as disk reads/writes and network
    activity, during the execution of your function. If you observe high I/O activity
    but relatively low CPU usage, it is a strong indication that your function is
    I/O-bound.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Monitor memory use: Track memory use during the execution of your function.
    To account only for the memory usage associated with the execution of your function,
    you can capture the memory usage before and after running the function and subtract
    the background memory usage accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use built-in operating system tools or Python libraries to monitor
    CPU, I/O activity, and memory usage. Here’s an example using the psutil library
    to monitor them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Replace `your_function()` with the function you want to analyze. This script
    will monitor the CPU usage, I/O activity and memory usage for 10 seconds with
    1-second intervals. If the average CPU usage is high, your function is likely
    CPU-bound. If the I/O activity (read and write bytes) is high but the CPU usage
    is relatively low, your function is likely I/O-bound.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that this is a simple example, and you might need to adjust the
    monitoring duration and interval to match the runtime of your function for more
    accurate results. Additionally, this approach does not account for other factors
    that could be affecting your function’s performance, such as memory consumption
    or network latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter’s Jupyter notebook in the book’s GitHub repository, we provide
    the code modified for our example. These are the results we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With this scale of data and simplicity of performed operations, the limiting
    factor is related to disk operations. In this case, we shouldn’t expect much improvement
    if we parallelize a calculation across multiple processors, although we will do
    so in the following section for illustration’s sake. Increasing CPU performance
    will likely have minimum impact if all the parallel tasks have to fight to read
    data from a single file.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring where your data analysis spends most of its time and energy is critical
    to understanding how to make it go faster. There is no point optimizing a calculation
    if the limiting factor is the data transfer bandwidth—swapping a hard drive for
    a faster one might give you a much more significant boost, costing only a few
    bucks.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Improving code performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the context of data analytics, you may encounter two terms used interchangeably:
    “code optimization” and “code scaling.” These refer to two distinct concepts.
    They are both crucial to improving the performance and efficiency of data processing
    pipelines, but it’s important to distinguish between them, as they have different
    roles in overall performance improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Code optimization* is the process of fine-tuning the algorithm, code, or data
    processing pipeline to improve its performance, speed, or resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Code scaling* refers to adapting the code to use larger or better performing
    hardware more efficiently, or to adapting it to different architectures, such
    as GPU or multithreading.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s consider some examples. In the case of the patient-monitoring system we
    mentioned in section 7.1, it is likely that each patient who requires monitoring
    will be provided with their own unit. (Many units may connect to a single monitoring
    dashboard at the nurses’ station, but the data analysis will be done independently
    for each patient, which is what counts here.) Each unit should perform the analysis
    quickly and accurately, with very high availability, or even redundancy, as a
    human life may be at stake. Code optimization may be the best approach here, as
    each unit will be self-contained, will need to be mobile so it can move with the
    patient if they are taken for tests, and will likely already be very expensive.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, a monitoring system for financial markets will likely not require
    such a focus on optimizing every line of code. To preserve the maintainability
    and usability of such a complex system, it will be much better to ensure that
    the code is clearly written and documented; adequate performance will be achieved
    by running the system on sufficiently powerful servers capable of handling the
    load. Additional capacity may then be obtained by scaling—adding more CPUs or
    network bandwidth, or introducing processing segmentation and parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will first try to optimize and then scale our `bootstrap_CI`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Optimizing code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can optimize code with various techniques, such as code refactoring, algorithmic
    improvements, or by using more efficient data structures and libraries. If it
    seems daunting, there’s a good reason. As much as code and algorithm optimization
    is a valuable activity, it’s often difficult and time-consuming if performed manually.
    Reviewing each line of code by hand and trying to optimize it while avoiding introducing
    bugs can conceivably be done for concise and simple snippets of code. For longer
    ones, more automated approaches are necessary. Let’s see if GPT models can be
    of help here.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Optimize this code, preferably without parallelization.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Name the resulting function `bootstrap_CI_optimized`.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**You can optimize the code without parallelization
    by using NumPy’s vectorized operations to generate all the bootstrap samples at
    once. Here’s an optimized version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this version, we create an index array `idx` with the shape `(n_bootstraps,
    len(data))` using `np.random.randint`. Then, we use advanced indexing to create
    the `bootstrap_samples` array, which contains all the bootstrap samples at once.
    Finally, we apply the `func` directly to the `bootstrap_samples` array along the
    specified axis. This version takes advantage of NumPy’s vectorized operations
    to efficiently compute the bootstrapped statistics without parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: It seems that despite the simplicity of the provided code (and the fact that
    it was generated by ChatGPT 4 in the first place), it looks like there was still
    room for improvement. Or was there?
  prefs: []
  type: TYPE_NORMAL
- en: Before running this code on the biggest cluster we can find, we need to check
    if the proposed change indeed brought improvements. A comparison of the execution
    time of the basic and optimized functions is presented in figure 7.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F03_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 Dependence of the execution time of base (solid line) and optimized
    (dashed line) bootstrapping functions on the size of the processed DataFrame
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Seems like code optimization did well. If you compare the scales of the two
    graphs, the time required for function execution dropped by a factor of almost
    seven. Looks like there is now much less time to drink coffee between calculation
    runs.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t trust it further than you can throw it
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While the optimized code may look great, and we are assured by the model that
    it performs the same function, it is imperative to subject the code to the same
    thorough testing as you would for human-developed code. See chapter 8 for a more
    complete discussion of the risks associated with using GPT models and the phenomenon
    of AI delusion.
  prefs: []
  type: TYPE_NORMAL
- en: Note that even though LLMs with very large or even theoretically infinite context
    sizes have been developed, it is likely that you may have to split long code into
    several prompts to fit into your particular model’s maximum context size. While
    a human expert would certainly ask for background information and a wider context,
    especially if questioned repeatedly about related concepts, ChatGPT will not.
    It’s therefore very important that you properly test the code to verify any assumptions
    you make about the output of the conversation with ChatGPT or the function of
    the final code. Also, using the iterative prompting techniques described in section
    7.3.2 may be helpful in getting ChatGPT to perform correct optimizations and prevent
    it from hallucinating too much.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 Scaling code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s assume that we are content with how our function runs on a single processor
    or thread. It may now be worthwhile to look at how it can be implemented on better-performing
    architectures, allowing you to achieve better performance still. An obvious example
    would be to invest in a faster processor or more RAM. Yet, potentially surprisingly,
    most implementations don’t take full advantage of the architectures of commonly
    available hardware. Almost all processors currently available on the market have
    multiple cores, and GPUs are installed in virtually every PC. Both of these allow
    several calculations to run in parallel, albeit with some technical differences.
    We will discuss these a bit later, but first let’s focus on how ChatGPT can help
    us utilize both.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling across multiple CPU cores
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can easily scale your code to utilize the multicore architectures available
    in virtually all modern processors with available libraries and the power of ChatGPT.
    Let’s try to parallelize our optimized bootstrap function.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Can you parallelize the following code?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Name the resulting function `bootstrap_CI_parallelized`.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**You can parallelize `bootstrap_CI_optimized`
    function using the `concurrent .futures` module. I’ll demonstrate using a `ThreadPoolExecutor`.
    Here’s the modified version of the `bootstrap_CI_optimized` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will parallelize the `bootstrap_CI_optimized` function across multiple
    threads. However, please note that if your data is stored in a Pandas DataFrame
    and you are using a custom function for `func`, you may need to handle the GIL
    (Global Interpreter Lock) in Python, which could limit the parallelization benefits
    in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have parallelized our function, it will run like a bat out of hell!
    Yeah!
  prefs: []
  type: TYPE_NORMAL
- en: The results of the run on 16 parallel threads are presented in figure 7.4\.
    The code is now considerably . . . slower! Looks like ChatGPT’s final remark was
    true.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F04_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 Dependence of the execution time of optimized (solid line) and parallelized
    (dashed line) bootstrapping functions on the size of the processed DataFrame
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Fortunately, there is a lesson to be learned here. In section 7.1, we distinguished
    between CPU-bound and I/O-bound tasks. Our `bootstrap_CI` function utilized 1.5%
    of the available processing power, and we mentioned that this function doesn’t
    seem like the best candidate for parallelization. We did it anyway, and now we’re
    suffering the consequences.
  prefs: []
  type: TYPE_NORMAL
- en: There ain’t no such thing as a free lunch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Parallelization is not free. There is an associated overhead cost related to
    dividing data into smaller chunks, starting and queuing the processes, and gluing
    the results together afterward. In our case, the main problem is the relatively
    minuscule size of the data, but this issue might also be important for large-scale
    tasks, depending on the structure of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Our ChatGPT advisor warned us that something called the Global Interpreter Lock
    (GIL) might limit our parallelization benefits. GIL is a mechanism that simplifies
    Python’s memory management but at the cost of serious limitations on the performance
    of multithreaded applications and their ability to utilize multiple CPU cores
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if moving from CPU to GPU will improve things considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling by using GPU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With certain exceptions, scaling across CPUs may provide significant benefits,
    but the GPU (graphics processing unit) is another piece of hardware available
    on most PCs nowadays. This piece of silicone is specifically optimized for performing
    a vast number of identical operations on vectors or matrices of data on hundreds
    of cores in parallel, a characteristic required for most computer-generated graphics
    calculations.
  prefs: []
  type: TYPE_NORMAL
- en: There is nothing preventing GPUs being used for calculations that have similar
    requirements but are not related to graphics. It is important, however, to understand
    that a GPU and CPU are very different, as we’ll discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s see how a GPU might be used in a simple example. The following
    code and techniques below use an Nvidia graphics card with a CUDA interface, allowing
    us to direct the code to utilize an Nvidia GPU for scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Can you adjust the `bootstrap_CI_optimized`
    function to run with CUDA? Name the resulting function `bootstrap_CI_cuda`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To run the `bootstrap_CI_optimized` function
    on CUDA, you can use the cupy library, which provides GPU-accelerated versions
    of many NumPy functions. First, install the cupy library if you haven’t already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, modify the `bootstrap_CI_optimized` function to use `cupy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the `func` argument must also be compatible with cupy arrays.
    If the `func` relies on NumPy functions, you may need to replace them with their
    equivalent cupy functions.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that, depending on your problem size and GPU, you might not see
    significant performance improvements. GPU acceleration tends to be more beneficial
    for larger data and more computationally intensive operations.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT’s final remark certainly doesn’t bring a lot of optimism, especially
    after our parallelization effort. Nevertheless, we need to stop delaying the inevitable
    and move to the results presented in figure 7.5\. Fortunately, our effort this
    time was not in vain. We can see a significant improvement in the code execution
    time.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F05_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 Dependence of the execution time of optimized (solid line) and GPU-utilizing
    (dashed line) bootstrapping functions on the size of the processed DataFrame
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To infinity and beyond
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: GPUs are not the end of the line, at least not for all applications. Google
    designed hardware specialized for high-performance machine learning tasks, particularly
    deep learning, called Tensor Processing Units (TPUs).
  prefs: []
  type: TYPE_NORMAL
- en: TPUs are particularly well-suited for large-scale training and inference of
    neural networks, offering faster processing times and better energy efficiency
    compared to CPUs and GPUs. They can be especially beneficial for natural language
    processing, computer vision, and reinforcement learning applications. Incorporating
    TPUs into your data analytics pipeline can further optimize performance and reduce
    the time required to train and deploy complex machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Following the piece of advice from our last conversation with ChatGPT, you should
    be aware that many popular Python libraries already have counterparts optimized
    for GPUs and TPUs (Tensor Processing Units) or provide built-in support to accelerate
    computations. Keep that in the back of your mind if you’re planning to utilize
    GPUs/TPUs in your work.
  prefs: []
  type: TYPE_NORMAL
- en: There is no need to memorize all these optimized libraries, as you can ask ChatGPT
    to provide you with a list, as in the following transcript. Similarly, you can
    ask ChatGPT to reference other implementations with specific niche uses or optimizations.
    It could save you hours of looking through the web.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Which Python libraries have GPU-optimized
    counterparts?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Many popular Python libraries have GPU-optimized
    counterparts or provide built-in GPU support to accelerate computations. Some
    of these libraries include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CuPy: A GPU-accelerated version of NumPy that runs on NVIDIA CUDA. It has a
    very similar interface to NumPy, making it easy to port existing code to CuPy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Numba: A Just-In-Time (JIT) compiler for Python functions that can generate
    GPU-accelerated code for NVIDIA CUDA-enabled GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CuPy: CuPy also provides GPU-accelerated versions of some SciPy functions,
    such as sparse matrix operations and linear algebra routines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ArrayFire: A high-performance library that provides GPU-optimized versions
    of some SciPy functions for NVIDIA, AMD, and Intel GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: 'cuDF: A part of RAPIDS, cuDF is a GPU-accelerated library for data manipulation
    and analysis, providing a pandas-like API for handling data on NVIDIA CUDA-enabled
    GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dask: While not strictly a GPU-optimized counterpart, Dask can work with cuDF
    to parallelize and distribute data processing tasks on GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'cuML: Also part of RAPIDS, cuML is a GPU-accelerated library for machine learning
    that provides an API compatible with scikit-learn. It supports various algorithms
    like linear regression, k-means clustering, and principal component analysis,
    optimized for NVIDIA CUDA-enabled GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dask-ML: Dask-ML extends Dask to parallelize machine learning algorithms, including
    support for GPU-accelerated libraries like CuPy and cuDF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow and PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: Both TensorFlow and PyTorch are popular deep learning libraries that provide
    built-in support for GPU acceleration using NVIDIA CUDA and cuDNN libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JAX:'
  prefs: []
  type: TYPE_NORMAL
- en: JAX is a library for composable transformations of Python+NumPy programs, including
    automatic differentiation, vectorization, and parallelization. It has built-in
    support for GPU acceleration using NVIDIA CUDA and also supports TPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of Python libraries with optimized counterparts
    or built-in GPU/TPU support. Depending on your specific use case and hardware,
    you might find other libraries that can leverage specialized chips for accelerated
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT learning cutoff
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As stated several times before, ChatGPT has limited knowledge beyond a certain
    point, depending on the version, so its knowledge of the latest developments and
    most recent versions of libraries may be limited. Nevertheless, it should provide
    a good starting point for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that GPU/TPU optimizations can have varying levels of performance
    improvements, so it’s essential to profile and test your code on the target hardware
    to ensure that you’re getting the desired performance gains.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling across CPUs vs. GPUs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The decision to scale operations across CPUs or GPUs depends on the task’s nature
    and the available hardware resources. Here’s a general guideline for when to use
    CPUs and GPUs for different types of operations.
  prefs: []
  type: TYPE_NORMAL
- en: Use CPUs for
  prefs: []
  type: TYPE_NORMAL
- en: Sequential tasks or tasks that involve branching or complex control structures,
    or that require a limited amount of parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks that have high I/O requirements or involve extensive communication between
    processes or threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks with a small or moderate dataset size, as, even though the available memory
    on CPUs is typically larger than that on GPUs, processing of large datasets on
    CPUs may be less effective than on GPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General-purpose tasks, as CPUs are designed to handle a wide variety of workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use GPUs for
  prefs: []
  type: TYPE_NORMAL
- en: Highly parallel tasks that can be divided into thousands of smaller tasks to
    be executed simultaneously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks involving many arithmetic operations, such as matrix multiplications,
    convolutions, or element-wise operations on large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks with large dataset sizes, as GPUs are designed to handle massive amounts
    of data quickly due to their high memory bandwidth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning, computer vision, or scientific simulations, which can benefit
    from the specialized architecture of GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, if the task can be broken down into many simpler, preferably similar,
    subtasks, and requires a large amount of computation, it’s likely better suited
    for a GPU. However, if the task involves complex control structures, extensive
    communication, or is I/O-bound, it’s likely better suited for a CPU. Additionally,
    you need to consider the available hardware resources, the efficiency of the libraries
    being used, and your task’s specific requirements before deciding whether to use
    CPUs or GPUs for your operations.
  prefs: []
  type: TYPE_NORMAL
- en: There is a very useful heuristic when deciding which approach to use. Unless
    you are prepared to rewrite large portions of the code to include parallelism,
    consider which libraries you use in your original code and whether they already
    have parallelized versions. If you are a heavy user of libraries with GPU-optimized
    counterparts and have the relevant GPU hardware available, it’s usually best to
    go with those. Otherwise, consider sticking with CPU parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: You can also combine both approaches, switching to using GPU-optimized libraries
    in the sections of the code most involved with calling these and then parallelizing
    the rest of the code using multiple CPUs. Be careful, though. As we showed in
    section 7.2.2, any parallelization comes with overhead, especially when communication
    or synchronization between different CPU cores is involved. You may end up with
    overheads far exceeding any benefits.
  prefs: []
  type: TYPE_NORMAL
- en: When you run out of PC cores, you may need to turn to cloud solutions. Those
    benefits come at the price of some extra work on your part to deploy your solution
    to your friendly cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Cloud-based deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As data continues to grow in volume, variety, and complexity, it may exceed
    the computing power of your PC machine. Instead of waiting unreasonably long to
    see the output (or a memory overflow error) or splashing out on the latest overpriced
    chips when they come into stock after being sold out to scalpers, you can reach
    into the vast power of cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 What is cloud computing?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cloud computing is a technology model that enables users to access and utilize
    computing resources, such as storage, processing power, and software applications,
    over the internet. Instead of investing in and maintaining physical hardware and
    software on-premises, users can leverage cloud service providers. The providers
    manage and maintain the infrastructure in large, remote data centers, providing
    on-demand access to a wide range of computing resources, storage, and managed
    services. You can scale your workload up or down as needed and analyze massive
    amounts of data without investing in expensive hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantages of cloud computing include scalability, flexibility, cost-efficiency,
    and reliability. You can also choose to use exotic architectures like TPUs (or
    GPUs) from the safe confines of your laptop.
  prefs: []
  type: TYPE_NORMAL
- en: 1+1 ≠ 2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Multiple interconnected computers, known as nodes, working together as a single
    unified system to perform complex computational tasks are referred to as *clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: In cluster computing, tasks are often divided into smaller subtasks and distributed
    across the nodes in the cluster. The nodes then work in parallel, processing their
    assigned subtasks simultaneously, with specific processes providing synchronization
    and fault-tolerance to the whole system. Since this is a complex topic, we won’t
    be delving deeper into these issues, assuming instead that users will be more
    likely to utilize one of the established frameworks that provide solutions to
    these issues than write their own.
  prefs: []
  type: TYPE_NORMAL
- en: Common cluster computing software frameworks include Apache Hadoop, Apache Spark,
    and the Message Passing Interface (MPI), which provide tools and libraries to
    simplify the development, deployment, and management of distributed applications
    across cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The code we used as an example to compare performance didn’t turn out to be
    the best candidate for parallelization. In this section, we will work with a function
    calculating straight-line (or Euclidean) distances between sets of points. It
    may not be very useful for most business cases, but it parallelizes beautifully—each
    pair’s distance can be calculated independently, so no communication between processors
    or nodes will be required, apart from gathering the final results.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Moving your code to the cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The simplest cloud computing use case is to deploy your code to a virtual machine
    (VM) in the cloud. The VM is like a computer within a computer, with its own operating
    system and all. It acts as an isolated space, enabling you to test software, run
    different applications, or work in separate environments without affecting the
    main system. You run your analytics on a VM just as you would on your local PC.
    ChatGPT can provide you with complete instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**How can I deploy my calculation to the cloud?
    Provide me with a walkthrough.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the exact steps change frequently, we’ve decided to omit the lengthy transcript
    here. It should suffice to say that ChatGPT can provide detailed walkthroughs,
    but only up to the point where the provider changes the interface and the steps
    differ. Caveat lector (reader beware).
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Depending on your specific use case and
    requirements, you might need to customize the environment, configure additional
    resources, or use other cloud services, such as managed data processing services
    or serverless computing options like Google Cloud Functions or AWS Lambda.'
  prefs: []
  type: TYPE_NORMAL
- en: You can ask ChatGPT about the details of each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT’s narrow context
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You’ll notice that, in the discussion with ChatGPT on the deployment of code
    to the cloud, the response only provided a way to run the code on a single virtual
    machine, since the prompt didn’t specify anything otherwise. This kind of deployment
    is usually the first step in moving your application to the cloud, so it’s still
    worth looking at, although this will usually provide only a moderate increase
    in computational power, limited by the power of a single VM, while having the
    significant benefit of requiring no changes to the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s look at how ChatGPT can help us take our code into the cloud,
    this time properly distributing it across multiple cloud resources so that we
    can harness the vast power of cluster computing. Let’s look at the solution proposed
    by ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is not loyal to one platform
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the responses in this section, you’ll see ChatGPT starting to answer using
    Amazon AWS. The prompt did not specify the choice of the cloud provider, although
    we have discussed using Google Cloud before. Don’t be afraid to stop ChatGPT if
    you don’t like where it is going and redirect it on a path you prefer by refining
    the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I now want to distribute the calculation
    across the cloud. How can I do that? (Forget saving to disk.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To distribute the calculation across the
    cloud, you can use Dask along with a cloud provider like Amazon Web Services (AWS),
    Google Cloud Platform (GCP), or Microsoft Azure. In this example, we’ll use Dask
    with AWS, but the process is similar for other cloud providers.'
  prefs: []
  type: TYPE_NORMAL
- en: First, you’ll need to set up a Dask cluster on AWS. You can follow the official
    Dask guide for setting up a Dask cluster on AWS using the dask-cloudprovider package.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve set up your Dask cluster
  prefs: []
  type: TYPE_NORMAL
- en: We stopped the response generation at this point.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Please change the last answer to use GCP'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To distribute the calculation across the
    cloud using Google Cloud Platform (GCP), you can use Dask along with the dask-cloudprovider
    package to create and manage Dask clusters on GCP.'
  prefs: []
  type: TYPE_NORMAL
- en: <Here, we again received an answer applicable at the time of ChatGPTs knowledge
    cutoff.>
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the code is similar to the previous examples but now uses GCP cluster
    as the underlying infrastructure. The key point here is that there are ready-made
    solutions, as mentioned in the previous sections, which can abstract away the
    technicalities of cloud or cluster computing and allow you to focus on coding
    the core problem rather than worrying whether the network communication is going
    to work correctly. Frameworks like Dask can provide these solutions and even allow
    you to switch almost seamlessly between different low-level providers like AWS
    or GCP.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to remember that by submitting code and prompts requesting different
    iterations of code, we may end up with an implementation containing a mixture
    of the different solutions, or we may end up with totally different functionality.
    Whatever modifications we request ChatGPT to make to the code, it’s vital that
    it is retested with the same, or even higher, scrutiny as if it were written from
    scratch. Furthermore, since parallelized or distributed code is usually much harder
    to test fully than a localized single-threaded version, care should be taken to
    apply best practices, which are beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Code conversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, let’s look at using ChatGPT to convert code between programming languages.
    This can serve multiple purposes, such as generating optimized code or adapting
    code to work within specific frameworks like Hadoop. When translating code from
    one language to another, developers have the opportunity to refactor, streamline,
    and improve the code in the target language. It can lead to optimization in several
    ways. Let’s look at some of these:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Improved performance**—*Converting code to a more efficient language or one
    that offers better support for specific hardware or platforms can lead to performance
    gains. For example, translating code from a high-level interpreted language like
    Python to a lower-level compiled language like C++ can result in faster execution
    times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reduced memory usage**—*Code conversion can lead to more memory-efficient
    implementations. For instance, certain data structures and algorithms in the target
    language might consume less memory than their counterparts in the source language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Better maintainability**—*Converting code to a more modern or widely used
    language can improve maintainability by leveraging newer language features, libraries,
    and community support. This can make the code easier to read, understand, and
    modify in the long run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Framework compatibility**—*Ensuring code compatibility with different frameworks
    often requires converting it to a supported language. For instance, in the next
    section, we’ll be discussing distributing the processing to the cloud, and one
    of the options would be to use Hadoop. However, when working with Hadoop, you
    might need to convert a Python script to Java or Scala to utilize the framework’s
    ecosystem efficiently. This ensures seamless integration with Hadoop’s Java-based
    libraries and tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the frameworks and compatibility restrictions change fairly often, it is
    best to check the current documentation online to make informed decisions regarding
    which framework is the best option for your specific implementation.
  prefs: []
  type: TYPE_NORMAL
- en: It’s essential to carefully consider the cost and complexity of code conversion,
    as it can be time-consuming and may introduce new bugs or issues, and manual code
    conversion is rarely worth the effort. Although automated tools for code conversion
    have existed before, the use of models like ChatGPT has made code conversion much
    easier and may actually bring sizable benefits even with significant chunks of
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Limited trust still applies
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Please remember that even though it may look like the code conversion is easy,
    thorough testing and performance monitoring are still necessary to ensure the
    translated code works as expected and meets the desired optimization goals. Techniques
    linked to unit testing, system testing, and integration testing, which are beyond
    the scope of this book, should be applied both before and after changing the code
    to reduce the risk of bugs creeping in.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some examples of code conversion using ChatGPT. Imagine you wrote
    the code in Python for one of your analyses but are now looking to reuse the same
    concept in a different part of the organization that has everything else written
    in Java. Although there are solutions for interoperability between the two languages,
    converting the code directly to Java might be the simplest solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Take this code and convert it into Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**![image](../Images/Response.png)**Here’s the equivalent Java code for the
    provided Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the Apache Commons Math library for the mean calculation, replacing
    the custom mean calculation from the previous example. Adjust the `calculateStatistics`
    method if you have a different custom statistics calculation logic.
  prefs: []
  type: TYPE_NORMAL
- en: It may be interesting at this point to investigate how much ChatGPT “understands”
    the code it is converting. It’s interesting to see how ChatGPT might be able to
    conceptualize the function of the code, which may lead to more efficient conversion
    with the right prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**What does this code do?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**This code defines a Java utility class,
    `BootstrapUtils`, that provides a method `bootstrapCI` for calculating bootstrap
    confidence intervals (CI) for a given dataset. Let’s break down the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code imports classes from the Apache Commons Math library for statistical
    calculations, including random data generation and percentile calculation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`BootstrapUtils` Class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Contains two methods: `bootstrapCI` and `calculateStatistics`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`bootstrapCI` Method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Takes three parameters: an array of data, the number of bootstrap samples (`nBootstraps`),
    and the confidence interval percentage (`ci`).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Creates a random data generator and a `Percentile` object with a specific estimation
    type.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializes an array to store bootstrap statistics and gets the size of the
    input data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bootstrap Sampling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterates `nBootstraps` times.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each iteration, it creates a bootstrap sample by randomly selecting data
    points from the original dataset.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Statistics Calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calls the `calculateStatistics` method to calculate the statistics of each bootstrap
    sample. The default implementation uses the mean calculation from the Apache Commons
    Math library.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Confidence Interval Calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses the `Percentile` object to calculate the lower and upper bounds of the
    confidence interval based on the percentiles of the bootstrapped statistics.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Return:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Returns an array containing the lower and upper bounds of the bootstrap confidence
    interval.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`calculateStatistics` Method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A private method for calculating statistics on a given array. The default implementation
    uses the mean calculation from the Apache Commons Math library.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`main` Method (Example Usage):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Demonstrates how to use the `bootstrapCI` method with an example dataset (`data`).
    It calculates the bootstrap confidence interval with 1000 bootstraps and a 95%
    confidence interval.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In summary, this Java code provides a flexible and reusable utility for calculating
    bootstrap confidence intervals. You can replace the `calculateStatistics` method
    with your custom statistics calculation logic if needed. The code is well-organized
    and modular, making it easy to understand and extend for different statistical
    calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve extracted the function of the code from ChatGPT, we can ask it
    for a more optimal solution in our target language. Please note that the prompt
    asking ChatGPT for the description of the function of the code is important in
    the flow of the conversation, as it steers ChatGPT toward the correct understanding
    of our intention to convert the function of the code rather than translate it
    instruction-by-instruction. Furthermore, it allows us to check if ChatGPT’s understanding
    of the function is correct. If it were not correct, we might adapt subsequent
    prompts to steer it toward the right output. In our case, it deduced the function
    correctly, so we can just ask it for an optimized solution in the next prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Can this be done using some optimized implementation?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Certainly! If you’re looking for optimized
    implementations, Java has libraries that provide efficient statistical calculations.
    One such library is the Apache Commons Math library. Here’s the modified code
    using Apache Commons Math for both bootstrap sampling and statistics calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This version uses `org.apache.commons.math3.stat.StatUtils.mean` for the mean
    calculation, which is an optimized implementation from Apache Commons Math. Make
    sure to include the Apache Commons Math library in your project dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding exchange, ChatGPT has translated the code directly. Further
    interrogation provided even better alternatives, such as by asking ChatGPT to
    replace parts of the code with optimized implementations from other libraries
    available for Java, although it’s up to you to explore this. Don’t leave it to
    the machine to do *all* the work.
  prefs: []
  type: TYPE_NORMAL
- en: By this point, we trust you’ve gained an understanding of how you can use generative
    AI to choose, implement, and assess various approaches to enhance the efficiency
    of the code employed in your analysis. However, it’s crucial to exercise discretion
    in choosing your optimization battles. If your analysis is a single, ad hoc occurrence,
    the time required to achieve optimal performance might surpass the potential time
    savings.
  prefs: []
  type: TYPE_NORMAL
- en: Things to ask ChatGPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Modify my code to run on multiple GPUs/CPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert this snippet I downloaded from StackOverflow to my favorite programming
    language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain what this code snippet does.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify my code to run on the Amazon/Google/Microsoft Azure cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Don’t try to re-implement common algorithms, as there are usually optimized
    implementations available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to use a different programming language, ChatGPT can significantly
    simplify migration. Just remember to retest the code thoroughly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code should be optimized to achieve results as quickly as possible on a given
    platform. If this is not fast enough, you can scale up by using multiple CPUs
    or GPUs or deploying to the cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frameworks like Dask can help you achieve scaling with minimum modifications
    to your code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
