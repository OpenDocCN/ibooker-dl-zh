["```py\nimport keras\n\nfilename = keras.utils.get_file(\n    origin=(\n        \"https://storage.googleapis.com/download.tensorflow.org/\"\n        \"data/shakespeare.txt\"\n    ),\n)\nshakespeare = open(filename, \"r\").read() \n```", "```py\n>>> shakespeare[:250]\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n```", "```py\nimport tensorflow as tf\n\n# The chunk size we will use during training. We only train on\n# sequences of 100 characters at a time.\nsequence_length = 100\n\ndef split_input(input, sequence_length):\n    for i in range(0, len(input), sequence_length):\n        yield input[i : i + sequence_length]\n\nfeatures = list(split_input(shakespeare[:-1], sequence_length))\nlabels = list(split_input(shakespeare[1:], sequence_length))\ndataset = tf.data.Dataset.from_tensor_slices((features, labels)) \n```", "```py\n>>> x, y = next(dataset.as_numpy_iterator())\n>>> x[:50], y[:50]\n(b\"First Citizen:\\nBefore we proceed any further, hear\",\n b\"irst Citizen:\\nBefore we proceed any further, hear \")\n```", "```py\nfrom keras import layers\n\ntokenizer = layers.TextVectorization(\n    standardize=None,\n    split=\"character\",\n    output_sequence_length=sequence_length,\n)\ntokenizer.adapt(dataset.map(lambda text, labels: text)) \n```", "```py\n>>> vocabulary_size = tokenizer.vocabulary_size()\n>>> vocabulary_size\n67\n```", "```py\ndataset = dataset.map(\n    lambda features, labels: (tokenizer(features), tokenizer(labels)),\n    num_parallel_calls=8,\n)\ntraining_data = dataset.shuffle(10_000).batch(64).cache() \n```", "```py\nembedding_dim = 256\nhidden_dim = 1024\n\ninputs = layers.Input(shape=(sequence_length,), dtype=\"int\", name=\"token_ids\")\nx = layers.Embedding(vocabulary_size, embedding_dim)(inputs)\nx = layers.GRU(hidden_dim, return_sequences=True)(x)\nx = layers.Dropout(0.1)(x)\n# Outputs a probability distribution over all potential tokens in our\n# vocabulary\noutputs = layers.Dense(vocabulary_size, activation=\"softmax\")(x)\nmodel = keras.Model(inputs, outputs) \n```", "```py\n>>> model.summary()\nModel: \"functional\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ token_ids (InputLayer)            │ (None, 100)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ embedding (Embedding)             │ (None, 100, 256)         │        17,152 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ gru (GRU)                         │ (None, 100, 1024)        │     3,938,304 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout (Dropout)                 │ (None, 100, 1024)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense (Dense)                     │ (None, 100, 67)          │        68,675 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 4,024,131 (15.35 MB)\n Trainable params: 4,024,131 (15.35 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"sparse_categorical_accuracy\"],\n)\nmodel.fit(training_data, epochs=20) \n```", "```py\n# Creates a model that receives and outputs the RNN state\ninputs = keras.Input(shape=(1,), dtype=\"int\", name=\"token_ids\")\ninput_state = keras.Input(shape=(hidden_dim,), name=\"state\")\n\nx = layers.Embedding(vocabulary_size, embedding_dim)(inputs)\nx, output_state = layers.GRU(hidden_dim, return_state=True)(\n    x, initial_state=input_state\n)\noutputs = layers.Dense(vocabulary_size, activation=\"softmax\")(x)\ngeneration_model = keras.Model(\n    inputs=(inputs, input_state),\n    outputs=(outputs, output_state),\n)\n# Copies the parameters from the original model\ngeneration_model.set_weights(model.get_weights()) \n```", "```py\ntokens = tokenizer.get_vocabulary()\ntoken_ids = range(vocabulary_size)\nchar_to_id = dict(zip(tokens, token_ids))\nid_to_char = dict(zip(token_ids, tokens))\n\nprompt = \"\"\"\nKING RICHARD III:\n\"\"\" \n```", "```py\ninput_ids = [char_to_id[c] for c in prompt]\nstate = keras.ops.zeros(shape=(1, hidden_dim))\nfor token_id in input_ids:\n    inputs = keras.ops.expand_dims([token_id], axis=0)\n    # Feeds the prompt character by character to update state\n    predictions, state = generation_model.predict((inputs, state), verbose=0) \n```", "```py\nimport numpy as np\n\ngenerated_ids = []\nmax_length = 250\n# Generates characters one by one, computing a new state each iteration\nfor i in range(max_length):\n    # The next character is the output index with the highest\n    # probability.\n    next_char = int(np.argmax(predictions, axis=-1)[0])\n    generated_ids.append(next_char)\n    inputs = keras.ops.expand_dims([next_char], axis=0)\n    predictions, state = generation_model.predict((inputs, state), verbose=0) \n```", "```py\noutput = \"\".join([id_to_char[token_id] for token_id in generated_ids])\nprint(prompt + output) \n```", "```py\nKING RICHARD III:\nStay, men! hear me speak.\n\nFRIAR LAURENCE:\nThou wouldst have done thee here that he hath made for them?\n\nBUCKINGHAM:\nWhat straight shall stop his dismal threatening son,\nThou bear them both. Here comes the king;\nThough I be good to put a wife to him, \n```", "```py\nimport pathlib\n\nzip_path = keras.utils.get_file(\n    origin=(\n        \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n    ),\n    fname=\"spa-eng\",\n    extract=True,\n)\ntext_path = pathlib.Path(zip_path) / \"spa-eng\" / \"spa.txt\" \n```", "```py\nwith open(text_path) as f:\n    lines = f.read().split(\"\\n\")[:-1]\ntext_pairs = []\nfor line in lines:\n    english, spanish = line.split(\"\\t\")\n    spanish = \"[start] \" + spanish + \" [end]\"\n    text_pairs.append((english, spanish)) \n```", "```py\n>>> import random\n>>> random.choice(text_pairs)\n(\"Who is in this room?\", \"[start] ¿Quién está en esta habitación? [end]\")\n```", "```py\nimport random\n\nrandom.shuffle(text_pairs)\nval_samples = int(0.15 * len(text_pairs))\ntrain_samples = len(text_pairs) - 2 * val_samples\ntrain_pairs = text_pairs[:train_samples]\nval_pairs = text_pairs[train_samples : train_samples + val_samples]\ntest_pairs = text_pairs[train_samples + val_samples :] \n```", "```py\nimport string\nimport re\n\nstrip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n    )\n\nvocab_size = 15000\nsequence_length = 20\n\nenglish_tokenizer = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\nspanish_tokenizer = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    standardize=custom_standardization,\n)\ntrain_english_texts = [pair[0] for pair in train_pairs]\ntrain_spanish_texts = [pair[1] for pair in train_pairs]\nenglish_tokenizer.adapt(train_english_texts)\nspanish_tokenizer.adapt(train_spanish_texts) \n```", "```py\nbatch_size = 64\n\ndef format_dataset(eng, spa):\n    eng = english_tokenizer(eng)\n    spa = spanish_tokenizer(spa)\n    features = {\"english\": eng, \"spanish\": spa[:, :-1]}\n    labels = spa[:, 1:]\n    sample_weights = labels != 0\n    return features, labels, sample_weights\n\ndef make_dataset(pairs):\n    eng_texts, spa_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    spa_texts = list(spa_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n    return dataset.shuffle(2048).cache()\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs) \n```", "```py\n>>> inputs, targets, sample_weights = next(iter(train_ds))\n>>> print(inputs[\"english\"].shape)\n(64, 20)\n>>> print(inputs[\"spanish\"].shape)\n(64, 20)\n>>> print(targets.shape)\n(64, 20)\n>>> print(sample_weights.shape)\n(64, 20)\n```", "```py\ninputs = keras.Input(shape=(sequence_length,), dtype=\"int32\")\nx = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\nx = layers.LSTM(32, return_sequences=True)(x)\noutputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\nmodel = keras.Model(inputs, outputs) \n```", "```py\nembed_dim = 256\nhidden_dim = 1024\n\nsource = keras.Input(shape=(None,), dtype=\"int32\", name=\"english\")\nx = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\nrnn_layer = layers.GRU(hidden_dim)\nrnn_layer = layers.Bidirectional(rnn_layer, merge_mode=\"sum\")\nencoder_output = rnn_layer(x) \n```", "```py\ntarget = keras.Input(shape=(None,), dtype=\"int32\", name=\"spanish\")\nx = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(target)\nrnn_layer = layers.GRU(hidden_dim, return_sequences=True)\nx = rnn_layer(x, initial_state=encoder_output)\nx = layers.Dropout(0.5)(x)\n# Predicts the next word of the translation, given the current word\ntarget_predictions = layers.Dense(vocab_size, activation=\"softmax\")(x)\nseq2seq_rnn = keras.Model([source, target], target_predictions) \n```", "```py\n>>> seq2seq_rnn.summary()\nModel: \"functional_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ english (InputLayer)  │ (None, None)      │           0 │ -                  │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ spanish (InputLayer)  │ (None, None)      │           0 │ -                  │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embedding_1           │ (None, None, 256) │   3,840,000 │ english[0][0]      │\n│ (Embedding)           │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ not_equal (NotEqual)  │ (None, None)      │           0 │ english[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embedding_2           │ (None, None, 256) │   3,840,000 │ spanish[0][0]      │\n│ (Embedding)           │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ bidirectional         │ (None, 1024)      │   7,876,608 │ embedding_1[0][0], │\n│ (Bidirectional)       │                   │             │ not_equal[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ gru_2 (GRU)           │ (None, None,      │   3,938,304 │ embedding_2[0][0], │\n│                       │ 1024)             │             │ bidirectional[0][… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_1 (Dropout)   │ (None, None,      │           0 │ gru_2[0][0]        │\n│                       │ 1024)             │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_1 (Dense)       │ (None, None,      │  15,375,000 │ dropout_1[0][0]    │\n│                       │ 15000)            │             │                    │\n└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n Total params: 34,869,912 (133.02 MB)\n Trainable params: 34,869,912 (133.02 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nseq2seq_rnn.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    weighted_metrics=[\"accuracy\"],\n)\nseq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds) \n```", "```py\nimport numpy as np\n\nspa_vocab = spanish_tokenizer.get_vocabulary()\nspa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n\ndef generate_translation(input_sentence):\n    tokenized_input_sentence = english_tokenizer([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(sequence_length):\n        tokenized_target_sentence = spanish_tokenizer([decoded_sentence])\n        inputs = [tokenized_input_sentence, tokenized_target_sentence]\n        next_token_predictions = seq2seq_rnn.predict(inputs, verbose=0)\n        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n        sampled_token = spa_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor _ in range(5):\n    input_sentence = random.choice(test_eng_texts)\n    print(\"-\")\n    print(input_sentence)\n    print(generate_translation(input_sentence)) \n```", "```py\n-\nYou know that.\n[start] tú lo sabes [end]\n-\n\"Thanks.\" \"You're welcome.\"\n[start] gracias tú [UNK] [end]\n-\nThe prisoner was set free yesterday.\n[start] el plan fue ayer a un atasco [end]\n-\nI will tell you tomorrow.\n[start] te lo voy mañana a decir [end]\n-\nI think they're happy.\n[start] yo creo que son felices [end] \n```", "```py\nscores = [score(target, source) for source in sources]\nscores = softmax(scores)\ncombined = np.sum(scores * sources) \n```", "```py\ndef dot_product_attention(target, source):\n    # Takes the dot-product between all target and source vectors,\n    # where b = batch size, t = target length, s = source length, and d\n    # = vector size\n    scores = np.einsum(\"btd,bsd->bts\", target, source)\n    scores = softmax(scores, axis=-1)\n    # Computes a weighted sum of all source vectors for each target\n    # vector\n    return np.einsum(\"bts,bsd->btd\", scores, source)\n\ndot_product_attention(target, source) \n```", "```py\nquery_dense = layers.Dense(dim)\nkey_dense = layers.Dense(dim)\nvalue_dense = layers.Dense(dim)\noutput_dense = layers.Dense(dim)\n\ndef parameterized_attention(query, key, value):\n    query = query_dense(query)\n    key = key_dense(key)\n    value = value_dense(value)\n    scores = np.einsum(\"btd,bsd->bts\", query, key)\n    scores = softmax(scores, axis=-1)\n    outputs = np.einsum(\"bts,bsd->btd\", scores, value)\n    return output_dense(outputs)\n\nparameterized_attention(query=target, key=source, value=source) \n```", "```py\nquery_dense = [layers.Dense(head_dim) for i in range(num_heads)]\nkey_dense = [layers.Dense(head_dim) for i in range(num_heads)]\nvalue_dense = [layers.Dense(head_dim) for i in range(num_heads)]\noutput_dense = layers.Dense(head_dim * num_heads)\n\ndef multi_head_attention(query, key, value):\n    head_outputs = []\n    for i in range(num_heads):\n        query = query_dense[i](query)\n        key = key_dense[i](key)\n        value = value_dense[i](value)\n        scores = np.einsum(\"btd,bsd->bts\", target, source)\n        scores = softmax(scores / math.sqrt(head_dim), axis=-1)\n        head_output = np.einsum(\"bts,bsd->btd\", scores, source)\n        head_outputs.append(head_output)\n    outputs = ops.concatenate(head_outputs, axis=-1)\n    return output_dense(outputs)\n\nmulti_head_attention(query=target, key=source, value=source) \n```", "```py\nmulti_head_attention = keras.layers.MultiHeadAttention(\n    num_heads=num_heads,\n    head_dim=head_dim,\n)\nmulti_head_attention(query=target, key=source, value=source) \n```", "```py\nmulti_head_attention(key=source, value=source, query=source) \n```", "```py\nclass TransformerEncoder(keras.Layer):\n    def __init__(self, hidden_dim, intermediate_dim, num_heads):\n        super().__init__()\n        key_dim = hidden_dim // num_heads\n        # Self-attention layers\n        self.self_attention = layers.MultiHeadAttention(num_heads, key_dim)\n        self.self_attention_layernorm = layers.LayerNormalization()\n        # Feedforward layers\n        self.feed_forward_1 = layers.Dense(intermediate_dim, activation=\"relu\")\n        self.feed_forward_2 = layers.Dense(hidden_dim)\n        self.feed_forward_layernorm = layers.LayerNormalization()\n\n    def call(self, source, source_mask):\n        # Self-attention computation\n        residual = x = source\n        mask = source_mask[:, None, :]\n        x = self.self_attention(query=x, key=x, value=x, attention_mask=mask)\n        x = x + residual\n        x = self.self_attention_layernorm(x)\n        # Feedforward computation\n        residual = x\n        x = self.feed_forward_1(x)\n        x = self.feed_forward_2(x)\n        x = x + residual\n        x = self.feed_forward_layernorm(x)\n        return x \n```", "```py\n# Input shape: (batch_size, sequence_length, embedding_dim)\ndef layer_normalization(batch_of_sequences):\n    # To compute mean and variance, we only pool data over the last\n    # axis.\n    mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n    variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n    return (batch_of_sequences - mean) / variance \n```", "```py\n# Input shape: (batch_size, height, width, channels)\ndef batch_normalization(batch_of_images):\n    # Pools data over the batch axis (axis 0), which creates\n    # interactions between samples in a batch\n    mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2))\n    variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2))\n    return (batch_of_images - mean) / variance \n```", "```py\nclass TransformerDecoder(keras.Layer):\n    def __init__(self, hidden_dim, intermediate_dim, num_heads):\n        super().__init__()\n        key_dim = hidden_dim // num_heads\n        # Self-attention layers\n        self.self_attention = layers.MultiHeadAttention(num_heads, key_dim)\n        self.self_attention_layernorm = layers.LayerNormalization()\n        # Cross-attention layers\n        self.cross_attention = layers.MultiHeadAttention(num_heads, key_dim)\n        self.cross_attention_layernorm = layers.LayerNormalization()\n        # Feedforward layers\n        self.feed_forward_1 = layers.Dense(intermediate_dim, activation=\"relu\")\n        self.feed_forward_2 = layers.Dense(hidden_dim)\n        self.feed_forward_layernorm = layers.LayerNormalization()\n\n    def call(self, target, source, source_mask):\n        # Self-attention computation\n        residual = x = target\n        x = self.self_attention(query=x, key=x, value=x, use_causal_mask=True)\n        x = x + residual\n        x = self.self_attention_layernorm(x)\n        # Cross-attention computation\n        residual = x\n        mask = source_mask[:, None, :]\n        x = self.cross_attention(\n            query=x, key=source, value=source, attention_mask=mask\n        )\n        x = x + residual\n        x = self.cross_attention_layernorm(x)\n        # Feedforward computation\n        residual = x\n        x = self.feed_forward_1(x)\n        x = self.feed_forward_2(x)\n        x = x + residual\n        x = self.feed_forward_layernorm(x)\n        return x \n```", "```py\n[\n    [1, 0, 0, 0, 0],\n    [1, 1, 0, 0, 0],\n    [1, 1, 1, 0, 0],\n    [1, 1, 1, 1, 0],\n    [1, 1, 1, 1, 1],\n] \n```", "```py\nhidden_dim = 256\nintermediate_dim = 2048\nnum_heads = 8\n\nsource = keras.Input(shape=(None,), dtype=\"int32\", name=\"english\")\nx = layers.Embedding(vocab_size, hidden_dim)(source)\nencoder_output = TransformerEncoder(hidden_dim, intermediate_dim, num_heads)(\n    source=x,\n    source_mask=source != 0,\n)\n\ntarget = keras.Input(shape=(None,), dtype=\"int32\", name=\"spanish\")\nx = layers.Embedding(vocab_size, hidden_dim)(target)\nx = TransformerDecoder(hidden_dim, intermediate_dim, num_heads)(\n    target=x,\n    source=encoder_output,\n    source_mask=source != 0,\n)\nx = layers.Dropout(0.5)(x)\ntarget_predictions = layers.Dense(vocab_size, activation=\"softmax\")(x)\ntransformer = keras.Model([source, target], target_predictions) \n```", "```py\n>>> transformer.summary()\nModel: \"functional_3\"\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ english (InputLayer)  │ (None, None)      │           0 │ -                  │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embedding_5           │ (None, None, 256) │   3,840,000 │ english[0][0]      │\n│ (Embedding)           │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ not_equal_4           │ (None, None)      │           0 │ english[0][0]      │\n│ (NotEqual)            │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ spanish (InputLayer)  │ (None, None)      │           0 │ -                  │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ transformer_encoder_1 │ (None, None, 256) │   1,315,072 │ embedding_5[0][0], │\n│ (TransformerEncoder)  │                   │             │ not_equal_4[0][0]  │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ not_equal_5           │ (None, None)      │           0 │ english[0][0]      │\n│ (NotEqual)            │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embedding_6           │ (None, None, 256) │   3,840,000 │ spanish[0][0]      │\n│ (Embedding)           │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ transformer_decoder_1 │ (None, None, 256) │   1,578,752 │ transformer_encod… │\n│ (TransformerDecoder)  │                   │             │ not_equal_5[0][0], │\n│                       │                   │             │ embedding_6[0][0]  │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_9 (Dropout)   │ (None, None, 256) │           0 │ transformer_decod… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_11 (Dense)      │ (None, None,      │   3,855,000 │ dropout_9[0][0]    │\n│                       │ 15000)            │             │                    │\n└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n Total params: 14,428,824 (55.04 MB)\n Trainable params: 14,428,824 (55.04 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\ntransformer.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    weighted_metrics=[\"accuracy\"],\n)\ntransformer.fit(train_ds, epochs=15, validation_data=val_ds) \n```", "```py\nfrom keras import ops\n\nclass PositionalEmbedding(keras.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim):\n        super().__init__()\n        self.token_embeddings = layers.Embedding(input_dim, output_dim)\n        self.position_embeddings = layers.Embedding(sequence_length, output_dim)\n\n    def call(self, inputs):\n        # Computes incrementing positions [0, 1, 2...] for each\n        # sequence in the batch\n        positions = ops.cumsum(ops.ones_like(inputs), axis=-1) - 1\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions \n```", "```py\nhidden_dim = 256\nintermediate_dim = 2056\nnum_heads = 8\n\nsource = keras.Input(shape=(None,), dtype=\"int32\", name=\"english\")\nx = PositionalEmbedding(sequence_length, vocab_size, hidden_dim)(source)\nencoder_output = TransformerEncoder(hidden_dim, intermediate_dim, num_heads)(\n    source=x,\n    source_mask=source != 0,\n)\n\ntarget = keras.Input(shape=(None,), dtype=\"int32\", name=\"spanish\")\nx = PositionalEmbedding(sequence_length, vocab_size, hidden_dim)(target)\nx = TransformerDecoder(hidden_dim, intermediate_dim, num_heads)(\n    target=x,\n    source=encoder_output,\n    source_mask=source != 0,\n)\nx = layers.Dropout(0.5)(x)\ntarget_predictions = layers.Dense(vocab_size, activation=\"softmax\")(x)\ntransformer = keras.Model([source, target], target_predictions) \n```", "```py\ntransformer.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    weighted_metrics=[\"accuracy\"],\n)\ntransformer.fit(train_ds, epochs=30, validation_data=val_ds) \n```", "```py\nimport numpy as np\n\nspa_vocab = spanish_tokenizer.get_vocabulary()\nspa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n\ndef generate_translation(input_sentence):\n    tokenized_input_sentence = english_tokenizer([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(sequence_length):\n        tokenized_target_sentence = spanish_tokenizer([decoded_sentence])\n        tokenized_target_sentence = tokenized_target_sentence[:, :-1]\n        inputs = [tokenized_input_sentence, tokenized_target_sentence]\n        next_token_predictions = transformer.predict(inputs, verbose=0)\n        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n        sampled_token = spa_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor _ in range(5):\n    input_sentence = random.choice(test_eng_texts)\n    print(\"-\")\n    print(input_sentence)\n    print(generate_translation(input_sentence)) \n```", "```py\n-\nThe resemblance between these two men is uncanny.\n[start] el parecido entre estos cantantes de dos hombres son asombrosa [end]\n-\nI'll see you at the library tomorrow.\n[start] te veré en la biblioteca mañana [end]\n-\nDo you know how to ride a bicycle?\n[start] sabes montar en bici [end]\n-\nTom didn't want to do their dirty work.\n[start] tom no quería hacer su trabajo [end]\n-\nIs he back already?\n[start] ya ha vuelto [end] \n```", "```py\nimport keras_hub\n\ntokenizer = keras_hub.models.Tokenizer.from_preset(\"roberta_base_en\")\nbackbone = keras_hub.models.Backbone.from_preset(\"roberta_base_en\") \n```", "```py\n>>> tokenizer(\"The quick brown fox\")\nArray([  133,  2119,  6219, 23602], dtype=int32)\n```", "```py\n>>> backbone.summary()\nModel: \"roberta_backbone\"\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ token_ids             │ (None, None)      │           0 │ -                  │\n│ (InputLayer)          │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embeddings            │ (None, None, 768) │  38,996,736 │ token_ids[0][0]    │\n│ (TokenAndPositionEmb… │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embeddings_layer_norm │ (None, None, 768) │       1,536 │ embeddings[0][0]   │\n│ (LayerNormalization)  │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embeddings_dropout    │ (None, None, 768) │           0 │ embeddings_layer_… │\n│ (Dropout)             │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ padding_mask          │ (None, None)      │           0 │ -                  │\n│ (InputLayer)          │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ transformer_layer_0   │ (None, None, 768) │   7,087,872 │ embeddings_dropou… │\n│ (TransformerEncoder)  │                   │             │ padding_mask[0][0] │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ transformer_layer_1   │ (None, None, 768) │   7,087,872 │ transformer_layer… │\n│ (TransformerEncoder)  │                   │             │ padding_mask[0][0] │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ ...                   │ ...               │ ...         │ ...                │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ transformer_layer_11  │ (None, None, 768) │   7,087,872 │ transformer_layer… │\n│ (TransformerEncoder)  │                   │             │ padding_mask[0][0] │\n└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n Total params: 124,052,736 (473.22 MB)\n Trainable params: 124,052,736 (473.22 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nfrom keras.utils import text_dataset_from_directory\n\nbatch_size = 16\ntrain_ds = text_dataset_from_directory(train_dir, batch_size=batch_size)\nval_ds = text_dataset_from_directory(val_dir, batch_size=batch_size)\ntest_ds = text_dataset_from_directory(test_dir, batch_size=batch_size) \n```", "```py\n[\n    [\"<s>\", \"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \".\", \"</s>\"],\n    [\"<s>\", \"the\", \"panda\", \"slept\", \".\", \"</s>\", \"<pad>\", \"<pad>\"],\n] \n```", "```py\ndef preprocess(text, label):\n    packer = keras_hub.layers.StartEndPacker(\n        sequence_length=512,\n        start_value=tokenizer.start_token_id,\n        end_value=tokenizer.end_token_id,\n        pad_value=tokenizer.pad_token_id,\n        return_padding_mask=True,\n    )\n    token_ids, padding_mask = packer(tokenizer(text))\n    return {\"token_ids\": token_ids, \"padding_mask\": padding_mask}, label\n\npreprocessed_train_ds = train_ds.map(preprocess)\npreprocessed_val_ds = val_ds.map(preprocess)\npreprocessed_test_ds = test_ds.map(preprocess) \n```", "```py\n>>> next(iter(preprocessed_train_ds))\n({\"token_ids\": <tf.Tensor: shape=(16, 512), dtype=int32, numpy=\n  array([[   0,  713,   56, ...,    1,    1,    1],\n         [   0, 1121,    5, ...,  101,   24,    2],\n         [   0,  713, 1569, ...,    1,    1,    1],\n         ...,\n         [   0,  100, 3996, ...,    1,    1,    1],\n         [   0,  100,   64, ..., 4655,  101,    2],\n         [   0,  734,    8, ...,    1,    1,    1]], dtype=int32)>,\n  \"padding_mask\": <tf.Tensor: shape=(16, 512), dtype=bool, numpy=\n  array([[ True,  True,  True, ..., False, False, False],\n         [ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ..., False, False, False],\n         ...,\n         [ True,  True,  True, ..., False, False, False],\n         [ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ..., False, False, False]])>},\n <tf.Tensor: shape=(16,), dtype=int32, numpy=array([0, 1, ...], dtype=int32)>)\n```", "```py\ninputs = backbone.input\nx = backbone(inputs)\n# Uses the hidden representation of the first token\nx = x[:, 0, :]\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(768, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nclassifier = keras.Model(inputs, outputs) \n```", "```py\nclassifier.compile(\n    optimizer=keras.optimizers.Adam(5e-5),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nclassifier.fit(\n    preprocessed_train_ds,\n    validation_data=preprocessed_val_ds,\n) \n```", "```py\n>>> classifier.evaluate(preprocessed_test_ds)\n[0.168127179145813, 0.9366399645805359]\n```"]