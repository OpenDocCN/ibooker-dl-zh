- en: Chapter 15\. Application Architectures Deep Dive
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章。应用架构深入探讨
- en: We are now in the exciting position that we can fully understand the architectures
    that we have been using for our state-of-the-art models for computer vision, natural
    language processing, and tabular analysis. In this chapter, we’re going to fill
    in all the missing details on how fastai’s application models work and show you
    how to build them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在处于一个令人兴奋的位置，我们可以完全理解我们为计算机视觉、自然语言处理和表格分析使用的最先进模型的架构。在本章中，我们将填补有关fastai应用模型如何工作的所有缺失细节，并向您展示如何构建它们。
- en: We will also go back to the custom data preprocessing pipeline we saw in [Chapter 11](ch11.xhtml#chapter_midlevel_data)
    for Siamese networks and show you how to use the components in the fastai library
    to build custom pretrained models for new tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将回到我们在[第11章](ch11.xhtml#chapter_midlevel_data)中看到的用于Siamese网络的自定义数据预处理流程，并向您展示如何使用fastai库中的组件为新任务构建自定义预训练模型。
- en: We’ll start with computer vision.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从计算机视觉开始。
- en: Computer Vision
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: For computer vision applications, we use the functions `cnn_learner` and `unet_learner`
    to build our models, depending on the task. In this section, we’ll explore how
    to build the `Learner` objects we used in Parts [I](part01.xhtml#part1) and [II](part02.xhtml#part2)
    of this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算机视觉应用，我们使用`cnn_learner`和`unet_learner`函数来构建我们的模型，具体取决于任务。在本节中，我们将探讨如何构建我们在本书的第[I](part01.xhtml#part1)部分和[II](part02.xhtml#part2)部分中使用的`Learner`对象。
- en: cnn_learner
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: cnn_learner
- en: Let’s take a look at what happens when we use the `cnn_learner` function. We
    begin by passing this function an architecture to use for the *body* of the network.
    Most of the time, we use a ResNet, which you already know how to create, so we
    don’t need to delve into that any further. Pretrained weights are downloaded as
    required and loaded into the ResNet.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们使用`cnn_learner`函数时会发生什么。我们首先向这个函数传递一个用于网络*主体*的架构。大多数情况下，我们使用ResNet，您已经知道如何创建，所以我们不需要深入研究。预训练权重将根据需要下载并加载到ResNet中。
- en: 'Then, for transfer learning, the network needs to be *cut*. This refers to
    slicing off the final layer, which is responsible only for ImageNet-specific categorization.
    In fact, we do not slice off only this layer, but everything from the adaptive
    average pooling layer onward. The reason for this will become clear in just a
    moment. Since different architectures might use different types of pooling layers,
    or even completely different kinds of *heads*, we don’t just search for the adaptive
    pooling layer to decide where to cut the pretrained model. Instead, we have a
    dictionary of information that is used for each model to determine where its body
    ends and its head starts. We call this `model_meta`—here it is for `resnet50`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于迁移学习，网络需要被*切割*。这指的是切掉最后一层，该层仅负责ImageNet特定的分类。实际上，我们不仅切掉这一层，还切掉自自适应平均池化层以及之后的所有内容。这样做的原因很快就会变得清楚。由于不同的架构可能使用不同类型的池化层，甚至完全不同类型的*头部*，我们不仅仅搜索自适应池化层来决定在哪里切割预训练模型。相反，我们有一个信息字典，用于确定每个模型的主体在哪里结束，头部从哪里开始。我们称之为`model_meta`—这是`resnet50`的信息：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Jargon: Body and Head'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行话：主体和头部
- en: The head of a neural net is the part that is specialized for a particular task.
    For a CNN, it’s generally the part after the adaptive average pooling layer. The
    body is everything else, and includes the stem (which we learned about in [Chapter 14](ch14.xhtml#chapter_resnet)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的头部是专门针对特定任务的部分。对于CNN，通常是自适应平均池化层之后的部分。主体是其他所有部分，包括干部（我们在[第14章](ch14.xhtml#chapter_resnet)中学到的）。
- en: 'If we take all of the layers prior to the cut point of `-2`, we get the part
    of the model that fastai will keep for transfer learning. Now, we put on our new
    head. This is created using the function `create_head`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取出在`-2`之前的所有层，我们就得到了fastai将保留用于迁移学习的模型部分。现在，我们放上我们的新头部。这是使用`create_head`函数创建的：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With this function, you can choose how many additional linear layers are added
    to the end, how much dropout to use after each one, and what kind of pooling to
    use. By default, fastai will apply both average pooling and max pooling, and will
    concatenate the two together (this is the `AdaptiveConcatPool2d` layer). This
    is not a particularly common approach, but it was developed independently at fastai
    and other research labs in recent years and tends to provide a small improvement
    over using just average pooling.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数，您可以选择在末尾添加多少个额外的线性层，每个线性层之后使用多少dropout，以及使用什么类型的池化。默认情况下，fastai将同时应用平均池化和最大池化，并将两者连接在一起（这是`AdaptiveConcatPool2d`层）。这不是一个特别常见的方法，但它在fastai和其他研究实验室近年来独立开发，并倾向于比仅使用平均池化提供小幅改进。
- en: fastai is a bit different from most libraries in that by default it adds two
    linear layers, rather than one, in the CNN head. The reason is that transfer learning
    can still be useful even, as we have seen, when transferring the pretrained model
    to very different domains. However, just using a single linear layer is unlikely
    to be enough in these cases; we have found that using two linear layers can allow
    transfer learning to be used more quickly and easily, in more situations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: fastai与大多数库有所不同，因为默认情况下它在CNN头部中添加两个线性层，而不是一个。原因是，正如我们所看到的，即使将预训练模型转移到非常不同的领域，迁移学习仍然可能是有用的。然而，在这些情况下，仅使用单个线性层可能不足够；我们发现使用两个线性层可以使迁移学习更快速、更容易地应用在更多情况下。
- en: One Last Batchnorm
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后一个Batchnorm
- en: One parameter to `create_head` that is worth looking at is `bn_final`. Setting
    this to `True` will cause a batchnorm layer to be added as your final layer. This
    can be useful in helping your model scale appropriately for your output activations.
    We haven’t seen this approach published anywhere as yet, but we have found that
    it works well in practice wherever we have used it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_head`的一个值得关注的参数是`bn_final`。将其设置为`True`将导致一个batchnorm层被添加为您的最终层。这有助于帮助您的模型适当地缩放输出激活。迄今为止，我们还没有看到这种方法在任何地方发表，但我们发现在实践中无论我们在哪里使用它，它都效果很好。'
- en: Let’s now take a look at what `unet_learner` did in the segmentation problem
    we showed in [Chapter 1](ch01.xhtml#chapter_intro).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看`unet_learner`在我们在[第1章](ch01.xhtml#chapter_intro)展示的分割问题中做了什么。
- en: unet_learner
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: unet_learner
- en: One of the most interesting architectures in deep learning is the one that we
    used for segmentation in [Chapter 1](ch01.xhtml#chapter_intro). Segmentation is
    a challenging task, because the output required is really an image, or a pixel
    grid, containing the predicted label for every pixel. Other tasks share a similar
    basic design, such as increasing the resolution of an image (*super-resolution*),
    adding color to a black-and-white image (*colorization*), or converting a photo
    into a synthetic painting (*style transfer*)—these tasks are covered by an [online
    chapter of this book](https://book.fast.ai), so be sure to check it out after
    you’ve read this chapter. In each case, we are starting with an image and converting
    it to another image of the same dimensions or aspect ratio, but with the pixels
    altered in some way. We refer to these as *generative vision models*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中最有趣的架构之一是我们在[第1章](ch01.xhtml#chapter_intro)中用于分割的架构。分割是一项具有挑战性的任务，因为所需的输出实际上是一幅图像，或者一个像素网格，包含了每个像素的预测标签。其他任务也有类似的基本设计，比如增加图像的分辨率（*超分辨率*）、给黑白图像添加颜色（*着色*）、或将照片转换为合成画作（*风格转移*）——这些任务在本书的[在线章节](https://book.fast.ai)中有介绍，所以在阅读完本章后一定要查看。在每种情况下，我们都是从一幅图像开始，将其转换为另一幅具有相同尺寸或纵横比的图像，但像素以某种方式被改变。我们将这些称为*生成式视觉模型*。
- en: The way we do this is to start with the exact same approach to developing a
    CNN head as we saw in the previous section. We start with a ResNet, for instance,
    and cut off the adaptive pooling layer and everything after that. Then we replace
    those layers with our custom head, which does the generative task.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的做法是从与前一节中看到的开发CNN头部的确切方法开始。例如，我们从一个ResNet开始，然后截断自适应池化层和之后的所有层。然后我们用我们的自定义头部替换这些层，执行生成任务。
- en: There was a lot of handwaving in that last sentence! How on earth do we create
    a CNN head that generates an image? If we start with, say, a 224-pixel input image,
    then at the end of the ResNet body we will have a 7×7 grid of convolutional activations.
    How can we convert that into a 224-pixel segmentation mask?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一句中有很多含糊之处！我们到底如何创建一个生成图像的CNN头部？如果我们从一个224像素的输入图像开始，那么在ResNet主体的末尾，我们将得到一个7×7的卷积激活网格。我们如何将其转换为一个224像素的分割掩模？
- en: Naturally, we do this with a neural network! So we need some kind of layer that
    can increase the grid size in a CNN. One simple approach is to replace every pixel
    in the 7×7 grid with four pixels in a 2×2 square. Each of those four pixels will
    have the same value—this is known as *nearest neighbor interpolation*. PyTorch
    provides a layer that does this for us, so one option is to create a head that
    contains stride-1 convolutional layers (along with batchnorm and ReLU layers as
    usual) interspersed with 2×2 nearest neighbor interpolation layers. In fact, you
    can try this now! See if you can create a custom head designed like this, and
    try it on the CamVid segmentation task. You should find that you get some reasonable
    results, although they won’t be as good as our [Chapter 1](ch01.xhtml#chapter_intro)
    results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们使用神经网络来做这个！所以我们需要一种能够在CNN中增加网格大小的层。一个简单的方法是用一个2×2的方块替换7×7网格中的每个像素。这四个像素中的每一个将具有相同的值——这被称为*最近邻插值*。PyTorch为我们提供了一个可以做到这一点的层，因此一个选项是创建一个包含步长为1的卷积层（以及通常的批归一化和ReLU层）和2×2最近邻插值层的头部。实际上，你现在可以尝试一下！看看你是否可以创建一个设计如此的自定义头部，并在CamVid分割任务上尝试一下。你应该会发现你得到了一些合理的结果，尽管它们不会像我们在[第1章](ch01.xhtml#chapter_intro)中的结果那样好。
- en: Another approach is to replace the nearest neighbor and convolution combination
    with a *transposed convolution*, otherwise known as a *stride half convolution*.
    This is identical to a regular convolution, but first zero padding is inserted
    between all the pixels in the input. This is easiest to see with a picture—[Figure 15-1](#transp_conv)
    shows a diagram from the excellent [convolutional arithmetic paper](https://oreil.ly/hu06c)
    we discussed in [Chapter 13](ch13.xhtml#chapter_convolutions), showing a 3×3 transposed
    convolution applied to a 3×3 image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是用*转置卷积*替换最近邻和卷积的组合，也被称为*步长一半卷积*。这与常规卷积相同，但首先在输入的所有像素之间插入零填充。这在图片上最容易看到——[图15-1](#transp_conv)显示了一张来自我们在[第13章](ch13.xhtml#chapter_convolutions)讨论过的优秀的[卷积算术论文](https://oreil.ly/hu06c)中的图表，展示了一个应用于3×3图像的3×3转置卷积。
- en: '![A transposed convolution](Images/dlcf_1501.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![一个转置卷积](Images/dlcf_1501.png)'
- en: Figure 15-1\. A transposed convolution (courtesy of Vincent Dumoulin and Francesco
    Visin)
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-1\. 一个转置卷积（由Vincent Dumoulin和Francesco Visin提供）
- en: As you see, the result is to increase the size of the input. You can try this
    out now by using fastai’s `ConvLayer` class; pass the parameter `transpose=True`
    to create a transposed convolution, instead of a regular one, in your custom head.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，结果是增加输入的大小。你现在可以通过使用fastai的`ConvLayer`类来尝试一下；在你的自定义头部中传递参数`transpose=True`来创建一个转置卷积，而不是一个常规卷积。
- en: Neither of these approaches, however, works really well. The problem is that
    our 7×7 grid simply doesn’t have enough information to create a 224×224-pixel
    output. It’s asking an awful lot of the activations of each of those grid cells
    to have enough information to fully regenerate every pixel in the output.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两种方法都不是很好。问题在于我们的7×7网格根本没有足够的信息来创建一个224×224像素的输出。要求每个网格单元的激活具有足够的信息来完全重建输出中的每个像素是非常困难的。
- en: 'The solution is to use *skip connections*, as in a ResNet, but skipping from
    the activations in the body of the ResNet all the way over to the activations
    of the transposed convolution on the opposite side of the architecture. This approach,
    illustrated in [Figure 15-2](#unet), was developed by Olaf Ronneberger et al.
    in the 2015 paper [“U-Net: Convolutional Networks for Biomedical Image Segmentation”](https://oreil.ly/6ely4).
    Although the paper focused on medical applications, the U-Net has revolutionized
    all kinds of generative vision models.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用*跳跃连接*，就像ResNet中那样，但是从ResNet主体中的激活一直跳到架构对面的转置卷积的激活。这种方法在2015年Olaf Ronneberger等人的论文[“U-Net:用于生物医学图像分割的卷积网络”](https://oreil.ly/6ely4)中有所阐述。尽管该论文侧重于医学应用，但U-Net已经彻底改变了各种生成视觉模型。
- en: '![The U-Net architecture](Images/dlcf_1502.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![U-Net架构](Images/dlcf_1502.png)'
- en: Figure 15-2\. The U-Net architecture (courtesy of Olaf Ronneberger, Philipp
    Fischer, and Thomas Brox)
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-2。U-Net架构（由Olaf Ronneberger、Philipp Fischer和Thomas Brox提供）
- en: This picture shows the CNN body on the left (in this case, it’s a regular CNN,
    not a ResNet, and they’re using 2×2 max pooling instead of stride-2 convolutions,
    since this paper was written before ResNets came along) and the transposed convolutional
    (“up-conv”) layers on the right. The extra skip connections are shown as gray
    arrows crossing from left to right (these are sometimes called *cross connections*).
    You can see why it’s called a *U-Net*!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图片展示了左侧的CNN主体（在这种情况下，它是一个常规的CNN，而不是ResNet，它们使用2×2最大池化而不是步幅为2的卷积，因为这篇论文是在ResNets出现之前写的），右侧是转置卷积（“上采样”）层。额外的跳跃连接显示为从左到右的灰色箭头（有时被称为*交叉连接*）。你可以看到为什么它被称为*U-Net*！
- en: With this architecture, the input to the transposed convolutions is not just
    the lower-resolution grid in the preceding layer, but also the higher-resolution
    grid in the ResNet head. This allows the U-Net to use all of the information of
    the original image, as it is needed. One challenge with U-Nets is that the exact
    architecture depends on the image size. fastai has a unique `DynamicUnet` class
    that autogenerates an architecture of the right size based on the data provided.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种架构，传递给转置卷积的输入不仅是前一层中较低分辨率的网格，还有ResNet头部中较高分辨率的网格。这使得U-Net可以根据需要使用原始图像的所有信息。U-Net的一个挑战是确切的架构取决于图像大小。fastai有一个独特的`DynamicUnet`类，根据提供的数据自动生成合适大小的架构。
- en: Let’s focus now on an example in which we leverage the fastai library to write
    a custom model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们专注于一个示例，其中我们利用fastai库编写一个自定义模型。
- en: A Siamese Network
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 孪生网络
- en: Let’s go back to the input pipeline we set up in [Chapter 11](ch11.xhtml#chapter_midlevel_data)
    for a Siamese network. As you may remember, it consisted of a pair of images with
    the label being `True` or `False`, depending on whether they were in the same
    class.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们在[第11章](ch11.xhtml#chapter_midlevel_data)中为孪生网络设置的输入管道。你可能还记得，它由一对图像组成，标签为`True`或`False`，取决于它们是否属于同一类。
- en: 'Using what we just saw, let’s build a custom model for this task and train
    it. How? We will use a pretrained architecture and pass our two images through
    it. Then we can concatenate the results and send them to a custom head that will
    return two predictions. In terms of modules, this looks like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们刚刚看到的内容，让我们为这个任务构建一个自定义模型并对其进行训练。如何做？我们将使用一个预训练的架构并将我们的两个图像传递给它。然后我们可以连接结果并将它们发送到一个自定义头部，该头部将返回两个预测。在模块方面，看起来像这样：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To create our encoder, we just need to take a pretrained model and cut it,
    as we explained before. The function `create_body` does that for us; we just have
    to pass it the place where we want to cut. As we saw earlier, per the dictionary
    of metadata for pretrained models, the cut value for a ResNet is `–2`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的编码器，我们只需要取一个预训练模型并切割它，就像我们之前解释的那样。函数`create_body`为我们执行此操作；我们只需传递我们想要切割的位置。正如我们之前看到的，根据预训练模型的元数据字典，ResNet的切割值为`-2`：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we can create our head. A look at the encoder tells us the last layer
    has 512 features, so this head will need to receive `512*4`. Why 4? First we have
    to multiply by 2 because we have two images. Then we need a second multiplication
    by 2 because of our concat-pool trick. So we create the head as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以创建我们的头部。查看编码器告诉我们最后一层有512个特征，所以这个头部将需要接收`512*4`。为什么是4？首先我们必须乘以2，因为我们有两个图像。然后我们需要第二次乘以2，因为我们的连接池技巧。因此我们创建头部如下：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With our encoder and head, we can now build our model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的编码器和头部，我们现在可以构建我们的模型：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Before using `Learner`, we have two more things to define. First, we must define
    the loss function we want to use. It’s regular cross entropy, but since our targets
    are Booleans, we need to convert them to integers or PyTorch will throw an error:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`Learner`之前，我们还需要定义两件事。首先，我们必须定义要使用的损失函数。它是常规的交叉熵，但由于我们的目标是布尔值，我们需要将它们转换为整数，否则PyTorch会抛出错误：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: More importantly, to take full advantage of transfer learning, we have to define
    a custom *splitter*. A splitter is a function that tells the fastai library how
    to split the model into parameter groups. These are used behind the scenes to
    train only the head of a model when we do transfer learning.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，为了充分利用迁移学习，我们必须定义一个自定义的*splitter*。*splitter*是一个告诉fastai库如何将模型分成参数组的函数。这些在幕后用于在进行迁移学习时仅训练模型的头部。
- en: 'Here we want two parameter groups: one for the encoder and one for the head.
    We can thus define the following splitter (`params` is just a function that returns
    all parameters of a given module):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们想要两个参数组：一个用于编码器，一个用于头部。因此我们可以定义以下*splitter*（`params`只是一个返回给定模块的所有参数的函数）：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we can define our `Learner` by passing the data, model, loss function,
    splitter, and any metric we want. Since we are not using a convenience function
    from fastai for transfer learning (like `cnn_learner`), we have to call `learn.freeze`
    manually. This will make sure only the last parameter group (in this case, the
    head) is trained:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过传递数据、模型、损失函数、分割器和任何我们想要的指标来定义我们的`Learner`。由于我们没有使用fastai的传输学习便利函数（如`cnn_learner`），我们必须手动调用`learn.freeze`。这将确保只有最后一个参数组（在本例中是头部）被训练：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we can directly train our model with the usual method:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以直接使用通常的方法训练我们的模型：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.367015 | 0.281242 | 0.885656 | 00:26 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.367015 | 0.281242 | 0.885656 | 00:26 |'
- en: '| 1 | 0.307688 | 0.214721 | 0.915426 | 00:26 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.307688 | 0.214721 | 0.915426 | 00:26 |'
- en: '| 2 | 0.275221 | 0.170615 | 0.936401 | 00:26 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.275221 | 0.170615 | 0.936401 | 00:26 |'
- en: '| 3 | 0.223771 | 0.159633 | 0.943843 | 00:26 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.223771 | 0.159633 | 0.943843 | 00:26 |'
- en: 'Now we unfreeze and fine-tune the whole model a bit more with discriminative
    learning rates (that is, a lower learning rate for the body and a higher one for
    the head):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们解冻并使用有区别的学习率微调整个模型一点（即，对于主体使用较低的学习率，对于头部使用较高的学习率）：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.212744 | 0.159033 | 0.944520 | 00:35 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.212744 | 0.159033 | 0.944520 | 00:35 |'
- en: '| 1 | 0.201893 | 0.159615 | 0.942490 | 00:35 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.201893 | 0.159615 | 0.942490 | 00:35 |'
- en: '| 2 | 0.204606 | 0.152338 | 0.945196 | 00:36 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.204606 | 0.152338 | 0.945196 | 00:36 |'
- en: '| 3 | 0.213203 | 0.148346 | 0.947903 | 00:36 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.213203 | 0.148346 | 0.947903 | 00:36 |'
- en: 94.8% is very good when we remember that a classifier trained the same way (with
    no data augmentation) had an error rate of 7%.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 94.8％是非常好的，当我们记得以相同方式训练的分类器（没有数据增强）的错误率为7％时。
- en: Now that we’ve seen how to create complete state-of-the-art computer vision
    models, let’s move on to NLP.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到如何创建完整的最先进的计算机视觉模型，让我们继续进行自然语言处理。
- en: Natural Language Processing
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Converting an AWD-LSTM language model into a transfer learning classifier, as
    we did in [Chapter 10](ch10.xhtml#chapter_nlp), follows a very similar process
    to what we did with `cnn_learner` in the first section of this chapter. We do
    not need a “meta” dictionary in this case, because we do not have such a variety
    of architectures to support in the body. All we need to do is select the stacked
    RNN for the encoder in the language model, which is a single PyTorch module. This
    encoder will provide an activation for every word of the input, because a language
    model needs to output a prediction for every next word.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将AWD-LSTM语言模型转换为迁移学习分类器，就像我们在[第10章](ch10.xhtml#chapter_nlp)中所做的那样，遵循与本章第一节中的`cnn_learner`相似的过程。在这种情况下，我们不需要一个“元”字典，因为我们没有这么多种类的体系结构需要在主体中支持。我们只需要选择语言模型中的堆叠RNN作为编码器，这是一个单独的PyTorch模块。这个编码器将为输入的每个单词提供一个激活，因为语言模型需要为每个下一个单词输出一个预测。
- en: 'To create a classifier from this, we use an approach described in [the ULMFiT
    paper](https://oreil.ly/3hdSj) as “BPTT for Text Classification (BPT3C)”:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要从中创建一个分类器，我们使用了[ULMFiT论文](https://oreil.ly/3hdSj)中描述的一种方法，称为“用于文本分类的BPTT（BPT3C）”：
- en: We divide the document into fixed-length batches of size *b*. At the beginning
    of each batch, the model is initialized with the final state of the previous batch;
    we keep track of the hidden states for mean and max-pooling; gradients are back-propagated
    to the batches whose hidden states contributed to the final prediction. In practice,
    we use variable length backpropagation sequences.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将文档分成固定长度为*b*的批次。在每个批次的开始，模型使用前一个批次的最终状态进行初始化；我们跟踪用于平均值和最大池化的隐藏状态；梯度被反向传播到隐藏状态对最终预测有贡献的批次。在实践中，我们使用可变长度的反向传播序列。
- en: In other words, the classifier contains a `for` loop, which loops over each
    batch of a sequence. The state is maintained across batches, and the activations
    of each batch are stored. At the end, we use the same average and max concatenated
    pooling trick that we use for computer vision models—but this time, we do not
    pool over CNN grid cells, but over RNN sequences.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，分类器包含一个`for`循环，循环遍历每个序列的批次。状态在批次之间保持不变，并且存储每个批次的激活。最后，我们使用相同的平均值和最大连接池技巧，这与我们用于计算机视觉模型的方法相同，但这一次，我们不是在CNN网格单元上进行池化，而是在RNN序列上进行池化。
- en: For this `for` loop, we need to gather our data in batches, but each text needs
    to be treated separately, as they each have their own labels. However, it’s very
    likely that those texts won’t all be of the same length, which means we won’t
    be able to put them all in the same array, as we did with the language model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个`for`循环，我们需要将我们的数据分批处理，但每个文本需要单独处理，因为它们各自有自己的标签。然而，这些文本很可能不会都是相同的长度，这意味着我们无法将它们都放在同一个数组中，就像我们在语言模型中所做的那样。
- en: 'That’s where padding is going to help: when grabbing a bunch of texts, we determine
    the one with the greatest length; then we fill the ones that are shorter with
    a special token called `xxpad`. To avoid extreme cases of having a text with 2,000
    tokens in the same batch as a text with 10 tokens (so a lot of padding, and a
    lot of wasted computation), we alter the randomness by making sure texts of comparable
    size are put together. The texts will still be in a somewhat random order for
    the training set (for the validation set, we can simply sort them by order of
    length), but not completely so.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是填充将会有所帮助的地方：当获取一堆文本时，我们确定最长的文本，然后用一个特殊的标记`xxpad`填充较短的文本。为了避免在同一批次中有一个包含2,000个标记的文本和一个包含10个标记的文本的极端情况（因此有很多填充和浪费的计算），我们通过确保相似大小的文本被放在一起来改变随机性。文本在训练集中仍然会以某种随机顺序排列（对于验证集，我们可以简单地按长度顺序排序），但不完全是这样。
- en: This is done automatically behind the scenes by the fastai library when creating
    our `DataLoaders`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由fastai库在创建我们的`DataLoaders`时在幕后自动完成的。
- en: Tabular
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表格
- en: Finally, let’s take a look at `fastai.tabular` models. (We don’t need to look
    at collaborative filtering separately, since we’ve already seen that these models
    are just tabular models or use the dot product approach, which we implemented
    earlier from scratch.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看`fastai.tabular`模型。（我们不需要单独查看协同过滤，因为我们已经看到这些模型只是表格模型或使用点积方法，我们之前从头开始实现。）
- en: 'Here is the `forward` method for `TabularModel`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`TabularModel`的`forward`方法：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We won’t show `__init__` here, since it’s not that interesting, but will look
    at each line of code in `forward` in turn. The first line is just testing whether
    there are any embeddings to deal with—we can skip this section if we have only
    continuous variables:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里显示`__init__`，因为这并不那么有趣，但会依次查看`forward`中的每行代码。第一行只是测试是否有任何嵌入需要处理-如果只有连续变量，我们可以跳过这一部分：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`self.embeds` contains the embedding matrices, so this gets the activations
    of each'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`self.embeds`包含嵌入矩阵，因此这会获取每个激活'
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'and concatenates them into a single tensor:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 并将它们连接成一个单一张量：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then dropout is applied. You can pass `emb_drop` to `__init__` to change this
    value:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后应用了辍学。您可以将`emb_drop`传递给`__init__`以更改此值：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we test whether there are any continuous variables to deal with:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们测试是否有任何连续变量需要处理：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: They are passed through a batchnorm layer
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通过一个批量归一化层
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'and concatenated with the embedding activations, if there were any:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 并与嵌入激活连接在一起，如果有的话：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, this is passed through the linear layers (each of which includes batchnorm,
    if `use_bn` is `True`, and dropout, if `ps` is set to some value or list of values):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些通过线性层传递（每个线性层包括批量归一化，如果`use_bn`为`True`，并且辍学，如果`ps`设置为某个值或值列表）：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Congratulations! Now you know every single piece of the architectures used in
    the fastai library!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在您已经了解了fastai库中使用的每个架构的所有细节！
- en: Conclusion
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As you can see, the details of deep learning architectures need not scare you
    now. You can look inside the code of fastai and PyTorch and see just what is going
    on. More importantly, try to understand *why* it’s going on. Take a look at the
    papers that are referenced in the code, and try to see how the code matches up
    to the algorithms that are described.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，深度学习架构的细节现在不应该让您感到恐惧。您可以查看fastai和PyTorch的代码，看看发生了什么。更重要的是，尝试理解*为什么*会发生这种情况。查看代码中引用的论文，并尝试看看代码如何与描述的算法相匹配。
- en: 'Now that we have investigated all of the pieces of a model and the data that
    is passed into it, we can consider what this means for practical deep learning.
    If you have unlimited data, unlimited memory, and unlimited time, then the advice
    is easy: train a huge model on all of your data for a really long time. But the
    reason that deep learning is not straightforward is that your data, memory, and
    time are typically limited. If you are running out of memory or time, the solution
    is to train a smaller model. If you are not able to train for long enough to overfit,
    you are not taking advantage of the capacity of your model.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经调查了模型的所有部分以及传递给它的数据，我们可以考虑这对于实际深度学习意味着什么。如果您拥有无限的数据，无限的内存和无限的时间，那么建议很简单：在所有数据上训练一个巨大的模型很长时间。但深度学习不简单的原因是您的数据、内存和时间通常是有限的。如果内存或时间不足，解决方案是训练一个较小的模型。如果您无法训练足够长时间以过拟合，那么您没有充分利用模型的容量。
- en: So, step 1 is to get to the point where you can overfit. Then the question is
    how to reduce that overfitting. [Figure 15-3](#reduce_overfit) shows how we recommend
    prioritizing the steps from there.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一步是达到过拟合的点。然后问题是如何减少过拟合。[图15-3](#reduce_overfit)显示了我们建议从那里优先考虑的步骤。
- en: '![Steps to reducing overfitting](Images/dlcf_1503.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![减少过拟合的步骤](Images/dlcf_1503.png)'
- en: Figure 15-3\. Steps to reducing overfitting
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-3。减少过拟合的步骤
- en: Many practitioners, when faced with an overfitting model, start at exactly the
    wrong end of this diagram. Their starting point is to use a smaller model or more
    regularization. Using a smaller model should be absolutely the last step you take,
    unless training your model is taking up too much time or memory. Reducing the
    size of your model reduces the ability of your model to learn subtle relationships
    in your data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者在面对过拟合模型时，从这个图表的完全错误的一端开始。他们的起点是使用更小的模型或更多的正则化。除非训练模型占用太多时间或内存，否则使用更小的模型应该是您采取的最后一步。减小模型的大小会降低模型学习数据中微妙关系的能力。
- en: Instead, your first step should be to seek to *create more data*. That could
    involve adding more labels to data that you already have, finding additional tasks
    that your model could be asked to solve (or, to think of it another way, identifying
    different kinds of labels that you could model), or creating additional synthetic
    data by using more or different data augmentation techniques. Thanks to the development
    of Mixup and similar approaches, effective data augmentation is now available
    for nearly all kinds of data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，您的第一步应该是寻求*创建更多数据*。这可能涉及向您已经拥有的数据添加更多标签，找到模型可以被要求解决的其他任务（或者，换个角度思考，识别您可以建模的不同类型的标签），或者通过使用更多或不同的数据增强技术创建额外的合成数据。由于Mixup和类似方法的发展，现在几乎所有类型的数据都可以获得有效的数据增强。
- en: Once you’ve got as much data as you think you can reasonably get hold of, and
    are using it as effectively as possible by taking advantage of all the labels
    that you can find and doing all the augmentation that makes sense, if you are
    still overfitting, you should think about using more generalizable architectures.
    For instance, adding batch normalization may improve generalization.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您获得了您认为可以合理获得的尽可能多的数据，并且通过利用您可以找到的所有标签并进行所有有意义的增强来尽可能有效地使用它，如果您仍然过拟合，您应该考虑使用更具一般化能力的架构。例如，添加批量归一化可能会提高泛化能力。
- en: If you are still overfitting after doing the best you can at using your data
    and tuning your architecture, you can take a look at regularization. Generally
    speaking, adding dropout to the last layer or two will do a good job of regularizing
    your model. However, as we learned from the story of the development of AWD-LSTM,
    adding dropout of different types throughout your model can often help even more.
    Generally speaking, a larger model with more regularization is more flexible,
    and can therefore be more accurate than a smaller model with less regularization.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在尽力使用数据和调整架构后仍然过拟合，您可以考虑正则化。一般来说，在最后一层或两层添加dropout可以很好地正则化您的模型。然而，正如我们从AWD-LSTM开发故事中学到的那样，在整个模型中添加不同类型的dropout通常会更有帮助。一般来说，具有更多正则化的较大模型更灵活，因此比具有较少正则化的较小模型更准确。
- en: Only after considering all of these options would we recommend that you try
    using a smaller version of your architecture.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在考虑了所有这些选项之后，我们才建议您尝试使用较小版本的架构。
- en: Questionnaire
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷
- en: What is the head of a neural net?
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络的头是什么？
- en: What is the body of a neural net?
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络的主体是什么？
- en: What is “cutting” a neural net? Why do we need to do this for transfer learning?
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是“剪切”神经网络？为什么我们需要在迁移学习中这样做？
- en: What is `model_meta`? Try printing it to see what’s inside.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`model_meta`是什么？尝试打印它以查看里面的内容。'
- en: Read the source code for `create_head` and make sure you understand what each
    line does.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读`create_head`的源代码，并确保你理解每一行的作用。
- en: Look at the output of `create_head` and make sure you understand why each layer
    is there, and how the `create_head` source created it.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`create_head`的输出，并确保你理解每一层的存在原因，以及`create_head`源代码是如何创建它的。
- en: Figure out how to change the dropout, layer size, and number of layers created
    by `create_cnn`, and see if you can find values that result in better accuracy
    from the pet recognizer.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出如何改变`create_cnn`创建的dropout、层大小和层数，并查看是否可以找到能够提高宠物识别准确性的值。
- en: What does `AdaptiveConcatPool2d` do?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`AdaptiveConcatPool2d`是什么作用？'
- en: What is nearest neighbor interpolation? How can it be used to upsample convolutional
    activations?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是最近邻插值？如何用它来上采样卷积激活？
- en: What is a transposed convolution? What is another name for it?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是转置卷积？还有另一个名称是什么？
- en: Create a conv layer with `transpose=True` and apply it to an image. Check the
    output shape.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个带有`transpose=True`的卷积层，并将其应用于图像。检查输出形状。
- en: Draw the U-Net architecture.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制U-Net架构。
- en: What is BPTT for Text Classification (BPT3C)?
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是用于文本分类的BPTT（BPT3C）？
- en: How do we handle different length sequences in BPT3C?
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在BPT3C中如何处理不同长度的序列？
- en: Try to run each line of `TabularModel.forward` separately, one line per cell,
    in a notebook, and look at the input and output shapes at each step.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在笔记本中逐行运行`TabularModel.forward`的每一行，每个单元格一行，并查看每个步骤的输入和输出形状。
- en: How is `self.layers` defined in `TabularModel`?
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`TabularModel`中的`self.layers`是如何定义的？'
- en: What are the five steps for preventing overfitting?
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预防过拟合的五个步骤是什么？
- en: Why don’t we reduce architecture complexity before trying other approaches to
    preventing overfitting?
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在尝试其他方法预防过拟合之前不减少架构复杂性？
- en: Further Research
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: Write your own custom head and try training the pet recognizer with it. See
    if you can get a better result than fastai’s default.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写自己的自定义头，并尝试使用它训练宠物识别器。看看是否可以获得比fastai默认更好的结果。
- en: Try switching between `AdaptiveConcatPool2d` and `AdaptiveAvgPool2d` in a CNN
    head and see what difference it makes.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在CNN头部之间切换`AdaptiveConcatPool2d`和`AdaptiveAvgPool2d`，看看会有什么不同。
- en: Write your own custom splitter to create a separate parameter group for every
    ResNet block, and a separate group for the stem. Try training with it, and see
    if it improves the pet recognizer.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写自己的自定义分割器，为每个ResNet块创建一个单独的参数组，以及一个单独的参数组用于干扰。尝试使用它进行训练，看看是否可以改善宠物识别器。
- en: Read the online chapter about generative image models, and create your own colorizer,
    super-resolution model, or style transfer model.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读关于生成图像模型的在线章节，并创建自己的着色器、超分辨率模型或风格转移模型。
- en: Create a custom head using nearest neighbor interpolation and use it to do segmentation
    on CamVid.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最近邻插值创建一个自定义头，并用它在CamVid上进行分割。
