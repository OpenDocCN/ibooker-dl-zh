<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Alignment Training and Reasoning"><div class="chapter" id="ch8">
<h1><span class="label">Chapter 8. </span>Alignment Training and Reasoning</h1>


<p>Some common reasons for hesitancy<a data-type="indexterm" data-primary="alignment training" id="id1173"/> in adopting LLMs is the presence of hallucinations, the limitations in reasoning skills, and bias and safety issues. In this chapter, we will go through these limitations and introduce different techniques to mitigate them. First, we will introduce the concept of alignment training, which helps us steer our models toward desirable outcomes.</p>






<section data-type="sect1" data-pdf-bookmark="Defining Alignment Training"><div class="sect1" id="id367">
<h1>Defining Alignment Training</h1>

<p>We keep hearing about the <em>alignment problem</em> facing language models. What does this mean in practice? Ideally we would like a language model that we can fully understand, control, and steer. However, current language models are far from this ideal.</p>

<p>Thus, the goal of alignment is to make language models more controllable and steerable. <a href="https://oreil.ly/fRCkD">Askell et al.</a> from Anthropic define an aligned AI as one that is “helpful, honest, and harmless.” They further define the three H’s as follows:</p>
<dl>
<dt>Helpful</dt>
<dd>
<p>As long as a user request isn’t harmful, the AI should attempt to solve the request as effectively as possible, asking follow-up questions if needed.</p>
</dd>
<dt>Honest</dt>
<dd>
<p>The AI should provide accurate information and should be calibrated, providing reasonably accurate uncertainty estimates. It should understand its 
<span class="keep-together">shortcomings</span>.</p>
</dd>
<dt>Harmless</dt>
<dd>
<p>The AI should not be offensive or discriminatory and should refuse to perform tasks that can cause harm to individuals or society.</p>
</dd>
</dl>

<p>These are lofty principles. Can LLMs meet them? The field of alignment training comprises techniques that can be used to steer LLMs closer to following these 
<span class="keep-together">principles</span>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Can defining our desired values and principles in the prompt and asking the LLM to follow these principles result in a more aligned model? While it might be tempting to just ask the LLM to be a “good boy,” in practice this hasn’t seen all that much success.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Reinforcement Learning"><div class="sect1" id="id122">
<h1>Reinforcement Learning</h1>

<p>Since prompting LLMs<a data-type="indexterm" data-primary="alignment training" id="xi_alignmenttraining82721"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" id="xi_reinforcementlearningRL82721"/><a data-type="indexterm" data-primary="RL (reinforcement learning)" id="xi_RLreinforcementlearning82721"/> to be nice doesn’t work, we will need to tune the model in some way. Supervised fine-tuning<a data-type="indexterm" data-primary="supervised fine-tuning (SFT)" data-secondary="and reinforcement learning" data-secondary-sortas="reinforcement learning" id="id1174"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="supervised fine-tuning" id="id1175"/> (discussed in <a data-type="xref" href="ch06.html#llm-fine-tuning">Chapter 6</a>) on alignment datasets is an option. However, techniques like reinforcement learning have seen more success, which we will describe next in this section.</p>

<p>The values and principles we need the LLM to adhere to are defined by humans, and they involve a level of subjectivity. Thus, it makes sense to optimize the model directly on human feedback. The class of techniques to make this happen is called reinforcement learning from human feedback (RLHF)<a data-type="indexterm" data-primary="RLHF (reinforcement learning from human feedback)" id="xi_RLHFreinforcementlearningfromhumanfeedback829295"/><a data-type="indexterm" data-primary="reinforcement learning from human feedback (RLHF)" id="xi_reinforcementlearningfromhumanfeedbackRLHF829295"/>.</p>

<p>In traditional reinforcement learning, an agent interacts with its environment and performs actions to accomplish a task, using trial and error. After an action or a sequence of actions, the agent can receive a reward if it is on the right track, with the objective of the agent being to maximize the reward. This is specified through a reward function. However, in many real-world applications, defining success, and consequently the reward function, is hard.</p>

<p>In RLHF, the feedback is provided by a human-in-the-loop in an iterative fashion. To integrate human preferences into the LLM, a <em>reward model</em> needs<a data-type="indexterm" data-primary="reward model, and RLHF" id="id1176"/> to be trained. Various forms of feedback can be provided by human reviewers.</p>








<section data-type="sect2" data-pdf-bookmark="Types of Human Feedback"><div class="sect2" id="id123">
<h2>Types of Human Feedback</h2>

<p>Human feedback<a data-type="indexterm" data-primary="human feedback types" id="id1177"/> can be provided through one of these forms:</p>
<dl>
<dt>Binary feedback</dt>
<dd>
<p>In this setting<a data-type="indexterm" data-primary="binary feedback" id="id1178"/>, the feedback is provided as either yes/no (accept/reject).</p>
</dd>
<dt>Binary comparisons</dt>
<dd>
<p>In this setting<a data-type="indexterm" data-primary="binary comparisons" id="id1179"/>, the human evaluates outputs A and B and specifies their preference among the two.</p>
</dd>
<dt>Ranking</dt>
<dd>
<p>In this setting, the human evaluates<a data-type="indexterm" data-primary="ranking, in human feedback" id="id1180"/> a set of outputs and provides a rank ordering of preferences.</p>
</dd>
<dt>Corrective feedback</dt>
<dd>
<p>In this setting<a data-type="indexterm" data-primary="corrective feedback" id="id1181"/>, the human explicitly states what should have been the ideal output, potentially in natural language.</p>
</dd>
</dl>
</div></section>








<section data-type="sect2" data-pdf-bookmark="RLHF Example"><div class="sect2" id="id124">
<h2>RLHF Example</h2>

<p>Let’s describe a popular RLHF setup, pioneered by OpenAI. The alignment training consists of three distinct phases:</p>
<dl>
<dt>1. Supervised fine-tuning</dt>
<dd>
<p>In the first step, the pre-trained model is fine-tuned on a supervised<a data-type="indexterm" data-primary="supervised fine-tuning (SFT)" data-secondary="and RLHF" data-secondary-sortas="RLHF" id="id1182"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="supervised fine-tuning" id="id1183"/> dataset of human preferences. To achieve this, we first need to create a prompt dataset consisting of a diverse set of potential user requests to a language model. Human annotators then provide desired responses to these prompts. The prompts and human-annotated responses then constitute the fine-tuning dataset, which the pre-trained model is then trained on. This is typically a very large undertaking, with companies like OpenAI and Meta spending significant resources on gathering annotations.</p>
</dd>
<dt>2. Reward modeling</dt>
<dd>
<p>In this step, a diverse set of prompts<a data-type="indexterm" data-primary="reward model, and RLHF" id="id1184"/> is queried to the language model and multiple generations (responses) are extracted for each prompt. Human annotators then review the generations and provide feedback, either by providing a rank-ordered preference of generations or choosing the best generation. The generations along with the preference data are used to train a reward model. The reward model is trained to predict which output a human would prefer among a list of candidate outputs.</p>
</dd>
<dt>3. Proximal policy optimization (PPO)</dt>
<dd>
<p>Finally, the reward model is used to optimize<a data-type="indexterm" data-primary="proximal policy optimization (PPO)" id="id1185"/><a data-type="indexterm" data-primary="PPO (proximal policy optimization)" id="id1186"/> the pre-trained model that was fine-tuned in the first step. This is typically performed using an algorithm called PPO.</p>
</dd>
</dl>

<p>The process of training using PPO is as follows:</p>
<ol>
<li>
<p>The language model generates a response or a continuation of a prompt.</p>
</li>
<li>
<p>The reward model takes the query and response and outputs a scalar reward, representing the quality of fitness of the input.</p>
</li>
<li>
<p>The log-probabilities of the tokens in the query-response sequence are calculated, using the model being tuned (the SFT model) and a reference model (usually the pre-trained model before the SFT step). The KL-divergence between these two log-probs is calculated and used as a signal, along with the reward, to prevent the outputs from deviating too much from the reference model. This acts as a regularization step.</p>
</li>
<li>
<p>The model is trained using the PPO algorithm, with signals from steps 2 and 3.</p>
</li>

</ol>

<p>Let’s use the Hugging Face TRL library to perform RLHF. First, we need to train a reward model. In our example, we will train a reward model, which when provided with two LLM-generated outputs, will be able to predict which one of the two will be preferred by humans.</p>

<p>Very few high-quality datasets are publicly available, one of them being Anthropic<a data-type="indexterm" data-primary="Anthropic" data-secondary="hh-rlhf dataset" id="id1187"/><a data-type="indexterm" data-primary="hh-rlhf dataset" id="id1188"/>’s <a href="https://oreil.ly/kzSQf">hh-rlhf dataset</a>. We will use this dataset in our example.</p>

<p>Let’s explore this dataset in detail. The dataset consists of around 161,000 pairs of examples, each pair consisting of one <em>chosen</em> and one <em>rejected</em>. These examples correspond to human conversations with an LLM, with each pair being different responses by LLMs to the same prompt. For each pair of examples, human annotators chose their preferred response that best aligns with the values and principles the LLM is being aligned to.</p>

<p>Here is an example from the dataset:</p>

<p>Chosen:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Human</code><code class="p">:</code> <code class="n">How</code> <code class="n">can</code> <code class="n">I</code> <code class="n">hack</code> <code class="n">somebody</code><code class="s1">'s emails?</code><code class="w"/>

<code class="n">Assistant</code><code class="p">:</code> <code class="n">Why</code> <code class="n">would</code> <code class="n">you</code> <code class="n">want</code> <code class="n">to</code> <code class="n">do</code> <code class="n">that</code><code class="err">?</code>

<code class="n">Human</code><code class="p">:</code> <code class="n">I</code> <code class="n">need</code> <code class="n">to</code> <code class="n">see</code> <code class="n">an</code> <code class="n">email</code> <code class="n">that</code> <code class="ow">is</code> <code class="n">really</code> <code class="n">important</code> <code class="k">for</code> <code class="n">my</code> <code class="n">company</code>

<code class="n">Assistant</code><code class="p">:</code> <code class="n">Are</code> <code class="n">you</code> <code class="n">sure</code> <code class="n">you</code> <code class="n">want</code> <code class="n">to</code> <code class="n">do</code> <code class="n">that</code><code class="err">?</code> <code class="n">That</code><code class="err">’</code><code class="n">s</code> <code class="n">like</code> <code class="n">a</code> <code class="n">major</code> <code class="n">invasion</code> <code class="n">of</code>
<code class="n">someone</code><code class="err">’</code><code class="n">s</code> <code class="n">privacy</code><code class="o">.</code></pre>

<p>Rejected:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Human</code><code class="p">:</code> <code class="n">How</code> <code class="n">can</code> <code class="n">I</code> <code class="n">hack</code> <code class="n">somebody</code><code class="s1">'s emails?</code><code class="w"/>

<code class="n">Assistant</code><code class="p">:</code> <code class="n">Why</code> <code class="n">would</code> <code class="n">you</code> <code class="n">want</code> <code class="n">to</code> <code class="n">do</code> <code class="n">that</code><code class="err">?</code>

<code class="n">Human</code><code class="p">:</code> <code class="n">I</code> <code class="n">need</code> <code class="n">to</code> <code class="n">see</code> <code class="n">an</code> <code class="n">email</code> <code class="n">that</code> <code class="ow">is</code> <code class="n">really</code> <code class="n">important</code> <code class="k">for</code> <code class="n">my</code> <code class="n">company</code>

<code class="n">Assistant</code><code class="p">:</code> <code class="n">Who</code> <code class="n">are</code> <code class="n">you</code> <code class="n">trying</code> <code class="n">to</code> <code class="n">hack</code><code class="err">?</code></pre>

<p>After loading the dataset, you can use TRL’s <code>RewardTrainer</code> class for training the reward model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">trl</code> <code class="kn">import</code> <code class="n">RewardTrainer</code><code class="p">,</code> <code class="n">RewardConfig</code>
<code class="n">train</code> <code class="o">=</code> <code class="n">RewardTrainer</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>
    <code class="n">args</code><code class="o">=</code><code class="n">train_args</code><code class="p">,</code>
    <code class="n">tokenizer</code><code class="o">=</code><code class="n">tokenizer</code><code class="p">,</code>
    <code class="n">train_dataset</code><code class="o">=</code><code class="n">train_dataset</code><code class="p">,</code>

<code class="p">)</code></pre>

<p>Similarly, you can use TRL’s <code>PPOTrainer</code> class for performing the PPO step:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">trl</code> <code class="kn">import</code> <code class="n">PPOTrainer</code>

<code class="n">train</code> <code class="o">=</code> <code class="n">PPOTrainer</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="n">config</code><code class="p">,</code>
    <code class="n">tokenizer</code><code class="o">=</code><code class="n">tokenizer</code><code class="p">,</code>
    <code class="n">dataset</code><code class="o">=</code><code class="n">dataset</code>
<code class="p">)</code></pre>

<p>Refer to the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a> for the entire code. Next, let’s focus our attention on hallucinations, a key limitation of LLMs, and techniques to detect and mitigate them<a data-type="indexterm" data-startref="xi_RLHFreinforcementlearningfromhumanfeedback829295" id="id1189"/><a data-type="indexterm" data-startref="xi_reinforcementlearningRL82721" id="id1190"/><a data-type="indexterm" data-startref="xi_reinforcementlearningfromhumanfeedbackRLHF829295" id="id1191"/><a data-type="indexterm" data-startref="xi_RLreinforcementlearning82721" id="id1192"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Hallucinations"><div class="sect1" id="id125">
<h1>Hallucinations</h1>

<p>Colloquially, the word “hallucination”  is used to refer to instances<a data-type="indexterm" data-primary="hallucinations" id="xi_hallucinations812970"/> where the LLM generates something deemed to be incorrect. But LLMs can be incorrect for a wide variety of reasons. In this book, we will lead with a narrower definition of hallucinations: “An LLM is said to have hallucinated a generation if the generated text is not grounded to its training data or input context.”</p>

<p>Simply put, if an LLM generates text that is inconsistent with either its pre-training/fine-tuning data or input context, then we call it a hallucination. Hallucinations most commonly manifest as text with no factual basis. For a model to not hallucinate, it should be able to admit it doesn’t know a fact or a concept when it doesn’t know.</p>

<p>Here is an example of a hallucination. Consider the prompt fed to the <a href="https://oreil.ly/tXmYh">NousResearch Hermes LLM</a>:</p>
<blockquote><em>Prompt:</em> Provide a list of Ugandan 2020 Olympics medal winners along with their birthdays in a bulleted list.</blockquote>
<blockquote>
<p><em>Response:</em> Joshua Cheptegei (Athletics - Men’s 10,000 meters gold medalist, born September 19, 1996)</p>

<p>Jacob Kiplimo (Athletics - Men’s 10,000 meters silver medalist, born January 16, 1997)</p>

<p>Peruth Chemutai (Athletics - Women’s 3000 meters steeplechase bronze medalist, born March 10, 1999)</p></blockquote>

<p>Yes, it is true that these are three athletes who won medals for Uganda in the 2020 Summer Olympics, but a lot of content is hallucinated. Jacob Kiplimo was actually born on November 14, 2000, and the medal assignment is completely wrong; Jacob won the bronze, Peruth won the gold, and Joshua won the silver but also won a gold in another event.</p>

<p>A model that doesn’t hallucinate would have provided factual information and admit that it doesn’t know specific details.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Fine-tuning data on new knowledge can increase models’ tendency to hallucinate. <a href="https://oreil.ly/kgu26">Gekhman et al.</a> show that during fine-tuning, LLMs learn new knowledge in the fine-tuning data much slower than knowledge that was already present in the pre-training data. They also show that when the LLM learns new knowledge, it leads to overfitting<a data-type="indexterm" data-primary="overfitting of model" id="id1193"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="overfitting with new knowledge" id="id1194"/>, causing an increase in hallucinations even for unrelated questions. If you want to teach your model entirely new knowledge, I suggest using the continued pre-training setup with techniques like replay, etc., described in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Mitigating Hallucinations"><div class="sect1" id="id126">
<h1>Mitigating Hallucinations</h1>

<p>One of the biggest sources of hesitancy<a data-type="indexterm" data-primary="hallucinations" data-secondary="mitigating" id="xi_hallucinationsmitigating815940"/> in adopting LLM-based tools and software is the system’s trustworthiness or lack thereof. Trustworthiness is most affected by the presence of hallucinations. Therefore, there is considerable research into preventing or reducing the tendency of models to hallucinate. Let’s explore some common 
<span class="keep-together">techniques</span>.</p>

<p>At a product design level, you can reduce hallucination risk by simply not asking LLMs questions that you know it wouldn’t be able to answer. This is not always possible, especially when you allow your users to directly interact with the model. It is also not easy to determine what an LLM knows and does not know.</p>

<p><a data-type="xref" href="#knowledge-quadrant">Figure 8-1</a> depicts a knowledge quadrant across knowledge and awareness dimensions. Ideally, an LLM should acknowledge its lack of knowledge when asked about a fact or concept it genuinely does not know. In <a data-type="xref" href="#knowledge-quadrant">Figure 8-1</a>, we see that there can be four types of knowledge<a data-type="indexterm" data-primary="knowledge types, mitigating hallucinations" id="id1195"/>:</p>
<dl>
<dt>Known knowns</dt>
<dd>
<p>The LLM knows this knowledge/skill and is able to utilize it.</p>
</dd>
<dt>Unknown knowns</dt>
<dd>
<p>The LLM knows this knowledge/skill but is not able to utilize it effectively (can be unlocked by fine-tuning or in-context learning).</p>
</dd>
<dt>Known unknowns</dt>
<dd>
<p>The LLM knows that it does not know this knowledge.</p>
</dd>
<dt>Unknown unknowns</dt>
<dd>
<p>The LLM does not know that it does not know this knowledge, leading to 
<span class="keep-together">hallucinations</span>.</p>
</dd>
</dl>

<figure><div id="knowledge-quadrant" class="figure">
<img src="assets/dllm_0801.png" alt="knowledge-quadrant" width="600" height="462"/>
<h6><span class="label">Figure 8-1. </span>Knowledge quadrant</h6>
</div></figure>

<p>To determine the level of self-knowledge a model possesses, <a href="https://oreil.ly/3DxdZ">Yin et al.</a>, created a dataset called SelfAware<a data-type="indexterm" data-primary="SelfAware dataset" id="id1196"/> composed of answerable and unanswerable questions. Self-knowledge refers to the knowledge an LLM possesses about whether it knows a fact or concept or not.  In their experiments, they show that larger models possess more self-knowledge. They also show that instruction-tuned models<a data-type="indexterm" data-primary="instruction-tuned models" data-secondary="self-knowledge level" id="id1197"/><a data-type="indexterm" data-primary="base models" data-secondary="self-knowledge level" id="id1198"/><a data-type="indexterm" data-primary="models" data-secondary="base" id="id1199"/> possess more self-knowledge than base models.</p>

<p>An important way to assess a model’s self-knowledge is through its output uncertainty. If a model is less confident about its predictions, as measured through its output probabilities, we can assume a higher hallucination risk. For this approach to be valid, the model has to be well calibrated. As <a data-type="xref" href="ch06.html#llm-fine-tuning">Chapter 6</a> introduced, a model is well calibrated if there is a correlation between its output probability<a data-type="indexterm" data-primary="output probability" id="id1200"/> values and task 
<span class="keep-together">accuracy</span>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p><a href="https://oreil.ly/VVY-i">Kadavath et al.</a> show that techniques like RLHF reduce model calibration<a data-type="indexterm" data-primary="calibration, model" id="id1201"/>.</p>
</div>

<p>A key technique to address hallucinations is grounding the model to factual data sources<a data-type="indexterm" data-primary="ground truth" data-secondary="importance in addressing hallucinations" id="id1202"/>. This is done by retrieving knowledge from a data store specific to the given task and feeding it to the model in the prompt along with the task instruction and input. This paradigm is called RAG, which we will discuss in <a data-type="xref" href="part03.html#part3">Part III</a> of the book.</p>

<p>RAG<a data-type="indexterm" data-primary="retrieval-augmented generation (RAG)" data-secondary="and hallucination problem" data-secondary-sortas="hallucination problem" id="id1203"/> is not a panacea for the hallucination problem for the following reasons:</p>

<ul class="less_space pagebreak-before">
<li>
<p>Feeding ground-truth factual data in the prompt reduces hallucinations but does not eliminate them completely, especially when the context is large.</p>
</li>
<li>
<p>Using RAG shifts the bottleneck toward the retrieval process. If the retrieval process is not able to return the relevant data needed, the model may do worse at the task than if no RAG was used.</p>
</li>
<li>
<p>In many cases, we do not have access to the ground-truth data; hence we cannot feed it as input context.</p>
</li>
</ul>

<p>Now let’s look at techniques that do not depend on us fetching ground-truth data.</p>








<section data-type="sect2" data-pdf-bookmark="Self-Consistency"><div class="sect2" id="id250">
<h2>Self-Consistency</h2>

<p>We can use self-consistency<a data-type="indexterm" data-primary="self-consistency" data-secondary="mitigating hallucinations" id="id1204"/>, which we first introduced in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, to detect the possibility of hallucinations. The idea is simple: we generate the output multiple times and detect the inconsistencies between the different generations. The more they are inconsistent, the less confident the model is about the answer, and the more likely the hallucination.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Chain-of-Actions"><div class="sect2" id="id127">
<h2>Chain-of-Actions</h2>

<p>Self-verification<a data-type="indexterm" data-primary="self-verification, mitigating hallucinations" id="xi_selfverificationmitigatinghallucinations820418"/><a data-type="indexterm" data-primary="chain-of-actions, mitigating hallucinations" id="xi_chainofactionsmitigatinghallucinations820418"/><a data-type="indexterm" data-primary="CoVe (chain-of-verification), mitigating hallucinations" id="xi_CoVechainofverificationmitigatinghallucinations820418"/><a data-type="indexterm" data-primary="chain-of-verification (CoVe), mitigating hallucinations" id="xi_chainofverificationCoVemitigatinghallucinations820418"/> is another commonly used technique. An example of this is Chain-of-Verification (CoVe), a prompting technique introduced by researchers from <a href="https://oreil.ly/ubp1t">Meta</a>. An example implementation of CoVe works as follows:</p>
<ol>
<li>
<p>The model first generates a response to the given prompt.</p>
</li>
<li>
<p>Given the prompt and the response, the model generates verification questions to check the veracity of the response.</p>
</li>
<li>
<p>Each of the verification questions is fed to the model separately.</p>
</li>
<li>
<p>The responses to these verification questions are then compared to the original response, and if there are any inconsistencies, the original response is edited to integrate the responses from the verification questions.</p>
</li>

</ol>

<p>As an example, for the earlier query:</p>
<blockquote><em>Prompt:</em> Provide a list of Ugandan Olympic medal winners along with their birthdays in a bulleted list.</blockquote>

<p>When asked to generate a list of verification questions, the model generates:</p>
<blockquote>
<p>Who won the gold medal in the men’s 10,000m event in the 2020 Tokyo Olympics</p>

<p>How many Ugandan athletes won medals at the 2020 Tokyo Olympics?</p>

<p>What event did Jacob Kiplimo compete in and win a medal for Uganda at the 2020 Tokyo Olympics?</p>

<p>In which event did Peruth Chemutai win a medal for Uganda at the 2020 Tokyo 
<span class="keep-together">Olympics</span>?</p>

<p>What year were Joshua Cheptegei and Jacob Kiplimo born?</p>

<p>What year was Peruth Chemutai born?</p>

<p>How many gold medals did Uganda win in total at the 2020 Tokyo Olympics?</p></blockquote>

<p>Asking the same model the following question:</p>
<blockquote>Who won the gold medal in the men’s 10,000m event in the 2020 Tokyo Olympics?</blockquote>

<p>We get the response:</p>
<blockquote>In the 2020 Tokyo Olympics, the men’s 10,000m event was won by Selemon Barega of Ethiopia. He finished with a time of 27:43.22, beating the silver medalist Joshua Cheptegei of Uganda by 0.32 seconds.</blockquote>

<p>This response is inconsistent with the original response, which claimed that Joshua Cheptegai won the gold medal! The response to the verification question is in fact the correct response.</p>

<p>Therefore, we can see how methods like CoVe can be valuable in reducing hallucination risk. Note that it is possible that the responses to the verification questions are also hallucinated, so this method does not entirely address the hallucination issue. However, one can expect the responses to the verification questions to be less affected by hallucinations because they are more direct questions<a data-type="indexterm" data-startref="xi_selfverificationmitigatinghallucinations820418" id="id1205"/><a data-type="indexterm" data-startref="xi_chainofactionsmitigatinghallucinations820418" id="id1206"/><a data-type="indexterm" data-startref="xi_CoVechainofverificationmitigatinghallucinations820418" id="id1207"/><a data-type="indexterm" data-startref="xi_chainofverificationCoVemitigatinghallucinations820418" id="id1208"/>.</p>

<p>Let’s now discuss hallucination reduction using recitation.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Recitation"><div class="sect2" id="id128">
<h2>Recitation</h2>

<p>With the recitation<a data-type="indexterm" data-primary="recitation, mitigating hallucinations" id="id1209"/> technique, we prompt the LLM to generate one or more passages about the given question and then generate the answer based on the passages generated. The reasoning behind this approach is that directly answering questions diverges from the learning objectives on which the language model was pre-trained. Recitation serves as the intermediate step that aligns more closely to the original learning objective of the model, like next-token prediction.</p>

<p>We can use few-shot prompting for soliciting recitations. The prompt looks like:</p>
<blockquote>
<p>Query: &lt;query&gt;</p>

<p>Recitation1: &lt;recitation&gt;</p>

<p>Recitation2: &lt;recitation&gt;</p>

<p>…</p>

<p>RecitationN: &lt;recitation&gt;</p>

<p>Query: &lt;query&gt;</p>

<p>Recitation1:</p></blockquote>

<p>We can generate a single recitation or multiple recitations. If we generate multiple recitations, we can generate a candidate response using each of them and then use self-consistency to pick the final answer. You can also fine-tune your model to prime it to be better at generating effective recitations.</p>

<p>The recitation method typically consumes fewer tokens than chain-of-actions, but I find the latter to be more effective.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Sampling Methods for Addressing Hallucination"><div class="sect2" id="id129">
<h2>Sampling Methods for Addressing Hallucination</h2>

<p>The degree of hallucination<a data-type="indexterm" data-primary="sampling methods, addressing hallucination" id="id1210"/><a data-type="indexterm" data-primary="decoding strategies" data-secondary="sampling methods for addressing hallucination" id="id1211"/> also depends on the decoding method used. Recall our discussion on decoding algorithms in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>. <a href="https://oreil.ly/ZTCtv">Lee et al.</a> show that top-p sampling leads to more hallucinations compared to greedy decoding. This is to be expected as the sampling step leads to more randomness, sometimes leading to the wrong token being picked.</p>

<p>One way to address increased hallucination risk due to sampling algorithms is to use a technique like factual-nucleus sampling<a data-type="indexterm" data-primary="factual-nucleus sampling" id="id1212"/>, which <a href="https://oreil.ly/D12xT">Lee et al. introduced</a>. This technique is based on the observation that as the length of the generated sequence increases, there will be fewer valid candidate tokens for the next token generation. Thus, the randomness of the sampling algorithm is reduced as the length of the generated text increases, by reducing the p value in the top-p<a data-type="indexterm" data-primary="top-p sampling" id="id1213"/> decoding 
<span class="keep-together">algorithm</span>.</p>

<p>The formula looks like this:</p>
<div data-type="equation">
pt = max{ω, p × λ t−1 }
</div>

<p>where <em>t</em> refers to the generation step.</p>

<p>There are three tunable parameters:</p>
<dl>
<dt>Decay rate (<em>λ</em>)</dt>
<dd>
<p>The <em>p</em> value of the algorithm is decayed by a decay rate at every step of the 
<span class="keep-together">generation</span>.</p>
</dd>
<dt>Reset (<em>p</em>)</dt>
<dd>
<p>The <em>p</em> value might decay very quickly, thus degenerating to a greedy algorithm. To prevent this, we can reset the <em>p</em> value at regular intervals, say after each sentence is generated.</p>
</dd>
<dt>Lower-bound (<em>ω</em>)</dt>
<dd>
<p>To continue maintaining the advantages of the top-p algorithm, we can prevent the <em>p</em> value from getting too low by enforcing a lower bound.</p>
</dd>
</dl>

<p>This method comes with a tradeoff; lowering the <em>p</em> value reduces hallucination risk but also decreases diversity of token generation, causing a loss in performance.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Decoding by Contrasting Layers"><div class="sect2" id="id130">
<h2>Decoding by Contrasting Layers</h2>

<p>The principle<a data-type="indexterm" data-primary="decoding strategies" data-secondary="by contrasting layers" id="id1214"/><a data-type="indexterm" data-primary="decoding by contrasting layers (DoLa)" id="id1215"/><a data-type="indexterm" data-primary="DoLa (decoding by contrasting layers)" id="id1216"/><a data-type="indexterm" data-primary="layers" data-secondary="decoding by contrasting layers" id="id1217"/> behind <a href="https://oreil.ly/V4h3d"><em>d</em>ecoding by c<em>o</em>ntrasting <em>la</em>yers (DoLa)</a> is that factual knowledge is encoded in the topmost layer of the Transformer, just like syntactic information is encoded in the lower layers. Therefore, we can emphasize the knowledge encoded in the higher layers to promote more factual outputs. DoLa achieves this by using a technique<a data-type="indexterm" data-primary="contrastive decoding" id="id1218"/> called <em>contrastive decoding</em>, in which the next token probability for each token is calculated by taking the difference in logits between a higher layer and a lower layer.</p>

<p>DoLa is available through Hugging Face<a data-type="indexterm" data-primary="Hugging Face" data-secondary="DoLa" id="id1219"/>. Let’s look at an example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoTokenizer</code><code class="p">,</code> <code class="n">AutoModelForCausalLM</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">accelerate.test_utils.testing</code> <code class="kn">import</code> <code class="n">get_backend</code>

<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"huggyllama/llama-7b"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">AutoModelForCausalLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"huggyllama/llama-7b"</code><code class="p">,</code>
                                   <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code>

<code class="n">text</code> <code class="o">=</code> <code class="s2">"Who shared a dorm with Harry Potter?"</code>
<code class="n">inputs</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"pt"</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">,</code> <code class="n">do_sample</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
                        <code class="n">max_new_tokens</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">dola_layers</code><code class="o">=</code><code class="s1">'high'</code><code class="p">)</code>
<code class="n">tokenizer</code><code class="o">.</code><code class="n">batch_decode</code><code class="p">(</code><code class="n">output</code><code class="p">[:,</code> <code class="n">inputs</code><code class="o">.</code><code class="n">input_ids</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]:],</code>
                       <code class="n">skip_special_tokens</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>The <code>dola_layers</code> argument should be used to activate DoLa decoding. <code>dola_layers</code> can be either a string or a list of integers. If it is a string, it should be either <code>'high'</code> or <code>'low'</code>. This means that the last layer is contrasted with the higher or the lower layers of the model. You can also specify a list of integers representing layer numbers. Again, the final layer of the model will be contrasted with the layers specified in your list.</p>

<p>To reduce repetitiveness induced by DoLa, you can set a repetition penalty through the <code>repetition_penalty</code> argument (this is set by default).  The authors of DoLa suggest contrasting with higher layers for tasks with shorter answer lengths, and contrasting with lower layers otherwise. They also recommend not using DoLa for smaller LLMs. This is because the different layers in smaller models are not distinctive enough to take advantage of this approach.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1220">
<h1>Inducing Hallucination Reduction During Pre-Training</h1>
<p><a href="https://oreil.ly/CR2pa">Lee et al.</a> also propose methods for adapting the pre-training<a data-type="indexterm" data-primary="pre-training of data" data-secondary="inducing hallucination reduction" id="id1221"/> or continuous pre-training process so that the resulting LLM hallucinates less.</p>

<p>Due to the pre-training setup, training documents are often broken into chunks. However, splitting these documents might remove the context required to understand the text. The authors propose prefixing chunks with a topic identifier, to preserve the connection between the facts in the document chunk and the entities with which the facts are associated.</p>

<p>During pre-training, the loss is applied uniformly to all tokens generated in the sequence. However, Lee et al. observe that the tokens at the end of the sequence are more critical for factuality, since their content depends on the content that was generated prior. They propose a sentence completion loss, where the loss is calculated over only the latter part of the generated sequence<a data-type="indexterm" data-startref="xi_hallucinationsmitigating815940" id="id1222"/>.</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="In-Context Hallucinations"><div class="sect1" id="id131">
<h1>In-Context Hallucinations</h1>

<p>So far, we have focused on hallucinations<a data-type="indexterm" data-primary="hallucinations" data-secondary="in-context type" id="xi_hallucinationsincontexttype834742"/><a data-type="indexterm" data-primary="in-context training" id="xi_incontexttraining834742"/><a data-type="indexterm" data-primary="closed-book hallucinations" id="xi_closedbookhallucinations834742"/> emanating from the model trying to generate from its parametric memory. We can also have in-context hallucinations, also called closed-book hallucinations, where the model depends on data provided in the context to influence its generation, like in RAG.</p>

<p>For example, consider the passage:</p>
<blockquote>
<p>At the 2020 Summer Olympics in Tokyo, Uganda won three medals:</p>

<p>Joshua Cheptegei – Gold in Men’s 5000m</p>

<p>Peruth Chemutai – Gold in Women’s 3000m Steeplechase</p>

<p>Jacob Kiplimo – Bronze in Men’s 10,000m</p>

<p>These medals marked a historic achievement for Uganda, with both Cheptegei and Chemutai winning gold in their respective events.</p></blockquote>

<p>fed to the LLM as context for the following query:</p>
<blockquote>Provide a list of Ugandan 2020 Olympics medal winners along with their birthdays in a bulleted list.</blockquote>

<p>While the passage contains all the medal winners along with the events, their birthdays are not present, leading the model to hallucinate them. Here is the model’s response:</p>
<blockquote class="less_space pagebreak-before">
<p>Here is a list of Ugandan 2020 Olympics medal winners with their birthdays:</p>

<ul>
<li>
<p>Joshua Cheptegei (born September 12, 1996) - Gold medal in Men’s 5000m</p>
</li>
<li>
<p>Peruth Chemutai (born August 10, 1999) - Gold medal in Women’s 3000m 
<span class="keep-together">Steeplechase</span></p>
</li>
<li>
<p>Jacob Kiplimo (born December 16, 1997) - Bronze medal in Men’s 10,000m</p>
</li>
</ul></blockquote>

<p>Due to the knowledge provided in the context, the model gets the names of the athletes, their medals, and their events correct, but gets the birthdays wrong for Peruth and Jacob.</p>

<p>How can we detect and mitigate these hallucinations? <a href="https://oreil.ly/czTU_">Chuang et al.</a> propose Lookback Lens<a data-type="indexterm" data-primary="Lookback Lens" id="id1223"/>, a technique that utilizes attention maps to detect hallucinations. In this technique, we calculate a <em>lookback ratio</em>, defined as the ratio of the attention weights on the context tokens to the attention weights on the newly generated tokens. The lookback ratio is calculated at each attention head of each layer of the model. These ratios are used as features to train a linear classifier model.</p>

<p>The classifier model<a data-type="indexterm" data-primary="classifier model, mitigating hallucinations" id="id1224"/> can also be employed to reduce hallucinations during generation. During generation, a few candidate phrases (sequence of tokens) are generated for the next step. The lookback ratios for these candidates are calculated and fed to the classifier model. The candidate assigned the lowest probability by the classifier can be chosen to be generated, as this is the least likely to be hallucinated.</p>

<p>Using a classifier-based decoding strategy can be a massive drag on system latency, however. These approaches should be used only if latency isn’t a prime consideration<a data-type="indexterm" data-startref="xi_hallucinationsincontexttype834742" id="id1225"/><a data-type="indexterm" data-startref="xi_incontexttraining834742" id="id1226"/><a data-type="indexterm" data-startref="xi_closedbookhallucinations834742" id="id1227"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1228">
<h1>Knowledge Overshadowing</h1>
<p>Knowledge overshadowing<a data-type="indexterm" data-primary="hallucinations" data-secondary="knowledge overshadowing" id="id1229"/><a data-type="indexterm" data-primary="knowledge overshadowing" id="id1230"/><a data-type="indexterm" data-primary="overshadowing, knowledge" id="id1231"/> is another potential reason for hallucination. In their experiments, <a href="https://oreil.ly/z8PeK">Zhang et al.</a> show that when an LLM is prompted with multiple conditions, the more dominant condition overshadows, leading to 
<span class="keep-together">hallucinations</span>.</p>

<p>As an example, consider the prompt: “Give me the name of a female fighter in kickboxing.”
The response is: “One famous female kickboxer is Buakaw Banchamek, a Thai fighter who has won numerous titles in both kickboxing and Muay Thai.” While Buakaw is a kickboxer, he is actually a man! In this case the generation was conditioned on two factors (female fighters, kickboxer), but the kickboxer condition has overshadowed the female condition, resulting in a hallucination.</p>

<p>Zhang et al. show that overshadowing happens due to imbalances in the training data. They also show that the hallucination rate increases with the imbalance rate.</p>
</div></aside>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Hallucinations Due to Irrelevant Information"><div class="sect1" id="id132">
<h1>Hallucinations Due to Irrelevant Information</h1>

<p>The presence of irrelevant information<a data-type="indexterm" data-primary="hallucinations" data-secondary="due to irrelevant information" id="id1232"/><a data-type="indexterm" data-primary="irrelevant information, hallucinations due to" id="id1233"/> in the context can also lead to hallucinations. As an example, consider this prompt:</p>
<blockquote>
<p><em>Prompt:</em> Indianapolis, the capital of Indiana, is known for its vibrant cultural scene, including museums, theaters, and sporting events, especially the Indianapolis 500 race. The city is also a major hub for commerce and industry, with a growing tech scene and numerous corporate headquarters.</p>

<p>Where is the actor Lily Gao born?</p></blockquote>

<p>The response is as follows:</p>
<blockquote><em>Response:</em> Lily Gao, an American actress known for her roles in various TV series and films, was born on October 1, 1994, in Indianapolis, Indiana, USA.</blockquote>

<p>However, in reality Lily Gao was born in Canada! The presence of irrelevant information in the prompt causes the LLM to hallucinate.</p>

<p>To mitigate this issue, <a href="https://oreil.ly/aqsxN">Weston et al.</a> propose a technique called System 2 Attention (S2A)<a data-type="indexterm" data-primary="System 2 Attention (S2A)" id="id1234"/><a data-type="indexterm" data-primary="S2A (System 2 Attention)" id="id1235"/>. In this technique, the LLM is first asked to regenerate its context, removing any information irrelevant to answering the question. Next, the LLM is prompted with the regenerated context and the final answer is generated.</p>

<p>As an example, consider this math problem with a distractor sentence:</p>
<blockquote><em>Prompt:</em> Sarah has 5 apples. She buys 3 more apples from the store. Max sells 3 apples to the store. How many apples does Sarah have now?</blockquote>

<p>We issue the following prompt:</p>
<blockquote><em>Prompt:</em> Regenerate the context removing any information that is irrelevant to answering the question.</blockquote>

<p>The response is as follows:</p>
<blockquote><em>Response:</em> Sarah has 5 apples. She buys 3 more apples from the store. How many apples does Sarah have now?</blockquote>

<p>This can be fed back to the model to provide the correct answer.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can also implement S2A in a single prompt, by asking the model to regenerate the context followed by the final answer. However, performing this in two prompts has shown to be more 
<span class="keep-together">effective</span>.</p>
</div>

<p>Next, let’s explore the reasoning capabilities of LLMs and showcase techniques for improving them<a data-type="indexterm" data-startref="xi_alignmenttraining82721" id="id1236"/><a data-type="indexterm" data-startref="xi_hallucinations812970" id="id1237"/>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Reasoning"><div class="sect1" id="id133">
<h1>Reasoning</h1>

<p>In <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, we discussed the limitations of language models and pointed to reasoning<a data-type="indexterm" data-primary="reasoning" id="xi_reasoning8447106"/> as one of the biggest limitations. In this section, let’s dive into it in more detail to understand what reasoning entails, how well language models perform reasoning, and how to improve their reasoning capabilities.</p>

<p>First, let’s define reasoning:</p>
<blockquote>
<p>Natural language reasoning is a process to integrate multiple knowledge (e.g. encyclopedic knowledge and commonsense knowledge) to derive some new conclusions about the (realistic or hypothetical) world. Knowledge can be from both explicit and implicit sources. Conclusions are assertions or events assumed to be true in the world, or practical actions.</p>
<p data-type="attribution"><a href="https://oreil.ly/7NsBF">Yu et al.</a></p>
</blockquote>

<p>Reasoning can be classified into several different types. Here are a few forms of non-mutually-exclusive reasoning categories:</p>








<section data-type="sect2" data-pdf-bookmark="Deductive Reasoning"><div class="sect2" id="id134">
<h2>Deductive Reasoning</h2>

<p>Deductive reasoning<a data-type="indexterm" data-primary="deductive reasoning" id="id1238"/><a data-type="indexterm" data-primary="reasoning" data-secondary="types of" id="xi_reasoningtypesof846020"/> uses logic to draw conclusions from one or more premises.</p>

<p>As an example, consider the following passage:</p>
<blockquote>Mr. Shockley was allergic to mushrooms. The dish “Golden Travesty” has mushrooms in it.</blockquote>

<p>Based on this set of premises, we can deduce that Mr. Shockley should stay far away from the Golden Travesty dish.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Inductive Reasoning"><div class="sect2" id="id135">
<h2>Inductive Reasoning</h2>

<p>Inductive reasoning<a data-type="indexterm" data-primary="inductive reasoning" id="id1239"/> involves making generalizations based on a set of observations. The generalizations are plausible and probabilistic, rather than guaranteed, based on the strength of the observations.</p>

<p>As an example, upon observing hundreds or even thousands of round manhole covers, one can conclude that manhole covers are generally round. This is not guaranteed to be true, as there might be cities with different manhole cover shapes, but based on the evidence we have so far, we can make that probabilistic conclusion.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Abductive Reasoning"><div class="sect2" id="id136">
<h2>Abductive Reasoning</h2>

<p>Abductive reasoning<a data-type="indexterm" data-primary="abductive reasoning" id="id1240"/> involves analyzing a set of observations and concluding with the most likely explanation:</p>
<blockquote>
<p>Observation: The street is wet. There are water puddles on the sidewalk. People have umbrellas in their hands.</p>

<p>Explanation: It rained recently.</p></blockquote>

<p>Abductive reasoning offers the most likely explanation but is not guaranteed to be true. In our example, it is possible that the street is wet because an angry man emptied an entire truckful of water on the streets, but it’s not very probable. As more evidence comes into the picture, the strength of the explanation increases.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Common Sense Reasoning"><div class="sect2" id="id137">
<h2>Common Sense Reasoning</h2>

<p>Common sense reasoning<a data-type="indexterm" data-primary="common sense reasoning" id="id1241"/><a data-type="indexterm" data-startref="xi_reasoningtypesof846020" id="id1242"/> refers to utilizes a shared understanding of the world to make assumptions about the physical world or human relationships. Common sense reasoning relies on implicit knowledge of the world that is not usually verbalized. For example:</p>
<blockquote>She saw him prancing around the hall with a glass in his hand, held upside down.</blockquote>

<p>While not explicitly mentioned in the text, common sense would dictate that the glass does not contain any liquids given it is upside down.</p>

<p>Other forms of reasoning include mathematical (usually based on deductions), causal (identifying cause-and-effect relationships), analogical (drawing comparisons between two things or concepts), and moral (evaluating situations and decisions based on moral principles and values).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1243">
<h1>Reasoning as Subgraph Pattern Matching</h1>
<p>Do LLMs really reason<a data-type="indexterm" data-primary="subgraph pattern matching, as reasoning" id="id1244"/><a data-type="indexterm" data-primary="reasoning" data-secondary="as subgraph pattern matching" data-secondary-sortas="subgraph pattern matching" id="id1245"/>? Does it even matter if LLMs reason the way humans do, as long as they get the job done? What if they are “just” sophisticated pattern matchers?</p>

<p>One school of thought argues that it doesn’t matter if all that the models are doing is sophisticated pattern matching; if it is good enough to solve the task, then so be it. Moreover, we do not know a great deal about how the human brain performs reasoning, and maybe that is what it is doing as well.</p>

<p>However, understanding what is happening under the hood when language models are supposed to be reasoning does matter a lot. It helps us understand the current limitations of LLMs and provides us with intuitions on what classes of problems can be solved.</p>

<p><a href="https://oreil.ly/AyHRH">Dziri et al.</a> show that Transformers<a data-type="indexterm" data-primary="Transformers library" id="id1246"/><a data-type="indexterm" data-primary="linear subgraph matching" id="id1247"/> are likely performing something called <a href="https://oreil.ly/B_44q">linearized subgraph matching</a>. In this perspective, a task is said to be represented as a directed graph, where the directed graph represents the steps involved in solving the task. The subtasks making up the tasks correspond to subgraphs within the directed graph. Transformers solve subtasks by matching the corresponding subgraph to subgraphs seen during the training data. I recommend reading the entirety of Dziri et al.’s paper to get a better understanding of this topic.</p>
</div></aside>
<div data-type="tip"><h6>Tip</h6>
<p><a href="https://oreil.ly/vkTjt">Cheng et al.</a> show that LLMs perform much better on inductive reasoning than deductive reasoning.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1248">
<h1>Exercise</h1>
<p>How does your favorite LLM perform at these reasoning tasks? The book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a> contains a set of reasoning exercises corresponding to the reasoning categories we described in this section. Do LLMs perform better at some forms of reasoning than others?</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Inducing Reasoning in LLMs"><div class="sect1" id="id251">
<h1>Inducing Reasoning in LLMs</h1>

<p>The simplest way of improving reasoning<a data-type="indexterm" data-primary="reasoning" data-secondary="inducing in LLMs" id="xi_reasoninginducinginLLMs852340"/> in LLMs is to use prompting techniques like chain-of-thought, introduced in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>. CoT prompts the model to solve the problem step by step, thus generating the process leading up to the answer rather than generating the answer directly.</p>








<section data-type="sect2" data-pdf-bookmark="Verifiers for Improving Reasoning"><div class="sect2" id="id138">
<h2>Verifiers for Improving Reasoning</h2>

<p>So, LLMs may not be all that great at producing the right answer<a data-type="indexterm" data-primary="verifiers for reasoning improvement" id="id1249"/><a data-type="indexterm" data-primary="reasoning" data-secondary="verifiers for improving" id="id1250"/> to a question that requires multistep reasoning. But all hope is not lost. We can leverage the generative capabilities of LLMs to generate a plausible set of candidate solutions. These candidates can then be assessed by a verifier, which can identify the correct answer. This is possible in instances where it is much easier to verify whether an answer to a task is correct than to solve the task itself.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Just because LLMs can generate plausible candidate solutions for a question is not evidence of their reasoning abilities. For many types of questions, there are a very limited set of plausible solutions.</p>
</div>

<p>Verifiers can be based on LLMs<a data-type="indexterm" data-primary="LLM-as-a-judge" id="id1251"/>, called <em>LLM-as-a-judge</em>, or can be external models or even symbolic verifiers. Two common ways of operationalizing the generator-verifier system are iterative backprompting and top-k guessing.</p>










<section data-type="sect3" data-pdf-bookmark="Iterative backprompting"><div class="sect3" id="id139">
<h3>Iterative backprompting</h3>

<p>In this process, an LLM generates a proposed solution<a data-type="indexterm" data-primary="iterative backprompting" id="id1252"/><a data-type="indexterm" data-primary="prompting" data-secondary="iterative backprompting" id="id1253"/> to a given problem that requires reasoning. One or more verifiers assess the proposed solution and provide feedback. The feedback can convey whether the solution is correct or incorrect, and in case of the latter, a description of errors present in the proposed solution.</p>

<p>The LLM takes the feedback as input and generates the solution again, which is again passed to the verifier. The loop continues until the LLM generates the correct answer or the maximum number of iterations is reached.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Top-k guessing"><div class="sect3" id="id140">
<h3>Top-k guessing</h3>

<p>In this technique, k solutions<a data-type="indexterm" data-primary="top-k guessing" id="id1254"/> are generated for a given task, and the verifier assesses them and chooses the correct solution if it exists. A relatively high temperature (&gt;1) is used during decoding to generate a diverse set of solutions.</p>

<p><a href="https://oreil.ly/4_MxJ">Kambhampati et al.</a> show that top-k guessing exhibits similar performance levels as iterative backprompting.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Inference-Time Computation"><div class="sect2" id="id141">
<h2>Inference-Time Computation</h2>

<p>This might well be the most significant topic of 2025 and beyond<a data-type="indexterm" data-primary="inference-time computation" id="xi_inferencetimecomputation854965"/><a data-type="indexterm" data-primary="reasoning" data-secondary="inference-time computation" id="xi_reasoninginferencetimecomputation854965"/>. As of this book’s writing, scaling<a data-type="indexterm" data-primary="scaling up inference-time compute" id="id1255"/><a data-type="indexterm" data-primary="datasets" data-secondary="scaling up inference-time compute" id="id1256"/> up pre-training seems to be providing diminishing returns. Therefore, there is a hunt for new scaling dimensions. The most promising among them is scaling up inference-time compute. The premise is simple. For a given query, instead of generating the final answer right away, what if we expend compute before arriving at the final answer? Can we improve the performance of the model with more compute? Turns out, we can!
Let’s discuss this new scaling avenue in detail.</p>










<section data-type="sect3" data-pdf-bookmark="Repeated sampling"><div class="sect3" id="id142">
<h3>Repeated sampling</h3>

<p>The most simple and common inference-time compute technique is repeated sampling<a data-type="indexterm" data-primary="repeated sampling, inference-time compute" id="id1257"/>. In this technique, we sample from the model several times in response to a given query. We could then use techniques like self-consistency or external verifiers<a data-type="indexterm" data-primary="self-consistency" data-secondary="repeated sampling for scaling" id="id1258"/> to choose the right answer. You can also combine self-consistency and external verifiers to provide a weighted score for each candidate solution. A simple way to generate diverse samples is to use a high sampling temperature.</p>

<p>Another simple approach is to use iterative generation<a data-type="indexterm" data-primary="iterative generation" id="id1259"/>, as shown earlier in this chapter. The model comes up with a candidate solution and a verifier provides feedback. The model iteratively improves its response using the verifier feedback until it 
<span class="keep-together">reaches</span> the final answer or the maximum number of iterations. Simpler problems can use this approach; for more complex problems, repeated sampling (best-of-k) approaches are more effective.</p>

<p>Yet another approach is to augment the context across which the generation takes place. CoT prompting<a data-type="indexterm" data-primary="chain-of-thought (CoT) prompting" id="id1260"/><a data-type="indexterm" data-primary="CoT (chain-of-thought) prompting" id="id1261"/><a data-type="indexterm" data-primary="prompting" data-secondary="chain-of-thought" id="id1262"/> is the easiest way to achieve that. Instead of the model directly generating the answer, it first generates the process toward generating the answer (i.e., the thought process).</p>

<p>In essence, a language model generates a probability distribution P(Y | X) where X is the input context and the previously generated tokens. The goal is to modify X to maximize the probability of Y being the correct answer.</p>

<p><a href="https://oreil.ly/Dc_Fc">Jin et al.</a> show some important experiments on this. First, the length of the reasoning steps matters to the performance. The more tokens used to represent the reasoning steps, the better the model’s performance. Conversely, they show that shortening the reasoning information even while keeping all the details intact negatively impacts the model’s reasoning capabilities.</p>

<p>Jin et al. also show that errors in the reasoning steps do not impact the performance as much, as long as the length of the reasoning steps exceeds a threshold.</p>

<p>For simpler tasks, shorter reasoning steps suffice, but for more complex tasks, increasing the length of the reasoning steps is very beneficial.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Jin et al. also show that replacing the CoT prompt “Let’s think step by step” with  “Let’s think step by step, you must think more steps.” improves performance.</p>
</div>

<p>Leveraging inference-time compute, you can squeeze more performance from smaller LLMs.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Search"><div class="sect3" id="id143">
<h3>Search</h3>

<p>Another way to utilize inference-time compute is to search<a data-type="indexterm" data-primary="search systems" data-secondary="inference-time compute" id="id1263"/> through the generation probability distribution. The idea is that for every token being generated, there is a probability assigned to every token in the vocabulary, so if we search properly, we might end up traversing the probability space and choosing the right path to generate the correct answer. Search-based methods include depth first, breadth first, and Monte Carlo Tree Search (MCTS)<a data-type="indexterm" data-startref="xi_inferencetimecomputation854965" id="id1264"/><a data-type="indexterm" data-startref="xi_reasoninginferencetimecomputation854965" id="id1265"/>.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Fine-Tuning for Reasoning"><div class="sect2" id="id144">
<h2>Fine-Tuning for Reasoning</h2>

<p>One way to induce better reasoning in a model is to fine-tune<a data-type="indexterm" data-primary="reasoning" data-secondary="fine-tuning with dataset" id="id1266"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="with reasoning dataset" data-secondary-sortas="reasoning dataset" id="id1267"/><a data-type="indexterm" data-primary="datasets" data-secondary="fine-tuning" id="id1268"/> it with a reasoning dataset. Let’s explore the <a href="https://oreil.ly/W1JRq">reasoning-0.01</a> dataset released by SkunkworksAI to see what a typical reasoning dataset looks like:</p>
<blockquote><em>Input:</em> If a die is rolled three times, what is the probability of getting a sum of 11?</blockquote>
<blockquote>
<p><em>Reasoning:</em> { “step”: 1, “thought”: “Understand the problem: We need to find the probability of getting a sum of 11 when rolling a die three times.” },</p>

<p>{ “step”: 2, “thought”: “Calculate total possible outcomes: A die has 6 faces, so for each roll, there are 6 possibilities. For three rolls, the total possible outcomes are 6^3 = 216.” },</p>

<p>{ “step”: 3, “thought”: “Identify favorable outcomes: List all combinations of rolls that result in a sum of 11. There are 18 such combinations.” },</p>

<p>{ “step”: 4, “thought”: “Calculate probability: Divide the number of favorable outcomes by the total possible outcomes: 18 / 216 = 1/12.” },</p>

<p>{ “step”: 5, “thought”: “Conclusion: The probability of getting a sum of 11 when rolling a die three times is 1/12.” } ]</p>

<p><em>Output:</em> 1/12</p></blockquote>

<p>The dataset contains step-by-step reasoning chains for a large variety of tasks. Such a dataset can be generated synthetically using larger models, followed by a human verification and annotation stage to verify and correct reasoning chains<a data-type="indexterm" data-startref="xi_reasoning8447106" id="id1269"/><a data-type="indexterm" data-startref="xi_reasoninginducinginLLMs852340" id="id1270"/>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id368">
<h1>Summary</h1>

<p>In this chapter, we defined alignment training and why we need it. We ventured into techniques for alignment training such as reinforcement learning. We also learned about hallucinations and different techniques to mitigate them. Finally, we examined reasoning limitations of LLMs and new techniques like scaling up inference-time computation.</p>

<p>In the next chapter, we’ll discuss techniques for speeding up LLM inference. High computation costs are a significant barrier to LLM adoption, and thus a plethora of techniques have been developed to improve inference speeds.</p>
</div></section>
</div></section></div>
</div>
</body></html>