- en: Foreword
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A miracle is taking place as you read these lines: the squiggles on this page
    are transforming into words and concepts and emotions as they navigate their way
    through your cortex. My thoughts from November 2021 have now successfully invaded
    your brain. If they manage to catch your attention and survive long enough in
    this harsh and highly competitive environment, they may have a chance to reproduce
    again as you share these thoughts with others. Thanks to language, thoughts have
    become airborne and highly contagious brain germs—and no vaccine is coming.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, most brain germs are harmless,^([1](foreword01.xhtml#idm46238741809312))
    and a few are wonderfully useful. In fact, humanity’s brain germs constitute two
    of our most precious treasures: knowledge and culture. Much as we can’t digest
    properly without healthy gut bacteria, we cannot think properly without healthy
    brain germs. Most of your thoughts are not actually yours: they arose and grew
    and evolved in many other brains before they infected you. So if we want to build
    intelligent machines, we will need to find a way to infect them too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that another miracle has been unfolding over the last few
    years: several breakthroughs in deep learning have given birth to powerful language
    models. Since you are reading this book, you have probably seen some astonishing
    demos of these language models, such as GPT-3, which given a short prompt such
    as “a frog meets a crocodile” can write a whole story. Although it’s not quite
    Shakespeare yet, it’s sometimes hard to believe that these texts were written
    by an artificial neural network. In fact, GitHub’s Copilot system is helping me
    write these lines: you’ll never know how much I really wrote.'
  prefs: []
  type: TYPE_NORMAL
- en: The revolution goes far beyond text generation. It encompasses the whole realm
    of natural language processing (NLP), from text classification to summarization,
    translation, question answering, chatbots, natural language understanding (NLU),
    and more. Wherever there’s language, speech or text, there’s an application for
    NLP. You can already ask your phone for tomorrow’s weather, or chat with a virtual
    help desk assistant to troubleshoot a problem, or get meaningful results from
    search engines that seem to truly understand your query. But the technology is
    so new that the best is probably yet to come.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like most advances in science, this recent revolution in NLP rests upon the
    hard work of hundreds of unsung heroes. But three key ingredients of its success
    do stand out:'
  prefs: []
  type: TYPE_NORMAL
- en: The *transformer* is a neural network architecture proposed in 2017 in a groundbreaking
    paper called [“Attention Is All You Need”](https://arxiv.org/abs/1706.03762),
    published by a team of Google researchers. In just a few years it swept across
    the field, crushing previous architectures that were typically based on recurrent
    neural networks (RNNs). The Transformer architecture is excellent at capturing
    patterns in long sequences of data and dealing with huge datasets—so much so that
    its use is now extending well beyond NLP, for example to image processing tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In most projects, you won’t have access to a huge dataset to train a model
    from scratch. Luckily, it’s often possible to download a model that was *pretrained*
    on a generic dataset: all you need to do then is fine-tune it on your own (much
    smaller) dataset. Pretraining has been mainstream in image processing since the
    early 2010s, but in NLP it was restricted to contextless word embeddings (i.e.,
    dense vector representations of individual words). For example, the word “bear”
    had the same pretrained embedding in “teddy bear” and in “to bear.” Then, in 2018,
    several papers proposed full-blown language models that could be pretrained and
    fine-tuned for a variety of NLP tasks; this completely changed the game.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model hubs* like Hugging Face’s have also been a game-changer. In the early
    days, pretrained models were just posted anywhere, so it wasn’t easy to find what
    you needed. Murphy’s law guaranteed that PyTorch users would only find TensorFlow
    models, and vice versa. And when you did find a model, figuring out how to fine-tune
    it wasn’t always easy. This is where Hugging Face’s Transformers library comes
    in: it’s open source, it supports both TensorFlow and PyTorch, and it makes it
    easy to download a state-of-the-art pretrained model from the Hugging Face Hub,
    configure it for your task, fine-tune it on your dataset, and evaluate it. Use
    of the library is growing quickly: in Q4 2021 it was used by over five thousand
    organizations and was installed using `pip` over four million times per month.
    Moreover, the library and its ecosystem are expanding beyond NLP: image processing
    models are available too. You can also download numerous datasets from the Hub
    to train or evaluate your models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So what more can you ask for? Well, this book! It was written by open source
    developers at Hugging Face—including the creator of the Transformers library!—and
    it shows: the breadth and depth of the information you will find in these pages
    is astounding. It covers everything from the Transformer architecture itself,
    to the Transformers library and the entire ecosystem around it. I particularly
    appreciated the hands-on approach: you can follow along in Jupyter notebooks,
    and all the code examples are straight to the point and simple to understand.
    The authors have extensive experience in training very large transformer models,
    and they provide a wealth of tips and tricks for getting everything to work efficiently.
    Last but not least, their writing style is direct and lively: it reads like a
    novel.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, I thoroughly enjoyed this book, and I’m certain you will too. Anyone
    interested in building products with state-of-the-art language-processing features
    needs to read it. It’s packed to the brim with all the right brain germs!
  prefs: []
  type: TYPE_NORMAL
- en: Aurélien Géron
  prefs: []
  type: TYPE_NORMAL
- en: November 2021, Auckland, NZ
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](foreword01.xhtml#idm46238741809312-marker)) For brain hygiene tips, see
    CGP Grey’s [excellent video on memes](https://youtu.be/rE3j_RHkqJc).
  prefs: []
  type: TYPE_NORMAL
