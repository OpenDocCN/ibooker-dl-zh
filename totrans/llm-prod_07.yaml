- en: '8 Large language model applications: Building an interactive experience'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 大型语言模型应用程序：构建交互式体验
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Building an interactive application that uses an LLM service
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建使用LLM服务的交互式应用程序
- en: Running LLMs on edge devices without a GPU
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在边缘设备上运行没有GPU的LLMs
- en: Building LLM agents that can solve multistep problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建能够解决多步问题的LLM代理
- en: No one cares how much you know until they know how much you care.—President
    Theodore Roosevelt
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非他们知道你有多在乎，否则没有人会在乎你懂得多少。——美国总统西奥多·罗斯福
- en: Throughout this book, we’ve taught you the ins and outs of LLMs—how to train
    them, how to deploy them, and, in the last chapter, how to build a prompt to guide
    a model to behave how you want it to. In this chapter, we will put it all together.
    We will show you how to build an application that can use your deployed LLM service
    and create a delightful experience for an actual user. The key word there is delightful.
    Creating a simple application is easy, as we will show, but creating one that
    delights? Well, that’s a bit more difficult. We’ll discuss multiple features you’ll
    want to add to your application and why. Then, we’ll discuss different places
    your application may live, including building such applications for edge devices.
    Lastly, we’ll dive into the world of LLM agents, building applications that can
    fulfill a role, not just a request.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们向您介绍了LLMs的方方面面——如何训练它们，如何部署它们，以及在前一章中，如何构建一个提示来引导模型按照您期望的方式行为。在本章中，我们将把这些内容综合起来。我们将向您展示如何构建一个应用程序，该应用程序可以使用您部署的LLM服务，并为实际用户创造愉快的体验。关键在于“愉快”。创建一个简单的应用程序很容易，正如我们将展示的那样，但创建一个令人愉快的应用程序？嗯，这要困难一些。我们将讨论您希望添加到应用程序中的多个功能及其原因。然后，我们将讨论应用程序可能存在的不同位置，包括为边缘设备构建此类应用程序。最后，我们将深入LLM代理的世界，构建能够履行角色而非仅仅满足请求的应用程序。
- en: 8.1 Building an application
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 构建应用程序
- en: 'It’s probably best that we start by explaining what we mean by LLM application.
    Afterall, *application* is a ubiquitous term that could mean lots of different
    things. For us, in this book, when we say *LLM application*, we mean the frontend—the
    Web App, Phone App, CLI, SDK, VSCode Extension (check out chapter 10!), or any
    other application that will act as the user interface and client for calling our
    LLM Service. Figure 8.1 shows both the frontend and backend separately to help
    focus on the piece of the puzzle we are discussing: the frontend. It’s a pretty
    important piece to the puzzle but also varies quite a bit! While every environment
    will come with its own challenges, we hope we can trust you to know the details
    for your particular use case. For example, if you are building an Android app,
    it’s up to you to learn Java or Kotlin. In this book, however, we will give you
    the building blocks you will need and introduce the important features to add.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最好先解释一下我们所说的LLM应用程序是什么意思。毕竟，“应用程序”是一个无处不在的术语，可能意味着很多不同的事情。对我们来说，在这本书中，当我们说“LLM应用程序”时，我们指的是前端——Web应用程序、手机应用程序、CLI、SDK、VSCode扩展（请参阅第10章！）或任何其他将作为用户界面和客户端来调用我们的LLM服务的应用程序。图8.1分别显示了前端和后端，以帮助我们专注于我们正在讨论的拼图的一部分：前端。这是拼图中的一个非常重要的部分，但变化也相当大！虽然每个环境都会带来自己的挑战，但我们希望您能了解您特定用例的细节。例如，如果您正在构建Android应用程序，学习Java或Kotlin的责任就落在您身上。然而，在这本书中，我们将为您提供所需的构建块，并介绍需要添加的重要功能。
- en: '![figure](../Images/8-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-1.png)'
- en: Figure 8.1 The LLM Application is the web app, phone app, command line interface,
    or another tool that acts as the client our users will use to interact with our
    LLM service.
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1 LLM应用程序是Web应用程序、手机应用程序、命令行界面或其他作为客户端的工具，我们的用户将使用它来与我们的LLM服务进行交互。
- en: 'The first step to building a successful LLM application is composing and experimenting
    with your prompt. Of course, having just discussed this in the last chapter, there
    are many additional features you should consider to offer a better user experience.
    The most basic LLM application is just a chatbox, which essentially consists of
    only three objects: an input field, a send button, and a text field to hold the
    conversation. It’s rather easy to build in almost every context. In addition,
    since one of our participants in the chat is a bot, most of the complexity of
    building a chat interface is also stripped away. For example, we don’t need to
    worry about eventual consistency, mixing up the order of our conversation, or
    whether both users are sending a message at the same time. If our user has a bad
    internet connection, we can throw a timeout error and let them resubmit.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 构建成功的LLM应用程序的第一步是编写和实验您的提示。当然，正如我们在上一章中讨论的那样，您应该考虑许多其他功能以提供更好的用户体验。最基本的LLM应用程序只是一个聊天框，它本质上只包含三个对象：一个输入字段、一个发送按钮和一个用于保存对话的文本字段。在几乎每个环境中构建它都相当容易。此外，由于我们的聊天参与者之一是一个机器人，构建聊天界面的大部分复杂性也被消除了。例如，我们不需要担心最终一致性、混淆对话顺序或是否两个用户同时发送消息。如果我们的用户网络连接不良，我们可以抛出一个超时错误，并让他们重新提交。
- en: However, while the interface is easy, not all the finishing touches are. In
    this section, we are going to share with you some tools of the trade to make your
    LLM application shine. We focus on best practices, like streaming responses, utilizing
    the chat history, and methods to handle and utilize prompt engineering. These
    allow us to craft, format, and clean our users’ prompts and the LLM’s responses
    under the hood, improving results and overall customer satisfaction. All this
    to say, building a basic application that utilizes an LLM is actually rather easy,
    but building a great application is a different story, and we want to build great
    applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管界面简单，但并非所有的细节都如此。在本节中，我们将与您分享一些行业工具，以使您的LLM应用程序更加出色。我们关注最佳实践，如流式响应、利用聊天历史以及处理和利用提示工程的方法。这些使我们能够在幕后构建、格式化和清理用户的提示和LLM的响应，从而提高结果和整体客户满意度。换句话说，构建一个利用LLM的基本应用程序实际上相当简单，但构建一个出色的应用程序则是另一回事，我们希望构建出色的应用程序。
- en: 8.1.1 Streaming on the frontend
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 前端流式传输
- en: In chapter 6, we showed you how to stream your LLM’s response on the server
    side, but that is meaningless if the response isn’t streamed on the client side
    as well. Streaming on the client side is where it all comes together. It’s where
    we show the text to the users as it is being generated. This provides an attractive
    user experience, as it makes it appear like the text is being typed right before
    our eyes and gives the users a sense that the model is actually thinking about
    what it will write next. Not only that, but it also provides a more springy and
    responsive experience, as we can give a feeling of instant feedback, which encourages
    our users to stick around until the model finishes generating. This also helps
    the user to be able to see where the output is going before it gets too far so
    they can stop generation and reprompt.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，我们向您展示了如何在服务器端流式传输LLM的响应，但如果客户端没有流式传输响应，那么这就没有意义。客户端的流式传输是所有这一切汇聚的地方。这是我们将文本展示给用户的地方，同时文本正在生成。这提供了一个吸引人的用户体验，因为它让文本看起来就像就在我们眼前被输入，并给用户一种模型实际上在思考接下来要写什么的感觉。不仅如此，它还提供了一个更有弹性和响应性的体验，因为我们能够提供即时反馈的感觉，这鼓励我们的用户留下来直到模型完成生成。这也帮助用户在输出变得太远之前看到输出在哪里，这样他们就可以停止生成并重新提示。
- en: In listing 8.1, we show you how to do this with just HTML, CSS, and vanilla
    JavaScript. This application is meant to be dead simple. Many of our readers likely
    aren’t frontend savvy, as that isn’t the focus of this book. Those who are most
    likely will be using some tooling for their framework of choice anyway. But a
    basic application with no frills allows us to get to the core of what’s happening.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表8.1中，我们向您展示了如何仅使用HTML、CSS和纯JavaScript来完成这项操作。这个应用程序旨在非常简单。我们的大部分读者可能并不擅长前端开发，因为这不是本书的重点。那些最有可能的人可能也会使用他们选择的框架的一些工具。但是，一个没有花哨功能的基本应用程序使我们能够深入理解正在发生的事情的核心。
- en: 'Since the application is so simple, we opted to put all the CSS and JavaScript
    together into the HTML, although it would be cleaner and a best practice to separate
    them. The CSS defines sizing to ensure our boxes are big enough to read; we won’t
    bother with colors or making it look pretty. Our HTML is as simple as it gets:
    a form containing a text input and a Send button that returns false on submit
    so the page doesn’t refresh. There’s also a `div` container to contain our chat
    messages. Most of the JavaScript is also not that interesting; it just handles
    adding our conversation to the chat. However, pay attention to the `sendToServer`
    function, which does most of the heavy lifting: sending our prompt, receiving
    a readable stream, and iterating over the results.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于应用如此简单，我们选择将所有CSS和JavaScript代码一起放入HTML中，尽管将它们分开会更整洁，也更符合最佳实践。CSS定义了尺寸，以确保我们的盒子足够大以便阅读；我们不会费心去处理颜色或让它看起来漂亮。我们的HTML尽可能简单：一个包含文本输入和发送按钮的表单，该按钮在提交时返回false，以防止页面刷新。还有一个`div`容器来包含我们的聊天消息。大部分JavaScript也不那么有趣；它只是处理将我们的对话添加到聊天中。然而，请注意`sendToServer`函数，它做了大部分繁重的工作：发送我们的提示，接收可读流，并遍历结果。
- en: 'NOTE  On the server side, we set up a `StreamingResponse` object, which gets
    converted to a `ReadableStream` on the JavaScript side. You can learn more about
    readable streams here: [https://mng.bz/75Dg](https://mng.bz/75Dg).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NOTE  在服务器端，我们设置了一个`StreamingResponse`对象，它在JavaScript端被转换为`ReadableStream`。你可以在这里了解更多关于可读流的信息：[https://mng.bz/75Dg](https://mng.bz/75Dg)。
- en: Listing 8.1 Streaming responses to end users
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.1 向最终用户流式传输响应
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Some very simple styling'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 一些非常简单的样式'
- en: '#2 Our body is simple with only three fields: a text input, send button, and
    container for chat.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们的身体很简单，只有三个字段：文本输入、发送按钮和聊天容器'
- en: '#3 JavaScript to handle communication with LLM and streaming response'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 用于处理与LLM和流式响应通信的JavaScript'
- en: '#4 When the Send button is pushed, moves text from input to chat box and sends
    the message to the LLM server'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 当发送按钮被按下时，将文本从输入框移动到聊天框，并将消息发送到LLM服务器'
- en: '#5 Adds new messages to the chat box'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 向聊天框添加新消息'
- en: '#6 Sends prompt to the server and streams the response back as tokens are received'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 将提示发送到服务器，并在接收到令牌时流式传输响应'
- en: '#7 Simple polyfill since StreamResponse still can’t be used as an iterator
    by most browsers'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 简单的polyfill，因为StreamResponse仍然不能被大多数浏览器用作迭代器'
- en: Figure 8.2 shows screenshots of our simple application from listing 8.1\. Showing
    words being streamed to the application would have been better in a movie or GIF,
    but since books don’t play GIFs, we’ll have to make do with several side-by-side
    screenshots instead. Regardless, the figure shows the results being streamed to
    the user token by token, providing a positive user experience.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2显示了列表8.1中的简单应用的截图。在电影或GIF中显示正在流式传输到应用中的单词会更好，但由于书籍不能播放GIF，我们只能将几个并排的截图凑合一下。无论如何，该图显示了结果按令牌逐个流式传输给用户，提供了积极的用户体验。
- en: '![figure](../Images/8-2.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-2.png)'
- en: Figure 8.2 Screenshots of our simple application showing the response being
    streamed
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2 显示我们的简单应用截图，显示正在流式传输的响应
- en: There’s nothing glamorous about our little application here, and that’s partly
    the point. This code is easy to copy and paste and can be used anywhere a web
    browser can run since it’s just an HTML file. It doesn’t take much to build a
    quick demo app once you have an LLM service running.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这个小应用没有什么华丽之处，这也有部分原因。这段代码很容易复制粘贴，并且可以在任何可以运行Web浏览器的位置使用，因为它只是一个HTML文件。一旦你有一个LLM服务运行，构建一个快速演示应用并不需要太多。
- en: 8.1.2 Keeping a history
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 保持历史记录
- en: One big problem with our simple application so far is that each message sent
    to our LLM is independent of other messages sent. This is a big problem because
    most applications that utilize an LLM do so in an interactive environment. Users
    will ask a question and then, based on the response, make additional questions
    or adjustments and clarifications to get better results. However, if you simply
    send the latest query as a prompt, the LLM will not have any context behind the
    new query. Independence is nice for coin flips, but it will make our LLM look
    like a birdbrain.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单应用的一个大问题是，发送到我们的LLM的每条消息都是独立的，与其他消息无关。这是一个大问题，因为大多数利用LLM的应用都是在交互式环境中使用的。用户会提出一个问题，然后根据响应提出更多问题或调整和澄清，以获得更好的结果。然而，如果你只是将最新的查询作为提示发送，LLM将不会有任何关于新查询的上下文。对于抛硬币来说，独立性是好的，但它会使我们的LLM看起来像一只笨鸟。
- en: What we need to do is keep a history of the conversation, both the user’s prompts
    and the LLM’s responses. If we do that, we can append that history to the new
    prompts as context. The LLM will be able to utilize this background information
    to make better responses. Figure 8.3 shows the overall flow of what we are trying
    to achieve.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的是保留对话的历史记录，包括用户的提示和LLM的响应。如果我们这样做，我们就可以将这段历史记录附加到新的提示中作为上下文。LLM将能够利用这些背景信息来做出更好的响应。图8.3显示了我们所尝试实现的总体流程。
- en: '![figure](../Images/8-3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-3.png)'
- en: Figure 8.3 Process flow for storing prompts and responses to a chat history,
    giving our model a memory of the conversation to improve outcomes
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3存储提示和响应到聊天历史的流程，为我们的模型提供对话记忆以改善结果
- en: Now that we know what we are building, let’s take a look at listing 8.2\. This
    time, we will be using Streamlit, a Python framework for building applications.
    It is simple and easy to use while still creating attractive frontends. From Streamlit,
    we will be utilizing a `chat_input` field so users can write and send their input,
    a `chat_message` field that will hold the conversation, and `session_state`, where
    we will create and store the `chat_history`. We will use that chat history to
    craft a better prompt. You’ll also notice that we continue to stream the responses,
    as demonstrated in the last section, but this time using Python.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了我们要构建什么，让我们看看列表8.2。这次，我们将使用Streamlit，这是一个用于构建应用的Python框架。它简单易用，同时还能创建吸引人的前端。从Streamlit，我们将利用`chat_input`字段，让用户可以编写并发送他们的输入，`chat_message`字段将保存对话，以及`session_state`，我们将在这里创建和存储`chat_history`。我们将使用这段聊天历史来构建更好的提示。你也会注意到我们继续流式传输响应，正如上一节所展示的，但这次使用Python。
- en: What is Streamlit?
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么是Streamlit？
- en: Streamlit is an open-source Python library that makes it easy to create web
    applications for machine learning, data science, and other fields. It allows you
    to quickly build interactive web apps using simple Python scripts. With Streamlit,
    you can create dashboards, data visualizations, and other interactive tools without
    needing to know web development languages like HTML, CSS, or JavaScript. Streamlit
    automatically handles the conversion of your Python code into a web app.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit是一个开源的Python库，它使得创建机器学习、数据科学和其他领域的Web应用变得容易。它允许你使用简单的Python脚本快速构建交互式Web应用。使用Streamlit，你可以创建仪表板、数据可视化和其他交互式工具，而无需了解HTML、CSS或JavaScript等Web开发语言。Streamlit自动处理将你的Python代码转换为Web应用。
- en: Listing 8.2 An example application using chat history to improve results
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.2使用聊天历史来改善结果的一个示例应用
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Points to your model’s API'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 指向你的模型API'
- en: '#2 Creates a chat history in the session state'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在会话状态中创建聊天历史'
- en: '#3 Δisplays chat from history'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 Δ显示历史聊天'
- en: '#4 Responds to user. Note: we use the walrus operator (:=) to assign the user’s
    input while also ensuring it is not None at the same time.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 对用户做出响应。注意：我们使用walrus操作符（:=）在同时确保用户输入不为None的情况下分配用户输入。'
- en: '#5 Δisplays user’s input'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 Δ显示用户输入'
- en: '#6 Adds user input to chat history'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 将用户输入添加到聊天历史'
- en: '#7 Streams response'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 流响应'
- en: '#8 Formats prompt adding chat history for additional context'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 格式化提示，添加聊天历史以提供额外上下文'
- en: '#9 Sends request'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 发送请求'
- en: '#10 Adds a blinking cursor to simulate typing'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 添加闪烁的光标以模拟输入'
- en: '#11 Adds LLM response to chat history'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 将LLM响应添加到聊天历史'
- en: Figure 8.4 is a screenshot capturing the LLM app that we just built. While our
    first example was quite ugly, you can see that Streamlit automatically creates
    a nice user interface, complete with finishing touches, like a picture of a human
    face for the user and a robot face for our LLM assistant. You’ll also notice that
    the model is taking in and comprehending the conversation history—albeit giving
    terrible responses. If we want to get better responses, one thing to be sure of
    is that your LLM has been trained on conversation data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4是捕获我们刚刚构建的LLM应用的屏幕截图。虽然我们的第一个例子相当丑陋，但你可以看到Streamlit自动创建了一个很好的用户界面，包括完成细节，如用户的人脸图片和我们的LLM助手的机器人脸。你也会注意到模型正在接收并理解对话历史——尽管给出的响应很糟糕。如果我们想要得到更好的响应，有一件事要确保的是，你的LLM已经在对话数据上进行了训练。
- en: '![figure](../Images/8-4.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-4.png)'
- en: Figure 8.4 Screenshot of our Streamlit app utilizing a chat history
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4我们的Streamlit应用利用聊天历史的屏幕截图
- en: Of course, utilizing the history leads to some problems. The first is that users
    can have relatively long conversations with our bot, but we are still limited
    in the token length we can feed to the model, and the longer the input, the longer
    the generation takes. At some point, the history will begin to be too long. The
    simplest approach to solving this problem is to drop older messages in favor of
    newer ones. Sure, our model may forget important details or instructions at the
    start of our conversation, but humans also tend to have a recency bias in conversations,
    so this tends to be OK—except, of course, for the fact that humans tend to expect
    computers never to forget anything.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，利用历史记录会导致一些问题。第一个问题是用户可以与我们的机器人进行相对较长的对话，但我们仍然受限于可以输入到模型中的令牌长度，输入越长，生成所需的时间就越长。在某个时候，历史记录会变得过长。解决这个问题的最简单方法是用较新的消息替换较旧的消息。当然，我们的模型可能会在对话开始时忘记重要的细节或指令，但人类在对话中也倾向于有近期偏差，所以这通常是可以接受的——当然，除了人类倾向于期望计算机永远不会忘记任何事情这一点。
- en: A more robust solution is to use the LLM to summarize the chat history and use
    the summary as context to our users’ queries instead of the full chat history.
    LLMs are often quite good at highlighting important pieces of information from
    a body of text, so this can be an effective way to compress a conversation. Compression
    can be done on demand or run as a background process. Figure 8.5 illustrates the
    summarization workflow for chat history compression.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更稳健的解决方案是使用LLM来总结聊天历史，并将总结作为我们用户查询的上下文，而不是使用完整的聊天历史。LLM通常非常擅长从大量文本中突出显示重要的信息片段，因此这可以是一种有效地压缩对话的方法。压缩可以按需进行，也可以作为后台进程运行。图8.5展示了聊天历史压缩的总结工作流程。
- en: '![figure](../Images/8-5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-5.png)'
- en: Figure 8.5 A process flow for an app with chat history utilizing summarization
    for chat history compression
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5 使用总结进行聊天历史压缩的应用程序流程图
- en: There are other strategies you can explore, as well as mixing and matching multiple
    methods. Another idea is to embed each chat and perform a search for relevant
    previous chat messages to add to the prompt context. But no matter how you choose
    to shorten the chat history, details are bound to be lost or forgotten the longer
    a conversation goes on or the larger the prompts and responses are.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以探索其他策略，以及混合和匹配多种方法。另一个想法是将每个聊天嵌入，并执行搜索以查找相关的先前聊天消息以添加到提示上下文中。但无论你选择如何缩短聊天历史，随着对话的持续进行或提示和响应的增大，细节很可能会丢失或被遗忘。
- en: 8.1.3 Chatbot interaction features
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 聊天机器人交互功能
- en: 'Chatting with an LLM bot isn’t like chatting with your friend. For one, the
    chatbot is always available and waiting for us to talk to it, so we can expect
    a response right away. There shouldn’t be opportunities for users to spam multiple
    messages to our bot before receiving feedback. But let’s face it, in the real
    world, there are connection problems or bad internet, the server could be overwhelmed,
    and there are a myriad of other reasons a request might fail. These differences
    encourage us to interact with a chatbot differently, and we should ensure we add
    several features for our users to improve their experience. Let’s consider several
    of them now:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLM聊天机器人聊天并不像与你的朋友聊天。一方面，聊天机器人总是随时待命，等待我们与之交谈，因此我们可以期待立即得到回应。用户在收到反馈之前不应该有机会向我们的机器人发送多条消息进行垃圾邮件式攻击。但让我们面对现实，在现实世界中，可能会有连接问题或网络状况不佳，服务器可能会过载，以及许多可能导致请求失败的其他原因。这些差异促使我们以不同的方式与聊天机器人互动，并且我们应该确保为我们的用户提供几个功能来改善他们的体验。现在让我们考虑其中的一些：
- en: '*Fallback response* —A response to give when an error occurs. To keep things
    clean, you’ll want to ensure a 1:1 ratio of LLM responses for every user query
    in your chat history. A fallback response ensures our chat history is clean and
    gives the user instructions on the best course of action, like trying again in
    a few minutes. Speaking of which, you should also consider disabling the Submit
    button when receiving a response to prevent weird problems from asynchronous conversations
    and out-of-order chat history.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回退响应*——当发生错误时提供的响应。为了保持整洁，你将希望确保每个用户查询在聊天历史中的LLM响应比例为1:1。回退响应确保我们的聊天历史整洁，并为用户提供最佳行动方案的说明，例如过几分钟再尝试。说到这一点，你还应该考虑在收到响应时禁用提交按钮，以防止异步对话和聊天历史顺序混乱导致的奇怪问题。'
- en: '*Stop button* —Interrupts a response midstream. An LLM can often be long-winded,
    continuing to respond long after answering the user’s questions. Often, it misunderstands
    a question and starts to answer it incorrectly. In these cases, it’s best to give
    the user a Stop button so they can interrupt the model and move on. This button
    is a simple cost-saving feature since we usually pay for output per token one
    way or another.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*停止按钮* — 在响应过程中中断。大型语言模型（LLM）往往很健谈，在回答用户问题后还会继续回应。它经常误解问题并开始错误地回答。在这些情况下，最好给用户提供一个停止按钮，以便他们可以中断模型并继续操作。这个按钮是一个简单的成本节约功能，因为我们通常以某种方式按输出令牌付费。'
- en: '*Retry button* —Resends the last query and replaces the response. LLMs have
    a bit of randomness to them, which can be great for creative writing, but it means
    they may respond unfavorably even to prompts they have responded correctly to
    multiple times before. Since we add the LLM chat history to new prompts to give
    context, a retry button allows users to attempt to get a better result and keep
    the conversation moving in the right direction. While retrying, it can make sense
    to adjust our prompting hyperparameters, for example, reducing temperature each
    time a user retries. This can help push the responses in the direction the user
    is likely expecting. Of course, this likely isn’t the best move if they are retrying
    because of a bad internet connection, so you’ll need to consider the adjustments
    carefully.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重试按钮* — 重新发送最后一个查询并替换响应。LLM有一定的随机性，这对于创意写作来说可能很好，但这也意味着它们可能对之前已经正确响应多次的提示做出不利的回应。由于我们将LLM聊天历史添加到新的提示中以提供上下文，因此重试按钮允许用户尝试获得更好的结果，并保持对话朝着正确的方向进行。在重试时，调整我们的提示超参数可能是有意义的，例如，每次用户重试时都降低温度。这可以帮助将响应推向用户可能期望的方向。当然，如果他们因为糟糕的网络连接而重试，这可能不是最好的选择，因此您需要仔细考虑调整。'
- en: '*Delete button* —Removes portions of the chat history. As mentioned, the chat
    history is used as context in future responses, but not every response is immediately
    identifiable as bad. We often see red herrings. For example, a chat assistant
    used while coding might hallucinate functions or methods that don’t exist, which
    can lead the conversation down a path that is hard to recover from. Of course,
    depending on your needs, the solution could be a soft delete, where we only remove
    it from the frontend and prompting space but not the backend.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*删除按钮* — 删除聊天历史的一部分。如前所述，聊天历史被用作未来响应的上下文，但并非每个响应都能立即被识别为不良。我们经常看到误导性的信息。例如，在编码时使用的聊天助手可能会幻想不存在的功能或方法，这可能导致对话走向难以恢复的路径。当然，根据您的需求，解决方案可能是一个软删除，我们只从前端和提示空间中删除它，但不从后端删除。'
- en: '*Feedback form* —A way to collect feedback on users’ experience. If you are
    training or finetuning your own LLMs, this data is highly valuable, as it can
    help your team improve results on the next training iteration. This data can often
    easily be applied when using RLHF. Of course, you won’t want to apply it directly,
    but first clean and filter out troll responses. Also, even if you aren’t training,
    it can help your team make decisions to switch models, improve prompting, and
    identify edge cases.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反馈表单* — 收集用户体验反馈的一种方式。如果您正在训练或微调自己的LLM，这些数据非常有价值，因为它可以帮助您的团队在下一个训练迭代中改进结果。这些数据在使用强化学习与人类反馈（RLHF）时通常可以很容易地应用。当然，您不会想直接应用它，但首先需要清理和过滤掉恶意的回复。即使您没有在训练，它也可以帮助您的团队做出切换模型、改进提示和识别边缘情况的决策。'
- en: In listing 8.3, we show how to use Gradio to set up an easy chatbot app. Gradio
    is an open source library for quickly creating customizable user interfaces for
    data science demos and web applications. It’s highly popular for its ease of integration
    within Juptyer notebooks, making it easy to create interfaces and edit your web
    app in a familiar environment. To create a chatbot with Gradio, we’ll use the
    `ChatInterface` and give it a function to make our API request. You’ll notice
    that Gradio expects the history to be part of the `generate` function, and streaming
    is just a matter of ensuring the function is a generator.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表8.3中，我们展示了如何使用Gradio设置简单的聊天机器人应用程序。Gradio是一个开源库，用于快速创建数据科学演示和Web应用程序的可定制用户界面。它因其易于与Jupyter笔记本集成而非常受欢迎，这使得在熟悉的环境中创建界面和编辑Web应用程序变得容易。要使用Gradio创建聊天机器人，我们将使用`ChatInterface`并给它一个函数来执行我们的API请求。您会注意到Gradio期望历史记录是`generate`函数的一部分，而流式传输只是确保函数是一个生成器的问题。
- en: What is Gradio?
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么是Gradio？
- en: Gradio is an open-source Python library that allows you to quickly create customizable
    UI components around your machine-learning models. It provides a simple interface
    for building interactive web-based applications for your models without requiring
    you to write any HTML, CSS, or JavaScript code. With Gradio, you can create input
    forms for your models, display the results, and even share your models with others
    through a web interface.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Gradio 是一个开源的 Python 库，允许您快速创建围绕您的机器学习模型的定制 UI 组件。它提供了一个简单的界面来构建您的模型的无 HTML、CSS
    或 JavaScript 代码的交互式网络应用程序。使用 Gradio，您可以创建用于模型的输入表单，显示结果，甚至可以通过网络界面与他人共享您的模型。
- en: Listing 8.3 Local LLM Gradio chat app with Stop, Retry, and Undo
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.3 本地 LLM Gradio 聊天应用，具有停止、重试和撤销功能
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Points to your model’s API'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 指向您的模型 API'
- en: '#2 Sends request'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 发送请求'
- en: '#3 Adds a blinking cursor to simulate typing'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 添加闪烁的光标以模拟输入'
- en: You can see how simple this code is, with very few lines needed. Gradio does
    all the heavy lifting for us. You might also be wondering where all our interaction
    features are. Well, the good news is that Gradio automatically adds most of these
    features for us. Don’t believe me? Check out the app we just created in figure
    8.6.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到这段代码是多么简单，需要的行数非常少。Gradio 为我们做了所有繁重的工作。您可能还在想我们的交互功能在哪里。好消息是，Gradio 会自动为我们添加大部分这些功能。您不相信吗？请查看图
    8.6 中我们刚刚创建的应用程序。
- en: '![figure](../Images/8-6.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-6.png)'
- en: '**Figure 8.6 Screenshot of our Gradio app, including interaction features Stop,
    Retry, and Undo for better ease of use**'
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**图 8.6 我们 Gradio 应用的截图，包括交互功能停止、重试和撤销，以提供更好的使用便捷性**'
- en: 'Chainlit: An application builder just for LLMs'
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Chainlit：专为 LLM 设计的应用程序构建器
- en: 'We have shown you how to build LLM applications with several different tools:
    Streamlit, Gradio, and even vanilla HTML and JavaScript. There are many great
    tools out there, and we can’t give personal attention to each one. But one more
    tool we think many of our readers will be interested in is Chainlit. Chainlit
    is a tool specifically built for building LLM applications and comes with most
    features out of the box, including ones not discussed here, like themes, CSS customization,
    authentication, and cloud hosting. It is likely one of the fastest ways to get
    up and running.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经向您展示了如何使用几种不同的工具构建 LLM 应用程序：Streamlit、Gradio，甚至是纯 HTML 和 JavaScript。有许多优秀的工具，我们无法对每一个都给予个人关注。但我们认为，许多读者可能会对我们接下来要介绍的工具
    Chainlit 感兴趣。Chainlit 是一个专门为构建 LLM 应用程序而构建的工具，它自带大多数功能，包括这里未讨论的功能，如主题、CSS 定制、身份验证和云托管。它可能是快速启动和运行的最快方式之一。
- en: Each quality-of-life improvement you can add to your application will help it
    stand out above the competition and potentially save you money. For the same reason,
    you should consider using a token counter, which we cover next.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为您的应用程序添加的每一项生活品质改进都将帮助它在竞争中脱颖而出，并可能为您节省金钱。出于同样的原因，您应该考虑使用标记计数器，我们将在下一节中介绍。
- en: 8.1.4 Token counting
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 标记计数
- en: One of the most basic but valuable pieces of information you can gather to offer
    a great user experience is the number of submitted tokens. Since LLMs have token
    limits, we’ll need to ensure the users’ prompts don’t exceed those limits. Giving
    feedback early and often will provide a better user experience. No one wants to
    type a long query only to find that it’s too much upon submitting.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以收集的最基本但非常有价值的信息之一是提交的标记数。由于 LLM 有标记限制，我们需要确保用户的提示不会超过这些限制。及时且频繁地提供反馈将提供更好的用户体验。没有人愿意输入一个长的查询，然后发现提交后太长了。
- en: Counting tokens also allows us to better prompt-engineer and improve results.
    For example, in a Q&A bot, if the user’s question is particularly short, we can
    add more context by extending how many search results our retrieval-augmented
    generation (RAG) system will return. If their question is long, we’ll want to
    limit it and ensure we have enough space to append our own context.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 计算标记数也使我们能够更好地提示工程和改进结果。例如，在一个问答机器人中，如果用户的提问特别短，我们可以通过扩展我们的检索增强生成（RAG）系统将返回的搜索结果数量来添加更多上下文。如果他们的提问很长，我们则希望限制它，并确保我们有足够的空间来附加我们的上下文。
- en: Tiktoken is just such a library to help with this task. It’s an extremely fast
    BPE tokenizer built specifically for OpenAI’s models. The package has been ported
    to multiple languages, including tiktoken-go for Golang, tiktoken-rs for Rust,
    and several others. In the next listing, we show a basic example of how to use
    it. It’s been optimized for speed, which allows us to encode and count tokens
    quickly, which is all we need to do.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Tiktoken就是这样的一个库，用于帮助完成这项任务。它是一个专为OpenAI模型构建的极快BPE分词器。该包已移植到多种语言，包括用于Golang的tiktoken-go、用于Rust的tiktoken-rs以及几个其他语言。在下一节中，我们将展示如何使用它的基本示例。它已经针对速度进行了优化，这使得我们能够快速编码和计数token，这正是我们所需要的。
- en: Listing 8.4 Using tiktoken to count tokens
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4 使用tiktoken计数token
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of course, the reader who hasn’t skipped ahead will recognize a few problems
    with using tiktoken, mainly because it’s built with OpenAI’s encoders in mind.
    If you are using your own tokenizer (which we recommend), it’s not going to be
    very accurate. We have seen several developers—out of laziness or not knowing
    a better solution—still use it for other models. Generally, they saw counts within
    ±5–10 tokens per 1,000 tokens when using tiktoken results for other models using
    similar BPE tokenizers. To them, the speed and latency gains justified the inaccuracy,
    but this was all word of mouth, so take it with a grain of salt.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，那些没有跳过前面的读者会认识到使用tiktoken存在一些问题，主要是因为它是针对OpenAI的编码器构建的。如果你使用自己的分词器（我们推荐这样做），它将不会非常准确。我们见过一些开发者——出于懒惰或不知道更好的解决方案——仍然将其用于其他模型。通常，当使用tiktoken的结果为使用类似BPE分词器的其他模型计数时，他们看到每1,000个token内的计数在±5–10个token之间。对他们来说，速度和延迟的增益足以弥补不准确，但这都是口头相传，所以请带着怀疑的态度看待。
- en: If you are using a different type of tokenizer, like SentencePiece, it’s often
    better to create your own token counter. For example, we do just that in our project
    in chapter 10\. As you can guess, the code follows the same pattern of encoding
    the string and counting the tokens. The hard part comes when porting it to the
    language that needs to run the counter. To do so, compile the tokenizer like you
    would any other ML model, as we discussed in section 6.1.1.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是不同类型的分词器，如SentencePiece，通常最好是创建自己的token计数器。例如，我们在第10章的项目中就是这样做的。正如你可以猜到的，代码遵循了相同的模式，即编码字符串和计数token。难点在于将其移植到需要运行计数器的语言。为此，就像我们第6.1.1节讨论的那样，将分词器编译成任何其他ML模型。
- en: 8.1.5 RAG applied
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.5 RAG应用
- en: RAG is an excellent way to add context and outside knowledge to your LLM to
    improve the accuracy of your results. In the last chapter, we discussed it in
    the context of a backend system. Here, we will be discussing it from the frontend
    perspective. Your RAG system can be set up on either side, each with its own pros
    and cons.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是向你的LLM添加上下文和外部知识以改进结果准确性的绝佳方式。在上一个章节中，我们讨论了它在后端系统中的应用。在这里，我们将从前端的角度来讨论它。你的RAG系统可以在任何一边设置，每边都有其自身的优缺点。
- en: Setting up RAG on the backend ensures a consistent experience for all users
    and gives us greater control as developers of how exactly the context data will
    be used. It also provides a bit more security to the data stored in the vector
    database, as it’s only accessible to the end users through the LLM. Of course,
    through clever prompt injection, it could potentially still be scraped, but it
    is still much more secure than simply allowing users to query your data directly.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在后端设置RAG确保了所有用户都能获得一致的经验，并让我们作为开发者对上下文数据如何使用有更大的控制权。它还为存储在向量数据库中的数据提供了一定的安全性，因为只有通过LLM才能被最终用户访问。当然，通过巧妙的提示注入，它仍然可能被爬取，但与简单地允许用户直接查询你的数据相比，它仍然要安全得多。
- en: RAG is more often set up on the frontend because doing so allows developers
    to take whatever generic LLM is available and insert business context. You don’t
    need to finetune a model on your dataset if you give the model your dataset at
    run time. Thus, RAG becomes a system to add personality and functionality to our
    LLM application versus simply being a tool to ensure the accuracy of results and
    reduce hallucinations.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: RAG更常在前端设置，因为这样做允许开发者将任何通用的LLM插入业务上下文。如果你在运行时给模型提供你的数据集，你不需要在数据集上微调模型。因此，RAG成为了一个为我们的LLM应用添加个性和功能，而不仅仅是确保结果准确性和减少幻觉的工具系统。
- en: In section 6.1.8, we showed you how to set up a RAG system; now we will show
    you how to utilize it for efficient query augmentation. In listing 8.5, we show
    you how to access and use the vector store we set up previously. We will continue
    to use OpenAI and Pinecone from our last example. We will also use LangChain,
    a Python framework which we discovered in the last chapter, to help create LLM
    applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在6.1.8节中，我们向您展示了如何设置一个RAG系统；现在我们将向您展示如何利用它进行高效的查询增强。在列表8.5中，我们向您展示如何访问和使用我们之前设置的向量存储。我们将继续使用OpenAI和Pinecone，这是我们上一个例子中的内容。我们还将使用LangChain，这是一个Python框架，我们在上一章中发现了它，以帮助创建LLM应用。
- en: Listing 8.5 RAG on the frontend
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.5 前端RAG
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Gets OpenAI API key from [platform.openai.com](https://platform.openai.com)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从[platform.openai.com](https://platform.openai.com)获取OpenAI API密钥'
- en: '#2 Finds API key in console at app.pinecone.io'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在app.pinecone.io控制台中找到API密钥'
- en: '#3 Sets up vectorstore'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 设置向量存储'
- en: '#4 Makes a query'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 进行查询'
- en: '#5 Our search query; returns the three most relevant docs'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 我们的搜索查询；返回三个最相关的文档'
- en: '#6 Now let’s use these results to enrich our LLM prompt; sets up the LLM'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 现在，让我们使用这些结果来丰富我们的LLM提示；设置LLM'
- en: '#7 Runs query with vectorstore'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 使用向量存储运行查询'
- en: '#8 Includes Wikipedia sources'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 包含维基百科来源'
- en: We think the most impressive part of this code is the fact that LangChain has
    a chain simply named “stuff” because, presumably, they couldn’t think of anything
    better. (If you want to learn more about the cryptically named module “stuff,”
    you can find the docs at [https://mng.bz/OBER](https://mng.bz/OBER).) But in actuality,
    the most impressive thing about this code is that we just have to define our LLM
    and vector store connections, and we are good to go to start making queries. Simple.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为这段代码最令人印象深刻的部分是LangChain有一个简单地命名为“stuff”的链，因为，据推测，他们想不出更好的名字。（如果您想了解更多关于这个神秘命名的模块“stuff”的信息，您可以在[https://mng.bz/OBER](https://mng.bz/OBER)找到文档。）但实际上，这段代码最令人印象深刻的是，我们只需要定义我们的LLM和向量存储连接，然后我们就可以开始进行查询了。简单。
- en: 8.2 Edge applications
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 边缘应用
- en: 'So far, we have discussed building LLM applications, assuming we will simply
    be using an API—one we deployed, but an API nonetheless. However, there are lots
    of situations where you might want to run the model on the local device inside
    the application itself. Doing so brings several challenges: mainly, we need to
    get a model small enough to transfer and run it on the edge device. We also need
    to be able to run it in the local environment, which likely doesn’t have an accelerator
    or GPU and may not even support Python—for example, running an app in a user’s
    web browser with JavaScript, in an Android app on a mobile phone with Java, or
    on limited hardware like a Raspberry Pi.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了构建LLM应用，假设我们只是简单地使用一个API——我们部署的API，但毕竟是一个API。然而，有许多情况您可能希望在应用程序内部本地设备上运行模型。这样做会带来几个挑战：主要，我们需要得到一个足够小的模型以便在边缘设备上传输和运行。我们还需要能够在本地环境中运行它，这很可能没有加速器或GPU，甚至可能不支持Python——例如，在用户的Web浏览器中用JavaScript运行应用程序，在手机上的Android应用程序中使用Java，或者在像树莓派这样的有限硬件上。
- en: 'In chapter 6, we started discussing the building blocks you need to work with
    edge devices. We showed you how to compile a model, giving examples using TensorRT
    or ONNX Runtime. TensorRT, coming from NVIDIA, is going to serve you better on
    a server with expensive NVIDIA hardware to go with it, so it is less useful for
    edge development. ONNX Runtime is a bit more flexible, but when working with edge
    devices, llama.cpp is often a better solution for LLMs, and it follows the same
    flow: compile the model to the correct format, move that model to the edge device,
    download and install the SDK for your language, and run the model. Let’s take
    a closer look at these steps for llama.cpp.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，我们开始讨论您需要与边缘设备一起工作的构建块。我们向您展示了如何编译模型，给出了使用TensorRT或ONNX Runtime的示例。TensorRT来自NVIDIA，如果您有一个配备昂贵NVIDIA硬件的服务器，它将为您服务得更好，因此对于边缘开发来说不太有用。ONNX
    Runtime更加灵活，但在与边缘设备一起工作时，llama.cpp通常是LLM的一个更好的解决方案，并且遵循相同的流程：将模型编译成正确的格式，将该模型移动到边缘设备，下载并安装您语言的SDK，然后运行模型。让我们更详细地看看这些步骤对于llama.cpp。
- en: 'The llama.cpp project started with the goal of converting an LLM to something
    that could be run on a MacBook without a GPU (Apple silicon chips are notorious
    for poor compatibility for many projects). Initially working to quantize the LLaMA
    model and store it in a binary format that could be used by the C++ language,
    the project has grown to support a couple of dozen LLM architectures and all major
    OS platforms, has bindings for a dozen languages, and even CUDA, metal, and OpenCL
    GPU backend support. Llama.cpp has created two different formats to store the
    quantized LLMs: the first GPT-Generated Model Language (GGML), which was later
    abandoned for the better GPT-Generated Unified Format (GGUF).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: llama.cpp 项目始于将一个大型语言模型（LLM）转换为可以在没有 GPU 的 MacBook 上运行的目标，因为苹果硅芯片在许多项目中以兼容性差而闻名。最初，该项目致力于量化
    LLaMA 模型并将其存储在 C++ 语言可以使用的二进制格式中，但随着项目的发展，它已经支持了数十种 LLM 架构和所有主要操作系统平台，并为十几种语言以及
    CUDA、metal 和 OpenCL GPU 后端提供了支持。Llama.cpp 创建了两种不同的格式来存储量化后的 LLM：第一种是 GPT-Generated
    Model Language（GGML），后来因为更好的 GPT-Generated Unified Format（GGUF）而被放弃。
- en: To use llama.cpp, the first thing we’ll need is a model stored in the GGUF format.
    To convert your own, you’d need to clone the llama.cpp project, install the dependencies,
    and then run the convert script that comes with the project. The steps have changed
    frequently enough that you’ll want to consult the latest information in the repo,
    but currently, it would look like
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 llama.cpp，我们首先需要的是一个存储在 GGUF 格式的模型。要将您自己的模型转换，您需要克隆 llama.cpp 项目，安装依赖项，然后运行项目附带的可转换脚本。这些步骤已经频繁更改，您可能需要查阅仓库中的最新信息，但目前看起来是这样的
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Of course, that last command simply displays the convert script’s Help menu
    for you to investigate the options and does not actually convert a model. For
    our purposes, we’ll download an already converted model. We briefly mentioned
    Tom Jobbins (TheBloke) in chapter 6, the man who has converted thousands of models,
    quantizing and finetuning them so they are in a state ready for use. All you have
    to do is download them from the Hugging Face Hub. So we’ll do that now. First,
    we’ll need the `huggingface-cli`, which comes as a dependency with most of Hugging
    Face’s Python packages, so you probably already have it, but you can install it
    directly as well. Then we’ll use it to download the model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，最后一个命令只是简单地显示转换脚本的帮助菜单，供您调查选项，实际上并没有转换模型。对于我们来说，我们将下载一个已经转换好的模型。我们在第 6 章中简要提到了汤姆·乔宾斯（TheBloke），这位将数千个模型进行量化并微调，使它们处于可以使用状态的男子。您只需从
    Hugging Face Hub 下载它们即可。所以我们现在就做。首先，我们需要 `huggingface-cli`，它通常作为 Hugging Face
    大多数 Python 包的依赖项提供，所以您可能已经有了它，但您也可以直接安装它。然后我们将使用它来下载模型：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we are downloading the WizardCoder-7B model that has already been converted
    to a GGUF format by TheBloke. We are going to save it locally in the models directory.
    We won’t use symbolic links (symlinks), meaning the model will actually exist
    in the folder we choose. Normally, `huggingface-cli` would download it to a cache
    directory and create a symlink to save space and avoid downloading models multiple
    times across projects. Lastly, the Hugging Face repo contains multiple versions
    of the model in different quantized states; here, we’ll select the 2-bit quantized
    version with the `include` flag. This extreme quantization will degrade the performance
    of the quality of our output for the model, but it’s the smallest model available
    in the repo (only 2.82 GB), which makes it great for demonstration purposes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在下载已经被 TheBloke 转换为 GGUF 格式的 WizardCoder-7B 模型。我们将将其保存在本地 models 目录中。我们不会使用符号链接（symlinks），这意味着模型实际上将存在于我们选择的文件夹中。通常，`huggingface-cli`
    会将其下载到缓存目录并创建一个符号链接以节省空间并避免在多个项目中多次下载模型。最后，Hugging Face 仓库包含多个版本的模型，它们处于不同的量化状态；在这里，我们将使用
    `include` 标志选择 2 位量化版本。这种极端的量化将降低模型输出质量的表现，但它是仓库中最小的模型（只有 2.82 GB），这使得它非常适合演示目的。
- en: 'Now that we have our model, we need to download and install the bindings for
    our language of choice and run it. For Python, that would mean installing `llama-cpp-python`
    via `pip`. In listing 8.6, we show you how to use the library to run a GGUF model.
    It’s pretty straightforward, with just two steps: load the model and run it. On
    one author’s CPU, it ran a little slower than about a token per second, which
    isn’t fast but impressive enough for a 7B parameter model without an accelerator.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的模型，我们需要下载并安装我们选择的语言的绑定，并运行它。对于Python，这意味着通过`pip`安装`llama-cpp-python`。在列表8.6中，我们展示了如何使用库来运行GGUF模型。这相当直接，只需两步：加载模型并运行。在一个作者的CPU上，它的运行速度略慢于每秒约一个标记，这并不快，但对于一个没有加速器的70亿参数模型来说已经足够令人印象深刻了。
- en: Listing 8.6 Using llama.cpp to run a quantized model on a CPU
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6 使用llama.cpp在CPU上运行量化模型
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The results are
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: While this example was in Python, there are bindings for Go, Rust, Node.js,
    Java, React Native, and more. Llama.cpp gives us all the tools we need to run
    LLMs in otherwise impossible environments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个例子是用Python编写的，但还有Go、Rust、Node.js、Java、React Native等语言的绑定。Llama.cpp为我们提供了在通常不可能的环境中运行LLM所需的所有工具。
- en: 8.3 LLM agents
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 LLM代理
- en: At this point in the book, we can finally discuss LLM agents. Agents are what
    most people are talking about when they start worrying about AI taking their jobs.
    If you think back to the last chapter, we showed how, with some clever prompt
    engineering and tooling, we could train models to answer multistep questions requiring
    searching for information and running calculations. Agents do the same thing on
    steroids. Full LLM applications are designed not just to answer multistep questions
    but to accomplish multistep tasks. For example, a coding agent could not only
    answer complicated questions about your code base but also edit it, submit PRs,
    review PRs, and write full projects from scratch.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到这本书的这一部分，我们终于可以讨论LLM代理了。当人们开始担心AI会取代他们的工作时，他们谈论的通常是代理。如果我们回顾上一章，我们展示了如何通过一些巧妙的提示工程和工具，训练模型来回答需要搜索信息和运行计算的多步问题。代理在这一点上做得更多。完整的LLM应用不仅旨在回答多步问题，还旨在完成多步任务。例如，一个编码代理不仅能回答关于你的代码库的复杂问题，还能编辑它，提交PR（Pull
    Request，即代码请求），审查PR，并从头开始编写完整的项目。
- en: 'Agents do not differ from other language models in any meaningful way. The
    big differences all go into the system surrounding and supporting the LLM. LLMs
    are, fundamentally, closed search systems. They can’t access anything they weren’t
    trained on explicitly. So for example, if we were to ask Llama 2, “How old was
    Justin Bieber the last time the Patriots won the Superbowl?” we would be dependent
    on Meta having trained that model on incredibly up-to-date information. There
    are three components that make up an agent:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 代理与其他语言模型在本质上没有任何区别。所有的大差异都体现在围绕和支撑LLM（大型语言模型）的系统之中。LLM本质上是一种封闭的搜索系统。它们无法访问它们没有明确训练过的任何内容。例如，如果我们问Llama
    2，“上一次爱国者队赢得超级碗时贾斯汀·比伯多大了？”我们就依赖于Meta已经训练了该模型，并且拥有极其最新的信息。构成代理的三个要素是：
- en: '*LLM* —No explanation necessary. By now, you know what these are and why they’re
    needed.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM* — 无需解释。到现在为止，你知道这些是什么以及为什么需要它们。'
- en: '*Memory* —Some way of reintroducing the LLM to what has happened at each step
    up to that point. Memory goes a long way toward agents performing well. This is
    the same idea as feeding in the chat history, but the model needs something more
    than just the literal history of events. There are several ways of completing
    this:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*记忆* — 以某种方式将LLM重新引入到到目前为止每个步骤所发生的事情中。记忆对于代理表现良好至关重要。这与输入聊天历史记录的想法相同，但模型需要比事件的字面历史记录更多的东西。有几种完成这个任务的方法：'
- en: '*Memory buffer* —Passes in all of the text that’s come before. Not recommended,
    as you’ll hit context limits quickly, and the “lost in the middle” problem will
    exacerbate this.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*记忆缓冲区* — 传递所有之前的文本。不推荐这样做，因为你很快就会遇到上下文限制，并且“丢失在中间”的问题会加剧这个问题。'
- en: '*Memory summarization* —Has the LLM take another pass at the text to summarize
    it for its own memory. Works pretty well; however, at a minimum, it doubles latency,
    and summarization will delete finer details faster than anyone would like.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*记忆总结* — LLM再次遍历文本以总结它自己的记忆。效果相当不错；然而，至少它会加倍延迟，并且总结会比任何人希望的更快地删除更细微的细节。'
- en: '*Structured memory storage* —Thinks ahead and creates a system you can draw
    from to get the actual best info for the model. It can be related to chunking
    articles and searching for an article title and then retrieving the most relevant
    chunk, or perhaps chaining retrievals to find the most relevant keywords or to
    make sure that the query is contained in the retrieval output. We recommend structured
    memory storage the most because even though it’s the hardest to set up, it achieves
    the best results in every scenario.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化内存存储* — 提前思考并创建一个可以从中获取模型实际最佳信息的系统。它可以与文章分块、搜索文章标题然后检索最相关的块相关联，或者通过链式检索找到最相关的关键词或确保查询包含在检索输出中。我们最推荐结构化内存存储，因为它虽然最难设置，但在每种情况下都能取得最佳结果。'
- en: '*External data retrieval tools* —The core of agent behavior. These tools give
    your LLM the ability to take actions, which allows it to perform agent-like tasks.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*外部数据检索工具* — 代理行为的核心。这些工具赋予你的LLM执行动作的能力，使其能够执行类似代理的任务。'
- en: We’ve covered a lot in this book, and agents are a bit of a culmination of much
    of what we’ve covered. They can be quite tricky to build, so to help you, we’ll
    break down the steps and give several examples. First, we’ll make some tools,
    then initialize some agents, and finally create a custom agent, all on our own.
    Throughout the process, you’ll see why it’s particularly difficult to get agents
    to work effectively and especially why LangChain and Guidance are great for getting
    started and difficult to get up and running.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们涵盖了大量的内容，代理是我们在前面所涵盖内容的一个总结。它们可能相当难以构建，因此为了帮助您，我们将分解步骤并提供几个示例。首先，我们将制作一些工具，然后初始化一些代理，最后创建一个自定义代理，所有这些都在我们自己的操作下完成。在整个过程中，您将看到为什么让代理有效地工作尤其困难，特别是为什么LangChain和Guidance对于入门和启动非常有用。
- en: In listing 8.7, we start off easy by demonstrating using some tools via LangChain.
    This example uses the Duck Duck Go search tool and the YouTube search tool. Notice
    that the LLM will simply give them a prompt, and the tools will handle the search
    and summary of results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表8.7中，我们通过LangChain演示了一些工具的使用，以简化示例。本例使用了Duck Duck Go搜索工具和YouTube搜索工具。请注意，LLM将简单地给出提示，而工具将处理搜索和结果摘要。
- en: Listing 8.7 LangChain search tools example
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.7 LangChain搜索工具示例
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Example of using tools'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用工具的示例'
- en: The generated text is
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本是
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we’ll demonstrate running an agent locally. In these examples, we use
    llama.cpp again; however, this time, we will use an instruction-based model, the
    4-bit quantized Mistral 7B Instruct model—a great open source model. You can get
    the model we are using by running the following command. Notice the similarities
    to when we pulled the WizardCoder model back in section 8.2:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将演示在本地运行代理。在这些示例中，我们再次使用llama.cpp；然而，这次我们将使用基于指令的模型，4位量化Mistral 7B Instruct模型——一个优秀的开源模型。您可以通过运行以下命令获取我们使用的模型。注意这与我们在第8.2节中拉取WizardCoder模型时的相似之处：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In listing 8.8, we demonstrate running two different types of agents you’ll
    likely find useful. The first agent generates some Python, runs it, and attempts
    to debug any problems it runs into. The second agent reads and analyzes a CSV
    file. For this agent, we’ll use the Slack dataset we pulled back in chapter 4\.
    Pay attention to the responses, and make a wager on whether they do a good job.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表8.8中，我们展示了运行两种不同类型的代理，这些代理您可能会觉得很有用。第一个代理生成一些Python代码，运行它，并尝试调试它遇到的问题。第二个代理读取并分析CSV文件。对于这个代理，我们将使用我们在第4章中拉取的Slack数据集。请注意响应，并下注它们是否做得很好。
- en: Listing 8.8 LangChain Python and CSV agents
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.8 LangChain Python和CSV代理
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 1 if NEON, any number if CUBLAS else 0'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 如果是NEON，则为1，如果是CUBLAS，则为任何数字，否则为0'
- en: '#2 Context window for the model'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 模型的上下文窗口'
- en: '#3 An agent that will generate Python code and execute it'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 一个将生成Python代码并执行它的代理'
- en: The output is
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE13]python'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE13]python'
- en: import torch
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入`torch`
- en: import torch.nn as nn
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入`torch.nn`作为`nn`
- en: import torch.optim as optim
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入`torch.optim`作为`optim`
- en: from torch.utils import data
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从`torch.utils`导入`data`
- en: from torchvision.datasets import make_classification
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从`torchvision.datasets`导入`make_classification`
- en: define synthetic data from normal distribution
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义从正态分布生成的合成数据
- en: data = make_classification(n_samples=1000, n_features=10, n_informative=5,
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: data = make_classification(n_samples=1000, n_features=10, n_informative=5,
- en: random_state=42)
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: random_state=42)
- en: X = data['features']
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: X = data['features']
- en: y = data['targets']
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: y = data['targets']
- en: '[PRE14]python'
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[PRE14]python'
- en: fix import
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fix导入
- en: import torch.utils.data as data
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入`torch.utils.data`作为`data`
- en: from torchvision.datasets import make_classification
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从`torchvision.datasets`导入`make_classification`
- en: '[PRE15]python'
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[PRE15]python'
- en: fix import
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修复导入
- en: from torchvision.datasets.make_classification import make_classification
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: from torchvision.datasets.make_classification import make_classification
- en: data = make_classification(n_samples=1000, n_features=10, n_informative=5,
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: data = make_classification(n_samples=1000, n_features=10, n_informative=5,
- en: random_state=42)
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: random_state=42)
- en: X = data['features']
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: X = data['features']
- en: y = data['targets']
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: y = data['targets']
- en: '[PRE16]'
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We continue with
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 An agent that will read a CSV and analyze it'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 一个能够读取CSV并分析它的代理'
- en: The generated text is
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本是
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Well, what do you think? Did either agent do a very good job? You’re likely
    thinking “No,” which should reassure you that AI isn’t going to take your job
    anytime soon. The Python agent wrote a PyTorch script that completely depends
    on a `make_ classification()` function that doesn’t exist, and the CSV agent decided
    that being polite is equivalent to saying, “Thank you.” Not a bad guess, but simply
    not a robust solution. Sure, part of the problem is likely the model we are using;
    a bigger one like GPT-4 might do better. We’ll leave it as an exercise for the
    reader to compare.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，你怎么看？有没有哪个代理做得非常好？你可能想“没有”，这应该会让你放心，AI不会很快取代你的工作。Python代理编写了一个完全依赖于一个不存在的`make_classification()`函数的PyTorch脚本，而CSV代理认为礼貌等同于说“谢谢”。这不是一个坏的猜测，但仅仅不是一个稳健的解决方案。当然，问题的一部分可能是我们使用的模型；一个更大的模型，比如GPT-4可能会做得更好。我们将把它留给读者作为练习来比较。
- en: Moving on, in listing 8.9, we build our own agent. We’ll define the tools the
    agent has access to, set up a memory space for the agent, and then initialize
    it. Next, we’ll define a system prompt so the agent knows how it should behave,
    making sure to explain to it what tools it has at its disposal and how to use
    them. We’ll also utilize few-shot prompting and instruction to give us the best
    chance of seeing good results. Lastly, we’ll run the agent. Let’s take a look.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在列表8.9中，我们构建自己的代理。我们将定义代理可以访问的工具，为代理设置一个记忆空间，然后初始化它。接下来，我们将定义一个系统提示，让代理知道它应该如何表现，确保向它解释它有什么工具可用以及如何使用它们。我们还将利用少量提示和指令来给我们最好的机会看到好的结果。最后，我们将运行代理。让我们看看。
- en: Listing 8.9 Agents and agent behavior
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.9 代理及其行为
- en: '[PRE19]json'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE19]json'
- en: '{{"action": "Calculator",'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{{"action": "Calculator",'
- en: '"action_input": "sqrt(4)"}}'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"action_input": "sqrt(4)"}}'
- en: '[PRE20]json'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE20]json'
- en: '{{"action": "DuckDuckGo Search",'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{{"action": "DuckDuckGo Search",'
- en: '"action_input": "When was the Jonas Brothers'' first concert"}}'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"action_input": "乔纳斯兄弟的第一场演唱会是在什么时候"}'
- en: 'Here are some previous conversations between the Assistant and User:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是助手和用户之间的一些先前对话：
- en: 'User: Hey how are you today?'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：嘿，你今天怎么样？
- en: 'Assistant: [PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：[PRE21]
- en: 'User: I''m great, what is the square root of 4?'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：我很好，4的平方根是多少？
- en: 'Assistant: [PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：[PRE22]
- en: 'User: 2.0'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：2.0
- en: 'Assistant: [PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：[PRE23]
- en: 'User: Thanks, when was the Jonas Brothers'' first concert?'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：谢谢，乔纳斯兄弟的第一场演唱会是什么时候？
- en: 'Assistant: [PRE24]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：[PRE24]
- en: 'User: 12.0'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：12.0
- en: 'Assistant: [PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：[PRE25]
- en: 'User: Thanks could you tell me what 4 to the power of 2 is?'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：谢谢，你能告诉我4的平方根是多少吗？
- en: 'Assistant: [PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：[PRE26]
- en: 'User: 16.0'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：16.0
- en: 'Assistant: [PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：[PRE27]
- en: Here is the latest conversation between Assistant and User."""
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是助手和用户之间的最新对话。”
- en: + E_SYS
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: + E_SYS
- en: )
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'new_prompt = agent.agent.create_prompt(system_message=sys_msg, tools=tools)
    #8'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 'new_prompt = agent.agent.create_prompt(system_message=sys_msg, tools=tools)
    #8'
- en: agent.agent.llm_chain.prompt = new_prompt
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: agent.agent.llm_chain.prompt = new_prompt
- en: 'instruction = (      #9'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'instruction = (      #9'
- en: B_INST
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B_INST
- en: + " Respond to the following in JSON with 'action' and 'action_input' "
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: + "用JSON格式响应以下内容，包含'动作'和'动作输入'"
- en: '"values "'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"values "'
- en: + E_INST
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: + E_INST
- en: )
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'human_msg = instruction + "\nUser: {input}"'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: human_msg = instruction + "\n用户：{输入}"
- en: agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg
- en: 'agent.run(             #10'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'agent.run(             #10'
- en: '"Tell me how old Justin Beiber was when the Patriots last won the "'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"告诉我当爱国者队最后一次赢得超级碗时贾斯汀·比伯多大了"'
- en: '"Superbowl."'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"超级碗。"'
- en: )
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#> Entering new AgentExecutor chain…'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#> 正在进入新的AgentExecutor链…'
- en: '#Assistant: {'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#助手：{'
- en: '"action": "DuckDuckGo Search",'
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '"action": "DuckDuckGo Search",'
- en: '"action\_input": "When did the New England Patriots last win the Super'
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '"action_input": "当新英格兰爱国者队最后一次赢得超级碗是在什么时候"'
- en: Bowl? Justin Bieber birthdate"
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汤匙？贾斯汀·比伯的出生日期"
- en: '#}'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#}'
- en: '#{'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#{'
- en: '"action": "Final Answer",'
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '"action": "Final Answer",'
- en: '"action\_input": "Justin Bieber was born on March 1, 1994\. The Patriots'
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '"action_input": "贾斯汀·比伯出生于1994年3月1日。爱国者队"'
- en: last won the Super Bowl in February 2018."
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后一次在2018年2月赢得了超级碗。
- en: '#}'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#}'
- en: '```'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '```'
- en: 'Not bad! It didn’t answer the question, but it got pretty close; it just needed
    to do some math. If you ran the example, you might have noticed it was a bit slow
    compared to using the llama.cpp Python interpreter. Unfortunately, for some reason,
    LangChain’s wrapper adds some significant time to compute, so be warned: if you
    need to go really fast, LangChain is not your vehicle. At least not yet. Regardless,
    LangChain has made some easy-to-use wrappers around popular Python libraries to
    make them usable as LLM tools. In these listings, we only used a handful, and
    there’s a lot more to choose from.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错！它没有回答问题，但已经很接近了；只是需要做一些数学计算。如果你运行了示例，你可能已经注意到它比使用 llama.cpp Python 解释器慢一些。不幸的是，由于某些原因，LangChain
    的包装器增加了计算时间，所以请注意：如果你需要非常快，LangChain 可能不是你的选择。至少目前还不是。无论如何，LangChain 已经围绕流行的 Python
    库创建了一些易于使用的包装器，使它们可以作为 LLM 工具使用。在这些列表中，我们只使用了其中的一小部分，还有更多可供选择。
- en: Overall, you can see that we were able to get the LLM to perform pretty well
    on some nontrivial tasks (and we were using a 4-bit quantized model, we might
    add). However, it was nowhere close to perfect. Agents are miraculous in that
    they work at all, but they are generally underwhelming in the tasks and levels
    they can perform—including the top-tier paid agents. The more you work with LLMs
    crafting many different prompts, the more you’ll find that LLMs are quite flaky,
    just like humans, which can be quite annoying to software engineers who are used
    to working with machines that are as consistent as anything this world has to
    offer. Getting LLMs to perform well on just one task is often difficult enough,
    but chaining several tasks together inside an agent is extremely difficult. We
    are still very much in the early stages of agent development, and we are excited
    to see where it goes.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，你可以看到我们能够使 LLM 在一些非平凡任务上表现相当不错（我们使用的是 4 位量化模型，我们可能会添加）。然而，它远远没有达到完美。代理之所以神奇，是因为它们能够工作，但它们在执行的任务和级别上通常令人失望——包括顶级付费代理。你与
    LLMs 一起制作更多不同的提示时，会发现 LLMs 非常不可靠，就像人类一样，这对习惯于与世界上任何东西一样一致的机器工作的软件工程师来说可能会相当令人烦恼。让
    LLMs 在单一任务上表现良好就已经足够困难，但在代理内部链接多个任务则更加困难。我们仍然处于代理开发的早期阶段，我们很期待看到它的发展方向。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Creating a simple LLM application is straightforward, but creating one that
    delights your users takes a bit more work.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个简单的 LLM 应用程序很简单，但要创建一个能让用户感到愉悦的应用程序则需要更多的工作。
- en: 'The key features you should include in your app include the following:'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该在应用程序中包含的关键功能包括以下内容：
- en: Streaming responses allows a more interactive and responsive experience.
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式响应允许更互动和响应式的体验。
- en: Feeding your model the chat history will prevent your model from having a birdbrain.
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将聊天历史输入到你的模型中可以防止你的模型变得像鸟一样愚蠢。
- en: Interactive features like Stop, Retry, and Delete buttons give users more control
    of the conversation.
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像停止、重试和删除按钮这样的交互功能，让用户对对话有更多的控制权。
- en: Token counting is useful for user feedback and allows users to edit responses
    to fit token limits.
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数令牌对于用户反馈很有用，并允许用户编辑响应以适应令牌限制。
- en: RAG on the frontend allows us to customize an application regardless of the
    LLM backend.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端上的 RAG 允许我们根据 LLM 后端定制应用程序。
- en: Llama.cpp is a powerful open source tool for compiling LLMs and running them
    on edge devices with constrained resources.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama.cpp 是一个强大的开源工具，用于编译 LLMs 并在资源受限的边缘设备上运行。
- en: Agents are LLM applications built to solve multistep problems and promise to
    automate jobs machines currently struggle with.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理是构建来解决多步骤问题的 LLM 应用程序，并承诺自动化机器目前难以处理的任务。
- en: Agents are extremely difficult to build due to the unpredictability of LLMs
    and sometimes require advanced prompt engineering to get reasonable results.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 LLMs 的不可预测性，构建代理非常困难，有时需要高级提示工程才能获得合理的结果。
