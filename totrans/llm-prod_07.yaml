- en: '8 Large language model applications: Building an interactive experience'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building an interactive application that uses an LLM service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running LLMs on edge devices without a GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building LLM agents that can solve multistep problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No one cares how much you know until they know how much you care.—President
    Theodore Roosevelt
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Throughout this book, we’ve taught you the ins and outs of LLMs—how to train
    them, how to deploy them, and, in the last chapter, how to build a prompt to guide
    a model to behave how you want it to. In this chapter, we will put it all together.
    We will show you how to build an application that can use your deployed LLM service
    and create a delightful experience for an actual user. The key word there is delightful.
    Creating a simple application is easy, as we will show, but creating one that
    delights? Well, that’s a bit more difficult. We’ll discuss multiple features you’ll
    want to add to your application and why. Then, we’ll discuss different places
    your application may live, including building such applications for edge devices.
    Lastly, we’ll dive into the world of LLM agents, building applications that can
    fulfill a role, not just a request.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Building an application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s probably best that we start by explaining what we mean by LLM application.
    Afterall, *application* is a ubiquitous term that could mean lots of different
    things. For us, in this book, when we say *LLM application*, we mean the frontend—the
    Web App, Phone App, CLI, SDK, VSCode Extension (check out chapter 10!), or any
    other application that will act as the user interface and client for calling our
    LLM Service. Figure 8.1 shows both the frontend and backend separately to help
    focus on the piece of the puzzle we are discussing: the frontend. It’s a pretty
    important piece to the puzzle but also varies quite a bit! While every environment
    will come with its own challenges, we hope we can trust you to know the details
    for your particular use case. For example, if you are building an Android app,
    it’s up to you to learn Java or Kotlin. In this book, however, we will give you
    the building blocks you will need and introduce the important features to add.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 The LLM Application is the web app, phone app, command line interface,
    or another tool that acts as the client our users will use to interact with our
    LLM service.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The first step to building a successful LLM application is composing and experimenting
    with your prompt. Of course, having just discussed this in the last chapter, there
    are many additional features you should consider to offer a better user experience.
    The most basic LLM application is just a chatbox, which essentially consists of
    only three objects: an input field, a send button, and a text field to hold the
    conversation. It’s rather easy to build in almost every context. In addition,
    since one of our participants in the chat is a bot, most of the complexity of
    building a chat interface is also stripped away. For example, we don’t need to
    worry about eventual consistency, mixing up the order of our conversation, or
    whether both users are sending a message at the same time. If our user has a bad
    internet connection, we can throw a timeout error and let them resubmit.'
  prefs: []
  type: TYPE_NORMAL
- en: However, while the interface is easy, not all the finishing touches are. In
    this section, we are going to share with you some tools of the trade to make your
    LLM application shine. We focus on best practices, like streaming responses, utilizing
    the chat history, and methods to handle and utilize prompt engineering. These
    allow us to craft, format, and clean our users’ prompts and the LLM’s responses
    under the hood, improving results and overall customer satisfaction. All this
    to say, building a basic application that utilizes an LLM is actually rather easy,
    but building a great application is a different story, and we want to build great
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Streaming on the frontend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 6, we showed you how to stream your LLM’s response on the server
    side, but that is meaningless if the response isn’t streamed on the client side
    as well. Streaming on the client side is where it all comes together. It’s where
    we show the text to the users as it is being generated. This provides an attractive
    user experience, as it makes it appear like the text is being typed right before
    our eyes and gives the users a sense that the model is actually thinking about
    what it will write next. Not only that, but it also provides a more springy and
    responsive experience, as we can give a feeling of instant feedback, which encourages
    our users to stick around until the model finishes generating. This also helps
    the user to be able to see where the output is going before it gets too far so
    they can stop generation and reprompt.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 8.1, we show you how to do this with just HTML, CSS, and vanilla
    JavaScript. This application is meant to be dead simple. Many of our readers likely
    aren’t frontend savvy, as that isn’t the focus of this book. Those who are most
    likely will be using some tooling for their framework of choice anyway. But a
    basic application with no frills allows us to get to the core of what’s happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the application is so simple, we opted to put all the CSS and JavaScript
    together into the HTML, although it would be cleaner and a best practice to separate
    them. The CSS defines sizing to ensure our boxes are big enough to read; we won’t
    bother with colors or making it look pretty. Our HTML is as simple as it gets:
    a form containing a text input and a Send button that returns false on submit
    so the page doesn’t refresh. There’s also a `div` container to contain our chat
    messages. Most of the JavaScript is also not that interesting; it just handles
    adding our conversation to the chat. However, pay attention to the `sendToServer`
    function, which does most of the heavy lifting: sending our prompt, receiving
    a readable stream, and iterating over the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  On the server side, we set up a `StreamingResponse` object, which gets
    converted to a `ReadableStream` on the JavaScript side. You can learn more about
    readable streams here: [https://mng.bz/75Dg](https://mng.bz/75Dg).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Streaming responses to end users
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Some very simple styling'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Our body is simple with only three fields: a text input, send button, and
    container for chat.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 JavaScript to handle communication with LLM and streaming response'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 When the Send button is pushed, moves text from input to chat box and sends
    the message to the LLM server'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Adds new messages to the chat box'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Sends prompt to the server and streams the response back as tokens are received'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Simple polyfill since StreamResponse still can’t be used as an iterator
    by most browsers'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 shows screenshots of our simple application from listing 8.1\. Showing
    words being streamed to the application would have been better in a movie or GIF,
    but since books don’t play GIFs, we’ll have to make do with several side-by-side
    screenshots instead. Regardless, the figure shows the results being streamed to
    the user token by token, providing a positive user experience.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Screenshots of our simple application showing the response being
    streamed
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There’s nothing glamorous about our little application here, and that’s partly
    the point. This code is easy to copy and paste and can be used anywhere a web
    browser can run since it’s just an HTML file. It doesn’t take much to build a
    quick demo app once you have an LLM service running.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Keeping a history
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One big problem with our simple application so far is that each message sent
    to our LLM is independent of other messages sent. This is a big problem because
    most applications that utilize an LLM do so in an interactive environment. Users
    will ask a question and then, based on the response, make additional questions
    or adjustments and clarifications to get better results. However, if you simply
    send the latest query as a prompt, the LLM will not have any context behind the
    new query. Independence is nice for coin flips, but it will make our LLM look
    like a birdbrain.
  prefs: []
  type: TYPE_NORMAL
- en: What we need to do is keep a history of the conversation, both the user’s prompts
    and the LLM’s responses. If we do that, we can append that history to the new
    prompts as context. The LLM will be able to utilize this background information
    to make better responses. Figure 8.3 shows the overall flow of what we are trying
    to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Process flow for storing prompts and responses to a chat history,
    giving our model a memory of the conversation to improve outcomes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we know what we are building, let’s take a look at listing 8.2\. This
    time, we will be using Streamlit, a Python framework for building applications.
    It is simple and easy to use while still creating attractive frontends. From Streamlit,
    we will be utilizing a `chat_input` field so users can write and send their input,
    a `chat_message` field that will hold the conversation, and `session_state`, where
    we will create and store the `chat_history`. We will use that chat history to
    craft a better prompt. You’ll also notice that we continue to stream the responses,
    as demonstrated in the last section, but this time using Python.
  prefs: []
  type: TYPE_NORMAL
- en: What is Streamlit?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Streamlit is an open-source Python library that makes it easy to create web
    applications for machine learning, data science, and other fields. It allows you
    to quickly build interactive web apps using simple Python scripts. With Streamlit,
    you can create dashboards, data visualizations, and other interactive tools without
    needing to know web development languages like HTML, CSS, or JavaScript. Streamlit
    automatically handles the conversion of your Python code into a web app.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 An example application using chat history to improve results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Points to your model’s API'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates a chat history in the session state'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Δisplays chat from history'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Responds to user. Note: we use the walrus operator (:=) to assign the user’s
    input while also ensuring it is not None at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Δisplays user’s input'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Adds user input to chat history'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Streams response'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Formats prompt adding chat history for additional context'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Sends request'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Adds a blinking cursor to simulate typing'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Adds LLM response to chat history'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 is a screenshot capturing the LLM app that we just built. While our
    first example was quite ugly, you can see that Streamlit automatically creates
    a nice user interface, complete with finishing touches, like a picture of a human
    face for the user and a robot face for our LLM assistant. You’ll also notice that
    the model is taking in and comprehending the conversation history—albeit giving
    terrible responses. If we want to get better responses, one thing to be sure of
    is that your LLM has been trained on conversation data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 Screenshot of our Streamlit app utilizing a chat history
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Of course, utilizing the history leads to some problems. The first is that users
    can have relatively long conversations with our bot, but we are still limited
    in the token length we can feed to the model, and the longer the input, the longer
    the generation takes. At some point, the history will begin to be too long. The
    simplest approach to solving this problem is to drop older messages in favor of
    newer ones. Sure, our model may forget important details or instructions at the
    start of our conversation, but humans also tend to have a recency bias in conversations,
    so this tends to be OK—except, of course, for the fact that humans tend to expect
    computers never to forget anything.
  prefs: []
  type: TYPE_NORMAL
- en: A more robust solution is to use the LLM to summarize the chat history and use
    the summary as context to our users’ queries instead of the full chat history.
    LLMs are often quite good at highlighting important pieces of information from
    a body of text, so this can be an effective way to compress a conversation. Compression
    can be done on demand or run as a background process. Figure 8.5 illustrates the
    summarization workflow for chat history compression.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 A process flow for an app with chat history utilizing summarization
    for chat history compression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are other strategies you can explore, as well as mixing and matching multiple
    methods. Another idea is to embed each chat and perform a search for relevant
    previous chat messages to add to the prompt context. But no matter how you choose
    to shorten the chat history, details are bound to be lost or forgotten the longer
    a conversation goes on or the larger the prompts and responses are.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.3 Chatbot interaction features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Chatting with an LLM bot isn’t like chatting with your friend. For one, the
    chatbot is always available and waiting for us to talk to it, so we can expect
    a response right away. There shouldn’t be opportunities for users to spam multiple
    messages to our bot before receiving feedback. But let’s face it, in the real
    world, there are connection problems or bad internet, the server could be overwhelmed,
    and there are a myriad of other reasons a request might fail. These differences
    encourage us to interact with a chatbot differently, and we should ensure we add
    several features for our users to improve their experience. Let’s consider several
    of them now:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fallback response* —A response to give when an error occurs. To keep things
    clean, you’ll want to ensure a 1:1 ratio of LLM responses for every user query
    in your chat history. A fallback response ensures our chat history is clean and
    gives the user instructions on the best course of action, like trying again in
    a few minutes. Speaking of which, you should also consider disabling the Submit
    button when receiving a response to prevent weird problems from asynchronous conversations
    and out-of-order chat history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stop button* —Interrupts a response midstream. An LLM can often be long-winded,
    continuing to respond long after answering the user’s questions. Often, it misunderstands
    a question and starts to answer it incorrectly. In these cases, it’s best to give
    the user a Stop button so they can interrupt the model and move on. This button
    is a simple cost-saving feature since we usually pay for output per token one
    way or another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retry button* —Resends the last query and replaces the response. LLMs have
    a bit of randomness to them, which can be great for creative writing, but it means
    they may respond unfavorably even to prompts they have responded correctly to
    multiple times before. Since we add the LLM chat history to new prompts to give
    context, a retry button allows users to attempt to get a better result and keep
    the conversation moving in the right direction. While retrying, it can make sense
    to adjust our prompting hyperparameters, for example, reducing temperature each
    time a user retries. This can help push the responses in the direction the user
    is likely expecting. Of course, this likely isn’t the best move if they are retrying
    because of a bad internet connection, so you’ll need to consider the adjustments
    carefully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delete button* —Removes portions of the chat history. As mentioned, the chat
    history is used as context in future responses, but not every response is immediately
    identifiable as bad. We often see red herrings. For example, a chat assistant
    used while coding might hallucinate functions or methods that don’t exist, which
    can lead the conversation down a path that is hard to recover from. Of course,
    depending on your needs, the solution could be a soft delete, where we only remove
    it from the frontend and prompting space but not the backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feedback form* —A way to collect feedback on users’ experience. If you are
    training or finetuning your own LLMs, this data is highly valuable, as it can
    help your team improve results on the next training iteration. This data can often
    easily be applied when using RLHF. Of course, you won’t want to apply it directly,
    but first clean and filter out troll responses. Also, even if you aren’t training,
    it can help your team make decisions to switch models, improve prompting, and
    identify edge cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In listing 8.3, we show how to use Gradio to set up an easy chatbot app. Gradio
    is an open source library for quickly creating customizable user interfaces for
    data science demos and web applications. It’s highly popular for its ease of integration
    within Juptyer notebooks, making it easy to create interfaces and edit your web
    app in a familiar environment. To create a chatbot with Gradio, we’ll use the
    `ChatInterface` and give it a function to make our API request. You’ll notice
    that Gradio expects the history to be part of the `generate` function, and streaming
    is just a matter of ensuring the function is a generator.
  prefs: []
  type: TYPE_NORMAL
- en: What is Gradio?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Gradio is an open-source Python library that allows you to quickly create customizable
    UI components around your machine-learning models. It provides a simple interface
    for building interactive web-based applications for your models without requiring
    you to write any HTML, CSS, or JavaScript code. With Gradio, you can create input
    forms for your models, display the results, and even share your models with others
    through a web interface.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Local LLM Gradio chat app with Stop, Retry, and Undo
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Points to your model’s API'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sends request'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Adds a blinking cursor to simulate typing'
  prefs: []
  type: TYPE_NORMAL
- en: You can see how simple this code is, with very few lines needed. Gradio does
    all the heavy lifting for us. You might also be wondering where all our interaction
    features are. Well, the good news is that Gradio automatically adds most of these
    features for us. Don’t believe me? Check out the app we just created in figure
    8.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 8.6 Screenshot of our Gradio app, including interaction features Stop,
    Retry, and Undo for better ease of use**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Chainlit: An application builder just for LLMs'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We have shown you how to build LLM applications with several different tools:
    Streamlit, Gradio, and even vanilla HTML and JavaScript. There are many great
    tools out there, and we can’t give personal attention to each one. But one more
    tool we think many of our readers will be interested in is Chainlit. Chainlit
    is a tool specifically built for building LLM applications and comes with most
    features out of the box, including ones not discussed here, like themes, CSS customization,
    authentication, and cloud hosting. It is likely one of the fastest ways to get
    up and running.'
  prefs: []
  type: TYPE_NORMAL
- en: Each quality-of-life improvement you can add to your application will help it
    stand out above the competition and potentially save you money. For the same reason,
    you should consider using a token counter, which we cover next.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.4 Token counting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most basic but valuable pieces of information you can gather to offer
    a great user experience is the number of submitted tokens. Since LLMs have token
    limits, we’ll need to ensure the users’ prompts don’t exceed those limits. Giving
    feedback early and often will provide a better user experience. No one wants to
    type a long query only to find that it’s too much upon submitting.
  prefs: []
  type: TYPE_NORMAL
- en: Counting tokens also allows us to better prompt-engineer and improve results.
    For example, in a Q&A bot, if the user’s question is particularly short, we can
    add more context by extending how many search results our retrieval-augmented
    generation (RAG) system will return. If their question is long, we’ll want to
    limit it and ensure we have enough space to append our own context.
  prefs: []
  type: TYPE_NORMAL
- en: Tiktoken is just such a library to help with this task. It’s an extremely fast
    BPE tokenizer built specifically for OpenAI’s models. The package has been ported
    to multiple languages, including tiktoken-go for Golang, tiktoken-rs for Rust,
    and several others. In the next listing, we show a basic example of how to use
    it. It’s been optimized for speed, which allows us to encode and count tokens
    quickly, which is all we need to do.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4 Using tiktoken to count tokens
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the reader who hasn’t skipped ahead will recognize a few problems
    with using tiktoken, mainly because it’s built with OpenAI’s encoders in mind.
    If you are using your own tokenizer (which we recommend), it’s not going to be
    very accurate. We have seen several developers—out of laziness or not knowing
    a better solution—still use it for other models. Generally, they saw counts within
    ±5–10 tokens per 1,000 tokens when using tiktoken results for other models using
    similar BPE tokenizers. To them, the speed and latency gains justified the inaccuracy,
    but this was all word of mouth, so take it with a grain of salt.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a different type of tokenizer, like SentencePiece, it’s often
    better to create your own token counter. For example, we do just that in our project
    in chapter 10\. As you can guess, the code follows the same pattern of encoding
    the string and counting the tokens. The hard part comes when porting it to the
    language that needs to run the counter. To do so, compile the tokenizer like you
    would any other ML model, as we discussed in section 6.1.1.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.5 RAG applied
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG is an excellent way to add context and outside knowledge to your LLM to
    improve the accuracy of your results. In the last chapter, we discussed it in
    the context of a backend system. Here, we will be discussing it from the frontend
    perspective. Your RAG system can be set up on either side, each with its own pros
    and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up RAG on the backend ensures a consistent experience for all users
    and gives us greater control as developers of how exactly the context data will
    be used. It also provides a bit more security to the data stored in the vector
    database, as it’s only accessible to the end users through the LLM. Of course,
    through clever prompt injection, it could potentially still be scraped, but it
    is still much more secure than simply allowing users to query your data directly.
  prefs: []
  type: TYPE_NORMAL
- en: RAG is more often set up on the frontend because doing so allows developers
    to take whatever generic LLM is available and insert business context. You don’t
    need to finetune a model on your dataset if you give the model your dataset at
    run time. Thus, RAG becomes a system to add personality and functionality to our
    LLM application versus simply being a tool to ensure the accuracy of results and
    reduce hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: In section 6.1.8, we showed you how to set up a RAG system; now we will show
    you how to utilize it for efficient query augmentation. In listing 8.5, we show
    you how to access and use the vector store we set up previously. We will continue
    to use OpenAI and Pinecone from our last example. We will also use LangChain,
    a Python framework which we discovered in the last chapter, to help create LLM
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.5 RAG on the frontend
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Gets OpenAI API key from [platform.openai.com](https://platform.openai.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Finds API key in console at app.pinecone.io'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets up vectorstore'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Makes a query'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Our search query; returns the three most relevant docs'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Now let’s use these results to enrich our LLM prompt; sets up the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Runs query with vectorstore'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Includes Wikipedia sources'
  prefs: []
  type: TYPE_NORMAL
- en: We think the most impressive part of this code is the fact that LangChain has
    a chain simply named “stuff” because, presumably, they couldn’t think of anything
    better. (If you want to learn more about the cryptically named module “stuff,”
    you can find the docs at [https://mng.bz/OBER](https://mng.bz/OBER).) But in actuality,
    the most impressive thing about this code is that we just have to define our LLM
    and vector store connections, and we are good to go to start making queries. Simple.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Edge applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have discussed building LLM applications, assuming we will simply
    be using an API—one we deployed, but an API nonetheless. However, there are lots
    of situations where you might want to run the model on the local device inside
    the application itself. Doing so brings several challenges: mainly, we need to
    get a model small enough to transfer and run it on the edge device. We also need
    to be able to run it in the local environment, which likely doesn’t have an accelerator
    or GPU and may not even support Python—for example, running an app in a user’s
    web browser with JavaScript, in an Android app on a mobile phone with Java, or
    on limited hardware like a Raspberry Pi.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter 6, we started discussing the building blocks you need to work with
    edge devices. We showed you how to compile a model, giving examples using TensorRT
    or ONNX Runtime. TensorRT, coming from NVIDIA, is going to serve you better on
    a server with expensive NVIDIA hardware to go with it, so it is less useful for
    edge development. ONNX Runtime is a bit more flexible, but when working with edge
    devices, llama.cpp is often a better solution for LLMs, and it follows the same
    flow: compile the model to the correct format, move that model to the edge device,
    download and install the SDK for your language, and run the model. Let’s take
    a closer look at these steps for llama.cpp.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The llama.cpp project started with the goal of converting an LLM to something
    that could be run on a MacBook without a GPU (Apple silicon chips are notorious
    for poor compatibility for many projects). Initially working to quantize the LLaMA
    model and store it in a binary format that could be used by the C++ language,
    the project has grown to support a couple of dozen LLM architectures and all major
    OS platforms, has bindings for a dozen languages, and even CUDA, metal, and OpenCL
    GPU backend support. Llama.cpp has created two different formats to store the
    quantized LLMs: the first GPT-Generated Model Language (GGML), which was later
    abandoned for the better GPT-Generated Unified Format (GGUF).'
  prefs: []
  type: TYPE_NORMAL
- en: To use llama.cpp, the first thing we’ll need is a model stored in the GGUF format.
    To convert your own, you’d need to clone the llama.cpp project, install the dependencies,
    and then run the convert script that comes with the project. The steps have changed
    frequently enough that you’ll want to consult the latest information in the repo,
    but currently, it would look like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, that last command simply displays the convert script’s Help menu
    for you to investigate the options and does not actually convert a model. For
    our purposes, we’ll download an already converted model. We briefly mentioned
    Tom Jobbins (TheBloke) in chapter 6, the man who has converted thousands of models,
    quantizing and finetuning them so they are in a state ready for use. All you have
    to do is download them from the Hugging Face Hub. So we’ll do that now. First,
    we’ll need the `huggingface-cli`, which comes as a dependency with most of Hugging
    Face’s Python packages, so you probably already have it, but you can install it
    directly as well. Then we’ll use it to download the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are downloading the WizardCoder-7B model that has already been converted
    to a GGUF format by TheBloke. We are going to save it locally in the models directory.
    We won’t use symbolic links (symlinks), meaning the model will actually exist
    in the folder we choose. Normally, `huggingface-cli` would download it to a cache
    directory and create a symlink to save space and avoid downloading models multiple
    times across projects. Lastly, the Hugging Face repo contains multiple versions
    of the model in different quantized states; here, we’ll select the 2-bit quantized
    version with the `include` flag. This extreme quantization will degrade the performance
    of the quality of our output for the model, but it’s the smallest model available
    in the repo (only 2.82 GB), which makes it great for demonstration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our model, we need to download and install the bindings for
    our language of choice and run it. For Python, that would mean installing `llama-cpp-python`
    via `pip`. In listing 8.6, we show you how to use the library to run a GGUF model.
    It’s pretty straightforward, with just two steps: load the model and run it. On
    one author’s CPU, it ran a little slower than about a token per second, which
    isn’t fast but impressive enough for a 7B parameter model without an accelerator.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.6 Using llama.cpp to run a quantized model on a CPU
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The results are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: While this example was in Python, there are bindings for Go, Rust, Node.js,
    Java, React Native, and more. Llama.cpp gives us all the tools we need to run
    LLMs in otherwise impossible environments.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 LLM agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point in the book, we can finally discuss LLM agents. Agents are what
    most people are talking about when they start worrying about AI taking their jobs.
    If you think back to the last chapter, we showed how, with some clever prompt
    engineering and tooling, we could train models to answer multistep questions requiring
    searching for information and running calculations. Agents do the same thing on
    steroids. Full LLM applications are designed not just to answer multistep questions
    but to accomplish multistep tasks. For example, a coding agent could not only
    answer complicated questions about your code base but also edit it, submit PRs,
    review PRs, and write full projects from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Agents do not differ from other language models in any meaningful way. The
    big differences all go into the system surrounding and supporting the LLM. LLMs
    are, fundamentally, closed search systems. They can’t access anything they weren’t
    trained on explicitly. So for example, if we were to ask Llama 2, “How old was
    Justin Bieber the last time the Patriots won the Superbowl?” we would be dependent
    on Meta having trained that model on incredibly up-to-date information. There
    are three components that make up an agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '*LLM* —No explanation necessary. By now, you know what these are and why they’re
    needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memory* —Some way of reintroducing the LLM to what has happened at each step
    up to that point. Memory goes a long way toward agents performing well. This is
    the same idea as feeding in the chat history, but the model needs something more
    than just the literal history of events. There are several ways of completing
    this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memory buffer* —Passes in all of the text that’s come before. Not recommended,
    as you’ll hit context limits quickly, and the “lost in the middle” problem will
    exacerbate this.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memory summarization* —Has the LLM take another pass at the text to summarize
    it for its own memory. Works pretty well; however, at a minimum, it doubles latency,
    and summarization will delete finer details faster than anyone would like.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Structured memory storage* —Thinks ahead and creates a system you can draw
    from to get the actual best info for the model. It can be related to chunking
    articles and searching for an article title and then retrieving the most relevant
    chunk, or perhaps chaining retrievals to find the most relevant keywords or to
    make sure that the query is contained in the retrieval output. We recommend structured
    memory storage the most because even though it’s the hardest to set up, it achieves
    the best results in every scenario.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*External data retrieval tools* —The core of agent behavior. These tools give
    your LLM the ability to take actions, which allows it to perform agent-like tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve covered a lot in this book, and agents are a bit of a culmination of much
    of what we’ve covered. They can be quite tricky to build, so to help you, we’ll
    break down the steps and give several examples. First, we’ll make some tools,
    then initialize some agents, and finally create a custom agent, all on our own.
    Throughout the process, you’ll see why it’s particularly difficult to get agents
    to work effectively and especially why LangChain and Guidance are great for getting
    started and difficult to get up and running.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 8.7, we start off easy by demonstrating using some tools via LangChain.
    This example uses the Duck Duck Go search tool and the YouTube search tool. Notice
    that the LLM will simply give them a prompt, and the tools will handle the search
    and summary of results.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.7 LangChain search tools example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Example of using tools'
  prefs: []
  type: TYPE_NORMAL
- en: The generated text is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll demonstrate running an agent locally. In these examples, we use
    llama.cpp again; however, this time, we will use an instruction-based model, the
    4-bit quantized Mistral 7B Instruct model—a great open source model. You can get
    the model we are using by running the following command. Notice the similarities
    to when we pulled the WizardCoder model back in section 8.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In listing 8.8, we demonstrate running two different types of agents you’ll
    likely find useful. The first agent generates some Python, runs it, and attempts
    to debug any problems it runs into. The second agent reads and analyzes a CSV
    file. For this agent, we’ll use the Slack dataset we pulled back in chapter 4\.
    Pay attention to the responses, and make a wager on whether they do a good job.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.8 LangChain Python and CSV agents
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 1 if NEON, any number if CUBLAS else 0'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Context window for the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 An agent that will generate Python code and execute it'
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]python'
  prefs: []
  type: TYPE_NORMAL
- en: import torch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import torch.nn as nn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import torch.optim as optim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from torch.utils import data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from torchvision.datasets import make_classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: define synthetic data from normal distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: data = make_classification(n_samples=1000, n_features=10, n_informative=5,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: random_state=42)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: X = data['features']
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: y = data['targets']
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE14]python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: fix import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import torch.utils.data as data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from torchvision.datasets import make_classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE15]python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: fix import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from torchvision.datasets.make_classification import make_classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: data = make_classification(n_samples=1000, n_features=10, n_informative=5,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: random_state=42)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: X = data['features']
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: y = data['targets']
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_H1
  type: TYPE_PRE
- en: We continue with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 An agent that will read a CSV and analyze it'
  prefs: []
  type: TYPE_NORMAL
- en: The generated text is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Well, what do you think? Did either agent do a very good job? You’re likely
    thinking “No,” which should reassure you that AI isn’t going to take your job
    anytime soon. The Python agent wrote a PyTorch script that completely depends
    on a `make_ classification()` function that doesn’t exist, and the CSV agent decided
    that being polite is equivalent to saying, “Thank you.” Not a bad guess, but simply
    not a robust solution. Sure, part of the problem is likely the model we are using;
    a bigger one like GPT-4 might do better. We’ll leave it as an exercise for the
    reader to compare.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, in listing 8.9, we build our own agent. We’ll define the tools the
    agent has access to, set up a memory space for the agent, and then initialize
    it. Next, we’ll define a system prompt so the agent knows how it should behave,
    making sure to explain to it what tools it has at its disposal and how to use
    them. We’ll also utilize few-shot prompting and instruction to give us the best
    chance of seeing good results. Lastly, we’ll run the agent. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.9 Agents and agent behavior
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]json'
  prefs: []
  type: TYPE_NORMAL
- en: '{{"action": "Calculator",'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"action_input": "sqrt(4)"}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]json'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{{"action": "DuckDuckGo Search",'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"action_input": "When was the Jonas Brothers'' first concert"}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are some previous conversations between the Assistant and User:'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: Hey how are you today?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: [PRE21]'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: I''m great, what is the square root of 4?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: [PRE22]'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: 2.0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: [PRE23]'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: Thanks, when was the Jonas Brothers'' first concert?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: [PRE24]'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: 12.0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: [PRE25]'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: Thanks could you tell me what 4 to the power of 2 is?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: [PRE26]'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: 16.0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: [PRE27]'
  prefs: []
  type: TYPE_NORMAL
- en: Here is the latest conversation between Assistant and User."""
  prefs: []
  type: TYPE_NORMAL
- en: + E_SYS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'new_prompt = agent.agent.create_prompt(system_message=sys_msg, tools=tools)
    #8'
  prefs: []
  type: TYPE_NORMAL
- en: agent.agent.llm_chain.prompt = new_prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'instruction = (      #9'
  prefs: []
  type: TYPE_NORMAL
- en: B_INST
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: + " Respond to the following in JSON with 'action' and 'action_input' "
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"values "'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: + E_INST
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'human_msg = instruction + "\nUser: {input}"'
  prefs: []
  type: TYPE_NORMAL
- en: agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg
  prefs: []
  type: TYPE_NORMAL
- en: 'agent.run(             #10'
  prefs: []
  type: TYPE_NORMAL
- en: '"Tell me how old Justin Beiber was when the Patriots last won the "'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Superbowl."'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#> Entering new AgentExecutor chain…'
  prefs: []
  type: TYPE_NORMAL
- en: '#Assistant: {'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "DuckDuckGo Search",'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"action\_input": "When did the New England Patriots last win the Super'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bowl? Justin Bieber birthdate"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '#}'
  prefs: []
  type: TYPE_NORMAL
- en: '#{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"action\_input": "Justin Bieber was born on March 1, 1994\. The Patriots'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: last won the Super Bowl in February 2018."
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '#}'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'Not bad! It didn’t answer the question, but it got pretty close; it just needed
    to do some math. If you ran the example, you might have noticed it was a bit slow
    compared to using the llama.cpp Python interpreter. Unfortunately, for some reason,
    LangChain’s wrapper adds some significant time to compute, so be warned: if you
    need to go really fast, LangChain is not your vehicle. At least not yet. Regardless,
    LangChain has made some easy-to-use wrappers around popular Python libraries to
    make them usable as LLM tools. In these listings, we only used a handful, and
    there’s a lot more to choose from.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, you can see that we were able to get the LLM to perform pretty well
    on some nontrivial tasks (and we were using a 4-bit quantized model, we might
    add). However, it was nowhere close to perfect. Agents are miraculous in that
    they work at all, but they are generally underwhelming in the tasks and levels
    they can perform—including the top-tier paid agents. The more you work with LLMs
    crafting many different prompts, the more you’ll find that LLMs are quite flaky,
    just like humans, which can be quite annoying to software engineers who are used
    to working with machines that are as consistent as anything this world has to
    offer. Getting LLMs to perform well on just one task is often difficult enough,
    but chaining several tasks together inside an agent is extremely difficult. We
    are still very much in the early stages of agent development, and we are excited
    to see where it goes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a simple LLM application is straightforward, but creating one that
    delights your users takes a bit more work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key features you should include in your app include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming responses allows a more interactive and responsive experience.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feeding your model the chat history will prevent your model from having a birdbrain.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactive features like Stop, Retry, and Delete buttons give users more control
    of the conversation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Token counting is useful for user feedback and allows users to edit responses
    to fit token limits.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG on the frontend allows us to customize an application regardless of the
    LLM backend.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Llama.cpp is a powerful open source tool for compiling LLMs and running them
    on edge devices with constrained resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents are LLM applications built to solve multistep problems and promise to
    automate jobs machines currently struggle with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents are extremely difficult to build due to the unpredictability of LLMs
    and sometimes require advanced prompt engineering to get reasonable results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
