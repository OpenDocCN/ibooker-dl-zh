<html><head></head><body><section data-pdf-bookmark="Chapter 17. Serving LLMs with Ollama" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch17_serving_llms_with_ollama_1748550058915113">&#13;
      <h1><span class="label">Chapter 17. </span>Serving LLMs with Ollama</h1>&#13;
      <p>We’ve explored how to use transformers to download a model and put together an easy pipeline that lets you use it for inference or fine-tuning. <a contenteditable="false" data-primary="Ollama" data-secondary="about" data-type="indexterm" id="id1819"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="Ollama platform" data-tertiary="about" data-type="indexterm" id="id1820"/>However, I’d be remiss if I didn’t show you the open source Ollama project, which ties it all together by giving you an environment that gives you a full wrapper around an LLM that you can either chat with in your terminal or use as a server that you can HTTP POST to and read the output from.</p>&#13;
      <p>Technologies like Ollama will be the vanguard of the next generation of LLMs, which will let you have dedicated servers inside your data center or dedicated processes on your computer. That will make them completely private to you.</p>&#13;
      <p>At its core, Ollama is an open source project that simplifies the process of downloading, running, and managing LLMs on your computer. It also handles nonfunctional difficult requirements, such as memory management and model optimization, and it provides standardized interfaces for interaction, such as the ability to HTTP POST to your models.</p>&#13;
      <p>Ollama is also a key strategic tool you should consider because it bridges the gap between cloud-based third-party services like GPT, Claude, and Gemini and locally deployed services. It goes beyond giving you a local development environment to giving you one that you could, for example, use within your own data center to serve multiple internal users.</p>&#13;
      <p>By running models locally, you can ensure the complete privacy of your data, eliminate network latency, and work offline. This is especially crucial in scenarios involving sensitive data or applications that require consistent, low-latency responses.</p>&#13;
      <p>Ollama also supports a growing library of popular open source models, including Llama, Mistral, and Gemma, and it also supports various specialized models that <span class="keep-together">are optimized for specific</span> tasks. Each model can be pulled and run with simple <span class="keep-together">commands,</span> in a way that’s similar to how Docker containers work. The platform handles model quantization automatically, optimizing models to run efficiently on consumer hardware while maintaining good performance.</p>&#13;
      <p>In this chapter, we’ll explore Ollama in three ways: installing it and getting started, looking at how you can instantiate specific models and use them, and exploring the RESTful APIs that let you build LLM applications that preserve privacy.</p>&#13;
      <section data-pdf-bookmark="Getting Started with Ollama" data-type="sect1"><div class="sect1" id="ch17_getting_started_with_ollama_1748550058915312">&#13;
        <h1>Getting Started with Ollama</h1>&#13;
        <p>The Ollama project is hosted at <a href="http://ollama.com">ollama.com</a>. It’s pretty straightforward<a contenteditable="false" data-primary="Ollama" data-secondary="getting started" data-type="indexterm" id="ch17strt"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="Ollama platform" data-tertiary="getting started" data-type="indexterm" id="ch17strt2"/><a contenteditable="false" data-primary="online resources" data-secondary="Ollama" data-type="indexterm" id="id1821"/> to get up and running, and the home screen gives download options for macOS, Linux, and Windows. Note also that the Windows version needs Windows Subsystem for Linux (WSL). For this chapter, I’m using the macOS version.</p>&#13;
        <p>When you navigate to the website, you’ll see a friendly welcome to download (see <a data-type="xref" href="#ch17_figure_1_1748550058907779">Figure 17-1</a>).</p>&#13;
        <figure><div class="figure" id="ch17_figure_1_1748550058907779">&#13;
          <img src="assets/aiml_1701.png"/>&#13;
          <h6><span class="label">Figure 17-1. </span>Getting started with Ollama</h6>&#13;
        </div></figure>&#13;
        <p>Once you’ve downloaded and installed Ollama, you can launch it, and you’ll see it in the system bar at the top of the screen. Your main interface with Ollama will be the command line. </p>&#13;
        <p>Then, with the <code>ollama run</code> command, you can download and use models. So, for example, if you want to use Gemma, then from Google, you can do the following:<a contenteditable="false" data-primary="Ollama" data-secondary="running gemma2:2b" data-type="indexterm" id="id1822"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="Ollama platform" data-tertiary="running gemma2:2b" data-type="indexterm" id="id1823"/><a contenteditable="false" data-primary="gemma2:2b on Ollama" data-type="indexterm" id="id1824"/></p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="o">&gt;</code><code class="n">ollama</code> <code class="n">run</code> <code class="n">gemma2</code><code class="p">:</code><code class="mi">2</code><code class="n">b</code></pre>&#13;
        <p class="pagebreak-before">You’ll want to be sure to note the parameters used, which you can find in the <a href="https://oreil.ly/VMLKO">model’s documentation page on Ollama</a>. While Ollama can and will quantize models that are optimized to run locally, it can’t perform miracles, and only models that will fit in your system resources—most importantly, memory—will work. In this case, I ran the <code>gemma2:2b</code> (2-billion parameter) version, which requires about 8 GB of GPU RAM. On macOS, the shared RAM with the M-Series chips works well, while running on an M1 Mac with 16 Gb, the Gemma 2B is fast and smooth with Ollama.</p>&#13;
        <p>You can see me chatting with Gemma in <a data-type="xref" href="#ch17_figure_2_1748550058907817">Figure 17-2</a>. These responses took less than one second to receive on my two-year-old laptop!</p>&#13;
        <figure><div class="figure" id="ch17_figure_2_1748550058907817">&#13;
          <img src="assets/aiml_1702.png"/>&#13;
          <h6><span class="label">Figure 17-2. </span>Using Ollama in a terminal</h6>&#13;
        </div></figure>&#13;
        <p>It’s great to have a localized chat like this, <a contenteditable="false" data-primary="Ollama" data-secondary="running Llama3.2" data-type="indexterm" id="id1825"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="Ollama platform" data-tertiary="running Llama3.2" data-type="indexterm" id="id1826"/><a contenteditable="false" data-primary="Llama3.2 running on Ollama" data-type="indexterm" id="id1827"/>and you can experiment with different models, including multimodal ones like Llama 3.2. </p>&#13;
        <p>So, for example, you could issue the following command:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">ollama</code> <code class="n">run</code> <code class="n">llama3</code><code class="mf">.2</code><code class="o">-</code><code class="n">vision</code></pre>&#13;
        <p>Then, within the terminal, you could do multimodal processing. For example, if your terminal supported it, and you dragged and dropped an image into the terminal, you could ask the model what it could see in the image. The multimodal power of Llama would parse the image for you, and Ollama would handle all the technical difficulties.</p>&#13;
        <p>In my case, all I had to do was give a prompt and then drag and drop the image onto it. So, I opened an Ollama chat window with the preceding command and then entered this prompt:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">Please</code> <code class="n">give</code> <code class="n">me</code> <code class="n">a</code> <code class="n">detailed</code> <code class="n">analysis</code> <code class="n">of</code> <code class="n">what</code><code class="s1">'s in this image.</code><code class="w"/>&#13;
<code class="n">Call</code> <code class="n">out</code> <code class="nb">any</code> <code class="n">major</code> <code class="ow">or</code> <code class="n">minor</code> <code class="n">features</code> <code class="ow">and</code> <code class="n">tell</code> <code class="n">me</code> <code class="n">everything</code> <code class="n">you</code> <code class="n">know</code> <code class="n">about</code> <code class="n">it</code><code class="o">.</code>&#13;
<code class="n">Are</code> <code class="n">there</code> <code class="nb">any</code> <code class="n">interesting</code> <code class="ow">and</code> <code class="n">fun</code> <code class="n">facts</code><code class="err">?</code>&#13;
<code class="n">Maybe</code> <code class="n">even</code> <code class="n">estimate</code> <code class="n">when</code> <code class="n">this</code> <code class="n">picture</code> <code class="n">was</code> <code class="n">taken</code><code class="o">.</code></pre>&#13;
        <p>And then I just dragged and dropped the image into the chat window, and Ollama did the rest. </p>&#13;
        <p>You can see in <a data-type="xref" href="#ch17_figure_4_1748550058907880">Figure 17-3</a> how detailed the results were. This is a photo I took of Osaka castle one morning while on a run in 2018. While Llama couldn’t guess the date, it was able to predict the season based on the foliage in the image. It got everything else correct and gave very detailed output!</p>&#13;
       &#13;
        <figure><div class="figure" id="ch17_figure_4_1748550058907880">&#13;
          <img src="assets/aiml_1703.png"/>&#13;
          <h6><span class="label">Figure 17-3. </span>Using Ollama for a multimodal model</h6>&#13;
        </div></figure>&#13;
        <p>While it’s really cool to have a local LLM that you can chat with in a privacy-preserving way, I think the real power in Ollama is in using it as a server that can then be the foundation of an application. We’ll explore that next.<a contenteditable="false" data-primary="" data-startref="ch17strt" data-type="indexterm" id="id1828"/><a contenteditable="false" data-primary="" data-startref="ch17strt2" data-type="indexterm" id="id1829"/></p>&#13;
      </div></section>&#13;
      <section class="pagebreak-before less_space" data-pdf-bookmark="Running Ollama as a Server" data-type="sect1"><div class="sect1" id="ch17_running_ollama_as_a_server_1748550058915379">&#13;
        <h1>Running Ollama as a Server</h1>&#13;
        <p>To put Ollama into server mode, you simply issue the following command:<a contenteditable="false" data-primary="Ollama" data-secondary="server mode" data-type="indexterm" id="id1830"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="Ollama platform" data-tertiary="server mode" data-type="indexterm" id="id1831"/></p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">ollama</code> <code class="n">serve</code></pre>&#13;
        <p>This will run the Ollama server on port 11434 by default, so you can hit it and ask it to do inference with a <code>curl</code> command to test it. </p>&#13;
        <p>In a separate terminal window, you issue a <code>curl</code> command. Here’s an example:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">curl</code> <code class="n">http</code><code class="p">:</code><code class="o">//</code><code class="n">localhost</code><code class="p">:</code><code class="mi">11434</code><code class="o">/</code><code class="n">api</code><code class="o">/</code><code class="n">generate</code> <code class="o">-</code><code class="n">d</code> <code class="s1">'{</code><code class="w"/>&#13;
  <code class="s2">"model"</code><code class="p">:</code> <code class="s2">"gemma2:2b"</code><code class="p">,</code>&#13;
  <code class="s2">"prompt"</code><code class="p">:</code> <code class="s2">"Why is the sky blue?"</code><code class="p">,</code>&#13;
  <code class="s2">"stream"</code><code class="p">:</code> <code class="n">false</code>&#13;
<code class="p">}</code><code class="s1">'</code><code class="w"/></pre>&#13;
        <p>Note the <code>stream</code> parameter. If you set it to true, you’ll get an active HTTP connection that will send the answer word by word. That will give you a faster time to the first word, which is very suitable for chat applications. And because the answer will appear little by little and usually faster than a person can read, it will make for a better user experience.</p>&#13;
        <p>On the other hand, if you set the <code>stream</code> parameter to <code>false</code>, as I have done here, it will take longer to send something back, but when it does, you’ll get everything at once. The time to the last token will probably be about the same as in streaming, but given that there will be no output for a little while, it will feel slower.</p>&#13;
        <p>The preceding <code>curl</code> to Gemma gave me this response:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="p">{</code><code class="s2">"model"</code><code class="p">:</code><code class="s2">"gemma2:2b"</code><code class="p">,</code><code class="s2">"created_at"</code><code class="p">:</code><code class="s2">"2024-12-09T18:10:05.711484Z"</code><code class="p">,</code>&#13;
    <code class="s2">"response"</code><code class="p">:</code><code class="s2">"The sky appears blue because ... phenomena! </code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code>&#13;
    <code class="s2">"done"</code><code class="p">:</code><code class="n">true</code><code class="p">,</code>&#13;
    <code class="s2">"done_reason"</code><code class="p">:</code><code class="s2">"stop"</code><code class="p">,</code>&#13;
    <code class="s2">"context"</code><code class="p">:[</code><code class="mi">106</code><code class="p">,</code><code class="mi">1645</code><code class="p">,</code><code class="mi">108</code><code class="p">,</code><code class="mi">4385</code><code class="p">,</code><code class="mi">603</code><code class="p">,</code><code class="mi">573</code><code class="p">,</code><code class="o">...</code><code class="p">,</code><code class="mi">235248</code><code class="p">,</code><code class="mi">108</code><code class="p">],</code>&#13;
    <code class="s2">"total_duration"</code><code class="p">:</code><code class="mi">7994972625</code><code class="p">,</code>&#13;
    <code class="s2">"load_duration"</code><code class="p">:</code><code class="mi">820325334</code><code class="p">,</code>&#13;
    <code class="s2">"prompt_eval_count"</code><code class="p">:</code><code class="mi">15</code><code class="p">,</code>&#13;
    <code class="s2">"prompt_eval_duration"</code><code class="p">:</code><code class="mi">2599000000</code><code class="p">,</code>&#13;
    <code class="s2">"eval_count"</code><code class="p">:</code><code class="mi">282</code><code class="p">,</code>&#13;
    <code class="s2">"eval_duration"</code><code class="p">:</code><code class="mi">4573000000</code><code class="p">}</code><code class="o">%</code></pre>&#13;
        <p>I trimmed the response text and the context for brevity. Ultimately, you’d use the response text in an application, but I wanted to also show you how you can build more robust applications showing you everything else that the model provided.</p>&#13;
        <p>The <code>done</code> parameter demonstrates that the prompt returned successfully. When the value is streaming, this parameter will be set to <code>false</code> until it has finished sending the text. That way, you can keep your UI updating the text word by word until the message is complete. </p>&#13;
        <p>The <code>done_reason</code> parameter is useful in checking for errors, particularly when streaming. It will usually contain <code>stop</code> for normal completion, but in other circumstances, it might say <code>length</code>, which indicates that you’ve hit a token limit; <code>canceled</code> if the user cancels the request (by interrupting streaming, for example); or <code>error</code>. </p>&#13;
        <p>The count values are also useful if you want to manage or report on token usage. The <code>prompt_eval_count</code> parameter tells you how many tokens were used in your prompt, and in this case, it was 15. Similarly, the <code>eval_count</code> parameter tells you how many tokens were used in the response, and in this case (of course), it was 282. The various duration numbers are in nanoseconds, so in this case, we can see that the total was 0.8 seconds (or more accurately, 820325334 nanoseconds).</p>&#13;
        <p>If you want to attach a file to your <code>curl</code> prompt (for example, to interpret the contents of an image), you can do so by encoding the image to <code>base64</code> and passing it in the images array. </p>&#13;
        <p>So, with the image of Osaka castle I used in <a data-type="xref" href="#ch17_figure_4_1748550058907880">Figure 17-3</a>, I could do the following:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">curl</code> <code class="o">-</code><code class="n">X</code> <code class="n">POST</code> \&#13;
  <code class="o">-</code><code class="n">H</code> <code class="s2">"Content-Type: application/json"</code> \&#13;
  <code class="o">-</code><code class="n">d</code> <code class="s2">"{</code><code class="se">\"</code><code class="s2">model</code><code class="se">\"</code><code class="s2">:</code><code class="se">\"</code><code class="s2">llama3.2-vision</code><code class="se">\"</code><code class="s2">,</code><code class="w"/>&#13;
       \<code class="s2">"prompt</code><code class="se">\"</code><code class="s2">:</code><code class="se">\"</code><code class="s2">What is in this image?</code><code class="se">\"</code><code class="s2">,</code><code class="w"/>&#13;
       \<code class="s2">"images</code><code class="se">\"</code><code class="s2">:[</code><code class="se">\"</code><code class="s2">$(cat ./osaka.jpg | base64 | tr -d '</code><code class="se">\n</code><code class="s2">')</code><code class="se">\"</code><code class="s2">],</code><code class="w"/>&#13;
       \<code class="s2">"stream</code><code class="se">\"</code><code class="s2">:false}"</code> \&#13;
       <code class="n">http</code><code class="p">:</code><code class="o">//</code><code class="n">localhost</code><code class="p">:</code><code class="mi">11434</code><code class="o">/</code><code class="n">api</code><code class="o">/</code><code class="n">generate</code></pre>&#13;
        <p>The key here is to note how the images are sent. They need to be <code>base64</code> encoded, where they are turned into a string-like blob that’s easy to put into a JSON payload, instead of uploading the binary image. But you should be careful with code here, because it depends on your system. The code I used—<code>$(cat ./osaka.jpg | base64 | tr -d '\n')</code>—is based on how to do <code>base64</code> encoding on a Mac. Different systems may produce different <code>base64</code> encodings for images, and they can lead to errors on the backend.</p>&#13;
        <p>The response, abbreviated for clarity, is this:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="p">{</code>&#13;
    <code class="s2">"model"</code><code class="p">:</code><code class="s2">"llama3.2-vision"</code><code class="p">,</code>&#13;
    <code class="s2">"created_at"</code><code class="p">:</code><code class="s2">"2024-12-10T17:15:06.264497Z"</code><code class="p">,</code>&#13;
    <code class="s2">"response"</code><code class="p">:</code><code class="s2">"The image depicts Osaka Castle..."</code><code class="p">,</code>&#13;
    <code class="s2">"done"</code><code class="p">:</code><code class="n">true</code><code class="p">,</code>&#13;
    <code class="s2">"done_reason"</code><code class="p">:</code>&#13;
    <code class="s2">"stop"</code><code class="p">,</code>&#13;
    <code class="s2">"context"</code><code class="p">:[</code><code class="mi">128006</code><code class="p">,</code><code class="mi">882</code><code class="p">,</code><code class="mi">128007</code><code class="p">,</code><code class="mi">271</code><code class="p">,</code><code class="mi">58</code><code class="p">,</code><code class="o">...</code><code class="p">],</code>&#13;
    <code class="s2">"total_duration"</code><code class="p">:</code><code class="mi">88817301209</code><code class="p">,</code>&#13;
    <code class="s2">"load_duration"</code><code class="p">:</code><code class="mi">21197292</code><code class="p">,</code>&#13;
    <code class="s2">"prompt_eval_count"</code><code class="p">:</code><code class="mi">19</code><code class="p">,</code>&#13;
    <code class="s2">"prompt_eval_duration"</code><code class="p">:</code><code class="mi">84560000000</code><code class="p">,</code>&#13;
    <code class="s2">"eval_count"</code><code class="p">:</code><code class="mi">56</code><code class="p">,</code>&#13;
    <code class="s2">"eval_duration"</code><code class="p">:</code><code class="mi">4050000000</code>&#13;
<code class="p">}</code><code class="o">%</code></pre>&#13;
        <p>If you’re trying this on your development box, you may notice that it starts up slowly as it loads the model into memory. It can take one to two minutes, but once it’s loaded and warmed up, successive inferences will be quicker.</p>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Building an App that Uses an Ollama LLM" data-type="sect1"><div class="sect1" id="ch17_building_an_app_that_uses_an_ollama_llm_1748550058915441">&#13;
        <h1>Building an App that Uses an Ollama LLM</h1>&#13;
        <p>It’s all very nice to be able<a contenteditable="false" data-primary="Ollama" data-secondary="building an app that uses Ollama LLM" data-type="indexterm" id="ch17app"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="Ollama platform" data-tertiary="building an app that uses Ollama LLM" data-type="indexterm" id="ch17app2"/> to chat with a local model or curl to it like we just saw. But the next step is to consider building applications that use local LLMs, and in particular, building applications that work within your network to keep LLM inference local. </p>&#13;
        <p>So, for example, consider <a data-type="xref" href="#ch17_figure_5_1748550058907900">Figure 17-4</a>, which depicts the architecture of a typical application that uses the API for an LLM like Gemini, GPT, or Claude. In this case, the user has an application that invokes the LLM service via an API over the internet.</p>&#13;
        <figure><div class="figure" id="ch17_figure_5_1748550058907900">&#13;
          <img alt="" src="assets/aiml_1704.png"/>&#13;
          <h6><span class="label">Figure 17-4. </span>Accessing an LLM via API over the internet</h6>&#13;
        </div></figure>&#13;
        <p>Another pattern is quite similar: a service provider provides a backend web server, and that backend uses LLM functionality on your behalf. Ultimately, it still “wraps” the LLM on your behalf (see <a data-type="xref" href="#ch17_figure_6_1748550058907917">Figure 17-5</a>).</p>&#13;
        <figure><div class="figure" id="ch17_figure_6_1748550058907917">&#13;
          <img alt="" src="assets/aiml_1705.png"/>&#13;
          <h6><span class="label">Figure 17-5. </span>Accessing an LLM via a backend web server</h6>&#13;
        </div></figure>&#13;
        <p>The issue here is that data<a contenteditable="false" data-primary="private data" data-secondary="Ollama protecting" data-type="indexterm" id="id1832"/><a contenteditable="false" data-primary="Ollama" data-secondary="data privacy" data-type="indexterm" id="id1833"/><a contenteditable="false" data-primary="datasets" data-secondary="privacy of data via Ollama" data-type="indexterm" id="id1834"/> that could be private to your users and that they share with you (in the blue boxes) gets passed to a third party across the internet (in the green boxes). This can lead to limitations in how your application might be useful to them. Consider scenarios where there’s information that should always be private or where there’s IP that you don’t want to share. In the early days of ChatGPT, a lot of companies banned its use for that reason. The classic case involves source code, where company IP might be sent to a competitor for analysis!</p>&#13;
        <p>You can mitigate this problem by using a technology like Ollama. In this case, the architecture would change so that instead of the data being passed across the internet, the API and the server for the LLM would both be on your home or company’s network, as in <a data-type="xref" href="#ch17_figure_7_1748550058907933">Figure 17-6</a>. </p>&#13;
        <figure><div class="figure" id="ch17_figure_7_1748550058907933">&#13;
          <img alt="" src="assets/aiml_1706.png"/>&#13;
          <h6><span class="label">Figure 17-6. </span>Using an LLM in your data center</h6>&#13;
        </div></figure>&#13;
        <p>Now, there are no privacy issues with sharing data with third parties. Additionally, you have control over the model version that is being used, so you don’t have to worry about the API causing regression issues. Look back to Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch17_figure_5_1748550058907900">17-4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch17_figure_6_1748550058907917">17-5</a>, and you’ll see that by accessing the LLMs over the internet, you’re taking a strong dependency on a particular version of an LLM. Given that LLMs are not deterministic, this effectively means the prompts that work today may not work tomorrow! </p>&#13;
        <p>So, with Ollama as a server to LLMs, you can build something like you saw in <a data-type="xref" href="#ch17_figure_7_1748550058907933">Figure 17-6</a>. In the development environment and what we’ll be doing in the rest of this chapter, there’s no data center—the Ollama server will just run at the localhost, but changing your code to one that you have access to over the local network will just mean a change of server address.</p>&#13;
        <section data-pdf-bookmark="The Scenario" data-type="sect2"><div class="sect2" id="ch17_the_scenario_1748550058915496">&#13;
          <h2>The Scenario</h2>&#13;
          <p>As an example of a simple scenario,<a contenteditable="false" data-primary="Ollama" data-secondary="building an app that uses Ollama LLM" data-tertiary="scenario" data-type="indexterm" id="id1835"/> let’s build the basis of an app that uses a local llama to do analysis of books. It will allow the user to specify a book (as a text file, for simplicity’s sake), and it will bundle that to a call to an LLM to have it analyze the book. The app will also primarily be driven off a prompt to that backend. Perhaps this type of app could be used by a publishing house to determine how it should give feedback to an author to change a book, or by a book agent to work with the author to help them make the book more sellable. As you can imagine, the book content is valuable IP, and it should not be shared with a backend from a third party for <span class="keep-together">analysis.</span></p>&#13;
          <p>So, for example, consider this prompt:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">You</code> <code class="n">are</code> <code class="n">an</code> <code class="n">expert</code> <code class="n">storyteller</code> <code class="n">who</code> <code class="n">understands</code> <code class="n">story</code> <code class="n">structure</code><code class="p">,</code>&#13;
<code class="n">nuance</code><code class="p">,</code> <code class="ow">and</code> <code class="n">content</code><code class="o">.</code> <code class="n">Attached</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">novel</code><code class="p">,</code> <code class="n">so</code> <code class="n">please</code> <code class="n">evaluate</code>&#13;
<code class="n">this</code> <code class="n">novel</code> <code class="k">for</code> <code class="n">storylines</code> <code class="ow">and</code> <code class="n">suggest</code> <code class="n">improvements</code> <code class="n">that</code> <code class="n">could</code> &#13;
<code class="n">be</code> <code class="n">made</code> <code class="ow">in</code> <code class="n">character</code> <code class="n">development</code><code class="p">,</code> <code class="n">plot</code><code class="p">,</code> <code class="ow">and</code> <code class="n">emotional</code> &#13;
<code class="n">content</code><code class="o">.</code> <code class="n">Be</code> <code class="k">as</code> <code class="n">verbose</code> <code class="k">as</code> <code class="n">needed</code> <code class="n">to</code> <code class="n">provide</code> <code class="n">an</code> <code class="ow">in</code><code class="o">-</code><code class="n">depth</code> &#13;
<code class="n">analysis</code> <code class="n">that</code> <code class="n">would</code> <code class="n">help</code> <code class="n">the</code> <code class="n">author</code> <code class="n">understand</code> <code class="n">how</code> <code class="n">their</code> <code class="n">work</code> &#13;
<code class="n">would</code> <code class="n">be</code> <code class="n">accepted</code><code class="o">.</code></pre>&#13;
          <p>Sending that to an LLM along with the text should have the desired effect: getting an analysis of the book based on the <em>artificial understanding</em> of the contents of the text and the generative abilities of a transformer-based model to create output guided by the prompt.</p>&#13;
          <p>On this book’s GitHub page,<a contenteditable="false" data-primary="online resources" data-secondary="full text of a novel" data-type="indexterm" id="id1836"/><a contenteditable="false" data-primary="GitHub" data-secondary="full text of a novel" data-type="indexterm" id="id1837"/><a contenteditable="false" data-primary="text of a novel online" data-type="indexterm" id="id1838"/><a contenteditable="false" data-primary="datasets" data-secondary="full text of a novel" data-type="indexterm" id="id1839"/> I’ve provided <a href="https://oreil.ly/pytorch_ch18">the full text of a novel</a> that I wrote several years ago and that I now have the full rights to, so you can try it with a real book like that one if you like. However, depending on your model, the context window size might not be big enough for a complete novel.</p>&#13;
          <p>Generally, I like to build a simple proof-of-concept as a Python file to see how well it works and test it on my local machine. There are constraints in doing this, but it will at least let us see if the concept is feasible.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Building a Python Proof-of-Concept" data-type="sect2"><div class="sect2" id="ch17_building_a_python_proof_of_concept_1748550058915545">&#13;
          <h2>Building a Python Proof-of-Concept</h2>&#13;
          <p>Now, let’s take a look at a Python script that can perform the analysis for us.<a contenteditable="false" data-primary="Ollama" data-secondary="building an app that uses Ollama LLM" data-tertiary="Python proof-of-concept" data-type="indexterm" id="ch17proof"/></p>&#13;
          <p>Let’s start with reading the contents of the file:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Read the file content</code>&#13;
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filepath</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'utf-8'</code><code class="p">)</code> <code class="k">as</code> <code class="n">file</code><code class="p">:</code>&#13;
    <code class="n">file_content</code> <code class="o">=</code> <code class="n">file</code><code class="o">.</code><code class="n">read</code><code class="p">()</code></pre>&#13;
          <p>We’re going to use Ollama on the backend, so let’s set up details for the request by specifying the URL and the content headers we’ll call:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Prepare the request</code>&#13;
<code class="n">url</code> <code class="o">=</code> <code class="s2">"http://localhost:11434/api/generate"</code>&#13;
<code class="n">headers</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Content-Type"</code><code class="p">:</code> <code class="s2">"application/json"</code><code class="p">}</code></pre>&#13;
          <p>Next, we’ll put together the payload that we’re going to post to the backend. This contains the model name (as a parameter called <code>model</code>), the prompt, and the stream flag. We don’t want to stream, so we’ll set the stream to <code>False</code>.</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">payload</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s2">"model"</code><code class="p">:</code> <code class="n">model</code><code class="p">,</code>&#13;
    <code class="s2">"prompt"</code><code class="p">:</code> <code class="sa">f</code><code class="s2">"You are an expert storyteller who understands </code><code class="w"/>&#13;
                <code class="n">story</code> <code class="n">structure</code><code class="p">,</code> <code class="n">nuance</code><code class="p">,</code> <code class="ow">and</code> <code class="n">content</code><code class="o">.</code> <code class="n">Attached</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">novel</code><code class="p">,</code> &#13;
                <code class="n">please</code> <code class="n">evaluate</code> <code class="n">this</code> <code class="n">novel</code> <code class="k">for</code> <code class="n">storylines</code><code class="p">,</code> <code class="ow">and</code> <code class="n">suggest</code> &#13;
                <code class="n">improvements</code> <code class="n">that</code> <code class="n">could</code> <code class="n">be</code> <code class="n">made</code> <code class="ow">in</code> <code class="n">character</code> <code class="n">development</code><code class="p">,</code> &#13;
                <code class="n">plot</code><code class="p">,</code> <code class="ow">and</code> <code class="n">emotional</code> <code class="n">content</code><code class="o">.</code> <code class="n">Be</code> <code class="k">as</code> <code class="n">verbose</code> <code class="k">as</code> <code class="n">needed</code> &#13;
                <code class="n">to</code> <code class="n">provide</code> <code class="n">an</code> <code class="ow">in</code><code class="o">-</code><code class="n">depth</code> <code class="n">analysis</code> <code class="n">that</code> <code class="n">would</code> <code class="n">help</code> <code class="n">the</code> <code class="n">author</code> &#13;
                <code class="n">understand</code> <code class="n">how</code> <code class="n">their</code> <code class="n">work</code> <code class="n">would</code> <code class="n">be</code> <code class="n">accepted</code><code class="p">:</code> <code class="p">{</code><code class="n">file_content</code><code class="p">}</code><code class="s2">",</code><code class="w"/>&#13;
    <code class="s2">"stream"</code><code class="p">:</code> <code class="kc">False</code>&#13;
<code class="p">}</code></pre>&#13;
          <p>Note that we’re appending <code>file_content</code> to the prompt. At this point, from the preceding code, it’s just a text blob.</p>&#13;
          <p>For the model parameter, you can use whatever you like. For this experiment, I tried using the very small <code>Gemma 2b</code> parameter model by specifying <code>gemma2:2b</code> as the model.</p>&#13;
          <p>Now, we can POST the request to Ollama like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">try</code><code class="p">:</code>&#13;
    <code class="c1"># Send the request</code>&#13;
    <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">,</code> <code class="n">json</code><code class="o">=</code><code class="n">payload</code><code class="p">)</code>&#13;
    <code class="n">response</code><code class="o">.</code><code class="n">raise_for_status</code><code class="p">()</code>  <code class="c1"># Raise an exception for bad status </code>&#13;
 &#13;
    <code class="c1"># Parse the response</code>&#13;
    <code class="n">result</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()[</code><code class="s1">'response'</code><code class="p">]</code>&#13;
 &#13;
    <code class="c1"># Return the response text from Ollama</code>&#13;
    <code class="k">return</code> <code class="n">result</code>&#13;
 &#13;
<code class="k">except</code> <code class="n">requests</code><code class="o">.</code><code class="n">exceptions</code><code class="o">.</code><code class="n">RequestException</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>&#13;
    <code class="k">raise</code> <code class="ne">Exception</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Error making request to Ollama: </code><code class="si">{</code><code class="nb">str</code><code class="p">(</code><code class="n">e</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
<code class="k">except</code> <code class="n">json</code><code class="o">.</code><code class="n">JSONDecodeError</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>&#13;
    <code class="k">raise</code> <code class="ne">Exception</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Error parsing Ollama response: </code><code class="si">{</code><code class="nb">str</code><code class="p">(</code><code class="n">e</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>&#13;
          <p>This is fully synchronous in that we post the data and block everything until we get the result. In a real application, you’d likely do that part asynchronously, but I’m keeping it simple for now.</p>&#13;
          <p>As we get the response back from the server, it will contain JSON fields, and the response field will contain the text, as we saw earlier in this chapter. We’ll return this as a <code>dict</code> data type.</p>&#13;
          <p>Next, we can wrap all this code in a function called <code>analyze</code>, with this signature:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">analyze_file</code><code class="p">(</code><code class="n">filepath</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">model</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"gemma2:2b"</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">:</code></pre>&#13;
          <p>Then, we can easily call it with this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">result</code> <code class="o">=</code> <code class="n">analyze_file</code><code class="p">(</code><code class="nb">str</code><code class="p">(</code><code class="n">input_path</code><code class="p">))</code></pre>&#13;
          <p class="pagebreak-before less_space">Given that Gemma is such a small model, the output it gives is very impressive! My local instance, via Ollama, was able to digest the book and give back a detailed analysis in just a few seconds. Here’s an excerpt (with spoilers if you haven’t read the book yet):</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="o">**</code><code class="n">Strengths</code><code class="p">:</code><code class="o">**</code>&#13;
 &#13;
<code class="o">*</code> <code class="o">**</code><code class="n">Intriguing</code> <code class="n">Premise</code><code class="p">:</code><code class="o">**</code> <code class="n">The</code> <code class="n">concept</code> <code class="n">of</code> <code class="n">a</code> <code class="n">deadly</code> <code class="n">plague</code> <code class="n">originating</code> &#13;
<code class="kn">from</code> <code class="nn">space</code> <code class="k">with</code> <code class="n">potentially</code> <code class="n">disastrous</code> <code class="n">consequences</code> <code class="ow">is</code> <code class="n">both</code> &#13;
<code class="n">suspenseful</code> <code class="ow">and</code> <code class="n">timely</code><code class="p">,</code> <code class="n">appealing</code> <code class="n">to</code> <code class="n">a</code> <code class="n">wider</code> <code class="n">audience</code><code class="o">.</code>&#13;
<code class="o">*</code> <code class="o">**</code><code class="n">Realistic</code> <code class="n">Characters</code><code class="p">:</code><code class="o">**</code>  <code class="n">The</code> <code class="n">characters</code> <code class="n">feel</code> <code class="n">grounded</code> <code class="n">despite</code> <code class="n">being</code> &#13;
<code class="n">involved</code> <code class="ow">in</code> <code class="n">extraordinary</code> <code class="n">events</code><code class="p">,</code> <code class="ow">and</code> <code class="n">their</code> <code class="n">personalities</code> <code class="n">shine</code> &#13;
<code class="n">through</code> <code class="p">(</code><code class="n">Aisha</code><code class="s1">'s determination, Soo-Kyung'</code><code class="n">s</code> <code class="n">wisdom</code><code class="p">)</code><code class="o">.</code> <code class="n">Their</code> <code class="n">connection</code> &#13;
<code class="n">adds</code> <code class="n">emotional</code> <code class="n">weight</code><code class="o">.</code>&#13;
<code class="o">*</code> <code class="o">**</code><code class="n">Suspenseful</code> <code class="n">Tone</code><code class="p">:</code><code class="o">**</code> <code class="n">The</code> <code class="n">story</code> <code class="n">builds</code> <code class="n">suspense</code> <code class="n">gradually</code><code class="o">.</code> <code class="n">You</code> <code class="n">expertly</code> &#13;
<code class="n">use</code> <code class="n">cliffhangers</code> <code class="n">like</code> <code class="n">the</code> <code class="n">three</code><code class="o">-</code><code class="n">year</code> <code class="n">deadline</code> <code class="k">for</code> <code class="n">the</code> <code class="n">plague</code> &#13;
<code class="n">to</code> <code class="n">leave</code> <code class="n">readers</code> <code class="n">wanting</code> <code class="n">more</code><code class="o">.</code>&#13;
<code class="o">*</code> <code class="o">**</code><code class="n">Worldbuilding</code> <code class="n">Potential</code><code class="p">:</code><code class="o">**</code>  <code class="n">The</code> <code class="n">mention</code> <code class="n">of</code> <code class="n">the</code> <code class="n">moon</code> <code class="n">base</code> <code class="ow">and</code> <code class="n">potential</code> &#13;
<code class="n">interstellar</code> <code class="n">jumps</code> <code class="n">introduces</code> <code class="n">a</code> <code class="n">rich</code> <code class="n">world</code> <code class="k">with</code> <code class="n">possibilities</code> &#13;
<code class="k">for</code> <code class="n">further</code> <code class="n">exploration</code><code class="o">.</code>&#13;
 </pre>&#13;
          <p>It’s pretty impressive work by Gemma to give me this analysis! Other than formatting issues (there are a lot of * characters, which Gemma may have added because it’s sci-fi), we have some great content here, and it’s worth looking further into building an app. So, let’s do that next and explore a web-based app that I can upload my files to so that I can get an analysis within a website.<a contenteditable="false" data-primary="" data-startref="ch17proof" data-type="indexterm" id="id1840"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Creating a Web App for Ollama" data-type="sect2"><div class="sect2" id="ch17_creating_a_web_app_for_ollama_1748550058915594">&#13;
          <h2>Creating a Web App for Ollama</h2>&#13;
          <p>In this scenario, you’ll create a <code>node.js</code> app<a contenteditable="false" data-primary="Ollama" data-secondary="building an app that uses Ollama LLM" data-tertiary="creating a web app" data-type="indexterm" id="id1841"/><a contenteditable="false" data-primary="node.js app that uses Ollama" data-type="indexterm" id="id1842"/><a contenteditable="false" data-primary="web app built that uses Ollama" data-type="indexterm" id="id1843"/> that provides a local website that the user can upload text to, and you’ll get an analysis back in the browser. The results look like those shown in <a data-type="xref" href="#ch17_figure_8_1748550058907947">Figure 17-7</a>.</p>&#13;
          <figure><div class="figure" id="ch17_figure_8_1748550058907947">&#13;
            <img src="assets/aiml_1707.png"/>&#13;
            <h6><span class="label">Figure 17-7. </span>The browser-based Gemma analysis tool</h6>&#13;
          </div></figure>&#13;
          <p>If you’re not familiar <a contenteditable="false" data-primary="node.js app that uses Ollama" data-secondary="node.js instructions online" data-type="indexterm" id="id1844"/><a contenteditable="false" data-primary="online resources" data-secondary="node.js instructions" data-type="indexterm" id="id1845"/>with <code>node.js</code> and how to install it, full instructions are on <a href="http://nodejs.org">the Node website</a>. The architecture of a simple node app is shown in <a data-type="xref" href="#ch17_figure_9_1748550058907961">Figure 17-8</a>.</p>&#13;
          <figure><div class="figure" id="ch17_figure_9_1748550058907961">&#13;
              <img src="assets/aiml_1708.png"/>&#13;
            <h6><span class="label">Figure 17-8. </span>The node app directory</h6>&#13;
          </div></figure>&#13;
          <p>In its simplest form, a <em>node app</em> is a directory containing a JavaScript file called <em>app.js</em> that contains the core application logic, a <em>package.json</em> file that gives details of the dependencies, and an <em>index.html</em> file that has the template for the app output.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="The app.js File" data-type="sect2"><div class="sect2" id="ch17_the_app_js_file_1748550058915645">&#13;
          <h2>The app.js File</h2>&#13;
          <p>This file contains the core logic<a contenteditable="false" data-primary="Ollama" data-secondary="building an app that uses Ollama LLM" data-tertiary="app.js file" data-type="indexterm" id="id1846"/> for the service. A <code>node.js</code> app will run on a server by listening to a particular port, and it starts with this code:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">const</code> <code class="n">PORT</code> <code class="o">=</code> <code class="n">process</code><code class="o">.</code><code class="n">env</code><code class="o">.</code><code class="n">PORT</code> <code class="o">||</code> <code class="mi">3000</code><code class="p">;</code>&#13;
<code class="n">app</code><code class="o">.</code><code class="n">listen</code><code class="p">(</code><code class="n">PORT</code><code class="p">,</code> <code class="p">()</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="n">console</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="err">`</code><code class="n">Server</code> <code class="n">running</code> <code class="n">on</code> <code class="n">port</code> <code class="err">$</code><code class="p">{</code><code class="n">PORT</code><code class="p">}</code><code class="err">`</code><code class="p">);</code>&#13;
<code class="p">});</code></pre>&#13;
          <p>To analyze a book, you can define an endpoint that the end user can post the book to with the <code>app.post</code> command in <code>node.js</code>:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s1">'/analyze'</code><code class="p">,</code> <code class="n">upload</code><code class="o">.</code><code class="n">single</code><code class="p">(</code><code class="s1">'novel'</code><code class="p">),</code> <code class="k">async</code> <code class="p">(</code><code class="n">req</code><code class="p">,</code> <code class="n">res</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="k">try</code> <code class="p">{</code>&#13;
    <code class="k">if</code> <code class="p">(</code><code class="err">!</code><code class="n">req</code><code class="o">.</code><code class="n">file</code><code class="p">)</code> <code class="p">{</code>&#13;
      <code class="k">return</code> <code class="n">res</code><code class="o">.</code><code class="n">status</code><code class="p">(</code><code class="mi">400</code><code class="p">)</code><code class="o">.</code><code class="n">send</code><code class="p">(</code><code class="s1">'No file uploaded'</code><code class="p">);</code>&#13;
    <code class="p">}</code></pre>&#13;
          <p>This function is asynchronous, and it accepts a file. If the file isn’t present in the upload, an error will return.</p>&#13;
          <p>If the code continues, then there’s a file present. This code will start a new job (allowing the server to operate multiple processes in parallel) that uploads the file, reads its text into <code>fileContent</code>, and then cleans it up:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="o">//</code> <code class="n">Generate</code> <code class="n">a</code> <code class="n">unique</code> <code class="n">job</code> <code class="n">ID</code>&#13;
<code class="n">const</code> <code class="n">jobId</code> <code class="o">=</code> <code class="n">Date</code><code class="o">.</code><code class="n">now</code><code class="p">()</code><code class="o">.</code><code class="n">toString</code><code class="p">();</code>&#13;
 &#13;
<code class="o">//</code> <code class="n">Store</code> <code class="n">job</code> <code class="n">status</code>&#13;
<code class="n">analysisJobs</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">jobId</code><code class="p">,</code> <code class="p">{</code> <code class="n">status</code><code class="p">:</code> <code class="s1">'processing'</code> <code class="p">});</code>&#13;
 &#13;
<code class="o">//</code> <code class="n">Start</code> <code class="n">analysis</code> <code class="ow">in</code> <code class="n">background</code>&#13;
<code class="n">const</code> <code class="n">fileContent</code> <code class="o">=</code> <code class="k">await</code> <code class="n">fs</code><code class="o">.</code><code class="n">readFile</code><code class="p">(</code><code class="n">req</code><code class="o">.</code><code class="n">file</code><code class="o">.</code><code class="n">path</code><code class="p">,</code> <code class="s1">'utf8'</code><code class="p">);</code>&#13;
 &#13;
<code class="o">//</code> <code class="n">Clean</code> <code class="n">up</code> <code class="n">uploaded</code> <code class="n">file</code>&#13;
<code class="k">await</code> <code class="n">fs</code><code class="o">.</code><code class="n">unlink</code><code class="p">(</code><code class="n">req</code><code class="o">.</code><code class="n">file</code><code class="o">.</code><code class="n">path</code><code class="p">);</code></pre>&#13;
          <p>The analysis of the novel is a long-running process. In it, the contents are appended to the prompt and uploaded to Ollama, which then passes it to Gemma—which, upon completion, sends us a result. We’ll look at the analysis code in a moment, but the wrapper for this that operates in the background is here:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="o">//</code> <code class="n">Process</code> <code class="ow">in</code> <code class="n">background</code>&#13;
<code class="n">analyzeNovel</code><code class="p">(</code><code class="n">fileContent</code><code class="p">)</code>&#13;
  <code class="o">.</code><code class="n">then</code><code class="p">(</code><code class="n">result</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
    <code class="n">analysisJobs</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">jobId</code><code class="p">,</code> <code class="p">{</code>&#13;
      <code class="n">status</code><code class="p">:</code> <code class="s1">'completed'</code><code class="p">,</code>&#13;
      <code class="n">result</code><code class="p">:</code> <code class="n">result</code>&#13;
    <code class="p">});</code>&#13;
  <code class="p">})</code>&#13;
  <code class="o">.</code><code class="n">catch</code><code class="p">(</code><code class="n">error</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
    <code class="n">analysisJobs</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">jobId</code><code class="p">,</code> <code class="p">{</code>&#13;
      <code class="n">status</code><code class="p">:</code> <code class="s1">'error'</code><code class="p">,</code>&#13;
      <code class="n">error</code><code class="p">:</code> <code class="n">error</code><code class="o">.</code><code class="n">message</code>&#13;
    <code class="p">});</code>&#13;
  <code class="p">});</code></pre>&#13;
          <p>It simply calls the <code>analyzeNovel</code> method, sending the file content. If the process completes successfully, the <code>JobId</code> is updated with <code>completed</code>; otherwise, it’s updated with details of the failure. Note that upon a successful completion, the result is passed to <code>analysisJobs</code>. Later, when we look at the web client, we’ll see that after uploading the content, it will repeatedly poll the status of the job until it gets either a completion or an error. At that point, it can display the appropriate output.</p>&#13;
          <p>The <code>analyzeNovel</code> function looks very similar to our Python code from earlier. First, we’ll create the request body with the prompt. It contains the <code>modelID</code>, the prompt text with the novel text appended, and the <code>stream</code> parameter set to false:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">async</code> <code class="n">function</code> <code class="n">analyzeNovel</code><code class="p">(</code><code class="n">text</code><code class="p">)</code> <code class="p">{</code>&#13;
  <code class="k">try</code> <code class="p">{</code>&#13;
    <code class="n">console</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="s1">'Sending request to Ollama...'</code><code class="p">);</code>&#13;
    <code class="n">const</code> <code class="n">requestBody</code> <code class="o">=</code> <code class="p">{</code>&#13;
      <code class="n">model</code><code class="p">:</code> <code class="s1">'gemma2:2b'</code><code class="p">,</code>&#13;
      <code class="n">prompt</code><code class="p">:</code> <code class="err">`</code><code class="n">You</code> <code class="n">are</code> <code class="n">an</code> <code class="n">expert</code> <code class="n">storyteller</code> <code class="n">who</code> <code class="n">understands</code> &#13;
      <code class="n">story</code> <code class="n">structure</code><code class="p">,</code> <code class="n">nuance</code><code class="p">,</code> <code class="ow">and</code> <code class="n">content</code><code class="o">.</code> <code class="n">Attached</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">novel</code><code class="o">.</code> &#13;
      <code class="n">Please</code> <code class="n">evaluate</code> <code class="n">this</code> <code class="n">novel</code> <code class="k">for</code> <code class="n">storylines</code> <code class="ow">and</code> <code class="n">suggest</code> <code class="n">improvements</code> &#13;
      <code class="n">that</code> <code class="n">could</code> <code class="n">be</code> <code class="n">made</code> <code class="ow">in</code> <code class="n">character</code> <code class="n">development</code><code class="p">,</code> <code class="n">plot</code><code class="p">,</code> <code class="ow">and</code> <code class="n">emotional</code> <code class="n">content</code><code class="o">.</code> &#13;
      <code class="n">Be</code> <code class="k">as</code> <code class="n">verbose</code> <code class="k">as</code> <code class="n">needed</code> <code class="n">to</code> <code class="n">provide</code> <code class="n">an</code> <code class="ow">in</code><code class="o">-</code><code class="n">depth</code> <code class="n">analysis</code> <code class="n">that</code> <code class="n">would</code> <code class="n">help</code> &#13;
      <code class="n">the</code> <code class="n">author</code> <code class="n">understand</code> <code class="n">how</code> <code class="n">their</code> <code class="n">work</code> <code class="n">would</code> <code class="n">be</code> <code class="n">accepted</code><code class="p">:</code>\<code class="n">n</code>\<code class="n">n</code><code class="err">$</code><code class="p">{</code><code class="n">text</code><code class="p">}</code><code class="err">`</code><code class="p">,</code>&#13;
      <code class="n">stream</code><code class="p">:</code> <code class="n">false</code>&#13;
    <code class="p">};</code></pre>&#13;
          <p>It will then post this to the Ollama backend and wait for the response. When it gets it, it will turn it into a string with <code>JSON.stringify</code>:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">const</code> <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">fetch</code><code class="p">(</code><code class="n">OLLAMA_URL</code><code class="p">,</code> <code class="p">{</code>&#13;
  <code class="n">method</code><code class="p">:</code> <code class="s1">'POST'</code><code class="p">,</code>&#13;
  <code class="n">headers</code><code class="p">:</code> <code class="p">{</code>&#13;
    <code class="s1">'Content-Type'</code><code class="p">:</code> <code class="s1">'application/json'</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
  <code class="n">body</code><code class="p">:</code> <code class="n">JSON</code><code class="o">.</code><code class="n">stringify</code><code class="p">(</code><code class="n">requestBody</code><code class="p">)</code>&#13;
<code class="p">});</code></pre>&#13;
          <p>If there’s an error, we’ll throw it; otherwise, we’ll read in the JSON payload from the HTTP response and filter out the response field, which contains the text response from the LLM. Note the two uses of the word<em> response</em> here. It can be confusing! The <em>response object</em> (<code>response.ok</code>, <code>response.status</code>, or <code>response.json</code>) is the HTTP response to the post that you made, while the <em>response property</em> (<code>data.response</code>) is the field within the <code>json</code> that contains the response from the LLM with the generated output:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">if</code> <code class="p">(</code><code class="err">!</code><code class="n">response</code><code class="o">.</code><code class="n">ok</code><code class="p">)</code> <code class="p">{</code>&#13;
  <code class="n">throw</code> <code class="n">new</code> <code class="n">Error</code><code class="p">(</code><code class="err">`</code><code class="n">HTTP</code> <code class="n">error</code><code class="err">!</code> <code class="n">status</code><code class="p">:</code> <code class="err">$</code><code class="p">{</code><code class="n">response</code><code class="o">.</code><code class="n">status</code><code class="p">}</code><code class="err">`</code><code class="p">);</code>&#13;
<code class="p">}</code>&#13;
 &#13;
<code class="n">const</code> <code class="n">data</code> <code class="o">=</code> <code class="k">await</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">();</code>&#13;
<code class="k">return</code> <code class="n">data</code><code class="o">.</code><code class="n">response</code><code class="p">;</code></pre>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="The Index.html File" data-type="sect2"><div class="sect2" id="ch17_the_index_html_file_1748550058915697">&#13;
          <h2>The Index.html File</h2>&#13;
          <p>In the public folder, a <a contenteditable="false" data-primary="Ollama" data-secondary="building an app that uses Ollama LLM" data-tertiary="index.html file" data-type="indexterm" id="id1847"/><a contenteditable="false" data-primary="node.js app that uses Ollama" data-secondary="index.html file" data-type="indexterm" id="id1848"/>file called <em>index.html</em> will render when you call the <code>node.js</code> server. By default, this is at <code>localhost:3000</code> if you’re running on your dev box. It will contain all the code to render the user interface for the app that we saw in <a data-type="xref" href="#ch17_figure_8_1748550058907947">Figure 17-7</a>.</p>&#13;
          <p>It interfaces with the backend through a form that is submitted via an HTTP-POST. The HTML code for the form looks like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="o">&lt;</code><code class="n">h1</code><code class="o">&gt;</code><code class="n">Novel</code> <code class="n">Analysis</code> <code class="n">Tool</code><code class="o">&lt;/</code><code class="n">h1</code><code class="o">&gt;</code>&#13;
<code class="o">&lt;</code><code class="n">div</code> <code class="n">class</code><code class="o">=</code><code class="s2">"upload-form"</code><code class="o">&gt;</code>&#13;
  <code class="o">&lt;</code><code class="n">h2</code><code class="o">&gt;</code><code class="n">Upload</code> <code class="n">your</code> <code class="n">novel</code><code class="o">&lt;/</code><code class="n">h2</code><code class="o">&gt;</code>&#13;
  <code class="o">&lt;</code><code class="n">form</code> <code class="nb">id</code><code class="o">=</code><code class="s2">"uploadForm"</code><code class="o">&gt;</code>&#13;
    <code class="o">&lt;</code><code class="nb">input</code> <code class="nb">type</code><code class="o">=</code><code class="s2">"file"</code> <code class="n">name</code><code class="o">=</code><code class="s2">"novel"</code> <code class="n">accept</code><code class="o">=</code><code class="s2">".txt"</code> <code class="n">required</code><code class="o">&gt;</code>&#13;
    <code class="o">&lt;</code><code class="n">br</code><code class="o">&gt;</code>&#13;
    <code class="o">&lt;</code><code class="n">button</code> <code class="nb">type</code><code class="o">=</code><code class="s2">"submit"</code> <code class="n">class</code><code class="o">=</code><code class="s2">"submit-button"</code><code class="o">&gt;</code><code class="n">Analyze</code> <code class="n">Novel</code><code class="o">&lt;/</code><code class="n">button</code><code class="o">&gt;</code>&#13;
  <code class="o">&lt;/</code><code class="n">form</code><code class="o">&gt;</code>&#13;
<code class="o">&lt;/</code><code class="n">div</code><code class="o">&gt;</code></pre>&#13;
          <p>The form is called <code>uploadForm</code>, so you can write code to execute when the user hits the submit button on this form like this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">document</code><code class="o">.</code><code class="n">getElementById</code><code class="p">(</code><code class="s1">'uploadForm'</code><code class="p">)</code>&#13;
        <code class="o">.</code><code class="n">addEventListener</code><code class="p">(</code><code class="s1">'submit'</code><code class="p">,</code> <code class="k">async</code> <code class="p">(</code><code class="n">e</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code><code class="n">e</code><code class="o">.</code><code class="n">preventDefault</code><code class="p">();</code>&#13;
 </pre>&#13;
          <p>The magic happens within this code by taking the attached file (as <code>FormData</code>) and passing it to the <code>/analyze</code> endpoint of the backend, as we defined using <code>app.post</code> in the <em>app.js</em> file (seen previously):</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="o">//</code> <code class="n">Upload</code> <code class="n">file</code>&#13;
<code class="n">const</code> <code class="n">formData</code> <code class="o">=</code> <code class="n">new</code> <code class="n">FormData</code><code class="p">(</code><code class="n">form</code><code class="p">);</code>&#13;
<code class="n">const</code> <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">fetch</code><code class="p">(</code><code class="s1">'/analyze'</code><code class="p">,</code> <code class="p">{</code>&#13;
  <code class="n">method</code><code class="p">:</code> <code class="s1">'POST'</code><code class="p">,</code>&#13;
  <code class="n">body</code><code class="p">:</code> <code class="n">formData</code>&#13;
<code class="p">});</code></pre>&#13;
          <p class="pagebreak-before less_space">Once this is done, the browser will get the <code>jobID</code> back from the server and continually poll the server asking for the status of that <code>jobID</code>. Once the status of the job is <span class="keep-together"><code>completed</code></span>, the server will output the results if the job succeeded or the error if it didn’t:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">if</code> <code class="p">(</code><code class="err">!</code><code class="n">response</code><code class="o">.</code><code class="n">ok</code><code class="p">)</code> <code class="n">throw</code> <code class="n">new</code> <code class="n">Error</code><code class="p">(</code><code class="s1">'Upload failed'</code><code class="p">);</code>&#13;
 &#13;
<code class="n">const</code> <code class="p">{</code> <code class="n">jobId</code> <code class="p">}</code> <code class="o">=</code> <code class="k">await</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">();</code>&#13;
 &#13;
<code class="o">//</code> <code class="n">Poll</code> <code class="k">for</code> <code class="n">results</code>&#13;
<code class="k">while</code> <code class="p">(</code><code class="n">true</code><code class="p">)</code> <code class="p">{</code>&#13;
  <code class="n">const</code> <code class="n">statusResponse</code> <code class="o">=</code> <code class="k">await</code> <code class="n">fetch</code><code class="p">(</code><code class="err">`</code><code class="o">/</code><code class="n">status</code><code class="o">/</code><code class="err">$</code><code class="p">{</code><code class="n">jobId</code><code class="p">}</code><code class="err">`</code><code class="p">);</code>&#13;
  <code class="k">if</code> <code class="p">(</code><code class="err">!</code><code class="n">statusResponse</code><code class="o">.</code><code class="n">ok</code><code class="p">)</code> <code class="n">throw</code> <code class="n">new</code> <code class="n">Error</code><code class="p">(</code><code class="s1">'Status check failed'</code><code class="p">);</code>&#13;
 &#13;
  <code class="n">const</code> <code class="n">status</code> <code class="o">=</code> <code class="k">await</code> <code class="n">statusResponse</code><code class="o">.</code><code class="n">json</code><code class="p">();</code>&#13;
 &#13;
  <code class="k">if</code> <code class="p">(</code><code class="n">status</code><code class="o">.</code><code class="n">status</code> <code class="o">===</code> <code class="s1">'completed'</code><code class="p">)</code> <code class="p">{</code>&#13;
    <code class="n">result</code><code class="o">.</code><code class="n">textContent</code> <code class="o">=</code> <code class="n">status</code><code class="o">.</code><code class="n">result</code><code class="p">;</code>&#13;
    <code class="n">result</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">display</code> <code class="o">=</code> <code class="s1">'block'</code><code class="p">;</code>&#13;
    <code class="n">loading</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">display</code> <code class="o">=</code> <code class="s1">'none'</code><code class="p">;</code>&#13;
    <code class="k">break</code><code class="p">;</code>&#13;
  <code class="p">}</code> <code class="k">else</code> <code class="k">if</code> <code class="p">(</code><code class="n">status</code><code class="o">.</code><code class="n">status</code> <code class="o">===</code> <code class="s1">'error'</code><code class="p">)</code> <code class="p">{</code>&#13;
    <code class="n">throw</code> <code class="n">new</code> <code class="n">Error</code><code class="p">(</code><code class="n">status</code><code class="o">.</code><code class="n">error</code><code class="p">);</code>&#13;
  <code class="p">}</code>&#13;
 &#13;
  <code class="o">//</code> <code class="n">Wait</code> <code class="n">before</code> <code class="n">polling</code> <code class="n">again</code>&#13;
  <code class="k">await</code> <code class="n">new</code> <code class="n">Promise</code><code class="p">(</code><code class="n">resolve</code> <code class="o">=&gt;</code> <code class="n">setTimeout</code><code class="p">(</code><code class="n">resolve</code><code class="p">,</code> <code class="mi">1000</code><code class="p">));</code>&#13;
<code class="p">}</code></pre>&#13;
          <p>The <code>setTimeout</code> code at the bottom ensures that we poll every second, but you could change this to reduce load on your server.</p>&#13;
          <p>To run this, simply navigate to the directory in your console and type this:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">node</code> <code class="n">app</code><code class="o">.</code><code class="n">js</code></pre>&#13;
          <p>And that’s it! You can get the fully working code in the downloadable files for this book, and I’ve also included a copy of the novel as a text file so you can try it out for yourself.<a contenteditable="false" data-primary="" data-startref="ch17app" data-type="indexterm" id="id1849"/><a contenteditable="false" data-primary="" data-startref="ch17app2" data-type="indexterm" id="id1850"/></p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch17_summary_1748550058915743">&#13;
        <h1>Summary</h1>&#13;
        <p>In this chapter, we looked at how the open source Ollama tool gives you the ability to wrap an LLM with an easy-to-use API that lets you build applications with it. You saw how to install Ollama and then explored some scenarios with it. You also downloaded and used models like the simple, lightweight Gemma from Google, as well as the powerful, multimodal Llama3.2-vision from Meta. You explored not just chatting with them but also attaching files to upload. Ollama gives you an HTTP endpoint that you saw how to experiment with by using a <code>curl</code> command to simulate HTTP traffic. Finally, you got into writing a prototype of a real-world LLM-based application that analyzed contents of books, first as a simple Python script that proved the concept and then as a more sophisticated web-based application in <code>node.js</code> that used an Ollama backend and a Gemma LLM to do the heavy lifting!</p>&#13;
        <p>In the next chapter, we’ll build on this and explore the concepts of RAG, and you’ll build apps that use local vector databases to enhance the knowledge of LLMs.</p>&#13;
      </div></section>&#13;
    </div></section></body></html>