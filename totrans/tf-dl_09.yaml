- en: Chapter 9\. Training Large Deep Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, you have seen how to train small models that can be completely trained
    on a good laptop computer. All of these models can be run fruitfully on GPU-equipped
    hardware with notable speed boosts (with the notable exception of reinforcement
    learning models for reasons discussed in the previous chapter). However, training
    larger models still requires considerable sophistication. In this chapter, we
    will discuss various types of hardware that can be used to train deep networks,
    including graphics processing units (GPUs), tensor processing units (TPUs), and
    neuromorphic chips. We will also briefly cover the principles of distributed training
    for larger deep learning models. We end the chapter with an in-depth case study,
    adapated from one of the TensorFlow tutorials, demonstrating how to train a CIFAR-10
    convolutional neural network on a server with multiple GPUs. We recommend that
    you attempt to try running this code yourself, but readily acknowledge that gaining
    access to a multi-GPU server is trickier than finding a good laptop. Luckily,
    access to multi-GPU servers on the cloud is becoming possible and is likely the
    best solution for industrial users of TensorFlow seeking to train large models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Custom Hardware for Deep Networks
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you’ve seen throughout the book, deep network training requires chains of
    tensorial operations performed repeatedly on minibatches of data. Tensorial operations
    are commonly transformed into matrix multiplication operations by software, so
    rapid training of deep networks fundamentally depends on the ability to perform
    matrix multiplication operations rapidly. While CPUs are perfectly capable of
    implementing matrix multiplications, the generality of CPU hardware means much
    effort will be wasted on overhead unneeded for mathematical operations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Hardware engineers have noted this fact for years, and there exist a variety
    of alternative hardware for working with deep networks. Such hardware can be broadly
    divided into *inference only* or *training and inference*. Inference-only hardware
    cannot be used to train new deep networks, but can be used to deploy trained models
    in production, allowing for potentially orders-of-magnitude increases in performance.
    Training and inference hardware allows for models to be trained natively. Currently,
    Nvidia’s GPU hardware holds a dominant position in the training and inference
    market due to significant investment in software and outreach by Nvidia’s teams,
    but a number of other competitors are snapping at the GPU’s heels. In this section,
    we will briefly cover some of these newer hardware alternatives. With the exception
    of GPUs and CPUs, most of these alternative forms of hardware are not yet widely
    available, so much of this section is forward looking.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: CPU Training
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although CPU training is by no means state of the art for training deep networks,
    it often does quite well for smaller models (as you’ve seen firsthand in this
    book). For reinforcement learning problems, a multicore CPU machine can even outperform
    GPU training.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: CPUs also see wide usage for inference-only applications of deep networks. Most
    companies have invested heavily in developing cloud servers built primarily on
    Intel server boxes. It’s very likely that the first generation of deep networks
    deployed widely (outside tech companies) will be primarily deployed into production
    on such Intel servers. While such CPU-based deployment isn’t sufficient for heavy-duty
    deployment of learning models, it is often plenty for first customer needs. [Figure 9-1](#ch9-cpu)
    illustrates a standard Intel CPU.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![cpu_pic.jpg](assets/tfdl_0901.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A CPU from Intel. CPUs are still the dominant form of computer
    hardware and are present in all modern laptops, desktops, servers, and phones.
    Most software is written to execute on CPUs. Numerical computations (such as neural
    network training) can be executed on CPUs, but might be slower than on customized
    hardware optimized for numerical methods.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: GPU Training
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPUs were first developed to perform computations needed by the graphics community.
    In a fortuitous coincidence, it turned out that the primitives used to define
    graphics shaders could be repurposed to perform deep learning. At their mathematical
    hearts, both graphics and machine learning rely critically on matrix multiplications.
    Empirically, GPU matrix multiplications offer speedups of an order of magnitude
    or two over CPU implementations. How do GPUs succeed at this feat? The trick is
    that GPUs make use of thousands of identical threads. Clever hackers have succeeded
    in decomposing matrix multiplications into massively parallel operations that
    can offer dramatic speedups. [Figure 9-2](#ch9-gpu) illustrates a GPU architecture.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Although there are a number of GPU vendors, Nvidia currently dominates the GPU
    market. Much of the power of Nvidia’s GPUs stems from its custom library CUDA
    (compute unified device architecture), which offers primitives that make it easier
    to write GPU programs. Nvidia offers a CUDA extension, CUDNN, for speeding up
    deep networks ([Figure 9-2](#ch9-gpu)). TensorFlow has built-in CUDNN support,
    so you can make use of CUDNN to speed up your networks as well through TensorFlow.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![GPU_architecture.jpg](assets/tfdl_0902.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. A GPU architecture from Nvidia. GPUs possess many more cores than
    CPUs and are well suited to performing numerical linear algebra, of the sort useful
    in both graphics and machine learning computations. GPUs have emerged as the dominant
    hardware platform for training deep networks.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How Important Are Transistor Sizes?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For years, the semiconductor industry has tracked progression of chip speeds
    by watching transistor sizes. As transistors got smaller, more of them could be
    packed onto a standard chip, and algorithms could run faster. At the time of writing
    of this book, Intel is currently operating on 10-nanometer transistors, and working
    on transitioning down to 7 nanometers. The rate of shrinkage of transistor sizes
    has slowed significantly in recent years, since formidable heat dissipation issues
    arise at these scales.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia’s GPUs partially buck this trend. They tend to use transistor sizes a
    generation or two behind Intel’s best, and focus on solving architectural and
    software bottlenecks instead of transistor engineering. So far, Nvidia’s strategy
    has paid dividends and the company has achieved market domination in the machine
    learning chip space.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: It’s not yet clear how far architectural and software optimizations can go.
    Will GPU optimizations soon run into the same Moore’s law roadblocks as CPUs?
    Or will clever architectural innovations enable years of faster GPUs? Only time
    can tell.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Processing Units
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tensor processing unit (TPU) is a custom ASIC (application specific integrated
    circuit) designed by Google to speed up deep learning workloads designed in TensorFlow.
    Unlike the GPU, the TPU is stripped down and implements only the bare minimum
    on-die needed to perform necessary matrix multiplications. Unlike the GPU, the
    TPU is dependent on an adjoining CPU to do much of its preprocessing work for
    it. This slimmed-down approach enables the TPU to achieve higher speeds than the
    GPU at lower energy costs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The first version of the TPU only allowed for inference on trained models, but
    the most recent version (TPU2) allows for training of (certain) deep networks
    as well. However, Google has not released many details about the TPU, and access
    is limited to Google collaborators, with plans to enable TPU access via the Google
    cloud. Nvidia is taking notes from the TPU, and it’s quite likely that future
    releases of Nvidia GPUs will come to resemble the TPU, so downstream users will
    likely benefit from Google’s innovations regardless of whether Google or Nvidia
    wins the consumer deep learning market. [Figure 9-3](#ch9-tpu) illustrates the
    TPU architecture design.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![TPU_architecture.jpg](assets/tfdl_0903.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. A tensor processing unit (TPU) architecture from Google. TPUs are
    specialized chips designed by Google to speed up deep learning workloads. The
    TPU is a coprocessor and not a standalone piece of hardware.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What Are ASICs?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both CPUs and GPUs are general-purpose chips. CPUs generally support instruction
    sets in assembly and are designed to be universal. Care is taken to enable a wide
    range of applications. GPUs are less universal, but still allow for a wide range
    of algorithms to be implemented via languages such as CUDA.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Application specific integrated circuits (ASICs) attempt to do away with the
    generality in favor of focusing on the needs of a particular application. Historically,
    ASICs have only achieved limited market penetration. The drumbeat of Moore’s law
    meant that general-purpose CPUs stayed only a breath or two behind custom ASICs,
    so the hardware design overhead was often not worth the effort.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: This state of affairs has started shifting in the last few years. The slowdown
    of transistor shrinkage has expanded ASIC usage. For example, Bitcoin mining depends
    entirely on custom ASICs that implement specialized cryptography operations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Field Programmable Gate Arrays
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Field programmable gate arrays (FPGAs) are a type of “field programmable” ASIC.
    Standard FPGAs can often be reconfigured via hardware description languages such
    as Verilog to implement new ASIC designs dynamically. While FPGAs are generally
    less efficient than custom ASICs, they can offer significant speed improvements
    over CPU implementations. Microsoft in particular has used FPGAs to perform deep
    learning inference and claims to have achieved significant speedups with their
    deployment. However, the approach has not yet caught on widely outside Microsoft.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Neuromorphic Chips
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The “neurons” in deep networks mathematically model the 1940s understanding
    of neuronal biology. Needless to say, biological understanding of neuronal behavior
    has progressed dramatically since then. For one, it’s now known that the nonlinear
    activations used in deep networks aren’t accurate models of neuronal nonlinearity.
    The “spike trains” is a better model (see [Figure 9-4](#ch9-neuromorphic)), where
    neurons activate in short-lived bursts (spikes) but fall to background most of
    the time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![spike_trains.jpg](assets/tfdl_0904.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Neurons often activate in short-lived bursts called spike trains
    (A). Neuromorphic chips attempt to model spiking behavior in computing hardware.
    Biological neurons are complex entities (B), so these models are still only approximate.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hardware engineers have spent significant effort exploring whether it’s possible
    to create chip designs based on spike trains rather than on existing circuit technologies
    (CPUs, GPUs, ASICs). These designers argue that today’s chip designs suffer from
    fundamental power limitations; the brain consumes many orders of magnitude less
    power than computer chips and smart designs should aim to learn from the brain’s
    architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: A number of projects have built large spike train chips attempting to expand
    upon this core thesis. IBM’s TrueNorth project has succeeded in building spike
    train processors with millions of “neurons” and demonstrated that this hardware
    can perform basic image recognition with significantly lower power requirements
    than existing chip designs. However, despite these successes, it is not clear
    how to translate modern deep architectures onto spike train chips. Without the
    ability to “compile” TensorFlow models onto spike train hardware, it’s unlikely
    that such projects will see widespread adoption in the near future.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Deep Network Training
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we surveyed a variety of hardware options for training
    deep networks. However, most organizations will likely only have access to CPUs
    and perhaps GPUs. Luckily, it’s possible to perform *distributed training* of
    deep networks, where multiple CPUs or GPUs are used to train models faster and
    more effectively. [Figure 9-5](#ch9-dist) illustrates the two major paradigms
    for training deep networks with multiple CPUs/GPUs, namely data parallel and model
    parallel training. You will learn about these methods in more detail in the next
    two sections.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![parallelism_modes.jpg](assets/tfdl_0905.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Data parallelism and model parallelism are the two main modes of
    distributed training of deep architectures. Data parallel training splits large
    datasets across multiple computing nodes, while model parallel training splits
    large models across multiple nodes. The next two sections will cover these two
    methods in greater depth.
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data Parallelism
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data parallelism is the most common type of multinode deep network training.
    Data parallel models split large datasets onto different machines. Most nodes
    are workers and have access to a fraction of the total data used to train the
    network. Each worker node has a complete copy of the model being trained. One
    node is designated as the supervisor that gathers updated weights from the workers
    at regular intervals and pushes averaged versions of the weights out to worker
    nodes. Note that you’ve already seen a data parallel example in this book; the
    A3C implementation presented in [Chapter 8](ch08.html#reinforcement_learning)
    is a simple example of data parallel deep network training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: As a historical note, Google’s predecessor to TensorFlow, DistBelief, was based
    on data parallel training on CPU servers. This system was capable of achieving
    distributed CPU speeds (using 32–128 nodes) that matched or exceeded GPU training
    speeds. [Figure 9-6](#ch9-downpourSGD) illustrates the data parallel training
    method implemented by Dist⁠Belief. However, the success of systems like DistBelief
    tends to depend on the presence of high throughput network interconnects that
    can allow for rapid model parameter sharing. Many organizations lack the network
    infrastructure that enables effective multinode data parallel CPU training. However,
    as the A3C example demonstrates, it is possible to perform data parallel training
    on a single node, using different CPU cores. For modern servers, it is also possible
    to perform data parallel training using multiple GPUs stocked within a single
    server, as we will show you later.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![downpour_sgd.png](assets/tfdl_0906.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. The Downpour stochastic gradient descent (SGD) method maintains
    multiple replicas of the model and trains them on different subsets of a dataset.
    The learned weights from these shards are periodically synced to global weights
    stored on a parameter server.
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model Parallelism
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The human brain provides the only known example of a generally intelligent piece
    of hardware, so there have naturally been comparisons drawn between the complexity
    of deep networks and the complexity of the brain. Simple arguments state the brain
    has roughly 100 billion neurons; would constructing deep networks with that many
    “neurons” suffice to achieve general intelligence? Unfortunately, such arguments
    miss the point that biological neurons are significantly more complex than “mathematical
    neurons.” As a result, simple comparisons yield little value. Nonetheless, building
    larger deep networks has been a major research focus over the last few years.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑是唯一已知的智能硬件的例子，因此自然会对深度网络的复杂性和大脑的复杂性进行比较。简单的论点指出，大脑大约有1000亿个神经元；构建具有这么多“神经元”的深度网络是否足以实现普遍智能？不幸的是，这种论点忽略了生物神经元比“数学神经元”复杂得多的事实。因此，简单的比较价值有限。尽管如此，近几年来，构建更大的深度网络一直是主要的研究重点。
- en: The major difficulty with training very large deep networks is that GPUs tend
    to have limited memory (dozens of gigabytes typically). Even with careful encodings,
    neural networks with more than a few hundred million parameters are not feasible
    to train on single GPUs due to memory requirements. Model parallel training algorithms
    attempt to sidestep this limitation by storing large deep networks on the memories
    of multiple GPUs. A few teams have successfully implemented these ideas on arrays
    of GPUs to train deep networks with billions of parameters. Unfortunately, these
    models have not thus far shown performance improvements justifying the extra difficulty.
    For now, it seems that the increase in experimental ease from using smaller models
    outweighs the gains from model parallelism.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非常大的深度网络的主要困难在于GPU的内存通常有限（通常为几十吉字节）。即使进行仔细编码，具有数亿个参数的神经网络也无法在单个GPU上训练，因为内存要求太高。模型并行训练算法试图通过将大型深度网络存储在多个GPU的内存中来规避这一限制。一些团队已成功在GPU阵列上实现了这些想法，以训练具有数十亿参数的深度网络。不幸的是，迄今为止，这些模型尚未显示出通过额外困难来证明性能改进的效果。目前看来，使用较小模型增加实验便利性的好处超过了模型并行ism的收益。
- en: Hardware Memory Interconnects
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件内存互连
- en: Enabling model parallelism requires having very high bandwidth connections between
    compute nodes since each gradient update by necessity requires internode communication.
    Note that while data parallelism requires strong interconnects, sync operations
    need only be performed sporadically after multiple local gradient updates.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 启用模型并行ism需要在计算节点之间具有非常高的带宽连接，因为每次梯度更新都需要节点间通信。请注意，虽然数据并行ism需要强大的互连，但同步操作只需要在多次本地梯度更新后偶尔执行。
- en: A few groups have used InfiniBand interconnects (InfiniBand is a high-throughput,
    low-latency networking standard), or Nvidia’s proprietary NVLINK interconnects
    to attempt to build such large models. However, the results from such experiments
    have been mixed thus far, and the hardware requirements for such systems tend
    to be expensive.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一些团队使用InfiniBand互连（InfiniBand是一种高吞吐量、低延迟的网络标准）或Nvidia的专有NVLink互连来尝试构建这样的大型模型。然而，迄今为止，这些实验的结果并不一致，而且这些系统的硬件要求往往昂贵。
- en: Data Parallel Training with Multiple GPUs on Cifar10
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Cifar10上使用多个GPU进行数据并行训练
- en: In this section, we will give you an in-depth walkthrough of how to train a
    data-parallel convolutional network on the Cifar10 benchmark set. Cifar10 consists
    of 60,000 images of size 32 × 32\. The Cifar10 dataset is often used to benchmark
    convolutional architectures. [Figure 9-7](#ch9-cifar10) displays sample images
    from the Cifar10 dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入介绍如何在Cifar10基准集上训练数据并行卷积网络。Cifar10由尺寸为32×32的60,000张图像组成。Cifar10数据集经常用于评估卷积架构。[图9-7](#ch9-cifar10)显示了Cifar10数据集中的样本图像。
- en: '![cifar10.png](assets/tfdl_0907.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![cifar10.png](assets/tfdl_0907.png)'
- en: Figure 9-7\. The Cifar10 dataset consists of 60,000 images drawn from 10 classes.
    Some sample images from various classes are displayed here.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7。Cifar10数据集包含来自10个类别的60,000张图像。这里显示了各种类别的一些样本图像。
- en: The architecture we will use in this section loads separate copies of the model
    architecture on different GPUs and periodically syncs learned weights across cores,
    as [Figure 9-8](#ch9-cifararch) illustrates.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中将使用的架构在不同的GPU上加载模型架构的单独副本，并定期同步跨核心学习的权重，如[图9-8](#ch9-cifararch)所示。
- en: '![cifar_parallelism.png](assets/tfdl_0908.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![cifar_parallelism.png](assets/tfdl_0908.png)'
- en: Figure 9-8\. The data parallel architecture you will train in this chapter.
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8。本章将训练的数据并行架构。
- en: Downloading and Loading the DATA
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载和加载数据
- en: The `read_cifar10()` method reads and parses the Cifar10 raw data files. [Example 9-1](#ch8-readcifar)
    uses `tf.FixedLengthRecordReader` to read raw data from the Cifar10 files.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_cifar10()`方法读取和解析Cifar10原始数据文件。[示例9-1](#ch8-readcifar)使用`tf.FixedLengthRecordReader`从Cifar10文件中读取原始数据。'
- en: Example 9-1\. This function reads and parses data from Cifar10 raw data files
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-1。此函数从Cifar10原始数据文件中读取和解析数据
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Deep Dive on the Architecture
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨架构
- en: The architecture for the network is a standard multilayer convnet, similar to
    a more complicated version of the LeNet5 architecture you saw in [Chapter 6](ch06.html#convolutional_neural_networks).
    The `inference()` method constructs the architecture ([Example 9-2](#ch8-inference)).
    This convolutional architecture follows a relatively standard architecture, with
    convolutional layers interspersed with local normalization layers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的架构是一个标准的多层卷积网络，类似于您在[第6章](ch06.html#convolutional_neural_networks)中看到的LeNet5架构的更复杂版本。`inference()`方法构建了架构（[示例9-2](#ch8-inference)）。这个卷积架构遵循一个相对标准的架构，其中卷积层与本地归一化层交替出现。
- en: Example 9-2\. This function builds the Cifar10 architecture
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-2。此函数构建Cifar10架构
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Missing Object Orientation?
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺少对象定位？
- en: Contrast the model code presented in this architecture with the policy code
    from the previous architecture. Note how the introduction of the `Layer` object
    allows for dramatically simplified code with concomitant improvements in readability.
    This sharp improvement in readability is part of the reason most developers prefer
    to use an object-oriented overlay on top of TensorFlow in practice.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将此架构中呈现的模型代码与先前架构中的策略代码进行对比。注意介绍`Layer`对象如何使代码大大简化，同时提高可读性。这种明显的可读性改进是大多数开发人员在实践中更喜欢在TensorFlow之上使用面向对象的覆盖的原因之一。
- en: That said, in this chapter, we use raw TensorFlow, since making classes like
    `TensorGraph` work with multiple GPUs would require significant additional overhead.
    In general, raw TensorFlow code offers maximum flexibility, but object orientation
    offers convenience. Pick the abstraction necessary for the problem at hand.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，在本章中，我们使用原始的TensorFlow，因为使类似`TensorGraph`这样的类与多个GPU一起工作将需要额外的开销。一般来说，原始的TensorFlow代码提供了最大的灵活性，但面向对象提供了便利。选择适合手头问题的抽象。
- en: Training on Multiple GPUs
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个GPU上训练
- en: We instantiate a separate version of the model and architecture on each GPU.
    We then use the CPU to average the weights for the separate GPU nodes ([Example 9-3](#ch8-train)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每个GPU上实例化模型和架构的单独版本。然后，我们使用CPU来平均各个GPU节点的权重([示例9-3](#ch8-train))。
- en: Example 9-3\. This function trains the Cifar10 model
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-3。此函数训练Cifar10模型
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code in [Example 9-4](#ch8-gradtower) performs the essential multi-GPU training.
    Note how different batches are dequeued for each GPU, but weight sharing via `tf.get_variable_score().reuse_variables()`
    enables training to happen correctly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例9-4](#ch8-gradtower)中的代码执行了基本的多GPU训练。注意每个GPU为不同批次出队，但通过`tf.get_variable_score().reuse_variables()`实现的权重共享使训练能够正确进行。'
- en: Example 9-4\. This snippet implements multi-GPU training
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-4。此代码片段实现了多GPU训练
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We end by applying the joint training operation and writing summary checkpoints
    as needed in [Example 9-5](#ch8-joint).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过在[示例9-5](#ch8-joint)中需要时应用联合训练操作并编写摘要检查点来结束。
- en: Example 9-5\. This snippet groups updates from the various GPUs and writes summary
    checkpoints as needed
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-5。此代码片段将来自各个GPU的更新分组并根据需要编写摘要检查点。
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Challenge for the Reader
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读者的挑战
- en: You now have all the pieces required to train this model in practice. Try running
    it on a suitable GPU server! You may want to use tools such as `nvidia-smi` to
    ensure that all GPUs are actually being used.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在拥有实践中训练此模型所需的所有要素。尝试在适合的GPU服务器上运行它！您可能需要使用`nvidia-smi`等工具来确保所有GPU实际上都在使用。
- en: Review
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: In this chapter, you learned about various types of hardware commonly used to
    train deep architectures. You also learned about data parallel and model parallel
    designs for training deep architectures on multiple CPUs or GPUs. We ended the
    chapter by walking through a case study on how to implement data parallel training
    of convolutional networks in TensorFlow.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了常用于训练深度架构的各种类型硬件。您还了解了在多个CPU或GPU上训练深度架构的数据并行和模型并行设计。我们通过一个案例研究来结束本章，介绍如何在TensorFlow中实现卷积网络的数据并行训练。
- en: In [Chapter 10](ch10.html#the_future_of_deep_learning), we will discuss the
    future of deep learning and how you can use the skills you’ve learned in this
    book effectively and ethically.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#the_future_of_deep_learning)中，我们将讨论深度学习的未来以及如何有效和道德地运用您在本书中学到的技能。
