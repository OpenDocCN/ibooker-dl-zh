- en: Chapter 9\. Training Large Deep Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, you have seen how to train small models that can be completely trained
    on a good laptop computer. All of these models can be run fruitfully on GPU-equipped
    hardware with notable speed boosts (with the notable exception of reinforcement
    learning models for reasons discussed in the previous chapter). However, training
    larger models still requires considerable sophistication. In this chapter, we
    will discuss various types of hardware that can be used to train deep networks,
    including graphics processing units (GPUs), tensor processing units (TPUs), and
    neuromorphic chips. We will also briefly cover the principles of distributed training
    for larger deep learning models. We end the chapter with an in-depth case study,
    adapated from one of the TensorFlow tutorials, demonstrating how to train a CIFAR-10
    convolutional neural network on a server with multiple GPUs. We recommend that
    you attempt to try running this code yourself, but readily acknowledge that gaining
    access to a multi-GPU server is trickier than finding a good laptop. Luckily,
    access to multi-GPU servers on the cloud is becoming possible and is likely the
    best solution for industrial users of TensorFlow seeking to train large models.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Hardware for Deep Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you’ve seen throughout the book, deep network training requires chains of
    tensorial operations performed repeatedly on minibatches of data. Tensorial operations
    are commonly transformed into matrix multiplication operations by software, so
    rapid training of deep networks fundamentally depends on the ability to perform
    matrix multiplication operations rapidly. While CPUs are perfectly capable of
    implementing matrix multiplications, the generality of CPU hardware means much
    effort will be wasted on overhead unneeded for mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware engineers have noted this fact for years, and there exist a variety
    of alternative hardware for working with deep networks. Such hardware can be broadly
    divided into *inference only* or *training and inference*. Inference-only hardware
    cannot be used to train new deep networks, but can be used to deploy trained models
    in production, allowing for potentially orders-of-magnitude increases in performance.
    Training and inference hardware allows for models to be trained natively. Currently,
    Nvidia’s GPU hardware holds a dominant position in the training and inference
    market due to significant investment in software and outreach by Nvidia’s teams,
    but a number of other competitors are snapping at the GPU’s heels. In this section,
    we will briefly cover some of these newer hardware alternatives. With the exception
    of GPUs and CPUs, most of these alternative forms of hardware are not yet widely
    available, so much of this section is forward looking.
  prefs: []
  type: TYPE_NORMAL
- en: CPU Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although CPU training is by no means state of the art for training deep networks,
    it often does quite well for smaller models (as you’ve seen firsthand in this
    book). For reinforcement learning problems, a multicore CPU machine can even outperform
    GPU training.
  prefs: []
  type: TYPE_NORMAL
- en: CPUs also see wide usage for inference-only applications of deep networks. Most
    companies have invested heavily in developing cloud servers built primarily on
    Intel server boxes. It’s very likely that the first generation of deep networks
    deployed widely (outside tech companies) will be primarily deployed into production
    on such Intel servers. While such CPU-based deployment isn’t sufficient for heavy-duty
    deployment of learning models, it is often plenty for first customer needs. [Figure 9-1](#ch9-cpu)
    illustrates a standard Intel CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![cpu_pic.jpg](assets/tfdl_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A CPU from Intel. CPUs are still the dominant form of computer
    hardware and are present in all modern laptops, desktops, servers, and phones.
    Most software is written to execute on CPUs. Numerical computations (such as neural
    network training) can be executed on CPUs, but might be slower than on customized
    hardware optimized for numerical methods.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: GPU Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPUs were first developed to perform computations needed by the graphics community.
    In a fortuitous coincidence, it turned out that the primitives used to define
    graphics shaders could be repurposed to perform deep learning. At their mathematical
    hearts, both graphics and machine learning rely critically on matrix multiplications.
    Empirically, GPU matrix multiplications offer speedups of an order of magnitude
    or two over CPU implementations. How do GPUs succeed at this feat? The trick is
    that GPUs make use of thousands of identical threads. Clever hackers have succeeded
    in decomposing matrix multiplications into massively parallel operations that
    can offer dramatic speedups. [Figure 9-2](#ch9-gpu) illustrates a GPU architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are a number of GPU vendors, Nvidia currently dominates the GPU
    market. Much of the power of Nvidia’s GPUs stems from its custom library CUDA
    (compute unified device architecture), which offers primitives that make it easier
    to write GPU programs. Nvidia offers a CUDA extension, CUDNN, for speeding up
    deep networks ([Figure 9-2](#ch9-gpu)). TensorFlow has built-in CUDNN support,
    so you can make use of CUDNN to speed up your networks as well through TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: '![GPU_architecture.jpg](assets/tfdl_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. A GPU architecture from Nvidia. GPUs possess many more cores than
    CPUs and are well suited to performing numerical linear algebra, of the sort useful
    in both graphics and machine learning computations. GPUs have emerged as the dominant
    hardware platform for training deep networks.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How Important Are Transistor Sizes?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For years, the semiconductor industry has tracked progression of chip speeds
    by watching transistor sizes. As transistors got smaller, more of them could be
    packed onto a standard chip, and algorithms could run faster. At the time of writing
    of this book, Intel is currently operating on 10-nanometer transistors, and working
    on transitioning down to 7 nanometers. The rate of shrinkage of transistor sizes
    has slowed significantly in recent years, since formidable heat dissipation issues
    arise at these scales.
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia’s GPUs partially buck this trend. They tend to use transistor sizes a
    generation or two behind Intel’s best, and focus on solving architectural and
    software bottlenecks instead of transistor engineering. So far, Nvidia’s strategy
    has paid dividends and the company has achieved market domination in the machine
    learning chip space.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not yet clear how far architectural and software optimizations can go.
    Will GPU optimizations soon run into the same Moore’s law roadblocks as CPUs?
    Or will clever architectural innovations enable years of faster GPUs? Only time
    can tell.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Processing Units
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tensor processing unit (TPU) is a custom ASIC (application specific integrated
    circuit) designed by Google to speed up deep learning workloads designed in TensorFlow.
    Unlike the GPU, the TPU is stripped down and implements only the bare minimum
    on-die needed to perform necessary matrix multiplications. Unlike the GPU, the
    TPU is dependent on an adjoining CPU to do much of its preprocessing work for
    it. This slimmed-down approach enables the TPU to achieve higher speeds than the
    GPU at lower energy costs.
  prefs: []
  type: TYPE_NORMAL
- en: The first version of the TPU only allowed for inference on trained models, but
    the most recent version (TPU2) allows for training of (certain) deep networks
    as well. However, Google has not released many details about the TPU, and access
    is limited to Google collaborators, with plans to enable TPU access via the Google
    cloud. Nvidia is taking notes from the TPU, and it’s quite likely that future
    releases of Nvidia GPUs will come to resemble the TPU, so downstream users will
    likely benefit from Google’s innovations regardless of whether Google or Nvidia
    wins the consumer deep learning market. [Figure 9-3](#ch9-tpu) illustrates the
    TPU architecture design.
  prefs: []
  type: TYPE_NORMAL
- en: '![TPU_architecture.jpg](assets/tfdl_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. A tensor processing unit (TPU) architecture from Google. TPUs are
    specialized chips designed by Google to speed up deep learning workloads. The
    TPU is a coprocessor and not a standalone piece of hardware.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What Are ASICs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both CPUs and GPUs are general-purpose chips. CPUs generally support instruction
    sets in assembly and are designed to be universal. Care is taken to enable a wide
    range of applications. GPUs are less universal, but still allow for a wide range
    of algorithms to be implemented via languages such as CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: Application specific integrated circuits (ASICs) attempt to do away with the
    generality in favor of focusing on the needs of a particular application. Historically,
    ASICs have only achieved limited market penetration. The drumbeat of Moore’s law
    meant that general-purpose CPUs stayed only a breath or two behind custom ASICs,
    so the hardware design overhead was often not worth the effort.
  prefs: []
  type: TYPE_NORMAL
- en: This state of affairs has started shifting in the last few years. The slowdown
    of transistor shrinkage has expanded ASIC usage. For example, Bitcoin mining depends
    entirely on custom ASICs that implement specialized cryptography operations.
  prefs: []
  type: TYPE_NORMAL
- en: Field Programmable Gate Arrays
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Field programmable gate arrays (FPGAs) are a type of “field programmable” ASIC.
    Standard FPGAs can often be reconfigured via hardware description languages such
    as Verilog to implement new ASIC designs dynamically. While FPGAs are generally
    less efficient than custom ASICs, they can offer significant speed improvements
    over CPU implementations. Microsoft in particular has used FPGAs to perform deep
    learning inference and claims to have achieved significant speedups with their
    deployment. However, the approach has not yet caught on widely outside Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: Neuromorphic Chips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The “neurons” in deep networks mathematically model the 1940s understanding
    of neuronal biology. Needless to say, biological understanding of neuronal behavior
    has progressed dramatically since then. For one, it’s now known that the nonlinear
    activations used in deep networks aren’t accurate models of neuronal nonlinearity.
    The “spike trains” is a better model (see [Figure 9-4](#ch9-neuromorphic)), where
    neurons activate in short-lived bursts (spikes) but fall to background most of
    the time.
  prefs: []
  type: TYPE_NORMAL
- en: '![spike_trains.jpg](assets/tfdl_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Neurons often activate in short-lived bursts called spike trains
    (A). Neuromorphic chips attempt to model spiking behavior in computing hardware.
    Biological neurons are complex entities (B), so these models are still only approximate.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hardware engineers have spent significant effort exploring whether it’s possible
    to create chip designs based on spike trains rather than on existing circuit technologies
    (CPUs, GPUs, ASICs). These designers argue that today’s chip designs suffer from
    fundamental power limitations; the brain consumes many orders of magnitude less
    power than computer chips and smart designs should aim to learn from the brain’s
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: A number of projects have built large spike train chips attempting to expand
    upon this core thesis. IBM’s TrueNorth project has succeeded in building spike
    train processors with millions of “neurons” and demonstrated that this hardware
    can perform basic image recognition with significantly lower power requirements
    than existing chip designs. However, despite these successes, it is not clear
    how to translate modern deep architectures onto spike train chips. Without the
    ability to “compile” TensorFlow models onto spike train hardware, it’s unlikely
    that such projects will see widespread adoption in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Deep Network Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we surveyed a variety of hardware options for training
    deep networks. However, most organizations will likely only have access to CPUs
    and perhaps GPUs. Luckily, it’s possible to perform *distributed training* of
    deep networks, where multiple CPUs or GPUs are used to train models faster and
    more effectively. [Figure 9-5](#ch9-dist) illustrates the two major paradigms
    for training deep networks with multiple CPUs/GPUs, namely data parallel and model
    parallel training. You will learn about these methods in more detail in the next
    two sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![parallelism_modes.jpg](assets/tfdl_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Data parallelism and model parallelism are the two main modes of
    distributed training of deep architectures. Data parallel training splits large
    datasets across multiple computing nodes, while model parallel training splits
    large models across multiple nodes. The next two sections will cover these two
    methods in greater depth.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data Parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data parallelism is the most common type of multinode deep network training.
    Data parallel models split large datasets onto different machines. Most nodes
    are workers and have access to a fraction of the total data used to train the
    network. Each worker node has a complete copy of the model being trained. One
    node is designated as the supervisor that gathers updated weights from the workers
    at regular intervals and pushes averaged versions of the weights out to worker
    nodes. Note that you’ve already seen a data parallel example in this book; the
    A3C implementation presented in [Chapter 8](ch08.html#reinforcement_learning)
    is a simple example of data parallel deep network training.
  prefs: []
  type: TYPE_NORMAL
- en: As a historical note, Google’s predecessor to TensorFlow, DistBelief, was based
    on data parallel training on CPU servers. This system was capable of achieving
    distributed CPU speeds (using 32–128 nodes) that matched or exceeded GPU training
    speeds. [Figure 9-6](#ch9-downpourSGD) illustrates the data parallel training
    method implemented by Dist⁠Belief. However, the success of systems like DistBelief
    tends to depend on the presence of high throughput network interconnects that
    can allow for rapid model parameter sharing. Many organizations lack the network
    infrastructure that enables effective multinode data parallel CPU training. However,
    as the A3C example demonstrates, it is possible to perform data parallel training
    on a single node, using different CPU cores. For modern servers, it is also possible
    to perform data parallel training using multiple GPUs stocked within a single
    server, as we will show you later.
  prefs: []
  type: TYPE_NORMAL
- en: '![downpour_sgd.png](assets/tfdl_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. The Downpour stochastic gradient descent (SGD) method maintains
    multiple replicas of the model and trains them on different subsets of a dataset.
    The learned weights from these shards are periodically synced to global weights
    stored on a parameter server.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model Parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The human brain provides the only known example of a generally intelligent piece
    of hardware, so there have naturally been comparisons drawn between the complexity
    of deep networks and the complexity of the brain. Simple arguments state the brain
    has roughly 100 billion neurons; would constructing deep networks with that many
    “neurons” suffice to achieve general intelligence? Unfortunately, such arguments
    miss the point that biological neurons are significantly more complex than “mathematical
    neurons.” As a result, simple comparisons yield little value. Nonetheless, building
    larger deep networks has been a major research focus over the last few years.
  prefs: []
  type: TYPE_NORMAL
- en: The major difficulty with training very large deep networks is that GPUs tend
    to have limited memory (dozens of gigabytes typically). Even with careful encodings,
    neural networks with more than a few hundred million parameters are not feasible
    to train on single GPUs due to memory requirements. Model parallel training algorithms
    attempt to sidestep this limitation by storing large deep networks on the memories
    of multiple GPUs. A few teams have successfully implemented these ideas on arrays
    of GPUs to train deep networks with billions of parameters. Unfortunately, these
    models have not thus far shown performance improvements justifying the extra difficulty.
    For now, it seems that the increase in experimental ease from using smaller models
    outweighs the gains from model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Memory Interconnects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enabling model parallelism requires having very high bandwidth connections between
    compute nodes since each gradient update by necessity requires internode communication.
    Note that while data parallelism requires strong interconnects, sync operations
    need only be performed sporadically after multiple local gradient updates.
  prefs: []
  type: TYPE_NORMAL
- en: A few groups have used InfiniBand interconnects (InfiniBand is a high-throughput,
    low-latency networking standard), or Nvidia’s proprietary NVLINK interconnects
    to attempt to build such large models. However, the results from such experiments
    have been mixed thus far, and the hardware requirements for such systems tend
    to be expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Data Parallel Training with Multiple GPUs on Cifar10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will give you an in-depth walkthrough of how to train a
    data-parallel convolutional network on the Cifar10 benchmark set. Cifar10 consists
    of 60,000 images of size 32 × 32\. The Cifar10 dataset is often used to benchmark
    convolutional architectures. [Figure 9-7](#ch9-cifar10) displays sample images
    from the Cifar10 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![cifar10.png](assets/tfdl_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. The Cifar10 dataset consists of 60,000 images drawn from 10 classes.
    Some sample images from various classes are displayed here.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The architecture we will use in this section loads separate copies of the model
    architecture on different GPUs and periodically syncs learned weights across cores,
    as [Figure 9-8](#ch9-cifararch) illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '![cifar_parallelism.png](assets/tfdl_0908.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-8\. The data parallel architecture you will train in this chapter.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Downloading and Loading the DATA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `read_cifar10()` method reads and parses the Cifar10 raw data files. [Example 9-1](#ch8-readcifar)
    uses `tf.FixedLengthRecordReader` to read raw data from the Cifar10 files.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-1\. This function reads and parses data from Cifar10 raw data files
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Deep Dive on the Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture for the network is a standard multilayer convnet, similar to
    a more complicated version of the LeNet5 architecture you saw in [Chapter 6](ch06.html#convolutional_neural_networks).
    The `inference()` method constructs the architecture ([Example 9-2](#ch8-inference)).
    This convolutional architecture follows a relatively standard architecture, with
    convolutional layers interspersed with local normalization layers.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-2\. This function builds the Cifar10 architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Missing Object Orientation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrast the model code presented in this architecture with the policy code
    from the previous architecture. Note how the introduction of the `Layer` object
    allows for dramatically simplified code with concomitant improvements in readability.
    This sharp improvement in readability is part of the reason most developers prefer
    to use an object-oriented overlay on top of TensorFlow in practice.
  prefs: []
  type: TYPE_NORMAL
- en: That said, in this chapter, we use raw TensorFlow, since making classes like
    `TensorGraph` work with multiple GPUs would require significant additional overhead.
    In general, raw TensorFlow code offers maximum flexibility, but object orientation
    offers convenience. Pick the abstraction necessary for the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Training on Multiple GPUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We instantiate a separate version of the model and architecture on each GPU.
    We then use the CPU to average the weights for the separate GPU nodes ([Example 9-3](#ch8-train)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-3\. This function trains the Cifar10 model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The code in [Example 9-4](#ch8-gradtower) performs the essential multi-GPU training.
    Note how different batches are dequeued for each GPU, but weight sharing via `tf.get_variable_score().reuse_variables()`
    enables training to happen correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-4\. This snippet implements multi-GPU training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We end by applying the joint training operation and writing summary checkpoints
    as needed in [Example 9-5](#ch8-joint).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-5\. This snippet groups updates from the various GPUs and writes summary
    checkpoints as needed
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Challenge for the Reader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You now have all the pieces required to train this model in practice. Try running
    it on a suitable GPU server! You may want to use tools such as `nvidia-smi` to
    ensure that all GPUs are actually being used.
  prefs: []
  type: TYPE_NORMAL
- en: Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about various types of hardware commonly used to
    train deep architectures. You also learned about data parallel and model parallel
    designs for training deep architectures on multiple CPUs or GPUs. We ended the
    chapter by walking through a case study on how to implement data parallel training
    of convolutional networks in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 10](ch10.html#the_future_of_deep_learning), we will discuss the
    future of deep learning and how you can use the skills you’ve learned in this
    book effectively and ethically.
  prefs: []
  type: TYPE_NORMAL
