["```py\npip install langchain==0.1.13\n```", "```py\npip install langchain-openai==0.1.1\n```", "```py\nfrom langchain_core.prompts.chat import ChatPromptTemplate\nprompt = ChatPromptTemplate.from_template(\n    '{text}\\n'                         #1\n    'Is the sentiment positive or negative?\\n'\n    'Answer (\"Positive\"/\"Negative\")\\n')\n```", "```py\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(\n    model='gpt-4o', temperature=0, \n    max_tokens=1)\n```", "```py\nfrom langchain_core.output_parsers.string import StrOutputParser\nparser = StrOutputParser()\n```", "```py\nfrom langchain_core.runnables.passthrough import RunnablePassthrough\nchain = ({'text':RunnablePassthrough()} | prompt | llm | parser)\n```", "```py`At the start of the chain, we mark this parameter as `RunnablePassthrough`. This gives us a lot of flexibility in terms of how we pass inputs to the chain. For instance, the following code illustrates how to process a list of inputs using the previously created chain:    ```", "```py    ### 10.2.3 Putting it together    Time to finalize our code for text classification! The code in listing [10.1](#code__LangChainClassification) takes as input a path to a .csv file containing a `text` column. Executing the code generates a result file containing an additional column called `class` with the classification result. In other words, the code does exactly the same thing as that from chapter 4, but this time using LangChain.    ##### Listing 10.1 Sentiment classification using LangChain    ```", "```py  #1 Creates a chain #2 Creates a prompt template #3 Creates an LLM object #4 Creates an output parser #5 Creates a chain #6 Reads the data #7 Creates a chain #8 Uses it #9 Stores the output   The `create_chain` function (**1**) implements the steps discussed in the last section. It generates a prompt template for classification (**2**), then a chat model (**3**), and finally an output parser (**4**). The result is a chain connecting all those components (**5**).    After reading the command-line parameters, the code reads the input data (**6**), creates a corresponding chain (**7**), and finally applies the chain to the list of input texts (**8**). The classification results are added to the input data and stored on disk (**9**).    ### 10.2.4 Trying it out    Time to try it! As usual, you will find the code for listing [10.1](#code__LangChainClassification) on the book’s companion website in the chapter 10 section. Download the code (the listing1.py file) and, optionally, a file containing reviews to classify (such as reviews.csv from chapter 4). Open the terminal, and switch to the folder containing the code. Assuming that reviews.csv is located in the same folder, run the following command:    ```", "```py    Check the folder containing the code. You should see a new file, result.csv, with the desired classification results. So far, we have only verified that we can do the same things using LangChain that we can do with OpenAI’s libraries directly (even though, arguably, the LangChain code is cleaner). In the next section, we’ll see that LangChain enables us to do much more than that.    ## 10.3 Agents: Putting the large language model into the driver’s seat    So far, you may have considered language models as (highly sophisticated) tools. Based on your input, the language model produces output. If data processing requires more than the language model can accomplish, it is up to you, the developer, to add the necessary infrastructure. For instance, assume that you’re building a question-answering system for math questions. Realizing that language models are bad at calculating things (which, ironically for a computer program, they are), you may consider the following approach: based on the user question, the language model translates the input into a mathematical formula. Then that formula is parsed and evaluated by a simple calculator tool. The output of that tool is sent to the user.    So far, so good. It gets more complicated in situations where you have not one but multiple math tools. Perhaps one tool solves differential equations, and another evaluates simple arithmetic equations. In such cases, you can expand your approach with a classification stage, mapping the user input to the most suitable tool. However, this approach breaks down in situations where answering the user question may require not applying a single tool but multiple invocations of different tools, possibly using the output of one tool as input for the next invocations. In such cases, manually covering each possible sequence of required tool invocations is simply not feasible.    This is the type of use case where *agents* become useful. Agents are a fairly novel way of using large language models. At the core of this approach is a change in perspective. Instead of considering the language model a tool used as a step within a pipeline designed by the developer, we make the language model an independent agent. Rather than trying to orchestrate the order in which the language model and other processing steps are applied (which we did in the last section), we leave it up to the language model to decide which processing steps are applied in which order. The advantage of this approach is that it is much more flexible, freeing us as developers from having to foresee each possible development in advance to create an associated branch in our processing logic.    Agents can be useful for complex data-analysis tasks where it is unclear, a priori, which data sources or processing methods may be required to satisfy a user’s request. Two terms are central to the agent approach, and we will look at them next: the *agent* and its *tools*.    Let’s start by discussing tools. A tool can encapsulate arbitrary functionality. It is a function that the language model can use if it deems it necessary. When we use LangChain or similar frameworks to implement agents, a tool is typically implemented as a Python function. Each tool must be associated with a description in natural language. This description is shown to the language model as part of the prompts. Based on this description, the language model can decide whether a tool seems helpful in a given context. To use a tool, the language model requires a description of the input parameters and the output semantics. Similar to human programmers, choosing meaningful parameter names and writing precise documentation helps language models use tools effectively. Because agents are implemented via language models, a full description of all available tools is typically provided as part of the input prompt.    Agents use tools whenever they are required to solve a complex task specified by the user. Agents are implemented via language models. Although fine-tuning can improve the performance of language models as agents, generic models should work in principle. The secret behind turning language models into agents lies less within the model itself but rather in the way it is prompted. At a minimum, corresponding prompts integrate the following components:    *   A description of a high-level task the agent should solve. This description is provided by the user. *   A list of available tools, together with a description of their functionality and their input and output parameters. *   A description of the expected output format. This enables mapping the output of the language model to tool invocations.    Given such a prompt, the language model can produce output requesting specific tool invocations. The infrastructure implementing the agent approach parses the output, maps it to corresponding tools and input parameter values, and obtains the invocation result. In the next iteration, the result of the tool invocation is added to the input prompt. In this way, the language model can essentially *access* the results of tool invocations. Based on that, the language model can choose to apply more tools (possibly using the results of prior tool invocations as inputs) or terminate if a final answer is available.    Figure [10.2](#fig__AgentOverview) summarizes this process. The user-specified task, together with a detailed description of all tools, forms the input to the language model. The output of the language model is parsed and mapped to an action. Either this action represents the invocation of a tool (in that case, the invocation command contains values for all input parameters of the tool), or it represents termination (in this case, the termination command contains what the language model believes is an answer to the input task). If the action is a tool invocation, the corresponding call is executed. The result is added to the prompt used in the next iteration. Iterations continue until the language model decides to terminate (or until a user-specified limit on the number of iterations is reached).  ![figure](../Images/CH10_F02_Trummer.png)  ##### Figure 10.2 Using language models as agents. Given a prompt describing the task and available tools, the language model decides on termination and tool invocations. Results of tool invocations are added to the prompt used for the next iteration.    At this point, you may be curious what the corresponding prompts look like. Let’s examine the standard prompt template used for agents in LangChain. You can download the prompt template from LangChain’s hub. If you want to do so, install the hub first using the following command in the terminal:    ```", "```py    Then run the following code in Python to print out the standard template for one of the most popular agent types:    ```", "```py    You should see the following output:    ```", "```py  #1 General scenario #2 Tool descriptions #3 Format description #4 User input #5 Prior results   This prompt template describes the general scenario (**1**) (there is a question that needs answering), available tools (**2**), and the process to solve the task (**3**). The prompt template contains multiple placeholders representing tool descriptions (**2**), the input from the user (**4**), and the results of prior iterations (**5**). As we will see in the following sections, LangChain offers various convenience functions to create and execute agents based on this and similar prompt templates.    ## 10.4 Building an agent for data analysis    In this section, we will use LangChain to build an agent for data analysis. This agent will be able to access different data sources with structured and unstructured data. What’s more, the agent will decide which of those sources to access and in which order. It may even use information obtained from one source to query a second source (e.g., to access a structured database about video game sales to identify the most sold game in a specific year and then use the game title to query the web for further information).    ### 10.4.1 Overview    Our data-analysis agent implements the approach we discussed in the previous section. It uses a language model to decide which tools to invoke in which order and with what input parameters. In our example scenario, we will provide the agent with tools to access a relational database (as well as obtain information about its structure, such as the names of available tables). We also provide the agent with a tool that enables web search (exploiting existing search engines in the background). Taken together, we get an agent that can query a relational database and use the web to obtain information that relates to the database content.    Let’s start our discussion with a more detailed description of the tools we will provide to the agent. In total, the agent will have access to the following five tools:    *   `sql_db_list_tables` lists all tables in the relational database. *   `sql_db_schema` returns the SQL schema of a table, given the table name. *   `sql_db_query_checker` enables the agent to validate an SQL query. *   `sql_db_query` evaluates an SQL query and returns the query result. *   `search` enables the agent to search the web via keywords, returning web text.    The first four tools help the agent access a relational database. The last tool enables the agent to retrieve information from the web. Given a user-specified task, the agent decides (using the underlying language model) which of these tools to invoke and in which order. Figure [10.3](#fig__DataAgent) illustrates this scenario.  ![figure](../Images/CH10_F03_Trummer.png)  ##### Figure 10.3 The data agent uses multiple tools to explore the structure and query a relational data- base. In addition, the agent can retrieve web text via the web search tool.    ### 10.4.2 Creating an agent with LangChain    Creating an agent with LangChain is fast! LangChain even offers specialized constructors for agents that access a structured database. We will use those features in the following code.    Agents are implemented via language models. To create an agent, we first have to create a language model object:    ```", "```py    We’re creating an OpenAI language model of type chat. More precisely, we refer to the GPT-4o model again.    Next, we create an object representing our relational database. We will query an SQLite database stored on disk. Assume that `dbpath` stores the path of the corresponding database file (typically, such files have the .db suffix). We can create a database object using the following code:    ```", "```py    We mentioned four tools for accessing the relational database. Fortunately, all of these tools will be automatically created from the database object. However, we still need to create a tool for web search.    We will use a built-in component of LangChain, the SerpAPI tool. To use this tool, you first need to register for an account on the SerpAPI website. Open your browser, go to [https://serpapi.com/](https://serpapi.com/), click the Register button, and create a corresponding account. To execute the code presented next, you will need to retrieve your API access key (available at [https://serpapi.com/dashboard](https://serpapi.com/dashboard)). You also need to install a LangChain extension to enable the web search tool. Go to the terminal, and run the following command:    ```", "```py    After that, all it takes is the following snippet of Python code to generate a tool for web search (assuming that `llm` contains the previously created language model object and `serpaikey` the SerpAPI access key):    ```", "```py    The `load_tools` function is used for standard tools by passing the names of the desired tools as parameters. In this case, we only need the web search tool, and we pass only a single entry in the list of tool names (`serpapi`). After the call to `load_tools`, we store the result in `extra_tools`: a list of tools with a single entry (the web search tool). We now have all the components we need to create an agent using LangChain.    Assume that `db` contains the database object, created previously, and `llm` the language model generated before. We initialize an agent for SQL-based data access using the following code:    ```", "```py    The `create_sql_agent` command is a convenience function offered by LangChain to create agents for SQL-based data access. The four previously mentioned tools for relational database access (useful for retrieving table names, showing table schemata, validating SQL queries, and, ultimately, issuing them) are added automatically without us having to add them explicitly. There is only one more tool we want in addition to the SQL-focused tools: the web search capability. Such tools are specified in a list via the `extra_tools` input parameter. Setting the `verbose` flag to `True` enables us to follow the “thought process” leading the agent to call specific tools (we will see some example output later). The agent type, `openai-tools` in this case, determines the precise prompt to use as well as which parsers to use to map the output of the language model to tool invocations.    After creating the agent, we use the following code to apply the agent to a specific task (we assume that the variable `task` stores a natural language description of the task we want to solve):    ```", "```py    ### 10.4.3 Complete code for data-analysis agent    Listing [10.2](#code__DataAgent) brings all of this together: after reading the SerpAPI API access key, as well as the path to the database file and a question from the command line, it creates a language model object (**1**), then a database (**2**), the web search tool (**3**), and, finally, the agent (**4**). It invokes the agent (**5**) on the input question. The output produced by the agent terminates with an answer to that question (or with a failure message if the agent is unable to find an answer).    ##### Listing 10.2 Agent for data analysis with web search capability    ```", "```py  #1 Creates an LLM client #2 Creates a database object #3 Adds a web search tool #4 Creates the agent #5 Invokes the agent with input   ### 10.4.4 Trying it out    Let’s see how that works in practice! Download the code for listing [10.2](#code__DataAgent) from the book’s companion website. Besides the code, you will need an SQLite database to try the data agent. We will use the SQLite database from chapter 5, storing information about video games (you can find the corresponding file on the book’s companion website under the Games SQLite link).    Open the terminal, and switch to the directory containing the code. We will assume that the database file, games.db, is located in the same directory. Run the following code (replace `[SerpAPI key]` with your search key, available at [https://](https://serpapi.com/dashboard) [serpapi.com/dashboard](https://serpapi.com/dashboard)):    ```", "```py    You should see output like the following (the output you see may differ slightly due to changing web content, small changes to the GPT-4o model, and a few other factors):    ```", "```py  #1 The agent retrieves the list of tables. #2 The agent retrieves the table schema. #3 The agent verifies the SQL query. #4 The agent queries for the top game. #5 The agent searches the web for FIFA 17. #6 The agent formulates the final answer.   Remember that we switched the agent’s output to verbose mode. That means the output contains a full log of tools invoked by the agent, as well as the agent’s reasoning process. Let’s take a closer look at the output to see what happened.    First, the agent retrieves a list of the tables available in the relational database (using `sql_db_list_tables`) (**1**). Clearly, that’s a reasonable step when confronted with a new database. The result of the tool invocation reveals that the database contains only a single table (called `games`). The agent becomes “curious” about the table contents. It invokes the `sql_db_schema` tool to get further information about the `games` table (**2**). Note that this tool consumes input parameters, specifically the name of the table to investigate. The log shows the values of all input parameters for each tool invocation.    The invocation of the `sql_db_schema` tool returns the SQL command that was used to create the `games` table, together with a small sample of the table’s content. Next, the agent considers an SQL query to retrieve relevant information about the input question (“What was the most sold game in 2016 and how is it played?”). In the first step, it validates that the following query is syntactically correct by invoking the `sql_db_query_checker` tool (**3**):    ```", "```py    At the same time, the agent uses the opportunity to “reflect” on the usefulness of the query under consideration, as evidenced by the output “The games table contains the information we need. I will query for the game with the highest global sales in 2016.” It may seem strange that a language model can benefit from this type of monologue instead of writing out tool invocations directly. Yet it has been shown that enabling agents to explicitly reason about the problem at hand and the steps they are taking to solve it can improve their performance [1]. That’s what’s happening here as well.    Finally, the agent decides to use the previously validated query to retrieve information from the database, using the `sql_db_query` tool (**4**). The SQL query returns the game that generated the most revenue in 2016: FIFA 17, a soccer simulation produced by Electronic Arts. But the input question asks for more than that: “What was the most sold game in 2016 and *how is it played*?” The second part of the question cannot be answered from database content. To its credit, the agent realizes that and tries to access the web instead: it issues a web search request using the `Search` tool for the search string “How to play FIFA 17” (**5**). Note that the agent was able to automatically formulate a suitable search string from the result of the SQL query and the input question. The result of the web search is a collection of text snippets (shown in the output) that contain information about how to play FIFA 2017.    Finally, the agent uses the information returned by the web search (in combination with information from the SQL database) to formulate a final answer (**6**). The final answer identifies FIFA 17 as the most popular game in 2016 and contains detailed instructions for how to play it well. We have seen that the agent can perform a complex sequence of tool invocations to find the desired answer without having to specify the process to follow by hand. If you’re interested, try querying the agent with a few more, possibly more complicated, questions and see whether it can answer them as well.    ## 10.5 Adding custom tools    So far, we have used standard tools offered by LangChain for the most common use cases. What happens if we have specialized requirements? For example, say you want to make a data source accessible via a custom API, or you have specialized analysis functions that an agent can apply to your data. In those cases, you can define your own custom tools and make them accessible to a LangChain agent.    ### 10.5.1 The currency converter    In the last section, we analyzed a data set about video game sales. The original data reports sales values in US dollars. What about other currencies? To enable agents to reason about game sales using multiple currencies, we will add a currency-converter tool. Given an amount in US dollars as input, together with the name of a target currency, this tool returns the equivalent value in the target currency.    Listing [10.3](#code__CustomAgent) shows how to add the currency-converter tool to our data agent. At its core, a tool is nothing but a Python function. Our currency converter is implemented by the `convert_currency` function (**2**). How does LangChain know that we want to turn the function into a tool? That’s done by the `@tool` decorator (**1**), which needs to directly precede the function name. Typically, we do not have to specify types for parameters and return values of Python functions (even though it does improve the readability of your code). If you plan to turn a function into a tool, however, you should specify all these types. The reason is as follows: to use your function properly as a tool, the agent needs to invoke it with parameters of the right type. All types you specify in the function header will be made accessible to the agent as part of the description of your tool. Hence, associating parameters with types helps your agent avoid unnecessary invocation errors.    Besides parameter types, the agent should know a little about what your tool can accomplish. The first important piece of information is the name of your function. By default, your tool will be named after your function. Don’t call your function `XYZ`, because that will make it very hard to understand what’s going on! The name of the function in listing [10.3](#code__CustomAgent), `convert_currency`, should make it pretty clear what the function does. Similarly, the names of the input parameters, `USD_amount` (of type `float`) and `currency` (of type `str`), are pretty self-explanatory (which is good!). The function output is a converted amount in the target currency or an error message if the requested target currency is not supported (that’s why the output type is a `Union` of string and float values). As a rule of thumb, if you plan to use a Python function as a tool, write it the same way you would to enable human coders to understand your function without reading its code in detail.    In addition to the names of the function and its parameters, the agent “sees” the function documentation (**3**). Again, make sure your documentation is well structured, and explain the semantics of your tool and associated parameters. In this case, the documentation describes the function of the tool, the semantics of the input parameters (even with an example of an admissible value for the second parameter), and the output semantics.    The `convert_currency` function uses a small database of currencies with associated conversion factors. For instance, it contains conversion factors for euros and yen but not many other currencies. If you’re creating a tool for your agent, take into account cases in which the agent does not use the tool properly. This may happen if the tool description is incomplete or if the language model makes a mistake (which happens even to state-of-the-art language models). In this case, we’re adding specialized handling for the case that the target currency is not supported (i.e., a corresponding conversion factor is missing) (**4**). If the target currency is not supported, the function returns a helpful error message that contains the full set of supported currencies. This helps the agent to restrict the parameter to the set of admissible options for the following invocations. If the target currency is supported, the function returns the converted amount (**5**).    ##### Listing 10.3 Data-analysis agent with currency-converter tool    ```", "```py  #1 Turns the function into a tool #2 Function signature with types #3 Function documentation #4 Helpful error message for the agent #5 Converts and returns the result #6 Adds the currency-converter tool   After creating a tool based on a Python function, we just need to make the tool available to our agent. Listing [10.3](#code__CustomAgent) creates almost the same agent as listing [10.2](#code__DataAgent), with the only difference being that we add the currency-converter tool (**6**). Because we’re using the SQL agent again, the converter tool and the web search tool are inserted into the list of extra tools (added on top of the standard tools for SQL access that are automatically provided to the agent). By default, the tool name equals the name of the function it is based on. Hence, we’re simply adding `convert_currency` to the list of extra tools (**6**) to enhance the agent with currency conversion abilities.    ### 10.5.2 Trying it out    Let’s see whether our agent is able to use our newly added tool! Download the code for listing [10.3](#code__CustomAgent) from the book’s companion website. You can use the same database file as before (and assume that the games.db file is located in the same folder as the code). Then, open the terminal and execute the following code (substituting your SerpAPI access key for `[SerpAPI key]`):    ```", "```py    Clearly, answering that question requires the currency-converter tool. When running the code, you will see output like the following:    ```", "```py  #1 The agent retrieves the database tables. #2 The agent queries for the table schema. #3 The agent verifies the SQL query. #4 The agent queries for sales in 2015. #5 The agent converts the currencies. #6 The agent formulates the final answer.   Similarly to before, the agent first explores the database by retrieving the set of tables (**1**) and then, after finding out that the database only contains a single table, retrieving the schema for that table (**2**). Correctly, the agent infers that the database contains useful information about the input question and first validates (**3**) and then executes (**4**) a corresponding SQL query. The SQL query returns the total value of computer game sales in 2015, expressed in US dollars. To answer the final part of the question (“How much is it in Yen?”), the agent then applies the currency-converter tool (**5**). Note that the agent chooses appropriate values for the two input parameters based on the function description and types. Finally, the agent formulates the answer to the input question (**6**).    ## 10.6 Indexing multimodal data with LlamaIndex    LangChain is by no means the only framework that makes it easier to use language models for data analysis. In this section, we discuss another framework that has recently appeared and is quickly gaining popularity: LlamaIndex.    ### 10.6.1 Overview    LlamaIndex shines for use cases where language models need to access large collections of data, possibly integrating various data types. In such cases, it is generally not advisable (or even possible) to directly feed all the data into the language model. Instead, we need a mechanism that quickly identifies relevant data for a given task, passing only relevant data to the language model. As the name suggests, LlamaIndex indexes data to quickly identify relevant subsets. More precisely, LlamaIndex associates pieces of data (e.g., chunks of text) with embedding vectors. We briefly discussed embedding vectors in chapter 4\\. In short, an embedding vector represents the semantics of text as a vector calculated by a language model. If two documents have similar embedding vectors (the distance between the vectors is small), we assume that they discuss similar topics.    A typical LlamaIndex data-processing pipeline entails the following steps. First, it loads data, possibly in various formats, and performs preprocessing. For example, preprocessing may entail dividing long text documents into smaller chunks that are more convenient to handle. Next, LlamaIndex indexes the data. As discussed before, this means associating data chunks with embedding vectors. By default, LlamaIndex uses fairly small language models (e.g., OpenAI’s ada models) to calculate embedding vectors. This makes the indexing step cheap. Furthermore, LlamaIndex can store the generated index (the embedding vectors) on disk to avoid having to regenerate them for each new task.    LlamaIndex offers support for various use cases based on the generated index. For instance, it can use indexed data to answer natural language questions. Given a question as input, it first calculates an embedding vector for the question text. Then it compares the vector representing the question to precalculated vectors representing data chunks. It identifies the data items with the most similar vectors. The associated data is included in the prompt, together with the input question. The goal is to generate an answer to the question, exploiting relevant data as context. Whereas small models are used for indexing, we typically use larger models to generate the final reply. Figure [10.4](#fig__LlamaIndex) illustrates this data-processing pipeline.  ![figure](../Images/CH10_F04_Trummer.png)  ##### Figure 10.4 Primary steps of a typical LlamaIndex data-processing pipeline. LlamaIndex loads and indexes data to enable fast retrieval. Given a question, LlamaIndex identifies relevant data items and submits them, together with the input question, to a language model to generate an answer.    ### 10.6.2 Installing LlamaIndex    Let’s implement the pipeline from the last section in Python. To use LlamaIndex, we first have to install a few packages. Go to your terminal, and run the following command:    ```", "```py    That will set you up with LlamaIndex’s core packages. However, you will use LlamaIndex to analyze a diverse collection of data formats. To enable LlamaIndex to properly access and parse all of them, you need to install a few additional packages. Run the following commands in your terminal:    ```", "```py    These libraries are necessary to analyze .pdf documents and PowerPoint presentations, all of which we will need for the following project.    ### 10.6.3 Implementing a simple question-answering system    You’re back at Banana and confronted with a challenging problem: being a global company, Banana has many different units. Your boss wants you to analyze data from different units, for example, to compare their performance. However, different units have widely varying preferences in terms of data formats. Some units publish their results as simple text documents, whereas others regularly turn out elaborate PowerPoint presentations. How do you integrate all those different data formats? Fortunately, LlamaIndex makes that easy.    Look at listing [10.4](#code__LlamaIndex): in just a few lines of Python code, it handles the task. The code accepts the following input parameters:    *   A link to a data repository. This repository may contain files of various types. *   A question to answer. LlamaIndex will use the data in the repository to answer it.    After parsing those parameters from the command line (**1**), we load data from the input repository (**2**). Fortunately, LlamaIndex makes this step very straightforward: no need to add handling for different file types and so on. Instead, passing the directory path is sufficient. Next, we index the data we just loaded (**3**). By default, LlamaIndex uses OpenAI’s ada models to calculate embedding vectors. Data conversions and chunking (e.g., splitting large text documents into pieces small enough to be processed by OpenAI’s ada models) are all handled automatically. Now we create a query engine on top of the index (**4**). This engine will automatically retrieve data related to an input question using the index. Finally, we use the previously generated engine to answer the input question (**5**) and print the result.    ##### Listing 10.4 A simple question-answering system with LlamaIndex    ```", "```py  #1 Parses the command-line parameters #2 Loads data from the directory #3 Indexes the data #4 Enables querying on the index #5 Generates the answer   Although LlamaIndex offers various ways to configure and specialize each step of this pipeline (and to create other pipelines), using the default settings in each step leads to particularly concise code.    ### 10.6.4 Trying it out    Let’s try our pipeline using some example data. You can download listing [10.4](#code__LlamaIndex) from the book’s companion website. Also download the bananareports.zip file from the website, and unzip it in the same folder as the code (use the Banana Reports link). Look inside the folder: you will find (short) business reports in text, .pdf documents, and PowerPoint presentations. Time to answer a few questions! Open your terminal, and change to the directory containing the code and the bananareports folder (after unzipping). Now run the following command:    ```", "```py    You should see output like the following:    ```", "```py    Have a look at the corresponding file. You will find that the Plantain unit did indeed make 30 million USD. Try asking for the other units (Pisang Raja and Cavendish); you should see the correct results. You can even try more complex questions—such as “Which unit made the most in 2023?”—requiring a comparison of different files. Again, the system should be able to find an accurate answer.    ## 10.7 Concluding remarks    LangChain and LlamaIndex are two popular frameworks for using language models. You have seen that implementing even complex applications, like those based on agents, is fairly quick when using those frameworks. LangChain and LlamaIndex have overlapping functionality: for example, both frameworks provide support for implementing agents based on language models. LlamaIndex particularly shines in scenarios that follow the high-level template we saw in the previous section (providing functionality on top of data indexes). LangChain has a more general focus on supporting developers in building complex applications using language models.    Both frameworks are relatively young at the time of writing, so the previous characterizations may not hold for future versions. In addition, their interfaces are evolving quickly, and running with the newest framework versions may require code changes. If your application does not require complex logic, you may consider using lower-level libraries such as the one by OpenAI.    Of course, this chapter does not cover these two frameworks in much detail. The intent is to give you an impression of what the frameworks can do for you, enabling you to make informed choices about which frameworks to study in more depth.    ## Summary    *   LangChain and LlamaIndex enable complex applications on top of language models. *   LangChain and LlamaIndex make it easy to create agents. Agents use language models to control invocations of various tools (standard tools as well as custom tools). They can solve complex tasks if given access to suitable tools. *   In LangChain, use the `@tool` decorator to turn functions into tools. *   LlamaIndex indexes various data types by creating embedding vectors. *   LlamaIndex makes it easy to explore and query indexed data.    ## 10.9 References    1.  Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. *Advances in Neural Information Processing Systems 35*, 24824–24837.```"]