<html><head></head><body><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. Introduction to PyTorch" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch01_introduction_to_pytorch_1748548870019566">
<h1><span class="label">Chapter 1. </span>Introduction to PyTorch</h1>

<p>When it comes to creating artificial intelligence (AI), machine learning (ML) and deep learning are great places to begin. When you’re getting started, however, it’s easy to get overwhelmed by the options and all the new terminology. This book aims to demystify things for you as a programmer. It takes you through writing code to implement concepts of ML and deep learning, and it also takes you through building models that behave more as a human does, with scenarios like computer vision, natural language processing (NLP), and more. Thus, these models become a form of synthesized, or artificial, intelligence.</p>

<p>But when we refer to <em>machine learning</em>, what exactly is it? Let’s take a quick look at that and consider it from a programmer’s perspective before we go any further. After that, in the rest of this chapter, we’ll show you how to install the tools of the trade, from PyTorch itself to environments where you can code and debug your PyTorch-based models.</p>

<section data-pdf-bookmark="What Is Machine Learning?" data-type="sect1"><div class="sect1" id="ch01_what_is_machine_learning_1748548870019841">
<h1>What Is Machine Learning?</h1>

<p>Before we get into the ins and outs of ML,<a contenteditable="false" data-primary="machine learning (ML)" data-secondary="description" data-tertiary="traditional programming" data-type="indexterm" id="ch1trad"/> let’s consider how it evolved from traditional programming. We’ll start by examining what traditional programming is, and then we’ll consider cases where it’s limited. After that, we’ll see how ML evolved to handle those cases and thus opened up new opportunities to implement new scenarios, thereby unlocking many of the concepts of AI.</p>

<p>Traditional programming involves writing rules that are expressed in a programming language and that act on data and give us answers. This applies just about everywhere we can program something with code.</p>

<p>For example, consider a game like the popular Breakout. Code determines the movement of the ball, the score, and the various conditions for winning or losing the game. Think about the scenario where the ball bounces off a brick, like in <a data-type="xref" href="#ch01_figure_1_1748548870009162">Figure 1-1</a>.</p>

<figure><div class="figure" id="ch01_figure_1_1748548870009162"><img alt="" src="assets/aiml_0101.png"/>
<h6><span class="label">Figure 1-1. </span>Code in a Breakout game</h6>
</div></figure>

<p>Here, the motion of the ball can be determined by its <code>dx</code> and <code>dy</code> properties. When the ball hits a brick, the brick is removed, the velocity of the ball increases, and the direction of the ball’s movement changes. The code acts on data about the game situation.</p>

<p>Alternatively, consider a financial services scenario. Say you have data about the company, such as its current stock price and earnings. By using code like that in <a data-type="xref" href="#ch01_figure_2_1748548870009198">Figure 1-2</a>, you can calculate a valuable ratio called the <em>price-to-earnings ratio</em> (or P/E, which stands for price divided by earnings).</p>

<figure><div class="figure" id="ch01_figure_2_1748548870009198"><img alt="" src="assets/aiml_0102.png"/>
<h6><span class="label">Figure 1-2. </span>Code in a financial services scenario</h6>
</div></figure>

<p>Your code reads the price, reads the earnings, and returns a value that is the former divided by the latter.</p>

<p>If I were to try to sum up traditional programming like this in a single diagram, it might look like <a data-type="xref" href="#ch01_figure_3_1748548870009219">Figure 1-3</a>.</p>

<figure><div class="figure" id="ch01_figure_3_1748548870009219"><img alt="" src="assets/aiml_0103.png"/>
<h6><span class="label">Figure 1-3. </span>High-level view of traditional programming</h6>
</div></figure>

<p>As you can see, you have rules expressed in a programming language. These rules act on data, and the result is answers.<a contenteditable="false" data-primary="" data-startref="ch1trad" data-type="indexterm" id="id486"/></p>
</div></section>

<section data-pdf-bookmark="Limitations of Traditional Programming" data-type="sect1"><div class="sect1" id="ch01_limitations_of_traditional_programming_1748548870019905">
<h1>Limitations of Traditional Programming</h1>

<p>The model from <a data-type="xref" href="#ch01_figure_3_1748548870009219">Figure 1-3</a> has been<a contenteditable="false" data-primary="machine learning (ML)" data-secondary="description" data-tertiary="traditional programming limitations" data-type="indexterm" id="ch1trlim"/> the backbone of development since its inception. But it has an inherent limitation: namely, the only scenarios that you can implement are ones for which you can derive rules. But what about other scenarios? Usually, it’s unfeasible to develop them because the code is too complex. It’s just not possible to write code to handle them.</p>

<p>Consider, for example, activity detection. Fitness monitors that can detect our activity are a recent innovation, not just because of the availability of cheap and small hardware but also because the algorithms to handle detection weren’t previously feasible. Let’s explore why.</p>

<p><a data-type="xref" href="#ch01_figure_4_1748548870009237">Figure 1-4</a> shows a naive activity detection algorithm for walking. It can consider the person’s speed and if that speed is less than a particular value, we can determine that they are probably walking.</p>

<figure><div class="figure" id="ch01_figure_4_1748548870009237"><img alt="" src="assets/aiml_0104.png"/>
<h6><span class="label">Figure 1-4. </span>Algorithm for activity detection</h6>
</div></figure>

<p>Given that our data is speed, we could also extend this to detect whether they are running, as in <a data-type="xref" href="#ch01_figure_5_1748548870009255">Figure 1-5</a>.</p>

<figure><div class="figure" id="ch01_figure_5_1748548870009255"><img alt="" src="assets/aiml_0105.png"/>
<h6><span class="label">Figure 1-5. </span>Extending the algorithm for running</h6>
</div></figure>

<p>As you can see, going by the speed, we might say that if it is less than a particular value (say, 4 mph) the person is walking, and otherwise, they are running. It still sort of works.</p>

<p>Now, suppose we want to extend this to another popular fitness activity, biking. The algorithm could look like the one in <a data-type="xref" href="#ch01_figure_6_1748548870009270">Figure 1-6</a>.</p>

<figure><div class="figure" id="ch01_figure_6_1748548870009270"><img alt="" src="assets/aiml_0106.png"/>
<h6><span class="label">Figure 1-6. </span>Extending the algorithm for biking</h6>
</div></figure>

<p class="pagebreak-before less_space">I know this algorithm is naive in that it just detects speed—some people run faster than others, and you might run downhill faster than you can cycle uphill—but on the whole, it still works. However, what happens if we want to implement another scenario, such as golfing (see <a data-type="xref" href="#ch01_figure_7_1748548870009286">Figure 1-7</a>)?</p>

<figure><div class="figure" id="ch01_figure_7_1748548870009286"><img alt="How do we write a golfing algorithm?" src="assets/aiml_0107.png"/>
<h6><span class="label">Figure 1-7. </span>How do we write a golfing algorithm?</h6>
</div></figure>

<p>Now, we’re stuck. Whether a person is golfing or not, they might walk for a bit, stop, do some activity, walk for a bit more, stop, etc. So how can we use this methodology to tell whether they’re playing golf?</p>

<p>Our ability to detect this activity using traditional rules has hit a wall. But maybe there’s a better way.<a contenteditable="false" data-primary="" data-startref="ch1trlim" data-type="indexterm" id="id487"/></p>

<p>Enter ML.</p>
</div></section>

<section data-pdf-bookmark="From Programming to Learning" data-type="sect1"><div class="sect1" id="ch01_from_programming_to_learning_1748548870019956">
<h1>From Programming to Learning</h1>

<p>Let’s look back at the diagram that we used<a contenteditable="false" data-primary="machine learning (ML)" data-secondary="description" data-tertiary="programming evolving to learning" data-type="indexterm" id="ch1pr2le"/> to demonstrate what traditional programming is (see <a data-type="xref" href="#ch01_figure_8_1748548870009301">Figure 1-8</a>). Here, we have rules that act on data and give us answers. In our activity detection scenario, the data was the speed at which the person was moving—and from that, we could write rules to detect their activity, be it walking, biking, or running. However, we hit a wall when it came to golfing because we couldn’t come up with rules to determine what that activity looks like.</p>

<figure><div class="figure" id="ch01_figure_8_1748548870009301"><img alt="" src="assets/aiml_0108.png"/>
<h6><span class="label">Figure 1-8. </span>The traditional programming flow</h6>
</div></figure>

<p>But what would happen if we were to flip the axes around on this diagram? Instead of us coming up with the <em>rules</em>, what if we were to come up with the <em>answers</em> and, along with the data, have a way of figuring out what the rules might be?</p>

<p class="pagebreak-before less_space"><a data-type="xref" href="#ch01_figure_9_1748548870009316">Figure 1-9</a> shows what this would look like, and we can say that this high-level diagram defines <em>machine learning</em>.</p>

<figure><div class="figure" id="ch01_figure_9_1748548870009316"><img alt="" src="assets/aiml_0109.png"/>
<h6><span class="label">Figure 1-9. </span>Changing the axes to get ML</h6>
</div></figure>

<p>So, what are the implications of this? Well, now, instead of <em>us</em> trying to figure out what the rules are, we can get lots of data about our scenario and label that data, and then the computer can figure out what the rules are that make one piece of data match a particular label and another piece of data match a different label.</p>

<p>How would this work for our activity detection scenario? Well, we can look at all the sensors that give us data about this person. If the person has a wearable device that detects information such as heart rate, location, and speed—and if we collect a lot of instances of this data while they’re doing different activities—then we end up with a scenario of having data that says, “This is what walking looks like,” “This is what running looks like,” and so on (see <a data-type="xref" href="#ch01_figure_10_1748548870009331">Figure 1-10</a>).</p>

<figure><div class="figure" id="ch01_figure_10_1748548870009331"><img alt="From coding to ML: gathering and labeling data" src="assets/aiml_0110.png"/>
<h6><span class="label">Figure 1-10. </span>From coding to ML: gathering and labeling data</h6>
</div></figure>

<p>Now, our job as programmers changes from figuring out the rules, to determining the activities, to writing the code that matches the data to the labels. If we can do this, then we can expand the scenarios that we can implement with code.</p>

<p>ML is a technique<a contenteditable="false" data-primary="machine learning (ML)" data-secondary="description" data-tertiary="TensorFlow" data-type="indexterm" id="id488"/> that enables us to do this, but to get started, we’ll need a framework—that’s where TensorFlow enters the picture. In the next section, we’ll take a look at what TensorFlow is and how to install it. Then, later in this chapter, you’ll write your first code that learns the pattern between two values, like in the preceding scenario. It’s a simple “Hello World” scenario, but it has the same foundational code pattern that’s used in extremely complex ones.</p>

<p>The field of AI is large and abstract, encompassing everything that has to do with making computers think and act the way human beings do. One of the ways a human takes on new behaviors is through learning by example, and the discipline of ML can thus be thought of as an on-ramp to the development of AI. By way of an ML field called <em>computer vision</em>, a machine can learn to see like a human, and by way of another ML field called <em>natural language processing</em>, it can learn to read text like a human. Many more such applications of ML are possible, and we’ll be covering the basics of ML in this book by using the TensorFlow framework.</p>
</div></section>

<section data-pdf-bookmark="What Is PyTorch?" data-type="sect1"><div class="sect1" id="ch01_what_is_pytorch_1748548870020005">
<h1>What Is PyTorch?</h1>

<p>PyTorch is an ML library<a contenteditable="false" data-primary="PyTorch" data-secondary="description" data-type="indexterm" id="ch1pytrch"/><a contenteditable="false" data-primary="Torch" data-type="indexterm" id="id489"/><a contenteditable="false" data-primary="Lua" data-type="indexterm" id="id490"/><a contenteditable="false" data-primary="PyTorch" data-secondary="as torch" data-secondary-sortas="torch" data-type="indexterm" id="id491"/> that is based on a previous library called <em>Torch</em>, which is an open source ML framework and scripting language that is itself based on a programming language called Lua. In 2017, development of Torch moved to PyTorch, which is a port of the framework in Python.</p>

<p>So, when installing PyTorch, you’ll often see it referred to as “torch.”</p>

<p>PyTorch was originally developed<a contenteditable="false" data-primary="Meta AI development of PyTorch" data-type="indexterm" id="id492"/><a contenteditable="false" data-primary="Linux Foundation" data-type="indexterm" id="id493"/> by Meta AI, but it was moved out to the Linux Foundation as a way of building developer confidence that it wasn’t made by and for a big tech company. It’s one of the two most popular ML libraries, alongside the TensorFlow/Keras ecosystem.</p>

<p>With the emergence of generative AI, and in particular the “open sourcing” of generative text and image models, PyTorch has exploded in popularity. It’s often used for training models (which we cover in Part I of this book) as well as for inference of models (which we cover in Part II of this book).</p>

<p>PyTorch could also be seen as an ecosystem of libraries, each of which is tailored to specific scenarios. The important libraries and scenarios to consider are as follows:</p>

<dl>
	<dt>TorchServe</dt>
	<dd>
	<p>This is an easy-to-use tool<a contenteditable="false" data-primary="TorchServe" data-type="indexterm" id="id494"/> that lets you deploy PyTorch models at scale. It’s designed to run in multiple environments, and it’s generally technology agnostic. It supports features such as multimodel serving, logging, metrics, and the easy creation of RESTful endpoints that let you do inference on models from a variety of clients.</p>
	</dd>
	<dt>Distributed training</dt>
	<dd>
	<p>When larger models don’t fit <a contenteditable="false" data-primary="distributed training" data-type="indexterm" id="id495"/><a contenteditable="false" data-primary="training" data-secondary="distributed training" data-type="indexterm" id="id496"/>onto a single chip or machine, there are technologies and techniques that allow you to share them across multiple devices. The <code>torch.distributed</code> libraries allow you easy and native support of asynchronous execution across multiple devices.</p>
	</dd>
	<dt>Mobile</dt>
	<dd>
	<p>An important surface for inference is,<a contenteditable="false" data-primary="Mobile" data-type="indexterm" id="id497"/> of course, mobile. It’s important for you to be able to deploy your AI work to Android and iOS devices, and PyTorch supports this through PyTorch Mobile.</p>
	</dd>
	<dt>Pretrained models</dt>
	<dd>
	<p>An active community of researchers<a contenteditable="false" data-primary="torchvision.models library" data-type="indexterm" id="id498"/><a contenteditable="false" data-primary="pretrained models" data-secondary="torchvision.models library" data-type="indexterm" id="id499"/> and developers have created a rich ecosystem of models that you can simply use with one line of code, wrapped in the torchvision.models library.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#fig-1-11">Figure 1-11</a> provides a high-level representation of this.</p>

<figure><div class="figure" id="fig-1-11"><img alt="" src="assets/aiml_0111.png"/>
<h6><span class="label">Figure 1-11. </span>PyTorch ecosystem</h6>
</div></figure>

<p>The process of creating ML models<a contenteditable="false" data-primary="training" data-secondary="about" data-type="indexterm" id="id500"/><a contenteditable="false" data-primary="models" data-secondary="about training" data-seealso="training" data-type="indexterm" id="id501"/> is called <em>training</em>, and it’s where a computer uses a set of algorithms to learn about inputs and what distinguishes them from one another. So, for example, if you want a computer to recognize cats and dogs, you can use lots of pictures of both to create a model, and the computer will use that model to try to figure out what makes a cat a cat and what makes a dog a dog. <a contenteditable="false" data-primary="inference" data-secondary="about" data-type="indexterm" id="id502"/><a contenteditable="false" data-primary="models" data-secondary="about inference" data-seealso="inference" data-type="indexterm" id="id503"/>Once the model is trained, the process of having it recognize or categorize future inputs is called <span class="keep-together"><em>inference</em></span>.</p>

<p>So, for training models, there are several things that you need to consider, and we will cover them in this book. Primarily, your choice will boil down to one of three things:</p>

<ul>
	<li>
	<p>Creating the model entirely from scratch yourself</p>
	</li>
	<li>
	<p>Using someone else’s model because it’s enough for your task</p>
	</li>
	<li>
	<p>Using parts of another person’s model that have already been trained and building on top of them</p>
	</li>
</ul>

<p>The last option on the list<a contenteditable="false" data-primary="transfer learning" data-secondary="about" data-type="indexterm" id="id504"/><a contenteditable="false" data-primary="computer vision" data-secondary="transfer learning" data-tertiary="about" data-type="indexterm" id="id505"/><a contenteditable="false" data-primary="feature detection" data-secondary="transfer learning" data-tertiary="about" data-type="indexterm" id="id506"/><a contenteditable="false" data-primary="pretrained models" data-secondary="transfer learning" data-tertiary="about" data-type="indexterm" id="id507"/> is called <em>transfer learning</em>, and we’ll cover it later in the book.</p>

<p>There are many ways to train a model.<a contenteditable="false" data-primary="training" data-secondary="chips for" data-type="indexterm" id="id508"/><a contenteditable="false" data-primary="CPU (central processing unit)" data-type="indexterm" id="id509"/><a contenteditable="false" data-primary="GPU (graphics processing unit)" data-type="indexterm" id="id510"/><a contenteditable="false" data-primary="tensor processing unit (TPU)" data-type="indexterm" id="id511"/><a contenteditable="false" data-primary="TPU (tensor processing unit)" data-type="indexterm" id="id512"/> For the most part, you’ll probably just use a single chip, whether it’s a central processing unit (CPU), a graphics processing unit (GPU), or something new called a tensor processing unit (TPU). <a contenteditable="false" data-primary="distributed training" data-secondary="PyTorch libraries" data-type="indexterm" id="id513"/><a contenteditable="false" data-primary="training" data-secondary="distributed training" data-tertiary="PyTorch libraries" data-type="indexterm" id="id514"/>In more advanced working and research environments, you can use parallel training across multiple chips, employing a distributed training where training is intelligently spanned across multiple chips. PyTorch supports this, too, through “distributed training” libraries, as shown in <a data-type="xref" href="#fig-1-11">Figure 1-11</a>.</p>

<p>The lifeblood of any model is its data. As we discussed earlier, if you want to create a model that can recognize cats and dogs, you need to train it with lots of examples of cats and dogs. But how can you manage these examples? Over time, you’ll see that this can often involve a lot more coding than the creation of the models themselves.</p>

<p>But luckily, the PyTorch ecosystem<a contenteditable="false" data-primary="PyTorch" data-secondary="datasets built in" data-type="indexterm" id="id515"/><a contenteditable="false" data-primary="datasets" data-secondary="about PyTorch built-in datasets" data-type="indexterm" id="id516"/> includes a number of built-in datasets that make this easy for you. We will also explore these throughout this book.</p>

<p>Beyond creating models, you’ll need to be able to get them into people’s hands so they can use them. To this end, PyTorch includes libraries for serving, where you can provide model inference over an HTTP connection for cloud or web users. <a contenteditable="false" data-primary="Mobile" data-type="indexterm" id="id517"/>For models to run on mobile or embedded systems, there’s PyTorch Mobile, which provides tools for model inference on Android and iOS.</p>

<p>Next, I’ll show you how to install PyTorch so that you can get started creating and using ML models with it!<a contenteditable="false" data-primary="" data-startref="ch1pytrch" data-type="indexterm" id="id518"/></p>
</div></section>

<section data-pdf-bookmark="Using PyTorch" data-type="sect1"><div class="sect1" id="ch01_using_pytorch_1748548870020057">
<h1>Using PyTorch</h1>

<p>In this section, we’ll look at the three main ways you can install and use PyTorch. We’ll start with how to install it on your developer box using the command line. Then, we’ll explore using the popular PyCharm IDE to install and use PyTorch. Finally, we’ll look at Google Colab and how you can use it to access your PyTorch code with a cloud-based backend in your browser.</p>

<section data-pdf-bookmark="Installing Porch in Python" data-type="sect2"><div class="sect2" id="ch01_installing_porch_in_python_1748548870020109">
<h2>Installing Porch in Python</h2>

<p>The <em>Py</em> in PyTorch stands<a contenteditable="false" data-primary="Python" data-secondary="installation URL" data-type="indexterm" id="id519"/><a contenteditable="false" data-primary="Python" data-secondary="learning Python online" data-type="indexterm" id="id520"/><a contenteditable="false" data-primary="online resources" data-secondary="Python installation and information" data-type="indexterm" id="id521"/> for Python, so it’s important to have a Python environment already set up. If you don’t have Python already, I strongly recommend you visit <a href="https://python.org">the Python website</a> to get up and running with it and <a href="https://learnpython.org">the Learn Python website</a> to learn the Python language syntax.</p>

<p>With Python, there are many ways<a contenteditable="false" data-primary="Python" data-secondary="PyTorch installation" data-type="indexterm" id="id522"/> to install frameworks, but the default one supported by the TensorFlow team is <code>pip</code>.</p>

<p>So, in your Python environment, installing PyTorch is as easy as using this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="o">&gt;</code> <code class="n">pip</code> <code class="n">install</code> <code class="n">torch</code> </pre>

<p>Once you’re up and running, you can test your PyTorch version with the following code:<a contenteditable="false" data-primary="PyTorch" data-secondary="version displayed" data-type="indexterm" id="id523"/></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">torch</code>
<code class="nb">print</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">__version__</code><code class="p">)</code></pre>

<p class="pagebreak-before">You should then see output like that in <a data-type="xref" href="#ch01_figure_12_1748548870009360">Figure 1-12</a>. It will print the currently running version of PyTorch—here, you can see that version 2.4.1 is installed.</p>

<figure><div class="figure" id="ch01_figure_12_1748548870009360"><img src="assets/aiml_0112.png"/>
<h6><span class="label">Figure 1-12. </span>Running PyTorch in Python</h6>
</div></figure>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you look closely at <a data-type="xref" href="#ch01_figure_12_1748548870009360">Figure 1-12</a>, you’ll see<a contenteditable="false" data-primary="CPU (central processing unit)" data-secondary="configuring PyTorch to use CPU" data-type="indexterm" id="id524"/><a contenteditable="false" data-primary="PyTorch" data-secondary="configuring for CPU" data-type="indexterm" id="id525"/> a note that shows that the torch device is “cpu.” In this case, I natively installed it on my Mac, and it is configured to use the CPU. However, this is not optimal for complex models, where an accelerator like a GPU or Metal may be necessary. We will cover installation of PyTorch for accelerators later in this book.</p>
</div>
</div></section>

<section data-pdf-bookmark="Using PyTorch in PyCharm" data-type="sect2"><div class="sect2" id="ch01_using_pytorch_in_pycharm_1748548870020156">
<h2>Using PyTorch in PyCharm</h2>

<p>I’m particularly fond of using the <a href="https://oreil.ly/I2mP2">free community version of PyCharm </a><a contenteditable="false" data-primary="PyCharm" data-secondary="about PyTorch running in" data-type="indexterm" id="id526"/><a contenteditable="false" data-primary="PyTorch" data-secondary="about running in PyCharm" data-type="indexterm" id="id527"/><a contenteditable="false" data-primary="PyCharm" data-secondary="virtual environment install of PyTorch" data-type="indexterm" id="ch1virin"/><a contenteditable="false" data-primary="PyTorch" data-secondary="installing" data-tertiary="PyCharm virtual environment" data-type="indexterm" id="ch1virin2"/> for building models using PyTorch. PyCharm is useful for many reasons, but one of my favorites is that it makes the management of virtual environments easy. This means you can have Python environments with versions of tools such as PyTorch that are specific to your particular project. So, for example, if you want to use PyTorch 1.x in one project and PyTorch 2.x in another, you can separate them with virtual environments and not have to deal with installing/uninstalling dependencies when you switch between them. Additionally, with PyCharm, you can do step-by-step debugging of your Python code—which is a must, especially if you’re just getting started!</p>

<p>For example, in <a data-type="xref" href="#ch01_figure_13_1748548870009375">Figure 1-13</a>, I have a new project that’s called <em>example1</em>, and I’m specifying that I’m going to create a new environment using Conda. When I create the project, I’ll have a clean, new, virtual Python environment into which I can install any version of PyTorch I want.</p>

<p>Once you’ve created a project, you can open the File → Settings dialog and choose the entry for “Project: <em>&lt;your project name&gt;</em>” from the menu on the left. In the menu on the left, you’ll see choices to change the settings for the Python Interpreter and the Project Structure. If you choose the Python Interpreter link, you’ll see the interpreter that you’re using, as well as a list of packages that are installed in this virtual environment (see <a data-type="xref" href="#ch01_figure_14_1748548870009389">Figure 1-14</a>).</p>

<figure><div class="figure" id="ch01_figure_13_1748548870009375"><img src="assets/aiml_0113.png"/>
<h6><span class="label">Figure 1-13. </span>Creating a new virtual environment using PyCharm</h6>
</div></figure>

<figure><div class="figure" id="ch01_figure_14_1748548870009389"><img src="assets/aiml_0114.png"/>
<h6><span class="label">Figure 1-14. </span>Adding packages to a virtual environment</h6>
</div></figure>

<p>You can then click the + button at the upper left, and a dialog will open showing the packages that are currently available. Type <strong><code>torch</code></strong> into the search box and you’ll see all available packages with <em>torch</em> in the name (see <a data-type="xref" href="#ch01_figure_15_1748548870009404">Figure 1-15</a>). Remember that the name of the package is <em>torch</em>, even if the technology is PyTorch.</p>

<figure><div class="figure" id="ch01_figure_15_1748548870009404"><img src="assets/aiml_0115.png"/>
<h6><span class="label">Figure 1-15. </span>Installing torch with PyCharm</h6>
</div></figure>

<p>Once you’ve selected torch or any other package you want to install, you can click the Install Package button and PyCharm will do the rest. Then, once torch is installed, you can write and debug your PyTorch code in Python.<a contenteditable="false" data-primary="" data-startref="ch1virin" data-type="indexterm" id="id528"/><a contenteditable="false" data-primary="" data-startref="ch1virin2" data-type="indexterm" id="id529"/></p>
</div></section>

<section data-pdf-bookmark="Using PyTorch in Google Colab" data-type="sect2"><div class="sect2" id="ch01_using_pytorch_in_google_colab_1748548870020203">
<h2>Using PyTorch in Google Colab</h2>

<p>Another option, perhaps the easiest one<a contenteditable="false" data-primary="PyTorch" data-secondary="installing" data-tertiary="Google Colab" data-type="indexterm" id="ch1col"/><a contenteditable="false" data-primary="Google Colab" data-secondary="PyTorch introduction" data-type="indexterm" id="ch1col2"/><a contenteditable="false" data-primary="Google Colab" data-secondary="about" data-type="indexterm" id="id530"/> for getting started, is to use <a href="https://oreil.ly/c0lab"><em>Google Colab</em></a>, which is a hosted Python environment that you can access via a browser. <a contenteditable="false" data-primary="GPU (graphics processing unit)" data-secondary="Google Colab GPU backend" data-type="indexterm" id="id531"/><a contenteditable="false" data-primary="TPU (tensor processing unit)" data-secondary="Google Colab TPU backend" data-type="indexterm" id="id532"/>What’s really neat about Colab is that it provides GPU and TPU backends so you can train models using state-of-the-art hardware at no cost.</p>

<p>When you visit the Colab website, you’ll be given the option to open previous Colabs or start a new notebook (see <a data-type="xref" href="#ch01_figure_16_1748548870009418">Figure 1-16</a>).  If you click the + New notebook button, it will open the editor, where you can add panes of code or text (see <a data-type="xref" href="#ch01_figure_17_1748548870009451">Figure 1-17</a>). You ⁠ can then execute the code by clicking the Play button (the arrow) to the left of the pane.</p>

<figure><div class="figure" id="ch01_figure_16_1748548870009418"><img src="assets/aiml_0116.png"/>
<h6><span class="label">Figure 1-16. </span>Getting started with Google Colab</h6>
</div></figure>

<figure><div class="figure" id="ch01_figure_17_1748548870009451"><img src="assets/aiml_0117.png"/>
<h6><span class="label">Figure 1-17. </span>Running PyTorch code in Colab</h6>
</div></figure>

<p>It’s always a good idea to check the PyTorch version, as shown here, to be sure you’re running the correct version for the task at hand.</p>

<p>You can also see that in <a data-type="xref" href="#ch01_figure_17_1748548870009451">Figure 1-17</a> the version shown<a contenteditable="false" data-primary="PyTorch" data-secondary="version displayed" data-type="indexterm" id="id533"/><a contenteditable="false" data-primary="GPU (graphics processing unit)" data-secondary="Cuda NVidia library" data-type="indexterm" id="id534"/><a contenteditable="false" data-primary="Cuda (cu)" data-type="indexterm" id="id535"/> is 2.4.1+cu121, and you might want to know what the <em>cu121</em> part is! The <em>cu</em> stands for <em>Cuda</em>, which is Nvidia’s library for accelerated ML on GPUs. So, the preceding message demonstrates that PyTorch 2.4.1 is installed, along with accelerators for Cuda version 12.1.</p>

<p>Often, Colab’s built-in versions<a contenteditable="false" data-primary="Google Colab" data-secondary="PyTorch introduction" data-tertiary="updating PyTorch" data-type="indexterm" id="id536"/><a contenteditable="false" data-primary="updating PyTorch on Google Colab" data-type="indexterm" id="id537"/><a contenteditable="false" data-primary="PyTorch" data-secondary="updating on Google Colab" data-type="indexterm" id="id538"/> of various libraries, including PyTorch, will be a version or two behind the latest release. If that’s the case, you can update it with <code>pip install</code> as shown earlier, by simply using a block of code like this, where you specify the desired version:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">torch</code><code class="o">==&lt;</code><code class="n">different</code> <code class="n">version</code> <code class="n">number</code><code class="o">&gt;</code></pre>

<p>Once you run this command, your current environment within Colab will use the desired version of PyTorch. <a contenteditable="false" data-primary="Cuda (cu)" data-secondary="updating PyTorch" data-type="indexterm" id="id539"/>However, you should be careful when doing this in Colab because the version of PyTorch you change to may not have Cuda drivers installed, meaning you could downgrade to using the CPU.<a contenteditable="false" data-primary="" data-startref="ch1col" data-type="indexterm" id="id540"/><a contenteditable="false" data-primary="" data-startref="ch1col2" data-type="indexterm" id="id541"/></p>
</div></section>
</div></section>

<section data-pdf-bookmark="Getting Started with Machine Learning" data-type="sect1"><div class="sect1" id="ch01_getting_started_with_machine_learning_1748548870020264">
<h1>Getting Started with Machine Learning</h1>

<p>As we saw earlier in the chapter,<a contenteditable="false" data-primary="machine learning (ML)" data-secondary="getting started with" data-type="indexterm" id="ch1mlstrt"/><a contenteditable="false" data-primary="PyTorch" data-secondary="getting started with ML" data-type="indexterm" id="ch1mlstrt2"/> the ML paradigm is one in which you have data, that data is labeled, and you want to figure out the rules that match the data to the labels. The simplest possible scenario to show this in code is as follows.</p>

<p>Consider these two sets of numbers:</p>

<pre data-code-language="python" data-type="programlisting">
<em><code class="n">x</code></em> <code class="o">=</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code>
<em><code class="n">y</code></em> <code class="o">=</code> <code class="err">–</code><code class="mi">3</code><code class="p">,</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">7</code></pre>

<p>There’s a relationship between the <em>x</em> and <em>y</em> values (for example, if <em>x</em> is –1, then <em>y</em> is –3; if <em>x</em> is 3, then <em>y </em>is 5; and so on). Can you see it?</p>

<p>After a few seconds, you probably saw that the pattern here is <em>y</em> = 2<em>x</em> – 1. How did you get that? Different people work it out in different ways, but I typically hear the observation that <em>x</em> increases by 1 in its sequence and <em>y</em> increases by 2; thus, <em>y</em> = 2<em>x</em> +/– something. Then, they look at when <em>x</em> = 0 and see that <em>y</em> = –1, so they figure that the answer could be <em>y </em>= 2<em>x</em> – 1. Next, they look at the other values and see that this hypothesis “fits,” and the answer is <em>y</em> = 2<em>x</em> – 1.</p>

<p>That’s very similar to the ML process. Let’s take a look at some code that you could write to have a neural network figure this out for you.</p>

<p class="pagebreak-before less_space">Here’s the full code, using PyTorch. Don’t worry if it doesn’t make sense yet; we’ll go through it line by line:<a contenteditable="false" data-primary="machine learning (ML)" data-secondary="getting started with" data-tertiary="PyTorch beginner code" data-type="indexterm" id="ch1bgn"/><a contenteditable="false" data-primary="PyTorch" data-secondary="getting started with ML" data-tertiary="PyTorch beginner code" data-type="indexterm" id="ch1bgn2"/><a contenteditable="false" data-primary="“Hello World” PyTorch code" data-primary-sortas="Hello World" data-type="indexterm" id="ch1bgn3"/></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torch.optim</code> <code class="k">as</code> <code class="nn">optim</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
 
<code class="c1"># Model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>
 
<code class="c1"># Loss and optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>
 
<code class="c1"># Data</code>
<code class="n">xs</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="err">–</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">2.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">3.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">4.0</code><code class="p">]],</code> 
                  <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
<code class="n">ys</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="err">–</code><code class="mf">3.0</code><code class="p">],</code> <code class="p">[</code><code class="err">–</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">3.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">5.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">7.0</code><code class="p">]],</code> 
                  <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
 
<code class="c1"># Train</code>
<code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">500</code><code class="p">):</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
    <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">xs</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">outputs</code><code class="p">,</code> <code class="n">ys</code><code class="p">)</code>
    <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
 
<code class="c1"># Predict</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mf">10.0</code><code class="p">]],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)))</code></pre>

<p class="less_space">The first few lines are the importers that ensure the correct libraries are available, so let’s jump to this line:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code></pre>

<p>You’ve probably heard of neural networks,<a contenteditable="false" data-primary="neural networks" data-secondary="getting started with ML" data-type="indexterm" id="id542"/><a contenteditable="false" data-primary="neural networks" data-secondary="getting started with ML" data-tertiary="neurons" data-type="indexterm" id="id543"/><a contenteditable="false" data-primary="neurons" data-secondary="getting started with ML" data-type="indexterm" id="id544"/><a contenteditable="false" data-primary="layers" data-secondary="getting started with ML" data-type="indexterm" id="id545"/> and you’ve probably seen diagrams that explain them by using layers of interconnected neurons, a little like in <a data-type="xref" href="#ch01_figure_18_1748548870009467">Figure 1-18</a>.</p>

<figure><div class="figure" id="ch01_figure_18_1748548870009467"><img alt="" src="assets/aiml_0118.png"/>
<h6><span class="label">Figure 1-18. </span>A typical neural network</h6>
</div></figure>

<p>When you see a neural network like this, you should consider each of the circles to be a <em>neuron</em> and each of the columns of circles to be a <em>layer</em>. So, in <a data-type="xref" href="#ch01_figure_18_1748548870009467">Figure 1-18</a>, there are three layers: the first has five neurons, the second has four, and the third has two.</p>

<p>These layers are organized in a sequence through which the data flows from left to right.</p>

<p>Now, if we look back at our code, you’ll see that we’re defining a sequence of something, with what’s contained in the brackets being the definition of the sequence:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code></pre>

<p>When using PyTorch, you define<a contenteditable="false" data-primary="layers" data-secondary="Sequential defining" data-tertiary="single layer" data-type="indexterm" id="id546"/><a contenteditable="false" data-primary="Sequential" data-secondary="single layer defined" data-type="indexterm" id="id547"/> your layers by using a <code>Sequential</code>, and inside the <code>Sequential</code>, you then specify what each layer looks like. We have only one line inside our <code>Sequential</code>, so the neural network this code defines will have only one layer.</p>

<p>Then, you define what the layer<a contenteditable="false" data-primary="Sequential" data-secondary="Linear layers defined" data-type="indexterm" id="id548"/><a contenteditable="false" data-primary="linear layers" data-secondary="single layer" data-type="indexterm" id="id549"/><a contenteditable="false" data-primary="layers" data-secondary="linear layers" data-tertiary="single layer" data-type="indexterm" id="id550"/> looks like by using the <code>torch.nn</code> libraries. There are lots of different layer types, but here, we’re using a <code>Linear</code> layer, in which a linear relationship (where the definition of a line is <em>y</em> = <em>wx</em> + <em>b</em>) can be defined or learned.</p>

<p>Our <code>Linear</code> layer<a contenteditable="false" data-primary="linear layers" data-secondary="parameters (1,1)" data-type="indexterm" id="id551"/><a contenteditable="false" data-primary="layers" data-secondary="linear layers" data-tertiary="parameters (1,1)" data-type="indexterm" id="id552"/><a contenteditable="false" data-primary="neurons" data-secondary="getting started with ML" data-tertiary="single neuron" data-type="indexterm" id="id553"/> has the (1,1) parameters specified, which indicates one feature “in” and one feature “out.” So ultimately, we have just one layer with one neuron in our entire neural network.</p>

<p>In other words, the Sequential containing a Linear with the parameters (1,1) ultimately looks like <a data-type="xref" href="#ch01_figure_19_1748548870009483">Figure 1-19</a>.</p>

<figure><div class="figure" id="ch01_figure_19_1748548870009483"><img alt="" src="assets/aiml_0119.png"/>
<h6><span class="label">Figure 1-19. </span>A neural network with one layer, containing one neuron</h6>
</div></figure>

<p>The next lines are where the fun really begins. Let’s look at them again:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Loss and optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code></pre>

<p>If you’ve done anything with ML before, you’ve probably seen that it involves a lot of mathematics—and if you haven’t done calculus in years, it might have seemed like a barrier to entry. Here’s the part where the math comes in—it’s the core of ML.</p>

<p>In a scenario such as this one,<a contenteditable="false" data-primary="loss functions" data-secondary="about machine learning" data-type="indexterm" id="ch1loss"/> the computer has <em>no idea</em> what the relationship between <em>x</em> and <em>y</em> is. So, it will make a guess. Say, for example, it guesses that <em>y</em> = 10<em>x</em> + 10. Then, it needs to measure how good or how bad that guess is—and that’s the job of the <em>loss function</em>.</p>

<p>The computer already knows the answers when <em>x</em> is –1, 0, 1, 2, 3, and 4, so the loss function can compare these to the answers for the guessed relationship. If it guessed <em>y</em> = 10<em>x</em> + 10, then when <em>x</em> is –1, <em>y</em> will be 0. However, the correct answer there was –3, so it’s a bit off. But when <em>x</em> is 4, the guessed answer is 50, whereas the correct one is 7. That’s really far off.</p>

<p>Armed with this knowledge,<a contenteditable="false" data-primary="optimizers" data-secondary="about machine learning" data-type="indexterm" id="id554"/><a contenteditable="false" data-primary="PyTorch" data-secondary="optimizer" data-type="indexterm" id="id555"/><a contenteditable="false" data-primary="machine learning (ML)" data-secondary="getting started with" data-tertiary="optimizer" data-type="indexterm" id="id556"/><a contenteditable="false" data-primary="stochastic gradient descent (sgd) optimizer" data-type="indexterm" id="id557"/><a contenteditable="false" data-primary="sgd (stochastic gradient descent) optimizer" data-type="indexterm" id="id558"/><a contenteditable="false" data-primary="optimizers" data-secondary="stochastic gradient descent" data-type="indexterm" id="id559"/> the computer can then make another guess. That’s the job of the <em>optimizer</em>. This is where the heavy calculus is used, but with PyTorch, that can be hidden from you. You just pick the appropriate optimizer to use for different scenarios. In this case, we picked one called <code>sgd</code>, which stands for <em>stochastic gradient descent</em>—a complex mathematical function that, when given the values, the previous guess, and the results of calculating the errors (or loss) on that guess, can then generate another guess. Over time, its job is to minimize the loss, and by doing so bring the guessed formula closer and closer to the correct answer.</p>

<p>Next, we simply format our numbers into the data format that the layers expect:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Data</code>
<code class="n">xs</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="err">–</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">2.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">3.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">4.0</code><code class="p">]],</code> 
                  <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
 
<code class="n">ys</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="err">–</code><code class="mf">3.0</code><code class="p">],</code> <code class="p">[</code><code class="err">–</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">3.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">5.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">7.0</code><code class="p">]],</code> 
                  <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code></pre>

<p>You’ll see the word <em>tensor</em> a lot in ML;<a contenteditable="false" data-primary="tensors" data-secondary="about machine learning" data-type="indexterm" id="id560"/><a contenteditable="false" data-primary="Google TensorFlow" data-secondary="about machine learning" data-type="indexterm" id="id561"/><a contenteditable="false" data-primary="PyTorch" data-secondary="data understood via tensors" data-type="indexterm" id="id562"/> it gives the TensorFlow framework its name. Think of a tensor as a way of storing data that’s like an array that is optimized for flexibility in array size. To have PyTorch understand our data, we will load the values into tensors representing the <em>x</em> and <em>y</em> values.</p>

<p>The <em>learning</em> process<a contenteditable="false" data-primary="learning process" data-secondary="about machine learning" data-type="indexterm" id="ch1lpml"/><a contenteditable="false" data-primary="machine learning (ML)" data-secondary="getting started with" data-tertiary="learning process" data-type="indexterm" id="ch1lpml2"/><a contenteditable="false" data-primary="neural networks" data-secondary="learning process" data-type="indexterm" id="ch1lpml3"/><a contenteditable="false" data-primary="training" data-secondary="learning process" data-type="indexterm" id="ch1lpml4"/> will then begin with the training loop like this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Train</code>
<code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">500</code><code class="p">):</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
    <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">xs</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">outputs</code><code class="p">,</code> <code class="n">ys</code><code class="p">)</code>
    <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code></pre>

<p>If you’re new to ML, this is probably the most difficult part to understand, so let’s go through it line by line.<a contenteditable="false" data-primary="accuracy" data-secondary="learning process" data-type="indexterm" id="id563"/></p>

<p>Remember that the ML process looks like <a data-type="xref" href="#ch01_figure_20_1748548870009497">Figure 1-20</a>.</p>

<figure><div class="figure" id="ch01_figure_20_1748548870009497"><img alt="" src="assets/aiml_0120.png"/>
<h6><span class="label">Figure 1-20. </span>The ML process</h6>
</div></figure>

<p>So, the preceding code implements this as follows:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code></pre>

<p>This line reads as “zero the gradients” of the optimizer. The calculus of learning involves navigating down a curve to find its minimum, and to do that, we need the gradient of the curve. The curve is calculated when we measure our accuracy, so we need to reset it at the beginning of each loop:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">xs</code><code class="p">)</code></pre>

<p>This line creates an array of the outputs that we calculate for the input <em>x</em> values. Even though we have given the computer the <em>correct</em> answers in our <em>Y</em> array, we want to measure the accuracy of the guess that the computer has made for the parameters defining this line. The first time through the loop, the <em>w</em> and <em>b</em> parameters within the neuron will be randomly initialized, so our guess might be <em>Y</em> = 10<em>x</em> + 10, for example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">outputs</code><code class="p">,</code> <code class="n">ys</code><code class="p">)</code></pre>

<p>This line then compares the outputs (aka our guesses) with the correct answers to calculate the <em>loss</em>—which is effectively a value that tells us how good or bad the guess is:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code></pre>

<p class="pagebreak-before less_space">This line is the essential part<a contenteditable="false" data-primary="backpropagation" data-type="indexterm" id="id564"/><a contenteditable="false" data-primary="loss functions" data-secondary="loss.backward function" data-type="indexterm" id="id565"/> of the learning process where a process called <em>backpropagation</em> happens. It’s where the math from the optimizer and the loss function combine to figure out the gradients for the new set of parameters. In our case, the error from <em>Y</em> = 10<em>x</em> + 10 is really high and not even close to our desired values, so the calculations done in figuring out the loss will give us a <em>direction</em> or gradient in which we should go to get closer to our desired results:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code></pre>

<p>This line of code finishes the job by updating the model parameters to the values based on the gradients calculated in the preceding backpropagation step.</p>

<p>We then repeat this process five hundred times, with the goal of finding a set of parameters for our single neuron that will give us <em>y</em> values that are close to our desired <em>y</em> values. If the set of parameters does so, it can then infer the <em>y</em> value for <em>x</em> values that the computer has never previously seen. Thus, it will have learned the relationship between the <em>x</em> and <em>y</em> values we provided.</p>

<p><a data-type="xref" href="#ch01_figure_21_1748548870009510">Figure 1-21</a> shows a screenshot of this running in a Colab notebook. Take a look at the loss values over time.</p>

<figure><div class="figure" id="ch01_figure_21_1748548870009510"><img src="assets/aiml_0121.png"/>
<h6><span class="label">Figure 1-21. </span>Training the neural network</h6>
</div></figure>

<p>We can see that over the first 10 epochs, the loss went from 5.64 to 0.86. That is, after only 10 tries, the network was performing about six times better than with its initial guess.</p>

<p>Then take a look at what happens by the 500th epoch (see <a data-type="xref" href="#ch01_figure_22_1748548870009531">Figure 1-22</a>).</p>

<figure><div class="figure" id="ch01_figure_22_1748548870009531"><img src="assets/aiml_0122.png"/>
<h6><span class="label">Figure 1-22. </span>Training the neural network—the last few epochs</h6>
</div></figure>

<p>We can now see that the loss is 9.52 × 10<sup>-6</sup>. The loss has gotten so small that the model has pretty much figured out that the relationship between the numbers is <em>y</em> = 2<em>x</em> – 1. This means that the <em>machine</em> has <em>learned</em> the pattern between them.<a contenteditable="false" data-primary="" data-startref="ch1lpml" data-type="indexterm" id="id566"/><a contenteditable="false" data-primary="" data-startref="ch1lpml2" data-type="indexterm" id="id567"/><a contenteditable="false" data-primary="" data-startref="ch1lpml3" data-type="indexterm" id="id568"/><a contenteditable="false" data-primary="" data-startref="ch1lpml4" data-type="indexterm" id="id569"/><a contenteditable="false" data-primary="" data-startref="ch1loss" data-type="indexterm" id="id570"/></p>

<p>If we want our neural network to try to predict a new value, we can use code like this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Predict</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">prediction</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mf">10.0</code><code class="p">]],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">))</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">prediction</code><code class="p">)</code></pre>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The term <em>prediction</em> is typically used<a contenteditable="false" data-primary="prediction" data-type="indexterm" id="id571"/><a contenteditable="false" data-primary="models" data-secondary="prediction" data-type="indexterm" id="id572"/><a contenteditable="false" data-primary="machine learning (ML)" data-secondary="getting started with" data-tertiary="prediction" data-type="indexterm" id="id573"/> when dealing with ML <span class="keep-together">models—but</span> don’t think of it as looking into the future! We use this term because we’re dealing with a certain amount of uncertainty. Think back to the activity detection scenario we spoke about earlier. When the person was moving at a certain speed, she was <em>probably</em> walking. Similarly, when a model learns about the patterns that exist between two things, it will tell us what the answer <em>probably</em> is. In other words, it is <em>predicting</em> the answer.<a contenteditable="false" data-primary="inference" data-secondary="about" data-type="indexterm" id="id574"/> (Later, you’ll also learn about <em>inference</em>, in which the model picks one answer among many and <em>infers</em> that it has picked the correct one.)</p>
</div>

<p>What do you think the answer will be when we ask the model to predict <em>y</em> when <em>x</em> is 10? You might instantly think 19, but that’s not correct. The model will pick a value <em>very close</em> to 19, and there are several reasons for this. First of all, our loss wasn’t 0. It was a very small amount, so we should expect any predicted answer to be off by a very small amount. Second, the neural network is trained on only a small amount of data—and in this case, it’s only six pairs of (<em>x</em>, <em>y</em>) values.</p>

<p>The model only has a single neuron in it,<a contenteditable="false" data-primary="neurons" data-secondary="getting started with ML" data-tertiary="learning a weight and a bias" data-type="indexterm" id="id575"/><a contenteditable="false" data-primary="weights" data-secondary="neurons learning" data-type="indexterm" id="id576"/><a contenteditable="false" data-primary="bias learned by neurons" data-type="indexterm" id="id577"/> and that neuron learns a <em>weight</em> and a <em>bias</em> so that <em>y</em> = <em>wx</em> + <em>b</em>. This looks exactly like the desired <em>y</em> = 2<em>x</em> – 1 relationship, in which we want the model to learn that <em>w</em> = 2 and <em>b</em> = –1. Given that the model was trained on only six items of data, we’d never expect the answer to be exactly these values; instead, we’d expect it to be something very close to them.</p>

<p>Now, run the code for yourself to see what you get. I got 18.991 when I ran it, but your answer may differ slightly because when the neural network is first initialized, there’s a random element: your initial guess will be slightly different from mine and from a third person’s.</p>

<section data-pdf-bookmark="Seeing What the Network Learned" data-type="sect2"><div class="sect2" id="ch01_seeing_what_the_network_learned_1748548870020320">
<h2>Seeing What the Network Learned</h2>

<p>This is obviously a very simple scenario<a contenteditable="false" data-primary="machine learning (ML)" data-secondary="getting started with" data-tertiary="seeing what the network learned" data-type="indexterm" id="id578"/><a contenteditable="false" data-primary="PyTorch" data-secondary="getting started with ML" data-tertiary="seeing what the network learned" data-type="indexterm" id="id579"/><a contenteditable="false" data-primary="neurons" data-secondary="getting started with ML" data-tertiary="learning a weight and a bias" data-type="indexterm" id="id580"/><a contenteditable="false" data-primary="weights" data-secondary="neurons learning" data-type="indexterm" id="id581"/><a contenteditable="false" data-primary="bias learned by neurons" data-type="indexterm" id="id582"/> in which we are matching <em>x’</em>s to <em>y’</em>s in a linear relationship. As mentioned in the previous section, neurons have weight and bias parameters. That makes a single neuron fine for learning a relationship like this; namely, when <em>y</em> = 2<em>x</em> – 1, the weight is 2 and the bias is –1.</p>

<p>With PyTorch, we can actually take a look at the weights and biases that are learned, with code like this:<a contenteditable="false" data-primary="neurons" data-secondary="getting started with ML" data-tertiary="PyTorch displaying weights and bias" data-type="indexterm" id="id583"/><a contenteditable="false" data-primary="weights" data-secondary="neurons learning" data-tertiary="PyTorch displaying weights" data-type="indexterm" id="id584"/><a contenteditable="false" data-primary="bias learned by neurons" data-secondary="PyTorch displaying bias" data-type="indexterm" id="id585"/></p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Access the first (and only) layer in the sequential model</code>
<code class="n">layer</code> <code class="o">=</code> <code class="n">model</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="c1"># Get weights and bias</code>
<code class="n">weights</code> <code class="o">=</code> <code class="n">layer</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
<code class="n">bias</code> <code class="o">=</code> <code class="n">layer</code><code class="o">.</code><code class="n">bias</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Weights:"</code><code class="p">,</code> <code class="n">weights</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Bias:"</code><code class="p">,</code> <code class="n">bias</code><code class="p">)</code></pre>

<p>Once the network finishes learning, you can print out the values (or weights) that the layer learned. In my case, the output was as follows:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">Weights</code><code class="p">:</code> <code class="p">[[</code><code class="mf">1.998695</code><code class="p">]]</code>
<code class="n">Bias</code><code class="p">:</code> <code class="p">[</code><code class="err">–</code><code class="mf">0.9959542</code><code class="p">]</code></pre>

<p>Thus, the learned relationship between <em>x</em> and <em>y</em> was <em>y</em> = 1.998695 <em>x</em> – 0.9959542.</p>

<p>This is pretty close to what we’d expect (<em>y</em> = 2<em>x</em> – 1), and we could argue that it’s even closer to reality because we’re <em>assuming</em> that the relationship will hold for other <span class="keep-together">values!</span><a contenteditable="false" data-primary="" data-startref="ch1mlstrt" data-type="indexterm" id="id586"/><a contenteditable="false" data-primary="" data-startref="ch1mlstrt2" data-type="indexterm" id="id587"/><a contenteditable="false" data-primary="" data-startref="ch1bgn" data-type="indexterm" id="id588"/><a contenteditable="false" data-primary="" data-startref="ch1bgn2" data-type="indexterm" id="id589"/><a contenteditable="false" data-primary="" data-startref="ch1bgn3" data-type="indexterm" id="id590"/></p>
</div></section>
</div></section>

<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch01_summary_1748548870020362">
<h1>Summary</h1>

<p>That’s it for your first “Hello World” of ML. You might be thinking that this seems like massive overkill for something as simple as determining a linear relationship between two values—and you’d be right. But the cool thing about this is that the pattern of code we’ve created here is the same pattern that’s used for far more complex scenarios. You’ll see those scenarios starting in <a data-type="xref" href="ch02.html#ch02_introduction_to_computer_vision_1748548889076080">Chapter 2</a>, where we’ll explore some basic computer vision techniques—in which the machine will learn to “see” patterns in pictures and identify what’s in them!<a contenteditable="false" data-primary="resources online" data-see="online resources" data-type="indexterm" id="id591"/><a contenteditable="false" data-primary="Colab" data-see="Google Colab" data-type="indexterm" id="id592"/><a contenteditable="false" data-primary="ML" data-see="machine learning (ML)" data-type="indexterm" id="id593"/><a contenteditable="false" data-primary="rectified linear unit" data-see="ReLU (rectified linear unit) activation function" data-type="indexterm" id="id594"/><a contenteditable="false" data-primary="Modified National Institute of Standards and Technology database" data-see="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-type="indexterm" id="id595"/><a contenteditable="false" data-primary="filters on pixels" data-see="convolutions" data-type="indexterm" id="id596"/><a contenteditable="false" data-primary="CNNs" data-see="convolutional neural networks (CNNs)" data-type="indexterm" id="id597"/><a contenteditable="false" data-primary="humans versus horses distinguished" data-see="horses versus humans distinguished" data-type="indexterm" id="id598"/><a contenteditable="false" data-primary="clothing" data-see="Fashion MNIST database" data-type="indexterm" id="id599"/><a contenteditable="false" data-primary="models" data-secondary="pretrained" data-see="pretrained models" data-type="indexterm" id="id600"/><a contenteditable="false" data-primary="Inception (Google)" data-see="Google Inception" data-type="indexterm" id="id601"/><a contenteditable="false" data-primary="ETL" data-see="Extract, Transfer, Load (ETL) process for managing data" data-type="indexterm" id="id602"/><a contenteditable="false" data-primary="large language models" data-see="LLMs (large language models)" data-type="indexterm" id="id603"/><a contenteditable="false" data-primary="NLP" data-see="natural language processing (NLP)" data-type="indexterm" id="id604"/><a contenteditable="false" data-primary="RNNs" data-see="recurrent neural networks (RNNs)" data-type="indexterm" id="id605"/><a contenteditable="false" data-primary="language" data-see="natural language processing (NLP)" data-type="indexterm" id="id606"/><a contenteditable="false" data-primary="LSTM" data-see="long short-term memory (LSTM)" data-type="indexterm" id="id607"/><a contenteditable="false" data-primary="LR" data-see="learning rate (LR)" data-type="indexterm" id="id608"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="text generation" data-see="text generator" data-type="indexterm" id="id609"/><a contenteditable="false" data-primary="MSE" data-see="mean squared error (MSE)" data-type="indexterm" id="id610"/><a contenteditable="false" data-primary="MAE" data-see="mean absolute error (MAE)" data-type="indexterm" id="id611"/><a contenteditable="false" data-primary="neural networks" data-secondary="DNN" data-see="deep neural networks (DNNs)" data-type="indexterm" id="id612"/><a contenteditable="false" data-primary="neural networks" data-secondary="RNNs" data-see="recurrent neural networks (RNNs)" data-type="indexterm" id="id613"/><a contenteditable="false" data-primary="sequence data" data-see="time series data" data-type="indexterm" id="id614"/><a contenteditable="false" data-primary="GISS" data-see="Goddard Institute for Space Studies (GISS) weather data" data-type="indexterm" id="id615"/><a contenteditable="false" data-primary="TensorFlow" data-see="Google TensorFlow" data-type="indexterm" id="id616"/><a contenteditable="false" data-primary="serving PyTorch models" data-see="hosting PyTorch models" data-type="indexterm" id="id617"/><a contenteditable="false" data-primary="K vector" data-see="key (K) vector" data-type="indexterm" id="id618"/><a contenteditable="false" data-primary="V vector" data-see="value (V) vector" data-type="indexterm" id="id619"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-see="RAG (retrieval-augmented generation)" data-type="indexterm" id="id620"/><a contenteditable="false" data-primary="APIs" data-secondary="diffusers library" data-see="diffusers library (Hugging Face)" data-type="indexterm" id="id621"/><a contenteditable="false" data-primary="tuning LLMs" data-see="fine-tuning LLMs" data-type="indexterm" id="id622"/><a contenteditable="false" data-primary="tuning generative image models" data-see="fine-tuning generative image models" data-type="indexterm" id="id623"/><a contenteditable="false" data-primary="code for book" data-see="book code online" data-type="indexterm" id="id624"/><a contenteditable="false" data-primary="central processing unit" data-see="CPU (central processing unit)" data-type="indexterm" id="id625"/></p>
</div></section>
</div></section></body></html>