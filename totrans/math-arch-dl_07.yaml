- en: '8 Training neural networks: Forward propagation and backpropagation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 è®­ç»ƒç¥ç»ç½‘ç»œï¼šå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æ¶µç›–
- en: Sigmoid functions as differential surrogates for Heaviside step functions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid å‡½æ•°ä½œä¸º Heaviside æ­¥å‡½æ•°çš„å¾®åˆ†æ›¿ä»£
- en: 'Layering in neural networks: expressing linear layers as matrix-vector multiplication'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä¸­çš„åˆ†å±‚ï¼šå°†çº¿æ€§å±‚è¡¨ç¤ºä¸ºçŸ©é˜µ-å‘é‡ä¹˜æ³•
- en: Regression loss, forward and backward propagation, and their math
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›å½’æŸå¤±ã€å‰å‘å’Œåå‘ä¼ æ’­åŠå…¶æ•°å­¦
- en: So far, we have seen that neural networks make complicated real-life decisions
    by modeling the decision-making process with mathematical functions. These functions
    can become arbitrarily involved, but fortunately, we have a simple building block
    called a *perceptron* that can be repeated systematically to model any arbitrary
    function. We need not even explicitly know the function being modeled in closed
    form. All we need is a reasonably sized set of sample inputs and corresponding
    correct outputs. This collection of input and output pairs is known as *training
    data*. Armed with this training data, we can *train* a multilayer perceptron (MLP,
    aka neural network) to emit reasonably correct outputs on inputs it has never
    seen before.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°ç¥ç»ç½‘ç»œé€šè¿‡ç”¨æ•°å­¦å‡½æ•°å»ºæ¨¡å†³ç­–è¿‡ç¨‹æ¥åšå‡ºå¤æ‚çš„ç°å®ç”Ÿæ´»å†³ç­–ã€‚è¿™äº›å‡½æ•°å¯ä»¥å˜å¾—éå¸¸å¤æ‚ï¼Œä½†å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„æ„å»ºå—ï¼Œç§°ä¸º
    *æ„ŸçŸ¥å™¨*ï¼Œå¯ä»¥ç³»ç»Ÿåœ°é‡å¤ä»¥å»ºæ¨¡ä»»ä½•ä»»æ„å‡½æ•°ã€‚æˆ‘ä»¬ç”šè‡³ä¸éœ€è¦æ˜¾å¼åœ°çŸ¥é“æ­£åœ¨å»ºæ¨¡çš„å‡½æ•°çš„é—­å¼å½¢å¼ã€‚æˆ‘ä»¬éœ€è¦çš„åªæ˜¯ä¸€ä¸ªåˆç†å¤§å°çš„æ ·æœ¬è¾“å…¥é›†å’Œç›¸åº”çš„æ­£ç¡®è¾“å‡ºã€‚è¿™ä¸ªè¾“å…¥å’Œè¾“å‡ºå¯¹çš„é›†åˆè¢«ç§°ä¸º
    *è®­ç»ƒæ•°æ®*ã€‚æœ‰äº†è¿™ä¸ªè®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥ *è®­ç»ƒ* ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼Œä¹Ÿç§°ä¸ºç¥ç»ç½‘ç»œï¼‰ï¼Œä½¿å…¶åœ¨ä»æœªè§è¿‡çš„è¾“å…¥ä¸Šäº§ç”Ÿåˆç†çš„æ­£ç¡®è¾“å‡ºã€‚
- en: Such neural networks, where we need to know the output for each input in the
    training data set, are known as *supervised* neural networks. The correct output
    for the training inputs is typically generated via a manual process called *labeling*.
    Labeling is expensive and time-consuming. Much research is going on toward unsupervised,
    semi-supervised, and self-supervised networks, eliminating or minimizing the labeling
    process. But as of now, the accuracy of unsupervised and self-supervised networks
    in general does not match that of supervised networks. In this chapter, we focus
    on supervised neural networks. In chapter [14](../Text/14.xhtml#ch-ae-vae), we
    will study unsupervised networks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§éœ€è¦æˆ‘ä»¬çŸ¥é“è®­ç»ƒæ•°æ®é›†ä¸­æ¯ä¸ªè¾“å…¥çš„è¾“å‡ºçš„ç¥ç»ç½‘ç»œè¢«ç§°ä¸º *ç›‘ç£* ç¥ç»ç½‘ç»œã€‚è®­ç»ƒè¾“å…¥çš„æ­£ç¡®è¾“å‡ºé€šå¸¸æ˜¯é€šè¿‡ä¸€ç§ç§°ä¸º *æ ‡æ³¨* çš„æ‰‹åŠ¨è¿‡ç¨‹ç”Ÿæˆçš„ã€‚æ ‡æ³¨æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚è®¸å¤šç ”ç©¶æ­£åœ¨è¿›è¡Œä¸­ï¼Œæ—¨åœ¨å¼€å‘æ— ç›‘ç£ã€åŠç›‘ç£å’Œè‡ªç›‘ç£ç½‘ç»œï¼Œä»¥æ¶ˆé™¤æˆ–æœ€å°åŒ–æ ‡æ³¨è¿‡ç¨‹ã€‚ä½†åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ— ç›‘ç£å’Œè‡ªç›‘ç£ç½‘ç»œåœ¨ä¸€èˆ¬æƒ…å†µä¸‹çš„å‡†ç¡®æ€§å¹¶ä¸åŒ¹é…ç›‘ç£ç½‘ç»œã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºç›‘ç£ç¥ç»ç½‘ç»œã€‚åœ¨ç¬¬
    [14](../Text/14.xhtml#ch-ae-vae) ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶æ— ç›‘ç£ç½‘ç»œã€‚
- en: What is this process of â€œtrainingâ€ a neural network? It essentially estimates
    the parameter values that would make the network emit output values as close as
    possible to the known correct outputs on the training inputs. In this chapter,
    we discuss how this is done. But before that, we have to learn a few other things.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªâ€œè®­ç»ƒâ€ç¥ç»ç½‘ç»œçš„è¿‡ç¨‹æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæœ¬è´¨ä¸Šä¼°è®¡äº†ä½¿ç½‘ç»œåœ¨è®­ç»ƒè¾“å…¥ä¸Šå‘å‡ºå°½å¯èƒ½æ¥è¿‘å·²çŸ¥æ­£ç¡®è¾“å‡ºçš„å‚æ•°å€¼ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºè¿™æ˜¯å¦‚ä½•å®Œæˆçš„ã€‚ä½†åœ¨é‚£ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»å­¦ä¹ ä¸€äº›å…¶ä»–çš„ä¸œè¥¿ã€‚
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng.bz/YAXa](http://mng.bz/YAXa)
    in the form of fully functional and executable Jupyter notebooks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæœ¬ç« çš„å®Œæ•´ PyTorch ä»£ç ä»¥å®Œå…¨åŠŸèƒ½æ€§å’Œå¯æ‰§è¡Œçš„ Jupyter ç¬”è®°æœ¬å½¢å¼ï¼Œå¯åœ¨ [http://mng.bz/YAXa](http://mng.bz/YAXa)
    è·å–ã€‚
- en: 8.1 Differentiable step-like functions
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 å¯å¾®çš„ç±»ä¼¼æ­¥éª¤å‡½æ•°
- en: In equation [7.3](../Text/07.xhtml#eq-perceptron), we expressed the perceptron
    as a combination of a Heaviside step function *Ï•* and an affine transformation
    ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*. This
    is the perceptron we used throughout chapter [7](../Text/07.xhtml#ch-func-approx)
    and with which we were able to express (model) pretty much all functions of interest.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–¹ç¨‹ [7.3](../Text/07.xhtml#eq-perceptron) ä¸­ï¼Œæˆ‘ä»¬å°†æ„ŸçŸ¥å™¨è¡¨ç¤ºä¸º Heaviside æ­¥å‡½æ•° *Ï•* å’Œä»¿å°„å˜æ¢
    ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b* çš„ç»„åˆã€‚è¿™æ˜¯æˆ‘ä»¬è´¯ç©¿ç¬¬
    [7](../Text/07.xhtml#ch-func-approx) ç« æ‰€ä½¿ç”¨çš„æ„ŸçŸ¥å™¨ï¼Œå¹¶ä¸”æˆ‘ä»¬èƒ½å¤Ÿè¡¨è¾¾ï¼ˆå»ºæ¨¡ï¼‰å‡ ä¹æ‰€æœ‰æ„Ÿå…´è¶£çš„åŠŸèƒ½ã€‚
- en: 'Despite its expressive power, the Heaviside step function has a drawback: it
    has a discontinuity at *x* = 0 and is *not differentiable*. Why is differentiability
    important? As we shall see in chapter [8](../Text/08.xhtml#ch-training-neural-networks)
    (and got a glimpse of in section [3.3](../Text/03.xhtml#sec-grad)), optimal training
    of a neural network requires evaluation of the gradient vector of a loss function
    with respect to weights. Since the gradient is nothing but a vector of partial
    derivatives, differentiability is needed for training.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡Heavisideæ­¥è¿›å‡½æ•°å…·æœ‰å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†å®ƒæœ‰ä¸€ä¸ªç¼ºç‚¹ï¼šå®ƒåœ¨ *x* = 0 å¤„æœ‰é—´æ–­ï¼Œå¹¶ä¸”ä¸å¯å¾®åˆ†ã€‚ä¸ºä»€ä¹ˆå¯å¾®æ€§å¾ˆé‡è¦ï¼Ÿæ­£å¦‚æˆ‘ä»¬å°†åœ¨ç¬¬ [8](../Text/08.xhtml#ch-training-neural-networks)
    ç« ï¼ˆå¹¶åœ¨ç¬¬ [3.3](../Text/03.xhtml#sec-grad) èŠ‚ä¸­ç•¥è§ä¸€æ–‘ï¼‰ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œç¥ç»ç½‘ç»œçš„ä¼˜åŒ–è®­ç»ƒéœ€è¦è¯„ä¼°æŸå¤±å‡½æ•°ç›¸å¯¹äºæƒé‡çš„æ¢¯åº¦å‘é‡ã€‚ç”±äºæ¢¯åº¦å®é™…ä¸Šæ˜¯ä¸€ä¸ªåå¯¼æ•°çš„å‘é‡ï¼Œå› æ­¤å¯å¾®æ€§å¯¹äºè®­ç»ƒæ˜¯å¿…éœ€çš„ã€‚
- en: In this section, we discuss a few functions that are differentiable and yet
    can mimic the Heaviside step function. The most significant among them is the
    sigmoid function.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºä¸€äº›å¯å¾®åˆ†çš„å‡½æ•°ï¼ŒåŒæ—¶å¯ä»¥æ¨¡æ‹Ÿ Heaviside æ­¥è¿›å‡½æ•°ã€‚å…¶ä¸­æœ€é‡è¦çš„æ˜¯ S å½¢å‡½æ•°ã€‚
- en: 8.1.1 Sigmoid function
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 S å½¢å‡½æ•°
- en: The sigmoid function is named after its characteristic S-shaped curve figure
    [8.1](../Text/08.xhtml#fig-sigmoid1d)). The corresponding equation is
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: S å½¢å‡½æ•°ä»¥å…¶ç‰¹å¾çš„ S å½¢æ›²çº¿å›¾è€Œå‘½å [8.1](../Text/08.xhtml#fig-sigmoid1d))ã€‚ç›¸åº”çš„æ–¹ç¨‹æ˜¯
- en: '![](../../OEBPS/Images/eq_08-01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-01.png)'
- en: Equation 8.1
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.1
- en: '![](../../OEBPS/Images/CH08_F01_Chaudhury.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/CH08_F01_Chaudhury.png)'
- en: Figure 8.1 Graph of a 1D sigmoid function
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.1 1D S å½¢å‡½æ•°çš„å›¾åƒ
- en: Parameterized sigmoid function
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°åŒ– S å½¢å‡½æ•°
- en: We can parametrize equation [1.5](../Text/01.xhtml#eq-sigmoid) as
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†æ–¹ç¨‹ [1.5](../Text/01.xhtml#eq-sigmoid) å‚æ•°åŒ–ä¸º
- en: '![](../../OEBPS/Images/eq_08-02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-02.png)'
- en: Equation 8.2
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.2
- en: This allows us to
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿ
- en: Adjust the steepness of the linear portion of the S curve by changing *w*
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æ”¹å˜ *w* è°ƒæ•´ S æ›²çº¿çº¿æ€§éƒ¨åˆ†çš„é™¡å³­ç¨‹åº¦
- en: Adjust the position of the curve by changing *b*
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æ”¹å˜ *b* è°ƒæ•´æ›²çº¿çš„ä½ç½®
- en: Figure [8.2](#fig-sigmoid-parameterized) shows how the parametrized sigmoid
    curve changes with different values for the parameters *w* and *b*. In particular,
    note that for large values of *w*, the parameterized sigmoid is virtually indistinguishable
    from the Heaviside step function (compare the dotted curve in figure [8.2a](#fig-sigmoid-parameterized-various-ws)
    with figure [7.7](../Text/07.xhtml#fig-step-1D)), even though it remains differentiable.
    This is exactly what we desire in neural networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ [8.2](#fig-sigmoid-parameterized) å±•ç¤ºäº†å‚æ•°åŒ–çš„ S å½¢æ›²çº¿å¦‚ä½•éšç€å‚æ•° *w* å’Œ *b* çš„ä¸åŒå€¼è€Œå˜åŒ–ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯·æ³¨æ„ï¼Œå¯¹äºå¤§çš„
    *w* å€¼ï¼Œå‚æ•°åŒ–çš„ S å½¢å‡½æ•°å‡ ä¹ä¸ Heaviside æ­¥è¿›å‡½æ•°æ— æ³•åŒºåˆ†ï¼ˆæ¯”è¾ƒå›¾ [8.2a](#fig-sigmoid-parameterized-various-ws)
    ä¸­çš„è™šçº¿æ›²çº¿ä¸å›¾ [7.7](../Text/07.xhtml#fig-step-1D)ï¼‰ï¼Œå°½ç®¡å®ƒä»ç„¶æ˜¯å¯å¾®åˆ†çš„ã€‚è¿™æ­£æ˜¯æˆ‘ä»¬åœ¨ç¥ç»ç½‘ç»œä¸­æ‰€æœŸæœ›çš„ã€‚
- en: '![](../../OEBPS/Images/CH08_F02_Chaudhury.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/CH08_F02_Chaudhury.png)'
- en: Figure 8.2 Sigmoid curves corresponding to various parameter values in equation
    [8.2](../Text/08.xhtml#eq-sigmoid-parameterized)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.2 æ–¹ç¨‹ [8.2](../Text/08.xhtml#eq-sigmoid-parameterized) ä¸­å¯¹åº”äºå„ç§å‚æ•°å€¼çš„ S å½¢æ›²çº¿
- en: Some properties of the sigmoid function
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: S å½¢å‡½æ•°çš„ä¸€äº›æ€§è´¨
- en: The sigmoid function has several interesting properties, some of which are listed
    here with proof outlines.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: S å½¢å‡½æ•°æœ‰å‡ ä¸ªæœ‰è¶£çš„æ€§è´¨ï¼Œå…¶ä¸­ä¸€äº›æ€§è´¨åœ¨æ­¤åˆ—å‡ºï¼Œå¹¶é™„æœ‰è¯æ˜æ¦‚è¦ã€‚
- en: '*Expression with positive x*:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ­£ x çš„è¡¨è¾¾å¼*ï¼š'
- en: '![](../../OEBPS/Images/eq_08-03.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-03.png)'
- en: Equation 8.3
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.3
- en: This expression can be easily proved by multiplying both the numerator and denominator
    of equation [1.5](../Text/01.xhtml#eq-sigmoid) by *e[x]*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†æ–¹ç¨‹ [1.5](../Text/01.xhtml#eq-sigmoid) çš„åˆ†å­å’Œåˆ†æ¯éƒ½ä¹˜ä»¥ *e[x]*ï¼Œå¯ä»¥è½»æ¾è¯æ˜è¿™ä¸ªè¡¨è¾¾å¼ã€‚
- en: '*Sigmoid of negative x*:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è´Ÿ x çš„ S å½¢å‡½æ•°*ï¼š'
- en: '![](../../OEBPS/Images/eq_08-04.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-04.png)'
- en: Equation 8.4
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.4
- en: '*Derivative of sigmoid*:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S å½¢å‡½æ•°çš„å¯¼æ•°*ï¼š'
- en: '![](../../OEBPS/Images/eq_08-05.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-05.png)'
- en: Equation 8.5
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.5
- en: Figure [8.3](#fig-sigmoid1d-with-derivative) shows the graph of the derivative
    of the sigmoid superimposed on the sigmoid graph itself. As expected, the derivative
    has its maximum value at the middle of the sigmoid curve (where the sigmoid is
    climbing more or less linearly) and is near zero at both ends (where the sigmoid
    is saturated and flat, hardly changing).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ [8.3](#fig-sigmoid1d-with-derivative) æ˜¾ç¤ºäº† S å½¢å‡½æ•°çš„å¯¼æ•°å›¾åƒå åŠ åœ¨ S å½¢å‡½æ•°å›¾åƒæœ¬èº«ä¸Šã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œå¯¼æ•°åœ¨
    S å½¢æ›²çº¿çš„ä¸­é—´ï¼ˆS å½¢å‡½æ•°å¤§è‡´çº¿æ€§ä¸Šå‡çš„åœ°æ–¹ï¼‰è¾¾åˆ°æœ€å¤§å€¼ï¼Œåœ¨ä¸¤ç«¯ï¼ˆS å½¢å‡½æ•°é¥±å’Œå’Œå¹³å¦ï¼Œå‡ ä¹ä¸å˜åŒ–çš„åœ°æ–¹ï¼‰æ¥è¿‘é›¶ã€‚
- en: '![](../../OEBPS/Images/CH08_F03_Chaudhury.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/CH08_F03_Chaudhury.png)'
- en: Figure 8.3 Graph of a 1D sigmoid function (solid curve) and its derivative (dashed
    curve)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.3 1D sigmoid å‡½æ•°ï¼ˆå®çº¿ï¼‰åŠå…¶å¯¼æ•°ï¼ˆè™šçº¿ï¼‰çš„å›¾
- en: 8.1.2 Tanh function
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 åŒæ›²æ­£åˆ‡å‡½æ•°
- en: '![](../../OEBPS/Images/CH08_F04_Chaudhury.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾åƒ](../../OEBPS/Images/CH08_F04_Chaudhury.png)'
- en: Figure 8.4 Graph of a 1D tanh function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.4 1D tanh å‡½æ•°çš„å›¾
- en: An alternative to the sigmoid function is the *hyperbolic tangent* tanh function,
    shown in figure [8.4](#fig-tanh1d). It is very similar to the sigmoid function,
    but the range of output values is from [âˆ’1,1] as opposed to [0,1]. In essence,
    it is the sigmoid function stretched and shifted so it is centered around 0\.
    The equation of the tanh function is given by
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid å‡½æ•°çš„å¦ä¸€ç§é€‰æ‹©æ˜¯*åŒæ›²æ­£åˆ‡*å‡½æ•° tanhï¼Œå¦‚å›¾[8.4](#fig-tanh1d)æ‰€ç¤ºã€‚å®ƒä¸ sigmoid å‡½æ•°éå¸¸ç›¸ä¼¼ï¼Œä½†è¾“å‡ºå€¼çš„èŒƒå›´æ˜¯ä»[âˆ’1,1]ï¼Œè€Œä¸æ˜¯[0,1]ã€‚æœ¬è´¨ä¸Šï¼Œå®ƒæ˜¯å°†
    sigmoid å‡½æ•°æ‹‰ä¼¸å¹¶å¹³ç§»ï¼Œä½¿å…¶ä»¥0ä¸ºä¸­å¿ƒã€‚tanh å‡½æ•°çš„æ–¹ç¨‹å¼å¦‚ä¸‹
- en: '![](../../OEBPS/Images/eq_08-06.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾åƒ](../../OEBPS/Images/eq_08-06.png)'
- en: Equation 8.6
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 8.6
- en: 'Why is tanh preferred over sigmoid? To understand this, consider figure [8.5](#fig-derivative-sigmoid-tanh-1d).
    It compares the derivatives of the sigmoid and tanh functions. As the plot shows,
    the derivative (gradient) of the function near *x* = 0 is much higher for tanh
    than for sigmoid. Stronger gradients mean faster convergence, as the weight updates
    happen in larger steps. Note that this holds mainly when the data is centered
    around 0: in most preprocessing steps, we standardize the data (make it 0 mean)
    before feeding it into the neural network.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆ tanh æ¯” sigmoid æ›´å—æ¬¢è¿ï¼Ÿä¸ºäº†ç†è§£è¿™ä¸€ç‚¹ï¼Œè¯·è€ƒè™‘å›¾[8.5](#fig-derivative-sigmoid-tanh-1d)ã€‚å®ƒæ¯”è¾ƒäº†
    sigmoid å’Œ tanh å‡½æ•°çš„å¯¼æ•°ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œå‡½æ•°åœ¨*x* = 0é™„è¿‘çš„å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰æ¯” sigmoid å‡½æ•°çš„å¯¼æ•°è¦é«˜å¾—å¤šã€‚æ›´å¼ºçš„æ¢¯åº¦æ„å‘³ç€æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸ºæƒé‡æ›´æ–°æ˜¯ä»¥æ›´å¤§çš„æ­¥éª¤å‘ç”Ÿçš„ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸»è¦é€‚ç”¨äºæ•°æ®é›†ä¸­åœ¨0é™„è¿‘çš„æƒ…å†µï¼šåœ¨å¤§å¤šæ•°é¢„å¤„ç†æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬åœ¨å°†å…¶è¾“å…¥ç¥ç»ç½‘ç»œä¹‹å‰å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆä½¿å…¶å‡å€¼ä¸º0ï¼‰ã€‚
- en: '![](../../OEBPS/Images/CH08_F05_Chaudhury.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾åƒ](../../OEBPS/Images/CH08_F05_Chaudhury.png)'
- en: Figure 8.5 Graph of the derivatives of 1D sigmoid and tanh functions
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.5 1D sigmoid å’Œ tanh å‡½æ•°çš„å¯¼æ•°å›¾
- en: 8.2 Why layering?
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 ä¸ºä»€ä¹ˆåˆ†å±‚ï¼Ÿ
- en: In section [7.5](../Text/07.xhtml#sec-layering), we encountered the idea of
    *layering* as the preferred way to organize multiple perceptrons. The main property
    of a layered network is that neurons in any layer take their input only from the
    outputs of the preceding layer. This means connections exist only between successive
    layers. No other connection exists in the MLP, which greatly simplifies the evaluation
    and training of the network, which will become apparent as we discuss forward
    propagation and backpropagation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[7.5](../Text/07.xhtml#sec-layering)èŠ‚ä¸­ï¼Œæˆ‘ä»¬é‡åˆ°äº†å°†*åˆ†å±‚*ä½œä¸ºç»„ç»‡å¤šä¸ªæ„ŸçŸ¥å™¨é¦–é€‰æ–¹æ³•çš„æƒ³æ³•ã€‚åˆ†å±‚ç½‘ç»œçš„ä¸»è¦ç‰¹æ€§æ˜¯ä»»ä½•å±‚çš„ç¥ç»å…ƒåªä»å‰ä¸€å±‚çš„è¾“å‡ºä¸­è·å–è¾“å…¥ã€‚è¿™æ„å‘³ç€åªå­˜åœ¨è¿ç»­å±‚ä¹‹é—´çš„è¿æ¥ã€‚åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸­ä¸å­˜åœ¨å…¶ä»–è¿æ¥ï¼Œè¿™æå¤§åœ°ç®€åŒ–äº†ç½‘ç»œçš„è¯„ä¼°å’Œè®­ç»ƒï¼Œè¿™ä¸€ç‚¹åœ¨æˆ‘ä»¬è®¨è®ºå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ—¶å°†å˜å¾—æ˜æ˜¾ã€‚
- en: Why have layers at all? We have seen that multiple perceptrons allow us to model
    problems that cannot be solved by a single perceptron (such as the XOR problem
    discussed in section [7.4.1](../Text/07.xhtml#sec-mlp-xor)). In theory, it is
    possible to model all mathematical functions (and hence solve all quantifiable
    problems) with neurons organized in a single hidden layer (see Cybenkoâ€™s theorem
    and proof in section [7.5.3](../Text/07.xhtml#sec-cybenko-uat)). However, that
    does not mean a single hidden layer is the most *efficient* way of doing all modelings.
    We can often model complicated problems with *fewer* perceptrons if we organize
    them in more than one layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦æœ‰å±‚ï¼Ÿæˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œå¤šä¸ªæ„ŸçŸ¥å™¨ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¨¡æ‹Ÿå•ä¸€ç”Ÿæˆå™¨æ— æ³•è§£å†³çš„é—®é¢˜ï¼ˆä¾‹å¦‚ç¬¬[7.4.1](../Text/07.xhtml#sec-mlp-xor)èŠ‚ä¸­è®¨è®ºçš„
    XOR é—®é¢˜ï¼‰ã€‚ä»ç†è®ºä¸Šè®²ï¼Œä½¿ç”¨ç»„ç»‡åœ¨å•ä¸ªéšè—å±‚ä¸­çš„ç¥ç»å…ƒå¯ä»¥æ¨¡æ‹Ÿæ‰€æœ‰æ•°å­¦å‡½æ•°ï¼ˆå› æ­¤è§£å†³æ‰€æœ‰å¯é‡åŒ–çš„é—®é¢˜ï¼‰ï¼ˆå‚è§ç¬¬[7.5.3](../Text/07.xhtml#sec-cybenko-uat)èŠ‚ä¸­
    Cybenko çš„å®šç†å’Œè¯æ˜ï¼‰ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ„å‘³ç€å•ä¸ªéšè—å±‚æ˜¯è¿›è¡Œæ‰€æœ‰å»ºæ¨¡çš„æœ€*æœ‰æ•ˆ*æ–¹å¼ã€‚å¦‚æœæˆ‘ä»¬å°†å®ƒä»¬ç»„ç»‡åœ¨å¤šä¸ªå±‚ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥ç”¨æ›´å°‘çš„æ„ŸçŸ¥å™¨æ¥æ¨¡æ‹Ÿå¤æ‚é—®é¢˜ã€‚
- en: Why do extra layers help? The primary reason is the extra nonlinearity. Each
    layer brings in its own nonlinear (such as sigmoid) function. Nonlinear functions,
    with proper parametrization, can model more complicated functions. Hence, a larger
    count of nonlinear functions in the model typically implies greater expressive
    power.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆé¢å¤–çš„å±‚æœ‰å¸®åŠ©ï¼Ÿä¸»è¦åŸå› æ˜¯é¢å¤–çš„éçº¿æ€§ã€‚æ¯ä¸€å±‚éƒ½å¼•å…¥å®ƒè‡ªå·±çš„éçº¿æ€§å‡½æ•°ï¼ˆä¾‹å¦‚ sigmoidï¼‰ã€‚é€šè¿‡é€‚å½“çš„å‚æ•°åŒ–ï¼Œéçº¿æ€§å‡½æ•°å¯ä»¥æ¨¡æ‹Ÿæ›´å¤æ‚çš„å‡½æ•°ã€‚å› æ­¤ï¼Œæ¨¡å‹ä¸­éçº¿æ€§å‡½æ•°çš„æ•°é‡è¶Šå¤šï¼Œé€šå¸¸æ„å‘³ç€è¡¨è¾¾åŠ›è¶Šå¼ºã€‚
- en: 8.3 Linear layers
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 çº¿æ€§å±‚
- en: 'Various types of layers are used in popular neural network architectures. In
    subsequent chapters, we shall look at different kinds of layers, such as convolution
    layers. But in this section, we examine the simplest and most basic type of layer:
    the *linear* layer. Here every perceptron from the previous layer is connected
    to every perceptron in the next layer. Such a layer is also known as *fully connected*
    layer. Thus if the previous layer has *m* neurons and the next layer has *n* neurons,
    there are *mn* connections, each with its own weight.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æµè¡Œçš„ç¥ç»ç½‘ç»œæ¶æ„ä¸­ä½¿ç”¨äº†å„ç§ç±»å‹çš„å±‚ã€‚åœ¨éšåçš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸åŒç±»å‹çš„å±‚ï¼Œä¾‹å¦‚å·ç§¯å±‚ã€‚ä½†åœ¨è¿™ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬æ£€æŸ¥æœ€ç®€å•å’Œæœ€åŸºæœ¬çš„å±‚ç±»å‹ï¼š*çº¿æ€§å±‚*ã€‚åœ¨è¿™é‡Œï¼Œå‰ä¸€å±‚çš„æ¯ä¸ªæ„ŸçŸ¥å™¨éƒ½ä¸ä¸‹ä¸€å±‚çš„æ¯ä¸ªæ„ŸçŸ¥å™¨ç›¸è¿ã€‚è¿™æ ·çš„å±‚ä¹Ÿè¢«ç§°ä¸º*å…¨è¿æ¥å±‚*ã€‚å› æ­¤ï¼Œå¦‚æœå‰ä¸€å±‚æœ‰
    *m* ä¸ªç¥ç»å…ƒï¼Œä¸‹ä¸€å±‚æœ‰ *n* ä¸ªç¥ç»å…ƒï¼Œé‚£ä¹ˆå°±æœ‰ *mn* ä¸ªè¿æ¥ï¼Œæ¯ä¸ªè¿æ¥éƒ½æœ‰è‡ªå·±çš„æƒé‡ã€‚
- en: NOTE We use the words *neuron* and *perceptron* interchangeably.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæˆ‘ä»¬äº¤æ›¿ä½¿ç”¨â€œç¥ç»å…ƒâ€å’Œâ€œæ„ŸçŸ¥å™¨â€è¿™ä¸¤ä¸ªè¯ã€‚
- en: Figure [8.6](#fig-linear-layer) shows a linear layer that is a slice of a bigger
    MLP. Figure [8.7](#fig-full-MLP-with-linear-layer) shows a bigger MLP with a linear
    layer. Consistent with previous chapters, we have used superscripts for layer
    IDs and subscripts for source and destination IDs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[8.6](#fig-linear-layer)æ˜¾ç¤ºäº†ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå®ƒæ˜¯æ›´å¤§çš„MLPçš„ä¸€éƒ¨åˆ†ã€‚å›¾[8.7](#fig-full-MLP-with-linear-layer)æ˜¾ç¤ºäº†ä¸€ä¸ªå¸¦æœ‰çº¿æ€§å±‚çš„æ›´å¤§çš„MLPã€‚ä¸å‰é¢çš„ç« èŠ‚ä¸€è‡´ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸Šæ ‡è¡¨ç¤ºå±‚IDï¼Œä¸‹æ ‡è¡¨ç¤ºæºå’Œç›®æ ‡IDã€‚
- en: 'The weight of the connection from the *k*th neuron in layer (*l* âˆ’ 1) to the
    *j*th neuron in layer *l* is denoted *w[jk]*^((*l*)). Here the subscript ordering
    is the destination (*j*) followed by the source (*k*). This is slightly counterintuitive
    but universally followed because it simplifies the matrix notation (described
    shortly). Note the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å±‚ (*l* âˆ’ 1) çš„ç¬¬ *k* ä¸ªç¥ç»å…ƒåˆ°å±‚ *l* çš„ç¬¬ *j* ä¸ªç¥ç»å…ƒçš„è¿æ¥æƒé‡è¡¨ç¤ºä¸º *w[jk]*^((*l*)ï¼‰ã€‚è¿™é‡Œä¸‹æ ‡é¡ºåºæ˜¯ç›®æ ‡
    (*j*) åè·Ÿæº (*k*ï¼‰ã€‚è¿™ç¨å¾®æœ‰äº›åç›´è§‰ï¼Œä½†æ™®ééµå¾ªï¼Œå› ä¸ºå®ƒç®€åŒ–äº†çŸ©é˜µè¡¨ç¤ºï¼ˆç¨åæè¿°ï¼‰ã€‚æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
- en: We have split a single perceptron (weighted sum followed by sigmoid) into two
    separate layers, weighted sum and sigmoid.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å•ä¸ªæ„ŸçŸ¥å™¨ï¼ˆåŠ æƒæ±‚å’Œåè·Ÿsigmoidï¼‰æ‹†åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å±‚ï¼ŒåŠ æƒæ±‚å’Œå’Œsigmoidã€‚
- en: We have used sigmoid instead of Heaviside as the nonlinear function.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨sigmoidå‡½æ•°è€Œä¸æ˜¯Heavisideå‡½æ•°ä½œä¸ºéçº¿æ€§å‡½æ•°ã€‚
- en: 8.3.1 Linear layers expressed as matrix-vector multiplication
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 çº¿æ€§å±‚è¡¨ç¤ºä¸ºçŸ©é˜µ-å‘é‡ä¹˜æ³•
- en: 'Letâ€™s revisit the perceptron in the context of the MLP. As we saw in equation
    [7.3](../Text/07.xhtml#eq-perceptron), a single perceptron takes a weighted sum
    of its inputs and then performs a step function on the result. In an MLP, the
    inputs to any perceptron in the *l*th layer come from the previous layer: the
    (*l* âˆ’ 1)th layer.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨MLPçš„èƒŒæ™¯ä¸‹å›é¡¾ä¸€ä¸‹æ„ŸçŸ¥å™¨ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æ–¹ç¨‹[7.3](../Text/07.xhtml#eq-perceptron)ä¸­çœ‹åˆ°çš„ï¼Œå•ä¸ªæ„ŸçŸ¥å™¨å¯¹å…¶è¾“å…¥è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œç„¶åå¯¹ç»“æœæ‰§è¡Œé˜¶è·ƒå‡½æ•°ã€‚åœ¨MLPä¸­ï¼Œç¬¬
    *l* å±‚ä¸­ä»»ä½•æ„ŸçŸ¥å™¨çš„è¾“å…¥æ¥è‡ªå‰ä¸€å±‚ï¼š(*l* âˆ’ 1)å±‚ã€‚
- en: '![](../../OEBPS/Images/CH08_F06_Chaudhury.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/CH08_F06_Chaudhury.png)'
- en: Figure 8.6 Linear layer outputting layer *l* from layer (*l* âˆ’ 1). The weights
    belonging to row 1 of the weight matrix (coming from all the input neurons, layer
    (*l* âˆ’ 1), which sum together to form output neuron 1) are shown in bold.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.6 çº¿æ€§å±‚è¾“å‡ºå±‚ *l* æ¥è‡ªå±‚ (*l* âˆ’ 1)ã€‚æƒé‡çŸ©é˜µçš„ç¬¬1è¡Œï¼ˆæ¥è‡ªæ‰€æœ‰è¾“å…¥ç¥ç»å…ƒï¼Œå±‚ (*l* âˆ’ 1)ï¼Œå®ƒä»¬ç›¸åŠ å½¢æˆè¾“å‡ºç¥ç»å…ƒ1ï¼‰çš„æƒé‡ä»¥ç²—ä½“æ˜¾ç¤ºã€‚
- en: '![](../../OEBPS/Images/CH08_F07_Chaudhury.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/CH08_F07_Chaudhury.png)'
- en: 'Figure 8.7 Multilayered neural networks: This is a complete deep neural network,
    a slice of which is shown in figure [8.6](#fig-linear-layer).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.7 å¤šå±‚ç¥ç»ç½‘ç»œï¼šè¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­ä¸€éƒ¨åˆ†å¦‚å›¾[8.6](#fig-linear-layer)æ‰€ç¤ºã€‚
- en: 'Let *a*[0]^((*l* âˆ’ 1)), *a*[1]^((*l* âˆ’ 1)), â‹¯, *a[m]*^((*l* âˆ’ 1)) denote the
    outputs of the *m* neurons in layer (*l* âˆ’ 1) (the left-most input column of nodes
    in figure [8.6](#fig-linear-layer)). And let *a*[0]^((*l*)), *a*[1]^((*l*)), â‹¯,
    *a[n]*^((*l*)) denote the outputs of the *n* neurons in layer *l*. Note that we
    typically use the symbol *a*, standing for activation, to denote the output of
    individual neurons. Now consider the *j*th neuron in layer *l*. For instance,
    check *z*[1]^((*l*)) in figure [8.6](#fig-linear-layer): note the weights going
    into it and the activations at their source. Its output is *a[j]*^((*l*)), where'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤ *a*[0]^((*l* âˆ’ 1))ï¼Œ*a*[1]^((*l* âˆ’ 1))ï¼Œâ‹¯ï¼Œ*a[m]*^((*l* âˆ’ 1)) è¡¨ç¤ºå±‚ (*l* âˆ’ 1) ä¸­
    *m* ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼ˆå›¾[8.6](#fig-linear-layer)ä¸­èŠ‚ç‚¹çš„æœ€å·¦ä¾§è¾“å…¥åˆ—ï¼‰ã€‚ä»¤ *a*[0]^((*l*))ï¼Œ*a*[1]^((*l*))ï¼Œâ‹¯ï¼Œ*a[n]*^((*l*)))
    è¡¨ç¤ºå±‚ *l* ä¸­ *n* ä¸ªç¥ç»å…ƒçš„è¾“å‡ºã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ç¬¦å· *a*ï¼Œä»£è¡¨æ¿€æ´»ï¼Œæ¥è¡¨ç¤ºå•ä¸ªç¥ç»å…ƒçš„è¾“å‡ºã€‚ç°åœ¨è€ƒè™‘å±‚ *l* ä¸­çš„ç¬¬ *j* ä¸ªç¥ç»å…ƒã€‚ä¾‹å¦‚ï¼Œæ£€æŸ¥å›¾[8.6](#fig-linear-layer)ä¸­çš„
    *z*[1]^((*l*))ï¼šæ³¨æ„è¿›å…¥å®ƒçš„æƒé‡å’Œå®ƒä»¬çš„æºæ¿€æ´»ã€‚å…¶è¾“å‡ºæ˜¯ *a[j]*^((*l*))ï¼Œå…¶ä¸­
- en: '![](../../OEBPS/Images/eq_08-06-a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-06-a.png)'
- en: 'We can rewrite the summation in these equations as a dot product between the
    weight and activation vectors:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™äº›æ–¹ç¨‹ä¸­çš„æ±‚å’Œé‡å†™ä¸ºæƒé‡å‘é‡å’Œæ¿€æ´»å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ï¼š
- en: '![](../../OEBPS/Images/eq_08-06-b.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 8.6-b](../../OEBPS/Images/eq_08-06-b.png)'
- en: The complete set of equations for all *j*s together can be written in a super-compact
    way using matrix-vector multiplication,
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ *j* çš„å®Œæ•´æ–¹ç¨‹é›†å¯ä»¥ç”¨çŸ©é˜µ-å‘é‡ä¹˜æ³•ä»¥è¶…çº§ç´§å‡‘çš„æ–¹å¼å†™å‡ºï¼Œ
- en: '![](../../OEBPS/Images/eq_08-07.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 8.7](../../OEBPS/Images/eq_08-07.png)'
- en: Equation 8.7
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 8.7
- en: where
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '*W*^((*l*)) is an *n* Ã— *m* matrix representing the weights of *all connections
    from layer *l* âˆ’ 1 to layer l*:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*^((*l*)) æ˜¯ä¸€ä¸ª *n* Ã— *m* çŸ©é˜µï¼Œè¡¨ç¤ºä»å±‚ *l* âˆ’ 1 åˆ°å±‚ l* çš„ *æ‰€æœ‰è¿æ¥* çš„æƒé‡ï¼š'
- en: '![](../../OEBPS/Images/eq_08-08.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 8.8](../../OEBPS/Images/eq_08-08.png)'
- en: Equation 8.8
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 8.8
- en: '![](../../OEBPS/Images/AR_a.png)^((*l*)) represents the activations for the
    entire layer *l*. Applying the sigmoid function to a vector is equivalent to applying
    it individually to each element of the vector:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_a.png)^((*l*)) è¡¨ç¤ºå±‚ *l* çš„æ¿€æ´»ã€‚å°† Sigmoid å‡½æ•°åº”ç”¨äºå‘é‡ç­‰åŒäºå°†å…¶é€ä¸ªåº”ç”¨äºå‘é‡çš„æ¯ä¸ªå…ƒç´ ï¼š'
- en: '![](../../OEBPS/Images/eq_08-08-a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 8.8-a](../../OEBPS/Images/eq_08-08-a.png)'
- en: The matrix-vector notation saves us from dealing with subscripts by working
    with all the weights, biases, activations, and so on in a *global fashion*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µ-å‘é‡è¡¨ç¤ºæ³•é€šè¿‡ä»¥å…¨å±€æ–¹å¼å¤„ç†æ‰€æœ‰æƒé‡ã€åç½®ã€æ¿€æ´»ç­‰ï¼Œä½¿æˆ‘ä»¬å…äºå¤„ç†ä¸‹æ ‡ã€‚
- en: 8.3.2 Forward propagation and grand output functions for an MLP of linear layers
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 çº¿æ€§å±‚ MLP çš„æ­£å‘ä¼ æ’­å’Œå…¨å±€è¾“å‡ºå‡½æ•°
- en: 'Equation [8.7](../Text/08.xhtml#eq-linlayer-forwardprop) describes the forward
    propagation of a single linear layer. The final output of an MLP with fully connected
    (aka linear) layers 0â‹¯*L* on input ![](../../OEBPS/Images/AR_x.png) can be obtained
    by repeated application of this equation:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ [8.7](../Text/08.xhtml#eq-linlayer-forwardprop) æè¿°äº†å•ä¸ªçº¿æ€§å±‚çš„æ­£å‘ä¼ æ’­ã€‚å…·æœ‰å…¨è¿æ¥ï¼ˆå³çº¿æ€§ï¼‰å±‚
    0â‹¯*L* çš„ MLP åœ¨è¾“å…¥ ![](../../OEBPS/Images/AR_x.png) ä¸Šçš„æœ€ç»ˆè¾“å‡ºå¯ä»¥é€šè¿‡é‡å¤åº”ç”¨æ­¤æ–¹ç¨‹è·å¾—ï¼š
- en: '*MLP*(![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_a.png)^((*L*))
    = *y* = *Ïƒ*(*W*^((*L*))â€¦*Ïƒ*(*W*^((1))*Ïƒ*(*W*^((0))![](../../OEBPS/Images/AR_x.png)
    + ![](../../OEBPS/Images/AR_b.png)^((0)))+![](../../OEBPS/Images/AR_b.png)^((1)))â‹¯+![](../../OEBPS/Images/AR_b.png)^((*L*)))'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*MLP*(![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_a.png)^((*L*))
    = *y* = *Ïƒ*(*W*^((*L*))â€¦*Ïƒ*(*W*^((1))*Ïƒ*(*W*^((0))![](../../OEBPS/Images/AR_x.png)
    + ![](../../OEBPS/Images/AR_b.png)^((0)))+![](../../OEBPS/Images/AR_b.png)^((1)))â‹¯+![](../../OEBPS/Images/AR_b.png)^((*L*)))'
- en: Equation 8.9
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 8.9
- en: 'In a computer implementation, this expression is evaluated step by step by
    repeated application of the linear layer:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—æœºå®ç°ä¸­ï¼Œè¿™ä¸ªè¡¨è¾¾å¼é€šè¿‡é‡å¤åº”ç”¨çº¿æ€§å±‚é€æ­¥è¯„ä¼°ï¼š
- en: '![](../../OEBPS/Images/eq_08-10.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 8.10](../../OEBPS/Images/eq_08-10.png)'
- en: Equation 8.10
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 8.10
- en: Itâ€™s easy to see that equation [8.10](#eq-forwardprop-layered) is a restatement
    of equation [8.7](../Text/08.xhtml#eq-linlayer-forwardprop).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå®¹æ˜“çœ‹å‡ºæ–¹ç¨‹å¼ [8.10](#eq-forwardprop-layered) æ˜¯æ–¹ç¨‹å¼ [8.7](../Text/08.xhtml#eq-linlayer-forwardprop)
    çš„é‡è¿°ã€‚
- en: Close examination of these equations reveals a beautiful property. The complicated
    equation [8.9](#eq-MLP-out-nosubscript) is *never explicitly evaluated*. Instead,
    we evaluate the outputs of successive layers, one layer at a time, as per equation
    [8.10](#eq-forwardprop-layered). Every layer can be evaluated by taking the previous
    layerâ€™s output as input. No other input is necessary. That is to say, we can evaluate
    ![](../../OEBPS/Images/AR_a.png)^((0)) directly from the input ![](../../OEBPS/Images/AR_x.png),
    then ![](../../OEBPS/Images/AR_a.png)^((1)) from ![](../../OEBPS/Images/AR_a.png)^((0)),
    ![](../../OEBPS/Images/AR_a.png)^((2)) from ![](../../OEBPS/Images/AR_a.png)^((1)),
    and so forth, all the way to ![](../../OEBPS/Images/AR_a.png)^((*L*)) (which is
    the grand output of the MLP). During the evaluation, we need to keep only the
    previous and current layers in memory at any given time. This process greatly
    simplifies the implementation as well as the conceptualization and is known as
    *forward propagation*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä»”ç»†æ£€æŸ¥è¿™äº›æ–¹ç¨‹æ­ç¤ºäº†ä¸€ä¸ªç¾ä¸½çš„ç‰¹æ€§ã€‚å¤æ‚çš„æ–¹ç¨‹å¼ [8.9](#eq-MLP-out-nosubscript) æ°¸è¿œä¸ä¼šè¢«æ˜ç¡®è¯„ä¼°ã€‚ç›¸åï¼Œæˆ‘ä»¬æŒ‰ç…§æ–¹ç¨‹å¼
    [8.10](#eq-forwardprop-layered) é€å±‚è¯„ä¼°è¿ç»­å±‚çš„è¾“å‡ºï¼Œä¸€æ¬¡è¯„ä¼°ä¸€å±‚ã€‚æ¯ä¸€å±‚éƒ½å¯ä»¥é€šè¿‡å°†å‰ä¸€å±‚çš„è¾“å‡ºä½œä¸ºè¾“å…¥æ¥è¯„ä¼°ã€‚ä¸éœ€è¦å…¶ä»–è¾“å…¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä»è¾“å…¥
    ![](../../OEBPS/Images/AR_x.png) è¯„ä¼° ![](../../OEBPS/Images/AR_a.png)^((0))ï¼Œç„¶åä»
    ![](../../OEBPS/Images/AR_a.png)^((0)) è¯„ä¼° ![](../../OEBPS/Images/AR_a.png)^((1))ï¼Œä»
    ![](../../OEBPS/Images/AR_a.png)^((1)) è¯„ä¼° ![](../../OEBPS/Images/AR_a.png)^((2))ï¼Œä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°
    ![](../../OEBPS/Images/AR_a.png)^((*L*))ï¼ˆè¿™æ˜¯ MLP çš„å…¨å±€è¾“å‡ºï¼‰ã€‚åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨ä»»ä½•ç»™å®šæ—¶é—´å†…å­˜ä¸­ä¿ç•™å‰ä¸€å±‚å’Œå½“å‰å±‚ã€‚è¿™ä¸ªè¿‡ç¨‹æå¤§åœ°ç®€åŒ–äº†å®ç°ä»¥åŠæ¦‚å¿µåŒ–ï¼Œè¢«ç§°ä¸º
    *æ­£å‘ä¼ æ’­*ã€‚
- en: Listing 8.1 PyTorch code for forward propagation
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 8.1 PyTorch ä»£ç ç”¨äºæ­£å‘ä¼ æ’­
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'â‘  x: activation of layer l-1 (1-d vector) *W[l]*: Weight matrix of layer l
    *b[l]*: Bias vector of layer l'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'â‘  x: å±‚l-1çš„æ¿€æ´»ï¼ˆ1ç»´å‘é‡ï¼‰ *W[l]*: å±‚lçš„æƒé‡çŸ©é˜µ *b[l]*: å±‚lçš„åå·®å‘é‡'
- en: â‘¡ Sigmoid activation function (nonlinear layer)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ Sigmoidæ¿€æ´»å‡½æ•°ï¼ˆéçº¿æ€§å±‚ï¼‰
- en: 'â‘¢ x: 1-d input vector W: list of matrices for layers 0 to L. b: list of vectors
    for layers 0 to L'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'â‘¢ x: 1ç»´è¾“å…¥å‘é‡ W: ä»å±‚0åˆ°Lçš„çŸ©é˜µåˆ—è¡¨ b: ä»å±‚0åˆ°Lçš„å‘é‡åˆ—è¡¨'
- en: â‘£ Loops through layers 0 to L
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: â‘£ éå†å±‚0åˆ°L
- en: â‘¤ Computes Z
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¤ è®¡ç®—Z
- en: â‘¥ Computes activation
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¥ è®¡ç®—æ¿€æ´»
- en: 8.4 Training and backpropagation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 è®­ç»ƒå’Œåå‘ä¼ æ’­
- en: Throughout the book, we have been discussing bits and pieces of this process.
    In sections [1.1](../Text/01.xhtml#alg-supervised_training) and [3.3](../Text/03.xhtml#sec-grad)
    (specifically, algorithm 3.1 ), we saw an overview of the process for training
    a supervised model (you are encouraged to reread those if necessary). Training
    is an iterative process by which the parameters of the neural network are estimated.
    The goal is to estimate the parameters (weights and biases) such that on the training
    inputs, the neural network outputs are close as possible to the known ground-truth
    outputs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨è®¨è®ºè¿™ä¸ªè¿‡ç¨‹çš„ç‚¹ç‚¹æ»´æ»´ã€‚åœ¨ç¬¬[1.1](../Text/01.xhtml#alg-supervised_training)èŠ‚å’Œç¬¬[3.3](../Text/03.xhtml#sec-grad)èŠ‚ï¼ˆç‰¹åˆ«æ˜¯ç®—æ³•3.1ï¼‰ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†è®­ç»ƒç›‘ç£æ¨¡å‹çš„è¿‡ç¨‹æ¦‚è¿°ï¼ˆå¦‚æœéœ€è¦ï¼Œè¯·é‡æ–°é˜…è¯»ï¼‰ã€‚è®­ç»ƒæ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œé€šè¿‡è¿™ä¸ªè¿‡ç¨‹ä¼°è®¡ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚ç›®æ ‡æ˜¯ä¼°è®¡å‚æ•°ï¼ˆæƒé‡å’Œåå·®ï¼‰ï¼Œä½¿å¾—åœ¨è®­ç»ƒè¾“å…¥ä¸Šï¼Œç¥ç»ç½‘ç»œçš„è¾“å‡ºå°½å¯èƒ½æ¥è¿‘å·²çŸ¥çš„çœŸå®è¾“å‡ºã€‚
- en: In general, iterative processes improve (get closer to the goal) gradually.
    In each iteration, we make small adjustments to the parameters. Here, *parameter*
    refers to the weights and biases of the MLP, the *w[jk]*^((*l*))s and *b[j]*^((*l*))s
    from section [8.2](#sec-linlayer-forwardprop). We keep adjusting the parameters
    so that in every iteration, the outputs on training data inputs come a little
    closer to the ground truth (GT). Eventually, after many iterations, we hopefully
    converge to optimal values. Note that there is no guarantee that the iterative
    process will converge to the best possible parameter values. The training might
    go completely astray or get stuck in a local minimum. (Local minima are explained
    in section [3.6](../Text/03.xhtml#sec-locglob-minima); you are encouraged to reread
    it if necessary.) There is no good way to know whether we have reached optimal
    values (global minima) for the weights and biases. We typically run the neural
    network on test data, and if the results are satisfactory, we stop training. Test
    data should be *held back* during training, meaning we should never use test data
    to train. In the unfortunate event that the network has not reached the desired
    level of accuracy, we typically throw in more training data and/or try a modified
    loss function and/or a different architecture. Simply retraining the network from
    a different random start may also work. This is an experimental science with a
    lot of trial and error.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œè¿­ä»£è¿‡ç¨‹ä¼šé€æ¸æ”¹å–„ï¼ˆæ¥è¿‘ç›®æ ‡ï¼‰ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å¯¹å‚æ•°è¿›è¡Œå°çš„è°ƒæ•´ã€‚åœ¨è¿™é‡Œï¼Œâ€œå‚æ•°â€æŒ‡çš„æ˜¯MLPçš„æƒé‡å’Œåå·®ï¼Œå³ç¬¬[8.2](#sec-linlayer-forwardprop)èŠ‚ä¸­çš„*w[jk]*^((*l*))så’Œ*b[j]*^((*l*))sã€‚æˆ‘ä»¬ä¸æ–­è°ƒæ•´å‚æ•°ï¼Œä½¿å¾—åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œè®­ç»ƒæ•°æ®è¾“å…¥ä¸Šçš„è¾“å‡ºè¶Šæ¥è¶Šæ¥è¿‘çœŸå®å€¼ï¼ˆGTï¼‰ã€‚æœ€ç»ˆï¼Œç»è¿‡å¤šæ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬å¸Œæœ›æ”¶æ•›åˆ°æœ€ä¼˜å€¼ã€‚è¯·æ³¨æ„ï¼Œæ²¡æœ‰ä¿è¯è¿­ä»£è¿‡ç¨‹ä¼šæ”¶æ•›åˆ°æœ€ä½³å¯èƒ½çš„å‚æ•°å€¼ã€‚è®­ç»ƒå¯èƒ½ä¼šå®Œå…¨åç¦»è½¨é“æˆ–é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚ï¼ˆå±€éƒ¨æœ€å°å€¼åœ¨ç¬¬[3.6](../Text/03.xhtml#sec-locglob-minima)èŠ‚ä¸­è§£é‡Šï¼›å¦‚æœéœ€è¦ï¼Œè¯·é‡æ–°é˜…è¯»ã€‚ï¼‰æ²¡æœ‰å¥½çš„æ–¹æ³•çŸ¥é“æˆ‘ä»¬æ˜¯å¦å·²ç»è¾¾åˆ°äº†æƒé‡å’Œåå·®çš„æœ€ä¼˜å€¼ï¼ˆå…¨å±€æœ€å°å€¼ï¼‰ã€‚æˆ‘ä»¬é€šå¸¸åœ¨æµ‹è¯•æ•°æ®ä¸Šè¿è¡Œç¥ç»ç½‘ç»œï¼Œå¦‚æœç»“æœä»¤äººæ»¡æ„ï¼Œæˆ‘ä»¬å°±åœæ­¢è®­ç»ƒã€‚æµ‹è¯•æ•°æ®åº”è¯¥åœ¨è®­ç»ƒæœŸé—´ä¿ç•™ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ°¸è¿œä¸åº”è¯¥ä½¿ç”¨æµ‹è¯•æ•°æ®æ¥è®­ç»ƒã€‚åœ¨ä¸å¹¸çš„æƒ…å†µä¸‹ï¼Œç½‘ç»œæ²¡æœ‰è¾¾åˆ°æ‰€éœ€çš„å‡†ç¡®åº¦æ°´å¹³ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæ·»åŠ æ›´å¤šçš„è®­ç»ƒæ•°æ®ï¼Œæˆ–è€…å°è¯•ä¿®æ”¹æŸå¤±å‡½æ•°å’Œ/æˆ–ä¸åŒçš„æ¶æ„ã€‚ç®€å•åœ°ä»ä¸åŒçš„éšæœºèµ·ç‚¹é‡æ–°è®­ç»ƒç½‘ç»œä¹Ÿå¯èƒ½æœ‰æ•ˆã€‚è¿™æ˜¯ä¸€ä¸ªå……æ»¡è¯•éªŒå’Œé”™è¯¯çš„å®éªŒç§‘å­¦ã€‚
- en: How do we know how to adjust the parameter values in each iteration? We define
    a loss (aka error) function. There are many popular formulations of loss functions,
    and we review many of them later, but their common property is that when the neural
    network output agrees more with the known output (GT), the loss becomes lower,
    and vice versa. Thus if *y* denotes the output of the neural network and *È³* is
    the GT, a reasonable expression for the loss is the *mean squared error* (MSE)
    function (*y* âˆ’ *È³*)Â². For now, we use the MSE loss as our representative loss
    function. Later we discuss others.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•çŸ¥é“å¦‚ä½•åœ¨æ¯æ¬¡è¿­ä»£ä¸­è°ƒæ•´å‚æ•°å€¼ï¼Ÿæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæŸå¤±ï¼ˆä¹Ÿç§°ä¸ºè¯¯å·®ï¼‰å‡½æ•°ã€‚æœ‰è®¸å¤šæµè¡Œçš„æŸå¤±å‡½æ•°å…¬å¼ï¼Œæˆ‘ä»¬ç¨åä¼šå›é¡¾å…¶ä¸­è®¸å¤šï¼Œä½†å®ƒä»¬çš„å…±åŒç‰¹æ€§æ˜¯å½“ç¥ç»ç½‘ç»œè¾“å‡ºä¸å·²çŸ¥è¾“å‡ºï¼ˆGTï¼‰æ›´ä¸€è‡´æ—¶ï¼ŒæŸå¤±ä¼šé™ä½ï¼Œåä¹‹äº¦ç„¶ã€‚å› æ­¤ï¼Œå¦‚æœ
    *y* è¡¨ç¤ºç¥ç»ç½‘ç»œçš„è¾“å‡ºï¼Œè€Œ *È³* æ˜¯GTï¼Œåˆ™æŸå¤±çš„ä¸€ä¸ªåˆç†è¡¨è¾¾å¼æ˜¯*å‡æ–¹è¯¯å·®*ï¼ˆMSEï¼‰å‡½æ•° (*y* âˆ’ *È³*)Â²ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬ä½¿ç”¨MSEæŸå¤±ä½œä¸ºæˆ‘ä»¬çš„ä»£è¡¨æ€§æŸå¤±å‡½æ•°ã€‚ç¨åæˆ‘ä»¬å°†è®¨è®ºå…¶ä»–å‡½æ•°ã€‚
- en: 'Once the loss function is defined, we have a crisp, quantitative definition
    of the goal of neural network training. The goal is to minimize the total loss
    over the entire training data set. Note the clause *entire training data set*:
    we do not want to do well on one or two input instances at the cost of doing badly
    over the rest. If we have to choose between a solution that gives 10% error on
    all of, say, 100 training input instances versus one that yields 0% error on 50
    training input instances but 40% on the remaining 50, we prefer the former.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æŸå¤±å‡½æ•°è¢«å®šä¹‰ï¼Œæˆ‘ä»¬å°±æœ‰äº†ç¥ç»ç½‘ç»œè®­ç»ƒç›®æ ‡çš„æ¸…æ™°ã€å®šé‡å®šä¹‰ã€‚ç›®æ ‡æ˜¯ä½¿æ•´ä¸ªè®­ç»ƒæ•°æ®é›†çš„æ€»æŸå¤±æœ€å°åŒ–ã€‚è¯·æ³¨æ„æ¡æ¬¾â€œæ•´ä¸ªè®­ç»ƒæ•°æ®é›†â€ï¼šæˆ‘ä»¬ä¸å¸Œæœ›åœ¨æŸå¤±å…¶ä»–æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåªåœ¨å°‘æ•°å‡ ä¸ªè¾“å…¥å®ä¾‹ä¸Šè¡¨ç°è‰¯å¥½ã€‚å¦‚æœæˆ‘ä»¬å¿…é¡»åœ¨æ‰€æœ‰100ä¸ªè®­ç»ƒè¾“å…¥å®ä¾‹ä¸Šäº§ç”Ÿ10%çš„é”™è¯¯ä¸åœ¨50ä¸ªè®­ç»ƒè¾“å…¥å®ä¾‹ä¸Šäº§ç”Ÿ0%çš„é”™è¯¯ä½†åœ¨å‰©ä½™çš„50ä¸ªå®ä¾‹ä¸Šäº§ç”Ÿ40%çš„é”™è¯¯ä¹‹é—´åšå‡ºé€‰æ‹©ï¼Œæˆ‘ä»¬æ›´å€¾å‘äºå‰è€…ã€‚
- en: Each weight in the MLP, *w[jk]*^((*l*)), is adjusted by an amount proportional
    to *Î´w[jk]*^((*l*)). Similarly, each bias *b[j]*^((*l*)) is adjusted by an amount
    proportional to *Î´b[j]*^((*l*)). We can denote all this compactly by saying we
    have a weight vector ![](../../OEBPS/Images/AR_w.png) and bias vector ![](../../OEBPS/Images/AR_b.png).
    In each iteration, we change ![](../../OEBPS/Images/AR_w.png) by amount *Î´*![](../../OEBPS/Images/AR_w.png)
    and ![](../../OEBPS/Images/AR_b.png) by *Î´*![](../../OEBPS/Images/AR_b.png) so
    that their new values are ![](../../OEBPS/Images/AR_w.png) âˆ’ *rÎ´*![](../../OEBPS/Images/AR_w.png)
    and ![](../../OEBPS/Images/AR_b.png) âˆ’ *rÎ´*![](../../OEBPS/Images/AR_b.png) *r*
    is a constant known as the *learning rate* that needs to be set at the beginning
    of training). In this context, it is worthwhile to note that in section [8.3.1](#sec-linlayer-matmult),
    we expressed the collection of weights in an MLP with a matrix, while here we
    are referring to the same thing as a vector. These are not incompatible because
    we can always rasterize the elements of a matrix (that is, walk over the elements
    of the matrix from top to bottom and from left to right) into a vector.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸­çš„æ¯ä¸ªæƒé‡ *w[jk]*^((*l*ï¼‰) éƒ½é€šè¿‡ä¸€ä¸ªä¸ *Î´w[jk]*^((*l*)ï¼‰æˆæ¯”ä¾‹çš„é‡è¿›è¡Œè°ƒæ•´ã€‚åŒæ ·ï¼Œæ¯ä¸ªåç½® *b[j]*^((*l*)ï¼‰ä¹Ÿé€šè¿‡ä¸€ä¸ªä¸
    *Î´b[j]*^((*l*)ï¼‰æˆæ¯”ä¾‹çš„é‡è¿›è¡Œè°ƒæ•´ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è¯´æˆ‘ä»¬æœ‰ä¸€ä¸ªæƒé‡å‘é‡ ![](../../OEBPS/Images/AR_w.png) å’Œåç½®å‘é‡
    ![](../../OEBPS/Images/AR_b.png) æ¥ç®€æ´åœ°è¡¨ç¤ºæ‰€æœ‰è¿™äº›ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ”¹å˜ ![](../../OEBPS/Images/AR_w.png)
    çš„é‡ *Î´*![](../../OEBPS/Images/AR_w.png) å’Œ ![](../../OEBPS/Images/AR_b.png) çš„é‡ *Î´*![](../../OEBPS/Images/AR_b.png)
    æ¥æ”¹å˜å®ƒä»¬ï¼Œä»¥ä¾¿å®ƒä»¬çš„æ–°å€¼æ˜¯ ![](../../OEBPS/Images/AR_w.png) âˆ’ *rÎ´*![](../../OEBPS/Images/AR_w.png)
    å’Œ ![](../../OEBPS/Images/AR_b.png) âˆ’ *rÎ´*![](../../OEBPS/Images/AR_b.png)ï¼Œå…¶ä¸­ *r*
    æ˜¯ä¸€ä¸ªç§°ä¸º*å­¦ä¹ ç‡*çš„å¸¸æ•°ï¼Œéœ€è¦åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®ï¼‰ã€‚åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨[8.3.1](#sec-linlayer-matmult)èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç”¨çŸ©é˜µè¡¨ç¤ºäº†MLPä¸­çš„æƒé‡é›†åˆï¼Œè€Œåœ¨è¿™é‡Œæˆ‘ä»¬ç”¨å‘é‡æ¥æŒ‡ä»£åŒä¸€äº‹ç‰©ã€‚è¿™äº›å¹¶ä¸çŸ›ç›¾ï¼Œå› ä¸ºæˆ‘ä»¬æ€»æ˜¯å¯ä»¥å°†çŸ©é˜µçš„å…ƒç´ ï¼ˆå³ä»ä¸Šåˆ°ä¸‹å’Œä»å·¦åˆ°å³éå†çŸ©é˜µçš„å…ƒç´ ï¼‰è½¬æ¢æˆä¸€ä¸ªå‘é‡ã€‚
- en: How do we estimate the adjustment amounts *Î´*![](../../OEBPS/Images/AR_w.png)
    and *Î´*![](../../OEBPS/Images/AR_b.png)? This is where the notion of gradients
    comes in. These were discussed in detail in sections [3.3.1](../Text/03.xhtml#sec-gradient),
    [3.3.2](../Text/03.xhtml#subsec-level-surf), and [3.5](../Text/03.xhtml#sec-gradient-descent)
    (again, you are encouraged to reread if necessary). In general, if a loss, denoted
    ğ•ƒ, is expressed as a function of the parameters, such as ğ•ƒ(![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png)), then the change in the parameters that optimally
    takes us toward lower loss is yielded by the gradient of the loss with respect
    to the parameters âˆ‡[![](../../OEBPS/Images/AR_w.png), *b*]ğ•ƒ(![](../../OEBPS/Images/AR_w.png),
    *b*). The high-level process is described later in the chapter in algorithm [3.2](../Text/03.xhtml#alg-supervised_training_detailed).
    Here we look at the guts of it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•ä¼°è®¡è°ƒæ•´é‡ *Î´*![MLP](../../OEBPS/Images/AR_w.png) å’Œ *Î´*![MLP](../../OEBPS/Images/AR_b.png)ï¼Ÿè¿™å°±æ˜¯æ¢¯åº¦æ¦‚å¿µå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚è¿™äº›å†…å®¹åœ¨
    [3.3.1](../Text/03.xhtml#sec-gradient)ã€[3.3.2](../Text/03.xhtml#subsec-level-surf)
    å’Œ [3.5](../Text/03.xhtml#sec-gradient-descent) èŠ‚ä¸­å·²æœ‰è¯¦ç»†è®¨è®ºï¼ˆå†æ¬¡æé†’ï¼Œå¦‚æœ‰å¿…è¦ï¼Œè¯·é‡æ–°é˜…è¯»ï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœæŸå¤±
    ğ•ƒ è¡¨ç¤ºä¸ºå‚æ•°çš„å‡½æ•°ï¼Œä¾‹å¦‚ ğ•ƒ(![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png))ï¼Œé‚£ä¹ˆä½¿æŸå¤±æœ€ä¼˜é™ä½çš„å‚æ•°å˜åŒ–ç”±æŸå¤±ç›¸å¯¹äºå‚æ•°çš„æ¢¯åº¦
    âˆ‡[![MLP](../../OEBPS/Images/AR_w.png), *b*]ğ•ƒ(![](../../OEBPS/Images/AR_w.png),
    *b*) å¾—å‡ºã€‚æœ¬ç« ç¨åå°†åœ¨ç®—æ³• [3.2](../Text/03.xhtml#alg-supervised_training_detailed) ä¸­æè¿°é«˜çº§è¿‡ç¨‹ã€‚è¿™é‡Œæˆ‘ä»¬æ¥çœ‹çœ‹å…¶æ ¸å¿ƒå†…å®¹ã€‚
- en: '8.4.1 Loss and its minimization: Goal of training'
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 æŸå¤±åŠå…¶æœ€å°åŒ–ï¼šè®­ç»ƒç›®æ ‡
- en: Given a training data set ğ•‹ that is a set of <input, GT output> pairs ğ•‹ = {âŸ¨![](../../OEBPS/Images/AR_x.png),
    *È³*âŸ©}, the loss can be expressed as
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›† ğ•ƒï¼Œå®ƒæ˜¯ä¸€ç»„ <è¾“å…¥ï¼ŒGT è¾“å‡º> å¯¹ ğ•ƒ = {âŸ¨![MLP](../../OEBPS/Images/AR_x.png), *È³*âŸ©}ï¼ŒæŸå¤±å¯ä»¥è¡¨ç¤ºä¸º
- en: '![](../../OEBPS/Images/eq_08-11.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![eq_08-11.png](../../OEBPS/Images/eq_08-11.png)'
- en: Equation 8.11
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.11
- en: where
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '![](../../OEBPS/Images/AR_y.png) = *MLP*(![](../../OEBPS/Images/AR_x.png))'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![MLP](../../OEBPS/Images/AR_y.png) = *MLP*(![](../../OEBPS/Images/AR_x.png))'
- en: as per equation [8.9](#eq-MLP-out-nosubscript).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ–¹ç¨‹ [8.9](#eq-MLP-out-nosubscript) æ‰€ç¤ºã€‚
- en: 'Now consider equation [8.7](../Text/08.xhtml#eq-linlayer-forwardprop) again.
    We can rasterize each layerâ€™s weight matrix *W*^((*l*)) into a vector and then
    concatenate all these vectors from successive layers to form a giant weight vector
    ![](../../OEBPS/Images/AR_w.png), the vector of all weights in the MLP:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å†æ¬¡è€ƒè™‘æ–¹ç¨‹ [8.7](../Text/08.xhtml#eq-linlayer-forwardprop)ã€‚æˆ‘ä»¬å¯ä»¥å°†æ¯ä¸€å±‚çš„æƒé‡çŸ©é˜µ *W*^((*l*))
    è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡ï¼Œç„¶åå°†è¿™äº›ä»è¿ç»­å±‚ä¸­ä¾æ¬¡è¿æ¥çš„å‘é‡ç»„åˆæˆä¸€ä¸ªå·¨å¤§çš„æƒé‡å‘é‡ ![MLP](../../OEBPS/Images/AR_w.png)ï¼Œå³ MLP
    ä¸­æ‰€æœ‰æƒé‡çš„å‘é‡ï¼š
- en: '![](../../OEBPS/Images/eq_08-11-a.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![eq_08-11-a.png](../../OEBPS/Images/eq_08-11-a.png)'
- en: 'Similarly, we can form a giant vector of all biases in the MLP:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥å½¢æˆä¸€ä¸ªåŒ…å« MLP ä¸­æ‰€æœ‰åå·®çš„å·¨å¤§å‘é‡ï¼š
- en: '![](../../OEBPS/Images/eq_08-11-b.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![eq_08-11-b.png](../../OEBPS/Images/eq_08-11-b.png)'
- en: 'The ultimate goal of training is to find ![](../../OEBPS/Images/AR_w.png) and
    ![](../../OEBPS/Images/AR_b.png) that will minimize the loss ğ•ƒ. In chapter [3](../Text/03.xhtml#chapter-intro-vec-mat),
    we saw that we can solve for the minimum by setting the gradients âˆ‡[![](../../OEBPS/Images/AR_w.png)]ğ•ƒ
    = 0 and âˆ‡[![](../../OEBPS/Images/AR_b.png)]ğ•ƒ = 0. Computing the loss gradient
    from a combination of equations [8.9](#eq-MLP-out-nosubscript) and [8.11](../Text/08.xhtml#eq-mse-loss)
    is intractable. Instead, we go for an iterative solution: *gradient descent* on
    the loss surface, as described in the next section.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒçš„æœ€ç»ˆç›®æ ‡æ˜¯æ‰¾åˆ° ![MLP](../../OEBPS/Images/AR_w.png) å’Œ ![MLP](../../OEBPS/Images/AR_b.png)ï¼Œå®ƒä»¬å°†æœ€å°åŒ–æŸå¤±
    ğ•ƒã€‚åœ¨ç¬¬ [3](../Text/03.xhtml#chapter-intro-vec-mat) ç« ä¸­ï¼Œæˆ‘ä»¬äº†è§£åˆ°æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®æ¢¯åº¦ âˆ‡[![MLP](../../OEBPS/Images/AR_w.png)]ğ•ƒ
    = 0 å’Œ âˆ‡[![MLP](../../OEBPS/Images/AR_b.png)]ğ•ƒ = 0 æ¥æ±‚è§£æœ€å°å€¼ã€‚ä»æ–¹ç¨‹ [8.9](#eq-MLP-out-nosubscript)
    å’Œ [8.11](../Text/08.xhtml#eq-mse-loss) çš„ç»„åˆä¸­è®¡ç®—æŸå¤±æ¢¯åº¦æ˜¯ä¸å¯è¡Œçš„ã€‚ç›¸åï¼Œæˆ‘ä»¬é‡‡ç”¨è¿­ä»£è§£æ³•ï¼šåœ¨æŸå¤±è¡¨é¢ä¸Šè¿›è¡Œ *æ¢¯åº¦ä¸‹é™*ï¼Œå¦‚ä¸‹ä¸€èŠ‚æ‰€è¿°ã€‚
- en: Listing 8.2 PyTorch code for MSE loss
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 8.2 ç”¨äº MSE æŸå¤±çš„ PyTorch ä»£ç 
- en: '[PRE1]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'â‘  a: Activation of layer L (1D vector) : Ground truth (1D vector)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'â‘  a: å±‚ L çš„æ¿€æ´»ï¼ˆ1D å‘é‡ï¼‰: çœŸå®å€¼ï¼ˆ1D å‘é‡ï¼‰'
- en: â‘¡ See equation [8.11](../Text/08.xhtml#eq-mse-loss).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ è§æ–¹ç¨‹ [8.11](../Text/08.xhtml#eq-mse-loss)ã€‚
- en: 8.4.2 Loss surface and gradient descent
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 æŸå¤±è¡¨é¢å’Œæ¢¯åº¦ä¸‹é™
- en: Geometrically, the loss function ğ•ƒ(![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png))
    can be viewed as a surface in a high-dimensional space. The domain of this space
    corresponds to all the dimensions in ![](../../OEBPS/Images/AR_w.png) plus all
    the dimensions in ![](../../OEBPS/Images/AR_b.png). This is shown in figure [8.8](../Text/08.xhtml#fig-gradient-descent-3d)
    with a 2D domain. In chapter [3](../Text/03.xhtml#chapter-intro-vec-mat), we also
    saw that given a function ğ•ƒ(![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png)),
    the best way to progress toward the minimum is to walk on the parameter space
    along the negative gradient. We adopt this approach to minimize the loss. We compute
    the gradients of the loss function with respect to weights and biases and update
    the weights and bias vectors by an amount proportional to the (negative) of these
    gradients. Doing this repeatedly takes us close to the minimum. In figure [8.8](../Text/08.xhtml#fig-gradient-descent-3d),
    the gradient descent path is shown with solid arrows, while an arbitrary non-optimal
    path to the minimum is shown with dashed arrows.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä½•ä¸Šï¼ŒæŸå¤±å‡½æ•° ğ•ƒ(![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png))
    å¯ä»¥çœ‹ä½œæ˜¯åœ¨é«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªæ›²é¢ã€‚è¿™ä¸ªç©ºé—´çš„åŸŸå¯¹åº”äº ![](../../OEBPS/Images/AR_w.png) ä¸­çš„æ‰€æœ‰ç»´åº¦åŠ ä¸Š ![](../../OEBPS/Images/AR_b.png)
    ä¸­çš„æ‰€æœ‰ç»´åº¦ã€‚è¿™å¦‚å›¾ [8.8](../Text/08.xhtml#fig-gradient-descent-3d) ä¸­çš„äºŒç»´åŸŸæ‰€ç¤ºã€‚åœ¨ç¬¬ [3](../Text/03.xhtml#chapter-intro-vec-mat)
    ç« ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿçœ‹åˆ°äº†ï¼Œç»™å®šä¸€ä¸ªå‡½æ•° ğ•ƒ(![](../../OEBPS/Images/AR_w.png), ![](../../OEBPS/Images/AR_b.png))ï¼Œå‘æœ€å°å€¼å‰è¿›çš„æœ€ä½³æ–¹å¼æ˜¯æ²¿ç€è´Ÿæ¢¯åº¦åœ¨å‚æ•°ç©ºé—´ä¸­è¡Œèµ°ã€‚æˆ‘ä»¬é‡‡ç”¨è¿™ç§æ–¹æ³•æ¥æœ€å°åŒ–æŸå¤±ã€‚æˆ‘ä»¬è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæƒé‡å’Œåå·®çš„æ¢¯åº¦ï¼Œå¹¶é€šè¿‡ä¸è¿™äº›æ¢¯åº¦çš„ï¼ˆè´Ÿï¼‰æˆæ¯”ä¾‹çš„é‡æ¥æ›´æ–°æƒé‡å’Œåå·®å‘é‡ã€‚é‡å¤è¿™æ ·åšå¯ä»¥ä½¿æˆ‘ä»¬æ¥è¿‘æœ€å°å€¼ã€‚åœ¨å›¾
    [8.8](../Text/08.xhtml#fig-gradient-descent-3d) ä¸­ï¼Œæ¢¯åº¦ä¸‹é™è·¯å¾„ç”¨å®çº¿ç®­å¤´è¡¨ç¤ºï¼Œè€Œåˆ°æœ€å°å€¼çš„ä»»æ„éæœ€ä¼˜è·¯å¾„ç”¨è™šçº¿ç®­å¤´è¡¨ç¤ºã€‚
- en: '![](../../OEBPS/Images/CH08_F08_Chaudhury.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH08_F08_Chaudhury.png)'
- en: Figure 8.8 A representative loss ğ•ƒ(*w*, *b*). Note that ![](../../OEBPS/Images/AR_w.png)
    and ![](../../OEBPS/Images/AR_b.png) have each been reduced to 1D for this .
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.8 ä¸€ä¸ªä»£è¡¨æ€§çš„æŸå¤± ğ•ƒ(*w*, *b*)ã€‚æ³¨æ„ï¼Œ![](../../OEBPS/Images/AR_w.png) å’Œ ![](../../OEBPS/Images/AR_b.png)
    éƒ½å·²ç»åœ¨è¿™ä¸ªå›¾ä¸­è¢«ç®€åŒ–ä¸º 1Dã€‚
- en: Thus the equations for updating weights and biases in gradient descent are
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ¢¯åº¦ä¸‹é™ä¸­æ›´æ–°æƒé‡å’Œåå·®çš„æ–¹ç¨‹æ˜¯
- en: '![](../../OEBPS/Images/eq_08-12.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_08-12.png)'
- en: Equation 8.12
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.12
- en: where *r* is a constant. Here
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *r* æ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚åœ¨è¿™é‡Œ
- en: '![](../../OEBPS/Images/eq_08-13.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_08-13.png)'
- en: Equation 8.13
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.13
- en: The vector update equation [8.12](../Text/08.xhtml#eq-wtsbiases-update-vector)
    can be expressed in terms of the scalar components as
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡æ›´æ–°æ–¹ç¨‹ [8.12](../Text/08.xhtml#eq-wtsbiases-update-vector) å¯ä»¥ç”¨æ ‡é‡åˆ†é‡è¡¨ç¤ºä¸º
- en: '![](../../OEBPS/Images/eq_08-14.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_08-14.png)'
- en: Equation 8.14
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.14
- en: Note that we have to reevaluate these partial derivatives in each iteration
    since their values will change in every iteration.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ¯æ¬¡è¿­ä»£ä¸­é‡æ–°è¯„ä¼°è¿™äº›åå¯¼æ•°ï¼Œå› ä¸ºå®ƒä»¬çš„å€¼ä¼šåœ¨æ¯æ¬¡è¿­ä»£ä¸­æ”¹å˜ã€‚
- en: 8.4.3 Why a gradient provides the best direction for descent
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 ä¸ºä»€ä¹ˆæ¢¯åº¦æä¾›äº†ä¸‹é™çš„æœ€ä½³æ–¹å‘
- en: Why does updating along the gradient reduce the function optimally? This is
    discussed in detail in chapter [3](../Text/03.xhtml#chapter-intro-vec-mat). Here
    we briefly recap the idea. Using multidimensional Taylor expansion, we can evaluate
    a function in the neighborhood of a known point. For instance, we can evaluate
    ğ•ƒ(![](../../OEBPS/Images/AR_w.png) + *Î´*![](../../OEBPS/Images/AR_w.png)) for
    small offset *Î´*![](../../OEBPS/Images/AR_w.png) from ![](../../OEBPS/Images/AR_w.png)
    as follows
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæ²¿ç€æ¢¯åº¦æ›´æ–°å¯ä»¥æœ€ä¼˜åœ°å‡å°‘å‡½æ•°ï¼Ÿè¿™åœ¨ç¬¬ [3](../Text/03.xhtml#chapter-intro-vec-mat) ç« ä¸­æœ‰è¯¦ç»†è®¨è®ºã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç®€è¦å›é¡¾ä¸€ä¸‹è¿™ä¸ªæƒ³æ³•ã€‚ä½¿ç”¨å¤šç»´æ³°å‹’å±•å¼€ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å·²çŸ¥ç‚¹çš„é‚»åŸŸå†…è¯„ä¼°ä¸€ä¸ªå‡½æ•°ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°
    ğ•ƒ(![](../../OEBPS/Images/AR_w.png) + *Î´*![](../../OEBPS/Images/AR_w.png))ï¼Œå…¶ä¸­ *Î´*![](../../OEBPS/Images/AR_w.png)
    æ˜¯ä» ![](../../OEBPS/Images/AR_w.png) å‡ºå‘çš„å°åç§»é‡ï¼Œå¦‚ä¸‹æ‰€ç¤º
- en: '![](../../OEBPS/Images/eq_08-15.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_08-15.png)'
- en: Equation 8.15
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.15
- en: 'where *H*, called the *Hessian matrix*, is defined as in equation [3.9](../Text/03.xhtml#eq-hessian).
    Since we are not going too far from ![](../../OEBPS/Images/AR_w.png), ||*Î´*![](../../OEBPS/Images/AR_w.png)||
    is small. This means the quadratic and higher-order terms are negligibly small,
    and we can drop them (the approximation is perfect in the limit when ||*Î´*![](../../OEBPS/Images/AR_w.png)||
    â†’ 0):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *H*ï¼Œç§°ä¸º *æµ·æ£®çŸ©é˜µ*ï¼Œå…¶å®šä¹‰å¦‚æ–¹ç¨‹ [3.9](../Text/03.xhtml#eq-hessian)ã€‚ç”±äºæˆ‘ä»¬æ²¡æœ‰ç¦»å¼€ ![](../../OEBPS/Images/AR_w.png)
    å¤ªè¿œï¼Œ||*Î´*![](../../OEBPS/Images/AR_w.png)|| å¾ˆå°ã€‚è¿™æ„å‘³ç€äºŒæ¬¡å’Œæ›´é«˜é˜¶çš„é¡¹å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬çœç•¥ï¼ˆå½“ ||*Î´*![](../../OEBPS/Images/AR_w.png)||
    â†’ 0 æ—¶ï¼Œè¿‘ä¼¼æ˜¯å®Œç¾çš„ï¼‰ï¼š
- en: '![](../../OEBPS/Images/eq_08-15-a.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_08-15-a.png)'
- en: 'But we know the dot product (![](../../OEBPS/Images/AR_delta.png))*^T* â–½![](../../OEBPS/Images/AR_w.png)ğ•ƒ
    will attain its maximum value when both the vectors point in the same direction:
    that is,'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬çŸ¥é“ç‚¹ç§¯ (![](../../OEBPS/Images/AR_delta.png))*^T* â–½![](../../OEBPS/Images/AR_w.png)ğ•ƒå°†åœ¨ä¸¤ä¸ªå‘é‡æŒ‡å‘åŒä¸€æ–¹å‘æ—¶è¾¾åˆ°å…¶æœ€å¤§å€¼ï¼šä¹Ÿå°±æ˜¯è¯´ï¼Œ
- en: '![](../../OEBPS/Images/eq_08-15-b.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_08-15-b.png)'
- en: for some constant of proportionality *r*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸä¸ªæ¯”ä¾‹å¸¸æ•° *r*ã€‚
- en: In implementation, *r* is called the *learning rate*. A higher learning rate
    causes the optimization to progress more rapidly but also runs the risk of overshooting
    the minimum. We learn about these in more detail later. For now, simply note that
    *r* is a *tunable hyperparameter* of the system.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®ç°ä¸­ï¼Œ*r* è¢«ç§°ä¸º *å­¦ä¹ ç‡*ã€‚è¾ƒé«˜çš„å­¦ä¹ ç‡ä¼šå¯¼è‡´ä¼˜åŒ–è¿‡ç¨‹æ›´å¿«åœ°è¿›å±•ï¼Œä½†ä¹Ÿå­˜åœ¨è¶…è¿‡æœ€å°å€¼çš„å±é™©ã€‚æˆ‘ä»¬å°†åœ¨åé¢æ›´è¯¦ç»†åœ°äº†è§£è¿™äº›å†…å®¹ã€‚ç°åœ¨ï¼Œåªéœ€æ³¨æ„*r*æ˜¯ç³»ç»Ÿçš„*å¯è°ƒè¶…å‚æ•°*ã€‚
- en: 'Thus, the largest decrease in value from ğ•ƒ(![](../../OEBPS/Images/AR_w.png))
    to ğ•ƒ(![](../../OEBPS/Images/AR_w.png) â€“ ![](../../OEBPS/Images/AR_delta.png))
    happens when *Î´*![](../../OEBPS/Images/AR_w.png) is along the negative gradient.
    This is why we move toward the negative gradient in gradient descent: it is the
    fastest way to reach the minimum. The straight arrows in figure [8.8](../Text/08.xhtml#fig-gradient-descent-3d)
    illustrate the direction of the gradient. The dashed arrows show an arbitrary
    nongradient path for comparison.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä»ğ•ƒ(![](../../OEBPS/Images/AR_w.png))åˆ°ğ•ƒ(![](../../OEBPS/Images/AR_w.png) â€“
    ![](../../OEBPS/Images/AR_delta.png))çš„æœ€å¤§å€¼å‡å°‘å‘ç”Ÿåœ¨*Î´*![](../../OEBPS/Images/AR_w.png)æ²¿ç€è´Ÿæ¢¯åº¦æ–¹å‘æ—¶ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨æ¢¯åº¦ä¸‹é™ä¸­æœç€è´Ÿæ¢¯åº¦ç§»åŠ¨ï¼šè¿™æ˜¯è¾¾åˆ°æœ€å°å€¼æœ€å¿«çš„æ–¹å¼ã€‚å›¾[8.8](../Text/08.xhtml#fig-gradient-descent-3d)ä¸­çš„ç›´çº¿ç®­å¤´è¯´æ˜äº†æ¢¯åº¦çš„æ–¹å‘ã€‚è™šçº¿ç®­å¤´æ˜¾ç¤ºäº†ä¸€ä¸ªä»»æ„çš„éæ¢¯åº¦è·¯å¾„ä»¥ä¾›æ¯”è¾ƒã€‚
- en: We can deal with the bias vector ![](../../OEBPS/Images/AR_b.png) similarly.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç±»ä¼¼åœ°å¤„ç†åç½®å‘é‡ ![](../../OEBPS/Images/AR_b.png)ã€‚
- en: 8.4.4 Gradient descent and local minima
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.4 æ¢¯åº¦ä¸‹é™å’Œå±€éƒ¨æœ€å°å€¼
- en: We should note that gradient descent can get stuck in a *local minimum*. Figure
    [8.9](../Text/08.xhtml#fig-non-convex-local-minima) shows this.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥æ³¨æ„ï¼Œæ¢¯åº¦ä¸‹é™å¯èƒ½ä¼šé™·å…¥ä¸€ä¸ª *å±€éƒ¨æœ€å°å€¼*ã€‚å›¾[8.9](../Text/08.xhtml#fig-non-convex-local-minima)å±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚
- en: '![](../../OEBPS/Images/CH08_F09_Chaudhury.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH08_F09_Chaudhury.png)'
- en: Figure 8.9 A nonconvex function with local and global minima. Depending on the
    point, gradient descent will take us to one or the other.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.9 å…·æœ‰å±€éƒ¨å’Œå…¨å±€æœ€å°å€¼çš„éå‡¸å‡½æ•°ã€‚æ ¹æ®ç‚¹ï¼Œæ¢¯åº¦ä¸‹é™å¯èƒ½ä¼šå¸¦æˆ‘ä»¬åˆ°å…¶ä¸­ä¸€ä¸ªã€‚
- en: 'In earlier eras, optimization techniques tried hard to avoid local minima and
    converge to the global minimum. Techniques like simulated annealing and tunneling
    were carefully designed to avoid local minima. Modern-day neural networks have
    adopted a different attitude: they do not try very hard to avoid local minima.
    Sometimes a local minimum is an acceptable (accurate enough) solution. Otherwise,
    we can retrain the neural network: it will start from a random position, so this
    time it may go to a better minimum.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ—©æœŸæ—¶ä»£ï¼Œä¼˜åŒ–æŠ€æœ¯åŠªåŠ›é¿å…å±€éƒ¨æœ€å°å€¼å¹¶æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ã€‚åƒæ¨¡æ‹Ÿé€€ç«å’Œéš§é“è¿™æ ·çš„æŠ€æœ¯è¢«ç²¾å¿ƒè®¾è®¡æ¥é¿å…å±€éƒ¨æœ€å°å€¼ã€‚ç°ä»£ç¥ç»ç½‘ç»œé‡‡å–äº†ä¸åŒçš„æ€åº¦ï¼šå®ƒä»¬å¹¶ä¸éå¸¸åŠªåŠ›åœ°é¿å…å±€éƒ¨æœ€å°å€¼ã€‚æœ‰æ—¶å±€éƒ¨æœ€å°å€¼æ˜¯ä¸€ä¸ªå¯æ¥å—çš„ï¼ˆè¶³å¤Ÿå‡†ç¡®ï¼‰è§£ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°è®­ç»ƒç¥ç»ç½‘ç»œï¼šå®ƒå°†ä»éšæœºä½ç½®å¼€å§‹ï¼Œæ‰€ä»¥è¿™æ¬¡å®ƒå¯èƒ½è¾¾åˆ°æ›´å¥½çš„æœ€å°å€¼ã€‚
- en: '![](../../OEBPS/Images/CH08_F10_Chaudhury.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH08_F10_Chaudhury.png)'
- en: Figure 8.10 MLP with layers 0, â€¦, *L*, one neuron per layer. Again, we have
    split every layer into a weighted sum and a sigmoid.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.10 å±‚0, â€¦, *L*çš„MLPï¼Œæ¯å±‚ä¸€ä¸ªç¥ç»å…ƒã€‚å†æ¬¡å¼ºè°ƒï¼Œæˆ‘ä»¬å°†æ¯ä¸€å±‚éƒ½åˆ†è§£ä¸ºä¸€ä¸ªåŠ æƒæ±‚å’Œå’Œä¸€ä¸ªsigmoidå‡½æ•°ã€‚
- en: 8.4.5 The backpropagation algorithm
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.5 åå‘ä¼ æ’­ç®—æ³•
- en: We have seen that gradient descent progresses by repeatedly updating the weights
    and biases via equation [8.12](../Text/08.xhtml#eq-wtsbiases-update-vector). This
    is equivalent to repeatedly updating individual weights and biases using individual
    partial derivatives via equation [8.14](#eq-wtsbiases-update-scalar).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œæ¢¯åº¦ä¸‹é™é€šè¿‡åå¤æ›´æ–°æƒé‡å’Œåç½®ï¼ˆæ–¹ç¨‹[8.12](../Text/08.xhtml#eq-wtsbiases-update-vector)ï¼‰æ¥è¿›å±•ã€‚è¿™ç›¸å½“äºé€šè¿‡æ–¹ç¨‹[8.14](#eq-wtsbiases-update-scalar)åå¤ä½¿ç”¨å•ä¸ªåå¯¼æ•°æ›´æ–°å•ä¸ªæƒé‡å’Œåç½®ã€‚
- en: Obtaining a closed-form solution for the gradients âˆ‡[![](../../OEBPS/Images/AR_w.png)]ğ•ƒ(![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png)), âˆ‡[![](../../OEBPS/Images/AR_b.png)]ğ•ƒ(![](../../OEBPS/Images/AR_w.png),
    ![](../../OEBPS/Images/AR_b.png)) from equations [8.9](#eq-MLP-out-nosubscript)
    and [8.11](../Text/08.xhtml#eq-mse-loss)â€”or, equivalently, obtaining a closed-form
    solution for the partial derivatives *âˆ‚*ğ•ƒ/*âˆ‚w[jk]*^((*l*)), *âˆ‚*ğ•ƒ/*âˆ‚b[j]*^((*l*)),
    â€”is very difficult. Backpropagation is an algorithm that allows us to evaluate
    the gradients and update the weights and biases one layer at a time, like forward
    propagation (equation [8.10](#eq-forwardprop-layered)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation algorithm on a simple network
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: We first discuss backpropagation on a simple MLP with only a single neuron per
    layer. The main simplification resulting from this is that individual weights
    and biases no longer need subscripts, with only one weight and one bias between
    two successive layers. They still need superscripts to indicate layer IDs, however.
    Figure [8.10](#fig-MLP-one-neuron-per-layer) shows this MLP. We use MSE loss (equation
    [8.11](../Text/08.xhtml#eq-mse-loss)), but we work on a single input-output pair
    *x[i]*, *y[i]*. The total loss (summation over all the training data instances)
    can easily be derived by repeating the same steps.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define an auxiliary variable:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-15-c.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: The physical significance of *Î´*^((*l*)) is that it is the rate of change of
    the loss with the (pre-activation) output of layer *l* (remember, in this network,
    layer *l* has a single neuron).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s establish a few important equations for the MLP in figure [8.10](#fig-MLP-one-neuron-per-layer):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '*Forward propagation for an arbitrary layer* *l* âˆˆ {0, *L*}'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-16.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Equation 8.16
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-17.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: Equation 8.17
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss*â€”Here we are working with a single training data instance, *x[i]*, whose
    GT output is *È³[i]*:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-17-a.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: '*Partial derivative of loss with respect to the weight and bias in terms of
    an auxiliary variable for the last layer, L*â€”Using the chain rule for partial
    derivatives,'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-17-b.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Examining the terms on the right, we see
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-17-c.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: (auxiliary variable for layer *L*). And using the forward propagation equations,
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-17-d.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Together, they lead to
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-17-e.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Similarly,
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-17-f.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Consequently, we have the following pair of equations expressing the partial
    derivative of loss with respect to weight and bias, respectively, in terms of
    the auxiliary variable for the last layer:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-18.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Equation 8.18
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-19.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Equation 8.19
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '*Auxiliary variable for the last layer, L*â€”Using the chain rule for partial
    derivatives,'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-19-a.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Using equation [8.5](../Text/08.xhtml#eq-sigmoid-derivative) for the derivative
    of a sigmoid, we get
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ–¹ç¨‹ [8.5](../Text/08.xhtml#eq-sigmoid-derivative) å¯¹ Sigmoid å‡½æ•°çš„å¯¼æ•°è¿›è¡Œè®¡ç®—ï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '*Î´*^((*L*)) = (*a*^((*L*))âˆ’*È³[i]*) *Ïƒ*(*z*^((*L*)))(1âˆ’*Ïƒ*(*z*^((*L*))))'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*Î´*^((*L*)) = (*a*^((*L*))âˆ’*È³[i]*) *Ïƒ*(*z*^((*L*)))(1âˆ’*Ïƒ*(*z*^((*L*))))'
- en: which, using equation [8.17](#eq-forwardprop-a-simple), leads to
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ–¹ç¨‹ [8.17](#eq-forwardprop-a-simple)ï¼Œè¿™å¯¼è‡´
- en: '*Î´*^((*L*)) = (*a*^((*L*))âˆ’*È³[i]*) *a*^((*L*))(1âˆ’*a*^((*L*)))'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*Î´*^((*L*)) = (*a*^((*L*))âˆ’*È³[i]*) *a*^((*L*))(1âˆ’*a*^((*L*)))'
- en: Equation 8.20
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.20
- en: '*Partial derivative of the loss with respect to the weight and bias in terms
    of an auxiliary variable for an arbitrary layer l*â€”Using the chain rule for partial
    derivatives,'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å…³äºä»»æ„å±‚ l* çš„è¾…åŠ©å˜é‡ï¼ŒæŸå¤±ç›¸å¯¹äºæƒé‡å’Œåç½®çš„åå¯¼æ•°â€”â€”ä½¿ç”¨åå¯¼æ•°çš„é“¾å¼æ³•åˆ™ï¼Œ'
- en: '![](../../OEBPS/Images/eq_08-20-a.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-20-a.png)'
- en: Using the definition of the auxiliary variable and the forward propagation equation
    [8.16](#eq-forwardprop-z-simple), this leads to
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¾…åŠ©å˜é‡çš„å®šä¹‰å’Œå‰å‘ä¼ æ’­æ–¹ç¨‹ [8.16](#eq-forwardprop-z-simple)ï¼Œè¿™å¯¼è‡´
- en: '![](../../OEBPS/Images/eq_08-21.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-21.png)'
- en: Equation 8.21
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.21
- en: Similarly,
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œ
- en: '![](../../OEBPS/Images/eq_08-21-a.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-21-a.png)'
- en: Using the definition of the auxiliary variable and the forward propagation equation
    [8.16](#eq-forwardprop-z-simple), this leads to
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¾…åŠ©å˜é‡çš„å®šä¹‰å’Œå‰å‘ä¼ æ’­æ–¹ç¨‹ [8.16](#eq-forwardprop-z-simple)ï¼Œè¿™å¯¼è‡´
- en: '![](../../OEBPS/Images/eq_08-22.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-22.png)'
- en: Equation 8.22
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.22
- en: '*Auxiliary variable for an arbitrary layer, l*â€”Using the chain rule for partial
    derivatives,'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ä»»æ„å±‚ l* çš„è¾…åŠ©å˜é‡â€”â€”ä½¿ç”¨åå¯¼æ•°çš„é“¾å¼æ³•åˆ™ï¼Œ'
- en: '![](../../OEBPS/Images/eq_08-22-a.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-22-a.png)'
- en: Using the definition of the auxiliary variable and the forward propagation equation
    [8.16](#eq-forwardprop-z-simple), this leads to
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¾…åŠ©å˜é‡çš„å®šä¹‰å’Œå‰å‘ä¼ æ’­æ–¹ç¨‹ [8.16](#eq-forwardprop-z-simple)ï¼Œè¿™å¯¼è‡´
- en: '![](../../OEBPS/Images/eq_08-22-b.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-22-b.png)'
- en: which yields
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å¾—åˆ°
- en: '*Î´*^((*l*)) = *Î´*^((*l*+1)) *w*^((*l*+1)) *a*^((*l*))(1âˆ’*a*^((*l*)))'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*Î´*^((*l*)) = *Î´*^((*l*+1)) *w*^((*l*+1)) *a*^((*l*))(1âˆ’*a*^((*l*)))'
- en: Equation 8.23
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 8.23
- en: 'We first encountered the one-layer-at-a-time property in section [8.3.2](#sec-layered-forwardprop)
    in connection with the forward propagation equations. Letâ€™s recap that in the
    context of training our simple network. Consider equations [8.16](#eq-forwardprop-z-simple)
    and [8.17](#eq-forwardprop-a-simple). We initialize the system with some values
    of weights *w*^((*l*)) and biases *b*^((*l*)). Using those, we can evaluate the
    layer 0 outputs. For starters, we can evaluate *z*^((0)) and *a*^((0)) easily
    (since all the inputs are known):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–æ¬¡é‡åˆ°é€å±‚å¤„ç†å±æ€§æ˜¯åœ¨ç¬¬ [8.3.2](#sec-layered-forwardprop) èŠ‚ä¸­ï¼Œä¸å‰å‘ä¼ æ’­æ–¹ç¨‹ç›¸å…³ã€‚è®©æˆ‘ä»¬åœ¨è®­ç»ƒç®€å•ç½‘ç»œçš„æƒ…å†µä¸‹å›é¡¾ä¸€ä¸‹è¿™ä¸€ç‚¹ã€‚è€ƒè™‘æ–¹ç¨‹
    [8.16](#eq-forwardprop-z-simple) å’Œ [8.17](#eq-forwardprop-a-simple)ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€äº›æƒé‡ *w*^((*l*))
    å’Œåç½® *b*^((*l*)) çš„å€¼åˆå§‹åŒ–ç³»ç»Ÿã€‚ä½¿ç”¨è¿™äº›å€¼ï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°ç¬¬ 0 å±‚çš„è¾“å‡ºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°è¯„ä¼° *z*^((0)) å’Œ *a*^((0))ï¼ˆå› ä¸ºæ‰€æœ‰è¾“å…¥éƒ½æ˜¯å·²çŸ¥çš„ï¼‰ï¼š
- en: '![](../../OEBPS/Images/eq_08-23-a.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_08-23-a.png)'
- en: 'Once we have *z*^((0)) and *a*^((0)), we can use them to evaluate *z*^((1))
    and *a*^((1)) via equations [8.16](#eq-forwardprop-z-simple) and [8.17](#eq-forwardprop-a-simple).
    But if we have *z*^((1)) and *a*^((1)), we can use them to evaluate *z*^((2))
    and *a*^((2)) via equations [8.16](#eq-forwardprop-z-simple) and [8.17](#eq-forwardprop-a-simple)
    again. And we can proceed in this fashion up to layer *L* to obtain *a*^((*L*)),
    which is the grand output of the MLP. In other words, we can iteratively evaluate
    the outputs of successive layers using *only* the outputs from the previous layer.
    No other layers need to be known. At any given iteration, we only have to keep
    the previous layer in memory: we can build the current layer from that. A single
    sequence of applications of equations [8.16](#eq-forwardprop-z-simple) and [8.17](#eq-forwardprop-a-simple)
    for layers 0 to *L* is known as a *forward pass*.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬æœ‰äº† *z*^((0)) å’Œ *a*^((0))ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒä»¬é€šè¿‡æ–¹ç¨‹ [8.16](#eq-forwardprop-z-simple) å’Œ
    [8.17](#eq-forwardprop-a-simple) æ¥è¯„ä¼° *z*^((1)) å’Œ *a*^((1))ã€‚ä½†å¦‚æœæˆ‘ä»¬æœ‰ *z*^((1)) å’Œ
    *a*^((1))ï¼Œæˆ‘ä»¬å†æ¬¡å¯ä»¥é€šè¿‡æ–¹ç¨‹ [8.16](#eq-forwardprop-z-simple) å’Œ [8.17](#eq-forwardprop-a-simple)
    ä½¿ç”¨å®ƒä»¬æ¥è¯„ä¼° *z*^((2)) å’Œ *a*^((2))ã€‚ä»¥æ­¤ç±»æ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€ç›´è¿›è¡Œåˆ°å±‚ *L*ï¼Œä»¥è·å¾— *a*^((*L*))ï¼Œè¿™æ˜¯å¤šå±‚æ„ŸçŸ¥å™¨ (MLP)
    çš„æœ€ç»ˆè¾“å‡ºã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è¿­ä»£åœ°è¯„ä¼°è¿ç»­å±‚çš„è¾“å‡ºï¼Œåªä½¿ç”¨å‰ä¸€å±‚çš„è¾“å‡ºã€‚ä¸éœ€è¦çŸ¥é“å…¶ä»–å±‚ã€‚åœ¨ä»»ä½•ç»™å®šè¿­ä»£ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦è®°ä½å‰ä¸€å±‚çš„è¾“å‡ºï¼šæˆ‘ä»¬å¯ä»¥ä»é‚£é‡Œæ„å»ºå½“å‰å±‚ã€‚å°†æ–¹ç¨‹
    [8.16](#eq-forwardprop-z-simple) å’Œ [8.17](#eq-forwardprop-a-simple) åº”ç”¨äºå±‚ 0 åˆ° *L*
    çš„å•ä¸ªåºåˆ—è¢«ç§°ä¸º *å‰å‘ä¼ æ’­*ã€‚
- en: A similar trick can be applied to evaluate the auxiliary variables, except we
    go *backward*. We can evaluate the auxiliary variable for the last layer, *Î´*^((*L*)),
    via equation [8.20](#eq-aux-lastlayer-simple). But once we have *Î´*^((*L*)), we
    can evaluate *Î´*^((*L* âˆ’ 1)) via equation [8.23](#eq-aux-simple). From that, we
    can evaluate *Î´*^((*L* âˆ’ 2)). We can proceed in this fashion all the way to layer
    0, evaluating successively *Î´*^((*L*)), *Î´*^((*L* âˆ’ 1)), â‹¯, *Î´*^((0)). Every time
    we evaluate a *Î´*^((*l*)), we can also evaluate the *âˆ‚*ğ•ƒ**/***âˆ‚w*^((*l*)) and
    *âˆ‚*ğ•ƒ**/***âˆ‚b*^((*l*)) for the same layer via equations [8.21](#eq-dw-aux-simple)
    and [8.22](#eq-db-aux-simple), respectively. We can also update the weight and
    bias of that layer right there using the just estimated partial derivatives, since
    the current values will never be needed again during training. Thus, starting
    from the last layer, we can update the weights and biases of all layers until
    layer 0 in this fashion. This is *backpropagation*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥åº”ç”¨ç±»ä¼¼çš„æŠ€å·§æ¥è¯„ä¼°è¾…åŠ©å˜é‡ï¼Œä½†æˆ‘ä»¬éœ€è¦è¿›è¡Œ*åå‘è®¡ç®—*ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ–¹ç¨‹ [8.20](#eq-aux-lastlayer-simple) è¯„ä¼°æœ€åä¸€å±‚çš„è¾…åŠ©å˜é‡ï¼Œ*Î´*^((*L*))ã€‚ä½†æ˜¯ä¸€æ—¦æˆ‘ä»¬æœ‰äº†
    *Î´*^((*L*))ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ–¹ç¨‹ [8.23](#eq-aux-simple) è¯„ä¼° *Î´*^((*L* âˆ’ 1))ã€‚ä»é‚£é‡Œï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼° *Î´*^((*L*
    âˆ’ 2))ã€‚æˆ‘ä»¬å¯ä»¥ä»¥æ­¤ç±»æ¨ï¼Œä¸€ç›´è®¡ç®—åˆ°å±‚ 0ï¼Œä¾æ¬¡è¯„ä¼° *Î´*^((*L*))ï¼Œ*Î´*^((*L* âˆ’ 1))ï¼Œâ‹¯ï¼Œ*Î´*^((0))ã€‚æ¯æ¬¡è¯„ä¼°ä¸€ä¸ª *Î´*^((*l*))ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æ–¹ç¨‹
    [8.21](#eq-dw-aux-simple) å’Œ [8.22](#eq-db-aux-simple) åˆ†åˆ«è¯„ä¼°åŒä¸€å±‚çš„ *âˆ‚*ğ•ƒ**/***âˆ‚w*^((*l*))
    å’Œ *âˆ‚*ğ•ƒ**/***âˆ‚b*^((*l*))ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨åˆšåˆšä¼°è®¡çš„åå¯¼æ•°ç«‹å³æ›´æ–°è¯¥å±‚çš„æƒé‡å’Œåç½®ï¼Œå› ä¸ºå½“å‰å€¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†ä¸å†éœ€è¦ã€‚å› æ­¤ï¼Œä»æœ€åä¸€å±‚å¼€å§‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥è¿™ç§æ–¹å¼æ›´æ–°æ‰€æœ‰å±‚çš„æƒé‡å’Œåç½®ï¼Œç›´åˆ°å±‚
    0ã€‚è¿™å°±æ˜¯*åå‘ä¼ æ’­*ã€‚
- en: 'Of course, we have to proceed in tandem: one forward propagation which sets
    the values of *z*s and *a*s) for layers 0 to *L*, followed by a backpropagation
    layer for *L* to 0. Repeat these steps until convergence.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæˆ‘ä»¬å¿…é¡»åŒæ—¶è¿›è¡Œï¼šä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œä¸ºå±‚ 0 åˆ° *L* è®¾ç½® *z* å’Œ *a* çš„å€¼ï¼Œç„¶åæ˜¯å±‚ *L* åˆ° 0 çš„åå‘ä¼ æ’­å±‚ã€‚é‡å¤è¿™äº›æ­¥éª¤ï¼Œç›´åˆ°æ”¶æ•›ã€‚
- en: NOTE Fully functional code for forward propagation, MSE loss, and backpropagation,
    executable via Jupyter Notebook, can be found at [http://mng.bz/pJrw](http://mng.bz/pJrw).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå‰å‘ä¼ æ’­ã€å‡æ–¹è¯¯å·®æŸå¤±å’Œåå‘ä¼ æ’­çš„å®Œæ•´ä»£ç ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯ä»¥åœ¨ [http://mng.bz/pJrw](http://mng.bz/pJrw)
    æ‰¾åˆ°ã€‚
- en: Listing 8.3 PyTorch code for forward and backward propagation
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 8.3 PyTorch ä»£ç ç”¨äºå‰å‘å’Œåå‘ä¼ æ’­
- en: '[PRE2]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: â‘  Forward propagation
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  å‰å‘ä¼ æ’­
- en: â‘¡ Computes MSE loss
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ è®¡ç®—å‡æ–¹è¯¯å·®æŸå¤±
- en: â‘¢ Arrays to store *Î´*^((*l*)), *âˆ‚*ğ•ƒ**/***âˆ‚w*^((*l*)), *âˆ‚*ğ•ƒ**/***âˆ‚b*^((*l*))
    for layers 0 to *L*
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¢ å­˜å‚¨å±‚ 0 åˆ° *L* çš„ *Î´*^((*l*))ï¼Œ*âˆ‚*ğ•ƒ**/***âˆ‚w*^((*l*))ï¼Œ*âˆ‚*ğ•ƒ**/***âˆ‚b*^((*l*)) çš„æ•°ç»„
- en: â‘£ Activation of the last layer - *a*^((*L*))
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: â‘£ æ¿€æ´»æœ€åä¸€å±‚ - *a*^((*L*))
- en: â‘¤ Computes the *Î´* and gradients for layer *L*
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¤ è®¡ç®—å±‚ *L* çš„ *Î´* å’Œæ¢¯åº¦
- en: â‘¥ Computes the *Î´* and gradients for layers 0 to *L* âˆ’ 1
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¥ è®¡ç®—å±‚ 0 åˆ° *L* âˆ’ 1 çš„ *Î´* å’Œæ¢¯åº¦
- en: Backpropagation algorithm on an arbitrary network of linear layers
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»»æ„çº¿æ€§å±‚ç½‘ç»œä¸Šçš„åå‘ä¼ æ’­ç®—æ³•
- en: In section [8.4.5.1](#sec-backprop-simplenet), we saw a simple network with
    only one neuron per layer. There was only one connection and hence one weight,
    one activation, and one auxiliary variable per layer. Consequently, we could drop
    the subscripts (although we had to keep the superscript indicating the layer)
    of all these variables. Now we examine a more generic network consisting of linear
    layers 0, â‹¯, *L*. An arbitrary slice of this network is shown in figure [8.6](#fig-linear-layer).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [8.4.5.1](#sec-backprop-simplenet) èŠ‚ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€ä¸ªåªæœ‰æ¯å±‚ä¸€ä¸ªç¥ç»å…ƒçš„ç®€å•ç½‘ç»œã€‚åªæœ‰ä¸€ä¸ªè¿æ¥ï¼Œå› æ­¤æ¯å±‚åªæœ‰ä¸€ä¸ªæƒé‡ã€ä¸€ä¸ªæ¿€æ´»å’Œä¸€ä¸ªè¾…åŠ©å˜é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœç•¥æ‰€æœ‰è¿™äº›å˜é‡çš„ä¸‹æ ‡ï¼ˆå°½ç®¡æˆ‘ä»¬å¿…é¡»ä¿ç•™è¡¨ç¤ºå±‚çš„ä¸Šæ ‡ï¼‰ã€‚ç°åœ¨æˆ‘ä»¬è€ƒå¯Ÿä¸€ä¸ªæ›´é€šç”¨çš„ç½‘ç»œï¼Œç”±çº¿æ€§å±‚
    0ï¼Œâ‹¯ï¼Œ*L* ç»„æˆã€‚è¿™ä¸ªç½‘ç»œçš„ä»»æ„åˆ‡ç‰‡å¦‚å›¾ [8.6](#fig-linear-layer) æ‰€ç¤ºã€‚
- en: The ultimate goal is to evaluate the partial derivatives of the loss with respect
    to the weights and biases. Using them, we can update the current weights and biases
    to optimally reduce the loss.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆç›®æ ‡æ˜¯è¯„ä¼°æŸå¤±ç›¸å¯¹äºæƒé‡å’Œåç½®çš„åå¯¼æ•°ã€‚ä½¿ç”¨å®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–°å½“å‰çš„æƒé‡å’Œåç½®ï¼Œä»¥æœ€ä¼˜åœ°å‡å°‘æŸå¤±ã€‚
- en: Our overall strategy is as follows. We use the auxiliary variables again. We
    first derive expressions that allow us to compute the auxiliary variable for the
    last layer. Then we derive an expression that allows us to compute auxiliary variables
    for an arbitrary layer, *l*, given the auxiliary variables for layer *l* + 1.
    Since we can directly compute auxiliary variables for the last layer, *L*, we
    can use this expression to compute auxiliary variables for the second-to-last
    layer *L* âˆ’ 1. But once we have them, we can compute auxiliary variables for layer
    *L* âˆ’ 2. We proceed like this until we reach layer 0. Thus we can compute all
    the auxiliary variables. We also derive expressions that allow us to compute,
    from the auxiliary variables, the partial derivatives of loss with respect to
    weights and biases. This gives us everything we need. Since we start by computing
    things pertaining to the last layer and proceed iteratively toward the initial
    layer, the process is called *backpropagation*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice the similarity between the expressions derived next and those
    derived for the one-neuron-per-layer network. The differences are explained:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '*Forward propagation (arbitrary layer l)*â€”Forward propagation through this
    network has already been described in section [8.3.1](#sec-linlayer-matmult) and
    can be succinctly represented by equation [8.7](../Text/08.xhtml#eq-linlayer-forwardprop)
    repeated here for handy reference). On the left are the scalar equations, for
    one neuron at a time; and on the right are the vector equations, for the entire
    layer. They are equivalent:'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-24.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: Equation 8.24
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Indices *j* and *k* iterate over all the neurons in the relevant layer. By convention,
    we always use these variables for arbitrary neurons in a layer. The variable *l*
    is used to index the layers. When indexing weights, we typically use *j* to indicate
    the destination and *k* to indicate the sourceâ€”remember that weights are indexed
    (destination, source) somewhat unexpectedly to simplify the math. Typically, vectors
    correspond to entire layers. Individual vector elements correspond to specific
    neurons and are indexed by *j* or *k*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss*â€”Unlike the simple network, here, the final *L*th layer can have multiple
    neurons. Hence the loss function becomes'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-25.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: Equation 8.25
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'where the summation happens over all neurons in the last layer. Note that ![](../../OEBPS/Images/AR_a.png)^((*L*))
    is the output of the MLP: that is, ![](../../OEBPS/Images/AR_a.png)^((*L*)) =
    ![](../../OEBPS/Images/AR_y.png) = *MLP*(![](../../OEBPS/Images/AR_x.png)) for
    the training input ![](../../OEBPS/Images/AR_x.png) (see equation [8.10](#eq-forwardprop-layered)).
    The GT output corresponding to ![](../../OEBPS/Images/AR_x.png) is the constant
    vector *È³*. The closer ![](../../OEBPS/Images/AR_y.png) is to *È³*, the smaller
    the loss. Note that we need to average the loss over the entire training data
    set. Here we are showing the loss computation for a single training data instance.
    The computation simply needs to be replicated for each training data instance,
    and the results averaged.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ±‚å’Œå‘ç”Ÿåœ¨æœ€åä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒä¸Šã€‚æ³¨æ„ï¼Œ![](../../OEBPS/Images/AR_a.png)^((*L*))æ˜¯MLPçš„è¾“å‡ºï¼Œå³![](../../OEBPS/Images/AR_a.png)^((*L*))
    = ![](../../OEBPS/Images/AR_y.png) = *MLP*(![](../../OEBPS/Images/AR_x.png))å¯¹äºè®­ç»ƒè¾“å…¥![](../../OEBPS/Images/AR_x.png)ï¼ˆè§æ–¹ç¨‹å¼[8.10](#eq-forwardprop-layered)ï¼‰ã€‚å¯¹åº”äº![](../../OEBPS/Images/AR_x.png)çš„GTè¾“å‡ºæ˜¯å¸¸æ•°å‘é‡*È³*ã€‚![](../../OEBPS/Images/AR_y.png)è¶Šæ¥è¿‘*È³*ï¼ŒæŸå¤±è¶Šå°ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šå¹³å‡æŸå¤±ã€‚è¿™é‡Œæˆ‘ä»¬å±•ç¤ºäº†å•ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹çš„æŸå¤±è®¡ç®—ã€‚è®¡ç®—åªéœ€ä¸ºæ¯ä¸ªè®­ç»ƒæ•°æ®å®ä¾‹å¤åˆ¶ï¼Œç„¶åå–å¹³å‡å€¼ã€‚
- en: '*Auxiliary variables*â€”Now that a layer has multiple neurons, we have one auxiliary
    variable per neuron. Thus the auxiliary variable has a subscript identifying the
    specific neuron in that layer. It continues to have a superscript indicating its
    layer. We define'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è¾…åŠ©å˜é‡*â€”â€”ç”±äºä¸€ä¸ªå±‚æœ‰å¤šä¸ªç¥ç»å…ƒï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªç¥ç»å…ƒæœ‰ä¸€ä¸ªè¾…åŠ©å˜é‡ã€‚å› æ­¤ï¼Œè¾…åŠ©å˜é‡æœ‰ä¸€ä¸ªä¸‹æ ‡æ¥æ ‡è¯†è¯¥å±‚ä¸­çš„ç‰¹å®šç¥ç»å…ƒã€‚å®ƒç»§ç»­æœ‰ä¸€ä¸ªä¸Šæ ‡æ¥è¡¨ç¤ºå…¶å±‚ã€‚æˆ‘ä»¬å®šä¹‰'
- en: '![](../../OEBPS/Images/eq_08-25-a.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼8-25-a](../../OEBPS/Images/eq_08-25-a.png)'
- en: '*Auxiliary variable* *for the last layer*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æœ€åä¸€å±‚çš„è¾…åŠ©å˜é‡*'
- en: '![](../../OEBPS/Images/eq_08-25-b.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼8-25-b](../../OEBPS/Images/eq_08-25-b.png)'
- en: Using equation [8.25](#eq-loss-mlp) and observing that only one of the terms
    in the summationâ€”the *j*th termâ€”will survive the differentiation with respect
    to *a[j]*^((*L*)) since the *a[j]*s are independent of each other), we get
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ–¹ç¨‹å¼[8.25](#eq-loss-mlp)å¹¶è§‚å¯Ÿï¼Œç”±äº*a[j]*så½¼æ­¤ç‹¬ç«‹ï¼Œå› æ­¤åœ¨ç›¸å¯¹äº*a[j]*^((*L*))çš„å¾®åˆ†è¿‡ç¨‹ä¸­ï¼Œæ±‚å’Œä¸­åªæœ‰ä¸€ä¸ªé¡¹â€”â€”ç¬¬*j*é¡¹ä¼šå¹¸å­˜ï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '![](../../OEBPS/Images/eq_08-25-c.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼8-25-c](../../OEBPS/Images/eq_08-25-c.png)'
- en: Also, using the lower-left equation from [8.24](../Text/08.xhtml#eq-MLP-out-nosubscript-1)
    and equation [8.5](../Text/08.xhtml#eq-sigmoid-derivative), we get
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä½¿ç”¨[8.24](../Text/08.xhtml#eq-MLP-out-nosubscript-1)ä¸­çš„å·¦ä¸‹æ–¹ç¨‹å’Œæ–¹ç¨‹å¼[8.5](../Text/08.xhtml#eq-sigmoid-derivative)ï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '![](../../OEBPS/Images/eq_08-25-d.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼8-25-d](../../OEBPS/Images/eq_08-25-d.png)'
- en: Combining these, we get
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆè¿™äº›ï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '![](../../OEBPS/Images/eq_08-26.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼8-26](../../OEBPS/Images/eq_08-26.png)'
- en: Equation 8.26
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼8.26
- en: '![](../../OEBPS/Images/eq_08-27.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼8-27](../../OEBPS/Images/eq_08-27.png)'
- en: Equation 8.27
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼8.27
- en: Here, âˆ˜ denotes the Hadamard product between two vectors. It is basically a
    vector of elementwise products of corresponding vector elements. Thus,
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œâˆ˜ è¡¨ç¤ºä¸¤ä¸ªå‘é‡ä¹‹é—´çš„Hadamardç§¯ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªå…ƒç´ çº§å¯¹åº”å‘é‡å…ƒç´ ä¹˜ç§¯çš„å‘é‡ã€‚å› æ­¤ï¼Œ
- en: '![](../../OEBPS/Images/eq_08-28.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼8-28](../../OEBPS/Images/eq_08-28.png)'
- en: Equation 8.28
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼8.28
- en: '![](../../OEBPS/Images/eq_08-29.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼8-29](../../OEBPS/Images/eq_08-29.png)'
- en: Equation 8.29
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼8.29
- en: Equations [8.26](#eq-auxvar-last-layer-scalar) and [8.27](#eq-auxvar-last-layer-vector)
    are identical. The former is a scalar equation expressing individual auxiliary
    variables of the last layer. The latter is a vector equation expressing all the
    auxiliary variables of the last layer together. We can compute these directly
    if we have performed a forward pass and have its results, the *a[j]*^((*L*))s
    available along with the training data GT.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼[8.26](#eq-auxvar-last-layer-scalar)å’Œ[8.27](#eq-auxvar-last-layer-vector)æ˜¯ç›¸åŒçš„ã€‚å‰è€…æ˜¯ä¸€ä¸ªè¡¨ç¤ºæœ€åä¸€å±‚å•ä¸ªè¾…åŠ©å˜é‡çš„æ ‡é‡æ–¹ç¨‹ã€‚åè€…æ˜¯ä¸€ä¸ªè¡¨ç¤ºæœ€åä¸€å±‚æ‰€æœ‰è¾…åŠ©å˜é‡çš„å‘é‡æ–¹ç¨‹ã€‚å¦‚æœæˆ‘ä»¬å·²ç»æ‰§è¡Œäº†å‰å‘ä¼ é€’å¹¶æœ‰äº†å…¶ç»“æœï¼ŒåŒ…æ‹¬*a[j]*^((*L*))sä»¥åŠè®­ç»ƒæ•°æ®çš„GTï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è®¡ç®—è¿™äº›æ–¹ç¨‹ã€‚
- en: '*Auxiliary variable for an arbitrary layer, l*â€”This is significantly different
    and harder to understand than the one-neuron-per-layer case. We are trying to
    evaluate *Î´[j]*^((*l*)) = *âˆ‚*ğ•ƒ**/***âˆ‚z[j]*^((*l*)) in the general case: that is,
    for an arbitrary layer *l*. The loss does not *directly* depend on the inner layer
    variable *z[j]*^((*l*)). The loss directly depends only on the last layer activations,
    which depend on the previous layer, and so forth. The *z*s in any one layer form
    a *complete* dependency set for the loss ğ•ƒ, meaning the loss can be expressed
    in terms of only these and no other variables. In particular, we can express the
    loss as ğ•ƒ(*z*[0]^((*l*+1)), *z*[1]^((*l*+1)), *z*[2]^((*l*+1)),â‹¯). You can form
    a mental picture that *z[j]*^((*l*)) fans out to ğ•ƒ *through* all the *z*s in the
    next layer, *z*[0]^((*l*+1)), *z*[1]^((*l*+1)), *z*[2]^((*l*+1)), and so on. Then,
    using the chain rule of partial differentiation,'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-29-a.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: Now, by definition,
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-29-b.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: And using equation [8.24](../Text/08.xhtml#eq-MLP-out-nosubscript-1),
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-29-c.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: while
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-29-d.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: 'Combining all these, we get the scalar expression for a single auxiliary variable.
    It is presented here along with its equivalent vector equation for the entire
    layer:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-30.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Equation 8.30
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-31.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Equation 8.31
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Here, âˆ˜ denotes the Hadamard multiplication explained earlier and *W*^((+1*l*))
    is the matrix representing the weights of *all connections from layer l to layer
    (*l*+1)* (see equation [8.8](../Text/08.xhtml#eq-MLP-weight-matrix)). Equations
    [8.30](#eq-auxvar-scalar) and [8.31](../Text/08.xhtml#eq-auxvar-vector) allow
    us to evaluate *Î´*^((*l*))s from the *Î´*^((*l*+1))s if the results of forward
    propagation (*a*s) are available. We have already shown that the auxiliary variables
    for the last layer are directly computable from the activations of that layer.
    Hence, we can evaluate all the layersâ€™ auxiliary variables.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '*Derivatives of loss with respect to weights and biases in terms of auxiliary
    variables*â€”We have already seen how to compute auxiliary variables. Now we will
    express the partial derivatives of loss with respect to weights and biases in
    terms of those. This will provide us with the gradients we need to update the
    weights and biases along the negative gradient, which is the optimal move to minimize
    loss:'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-32.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: Equation 8.32
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-33.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: Equation 8.33
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Equations [8.32](#eq-partialderiv-loss-wt-scalar) and [8.33](../Text/08.xhtml#eq-partialderiv-loss-wt-vector)
    are equivalent. The first is scalar and pertains to individual weights in layer
    *l*, and the second describes the entire layer. Similarly, equations [8.34](#eq-partialderiv-loss-bias-scalar)
    and [8.35](#eq-partialderiv-loss-bias-vector) are equivalent:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-34.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: Equation 8.34
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-35.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Equation 8.35
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The first is scalar and pertains to individual biases in layer *l*, and the
    second describes the entire layer.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.6 Putting it all together: Overall training algorithm'
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previously, we discussed forward propagation: passing an input vector ![](../../OEBPS/Images/AR_x.png)
    through a sequence of linear layers and generating an output prediction. We learned
    about MSE loss, ğ•ƒ, which calculates the deviation of the output prediction from
    the GT, *y*. We also learned to compute the gradients of ğ•ƒ with respect to parameters
    *W* and *b* using backpropagation. In the following algorithm, we describe how
    these components come together in the training process:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 8.5 Training a neural network
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Initialize ![](../../OEBPS/Images/AR_w.png), *b* with random values
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '**while** ğ•ƒ > *threshold* **do**'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: âŠ³ Forward pass
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '**for** *l* â† 0 to *L* **do**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_z.png)^((*l*)) = *W*^((*l*)) ![](../../OEBPS/Images/AR_a.png)^((*l*â€“1))
    + ![](../../OEBPS/Images/AR_b.png)^((*l*))'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: '![](../../OEBPS/Images/AR_a.png)^((*l*)) = *Ïƒ*(![](../../OEBPS/Images/AR_z.png)^((*l*)))'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: '**end** **for**'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: âŠ³ Loss
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: ğ•ƒ = 1/2 ||![](../../OEBPS/Images/AR_a.png)^((*L*)) â€“ *È³*||Â²
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: âŠ³ Gradients for the last layer
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_delta.png) ^((*L*)) = (![](../../OEBPS/Images/AR_a.png)^((*L*))
    â€“ *È³*) âˆ˜ ![](../../OEBPS/Images/AR_a.png)^((*L*)) âˆ˜ (![](../../OEBPS/Images/AR_1.png)
    â€“ ![](../../OEBPS/Images/AR_a.png)^((*L*)))'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: â–½*W*^((*L*))ğ•ƒ = ![](../../OEBPS/Images/AR_delta.png) ^((*L*))(![](../../OEBPS/Images/AR_a.png)^((*L*â€“1)))^T
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: â–½*b*^((*L*))ğ•ƒ = ![](../../OEBPS/Images/AR_delta.png) ^((*L*))
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: âŠ³ Gradients for the remaining layers
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '**for** *l* â† *L* â€“ 1 to 0 **do**'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_delta.png) ^((*l*)) = ((*W*^((*l*+1)))*^T* ![](../../OEBPS/Images/AR_delta.png)^((*l*+1)))
    ![](../../OEBPS/Images/AR_a.png)^((*l*)) âˆ˜ (![](../../OEBPS/Images/AR_1.png) â€“
    ![](../../OEBPS/Images/AR_a.png)^((*l*)))'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
- en: â–½*W*^((*l*))ğ•ƒ = ![](../../OEBPS/Images/AR_delta.png) ^((*l*)) (![](../../OEBPS/Images/AR_a.png)^((*l*â€“1)))^T
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: â–½*b*^((*l*))ğ•ƒ = ![](../../OEBPS/Images/AR_delta.png) ^((*l*))
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '**end** **for**'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: âŠ³ Parameter update
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '*W* = *W* â€“ *r*â–½*[W]*ğ•ƒ'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '*b* = *b* â€“ *r*â–½*[b]*ğ•ƒ'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '**end** **while**'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Training a neural network in PyTorch
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that weâ€™ve seen how the training process works, letâ€™s look at how this
    can be implemented in PyTorch. For this purpose, letâ€™s take the following example.
    Consider an e-commerce company thatâ€™s trying to solve the problem of demand prediction:
    the company would like to estimate the number of mobile phones that will be sold
    in the upcoming week so that it can manage its inventory accordingly. Our goal
    is to develop a model that can make such a prediction. Letâ€™s assume that the demand
    for a given week is a function of three variables: (a) the number of mobile phones
    sold in the previous week, (b) discounts offered, and (c) the number of weeks
    to the next holiday. Letâ€™s call these variables `prev_week_sales`, `discount_fraction`,
    and `weeks_to_next_holidays`, respectively. This example can be modeled as a regression
    problem wherein we predict the number of mobile phones sold in the upcoming week
    from an input vector of the form [`prev_week_sales`, `discount_fraction`, `weeks_to_next_holidays`].'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/O1Ra](http://mng.bz/O1Ra).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: From historical data, we generate a large data set, `X`, that contains the values
    of the three variables for the last *N* weeks. `X` is represented as an *N* x
    3 matrix, with each row representing an individual training data instance and
    *N* being the total number of data points available. We also have a GT vector
    *È³* of length *N*, containing the actual sales of mobile phones for each of the
    weeks in the training data set. Table [8.1](#tab-demand-prediction-data-set) shows
    sample data points from our training set.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Sample training data for demand prediction
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '| Previous week sales | Discount fraction (%) | Weeks to next holidays | Number
    of units sold |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| 76,440 | 63 | 2 | 94,182 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| 41,512 | 50 | 3 | 51,531 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| 77,395 | 77 | 9 | 95,938 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| â€¦ | â€¦ | â€¦ | â€¦ |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| 21,532 | 70 | 4 | 28,559 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: NOTE In this section, `X` and *È³* refer to the entire batch of training data
    instances. This may be infeasible in practical settings because of large data
    sets. To address this, we typically use mini-batches of `X` and *È³*. We introduce
    the concept of mini-batches formally in the next chapter.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'One important point about the data set is that the range of values for each
    feature is completely different. For example, the previous weekâ€™s sales are expressed
    as a number on the order of tens of thousands of units, whereas the discount fraction
    is a percentage number between 0 and 100\. In machine learning, it is a good practice
    to bring all the values to a common scale, because doing so can help improve the
    speed of training and reduce the chance of getting stuck at a local minimum. For
    our example, letâ€™s use min-max normalization to scale all the features to 0â€“1\.
    The following code snippet shows how to perform min-max normalization in PyTorch.
    For the rest of the discussion, we assume that we are operating on the normalized
    data:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: â‘  Clones the data so as not to mutilate the original data
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Calculates the min and max values of each column of *X*
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Normalizes *X*
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Calculates the min and max values of *y*
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Normalizes *y*
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the regression problem, letâ€™s first define a two-layer neural network
    model that can take in 3D input vectors of the form [`prev_week_sales`, `discount_fraction`,
    `is_holidays_ongoing`] and generate output predictions. The following code snippet
    gives the PyTorch implementation:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: â‘  Defines the network as a sequence of linear and sigmoid layers
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ First hidden layer with a weight matrix of size input_size Ã— hidden1_size)
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Second hidden layer with a weight matrix of size hidden1_size Ã— hidden2_size)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Output layer with a weight matrix of size hidden2_size Ã— output_size)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ *X* is an *N* Ã— 3 matrix. Each row is a (3D vector) representing a single
    data point.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Neural network models in PyTorch should subclass `torch.nn.Module` and implement
    the `forward` method. Our two-layer neural network contains two linear layers,
    each followed by a sigmoid (nonlinear) activation layer. Finally, we have a linear
    layer that converts the final activation into the output prediction. These layers
    are chained together using the `torch.nn.Sequential` class to form the two-layer
    neural network. Whenever our model is called using `nn(X)`, the `forward` method
    is invoked, and the input `X` is passed through the individual layers to obtain
    the final output.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined the neural network and its forward pass, we need to
    define the loss function. We can use the MSE loss defined in equation [8.11](../Text/08.xhtml#eq-mse-loss).
    The loss function essentially compares the demand predicted by the neural network
    model with the actual demand from the GT and returns larger values when the difference
    is higher and smaller values when the difference is lower. MSE loss is readily
    available in PyTorch through the `torch.nn.MSELoss` class. The following code
    snippet shows a sample invocation:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: â‘  Instantiates the loss function
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'â‘¡ compute loss _pred: Output of the neural network _gt: ground truth'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need a way to compute the gradients of the loss with respect to
    the parameters of our model so we can start the training process. Luckily, we
    donâ€™t have to explicitly compute the gradients ourselves because PyTorch automatically
    does this for us using automatic differentiation, aka autograd. (Refer to section
    [3.1](../Text/03.xhtml#sec3_1) for more details about autograd.) For our current
    example, we can instruct PyTorch to run backpropagation and compute gradients
    by calling `loss.backward()`. With this, weâ€™re ready to start training. PyTorch
    code for training the neural network is shown next.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4 Training a neural network
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: â‘  Instantiates the neural network
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Instantiates the loss function
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Instantiates the optimizer
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Training loop
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Forward pass
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: â‘¥ Computes the loss
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: â‘¦ Clears the gradients and prevents accumulation of gradients from the previous
    step
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: â‘§ Runs backpropagation (computes gradients)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: â‘¨ Updates the weights
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: In the training loop, we iteratively run the forward pass, compute the loss,
    calculate the gradients, and update the weights. The neural network is initialized
    with random weights and hence makes arbitrary predictions for the demand in the
    early iterations of the training loop. This translates to a high initial loss
    value. However, as training proceeds, the weights are updated to minimize the
    loss value, and the predicted demand comes closer to the actual GT. To update
    the weights, we use what is known as an *optimizer*. During training, the gradients
    are computed by calling the `backward()` function on the `loss` object. Following
    that, the `optimizer.step` call updates the weights and biases. In this example,
    we used the stochastic gradient descentâ€“based optimizer, which can be invoked
    using `torch.optim.SGD`. PyTorch offers various optimizers, such as Adam, AdaGrad,
    and so on, which will be discussed in detail in the next chapter. We typically
    run the training loop until the loss reaches a value low enough to be acceptable.
    Once the training loop completes, we have a model that can readily take in new
    data points and generate output predictions.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sigmoid function *Ïƒ*(*x*) = 1/1+*e^(â€“x)* has an S-shaped graph, is a differential
    version of the Heaviside step function, and, as such, is used in perceptrons.
    Thus the overall perceptron function becomes *P*(![](../../OEBPS/Images/AR_x.png))
    â‰¡ *Ïƒ*(![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*).
    It is parametrized by ![](../../OEBPS/Images/AR_w.png) and *b*, which control
    the slope and position, respectively, of the S-shaped curve.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks solve real-life problems that require intelligence by approximating
    the function that solves the problem in question. They are built of multiple perceptrons
    interconnected by weighted edges. Instead of connecting perceptrons haphazardly,
    we connect them as layers. In a layered network, a perceptron is only connected
    to perceptrons from the immediately preceding layer. Intra-layer and other connections
    are not allowed.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised neural networks have manually generated outputs for a sample set
    of input values (ground truth). This entire data set consisting of inputs and
    known outputs is known as the training data set.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss is defined as the mismatch between the ground truth and actual output generated
    by the neural network on training data inputs. The simplest way to compute loss
    is to take the Euclidean distance between the neural network-generated output
    and ground-truth vectors. This is called the MSE (mean squared error) loss. Mathematically,
    given a training data set ğ•‹ that is a set of <input, GT output> pairs ğ•‹ = {âŸ¨![](../../OEBPS/Images/AR_x.png),
    *È³*âŸ©}, the loss can be expressed as
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-35-a.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: where the output is ![](../../OEBPS/Images/AR_y.png)*[i]* = *MLP*(![](../../OEBPS/Images/AR_x.png)*[i]*).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Training is the process of optimizing the connection weights and biases of a
    specific neural network so that the loss is minimal. Note that during inferencing,
    the neural network typically sees data it has never seen during training. Inferencing
    outputs are good only if the distribution of training inputs roughly matches the
    overall input distribution.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We minimize the loss by iteratively adjusting the weights and biases. The quickest
    way to reach the closest minimum of a multivariate function is to follow the gradient.
    Hence, we adjust the weights and biases following the gradient of the loss function.
    Mathematically,
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-35-b.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: 'A forward pass is the process of generating outputs from inputs with a neural
    network: more specifically, a multilayer perceptron MLP). Thus an MLP does inferencing
    via a forward pass. A beautiful property of a layered network is that we can do
    a forward pass dealing with one layer at a time, proceeding iteratively from layer
    0 (closest to the input) to the output layer. Mathematically,'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-35-c.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: where *W*^((*l*)), ![](../../OEBPS/Images/AR_b.png)^((*l*)) represent the weights
    and biases for layer *l*, and ![](../../OEBPS/Images/AR_a.png)^((*l*)) represent
    the output for layer *l* activation), which is also the input for layer *l* +
    1.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: A backward pass is the process by which the gradients of the loss with respect
    to all the weights and biases are generated. It relies on the result of the preceding
    forward pass and proceeds from the output layer toward the input layer. It uses
    auxiliary variables ![](../../OEBPS/Images/AR_bw.png)^((*l*)), which can be computed
    by iterating backward from the last (closest to output) layer to the first (closest
    to input) layerâ€”hence the name *backward propagation*â€”and all the required gradients
    can be computed from those auxiliary variables. Mathematically,
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_08-35-d.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: Training progresses by alternating forward and backward passes on the training
    data set.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
