- en: 13 Music generation with MuseGAN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 使用 MuseGAN 进行音乐生成
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Music representation using musical instrument digital interface
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用乐器数字接口进行音乐表示
- en: Treating music generation as an object creation problem similar to image generation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将音乐生成视为与图像生成类似的对象创建问题
- en: Building and training a generative adversarial network to generate music
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练生成对抗网络以生成音乐
- en: Generating music using the trained MuseGAN model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的 MuseGAN 模型生成音乐
- en: Up to now, we have successfully generated shapes, numbers, images, and text.
    In this chapter and the next, we will explore two different ways of generating
    lifelike music. This chapter will apply the techniques from image GANs, treating
    a piece of music as a multidimensional object akin to an image. The generator
    will produce a complete piece of music and submit it to the critic (serving as
    the discriminator because we use the Wasserstein distance with gradient penalty,
    as discussed in chapter 5) for evaluation. The generator will then modify the
    music based on the critic’s feedback until it closely resembles real music from
    the training dataset. In the next chapter, we will treat music as a sequence of
    musical events, employing natural language processing (NLP) techniques. We will
    use a GPT-style Transformer to predict the most probable musical event in a sequence
    based on previous events. This Transformer will generate a long sequence of musical
    events that can be converted into realistic-sounding music.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经成功生成了形状、数字、图像和文本。在本章和下一章中，我们将探讨两种不同的生成逼真音乐的方法。本章将应用图像 GANs 的技术，将音乐视为类似于图像的多维对象。生成器将生成一首完整的音乐作品并将其提交给评论家（作为判别器，因为我们使用
    Wasserstein 距离和梯度惩罚，如第 5 章所述）进行评估。然后，生成器将根据评论家的反馈修改音乐，直到它与训练数据集中的真实音乐非常相似。在下一章中，我们将音乐视为一系列音乐事件，采用自然语言处理（NLP）技术。我们将使用
    GPT 风格的 Transformer 来根据先前的事件预测序列中最可能的音乐事件。这个 Transformer 将生成一系列音乐事件，这些事件可以转换为听起来逼真的音乐。
- en: The field of music generation using AI has gained significant attention; MuseGAN
    is a prominent model, which was introduced by Dong, Hsiao, Yang, and Yang in 2017.^([1](#footnote-000))
    MuseGAN is a deep neural network that utilizes generative adversarial networks
    (GANs) to create multitrack music, with the word Muse signifying the creative
    inspiration behind music. The model is adept at understanding the complex interactions
    between different tracks that represent different musical instruments or different
    voices (which is the case in our training data). As a result, MuseGAN can generate
    compositions that are harmonious and cohesive.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人工智能进行音乐生成的领域已经引起了广泛关注；MuseGAN 是一个突出的模型，由 Dong、Hsiao、Yang 和 Yang 在 2017 年提出。[1](#footnote-000)
    MuseGAN 是一个深度神经网络，利用生成对抗网络（GANs）来创建多轨音乐，其中的“Muse”一词象征着音乐背后的创造性灵感。该模型擅长理解代表不同乐器或不同声音的不同轨道之间的复杂交互（这在我们的训练数据中是这种情况）。因此，MuseGAN
    可以生成和谐且统一的乐曲。
- en: 'MuseGAN, similar to other GAN models, consists of two primary components: the
    generator and the critic (who provides a continuous measure of how real the sample
    is rather than classifying a sample into real or fake). The generator’s task is
    to generate music, whereas the critic assesses the music’s quality and offers
    feedback to the generator. This adversarial interaction enables the generator
    to gradually improve, leading to the creation of more realistic and appealing
    music.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他 GAN 模型类似，MuseGAN 由两个主要组件组成：生成器和评论家（评论家提供对样本真实性的连续度量，而不是将样本分类为真实或虚假）。生成器的任务是生成音乐，而评论家评估音乐的质量并向生成器提供反馈。这种对抗性交互使得生成器能够逐步改进，从而创造出更真实、更具吸引力的音乐。
- en: Suppose you’re an avid fan of Johann Sebastian Bach and have listened to all
    his compositions. You might wonder if it’s possible to use MuseGAN to create synthetic
    music that mimics his style. The answer is yes, and you’ll learn how to do that
    in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一位热衷于约翰·塞巴斯蒂安·巴赫的粉丝，并且已经听过他所有的作品。你可能想知道是否可以使用 MuseGAN 创建模仿他风格的合成音乐。答案是肯定的，你将在本章中学习如何做到这一点。
- en: 'Specifically, you’ll first explore how to represent a piece of multitrack music
    as a multidimensional object. A track is essentially an individual line of music
    or sound, which can be a different instrument such as piano, bass, or drums or
    a different voice such as soprano, alto, tenor, or bass. When composing a track
    in electronic music, you typically organize it into bars (segments of time), subdivide
    each bar into steps for finer control over rhythm, and then assign a specific
    note to each step to craft your melodies and rhythms. As a result, each piece
    of music in our training set is structured with a (4, 2, 16, 84) shape: this means
    there are four music tracks, with each track consisting of 2 bars, each bar containing
    16 steps, and each step capable of playing one of the 84 different notes.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你将首先探索如何将一段多轨音乐表示为一个多维对象。一个轨道本质上是一行音乐或声音，可以是不同的乐器，如钢琴、贝斯或鼓，或者不同的声音，如女高音、女低音、男高音或男低音。在创作电子音乐中的轨道时，你通常将其组织成小节（时间段），然后将每个小节细分为步骤以更好地控制节奏，接着为每个步骤分配一个特定的音符来创作旋律和节奏。因此，我们训练集中的每首音乐都是以（4，2，16，84）的形状结构化的：这意味着有四个音乐轨道，每个轨道由2个小节组成，每个小节包含16个步骤，每个步骤可以播放84种不同音符中的任意一种。
- en: The style of the music generated by our MuseGAN will be influenced by the training
    data. Since you are interested in Bach’s work, you’ll be training MuseGAN with
    The JSB Chorales dataset, which is a collection of chorales composed by Bach,
    arranged for four tracks. These chorales have been converted into a piano roll
    representation, a method used for visualizing and encoding music, especially for
    digital processing purposes. You’ll learn how to transform a piece of music represented
    in the shape of (4, 2, 16, 84) into a musical instrument digital interface (MIDI)
    file, which can then be played on your computer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的MuseGAN音乐的风格将受到训练数据的影响。由于你对巴赫的作品感兴趣，你将使用《JSB圣歌集》数据集来训练MuseGAN，这是一个由巴赫创作的圣歌集合，为四个轨道编排。这些圣歌已被转换为钢琴卷表示，这是一种用于可视化和编码音乐的方法，特别是用于数字处理目的。你将学习如何将形状为（4，2，16，84）的音乐作品转换为音乐乐器数字接口（MIDI）文件，然后可以在你的电脑上播放。
- en: While the generator uses just one single noise vector from the latent space
    to generate different formats of content such as shapes, numbers, and images in
    earlier chapters, the generator in MuseGAN will use four noise vectors when producing
    a piece of music. The use of four separate noise vectors (chords, style, melody,
    and groove, which I’ll explain in detail later in this chapter) is a design choice
    that allows for greater control and diversity in the music generation process.
    Each of these noise vectors represents a different aspect of music, and by manipulating
    them individually, the model can generate more complex and nuanced compositions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在前面章节中，生成器仅使用来自潜在空间的一个单一噪声向量来生成不同格式的内容，如形状、数字和图像，但MuseGAN中的生成器在生成音乐时会使用四个噪声向量。使用四个独立的噪声向量（和弦、风格、旋律和节奏，我将在本章后面详细解释）是设计选择，它允许在音乐生成过程中有更大的控制和多样性。这些噪声向量中的每一个都代表音乐的不同方面，通过单独操纵它们，模型可以生成更复杂和细腻的作品。
- en: Once the model is trained, we’ll discard the critic network, a common practice
    in GAN models. We’ll then utilize the trained generator to produce music pieces
    by inputting four noise vectors from the latent space. The music generated in
    this way closely mirrors the style of Bach.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，我们将丢弃批评网络，这是GAN模型中的常见做法。然后，我们将利用训练好的生成器，通过输入来自潜在空间的四个噪声向量来生成音乐作品。以这种方式生成的音乐与巴赫的风格非常相似。
- en: 13.1 Digital music representation
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 数字音乐表示
- en: Our goal is to master the art of building and training a GAN model from scratch
    for music generation. To achieve this, we need to start with the fundamentals
    of music theory, including understanding musical notes, octaves, and pitch numbers.
    Following that, we’ll dive into the inner workings of digital music, specifically
    focusing on MIDI files.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是掌握从头开始构建和训练用于音乐生成的GAN模型的艺术。为了实现这一目标，我们需要从音乐理论的基础知识开始，包括理解音符、八度和音高数字。随后，我们将深入研究数字音乐的内部工作原理，特别是关注MIDI文件。
- en: 'Depending on the type of machine learning model we use for music generation,
    the representation of a piece of music in digital form will vary. For instance,
    in this chapter, we’ll represent music as a multidimensional object, while in
    the next chapter, we’ll use a different format: a sequence of indexes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们用于音乐生成的机器学习模型的类型，音乐作品的数字表示可能会有所不同。例如，在本章中，我们将音乐表示为一个多维对象，而在下一章中，我们将使用不同的格式：一系列索引。
- en: In this section, we’ll cover basic music theory and then move on to represent
    music digitally using piano rolls. You’ll learn to load and play an example MIDI
    file on your computer. We’ll also introduce the music21 Python library, which
    you’ll install and use to visualize the staff notes associated with the music
    piece. Finally, you’ll learn to represent a piece of music as a multidimensional
    object with the shape of (4, 2, 16, 84).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍基本的音乐理论，然后转向使用钢琴卷来数字化表示音乐。你将学习如何在电脑上加载和播放一个示例MIDI文件。我们还将介绍music21
    Python库，你将安装并使用它来可视化与音乐作品相关的乐谱音符。最后，你将学习将一首音乐作品表示为一个具有形状（4，2，16，84）的多维对象。
- en: 13.1.1 Musical notes, octave, and pitch
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 音乐音符、八度和音高
- en: In this chapter, we’ll be working with a training dataset that represents music
    pieces as 4D objects. To grasp the meaning of the music pieces in the training
    data, it’s essential to first familiarize ourselves with some fundamental concepts
    in music theory, such as musical notes, octaves, and pitch. These concepts are
    interrelated and crucial for understanding the dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理一个表示音乐作品为4D对象的训练数据集。要理解训练数据中的音乐作品的意义，首先熟悉音乐理论中的某些基本概念是至关重要的，例如音乐音符、八度和音高。这些概念相互关联，对于理解数据集至关重要。
- en: Figure 13.1 illustrates the relationships among these concepts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1说明了这些概念之间的关系。
- en: '![](../../OEBPS/Images/CH13_F01_Liu.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH13_F01_Liu.png)'
- en: 'Figure 13.1 The relationship between musical notes, octaves, and pitches (also
    known as note numbers). The first column displays the 11 octaves (ranging from
    –1 to 9), representing different levels of musical sound. Each octave is subdivided
    into 12 semitones, which are listed in the top row: C, C#, D, D#, ..., B. Within
    each octave, each note is assigned a specific pitch number, ranging from 0 to
    127, as indicated in the figure.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 音乐音符、八度和音高的关系（也称为音符编号）。第一列显示了11个八度（范围从-1到9），代表不同的音乐声音级别。每个八度被细分为12个半音，这些半音列在图的最上面一行：C、C#、D、D#、...、B。在每个八度内，每个音符都被分配一个特定的音高编号，范围从0到127，如图所示。
- en: 'A musical note is a symbol representing a specific sound in music. These notes
    are the foundational elements of music, used to craft melodies, chords, and rhythms.
    Each note is assigned a name (such as A, B, C, D, E, F, G) and corresponds to
    a specific frequency, which determines its pitch: whether the note sounds high
    or low. For instance, a middle C (C4) typically has a frequency of about 262 hertz,
    meaning its sound waves vibrate 262 times per second.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐音符是代表音乐中特定声音的符号。这些音符是音乐的基础元素，用于创作旋律、和弦和节奏。每个音符都被分配一个名称（如A、B、C、D、E、F、G）并对应一个特定的频率，这决定了它的音高：音符听起来是高还是低。例如，中C（C4）通常的频率约为262赫兹，这意味着其声波每秒振动262次。
- en: You might be wondering about the meaning of the term “middle C (C4).” The number
    4 in C4 refers to the octave, which is the distance between one level of musical
    pitch and the next. In figure 13.1, the far-left column displays 11 octave levels,
    ranging from –1 to 9\. The frequency of a sound doubles as you move from one octave
    level to the next. For example, note A4 is usually tuned to 440 hertz, while A5,
    one octave above A4, is tuned to 880 hertz..
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道“中C（C4）”这个术语的含义。C4中的数字4指的是八度，它是从一个音乐音高级别到下一个级别的距离。在图13.1中，最左边的列显示了11个八度级别，范围从-1到9。当你从一个八度级别移动到下一个时，声音的频率会翻倍。例如，A4音符通常调校为440赫兹，而比A4高一个八度的A5，则调校为880赫兹。
- en: 'In Western music, an octave is divided into 12 semitones, each corresponding
    to a specific note. The top row of figure 13.1 lists these 12 semitones: C, C#,
    D, D#, ..., B. Moving up or down by 12 semitones takes you to the same note name
    but in a higher or lower octave. As mentioned earlier, A5 is one octave above
    A4.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在西方音乐中，一个八度被分为12个半音，每个半音对应一个特定的音符。图13.1的顶部行列出了这12个半音：C、C#、D、D#、...、B。上下移动12个半音会到达相同的音符名称，但处于更高的或更低的八度。如前所述，A5比A4高一个八度。
- en: Each note within a specific octave is assigned a pitch number, ranging from
    0 to 127, as depicted in figure 13.1\. For example, the note C4 has a pitch number
    of 60, while F3 has a pitch number of 53\. The pitch number is a more efficient
    way to represent musical notes since it specifies both the octave level and the
    semitone. The training data you’ll be using in this chapter is encoded using pitch
    numbers for this very reason.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特定八度内的音符被分配一个音高数字，范围从0到127，如图13.1所示。例如，C4音符的音高数字为60，而F3的音高数字为53。音高数字是表示音乐音符的一种更有效的方式，因为它指定了八度水平和半音。你将在本章中使用的训练数据就是用音高数字编码的，正是出于这个原因。
- en: 13.1.2 An introduction to multitrack music
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 多轨音乐简介
- en: Let’s first talk about how multitrack music works and how it is represented
    digitally. In electronic music production, a “track” typically refers to an individual
    layer or component of the music, such as a drum track, a bass track, or a melody
    track. In classical music, tracks might represent different vocal parts, like
    soprano, alto, tenor, and bass. For instance, the training dataset we’re using
    in this chapter, the JSB Chorales dataset, consists of four tracks corresponding
    to four vocal parts. In music production, each track can be individually edited
    and processed within a digital audio workstation (DAW). These tracks are composed
    of various musical elements, including bars, steps, and notes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们谈谈多轨音乐是如何工作的以及它是如何以数字形式表示的。在电子音乐制作中，“轨”通常指的是音乐的一个单独的层次或组成部分，例如鼓轨、贝斯轨或旋律轨。在古典音乐中，轨可能代表不同的声乐部分，如女高音、女低音、男高音和男低音。例如，我们在这章中使用的训练数据集，JSB
    Chorales 数据集，包含四个轨，对应四个声乐部分。在音乐制作中，每个轨都可以在数字音频工作站（DAW）中单独编辑和处理。这些轨由各种音乐元素组成，包括小节、步骤和音符。
- en: A bar (or measure) is a segment of time defined by a specified number of beats,
    with each beat having a certain note duration. In many popular music genres, a
    bar typically contains four beats, although this can vary based on the time signature
    of the piece. The total number of bars in a track is determined by the track’s
    length and structure. For example, in our training dataset, each track comprises
    two bars.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 小节（或度量）是由特定数量的拍子定义的时间段，每个拍子有特定的音符持续时间。在许多流行的音乐流派中，小节通常包含四个拍子，尽管这可以根据作品的拍号而变化。一个轨中的小节数量由轨的长度和结构决定。例如，在我们的训练数据集中，每个轨由两个小节组成。
- en: In the context of step sequencing, a technique commonly used for programming
    rhythms and melodies in electronic music, a “step” represents a subdivision of
    a bar. In a standard 4/4 time signature (four beats in a bar and four steps in
    a beat), you might find 16 steps per bar, with each step corresponding to a sixteenth
    of a bar.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤序列的上下文中，这是一种在电子音乐中编程节奏和旋律的常用技术，一个“步骤”代表一小节的一个细分。在一个标准的4/4拍（一小节有四个拍子，每个拍子有四个步骤）中，你可能会发现每小节有16个步骤，每个步骤对应一小节的十六分之一。
- en: Lastly, each step contains a musical note. In our dataset, we limit the range
    to the 84 most frequently used notes (with pitch numbers from 0 to 83). Therefore,
    the musical note in a step is encoded as a one-hot vector with 84 values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每个步骤包含一个音符。在我们的数据集中，我们限制范围为最常用的84个音符（音高数字从0到83）。因此，步骤中的音符被编码为一个包含84个值的one-hot向量。
- en: To illustrate these concepts with a practical example, download the file example.midi
    from the book’s GitHub repository at [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)
    and save it in the /files/ directory on your computer. A file with the .midi extension
    is a MIDI file. MIDI is a technical standard that outlines a protocol, digital
    interface, and connectors for enabling electronic musical instruments, computers,
    and other related devices to connect and communicate with each other.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用实际例子说明这些概念，请从本书的GitHub仓库[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)下载文件example.midi，并将其保存在你电脑的/files/目录下。具有.midi扩展名的文件是MIDI文件。MIDI是一种技术标准，概述了协议、数字接口和连接器，以使电子乐器、计算机和其他相关设备能够相互连接和通信。
- en: 'MIDI files can be played on most music players on your computer. To get a sense
    of the type of music in our training data, open the file example.midi you just
    downloaded with a music player on your computer. It should sound like this music
    file I placed on my website: [https://mng.bz/lrJB](https://mng.bz/lrJB). The file
    example.midi is converted from one of the music pieces in the training dataset
    in this chapter. Later you’ll learn how to convert a piece of music in the training
    dataset with a shape of (4, 2, 16, 84) into a MIDI file that can be played on
    your computer.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MIDI文件可以在您计算机上的大多数音乐播放器上播放。为了了解我们训练数据中的音乐类型，请使用您计算机上的音乐播放器打开您刚刚下载的文件example.midi。它应该听起来像我在我的网站上放置的这个音乐文件：[https://mng.bz/lrJB](https://mng.bz/lrJB)。文件example.midi是从本章训练数据集中的某个音乐作品转换而来的。稍后您将学习如何将形状为(4,
    2, 16, 84)的音乐作品转换为可以在您计算机上播放的MIDI文件。
- en: 'We’ll use the music21 Python library, a powerful and comprehensive toolkit
    designed for music analysis, composition, and manipulation, to visualize how various
    music concepts work. Therefore, run the following line of code in a new cell in
    the Jupyter Notebook app on your computer:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用music21 Python库，这是一个专为音乐分析、创作和操作设计的强大且全面的工具包，来可视化各种音乐概念是如何工作的。因此，请在您的计算机上的Jupyter
    Notebook应用程序的新单元中运行以下代码行：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The music21 library enables you to visualize music as staff notation to have
    a better understanding of tracks, bars, steps, and notes. To achieve this, you
    must first install the MuseScore application on your computer. Visit [https://musescore.org/en/download](https://musescore.org/en/download)
    and download the most recent version of the MuseScore app for your operating system.
    As of this writing, the latest version is MuseScore 4, which we’ll use as our
    example. Ensure you know the file path of the MuseScore app on your computer.
    For instance, in Windows, the path is C:\Program Files\MuseScore 4\bin\MuseScore4.exe.
    Run the code cell in the following listing to visualize the staff notation for
    the file example.midi.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: music21库允许您将音乐以乐谱形式可视化，以便更好地理解轨道、小节、音级和音符。为了实现这一点，您必须首先在您的计算机上安装MuseScore应用程序。访问[https://musescore.org/en/download](https://musescore.org/en/download)并下载适用于您操作系统的最新版本的MuseScore应用程序。截至本文撰写时，最新版本是MuseScore
    4，我们将以此为例。确保您知道MuseScore应用程序在您计算机上的文件路径。例如，在Windows上，路径是C:\Program Files\MuseScore
    4\bin\MuseScore4.exe。运行以下列表中的代码单元以可视化文件example.midi的乐谱。
- en: Listing 13.1 Visualizing the staff notation using the music21 library
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.1 使用music21库可视化乐谱
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Shows the image in Jupyter notebook instead of in the original app
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在Jupyter笔记本中显示图像，而不是在原始应用程序中
- en: ② Opens the MIDI file
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ② 打开MIDI文件
- en: ③ Defines the path of the MuseScore app
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定义MuseScore应用程序的路径
- en: ④ Shows the staff notation
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 显示乐谱
- en: For users of the macOS operating system, change the path in the preceding code
    cell to /Applications/MuseScore 4.app/Contents/MacOS/mscore. For Linux users,
    modify the path to /home/[user name]/.local/bin/mscore4portable, substituting
    [user name] with your actual username. For instance, my username is `mark`, so
    the path is /home/mark/.local/bin/mscore4portable.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于macOS操作系统的用户，将前面代码单元中的路径更改为/Applications/MuseScore 4.app/Contents/MacOS/mscore。对于Linux用户，将路径更改为/home/[用户名]/.local/bin/mscore4portable，将[用户名]替换为您实际的用户名。例如，我的用户名是`mark`，所以路径是/home/mark/.local/bin/mscore4portable。
- en: Executing the previous code cell will display a staff notation similar to what
    is illustrated in figure 13.2\. Please note that the annotations in the figure
    are added by me, so you will only see the staff notation without any annotations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码单元将显示类似于图13.2所示的乐谱。请注意，图中的注释是我添加的，所以您将只看到乐谱而没有任何注释。
- en: '![](../../OEBPS/Images/CH13_F02_Liu.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH13_F02_Liu.png)'
- en: 'Figure 13.2 Staff notation for a piece of music in JSB Chorales dataset. The
    music has four tracks, representing the four voices in a chorale: soprano, alto,
    tenor, and bass. The notation is structured into two bars for each track, with
    the left and right halves representing the first and second bars, respectively.
    Each bar consists of 16 steps, aligning with the 4/4 time signature where a bar
    contains four beats, each subdivided into four sixteenth notes. A total of 84
    different pitches are possible, and each note is represented as a one-hot vector
    with 84 values.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 JSB Chorales数据集中一首音乐作品的乐谱。音乐有四个轨道，代表合唱中的四个声部：女高音、女低音、男高音和男低音。乐谱结构为每个轨道两个小节，左右两侧分别代表第一和第二小节。每个小节由16个步骤组成，与4/4拍号相匹配，其中一小节包含四个节拍，每个节拍细分为四个十六分音符。总共可能有84个不同的音高，每个音符都表示为一个包含84个值的one-hot向量。
- en: 'The JSB Chorales dataset, which consists of chorale music pieces by Johann
    Sebastian Bach, is often used for training machine learning models in music generation
    tasks. The shape (4, 2, 16, 84) of each music piece in the dataset can be explained
    as follows. Four represents the four voices in a chorale: soprano, alto, tenor,
    and bass. Each voice is treated as a separate track in the dataset. Each piece
    is divided into two bars (also called measures). The dataset is formatted this
    way to standardize the length of the music pieces for training purposes. The number
    16 represents the number of steps (or subdivisions) in each bar. Finally, the
    note is one-hot encoded with 84 values, denoting the number of possible pitches
    (or notes) that can be played in each step.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: JSB Chorales数据集，由约翰·塞巴斯蒂安·巴赫的合唱作品组成，常用于音乐生成任务中训练机器学习模型。数据集中每首音乐作品的形状（4, 2, 16,
    84）可以这样解释。数字4代表合唱中的四个声部：女高音、女低音、男高音和男低音。每个声部在数据集中被视为一个单独的轨道。每首作品分为两个小节（也称为乐句）。数据集以这种方式格式化，以便标准化音乐作品的长度，用于训练目的。数字16代表每个小节中的步骤数（或细分）。最后，音符以84个值进行one-hot编码，表示每个步骤中可以演奏的可能音高（或音符）的数量。
- en: '13.1.3 Digitally represent music: Piano rolls'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.3 数字化音乐表示：钢琴卷轴
- en: A piano roll is a visual representation of music often used in MIDI sequencing
    software and DAWs. It is named after the traditional piano rolls used in player
    pianos, which contained a physical roll of paper with holes punched in it to represent
    musical notes. In a digital context, the piano roll serves a similar function
    but in a virtual format.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 钢琴卷轴是音乐的一种视觉表示，常用于MIDI编曲软件和DAWs中。它以传统钢琴卷轴命名，这种卷轴在自动演奏钢琴中使用，其中包含带有孔的物理卷纸，以表示音符。在数字环境中，钢琴卷轴发挥着类似的功能，但以虚拟格式存在。
- en: The piano roll is displayed as a grid, with time represented horizontally (from
    left to right) and pitch represented vertically (from bottom to top). Each row
    corresponds to a specific musical note, with higher notes at the top and lower
    notes at the bottom, similar to the layout of a piano keyboard.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 钢琴卷轴以网格形式显示，时间水平表示（从左到右），音高垂直表示（从下到上）。每一行对应一个特定的音符，高音在顶部，低音在底部，类似于钢琴键盘的布局。
- en: Notes are represented as bars or blocks on the grid. The position of a note
    block along the vertical axis indicates its pitch, while its position along the
    horizontal axis indicates its timing in the music. The length of the note block
    represents the duration of the note.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 音符以网格上的条形或块表示。音符块沿垂直轴的位置表示其音高，而沿水平轴的位置表示其在音乐中的时间。音符块的长度表示音符的持续时间。
- en: 'Let’s use the music21 library to illustrate what a piano roll looks like. Run
    this line of code in a new cell in your Jupyter Notebook app:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用music21库来展示钢琴卷轴的样子。在您的Jupyter Notebook应用的新单元格中运行以下代码行：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output is shown in figure 13.3.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如图13.3所示。
- en: 'The music21 library also allows you to see the quantized notes corresponding
    to the preceding piano roll:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: music21库还允许您查看与前面钢琴卷轴对应的量化音符：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output is
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../../OEBPS/Images/CH13_F03_Liu.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH13_F03_Liu.png)'
- en: Figure 13.3 The piano roll for a piece of music. The piano roll is a graphical
    representation of a musical piece, depicted as a grid with time progressing horizontally
    from left to right and pitch represented vertically from bottom to top. Each row
    on the grid corresponds to a distinct musical note, arranged in a manner akin
    to the keyboard of a piano, with higher notes positioned at the top and lower
    notes at the bottom. This specific piece of music comprises two bars, resulting
    in two distinct sections visible in the graph. The vertical placement of a note
    block signifies its pitch, while its horizontal location indicates when the note
    is played in the piece. Additionally, the length of the note block reflects the
    duration for which the note is sustained.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 一段音乐的钢琴卷。钢琴卷是音乐作品的图形表示，以网格形式呈现，时间从左到右水平进展，音高从下到上垂直表示。网格上的每一行对应一个独特的音乐音符，排列方式类似于钢琴键盘，高音符位于顶部，低音符位于底部。这首特定的音乐由两个小节组成，因此在图中可以看到两个不同的部分。音符块的垂直位置表示其音高，而其水平位置表示音符在作品中的演奏时间。此外，音符块的长度反映了音符持续的时长。
- en: I omitted most of the output. The first value in each line in the previous output
    represents time. It increases by 0.25 seconds after each line in most cases. If
    the time increase in the next line is more than 0.25 seconds, it means a note
    lasts more than 0.25 seconds. As you can see, the starting note is E4\. After
    0.25 seconds, the note changes to A4, and then G4, and so on. This explains the
    first three blocks (far left) in figure 13.3, which have values E, A, and G, respectively.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我省略了大部分输出。前一个输出中每行的第一个值代表时间。在大多数情况下，每行之后时间增加0.25秒。如果下一行的时间增加超过0.25秒，这意味着音符持续的时间超过0.25秒。正如你所见，起始音符是E4。0.25秒后，音符变为A4，然后是G4，以此类推。这解释了图13.3中（最左侧）的前三个块，它们分别具有E、A和G的值。
- en: 'You might be curious about how to convert the sequence of musical notes into
    an object with the shape (4, 2, 16, 84). To understand this, let’s examine the
    pitch number at each time step in the musical notes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能好奇如何将音乐音符序列转换成形状为（4, 2, 16, 84）的对象。为了理解这一点，让我们检查音乐音符中每个时间步的音高数字：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output is
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding code block has converted the musical note in each time step into
    a pitch number, in the range of 0 to 83 based on the mapping used in figure 13.1\.
    Each of the pitch numbers is then converted to a one-hot variable with 84 values,
    with value –1 everywhere, except 1 in one position. We use –1 and 1 in one-hot
    encoding instead of 0 and 1 because placing values between –1 and 1 centers the
    data around 0, which can make training more stable and faster. Many activation
    functions and weight initialization methods assume input data is centered around
    0\. Figure 13.4 illustrates how a piece of MIDI music is encoded into an object
    in the shape of (4, 2, 16, 84).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块已将每个时间步的音乐音符转换为基于图13.1中使用的映射的音高数字，范围在0到83之间。然后，每个音高数字被转换为具有84个值的one-hot变量，其中所有值均为-1，只有一个位置为1。我们使用-1和1而不是0和1进行one-hot编码，因为将值放置在-1和1之间可以将数据围绕0中心化，这可以使训练更加稳定和快速。许多激活函数和权重初始化方法都假设输入数据围绕0中心化。图13.4说明了如何将一段MIDI音乐编码成形状为（4,
    2, 16, 84）的对象。
- en: '![](../../OEBPS/Images/CH13_F04_Liu.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH13_F04_Liu.png)'
- en: Figure 13.4 How to represent a piece of music using a 4D object. In our training
    data, each piece of music is represented by a 4D object in the shape of (4, 2,
    16, 84). The first dimension represents the four music tracks, which are the four
    voices in the music (soprano, alto, tenor, and bass). Each music track is divided
    into two bars. There are four beats in each bar, and each beat has four notes;
    we therefore have 16 notes in a bar. Finally, each note is represented by a one-hot
    variable with 84 values, with –1 everywhere and 1 in one place.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 如何使用4D对象表示一段音乐。在我们的训练数据中，每段音乐都由形状为（4, 2, 16, 84）的4D对象表示。第一个维度代表四个音乐轨道，即音乐中的四个声部（女高音、女低音、男高音和男低音）。每个音乐轨道分为两个小节。每个小节有四个节拍，每个节拍有四个音符；因此，每个小节有16个音符。最后，每个音符由一个具有84个值的one-hot变量表示，其中所有值均为-1，只有一个位置为1。
- en: Figure 13.4 explains the dimensions of the music object shaped (4, 2, 16, 84).
    In essence, each musical piece comprises four tracks, with each track containing
    two bars. Each bar is subdivided into 16 notes. Given that the pitch numbers range
    from 0 to 83 in our training set, each note is represented by a one-hot vector
    with 84 values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4解释了音乐对象形状的维度（4, 2, 16, 84）。本质上，每首音乐作品包含四个轨道，每个轨道包含两个小节。每个小节被细分为16个音符。鉴于我们的训练集中音高数字从0到83，每个音符由一个包含84个值的one-hot向量表示。
- en: In subsequent discussions on preparing training data, we will explore how to
    transform an object with the shape (4, 2, 16, 84) back into a music piece in MIDI
    format, enabling playback on a computer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续关于准备训练数据的讨论中，我们将探讨如何将形状为（4, 2, 16, 84）的对象转换回MIDI格式的音乐作品，以便在计算机上播放。
- en: 13.2 A blueprint for music generation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 音乐生成的蓝图
- en: When creating music, we need to incorporate more detailed inputs for enhanced
    control and variety. Unlike the approach of utilizing a single noise vector from
    the latent space for generating shapes, numbers, and images, we will employ four
    distinct noise vectors in the music generation process. Since each music piece
    comprises four tracks and two bars, we’ll utilize four vectors to manage this
    structure. We’ll use one vector to govern all tracks and bars collectively, another
    vector to control each bar across all tracks, a third vector to oversee all tracks
    across bars, and a fourth one to manage each individual bar in each track. This
    section will introduce you to the concepts of chords, style, melody, and groove
    and explain how they influence various aspects of the music generation. After
    that, we’ll discuss the steps involved in building and training the MuseGAN model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在创作音乐时，我们需要融入更详细的输入以增强控制和多样性。与仅从潜在空间中利用单个噪声向量来生成形状、数字和图像的方法不同，我们在音乐生成过程中将使用四个不同的噪声向量。由于每首音乐作品包含四个轨道和两个小节，我们将使用四个向量来管理这种结构。我们将使用一个向量来控制所有轨道和小节，另一个向量来控制所有轨道中的每个小节，第三个向量来监督所有轨道跨越小节，第四个向量来管理每个轨道中的每个单独小节。本节将向您介绍和弦、风格、旋律和节奏的概念，并解释它们如何影响音乐生成的各个方面。之后，我们将讨论构建和训练MuseGAN模型所涉及的步骤。
- en: 13.2.1 Constructing music with chords, style, melody, and groove
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 使用和弦、风格、旋律和节奏构建音乐
- en: Later, in the music generation stage, we obtain four noise vectors (chords,
    style, melody, and groove) from the latent space and feed them to the generator
    to create a piece of music. You may be wondering the meaning of these four pieces
    of information. In music, chords, style, melody, and groove are key elements that
    contribute to a piece’s overall sound and feel. Next I provide a brief explanation
    of each element.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在音乐生成阶段稍后，我们从潜在空间中获取四个噪声向量（和弦、风格、旋律和节奏），并将它们输入到生成器中，以创建一首音乐作品。你可能想知道这四条信息的含义。在音乐中，和弦、风格、旋律和节奏是构成作品整体声音和感觉的关键元素。接下来，我将简要解释每个元素。
- en: Style refers to the characteristic way in which music is composed, performed,
    and experienced. It includes the genre (such as jazz, classical, rock, and so
    on), the era in which the music was created, and the unique approach of the composer
    or performer. Style is influenced by cultural, historical, and personal factors,
    and it helps to define the music’s identity.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 风格指的是音乐创作、表演和体验的特征方式。它包括音乐类型（如爵士、古典、摇滚等）、音乐创作的时代以及作曲家或表演者的独特方法。风格受文化、历史和个人因素的影响，有助于定义音乐的个性。
- en: Groove is the rhythmic feel or swing in music, especially in styles like funk,
    jazz, and soul. It’s what makes you want to tap your foot or dance. A groove is
    created by the pattern of accents, the interplay between the rhythm section (drums,
    bass, etc.), and the tempo. It’s the element that gives music its sense of motion
    and flow.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 节奏感是音乐中的节奏感或摇摆，尤其是在放克、爵士和灵魂乐等风格中。这是让你想要脚打拍子或跳舞的原因。节奏感是通过强调模式、节奏部分（如鼓、贝斯等）之间的互动和速度来创造的。它是赋予音乐运动感和流畅感的关键元素。
- en: Chords are combinations of two or more notes played simultaneously. They provide
    the harmonic foundation for music. Chords are built on scales and are used to
    create progressions that give music its structure and emotional depth. Different
    chord types (major, minor, diminished, augmented, etc.) and their arrangements
    can evoke various moods and feelings in the listener.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Finally, melody is the sequence of notes that is most easily recognizable in
    a piece of music. It’s the part that you might hum or sing along to. Melodies
    are often built from scales and are characterized by their pitch, rhythm, and
    contour (the pattern of rises and falls in pitch). A good melody is memorable
    and expressive, conveying the main musical and emotional themes of the piece.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Together, these elements work in harmony to create the overall sound and experience
    of a musical piece. Each element has its role, but they all interact and influence
    each other to produce the final music piece. Specifically, a music piece consists
    of four tracks, each with two bars, resulting in eight bar/track combinations.
    We’ll use one noise vector for style, applied to all eight bars. We’ll use eight
    different noise vectors for melody, each used in a unique bar. There are four
    noise vectors for groove, each applied to a different track, remaining the same
    across both bars. Two noise vectors will be used for chords, one for each bar.
    Figure 13.5 provides a diagram of how these four elements contribute to the creation
    of a complete piece of music.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH13_F05_Liu.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 Music generation using chords, style, melody, and groove. Each music
    composition consists of four tracks and spans two bars. We will extract four noise
    vectors from the latent space for this purpose. The first vector, representing
    chords, has a dimension of (1, 32). This vector will be processed through a temporal
    network to expand the chords into two (1, 32) vectors, corresponding to the two
    bars, with identical values across all tracks. The second vector, denoting style,
    also has a dimension of (1, 32) and remains constant across all tracks and bars.
    The third vector, melody, is shaped as (4, 32). It will be stretched through a
    temporal network into two (4, 32) vectors, resulting in eight (1, 32) vectors,
    each representing a unique track and bar combination. Lastly, the fourth vector,
    groove, with a dimension of (4, 32), will be applied to the four tracks, maintaining
    the same values for both bars.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The generator creates a piece of music by generating one bar in one track at
    a time. For this, it requires four noise vectors, each with a shape of (1, 32),
    as input. These vectors represent chords, style, melody, and groove, and each
    controls a distinct aspect of the music, as previously explained. Since the music
    piece consists of four tracks, each with two bars, there are a total of eight
    bar/track combinations. Consequently, we need eight sets of chords, style, melody,
    and groove to generate all parts of the music piece.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: We’ll obtain four noise vectors from the latent space corresponding to chords,
    style, melody, and groove. We’ll also introduce a temporal network later, whose
    role is to expand the input along the bar dimension. With two bars, this means
    doubling the size of the input. Music is inherently temporal, with patterns and
    structures that unfold over time. The temporal network in MuseGAN is designed
    to capture these temporal dependencies, ensuring that the generated music has
    a coherent and logical progression.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The noise vector for chords has a shape of (1, 32). After processing it through
    the temporal network, we obtain two (1, 32) sized vectors. The first vector is
    used across all four tracks in the first bar, while the second vector is used
    across all four tracks in the second bar.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The noise vector for style, also with a shape of (1, 32), is applied uniformly
    across all eight track/bar combinations. Note that we’ll not pass the style vector
    through the temporal network since the style vector is designed to be the same
    across bars.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The noise vector for melody has a shape of (4, 32). When passed through the
    temporal network, it yields two (4, 32) sized vectors, which further break down
    into eight (1, 32) sized vectors. Each of these is used in a unique track/bar
    combination.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the noise vector for groove, shaped as (4, 32), is used such that each
    (1, 32) sized vector is applied to a different track, remaining the same across
    both bars. We won’t pass the groove vector through the temporal network since
    the groove vector is designed to be the same across bars.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: After generating a bar of music for each of the eight bar/track combinations,
    we’ll merge them to create a full piece of music, consisting of four distinct
    tracks, each comprising two unique bars.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.2 A blueprint to train a MuseGAN
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chapter 1 provided an overview of the foundational concepts behind GANs. In
    chapters 3 to 5, you explored the creation and training of GANs for generating
    shapes, numbers, and images. This subsection will summarize the steps for building
    and training MuseGAN, highlighting the differences from the previous chapters.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: The style of music generated by MuseGAN is influenced by the training data’s
    style. Therefore, you should first collect a dataset of Bach’s compositions in
    a format suitable for training. Next, you’ll create a MuseGAN model, which consists
    of a generator and a critic. The generator network takes four random noise vectors
    as input (chords, style, melody, and groove) and outputs a piece of music. The
    critic network evaluates a piece of music and assigns a rating, with higher scores
    for real music (from the training set) and lower scores for fake music (produced
    by the generator). Both the generator and critic networks utilize deep convolutional
    layers to capture the spatial features of the inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH13_F06_Liu.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 A diagram of the steps involved in training MuseGAN to generate
    music. The generator produces a fake music piece by drawing four random noise
    vectors from the latent space (top left) and presents it to the critic (middle).
    The critic evaluates the piece and assigns a rating. A high rating suggests that
    the piece is likely from the training dataset, while a lower rating indicates
    that the piece is likely fake (generated by the generator). Additionally, an interpolated
    music piece created from a mix of real and fake samples (top left) is presented
    to the critic. The training process incorporates a gradient penalty based on the
    critic’s rating of this interpolated piece, which is added to the total loss.
    The ratings are then compared to the ground truth, allowing both the critic and
    the generator to learn from these evaluations. After numerous training iterations,
    the generator becomes proficient at producing music pieces that are virtually
    indistinguishable from real samples.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.6 illustrates the training process of MuseGAN. The generator (the
    bottom left of the figure) receives four random noise vectors (chords, style,
    melody, and groove) as input and produces fake music pieces (step 1 in figure
    13.6). These noise vectors are drawn from the latent space, which represents the
    range of potential outputs the GAN can generate, enabling the creation of diverse
    data samples. These fake music pieces, along with real ones from the training
    set (top right), are then evaluated by the critic (step 3). The critic (bottom
    center) assigns scores to all music pieces, aiming to give high scores to real
    music and low scores to fake music (step 4).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: To guide the adjustment of model parameters, appropriate loss functions must
    be chosen for both the generator and the critic. The generator’s loss function
    is designed to encourage the production of data points that closely resemble those
    from the training dataset. Specifically, the loss function for the generator is
    the negative of the critic’s rating. By minimizing this loss function, the generator
    strives to create music pieces that receive high ratings from the critic. On the
    other hand, the critic’s loss function is formulated to encourage accurate assessment
    of real and generated data points. Thus, the loss function for the critic is the
    rating itself if the music piece is from the training set and the negative of
    the rating if it is generated by the generator. In essence, the critic aims to
    assign high ratings to real music pieces and low ratings to fake ones.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we incorporate the Wasserstein distance with gradient penalty
    into the loss function, as we did in chapter 5, to enhance the training stability
    and performance of GAN models. To achieve this, an interpolated music piece, blending
    real and fake music (top left in figure 13.6), is evaluated by the critic. The
    gradient penalty, based on the critic’s rating of this interpolated piece, is
    then added to the total loss during the training process.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the training loop, we alternate between training the critic and the
    generator. In each training iteration, we sample a batch of real music pieces
    from the training set and a batch of fake music pieces generated by the generator.
    We calculate the total loss by comparing the critic’s ratings (i.e., scores) with
    the ground truth (whether a music piece is real or fake). We then slightly adjust
    the weights in both the generator and critic networks so that, in subsequent iterations,
    the generator produces more realistic music pieces, and the critic assigns higher
    scores to real music and lower scores to fake music.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Once MuseGAN is fully trained, music can be created by inputting four random
    noise vectors into the trained generator.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 Preparing the training data for MuseGAN
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use chorale compositions by Johann Sebastian Bach as our training dataset,
    expecting the generated music to resemble Bach’s style. If you prefer the style
    of a different musician, you can use their work as the training data instead.
    In this section, we’ll start by downloading the training data and organizing it
    into batches for later training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we’ve learned that the music pieces in the training set will be
    represented as 4D objects. In this section, you’ll also learn how to convert these
    multidimensional objects into playable music pieces on a computer. This conversion
    is essential because MuseGAN generates multidimensional objects similar to those
    in the training set. Later in the chapter, we’ll transform the multidimensional
    objects produced by MuseGAN into MIDI files, enabling you to listen to the generated
    music on your computer.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.1 Downloading the training data
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll use the JSB Chorales piano rolls dataset as our training set. Go to Cheng-Zhi
    Anna Huang’s GitHub repository ([https://github.com/czhuang/JSB-Chorales-dataset](https://github.com/czhuang/JSB-Chorales-dataset))
    and download the music file Jsb16thSeparated.npz. Save the file in the /files/
    directory on your computer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, download the two utility modules midi_util.py and MuseGAN_util.py from
    the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and save them in the /utils/ directory on your computer. The code in this chapter
    is adapted from the excellent GitHub repository by Azamat Kanametov ([https://github.com/akanametov/musegan](https://github.com/akanametov/musegan)).
    With these files in place, we can now load the music files and organize them into
    batches for processing:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We load the dataset you just downloaded into Python, then extract the first
    song and name it `first_song`. Since songs are represented as multidimensional
    objects, we print out the shape of the first song. Finally, we place the training
    data in batches of 64, to be used later in the chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code block is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each song in the dataset has a shape of (4, 2, 16, 84), as shown in the previous
    output. This indicates that each song consists of four tracks, each with two bars.
    Each bar contains 16 time steps, and at each time step, the musical note is represented
    by a one-hot vector with 84 values. In each one-hot vector, all values are set
    to –1, except for one position where the value is set to 1, indicating the presence
    of a note. You can verify the range of values in the dataset as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output is
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The previous output shows that the values in each music piece are either –1
    or 1.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.2 Converting multidimensional objects to music pieces
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, the songs are formatted as PyTorch tensors and are ready to be inputted
    into the MuseGAN model. However, before we proceed, it’s important to gain a better
    understanding of how to convert these multidimensional objects into playable music
    pieces on your computer. This will help us later to convert generated music pieces
    into playable files.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we’ll convert all the 84-value one-hot variables into pitch numbers
    ranging from 0 to 83:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Converts 84-value one-hot vectors to numbers between 0 and 83
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: ② Reshapes the result to (32, 4)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the output displayed here, each column represents a music track, with numbers
    ranging from 0 to 83\. These numbers correspond to pitch numbers, as you have
    seen earlier in figure 13.1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll proceed to convert the tensor `midi_note_score` in the previous code
    block into an actual MIDI file, allowing you to play it on your computer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.2 Converting pitch numbers to a MIDI file
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Iterates through four music tracks
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterates through all notes in each track
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: ③ Adds 0.25 seconds to each time step
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adds the note to the music stream
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: After running the preceding code cell, you’ll see a MIDI file, `first_song.midi`,
    on your computer. Play it with a music player on your computer to get a sense
    of what type of music we are using to train the MuseGAN.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 13.1
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Convert the second song in the training dataset into a MIDI file. Save it as
    `second_song.midi` and play it using a music player on your computer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Building a MuseGAN
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In essence, we will treat a music piece as an object with multiple dimensions.
    Using techniques from chapters 4 to 6, we will tackle this task using deep convolutional
    neural networks for their ability to effectively extract spatial features from
    multidimensional objects. In MuseGAN, we’ll construct a generator and a critic,
    similar to how a generator in image creation refines an image based on a critic’s
    feedback. The generator will produce a music piece as a 4D object.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Both real music from our training set and fake music from the generator will
    be presented to the critic. The critic will score each piece from negative infinity
    to positive infinity, with higher scores indicating a higher likelihood of the
    music being real. The critic aims to give high scores to real music and low scores
    to fake music. Conversely, the generator aims to produce music that is indistinguishable
    from real music, thereby receiving high scores from the critic.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will build a MuseGAN model, comprising a generator network
    and a critic network. The critic network employs deep convolutional layers to
    extract distinct features from multidimensional objects, enhancing its ability
    to evaluate music pieces. On the other hand, the generator network utilizes deep
    transposed convolutional layers to produce feature maps aimed at generating realistic
    music pieces. Later, we will train the MuseGAN model using music pieces from the
    training set.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.1 A critic in MuseGAN
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As explained in chapter 5, incorporating the Wasserstein distance into the loss
    function can help stabilize training. Therefore, in MuseGAN, we adopt a similar
    approach and use a critic instead of a discriminator. The critic is not a binary
    classifier; rather, it evaluates the output of the generator (in this case, a
    music piece) and assigns a score ranging from –∞ to ∞. A higher score indicates
    a greater likelihood that the music is real (i.e., from the training set).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: We construct a music critic neural network as shown in the following listing,
    and its definition can be found in the file MuseGAN_util.py that you downloaded
    earlier.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.3 The critic network in MuseGAN
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Passes the input through several Conv3d layers
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: ② Flattens the output
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: ③ Passes the output through two linear layers
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The input to the critic network is a music piece with dimensions (4, 2, 16,
    84). The network primarily consists of several Conv3d layers. These layers treat
    each track of the music piece as a 3D object and apply filters to extract spatial
    features. The operation of the Conv3d layers is similar to the Conv2d layers used
    in image generation, as discussed in earlier chapters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the final layer of the critic model is linear, and
    we do not apply any activation function to its output. As a result, the output
    from the critic model is a value ranging from –∞ to ∞, which can be interpreted
    as the critic’s rating of a music piece.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.2 A generator in MuseGAN
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed earlier in this chapter, the generator will produce one bar of
    music at a time, and we will then combine these eight bars to form a complete
    piece of music.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using just a single noise vector, the generator in MuseGAN takes
    four independent noise vectors as input to control various aspects of the music
    being generated. Two of these vectors will be processed through a temporal network
    to extend them along the bar dimension. While the style and groove vectors are
    designed to remain constant across both bars, the chords and melody vectors are
    designed to vary between bars. Therefore, we will first establish a temporal network
    to stretch the chords and melody vectors across the two bars, ensuring that the
    generated music has a coherent and logical progression over time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'In the local module `MuseGAN_util` you downloaded earlier, we define the `TemporalNetwork()`
    class as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① The input dimension to the TemporalNetwork() class is (1, 32).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: ② The output dimension is (2, 32).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The `TemporalNetwork()` class described here employs two ConvTranspose2d layers
    to expand a single noise vector into two distinct noise vectors, each corresponding
    to one of the two bars. As we covered in chapter 4, transposed convolutional layers
    serve the purpose of upsampling and generating feature maps. In this context,
    they are utilized to extend noise vectors across different bars.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of generating all bars in all tracks at once, we’ll generate the music
    one bar at a time. Doing so allows MuseGAN to balance computational efficiency,
    flexibility, and musical coherence, resulting in more structured and appealing
    musical compositions. Therefore, we proceed to construct a bar generator that
    is responsible for generating a segment of the music piece: one bar within a track.
    We introduce the `BarGenerator()` class within the local `MuseGAN_util` module:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① We concatenate chords, style, melody, and groove into one vector, with a size
    of 4 * 32.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: ② The input is then reshaped into 2D, and we use several ConvTranspose2d layers
    for upsampling and music feature generation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '③ The output has a shape of (1, 1, 16, 84): 1 track, 1 bar, and 16 notes, and
    each note is represented by a 84-value vector.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The `BarGenerator()` class accepts four noise vectors as input, each representing
    chords, style, melody, and groove for a specific bar in a different track, all
    with a shape of (1, 32). These vectors are concatenated into a single 128-value
    vector before being fed into the `BarGenerator()` class. The output from the `BarGenerator()`
    class is a bar of music, with dimensions (1, 1, 16, 84), indicating 1 track, 1
    bar, and 16 notes, with each note represented by an 84-value vector.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will employ the `MuseGenerator()` class to generate a complete piece
    of music, consisting of four tracks with two bars per track. Each bar is constructed
    using the `BarGenerator()` class defined earlier. To achieve this, we define the
    `MuseGenerator()` class in the local MuseGAN_util module.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.4 The music generator in MuseGAN
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Iterates through two bars
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterates through four tracks
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: ③ Concatenates chords, style, melody, and groove into one input
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: ④ Generates one bar using the bar generator
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Concatenates eight bars into one complete piece of music
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The generator takes four noise vectors as inputs. It iterates through four tracks
    and two bars. In each iteration, it utilizes the bar generator to create a single
    bar of music. Upon completing all iterations, the MuseGenerator() class merges
    the eight bars into one cohesive music piece, which has dimensions of (4, 2, 16,
    84).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.3 Optimizers and the loss function
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We create a generator and a critic based on the `MuseGenerator()` and `MuseCritic()`
    classes in the local module:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we discussed in chapter 5, the critic generates a rating instead of a classification,
    so the loss function is defined as the negative average of the product between
    the prediction and the target. As a result, we define the following `loss_fn()`
    function in the local module `MuseGAN_util`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: During training, for the generator, we’ll assign a value of 1 to the target
    argument in the `loss_fn()` function. This setting aims to guide the generator
    in producing music that can achieve the highest possible rating (i.e., the variable
    pred in the `loss_fn()` function). For the critic, we’ll set the target to 1 for
    real music and –1 for fake music in the loss function. This setting guides the
    critic to assign a high rating to real music and a low rating to fake music.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the approach in chapter 5, we incorporate the Wasserstein distance
    with a gradient penalty into the critic’s loss function to ensure training stability.
    The gradient penalty is defined in the `MuseGAN_util.py` file as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `GradientPenalty()` class requires two inputs: interpolated music, which
    is a blend of real and fake music, and the ratings assigned by the critic network
    to this interpolated music. The class computes the gradient of the critic’s ratings
    concerning the interpolated music. The gradient penalty is then calculated as
    the squared difference between the norms of these gradients and the target value
    of 1, following a similar approach to what we did in chapter 5.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we’ll use the Adam optimizer for both the critic and the generator:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With that, we have successfully constructed a MuseGAN, which is now ready to
    be trained using the data we prepared earlier in the chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 13.5 Training the MuseGAN to generate music
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have both the MuseGAN model and the training data, we’ll proceed
    to train the model in this section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Similar to our approach in chapters 3 and 4, when training GANs, we’ll alternate
    between training the critic and the generator. In each training iteration, we’ll
    sample a batch of real music from the training dataset and a batch of generated
    music from the generator and present them to the critic for evaluation. During
    critic training, we compare the critic’s ratings with the ground truth and adjust
    the critic network’s weights slightly so that, in the next iteration, the ratings
    will be as high as possible for real music and as low as possible for generated
    music. During generator training, we feed generated music to the critic model
    to obtain a rating and then slightly adjust the generator network’s weights so
    that, in the next iteration, the rating will be higher (as the generator aims
    to create music pieces that fool the critic into thinking they are real). We repeat
    this process for many iterations, gradually enabling the generator network to
    create more realistic music pieces.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we’ll discard the critic network and use the trained
    generator to create music pieces by feeding it four noise vectors (chords, style,
    melody, and groove).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 13.5.1 Training the MuseGAN
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we embark on the training loops for the MuseGAN model, we first define
    a few hyperparameters and helper functions. The hyperparameter `repeat` controls
    how many times we train the critic in each iteration, `display_step` specifies
    how often we display output, and `epochs` is the number of epochs we train the
    model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.5 Hyperparameters and helper functions
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① Defines a few hyperparameters
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines alpha to create interpolated music
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a gp() function to calculate gradient penalty
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines a noise() function to retrieve four random noise vectors
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: The batch size is set at 64, and this helps us determine how many sets of random
    noise vectors to retrieve to create a batch of fake music. We’ll train the critic
    for five iterations and the generator just once in each training loop because
    an effective critic is essential for training the generator. We’ll display training
    losses after every 10 epochs. We’ll train the model for 500 epochs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: We instantiate the `GradientPenalty()` class in the local module to create a
    `gp()` function to calculate the gradient penalty. We also define a `noise()`
    function to generate four random noise vectors to feed to the generator.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the following function, `train_epoch()`, to train the model
    for one epoch.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.6 Training the MuseGAN model for one epoch
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① Iterates through all batches
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: ② Trains the critic five times in each iteration
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '③ The total loss for the critic has three components: loss from evaluating
    real music, loss from evaluating fake music, and the gradient penalty loss.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: ④ Trains the generator
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The training process is very much like that we used in chapter 5 when we train
    the conditional GAN with gradient penalty.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'We now train the model for 500 epochs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you use GPU training, it takes about an hour. Otherwise, it may take several
    hours. Once done, you can save the trained generator to the local folder as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Alternatively, you can download the trained generator from my website: [https://mng.bz/Bglr](https://mng.bz/Bglr).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discard the critic network and use the trained generator to create
    music that mimics the style of Bach.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 13.5.2 Generating music with the trained MuseGAN
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To generate music with the trained generator, we’ll feed four noise vectors
    from the latent space to the generator. Note that we can generate multiple music
    objects at the same time and decode them together to form one continuous piece
    of music. You’ll learn how to do that in this subsection.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'We first load the trained weights in the generator:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Rather than producing a single 4D music object, we can simultaneously generate
    multiple 4D music objects and convert them into one continuous piece of music
    later. For instance, if we aim to create five music objects, we begin by sampling
    five sets of noise vectors from the latent spaces. Each set consists of four vectors:
    chords, style, melody, and groove, like so:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Each generated music object can be transformed into a music piece that lasts
    approximately 8 seconds. In this case, we choose to generate five music objects
    and decode them into a single music piece later, resulting in a duration of about
    40 seconds. You can adjust the value of the variable `num_pieces` according to
    your preference, depending on the desired length of the music piece.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we supply the generator with the five sets of latent variables to produce
    a set of music objects:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output, `preds`, consists of five music objects. Next, we decode these
    objects into a single piece of music, represented as a MIDI file:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We import the `convert_to_midi()` function from the local module `midi_util`.
    Open the file `midi_util.py` that you downloaded earlier and review the definition
    of the `convert_to_midi()` function. This process is similar to what we have done
    earlier in this chapter when we converted the first music object in the training
    set into the file `first_song.midi`. Since MIDI files represent sequences of notes
    over time, we simply concatenate the five music pieces corresponding to the five
    music objects into one extended sequence of notes. This combined sequence is then
    saved as `MuseGAN_song.midi` on your computer.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Find the generated music piece, `MuseGAN_song.midi`, on your computer. Open
    it with a music player of your choice and listen to see if it resembles the music
    pieces from the training set. For comparison, you can listen to a piece of music
    generated by the trained model on my website at [https://mng.bz/dZJv](https://mng.bz/dZJv).
    Note that since the input to the generator, the noise vectors, are randomly drawn
    from the latent space, the music pieces you generate will sound different.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 13.2
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Obtain three sets of random noise vectors (each set should contain chords, style,
    melody, and groove) from the latent space. Feed them to the trained generator
    to obtain three music objects. Decode them into one single piece of music in the
    form of a MIDI file. Save it as `generated_song.midi` on your computer, and play
    it using a music player.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you’ve learned how to build and train a MuseGAN to generate
    music in the style of Bach. Specifically, you’ve approached a piece of music as
    a 4D object and applied the techniques from chapter 4 on deep convolutional layers
    to develop a GAN model. In the next chapter, you’ll explore a different way of
    generating music: treating a piece of music as a sequence of indexes and utilizing
    techniques from NLP to generate music pieces by predicting one index at a time.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MuseGAN treats a piece of music as a multidimensional object akin to an image.
    The generator produces a piece of music and submits it, along with real music
    pieces from the training set, to the critic for evaluation. The generator then
    modifies the music based on the critic’s feedback until it closely resembles real
    music from the training dataset.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Musical notes, octaves, and pitch are fundamental concepts in music theory.
    Octaves represent different levels of musical sound. Each octave is subdivided
    into 12 semitones: C, C#, D, D#, E, F, F#, G, G#, A, A#, B. Within an octave,
    a note is assigned a specific pitch number.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In electronic music production, a track typically refers to an individual layer
    or component of the music. Each track contains multiple bars (or measures). A
    bar is further divided into multiple steps.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To represent a piece of music as a multidimensional object, we structure it
    with a (4, 2, 16, 84) shape: 4 music tracks, with each track consisting of 2 bars,
    each bar containing 16 steps, and each step capable of playing 1 of the 84 different
    notes.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In music creation, incorporating more detailed inputs is essential for achieving
    greater control and variety. Instead of using a single noise vector from the latent
    space for generating shapes, numbers, and images as in previous chapters, we employ
    four distinct noise vectors in the music generation process. Given that each music
    piece consists of four tracks and two bars, we use these four vectors to effectively
    manage this structure. One vector controls all tracks and bars collectively, another
    controls each bar across all tracks, a third oversees all tracks across bars,
    and the fourth manages each individual bar in each track.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](#footnote-000-backlink))  Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, Yi-Hsuan
    Yang, 2017, “MuseGAN: Multi-track Sequential Generative Adversarial Networks for
    Symbolic Music Generation and Accompaniment.” [https://arxiv.org/abs/1709.06298](https://arxiv.org/abs/1709.06298).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
