- en: 13 Music generation with MuseGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Music representation using musical instrument digital interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Treating music generation as an object creation problem similar to image generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training a generative adversarial network to generate music
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating music using the trained MuseGAN model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to now, we have successfully generated shapes, numbers, images, and text.
    In this chapter and the next, we will explore two different ways of generating
    lifelike music. This chapter will apply the techniques from image GANs, treating
    a piece of music as a multidimensional object akin to an image. The generator
    will produce a complete piece of music and submit it to the critic (serving as
    the discriminator because we use the Wasserstein distance with gradient penalty,
    as discussed in chapter 5) for evaluation. The generator will then modify the
    music based on the critic’s feedback until it closely resembles real music from
    the training dataset. In the next chapter, we will treat music as a sequence of
    musical events, employing natural language processing (NLP) techniques. We will
    use a GPT-style Transformer to predict the most probable musical event in a sequence
    based on previous events. This Transformer will generate a long sequence of musical
    events that can be converted into realistic-sounding music.
  prefs: []
  type: TYPE_NORMAL
- en: The field of music generation using AI has gained significant attention; MuseGAN
    is a prominent model, which was introduced by Dong, Hsiao, Yang, and Yang in 2017.^([1](#footnote-000))
    MuseGAN is a deep neural network that utilizes generative adversarial networks
    (GANs) to create multitrack music, with the word Muse signifying the creative
    inspiration behind music. The model is adept at understanding the complex interactions
    between different tracks that represent different musical instruments or different
    voices (which is the case in our training data). As a result, MuseGAN can generate
    compositions that are harmonious and cohesive.
  prefs: []
  type: TYPE_NORMAL
- en: 'MuseGAN, similar to other GAN models, consists of two primary components: the
    generator and the critic (who provides a continuous measure of how real the sample
    is rather than classifying a sample into real or fake). The generator’s task is
    to generate music, whereas the critic assesses the music’s quality and offers
    feedback to the generator. This adversarial interaction enables the generator
    to gradually improve, leading to the creation of more realistic and appealing
    music.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you’re an avid fan of Johann Sebastian Bach and have listened to all
    his compositions. You might wonder if it’s possible to use MuseGAN to create synthetic
    music that mimics his style. The answer is yes, and you’ll learn how to do that
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, you’ll first explore how to represent a piece of multitrack music
    as a multidimensional object. A track is essentially an individual line of music
    or sound, which can be a different instrument such as piano, bass, or drums or
    a different voice such as soprano, alto, tenor, or bass. When composing a track
    in electronic music, you typically organize it into bars (segments of time), subdivide
    each bar into steps for finer control over rhythm, and then assign a specific
    note to each step to craft your melodies and rhythms. As a result, each piece
    of music in our training set is structured with a (4, 2, 16, 84) shape: this means
    there are four music tracks, with each track consisting of 2 bars, each bar containing
    16 steps, and each step capable of playing one of the 84 different notes.'
  prefs: []
  type: TYPE_NORMAL
- en: The style of the music generated by our MuseGAN will be influenced by the training
    data. Since you are interested in Bach’s work, you’ll be training MuseGAN with
    The JSB Chorales dataset, which is a collection of chorales composed by Bach,
    arranged for four tracks. These chorales have been converted into a piano roll
    representation, a method used for visualizing and encoding music, especially for
    digital processing purposes. You’ll learn how to transform a piece of music represented
    in the shape of (4, 2, 16, 84) into a musical instrument digital interface (MIDI)
    file, which can then be played on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: While the generator uses just one single noise vector from the latent space
    to generate different formats of content such as shapes, numbers, and images in
    earlier chapters, the generator in MuseGAN will use four noise vectors when producing
    a piece of music. The use of four separate noise vectors (chords, style, melody,
    and groove, which I’ll explain in detail later in this chapter) is a design choice
    that allows for greater control and diversity in the music generation process.
    Each of these noise vectors represents a different aspect of music, and by manipulating
    them individually, the model can generate more complex and nuanced compositions.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we’ll discard the critic network, a common practice
    in GAN models. We’ll then utilize the trained generator to produce music pieces
    by inputting four noise vectors from the latent space. The music generated in
    this way closely mirrors the style of Bach.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 Digital music representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our goal is to master the art of building and training a GAN model from scratch
    for music generation. To achieve this, we need to start with the fundamentals
    of music theory, including understanding musical notes, octaves, and pitch numbers.
    Following that, we’ll dive into the inner workings of digital music, specifically
    focusing on MIDI files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the type of machine learning model we use for music generation,
    the representation of a piece of music in digital form will vary. For instance,
    in this chapter, we’ll represent music as a multidimensional object, while in
    the next chapter, we’ll use a different format: a sequence of indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll cover basic music theory and then move on to represent
    music digitally using piano rolls. You’ll learn to load and play an example MIDI
    file on your computer. We’ll also introduce the music21 Python library, which
    you’ll install and use to visualize the staff notes associated with the music
    piece. Finally, you’ll learn to represent a piece of music as a multidimensional
    object with the shape of (4, 2, 16, 84).
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.1 Musical notes, octave, and pitch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we’ll be working with a training dataset that represents music
    pieces as 4D objects. To grasp the meaning of the music pieces in the training
    data, it’s essential to first familiarize ourselves with some fundamental concepts
    in music theory, such as musical notes, octaves, and pitch. These concepts are
    interrelated and crucial for understanding the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 illustrates the relationships among these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH13_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1 The relationship between musical notes, octaves, and pitches (also
    known as note numbers). The first column displays the 11 octaves (ranging from
    –1 to 9), representing different levels of musical sound. Each octave is subdivided
    into 12 semitones, which are listed in the top row: C, C#, D, D#, ..., B. Within
    each octave, each note is assigned a specific pitch number, ranging from 0 to
    127, as indicated in the figure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A musical note is a symbol representing a specific sound in music. These notes
    are the foundational elements of music, used to craft melodies, chords, and rhythms.
    Each note is assigned a name (such as A, B, C, D, E, F, G) and corresponds to
    a specific frequency, which determines its pitch: whether the note sounds high
    or low. For instance, a middle C (C4) typically has a frequency of about 262 hertz,
    meaning its sound waves vibrate 262 times per second.'
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering about the meaning of the term “middle C (C4).” The number
    4 in C4 refers to the octave, which is the distance between one level of musical
    pitch and the next. In figure 13.1, the far-left column displays 11 octave levels,
    ranging from –1 to 9\. The frequency of a sound doubles as you move from one octave
    level to the next. For example, note A4 is usually tuned to 440 hertz, while A5,
    one octave above A4, is tuned to 880 hertz..
  prefs: []
  type: TYPE_NORMAL
- en: 'In Western music, an octave is divided into 12 semitones, each corresponding
    to a specific note. The top row of figure 13.1 lists these 12 semitones: C, C#,
    D, D#, ..., B. Moving up or down by 12 semitones takes you to the same note name
    but in a higher or lower octave. As mentioned earlier, A5 is one octave above
    A4.'
  prefs: []
  type: TYPE_NORMAL
- en: Each note within a specific octave is assigned a pitch number, ranging from
    0 to 127, as depicted in figure 13.1\. For example, the note C4 has a pitch number
    of 60, while F3 has a pitch number of 53\. The pitch number is a more efficient
    way to represent musical notes since it specifies both the octave level and the
    semitone. The training data you’ll be using in this chapter is encoded using pitch
    numbers for this very reason.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.2 An introduction to multitrack music
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s first talk about how multitrack music works and how it is represented
    digitally. In electronic music production, a “track” typically refers to an individual
    layer or component of the music, such as a drum track, a bass track, or a melody
    track. In classical music, tracks might represent different vocal parts, like
    soprano, alto, tenor, and bass. For instance, the training dataset we’re using
    in this chapter, the JSB Chorales dataset, consists of four tracks corresponding
    to four vocal parts. In music production, each track can be individually edited
    and processed within a digital audio workstation (DAW). These tracks are composed
    of various musical elements, including bars, steps, and notes.
  prefs: []
  type: TYPE_NORMAL
- en: A bar (or measure) is a segment of time defined by a specified number of beats,
    with each beat having a certain note duration. In many popular music genres, a
    bar typically contains four beats, although this can vary based on the time signature
    of the piece. The total number of bars in a track is determined by the track’s
    length and structure. For example, in our training dataset, each track comprises
    two bars.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of step sequencing, a technique commonly used for programming
    rhythms and melodies in electronic music, a “step” represents a subdivision of
    a bar. In a standard 4/4 time signature (four beats in a bar and four steps in
    a beat), you might find 16 steps per bar, with each step corresponding to a sixteenth
    of a bar.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, each step contains a musical note. In our dataset, we limit the range
    to the 84 most frequently used notes (with pitch numbers from 0 to 83). Therefore,
    the musical note in a step is encoded as a one-hot vector with 84 values.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate these concepts with a practical example, download the file example.midi
    from the book’s GitHub repository at [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)
    and save it in the /files/ directory on your computer. A file with the .midi extension
    is a MIDI file. MIDI is a technical standard that outlines a protocol, digital
    interface, and connectors for enabling electronic musical instruments, computers,
    and other related devices to connect and communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'MIDI files can be played on most music players on your computer. To get a sense
    of the type of music in our training data, open the file example.midi you just
    downloaded with a music player on your computer. It should sound like this music
    file I placed on my website: [https://mng.bz/lrJB](https://mng.bz/lrJB). The file
    example.midi is converted from one of the music pieces in the training dataset
    in this chapter. Later you’ll learn how to convert a piece of music in the training
    dataset with a shape of (4, 2, 16, 84) into a MIDI file that can be played on
    your computer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the music21 Python library, a powerful and comprehensive toolkit
    designed for music analysis, composition, and manipulation, to visualize how various
    music concepts work. Therefore, run the following line of code in a new cell in
    the Jupyter Notebook app on your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The music21 library enables you to visualize music as staff notation to have
    a better understanding of tracks, bars, steps, and notes. To achieve this, you
    must first install the MuseScore application on your computer. Visit [https://musescore.org/en/download](https://musescore.org/en/download)
    and download the most recent version of the MuseScore app for your operating system.
    As of this writing, the latest version is MuseScore 4, which we’ll use as our
    example. Ensure you know the file path of the MuseScore app on your computer.
    For instance, in Windows, the path is C:\Program Files\MuseScore 4\bin\MuseScore4.exe.
    Run the code cell in the following listing to visualize the staff notation for
    the file example.midi.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.1 Visualizing the staff notation using the music21 library
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Shows the image in Jupyter notebook instead of in the original app
  prefs: []
  type: TYPE_NORMAL
- en: ② Opens the MIDI file
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines the path of the MuseScore app
  prefs: []
  type: TYPE_NORMAL
- en: ④ Shows the staff notation
  prefs: []
  type: TYPE_NORMAL
- en: For users of the macOS operating system, change the path in the preceding code
    cell to /Applications/MuseScore 4.app/Contents/MacOS/mscore. For Linux users,
    modify the path to /home/[user name]/.local/bin/mscore4portable, substituting
    [user name] with your actual username. For instance, my username is `mark`, so
    the path is /home/mark/.local/bin/mscore4portable.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the previous code cell will display a staff notation similar to what
    is illustrated in figure 13.2\. Please note that the annotations in the figure
    are added by me, so you will only see the staff notation without any annotations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH13_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2 Staff notation for a piece of music in JSB Chorales dataset. The
    music has four tracks, representing the four voices in a chorale: soprano, alto,
    tenor, and bass. The notation is structured into two bars for each track, with
    the left and right halves representing the first and second bars, respectively.
    Each bar consists of 16 steps, aligning with the 4/4 time signature where a bar
    contains four beats, each subdivided into four sixteenth notes. A total of 84
    different pitches are possible, and each note is represented as a one-hot vector
    with 84 values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSB Chorales dataset, which consists of chorale music pieces by Johann
    Sebastian Bach, is often used for training machine learning models in music generation
    tasks. The shape (4, 2, 16, 84) of each music piece in the dataset can be explained
    as follows. Four represents the four voices in a chorale: soprano, alto, tenor,
    and bass. Each voice is treated as a separate track in the dataset. Each piece
    is divided into two bars (also called measures). The dataset is formatted this
    way to standardize the length of the music pieces for training purposes. The number
    16 represents the number of steps (or subdivisions) in each bar. Finally, the
    note is one-hot encoded with 84 values, denoting the number of possible pitches
    (or notes) that can be played in each step.'
  prefs: []
  type: TYPE_NORMAL
- en: '13.1.3 Digitally represent music: Piano rolls'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A piano roll is a visual representation of music often used in MIDI sequencing
    software and DAWs. It is named after the traditional piano rolls used in player
    pianos, which contained a physical roll of paper with holes punched in it to represent
    musical notes. In a digital context, the piano roll serves a similar function
    but in a virtual format.
  prefs: []
  type: TYPE_NORMAL
- en: The piano roll is displayed as a grid, with time represented horizontally (from
    left to right) and pitch represented vertically (from bottom to top). Each row
    corresponds to a specific musical note, with higher notes at the top and lower
    notes at the bottom, similar to the layout of a piano keyboard.
  prefs: []
  type: TYPE_NORMAL
- en: Notes are represented as bars or blocks on the grid. The position of a note
    block along the vertical axis indicates its pitch, while its position along the
    horizontal axis indicates its timing in the music. The length of the note block
    represents the duration of the note.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the music21 library to illustrate what a piano roll looks like. Run
    this line of code in a new cell in your Jupyter Notebook app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output is shown in figure 13.3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The music21 library also allows you to see the quantized notes corresponding
    to the preceding piano roll:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../../OEBPS/Images/CH13_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 The piano roll for a piece of music. The piano roll is a graphical
    representation of a musical piece, depicted as a grid with time progressing horizontally
    from left to right and pitch represented vertically from bottom to top. Each row
    on the grid corresponds to a distinct musical note, arranged in a manner akin
    to the keyboard of a piano, with higher notes positioned at the top and lower
    notes at the bottom. This specific piece of music comprises two bars, resulting
    in two distinct sections visible in the graph. The vertical placement of a note
    block signifies its pitch, while its horizontal location indicates when the note
    is played in the piece. Additionally, the length of the note block reflects the
    duration for which the note is sustained.
  prefs: []
  type: TYPE_NORMAL
- en: I omitted most of the output. The first value in each line in the previous output
    represents time. It increases by 0.25 seconds after each line in most cases. If
    the time increase in the next line is more than 0.25 seconds, it means a note
    lasts more than 0.25 seconds. As you can see, the starting note is E4\. After
    0.25 seconds, the note changes to A4, and then G4, and so on. This explains the
    first three blocks (far left) in figure 13.3, which have values E, A, and G, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be curious about how to convert the sequence of musical notes into
    an object with the shape (4, 2, 16, 84). To understand this, let’s examine the
    pitch number at each time step in the musical notes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block has converted the musical note in each time step into
    a pitch number, in the range of 0 to 83 based on the mapping used in figure 13.1\.
    Each of the pitch numbers is then converted to a one-hot variable with 84 values,
    with value –1 everywhere, except 1 in one position. We use –1 and 1 in one-hot
    encoding instead of 0 and 1 because placing values between –1 and 1 centers the
    data around 0, which can make training more stable and faster. Many activation
    functions and weight initialization methods assume input data is centered around
    0\. Figure 13.4 illustrates how a piece of MIDI music is encoded into an object
    in the shape of (4, 2, 16, 84).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH13_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 How to represent a piece of music using a 4D object. In our training
    data, each piece of music is represented by a 4D object in the shape of (4, 2,
    16, 84). The first dimension represents the four music tracks, which are the four
    voices in the music (soprano, alto, tenor, and bass). Each music track is divided
    into two bars. There are four beats in each bar, and each beat has four notes;
    we therefore have 16 notes in a bar. Finally, each note is represented by a one-hot
    variable with 84 values, with –1 everywhere and 1 in one place.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 explains the dimensions of the music object shaped (4, 2, 16, 84).
    In essence, each musical piece comprises four tracks, with each track containing
    two bars. Each bar is subdivided into 16 notes. Given that the pitch numbers range
    from 0 to 83 in our training set, each note is represented by a one-hot vector
    with 84 values.
  prefs: []
  type: TYPE_NORMAL
- en: In subsequent discussions on preparing training data, we will explore how to
    transform an object with the shape (4, 2, 16, 84) back into a music piece in MIDI
    format, enabling playback on a computer.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 A blueprint for music generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When creating music, we need to incorporate more detailed inputs for enhanced
    control and variety. Unlike the approach of utilizing a single noise vector from
    the latent space for generating shapes, numbers, and images, we will employ four
    distinct noise vectors in the music generation process. Since each music piece
    comprises four tracks and two bars, we’ll utilize four vectors to manage this
    structure. We’ll use one vector to govern all tracks and bars collectively, another
    vector to control each bar across all tracks, a third vector to oversee all tracks
    across bars, and a fourth one to manage each individual bar in each track. This
    section will introduce you to the concepts of chords, style, melody, and groove
    and explain how they influence various aspects of the music generation. After
    that, we’ll discuss the steps involved in building and training the MuseGAN model.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.1 Constructing music with chords, style, melody, and groove
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Later, in the music generation stage, we obtain four noise vectors (chords,
    style, melody, and groove) from the latent space and feed them to the generator
    to create a piece of music. You may be wondering the meaning of these four pieces
    of information. In music, chords, style, melody, and groove are key elements that
    contribute to a piece’s overall sound and feel. Next I provide a brief explanation
    of each element.
  prefs: []
  type: TYPE_NORMAL
- en: Style refers to the characteristic way in which music is composed, performed,
    and experienced. It includes the genre (such as jazz, classical, rock, and so
    on), the era in which the music was created, and the unique approach of the composer
    or performer. Style is influenced by cultural, historical, and personal factors,
    and it helps to define the music’s identity.
  prefs: []
  type: TYPE_NORMAL
- en: Groove is the rhythmic feel or swing in music, especially in styles like funk,
    jazz, and soul. It’s what makes you want to tap your foot or dance. A groove is
    created by the pattern of accents, the interplay between the rhythm section (drums,
    bass, etc.), and the tempo. It’s the element that gives music its sense of motion
    and flow.
  prefs: []
  type: TYPE_NORMAL
- en: Chords are combinations of two or more notes played simultaneously. They provide
    the harmonic foundation for music. Chords are built on scales and are used to
    create progressions that give music its structure and emotional depth. Different
    chord types (major, minor, diminished, augmented, etc.) and their arrangements
    can evoke various moods and feelings in the listener.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, melody is the sequence of notes that is most easily recognizable in
    a piece of music. It’s the part that you might hum or sing along to. Melodies
    are often built from scales and are characterized by their pitch, rhythm, and
    contour (the pattern of rises and falls in pitch). A good melody is memorable
    and expressive, conveying the main musical and emotional themes of the piece.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these elements work in harmony to create the overall sound and experience
    of a musical piece. Each element has its role, but they all interact and influence
    each other to produce the final music piece. Specifically, a music piece consists
    of four tracks, each with two bars, resulting in eight bar/track combinations.
    We’ll use one noise vector for style, applied to all eight bars. We’ll use eight
    different noise vectors for melody, each used in a unique bar. There are four
    noise vectors for groove, each applied to a different track, remaining the same
    across both bars. Two noise vectors will be used for chords, one for each bar.
    Figure 13.5 provides a diagram of how these four elements contribute to the creation
    of a complete piece of music.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH13_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 Music generation using chords, style, melody, and groove. Each music
    composition consists of four tracks and spans two bars. We will extract four noise
    vectors from the latent space for this purpose. The first vector, representing
    chords, has a dimension of (1, 32). This vector will be processed through a temporal
    network to expand the chords into two (1, 32) vectors, corresponding to the two
    bars, with identical values across all tracks. The second vector, denoting style,
    also has a dimension of (1, 32) and remains constant across all tracks and bars.
    The third vector, melody, is shaped as (4, 32). It will be stretched through a
    temporal network into two (4, 32) vectors, resulting in eight (1, 32) vectors,
    each representing a unique track and bar combination. Lastly, the fourth vector,
    groove, with a dimension of (4, 32), will be applied to the four tracks, maintaining
    the same values for both bars.
  prefs: []
  type: TYPE_NORMAL
- en: The generator creates a piece of music by generating one bar in one track at
    a time. For this, it requires four noise vectors, each with a shape of (1, 32),
    as input. These vectors represent chords, style, melody, and groove, and each
    controls a distinct aspect of the music, as previously explained. Since the music
    piece consists of four tracks, each with two bars, there are a total of eight
    bar/track combinations. Consequently, we need eight sets of chords, style, melody,
    and groove to generate all parts of the music piece.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll obtain four noise vectors from the latent space corresponding to chords,
    style, melody, and groove. We’ll also introduce a temporal network later, whose
    role is to expand the input along the bar dimension. With two bars, this means
    doubling the size of the input. Music is inherently temporal, with patterns and
    structures that unfold over time. The temporal network in MuseGAN is designed
    to capture these temporal dependencies, ensuring that the generated music has
    a coherent and logical progression.
  prefs: []
  type: TYPE_NORMAL
- en: The noise vector for chords has a shape of (1, 32). After processing it through
    the temporal network, we obtain two (1, 32) sized vectors. The first vector is
    used across all four tracks in the first bar, while the second vector is used
    across all four tracks in the second bar.
  prefs: []
  type: TYPE_NORMAL
- en: The noise vector for style, also with a shape of (1, 32), is applied uniformly
    across all eight track/bar combinations. Note that we’ll not pass the style vector
    through the temporal network since the style vector is designed to be the same
    across bars.
  prefs: []
  type: TYPE_NORMAL
- en: The noise vector for melody has a shape of (4, 32). When passed through the
    temporal network, it yields two (4, 32) sized vectors, which further break down
    into eight (1, 32) sized vectors. Each of these is used in a unique track/bar
    combination.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the noise vector for groove, shaped as (4, 32), is used such that each
    (1, 32) sized vector is applied to a different track, remaining the same across
    both bars. We won’t pass the groove vector through the temporal network since
    the groove vector is designed to be the same across bars.
  prefs: []
  type: TYPE_NORMAL
- en: After generating a bar of music for each of the eight bar/track combinations,
    we’ll merge them to create a full piece of music, consisting of four distinct
    tracks, each comprising two unique bars.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.2 A blueprint to train a MuseGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chapter 1 provided an overview of the foundational concepts behind GANs. In
    chapters 3 to 5, you explored the creation and training of GANs for generating
    shapes, numbers, and images. This subsection will summarize the steps for building
    and training MuseGAN, highlighting the differences from the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The style of music generated by MuseGAN is influenced by the training data’s
    style. Therefore, you should first collect a dataset of Bach’s compositions in
    a format suitable for training. Next, you’ll create a MuseGAN model, which consists
    of a generator and a critic. The generator network takes four random noise vectors
    as input (chords, style, melody, and groove) and outputs a piece of music. The
    critic network evaluates a piece of music and assigns a rating, with higher scores
    for real music (from the training set) and lower scores for fake music (produced
    by the generator). Both the generator and critic networks utilize deep convolutional
    layers to capture the spatial features of the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH13_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 A diagram of the steps involved in training MuseGAN to generate
    music. The generator produces a fake music piece by drawing four random noise
    vectors from the latent space (top left) and presents it to the critic (middle).
    The critic evaluates the piece and assigns a rating. A high rating suggests that
    the piece is likely from the training dataset, while a lower rating indicates
    that the piece is likely fake (generated by the generator). Additionally, an interpolated
    music piece created from a mix of real and fake samples (top left) is presented
    to the critic. The training process incorporates a gradient penalty based on the
    critic’s rating of this interpolated piece, which is added to the total loss.
    The ratings are then compared to the ground truth, allowing both the critic and
    the generator to learn from these evaluations. After numerous training iterations,
    the generator becomes proficient at producing music pieces that are virtually
    indistinguishable from real samples.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.6 illustrates the training process of MuseGAN. The generator (the
    bottom left of the figure) receives four random noise vectors (chords, style,
    melody, and groove) as input and produces fake music pieces (step 1 in figure
    13.6). These noise vectors are drawn from the latent space, which represents the
    range of potential outputs the GAN can generate, enabling the creation of diverse
    data samples. These fake music pieces, along with real ones from the training
    set (top right), are then evaluated by the critic (step 3). The critic (bottom
    center) assigns scores to all music pieces, aiming to give high scores to real
    music and low scores to fake music (step 4).
  prefs: []
  type: TYPE_NORMAL
- en: To guide the adjustment of model parameters, appropriate loss functions must
    be chosen for both the generator and the critic. The generator’s loss function
    is designed to encourage the production of data points that closely resemble those
    from the training dataset. Specifically, the loss function for the generator is
    the negative of the critic’s rating. By minimizing this loss function, the generator
    strives to create music pieces that receive high ratings from the critic. On the
    other hand, the critic’s loss function is formulated to encourage accurate assessment
    of real and generated data points. Thus, the loss function for the critic is the
    rating itself if the music piece is from the training set and the negative of
    the rating if it is generated by the generator. In essence, the critic aims to
    assign high ratings to real music pieces and low ratings to fake ones.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we incorporate the Wasserstein distance with gradient penalty
    into the loss function, as we did in chapter 5, to enhance the training stability
    and performance of GAN models. To achieve this, an interpolated music piece, blending
    real and fake music (top left in figure 13.6), is evaluated by the critic. The
    gradient penalty, based on the critic’s rating of this interpolated piece, is
    then added to the total loss during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the training loop, we alternate between training the critic and the
    generator. In each training iteration, we sample a batch of real music pieces
    from the training set and a batch of fake music pieces generated by the generator.
    We calculate the total loss by comparing the critic’s ratings (i.e., scores) with
    the ground truth (whether a music piece is real or fake). We then slightly adjust
    the weights in both the generator and critic networks so that, in subsequent iterations,
    the generator produces more realistic music pieces, and the critic assigns higher
    scores to real music and lower scores to fake music.
  prefs: []
  type: TYPE_NORMAL
- en: Once MuseGAN is fully trained, music can be created by inputting four random
    noise vectors into the trained generator.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 Preparing the training data for MuseGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use chorale compositions by Johann Sebastian Bach as our training dataset,
    expecting the generated music to resemble Bach’s style. If you prefer the style
    of a different musician, you can use their work as the training data instead.
    In this section, we’ll start by downloading the training data and organizing it
    into batches for later training.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we’ve learned that the music pieces in the training set will be
    represented as 4D objects. In this section, you’ll also learn how to convert these
    multidimensional objects into playable music pieces on a computer. This conversion
    is essential because MuseGAN generates multidimensional objects similar to those
    in the training set. Later in the chapter, we’ll transform the multidimensional
    objects produced by MuseGAN into MIDI files, enabling you to listen to the generated
    music on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.1 Downloading the training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll use the JSB Chorales piano rolls dataset as our training set. Go to Cheng-Zhi
    Anna Huang’s GitHub repository ([https://github.com/czhuang/JSB-Chorales-dataset](https://github.com/czhuang/JSB-Chorales-dataset))
    and download the music file Jsb16thSeparated.npz. Save the file in the /files/
    directory on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, download the two utility modules midi_util.py and MuseGAN_util.py from
    the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and save them in the /utils/ directory on your computer. The code in this chapter
    is adapted from the excellent GitHub repository by Azamat Kanametov ([https://github.com/akanametov/musegan](https://github.com/akanametov/musegan)).
    With these files in place, we can now load the music files and organize them into
    batches for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We load the dataset you just downloaded into Python, then extract the first
    song and name it `first_song`. Since songs are represented as multidimensional
    objects, we print out the shape of the first song. Finally, we place the training
    data in batches of 64, to be used later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Each song in the dataset has a shape of (4, 2, 16, 84), as shown in the previous
    output. This indicates that each song consists of four tracks, each with two bars.
    Each bar contains 16 time steps, and at each time step, the musical note is represented
    by a one-hot vector with 84 values. In each one-hot vector, all values are set
    to –1, except for one position where the value is set to 1, indicating the presence
    of a note. You can verify the range of values in the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The previous output shows that the values in each music piece are either –1
    or 1.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.2 Converting multidimensional objects to music pieces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, the songs are formatted as PyTorch tensors and are ready to be inputted
    into the MuseGAN model. However, before we proceed, it’s important to gain a better
    understanding of how to convert these multidimensional objects into playable music
    pieces on your computer. This will help us later to convert generated music pieces
    into playable files.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we’ll convert all the 84-value one-hot variables into pitch numbers
    ranging from 0 to 83:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Converts 84-value one-hot vectors to numbers between 0 and 83
  prefs: []
  type: TYPE_NORMAL
- en: ② Reshapes the result to (32, 4)
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the output displayed here, each column represents a music track, with numbers
    ranging from 0 to 83\. These numbers correspond to pitch numbers, as you have
    seen earlier in figure 13.1.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll proceed to convert the tensor `midi_note_score` in the previous code
    block into an actual MIDI file, allowing you to play it on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.2 Converting pitch numbers to a MIDI file
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through four music tracks
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterates through all notes in each track
  prefs: []
  type: TYPE_NORMAL
- en: ③ Adds 0.25 seconds to each time step
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adds the note to the music stream
  prefs: []
  type: TYPE_NORMAL
- en: After running the preceding code cell, you’ll see a MIDI file, `first_song.midi`,
    on your computer. Play it with a music player on your computer to get a sense
    of what type of music we are using to train the MuseGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 13.1
  prefs: []
  type: TYPE_NORMAL
- en: Convert the second song in the training dataset into a MIDI file. Save it as
    `second_song.midi` and play it using a music player on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Building a MuseGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In essence, we will treat a music piece as an object with multiple dimensions.
    Using techniques from chapters 4 to 6, we will tackle this task using deep convolutional
    neural networks for their ability to effectively extract spatial features from
    multidimensional objects. In MuseGAN, we’ll construct a generator and a critic,
    similar to how a generator in image creation refines an image based on a critic’s
    feedback. The generator will produce a music piece as a 4D object.
  prefs: []
  type: TYPE_NORMAL
- en: Both real music from our training set and fake music from the generator will
    be presented to the critic. The critic will score each piece from negative infinity
    to positive infinity, with higher scores indicating a higher likelihood of the
    music being real. The critic aims to give high scores to real music and low scores
    to fake music. Conversely, the generator aims to produce music that is indistinguishable
    from real music, thereby receiving high scores from the critic.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will build a MuseGAN model, comprising a generator network
    and a critic network. The critic network employs deep convolutional layers to
    extract distinct features from multidimensional objects, enhancing its ability
    to evaluate music pieces. On the other hand, the generator network utilizes deep
    transposed convolutional layers to produce feature maps aimed at generating realistic
    music pieces. Later, we will train the MuseGAN model using music pieces from the
    training set.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.1 A critic in MuseGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As explained in chapter 5, incorporating the Wasserstein distance into the loss
    function can help stabilize training. Therefore, in MuseGAN, we adopt a similar
    approach and use a critic instead of a discriminator. The critic is not a binary
    classifier; rather, it evaluates the output of the generator (in this case, a
    music piece) and assigns a score ranging from –∞ to ∞. A higher score indicates
    a greater likelihood that the music is real (i.e., from the training set).
  prefs: []
  type: TYPE_NORMAL
- en: We construct a music critic neural network as shown in the following listing,
    and its definition can be found in the file MuseGAN_util.py that you downloaded
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.3 The critic network in MuseGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Passes the input through several Conv3d layers
  prefs: []
  type: TYPE_NORMAL
- en: ② Flattens the output
  prefs: []
  type: TYPE_NORMAL
- en: ③ Passes the output through two linear layers
  prefs: []
  type: TYPE_NORMAL
- en: The input to the critic network is a music piece with dimensions (4, 2, 16,
    84). The network primarily consists of several Conv3d layers. These layers treat
    each track of the music piece as a 3D object and apply filters to extract spatial
    features. The operation of the Conv3d layers is similar to the Conv2d layers used
    in image generation, as discussed in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the final layer of the critic model is linear, and
    we do not apply any activation function to its output. As a result, the output
    from the critic model is a value ranging from –∞ to ∞, which can be interpreted
    as the critic’s rating of a music piece.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.2 A generator in MuseGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed earlier in this chapter, the generator will produce one bar of
    music at a time, and we will then combine these eight bars to form a complete
    piece of music.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using just a single noise vector, the generator in MuseGAN takes
    four independent noise vectors as input to control various aspects of the music
    being generated. Two of these vectors will be processed through a temporal network
    to extend them along the bar dimension. While the style and groove vectors are
    designed to remain constant across both bars, the chords and melody vectors are
    designed to vary between bars. Therefore, we will first establish a temporal network
    to stretch the chords and melody vectors across the two bars, ensuring that the
    generated music has a coherent and logical progression over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the local module `MuseGAN_util` you downloaded earlier, we define the `TemporalNetwork()`
    class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① The input dimension to the TemporalNetwork() class is (1, 32).
  prefs: []
  type: TYPE_NORMAL
- en: ② The output dimension is (2, 32).
  prefs: []
  type: TYPE_NORMAL
- en: The `TemporalNetwork()` class described here employs two ConvTranspose2d layers
    to expand a single noise vector into two distinct noise vectors, each corresponding
    to one of the two bars. As we covered in chapter 4, transposed convolutional layers
    serve the purpose of upsampling and generating feature maps. In this context,
    they are utilized to extend noise vectors across different bars.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of generating all bars in all tracks at once, we’ll generate the music
    one bar at a time. Doing so allows MuseGAN to balance computational efficiency,
    flexibility, and musical coherence, resulting in more structured and appealing
    musical compositions. Therefore, we proceed to construct a bar generator that
    is responsible for generating a segment of the music piece: one bar within a track.
    We introduce the `BarGenerator()` class within the local `MuseGAN_util` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① We concatenate chords, style, melody, and groove into one vector, with a size
    of 4 * 32.
  prefs: []
  type: TYPE_NORMAL
- en: ② The input is then reshaped into 2D, and we use several ConvTranspose2d layers
    for upsampling and music feature generation.
  prefs: []
  type: TYPE_NORMAL
- en: '③ The output has a shape of (1, 1, 16, 84): 1 track, 1 bar, and 16 notes, and
    each note is represented by a 84-value vector.'
  prefs: []
  type: TYPE_NORMAL
- en: The `BarGenerator()` class accepts four noise vectors as input, each representing
    chords, style, melody, and groove for a specific bar in a different track, all
    with a shape of (1, 32). These vectors are concatenated into a single 128-value
    vector before being fed into the `BarGenerator()` class. The output from the `BarGenerator()`
    class is a bar of music, with dimensions (1, 1, 16, 84), indicating 1 track, 1
    bar, and 16 notes, with each note represented by an 84-value vector.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will employ the `MuseGenerator()` class to generate a complete piece
    of music, consisting of four tracks with two bars per track. Each bar is constructed
    using the `BarGenerator()` class defined earlier. To achieve this, we define the
    `MuseGenerator()` class in the local MuseGAN_util module.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.4 The music generator in MuseGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through two bars
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterates through four tracks
  prefs: []
  type: TYPE_NORMAL
- en: ③ Concatenates chords, style, melody, and groove into one input
  prefs: []
  type: TYPE_NORMAL
- en: ④ Generates one bar using the bar generator
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Concatenates eight bars into one complete piece of music
  prefs: []
  type: TYPE_NORMAL
- en: The generator takes four noise vectors as inputs. It iterates through four tracks
    and two bars. In each iteration, it utilizes the bar generator to create a single
    bar of music. Upon completing all iterations, the MuseGenerator() class merges
    the eight bars into one cohesive music piece, which has dimensions of (4, 2, 16,
    84).
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.3 Optimizers and the loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We create a generator and a critic based on the `MuseGenerator()` and `MuseCritic()`
    classes in the local module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed in chapter 5, the critic generates a rating instead of a classification,
    so the loss function is defined as the negative average of the product between
    the prediction and the target. As a result, we define the following `loss_fn()`
    function in the local module `MuseGAN_util`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: During training, for the generator, we’ll assign a value of 1 to the target
    argument in the `loss_fn()` function. This setting aims to guide the generator
    in producing music that can achieve the highest possible rating (i.e., the variable
    pred in the `loss_fn()` function). For the critic, we’ll set the target to 1 for
    real music and –1 for fake music in the loss function. This setting guides the
    critic to assign a high rating to real music and a low rating to fake music.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the approach in chapter 5, we incorporate the Wasserstein distance
    with a gradient penalty into the critic’s loss function to ensure training stability.
    The gradient penalty is defined in the `MuseGAN_util.py` file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `GradientPenalty()` class requires two inputs: interpolated music, which
    is a blend of real and fake music, and the ratings assigned by the critic network
    to this interpolated music. The class computes the gradient of the critic’s ratings
    concerning the interpolated music. The gradient penalty is then calculated as
    the squared difference between the norms of these gradients and the target value
    of 1, following a similar approach to what we did in chapter 5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we’ll use the Adam optimizer for both the critic and the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With that, we have successfully constructed a MuseGAN, which is now ready to
    be trained using the data we prepared earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 13.5 Training the MuseGAN to generate music
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have both the MuseGAN model and the training data, we’ll proceed
    to train the model in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to our approach in chapters 3 and 4, when training GANs, we’ll alternate
    between training the critic and the generator. In each training iteration, we’ll
    sample a batch of real music from the training dataset and a batch of generated
    music from the generator and present them to the critic for evaluation. During
    critic training, we compare the critic’s ratings with the ground truth and adjust
    the critic network’s weights slightly so that, in the next iteration, the ratings
    will be as high as possible for real music and as low as possible for generated
    music. During generator training, we feed generated music to the critic model
    to obtain a rating and then slightly adjust the generator network’s weights so
    that, in the next iteration, the rating will be higher (as the generator aims
    to create music pieces that fool the critic into thinking they are real). We repeat
    this process for many iterations, gradually enabling the generator network to
    create more realistic music pieces.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we’ll discard the critic network and use the trained
    generator to create music pieces by feeding it four noise vectors (chords, style,
    melody, and groove).
  prefs: []
  type: TYPE_NORMAL
- en: 13.5.1 Training the MuseGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we embark on the training loops for the MuseGAN model, we first define
    a few hyperparameters and helper functions. The hyperparameter `repeat` controls
    how many times we train the critic in each iteration, `display_step` specifies
    how often we display output, and `epochs` is the number of epochs we train the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.5 Hyperparameters and helper functions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a few hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines alpha to create interpolated music
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a gp() function to calculate gradient penalty
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines a noise() function to retrieve four random noise vectors
  prefs: []
  type: TYPE_NORMAL
- en: The batch size is set at 64, and this helps us determine how many sets of random
    noise vectors to retrieve to create a batch of fake music. We’ll train the critic
    for five iterations and the generator just once in each training loop because
    an effective critic is essential for training the generator. We’ll display training
    losses after every 10 epochs. We’ll train the model for 500 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: We instantiate the `GradientPenalty()` class in the local module to create a
    `gp()` function to calculate the gradient penalty. We also define a `noise()`
    function to generate four random noise vectors to feed to the generator.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the following function, `train_epoch()`, to train the model
    for one epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.6 Training the MuseGAN model for one epoch
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through all batches
  prefs: []
  type: TYPE_NORMAL
- en: ② Trains the critic five times in each iteration
  prefs: []
  type: TYPE_NORMAL
- en: '③ The total loss for the critic has three components: loss from evaluating
    real music, loss from evaluating fake music, and the gradient penalty loss.'
  prefs: []
  type: TYPE_NORMAL
- en: ④ Trains the generator
  prefs: []
  type: TYPE_NORMAL
- en: The training process is very much like that we used in chapter 5 when we train
    the conditional GAN with gradient penalty.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now train the model for 500 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If you use GPU training, it takes about an hour. Otherwise, it may take several
    hours. Once done, you can save the trained generator to the local folder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can download the trained generator from my website: [https://mng.bz/Bglr](https://mng.bz/Bglr).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discard the critic network and use the trained generator to create
    music that mimics the style of Bach.
  prefs: []
  type: TYPE_NORMAL
- en: 13.5.2 Generating music with the trained MuseGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To generate music with the trained generator, we’ll feed four noise vectors
    from the latent space to the generator. Note that we can generate multiple music
    objects at the same time and decode them together to form one continuous piece
    of music. You’ll learn how to do that in this subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first load the trained weights in the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Rather than producing a single 4D music object, we can simultaneously generate
    multiple 4D music objects and convert them into one continuous piece of music
    later. For instance, if we aim to create five music objects, we begin by sampling
    five sets of noise vectors from the latent spaces. Each set consists of four vectors:
    chords, style, melody, and groove, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Each generated music object can be transformed into a music piece that lasts
    approximately 8 seconds. In this case, we choose to generate five music objects
    and decode them into a single music piece later, resulting in a duration of about
    40 seconds. You can adjust the value of the variable `num_pieces` according to
    your preference, depending on the desired length of the music piece.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we supply the generator with the five sets of latent variables to produce
    a set of music objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, `preds`, consists of five music objects. Next, we decode these
    objects into a single piece of music, represented as a MIDI file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We import the `convert_to_midi()` function from the local module `midi_util`.
    Open the file `midi_util.py` that you downloaded earlier and review the definition
    of the `convert_to_midi()` function. This process is similar to what we have done
    earlier in this chapter when we converted the first music object in the training
    set into the file `first_song.midi`. Since MIDI files represent sequences of notes
    over time, we simply concatenate the five music pieces corresponding to the five
    music objects into one extended sequence of notes. This combined sequence is then
    saved as `MuseGAN_song.midi` on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Find the generated music piece, `MuseGAN_song.midi`, on your computer. Open
    it with a music player of your choice and listen to see if it resembles the music
    pieces from the training set. For comparison, you can listen to a piece of music
    generated by the trained model on my website at [https://mng.bz/dZJv](https://mng.bz/dZJv).
    Note that since the input to the generator, the noise vectors, are randomly drawn
    from the latent space, the music pieces you generate will sound different.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 13.2
  prefs: []
  type: TYPE_NORMAL
- en: Obtain three sets of random noise vectors (each set should contain chords, style,
    melody, and groove) from the latent space. Feed them to the trained generator
    to obtain three music objects. Decode them into one single piece of music in the
    form of a MIDI file. Save it as `generated_song.midi` on your computer, and play
    it using a music player.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you’ve learned how to build and train a MuseGAN to generate
    music in the style of Bach. Specifically, you’ve approached a piece of music as
    a 4D object and applied the techniques from chapter 4 on deep convolutional layers
    to develop a GAN model. In the next chapter, you’ll explore a different way of
    generating music: treating a piece of music as a sequence of indexes and utilizing
    techniques from NLP to generate music pieces by predicting one index at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MuseGAN treats a piece of music as a multidimensional object akin to an image.
    The generator produces a piece of music and submits it, along with real music
    pieces from the training set, to the critic for evaluation. The generator then
    modifies the music based on the critic’s feedback until it closely resembles real
    music from the training dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Musical notes, octaves, and pitch are fundamental concepts in music theory.
    Octaves represent different levels of musical sound. Each octave is subdivided
    into 12 semitones: C, C#, D, D#, E, F, F#, G, G#, A, A#, B. Within an octave,
    a note is assigned a specific pitch number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In electronic music production, a track typically refers to an individual layer
    or component of the music. Each track contains multiple bars (or measures). A
    bar is further divided into multiple steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To represent a piece of music as a multidimensional object, we structure it
    with a (4, 2, 16, 84) shape: 4 music tracks, with each track consisting of 2 bars,
    each bar containing 16 steps, and each step capable of playing 1 of the 84 different
    notes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In music creation, incorporating more detailed inputs is essential for achieving
    greater control and variety. Instead of using a single noise vector from the latent
    space for generating shapes, numbers, and images as in previous chapters, we employ
    four distinct noise vectors in the music generation process. Given that each music
    piece consists of four tracks and two bars, we use these four vectors to effectively
    manage this structure. One vector controls all tracks and bars collectively, another
    controls each bar across all tracks, a third oversees all tracks across bars,
    and the fourth manages each individual bar in each track.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](#footnote-000-backlink))  Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, Yi-Hsuan
    Yang, 2017, “MuseGAN: Multi-track Sequential Generative Adversarial Networks for
    Symbolic Music Generation and Accompaniment.” [https://arxiv.org/abs/1709.06298](https://arxiv.org/abs/1709.06298).'
  prefs: []
  type: TYPE_NORMAL
