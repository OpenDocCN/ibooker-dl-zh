["```py\nimport os\n...\n\nAZURE_ENDPOINT = os.getenv(\"AOAI_ENDPOINT\")       #1\nAOAI_API_KEY = os.getenv(\"AOAI_KEY\")             #1\nMODEL = \"gpt35\"                                  #1\nAPI_VERSION = \"2024-02-15-preview\"               #1\n\nheaders = {\n    \"api-key\": AOAI_API_KEY,\n    \"Content-Type\": \"application/json\"\n}\n\ndef get_payload():                                      #2\n    return {\n        \"model\": MODEL,\n        \"max_tokens\": 50,\n        \"messages\": [{\"role\": \"system\", \"content\": \n                     ↪\"You are a helpful assistant.\"}, \n                     {\"role\": \"user\", \"content\": \"Hello, world!\"}],\n        \"temperature\": 0.95,\n        \"stream\": True                                  #3\n    }\n\ndef call_api_and_measure_latency():                    #4\n    payload = get_payload()\n    start_time = time.time()         #5\n    response = requests.post(AZURE_ENDPOINT,\n                             headers=headers,\n                             json=payload, timeout=20)\n    latency = time.time() - start_time                   #6\n    return latency, response.status_code\n\nnum_requests = 50                                   #7\n\ndef main():\n    with ThreadPoolExecutor(max_workers=20) as executor:   #8\n        futures = [executor.submit(call_api_and_measure_latency)\n                   ↪for _ in range(num_requests)]\n        latencies = []\n        for future in tqdm(as_completed(futures), total=num_requests):\n            latency, status_code = future.result()\n            print(f\"Latency: {latency}s, Status Code: {status_code}\")\n            latencies.append(latency)\n\n    average_latency = sum(latencies) / len(latencies)      #9\n    print(f\"Average Latency: {average_latency}s\")\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nStarting PTU test...\nMedian Latency: 1.582270622253418s\nAverage Latency: 2.947581880092621s\nMin Latency: 0.7084167003631592s\nMax Latency: 11.790298700332642s\n\nStarting PAYGO test...\nMedian Latency: 2.391003727912903s\nAverage Latency: 6.372000885009766s\nMin Latency: 0.4583735466003418s\nMax Latency: 89.96037220954895s\n```", "```py\ntest_inputs = [\"Hello\", \"How are you?\", \n     ↪\"What's the capital of Hawaii?\", \"Tell me a dad joke\", \n     ↪\"Tell me a story\", \"What's your favorite movie?\", \n     ↪\"What's the meaning of life?\", \"What's the capital of India?\",\n     ↪\"What's the square root of 1976?\", \"What's the largest mammal?\",\n     ↪\"Write a story about a Panda F1 driver in less \n        ↪than {MAX_TOKENS} words\"]\n\ndef main():\n    for client, model, test_name in [(ptu_client, \n      ↪PTU_MODEL, \"PTU\"), (paygo_client, PAYGO_MODEL, \"PAYGO\")]:\n        print(f\"Starting {test_name} test...\")\n        with ThreadPoolExecutor(max_workers=20) as executor:\n            latencies = []\n            futures = [executor.submit(call_completion_api, \n                       ↪client, model, input) for input in \n                       ↪random.choices(test_inputs, k=NUM_INTERATION)]\n            for future in tqdm(as_completed(futures), \n            ↪total=NUM_INTERATION):\n                latency, token_count = future.result()\n                if latency is not None and token_count is not None:\n                    logging.info(f\"Latency: {latency}s, \n                    ↪Token Count: {token_count}\")\n                    latencies.append(latency)\n\n        # Calculate and print metrics\n        average_latency = sum(latencies) / len(latencies)\n        ↪if latencies else None\n        min_latency = min(latencies) if latencies else None\n        max_latency = max(latencies) if latencies else None\n        median_latency = statistics.median(latencies) \n        ↪if latencies else None\n\n        print(f\"Median Latency: {median_latency}s\")\n        print(f\"Average Latency: {average_latency}s\")\n        print(f\"Min Latency: {min_latency}s\")\n        print(f\"Max Latency: {max_latency}s\")\n```", "```py\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_backoff(conversation):\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=conversation,\n        temperature=TEMPERATURE,\n        max_tokens=MAX_TOKENS,\n    )\n    return response\n```", "```py\nservices:\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:latest\n    command: mlflow server --backend-store-uri /mlflow/mlruns \n    ↪--default-artifact-root /mlflow/artifacts --host 0.0.0.0\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./mlflow/mlruns:/mlflow/mlruns\n      - ./mlflow/artifacts:/mlflow/artifacts\n\n  prometheus:\n    image: prom/prometheus:latest\n    command: --config.file=/etc/prometheus/prometheus.yml\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - ./prometheus/data:/prometheus/data\n    depends_on:\n      - mlflow\n```", "```py\nimport prometheus_client as prom\nimport mlflow\n...\n\n# Set OpenAI API key\nAPI_KEY = os.getenv(\"OPENAI_API_BOOK_KEY\")\nMODEL = \"gpt-3.5-turbo\"\nMLFLOW_URI = \"http://localhost:5000\"\n...\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=API_KEY)\n\n# Set MLflow tracking URI\nmlflow.set_tracking_uri(MLFLOW_URI)\nmlflow.set_experiment(\"GenAI_book\")\n\ndef generate_text(conversation, max_tokens=100)->str:\n    start_time = time.time()\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=conversation,\n    )\n    latency = time.time() - start_time\n    message_response = response.choices[0].message.content\n\n    # Count tokens in the prompt, and the completion\n    prompt_tokens = count_tokens(conversation[-1]['content'])\n    conversation_tokens = count_tokens(str(conversation))\n    completion_tokens = count_tokens(message_response)\n\n    # Log metrics using MLflow\n    with mlflow.start_run():\n        mlflow.log_metrics({\n            \"request_count\": 1,\n            \"request_latency\": latency,\n            \"prompt_tokens\": prompt_tokens,\n            \"completion_tokens\": completion_tokens,\n            \"conversation_tokens\": conversation_tokens\n            })\n        mlflow.log_params({\n            \"model\": MODEL,\n            \"temperature\": TEMPERATURE,\n            \"top_p\": TOP_P,\n            \"frequency_penalty\": FREQUENCY_PENALTY,\n            \"presence_penalty\": PRESENCE_PENALTY\n            })        \n\n    return message_response\n\nif __name__ == \"__main__\":\n    conversation = [{\"role\": \"system\", \"content\": \n                   ↪\"You are a helpful assistant.\"}]\n\n    while True:\n        user_input = input(f\"You: \")\n        conversation.append({\"role\": \"user\", \"content\": user_input})\n        output = generate_text(conversation, 256)\n        print_ai_output(output)\n        conversation.append({\"role\": \"assistant\", \"content\": output})\n```", "```py\nimport os\nfrom traceloop.sdk import Traceloop\n...\n\nLOAD_TEST_ITERATIONS = 50\n\n# Set OpenAI\nAPI_KEY = os.getenv(\"AOAI_PTU_KEY\")\nENDPOINT = os.getenv(\"AOAI_PTU_ENDPOINT\")\n...\n\n# Initialize Traceloop\nTRACELOOP_API_KEY = os.getenv(\"TRACELOOP_API_KEY\")\nTraceloop.init(api_key=TRACELOOP_API_KEY)\n\nclient = AzureOpenAI(\n    azure_endpoint = ENDPOINT,\n    api_key=API_KEY,\n    api_version=\"2024-02-15-preview\"\n)\n\n# Define the conversation as a list of messages\nconversation = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n]\n\n# Define a list of test inputs\ntest_inputs = [\"Hello\", \"How are you?\", \"What's the weather like?\", \n     ↪\"Tell me a joke\", \"Tell me a story\", \"What's your favorite movie?\",\n     ↪\"What's the meaning of life?\", \"What's the capital of France?\", \n     ↪\"What's the square root of 144?\", \"What's the largest mammal?\"]\n\nprint(\"Starting load test...\")\nfor _ in tqdm(range(LOAD_TEST_ITERATIONS)):\n    # Generate a random user input\n    user_input = random.choice(test_inputs)\n\n    # Add user input to the conversation\n    conversation.append({\"role\": \"user\", \"content\": user_input})\n\n    # Make the API call\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=conversation,\n        temperature=TEMPERATURE,\n        max_tokens=MAX_TOKENS,\n    )\n\nprint(\"Load test complete.\")\n```", "```py\nimport os\nfrom openai import AzureOpenAI\nfrom azure.identity import DefaultAzureCredential, \n↪get_bearer_token_provider\n\nAZURE_ENDPOINT = os.getenv(\"AOAI_ENDPOINT\")\nAPI_VERSION = \"2024-02-15-preview\"\n\ntoken_provider = get_bearer_token_provider(\n    DefaultAzureCredential(),\n    \"https://cognitiveservices.azure.com/.default\"\n)\n\nclient = AzureOpenAI(\n    api_version=API_VERSION,\n    azure_endpoint=AZURE_ENDPOINT,\n    azure_ad_token_provider=token_provider,\n)\n```", "```py\n11:33:52 [RedisVL] INFO   Indices:\n11:33:52 [RedisVL] INFO   1\\. Posts\n```", "```py\nfrom openai import AzureOpenAI\nfrom redisvl.extensions.llmcache import SemanticCache\nimport numpy as np\n...\n# Set your OpenAI API key\nAOAI_API_KEY = os.getenv(\"AOAI_KEY\")\n...\n\ndef initialize_cache():\n    # Initialize the semantic cache\n    llmcache = SemanticCache(\n        name=\"GenAIBookCache\",             #1\n        prefix=\"bookcache\",                   #2\n        redis_url=\"redis://localhost:6379\",   #3\n        distance_threshold=0.1            #4\n    )\n    return llmcache\n\n# Define a list of questions\ninput_questions = [\"What is the capital of UK?\", ... \n                   \"What is the capital of Japan?\"]\n\ndef generate_response(conversation, max_tokens=25)->str:\n    response = client.chat.completions.create(\n        ...\n    )\n    return response.choices[0].message.content\n\ndef answer_question(question: str) -> str:\n    conversation = [{\"role\": \"assistant\", \"content\": question}]\n\n    results = llmcache.check(prompt=question)\n    if results:\n        answer = results[0][\"response\"]\n    else:\n        answer = generate_response(conversation)\n        llmcache.store(prompt=question, response=answer)\n    return answer\n\nif __name__ == \"__main__\":\n    llmcache = initialize_cache()\n\n    times_without_cache = []\n    times_with_cache = []\n\n    for question in input_questions:\n        # Without caching\n        start_time = time.time()\n        answer = generate_response([{\"role\": \"assistant\",\n                   ↪\"content\": question}])\n        end_time = time.time()\n        times_without_cache.append(end_time-start_time)\n\n        # With caching\n        start_time = time.time()\n        answer = answer_question(question)\n        end_time = time.time()\n        times_with_cache.append(end_time-start_time)\n\n    avg_time_without_cache = np.mean(times_without_cache)\n    avg_time_with_cache = np.mean(times_with_cache)\n\n    print(f\"Avg time taken without cache: {avg_time_without_cache}\")\n    print(f\"Avg time taken with LLM cache enabled: {avg_time_with_cache}\")\n    print(f\"Percentage of time saved: {round((avg_time_without_cache – \n            ↪avg_time_with_cache) / avg_time_without_cache * 100, 2)}%\")\n```", "```py\n11:16:17 redisvl.index.index INFO   Index already exists, not overwriting.\nCache hit for prompt: What is the capital of UK?, answer: London\n...\nCache miss for prompt: What is the capital of India?, added to \n↪cache with response: The capital of India is New Delhi.\nAvg time taken without cache: 0.7652951717376709\nAvg time taken with LLM cache enabled: 0.23438820838928223\nPercentage of time saved: 69.37%\n```"]