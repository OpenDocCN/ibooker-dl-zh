["```py\nnamespace tflite {\nnamespace ops {\nnamespace micro {\nTfLiteRegistration* Register_DEPTHWISE_CONV_2D();\nTfLiteRegistration* Register_FULLY_CONNECTED();\nTfLiteRegistration* Register_SOFTMAX();\n}  // namespace micro\n}  // namespace ops\n}  // namespace tflite\n```", "```py\n// Set up logging.\ntflite::MicroErrorReporter micro_error_reporter;\ntflite::ErrorReporter* error_reporter = &micro_error_reporter;\n// Map the model into a usable data structure. This doesn't involve any\n// copying or parsing, it's a very lightweight operation.\nconst tflite::Model* model =\n    ::tflite::GetModel(g_tiny_conv_micro_features_model_data);\nif (model->version() != TFLITE_SCHEMA_VERSION) {\n  error_reporter->Report(\n      \"Model provided is schema version %d not equal \"\n      \"to supported version %d.\\n\",\n      model->version(), TFLITE_SCHEMA_VERSION);\n}\n```", "```py\ntflite::MicroMutableOpResolver micro_mutable_op_resolver;\nmicro_mutable_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_DEPTHWISE_CONV_2D,\n    tflite::ops::micro::Register_DEPTHWISE_CONV_2D());\nmicro_mutable_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_FULLY_CONNECTED,\n    tflite::ops::micro::Register_FULLY_CONNECTED());\nmicro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,\n                                      tflite::ops::micro::Register_SOFTMAX());\n```", "```py\n// Create an area of memory to use for input, output, and intermediate arrays.\nconst int tensor_arena_size = 10 * 1024;\nuint8_t tensor_arena[tensor_arena_size];\n// Build an interpreter to run the model with.\ntflite::MicroInterpreter interpreter(model, micro_mutable_op_resolver, tensor_arena,\n                                     tensor_arena_size, error_reporter);\ninterpreter.AllocateTensors();\n```", "```py\n// Get information about the memory area to use for the model's input.\nTfLiteTensor* input = interpreter.input(0);\n// Make sure the input has the properties we expect.\nTF_LITE_MICRO_EXPECT_NE(nullptr, input);\nTF_LITE_MICRO_EXPECT_EQ(4, input->dims->size);\nTF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[0]);\nTF_LITE_MICRO_EXPECT_EQ(49, input->dims->data[1]);\nTF_LITE_MICRO_EXPECT_EQ(40, input->dims->data[2]);\nTF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[3]);\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteUInt8, input->type);\n```", "```py\n// Copy a spectrogram created from a .wav audio file of someone saying \"Yes\"\n// into the memory area used for the input.\nconst uint8_t* yes_features_data = g_yes_micro_f2e59fea_nohash_1_data;\nfor (int i = 0; i < input->bytes; ++i) {\n  input->data.uint8[i] = yes_features_data[i];\n}\n```", "```py\n// Run the model on this input and make sure it succeeds.\nTfLiteStatus invoke_status = interpreter.Invoke();\nif (invoke_status != kTfLiteOk) {\n  error_reporter->Report(\"Invoke failed\\n\");\n}\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\n\n// Get the output from the model, and make sure it's the expected size and\n// type.\nTfLiteTensor* output = interpreter.output(0);\nTF_LITE_MICRO_EXPECT_EQ(2, output->dims->size);\nTF_LITE_MICRO_EXPECT_EQ(1, output->dims->data[0]);\nTF_LITE_MICRO_EXPECT_EQ(4, output->dims->data[1]);\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteUInt8, output->type);\n```", "```py\n// There are four possible classes in the output, each with a score.\nconst int kSilenceIndex = 0;\nconst int kUnknownIndex = 1;\nconst int kYesIndex = 2;\nconst int kNoIndex = 3;\n\n// Make sure that the expected \"Yes\" score is higher than the other classes.\nuint8_t silence_score = output->data.uint8[kSilenceIndex];\nuint8_t unknown_score = output->data.uint8[kUnknownIndex];\nuint8_t yes_score = output->data.uint8[kYesIndex];\nuint8_t no_score = output->data.uint8[kNoIndex];\nTF_LITE_MICRO_EXPECT_GT(yes_score, silence_score);\nTF_LITE_MICRO_EXPECT_GT(yes_score, unknown_score);\nTF_LITE_MICRO_EXPECT_GT(yes_score, no_score);\n```", "```py\n// Now test with a different input, from a recording of \"No\".\nconst uint8_t* no_features_data = g_no_micro_f9643d42_nohash_4_data;\nfor (int i = 0; i < input->bytes; ++i) {\n  input->data.uint8[i] = no_features_data[i];\n}\n// Run the model on this \"No\" input.\ninvoke_status = interpreter.Invoke();\nif (invoke_status != kTfLiteOk) {\n  error_reporter->Report(\"Invoke failed\\n\");\n}\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\n```", "```py\n// Make sure that the expected \"No\" score is higher than the other classes.\nsilence_score = output->data.uint8[kSilenceIndex];\nunknown_score = output->data.uint8[kUnknownIndex];\nyes_score = output->data.uint8[kYesIndex];\nno_score = output->data.uint8[kNoIndex];\nTF_LITE_MICRO_EXPECT_GT(no_score, silence_score);\nTF_LITE_MICRO_EXPECT_GT(no_score, unknown_score);\nTF_LITE_MICRO_EXPECT_GT(no_score, yes_score);\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\n  test_micro_speech_test\n```", "```py\nTfLiteStatus GetAudioSamples(tflite::ErrorReporter* error_reporter,\n                             int start_ms, int duration_ms,\n                             int* audio_samples_size, int16_t** audio_samples);\n```", "```py\nTF_LITE_MICRO_TEST(TestAudioProvider) {\n  tflite::MicroErrorReporter micro_error_reporter;\n  tflite::ErrorReporter* error_reporter = &micro_error_reporter;\n\n  int audio_samples_size = 0;\n  int16_t* audio_samples = nullptr;\n  TfLiteStatus get_status =\n      GetAudioSamples(error_reporter, 0, kFeatureSliceDurationMs,\n                      &audio_samples_size, &audio_samples);\n  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, get_status);\n  TF_LITE_MICRO_EXPECT_LE(audio_samples_size, kMaxAudioSampleSize);\n  TF_LITE_MICRO_EXPECT_NE(audio_samples, nullptr);\n\n  // Make sure we can read all of the returned memory locations.\n  int total = 0;\n  for (int i = 0; i < audio_samples_size; ++i) {\n    total += audio_samples[i];\n  }\n}\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\n  test_audio_provider_test\n```", "```py\nclass FeatureProvider {\n public:\n  // Create the provider, and bind it to an area of memory. This memory should\n  // remain accessible for the lifetime of the provider object, since subsequent\n  // calls will fill it with feature data. The provider does no memory\n  // management of this data.\n  FeatureProvider(int feature_size, uint8_t* feature_data);\n  ~FeatureProvider();\n\n  // Fills the feature data with information from audio inputs, and returns how\n  // many feature slices were updated.\n  TfLiteStatus PopulateFeatureData(tflite::ErrorReporter* error_reporter,\n                                   int32_t last_time_in_ms, int32_t time_in_ms,\n                                   int* how_many_new_slices);\n\n private:\n  int feature_size_;\n  uint8_t* feature_data_;\n  // Make sure we don't try to use cached information if this is the first call\n  // into the provider.\n  bool is_first_run_;\n};\n```", "```py\nTF_LITE_MICRO_TEST(TestFeatureProviderMockYes) {\n  tflite::MicroErrorReporter micro_error_reporter;\n  tflite::ErrorReporter* error_reporter = &micro_error_reporter;\n\n  uint8_t feature_data[kFeatureElementCount];\n  FeatureProvider feature_provider(kFeatureElementCount, feature_data);\n\n  int how_many_new_slices = 0;\n  TfLiteStatus populate_status = feature_provider.PopulateFeatureData(\n      error_reporter, /* last_time_in_ms= */ 0, /* time_in_ms= */ 970,\n      &how_many_new_slices);\n  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, populate_status);\n  TF_LITE_MICRO_EXPECT_EQ(kFeatureSliceCount, how_many_new_slices);\n\n  for (int i = 0; i < kFeatureElementCount; ++i) {\n    TF_LITE_MICRO_EXPECT_EQ(g_yes_micro_f2e59fea_nohash_1_data[i],\n                            feature_data[i]);\n  }\n}\n```", "```py\nFeatureProvider feature_provider(kFeatureElementCount, feature_data);\n```", "```py\nTfLiteStatus populate_status = feature_provider.PopulateFeatureData(\n      error_reporter, /* last_time_in_ms= */ 0, /* time_in_ms= */ 970,\n      &how_many_new_slices);\n```", "```py\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, populate_status);\nTF_LITE_MICRO_EXPECT_EQ(kFeatureSliceCount, how_many_new_slices);\nfor (int i = 0; i < kFeatureElementCount; ++i) {\n  TF_LITE_MICRO_EXPECT_EQ(g_yes_micro_f2e59fea_nohash_1_data[i],\n                          feature_data[i]);\n}\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\n  test_feature_provider_mock_test\n```", "```py\n// Quantize the time into steps as long as each window stride, so we can\n// figure out which audio data we need to fetch.\nconst int last_step = (last_time_in_ms / kFeatureSliceStrideMs);\nconst int current_step = (time_in_ms / kFeatureSliceStrideMs);\n\nint slices_needed = current_step - last_step;\n```", "```py\nif (is_first_run_) {\n  TfLiteStatus init_status = InitializeMicroFeatures(error_reporter);\n  if (init_status != kTfLiteOk) {\n    return init_status;\n  }\n  is_first_run_ = false;\n  slices_needed = kFeatureSliceCount;\n}\nif (slices_needed > kFeatureSliceCount) {\n  slices_needed = kFeatureSliceCount;\n}\n*how_many_new_slices = slices_needed;\n```", "```py\nconst int slices_to_keep = kFeatureSliceCount - slices_needed;\nconst int slices_to_drop = kFeatureSliceCount - slices_to_keep;\n// If we can avoid recalculating some slices, just move the existing data\n// up in the spectrogram, to perform something like this:\n// last time = 80ms          current time = 120ms\n// +-----------+             +-----------+\n// | data@20ms |         --> | data@60ms |\n// +-----------+       --    +-----------+\n// | data@40ms |     --  --> | data@80ms |\n// +-----------+   --  --    +-----------+\n// | data@60ms | --  --      |  <empty>  |\n// +-----------+   --        +-----------+\n// | data@80ms | --          |  <empty>  |\n// +-----------+             +-----------+\nif (slices_to_keep > 0) {\n  for (int dest_slice = 0; dest_slice < slices_to_keep; ++dest_slice) {\n    uint8_t* dest_slice_data =\n        feature_data_ + (dest_slice * kFeatureSliceSize);\n    const int src_slice = dest_slice + slices_to_drop;\n    const uint8_t* src_slice_data =\n        feature_data_ + (src_slice * kFeatureSliceSize);\n    for (int i = 0; i < kFeatureSliceSize; ++i) {\n      dest_slice_data[i] = src_slice_data[i];\n    }\n  }\n}\n```", "```py\nfor (int new_slice = slices_to_keep; new_slice < kFeatureSliceCount;\n     ++new_slice) {\n  const int new_step = (current_step - kFeatureSliceCount + 1) + new_slice;\n  const int32_t slice_start_ms = (new_step * kFeatureSliceStrideMs);\n  int16_t* audio_samples = nullptr;\n  int audio_samples_size = 0;\n  GetAudioSamples(error_reporter, slice_start_ms, kFeatureSliceDurationMs,\n                  &audio_samples_size, &audio_samples);\n  if (audio_samples_size < kMaxAudioSampleSize) {\n    error_reporter->Report(\"Audio data size %d too small, want %d\",\n                           audio_samples_size, kMaxAudioSampleSize);\n    return kTfLiteError;\n  }\n```", "```py\n  uint8_t* new_slice_data = feature_data_ + (new_slice * kFeatureSliceSize);\n  size_t num_samples_read;\n  TfLiteStatus generate_status = GenerateMicroFeatures(\n      error_reporter, audio_samples, audio_samples_size, kFeatureSliceSize,\n      new_slice_data, &num_samples_read);\n  if (generate_status != kTfLiteOk) {\n    return generate_status;\n  }\n}\n```", "```py\nclass RecognizeCommands {\n public:\n  explicit RecognizeCommands(tflite::ErrorReporter* error_reporter,\n                             int32_t average_window_duration_ms = 1000,\n                             uint8_t detection_threshold = 200,\n                             int32_t suppression_ms = 1500,\n                             int32_t minimum_count = 3);\n\n  // Call this with the results of running a model on sample data.\n  TfLiteStatus ProcessLatestResults(const TfLiteTensor* latest_results,\n                                    const int32_t current_time_ms,\n                                    const char** found_command, uint8_t* score,\n                                    bool* is_new_command);\n```", "```py\nTfLiteStatus RecognizeCommands::ProcessLatestResults(\n    const TfLiteTensor* latest_results, const int32_t current_time_ms,\n    const char** found_command, uint8_t* score, bool* is_new_command) {\n  if ((latest_results->dims->size != 2) ||\n      (latest_results->dims->data[0] != 1) ||\n      (latest_results->dims->data[1] != kCategoryCount)) {\n    error_reporter_->Report(\n        \"The results for recognition should contain %d elements, but there are \"\n        \"%d in an %d-dimensional shape\",\n        kCategoryCount, latest_results->dims->data[1],\n        latest_results->dims->size);\n    return kTfLiteError;\n  }\n\n  if (latest_results->type != kTfLiteUInt8) {\n    error_reporter_->Report(\n        \"The results for recognition should be uint8 elements, but are %d\",\n        latest_results->type);\n    return kTfLiteError;\n  }\n```", "```py\nif ((!previous_results_.empty()) &&\n    (current_time_ms < previous_results_.front().time_)) {\n  error_reporter_->Report(\n      \"Results must be fed in increasing time order, but received a \"\n      \"timestamp of %d that was earlier than the previous one of %d\",\n      current_time_ms, previous_results_.front().time_);\n  return kTfLiteError;\n}\n```", "```py\n// Add the latest results to the head of the queue.\nprevious_results_.push_back({current_time_ms, latest_results->data.uint8});\n// Prune any earlier results that are too old for the averaging window.\nconst int64_t time_limit = current_time_ms - average_window_duration_ms_;\nwhile ((!previous_results_.empty()) &&\n       previous_results_.front().time_ < time_limit) {\n  previous_results_.pop_front();\n```", "```py\n// If there are too few results, assume the result will be unreliable and\n// bail.\nconst int64_t how_many_results = previous_results_.size();\nconst int64_t earliest_time = previous_results_.front().time_;\nconst int64_t samples_duration = current_time_ms - earliest_time;\nif ((how_many_results < minimum_count_) ||\n    (samples_duration < (average_window_duration_ms_ / 4))) {\n  *found_command = previous_top_label_;\n  *score = 0;\n  *is_new_command = false;\n  return kTfLiteOk;\n}\n```", "```py\n// Calculate the average score across all the results in the window.\nint32_t average_scores[kCategoryCount];\nfor (int offset = 0; offset < previous_results_.size(); ++offset) {\n  PreviousResultsQueue::Result previous_result =\n      previous_results_.from_front(offset);\n  const uint8_t* scores = previous_result.scores_;\n  for (int i = 0; i < kCategoryCount; ++i) {\n    if (offset == 0) {\n      average_scores[i] = scores[i];\n    } else {\n      average_scores[i] += scores[i];\n    }\n  }\n}\nfor (int i = 0; i < kCategoryCount; ++i) {\n  average_scores[i] /= how_many_results;\n}\n```", "```py\n// Find the current highest scoring category.\nint current_top_index = 0;\nint32_t current_top_score = 0;\nfor (int i = 0; i < kCategoryCount; ++i) {\n  if (average_scores[i] > current_top_score) {\n    current_top_score = average_scores[i];\n    current_top_index = i;\n  }\n}\nconst char* current_top_label = kCategoryLabels[current_top_index];\n```", "```py\n// If we've recently had another label trigger, assume one that occurs too\n// soon afterwards is a bad result.\nint64_t time_since_last_top;\nif ((previous_top_label_ == kCategoryLabels[0]) ||\n    (previous_top_label_time_ == std::numeric_limits<int32_t>::min())) {\n  time_since_last_top = std::numeric_limits<int32_t>::max();\n} else {\n  time_since_last_top = current_time_ms - previous_top_label_time_;\n}\nif ((current_top_score > detection_threshold_) &&\n    ((current_top_label != previous_top_label_) ||\n     (time_since_last_top > suppression_ms_))) {\n  previous_top_label_ = current_top_label;\n  previous_top_label_time_ = current_time_ms;\n  *is_new_command = true;\n} else {\n  *is_new_command = false;\n}\n*found_command = current_top_label;\n*score = current_top_score;\n```", "```py\nTF_LITE_MICRO_TEST(RecognizeCommandsTestBasic) {\n  tflite::MicroErrorReporter micro_error_reporter;\n  tflite::ErrorReporter* error_reporter = &micro_error_reporter;\n\n  RecognizeCommands recognize_commands(error_reporter);\n```", "```py\nTfLiteTensor results = tflite::testing::CreateQuantizedTensor(\n    {255, 0, 0, 0}, tflite::testing::IntArrayFromInitializer({2, 1, 4}),\n    \"input_tensor\", 0.0f, 128.0f);\n```", "```py\nconst char* found_command;\nuint8_t score;\nbool is_new_command;\n```", "```py\nTF_LITE_MICRO_EXPECT_EQ(\n    kTfLiteOk, recognize_commands.ProcessLatestResults(\n                   &results, 0, &found_command, &score, &is_new_command));\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\n  test_recognize_commands_test\n```", "```py\nvoid RespondToCommand(tflite::ErrorReporter* error_reporter,\n                      int32_t current_time, const char* found_command,\n                      uint8_t score, bool is_new_command) {\n  if (is_new_command) {\n    error_reporter->Report(\"Heard %s (%d) @%dms\", found_command, score,\n                           current_time);\n  }\n}\n```", "```py\nTF_LITE_MICRO_TEST(TestCallability) {\n  tflite::MicroErrorReporter micro_error_reporter;\n  tflite::ErrorReporter* error_reporter = &micro_error_reporter;\n\n  // This will have external side-effects (like printing to the debug console\n  // or lighting an LED) that are hard to observe, so the most we can do is\n  // make sure the call doesn't crash.\n  RespondToCommand(error_reporter, 0, \"foo\", 0, true);\n}\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\n  test_command_responder_test\n```", "```py\nnamespace tflite {\nnamespace ops {\nnamespace micro {\nTfLiteRegistration* Register_DEPTHWISE_CONV_2D();\nTfLiteRegistration* Register_FULLY_CONNECTED();\nTfLiteRegistration* Register_SOFTMAX();\n}  // namespace micro\n}  // namespace ops\n}  // namespace tflite\n```", "```py\nnamespace {\ntflite::ErrorReporter* error_reporter = nullptr;\nconst tflite::Model* model = nullptr;\ntflite::MicroInterpreter* interpreter = nullptr;\nTfLiteTensor* model_input = nullptr;\nFeatureProvider* feature_provider = nullptr;\nRecognizeCommands* recognizer = nullptr;\nint32_t previous_time = 0;\n\n// Create an area of memory to use for input, output, and intermediate arrays.\n// The size of this will depend on the model you're using, and may need to be\n// determined by experimentation.\nconstexpr int kTensorArenaSize = 10 * 1024;\nuint8_t tensor_arena[kTensorArenaSize];\n}  // namespace\n```", "```py\nvoid setup() {\n  // Set up logging.\n  static tflite::MicroErrorReporter micro_error_reporter;\n  error_reporter = &micro_error_reporter;\n\n  // Map the model into a usable data structure. This doesn't involve any\n  // copying or parsing, it's a very lightweight operation.\n  model = tflite::GetModel(g_tiny_conv_micro_features_model_data);\n  if (model->version() != TFLITE_SCHEMA_VERSION) {\n    error_reporter->Report(\n        \"Model provided is schema version %d not equal \"\n        \"to supported version %d.\",\n        model->version(), TFLITE_SCHEMA_VERSION);\n    return;\n  }\n\n  // Pull in only the operation implementations we need.\n  static tflite::MicroMutableOpResolver micro_mutable_op_resolver;\n  micro_mutable_op_resolver.AddBuiltin(\n      tflite::BuiltinOperator_DEPTHWISE_CONV_2D,\n      tflite::ops::micro::Register_DEPTHWISE_CONV_2D());\n  micro_mutable_op_resolver.AddBuiltin(\n      tflite::BuiltinOperator_FULLY_CONNECTED,\n      tflite::ops::micro::Register_FULLY_CONNECTED());\n  micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,\n                                       tflite::ops::micro::Register_SOFTMAX());\n\n  // Build an interpreter to run the model with.\n  static tflite::MicroInterpreter static_interpreter(\n      model, micro_mutable_op_resolver, tensor_arena, kTensorArenaSize,\n      error_reporter);\n  interpreter = &static_interpreter;\n\n  // Allocate memory from the tensor_arena for the model's tensors.\n  TfLiteStatus allocate_status = interpreter->AllocateTensors();\n  if (allocate_status != kTfLiteOk) {\n    error_reporter->Report(\"AllocateTensors() failed\");\n    return;\n  }\n```", "```py\n  // Get information about the memory area to use for the model's input.\n  model_input = interpreter->input(0);\n  if ((model_input->dims->size != 4) || (model_input->dims->data[0] != 1) ||\n      (model_input->dims->data[1] != kFeatureSliceCount) ||\n      (model_input->dims->data[2] != kFeatureSliceSize) ||\n      (model_input->type != kTfLiteUInt8)) {\n    error_reporter->Report(\"Bad input tensor parameters in model\");\n    return;\n  }\n```", "```py\n  // Prepare to access the audio spectrograms from a microphone or other source\n  // that will provide the inputs to the neural network.\n  static FeatureProvider static_feature_provider(kFeatureElementCount,\n                                                 model_input->data.uint8);\n  feature_provider = &static_feature_provider;\n```", "```py\n  static RecognizeCommands static_recognizer(error_reporter);\n  recognizer = &static_recognizer;\n\n  previous_time = 0;\n}\n```", "```py\nvoid loop() {\n  // Fetch the spectrogram for the current time.\n  const int32_t current_time = LatestAudioTimestamp();\n  int how_many_new_slices = 0;\n  TfLiteStatus feature_status = feature_provider->PopulateFeatureData(\n      error_reporter, previous_time, current_time, &how_many_new_slices);\n  if (feature_status != kTfLiteOk) {\n    error_reporter->Report(\"Feature generation failed\");\n    return;\n  }\n  previous_time = current_time;\n  // If no new audio samples have been received since last time, don't bother\n  // running the network model.\n  if (how_many_new_slices == 0) {\n    return;\n  }\n```", "```py\n  // Run the model on the spectrogram input and make sure it succeeds.\n  TfLiteStatus invoke_status = interpreter->Invoke();\n  if (invoke_status != kTfLiteOk) {\n    error_reporter->Report(\"Invoke failed\");\n    return;\n  }\n```", "```py\n  // Obtain a pointer to the output tensor\n  TfLiteTensor* output = interpreter->output(0);\n  // Determine whether a command was recognized based on the output of inference\n  const char* found_command = nullptr;\n  uint8_t score = 0;\n  bool is_new_command = false;\n  TfLiteStatus process_status = recognizer->ProcessLatestResults(\n      output, current_time, &found_command, &score, &is_new_command);\n  if (process_status != kTfLiteOk) {\n    error_reporter->Report(\"RecognizeCommands::ProcessLatestResults() failed\");\n    return;\n  }\n```", "```py\n  // Do something based on the recognized command. The default implementation\n  // just prints to the error console, but you should replace this with your\n  // own function for a real application.\n  RespondToCommand(error_reporter, current_time, found_command, score,\n                   is_new_command);\n}\n```", "```py\nint main(int argc, char* argv[]) {\n  setup();\n  while (true) {\n    loop();\n  }\n}\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile micro_speech\n```", "```py\ntensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/micro_speech\n```", "```py\nHeard yes (201) @4056ms\nHeard no (205) @6448ms\nHeard unknown (201) @13696ms\nHeard yes (205) @15000ms\nHeard yes (205) @16856ms\nHeard unknown (204) @18704ms\nHeard no (206) @21000ms\n```", "```py\n#include \"tensorflow/lite/micro/examples/micro_speech/command_responder.h\"\n#include \"Arduino.h\"\n```", "```py\n// Toggles the LED every inference, and keeps it on for 3 seconds if a \"yes\"\n// was heard\nvoid RespondToCommand(tflite::ErrorReporter* error_reporter,\n                      int32_t current_time, const char* found_command,\n                      uint8_t score, bool is_new_command) {\n```", "```py\nstatic bool is_initialized = false;\nif (!is_initialized) {\n  pinMode(LED_BUILTIN, OUTPUT);\n  is_initialized = true;\n}\n```", "```py\nstatic int32_t last_yes_time = 0;\nstatic int count = 0;\n```", "```py\nif (is_new_command) {\n  error_reporter->Report(\"Heard %s (%d) @%dms\", found_command, score,\n                         current_time);\n  // If we heard a \"yes\", switch on an LED and store the time.\n  if (found_command[0] == 'y') {\n    last_yes_time = current_time;\n    digitalWrite(LED_BUILTIN, HIGH);\n  }\n}\n```", "```py\n// If last_yes_time is non-zero but was >3 seconds ago, zero it\n// and switch off the LED.\nif (last_yes_time != 0) {\n  if (last_yes_time < (current_time - 3000)) {\n    last_yes_time = 0;\n    digitalWrite(LED_BUILTIN, LOW);\n  }\n  // If it is non-zero but <3 seconds ago, do nothing.\n  return;\n}\n```", "```py\n// Otherwise, toggle the LED every time an inference is performed.\n++count;\nif (count & 1) {\n  digitalWrite(LED_BUILTIN, HIGH);\n} else {\n  digitalWrite(LED_BUILTIN, LOW);\n}\n```", "```py\n#include \"tensorflow/lite/micro/examples/micro_speech/command_responder.h\"\n#include \"am_bsp.h\"\n```", "```py\n// This implementation will light up the LEDs on the board in response to\n// different commands.\nvoid RespondToCommand(tflite::ErrorReporter* error_reporter,\n                      int32_t current_time, const char* found_command,\n                      uint8_t score, bool is_new_command) {\n  static bool is_initialized = false;\n  if (!is_initialized) {\n    am_hal_gpio_pinconfig(AM_BSP_GPIO_LED_RED, g_AM_HAL_GPIO_OUTPUT_12);\n    am_hal_gpio_pinconfig(AM_BSP_GPIO_LED_BLUE, g_AM_HAL_GPIO_OUTPUT_12);\n    am_hal_gpio_pinconfig(AM_BSP_GPIO_LED_GREEN, g_AM_HAL_GPIO_OUTPUT_12);\n    am_hal_gpio_pinconfig(AM_BSP_GPIO_LED_YELLOW, g_AM_HAL_GPIO_OUTPUT_12);\n    is_initialized = true;\n  }\n```", "```py\nstatic int count = 0;\n// Toggle the blue LED every time an inference is performed.\n++count;\nif (count & 1) {\n  am_hal_gpio_output_set(AM_BSP_GPIO_LED_BLUE);\n} else {\n  am_hal_gpio_output_clear(AM_BSP_GPIO_LED_BLUE);\n}\n```", "```py\nam_hal_gpio_output_clear(AM_BSP_GPIO_LED_RED);\nam_hal_gpio_output_clear(AM_BSP_GPIO_LED_YELLOW);\nam_hal_gpio_output_clear(AM_BSP_GPIO_LED_GREEN);\n```", "```py\nif (is_new_command) {\n  error_reporter->Report(\"Heard %s (%d) @%dms\", found_command, score,\n                         current_time);\n  if (found_command[0] == 'y') {\n    am_hal_gpio_output_set(AM_BSP_GPIO_LED_YELLOW);\n  }\n  if (found_command[0] == 'n') {\n    am_hal_gpio_output_set(AM_BSP_GPIO_LED_RED);\n  }\n  if (found_command[0] == 'u') {\n    am_hal_gpio_output_set(AM_BSP_GPIO_LED_GREEN);\n  }\n}\n```", "```py\ngit clone https://github.com/tensorflow/tensorflow.git\ncd tensorflow\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\n  TARGET=sparkfun_edge TAGS=cmsis-nn micro_speech_bin\n```", "```py\ntensorflow/lite/micro/tools/make/gen/ \\\n  sparkfun_edge_cortex-m4/bin/micro_speech.bin\n```", "```py\ntest -f tensorflow/lite/micro/tools/make/gen/ \\\n  sparkfun_edge_cortex-m4/bin/micro_speech.bin \\\n  &&  echo \"Binary was successfully created\" || echo \"Binary is missing\"\n```", "```py\ncp tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/ \\\n  tools/apollo3_scripts/keys_info0.py \\\n  tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/ \\\n  tools/apollo3_scripts/keys_info.py\n```", "```py\npython3 tensorflow/lite/micro/tools/make/downloads/ \\\n  AmbiqSuite-Rel2.0.0/tools/apollo3_scripts/create_cust_image_blob.py \\\n  --bin tensorflow/lite/micro/tools/make/gen/ \\\n  sparkfun_edge_cortex-m4/bin/micro_speech.bin \\\n  --load-address 0xC000 \\\n  --magic-num 0xCB -o main_nonsecure_ota \\\n  --version 0x0\n```", "```py\npython3 tensorflow/lite/micro/tools/make/downloads/ \\\n  AmbiqSuite-Rel2.0.0/tools/apollo3_scripts/create_cust_wireupdate_blob.py \\\n  --load-address 0x20000 \\\n  --bin main_nonsecure_ota.bin \\\n  -i 6 -o main_nonsecure_wire \\\n  --options 0x1\n```", "```py\n# macOS:\nls /dev/cu*\n\n# Linux:\nls /dev/tty*\n```", "```py\n/dev/cu.Bluetooth-Incoming-Port\n/dev/cu.MALS\n/dev/cu.SOC\n```", "```py\n# macOS:\nls /dev/cu*\n\n# Linux:\nls /dev/tty*\n```", "```py\n/dev/cu.Bluetooth-Incoming-Port\n/dev/cu.MALS\n/dev/cu.SOC\n/dev/cu.wchusbserial-1450\n```", "```py\nexport DEVICENAME=<*your device name here*>\n\n```", "```py\nexport BAUD_RATE=921600\n```", "```py\npython3 tensorflow/lite/micro/tools/make/downloads/ \\\n  AmbiqSuite-Rel2.0.0/tools/apollo3_scripts/uart_wired_update.py \\\n  -b ${BAUD_RATE} ${DEVICENAME} \\\n  -r 1 -f main_nonsecure_wire.bin \\\n  -i 6\n```", "```py\nConnecting with Corvette over serial port /dev/cu.usbserial-1440...\nSending Hello.\nReceived response for Hello\nReceived Status\nlength =  0x58\nversion =  0x3\nMax Storage =  0x4ffa0\nStatus =  0x2\nState =  0x7\nAMInfo =\n0x1\n0xff2da3ff\n0x55fff\n0x1\n0x49f40003\n0xffffffff\n[...lots more 0xffffffff...]\nSending OTA Descriptor =  0xfe000\nSending Update Command.\nnumber of updates needed =  1\nSending block of size  0x158b0  from  0x0  to  0x158b0\nSending Data Packet of length  8180\nSending Data Packet of length  8180\n[...lots more Sending Data Packet of length  8180...]\n```", "```py\n[...lots more Sending Data Packet of length  8180...]\nSending Data Packet of length  8180\nSending Data Packet of length  6440\nSending Reset Command.\nDone.\n```", "```py\nscreen ${DEVICENAME} 115200\n```", "```py\nApollo3 Burst Mode is Available\n\n                               Apollo3 operating in Burst Mode (96MHz)\n```", "```py\nHeard yes (202) @65536ms\n```", "```py\n#include \"tensorflow/lite/micro/examples/micro_speech/command_responder.h\"\n#include \"LCD_DISCO_F746NG.h\"\n```", "```py\nLCD_DISCO_F746NG lcd;\n```", "```py\n// When a command is detected, write it to the display and log it to the\n// serial port.\nvoid RespondToCommand(tflite::ErrorReporter *error_reporter,\n                      int32_t current_time, const char *found_command,\n                      uint8_t score, bool is_new_command) {\n  if (is_new_command) {\n    error_reporter->Report(\"Heard %s (%d) @%dms\", found_command, score,\n                           current_time);\n```", "```py\nif (*found_command == 'y') {\n  lcd.Clear(0xFF0F9D58);\n  lcd.DisplayStringAt(0, LINE(5), (uint8_t *)\"Heard yes!\", CENTER_MODE);\n```", "```py\n} else if (*found_command == 'n') {\n  lcd.Clear(0xFFDB4437);\n  lcd.DisplayStringAt(0, LINE(5), (uint8_t *)\"Heard no :(\", CENTER_MODE);\n} else if (*found_command == 'u') {\n  lcd.Clear(0xFFF4B400);\n  lcd.DisplayStringAt(0, LINE(5), (uint8_t *)\"Heard unknown\", CENTER_MODE);\n} else {\n  lcd.Clear(0xFF4285F4);\n  lcd.DisplayStringAt(0, LINE(5), (uint8_t *)\"Heard silence\", CENTER_MODE);\n}\n```", "```py\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\n  TARGET=mbed TAGS=\"cmsis-nn disco_f746ng\" generate_micro_speech_mbed_project\n```", "```py\ntensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/ \\\n  micro_speech/mbed\n```", "```py\ncd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed\n```", "```py\nmbed config root .\n```", "```py\nmbed deploy\n```", "```py\npython -c 'import fileinput, glob;\nfor filename in glob.glob(\"mbed-os/tools/profiles/*.json\"):\n  for line in fileinput.input(filename, inplace=True):\n    print(line.replace(\"\\\"-std=gnu++98\\\"\",\"\\\"-std=c++11\\\", \\\"-fpermissive\\\"\"))'\n```", "```py\nmbed compile -m DISCO_F746NG -t GCC_ARM\n```", "```py\n./BUILD/DISCO_F746NG/GCC_ARM/mbed.bin\n```", "```py\ncp ./BUILD/DISCO_F746NG/GCC_ARM/mbed.bin /Volumes/DIS_F746NG/\n```", "```py\nls /dev/tty*\n```", "```py\n/dev/tty.usbmodem1454203\n```", "```py\nscreen /dev/<*tty.devicename 9600*>\n\n```", "```py\nHeard yes (202) @65536ms\n```"]