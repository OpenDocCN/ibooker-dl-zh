- en: 6 Constructing knowledge graphs with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Structured data extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different approaches to extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will explore the process of constructing knowledge graphs
    using LLMs from unstructured sources like text documents. The focus will be on
    how LLMs can extract and structure data from raw text, transforming it into usable
    formats for building knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: In previous chapters, you learned about basic techniques for document chunking,
    embedding, and retrieval (chapter 2), as well as more advanced methods for improving
    retrieval accuracy (chapter 3). However, as you learned in chapter 4, relying
    solely on text embeddings can lead to challenges in scenarios where data needs
    to be structured to answer questions that require filtering, counting, or aggregation
    operations. To solve the limitations of only using text embeddings, you will learn
    how to transform unstructured data into structured formats suitable for knowledge
    graph construction, using LLMs for automated data extraction. By the end of the
    chapter, you will be able to extract structured information from raw text, design
    a knowledge graph model for the extracted data, and import this data into a graph
    database.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll begin by exploring a common challenge in legal document retrieval---managing
    multiple contracts and their terms---and learn how structured data extraction
    provides a solution. Throughout the chapter, you’ll follow examples that illustrate
    the process and guide you step by step through the workflow of constructing a
    knowledge graph from unstructured text.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Extracting structured data from text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much of the information found online, and even within companies, exists in unstructured
    formats like various documents. However, there are situations where the simple
    retrieval technique using only text embeddings falls short. Legal documents are
    one such example.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you’re asking about the payment terms in a contract with ACME
    Inc., it’s crucial to ensure that the terms are actually from that specific contract
    and not from others. When you simply chunk and retrieve across multiple legal
    documents, the top `k` chunks you get at retrieval could come from different,
    unrelated documents, causing confusion, as shown in figure 6.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 Basic vector retrieval strategy might return chunks from various
    contracts.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 6.1 illustrates how contract documents are broken down into text chunks
    and indexed using text embeddings. When an end user asks a specific question,
    such as about the payment terms of a particular contract, the system retrieves
    the most relevant chunks. However, if multiple contracts contain different payment
    terms, the retrieval process may unintentionally pull information from various
    documents, mixing relevant chunks from the target contract with irrelevant ones
    from others. This happens because the system focuses on retrieving top-ranked
    text chunks based on similarity, without always distinguishing whether the chunks
    come from the correct contract. As a result, chunks that share keywords like “payment”
    or “terms” but belong to different contracts may be included, leading to a fragmented
    and inconsistent view of the terms. This confusion can then be responsible when
    the LLM tries to synthesize these mixed chunks into a coherent answer, ultimately
    increasing the risk of inaccurate or misleading information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, consider the following question: How many active contracts do
    we currently have with ACME Inc.? To answer this, you would first need to filter
    all contracts based on their active status and then count the relevant ones. These
    types of queries resemble traditional business intelligence questions, where the
    text-embedding approach falls short.'
  prefs: []
  type: TYPE_NORMAL
- en: Text embeddings are primarily designed to retrieve semantically similar content,
    not to handle operations like filtering, sorting, or aggregating data. To handle
    such operations, structured data is required, as text embeddings alone are not
    well-suited for these operations.
  prefs: []
  type: TYPE_NORMAL
- en: For some domains, structuring data is vital when implementing RAG applications.
    Luckily, LLMs excel at extracting structured data from text due to their deep
    understanding of natural language, allowing them to identify relevant information
    accurately. They can be finetuned or guided through specific prompts to locate
    and extract required data points, converting unstructured information into a structured
    format like tables or key–value pairs. Using LLMs for structured data extraction
    is particularly useful when dealing with large volumes of documents where manually
    identifying and organizing such information would be labor intensive and time
    consuming. By automating the extraction process, LLMs enable businesses to transform
    unstructured information into actionable, structured data, which can then be used
    for further analysis or RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you’re working at a company as a software engineer, and you’re part
    of a team tasked with building a chatbot that can answer questions based on the
    company’s legal documents. Since this is a large-scale project, the team is divided
    into two groups: one focused on data preparation and the other on implementing
    the retrieval systems described in chapters 4 and 5\. You’re assigned to the data
    preparation team, where your job is to process legal documents and extract structured
    information. This information will be used to build a knowledge graph, following
    the workflow visualized in figure 6.2.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Building knowledge graphs from text by using LLMs to extract structured
    data information
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The workflow visualized in figure 6.2 begins with contract documents as input,
    which are processed using an LLM to extract structured information. In the legal
    domain, you can extract various details such as involved parties, dates, terms,
    and more. Here, the structured output is represented in a JSON format, and this
    structured information is then stored in Neo4j, which will serve as the foundation
    for the legal chatbot’s data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'These two examples highlight the limitations of simple text embeddings when
    it comes to handling specific, structured queries, such as asking for payment
    terms in a contract or counting active agreements. In both cases, accurate answers
    require structured data rather than relying solely on the semantic similarity
    of unstructured text. In the remainder of this chapter, we’ll dive deeper into
    how LLMs can be effectively used to extract structured data from complex documents
    and how this structured output plays a critical role in constructing reliable
    knowledge graphs for advanced retrieval tasks. To follow along, you’ll need access
    to a running, blank Neo4j instance. This can be a local installation or a cloud-hosted
    instance; just make sure it’s empty. You can follow the implementation directly
    in the accompanying Jupyter notebook available here: [https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch06.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch06.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Structured Outputs model definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Extracting structured data from text is not a new idea; it has been a vital
    task in data processing for many years. Historically, this process was known as
    *information extraction* and required complex systems, often relying on multiple
    machine learning models working together. These systems were typically expensive
    to build and maintain, requiring a team of skilled engineers and domain experts
    to ensure they functioned correctly. Due to these reasons, only large organizations
    with substantial resources could afford to implement such solutions. The high
    cost and technical barriers made it inaccessible for many businesses and individuals.
    However, advancements in LLMs have dramatically simplified the process. Today,
    users can prompt an LLM to extract structured information with a much lower technical
    threshold instead of building and training multiple models. This shift has opened
    up a wide range of use cases for structured data extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting structured data using LLMs has become such a common use case that
    OpenAI introduced a Structured Outputs feature in its API to simplify and standardize
    the process. This feature allows developers to define the expected output format
    ahead of time, ensuring that the model’s response adheres to a specific structure.
    Structured Outputs is not a separate library; it is a built-in capability of the
    OpenAI API, available through function calling or schema definitions. For example,
    in Python, developers often use libraries like Pydantic to define data schemas.
    These schemas can then be passed to the model, guiding it to produce outputs that
    match the specified format, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Defining the desired output using the Pydantic library
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `CalendarEvent` class in listing 6.1 represents a structured way to capture
    details about an event. It includes a name for the event, a date when it will
    occur, and a list of participants. By defining these attributes explicitly, it
    ensures that any event data conforms to this structure, making it easier to extract
    and work with event information in a reliable and consistent manner. The available
    types for attributes are
  prefs: []
  type: TYPE_NORMAL
- en: String
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boolean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: anyOf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s examine the definition of the `date` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 `date` attribute
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code in listing 6.2 provides instructions on how to extract data for the
    `date` attribute. Naming the attribute `date` signals to the model to focus on
    date-related information. By using the `str` type, we specify that the extracted
    information should be represented as a string, as there’s no native datetime type
    available. Additionally, the `description` clarifies the desired `yyyy-MM-dd`
    format. This step is crucial because, although the model knows it’s dealing with
    a string, the description ensures that the date follows the specific format. Without
    this guidance, the `str` type alone might not convey enough detail about the expected
    output structure.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Outputs significantly simplifies the development process by ensuring
    that the LLM responses adhere to a predefined schema. This reduces the need for
    post-processing and validation, allowing developers to focus on using the data
    within their systems. The feature provides type safety, guaranteeing that responses
    are always correctly formatted, and eliminates the need for complex prompts to
    achieve consistent output, making the process more efficient and reliable overall.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in extracting structured output from legal documents is to define
    the contract data model that needs to be extracted. Since you’re a software engineer
    and not a legal expert, it’s important to consult someone with domain knowledge
    to determine which information is most important to extract. Additionally, speaking
    with end users about the specific questions they want answered can provide valuable
    insights.
  prefs: []
  type: TYPE_NORMAL
- en: Following these initial discussions, you propose the contract data model shown
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Defining the desired output using a Pydantic object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Description of the extracted object'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Using enum to define the possible values an LLM can use'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 An attribute can be an object like the Organization in this example.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Since the datetime type isn’t available, you want to define the date format
    to be extracted.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 You can use Optional for attributes that might not appear in all contracts.'
  prefs: []
  type: TYPE_NORMAL
- en: The class name, `Contract`, along with the concise docstring, “Represents the
    key details of the contract,” provide the LLM with a high-level understanding
    that the desired output should capture essential contractual information. This
    guides the model to focus on extracting and organizing key details, such as the
    contract type, involved parties, dates, and financials.
  prefs: []
  type: TYPE_NORMAL
- en: In general, attributes can be categorized as either mandatory or optional. When
    an attribute is optional, you designate it with an `Optional` type, indicating
    to the LLM that the information may or may not be present. It’s vital to mark
    attributes as optional when information could be missing, as otherwise, some LLMs
    may hallucinate values in an attempt to fill the gaps. For instance, `total_amount`
    is optional since some contracts are simply agreements with no monetary exchange.
    Conversely, the `effective_date` attribute is mandatory, as you expect each contract
    to have a starting date.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how each attribute includes a `description` value to provide clear guidance
    to the LLM, ensuring it extracts the desired information accurately. This is a
    good practice, even when some attributes seem obvious. In some cases, you may
    also want to specify the allowed values for a particular attribute. You can achieve
    this by using the `enum` parameter. For example, the `contract_type` attribute
    utilizes the `enum` parameter to inform the LLM of the specific categories to
    apply. The following listing contains the available values for the `contract_type`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Contract type enum values
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Clearly, the list in listing 6.4 is not exhaustive, as there are additional
    options that could be included.
  prefs: []
  type: TYPE_NORMAL
- en: Some attributes may be more complex and can be defined as custom objects. For
    instance, the `parties` attribute is a list of `Organization` objects. A list
    is used because contracts typically involve multiple parties, and a custom object
    allows for extracting more than just a simple string about a specific attribute.
    The code in the following listing defines the `Organization` object.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Custom `Organization` object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 You can provide possible values in the description instead of enum if you
    aren’t providing all possible values but only examples.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Organization` object in listing 6.5 captures the key details of an organization
    involved in the contract, including its name, primary location, and role. The
    `location` attribute is a nested `Location` object, allowing us to structure the
    information into values like city, state, and country. As you can see, we can
    have nested objects, but the typical advice is to avoid too many levels of nested
    objects for better performance. For the `role` attribute, we’ve provided examples
    like “provider” and “client” but opted not to use an enum to avoid restricting
    the values. This flexibility is important, as the exact roles may vary and aren’t
    entirely predictable. By defining the organization this way, the LLM is guided
    to extract more detailed and structured information about the parties involved.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you need to define the `Location` object.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 Custom `Location` object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 LLMs are familiar with ISO standards being used for countries, so you instruct
    the model to standardize values based on a specific standard.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Location` object represents a physical address, capturing details such
    as the street address, city, state or region, and country. All attributes, except
    for the `country`, are optional, allowing flexibility when full location details
    may not be available. For the `country` attribute, we guide the LLM to use the
    two-letter ISO standard, ensuring consistency and making it easier to work with
    and process across different systems. This structure enables the LLM to extract
    standardized, usable information while allowing for incomplete or partial data
    when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve now defined the contract data model, which can be used to extract relevant
    information from the company’s contracts. This model will serve as the blueprint
    for guiding LLMs in structured data extraction. With a clear understanding of
    the data structure in place, it’s time to explore how you can effectively prompt
    the LLM to extract this information.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Structured Outputs extraction request
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the contract data model defined, you now have a data definition that LLMs
    can follow to extract structured information. The next step is to ensure that
    the LLM understands exactly how to output this data in a consistent format. This
    is where OpenAI’s Structured Outputs feature comes in. By using this feature,
    you can guide the LLM’s behavior to output data that strictly adheres to the contract
    model while using the same chat template introduced in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The Structured Outputs documentation ([https://mng.bz/oZZp](https://mng.bz/oZZp))
    uses system messages to additionally guide the LLM to focus on the task at hand.
    By using a system message, as shown in the following listing, you can provide
    clear instructions to steer the model’s behavior effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 System message for structured output extraction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: It’s difficult to provide precise instructions for crafting the ideal system
    message. What’s clear is that you should define the domain and provide the LLM
    with context on how the output will be used. Beyond that, it often comes down
    to trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you define a function that takes any text as input and outputs a dictionary
    as defined by the contract data model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 System message for structured output extraction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Passing in system message as first message'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The document is passed as a user message without any additional instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The output format is defined using the response_format parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: The extract function in listing 6.8 processes a text document and returns a
    dictionary based on the contract data model. It utilizes the latest GPT-4o model
    available at the time of writing, which supports structured output. The function
    sends a system message to guide the LLM, followed by the raw user-provided document
    text without any modifications. The response is then formatted according to the
    `Contract` data model and returned as a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: To see this process in action, let’s now look at how we can apply this method
    using a real-world dataset. Since accessing proprietary contracts can be difficult
    due to confidentiality, you will use a public dataset titled the Contract Understanding
    Atticus Dataset (CUAD).
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 CUAD dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While all companies have contracts and legal documents, these are typically
    not public due to the sensitive nature of the information they contain. For the
    purpose of this demonstration, we will use a single text document from the CUAD
    dataset (Hendrycks et al., 2021). CUAD is a specialized corpus created for training
    AI models to understand and review legal contracts.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows an improved version. The contract is available in
    the accompanying GitHub repository of the book, eliminating the need to download
    the entire dataset. The code handles opening the file and reading its content.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 Reading the contract text document
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Reads the file'
  prefs: []
  type: TYPE_NORMAL
- en: You can now process the contract by executing the code shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.10 Extracting structured information from text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The results will look similar to the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Results of the extraction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The extracted contract data is organized into structured fields, though not
    all attributes are fully populated. For instance, some fields like `end_date`
    and `total_amount` are marked as `None`, indicating missing or unspecified information.
    Meanwhile, attributes such as the `contract_scope` contain more detailed, descriptive
    text that outlines the operational details of the agreement, such as the services
    provided and responsibilities. The structure includes a clear breakdown of the
    parties involved, their roles, and locations. The contract also specifies its
    start date and renewal conditions, but other financial or termination details
    remain undefined as they are missing in the contract.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Download the CUAD dataset and explore creating various contract data models
    based on different types of contracts. Once you’ve defined different models, you
    can test and refine them by analyzing how well they capture and categorize the
    key legal information across the contracts.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you successfully extracted structured data from a contract
    document using the CUAD dataset and the contract data model defined earlier. The
    LLM was guided to identify key contract details, and the results were formatted
    in a structured way, allowing you to organize important information such as contract
    type, parties, and terms. This process demonstrates how LLMs can efficiently transform
    unstructured legal documents into actionable data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve seen how to extract structured information from legal contracts,
    the next section will focus on how to incorporate this data into a knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Constructing the graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the final step in the chapter, you’ll import the extracted structured output
    into Neo4j. This follows the standard approach for importing structured data.
    First, you should design a suitable graph model that represents the relationships
    and entities in your data. Graph modeling is beyond the scope of this book, but
    you can use LLMs to assist in defining the graph schema or look at other learning
    material such as Neo4j Graph Academy.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a contract graph model is illustrated in figure 6.3, which you
    will be using in this step. The graph model represents a contract system with
    three main entities: `Contract`, `Organization`, and `Location`. The `Contract`
    node stores details such as its ID, type, effective date, term, total amount,
    governing law, and scope.'
  prefs: []
  type: TYPE_NORMAL
- en: Organizations are linked to contracts through the `HAS_PARTY` relationship,
    and each organization has a `HAS_LOCATION` relationship to a `Location` node,
    which captures the organization’s address, city, state, and country. Locations
    are represented as separate nodes to accommodate the possibility that a single
    organization may have multiple addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve defined the graph model, the next step is to begin the process
    of constructing the knowledge graph. This involves several key steps, each of
    which will be covered in the following subsections. First, you’ll define unique
    constraints and indexes to ensure data integrity and improve performance. After
    that, you’ll import the structured contract data into Neo4j using a Cypher statement.
    Once the data is loaded, you will visualize the graph to confirm that all entities
    and relationships are correctly represented. Finally, we’ll address important
    data refinement tasks, such as entity resolution, which ensures that different
    representations of the same real-world entity are merged correctly, and we’ll
    touch on how to handle both structured and unstructured data in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 Contract graph model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.2.1 Data import
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Defining unique constraints and indexes wherever applicable is a best practice,
    as it not only ensures the integrity of the graph but also enhances query performance.
    The code in the following listing defines unique constraints for `Contract`, `Organization`,
    and `Location` nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.12 Defining the unique constraints
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Next, you need to prepare an import Cypher statement that will take the dictionary
    output and load it into Neo4j, adhering to the graph schema outlined in figure
    6.3\. The import Cypher statement is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.13 Defining the import Cypher statement
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates the Contract node using a random UUID as unique identifier'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates the Party nodes and their locations'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Links parties to their location'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Links parties to the contract'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining Cypher statements, such as the one in listing 6.13, is outside the
    scope of this book. However, if you need assistance, LLMs can help clarify the
    details and provide a deeper understanding of the Cypher statement. However, we
    want to highlight that the query in listing 6.13 is not idempotent due to the
    use of `randomUUID()` for the contract ID. As a result, running the query multiple
    times will create duplicate contract entries in the database, each with a unique
    ID.
  prefs: []
  type: TYPE_NORMAL
- en: Now that everything is prepared, you can execute the code in the following listing
    to import the contract into Neo4j.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.14 Importing the contract data into Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once the import is successful, you can open the Neo4j browser to explore the
    generated graph, which should closely resemble the visualization shown in figure
    6.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 Contract graph data visualized
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The visualization in figure 6.4 depicts a graph where a central “Licensing
    Agreement” (representing a contract) is linked to two organizations: “Mortgage
    Logic.com, Inc.” and “TrueLink, Inc.” via the relationship `HAS_PARTY`. Each organization
    is further connected to a “US” node representing their location through the `LOCATED_AT`
    relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Entity resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ve successfully imported the graph, but your work isn’t done yet. In most
    cases, especially when dealing with natural language processing or LLM-driven
    data processing, some level of data cleaning is necessary. One of the most crucial
    steps in this cleaning process is entity resolution. Entity resolution refers
    to the process of identifying and merging different representations of the same
    real-world entity within a dataset or knowledge graph. When working with large
    and diverse datasets, it’s common for the same entity to appear in multiple forms
    due to inconsistencies like spelling variations, different naming conventions,
    or even slight discrepancies in data formats, as shown in figure 6.5, where we
    see three nodes representing variations of the same entity. The three names are
  prefs: []
  type: TYPE_NORMAL
- en: UTI Asset Management Company
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UTI Asset Management Company Limited
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UTI Asset Management Company Ltd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/6-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 Potential duplicates
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Entity resolution in this context involves identifying that all these variations
    refer to the same real-world organization, despite minor differences in naming
    conventions (such as “Limited” vs. “Ltd”). The goal of entity resolution is to
    unify these disparate references into a single, coherent node within the graph.
    This not only improves data integrity but also enhances the graph’s ability to
    make more accurate inferences and relationships. Techniques used in entity resolution
    include string matching, clustering algorithms, and even machine learning methods
    that use the context surrounding each entity to detect and resolve duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that entity resolution is highly use case and domain
    specific. A generic, one-size-fits-all solution rarely works because each domain
    has its own naming conventions, data schemas, and nuances in how entities are
    represented. For instance, the methods and thresholds that might work well for
    resolving organizations in a financial dataset could produce suboptimal results
    when dealing with biological entities in a healthcare setting. Consequently, one
    of the most effective strategies is to develop domain-specific ontologies or rules
    that reflect your particular data context. Additionally, using subject matter
    experts to define matching criteria and using iterative feedback loops—where potential
    matches are verified or corrected—can greatly improve accuracy. By combining domain
    expertise with context-aware machine learning or clustering techniques, you can
    develop a more robust and flexible approach to entity resolution. This will ensure
    that you capture the subtle details that matter most in your unique data environment.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Adding unstructured data to the graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge graphs are increasingly used to store both structured and unstructured
    data, a scenario that has become even more common with the advent of LLMs. In
    this context, LLMs can be used to extract structured data from unstructured sources
    like text documents. However, storing the original unstructured documents and
    the extracted structured data within the graph preserves the richness of the original
    data while enabling more precise querying and analysis of the extracted information.
    An expanded graph schema where structured and unstructured information is combined
    is presented in figure 6.6\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 Expanded graph model with added unstructured data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When incorporating unstructured data into a graph, it’s common to use a simple
    chunking strategy based on token count or word length to split text into manageable
    segments. While this naive approach works for general use cases, certain domains,
    such as legal contracts, benefit from more specialized chunking methods. For example,
    splitting a contract by its clauses preserves its semantic structure and improves
    the quality of downstream analysis. This smarter approach allows the graph to
    capture more meaningful relationships, enabling richer insights and more accurate
    inferences.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has guided you through constructing knowledge graphs from unstructured
    data using LLMs. You explored the limitations of text embeddings in handling structured
    queries and learned how structured data extraction provides a solution. By defining
    data models, prompting LLMs for extraction, and importing the results into a graph
    database, you saw how to transform raw text into usable data for knowledge graphs.
    Additionally, we covered key tasks like entity resolution and combining structured
    and unstructured data for richer insights. With this knowledge, you can now apply
    structured data extraction in practical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simply chunking documents for retrieval can result in inaccurate or mixed results,
    especially in domains like legal documents where document boundaries matter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval tasks like filtering, sorting, and aggregating require structured
    data, as text embeddings alone are not suited for such operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are effective at extracting structured data from unstructured text, converting
    it into usable formats like tables or key–value pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured output features in LLMs allow developers to define schemas, ensuring
    responses follow a specific format and reducing the need for postprocessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a clear data model with attributes such as contract type, parties,
    and dates is essential for guiding LLMs to extract relevant information accurately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity resolution in knowledge graphs is important for merging different representations
    of the same entity, improving data consistency and accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining structured and unstructured data in knowledge graphs preserves the
    richness of the source material while enabling more precise querying.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
