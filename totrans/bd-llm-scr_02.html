<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span></span> <span class="chapter-title-text">Coding attention mechanisms</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">The reasons for using attention mechanisms in neural networks</li> 
    <li class="readable-text" id="p3">A basic self-attention framework, progressing to an enhanced self-attention mechanism </li> 
    <li class="readable-text" id="p4">A causal attention module that allows LLMs to generate one token at a time</li> 
    <li class="readable-text" id="p5">Masking randomly selected attention weights with dropout to reduce overfitting</li> 
    <li class="readable-text" id="p6">Stacking multiple causal attention modules into a multi-head attention module</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>At this point, you know how to prepare the input text for training LLMs by splitting text into individual word and subword tokens, which can be encoded into vector representations, embeddings, for the LLM. </p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>Now, we will look at an integral part of the LLM architecture itself, attention mechanisms, as illustrated in figure 3.1. We will largely look at attention mechanisms in isolation and focus on them at a mechanistic level. Then we will code the remaining parts of the LLM surrounding the self-attention mechanism to see it in action and to create a model to generate text.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p9">  
   <img alt="figure" src="../Images/3-1.png" width="1012" height="439"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.1</span> The three main stages of coding an LLM. This chapter focuses on step 2 of stage 1: implementing attention mechanisms, which are an integral part of the LLM architecture.</h5>
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>We will implement four different variants of attention mechanisms, as illustrated in figure 3.2.<span class="aframe-location"/> These different attention variants build on each other, and the goal is to arrive at a compact and efficient implementation of multi-head attention that we can then plug into the LLM architecture we will code in the next chapter.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p11">  
   <img alt="figure" src="../Images/3-2.png" width="907" height="409"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.2</span> The figure depicts different attention mechanisms we will code in this chapter, starting with a simplified version of self-attention before adding the trainable weights. The causal attention mechanism adds a mask to self-attention that allows the LLM to generate one word at a time. Finally, multi-head attention organizes the attention mechanism into multiple heads, allowing the model to capture various aspects of the input data in parallel.</h5>
  </div> 
  <div class="readable-text" id="p12"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.1</span> The problem with modeling long sequences</h2> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>Before we dive into the <em>self-attention </em>mechanism at the heart of LLMs, let‚Äôs consider the problem with pre-LLM architectures that do not include attention mechanisms. Suppose we want to develop a language translation model that translates text from one language into another. As shown in figure 3.3, we can‚Äôt simply translate a text word by word due to the grammatical structures in the source and target language.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p14">  
   <img alt="figure" src="../Images/3-3.png" width="927" height="669"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.3</span> When translating text from one language to another, such as German to English, it‚Äôs not possible to merely translate word by word. Instead, the translation process requires contextual understanding and grammatical alignment. </h5>
  </div> 
  <div class="readable-text" id="p15"> 
   <p>To address this problem, it is common to use a deep neural network with two submodules, an <em>encoder</em> and a <em>decoder</em>. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>Before the advent of transformers, <em>recurrent neural networks</em> (RNNs) were the most popular encoder‚Äìdecoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text. If you are unfamiliar with RNNs, don‚Äôt worry‚Äîyou don‚Äôt need to know the detailed workings of RNNs to follow this discussion; our focus here is more on the general concept of the encoder‚Äìdecoder setup. </p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>In an encoder‚Äìdecoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state, as illustrated in figure 3.4. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p18">  
   <img alt="figure" src="../Images/3-4.png" width="915" height="439"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.4</span> Before the advent of transformer models, encoder‚Äìdecoder RNNs were a popular choice for machine translation. The encoder takes a sequence of tokens from the source language as input, where a hidden state (an intermediate neural network layer) of the encoder encodes a compressed representation of the entire input sequence. Then, the decoder uses its current hidden state to begin the translation, token by token.</h5>
  </div> 
  <div class="readable-text" id="p19"> 
   <p>While we don‚Äôt need to know the inner workings of these encoder‚Äìdecoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. You can think of this hidden state as an embedding vector, a concept we discussed in chapter 2.</p> 
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>The big limitation of encoder‚Äìdecoder RNNs is that the RNN can‚Äôt directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances.</p> 
  </div> 
  <div class="readable-text intended-text" id="p21"> 
   <p>Fortunately, it is not essential to understand RNNs to build an LLM. Just remember that encoder‚Äìdecoder RNNs had a shortcoming that motivated the design of attention mechanisms.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.2</span> Capturing data dependencies with attention mechanisms</h2> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>Although RNNs work fine for translating short sentences, they don‚Äôt work well for longer texts as they don‚Äôt have direct access to previous words in the input. One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder (figure 3.4).</p> 
  </div> 
  <div class="readable-text intended-text" id="p24"> 
   <p>Hence, researchers developed the <em>Bahdanau attention</em> mechanism for RNNs in 2014 (named after the first author of the respective paper; for more information, see appendix B), which modifies the encoder‚Äìdecoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step as illustrated in figure 3.5.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p25">  
   <img alt="figure" src="../Images/3-5.png" width="924" height="509"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.5</span> Using an attention mechanism, the text-generating decoder part of the network can access all input tokens selectively. This means that some input tokens are more important than others for generating a given output token. The importance is determined by the attention weights, which we will compute later. Note that this figure shows the general idea behind attention and does not depict the exact implementation of the Bahdanau mechanism, which is an RNN method outside this book‚Äôs scope.</h5>
  </div> 
  <div class="readable-text" id="p26"> 
   <p>Interestingly, only three years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original <em>transformer</em> architecture (discussed in chapter 1) including a self-attention mechanism inspired by the Bahdanau attention mechanism. </p> 
  </div> 
  <div class="readable-text intended-text" id="p27"> 
   <p>Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of, or ‚Äúattend to,‚Äù all other positions in the same sequence when computing the representation of a sequence. Self-attention is a key component of contemporary LLMs based on the transformer architecture, such as the GPT series. </p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>This chapter focuses on coding and understanding this self-attention mechanism used in GPT-like models, as illustrated in figure 3.6. In the next chapter, we will code the remaining parts of the LLM.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p29">  
   <img alt="figure" src="../Images/3-6.png" width="664" height="563"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.6</span> Self-attention is a mechanism in transformers used to compute more efficient input representations by allowing each position in a sequence to interact with and weigh the importance of all other positions within the same sequence. In this chapter, we will code this self-attention mechanism from the ground up before we code the remaining parts of the GPT-like LLM in the following chapter.</h5>
  </div> 
  <div class="readable-text" id="p30"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.3</span> Attending to different parts of the input with self-attention</h2> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>We‚Äôll now cover the inner workings of the self-attention mechanism and learn how to code it from the ground up. Self-attention serves as the cornerstone of every LLM based on the transformer architecture. This topic may require a lot of focus and attention (no pun intended), but once you grasp its fundamentals, you will have conquered one of the toughest aspects of this book and LLM implementation in general.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p32"> 
    <h5 class=" callout-container-h5 readable-text-h5">The ‚Äúself‚Äù in self-attention </h5> 
   </div> 
   <div class="readable-text" id="p33"> 
    <p>In self-attention, the ‚Äúself‚Äù refers to the mechanism‚Äôs ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image. </p> 
   </div> 
   <div class="readable-text" id="p34"> 
    <p>This is in contrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models where the attention might be between an input sequence and an output sequence, such as the example depicted in figure 3.5.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>Since self-attention can appear complex, especially if you are encountering it for the first time, we will begin by examining a simplified version of it. Then we will implement the self-attention mechanism with trainable weights used in LLMs.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.3.1</span> A simple self-attention mechanism without trainable weights</h3> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>Let‚Äôs begin by implementing a simplified variant of self-attention, free from any trainable weights, as summarized in figure 3.7. The goal is to illustrate a few key concepts in self-attention before adding trainable weights.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p38">  
   <img alt="figure" src="../Images/3-7.png" width="742" height="442"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.7</span> The goal of self-attention is to compute a context vector for each input element that combines information from all other input elements. In this example, we compute the context vector z<sup>(2)</sup>. The importance or contribution of each input element for computing z<sup>(2)</sup> is determined by the attention weights <span class="regular-symbol">a</span><sub>21</sub> to <span class="regular-symbol">a</span><sub>2T</sub>. When computing z<sup>(2)</sup>, the attention weights are calculated with respect to input element x<sup>(2)</sup> and all other inputs.</h5>
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Figure 3.7 shows an input sequence, denoted as <em>x</em>, consisting of <em>T</em> elements represented as <em>x</em>(1) to <em>x</em>(T). This sequence typically represents text, such as a sentence, that has already been transformed into token embeddings.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>For example, consider an input text like ‚ÄúYour journey starts with one step.‚Äù In this case, each element of the sequence, such as <em>x</em>(1), corresponds to a <em>d</em>-dimensional embedding vector representing a specific token, like ‚ÄúYour.‚Äù Figure 3.7 shows these input vectors as three-dimensional embeddings.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>In self-attention, our goal is to calculate context vectors <em>z</em>(i) for each element <em>x</em>(i) in the input sequence. A <em>context vector</em> can be interpreted as an enriched embedding vector.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>To illustrate this concept, let‚Äôs focus on the embedding vector of the second input element, <em>x</em>(2) (which corresponds to the token ‚Äújourney‚Äù), and the corresponding context vector, <em>z</em>(2), shown at the bottom of figure 3.7. This enhanced context vector, <em>z</em>(2), is an embedding that contains information about <em>x</em>(2) and all other input elements, <em>x</em>(1) to <em>x</em>(T).</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>Context vectors play a crucial role in self-attention. Their purpose is to create enriched representations of each element in an input sequence (like a sentence) by incorporating information from all other elements in the sequence (figure 3.7). This is essential in LLMs, which need to understand the relationship and relevance of words in a sentence to each other. Later, we will add trainable weights that help an LLM learn to construct these context vectors so that they are relevant for the LLM to generate the next token. But first, let‚Äôs implement a simplified self-attention mechanism to compute these weights and the resulting context vector one step at a time. </p> 
  </div> 
  <div class="readable-text intended-text" id="p44"> 
   <p>Consider the following input sentence, which has already been embedded into three-dimensional vectors (see chapter 2). I‚Äôve chosen a small embedding dimension to ensure it fits on the page without line breaks:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p45"> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>The first step of implementing self-attention is to compute the intermediate values <span class="regular-symbol">w</span>, referred to as attention scores, as illustrated in figure 3.8. Due to spatial constraints, the figure displays the values of the preceding <code>inputs</code> tensor in a truncated version; for example, 0.87 is truncated to 0.8. In this truncated version, the embeddings of the words ‚Äújourney‚Äù and ‚Äústarts‚Äù may appear similar by random chance.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p47">  
   <img alt="figure" src="../Images/3-8.png" width="1100" height="407"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.8</span> The overall goal is to illustrate the computation of the context vector z<sup>(2)</sup> using the second input element, x<sup>(2)</sup> as a query. This figure shows the first intermediate step, computing the attention scores <span class="regular-symbol">w</span> between the query x<sup>(2)</sup> and all other input elements as a dot product. (Note that the numbers are truncated to one digit after the decimal point to reduce visual clutter.)</h5>
  </div> 
  <div class="readable-text" id="p48"> 
   <p>Figure 3.8 illustrates how we calculate the intermediate attention scores between the query token and each input token. We determine these scores by computing the dot product of the query, <em>x</em>(2), with every other input token:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p49"> 
   <div class="code-area-container"> 
    <pre class="code-area">query = inputs[1]                           <span class="aframe-location"/> #1
attn_scores_2 = torch.empty(inputs.shape[0])
for i, x_i in enumerate(inputs):
    attn_scores_2[i] = torch.dot(x_i, query)
print(attn_scores_2)</pre> 
    <div class="code-annotations-overlay-container">
     #1 The second input token serves as the query.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>The computed attention scores are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p51"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p52"> 
    <h5 class=" callout-container-h5 readable-text-h5">Understanding dot products </h5> 
   </div> 
   <div class="readable-text" id="p53"> 
    <p>A dot product is essentially a concise way of multiplying two vectors element-wise and then summing the products, which can be demonstrated as follows:</p> 
   </div> 
   <div class="browsable-container listing-container" id="p54"> 
    <div class="code-area-container"> 
     <pre class="code-area">res = 0.
for idx, element in enumerate(inputs[0]):
    res += inputs[0][idx] * query[idx]
print(res)
print(torch.dot(inputs[0], query))</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p55"> 
    <p>The output confirms that the sum of the element-wise multiplication gives the same results as the dot product:</p> 
   </div> 
   <div class="browsable-container listing-container" id="p56"> 
    <div class="code-area-container"> 
     <pre class="code-area">tensor(0.9544)
tensor(0.9544)</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p57"> 
    <p>Beyond viewing the dot product operation as a mathematical tool that combines two vectors to yield a scalar value, the dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a greater degree of alignment or similarity between the vectors. In the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or ‚Äúattends to,‚Äù any other element: the higher the dot product, the higher the similarity and attention score between two elements.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>In the next step, as shown in figure 3.9, we normalize each of the attention scores we computed previously. The main goal behind the normalization is to obtain attention weights that sum up to 1. This normalization is a convention that is useful for interpretation and maintaining training stability in an LLM. Here‚Äôs a straightforward method for achieving this normalization step:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p59"> 
   <div class="code-area-container"> 
    <pre class="code-area">attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()
print("Attention weights:", attn_weights_2_tmp)
print("Sum:", attn_weights_2_tmp.sum())<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p60">  
   <img alt="figure" src="../Images/3-9.png" width="927" height="347"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.9</span> After computing the attention scores <span class="regular-symbol">w</span><sub>21</sub> to <span class="regular-symbol">w</span><sub>2T </sub>with respect to the input query x<sup>(2)</sup>, the next step is to obtain the attention weights <span class="regular-symbol">a</span><sub>21</sub> to <span class="regular-symbol">a</span><sub>2T </sub>by normalizing the attention scores.</h5>
  </div> 
  <div class="readable-text" id="p61"> 
   <p>As the output shows, the attention weights now sum to 1: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p62"> 
   <div class="code-area-container"> 
    <pre class="code-area">Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])
Sum: tensor(1.0000)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>In practice, it‚Äôs more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. The following is a basic implementation of the softmax function for normalizing the attention scores:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p64"> 
   <div class="code-area-container"> 
    <pre class="code-area">def softmax_naive(x):
    return torch.exp(x) / torch.exp(x).sum(dim=0)

attn_weights_2_naive = softmax_naive(attn_scores_2)
print("Attention weights:", attn_weights_2_naive)
print("Sum:", attn_weights_2_naive.sum())</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>As the output shows, the softmax function also meets the objective and normalizes the attention weights such that they sum to 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p66"> 
   <div class="code-area-container"> 
    <pre class="code-area">Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
Sum: tensor(1.)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>In addition, the softmax function ensures that the attention weights are always positive. This makes the output interpretable as probabilities or relative importance, where higher weights indicate greater importance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>Note that this naive softmax implementation (<code>softmax_naive</code>) may encounter numerical instability problems, such as overflow and underflow, when dealing with large or small input values. Therefore, in practice, it‚Äôs advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p69"> 
   <div class="code-area-container"> 
    <pre class="code-area">attn_weights_2 = torch.softmax(attn_scores_2, dim=0)
print("Attention weights:", attn_weights_2)
print("Sum:", attn_weights_2.sum())</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>In this case, it yields the same results as our previous <code>softmax_naive</code> function:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p71"> 
   <div class="code-area-container"> 
    <pre class="code-area">Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
Sum: tensor(1.)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>Now that we have computed the normalized attention weights, we are ready for the final step, as shown in figure 3.10: calculating the context vector <em>z</em>(2) by multiplying the embedded input tokens, <em>x</em>(i), with the corresponding attention weights and then summing the resulting vectors. Thus, context vector <em>z</em>(2) is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p73"> 
   <div class="code-area-container"> 
    <pre class="code-area">query = inputs[1]        <span class="aframe-location"/> #1
context_vec_2 = torch.zeros(query.shape)
for i,x_i in enumerate(inputs):
    context_vec_2 += attn_weights_2[i]*x_i
print(context_vec_2)<span class="aframe-location"/></pre> 
    <div class="code-annotations-overlay-container">
     #1 The second input token is the query.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p74">  
   <img alt="figure" src="../Images/3-10.png" width="1100" height="414"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.10</span> The final step, after calculating and normalizing the attention scores to obtain the attention weights for query x<sup>(2)</sup>, is to compute the context vector z<sup>(2)</sup>. This context vector is a combination of all input vectors x<sup>(1)</sup> to x<sup>(</sup><sup><em>T‚Äâ</em></sup><sup>)</sup> weighted by the attention weights.</h5>
  </div> 
  <div class="readable-text" id="p75"> 
   <p>The results of this computation are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([0.4419, 0.6515, 0.5683])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Next, we will generalize this procedure for computing context vectors to calculate all context vectors simultaneously.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.3.2</span> Computing attention weights for all input tokens</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>So far, we have computed attention weights and the context vector for input 2, as shown in the highlighted row in figure 3.11. Now let‚Äôs extend this computation to calculate attention weights and context vectors for all inputs.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/3-11.png" width="684" height="354"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.11</span> The highlighted row shows the attention weights for the second input element as a query. Now we will generalize the computation to obtain all other attention weights. (Please note that the numbers in this figure are truncated to two digits after the decimal point to reduce visual clutter. The values in each row should add up to 1.0 or 100%.)</h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <p>We follow the same three steps as before (see figure 3.12), except that we make a few modifications in the code to compute all context vectors instead of only the second one, <em>z</em>(2):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p82"> 
   <div class="code-area-container"> 
    <pre class="code-area">attn_scores = torch.empty(6, 6)
for i, x_i in enumerate(inputs):
    for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)
print(attn_scores)<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p83">  
   <img alt="figure" src="../Images/3-12.png" width="677" height="219"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.12</span> In step 1, we add an additional <code>for</code> loop to compute the dot products for all pairs of inputs.</h5>
  </div> 
  <div class="readable-text" id="p84"> 
   <p>The resulting attention scores are as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>Each element in the tensor represents an attention score between each pair of inputs, as we saw in figure 3.11. Note that the values in that figure are normalized, which is why they differ from the unnormalized attention scores in the preceding tensor. We will take care of the normalization later. </p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>When computing the preceding attention score tensor, we used <code>for</code> loops in Python. However, <code>for</code> loops are generally slow, and we can achieve the same results using matrix multiplication:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p88"> 
   <div class="code-area-container"> 
    <pre class="code-area">attn_scores = inputs @ inputs.T
print(attn_scores)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>We can visually confirm that the results are the same as before:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p90"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>In step 2 of figure 3.12, we normalize each row so that the values in each row sum to 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p92"> 
   <div class="code-area-container"> 
    <pre class="code-area">attn_weights = torch.softmax(attn_scores, dim=-1)
print(attn_weights)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>This returns the following attention weight tensor that matches the values shown in figure 3.10:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],
        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],
        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],
        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],
        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],
        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>In the context of using PyTorch, the dim parameter in functions like <code>torch.softmax</code> specifies the dimension of the input tensor along which the function will be computed. By setting <code>dim=-1</code>, we are instructing the <code>softmax</code> function to apply the normalization along the last dimension of the <code>attn_scores</code> tensor. If <code>attn_scores</code> is a two-dimensional tensor (for example, with a shape of [rows, columns]), it will normalize across the columns so that the values in each row (summing over the column dimension) sum up to 1.</p> 
  </div> 
  <div class="readable-text intended-text" id="p96"> 
   <p>We can verify that the rows indeed all sum to 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p97"> 
   <div class="code-area-container"> 
    <pre class="code-area">row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
print("Row 2 sum:", row_2_sum)
print("All row sums:", attn_weights.sum(dim=-1))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p99"> 
   <div class="code-area-container"> 
    <pre class="code-area">Row 2 sum: 1.0
All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>In the third and final step of figure 3.12, we use these attention weights to compute all context vectors via matrix multiplication:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <div class="code-area-container"> 
    <pre class="code-area">all_context_vecs = attn_weights @ inputs
print(all_context_vecs)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>In the resulting output tensor, each row contains a three-dimensional context vector:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p103"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.4421, 0.5931, 0.5790],
        [0.4419, 0.6515, 0.5683],
        [0.4431, 0.6496, 0.5671],
        [0.4304, 0.6298, 0.5510],
        [0.4671, 0.5910, 0.5266],
        [0.4177, 0.6503, 0.5645]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>We can double-check that the code is correct by comparing the second row with the context vector <em>z</em><sup>(2)</sup> that we computed in section 3.3.1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Previous 2nd context vector:", context_vec_2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>Based on the result, we can see that the previously calculated <code>context_vec_2</code> matches the second row in the previous tensor exactly: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p107"> 
   <div class="code-area-container"> 
    <pre class="code-area">Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>This concludes the code walkthrough of a simple self-attention mechanism. Next, we will add trainable weights, enabling the LLM to learn from data and improve its performance on specific tasks.</p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.4</span> Implementing self-attention with trainable weights</h2> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>Our next step will be to implement the self-attention mechanism used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called <em>scaled dot-product attention</em>. Figure 3.13 shows how this self-attention mechanism fits into the broader context of implementing an LLM. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p111">  
   <img alt="figure" src="../Images/3-13.png" width="1012" height="574"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.13</span> Previously, we coded a simplified attention mechanism to understand the basic mechanism behind attention mechanisms. Now, we add trainable weights to this attention mechanism. Later, we will extend this self-attention mechanism by adding a causal mask and multiple heads.</h5>
  </div> 
  <div class="readable-text" id="p112"> 
   <p>As illustrated in figure 3.13, the self-attention mechanism with trainable weights builds on the previous concepts: we want to compute context vectors as weighted sums over the input vectors specific to a certain input element. As you will see, there are only slight differences compared to the basic self-attention mechanism we coded earlier.</p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>The most notable difference is the introduction of weight matrices that are updated during model training. These trainable weight matrices are crucial so that the model (specifically, the attention module inside the model) can learn to produce ‚Äúgood‚Äù context vectors. (We will train the LLM in chapter 5.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>We will tackle this self-attention mechanism in the two subsections. First, we will code it step by step as before. Second, we will organize the code into a compact Python class that can be imported into the LLM architecture.</p> 
  </div> 
  <div class="readable-text" id="p115"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.4.1</span> Computing the attention weights step by step</h3> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>We will implement the self-attention mechanism step by step by introducing the three trainable weight matrices <em>W</em><sub>q</sub>, <em>W</em><sub>k</sub>, and <em>W</em><sub>v</sub>. These three matrices are used to project the embedded input tokens, <em>x</em><sup>(i)</sup>, into query, key, and value vectors, respectively, as illustrated in figure 3.14.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p117">  
   <img alt="figure" src="../Images/3-14.png" width="1012" height="434"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.14</span> In the first step of the self-attention mechanism with trainable weight matrices, we compute query (q), key (k), and value (v) vectors for input elements x. Similar to previous sections, we designate the second input, x<sup>(2)</sup>, as the query input. The query vector q<sup>(2)</sup> is obtained via matrix multiplication between the input x<sup>(2)</sup> and the weight matrix W<sub>q</sub>. Similarly, we obtain the key and value vectors via matrix multiplication involving the weight matrices W<sub>k</sub> and W<sub>v</sub>.</h5>
  </div> 
  <div class="readable-text" id="p118"> 
   <p>Earlier, we defined the second input element <em>x</em><sup>(2)</sup> as the query when we computed the simplified attention weights to compute the context vector <em>z</em><sup>(2)</sup>. Then we generalized this to compute all context vectors <em>z</em><sup>(1)</sup><em> ... z</em><sup>(T)</sup> for the six-word input sentence ‚ÄúYour journey starts with one step.‚Äù </p> 
  </div> 
  <div class="readable-text intended-text" id="p119"> 
   <p>Similarly, we start here by computing only one context vector, <em>z</em><sup>(2)</sup>, for illustration purposes. We will then modify this code to calculate all context vectors.</p> 
  </div> 
  <div class="readable-text intended-text" id="p120"> 
   <p>Let‚Äôs begin by defining a few variables:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p121"> 
   <div class="code-area-container"> 
    <pre class="code-area">x_2 = inputs[1]    <span class="aframe-location"/> #1
d_in = inputs.shape[1]     <span class="aframe-location"/> #2
d_out = 2        <span class="aframe-location"/> #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 The second input element
     <br/>#2 The input embedding size, d=3
     <br/>#3 The output embedding size, d_out=2
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Note that in GPT-like models, the input and output dimensions are usually the same, but to better follow the computation, we‚Äôll use different input (<code>d_in=3</code>) and output (<code>d_out=2</code>) dimensions here.</p> 
  </div> 
  <div class="readable-text intended-text" id="p123"> 
   <p>Next, we initialize the three weight matrices <em>W</em><sub>q</sub>, <em>W</em><sub>k</sub>, and <em>W</em><sub>v</sub> shown in figure 3.14:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p124"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>We set <code>requires_grad=False</code> to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would set <code>requires_grad=True</code> to update these matrices during model training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>Next, we compute the query, key, and value vectors:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p127"> 
   <div class="code-area-container"> 
    <pre class="code-area">query_2 = x_2 @ W_query 
key_2 = x_2 @ W_key 
value_2 = x_2 @ W_value
print(query_2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>The output for the query results in a two-dimensional vector since we set the number of columns of the corresponding weight matrix, via <code>d_out</code>, to 2:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p129"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([0.4306, 1.4551])</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p130"> 
    <h5 class=" callout-container-h5 readable-text-h5">Weight parameters vs. attention weights </h5> 
   </div> 
   <div class="readable-text" id="p131"> 
    <p>In the weight matrices <em>W</em>, the term ‚Äúweight‚Äù is short for ‚Äúweight parameters,‚Äù the values of a neural network that are optimized during training. This is not to be confused with the attention weights. As we already saw, attention weights determine the extent to which a context vector depends on the different parts of the input (i.e., to what extent the network focuses on different parts of the input). </p> 
   </div> 
   <div class="readable-text" id="p132"> 
    <p>In summary, weight parameters are the fundamental, learned coefficients that define the network‚Äôs connections, while attention weights are dynamic, context-specific values.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p133"> 
   <p>Even though our temporary goal is only to compute the one context vector, <em>z</em><sup>(2)</sup>, we still require the key and value vectors for all input elements as they are involved in computing the attention weights with respect to the query <em>q‚Äâ</em><sup>(2)</sup> (see figure 3.14).</p> 
  </div> 
  <div class="readable-text intended-text" id="p134"> 
   <p>We can obtain all keys and values via matrix multiplication:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p135"> 
   <div class="code-area-container"> 
    <pre class="code-area">keys = inputs @ W_key 
values = inputs @ W_value
print("keys.shape:", keys.shape)
print("values.shape:", values.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>As we can tell from the outputs, we successfully projected the six input tokens from a three-dimensional onto a two-dimensional embedding space:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p137"> 
   <div class="code-area-container"> 
    <pre class="code-area">keys.shape: torch.Size([6, 2])
values.shape: torch.Size([6, 2])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>The second step is to compute the attention scores, as shown in figure 3.15.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p139">  
   <img alt="figure" src="../Images/3-15.png" width="1012" height="464"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.15</span> The attention score computation is a dot-product computation similar to what we used in the simplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing the dot-product between the input elements but using the query and key obtained by transforming the inputs via the respective weight matrices.</h5>
  </div> 
  <div class="readable-text" id="p140"> 
   <p>First, let‚Äôs compute the attention score <span class="regular-symbol">œâ</span><sub>22</sub>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p141"> 
   <div class="code-area-container"> 
    <pre class="code-area">keys_2 = keys[1]            <span class="aframe-location"/> #1
attn_score_22 = query_2.dot(keys_2)
print(attn_score_22)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Remember that Python starts indexing at 0.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>The result for the unnormalized attention score is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p143"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor(1.8524)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>Again, we can generalize this computation to all attention scores via matrix multiplication:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p145"> 
   <div class="code-area-container"> 
    <pre class="code-area">attn_scores_2 = query_2 @ keys.T      <span class="aframe-location"/> #1
print(attn_scores_2)</pre> 
    <div class="code-annotations-overlay-container">
     #1 All attention scores for given query
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>As we can see, as a quick check, the second element in the output matches the <code>attn_score_22</code> we computed previously:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p147"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p148"> 
   <p>Now, we want to go from the attention scores to the attention weights, as illustrated in figure 3.16. We compute the attention weights by scaling the attention scores and using the softmax function. However, now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (taking the square root is mathematically the same as exponentiating by 0.5):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p149"> 
   <div class="code-area-container"> 
    <pre class="code-area">d_k = keys.shape[-1]
attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)
print(attn_weights_2)<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p150">  
   <img alt="figure" src="../Images/3-16.png" width="1009" height="469"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.16</span> After computing the attention scores <span class="regular-symbol">œâ</span>, the next step is to normalize these scores using the softmax function to obtain the attention weights <span class="regular-symbol">ùõº</span>.</h5>
  </div> 
  <div class="readable-text" id="p151"> 
   <p>The resulting attention weights are</p> 
  </div> 
  <div class="browsable-container listing-container" id="p152"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p153"> 
    <h5 class=" callout-container-h5 readable-text-h5">The rationale behind scaled-dot product attention</h5> 
   </div> 
   <div class="readable-text" id="p154"> 
    <p>The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.</p> 
   </div> 
   <div class="readable-text" id="p155"> 
    <p>The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention. </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p156"> 
   <p>Now, the final step is to compute the context vectors, as illustrated in figure 3.17.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p157">  
   <img alt="figure" src="../Images/3-17.png" width="1012" height="574"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.17</span> In the final step of the self-attention computation, we compute the context vector by combining all value vectors via the attention weights. </h5>
  </div> 
  <div class="readable-text" id="p158"> 
   <p>Similar to when we computed the context vector as a weighted sum over the input vectors (see section 3.3), we now compute the context vector as a weighted sum over the value vectors. Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. Also as before, we can use matrix multiplication to obtain the output in one step:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p159"> 
   <div class="code-area-container"> 
    <pre class="code-area">context_vec_2 = attn_weights_2 @ values
print(context_vec_2)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p160"> 
   <p>The contents of the resulting vector are as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p161"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([0.3061, 0.8210])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p162"> 
   <p>So far, we‚Äôve only computed a single context vector, <em>z</em><sup>(2)</sup>. Next, we will generalize the code to compute all context vectors in the input sequence, <em>z</em><sup>(1)</sup> to <em>z</em><sup>(T)</sup>.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p163"> 
    <h5 class=" callout-container-h5 readable-text-h5">Why query, key, and value?</h5> 
   </div> 
   <div class="readable-text" id="p164"> 
    <p>The terms ‚Äúkey,‚Äù ‚Äúquery,‚Äù and ‚Äúvalue‚Äù in the context of attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.</p> 
   </div> 
   <div class="readable-text" id="p165"> 
    <p>A <em>query</em> is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.</p> 
   </div> 
   <div class="readable-text" id="p166"> 
    <p>The <em>key</em> is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match the query. </p> 
   </div> 
   <div class="readable-text" id="p167"> 
    <p>The <em>value</em> in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p168"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.4.2</span> Implementing a compact self-attention Python class</h3> 
  </div> 
  <div class="readable-text" id="p169"> 
   <p>At this point, we have gone through a lot of steps to compute the self-attention outputs. We did so mainly for illustration purposes so we could go through one step at a time. In practice, with the LLM implementation in the next chapter in mind, it is helpful to organize this code into a Python class, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p170"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.1</span> A compact self-attention class</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch.nn as nn
class SelfAttention_v1(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
        self.W_query = nn.Parameter(torch.rand(d_in, d_out))
        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))
        self.W_value = nn.Parameter(torch.rand(d_in, d_out))

    def forward(self, x):
        keys = x @ self.W_key
        queries = x @ self.W_query
        values = x @ self.W_value
        attn_scores = queries @ keys.T # omega
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1
        )
        context_vec = attn_weights @ values
        return context_vec</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>In this PyTorch code, <code>SelfAttention_v1</code> is a class derived from <code>nn.Module</code>, which is a fundamental building block of PyTorch models that provides necessary functionalities for model layer creation and management. </p> 
  </div> 
  <div class="readable-text intended-text" id="p172"> 
   <p>The <code>__init__</code> method initializes trainable weight matrices (<code>W_query</code>, <code>W_key</code>, and <code>W_value</code>) for queries, keys, and values, each transforming the input dimension <code>d_in</code> to an output dimension <code>d_out</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p173"> 
   <p>During the forward pass, using the forward method, we compute the attention scores (<code>attn_scores</code>) by multiplying queries and keys, normalizing these scores using softmax. Finally, we create a context vector by weighting the values with these normalized attention scores.</p> 
  </div> 
  <div class="readable-text intended-text" id="p174"> 
   <p>We can use this class as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p175"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
sa_v1 = SelfAttention_v1(d_in, d_out)
print(sa_v1(inputs))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p176"> 
   <p>Since <code>inputs</code> contains six embedding vectors, this results in a matrix storing the six context vectors:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p177"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.2996, 0.8053],
        [0.3061, 0.8210],
        [0.3058, 0.8203],
        [0.2948, 0.7939],
        [0.2927, 0.7891],
        [0.2990, 0.8040]], grad_fn=&lt;MmBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p178"> 
   <p>As a quick check, notice that the second row (<code>[0.3061,</code> <code>0.8210]</code>) matches the contents of <code>context_vec_2</code> in the previous section. Figure 3.18 summarizes the self-attention mechanism we just implemented.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p179">  
   <img alt="figure" src="../Images/3-18.png" width="1009" height="863"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.18</span> In self-attention, we transform the input vectors in the input matrix X with the three weight matrices, W<sub>q</sub>, W<sub>k</sub>, and W<sub>v</sub>. Then we compute the attention weight matrix based on the resulting queries (Q) and keys (K). Using the attention weights and values (V), we then compute the context vectors (Z). For visual clarity, we focus on a single input text with n tokens, not a batch of multiple inputs. Consequently, the three-dimensional input tensor is simplified to a two-dimensional matrix in this context. This approach allows for a more straightforward visualization and understanding of the processes involved. For consistency with later figures, the values in the attention matrix do not depict the real attention weights. (The numbers in this figure are truncated to two digits after the decimal point to reduce visual clutter. The values in each row should add up to 1.0 or 100%.)</h5>
  </div> 
  <div class="readable-text intended-text" id="p180"> 
   <p>Self-attention involves the trainable weight matrices <em>W</em><sub>q</sub>, <em>W</em><sub>k</sub>, and <em>W</em><sub>v</sub>. These matrices transform input data into queries, keys, and values, respectively, which are crucial components of the attention mechanism. As the model is exposed to more data during training, it adjusts these trainable weights, as we will see in upcoming chapters.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>We can improve the <code>SelfAttention_v1</code> implementation further by utilizing PyTorch‚Äôs <code>nn.Linear</code> layers, which effectively perform matrix multiplication when the bias units are disabled. Additionally, a significant advantage of using <code>nn.Linear</code> instead of manually implementing <code>nn.Parameter(torch.rand(...))</code> is that <code>nn.Linear</code> has an optimized weight initialization scheme, contributing to more stable and effective model training.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p182"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.2</span> A self-attention class using PyTorch‚Äôs Linear layers</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class SelfAttention_v2(nn.Module):
    def __init__(self, d_in, d_out, qkv_bias=False):
        super().__init__()
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

    def forward(self, x):
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1
        )
        context_vec = attn_weights @ values
        return context_vec</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p183"> 
   <p>You can use the <code>SelfAttention_v2</code> similar to <code>SelfAttention_v1</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p184"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(789)
sa_v2 = SelfAttention_v2(d_in, d_out)
print(sa_v2(inputs))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p186"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[-0.0739,  0.0713],
        [-0.0748,  0.0703],
        [-0.0749,  0.0702],
        [-0.0760,  0.0685],
        [-0.0763,  0.0679],
        [-0.0754,  0.0693]], grad_fn=&lt;MmBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p187"> 
   <p>Note that <code>SelfAttention_v1</code> and <code>SelfAttention_v2</code> give different outputs because they use different initial weights for the weight matrices since <code>nn.Linear</code> uses a more sophisticated weight initialization scheme.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p188"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2</h5> 
   </div> 
   <div class="readable-text" id="p189"> 
    <p>Note that <code>nn.Linear</code> in <code>SelfAttention_v2</code> uses a different weight initialization scheme as <code>nn.Parameter(torch.rand(d_in,</code> <code>d_out))</code> used in <code>SelfAttention_v1</code>, which causes both mechanisms to produce different results. To check that both implementations, <code>SelfAttention_v1</code> and <code>SelfAttention_v2</code>, are otherwise similar, we can transfer the weight matrices from a <code>SelfAttention_v2</code> object to a <code>SelfAttention_v1</code>, such that both objects then produce the same results.</p> 
   </div> 
   <div class="readable-text" id="p190"> 
    <p>Your task is to correctly assign the weights from an instance of <code>SelfAttention_v2</code> to an instance of <code>SelfAttention_v1</code>. To do this, you need to understand the relationship between the weights in both versions. (Hint: <code>nn.Linear</code> stores the weight matrix in a transposed form.) After the assignment, you should observe that both instances produce the same outputs.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p191"> 
   <p>Next, we will make enhancements to the self-attention mechanism, focusing specifically on incorporating causal and multi-head elements. The causal aspect involves modifying the attention mechanism to prevent the model from accessing future information in the sequence, which is crucial for tasks like language modeling, where each word prediction should only depend on previous words. </p> 
  </div> 
  <div class="readable-text intended-text" id="p192"> 
   <p>The multi-head component involves splitting the attention mechanism into multiple ‚Äúheads.‚Äù Each head learns different aspects of the data, allowing the model to simultaneously attend to information from different representation subspaces at different positions. This improves the model‚Äôs performance in complex tasks.</p> 
  </div> 
  <div class="readable-text" id="p193"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.5</span> Hiding future words with causal attention</h2> 
  </div> 
  <div class="readable-text" id="p194"> 
   <p>For many LLM tasks, you will want the self-attention mechanism to consider only the tokens that appear prior to the current position when predicting the next token in a sequence. Causal attention, also known as <em>masked attention</em>, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.</p> 
  </div> 
  <div class="readable-text intended-text" id="p195"> 
   <p>Now, we will modify the standard self-attention mechanism to create a <em>causal attention</em> mechanism, which is essential for developing an LLM in the subsequent chapters. To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text, as illustrated in figure 3.19. <span class="aframe-location"/>We mask out the attention weights above the diagonal, and we normalize the nonmasked attention weights such that the attention weights sum to 1 in each row. Later, we will implement this masking and normalization procedure in code.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p196">  
   <img alt="figure" src="../Images/3-19.png" width="927" height="431"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.19</span> In causal attention, we mask out the attention weights above the diagonal such that for a given input, the LLM can‚Äôt access future tokens when computing the context vectors using the attention weights. For example, for the word ‚Äújourney‚Äù in the second row, we only keep the attention weights for the words before (‚ÄúYour‚Äù) and in the current position (‚Äújourney‚Äù).</h5>
  </div> 
  <div class="readable-text" id="p197"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.5.1</span> Applying a causal attention mask</h3> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>Our next step is to implement the causal attention mask in code. To implement the steps to apply a causal attention mask to obtain the masked attention weights, as summarized in figure 3.20, let‚Äôs work with the attention scores and weights from the previous section to code the causal attention mechanism. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p199">  
   <img alt="figure" src="../Images/3-20.png" width="929" height="214"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.20</span> One way to obtain the masked attention weight matrix in causal attention is to apply the softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing the resulting matrix.</h5>
  </div> 
  <div class="readable-text" id="p200"> 
   <p>In the first step, we compute the attention weights using the softmax function as we have done previously:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p201"> 
   <div class="code-area-container"> 
    <pre class="code-area">queries = sa_v2.W_query(inputs)    <span class="aframe-location"/> #1
keys = sa_v2.W_key(inputs) 
attn_scores = queries @ keys.T
attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
print(attn_weights)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Reuses the query and key weight matrices of the SelfAttention_v2 object from the previous section for convenience
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p202"> 
   <p>This results in the following attention weights:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p203"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],
        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],
        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],
        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],
        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=&lt;SoftmaxBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p204"> 
   <p>We can implement the second step using PyTorch‚Äôs <code>tril</code> function to create a mask where the values above the diagonal are zero:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p205"> 
   <div class="code-area-container"> 
    <pre class="code-area">context_length = attn_scores.shape[0]
mask_simple = torch.tril(torch.ones(context_length, context_length))
print(mask_simple)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p206"> 
   <p>The resulting mask is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p207"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[1., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1.]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p208"> 
   <p>Now, we can multiply this mask with the attention weights to zero-out the values above the diagonal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p209"> 
   <div class="code-area-container"> 
    <pre class="code-area">masked_simple = attn_weights*mask_simple
print(masked_simple)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p210"> 
   <p>As we can see, the elements above the diagonal are successfully zeroed out:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p211"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],
        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],
        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=&lt;MulBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p212"> 
   <p>The third step is to renormalize the attention weights to sum up to 1 again in each row. We can achieve this by dividing each element in each row by the sum in each row:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p213"> 
   <div class="code-area-container"> 
    <pre class="code-area">row_sums = masked_simple.sum(dim=-1, keepdim=True)
masked_simple_norm = masked_simple / row_sums
print(masked_simple_norm)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p214"> 
   <p>The result is an attention weight matrix where the attention weights above the diagonal are zeroed-out, and the rows sum to 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p215"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],
        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],
        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=&lt;DivBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p216"> 
    <h5 class=" callout-container-h5 readable-text-h5">Information leakage</h5> 
   </div> 
   <div class="readable-text" id="p217"> 
    <p>When we apply a mask and then renormalize the attention weights, it might initially appear that information from future tokens (which we intend to mask) could still influence the current token because their values are part of the softmax calculation. However, the key insight is that when we renormalize the attention weights after masking, what we‚Äôre essentially doing is recalculating the softmax over a smaller subset (since masked positions don‚Äôt contribute to the softmax value).</p> 
   </div> 
   <div class="readable-text" id="p219"> 
    <p>The mathematical elegance of softmax is that despite initially including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified‚Äîthey don‚Äôt contribute to the softmax score in any meaningful way.</p> 
   </div> 
   <div class="readable-text" id="p220"> 
    <p>In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there‚Äôs no information leakage from future (or otherwise masked) tokens as we intended.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p221"> 
   <p>While we could wrap up our implementation of causal attention at this point, we can still improve it. Let‚Äôs take a mathematical property of the softmax function and implement the computation of the masked attention weights more efficiently in fewer steps, as shown in figure 3.21.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p222">  
   <img alt="figure" src="../Images/3-21.png" width="752" height="134"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.21</span> A more efficient way to obtain the masked attention weight matrix in causal attention is to mask the attention scores with negative infinity values before applying the softmax function.</h5>
  </div> 
  <div class="readable-text" id="p223"> 
   <p>The softmax function converts its inputs into a probability distribution. When negative infinity values (<code>-</code><span class="regular-symbol">‚àû</span>) are present in a row, the softmax function treats them as zero probability. (Mathematically, this is because <em>e‚Äâ‚Äâ</em><sup>‚Äì</sup><sup>‚àû</sup> approaches 0.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p224"> 
   <p>We can implement this more efficient masking ‚Äútrick‚Äù by creating a mask with 1s above the diagonal and then replacing these 1s with negative infinity (<code>-inf</code>) values:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p225"> 
   <div class="code-area-container"> 
    <pre class="code-area">mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
masked = attn_scores.masked_fill(mask.bool(), -torch.inf)
print(masked)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p226"> 
   <p>This results in the following mask:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p227"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],
        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],
        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],
        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],
        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],
        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],
       grad_fn=&lt;MaskedFillBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p228"> 
   <p>Now all we need to do is apply the softmax function to these masked results, and we are done:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p229"> 
   <div class="code-area-container"> 
    <pre class="code-area">attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)
print(attn_weights)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p230"> 
   <p>As we can see based on the output, the values in each row sum to 1, and no further normalization is necessary:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p231"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],
        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],
        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=&lt;SoftmaxBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p232"> 
   <p>We could now use the modified attention weights to compute the context vectors via <code>context_vec</code> <code>=</code> <code>attn_weights</code> <code>@</code> <code>values</code>, as in section 3.4. However, we will first cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs.</p> 
  </div> 
  <div class="readable-text" id="p233"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.5.2</span> Masking additional attention weights with dropout</h3> 
  </div> 
  <div class="readable-text" id="p234"> 
   <p><em>Dropout</em> in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively ‚Äúdropping‚Äù them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It‚Äôs important to emphasize that dropout is only used during training and is disabled afterward.</p> 
  </div> 
  <div class="readable-text intended-text" id="p235"> 
   <p>In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors. Here we will apply the dropout mask after computing the attention weights, as illustrated in figure 3.22, because it‚Äôs the more common variant in practice.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p236">  
   <img alt="figure" src="../Images/3-22.png" width="1100" height="1095"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.22</span> Using the causal attention mask (upper left), we apply an additional dropout mask (upper right) to zero out additional attention weights to reduce overfitting during training.</h5>
  </div> 
  <div class="readable-text intended-text" id="p237"> 
   <p>In the following code example, we use a dropout rate of 50%, which means masking out half of the attention weights. (When we train the GPT model in later chapters, we will use a lower dropout rate, such as 0.1 or 0.2.) We apply PyTorch‚Äôs dropout implementation first to a 6 √ó 6 tensor consisting of 1s for simplicity:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p238"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
dropout = torch.nn.Dropout(0.5)   <span class="aframe-location"/> #1
example = torch.ones(6, 6)     <span class="aframe-location"/> #2
print(dropout(example))</pre> 
    <div class="code-annotations-overlay-container">
     #1 We choose a dropout rate of 50%.
     <br/>#2 Here, we create a matrix of 1s.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p239"> 
   <p>As we can see, approximately half of the values are zeroed out:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p240"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[2., 2., 0., 2., 2., 0.],
        [0., 0., 0., 2., 0., 2.],
        [2., 2., 2., 2., 0., 2.],
        [0., 2., 2., 0., 0., 2.],
        [0., 2., 0., 2., 0., 2.],
        [0., 2., 2., 2., 2., 0.]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p241"> 
   <p>When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 = 2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases.</p> 
  </div> 
  <div class="readable-text intended-text" id="p242"> 
   <p>Now let‚Äôs apply dropout to the attention weight matrix itself:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p243"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
print(dropout(attn_weights))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p244"> 
   <p>The resulting attention weight matrix now has additional elements zeroed out and the remaining 1s rescaled:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p245"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[2.0000, 0.0000, 0 .0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],
        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],
       grad_fn=&lt;MulBackward0&gt;</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p246"> 
   <p>Note that the resulting dropout outputs may look different depending on your operating system; you can read more about this inconsistency here on the PyTorch issue tracker at <a href="https://github.com/pytorch/pytorch/issues/121595">https://github.com/pytorch/pytorch/issues/121595</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p247"> 
   <p>Having gained an understanding of causal attention and dropout masking, we can now develop a concise Python class. This class is designed to facilitate the efficient application of these two techniques.</p> 
  </div> 
  <div class="readable-text" id="p248"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.5.3</span> Implementing a compact causal attention class</h3> 
  </div> 
  <div class="readable-text" id="p249"> 
   <p>We will now incorporate the causal attention and dropout modifications into the <code>SelfAttention</code> Python class we developed in section 3.4. This class will then serve as a template for developing <em>multi-head attention</em>, which is the final attention class we will implement.</p> 
  </div> 
  <div class="readable-text intended-text" id="p250"> 
   <p>But before we begin, let‚Äôs ensure that the code can handle batches consisting of more than one input so that the <code>CausalAttention</code> class supports the batch outputs produced by the data loader we implemented in chapter 2.</p> 
  </div> 
  <div class="readable-text intended-text" id="p251"> 
   <p>For simplicity, to simulate such batch inputs, we duplicate the input text example:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p252"> 
   <div class="code-area-container"> 
    <pre class="code-area">batch = torch.stack((inputs, inputs), dim=0)
print(batch.shape)               <span class="aframe-location"/> #1</pre> 
    <div class="code-annotations-overlay-container">
     #1 Two inputs with six tokens each; each token has embedding dimension 3.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p253"> 
   <p>This results in a three-dimensional tensor consisting of two input texts with six tokens each, where each token is a three-dimensional embedding vector:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p254"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.Size([2, 6, 3])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p255"> 
   <p>The following <code>CausalAttention</code> class is similar to the <code>SelfAttention</code> class we implemented earlier, except that we added the dropout and causal mask components.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p256"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.3</span> A compact causal attention class</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class CausalAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length,
                dropout, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.dropout = nn.Dropout(dropout)           <span class="aframe-location"/> #1
        self.register_buffer(
           'mask',
           torch.triu(torch.ones(context_length, context_length),
           diagonal=1)
        )            <span class="aframe-location"/> #2

    def forward(self, x):
        b, num_tokens, d_in = x.shape                  <span class="aframe-location"/> #3
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        attn_scores = queries @ keys.transpose(1, 2)   
        attn_scores.masked_fill_(                   <span class="aframe-location"/> #4
            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) 
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1
        )
        attn_weights = self.dropout(attn_weights)

        context_vec = attn_weights @ values
        return context_vec</pre> 
    <div class="code-annotations-overlay-container">
     #1 Compared to the previous SelfAttention_v1 class, we added a dropout layer.
     <br/>#2 The register_buffer call is also a new addition (more information is provided in the following text).
     <br/>#3 We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).
     <br/>#4 In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory copies.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p257"> 
   <p>While all added code lines should be familiar at this point, we now added a <code>self .register_buffer()</code> call in the <code>__init__</code> method. The use of <code>register_buffer</code> in PyTorch is not strictly necessary for all use cases but offers several advantages here. For instance, when we use the <code>CausalAttention</code> class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training our LLM. This means we don‚Äôt need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors.</p> 
  </div> 
  <div class="readable-text intended-text" id="p258"> 
   <p>We can use the <code>CausalAttention</code> class as follows, similar to <code>SelfAttention</code> previously:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p259"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
context_length = batch.shape[1]
ca = CausalAttention(d_in, d_out, context_length, 0.0)
context_vecs = ca(batch)
print("context_vecs.shape:", context_vecs.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p260"> 
   <p>The resulting context vector is a three-dimensional tensor where each token is now represented by a two-dimensional embedding:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p261"> 
   <div class="code-area-container"> 
    <pre class="code-area">context_vecs.shape: torch.Size([2, 6, 2])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p262"> 
   <p>Figure 3.23 summarizes what we have accomplished so far. We have focused on the concept and implementation of causal attention in neural networks. Next, we will expand on this concept and implement a multi-head attention module that implements several causal attention mechanisms in parallel.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p263">  
   <img alt="figure" src="../Images/3-23.png" width="1012" height="239"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.23</span> Here‚Äôs what we‚Äôve done so far. We began with a simplified attention mechanism, added trainable weights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code multi-head attention, which we will use in our LLM.</h5>
  </div> 
  <div class="readable-text" id="p264"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.6</span> Extending single-head attention to multi-head attention</h2> 
  </div> 
  <div class="readable-text" id="p265"> 
   <p>Our final step will be to extend the previously implemented causal attention class over multiple heads. This is also called <em>multi-head attention</em>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p266"> 
   <p>The term ‚Äúmulti-head‚Äù refers to dividing the attention mechanism into multiple ‚Äúheads,‚Äù each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.</p> 
  </div> 
  <div class="readable-text intended-text" id="p267"> 
   <p>We will tackle this expansion from causal attention to multi-head attention. First, we will intuitively build a multi-head attention module by stacking multiple <code>CausalAttention</code> modules. Then we will then implement the same multi-head attention module in a more complicated but more computationally efficient way.</p> 
  </div> 
  <div class="readable-text" id="p268"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.6.1</span> Stacking multiple single-head attention layers</h3> 
  </div> 
  <div class="readable-text" id="p269"> 
   <p>In practical terms, implementing multi-head attention involves creating multiple instances of the self-attention mechanism (see figure 3.18), each with its own weights, and then combining their outputs. Using multiple instances of the self-attention mechanism can be computationally intensive, but it‚Äôs crucial for the kind of complex pattern recognition that models like transformer-based LLMs are known for. </p> 
  </div> 
  <div class="readable-text intended-text" id="p270"> 
   <p>Figure 3.24 illustrates the structure of a multi-head attention module, which consists of multiple single-head attention modules, as previously depicted in figure 3.18, stacked on top of each other.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p271">  
   <img alt="figure" src="../Images/3-24.png" width="1005" height="724"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.24</span> The multi-head attention module includes two single-head attention modules stacked on top of each other. So, instead of using a single matrix W<sub>v</sub> for computing the value matrices, in a multi-head attention module with two heads, we now have two value weight matrices: W<sub>v1</sub> and W<sub>v2</sub>. The same applies to the other weight matrices, W<sub>Q</sub> and W<sub>k</sub>. We obtain two sets of context vectors Z<sub>1</sub> and Z<sub>2</sub> that we can combine into a single context vector matrix Z.</h5>
  </div> 
  <div class="readable-text" id="p272"> 
   <p>As mentioned before, the main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections‚Äîthe results of multiplying the input data (like the query, key, and value vectors in attention mechanisms) by a weight matrix. In code, we can achieve this by implementing a simple <code>MultiHeadAttentionWrapper</code> class that stacks multiple instances of our previously implemented <code>CausalAttention</code> module.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p273"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.4</span> A wrapper class to implement multi-head attention</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class MultiHeadAttentionWrapper(nn.Module):
    def __init__(self, d_in, d_out, context_length,
                 dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.heads = nn.ModuleList(
            [CausalAttention(
                 d_in, d_out, context_length, dropout, qkv_bias
             ) 
             for _ in range(num_heads)]
        )

    def forward(self, x):
        return torch.cat([head(x) for head in self.heads], dim=-1)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p274"> 
   <p>For example, if we use this <code>MultiHeadAttentionWrapper</code> class with two attention heads (via <code>num_heads=2</code>) and <code>CausalAttention</code> output dimension <code>d_out=2</code>, we get a four-dimensional context vector (<code>d_out*num_heads=4</code>), as depicted in figure 3.25.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p275">  
   <img alt="figure" src="../Images/3-25.png" width="769" height="364"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.25</span> Using the <code>MultiHeadAttentionWrapper</code>, we specified the number of attention heads (<code>num_heads</code>). If we set <code>num_heads=2</code>, as in this example, we obtain a tensor with two sets of context vector matrices. In each context vector matrix, the rows represent the context vectors corresponding to the tokens, and the columns correspond to the embedding dimension specified via <code>d_out=4</code>. We concatenate these context vector matrices along the column dimension. Since we have two attention heads and an embedding dimension of 2, the final embedding dimension is 2 √ó 2 = 4.</h5>
  </div> 
  <div class="readable-text" id="p276"> 
   <p>To illustrate this further with a concrete example, we can use the <code>MultiHeadAttentionWrapper</code> class similar to the <code>CausalAttention</code> class before:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p277"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
context_length = batch.shape[1] # This is the number of tokens
d_in, d_out = 3, 2
mha = MultiHeadAttentionWrapper(
    d_in, d_out, context_length, 0.0, num_heads=2
)
context_vecs = mha(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p278"> 
   <p>This results in the following tensor representing the context vectors:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p279"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]],

        [[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=&lt;CatBackward0&gt;)
context_vecs.shape: torch.Size([2, 6, 4])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p280"> 
   <p>The first dimension of the resulting <code>context_vecs</code> tensor is 2 since we have two input texts (the input texts are duplicated, which is why the context vectors are exactly the same for those). The second dimension refers to the 6 tokens in each input. The third dimension refers to the four-dimensional embedding of each token.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p281"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 3.2 Returning two-dimensional embedding vectors </h5> 
   </div> 
   <div class="readable-text" id="p282"> 
    <p>Change the input arguments for the <code>MultiHeadAttentionWrapper(...,</code> <code>num_ heads=2)</code> call such that the output context vectors are two-dimensional instead of four dimensional while keeping the setting <code>num_heads=2</code>. Hint: You don‚Äôt have to modify the class implementation; you just have to change one of the other input arguments.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p283"> 
   <p>Up to this point, we have implemented a <code>MultiHeadAttentionWrapper</code> that combined multiple single-head attention modules. However, these are processed sequentially via <code>[head(x)</code> <code>for</code> <code>head</code> <code>in</code> <code>self.heads]</code> in the forward method. We can improve this implementation by processing the heads in parallel. One way to achieve this is by computing the outputs for all attention heads simultaneously via matrix multiplication.</p> 
  </div> 
  <div class="readable-text" id="p284"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.6.2</span> Implementing multi-head attention with weight splits</h3> 
  </div> 
  <div class="readable-text" id="p285"> 
   <p>So far, we have created a <code>MultiHeadAttentionWrapper</code> to implement multi-head attention by stacking multiple single-head attention modules. This was done by instantiating and combining several <code>CausalAttention</code> objects.</p> 
  </div> 
  <div class="readable-text intended-text" id="p286"> 
   <p>Instead of maintaining two separate classes, <code>MultiHeadAttentionWrapper</code> and <code>CausalAttention</code>, we can combine these concepts into a single <code>MultiHeadAttention</code> class. Also, in addition to merging the <code>MultiHeadAttentionWrapper</code> with the <code>CausalAttention</code> code, we will make some other modifications to implement multi-head attention more efficiently.</p> 
  </div> 
  <div class="readable-text intended-text" id="p287"> 
   <p>In the <code>MultiHeadAttentionWrapper</code>, multiple heads are implemented by creating a list of <code>CausalAttention</code> objects (<code>self.heads</code>), each representing a separate attention head. The <code>CausalAttention</code> class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following <code>MultiHeadAttention</code> class integrates the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention.</p> 
  </div> 
  <div class="readable-text intended-text" id="p288"> 
   <p>Let‚Äôs take a look at the <code>MultiHeadAttention</code> class before we discuss it further.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p289"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.5</span> An efficient multi-head attention class</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, 
                 context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert (d_out % num_heads == 0), \
            "d_out must be divisible by num_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads   <span class="aframe-location"/> #1
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)   <span class="aframe-location"/> #2
        self.dropout = nn.Dropout(dropout)
        self.register_buffer(
            "mask",
            torch.triu(torch.ones(context_length, context_length),
                       diagonal=1)
        )

    def forward(self, x):
        b, num_tokens, d_in = x.shape
        keys = self.W_key(x)        <span class="aframe-location"/> #3
        queries = self.W_query(x)    #3
        values = self.W_value(x)     #3

        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      <span class="aframe-location"/> #4
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  
        queries = queries.view(                                             
            b, num_tokens, self.num_heads, self.head_dim                    
        )                                                                   

        keys = keys.transpose(1, 2)         <span class="aframe-location"/> #5
        queries = queries.transpose(1, 2)    #5
        values = values.transpose(1, 2)      #5

        attn_scores = queries @ keys.transpose(2, 3)  <span class="aframe-location"/> #6
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]   <span class="aframe-location"/> #7

        attn_scores.masked_fill_(mask_bool, -torch.inf)    <span class="aframe-location"/> #8

        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        context_vec = (attn_weights @ values).transpose(1, 2)  <span class="aframe-location"/> #9
 #10
        context_vec = context_vec.contiguous().view(
            b, num_tokens, self.d_out
        )
        context_vec = self.out_proj(context_vec)   <span class="aframe-location"/> #11
        return context_vec</pre> 
    <div class="code-annotations-overlay-container">
     #1 Reduces the projection dim to match the desired output dim
     <br/>#2 Uses a Linear layer to combine head outputs
     <br/>#3 Tensor shape: (b, num_tokens, d_out)
     <br/>#4 We implicitly split the matrix by adding a num_heads dimension. Then we unroll the last dim: (b, num_tokens, d_out) -&amp;gt; (b, num_tokens, num_heads, head_dim).
     <br/>#5 Transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)
     <br/>#6 Computes dot product for each head
     <br/>#7 Masks truncated to the number of tokens
     <br/>#8 Uses the mask to fill attention scores
     <br/>#9 Tensor shape: (b, num_tokens, n_heads, head_dim)
     <br/>#10 Combines heads, where self.d_out = self.num_heads * self.head_dim
     <br/>#11 Adds an optional linear projection
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p290"> 
   <p>Even though the reshaping (<code>.view</code>) and transposing (<code>.transpose</code>) of tensors inside the <code>MultiHeadAttention</code> class looks very mathematically complicated, the <code>MultiHeadAttention</code> class implements the same concept as the <code>MultiHeadAttentionWrapper</code> earlier. </p> 
  </div> 
  <div class="readable-text intended-text" id="p291"> 
   <p>On a big-picture level, in the previous <code>MultiHeadAttentionWrapper</code>, we stacked multiple single-head attention layers that we combined into a multi-head attention layer. The <code>MultiHeadAttention</code> class takes an integrated approach. It starts with a multi-head layer and then internally splits this layer into individual attention heads, as illustrated in figure 3.26.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p292">  
   <img alt="figure" src="../Images/3-26.png" width="1100" height="942"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 3.26</span> In the <code>MultiHeadAttentionWrapper</code> class with two attention heads, we initialized two weight matrices, W<sub>q1</sub> and W<sub>q2</sub>, and computed two query matrices, Q<sub>1</sub> and Q<sub>2</sub> (top). In the <code>MultiheadAttention</code> class, we initialize one larger weight matrix W<sub>q</sub>, only perform one matrix multiplication with the inputs to obtain a query matrix Q, and then split the query matrix into Q<sub>1</sub> and Q<sub>2</sub> (bottom). We do the same for the keys and values, which are not shown to reduce visual clutter.</h5>
  </div> 
  <div class="readable-text intended-text" id="p293"> 
   <p>The splitting of the query, key, and value tensors is achieved through tensor reshaping and transposing operations using PyTorch‚Äôs <code>.view</code> and <code>.transpose</code> methods. The input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads. </p> 
  </div> 
  <div class="readable-text intended-text" id="p294"> 
   <p>The key operation is to split the <code>d_out</code> dimension into <code>num_heads</code> and <code>head_dim</code>, where <code>head_dim</code> <code>=</code> <code>d_out</code> <code>/</code> <code>num_heads</code>. This splitting is then achieved using the <code>.view</code> method: a tensor of dimensions <code>(b,</code> <code>num_tokens,</code> <code>d_out)</code> is reshaped to dimension <code>(b,</code> <code>num_tokens,</code> <code>num_heads,</code> <code>head_dim)</code>.</p> 
  </div> 
  <div class="readable-text" id="p295"> 
   <p>The tensors are then transposed to bring the <code>num_heads</code> dimension before the <code>num_ tokens</code> dimension, resulting in a shape of <code>(b,</code> <code>num_heads,</code> <code>num_tokens,</code> <code>head_dim)</code>. This transposition is crucial for correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications efficiently.</p> 
  </div> 
  <div class="readable-text intended-text" id="p296"> 
   <p>To illustrate this batched matrix multiplication, suppose we have the following tensor:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p297"> 
   <div class="code-area-container"> 
    <pre class="code-area">a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],   <span class="aframe-location"/> #1
                    [0.8993, 0.0390, 0.9268, 0.7388],
                    [0.7179, 0.7058, 0.9156, 0.4340]],

                   [[0.0772, 0.3565, 0.1479, 0.5331],
                    [0.4066, 0.2318, 0.4545, 0.9737],
                    [0.4606, 0.5159, 0.4220, 0.5786]]]])</pre> 
    <div class="code-annotations-overlay-container">
     #1 The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4).
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p298"> 
   <p>Now we perform a batched matrix multiplication between the tensor itself and a view of the tensor where we transposed the last two dimensions, <code>num_tokens</code> and <code>head_dim</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p299"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(a @ a.transpose(2, 3))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p300"> 
   <p>The result is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p301"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[[[1.3208, 1.1631, 1.2879],
          [1.1631, 2.2150, 1.8424],
          [1.2879, 1.8424, 2.0402]],

         [[0.4391, 0.7003, 0.5903],
          [0.7003, 1.3737, 1.0620],
          [0.5903, 1.0620, 0.9912]]]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p302"> 
   <p>In this case, the matrix multiplication implementation in PyTorch handles the four-dimensional input tensor so that the matrix multiplication is carried out between the two last dimensions <code>(num_tokens,</code> <code>head_dim)</code> and then repeated for the individual heads. </p> 
  </div> 
  <div class="readable-text intended-text" id="p303"> 
   <p>For instance, the preceding becomes a more compact way to compute the matrix multiplication for each head separately:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p304"> 
   <div class="code-area-container"> 
    <pre class="code-area">first_head = a[0, 0, :, :]
first_res = first_head @ first_head.T
print("First head:\n", first_res)

second_head = a[0, 1, :, :]
second_res = second_head @ second_head.T
print("\nSecond head:\n", second_res)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p305"> 
   <p>The results are exactly the same results as those we obtained when using the batched matrix multiplication <code>print(a</code> <code>@</code> <code>a.transpose(2,</code> <code>3))</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p306"> 
   <div class="code-area-container"> 
    <pre class="code-area">First head:
 tensor([[1.3208, 1.1631, 1.2879],
        [1.1631, 2.2150, 1.8424],
        [1.2879, 1.8424, 2.0402]])

Second head:
 tensor([[0.4391, 0.7003, 0.5903],
        [0.7003, 1.3737, 1.0620],
        [0.5903, 1.0620, 0.9912]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p307"> 
   <p>Continuing with <code>MultiHeadAttention</code>, after computing the attention weights and context vectors, the context vectors from all heads are transposed back to the shape <code>(b,</code> <code>num_tokens,</code> <code>num_heads,</code> <code>head_dim)</code>. These vectors are then reshaped (flattened) into the shape <code>(b,</code> <code>num_tokens,</code> <code>d_out)</code>, effectively combining the outputs from all heads.</p> 
  </div> 
  <div class="readable-text intended-text" id="p308"> 
   <p>Additionally, we added an output projection layer (<code>self.out_proj</code>) to <code>MultiHeadAttention</code> after combining the heads, which is not present in the <code>CausalAttention</code> class. This output projection layer is not strictly necessary (see appendix B for more details), but it is commonly used in many LLM architectures, which is why I added it here for completeness.</p> 
  </div> 
  <div class="readable-text intended-text" id="p309"> 
   <p>Even though the <code>MultiHeadAttention</code> class looks more complicated than the <code>MultiHeadAttentionWrapper</code> due to the additional reshaping and transposition of tensors, it is more efficient. The reason is that we only need one matrix multiplication to compute the keys, for instance, <code>keys</code> <code>=</code> <code>self.W_key(x)</code> (the same is true for the queries and values). In the <code>MultiHeadAttentionWrapper</code>, we needed to repeat this matrix multiplication, which is computationally one of the most expensive steps, for each attention head.</p> 
  </div> 
  <div class="readable-text intended-text" id="p310"> 
   <p>The <code>MultiHeadAttention</code> class can be used similar to the <code>SelfAttention</code> and <code>CausalAttention</code> classes we implemented earlier:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p311"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
batch_size, context_length, d_in = batch.shape
d_out = 2
mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)
context_vecs = mha(batch)
print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p312"> 
   <p>The results show that the output dimension is directly controlled by the <code>d_out</code> argument:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p313"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[[0.3190, 0.4858],
         [0.2943, 0.3897],
         [0.2856, 0.3593],
         [0.2693, 0.3873],
         [0.2639, 0.3928],
         [0.2575, 0.4028]],

        [[0.3190, 0.4858],
         [0.2943, 0.3897],
         [0.2856, 0.3593],
         [0.2693, 0.3873],
         [0.2639, 0.3928],
         [0.2575, 0.4028]]], grad_fn=&lt;ViewBackward0&gt;)
context_vecs.shape: torch.Size([2, 6, 2])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p314"> 
   <p>We have now implemented the <code>MultiHeadAttention</code> class that we will use when we implement and train the LLM. Note that while the code is fully functional, I used relatively small embedding sizes and numbers of attention heads to keep the outputs readable.</p> 
  </div> 
  <div class="readable-text intended-text" id="p315"> 
   <p>For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion parameters) has 25 attention heads and a context vector embedding size of 1,600. The embedding sizes of the token inputs and context embeddings are the same in GPT models (<code>d_in</code> <code>=</code> <code>d_out</code>).</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p316"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 3.3 Initializing GPT-2 size attention modules</h5> 
   </div> 
   <div class="readable-text" id="p317"> 
    <p>Using the <code>MultiHeadAttention</code> class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1,024 tokens.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p318"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p319"> Attention mechanisms transform input elements into enhanced context vector representations that incorporate information about all inputs. </li> 
   <li class="readable-text" id="p320"> A self-attention mechanism computes the context vector representation as a weighted sum over the inputs. </li> 
   <li class="readable-text" id="p321"> In a simplified attention mechanism, the attention weights are computed via dot products. </li> 
   <li class="readable-text" id="p322"> A dot product is a concise way of multiplying two vectors element-wise and then summing the products. </li> 
   <li class="readable-text" id="p323"> Matrix multiplications, while not strictly required, help us implement computations more efficiently and compactly by replacing nested <code>for</code> loops. </li> 
   <li class="readable-text" id="p324"> In self-attention mechanisms used in LLMs, also called scaled-dot product attention, we include trainable weight matrices to compute intermediate transformations of the inputs: queries, values, and keys. </li> 
   <li class="readable-text" id="p325"> When working with LLMs that read and generate text from left to right, we add a causal attention mask to prevent the LLM from accessing future tokens. </li> 
   <li class="readable-text" id="p326"> In addition to causal attention masks to zero-out attention weights, we can add a dropout mask to reduce overfitting in LLMs. </li> 
   <li class="readable-text" id="p327"> The attention modules in transformer-based LLMs involve multiple instances of causal attention, which is called multi-head attention. </li> 
   <li class="readable-text" id="p328"> We can create a multi-head attention module by stacking multiple instances of causal attention modules. </li> 
   <li class="readable-text" id="p329"> A more efficient way of creating multi-head attention modules involves batched matrix multiplications. </li> 
  </ul>
 </div></div></body></html>