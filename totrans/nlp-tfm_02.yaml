- en: Chapter 1\. Hello Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2017, researchers at Google published a paper that proposed a novel neural
    network architecture for sequence modeling.^([1](ch01.xhtml#idm46238735114624))
    Dubbed the *Transformer*, this architecture outperformed recurrent neural networks
    (RNNs) on machine translation tasks, both in terms of translation quality and
    training cost.
  prefs: []
  type: TYPE_NORMAL
- en: In parallel, an effective transfer learning method called ULMFiT showed that
    training long short-term memory (LSTM) networks on a very large and diverse corpus
    could produce state-of-the-art text classifiers with little labeled data.^([2](ch01.xhtml#idm46238735221744))
  prefs: []
  type: TYPE_NORMAL
- en: 'These advances were the catalysts for two of today’s most well-known transformers:
    the Generative Pretrained Transformer (GPT)^([3](ch01.xhtml#idm46238735216480))
    and Bidirectional Encoder Representations from Transformers (BERT).^([4](ch01.xhtml#idm46238735215072))
    By combining the Transformer architecture with unsupervised learning, these models
    removed the need to train task-specific architectures from scratch and broke almost
    every benchmark in NLP by a significant margin. Since the release of GPT and BERT,
    a zoo of transformer models has emerged; a timeline of the most prominent entries
    is shown in [Figure 1-1](#transformer-timeline).'
  prefs: []
  type: TYPE_NORMAL
- en: '![transformer-timeline](Images/nlpt_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. The transformers timeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'But we’re getting ahead of ourselves. To understand what is novel about transformers,
    we first need to explain:'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder-decoder framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter we’ll introduce the core concepts that underlie the pervasiveness
    of transformers, take a tour of some of the tasks that they excel at, and conclude
    with a look at the Hugging Face ecosystem of tools and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by exploring the encoder-decoder framework and the architectures
    that preceded the rise of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: The Encoder-Decoder Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prior to transformers, recurrent architectures such as LSTMs were the state
    of the art in NLP. These architectures contain a feedback loop in the network
    connections that allows information to propagate from one step to another, making
    them ideal for modeling sequential data like text. As illustrated on the left
    side of [Figure 1-2](#rnn), an RNN receives some input (which could be a word
    or character), feeds it through the network, and outputs a vector called the *hidden
    state*. At the same time, the model feeds some information back to itself through
    the feedback loop, which it can then use in the next step. This can be more clearly
    seen if we “unroll” the loop as shown on the right side of [Figure 1-2](#rnn):
    the RNN passes information about its state at each step to the next operation
    in the sequence. This allows an RNN to keep track of information from previous
    steps, and use it for its output predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![rnn](Images/nlpt_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Unrolling an RNN in time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These architectures were (and continue to be) widely used for NLP tasks, speech
    processing, and time series. You can find a wonderful exposition of their capabilities
    in Andrej Karpathy’s blog post, [“The Unreasonable Effectiveness of Recurrent
    Neural Networks”](https://oreil.ly/Q55o0).
  prefs: []
  type: TYPE_NORMAL
- en: One area where RNNs played an important role was in the development of machine
    translation systems, where the objective is to map a sequence of words in one
    language to another. This kind of task is usually tackled with an *encoder-decoder*
    or *sequence-to-sequence* architecture,^([5](ch01.xhtml#idm46238728352576)) which
    is well suited for situations where the input and output are both sequences of
    arbitrary length. The job of the encoder is to encode the information from the
    input sequence into a numerical representation that is often called the *last
    hidden state*. This state is then passed to the decoder, which generates the output
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the encoder and decoder components can be any kind of neural network
    architecture that can model sequences. This is illustrated for a pair of RNNs
    in [Figure 1-3](#enc-dec), where the English sentence “Transformers are great!”
    is encoded as a hidden state vector that is then decoded to produce the German
    translation “Transformer sind grossartig!” The input words are fed sequentially
    through the encoder and the output words are generated one at a time, from top
    to bottom.
  prefs: []
  type: TYPE_NORMAL
- en: '![enc-dec](Images/nlpt_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. An encoder-decoder architecture with a pair of RNNs (in general,
    there are many more recurrent layers than those shown here)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Although elegant in its simplicity, one weakness of this architecture is that
    the final hidden state of the encoder creates an *information bottleneck*: it
    has to represent the meaning of the whole input sequence because this is all the
    decoder has access to when generating the output. This is especially challenging
    for long sequences, where information at the start of the sequence might be lost
    in the process of compressing everything to a single, fixed representation.'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is a way out of this bottleneck by allowing the decoder to
    have access to all of the encoder’s hidden states. The general mechanism for this
    is called *attention*,^([6](ch01.xhtml#idm46238728612528)) and it is a key component
    in many modern neural network architectures. Understanding how attention was developed
    for RNNs will put us in good shape to understand one of the main building blocks
    of the Transformer architecture. Let’s take a deeper look.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main idea behind attention is that instead of producing a single hidden
    state for the input sequence, the encoder outputs a hidden state at each step
    that the decoder can access. However, using all the states at the same time would
    create a huge input for the decoder, so some mechanism is needed to prioritize
    which states to use. This is where attention comes in: it lets the decoder assign
    a different amount of weight, or “attention,” to each of the encoder states at
    every decoding timestep. This process is illustrated in [Figure 1-4](#enc-dec-attn),
    where the role of attention is shown for predicting the third token in the output
    sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![enc-dec-attn](Images/nlpt_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. An encoder-decoder architecture with an attention mechanism for
    a pair of RNNs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By focusing on which input tokens are most relevant at each timestep, these
    attention-based models are able to learn nontrivial alignments between the words
    in a generated translation and those in a source sentence. For example, [Figure 1-5](#attention-alignment)
    visualizes the attention weights for an English to French translation model, where
    each pixel denotes a weight. The figure shows how the decoder is able to correctly
    align the words “zone” and “Area”, which are ordered differently in the two languages.
  prefs: []
  type: TYPE_NORMAL
- en: '![attention-alignment](Images/nlpt_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. RNN encoder-decoder alignment of words in English and the generated
    translation in French (courtesy of Dzmitry Bahdanau)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Although attention enabled the production of much better translations, there
    was still a major shortcoming with using recurrent models for the encoder and
    decoder: the computations are inherently sequential and cannot be parallelized
    across the input sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the transformer, a new modeling paradigm was introduced: dispense with
    recurrence altogether, and instead rely entirely on a special form of attention
    called *self-attention*. We’ll cover self-attention in more detail in [Chapter 3](ch03.xhtml#chapter_anatomy),
    but the basic idea is to allow attention to operate on all the states in the *same
    layer* of the neural network. This is shown in [Figure 1-6](#transformer-self-attn),
    where both the encoder and the decoder have their own self-attention mechanisms,
    whose outputs are fed to feed-forward neural networks (FF NNs). This architecture
    can be trained much faster than recurrent models and paved the way for many of
    the recent breakthroughs in NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![transformer-self-attn](Images/nlpt_0106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. Encoder-decoder architecture of the original Transformer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the original Transformer paper, the translation model was trained from scratch
    on a large corpus of sentence pairs in various languages. However, in many practical
    applications of NLP we do not have access to large amounts of labeled text data
    to train our models on. A final piece was missing to get the transformer revolution
    started: transfer learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is nowadays common practice in computer vision to use transfer learning to
    train a convolutional neural network like ResNet on one task, and then adapt it
    to or *fine-tune* it on a new task. This allows the network to make use of the
    knowledge learned from the original task. Architecturally, this involves splitting
    the model into of a *body* and a *head*, where the head is a task-specific network.
    During training, the weights of the body learn broad features of the source domain,
    and these weights are used to initialize a new model for the new task.^([7](ch01.xhtml#idm46238728434256))
    Compared to traditional supervised learning, this approach typically produces
    high-quality models that can be trained much more efficiently on a variety of
    downstream tasks, and with much less labeled data. A comparison of the two approaches
    is shown in [Figure 1-7](#transfer-learning).
  prefs: []
  type: TYPE_NORMAL
- en: '![transfer-learning](Images/nlpt_0107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. Comparison of traditional supervised learning (left) and transfer
    learning (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In computer vision, the models are first trained on large-scale datasets such
    as [ImageNet](https://image-net.org), which contain millions of images. This process
    is called *pretraining* and its main purpose is to teach the models the basic
    features of images, such as edges or colors. These pretrained models can then
    be fine-tuned on a downstream task such as classifying flower species with a relatively
    small number of labeled examples (usually a few hundred per class). Fine-tuned
    models typically achieve a higher accuracy than supervised models trained from
    scratch on the same amount of labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Although transfer learning became the standard approach in computer vision,
    for many years it was not clear what the analogous pretraining process was for
    NLP. As a result, NLP applications typically required large amounts of labeled
    data to achieve high performance. And even then, that performance did not compare
    to what was achieved in the vision domain.
  prefs: []
  type: TYPE_NORMAL
- en: In 2017 and 2018, several research groups proposed new approaches that finally
    made transfer learning work for NLP. It started with an insight from researchers
    at OpenAI who obtained strong performance on a sentiment classification task by
    using features extracted from unsupervised pretraining.^([8](ch01.xhtml#idm46238727454704))
    This was followed by ULMFiT, which introduced a general framework to adapt pretrained
    LSTM models for various tasks.^([9](ch01.xhtml#idm46238727453120))
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in [Figure 1-8](#ulmfit), ULMFiT involves three main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial training objective is quite simple: predict the next word based
    on the previous words. This task is referred to as *language modeling*. The elegance
    of this approach lies in the fact that no labeled data is required, and one can
    make use of abundantly available text from sources such as Wikipedia.^([10](ch01.xhtml#idm46238727446112))'
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation
  prefs: []
  type: TYPE_NORMAL
- en: Once the language model is pretrained on a large-scale corpus, the next step
    is to adapt it to the in-domain corpus (e.g., from Wikipedia to the IMDb corpus
    of movie reviews, as in [Figure 1-8](#ulmfit)). This stage still uses language
    modeling, but now the model has to predict the next word in the target corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: In this step, the language model is fine-tuned with a classification layer for
    the target task (e.g., classifying the sentiment of movie reviews in [Figure 1-8](#ulmfit)).
  prefs: []
  type: TYPE_NORMAL
- en: '![ulmfit](Images/nlpt_0108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. The ULMFiT process (courtesy of Jeremy Howard)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By introducing a viable framework for pretraining and transfer learning in
    NLP, ULMFiT provided the missing piece to make transformers take off. In 2018,
    two transformers were released that combined self-attention with transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT
  prefs: []
  type: TYPE_NORMAL
- en: Uses only the decoder part of the Transformer architecture, and the same language
    modeling approach as ULMFiT. GPT was pretrained on the BookCorpus,^([11](ch01.xhtml#idm46238728418272))
    which consists of 7,000 unpublished books from a variety of genres including Adventure,
    Fantasy, and Romance.
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs: []
  type: TYPE_NORMAL
- en: Uses the encoder part of the Transformer architecture, and a special form of
    language modeling called *masked language modeling*. The objective of masked language
    modeling is to predict randomly masked words in a text. For example, given a sentence
    like “I looked at my `[MASK]` and saw that `[MASK]` was late.” the model needs
    to predict the most likely candidates for the masked words that are denoted by
    `[MASK]`. BERT was pretrained on the BookCorpus and English Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: GPT and BERT set a new state of the art across a variety of NLP benchmarks and
    ushered in the age of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: However, with different research labs releasing their models in incompatible
    frameworks (PyTorch or TensorFlow), it wasn’t always easy for NLP practitioners
    to port these models to their own applications. With the release of ![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers](https://oreil.ly/Z79jF), a unified API across more than 50 architectures
    was progressively built. This library catalyzed the explosion of research into
    transformers and quickly trickled down to NLP practitioners, making it easy to
    integrate these models into many real-life applications today. Let’s have a look!
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face Transformers: Bridging the Gap'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Applying a novel machine learning architecture to a new task can be a complex
    undertaking, and usually involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the model architecture in code, typically based on PyTorch or TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the pretrained weights (if available) from a server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the inputs, pass them through the model, and apply some task-specific
    postprocessing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement dataloaders and define loss functions and optimizers to train the
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these steps requires custom logic for each model and task. Traditionally
    (but not always!), when research groups publish a new article, they will also
    release the code along with the model weights. However, this code is rarely standardized
    and often requires days of engineering to adapt to new use cases.
  prefs: []
  type: TYPE_NORMAL
- en: This is where ![nlpt_pin01](Images/nlpt_pin01.png) Transformers comes to the
    NLP practitioner’s rescue! It provides a standardized interface to a wide range
    of transformer models as well as code and tools to adapt these models to new use
    cases. The library currently supports three major deep learning frameworks (PyTorch,
    TensorFlow, and JAX) and allows you to easily switch between them. In addition,
    it provides task-specific heads so you can easily fine-tune transformers on downstream
    tasks such as text classification, named entity recognition, and question answering.
    This reduces the time it takes a practitioner to train and test a handful of models
    from a week to a single afternoon!
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see this for yourself in the next section, where we show that with just
    a few lines of code, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers can be
    applied to tackle some of the most common NLP applications that you’re likely
    to encounter in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: A Tour of Transformer Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every NLP task starts with a piece of text, like the following made-up customer
    feedback about a certain online order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Depending on your application, the text you’re working with could be a legal
    contract, a product description, or something else entirely. In the case of customer
    feedback, you would probably like to know whether the feedback is positive or
    negative. This task is called *sentiment analysis* and is part of the broader
    topic of *text classification* that we’ll explore in [Chapter 2](ch02.xhtml#chapter_classification).
    For now, let’s have a look at what it takes to extract the sentiment from our
    piece of text using ![nlpt_pin01](Images/nlpt_pin01.png) Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Text Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ll see in later chapters, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    has a layered API that allows you to interact with the library at various levels
    of abstraction. In this chapter we’ll start with *pipelines*, which abstract away
    all the steps needed to convert raw text into a set of predictions from a fine-tuned
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we instantiate a pipeline
    by calling the `pipeline()` function and providing the name of the task we are
    interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first time you run this code you’ll see a few progress bars appear because
    the pipeline automatically downloads the model weights from the [Hugging Face
    Hub](https://oreil.ly/zLK11). The second time you instantiate the pipeline, the
    library will notice that you’ve already downloaded the weights and will use the
    cached version instead. By default, the `text-classification` pipeline uses a
    model that’s designed for sentiment analysis, but it also supports multiclass
    and multilabel classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our pipeline, let’s generate some predictions! Each pipeline
    takes a string of text (or a list of strings) as input and returns a list of predictions.
    Each prediction is a Python dictionary, so we can use Pandas to display them nicely
    as a `Data⁠Frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|  | label | score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | NEGATIVE | 0.901546 |'
  prefs: []
  type: TYPE_TB
- en: In this case the model is very confident that the text has a negative sentiment,
    which makes sense given that we’re dealing with a complaint from an angry customer!
    Note that for sentiment analysis tasks the pipeline only returns one of the `POSITIVE`
    or `NEGATIVE` labels, since the other can be inferred by computing `1-score`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a look at another common task, identifying named entities in
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Predicting the sentiment of customer feedback is a good first step, but you
    often want to know if the feedback was about a particular item or service. In
    NLP, real-world objects like products, places, and people are called *named entities*,
    and extracting them from text is called *named entity recognition* (NER). We can
    apply NER by loading the corresponding pipeline and feeding our customer review
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|  | entity_group | score | word | start | end |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | ORG | 0.879010 | Amazon | 5 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | MISC | 0.990859 | Optimus Prime | 36 | 49 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | LOC | 0.999755 | Germany | 90 | 97 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | MISC | 0.556569 | Mega | 208 | 212 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | PER | 0.590256 | ##tron | 212 | 216 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | ORG | 0.669692 | Decept | 253 | 259 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | MISC | 0.498350 | ##icons | 259 | 264 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | MISC | 0.775361 | Megatron | 350 | 358 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | MISC | 0.987854 | Optimus Prime | 367 | 380 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | PER | 0.812096 | Bumblebee | 502 | 511 |'
  prefs: []
  type: TYPE_TB
- en: 'You can see that the pipeline detected all the entities and also assigned a
    category such as `ORG` (organization), `LOC` (location), or `PER` (person) to
    each of them. Here we used the `aggregation_strategy` argument to group the words
    according to the model’s predictions. For example, the entity “Optimus Prime”
    is composed of two words, but is assigned a single category: `MISC` (miscellaneous).
    The scores tell us how confident the model was about the entities it identified.
    We can see that it was least confident about “Decepticons” and the first occurrence
    of “Megatron”, both of which it failed to group as a single entity.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: See those weird hash symbols (`#`) in the `word` column in the previous table?
    These are produced by the model’s *tokenizer*, which splits words into atomic
    units called *tokens*. You’ll learn all about tokenization in [Chapter 2](ch02.xhtml#chapter_classification).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting all the named entities in a text is nice, but sometimes we would
    like to ask more targeted questions. This is where we can use *question answering*.
  prefs: []
  type: TYPE_NORMAL
- en: Question Answering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In question answering, we provide the model with a passage of text called the
    *context*, along with a question whose answer we’d like to extract. The model
    then returns the span of text corresponding to the answer. Let’s see what we get
    when we ask a specific question about our customer feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|  | score | start | end | answer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.631291 | 335 | 358 | an exchange of Megatron |'
  prefs: []
  type: TYPE_TB
- en: We can see that along with the answer, the pipeline also returned `start` and
    `end` integers that correspond to the character indices where the answer span
    was found (just like with NER tagging). There are several flavors of question
    answering that we will investigate in [Chapter 7](ch07.xhtml#chapter_qa), but
    this particular kind is called *extractive question answering* because the answer
    is extracted directly from the text.
  prefs: []
  type: TYPE_NORMAL
- en: With this approach you can read and extract relevant information quickly from
    a customer’s feedback. But what if you get a mountain of long-winded complaints
    and you don’t have the time to read them all? Let’s see if a summarization model
    can help!
  prefs: []
  type: TYPE_NORMAL
- en: Summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of text summarization is to take a long text as input and generate
    a short version with all the relevant facts. This is a much more complicated task
    than the previous ones since it requires the model to *generate* coherent text.
    In what should be a familiar pattern by now, we can instantiate a summarization
    pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This summary isn’t too bad! Although parts of the original text have been copied,
    the model was able to capture the essence of the problem and correctly identify
    that “Bumblebee” (which appeared at the end) was the author of the complaint.
    In this example you can also see that we passed some keyword arguments like `max_length`
    and `clean_up_tokenization_spaces` to the pipeline; these allow us to tweak the
    outputs at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: But what happens when you get feedback that is in a language you don’t understand?
    You could use Google Translate, or you can use your very own transformer to translate
    it for you!
  prefs: []
  type: TYPE_NORMAL
- en: Translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like summarization, translation is a task where the output consists of generated
    text. Let’s use a translation pipeline to translate an English text to German:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Again, the model produced a very good translation that correctly uses German’s
    formal pronouns, like “Ihrem” and “Sie.” Here we’ve also shown how you can override
    the default model in the pipeline to pick the best one for your application—and
    you can find models for thousands of language pairs on the Hugging Face Hub. Before
    we take a step back and look at the whole Hugging Face ecosystem, let’s examine
    one last application.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s say you would like to be able to provide faster replies to customer feedback
    by having access to an autocomplete function. With a text generation model you
    can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: OK, maybe we wouldn’t want to use this completion to calm Bumblebee down, but
    you get the general idea.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve seen a few cool applications of transformer models, you might
    be wondering where the training happens. All of the models that we’ve used in
    this chapter are publicly available and already fine-tuned for the task at hand.
    In general, however, you’ll want to fine-tune models on your own data, and in
    the following chapters you will learn how to do just that.
  prefs: []
  type: TYPE_NORMAL
- en: But training a model is just a small piece of any NLP project—being able to
    efficiently process data, share results with colleagues, and make your work reproducible
    are key components too. Fortunately, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    is surrounded by a big ecosystem of useful tools that support much of the modern
    machine learning workflow. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What started with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers has quickly
    grown into a whole ecosystem consisting of many libraries and tools to accelerate
    your NLP and machine learning projects. The Hugging Face ecosystem consists of
    mainly two parts: a family of libraries and the Hub, as shown in [Figure 1-9](#ecosystem).
    The libraries provide the code while the Hub provides the pretrained model weights,
    datasets, scripts for the evaluation metrics, and more. In this section we’ll
    have a brief look at the various components. We’ll skip ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, as we’ve already discussed it and we will see a lot more of it throughout
    the course of the book.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ecosystem](Images/nlpt_0109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. An overview of the Hugging Face ecosystem
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Hugging Face Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As outlined earlier, transfer learning is one of the key factors driving the
    success of transformers because it makes it possible to reuse pretrained models
    for new tasks. Consequently, it is crucial to be able to load pretrained models
    quickly and run experiments with them.
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face Hub hosts over 20,000 freely available models. As shown in
    [Figure 1-10](#hub-overview), there are filters for tasks, frameworks, datasets,
    and more that are designed to help you navigate the Hub and quickly find promising
    candidates. As we’ve seen with the pipelines, loading a promising model in your
    code is then literally just one line of code away. This makes experimenting with
    a wide range of models simple, and allows you to focus on the domain-specific
    parts of your project.
  prefs: []
  type: TYPE_NORMAL
- en: '![hub-overview](Images/nlpt_0110.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-10\. The Models page of the Hugging Face Hub, showing filters on the
    left and a list of models on the right
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to model weights, the Hub also hosts datasets and scripts for computing
    metrics, which let you reproduce published results or leverage additional data
    for your application.
  prefs: []
  type: TYPE_NORMAL
- en: The Hub also provides *model* and *dataset* *cards* to document the contents
    of models and datasets and help you make an informed decision about whether they’re
    the right ones for you. One of the coolest features of the Hub is that you can
    try out any model directly through the various task-specific interactive widgets
    as shown in [Figure 1-11](#hub-model-card).
  prefs: []
  type: TYPE_NORMAL
- en: '![hub-model-card](Images/nlpt_0111.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-11\. An example model card from the Hugging Face Hub: the inference
    widget, which allows you to interact with the model, is shown on the right'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s continue our tour with ![nlpt_pin01](Images/nlpt_pin01.png) Tokenizers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PyTorch](https://oreil.ly/AyTYC) and [TensorFlow](https://oreil.ly/JOKgq)
    also offer hubs of their own and are worth checking out if a particular model
    or dataset is not available on the Hugging Face Hub.'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Tokenizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Behind each of the pipeline examples that we’ve seen in this chapter is a tokenization
    step that splits the raw text into smaller pieces called tokens. We’ll see how
    this works in detail in [Chapter 2](ch02.xhtml#chapter_classification), but for
    now it’s enough to understand that tokens may be words, parts of words, or just
    characters like punctuation. Transformer models are trained on numerical representations
    of these tokens, so getting this step right is pretty important for the whole
    NLP project!
  prefs: []
  type: TYPE_NORMAL
- en: '![nlpt_pin01](Images/nlpt_pin01.png) [Tokenizers](https://oreil.ly/Z79jF) provides
    many tokenization strategies and is extremely fast at tokenizing text thanks to
    its Rust backend.^([12](ch01.xhtml#idm46238728732576)) It also takes care of all
    the pre- and postprocessing steps, such as normalizing the inputs and transforming
    the model outputs to the required format. With ![nlpt_pin01](Images/nlpt_pin01.png)
    Tokenizers, we can load a tokenizer in the same way we can load pretrained model
    weights with ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ Transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: We need a dataset and metrics to train and evaluate models, so let’s take a
    look at ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ Datasets, which is in charge of
    that aspect.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loading, processing, and storing datasets can be a cumbersome process, especially
    when the datasets get too large to fit in your laptop’s RAM. In addition, you
    usually need to implement various scripts to download the data and transform it
    into a standard format.
  prefs: []
  type: TYPE_NORMAL
- en: '![nlpt_pin01](Images/nlpt_pin01.png) [Datasets](https://oreil.ly/959YT) simplifies
    this process by providing a standard interface for thousands of datasets that
    can be found on the [Hub](https://oreil.ly/Rdhcu). It also provides smart caching
    (so you don’t have to redo your preprocessing each time you run your code) and
    avoids RAM limitations by leveraging a special mechanism called *memory mapping*
    that stores the contents of a file in virtual memory and enables multiple processes
    to modify a file more efficiently. The library is also interoperable with popular
    frameworks like Pandas and NumPy, so you don’t have to leave the comfort of your
    favorite data wrangling tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Having a good dataset and powerful model is worthless, however, if you can’t
    reliably measure the performance. Unfortunately, classic NLP metrics come with
    many different implementations that can vary slightly and lead to deceptive results.
    By providing the scripts for many metrics, ![nlpt_pin01](Images/nlpt_pin01.png)
    Datasets helps make experiments more reproducible and the results more trustworthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, ![nlpt_pin01](Images/nlpt_pin01.png)
    Tokenizers, and ![nlpt_pin01](Images/nlpt_pin01.png) Datasets libraries we have
    everything we need to train our very own transformer models! However, as we’ll
    see in [Chapter 10](ch10.xhtml#chapter_fromscratch) there are situations where
    we need fine-grained control over the training loop. That’s where the last library
    of the ecosystem comes into play: ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate.'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Accelerate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’ve ever had to write your own training script in PyTorch, chances are
    that you’ve had some headaches when trying to port the code that runs on your
    laptop to the code that runs on your organization’s cluster. ![nlpt_pin01](Images/nlpt_pin01.png)
    [Accelerate](https://oreil.ly/iRfDe) adds a layer of abstraction to your normal
    training loops that takes care of all the custom logic necessary for the training
    infrastructure. This literally accelerates your workflow by simplifying the change
    of infrastructure when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: This sums up the core components of Hugging Face’s open source ecosystem. But
    before wrapping up this chapter, let’s take a look at a few of the common challenges
    that come with trying to deploy transformers in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Main Challenges with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we’ve gotten a glimpse of the wide range of NLP tasks that
    can be tackled with transformer models. Reading the media headlines, it can sometimes
    sound like their capabilities are limitless. However, despite their usefulness,
    transformers are far from being a silver bullet. Here are a few challenges associated
    with them that we will explore throughout the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Language*'
  prefs: []
  type: TYPE_NORMAL
- en: NLP research is dominated by the English language. There are several models
    for other languages, but it is harder to find pretrained models for rare or low-resource
    languages. In [Chapter 4](ch04.xhtml#chapter_ner), we’ll explore multilingual
    transformers and their ability to perform zero-shot cross-lingual transfer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data availability*'
  prefs: []
  type: TYPE_NORMAL
- en: Although we can use transfer learning to dramatically reduce the amount of labeled
    training data our models need, it is still a lot compared to how much a human
    needs to perform the task. Tackling scenarios where you have little to no labeled
    data is the subject of [Chapter 9](ch09.xhtml#chapter_fewlabels).
  prefs: []
  type: TYPE_NORMAL
- en: '*Working with long documents*'
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention works extremely well on paragraph-long texts, but it becomes
    very expensive when we move to longer texts like whole documents. Approaches to
    mitigate this are discussed in [Chapter 11](ch11.xhtml#chapter_future).
  prefs: []
  type: TYPE_NORMAL
- en: '*Opacity*'
  prefs: []
  type: TYPE_NORMAL
- en: As with other deep learning models, transformers are to a large extent opaque.
    It is hard or impossible to unravel “why” a model made a certain prediction. This
    is an especially hard challenge when these models are deployed to make critical
    decisions. We’ll explore some ways to probe the errors of transformer models in
    Chapters [2](ch02.xhtml#chapter_classification) and [4](ch04.xhtml#chapter_ner).
  prefs: []
  type: TYPE_NORMAL
- en: '*Bias*'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models are predominantly pretrained on text data from the internet.
    This imprints all the biases that are present in the data into the models. Making
    sure that these are neither racist, sexist, or worse is a challenging task. We
    discuss some of these issues in more detail in [Chapter 10](ch10.xhtml#chapter_fromscratch).
  prefs: []
  type: TYPE_NORMAL
- en: Although daunting, many of these challenges can be overcome. As well as in the
    specific chapters mentioned, we will touch on these topics in almost every chapter
    ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, by now you are excited to learn how to start training and integrating
    these versatile models into your own applications! You’ve seen in this chapter
    that with just a few lines of code you can use state-of-the-art models for classification,
    named entity recognition, question answering, translation, and summarization,
    but this is really just the “tip of the iceberg.”
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters you will learn how to adapt transformers to a wide
    range of use cases, such as building a text classifier, or a lightweight model
    for production, or even training a language model from scratch. We’ll be taking
    a hands-on approach, which means that for every concept covered there will be
    accompanying code that you can run on Google Colab or your own GPU machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’re armed with the basic concepts behind transformers, it’s time
    to get our hands dirty with our first application: text classification. That’s
    the topic of the next chapter!'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch01.xhtml#idm46238735114624-marker)) A. Vaswani et al., [“Attention Is
    All You Need”](https://arxiv.org/abs/1706.03762), (2017). This title was so catchy
    that no less than [50 follow-up papers](https://oreil.ly/wT8Ih) have included
    “all you need” in their titles!
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch01.xhtml#idm46238735221744-marker)) J. Howard and S. Ruder, [“Universal
    Language Model Fine-Tuning for Text Classification”](https://arxiv.org/abs/1801.06146),
    (2018).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch01.xhtml#idm46238735216480-marker)) A. Radford et al., [“Improving Language
    Understanding by Generative Pre-Training”](https://openai.com/blog/language-unsupervised),
    (2018).
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch01.xhtml#idm46238735215072-marker)) J. Devlin et al., [“BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”](https://arxiv.org/abs/1810.04805),
    (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch01.xhtml#idm46238728352576-marker)) I. Sutskever, O. Vinyals, and Q.V.
    Le, [“Sequence to Sequence Learning with Neural Networks”](https://arxiv.org/abs/1409.3215),
    (2014).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch01.xhtml#idm46238728612528-marker)) D. Bahdanau, K. Cho, and Y. Bengio,
    [“Neural Machine Translation by Jointly Learning to Align and Translate”](https://arxiv.org/abs/1409.0473),
    (2014).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch01.xhtml#idm46238728434256-marker)) Weights are the learnable parameters
    of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch01.xhtml#idm46238727454704-marker)) A. Radford, R. Jozefowicz, and I.
    Sutskever, [“Learning to Generate Reviews and Discovering Sentiment”](https://arxiv.org/abs/1704.01444),
    (2017).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch01.xhtml#idm46238727453120-marker)) A related work at this time was
    ELMo (Embeddings from Language Models), which showed how pretraining LSTMs could
    produce high-quality word embeddings for downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch01.xhtml#idm46238727446112-marker)) This is more true for English than
    for most of the world’s languages, where obtaining a large corpus of digitized
    text can be difficult. Finding ways to bridge this gap is an active area of NLP
    research and activism.
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch01.xhtml#idm46238728418272-marker)) Y. Zhu et al., [“Aligning Books
    and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading
    Books”](https://arxiv.org/abs/1506.06724), (2015).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch01.xhtml#idm46238728732576-marker)) [Rust](https://rust-lang.org) is
    a high-performance programming language.
  prefs: []
  type: TYPE_NORMAL
