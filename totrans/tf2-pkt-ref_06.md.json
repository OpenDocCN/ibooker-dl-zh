["```py\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport numpy as np\nimport matplotlib.pylab as plt\n\n(train_images, train_labels), (test_images, test_labels) = \ndatasets.cifar10.load_data()\n```", "```py\nprint(type(train_images))\n```", "```py\n<class 'numpy.ndarray'>\n```", "```py\nprint(train_images.shape, train_labels.shape)\n```", "```py\n(50000, 32, 32, 3) (50000, 1)\n```", "```py\nprint(test_images.shape, test_labels.shape)\n```", "```py\n(10000, 32, 32, 3) (10000, 1)\n```", "```py\nCLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat',\n               'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n```", "```py\nunique, counts = np.unique(train_labels, return_counts=True)\n```", "```py\nprint(np.asarray((unique, counts)))\n```", "```py\n[[ 0 1 2 3 4 5 6 7 8 9]\n [5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]]\n```", "```py\nunique, counts = np.unique(test_labels, return_counts=True)\nprint(np.asarray((unique, counts)))\n```", "```py\n[[ 0 1 2 3 4 5 6 7 8 9]\n [1000 1000 1000 1000 1000 1000 1000 1000 1000 1000]]\n```", "```py\nselected_elements = random.sample(a_list, 25)\n```", "```py\nrange(len(train_labels))\n```", "```py\nlist(range(len(train_labels)))\n```", "```py\ntrain_idx = list(range(len(train_labels)))\n```", "```py\nimport random\nrandom.seed(2)\nrandom_sel = random.sample(train_idx, 25)\n```", "```py\n[3706,\n 6002,\n 5562,\n 23662,\n 11081,\n 48232,\n\u2026\n```", "```py\nplt.figure(figsize=(10,10))\nfor i in range(len(random_sel)):\n plt.subplot(5,5,i+1)\n plt.xticks([])\n plt.yticks([])\n plt.grid(False)\n plt.imshow(train_images[random_sel[i]], cmap=plt.cm.binary)\n plt.xlabel(CLASS_NAMES[train_labels[random_sel[i]][0]])\nplt.show()\n```", "```py\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[:500], test_labels[:500]))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[500:], test_labels[500:]))\n```", "```py\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n(train_images, train_labels))\n```", "```py\ntrain_dataset_size = len(list(train_dataset.as_numpy_iterator()))\nprint('Training data sample size: ', train_dataset_size)\n```", "```py\nTraining data sample size: 50000\n```", "```py\nTRAIN_BATCH_SIZE = 200\ntrain_dataset = train_dataset.shuffle(50000).batch(\nTRAIN_BATCH_SIZE)\n```", "```py\nvalidation_dataset = validation_dataset.batch(500)\ntest_dataset = test_dataset.batch(500)\n\nSTEPS_PER_EPOCH = train_dataset_size // TRAIN_BATCH_SIZE\nVALIDATION_STEPS = 1 #validation data // validation batch size\n```", "```py\nmodel = tf.keras.Sequential([\n tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu',\n kernel_initializer='glorot_uniform', padding='same', \n input_shape = (32,32,3)),\n tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu',\n kernel_initializer='glorot_uniform', \n padding='same'),\n tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n tf.keras.layers.Flatten(),\n tf.keras.layers.Dense(256, activation='relu', \n kernel_initializer='glorot_uniform'),\n tf.keras.layers.Dense(10, activation='softmax', \n name = 'custom_class')\n])\nmodel.build([None, 32, 32, 3])\n```", "```py\nmodel.compile(\n loss='sparse_categorical_crossentropy',\n optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, \n momentum=0.9),\n metrics=['accuracy'])\n```", "```py\ntf.keras.utils.plot_model(model, show_shapes=True)\n```", "```py\npip install pydot\npip install graphviz\n```", "```py\nhist = model.fit(\n train_dataset,\n epochs=5, steps_per_epoch=STEPS_PER_EPOCH,\n validation_data=validation_dataset,\n validation_steps=VALIDATION_STEPS).history\n```", "```py\nclass vehicle():\n def __init__(self, make, model, horsepower, weight):\n self.make = make\n self.model = model\n self.horsepower = horsepower\n self.weight = weight\n\n def horsepower_to_weight_ratio(self, horsepower, weight):\n hp_2_weight_ratio = horsepower / weight\n return hp_2_weight_ratio\n```", "```py\nclass truck(vehicle):\n def __init__(self, make, model, horsepower, weight, payload):\n super().__init__(make, model, horsepower, weight)\n self.payload = payload\n\n def __call__(self, horsepower, payload):\n hp_2_payload_ratio = horsepower / payload\n return hp_2_payload_ratio\n```", "```py\nMAKE = 'Tesla'\nMODEL = 'Cybertruck'\nHORSEPOWER = 800 #HP\nWEIGHT = 3000 #kg\nPAYLOAD = 1600 #kg\n\nMyTruck = truck(MAKE, MODEL, HORSEPOWER, WEIGHT, PAYLOAD)\n```", "```py\nprint('Make: ', MyTruck.make,\n '\\nModel: ', MyTruck.model,\n '\\nHorsepower (HP): ', MyTruck.horsepower,\n '\\nWeight (kg): ', MyTruck.weight,\n '\\nPayload (kg): ', MyTruck.payload)\n```", "```py\nMake: Tesla\nModel: Cybertruck\nHorsepower (HP): 800\nWeight (kg): 3000\nPayload (kg): 1600\n```", "```py\nMyTruck(HORSEPOWER, PAYLOAD)\n```", "```py\nMyTruck(HORSEPOWER, PAYLOAD)\n```", "```py\nMyTruck.horsepower_to_weight_ratio(HORSEPOWER, WEIGHT)\n```", "```py\nclass myModel(tf.keras.Model):\n def __init__(self, input_dim):\n super(myModel, self).__init__()\n self.conv2d_initial = tf.keras.layers.Conv2D(32, \n kernel_size=(3, 3),\n activation='relu',\n kernel_initializer='glorot_uniform',\n padding='same',\n input_shape = (input_dim,input_dim,3))\n self.cov2d_mid = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n activation='relu',\n kernel_initializer='glorot_uniform',\n padding='same')\n self.maxpool2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n self.flatten = tf.keras.layers.Flatten()\n self.dense = tf.keras.layers.Dense(256, activation='relu',\n kernel_initializer='glorot_uniform')\n self.fc = tf.keras.layers.Dense(10, activation='softmax',\n name = 'custom_class')\n\n def call(self, input_dim):\n x = self.conv2d_initial(input_dim)\n x = self.maxpool2d(x)\n x = self.cov2d_mid(x)\n x = self.maxpool2d(x)\n x = self.flatten(x)\n x = self.dense(x)\n x = self.fc(x)\n\n return x\n```", "```py\nself.conv2d_initial = tf.keras.layers.Conv2D(32, \n kernel_size=(3, 3),\n activation='relu',\n kernel_initializer='glorot_uniform',\n padding='same',\n input_shape = (input_dim,input_dim,3))\n```", "```py\nself.maxpool2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n```", "```py\nself.maxpool2d_2 = tf.keras.layers.MaxPooling2D(pool_size=(5, 5))\n```", "```py\ndef call(self, input_dim)\n```", "```py\nmdl = myModel(32)\n```", "```py\nmdl.compile(loss='sparse_categorical_crossentropy',\n optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, \n momentum=0.9),\n metrics=['accuracy'])\n```", "```py\nmdl_hist = mdl.fit(\n train_dataset,\n epochs=5, steps_per_epoch=STEPS_PER_EPOCH,\n validation_data=validation_dataset,\n validation_steps=VALIDATION_STEPS).history\n```", "```py\noptimizer = tf.keras.optimizers.SGD(\nlearning_rate=0.1, \n momentum=0.9)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\nfrom_logits=True)\n```", "```py\ntrain_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\nval_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n```", "```py\n@tf.function\ndef train_step(train_data, train_label):\n    with tf.GradientTape() as tape:\n    logits = model(train_data, training=True)\n    loss_value = loss_fn(train_label, logits)\n    grads = tape.gradient(loss_value, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    train_acc_metric.update_state(train_label, logits)\n    return loss_value\n```", "```py\nwith tf.GradientTape() as tape\n```", "```py\nlogits = model(train_data, training=True)\n```", "```py\nloss_value = loss_fn(train_label, logits)\n```", "```py\n@tf.function\ndef test_step(validation_data, validation_label):\n val_logits = model(validation_data, training=False)\n val_acc_metric.update_state(validation_label, val_logits)\n```", "```py\nimport time\n\nepochs = 2\nfor epoch in range(epochs):\n print(\"\\nStarting epoch %d\" % (epoch,))\n start_time = time.time()\n\n # Iterate dataset batches\n for step, (x_batch_train, y_batch_train) in \n enumerate(train_dataset):\n loss_value = train_step(x_batch_train, y_batch_train)\n\n    # In every 100 batches, log results.\n    if step % 100 == 0:\n         print(\n         \"Training loss (for one batch) at step %d: %.4f\"\n         % (step, float(loss_value))\n         )\n print(\"Sample processed so far: %d samples\" % \n ((step + 1) * TRAIN_BATCH_SIZE))\n\n # Show accuracy metrics after each epoch is completed\n train_accuracy = train_acc_metric.result()\n print(\"Training accuracy over epoch: %.4f\" % \n (float(train_accuracy),))\n\n # Reset training metrics before next epoch starts\n train_acc_metric.reset_states()\n\n # Test with validation data at end of each epoch\n for x_batch_val, y_batch_val in validation_dataset:\n test_step(x_batch_val, y_batch_val)\n\n val_accuracy = val_acc_metric.result()\n val_acc_metric.reset_states()\n print(\"Validation accuracy: %.4f\" % (float(val_accuracy),))\n print(\"Time taken: %.2fs\" % (time.time() - start_time))\n```"]